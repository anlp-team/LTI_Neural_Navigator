{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/A._Waibel_End-to-End_Evaluation_for_Low-Latency_Simultaneous_Speech_Translation_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What models were used for WavLM and BART?", "answer": " XLS-R models (Babu et al., 2021) for the encoder and the MBART-50 model (Liu et al., 2020b) for the decoder", "ref_chunk": "WavLM (Chen et al., 2022) and BART (Lewis et al., 2019)6, while for Multilin- gual ASR we utilized the XLS-R models (Babu et al., 2021) for the encoder and the MBART- 50 model (Liu et al., 2020b) for the decoder fol- lowing (Pham et al., 2022). On the other hand, the translation models are based on the pretrained 6With the recipe available at here. 5 8 C=0.5 Revision mode 3 4 C=2 30 Fixed mode 9Latency (s) 32 C=3 C=1 34 7 36BLEU score 6 Figure 5: caded model) C: LA2_chunk_size (s). Latency vs. the cas- in revision mode or fixed mode. quality (for DeltaLM (Ma et al., 2021). For the en\u2192X direc- tion, the models are fine-tuned to optimize for ACL talks based on Liu et al. (2023). For other direc- tions, DeltaLM is fine-tuned on the combination of commonly available datasets7. Finally, for the end-to-end ST system, we used the language-agnostic model from Huber et al. (2022) that can decode en-de ST and de ASR. 7 Results and Discussion 7.1 Quality vs Latency trade-off In the first experiment, we assess the trade-off be- tween translation quality and latency by modifying the LA2_chunk_size parameter. The results are shown in Table 2. As can be seen, as we increase chunk size, the translation quality improves while the latency gets worse, both for cascaded ST and end-to-end ST. This is expected, since higher chunk size means longer input given to the model at each step, thus the output has better quality due to hav- ing more context, while the latency gets worse due to more waiting time for collecting the input. 7.2 Revision mode vs fixed mode Second, we report the results of comparing the revision mode to the fixed mode with different LA2_chunk_size values when performing cas- caded translation on the en-de ACL dev set. As can be seen in Figure 5, in general, revision mode has better BLEU score yet worse latency than fixed mode. This is expected, since for the revision mode, when more input audio is available, the system can correct its previous output, thus ending up having better translation quality yet worse latency due to the additional re-translation overhead. 7Paracrawl, UNPC, EUBookshop, MultiUN, EuroPat, TildeMODEL, DGT, Europarl, QED and NewsCommentary. C=0.5 Cascaded ST C=2 4 3 7Latency (s) 30 1 C=1 End-to-End ST 25 35BLEU score 2 C=3 6 5 Figure 6: Latency vs. for C: LA2_chunk_size (s). quality (in revision mode) the cascaded ST or End-to-End ST model. 7.3 Cascaded vs End-to-End Third, we report the results of comparing the cas- caded setting to the end-to-end setting when per- forming online translation with revision mode on the ACL dev set. As can be seen in Figure 6, in general, cascaded ST has better BLEU score yet worse latency than end-to-end ST. Cascaded ST has worse latency since it contains two components and each component has to do computation. However, we observe that, with a similar latency of \u223c 3.5 seconds, cascaded ST still obtains a better BLEU score. On the other hand, end-to-end ST has a bet- ter minimum latency that can be achieved (almost two seconds lower than the cascaded system). 7.4 Load balancing In order to assess the system\u2019s capability to balance loads, we conduct experiments on running multi- ple sessions simultaneously using the same hosted model, with and without scaling the system\u2019s num- ber of middleware workers. For speech processing, we test parallel sessions on ACL dev en-de using the end-to-end ST model. For text processing, we test one cascaded ST session on ACL dev where the number of parallel sessions is the number of requested MT languages. In all experiments, we set LA2_chunk_size = 2. We report only the en-de results. The results are shown in Table 3. As expected, the latency gets worse as the number of parallel ses- sions increases. Using multiple middleware work- ers counteracts that to some extent by making sure that the backend model is always busy and not wait- ing for the next request. Furthermore, we see that when the number of parallel sessions increases, the flickering rate decreases. This is because during higher load, fewer requests are sent to the backend and we observe less flickering. Here our automatic load balancing can be seen in action. w s 1 1 2 5 1 2 5 5 Speech processing F \u2193 L \u2193 B \u2191 0.5 3.2 25.9 0.5 20.2 26.1 0.2 28.2 21.3 0.6 3.2 26.2 0.5 4.6 26.5 0.3 16.7 25.3 Text processing L \u2193 F \u2193 B \u2191 0.5 6.7 34.9 0.4 8.4 34.6 0.2 28.1 34.8 0.5 6.1 35.1 0.5 8.0 33.6 0.2 15.9 34.5 Table 3: Quality, latency and flickering rate when scaling the number of sessions (with one hosted model per number of middle- ware workers, s: number of parallel sessions, B: Qual- ity (BLEU score), L: Latency (s), F: Flickering rate. LA2_chunk_size is set to 2 seconds. language). w: 8 Related work SimulEval (Ma et al., 2020) provides an evaluation framework for low-latency simultaneous speech translation with a decoupled client-server archi- tecture allowing to plug-in translation models and stability detection policies. As the main difference we leave the audio segmentation up to the model whereas Ma et al. (2020) rely on a pre-segmentation of the audio, we factor in the computational latency in addition to the model latency and explore the scaling behavior in multi-session scenarios, both for a more realistic deployment scenario. Similar to this work Franceschini et al. (2020) implement a low-latency speech translation pipeline, however, their architecture does not scale well to multiple sessions and is not well suited for end-to-end eval- uation. 9 Limitations and Conclusion Since we run and evaluate the experiments in a realistic real-world scenario, it is difficult to exactly reproduce the results. The experiments are non- deterministic, e.g., because of network latencies. Furthermore, the results depend on the speed of the used hardware, especially the used hardware for the backend models. Additionally, we expect that each streaming algorithm implemented returns"}, {"question": " In the experiment assessing the trade-off between translation quality and latency, what parameter was modified?", "answer": " LA2_chunk_size parameter", "ref_chunk": "WavLM (Chen et al., 2022) and BART (Lewis et al., 2019)6, while for Multilin- gual ASR we utilized the XLS-R models (Babu et al., 2021) for the encoder and the MBART- 50 model (Liu et al., 2020b) for the decoder fol- lowing (Pham et al., 2022). On the other hand, the translation models are based on the pretrained 6With the recipe available at here. 5 8 C=0.5 Revision mode 3 4 C=2 30 Fixed mode 9Latency (s) 32 C=3 C=1 34 7 36BLEU score 6 Figure 5: caded model) C: LA2_chunk_size (s). Latency vs. the cas- in revision mode or fixed mode. quality (for DeltaLM (Ma et al., 2021). For the en\u2192X direc- tion, the models are fine-tuned to optimize for ACL talks based on Liu et al. (2023). For other direc- tions, DeltaLM is fine-tuned on the combination of commonly available datasets7. Finally, for the end-to-end ST system, we used the language-agnostic model from Huber et al. (2022) that can decode en-de ST and de ASR. 7 Results and Discussion 7.1 Quality vs Latency trade-off In the first experiment, we assess the trade-off be- tween translation quality and latency by modifying the LA2_chunk_size parameter. The results are shown in Table 2. As can be seen, as we increase chunk size, the translation quality improves while the latency gets worse, both for cascaded ST and end-to-end ST. This is expected, since higher chunk size means longer input given to the model at each step, thus the output has better quality due to hav- ing more context, while the latency gets worse due to more waiting time for collecting the input. 7.2 Revision mode vs fixed mode Second, we report the results of comparing the revision mode to the fixed mode with different LA2_chunk_size values when performing cas- caded translation on the en-de ACL dev set. As can be seen in Figure 5, in general, revision mode has better BLEU score yet worse latency than fixed mode. This is expected, since for the revision mode, when more input audio is available, the system can correct its previous output, thus ending up having better translation quality yet worse latency due to the additional re-translation overhead. 7Paracrawl, UNPC, EUBookshop, MultiUN, EuroPat, TildeMODEL, DGT, Europarl, QED and NewsCommentary. C=0.5 Cascaded ST C=2 4 3 7Latency (s) 30 1 C=1 End-to-End ST 25 35BLEU score 2 C=3 6 5 Figure 6: Latency vs. for C: LA2_chunk_size (s). quality (in revision mode) the cascaded ST or End-to-End ST model. 7.3 Cascaded vs End-to-End Third, we report the results of comparing the cas- caded setting to the end-to-end setting when per- forming online translation with revision mode on the ACL dev set. As can be seen in Figure 6, in general, cascaded ST has better BLEU score yet worse latency than end-to-end ST. Cascaded ST has worse latency since it contains two components and each component has to do computation. However, we observe that, with a similar latency of \u223c 3.5 seconds, cascaded ST still obtains a better BLEU score. On the other hand, end-to-end ST has a bet- ter minimum latency that can be achieved (almost two seconds lower than the cascaded system). 7.4 Load balancing In order to assess the system\u2019s capability to balance loads, we conduct experiments on running multi- ple sessions simultaneously using the same hosted model, with and without scaling the system\u2019s num- ber of middleware workers. For speech processing, we test parallel sessions on ACL dev en-de using the end-to-end ST model. For text processing, we test one cascaded ST session on ACL dev where the number of parallel sessions is the number of requested MT languages. In all experiments, we set LA2_chunk_size = 2. We report only the en-de results. The results are shown in Table 3. As expected, the latency gets worse as the number of parallel ses- sions increases. Using multiple middleware work- ers counteracts that to some extent by making sure that the backend model is always busy and not wait- ing for the next request. Furthermore, we see that when the number of parallel sessions increases, the flickering rate decreases. This is because during higher load, fewer requests are sent to the backend and we observe less flickering. Here our automatic load balancing can be seen in action. w s 1 1 2 5 1 2 5 5 Speech processing F \u2193 L \u2193 B \u2191 0.5 3.2 25.9 0.5 20.2 26.1 0.2 28.2 21.3 0.6 3.2 26.2 0.5 4.6 26.5 0.3 16.7 25.3 Text processing L \u2193 F \u2193 B \u2191 0.5 6.7 34.9 0.4 8.4 34.6 0.2 28.1 34.8 0.5 6.1 35.1 0.5 8.0 33.6 0.2 15.9 34.5 Table 3: Quality, latency and flickering rate when scaling the number of sessions (with one hosted model per number of middle- ware workers, s: number of parallel sessions, B: Qual- ity (BLEU score), L: Latency (s), F: Flickering rate. LA2_chunk_size is set to 2 seconds. language). w: 8 Related work SimulEval (Ma et al., 2020) provides an evaluation framework for low-latency simultaneous speech translation with a decoupled client-server archi- tecture allowing to plug-in translation models and stability detection policies. As the main difference we leave the audio segmentation up to the model whereas Ma et al. (2020) rely on a pre-segmentation of the audio, we factor in the computational latency in addition to the model latency and explore the scaling behavior in multi-session scenarios, both for a more realistic deployment scenario. Similar to this work Franceschini et al. (2020) implement a low-latency speech translation pipeline, however, their architecture does not scale well to multiple sessions and is not well suited for end-to-end eval- uation. 9 Limitations and Conclusion Since we run and evaluate the experiments in a realistic real-world scenario, it is difficult to exactly reproduce the results. The experiments are non- deterministic, e.g., because of network latencies. Furthermore, the results depend on the speed of the used hardware, especially the used hardware for the backend models. Additionally, we expect that each streaming algorithm implemented returns"}, {"question": " Why does the translation quality improve as the chunk size increases?", "answer": " More context is given to the model at each step", "ref_chunk": "WavLM (Chen et al., 2022) and BART (Lewis et al., 2019)6, while for Multilin- gual ASR we utilized the XLS-R models (Babu et al., 2021) for the encoder and the MBART- 50 model (Liu et al., 2020b) for the decoder fol- lowing (Pham et al., 2022). On the other hand, the translation models are based on the pretrained 6With the recipe available at here. 5 8 C=0.5 Revision mode 3 4 C=2 30 Fixed mode 9Latency (s) 32 C=3 C=1 34 7 36BLEU score 6 Figure 5: caded model) C: LA2_chunk_size (s). Latency vs. the cas- in revision mode or fixed mode. quality (for DeltaLM (Ma et al., 2021). For the en\u2192X direc- tion, the models are fine-tuned to optimize for ACL talks based on Liu et al. (2023). For other direc- tions, DeltaLM is fine-tuned on the combination of commonly available datasets7. Finally, for the end-to-end ST system, we used the language-agnostic model from Huber et al. (2022) that can decode en-de ST and de ASR. 7 Results and Discussion 7.1 Quality vs Latency trade-off In the first experiment, we assess the trade-off be- tween translation quality and latency by modifying the LA2_chunk_size parameter. The results are shown in Table 2. As can be seen, as we increase chunk size, the translation quality improves while the latency gets worse, both for cascaded ST and end-to-end ST. This is expected, since higher chunk size means longer input given to the model at each step, thus the output has better quality due to hav- ing more context, while the latency gets worse due to more waiting time for collecting the input. 7.2 Revision mode vs fixed mode Second, we report the results of comparing the revision mode to the fixed mode with different LA2_chunk_size values when performing cas- caded translation on the en-de ACL dev set. As can be seen in Figure 5, in general, revision mode has better BLEU score yet worse latency than fixed mode. This is expected, since for the revision mode, when more input audio is available, the system can correct its previous output, thus ending up having better translation quality yet worse latency due to the additional re-translation overhead. 7Paracrawl, UNPC, EUBookshop, MultiUN, EuroPat, TildeMODEL, DGT, Europarl, QED and NewsCommentary. C=0.5 Cascaded ST C=2 4 3 7Latency (s) 30 1 C=1 End-to-End ST 25 35BLEU score 2 C=3 6 5 Figure 6: Latency vs. for C: LA2_chunk_size (s). quality (in revision mode) the cascaded ST or End-to-End ST model. 7.3 Cascaded vs End-to-End Third, we report the results of comparing the cas- caded setting to the end-to-end setting when per- forming online translation with revision mode on the ACL dev set. As can be seen in Figure 6, in general, cascaded ST has better BLEU score yet worse latency than end-to-end ST. Cascaded ST has worse latency since it contains two components and each component has to do computation. However, we observe that, with a similar latency of \u223c 3.5 seconds, cascaded ST still obtains a better BLEU score. On the other hand, end-to-end ST has a bet- ter minimum latency that can be achieved (almost two seconds lower than the cascaded system). 7.4 Load balancing In order to assess the system\u2019s capability to balance loads, we conduct experiments on running multi- ple sessions simultaneously using the same hosted model, with and without scaling the system\u2019s num- ber of middleware workers. For speech processing, we test parallel sessions on ACL dev en-de using the end-to-end ST model. For text processing, we test one cascaded ST session on ACL dev where the number of parallel sessions is the number of requested MT languages. In all experiments, we set LA2_chunk_size = 2. We report only the en-de results. The results are shown in Table 3. As expected, the latency gets worse as the number of parallel ses- sions increases. Using multiple middleware work- ers counteracts that to some extent by making sure that the backend model is always busy and not wait- ing for the next request. Furthermore, we see that when the number of parallel sessions increases, the flickering rate decreases. This is because during higher load, fewer requests are sent to the backend and we observe less flickering. Here our automatic load balancing can be seen in action. w s 1 1 2 5 1 2 5 5 Speech processing F \u2193 L \u2193 B \u2191 0.5 3.2 25.9 0.5 20.2 26.1 0.2 28.2 21.3 0.6 3.2 26.2 0.5 4.6 26.5 0.3 16.7 25.3 Text processing L \u2193 F \u2193 B \u2191 0.5 6.7 34.9 0.4 8.4 34.6 0.2 28.1 34.8 0.5 6.1 35.1 0.5 8.0 33.6 0.2 15.9 34.5 Table 3: Quality, latency and flickering rate when scaling the number of sessions (with one hosted model per number of middle- ware workers, s: number of parallel sessions, B: Qual- ity (BLEU score), L: Latency (s), F: Flickering rate. LA2_chunk_size is set to 2 seconds. language). w: 8 Related work SimulEval (Ma et al., 2020) provides an evaluation framework for low-latency simultaneous speech translation with a decoupled client-server archi- tecture allowing to plug-in translation models and stability detection policies. As the main difference we leave the audio segmentation up to the model whereas Ma et al. (2020) rely on a pre-segmentation of the audio, we factor in the computational latency in addition to the model latency and explore the scaling behavior in multi-session scenarios, both for a more realistic deployment scenario. Similar to this work Franceschini et al. (2020) implement a low-latency speech translation pipeline, however, their architecture does not scale well to multiple sessions and is not well suited for end-to-end eval- uation. 9 Limitations and Conclusion Since we run and evaluate the experiments in a realistic real-world scenario, it is difficult to exactly reproduce the results. The experiments are non- deterministic, e.g., because of network latencies. Furthermore, the results depend on the speed of the used hardware, especially the used hardware for the backend models. Additionally, we expect that each streaming algorithm implemented returns"}, {"question": " What are the results of comparing revision mode to fixed mode in terms of BLEU score and latency?", "answer": " Revision mode has better BLEU score but worse latency than fixed mode", "ref_chunk": "WavLM (Chen et al., 2022) and BART (Lewis et al., 2019)6, while for Multilin- gual ASR we utilized the XLS-R models (Babu et al., 2021) for the encoder and the MBART- 50 model (Liu et al., 2020b) for the decoder fol- lowing (Pham et al., 2022). On the other hand, the translation models are based on the pretrained 6With the recipe available at here. 5 8 C=0.5 Revision mode 3 4 C=2 30 Fixed mode 9Latency (s) 32 C=3 C=1 34 7 36BLEU score 6 Figure 5: caded model) C: LA2_chunk_size (s). Latency vs. the cas- in revision mode or fixed mode. quality (for DeltaLM (Ma et al., 2021). For the en\u2192X direc- tion, the models are fine-tuned to optimize for ACL talks based on Liu et al. (2023). For other direc- tions, DeltaLM is fine-tuned on the combination of commonly available datasets7. Finally, for the end-to-end ST system, we used the language-agnostic model from Huber et al. (2022) that can decode en-de ST and de ASR. 7 Results and Discussion 7.1 Quality vs Latency trade-off In the first experiment, we assess the trade-off be- tween translation quality and latency by modifying the LA2_chunk_size parameter. The results are shown in Table 2. As can be seen, as we increase chunk size, the translation quality improves while the latency gets worse, both for cascaded ST and end-to-end ST. This is expected, since higher chunk size means longer input given to the model at each step, thus the output has better quality due to hav- ing more context, while the latency gets worse due to more waiting time for collecting the input. 7.2 Revision mode vs fixed mode Second, we report the results of comparing the revision mode to the fixed mode with different LA2_chunk_size values when performing cas- caded translation on the en-de ACL dev set. As can be seen in Figure 5, in general, revision mode has better BLEU score yet worse latency than fixed mode. This is expected, since for the revision mode, when more input audio is available, the system can correct its previous output, thus ending up having better translation quality yet worse latency due to the additional re-translation overhead. 7Paracrawl, UNPC, EUBookshop, MultiUN, EuroPat, TildeMODEL, DGT, Europarl, QED and NewsCommentary. C=0.5 Cascaded ST C=2 4 3 7Latency (s) 30 1 C=1 End-to-End ST 25 35BLEU score 2 C=3 6 5 Figure 6: Latency vs. for C: LA2_chunk_size (s). quality (in revision mode) the cascaded ST or End-to-End ST model. 7.3 Cascaded vs End-to-End Third, we report the results of comparing the cas- caded setting to the end-to-end setting when per- forming online translation with revision mode on the ACL dev set. As can be seen in Figure 6, in general, cascaded ST has better BLEU score yet worse latency than end-to-end ST. Cascaded ST has worse latency since it contains two components and each component has to do computation. However, we observe that, with a similar latency of \u223c 3.5 seconds, cascaded ST still obtains a better BLEU score. On the other hand, end-to-end ST has a bet- ter minimum latency that can be achieved (almost two seconds lower than the cascaded system). 7.4 Load balancing In order to assess the system\u2019s capability to balance loads, we conduct experiments on running multi- ple sessions simultaneously using the same hosted model, with and without scaling the system\u2019s num- ber of middleware workers. For speech processing, we test parallel sessions on ACL dev en-de using the end-to-end ST model. For text processing, we test one cascaded ST session on ACL dev where the number of parallel sessions is the number of requested MT languages. In all experiments, we set LA2_chunk_size = 2. We report only the en-de results. The results are shown in Table 3. As expected, the latency gets worse as the number of parallel ses- sions increases. Using multiple middleware work- ers counteracts that to some extent by making sure that the backend model is always busy and not wait- ing for the next request. Furthermore, we see that when the number of parallel sessions increases, the flickering rate decreases. This is because during higher load, fewer requests are sent to the backend and we observe less flickering. Here our automatic load balancing can be seen in action. w s 1 1 2 5 1 2 5 5 Speech processing F \u2193 L \u2193 B \u2191 0.5 3.2 25.9 0.5 20.2 26.1 0.2 28.2 21.3 0.6 3.2 26.2 0.5 4.6 26.5 0.3 16.7 25.3 Text processing L \u2193 F \u2193 B \u2191 0.5 6.7 34.9 0.4 8.4 34.6 0.2 28.1 34.8 0.5 6.1 35.1 0.5 8.0 33.6 0.2 15.9 34.5 Table 3: Quality, latency and flickering rate when scaling the number of sessions (with one hosted model per number of middle- ware workers, s: number of parallel sessions, B: Qual- ity (BLEU score), L: Latency (s), F: Flickering rate. LA2_chunk_size is set to 2 seconds. language). w: 8 Related work SimulEval (Ma et al., 2020) provides an evaluation framework for low-latency simultaneous speech translation with a decoupled client-server archi- tecture allowing to plug-in translation models and stability detection policies. As the main difference we leave the audio segmentation up to the model whereas Ma et al. (2020) rely on a pre-segmentation of the audio, we factor in the computational latency in addition to the model latency and explore the scaling behavior in multi-session scenarios, both for a more realistic deployment scenario. Similar to this work Franceschini et al. (2020) implement a low-latency speech translation pipeline, however, their architecture does not scale well to multiple sessions and is not well suited for end-to-end eval- uation. 9 Limitations and Conclusion Since we run and evaluate the experiments in a realistic real-world scenario, it is difficult to exactly reproduce the results. The experiments are non- deterministic, e.g., because of network latencies. Furthermore, the results depend on the speed of the used hardware, especially the used hardware for the backend models. Additionally, we expect that each streaming algorithm implemented returns"}, {"question": " What are some of the common datasets used for fine-tuning DeltaLM models?", "answer": " Paracrawl, UNPC, EUBookshop, MultiUN, EuroPat, TildeMODEL, DGT, Europarl, QED and NewsCommentary", "ref_chunk": "WavLM (Chen et al., 2022) and BART (Lewis et al., 2019)6, while for Multilin- gual ASR we utilized the XLS-R models (Babu et al., 2021) for the encoder and the MBART- 50 model (Liu et al., 2020b) for the decoder fol- lowing (Pham et al., 2022). On the other hand, the translation models are based on the pretrained 6With the recipe available at here. 5 8 C=0.5 Revision mode 3 4 C=2 30 Fixed mode 9Latency (s) 32 C=3 C=1 34 7 36BLEU score 6 Figure 5: caded model) C: LA2_chunk_size (s). Latency vs. the cas- in revision mode or fixed mode. quality (for DeltaLM (Ma et al., 2021). For the en\u2192X direc- tion, the models are fine-tuned to optimize for ACL talks based on Liu et al. (2023). For other direc- tions, DeltaLM is fine-tuned on the combination of commonly available datasets7. Finally, for the end-to-end ST system, we used the language-agnostic model from Huber et al. (2022) that can decode en-de ST and de ASR. 7 Results and Discussion 7.1 Quality vs Latency trade-off In the first experiment, we assess the trade-off be- tween translation quality and latency by modifying the LA2_chunk_size parameter. The results are shown in Table 2. As can be seen, as we increase chunk size, the translation quality improves while the latency gets worse, both for cascaded ST and end-to-end ST. This is expected, since higher chunk size means longer input given to the model at each step, thus the output has better quality due to hav- ing more context, while the latency gets worse due to more waiting time for collecting the input. 7.2 Revision mode vs fixed mode Second, we report the results of comparing the revision mode to the fixed mode with different LA2_chunk_size values when performing cas- caded translation on the en-de ACL dev set. As can be seen in Figure 5, in general, revision mode has better BLEU score yet worse latency than fixed mode. This is expected, since for the revision mode, when more input audio is available, the system can correct its previous output, thus ending up having better translation quality yet worse latency due to the additional re-translation overhead. 7Paracrawl, UNPC, EUBookshop, MultiUN, EuroPat, TildeMODEL, DGT, Europarl, QED and NewsCommentary. C=0.5 Cascaded ST C=2 4 3 7Latency (s) 30 1 C=1 End-to-End ST 25 35BLEU score 2 C=3 6 5 Figure 6: Latency vs. for C: LA2_chunk_size (s). quality (in revision mode) the cascaded ST or End-to-End ST model. 7.3 Cascaded vs End-to-End Third, we report the results of comparing the cas- caded setting to the end-to-end setting when per- forming online translation with revision mode on the ACL dev set. As can be seen in Figure 6, in general, cascaded ST has better BLEU score yet worse latency than end-to-end ST. Cascaded ST has worse latency since it contains two components and each component has to do computation. However, we observe that, with a similar latency of \u223c 3.5 seconds, cascaded ST still obtains a better BLEU score. On the other hand, end-to-end ST has a bet- ter minimum latency that can be achieved (almost two seconds lower than the cascaded system). 7.4 Load balancing In order to assess the system\u2019s capability to balance loads, we conduct experiments on running multi- ple sessions simultaneously using the same hosted model, with and without scaling the system\u2019s num- ber of middleware workers. For speech processing, we test parallel sessions on ACL dev en-de using the end-to-end ST model. For text processing, we test one cascaded ST session on ACL dev where the number of parallel sessions is the number of requested MT languages. In all experiments, we set LA2_chunk_size = 2. We report only the en-de results. The results are shown in Table 3. As expected, the latency gets worse as the number of parallel ses- sions increases. Using multiple middleware work- ers counteracts that to some extent by making sure that the backend model is always busy and not wait- ing for the next request. Furthermore, we see that when the number of parallel sessions increases, the flickering rate decreases. This is because during higher load, fewer requests are sent to the backend and we observe less flickering. Here our automatic load balancing can be seen in action. w s 1 1 2 5 1 2 5 5 Speech processing F \u2193 L \u2193 B \u2191 0.5 3.2 25.9 0.5 20.2 26.1 0.2 28.2 21.3 0.6 3.2 26.2 0.5 4.6 26.5 0.3 16.7 25.3 Text processing L \u2193 F \u2193 B \u2191 0.5 6.7 34.9 0.4 8.4 34.6 0.2 28.1 34.8 0.5 6.1 35.1 0.5 8.0 33.6 0.2 15.9 34.5 Table 3: Quality, latency and flickering rate when scaling the number of sessions (with one hosted model per number of middle- ware workers, s: number of parallel sessions, B: Qual- ity (BLEU score), L: Latency (s), F: Flickering rate. LA2_chunk_size is set to 2 seconds. language). w: 8 Related work SimulEval (Ma et al., 2020) provides an evaluation framework for low-latency simultaneous speech translation with a decoupled client-server archi- tecture allowing to plug-in translation models and stability detection policies. As the main difference we leave the audio segmentation up to the model whereas Ma et al. (2020) rely on a pre-segmentation of the audio, we factor in the computational latency in addition to the model latency and explore the scaling behavior in multi-session scenarios, both for a more realistic deployment scenario. Similar to this work Franceschini et al. (2020) implement a low-latency speech translation pipeline, however, their architecture does not scale well to multiple sessions and is not well suited for end-to-end eval- uation. 9 Limitations and Conclusion Since we run and evaluate the experiments in a realistic real-world scenario, it is difficult to exactly reproduce the results. The experiments are non- deterministic, e.g., because of network latencies. Furthermore, the results depend on the speed of the used hardware, especially the used hardware for the backend models. Additionally, we expect that each streaming algorithm implemented returns"}, {"question": " What is the advantage of cascaded ST over end-to-end ST in terms of latency?", "answer": " End-to-end ST has a better minimum latency that can be achieved", "ref_chunk": "WavLM (Chen et al., 2022) and BART (Lewis et al., 2019)6, while for Multilin- gual ASR we utilized the XLS-R models (Babu et al., 2021) for the encoder and the MBART- 50 model (Liu et al., 2020b) for the decoder fol- lowing (Pham et al., 2022). On the other hand, the translation models are based on the pretrained 6With the recipe available at here. 5 8 C=0.5 Revision mode 3 4 C=2 30 Fixed mode 9Latency (s) 32 C=3 C=1 34 7 36BLEU score 6 Figure 5: caded model) C: LA2_chunk_size (s). Latency vs. the cas- in revision mode or fixed mode. quality (for DeltaLM (Ma et al., 2021). For the en\u2192X direc- tion, the models are fine-tuned to optimize for ACL talks based on Liu et al. (2023). For other direc- tions, DeltaLM is fine-tuned on the combination of commonly available datasets7. Finally, for the end-to-end ST system, we used the language-agnostic model from Huber et al. (2022) that can decode en-de ST and de ASR. 7 Results and Discussion 7.1 Quality vs Latency trade-off In the first experiment, we assess the trade-off be- tween translation quality and latency by modifying the LA2_chunk_size parameter. The results are shown in Table 2. As can be seen, as we increase chunk size, the translation quality improves while the latency gets worse, both for cascaded ST and end-to-end ST. This is expected, since higher chunk size means longer input given to the model at each step, thus the output has better quality due to hav- ing more context, while the latency gets worse due to more waiting time for collecting the input. 7.2 Revision mode vs fixed mode Second, we report the results of comparing the revision mode to the fixed mode with different LA2_chunk_size values when performing cas- caded translation on the en-de ACL dev set. As can be seen in Figure 5, in general, revision mode has better BLEU score yet worse latency than fixed mode. This is expected, since for the revision mode, when more input audio is available, the system can correct its previous output, thus ending up having better translation quality yet worse latency due to the additional re-translation overhead. 7Paracrawl, UNPC, EUBookshop, MultiUN, EuroPat, TildeMODEL, DGT, Europarl, QED and NewsCommentary. C=0.5 Cascaded ST C=2 4 3 7Latency (s) 30 1 C=1 End-to-End ST 25 35BLEU score 2 C=3 6 5 Figure 6: Latency vs. for C: LA2_chunk_size (s). quality (in revision mode) the cascaded ST or End-to-End ST model. 7.3 Cascaded vs End-to-End Third, we report the results of comparing the cas- caded setting to the end-to-end setting when per- forming online translation with revision mode on the ACL dev set. As can be seen in Figure 6, in general, cascaded ST has better BLEU score yet worse latency than end-to-end ST. Cascaded ST has worse latency since it contains two components and each component has to do computation. However, we observe that, with a similar latency of \u223c 3.5 seconds, cascaded ST still obtains a better BLEU score. On the other hand, end-to-end ST has a bet- ter minimum latency that can be achieved (almost two seconds lower than the cascaded system). 7.4 Load balancing In order to assess the system\u2019s capability to balance loads, we conduct experiments on running multi- ple sessions simultaneously using the same hosted model, with and without scaling the system\u2019s num- ber of middleware workers. For speech processing, we test parallel sessions on ACL dev en-de using the end-to-end ST model. For text processing, we test one cascaded ST session on ACL dev where the number of parallel sessions is the number of requested MT languages. In all experiments, we set LA2_chunk_size = 2. We report only the en-de results. The results are shown in Table 3. As expected, the latency gets worse as the number of parallel ses- sions increases. Using multiple middleware work- ers counteracts that to some extent by making sure that the backend model is always busy and not wait- ing for the next request. Furthermore, we see that when the number of parallel sessions increases, the flickering rate decreases. This is because during higher load, fewer requests are sent to the backend and we observe less flickering. Here our automatic load balancing can be seen in action. w s 1 1 2 5 1 2 5 5 Speech processing F \u2193 L \u2193 B \u2191 0.5 3.2 25.9 0.5 20.2 26.1 0.2 28.2 21.3 0.6 3.2 26.2 0.5 4.6 26.5 0.3 16.7 25.3 Text processing L \u2193 F \u2193 B \u2191 0.5 6.7 34.9 0.4 8.4 34.6 0.2 28.1 34.8 0.5 6.1 35.1 0.5 8.0 33.6 0.2 15.9 34.5 Table 3: Quality, latency and flickering rate when scaling the number of sessions (with one hosted model per number of middle- ware workers, s: number of parallel sessions, B: Qual- ity (BLEU score), L: Latency (s), F: Flickering rate. LA2_chunk_size is set to 2 seconds. language). w: 8 Related work SimulEval (Ma et al., 2020) provides an evaluation framework for low-latency simultaneous speech translation with a decoupled client-server archi- tecture allowing to plug-in translation models and stability detection policies. As the main difference we leave the audio segmentation up to the model whereas Ma et al. (2020) rely on a pre-segmentation of the audio, we factor in the computational latency in addition to the model latency and explore the scaling behavior in multi-session scenarios, both for a more realistic deployment scenario. Similar to this work Franceschini et al. (2020) implement a low-latency speech translation pipeline, however, their architecture does not scale well to multiple sessions and is not well suited for end-to-end eval- uation. 9 Limitations and Conclusion Since we run and evaluate the experiments in a realistic real-world scenario, it is difficult to exactly reproduce the results. The experiments are non- deterministic, e.g., because of network latencies. Furthermore, the results depend on the speed of the used hardware, especially the used hardware for the backend models. Additionally, we expect that each streaming algorithm implemented returns"}, {"question": " What do the results of the load balancing experiment show when the number of parallel sessions increases?", "answer": " The latency gets worse, but using multiple middleware workers helps counteract that", "ref_chunk": "WavLM (Chen et al., 2022) and BART (Lewis et al., 2019)6, while for Multilin- gual ASR we utilized the XLS-R models (Babu et al., 2021) for the encoder and the MBART- 50 model (Liu et al., 2020b) for the decoder fol- lowing (Pham et al., 2022). On the other hand, the translation models are based on the pretrained 6With the recipe available at here. 5 8 C=0.5 Revision mode 3 4 C=2 30 Fixed mode 9Latency (s) 32 C=3 C=1 34 7 36BLEU score 6 Figure 5: caded model) C: LA2_chunk_size (s). Latency vs. the cas- in revision mode or fixed mode. quality (for DeltaLM (Ma et al., 2021). For the en\u2192X direc- tion, the models are fine-tuned to optimize for ACL talks based on Liu et al. (2023). For other direc- tions, DeltaLM is fine-tuned on the combination of commonly available datasets7. Finally, for the end-to-end ST system, we used the language-agnostic model from Huber et al. (2022) that can decode en-de ST and de ASR. 7 Results and Discussion 7.1 Quality vs Latency trade-off In the first experiment, we assess the trade-off be- tween translation quality and latency by modifying the LA2_chunk_size parameter. The results are shown in Table 2. As can be seen, as we increase chunk size, the translation quality improves while the latency gets worse, both for cascaded ST and end-to-end ST. This is expected, since higher chunk size means longer input given to the model at each step, thus the output has better quality due to hav- ing more context, while the latency gets worse due to more waiting time for collecting the input. 7.2 Revision mode vs fixed mode Second, we report the results of comparing the revision mode to the fixed mode with different LA2_chunk_size values when performing cas- caded translation on the en-de ACL dev set. As can be seen in Figure 5, in general, revision mode has better BLEU score yet worse latency than fixed mode. This is expected, since for the revision mode, when more input audio is available, the system can correct its previous output, thus ending up having better translation quality yet worse latency due to the additional re-translation overhead. 7Paracrawl, UNPC, EUBookshop, MultiUN, EuroPat, TildeMODEL, DGT, Europarl, QED and NewsCommentary. C=0.5 Cascaded ST C=2 4 3 7Latency (s) 30 1 C=1 End-to-End ST 25 35BLEU score 2 C=3 6 5 Figure 6: Latency vs. for C: LA2_chunk_size (s). quality (in revision mode) the cascaded ST or End-to-End ST model. 7.3 Cascaded vs End-to-End Third, we report the results of comparing the cas- caded setting to the end-to-end setting when per- forming online translation with revision mode on the ACL dev set. As can be seen in Figure 6, in general, cascaded ST has better BLEU score yet worse latency than end-to-end ST. Cascaded ST has worse latency since it contains two components and each component has to do computation. However, we observe that, with a similar latency of \u223c 3.5 seconds, cascaded ST still obtains a better BLEU score. On the other hand, end-to-end ST has a bet- ter minimum latency that can be achieved (almost two seconds lower than the cascaded system). 7.4 Load balancing In order to assess the system\u2019s capability to balance loads, we conduct experiments on running multi- ple sessions simultaneously using the same hosted model, with and without scaling the system\u2019s num- ber of middleware workers. For speech processing, we test parallel sessions on ACL dev en-de using the end-to-end ST model. For text processing, we test one cascaded ST session on ACL dev where the number of parallel sessions is the number of requested MT languages. In all experiments, we set LA2_chunk_size = 2. We report only the en-de results. The results are shown in Table 3. As expected, the latency gets worse as the number of parallel ses- sions increases. Using multiple middleware work- ers counteracts that to some extent by making sure that the backend model is always busy and not wait- ing for the next request. Furthermore, we see that when the number of parallel sessions increases, the flickering rate decreases. This is because during higher load, fewer requests are sent to the backend and we observe less flickering. Here our automatic load balancing can be seen in action. w s 1 1 2 5 1 2 5 5 Speech processing F \u2193 L \u2193 B \u2191 0.5 3.2 25.9 0.5 20.2 26.1 0.2 28.2 21.3 0.6 3.2 26.2 0.5 4.6 26.5 0.3 16.7 25.3 Text processing L \u2193 F \u2193 B \u2191 0.5 6.7 34.9 0.4 8.4 34.6 0.2 28.1 34.8 0.5 6.1 35.1 0.5 8.0 33.6 0.2 15.9 34.5 Table 3: Quality, latency and flickering rate when scaling the number of sessions (with one hosted model per number of middle- ware workers, s: number of parallel sessions, B: Qual- ity (BLEU score), L: Latency (s), F: Flickering rate. LA2_chunk_size is set to 2 seconds. language). w: 8 Related work SimulEval (Ma et al., 2020) provides an evaluation framework for low-latency simultaneous speech translation with a decoupled client-server archi- tecture allowing to plug-in translation models and stability detection policies. As the main difference we leave the audio segmentation up to the model whereas Ma et al. (2020) rely on a pre-segmentation of the audio, we factor in the computational latency in addition to the model latency and explore the scaling behavior in multi-session scenarios, both for a more realistic deployment scenario. Similar to this work Franceschini et al. (2020) implement a low-latency speech translation pipeline, however, their architecture does not scale well to multiple sessions and is not well suited for end-to-end eval- uation. 9 Limitations and Conclusion Since we run and evaluate the experiments in a realistic real-world scenario, it is difficult to exactly reproduce the results. The experiments are non- deterministic, e.g., because of network latencies. Furthermore, the results depend on the speed of the used hardware, especially the used hardware for the backend models. Additionally, we expect that each streaming algorithm implemented returns"}, {"question": " What evaluation framework provides low-latency simultaneous speech translation with a decoupled client-server architecture?", "answer": " SimulEval (Ma et al., 2020)", "ref_chunk": "WavLM (Chen et al., 2022) and BART (Lewis et al., 2019)6, while for Multilin- gual ASR we utilized the XLS-R models (Babu et al., 2021) for the encoder and the MBART- 50 model (Liu et al., 2020b) for the decoder fol- lowing (Pham et al., 2022). On the other hand, the translation models are based on the pretrained 6With the recipe available at here. 5 8 C=0.5 Revision mode 3 4 C=2 30 Fixed mode 9Latency (s) 32 C=3 C=1 34 7 36BLEU score 6 Figure 5: caded model) C: LA2_chunk_size (s). Latency vs. the cas- in revision mode or fixed mode. quality (for DeltaLM (Ma et al., 2021). For the en\u2192X direc- tion, the models are fine-tuned to optimize for ACL talks based on Liu et al. (2023). For other direc- tions, DeltaLM is fine-tuned on the combination of commonly available datasets7. Finally, for the end-to-end ST system, we used the language-agnostic model from Huber et al. (2022) that can decode en-de ST and de ASR. 7 Results and Discussion 7.1 Quality vs Latency trade-off In the first experiment, we assess the trade-off be- tween translation quality and latency by modifying the LA2_chunk_size parameter. The results are shown in Table 2. As can be seen, as we increase chunk size, the translation quality improves while the latency gets worse, both for cascaded ST and end-to-end ST. This is expected, since higher chunk size means longer input given to the model at each step, thus the output has better quality due to hav- ing more context, while the latency gets worse due to more waiting time for collecting the input. 7.2 Revision mode vs fixed mode Second, we report the results of comparing the revision mode to the fixed mode with different LA2_chunk_size values when performing cas- caded translation on the en-de ACL dev set. As can be seen in Figure 5, in general, revision mode has better BLEU score yet worse latency than fixed mode. This is expected, since for the revision mode, when more input audio is available, the system can correct its previous output, thus ending up having better translation quality yet worse latency due to the additional re-translation overhead. 7Paracrawl, UNPC, EUBookshop, MultiUN, EuroPat, TildeMODEL, DGT, Europarl, QED and NewsCommentary. C=0.5 Cascaded ST C=2 4 3 7Latency (s) 30 1 C=1 End-to-End ST 25 35BLEU score 2 C=3 6 5 Figure 6: Latency vs. for C: LA2_chunk_size (s). quality (in revision mode) the cascaded ST or End-to-End ST model. 7.3 Cascaded vs End-to-End Third, we report the results of comparing the cas- caded setting to the end-to-end setting when per- forming online translation with revision mode on the ACL dev set. As can be seen in Figure 6, in general, cascaded ST has better BLEU score yet worse latency than end-to-end ST. Cascaded ST has worse latency since it contains two components and each component has to do computation. However, we observe that, with a similar latency of \u223c 3.5 seconds, cascaded ST still obtains a better BLEU score. On the other hand, end-to-end ST has a bet- ter minimum latency that can be achieved (almost two seconds lower than the cascaded system). 7.4 Load balancing In order to assess the system\u2019s capability to balance loads, we conduct experiments on running multi- ple sessions simultaneously using the same hosted model, with and without scaling the system\u2019s num- ber of middleware workers. For speech processing, we test parallel sessions on ACL dev en-de using the end-to-end ST model. For text processing, we test one cascaded ST session on ACL dev where the number of parallel sessions is the number of requested MT languages. In all experiments, we set LA2_chunk_size = 2. We report only the en-de results. The results are shown in Table 3. As expected, the latency gets worse as the number of parallel ses- sions increases. Using multiple middleware work- ers counteracts that to some extent by making sure that the backend model is always busy and not wait- ing for the next request. Furthermore, we see that when the number of parallel sessions increases, the flickering rate decreases. This is because during higher load, fewer requests are sent to the backend and we observe less flickering. Here our automatic load balancing can be seen in action. w s 1 1 2 5 1 2 5 5 Speech processing F \u2193 L \u2193 B \u2191 0.5 3.2 25.9 0.5 20.2 26.1 0.2 28.2 21.3 0.6 3.2 26.2 0.5 4.6 26.5 0.3 16.7 25.3 Text processing L \u2193 F \u2193 B \u2191 0.5 6.7 34.9 0.4 8.4 34.6 0.2 28.1 34.8 0.5 6.1 35.1 0.5 8.0 33.6 0.2 15.9 34.5 Table 3: Quality, latency and flickering rate when scaling the number of sessions (with one hosted model per number of middle- ware workers, s: number of parallel sessions, B: Qual- ity (BLEU score), L: Latency (s), F: Flickering rate. LA2_chunk_size is set to 2 seconds. language). w: 8 Related work SimulEval (Ma et al., 2020) provides an evaluation framework for low-latency simultaneous speech translation with a decoupled client-server archi- tecture allowing to plug-in translation models and stability detection policies. As the main difference we leave the audio segmentation up to the model whereas Ma et al. (2020) rely on a pre-segmentation of the audio, we factor in the computational latency in addition to the model latency and explore the scaling behavior in multi-session scenarios, both for a more realistic deployment scenario. Similar to this work Franceschini et al. (2020) implement a low-latency speech translation pipeline, however, their architecture does not scale well to multiple sessions and is not well suited for end-to-end eval- uation. 9 Limitations and Conclusion Since we run and evaluate the experiments in a realistic real-world scenario, it is difficult to exactly reproduce the results. The experiments are non- deterministic, e.g., because of network latencies. Furthermore, the results depend on the speed of the used hardware, especially the used hardware for the backend models. Additionally, we expect that each streaming algorithm implemented returns"}, {"question": " What limitations are mentioned in the text regarding the experiments conducted?", "answer": " The experiments are non-deterministic and depend on factors like network latencies and hardware speed", "ref_chunk": "WavLM (Chen et al., 2022) and BART (Lewis et al., 2019)6, while for Multilin- gual ASR we utilized the XLS-R models (Babu et al., 2021) for the encoder and the MBART- 50 model (Liu et al., 2020b) for the decoder fol- lowing (Pham et al., 2022). On the other hand, the translation models are based on the pretrained 6With the recipe available at here. 5 8 C=0.5 Revision mode 3 4 C=2 30 Fixed mode 9Latency (s) 32 C=3 C=1 34 7 36BLEU score 6 Figure 5: caded model) C: LA2_chunk_size (s). Latency vs. the cas- in revision mode or fixed mode. quality (for DeltaLM (Ma et al., 2021). For the en\u2192X direc- tion, the models are fine-tuned to optimize for ACL talks based on Liu et al. (2023). For other direc- tions, DeltaLM is fine-tuned on the combination of commonly available datasets7. Finally, for the end-to-end ST system, we used the language-agnostic model from Huber et al. (2022) that can decode en-de ST and de ASR. 7 Results and Discussion 7.1 Quality vs Latency trade-off In the first experiment, we assess the trade-off be- tween translation quality and latency by modifying the LA2_chunk_size parameter. The results are shown in Table 2. As can be seen, as we increase chunk size, the translation quality improves while the latency gets worse, both for cascaded ST and end-to-end ST. This is expected, since higher chunk size means longer input given to the model at each step, thus the output has better quality due to hav- ing more context, while the latency gets worse due to more waiting time for collecting the input. 7.2 Revision mode vs fixed mode Second, we report the results of comparing the revision mode to the fixed mode with different LA2_chunk_size values when performing cas- caded translation on the en-de ACL dev set. As can be seen in Figure 5, in general, revision mode has better BLEU score yet worse latency than fixed mode. This is expected, since for the revision mode, when more input audio is available, the system can correct its previous output, thus ending up having better translation quality yet worse latency due to the additional re-translation overhead. 7Paracrawl, UNPC, EUBookshop, MultiUN, EuroPat, TildeMODEL, DGT, Europarl, QED and NewsCommentary. C=0.5 Cascaded ST C=2 4 3 7Latency (s) 30 1 C=1 End-to-End ST 25 35BLEU score 2 C=3 6 5 Figure 6: Latency vs. for C: LA2_chunk_size (s). quality (in revision mode) the cascaded ST or End-to-End ST model. 7.3 Cascaded vs End-to-End Third, we report the results of comparing the cas- caded setting to the end-to-end setting when per- forming online translation with revision mode on the ACL dev set. As can be seen in Figure 6, in general, cascaded ST has better BLEU score yet worse latency than end-to-end ST. Cascaded ST has worse latency since it contains two components and each component has to do computation. However, we observe that, with a similar latency of \u223c 3.5 seconds, cascaded ST still obtains a better BLEU score. On the other hand, end-to-end ST has a bet- ter minimum latency that can be achieved (almost two seconds lower than the cascaded system). 7.4 Load balancing In order to assess the system\u2019s capability to balance loads, we conduct experiments on running multi- ple sessions simultaneously using the same hosted model, with and without scaling the system\u2019s num- ber of middleware workers. For speech processing, we test parallel sessions on ACL dev en-de using the end-to-end ST model. For text processing, we test one cascaded ST session on ACL dev where the number of parallel sessions is the number of requested MT languages. In all experiments, we set LA2_chunk_size = 2. We report only the en-de results. The results are shown in Table 3. As expected, the latency gets worse as the number of parallel ses- sions increases. Using multiple middleware work- ers counteracts that to some extent by making sure that the backend model is always busy and not wait- ing for the next request. Furthermore, we see that when the number of parallel sessions increases, the flickering rate decreases. This is because during higher load, fewer requests are sent to the backend and we observe less flickering. Here our automatic load balancing can be seen in action. w s 1 1 2 5 1 2 5 5 Speech processing F \u2193 L \u2193 B \u2191 0.5 3.2 25.9 0.5 20.2 26.1 0.2 28.2 21.3 0.6 3.2 26.2 0.5 4.6 26.5 0.3 16.7 25.3 Text processing L \u2193 F \u2193 B \u2191 0.5 6.7 34.9 0.4 8.4 34.6 0.2 28.1 34.8 0.5 6.1 35.1 0.5 8.0 33.6 0.2 15.9 34.5 Table 3: Quality, latency and flickering rate when scaling the number of sessions (with one hosted model per number of middle- ware workers, s: number of parallel sessions, B: Qual- ity (BLEU score), L: Latency (s), F: Flickering rate. LA2_chunk_size is set to 2 seconds. language). w: 8 Related work SimulEval (Ma et al., 2020) provides an evaluation framework for low-latency simultaneous speech translation with a decoupled client-server archi- tecture allowing to plug-in translation models and stability detection policies. As the main difference we leave the audio segmentation up to the model whereas Ma et al. (2020) rely on a pre-segmentation of the audio, we factor in the computational latency in addition to the model latency and explore the scaling behavior in multi-session scenarios, both for a more realistic deployment scenario. Similar to this work Franceschini et al. (2020) implement a low-latency speech translation pipeline, however, their architecture does not scale well to multiple sessions and is not well suited for end-to-end eval- uation. 9 Limitations and Conclusion Since we run and evaluate the experiments in a realistic real-world scenario, it is difficult to exactly reproduce the results. The experiments are non- deterministic, e.g., because of network latencies. Furthermore, the results depend on the speed of the used hardware, especially the used hardware for the backend models. Additionally, we expect that each streaming algorithm implemented returns"}, {"question": " What differentiates the approach of this work from Ma et al. (2020) in the evaluation of low-latency speech translation?", "answer": " This work factors in computational latency in addition to model latency and explores scaling behavior in multi-session scenarios", "ref_chunk": "WavLM (Chen et al., 2022) and BART (Lewis et al., 2019)6, while for Multilin- gual ASR we utilized the XLS-R models (Babu et al., 2021) for the encoder and the MBART- 50 model (Liu et al., 2020b) for the decoder fol- lowing (Pham et al., 2022). On the other hand, the translation models are based on the pretrained 6With the recipe available at here. 5 8 C=0.5 Revision mode 3 4 C=2 30 Fixed mode 9Latency (s) 32 C=3 C=1 34 7 36BLEU score 6 Figure 5: caded model) C: LA2_chunk_size (s). Latency vs. the cas- in revision mode or fixed mode. quality (for DeltaLM (Ma et al., 2021). For the en\u2192X direc- tion, the models are fine-tuned to optimize for ACL talks based on Liu et al. (2023). For other direc- tions, DeltaLM is fine-tuned on the combination of commonly available datasets7. Finally, for the end-to-end ST system, we used the language-agnostic model from Huber et al. (2022) that can decode en-de ST and de ASR. 7 Results and Discussion 7.1 Quality vs Latency trade-off In the first experiment, we assess the trade-off be- tween translation quality and latency by modifying the LA2_chunk_size parameter. The results are shown in Table 2. As can be seen, as we increase chunk size, the translation quality improves while the latency gets worse, both for cascaded ST and end-to-end ST. This is expected, since higher chunk size means longer input given to the model at each step, thus the output has better quality due to hav- ing more context, while the latency gets worse due to more waiting time for collecting the input. 7.2 Revision mode vs fixed mode Second, we report the results of comparing the revision mode to the fixed mode with different LA2_chunk_size values when performing cas- caded translation on the en-de ACL dev set. As can be seen in Figure 5, in general, revision mode has better BLEU score yet worse latency than fixed mode. This is expected, since for the revision mode, when more input audio is available, the system can correct its previous output, thus ending up having better translation quality yet worse latency due to the additional re-translation overhead. 7Paracrawl, UNPC, EUBookshop, MultiUN, EuroPat, TildeMODEL, DGT, Europarl, QED and NewsCommentary. C=0.5 Cascaded ST C=2 4 3 7Latency (s) 30 1 C=1 End-to-End ST 25 35BLEU score 2 C=3 6 5 Figure 6: Latency vs. for C: LA2_chunk_size (s). quality (in revision mode) the cascaded ST or End-to-End ST model. 7.3 Cascaded vs End-to-End Third, we report the results of comparing the cas- caded setting to the end-to-end setting when per- forming online translation with revision mode on the ACL dev set. As can be seen in Figure 6, in general, cascaded ST has better BLEU score yet worse latency than end-to-end ST. Cascaded ST has worse latency since it contains two components and each component has to do computation. However, we observe that, with a similar latency of \u223c 3.5 seconds, cascaded ST still obtains a better BLEU score. On the other hand, end-to-end ST has a bet- ter minimum latency that can be achieved (almost two seconds lower than the cascaded system). 7.4 Load balancing In order to assess the system\u2019s capability to balance loads, we conduct experiments on running multi- ple sessions simultaneously using the same hosted model, with and without scaling the system\u2019s num- ber of middleware workers. For speech processing, we test parallel sessions on ACL dev en-de using the end-to-end ST model. For text processing, we test one cascaded ST session on ACL dev where the number of parallel sessions is the number of requested MT languages. In all experiments, we set LA2_chunk_size = 2. We report only the en-de results. The results are shown in Table 3. As expected, the latency gets worse as the number of parallel ses- sions increases. Using multiple middleware work- ers counteracts that to some extent by making sure that the backend model is always busy and not wait- ing for the next request. Furthermore, we see that when the number of parallel sessions increases, the flickering rate decreases. This is because during higher load, fewer requests are sent to the backend and we observe less flickering. Here our automatic load balancing can be seen in action. w s 1 1 2 5 1 2 5 5 Speech processing F \u2193 L \u2193 B \u2191 0.5 3.2 25.9 0.5 20.2 26.1 0.2 28.2 21.3 0.6 3.2 26.2 0.5 4.6 26.5 0.3 16.7 25.3 Text processing L \u2193 F \u2193 B \u2191 0.5 6.7 34.9 0.4 8.4 34.6 0.2 28.1 34.8 0.5 6.1 35.1 0.5 8.0 33.6 0.2 15.9 34.5 Table 3: Quality, latency and flickering rate when scaling the number of sessions (with one hosted model per number of middle- ware workers, s: number of parallel sessions, B: Qual- ity (BLEU score), L: Latency (s), F: Flickering rate. LA2_chunk_size is set to 2 seconds. language). w: 8 Related work SimulEval (Ma et al., 2020) provides an evaluation framework for low-latency simultaneous speech translation with a decoupled client-server archi- tecture allowing to plug-in translation models and stability detection policies. As the main difference we leave the audio segmentation up to the model whereas Ma et al. (2020) rely on a pre-segmentation of the audio, we factor in the computational latency in addition to the model latency and explore the scaling behavior in multi-session scenarios, both for a more realistic deployment scenario. Similar to this work Franceschini et al. (2020) implement a low-latency speech translation pipeline, however, their architecture does not scale well to multiple sessions and is not well suited for end-to-end eval- uation. 9 Limitations and Conclusion Since we run and evaluate the experiments in a realistic real-world scenario, it is difficult to exactly reproduce the results. The experiments are non- deterministic, e.g., because of network latencies. Furthermore, the results depend on the speed of the used hardware, especially the used hardware for the backend models. Additionally, we expect that each streaming algorithm implemented returns"}], "doc_text": "WavLM (Chen et al., 2022) and BART (Lewis et al., 2019)6, while for Multilin- gual ASR we utilized the XLS-R models (Babu et al., 2021) for the encoder and the MBART- 50 model (Liu et al., 2020b) for the decoder fol- lowing (Pham et al., 2022). On the other hand, the translation models are based on the pretrained 6With the recipe available at here. 5 8 C=0.5 Revision mode 3 4 C=2 30 Fixed mode 9Latency (s) 32 C=3 C=1 34 7 36BLEU score 6 Figure 5: caded model) C: LA2_chunk_size (s). Latency vs. the cas- in revision mode or fixed mode. quality (for DeltaLM (Ma et al., 2021). For the en\u2192X direc- tion, the models are fine-tuned to optimize for ACL talks based on Liu et al. (2023). For other direc- tions, DeltaLM is fine-tuned on the combination of commonly available datasets7. Finally, for the end-to-end ST system, we used the language-agnostic model from Huber et al. (2022) that can decode en-de ST and de ASR. 7 Results and Discussion 7.1 Quality vs Latency trade-off In the first experiment, we assess the trade-off be- tween translation quality and latency by modifying the LA2_chunk_size parameter. The results are shown in Table 2. As can be seen, as we increase chunk size, the translation quality improves while the latency gets worse, both for cascaded ST and end-to-end ST. This is expected, since higher chunk size means longer input given to the model at each step, thus the output has better quality due to hav- ing more context, while the latency gets worse due to more waiting time for collecting the input. 7.2 Revision mode vs fixed mode Second, we report the results of comparing the revision mode to the fixed mode with different LA2_chunk_size values when performing cas- caded translation on the en-de ACL dev set. As can be seen in Figure 5, in general, revision mode has better BLEU score yet worse latency than fixed mode. This is expected, since for the revision mode, when more input audio is available, the system can correct its previous output, thus ending up having better translation quality yet worse latency due to the additional re-translation overhead. 7Paracrawl, UNPC, EUBookshop, MultiUN, EuroPat, TildeMODEL, DGT, Europarl, QED and NewsCommentary. C=0.5 Cascaded ST C=2 4 3 7Latency (s) 30 1 C=1 End-to-End ST 25 35BLEU score 2 C=3 6 5 Figure 6: Latency vs. for C: LA2_chunk_size (s). quality (in revision mode) the cascaded ST or End-to-End ST model. 7.3 Cascaded vs End-to-End Third, we report the results of comparing the cas- caded setting to the end-to-end setting when per- forming online translation with revision mode on the ACL dev set. As can be seen in Figure 6, in general, cascaded ST has better BLEU score yet worse latency than end-to-end ST. Cascaded ST has worse latency since it contains two components and each component has to do computation. However, we observe that, with a similar latency of \u223c 3.5 seconds, cascaded ST still obtains a better BLEU score. On the other hand, end-to-end ST has a bet- ter minimum latency that can be achieved (almost two seconds lower than the cascaded system). 7.4 Load balancing In order to assess the system\u2019s capability to balance loads, we conduct experiments on running multi- ple sessions simultaneously using the same hosted model, with and without scaling the system\u2019s num- ber of middleware workers. For speech processing, we test parallel sessions on ACL dev en-de using the end-to-end ST model. For text processing, we test one cascaded ST session on ACL dev where the number of parallel sessions is the number of requested MT languages. In all experiments, we set LA2_chunk_size = 2. We report only the en-de results. The results are shown in Table 3. As expected, the latency gets worse as the number of parallel ses- sions increases. Using multiple middleware work- ers counteracts that to some extent by making sure that the backend model is always busy and not wait- ing for the next request. Furthermore, we see that when the number of parallel sessions increases, the flickering rate decreases. This is because during higher load, fewer requests are sent to the backend and we observe less flickering. Here our automatic load balancing can be seen in action. w s 1 1 2 5 1 2 5 5 Speech processing F \u2193 L \u2193 B \u2191 0.5 3.2 25.9 0.5 20.2 26.1 0.2 28.2 21.3 0.6 3.2 26.2 0.5 4.6 26.5 0.3 16.7 25.3 Text processing L \u2193 F \u2193 B \u2191 0.5 6.7 34.9 0.4 8.4 34.6 0.2 28.1 34.8 0.5 6.1 35.1 0.5 8.0 33.6 0.2 15.9 34.5 Table 3: Quality, latency and flickering rate when scaling the number of sessions (with one hosted model per number of middle- ware workers, s: number of parallel sessions, B: Qual- ity (BLEU score), L: Latency (s), F: Flickering rate. LA2_chunk_size is set to 2 seconds. language). w: 8 Related work SimulEval (Ma et al., 2020) provides an evaluation framework for low-latency simultaneous speech translation with a decoupled client-server archi- tecture allowing to plug-in translation models and stability detection policies. As the main difference we leave the audio segmentation up to the model whereas Ma et al. (2020) rely on a pre-segmentation of the audio, we factor in the computational latency in addition to the model latency and explore the scaling behavior in multi-session scenarios, both for a more realistic deployment scenario. Similar to this work Franceschini et al. (2020) implement a low-latency speech translation pipeline, however, their architecture does not scale well to multiple sessions and is not well suited for end-to-end eval- uation. 9 Limitations and Conclusion Since we run and evaluate the experiments in a realistic real-world scenario, it is difficult to exactly reproduce the results. The experiments are non- deterministic, e.g., because of network latencies. Furthermore, the results depend on the speed of the used hardware, especially the used hardware for the backend models. Additionally, we expect that each streaming algorithm implemented returns"}