{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_FacTool:_Factuality_Detection_in_Generative_AI_-_A_Tool_Augmented_Framework_for_Multi-Task_and_Multi-Domain_Scenarios_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the focus of factuality detection in natural language processing?", "answer": " The focus is on determining whether a claim can be supported by evidence.", "ref_chunk": "are: 2 Related Work Factuality Detection in Natural Language Pro- cessing Factuality detection was a topic of rig- orous study even before the advent of generative AI. Existing works can be organized by their dif- ferences in terms of the \u201cresponse\u201d to be veri- fied, the \u201cclaim\u201d extracted from the response, and supporting \u201cevidence\u201d. As illustrated in Tab. 1, the creation of the FEVER dataset (Thorne et al., 2018a) spawned models (Zhong et al., 2020; Kr- ishna et al., 2022) that determine whether a given fine-grained claim made based on Wikipedia1 arti- cles is correct. In this task setting, both the claim and related evidence are given. FactCC (Kryscin- ski et al., 2020) and QAGS-based models (Wang et al., 2020) adopted different task formulations to detect factual consistency, i.e., given the ev- idence text, and the goal is to determine if the generated summaries or summary sentences are factually consistent with the given text. WICE- based methods (Kamoi et al., 2023) decide if a fact from a Wikipedia sentence could be supported We revisit the task of factuality detection and extend it in a way that allows for a better audit of current generative AI models. 1https://www.wikipedia.org/ by provided evidence. RARR (Gao et al., 2022a) proposed a new approach by directly prompting LLMs to generate queries, retrieve evidence and determine factuality. Existing works typically rely on either a given claim or given evidence and target a specific use case. However, in this paper, we introduce a more challenging yet practical task setting, i.e., factuality detection without explicit claims or evidence, and propose a framework capable of addressing this challenge in a variety of scenarios. Tool use in Large Pretrained Language Models Language models store limited knowledge within their parameters. To overcome this limitation, vari- ous tools have been introduced to assist language models in order to further expand their capabili- ties. For example, Press et al. (2022); Komeili et al. (2022) gathered information from the Internet to enhance question answering and dialog systems, respectively. Schick et al. (2023) trained a model capable of interacting with five tools including a calculator, a translation system, etc. Recently, Shen et al. (2023) introduced a framework that employs LLMs to connect various AI models from the ma- chine learning communities to tackle AI tasks. Fur- thermore, Liang et al. (2023) proposed a new AI ecosystem that connects LLMs with millions of existing APIs to accomplish tasks. In this work, we explore tool use in LLMs for the task of factuality detection. 3 Revisiting Factuality in Generative AI 3.1 Definition Versatile Factuality In most previous works, fac- tuality has been defined as whether a claim in a text can be supported by evidence from a separate, trustworthy knowledge base, with applications in fact-checking (Thorne et al., 2018b) (where the knowledge base is a large source like Wikipedia) and summarization (Kryscinski et al., 2020) (where the knowledge base is an input document or doc- uments). In this paper, we extend this definition to whether the claims made in generated signals (which could be text, code, or mathematical ex- pressions and so on) can be supported by evidence under specific rules. Specifically, these rules can range from consistency with a knowledge base de- rived from Wikipedia, to a verification rule spec- ified within a Python library, or an operational rule derived from mathematics. By adopting this broader definition, we are able to establish a uni- fied framework for addressing factuality issues in generative AI beyond just the textual domain. Fine-grained Factuality One can usually detect the factuality of a given generated signal (e.g., text) at different levels of granularity, such as sentences, and documents. A more granular assessment can be particularly valuable because it (1) not only allows users to pinpoint where inaccuracies oc- cur (Liu et al., 2021) but also (2) serves as a reward model for developers to refine their generative sys- tems (Lightman et al., 2023). However, implementing fine-grained factuality detection is challenging due to two reasons: (1) specifying the desired granularity level without am- biguity, and (2) extracting claims in line with the predetermined granularity level. In this paper, we argue that by utilizing the powerful instruction- following ability and the natural language inter- face of LLMs, we can effectively address the chal- lenge of defining and extracting fine-grained claims through claim definition-based few-shot prompting. More details can be found in \u00a74.1. Structurally speaking, given a prompt (e.g., a query or instruction) and the corresponding model- generated response, the fine-grained factuality de- tection task involves the following concepts: Prompt (p) a query or instruction that users pro- vide to the generative model. Response (r) a piece of text (usually in long form) generated by the generative model. Claim (c) a statement inferred from the model re- sponse, whose granularity is defined by a natural language text. Evidence (e) The available information (e.g., knowledge base, pre-defined rules) that support or demonstrate the truth or validity of a claim. 3.2 Instantiations in Different Scenarios Using the above task definition, we can define fac- tuality in different application scenarios (see also in Tab.2). Knowledge-based QA Knowledge-based (KB) QA (Chen et al., 2017) aims to answer questions using a given knowledge base or open-domain data source (e.g., Wikipedia). In this task, we define factuality as how well each claim in the generated answer is supported by world knowledge. In this paper, we consider a more challenging scenario: open-domain QA that requires long-form answers, rather than short ones. Tasks Prompt (p) Response (r) Claim (c) Evidence (e) KB-based QA Code Generation Code Query Math Problems Sci. Lit Review Question Math problems Scientific question Long-form answer Atomic component unit Executable code Math solution Long-form review Web searched results Python library Calculator Code snippet Math calculation Tuple (paper title, year, authors) Google scholar Table 2: Factuality definition in different tasks. \u201cSci. Lit Review\u201d represents scientific literature review. Code Generation The code generation task (Yin and Neubig, 2017) involves generating executable code based on a given query. We define factual- ity"}, {"question": " What is the purpose of the FEVER dataset?", "answer": " The FEVER dataset is used to determine whether fine-grained claims based on Wikipedia articles are correct.", "ref_chunk": "are: 2 Related Work Factuality Detection in Natural Language Pro- cessing Factuality detection was a topic of rig- orous study even before the advent of generative AI. Existing works can be organized by their dif- ferences in terms of the \u201cresponse\u201d to be veri- fied, the \u201cclaim\u201d extracted from the response, and supporting \u201cevidence\u201d. As illustrated in Tab. 1, the creation of the FEVER dataset (Thorne et al., 2018a) spawned models (Zhong et al., 2020; Kr- ishna et al., 2022) that determine whether a given fine-grained claim made based on Wikipedia1 arti- cles is correct. In this task setting, both the claim and related evidence are given. FactCC (Kryscin- ski et al., 2020) and QAGS-based models (Wang et al., 2020) adopted different task formulations to detect factual consistency, i.e., given the ev- idence text, and the goal is to determine if the generated summaries or summary sentences are factually consistent with the given text. WICE- based methods (Kamoi et al., 2023) decide if a fact from a Wikipedia sentence could be supported We revisit the task of factuality detection and extend it in a way that allows for a better audit of current generative AI models. 1https://www.wikipedia.org/ by provided evidence. RARR (Gao et al., 2022a) proposed a new approach by directly prompting LLMs to generate queries, retrieve evidence and determine factuality. Existing works typically rely on either a given claim or given evidence and target a specific use case. However, in this paper, we introduce a more challenging yet practical task setting, i.e., factuality detection without explicit claims or evidence, and propose a framework capable of addressing this challenge in a variety of scenarios. Tool use in Large Pretrained Language Models Language models store limited knowledge within their parameters. To overcome this limitation, vari- ous tools have been introduced to assist language models in order to further expand their capabili- ties. For example, Press et al. (2022); Komeili et al. (2022) gathered information from the Internet to enhance question answering and dialog systems, respectively. Schick et al. (2023) trained a model capable of interacting with five tools including a calculator, a translation system, etc. Recently, Shen et al. (2023) introduced a framework that employs LLMs to connect various AI models from the ma- chine learning communities to tackle AI tasks. Fur- thermore, Liang et al. (2023) proposed a new AI ecosystem that connects LLMs with millions of existing APIs to accomplish tasks. In this work, we explore tool use in LLMs for the task of factuality detection. 3 Revisiting Factuality in Generative AI 3.1 Definition Versatile Factuality In most previous works, fac- tuality has been defined as whether a claim in a text can be supported by evidence from a separate, trustworthy knowledge base, with applications in fact-checking (Thorne et al., 2018b) (where the knowledge base is a large source like Wikipedia) and summarization (Kryscinski et al., 2020) (where the knowledge base is an input document or doc- uments). In this paper, we extend this definition to whether the claims made in generated signals (which could be text, code, or mathematical ex- pressions and so on) can be supported by evidence under specific rules. Specifically, these rules can range from consistency with a knowledge base de- rived from Wikipedia, to a verification rule spec- ified within a Python library, or an operational rule derived from mathematics. By adopting this broader definition, we are able to establish a uni- fied framework for addressing factuality issues in generative AI beyond just the textual domain. Fine-grained Factuality One can usually detect the factuality of a given generated signal (e.g., text) at different levels of granularity, such as sentences, and documents. A more granular assessment can be particularly valuable because it (1) not only allows users to pinpoint where inaccuracies oc- cur (Liu et al., 2021) but also (2) serves as a reward model for developers to refine their generative sys- tems (Lightman et al., 2023). However, implementing fine-grained factuality detection is challenging due to two reasons: (1) specifying the desired granularity level without am- biguity, and (2) extracting claims in line with the predetermined granularity level. In this paper, we argue that by utilizing the powerful instruction- following ability and the natural language inter- face of LLMs, we can effectively address the chal- lenge of defining and extracting fine-grained claims through claim definition-based few-shot prompting. More details can be found in \u00a74.1. Structurally speaking, given a prompt (e.g., a query or instruction) and the corresponding model- generated response, the fine-grained factuality de- tection task involves the following concepts: Prompt (p) a query or instruction that users pro- vide to the generative model. Response (r) a piece of text (usually in long form) generated by the generative model. Claim (c) a statement inferred from the model re- sponse, whose granularity is defined by a natural language text. Evidence (e) The available information (e.g., knowledge base, pre-defined rules) that support or demonstrate the truth or validity of a claim. 3.2 Instantiations in Different Scenarios Using the above task definition, we can define fac- tuality in different application scenarios (see also in Tab.2). Knowledge-based QA Knowledge-based (KB) QA (Chen et al., 2017) aims to answer questions using a given knowledge base or open-domain data source (e.g., Wikipedia). In this task, we define factuality as how well each claim in the generated answer is supported by world knowledge. In this paper, we consider a more challenging scenario: open-domain QA that requires long-form answers, rather than short ones. Tasks Prompt (p) Response (r) Claim (c) Evidence (e) KB-based QA Code Generation Code Query Math Problems Sci. Lit Review Question Math problems Scientific question Long-form answer Atomic component unit Executable code Math solution Long-form review Web searched results Python library Calculator Code snippet Math calculation Tuple (paper title, year, authors) Google scholar Table 2: Factuality definition in different tasks. \u201cSci. Lit Review\u201d represents scientific literature review. Code Generation The code generation task (Yin and Neubig, 2017) involves generating executable code based on a given query. We define factual- ity"}, {"question": " How do FactCC and QAGS-based models approach factuality detection?", "answer": " FactCC and QAGS-based models detect factual consistency by checking if generated summaries or summary sentences are factually consistent with the given evidence text.", "ref_chunk": "are: 2 Related Work Factuality Detection in Natural Language Pro- cessing Factuality detection was a topic of rig- orous study even before the advent of generative AI. Existing works can be organized by their dif- ferences in terms of the \u201cresponse\u201d to be veri- fied, the \u201cclaim\u201d extracted from the response, and supporting \u201cevidence\u201d. As illustrated in Tab. 1, the creation of the FEVER dataset (Thorne et al., 2018a) spawned models (Zhong et al., 2020; Kr- ishna et al., 2022) that determine whether a given fine-grained claim made based on Wikipedia1 arti- cles is correct. In this task setting, both the claim and related evidence are given. FactCC (Kryscin- ski et al., 2020) and QAGS-based models (Wang et al., 2020) adopted different task formulations to detect factual consistency, i.e., given the ev- idence text, and the goal is to determine if the generated summaries or summary sentences are factually consistent with the given text. WICE- based methods (Kamoi et al., 2023) decide if a fact from a Wikipedia sentence could be supported We revisit the task of factuality detection and extend it in a way that allows for a better audit of current generative AI models. 1https://www.wikipedia.org/ by provided evidence. RARR (Gao et al., 2022a) proposed a new approach by directly prompting LLMs to generate queries, retrieve evidence and determine factuality. Existing works typically rely on either a given claim or given evidence and target a specific use case. However, in this paper, we introduce a more challenging yet practical task setting, i.e., factuality detection without explicit claims or evidence, and propose a framework capable of addressing this challenge in a variety of scenarios. Tool use in Large Pretrained Language Models Language models store limited knowledge within their parameters. To overcome this limitation, vari- ous tools have been introduced to assist language models in order to further expand their capabili- ties. For example, Press et al. (2022); Komeili et al. (2022) gathered information from the Internet to enhance question answering and dialog systems, respectively. Schick et al. (2023) trained a model capable of interacting with five tools including a calculator, a translation system, etc. Recently, Shen et al. (2023) introduced a framework that employs LLMs to connect various AI models from the ma- chine learning communities to tackle AI tasks. Fur- thermore, Liang et al. (2023) proposed a new AI ecosystem that connects LLMs with millions of existing APIs to accomplish tasks. In this work, we explore tool use in LLMs for the task of factuality detection. 3 Revisiting Factuality in Generative AI 3.1 Definition Versatile Factuality In most previous works, fac- tuality has been defined as whether a claim in a text can be supported by evidence from a separate, trustworthy knowledge base, with applications in fact-checking (Thorne et al., 2018b) (where the knowledge base is a large source like Wikipedia) and summarization (Kryscinski et al., 2020) (where the knowledge base is an input document or doc- uments). In this paper, we extend this definition to whether the claims made in generated signals (which could be text, code, or mathematical ex- pressions and so on) can be supported by evidence under specific rules. Specifically, these rules can range from consistency with a knowledge base de- rived from Wikipedia, to a verification rule spec- ified within a Python library, or an operational rule derived from mathematics. By adopting this broader definition, we are able to establish a uni- fied framework for addressing factuality issues in generative AI beyond just the textual domain. Fine-grained Factuality One can usually detect the factuality of a given generated signal (e.g., text) at different levels of granularity, such as sentences, and documents. A more granular assessment can be particularly valuable because it (1) not only allows users to pinpoint where inaccuracies oc- cur (Liu et al., 2021) but also (2) serves as a reward model for developers to refine their generative sys- tems (Lightman et al., 2023). However, implementing fine-grained factuality detection is challenging due to two reasons: (1) specifying the desired granularity level without am- biguity, and (2) extracting claims in line with the predetermined granularity level. In this paper, we argue that by utilizing the powerful instruction- following ability and the natural language inter- face of LLMs, we can effectively address the chal- lenge of defining and extracting fine-grained claims through claim definition-based few-shot prompting. More details can be found in \u00a74.1. Structurally speaking, given a prompt (e.g., a query or instruction) and the corresponding model- generated response, the fine-grained factuality de- tection task involves the following concepts: Prompt (p) a query or instruction that users pro- vide to the generative model. Response (r) a piece of text (usually in long form) generated by the generative model. Claim (c) a statement inferred from the model re- sponse, whose granularity is defined by a natural language text. Evidence (e) The available information (e.g., knowledge base, pre-defined rules) that support or demonstrate the truth or validity of a claim. 3.2 Instantiations in Different Scenarios Using the above task definition, we can define fac- tuality in different application scenarios (see also in Tab.2). Knowledge-based QA Knowledge-based (KB) QA (Chen et al., 2017) aims to answer questions using a given knowledge base or open-domain data source (e.g., Wikipedia). In this task, we define factuality as how well each claim in the generated answer is supported by world knowledge. In this paper, we consider a more challenging scenario: open-domain QA that requires long-form answers, rather than short ones. Tasks Prompt (p) Response (r) Claim (c) Evidence (e) KB-based QA Code Generation Code Query Math Problems Sci. Lit Review Question Math problems Scientific question Long-form answer Atomic component unit Executable code Math solution Long-form review Web searched results Python library Calculator Code snippet Math calculation Tuple (paper title, year, authors) Google scholar Table 2: Factuality definition in different tasks. \u201cSci. Lit Review\u201d represents scientific literature review. Code Generation The code generation task (Yin and Neubig, 2017) involves generating executable code based on a given query. We define factual- ity"}, {"question": " What is the unique approach proposed by RARR for factuality detection?", "answer": " RARR proposes a method that prompts LLMs to generate queries, retrieve evidence, and determine factuality directly.", "ref_chunk": "are: 2 Related Work Factuality Detection in Natural Language Pro- cessing Factuality detection was a topic of rig- orous study even before the advent of generative AI. Existing works can be organized by their dif- ferences in terms of the \u201cresponse\u201d to be veri- fied, the \u201cclaim\u201d extracted from the response, and supporting \u201cevidence\u201d. As illustrated in Tab. 1, the creation of the FEVER dataset (Thorne et al., 2018a) spawned models (Zhong et al., 2020; Kr- ishna et al., 2022) that determine whether a given fine-grained claim made based on Wikipedia1 arti- cles is correct. In this task setting, both the claim and related evidence are given. FactCC (Kryscin- ski et al., 2020) and QAGS-based models (Wang et al., 2020) adopted different task formulations to detect factual consistency, i.e., given the ev- idence text, and the goal is to determine if the generated summaries or summary sentences are factually consistent with the given text. WICE- based methods (Kamoi et al., 2023) decide if a fact from a Wikipedia sentence could be supported We revisit the task of factuality detection and extend it in a way that allows for a better audit of current generative AI models. 1https://www.wikipedia.org/ by provided evidence. RARR (Gao et al., 2022a) proposed a new approach by directly prompting LLMs to generate queries, retrieve evidence and determine factuality. Existing works typically rely on either a given claim or given evidence and target a specific use case. However, in this paper, we introduce a more challenging yet practical task setting, i.e., factuality detection without explicit claims or evidence, and propose a framework capable of addressing this challenge in a variety of scenarios. Tool use in Large Pretrained Language Models Language models store limited knowledge within their parameters. To overcome this limitation, vari- ous tools have been introduced to assist language models in order to further expand their capabili- ties. For example, Press et al. (2022); Komeili et al. (2022) gathered information from the Internet to enhance question answering and dialog systems, respectively. Schick et al. (2023) trained a model capable of interacting with five tools including a calculator, a translation system, etc. Recently, Shen et al. (2023) introduced a framework that employs LLMs to connect various AI models from the ma- chine learning communities to tackle AI tasks. Fur- thermore, Liang et al. (2023) proposed a new AI ecosystem that connects LLMs with millions of existing APIs to accomplish tasks. In this work, we explore tool use in LLMs for the task of factuality detection. 3 Revisiting Factuality in Generative AI 3.1 Definition Versatile Factuality In most previous works, fac- tuality has been defined as whether a claim in a text can be supported by evidence from a separate, trustworthy knowledge base, with applications in fact-checking (Thorne et al., 2018b) (where the knowledge base is a large source like Wikipedia) and summarization (Kryscinski et al., 2020) (where the knowledge base is an input document or doc- uments). In this paper, we extend this definition to whether the claims made in generated signals (which could be text, code, or mathematical ex- pressions and so on) can be supported by evidence under specific rules. Specifically, these rules can range from consistency with a knowledge base de- rived from Wikipedia, to a verification rule spec- ified within a Python library, or an operational rule derived from mathematics. By adopting this broader definition, we are able to establish a uni- fied framework for addressing factuality issues in generative AI beyond just the textual domain. Fine-grained Factuality One can usually detect the factuality of a given generated signal (e.g., text) at different levels of granularity, such as sentences, and documents. A more granular assessment can be particularly valuable because it (1) not only allows users to pinpoint where inaccuracies oc- cur (Liu et al., 2021) but also (2) serves as a reward model for developers to refine their generative sys- tems (Lightman et al., 2023). However, implementing fine-grained factuality detection is challenging due to two reasons: (1) specifying the desired granularity level without am- biguity, and (2) extracting claims in line with the predetermined granularity level. In this paper, we argue that by utilizing the powerful instruction- following ability and the natural language inter- face of LLMs, we can effectively address the chal- lenge of defining and extracting fine-grained claims through claim definition-based few-shot prompting. More details can be found in \u00a74.1. Structurally speaking, given a prompt (e.g., a query or instruction) and the corresponding model- generated response, the fine-grained factuality de- tection task involves the following concepts: Prompt (p) a query or instruction that users pro- vide to the generative model. Response (r) a piece of text (usually in long form) generated by the generative model. Claim (c) a statement inferred from the model re- sponse, whose granularity is defined by a natural language text. Evidence (e) The available information (e.g., knowledge base, pre-defined rules) that support or demonstrate the truth or validity of a claim. 3.2 Instantiations in Different Scenarios Using the above task definition, we can define fac- tuality in different application scenarios (see also in Tab.2). Knowledge-based QA Knowledge-based (KB) QA (Chen et al., 2017) aims to answer questions using a given knowledge base or open-domain data source (e.g., Wikipedia). In this task, we define factuality as how well each claim in the generated answer is supported by world knowledge. In this paper, we consider a more challenging scenario: open-domain QA that requires long-form answers, rather than short ones. Tasks Prompt (p) Response (r) Claim (c) Evidence (e) KB-based QA Code Generation Code Query Math Problems Sci. Lit Review Question Math problems Scientific question Long-form answer Atomic component unit Executable code Math solution Long-form review Web searched results Python library Calculator Code snippet Math calculation Tuple (paper title, year, authors) Google scholar Table 2: Factuality definition in different tasks. \u201cSci. Lit Review\u201d represents scientific literature review. Code Generation The code generation task (Yin and Neubig, 2017) involves generating executable code based on a given query. We define factual- ity"}, {"question": " What challenge is introduced in the paper regarding factuality detection?", "answer": " The paper introduces the challenge of factuality detection without explicit claims or evidence.", "ref_chunk": "are: 2 Related Work Factuality Detection in Natural Language Pro- cessing Factuality detection was a topic of rig- orous study even before the advent of generative AI. Existing works can be organized by their dif- ferences in terms of the \u201cresponse\u201d to be veri- fied, the \u201cclaim\u201d extracted from the response, and supporting \u201cevidence\u201d. As illustrated in Tab. 1, the creation of the FEVER dataset (Thorne et al., 2018a) spawned models (Zhong et al., 2020; Kr- ishna et al., 2022) that determine whether a given fine-grained claim made based on Wikipedia1 arti- cles is correct. In this task setting, both the claim and related evidence are given. FactCC (Kryscin- ski et al., 2020) and QAGS-based models (Wang et al., 2020) adopted different task formulations to detect factual consistency, i.e., given the ev- idence text, and the goal is to determine if the generated summaries or summary sentences are factually consistent with the given text. WICE- based methods (Kamoi et al., 2023) decide if a fact from a Wikipedia sentence could be supported We revisit the task of factuality detection and extend it in a way that allows for a better audit of current generative AI models. 1https://www.wikipedia.org/ by provided evidence. RARR (Gao et al., 2022a) proposed a new approach by directly prompting LLMs to generate queries, retrieve evidence and determine factuality. Existing works typically rely on either a given claim or given evidence and target a specific use case. However, in this paper, we introduce a more challenging yet practical task setting, i.e., factuality detection without explicit claims or evidence, and propose a framework capable of addressing this challenge in a variety of scenarios. Tool use in Large Pretrained Language Models Language models store limited knowledge within their parameters. To overcome this limitation, vari- ous tools have been introduced to assist language models in order to further expand their capabili- ties. For example, Press et al. (2022); Komeili et al. (2022) gathered information from the Internet to enhance question answering and dialog systems, respectively. Schick et al. (2023) trained a model capable of interacting with five tools including a calculator, a translation system, etc. Recently, Shen et al. (2023) introduced a framework that employs LLMs to connect various AI models from the ma- chine learning communities to tackle AI tasks. Fur- thermore, Liang et al. (2023) proposed a new AI ecosystem that connects LLMs with millions of existing APIs to accomplish tasks. In this work, we explore tool use in LLMs for the task of factuality detection. 3 Revisiting Factuality in Generative AI 3.1 Definition Versatile Factuality In most previous works, fac- tuality has been defined as whether a claim in a text can be supported by evidence from a separate, trustworthy knowledge base, with applications in fact-checking (Thorne et al., 2018b) (where the knowledge base is a large source like Wikipedia) and summarization (Kryscinski et al., 2020) (where the knowledge base is an input document or doc- uments). In this paper, we extend this definition to whether the claims made in generated signals (which could be text, code, or mathematical ex- pressions and so on) can be supported by evidence under specific rules. Specifically, these rules can range from consistency with a knowledge base de- rived from Wikipedia, to a verification rule spec- ified within a Python library, or an operational rule derived from mathematics. By adopting this broader definition, we are able to establish a uni- fied framework for addressing factuality issues in generative AI beyond just the textual domain. Fine-grained Factuality One can usually detect the factuality of a given generated signal (e.g., text) at different levels of granularity, such as sentences, and documents. A more granular assessment can be particularly valuable because it (1) not only allows users to pinpoint where inaccuracies oc- cur (Liu et al., 2021) but also (2) serves as a reward model for developers to refine their generative sys- tems (Lightman et al., 2023). However, implementing fine-grained factuality detection is challenging due to two reasons: (1) specifying the desired granularity level without am- biguity, and (2) extracting claims in line with the predetermined granularity level. In this paper, we argue that by utilizing the powerful instruction- following ability and the natural language inter- face of LLMs, we can effectively address the chal- lenge of defining and extracting fine-grained claims through claim definition-based few-shot prompting. More details can be found in \u00a74.1. Structurally speaking, given a prompt (e.g., a query or instruction) and the corresponding model- generated response, the fine-grained factuality de- tection task involves the following concepts: Prompt (p) a query or instruction that users pro- vide to the generative model. Response (r) a piece of text (usually in long form) generated by the generative model. Claim (c) a statement inferred from the model re- sponse, whose granularity is defined by a natural language text. Evidence (e) The available information (e.g., knowledge base, pre-defined rules) that support or demonstrate the truth or validity of a claim. 3.2 Instantiations in Different Scenarios Using the above task definition, we can define fac- tuality in different application scenarios (see also in Tab.2). Knowledge-based QA Knowledge-based (KB) QA (Chen et al., 2017) aims to answer questions using a given knowledge base or open-domain data source (e.g., Wikipedia). In this task, we define factuality as how well each claim in the generated answer is supported by world knowledge. In this paper, we consider a more challenging scenario: open-domain QA that requires long-form answers, rather than short ones. Tasks Prompt (p) Response (r) Claim (c) Evidence (e) KB-based QA Code Generation Code Query Math Problems Sci. Lit Review Question Math problems Scientific question Long-form answer Atomic component unit Executable code Math solution Long-form review Web searched results Python library Calculator Code snippet Math calculation Tuple (paper title, year, authors) Google scholar Table 2: Factuality definition in different tasks. \u201cSci. Lit Review\u201d represents scientific literature review. Code Generation The code generation task (Yin and Neubig, 2017) involves generating executable code based on a given query. We define factual- ity"}, {"question": " How do language models overcome their limited knowledge?", "answer": " Language models use various tools to overcome their limited knowledge and expand their capabilities.", "ref_chunk": "are: 2 Related Work Factuality Detection in Natural Language Pro- cessing Factuality detection was a topic of rig- orous study even before the advent of generative AI. Existing works can be organized by their dif- ferences in terms of the \u201cresponse\u201d to be veri- fied, the \u201cclaim\u201d extracted from the response, and supporting \u201cevidence\u201d. As illustrated in Tab. 1, the creation of the FEVER dataset (Thorne et al., 2018a) spawned models (Zhong et al., 2020; Kr- ishna et al., 2022) that determine whether a given fine-grained claim made based on Wikipedia1 arti- cles is correct. In this task setting, both the claim and related evidence are given. FactCC (Kryscin- ski et al., 2020) and QAGS-based models (Wang et al., 2020) adopted different task formulations to detect factual consistency, i.e., given the ev- idence text, and the goal is to determine if the generated summaries or summary sentences are factually consistent with the given text. WICE- based methods (Kamoi et al., 2023) decide if a fact from a Wikipedia sentence could be supported We revisit the task of factuality detection and extend it in a way that allows for a better audit of current generative AI models. 1https://www.wikipedia.org/ by provided evidence. RARR (Gao et al., 2022a) proposed a new approach by directly prompting LLMs to generate queries, retrieve evidence and determine factuality. Existing works typically rely on either a given claim or given evidence and target a specific use case. However, in this paper, we introduce a more challenging yet practical task setting, i.e., factuality detection without explicit claims or evidence, and propose a framework capable of addressing this challenge in a variety of scenarios. Tool use in Large Pretrained Language Models Language models store limited knowledge within their parameters. To overcome this limitation, vari- ous tools have been introduced to assist language models in order to further expand their capabili- ties. For example, Press et al. (2022); Komeili et al. (2022) gathered information from the Internet to enhance question answering and dialog systems, respectively. Schick et al. (2023) trained a model capable of interacting with five tools including a calculator, a translation system, etc. Recently, Shen et al. (2023) introduced a framework that employs LLMs to connect various AI models from the ma- chine learning communities to tackle AI tasks. Fur- thermore, Liang et al. (2023) proposed a new AI ecosystem that connects LLMs with millions of existing APIs to accomplish tasks. In this work, we explore tool use in LLMs for the task of factuality detection. 3 Revisiting Factuality in Generative AI 3.1 Definition Versatile Factuality In most previous works, fac- tuality has been defined as whether a claim in a text can be supported by evidence from a separate, trustworthy knowledge base, with applications in fact-checking (Thorne et al., 2018b) (where the knowledge base is a large source like Wikipedia) and summarization (Kryscinski et al., 2020) (where the knowledge base is an input document or doc- uments). In this paper, we extend this definition to whether the claims made in generated signals (which could be text, code, or mathematical ex- pressions and so on) can be supported by evidence under specific rules. Specifically, these rules can range from consistency with a knowledge base de- rived from Wikipedia, to a verification rule spec- ified within a Python library, or an operational rule derived from mathematics. By adopting this broader definition, we are able to establish a uni- fied framework for addressing factuality issues in generative AI beyond just the textual domain. Fine-grained Factuality One can usually detect the factuality of a given generated signal (e.g., text) at different levels of granularity, such as sentences, and documents. A more granular assessment can be particularly valuable because it (1) not only allows users to pinpoint where inaccuracies oc- cur (Liu et al., 2021) but also (2) serves as a reward model for developers to refine their generative sys- tems (Lightman et al., 2023). However, implementing fine-grained factuality detection is challenging due to two reasons: (1) specifying the desired granularity level without am- biguity, and (2) extracting claims in line with the predetermined granularity level. In this paper, we argue that by utilizing the powerful instruction- following ability and the natural language inter- face of LLMs, we can effectively address the chal- lenge of defining and extracting fine-grained claims through claim definition-based few-shot prompting. More details can be found in \u00a74.1. Structurally speaking, given a prompt (e.g., a query or instruction) and the corresponding model- generated response, the fine-grained factuality de- tection task involves the following concepts: Prompt (p) a query or instruction that users pro- vide to the generative model. Response (r) a piece of text (usually in long form) generated by the generative model. Claim (c) a statement inferred from the model re- sponse, whose granularity is defined by a natural language text. Evidence (e) The available information (e.g., knowledge base, pre-defined rules) that support or demonstrate the truth or validity of a claim. 3.2 Instantiations in Different Scenarios Using the above task definition, we can define fac- tuality in different application scenarios (see also in Tab.2). Knowledge-based QA Knowledge-based (KB) QA (Chen et al., 2017) aims to answer questions using a given knowledge base or open-domain data source (e.g., Wikipedia). In this task, we define factuality as how well each claim in the generated answer is supported by world knowledge. In this paper, we consider a more challenging scenario: open-domain QA that requires long-form answers, rather than short ones. Tasks Prompt (p) Response (r) Claim (c) Evidence (e) KB-based QA Code Generation Code Query Math Problems Sci. Lit Review Question Math problems Scientific question Long-form answer Atomic component unit Executable code Math solution Long-form review Web searched results Python library Calculator Code snippet Math calculation Tuple (paper title, year, authors) Google scholar Table 2: Factuality definition in different tasks. \u201cSci. Lit Review\u201d represents scientific literature review. Code Generation The code generation task (Yin and Neubig, 2017) involves generating executable code based on a given query. We define factual- ity"}, {"question": " What is the unified framework proposed for addressing factuality issues in generative AI?", "answer": " The unified framework extends the definition of factuality to include claims made in generated signals that can be supported by evidence under specific rules.", "ref_chunk": "are: 2 Related Work Factuality Detection in Natural Language Pro- cessing Factuality detection was a topic of rig- orous study even before the advent of generative AI. Existing works can be organized by their dif- ferences in terms of the \u201cresponse\u201d to be veri- fied, the \u201cclaim\u201d extracted from the response, and supporting \u201cevidence\u201d. As illustrated in Tab. 1, the creation of the FEVER dataset (Thorne et al., 2018a) spawned models (Zhong et al., 2020; Kr- ishna et al., 2022) that determine whether a given fine-grained claim made based on Wikipedia1 arti- cles is correct. In this task setting, both the claim and related evidence are given. FactCC (Kryscin- ski et al., 2020) and QAGS-based models (Wang et al., 2020) adopted different task formulations to detect factual consistency, i.e., given the ev- idence text, and the goal is to determine if the generated summaries or summary sentences are factually consistent with the given text. WICE- based methods (Kamoi et al., 2023) decide if a fact from a Wikipedia sentence could be supported We revisit the task of factuality detection and extend it in a way that allows for a better audit of current generative AI models. 1https://www.wikipedia.org/ by provided evidence. RARR (Gao et al., 2022a) proposed a new approach by directly prompting LLMs to generate queries, retrieve evidence and determine factuality. Existing works typically rely on either a given claim or given evidence and target a specific use case. However, in this paper, we introduce a more challenging yet practical task setting, i.e., factuality detection without explicit claims or evidence, and propose a framework capable of addressing this challenge in a variety of scenarios. Tool use in Large Pretrained Language Models Language models store limited knowledge within their parameters. To overcome this limitation, vari- ous tools have been introduced to assist language models in order to further expand their capabili- ties. For example, Press et al. (2022); Komeili et al. (2022) gathered information from the Internet to enhance question answering and dialog systems, respectively. Schick et al. (2023) trained a model capable of interacting with five tools including a calculator, a translation system, etc. Recently, Shen et al. (2023) introduced a framework that employs LLMs to connect various AI models from the ma- chine learning communities to tackle AI tasks. Fur- thermore, Liang et al. (2023) proposed a new AI ecosystem that connects LLMs with millions of existing APIs to accomplish tasks. In this work, we explore tool use in LLMs for the task of factuality detection. 3 Revisiting Factuality in Generative AI 3.1 Definition Versatile Factuality In most previous works, fac- tuality has been defined as whether a claim in a text can be supported by evidence from a separate, trustworthy knowledge base, with applications in fact-checking (Thorne et al., 2018b) (where the knowledge base is a large source like Wikipedia) and summarization (Kryscinski et al., 2020) (where the knowledge base is an input document or doc- uments). In this paper, we extend this definition to whether the claims made in generated signals (which could be text, code, or mathematical ex- pressions and so on) can be supported by evidence under specific rules. Specifically, these rules can range from consistency with a knowledge base de- rived from Wikipedia, to a verification rule spec- ified within a Python library, or an operational rule derived from mathematics. By adopting this broader definition, we are able to establish a uni- fied framework for addressing factuality issues in generative AI beyond just the textual domain. Fine-grained Factuality One can usually detect the factuality of a given generated signal (e.g., text) at different levels of granularity, such as sentences, and documents. A more granular assessment can be particularly valuable because it (1) not only allows users to pinpoint where inaccuracies oc- cur (Liu et al., 2021) but also (2) serves as a reward model for developers to refine their generative sys- tems (Lightman et al., 2023). However, implementing fine-grained factuality detection is challenging due to two reasons: (1) specifying the desired granularity level without am- biguity, and (2) extracting claims in line with the predetermined granularity level. In this paper, we argue that by utilizing the powerful instruction- following ability and the natural language inter- face of LLMs, we can effectively address the chal- lenge of defining and extracting fine-grained claims through claim definition-based few-shot prompting. More details can be found in \u00a74.1. Structurally speaking, given a prompt (e.g., a query or instruction) and the corresponding model- generated response, the fine-grained factuality de- tection task involves the following concepts: Prompt (p) a query or instruction that users pro- vide to the generative model. Response (r) a piece of text (usually in long form) generated by the generative model. Claim (c) a statement inferred from the model re- sponse, whose granularity is defined by a natural language text. Evidence (e) The available information (e.g., knowledge base, pre-defined rules) that support or demonstrate the truth or validity of a claim. 3.2 Instantiations in Different Scenarios Using the above task definition, we can define fac- tuality in different application scenarios (see also in Tab.2). Knowledge-based QA Knowledge-based (KB) QA (Chen et al., 2017) aims to answer questions using a given knowledge base or open-domain data source (e.g., Wikipedia). In this task, we define factuality as how well each claim in the generated answer is supported by world knowledge. In this paper, we consider a more challenging scenario: open-domain QA that requires long-form answers, rather than short ones. Tasks Prompt (p) Response (r) Claim (c) Evidence (e) KB-based QA Code Generation Code Query Math Problems Sci. Lit Review Question Math problems Scientific question Long-form answer Atomic component unit Executable code Math solution Long-form review Web searched results Python library Calculator Code snippet Math calculation Tuple (paper title, year, authors) Google scholar Table 2: Factuality definition in different tasks. \u201cSci. Lit Review\u201d represents scientific literature review. Code Generation The code generation task (Yin and Neubig, 2017) involves generating executable code based on a given query. We define factual- ity"}, {"question": " What is the significance of fine-grained factuality detection?", "answer": " Fine-grained factuality detection allows users to pinpoint inaccuracies and serves as a reward model for developers to refine generative systems.", "ref_chunk": "are: 2 Related Work Factuality Detection in Natural Language Pro- cessing Factuality detection was a topic of rig- orous study even before the advent of generative AI. Existing works can be organized by their dif- ferences in terms of the \u201cresponse\u201d to be veri- fied, the \u201cclaim\u201d extracted from the response, and supporting \u201cevidence\u201d. As illustrated in Tab. 1, the creation of the FEVER dataset (Thorne et al., 2018a) spawned models (Zhong et al., 2020; Kr- ishna et al., 2022) that determine whether a given fine-grained claim made based on Wikipedia1 arti- cles is correct. In this task setting, both the claim and related evidence are given. FactCC (Kryscin- ski et al., 2020) and QAGS-based models (Wang et al., 2020) adopted different task formulations to detect factual consistency, i.e., given the ev- idence text, and the goal is to determine if the generated summaries or summary sentences are factually consistent with the given text. WICE- based methods (Kamoi et al., 2023) decide if a fact from a Wikipedia sentence could be supported We revisit the task of factuality detection and extend it in a way that allows for a better audit of current generative AI models. 1https://www.wikipedia.org/ by provided evidence. RARR (Gao et al., 2022a) proposed a new approach by directly prompting LLMs to generate queries, retrieve evidence and determine factuality. Existing works typically rely on either a given claim or given evidence and target a specific use case. However, in this paper, we introduce a more challenging yet practical task setting, i.e., factuality detection without explicit claims or evidence, and propose a framework capable of addressing this challenge in a variety of scenarios. Tool use in Large Pretrained Language Models Language models store limited knowledge within their parameters. To overcome this limitation, vari- ous tools have been introduced to assist language models in order to further expand their capabili- ties. For example, Press et al. (2022); Komeili et al. (2022) gathered information from the Internet to enhance question answering and dialog systems, respectively. Schick et al. (2023) trained a model capable of interacting with five tools including a calculator, a translation system, etc. Recently, Shen et al. (2023) introduced a framework that employs LLMs to connect various AI models from the ma- chine learning communities to tackle AI tasks. Fur- thermore, Liang et al. (2023) proposed a new AI ecosystem that connects LLMs with millions of existing APIs to accomplish tasks. In this work, we explore tool use in LLMs for the task of factuality detection. 3 Revisiting Factuality in Generative AI 3.1 Definition Versatile Factuality In most previous works, fac- tuality has been defined as whether a claim in a text can be supported by evidence from a separate, trustworthy knowledge base, with applications in fact-checking (Thorne et al., 2018b) (where the knowledge base is a large source like Wikipedia) and summarization (Kryscinski et al., 2020) (where the knowledge base is an input document or doc- uments). In this paper, we extend this definition to whether the claims made in generated signals (which could be text, code, or mathematical ex- pressions and so on) can be supported by evidence under specific rules. Specifically, these rules can range from consistency with a knowledge base de- rived from Wikipedia, to a verification rule spec- ified within a Python library, or an operational rule derived from mathematics. By adopting this broader definition, we are able to establish a uni- fied framework for addressing factuality issues in generative AI beyond just the textual domain. Fine-grained Factuality One can usually detect the factuality of a given generated signal (e.g., text) at different levels of granularity, such as sentences, and documents. A more granular assessment can be particularly valuable because it (1) not only allows users to pinpoint where inaccuracies oc- cur (Liu et al., 2021) but also (2) serves as a reward model for developers to refine their generative sys- tems (Lightman et al., 2023). However, implementing fine-grained factuality detection is challenging due to two reasons: (1) specifying the desired granularity level without am- biguity, and (2) extracting claims in line with the predetermined granularity level. In this paper, we argue that by utilizing the powerful instruction- following ability and the natural language inter- face of LLMs, we can effectively address the chal- lenge of defining and extracting fine-grained claims through claim definition-based few-shot prompting. More details can be found in \u00a74.1. Structurally speaking, given a prompt (e.g., a query or instruction) and the corresponding model- generated response, the fine-grained factuality de- tection task involves the following concepts: Prompt (p) a query or instruction that users pro- vide to the generative model. Response (r) a piece of text (usually in long form) generated by the generative model. Claim (c) a statement inferred from the model re- sponse, whose granularity is defined by a natural language text. Evidence (e) The available information (e.g., knowledge base, pre-defined rules) that support or demonstrate the truth or validity of a claim. 3.2 Instantiations in Different Scenarios Using the above task definition, we can define fac- tuality in different application scenarios (see also in Tab.2). Knowledge-based QA Knowledge-based (KB) QA (Chen et al., 2017) aims to answer questions using a given knowledge base or open-domain data source (e.g., Wikipedia). In this task, we define factuality as how well each claim in the generated answer is supported by world knowledge. In this paper, we consider a more challenging scenario: open-domain QA that requires long-form answers, rather than short ones. Tasks Prompt (p) Response (r) Claim (c) Evidence (e) KB-based QA Code Generation Code Query Math Problems Sci. Lit Review Question Math problems Scientific question Long-form answer Atomic component unit Executable code Math solution Long-form review Web searched results Python library Calculator Code snippet Math calculation Tuple (paper title, year, authors) Google scholar Table 2: Factuality definition in different tasks. \u201cSci. Lit Review\u201d represents scientific literature review. Code Generation The code generation task (Yin and Neubig, 2017) involves generating executable code based on a given query. We define factual- ity"}, {"question": " How does the paper suggest addressing the challenge of defining and extracting fine-grained claims?", "answer": " The paper suggests using instruction-following ability and natural language interface of LLMs for defining and extracting fine-grained claims.", "ref_chunk": "are: 2 Related Work Factuality Detection in Natural Language Pro- cessing Factuality detection was a topic of rig- orous study even before the advent of generative AI. Existing works can be organized by their dif- ferences in terms of the \u201cresponse\u201d to be veri- fied, the \u201cclaim\u201d extracted from the response, and supporting \u201cevidence\u201d. As illustrated in Tab. 1, the creation of the FEVER dataset (Thorne et al., 2018a) spawned models (Zhong et al., 2020; Kr- ishna et al., 2022) that determine whether a given fine-grained claim made based on Wikipedia1 arti- cles is correct. In this task setting, both the claim and related evidence are given. FactCC (Kryscin- ski et al., 2020) and QAGS-based models (Wang et al., 2020) adopted different task formulations to detect factual consistency, i.e., given the ev- idence text, and the goal is to determine if the generated summaries or summary sentences are factually consistent with the given text. WICE- based methods (Kamoi et al., 2023) decide if a fact from a Wikipedia sentence could be supported We revisit the task of factuality detection and extend it in a way that allows for a better audit of current generative AI models. 1https://www.wikipedia.org/ by provided evidence. RARR (Gao et al., 2022a) proposed a new approach by directly prompting LLMs to generate queries, retrieve evidence and determine factuality. Existing works typically rely on either a given claim or given evidence and target a specific use case. However, in this paper, we introduce a more challenging yet practical task setting, i.e., factuality detection without explicit claims or evidence, and propose a framework capable of addressing this challenge in a variety of scenarios. Tool use in Large Pretrained Language Models Language models store limited knowledge within their parameters. To overcome this limitation, vari- ous tools have been introduced to assist language models in order to further expand their capabili- ties. For example, Press et al. (2022); Komeili et al. (2022) gathered information from the Internet to enhance question answering and dialog systems, respectively. Schick et al. (2023) trained a model capable of interacting with five tools including a calculator, a translation system, etc. Recently, Shen et al. (2023) introduced a framework that employs LLMs to connect various AI models from the ma- chine learning communities to tackle AI tasks. Fur- thermore, Liang et al. (2023) proposed a new AI ecosystem that connects LLMs with millions of existing APIs to accomplish tasks. In this work, we explore tool use in LLMs for the task of factuality detection. 3 Revisiting Factuality in Generative AI 3.1 Definition Versatile Factuality In most previous works, fac- tuality has been defined as whether a claim in a text can be supported by evidence from a separate, trustworthy knowledge base, with applications in fact-checking (Thorne et al., 2018b) (where the knowledge base is a large source like Wikipedia) and summarization (Kryscinski et al., 2020) (where the knowledge base is an input document or doc- uments). In this paper, we extend this definition to whether the claims made in generated signals (which could be text, code, or mathematical ex- pressions and so on) can be supported by evidence under specific rules. Specifically, these rules can range from consistency with a knowledge base de- rived from Wikipedia, to a verification rule spec- ified within a Python library, or an operational rule derived from mathematics. By adopting this broader definition, we are able to establish a uni- fied framework for addressing factuality issues in generative AI beyond just the textual domain. Fine-grained Factuality One can usually detect the factuality of a given generated signal (e.g., text) at different levels of granularity, such as sentences, and documents. A more granular assessment can be particularly valuable because it (1) not only allows users to pinpoint where inaccuracies oc- cur (Liu et al., 2021) but also (2) serves as a reward model for developers to refine their generative sys- tems (Lightman et al., 2023). However, implementing fine-grained factuality detection is challenging due to two reasons: (1) specifying the desired granularity level without am- biguity, and (2) extracting claims in line with the predetermined granularity level. In this paper, we argue that by utilizing the powerful instruction- following ability and the natural language inter- face of LLMs, we can effectively address the chal- lenge of defining and extracting fine-grained claims through claim definition-based few-shot prompting. More details can be found in \u00a74.1. Structurally speaking, given a prompt (e.g., a query or instruction) and the corresponding model- generated response, the fine-grained factuality de- tection task involves the following concepts: Prompt (p) a query or instruction that users pro- vide to the generative model. Response (r) a piece of text (usually in long form) generated by the generative model. Claim (c) a statement inferred from the model re- sponse, whose granularity is defined by a natural language text. Evidence (e) The available information (e.g., knowledge base, pre-defined rules) that support or demonstrate the truth or validity of a claim. 3.2 Instantiations in Different Scenarios Using the above task definition, we can define fac- tuality in different application scenarios (see also in Tab.2). Knowledge-based QA Knowledge-based (KB) QA (Chen et al., 2017) aims to answer questions using a given knowledge base or open-domain data source (e.g., Wikipedia). In this task, we define factuality as how well each claim in the generated answer is supported by world knowledge. In this paper, we consider a more challenging scenario: open-domain QA that requires long-form answers, rather than short ones. Tasks Prompt (p) Response (r) Claim (c) Evidence (e) KB-based QA Code Generation Code Query Math Problems Sci. Lit Review Question Math problems Scientific question Long-form answer Atomic component unit Executable code Math solution Long-form review Web searched results Python library Calculator Code snippet Math calculation Tuple (paper title, year, authors) Google scholar Table 2: Factuality definition in different tasks. \u201cSci. Lit Review\u201d represents scientific literature review. Code Generation The code generation task (Yin and Neubig, 2017) involves generating executable code based on a given query. We define factual- ity"}, {"question": " What concepts are involved in the fine-grained factuality detection task?", "answer": " The concepts involved are prompt, response, claim, and evidence.", "ref_chunk": "are: 2 Related Work Factuality Detection in Natural Language Pro- cessing Factuality detection was a topic of rig- orous study even before the advent of generative AI. Existing works can be organized by their dif- ferences in terms of the \u201cresponse\u201d to be veri- fied, the \u201cclaim\u201d extracted from the response, and supporting \u201cevidence\u201d. As illustrated in Tab. 1, the creation of the FEVER dataset (Thorne et al., 2018a) spawned models (Zhong et al., 2020; Kr- ishna et al., 2022) that determine whether a given fine-grained claim made based on Wikipedia1 arti- cles is correct. In this task setting, both the claim and related evidence are given. FactCC (Kryscin- ski et al., 2020) and QAGS-based models (Wang et al., 2020) adopted different task formulations to detect factual consistency, i.e., given the ev- idence text, and the goal is to determine if the generated summaries or summary sentences are factually consistent with the given text. WICE- based methods (Kamoi et al., 2023) decide if a fact from a Wikipedia sentence could be supported We revisit the task of factuality detection and extend it in a way that allows for a better audit of current generative AI models. 1https://www.wikipedia.org/ by provided evidence. RARR (Gao et al., 2022a) proposed a new approach by directly prompting LLMs to generate queries, retrieve evidence and determine factuality. Existing works typically rely on either a given claim or given evidence and target a specific use case. However, in this paper, we introduce a more challenging yet practical task setting, i.e., factuality detection without explicit claims or evidence, and propose a framework capable of addressing this challenge in a variety of scenarios. Tool use in Large Pretrained Language Models Language models store limited knowledge within their parameters. To overcome this limitation, vari- ous tools have been introduced to assist language models in order to further expand their capabili- ties. For example, Press et al. (2022); Komeili et al. (2022) gathered information from the Internet to enhance question answering and dialog systems, respectively. Schick et al. (2023) trained a model capable of interacting with five tools including a calculator, a translation system, etc. Recently, Shen et al. (2023) introduced a framework that employs LLMs to connect various AI models from the ma- chine learning communities to tackle AI tasks. Fur- thermore, Liang et al. (2023) proposed a new AI ecosystem that connects LLMs with millions of existing APIs to accomplish tasks. In this work, we explore tool use in LLMs for the task of factuality detection. 3 Revisiting Factuality in Generative AI 3.1 Definition Versatile Factuality In most previous works, fac- tuality has been defined as whether a claim in a text can be supported by evidence from a separate, trustworthy knowledge base, with applications in fact-checking (Thorne et al., 2018b) (where the knowledge base is a large source like Wikipedia) and summarization (Kryscinski et al., 2020) (where the knowledge base is an input document or doc- uments). In this paper, we extend this definition to whether the claims made in generated signals (which could be text, code, or mathematical ex- pressions and so on) can be supported by evidence under specific rules. Specifically, these rules can range from consistency with a knowledge base de- rived from Wikipedia, to a verification rule spec- ified within a Python library, or an operational rule derived from mathematics. By adopting this broader definition, we are able to establish a uni- fied framework for addressing factuality issues in generative AI beyond just the textual domain. Fine-grained Factuality One can usually detect the factuality of a given generated signal (e.g., text) at different levels of granularity, such as sentences, and documents. A more granular assessment can be particularly valuable because it (1) not only allows users to pinpoint where inaccuracies oc- cur (Liu et al., 2021) but also (2) serves as a reward model for developers to refine their generative sys- tems (Lightman et al., 2023). However, implementing fine-grained factuality detection is challenging due to two reasons: (1) specifying the desired granularity level without am- biguity, and (2) extracting claims in line with the predetermined granularity level. In this paper, we argue that by utilizing the powerful instruction- following ability and the natural language inter- face of LLMs, we can effectively address the chal- lenge of defining and extracting fine-grained claims through claim definition-based few-shot prompting. More details can be found in \u00a74.1. Structurally speaking, given a prompt (e.g., a query or instruction) and the corresponding model- generated response, the fine-grained factuality de- tection task involves the following concepts: Prompt (p) a query or instruction that users pro- vide to the generative model. Response (r) a piece of text (usually in long form) generated by the generative model. Claim (c) a statement inferred from the model re- sponse, whose granularity is defined by a natural language text. Evidence (e) The available information (e.g., knowledge base, pre-defined rules) that support or demonstrate the truth or validity of a claim. 3.2 Instantiations in Different Scenarios Using the above task definition, we can define fac- tuality in different application scenarios (see also in Tab.2). Knowledge-based QA Knowledge-based (KB) QA (Chen et al., 2017) aims to answer questions using a given knowledge base or open-domain data source (e.g., Wikipedia). In this task, we define factuality as how well each claim in the generated answer is supported by world knowledge. In this paper, we consider a more challenging scenario: open-domain QA that requires long-form answers, rather than short ones. Tasks Prompt (p) Response (r) Claim (c) Evidence (e) KB-based QA Code Generation Code Query Math Problems Sci. Lit Review Question Math problems Scientific question Long-form answer Atomic component unit Executable code Math solution Long-form review Web searched results Python library Calculator Code snippet Math calculation Tuple (paper title, year, authors) Google scholar Table 2: Factuality definition in different tasks. \u201cSci. Lit Review\u201d represents scientific literature review. Code Generation The code generation task (Yin and Neubig, 2017) involves generating executable code based on a given query. We define factual- ity"}], "doc_text": "are: 2 Related Work Factuality Detection in Natural Language Pro- cessing Factuality detection was a topic of rig- orous study even before the advent of generative AI. Existing works can be organized by their dif- ferences in terms of the \u201cresponse\u201d to be veri- fied, the \u201cclaim\u201d extracted from the response, and supporting \u201cevidence\u201d. As illustrated in Tab. 1, the creation of the FEVER dataset (Thorne et al., 2018a) spawned models (Zhong et al., 2020; Kr- ishna et al., 2022) that determine whether a given fine-grained claim made based on Wikipedia1 arti- cles is correct. In this task setting, both the claim and related evidence are given. FactCC (Kryscin- ski et al., 2020) and QAGS-based models (Wang et al., 2020) adopted different task formulations to detect factual consistency, i.e., given the ev- idence text, and the goal is to determine if the generated summaries or summary sentences are factually consistent with the given text. WICE- based methods (Kamoi et al., 2023) decide if a fact from a Wikipedia sentence could be supported We revisit the task of factuality detection and extend it in a way that allows for a better audit of current generative AI models. 1https://www.wikipedia.org/ by provided evidence. RARR (Gao et al., 2022a) proposed a new approach by directly prompting LLMs to generate queries, retrieve evidence and determine factuality. Existing works typically rely on either a given claim or given evidence and target a specific use case. However, in this paper, we introduce a more challenging yet practical task setting, i.e., factuality detection without explicit claims or evidence, and propose a framework capable of addressing this challenge in a variety of scenarios. Tool use in Large Pretrained Language Models Language models store limited knowledge within their parameters. To overcome this limitation, vari- ous tools have been introduced to assist language models in order to further expand their capabili- ties. For example, Press et al. (2022); Komeili et al. (2022) gathered information from the Internet to enhance question answering and dialog systems, respectively. Schick et al. (2023) trained a model capable of interacting with five tools including a calculator, a translation system, etc. Recently, Shen et al. (2023) introduced a framework that employs LLMs to connect various AI models from the ma- chine learning communities to tackle AI tasks. Fur- thermore, Liang et al. (2023) proposed a new AI ecosystem that connects LLMs with millions of existing APIs to accomplish tasks. In this work, we explore tool use in LLMs for the task of factuality detection. 3 Revisiting Factuality in Generative AI 3.1 Definition Versatile Factuality In most previous works, fac- tuality has been defined as whether a claim in a text can be supported by evidence from a separate, trustworthy knowledge base, with applications in fact-checking (Thorne et al., 2018b) (where the knowledge base is a large source like Wikipedia) and summarization (Kryscinski et al., 2020) (where the knowledge base is an input document or doc- uments). In this paper, we extend this definition to whether the claims made in generated signals (which could be text, code, or mathematical ex- pressions and so on) can be supported by evidence under specific rules. Specifically, these rules can range from consistency with a knowledge base de- rived from Wikipedia, to a verification rule spec- ified within a Python library, or an operational rule derived from mathematics. By adopting this broader definition, we are able to establish a uni- fied framework for addressing factuality issues in generative AI beyond just the textual domain. Fine-grained Factuality One can usually detect the factuality of a given generated signal (e.g., text) at different levels of granularity, such as sentences, and documents. A more granular assessment can be particularly valuable because it (1) not only allows users to pinpoint where inaccuracies oc- cur (Liu et al., 2021) but also (2) serves as a reward model for developers to refine their generative sys- tems (Lightman et al., 2023). However, implementing fine-grained factuality detection is challenging due to two reasons: (1) specifying the desired granularity level without am- biguity, and (2) extracting claims in line with the predetermined granularity level. In this paper, we argue that by utilizing the powerful instruction- following ability and the natural language inter- face of LLMs, we can effectively address the chal- lenge of defining and extracting fine-grained claims through claim definition-based few-shot prompting. More details can be found in \u00a74.1. Structurally speaking, given a prompt (e.g., a query or instruction) and the corresponding model- generated response, the fine-grained factuality de- tection task involves the following concepts: Prompt (p) a query or instruction that users pro- vide to the generative model. Response (r) a piece of text (usually in long form) generated by the generative model. Claim (c) a statement inferred from the model re- sponse, whose granularity is defined by a natural language text. Evidence (e) The available information (e.g., knowledge base, pre-defined rules) that support or demonstrate the truth or validity of a claim. 3.2 Instantiations in Different Scenarios Using the above task definition, we can define fac- tuality in different application scenarios (see also in Tab.2). Knowledge-based QA Knowledge-based (KB) QA (Chen et al., 2017) aims to answer questions using a given knowledge base or open-domain data source (e.g., Wikipedia). In this task, we define factuality as how well each claim in the generated answer is supported by world knowledge. In this paper, we consider a more challenging scenario: open-domain QA that requires long-form answers, rather than short ones. Tasks Prompt (p) Response (r) Claim (c) Evidence (e) KB-based QA Code Generation Code Query Math Problems Sci. Lit Review Question Math problems Scientific question Long-form answer Atomic component unit Executable code Math solution Long-form review Web searched results Python library Calculator Code snippet Math calculation Tuple (paper title, year, authors) Google scholar Table 2: Factuality definition in different tasks. \u201cSci. Lit Review\u201d represents scientific literature review. Code Generation The code generation task (Yin and Neubig, 2017) involves generating executable code based on a given query. We define factual- ity"}