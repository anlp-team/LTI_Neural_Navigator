{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Fernando_Diaz_Overview_of_the_TREC_2021_Fair_Ranking_Track_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the target group exposure \u03b3\u2217 used to measure?", "answer": " The expected exposure loss", "ref_chunk": "distribution of fully-known documents (both gender and location are known) with the global intersectional population (global population by location and equality by gender). Average the distribution of documents with unknown location but known gender with the equality gender distribution. Average the distribution of documents with unknown gender but known location with the world pop- ulation. The result is the target group exposure \u03b3\u2217. We use this to measure the expected exposure loss: M2(Lq) = k\u03b3 \u2212 \u03b3\u2217k2 = \u03b3 \u00b7 \u03b3 \u2212 2\u03b3 \u00b7 \u03b3\u2217 + \u03b3\u2217 \u00b7 \u03b3\u2217 EE-D(Lq) = \u03b3\u2217 \u00b7 \u03b3\u2217 EE-R(Lq) = \u03b3 \u00b7 \u03b3\u2217 7 (4) (5) (6) (7) (8) (9) (10) nDCG AWRF Score 95% CI UoGTrDExpDisT1 UoGTrDRelDiT1 UoGTrDivPropT1 UoGTrDExpDisLT1 RUN1 UoGTrRelT1 RMITRet 1step pair 2step pair 1step pair list 2step pair list RMITRetRerank 1 RMITRetRerank 2 0.2071 0.2001 0.2157 0.1776 0.2169 0.2120 0.2075 0.0838 0.0824 0.0820 0.0786 0.0035 0.0035 0.8299 0.8072 0.7112 0.8197 0.6627 0.6559 0.6413 0.6940 0.6943 0.6908 0.6912 0.6180 0.6158 0.1761 0.1639 0.1532 0.1459 0.1425 0.1373 0.1317 0.0648 0.0638 0.0623 0.0607 0.0026 0.0026 (0.145, 0.212) (0.138, 0.193) (0.128, 0.184) (0.122, 0.173) (0.119, 0.172) (0.113, 0.165) (0.110, 0.159) (0.046, 0.090) (0.045, 0.089) (0.045, 0.085) (0.044, 0.083) (0.001, 0.009) (0.001, 0.009) Table 1: Task 1 runs. Higher score is better (for all metrics). Lower M2 is better. It decomposes into two submetrics, the expected exposure disparity (EE-D) that measures overall inequality in exposure independent of relevance, for which lower is better; and the expected exposure relevance (EE-L) that measures exposure/relevance alignment, for which higher is better [1]. 5 Results This year four di\ufb00erent teams submitted a total of 24 runs. All four teams participated in Task 1: Single Rankings (13 runs total), while only three of the four groups participated in Task 2: Multiple Rankings (11 runs total). 5.1 Task 1: WikiProject Coordinators (Single Rankings) Approaches for Task 1 included: RoBERTa model to compute embeddings for text \ufb01elds. A \ufb01ltering approach to select top ranked documents from either competing rankers or the union of rankers. BM25 ranking from pyserini and re-ranked using MMR implicit diversi\ufb01cation (without explicit fairness groups). Lambda varied between runs. BM25 initial ranking with iterative reranking using fairness calculations to select documents to add to the ranking. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion. Optimisation to consider a protected group\u2019s distribution in the background collection and the total predicted relevance of the group in the candidate results set. Allocating positions in the generated ranking to a protected group proportionally with respect to the total relevance score of the group within the candidate results set. 8 0.15 0.05 0.80 0.85AWRF 0.70 0.75 0.60 0.10 0.20nDCG 0.65 0.00 Figure 1: Task 1 submissions by individual component metrics (NDCG and AWRF). Higher values are better for both metrics. Relevance-only approaches. Table 1 shows the submitted systems ranked by the o\ufb03cial Task 1 metric M1 and its component parts nDCG and AWRF. Figure 1 plots the runs with the component metrics on the x and y axes. Notably, each of the approaches from a participating team are clustered in terms of the component metrics and the o\ufb03cial M1 metric. 5.2 Task 2: Wikipedia Editors (Multiple Rankings) Approaches for Task 2 included: A randomized method with BERT and a two-staged Plackett-Luce sampling where relevance scores are combined with work needed. An iterative approach that uses RoBERTa and computes a score for each of the top-K documents in the current state, based on the expected exposure of each group so far and the original estimated relevance score, integrating an article\u2019s quality score. BM25 plus re-ranking iteratively selecting documents by combining relevance, fairness and quality scores. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion to prioritise highly relevant documents while matching the distributions of the protected groups in the generated ranking to their distributions in the background population. Minimising the predicted divergence, or skew, in the distributions of the protected groups over all of the rankings within a sequence, compared to the background population. Minimising the disparity between a group\u2019s expected and actual exposures and learning the importance of the group relevance and background distributions. 9 EE-R EE-D EE-L EE-L 95% CI RUN task2 pl control 0.6 UoGTrRelT2 pl control 0.8 pl control 0.92 PL IRLab 07 PL IRLab 05 UoGTrDivPropT2 UoGTrDRelDiT2 UoGTrDExpDisT2 UoGTrLambT2 9.5508 8.8091 11.8281 8.6654 8.4802 5.2790 4.9331 4.9372 3.4770 3.7459 2.2447 4.1557 3.2733 9.4609 3.2550 3.1486 1.5327 1.4029 7.1005 5.5891 6.1356 3.4644 14.9007 15.5017 15.6514 15.7708 16.0348 20.8213 21.3832 27.0726 28.4816 28.4903 28.8216 (12.303, 19.946) (12.552, 20.477) (13.057, 20.148) (12.746, 21.251) (12.820, 21.158) (16.283, 28.089) (16.579, 28.293) (21.098, 35.870) (22.366, 37.739) (22.571, 37.548) (22.799, 37.718) Table 2: Task 2 runs. Lower EE-L is better. Relevance-only ranking. Table 2 shows the submitted systems ranked by the o\ufb03cial Task 2 metric EE-L and its component parts EE-D and EE-R. Figure 2 plots the runs with the component metrics on the x and y axes. Overall, the submitted systems generally performed better for one of the component metrics than they did for the other. There are, however, a cluster of four points in Figure 2 that make headway in the trade-o\ufb00 between EE-D and EE-L. 6 Limitations The data and metrics in this task address a few speci\ufb01c types of unfairness, and do so partially. This is fundamentally true of any fairness intervention, and does not in any way diminish the value of the e\ufb00ort \u2014 it is impossible for any data set, task de\ufb01nition, or metric to fully capture fairness in a universal way, and all data and analyses have limitations. Some of the limitations of the data and task include: Fairness criteria \u2013 Geography: For each Wikipedia article, we ascertained which, if any, continents are relevant to the content.5 This was determined by directly looking up several community-maintained (Wiki-"}, {"question": " How is the expected exposure disparity (EE-D) defined?", "answer": " It measures overall inequality in exposure independent of relevance", "ref_chunk": "distribution of fully-known documents (both gender and location are known) with the global intersectional population (global population by location and equality by gender). Average the distribution of documents with unknown location but known gender with the equality gender distribution. Average the distribution of documents with unknown gender but known location with the world pop- ulation. The result is the target group exposure \u03b3\u2217. We use this to measure the expected exposure loss: M2(Lq) = k\u03b3 \u2212 \u03b3\u2217k2 = \u03b3 \u00b7 \u03b3 \u2212 2\u03b3 \u00b7 \u03b3\u2217 + \u03b3\u2217 \u00b7 \u03b3\u2217 EE-D(Lq) = \u03b3\u2217 \u00b7 \u03b3\u2217 EE-R(Lq) = \u03b3 \u00b7 \u03b3\u2217 7 (4) (5) (6) (7) (8) (9) (10) nDCG AWRF Score 95% CI UoGTrDExpDisT1 UoGTrDRelDiT1 UoGTrDivPropT1 UoGTrDExpDisLT1 RUN1 UoGTrRelT1 RMITRet 1step pair 2step pair 1step pair list 2step pair list RMITRetRerank 1 RMITRetRerank 2 0.2071 0.2001 0.2157 0.1776 0.2169 0.2120 0.2075 0.0838 0.0824 0.0820 0.0786 0.0035 0.0035 0.8299 0.8072 0.7112 0.8197 0.6627 0.6559 0.6413 0.6940 0.6943 0.6908 0.6912 0.6180 0.6158 0.1761 0.1639 0.1532 0.1459 0.1425 0.1373 0.1317 0.0648 0.0638 0.0623 0.0607 0.0026 0.0026 (0.145, 0.212) (0.138, 0.193) (0.128, 0.184) (0.122, 0.173) (0.119, 0.172) (0.113, 0.165) (0.110, 0.159) (0.046, 0.090) (0.045, 0.089) (0.045, 0.085) (0.044, 0.083) (0.001, 0.009) (0.001, 0.009) Table 1: Task 1 runs. Higher score is better (for all metrics). Lower M2 is better. It decomposes into two submetrics, the expected exposure disparity (EE-D) that measures overall inequality in exposure independent of relevance, for which lower is better; and the expected exposure relevance (EE-L) that measures exposure/relevance alignment, for which higher is better [1]. 5 Results This year four di\ufb00erent teams submitted a total of 24 runs. All four teams participated in Task 1: Single Rankings (13 runs total), while only three of the four groups participated in Task 2: Multiple Rankings (11 runs total). 5.1 Task 1: WikiProject Coordinators (Single Rankings) Approaches for Task 1 included: RoBERTa model to compute embeddings for text \ufb01elds. A \ufb01ltering approach to select top ranked documents from either competing rankers or the union of rankers. BM25 ranking from pyserini and re-ranked using MMR implicit diversi\ufb01cation (without explicit fairness groups). Lambda varied between runs. BM25 initial ranking with iterative reranking using fairness calculations to select documents to add to the ranking. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion. Optimisation to consider a protected group\u2019s distribution in the background collection and the total predicted relevance of the group in the candidate results set. Allocating positions in the generated ranking to a protected group proportionally with respect to the total relevance score of the group within the candidate results set. 8 0.15 0.05 0.80 0.85AWRF 0.70 0.75 0.60 0.10 0.20nDCG 0.65 0.00 Figure 1: Task 1 submissions by individual component metrics (NDCG and AWRF). Higher values are better for both metrics. Relevance-only approaches. Table 1 shows the submitted systems ranked by the o\ufb03cial Task 1 metric M1 and its component parts nDCG and AWRF. Figure 1 plots the runs with the component metrics on the x and y axes. Notably, each of the approaches from a participating team are clustered in terms of the component metrics and the o\ufb03cial M1 metric. 5.2 Task 2: Wikipedia Editors (Multiple Rankings) Approaches for Task 2 included: A randomized method with BERT and a two-staged Plackett-Luce sampling where relevance scores are combined with work needed. An iterative approach that uses RoBERTa and computes a score for each of the top-K documents in the current state, based on the expected exposure of each group so far and the original estimated relevance score, integrating an article\u2019s quality score. BM25 plus re-ranking iteratively selecting documents by combining relevance, fairness and quality scores. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion to prioritise highly relevant documents while matching the distributions of the protected groups in the generated ranking to their distributions in the background population. Minimising the predicted divergence, or skew, in the distributions of the protected groups over all of the rankings within a sequence, compared to the background population. Minimising the disparity between a group\u2019s expected and actual exposures and learning the importance of the group relevance and background distributions. 9 EE-R EE-D EE-L EE-L 95% CI RUN task2 pl control 0.6 UoGTrRelT2 pl control 0.8 pl control 0.92 PL IRLab 07 PL IRLab 05 UoGTrDivPropT2 UoGTrDRelDiT2 UoGTrDExpDisT2 UoGTrLambT2 9.5508 8.8091 11.8281 8.6654 8.4802 5.2790 4.9331 4.9372 3.4770 3.7459 2.2447 4.1557 3.2733 9.4609 3.2550 3.1486 1.5327 1.4029 7.1005 5.5891 6.1356 3.4644 14.9007 15.5017 15.6514 15.7708 16.0348 20.8213 21.3832 27.0726 28.4816 28.4903 28.8216 (12.303, 19.946) (12.552, 20.477) (13.057, 20.148) (12.746, 21.251) (12.820, 21.158) (16.283, 28.089) (16.579, 28.293) (21.098, 35.870) (22.366, 37.739) (22.571, 37.548) (22.799, 37.718) Table 2: Task 2 runs. Lower EE-L is better. Relevance-only ranking. Table 2 shows the submitted systems ranked by the o\ufb03cial Task 2 metric EE-L and its component parts EE-D and EE-R. Figure 2 plots the runs with the component metrics on the x and y axes. Overall, the submitted systems generally performed better for one of the component metrics than they did for the other. There are, however, a cluster of four points in Figure 2 that make headway in the trade-o\ufb00 between EE-D and EE-L. 6 Limitations The data and metrics in this task address a few speci\ufb01c types of unfairness, and do so partially. This is fundamentally true of any fairness intervention, and does not in any way diminish the value of the e\ufb00ort \u2014 it is impossible for any data set, task de\ufb01nition, or metric to fully capture fairness in a universal way, and all data and analyses have limitations. Some of the limitations of the data and task include: Fairness criteria \u2013 Geography: For each Wikipedia article, we ascertained which, if any, continents are relevant to the content.5 This was determined by directly looking up several community-maintained (Wiki-"}, {"question": " How many teams submitted runs in Task 1?", "answer": " Four different teams submitted a total of 24 runs", "ref_chunk": "distribution of fully-known documents (both gender and location are known) with the global intersectional population (global population by location and equality by gender). Average the distribution of documents with unknown location but known gender with the equality gender distribution. Average the distribution of documents with unknown gender but known location with the world pop- ulation. The result is the target group exposure \u03b3\u2217. We use this to measure the expected exposure loss: M2(Lq) = k\u03b3 \u2212 \u03b3\u2217k2 = \u03b3 \u00b7 \u03b3 \u2212 2\u03b3 \u00b7 \u03b3\u2217 + \u03b3\u2217 \u00b7 \u03b3\u2217 EE-D(Lq) = \u03b3\u2217 \u00b7 \u03b3\u2217 EE-R(Lq) = \u03b3 \u00b7 \u03b3\u2217 7 (4) (5) (6) (7) (8) (9) (10) nDCG AWRF Score 95% CI UoGTrDExpDisT1 UoGTrDRelDiT1 UoGTrDivPropT1 UoGTrDExpDisLT1 RUN1 UoGTrRelT1 RMITRet 1step pair 2step pair 1step pair list 2step pair list RMITRetRerank 1 RMITRetRerank 2 0.2071 0.2001 0.2157 0.1776 0.2169 0.2120 0.2075 0.0838 0.0824 0.0820 0.0786 0.0035 0.0035 0.8299 0.8072 0.7112 0.8197 0.6627 0.6559 0.6413 0.6940 0.6943 0.6908 0.6912 0.6180 0.6158 0.1761 0.1639 0.1532 0.1459 0.1425 0.1373 0.1317 0.0648 0.0638 0.0623 0.0607 0.0026 0.0026 (0.145, 0.212) (0.138, 0.193) (0.128, 0.184) (0.122, 0.173) (0.119, 0.172) (0.113, 0.165) (0.110, 0.159) (0.046, 0.090) (0.045, 0.089) (0.045, 0.085) (0.044, 0.083) (0.001, 0.009) (0.001, 0.009) Table 1: Task 1 runs. Higher score is better (for all metrics). Lower M2 is better. It decomposes into two submetrics, the expected exposure disparity (EE-D) that measures overall inequality in exposure independent of relevance, for which lower is better; and the expected exposure relevance (EE-L) that measures exposure/relevance alignment, for which higher is better [1]. 5 Results This year four di\ufb00erent teams submitted a total of 24 runs. All four teams participated in Task 1: Single Rankings (13 runs total), while only three of the four groups participated in Task 2: Multiple Rankings (11 runs total). 5.1 Task 1: WikiProject Coordinators (Single Rankings) Approaches for Task 1 included: RoBERTa model to compute embeddings for text \ufb01elds. A \ufb01ltering approach to select top ranked documents from either competing rankers or the union of rankers. BM25 ranking from pyserini and re-ranked using MMR implicit diversi\ufb01cation (without explicit fairness groups). Lambda varied between runs. BM25 initial ranking with iterative reranking using fairness calculations to select documents to add to the ranking. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion. Optimisation to consider a protected group\u2019s distribution in the background collection and the total predicted relevance of the group in the candidate results set. Allocating positions in the generated ranking to a protected group proportionally with respect to the total relevance score of the group within the candidate results set. 8 0.15 0.05 0.80 0.85AWRF 0.70 0.75 0.60 0.10 0.20nDCG 0.65 0.00 Figure 1: Task 1 submissions by individual component metrics (NDCG and AWRF). Higher values are better for both metrics. Relevance-only approaches. Table 1 shows the submitted systems ranked by the o\ufb03cial Task 1 metric M1 and its component parts nDCG and AWRF. Figure 1 plots the runs with the component metrics on the x and y axes. Notably, each of the approaches from a participating team are clustered in terms of the component metrics and the o\ufb03cial M1 metric. 5.2 Task 2: Wikipedia Editors (Multiple Rankings) Approaches for Task 2 included: A randomized method with BERT and a two-staged Plackett-Luce sampling where relevance scores are combined with work needed. An iterative approach that uses RoBERTa and computes a score for each of the top-K documents in the current state, based on the expected exposure of each group so far and the original estimated relevance score, integrating an article\u2019s quality score. BM25 plus re-ranking iteratively selecting documents by combining relevance, fairness and quality scores. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion to prioritise highly relevant documents while matching the distributions of the protected groups in the generated ranking to their distributions in the background population. Minimising the predicted divergence, or skew, in the distributions of the protected groups over all of the rankings within a sequence, compared to the background population. Minimising the disparity between a group\u2019s expected and actual exposures and learning the importance of the group relevance and background distributions. 9 EE-R EE-D EE-L EE-L 95% CI RUN task2 pl control 0.6 UoGTrRelT2 pl control 0.8 pl control 0.92 PL IRLab 07 PL IRLab 05 UoGTrDivPropT2 UoGTrDRelDiT2 UoGTrDExpDisT2 UoGTrLambT2 9.5508 8.8091 11.8281 8.6654 8.4802 5.2790 4.9331 4.9372 3.4770 3.7459 2.2447 4.1557 3.2733 9.4609 3.2550 3.1486 1.5327 1.4029 7.1005 5.5891 6.1356 3.4644 14.9007 15.5017 15.6514 15.7708 16.0348 20.8213 21.3832 27.0726 28.4816 28.4903 28.8216 (12.303, 19.946) (12.552, 20.477) (13.057, 20.148) (12.746, 21.251) (12.820, 21.158) (16.283, 28.089) (16.579, 28.293) (21.098, 35.870) (22.366, 37.739) (22.571, 37.548) (22.799, 37.718) Table 2: Task 2 runs. Lower EE-L is better. Relevance-only ranking. Table 2 shows the submitted systems ranked by the o\ufb03cial Task 2 metric EE-L and its component parts EE-D and EE-R. Figure 2 plots the runs with the component metrics on the x and y axes. Overall, the submitted systems generally performed better for one of the component metrics than they did for the other. There are, however, a cluster of four points in Figure 2 that make headway in the trade-o\ufb00 between EE-D and EE-L. 6 Limitations The data and metrics in this task address a few speci\ufb01c types of unfairness, and do so partially. This is fundamentally true of any fairness intervention, and does not in any way diminish the value of the e\ufb00ort \u2014 it is impossible for any data set, task de\ufb01nition, or metric to fully capture fairness in a universal way, and all data and analyses have limitations. Some of the limitations of the data and task include: Fairness criteria \u2013 Geography: For each Wikipedia article, we ascertained which, if any, continents are relevant to the content.5 This was determined by directly looking up several community-maintained (Wiki-"}, {"question": " What is the purpose of the RoBERTa model in Task 1?", "answer": " To compute embeddings for text fields", "ref_chunk": "distribution of fully-known documents (both gender and location are known) with the global intersectional population (global population by location and equality by gender). Average the distribution of documents with unknown location but known gender with the equality gender distribution. Average the distribution of documents with unknown gender but known location with the world pop- ulation. The result is the target group exposure \u03b3\u2217. We use this to measure the expected exposure loss: M2(Lq) = k\u03b3 \u2212 \u03b3\u2217k2 = \u03b3 \u00b7 \u03b3 \u2212 2\u03b3 \u00b7 \u03b3\u2217 + \u03b3\u2217 \u00b7 \u03b3\u2217 EE-D(Lq) = \u03b3\u2217 \u00b7 \u03b3\u2217 EE-R(Lq) = \u03b3 \u00b7 \u03b3\u2217 7 (4) (5) (6) (7) (8) (9) (10) nDCG AWRF Score 95% CI UoGTrDExpDisT1 UoGTrDRelDiT1 UoGTrDivPropT1 UoGTrDExpDisLT1 RUN1 UoGTrRelT1 RMITRet 1step pair 2step pair 1step pair list 2step pair list RMITRetRerank 1 RMITRetRerank 2 0.2071 0.2001 0.2157 0.1776 0.2169 0.2120 0.2075 0.0838 0.0824 0.0820 0.0786 0.0035 0.0035 0.8299 0.8072 0.7112 0.8197 0.6627 0.6559 0.6413 0.6940 0.6943 0.6908 0.6912 0.6180 0.6158 0.1761 0.1639 0.1532 0.1459 0.1425 0.1373 0.1317 0.0648 0.0638 0.0623 0.0607 0.0026 0.0026 (0.145, 0.212) (0.138, 0.193) (0.128, 0.184) (0.122, 0.173) (0.119, 0.172) (0.113, 0.165) (0.110, 0.159) (0.046, 0.090) (0.045, 0.089) (0.045, 0.085) (0.044, 0.083) (0.001, 0.009) (0.001, 0.009) Table 1: Task 1 runs. Higher score is better (for all metrics). Lower M2 is better. It decomposes into two submetrics, the expected exposure disparity (EE-D) that measures overall inequality in exposure independent of relevance, for which lower is better; and the expected exposure relevance (EE-L) that measures exposure/relevance alignment, for which higher is better [1]. 5 Results This year four di\ufb00erent teams submitted a total of 24 runs. All four teams participated in Task 1: Single Rankings (13 runs total), while only three of the four groups participated in Task 2: Multiple Rankings (11 runs total). 5.1 Task 1: WikiProject Coordinators (Single Rankings) Approaches for Task 1 included: RoBERTa model to compute embeddings for text \ufb01elds. A \ufb01ltering approach to select top ranked documents from either competing rankers or the union of rankers. BM25 ranking from pyserini and re-ranked using MMR implicit diversi\ufb01cation (without explicit fairness groups). Lambda varied between runs. BM25 initial ranking with iterative reranking using fairness calculations to select documents to add to the ranking. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion. Optimisation to consider a protected group\u2019s distribution in the background collection and the total predicted relevance of the group in the candidate results set. Allocating positions in the generated ranking to a protected group proportionally with respect to the total relevance score of the group within the candidate results set. 8 0.15 0.05 0.80 0.85AWRF 0.70 0.75 0.60 0.10 0.20nDCG 0.65 0.00 Figure 1: Task 1 submissions by individual component metrics (NDCG and AWRF). Higher values are better for both metrics. Relevance-only approaches. Table 1 shows the submitted systems ranked by the o\ufb03cial Task 1 metric M1 and its component parts nDCG and AWRF. Figure 1 plots the runs with the component metrics on the x and y axes. Notably, each of the approaches from a participating team are clustered in terms of the component metrics and the o\ufb03cial M1 metric. 5.2 Task 2: Wikipedia Editors (Multiple Rankings) Approaches for Task 2 included: A randomized method with BERT and a two-staged Plackett-Luce sampling where relevance scores are combined with work needed. An iterative approach that uses RoBERTa and computes a score for each of the top-K documents in the current state, based on the expected exposure of each group so far and the original estimated relevance score, integrating an article\u2019s quality score. BM25 plus re-ranking iteratively selecting documents by combining relevance, fairness and quality scores. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion to prioritise highly relevant documents while matching the distributions of the protected groups in the generated ranking to their distributions in the background population. Minimising the predicted divergence, or skew, in the distributions of the protected groups over all of the rankings within a sequence, compared to the background population. Minimising the disparity between a group\u2019s expected and actual exposures and learning the importance of the group relevance and background distributions. 9 EE-R EE-D EE-L EE-L 95% CI RUN task2 pl control 0.6 UoGTrRelT2 pl control 0.8 pl control 0.92 PL IRLab 07 PL IRLab 05 UoGTrDivPropT2 UoGTrDRelDiT2 UoGTrDExpDisT2 UoGTrLambT2 9.5508 8.8091 11.8281 8.6654 8.4802 5.2790 4.9331 4.9372 3.4770 3.7459 2.2447 4.1557 3.2733 9.4609 3.2550 3.1486 1.5327 1.4029 7.1005 5.5891 6.1356 3.4644 14.9007 15.5017 15.6514 15.7708 16.0348 20.8213 21.3832 27.0726 28.4816 28.4903 28.8216 (12.303, 19.946) (12.552, 20.477) (13.057, 20.148) (12.746, 21.251) (12.820, 21.158) (16.283, 28.089) (16.579, 28.293) (21.098, 35.870) (22.366, 37.739) (22.571, 37.548) (22.799, 37.718) Table 2: Task 2 runs. Lower EE-L is better. Relevance-only ranking. Table 2 shows the submitted systems ranked by the o\ufb03cial Task 2 metric EE-L and its component parts EE-D and EE-R. Figure 2 plots the runs with the component metrics on the x and y axes. Overall, the submitted systems generally performed better for one of the component metrics than they did for the other. There are, however, a cluster of four points in Figure 2 that make headway in the trade-o\ufb00 between EE-D and EE-L. 6 Limitations The data and metrics in this task address a few speci\ufb01c types of unfairness, and do so partially. This is fundamentally true of any fairness intervention, and does not in any way diminish the value of the e\ufb00ort \u2014 it is impossible for any data set, task de\ufb01nition, or metric to fully capture fairness in a universal way, and all data and analyses have limitations. Some of the limitations of the data and task include: Fairness criteria \u2013 Geography: For each Wikipedia article, we ascertained which, if any, continents are relevant to the content.5 This was determined by directly looking up several community-maintained (Wiki-"}, {"question": " What is the metric M2 used for?", "answer": " To measure the expected exposure loss", "ref_chunk": "distribution of fully-known documents (both gender and location are known) with the global intersectional population (global population by location and equality by gender). Average the distribution of documents with unknown location but known gender with the equality gender distribution. Average the distribution of documents with unknown gender but known location with the world pop- ulation. The result is the target group exposure \u03b3\u2217. We use this to measure the expected exposure loss: M2(Lq) = k\u03b3 \u2212 \u03b3\u2217k2 = \u03b3 \u00b7 \u03b3 \u2212 2\u03b3 \u00b7 \u03b3\u2217 + \u03b3\u2217 \u00b7 \u03b3\u2217 EE-D(Lq) = \u03b3\u2217 \u00b7 \u03b3\u2217 EE-R(Lq) = \u03b3 \u00b7 \u03b3\u2217 7 (4) (5) (6) (7) (8) (9) (10) nDCG AWRF Score 95% CI UoGTrDExpDisT1 UoGTrDRelDiT1 UoGTrDivPropT1 UoGTrDExpDisLT1 RUN1 UoGTrRelT1 RMITRet 1step pair 2step pair 1step pair list 2step pair list RMITRetRerank 1 RMITRetRerank 2 0.2071 0.2001 0.2157 0.1776 0.2169 0.2120 0.2075 0.0838 0.0824 0.0820 0.0786 0.0035 0.0035 0.8299 0.8072 0.7112 0.8197 0.6627 0.6559 0.6413 0.6940 0.6943 0.6908 0.6912 0.6180 0.6158 0.1761 0.1639 0.1532 0.1459 0.1425 0.1373 0.1317 0.0648 0.0638 0.0623 0.0607 0.0026 0.0026 (0.145, 0.212) (0.138, 0.193) (0.128, 0.184) (0.122, 0.173) (0.119, 0.172) (0.113, 0.165) (0.110, 0.159) (0.046, 0.090) (0.045, 0.089) (0.045, 0.085) (0.044, 0.083) (0.001, 0.009) (0.001, 0.009) Table 1: Task 1 runs. Higher score is better (for all metrics). Lower M2 is better. It decomposes into two submetrics, the expected exposure disparity (EE-D) that measures overall inequality in exposure independent of relevance, for which lower is better; and the expected exposure relevance (EE-L) that measures exposure/relevance alignment, for which higher is better [1]. 5 Results This year four di\ufb00erent teams submitted a total of 24 runs. All four teams participated in Task 1: Single Rankings (13 runs total), while only three of the four groups participated in Task 2: Multiple Rankings (11 runs total). 5.1 Task 1: WikiProject Coordinators (Single Rankings) Approaches for Task 1 included: RoBERTa model to compute embeddings for text \ufb01elds. A \ufb01ltering approach to select top ranked documents from either competing rankers or the union of rankers. BM25 ranking from pyserini and re-ranked using MMR implicit diversi\ufb01cation (without explicit fairness groups). Lambda varied between runs. BM25 initial ranking with iterative reranking using fairness calculations to select documents to add to the ranking. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion. Optimisation to consider a protected group\u2019s distribution in the background collection and the total predicted relevance of the group in the candidate results set. Allocating positions in the generated ranking to a protected group proportionally with respect to the total relevance score of the group within the candidate results set. 8 0.15 0.05 0.80 0.85AWRF 0.70 0.75 0.60 0.10 0.20nDCG 0.65 0.00 Figure 1: Task 1 submissions by individual component metrics (NDCG and AWRF). Higher values are better for both metrics. Relevance-only approaches. Table 1 shows the submitted systems ranked by the o\ufb03cial Task 1 metric M1 and its component parts nDCG and AWRF. Figure 1 plots the runs with the component metrics on the x and y axes. Notably, each of the approaches from a participating team are clustered in terms of the component metrics and the o\ufb03cial M1 metric. 5.2 Task 2: Wikipedia Editors (Multiple Rankings) Approaches for Task 2 included: A randomized method with BERT and a two-staged Plackett-Luce sampling where relevance scores are combined with work needed. An iterative approach that uses RoBERTa and computes a score for each of the top-K documents in the current state, based on the expected exposure of each group so far and the original estimated relevance score, integrating an article\u2019s quality score. BM25 plus re-ranking iteratively selecting documents by combining relevance, fairness and quality scores. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion to prioritise highly relevant documents while matching the distributions of the protected groups in the generated ranking to their distributions in the background population. Minimising the predicted divergence, or skew, in the distributions of the protected groups over all of the rankings within a sequence, compared to the background population. Minimising the disparity between a group\u2019s expected and actual exposures and learning the importance of the group relevance and background distributions. 9 EE-R EE-D EE-L EE-L 95% CI RUN task2 pl control 0.6 UoGTrRelT2 pl control 0.8 pl control 0.92 PL IRLab 07 PL IRLab 05 UoGTrDivPropT2 UoGTrDRelDiT2 UoGTrDExpDisT2 UoGTrLambT2 9.5508 8.8091 11.8281 8.6654 8.4802 5.2790 4.9331 4.9372 3.4770 3.7459 2.2447 4.1557 3.2733 9.4609 3.2550 3.1486 1.5327 1.4029 7.1005 5.5891 6.1356 3.4644 14.9007 15.5017 15.6514 15.7708 16.0348 20.8213 21.3832 27.0726 28.4816 28.4903 28.8216 (12.303, 19.946) (12.552, 20.477) (13.057, 20.148) (12.746, 21.251) (12.820, 21.158) (16.283, 28.089) (16.579, 28.293) (21.098, 35.870) (22.366, 37.739) (22.571, 37.548) (22.799, 37.718) Table 2: Task 2 runs. Lower EE-L is better. Relevance-only ranking. Table 2 shows the submitted systems ranked by the o\ufb03cial Task 2 metric EE-L and its component parts EE-D and EE-R. Figure 2 plots the runs with the component metrics on the x and y axes. Overall, the submitted systems generally performed better for one of the component metrics than they did for the other. There are, however, a cluster of four points in Figure 2 that make headway in the trade-o\ufb00 between EE-D and EE-L. 6 Limitations The data and metrics in this task address a few speci\ufb01c types of unfairness, and do so partially. This is fundamentally true of any fairness intervention, and does not in any way diminish the value of the e\ufb00ort \u2014 it is impossible for any data set, task de\ufb01nition, or metric to fully capture fairness in a universal way, and all data and analyses have limitations. Some of the limitations of the data and task include: Fairness criteria \u2013 Geography: For each Wikipedia article, we ascertained which, if any, continents are relevant to the content.5 This was determined by directly looking up several community-maintained (Wiki-"}, {"question": " What is the purpose of BM25 ranking in Task 1?", "answer": " It ranks top documents for the union of rankers", "ref_chunk": "distribution of fully-known documents (both gender and location are known) with the global intersectional population (global population by location and equality by gender). Average the distribution of documents with unknown location but known gender with the equality gender distribution. Average the distribution of documents with unknown gender but known location with the world pop- ulation. The result is the target group exposure \u03b3\u2217. We use this to measure the expected exposure loss: M2(Lq) = k\u03b3 \u2212 \u03b3\u2217k2 = \u03b3 \u00b7 \u03b3 \u2212 2\u03b3 \u00b7 \u03b3\u2217 + \u03b3\u2217 \u00b7 \u03b3\u2217 EE-D(Lq) = \u03b3\u2217 \u00b7 \u03b3\u2217 EE-R(Lq) = \u03b3 \u00b7 \u03b3\u2217 7 (4) (5) (6) (7) (8) (9) (10) nDCG AWRF Score 95% CI UoGTrDExpDisT1 UoGTrDRelDiT1 UoGTrDivPropT1 UoGTrDExpDisLT1 RUN1 UoGTrRelT1 RMITRet 1step pair 2step pair 1step pair list 2step pair list RMITRetRerank 1 RMITRetRerank 2 0.2071 0.2001 0.2157 0.1776 0.2169 0.2120 0.2075 0.0838 0.0824 0.0820 0.0786 0.0035 0.0035 0.8299 0.8072 0.7112 0.8197 0.6627 0.6559 0.6413 0.6940 0.6943 0.6908 0.6912 0.6180 0.6158 0.1761 0.1639 0.1532 0.1459 0.1425 0.1373 0.1317 0.0648 0.0638 0.0623 0.0607 0.0026 0.0026 (0.145, 0.212) (0.138, 0.193) (0.128, 0.184) (0.122, 0.173) (0.119, 0.172) (0.113, 0.165) (0.110, 0.159) (0.046, 0.090) (0.045, 0.089) (0.045, 0.085) (0.044, 0.083) (0.001, 0.009) (0.001, 0.009) Table 1: Task 1 runs. Higher score is better (for all metrics). Lower M2 is better. It decomposes into two submetrics, the expected exposure disparity (EE-D) that measures overall inequality in exposure independent of relevance, for which lower is better; and the expected exposure relevance (EE-L) that measures exposure/relevance alignment, for which higher is better [1]. 5 Results This year four di\ufb00erent teams submitted a total of 24 runs. All four teams participated in Task 1: Single Rankings (13 runs total), while only three of the four groups participated in Task 2: Multiple Rankings (11 runs total). 5.1 Task 1: WikiProject Coordinators (Single Rankings) Approaches for Task 1 included: RoBERTa model to compute embeddings for text \ufb01elds. A \ufb01ltering approach to select top ranked documents from either competing rankers or the union of rankers. BM25 ranking from pyserini and re-ranked using MMR implicit diversi\ufb01cation (without explicit fairness groups). Lambda varied between runs. BM25 initial ranking with iterative reranking using fairness calculations to select documents to add to the ranking. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion. Optimisation to consider a protected group\u2019s distribution in the background collection and the total predicted relevance of the group in the candidate results set. Allocating positions in the generated ranking to a protected group proportionally with respect to the total relevance score of the group within the candidate results set. 8 0.15 0.05 0.80 0.85AWRF 0.70 0.75 0.60 0.10 0.20nDCG 0.65 0.00 Figure 1: Task 1 submissions by individual component metrics (NDCG and AWRF). Higher values are better for both metrics. Relevance-only approaches. Table 1 shows the submitted systems ranked by the o\ufb03cial Task 1 metric M1 and its component parts nDCG and AWRF. Figure 1 plots the runs with the component metrics on the x and y axes. Notably, each of the approaches from a participating team are clustered in terms of the component metrics and the o\ufb03cial M1 metric. 5.2 Task 2: Wikipedia Editors (Multiple Rankings) Approaches for Task 2 included: A randomized method with BERT and a two-staged Plackett-Luce sampling where relevance scores are combined with work needed. An iterative approach that uses RoBERTa and computes a score for each of the top-K documents in the current state, based on the expected exposure of each group so far and the original estimated relevance score, integrating an article\u2019s quality score. BM25 plus re-ranking iteratively selecting documents by combining relevance, fairness and quality scores. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion to prioritise highly relevant documents while matching the distributions of the protected groups in the generated ranking to their distributions in the background population. Minimising the predicted divergence, or skew, in the distributions of the protected groups over all of the rankings within a sequence, compared to the background population. Minimising the disparity between a group\u2019s expected and actual exposures and learning the importance of the group relevance and background distributions. 9 EE-R EE-D EE-L EE-L 95% CI RUN task2 pl control 0.6 UoGTrRelT2 pl control 0.8 pl control 0.92 PL IRLab 07 PL IRLab 05 UoGTrDivPropT2 UoGTrDRelDiT2 UoGTrDExpDisT2 UoGTrLambT2 9.5508 8.8091 11.8281 8.6654 8.4802 5.2790 4.9331 4.9372 3.4770 3.7459 2.2447 4.1557 3.2733 9.4609 3.2550 3.1486 1.5327 1.4029 7.1005 5.5891 6.1356 3.4644 14.9007 15.5017 15.6514 15.7708 16.0348 20.8213 21.3832 27.0726 28.4816 28.4903 28.8216 (12.303, 19.946) (12.552, 20.477) (13.057, 20.148) (12.746, 21.251) (12.820, 21.158) (16.283, 28.089) (16.579, 28.293) (21.098, 35.870) (22.366, 37.739) (22.571, 37.548) (22.799, 37.718) Table 2: Task 2 runs. Lower EE-L is better. Relevance-only ranking. Table 2 shows the submitted systems ranked by the o\ufb03cial Task 2 metric EE-L and its component parts EE-D and EE-R. Figure 2 plots the runs with the component metrics on the x and y axes. Overall, the submitted systems generally performed better for one of the component metrics than they did for the other. There are, however, a cluster of four points in Figure 2 that make headway in the trade-o\ufb00 between EE-D and EE-L. 6 Limitations The data and metrics in this task address a few speci\ufb01c types of unfairness, and do so partially. This is fundamentally true of any fairness intervention, and does not in any way diminish the value of the e\ufb00ort \u2014 it is impossible for any data set, task de\ufb01nition, or metric to fully capture fairness in a universal way, and all data and analyses have limitations. Some of the limitations of the data and task include: Fairness criteria \u2013 Geography: For each Wikipedia article, we ascertained which, if any, continents are relevant to the content.5 This was determined by directly looking up several community-maintained (Wiki-"}, {"question": " What is the significance of the Task 1 metric M1?", "answer": " It ranks the submitted systems based on the official metric", "ref_chunk": "distribution of fully-known documents (both gender and location are known) with the global intersectional population (global population by location and equality by gender). Average the distribution of documents with unknown location but known gender with the equality gender distribution. Average the distribution of documents with unknown gender but known location with the world pop- ulation. The result is the target group exposure \u03b3\u2217. We use this to measure the expected exposure loss: M2(Lq) = k\u03b3 \u2212 \u03b3\u2217k2 = \u03b3 \u00b7 \u03b3 \u2212 2\u03b3 \u00b7 \u03b3\u2217 + \u03b3\u2217 \u00b7 \u03b3\u2217 EE-D(Lq) = \u03b3\u2217 \u00b7 \u03b3\u2217 EE-R(Lq) = \u03b3 \u00b7 \u03b3\u2217 7 (4) (5) (6) (7) (8) (9) (10) nDCG AWRF Score 95% CI UoGTrDExpDisT1 UoGTrDRelDiT1 UoGTrDivPropT1 UoGTrDExpDisLT1 RUN1 UoGTrRelT1 RMITRet 1step pair 2step pair 1step pair list 2step pair list RMITRetRerank 1 RMITRetRerank 2 0.2071 0.2001 0.2157 0.1776 0.2169 0.2120 0.2075 0.0838 0.0824 0.0820 0.0786 0.0035 0.0035 0.8299 0.8072 0.7112 0.8197 0.6627 0.6559 0.6413 0.6940 0.6943 0.6908 0.6912 0.6180 0.6158 0.1761 0.1639 0.1532 0.1459 0.1425 0.1373 0.1317 0.0648 0.0638 0.0623 0.0607 0.0026 0.0026 (0.145, 0.212) (0.138, 0.193) (0.128, 0.184) (0.122, 0.173) (0.119, 0.172) (0.113, 0.165) (0.110, 0.159) (0.046, 0.090) (0.045, 0.089) (0.045, 0.085) (0.044, 0.083) (0.001, 0.009) (0.001, 0.009) Table 1: Task 1 runs. Higher score is better (for all metrics). Lower M2 is better. It decomposes into two submetrics, the expected exposure disparity (EE-D) that measures overall inequality in exposure independent of relevance, for which lower is better; and the expected exposure relevance (EE-L) that measures exposure/relevance alignment, for which higher is better [1]. 5 Results This year four di\ufb00erent teams submitted a total of 24 runs. All four teams participated in Task 1: Single Rankings (13 runs total), while only three of the four groups participated in Task 2: Multiple Rankings (11 runs total). 5.1 Task 1: WikiProject Coordinators (Single Rankings) Approaches for Task 1 included: RoBERTa model to compute embeddings for text \ufb01elds. A \ufb01ltering approach to select top ranked documents from either competing rankers or the union of rankers. BM25 ranking from pyserini and re-ranked using MMR implicit diversi\ufb01cation (without explicit fairness groups). Lambda varied between runs. BM25 initial ranking with iterative reranking using fairness calculations to select documents to add to the ranking. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion. Optimisation to consider a protected group\u2019s distribution in the background collection and the total predicted relevance of the group in the candidate results set. Allocating positions in the generated ranking to a protected group proportionally with respect to the total relevance score of the group within the candidate results set. 8 0.15 0.05 0.80 0.85AWRF 0.70 0.75 0.60 0.10 0.20nDCG 0.65 0.00 Figure 1: Task 1 submissions by individual component metrics (NDCG and AWRF). Higher values are better for both metrics. Relevance-only approaches. Table 1 shows the submitted systems ranked by the o\ufb03cial Task 1 metric M1 and its component parts nDCG and AWRF. Figure 1 plots the runs with the component metrics on the x and y axes. Notably, each of the approaches from a participating team are clustered in terms of the component metrics and the o\ufb03cial M1 metric. 5.2 Task 2: Wikipedia Editors (Multiple Rankings) Approaches for Task 2 included: A randomized method with BERT and a two-staged Plackett-Luce sampling where relevance scores are combined with work needed. An iterative approach that uses RoBERTa and computes a score for each of the top-K documents in the current state, based on the expected exposure of each group so far and the original estimated relevance score, integrating an article\u2019s quality score. BM25 plus re-ranking iteratively selecting documents by combining relevance, fairness and quality scores. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion to prioritise highly relevant documents while matching the distributions of the protected groups in the generated ranking to their distributions in the background population. Minimising the predicted divergence, or skew, in the distributions of the protected groups over all of the rankings within a sequence, compared to the background population. Minimising the disparity between a group\u2019s expected and actual exposures and learning the importance of the group relevance and background distributions. 9 EE-R EE-D EE-L EE-L 95% CI RUN task2 pl control 0.6 UoGTrRelT2 pl control 0.8 pl control 0.92 PL IRLab 07 PL IRLab 05 UoGTrDivPropT2 UoGTrDRelDiT2 UoGTrDExpDisT2 UoGTrLambT2 9.5508 8.8091 11.8281 8.6654 8.4802 5.2790 4.9331 4.9372 3.4770 3.7459 2.2447 4.1557 3.2733 9.4609 3.2550 3.1486 1.5327 1.4029 7.1005 5.5891 6.1356 3.4644 14.9007 15.5017 15.6514 15.7708 16.0348 20.8213 21.3832 27.0726 28.4816 28.4903 28.8216 (12.303, 19.946) (12.552, 20.477) (13.057, 20.148) (12.746, 21.251) (12.820, 21.158) (16.283, 28.089) (16.579, 28.293) (21.098, 35.870) (22.366, 37.739) (22.571, 37.548) (22.799, 37.718) Table 2: Task 2 runs. Lower EE-L is better. Relevance-only ranking. Table 2 shows the submitted systems ranked by the o\ufb03cial Task 2 metric EE-L and its component parts EE-D and EE-R. Figure 2 plots the runs with the component metrics on the x and y axes. Overall, the submitted systems generally performed better for one of the component metrics than they did for the other. There are, however, a cluster of four points in Figure 2 that make headway in the trade-o\ufb00 between EE-D and EE-L. 6 Limitations The data and metrics in this task address a few speci\ufb01c types of unfairness, and do so partially. This is fundamentally true of any fairness intervention, and does not in any way diminish the value of the e\ufb00ort \u2014 it is impossible for any data set, task de\ufb01nition, or metric to fully capture fairness in a universal way, and all data and analyses have limitations. Some of the limitations of the data and task include: Fairness criteria \u2013 Geography: For each Wikipedia article, we ascertained which, if any, continents are relevant to the content.5 This was determined by directly looking up several community-maintained (Wiki-"}, {"question": " How many teams participated in Task 2?", "answer": " Three of the four groups participated in Task 2", "ref_chunk": "distribution of fully-known documents (both gender and location are known) with the global intersectional population (global population by location and equality by gender). Average the distribution of documents with unknown location but known gender with the equality gender distribution. Average the distribution of documents with unknown gender but known location with the world pop- ulation. The result is the target group exposure \u03b3\u2217. We use this to measure the expected exposure loss: M2(Lq) = k\u03b3 \u2212 \u03b3\u2217k2 = \u03b3 \u00b7 \u03b3 \u2212 2\u03b3 \u00b7 \u03b3\u2217 + \u03b3\u2217 \u00b7 \u03b3\u2217 EE-D(Lq) = \u03b3\u2217 \u00b7 \u03b3\u2217 EE-R(Lq) = \u03b3 \u00b7 \u03b3\u2217 7 (4) (5) (6) (7) (8) (9) (10) nDCG AWRF Score 95% CI UoGTrDExpDisT1 UoGTrDRelDiT1 UoGTrDivPropT1 UoGTrDExpDisLT1 RUN1 UoGTrRelT1 RMITRet 1step pair 2step pair 1step pair list 2step pair list RMITRetRerank 1 RMITRetRerank 2 0.2071 0.2001 0.2157 0.1776 0.2169 0.2120 0.2075 0.0838 0.0824 0.0820 0.0786 0.0035 0.0035 0.8299 0.8072 0.7112 0.8197 0.6627 0.6559 0.6413 0.6940 0.6943 0.6908 0.6912 0.6180 0.6158 0.1761 0.1639 0.1532 0.1459 0.1425 0.1373 0.1317 0.0648 0.0638 0.0623 0.0607 0.0026 0.0026 (0.145, 0.212) (0.138, 0.193) (0.128, 0.184) (0.122, 0.173) (0.119, 0.172) (0.113, 0.165) (0.110, 0.159) (0.046, 0.090) (0.045, 0.089) (0.045, 0.085) (0.044, 0.083) (0.001, 0.009) (0.001, 0.009) Table 1: Task 1 runs. Higher score is better (for all metrics). Lower M2 is better. It decomposes into two submetrics, the expected exposure disparity (EE-D) that measures overall inequality in exposure independent of relevance, for which lower is better; and the expected exposure relevance (EE-L) that measures exposure/relevance alignment, for which higher is better [1]. 5 Results This year four di\ufb00erent teams submitted a total of 24 runs. All four teams participated in Task 1: Single Rankings (13 runs total), while only three of the four groups participated in Task 2: Multiple Rankings (11 runs total). 5.1 Task 1: WikiProject Coordinators (Single Rankings) Approaches for Task 1 included: RoBERTa model to compute embeddings for text \ufb01elds. A \ufb01ltering approach to select top ranked documents from either competing rankers or the union of rankers. BM25 ranking from pyserini and re-ranked using MMR implicit diversi\ufb01cation (without explicit fairness groups). Lambda varied between runs. BM25 initial ranking with iterative reranking using fairness calculations to select documents to add to the ranking. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion. Optimisation to consider a protected group\u2019s distribution in the background collection and the total predicted relevance of the group in the candidate results set. Allocating positions in the generated ranking to a protected group proportionally with respect to the total relevance score of the group within the candidate results set. 8 0.15 0.05 0.80 0.85AWRF 0.70 0.75 0.60 0.10 0.20nDCG 0.65 0.00 Figure 1: Task 1 submissions by individual component metrics (NDCG and AWRF). Higher values are better for both metrics. Relevance-only approaches. Table 1 shows the submitted systems ranked by the o\ufb03cial Task 1 metric M1 and its component parts nDCG and AWRF. Figure 1 plots the runs with the component metrics on the x and y axes. Notably, each of the approaches from a participating team are clustered in terms of the component metrics and the o\ufb03cial M1 metric. 5.2 Task 2: Wikipedia Editors (Multiple Rankings) Approaches for Task 2 included: A randomized method with BERT and a two-staged Plackett-Luce sampling where relevance scores are combined with work needed. An iterative approach that uses RoBERTa and computes a score for each of the top-K documents in the current state, based on the expected exposure of each group so far and the original estimated relevance score, integrating an article\u2019s quality score. BM25 plus re-ranking iteratively selecting documents by combining relevance, fairness and quality scores. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion to prioritise highly relevant documents while matching the distributions of the protected groups in the generated ranking to their distributions in the background population. Minimising the predicted divergence, or skew, in the distributions of the protected groups over all of the rankings within a sequence, compared to the background population. Minimising the disparity between a group\u2019s expected and actual exposures and learning the importance of the group relevance and background distributions. 9 EE-R EE-D EE-L EE-L 95% CI RUN task2 pl control 0.6 UoGTrRelT2 pl control 0.8 pl control 0.92 PL IRLab 07 PL IRLab 05 UoGTrDivPropT2 UoGTrDRelDiT2 UoGTrDExpDisT2 UoGTrLambT2 9.5508 8.8091 11.8281 8.6654 8.4802 5.2790 4.9331 4.9372 3.4770 3.7459 2.2447 4.1557 3.2733 9.4609 3.2550 3.1486 1.5327 1.4029 7.1005 5.5891 6.1356 3.4644 14.9007 15.5017 15.6514 15.7708 16.0348 20.8213 21.3832 27.0726 28.4816 28.4903 28.8216 (12.303, 19.946) (12.552, 20.477) (13.057, 20.148) (12.746, 21.251) (12.820, 21.158) (16.283, 28.089) (16.579, 28.293) (21.098, 35.870) (22.366, 37.739) (22.571, 37.548) (22.799, 37.718) Table 2: Task 2 runs. Lower EE-L is better. Relevance-only ranking. Table 2 shows the submitted systems ranked by the o\ufb03cial Task 2 metric EE-L and its component parts EE-D and EE-R. Figure 2 plots the runs with the component metrics on the x and y axes. Overall, the submitted systems generally performed better for one of the component metrics than they did for the other. There are, however, a cluster of four points in Figure 2 that make headway in the trade-o\ufb00 between EE-D and EE-L. 6 Limitations The data and metrics in this task address a few speci\ufb01c types of unfairness, and do so partially. This is fundamentally true of any fairness intervention, and does not in any way diminish the value of the e\ufb00ort \u2014 it is impossible for any data set, task de\ufb01nition, or metric to fully capture fairness in a universal way, and all data and analyses have limitations. Some of the limitations of the data and task include: Fairness criteria \u2013 Geography: For each Wikipedia article, we ascertained which, if any, continents are relevant to the content.5 This was determined by directly looking up several community-maintained (Wiki-"}, {"question": " What is the key focus of the fairness component in Task 2 approaches?", "answer": " To be fair to both the geographic location attribute and an inferred demographic attribute", "ref_chunk": "distribution of fully-known documents (both gender and location are known) with the global intersectional population (global population by location and equality by gender). Average the distribution of documents with unknown location but known gender with the equality gender distribution. Average the distribution of documents with unknown gender but known location with the world pop- ulation. The result is the target group exposure \u03b3\u2217. We use this to measure the expected exposure loss: M2(Lq) = k\u03b3 \u2212 \u03b3\u2217k2 = \u03b3 \u00b7 \u03b3 \u2212 2\u03b3 \u00b7 \u03b3\u2217 + \u03b3\u2217 \u00b7 \u03b3\u2217 EE-D(Lq) = \u03b3\u2217 \u00b7 \u03b3\u2217 EE-R(Lq) = \u03b3 \u00b7 \u03b3\u2217 7 (4) (5) (6) (7) (8) (9) (10) nDCG AWRF Score 95% CI UoGTrDExpDisT1 UoGTrDRelDiT1 UoGTrDivPropT1 UoGTrDExpDisLT1 RUN1 UoGTrRelT1 RMITRet 1step pair 2step pair 1step pair list 2step pair list RMITRetRerank 1 RMITRetRerank 2 0.2071 0.2001 0.2157 0.1776 0.2169 0.2120 0.2075 0.0838 0.0824 0.0820 0.0786 0.0035 0.0035 0.8299 0.8072 0.7112 0.8197 0.6627 0.6559 0.6413 0.6940 0.6943 0.6908 0.6912 0.6180 0.6158 0.1761 0.1639 0.1532 0.1459 0.1425 0.1373 0.1317 0.0648 0.0638 0.0623 0.0607 0.0026 0.0026 (0.145, 0.212) (0.138, 0.193) (0.128, 0.184) (0.122, 0.173) (0.119, 0.172) (0.113, 0.165) (0.110, 0.159) (0.046, 0.090) (0.045, 0.089) (0.045, 0.085) (0.044, 0.083) (0.001, 0.009) (0.001, 0.009) Table 1: Task 1 runs. Higher score is better (for all metrics). Lower M2 is better. It decomposes into two submetrics, the expected exposure disparity (EE-D) that measures overall inequality in exposure independent of relevance, for which lower is better; and the expected exposure relevance (EE-L) that measures exposure/relevance alignment, for which higher is better [1]. 5 Results This year four di\ufb00erent teams submitted a total of 24 runs. All four teams participated in Task 1: Single Rankings (13 runs total), while only three of the four groups participated in Task 2: Multiple Rankings (11 runs total). 5.1 Task 1: WikiProject Coordinators (Single Rankings) Approaches for Task 1 included: RoBERTa model to compute embeddings for text \ufb01elds. A \ufb01ltering approach to select top ranked documents from either competing rankers or the union of rankers. BM25 ranking from pyserini and re-ranked using MMR implicit diversi\ufb01cation (without explicit fairness groups). Lambda varied between runs. BM25 initial ranking with iterative reranking using fairness calculations to select documents to add to the ranking. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion. Optimisation to consider a protected group\u2019s distribution in the background collection and the total predicted relevance of the group in the candidate results set. Allocating positions in the generated ranking to a protected group proportionally with respect to the total relevance score of the group within the candidate results set. 8 0.15 0.05 0.80 0.85AWRF 0.70 0.75 0.60 0.10 0.20nDCG 0.65 0.00 Figure 1: Task 1 submissions by individual component metrics (NDCG and AWRF). Higher values are better for both metrics. Relevance-only approaches. Table 1 shows the submitted systems ranked by the o\ufb03cial Task 1 metric M1 and its component parts nDCG and AWRF. Figure 1 plots the runs with the component metrics on the x and y axes. Notably, each of the approaches from a participating team are clustered in terms of the component metrics and the o\ufb03cial M1 metric. 5.2 Task 2: Wikipedia Editors (Multiple Rankings) Approaches for Task 2 included: A randomized method with BERT and a two-staged Plackett-Luce sampling where relevance scores are combined with work needed. An iterative approach that uses RoBERTa and computes a score for each of the top-K documents in the current state, based on the expected exposure of each group so far and the original estimated relevance score, integrating an article\u2019s quality score. BM25 plus re-ranking iteratively selecting documents by combining relevance, fairness and quality scores. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion to prioritise highly relevant documents while matching the distributions of the protected groups in the generated ranking to their distributions in the background population. Minimising the predicted divergence, or skew, in the distributions of the protected groups over all of the rankings within a sequence, compared to the background population. Minimising the disparity between a group\u2019s expected and actual exposures and learning the importance of the group relevance and background distributions. 9 EE-R EE-D EE-L EE-L 95% CI RUN task2 pl control 0.6 UoGTrRelT2 pl control 0.8 pl control 0.92 PL IRLab 07 PL IRLab 05 UoGTrDivPropT2 UoGTrDRelDiT2 UoGTrDExpDisT2 UoGTrLambT2 9.5508 8.8091 11.8281 8.6654 8.4802 5.2790 4.9331 4.9372 3.4770 3.7459 2.2447 4.1557 3.2733 9.4609 3.2550 3.1486 1.5327 1.4029 7.1005 5.5891 6.1356 3.4644 14.9007 15.5017 15.6514 15.7708 16.0348 20.8213 21.3832 27.0726 28.4816 28.4903 28.8216 (12.303, 19.946) (12.552, 20.477) (13.057, 20.148) (12.746, 21.251) (12.820, 21.158) (16.283, 28.089) (16.579, 28.293) (21.098, 35.870) (22.366, 37.739) (22.571, 37.548) (22.799, 37.718) Table 2: Task 2 runs. Lower EE-L is better. Relevance-only ranking. Table 2 shows the submitted systems ranked by the o\ufb03cial Task 2 metric EE-L and its component parts EE-D and EE-R. Figure 2 plots the runs with the component metrics on the x and y axes. Overall, the submitted systems generally performed better for one of the component metrics than they did for the other. There are, however, a cluster of four points in Figure 2 that make headway in the trade-o\ufb00 between EE-D and EE-L. 6 Limitations The data and metrics in this task address a few speci\ufb01c types of unfairness, and do so partially. This is fundamentally true of any fairness intervention, and does not in any way diminish the value of the e\ufb00ort \u2014 it is impossible for any data set, task de\ufb01nition, or metric to fully capture fairness in a universal way, and all data and analyses have limitations. Some of the limitations of the data and task include: Fairness criteria \u2013 Geography: For each Wikipedia article, we ascertained which, if any, continents are relevant to the content.5 This was determined by directly looking up several community-maintained (Wiki-"}, {"question": " What are the limitations of the data and metrics discussed in the text?", "answer": " They address specific types of unfairness and have limitations in capturing universal fairness", "ref_chunk": "distribution of fully-known documents (both gender and location are known) with the global intersectional population (global population by location and equality by gender). Average the distribution of documents with unknown location but known gender with the equality gender distribution. Average the distribution of documents with unknown gender but known location with the world pop- ulation. The result is the target group exposure \u03b3\u2217. We use this to measure the expected exposure loss: M2(Lq) = k\u03b3 \u2212 \u03b3\u2217k2 = \u03b3 \u00b7 \u03b3 \u2212 2\u03b3 \u00b7 \u03b3\u2217 + \u03b3\u2217 \u00b7 \u03b3\u2217 EE-D(Lq) = \u03b3\u2217 \u00b7 \u03b3\u2217 EE-R(Lq) = \u03b3 \u00b7 \u03b3\u2217 7 (4) (5) (6) (7) (8) (9) (10) nDCG AWRF Score 95% CI UoGTrDExpDisT1 UoGTrDRelDiT1 UoGTrDivPropT1 UoGTrDExpDisLT1 RUN1 UoGTrRelT1 RMITRet 1step pair 2step pair 1step pair list 2step pair list RMITRetRerank 1 RMITRetRerank 2 0.2071 0.2001 0.2157 0.1776 0.2169 0.2120 0.2075 0.0838 0.0824 0.0820 0.0786 0.0035 0.0035 0.8299 0.8072 0.7112 0.8197 0.6627 0.6559 0.6413 0.6940 0.6943 0.6908 0.6912 0.6180 0.6158 0.1761 0.1639 0.1532 0.1459 0.1425 0.1373 0.1317 0.0648 0.0638 0.0623 0.0607 0.0026 0.0026 (0.145, 0.212) (0.138, 0.193) (0.128, 0.184) (0.122, 0.173) (0.119, 0.172) (0.113, 0.165) (0.110, 0.159) (0.046, 0.090) (0.045, 0.089) (0.045, 0.085) (0.044, 0.083) (0.001, 0.009) (0.001, 0.009) Table 1: Task 1 runs. Higher score is better (for all metrics). Lower M2 is better. It decomposes into two submetrics, the expected exposure disparity (EE-D) that measures overall inequality in exposure independent of relevance, for which lower is better; and the expected exposure relevance (EE-L) that measures exposure/relevance alignment, for which higher is better [1]. 5 Results This year four di\ufb00erent teams submitted a total of 24 runs. All four teams participated in Task 1: Single Rankings (13 runs total), while only three of the four groups participated in Task 2: Multiple Rankings (11 runs total). 5.1 Task 1: WikiProject Coordinators (Single Rankings) Approaches for Task 1 included: RoBERTa model to compute embeddings for text \ufb01elds. A \ufb01ltering approach to select top ranked documents from either competing rankers or the union of rankers. BM25 ranking from pyserini and re-ranked using MMR implicit diversi\ufb01cation (without explicit fairness groups). Lambda varied between runs. BM25 initial ranking with iterative reranking using fairness calculations to select documents to add to the ranking. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion. Optimisation to consider a protected group\u2019s distribution in the background collection and the total predicted relevance of the group in the candidate results set. Allocating positions in the generated ranking to a protected group proportionally with respect to the total relevance score of the group within the candidate results set. 8 0.15 0.05 0.80 0.85AWRF 0.70 0.75 0.60 0.10 0.20nDCG 0.65 0.00 Figure 1: Task 1 submissions by individual component metrics (NDCG and AWRF). Higher values are better for both metrics. Relevance-only approaches. Table 1 shows the submitted systems ranked by the o\ufb03cial Task 1 metric M1 and its component parts nDCG and AWRF. Figure 1 plots the runs with the component metrics on the x and y axes. Notably, each of the approaches from a participating team are clustered in terms of the component metrics and the o\ufb03cial M1 metric. 5.2 Task 2: Wikipedia Editors (Multiple Rankings) Approaches for Task 2 included: A randomized method with BERT and a two-staged Plackett-Luce sampling where relevance scores are combined with work needed. An iterative approach that uses RoBERTa and computes a score for each of the top-K documents in the current state, based on the expected exposure of each group so far and the original estimated relevance score, integrating an article\u2019s quality score. BM25 plus re-ranking iteratively selecting documents by combining relevance, fairness and quality scores. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion to prioritise highly relevant documents while matching the distributions of the protected groups in the generated ranking to their distributions in the background population. Minimising the predicted divergence, or skew, in the distributions of the protected groups over all of the rankings within a sequence, compared to the background population. Minimising the disparity between a group\u2019s expected and actual exposures and learning the importance of the group relevance and background distributions. 9 EE-R EE-D EE-L EE-L 95% CI RUN task2 pl control 0.6 UoGTrRelT2 pl control 0.8 pl control 0.92 PL IRLab 07 PL IRLab 05 UoGTrDivPropT2 UoGTrDRelDiT2 UoGTrDExpDisT2 UoGTrLambT2 9.5508 8.8091 11.8281 8.6654 8.4802 5.2790 4.9331 4.9372 3.4770 3.7459 2.2447 4.1557 3.2733 9.4609 3.2550 3.1486 1.5327 1.4029 7.1005 5.5891 6.1356 3.4644 14.9007 15.5017 15.6514 15.7708 16.0348 20.8213 21.3832 27.0726 28.4816 28.4903 28.8216 (12.303, 19.946) (12.552, 20.477) (13.057, 20.148) (12.746, 21.251) (12.820, 21.158) (16.283, 28.089) (16.579, 28.293) (21.098, 35.870) (22.366, 37.739) (22.571, 37.548) (22.799, 37.718) Table 2: Task 2 runs. Lower EE-L is better. Relevance-only ranking. Table 2 shows the submitted systems ranked by the o\ufb03cial Task 2 metric EE-L and its component parts EE-D and EE-R. Figure 2 plots the runs with the component metrics on the x and y axes. Overall, the submitted systems generally performed better for one of the component metrics than they did for the other. There are, however, a cluster of four points in Figure 2 that make headway in the trade-o\ufb00 between EE-D and EE-L. 6 Limitations The data and metrics in this task address a few speci\ufb01c types of unfairness, and do so partially. This is fundamentally true of any fairness intervention, and does not in any way diminish the value of the e\ufb00ort \u2014 it is impossible for any data set, task de\ufb01nition, or metric to fully capture fairness in a universal way, and all data and analyses have limitations. Some of the limitations of the data and task include: Fairness criteria \u2013 Geography: For each Wikipedia article, we ascertained which, if any, continents are relevant to the content.5 This was determined by directly looking up several community-maintained (Wiki-"}], "doc_text": "distribution of fully-known documents (both gender and location are known) with the global intersectional population (global population by location and equality by gender). Average the distribution of documents with unknown location but known gender with the equality gender distribution. Average the distribution of documents with unknown gender but known location with the world pop- ulation. The result is the target group exposure \u03b3\u2217. We use this to measure the expected exposure loss: M2(Lq) = k\u03b3 \u2212 \u03b3\u2217k2 = \u03b3 \u00b7 \u03b3 \u2212 2\u03b3 \u00b7 \u03b3\u2217 + \u03b3\u2217 \u00b7 \u03b3\u2217 EE-D(Lq) = \u03b3\u2217 \u00b7 \u03b3\u2217 EE-R(Lq) = \u03b3 \u00b7 \u03b3\u2217 7 (4) (5) (6) (7) (8) (9) (10) nDCG AWRF Score 95% CI UoGTrDExpDisT1 UoGTrDRelDiT1 UoGTrDivPropT1 UoGTrDExpDisLT1 RUN1 UoGTrRelT1 RMITRet 1step pair 2step pair 1step pair list 2step pair list RMITRetRerank 1 RMITRetRerank 2 0.2071 0.2001 0.2157 0.1776 0.2169 0.2120 0.2075 0.0838 0.0824 0.0820 0.0786 0.0035 0.0035 0.8299 0.8072 0.7112 0.8197 0.6627 0.6559 0.6413 0.6940 0.6943 0.6908 0.6912 0.6180 0.6158 0.1761 0.1639 0.1532 0.1459 0.1425 0.1373 0.1317 0.0648 0.0638 0.0623 0.0607 0.0026 0.0026 (0.145, 0.212) (0.138, 0.193) (0.128, 0.184) (0.122, 0.173) (0.119, 0.172) (0.113, 0.165) (0.110, 0.159) (0.046, 0.090) (0.045, 0.089) (0.045, 0.085) (0.044, 0.083) (0.001, 0.009) (0.001, 0.009) Table 1: Task 1 runs. Higher score is better (for all metrics). Lower M2 is better. It decomposes into two submetrics, the expected exposure disparity (EE-D) that measures overall inequality in exposure independent of relevance, for which lower is better; and the expected exposure relevance (EE-L) that measures exposure/relevance alignment, for which higher is better [1]. 5 Results This year four di\ufb00erent teams submitted a total of 24 runs. All four teams participated in Task 1: Single Rankings (13 runs total), while only three of the four groups participated in Task 2: Multiple Rankings (11 runs total). 5.1 Task 1: WikiProject Coordinators (Single Rankings) Approaches for Task 1 included: RoBERTa model to compute embeddings for text \ufb01elds. A \ufb01ltering approach to select top ranked documents from either competing rankers or the union of rankers. BM25 ranking from pyserini and re-ranked using MMR implicit diversi\ufb01cation (without explicit fairness groups). Lambda varied between runs. BM25 initial ranking with iterative reranking using fairness calculations to select documents to add to the ranking. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion. Optimisation to consider a protected group\u2019s distribution in the background collection and the total predicted relevance of the group in the candidate results set. Allocating positions in the generated ranking to a protected group proportionally with respect to the total relevance score of the group within the candidate results set. 8 0.15 0.05 0.80 0.85AWRF 0.70 0.75 0.60 0.10 0.20nDCG 0.65 0.00 Figure 1: Task 1 submissions by individual component metrics (NDCG and AWRF). Higher values are better for both metrics. Relevance-only approaches. Table 1 shows the submitted systems ranked by the o\ufb03cial Task 1 metric M1 and its component parts nDCG and AWRF. Figure 1 plots the runs with the component metrics on the x and y axes. Notably, each of the approaches from a participating team are clustered in terms of the component metrics and the o\ufb03cial M1 metric. 5.2 Task 2: Wikipedia Editors (Multiple Rankings) Approaches for Task 2 included: A randomized method with BERT and a two-staged Plackett-Luce sampling where relevance scores are combined with work needed. An iterative approach that uses RoBERTa and computes a score for each of the top-K documents in the current state, based on the expected exposure of each group so far and the original estimated relevance score, integrating an article\u2019s quality score. BM25 plus re-ranking iteratively selecting documents by combining relevance, fairness and quality scores. Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversi\ufb01cation plus data fusion to prioritise highly relevant documents while matching the distributions of the protected groups in the generated ranking to their distributions in the background population. Minimising the predicted divergence, or skew, in the distributions of the protected groups over all of the rankings within a sequence, compared to the background population. Minimising the disparity between a group\u2019s expected and actual exposures and learning the importance of the group relevance and background distributions. 9 EE-R EE-D EE-L EE-L 95% CI RUN task2 pl control 0.6 UoGTrRelT2 pl control 0.8 pl control 0.92 PL IRLab 07 PL IRLab 05 UoGTrDivPropT2 UoGTrDRelDiT2 UoGTrDExpDisT2 UoGTrLambT2 9.5508 8.8091 11.8281 8.6654 8.4802 5.2790 4.9331 4.9372 3.4770 3.7459 2.2447 4.1557 3.2733 9.4609 3.2550 3.1486 1.5327 1.4029 7.1005 5.5891 6.1356 3.4644 14.9007 15.5017 15.6514 15.7708 16.0348 20.8213 21.3832 27.0726 28.4816 28.4903 28.8216 (12.303, 19.946) (12.552, 20.477) (13.057, 20.148) (12.746, 21.251) (12.820, 21.158) (16.283, 28.089) (16.579, 28.293) (21.098, 35.870) (22.366, 37.739) (22.571, 37.548) (22.799, 37.718) Table 2: Task 2 runs. Lower EE-L is better. Relevance-only ranking. Table 2 shows the submitted systems ranked by the o\ufb03cial Task 2 metric EE-L and its component parts EE-D and EE-R. Figure 2 plots the runs with the component metrics on the x and y axes. Overall, the submitted systems generally performed better for one of the component metrics than they did for the other. There are, however, a cluster of four points in Figure 2 that make headway in the trade-o\ufb00 between EE-D and EE-L. 6 Limitations The data and metrics in this task address a few speci\ufb01c types of unfairness, and do so partially. This is fundamentally true of any fairness intervention, and does not in any way diminish the value of the e\ufb00ort \u2014 it is impossible for any data set, task de\ufb01nition, or metric to fully capture fairness in a universal way, and all data and analyses have limitations. Some of the limitations of the data and task include: Fairness criteria \u2013 Geography: For each Wikipedia article, we ascertained which, if any, continents are relevant to the content.5 This was determined by directly looking up several community-maintained (Wiki-"}