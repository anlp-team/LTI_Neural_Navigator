{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_SantaCoder:_don't_reach_for_the_stars!_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the expected trade-off between precision and recall in the deduplication process?,        answer: Increasing the n-gram size reduces false positives, but also increases false negatives.    ", "ref_chunk": "Match (alphanumeric token sequence) Exact Match (sha-256) Exact Match (non-whitespace text) Exact Match (hash) Near-deduplication (MinHash) Exact Match (\u201dunique python \ufb01les\u201d) Table 4: Various deduplication methods adopted for different model training data. calculation leads to many false positives, around 20% at 0.85. Increasing the n-gram size reduces false positives, but also increases false negatives. This is an expected trade-off between precision and recall; 2) A lower threshold would cause more documents to be removed at the cost of processing time. In our experiments, we have observed good duplicates occur with a similarity as low as 0.65, even though the FP and FN rates don\u2019t change much. We \ufb01nd that combining 5-grams and a 0.7 threshold strikes a good balance between false positives and false negatives while removing an additional 16%\u201320% \ufb01les. In particular, the increased false negatives occur mostly among documents with lower real Jaccard similarity bounds, whereas doc- uments with higher similarities (> 0.85) even have a decreased false negative rate (from 35% to 24%). Due to time constraints, we apply such deduplication on the already deduplicated datasets using the Stack v1 hyperparameters. We will refer to the \ufb01nal results as more near-deduplication or near-deduplication alt. Unlike other data preprocessing or \ufb01ltering techniques that target one document at a time, near- deduplication requires a centralized index that can be prohibitive for large data processing. We have released the deduplication code used in this paper on GitHub10 and will release a distributed version soon. For reference, it takes about 10 hours to deduplicate 42 million Java documents using plain multiprocessing while it takes less than 40 minutes in a distributed (but comparable) environment. Tokenizer fertility Can we use the tokenizer to remove low-quality \ufb01les from the dataset? We experiment with \ufb01ltering \ufb01les with a low character-to-token ratio11. For each language, we \ufb01nd that \ufb01les with a ratio below the 5th percentile are usually of poor quality, but increasing the threshold may eliminate some good-quality \ufb01les. We therefore set the cutoff value for this ratio to the following values: 2.5 for Python, 2.9 for Java, and 2.6 for JavaScript. This \ufb01lters out roughly 4% to 5% of data. Note that these values depend highly on the tokenizer and the data. This \ufb01lter may also be biased against \ufb01les with non-English comments. 5.4 EVALUATION Text2code evaluation The text2code task involves generating the body of a function from a prompt that includes a function description, the function signature (its name and arguments), and optionally a handful of example inputs and outputs. Every problem is accompanied by a set of hidden test cases, which are used to determine if the generated function is correct. We use the MultiPL-E text2code benchmark Cassano et al. (2022), which is derived from HumanEval Chen et al. (2021) and MBPP Austin et al. (2021) (the \u201csanitized\u201d subset of MBPP.). Whereas the latter two benchmarks target Python, MultiPL-E has a suite of compilers that translate HumanEval and MBPP to 18 other programming languages. Since our models are only trained on Java, JavaScript, and Python, we only evaluate them on these three languages. We use the methodology of Chen et al. (2021) and we calculate pass@k rates for (k = 1, 10, 100) for every problem. Intuitively, pass@1 estimates the likelihood a model will generate a correct solution in a single attempt, whereas pass@10 and pass@100 estimate the likelihood that the model will generate a correct solution given 10 and 100 attempts respectively. Following the literature, 10https://github.com/bigcode-project/bigcode-dataset 11We slightly abuse the term tokenizer fertility in this work as it usually refers to the average number of subwords per token, where a token is determined by the true tokenizer of the programming language. See e.g. (Rust et al., 2021) 8 Preprint Language Attention FIM HumanEval MBPP Java Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.35 0.36 0.37 0.54 0.55 0.55 JavaScript Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.33 0.37 0.37 0.64 0.67 0.65 Python Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.36 0.38 0.39 0.67 0.70 0.68 Table 5: Pass@100 results for the architecture ablations on HumanEval and MBPP. Model Java JavaScript Python Baseline GitHub stars Comments-to-code More near deduplication Tokenizer fertility 0.64 0.54 0.62 0.66 0.67 0.61 0.57 0.59 0.57 0.65 0.42 0.37 0.44 0.45 0.45 Final 0.62 0.60 0.44 Table 6: Fill-in-the-middle results for the data \ufb01ltering ablations on MultiPL-HumanEval. Each number reports the fraction of lines where the model exactly reproduces a single line of code that is held out from the body of a function in a held out problem. we sample 200 completions at temperatures 0.2 and 0.8 and use 0.2 to estimate pass@1 and 0.8 for pass@10 and pass@100. Fill-in-the-middle evaluation To evaluate \ufb01ll-in-the-middle, we use the single-line exact match metric, which was introduced by Fried et al. (2022) and also employed by Bavarian et al. (2022). For every benchmark problem, we mask out a single line of text from the function body (i.e., not from the function description or signature), and prompt the model to \ufb01ll in that line of code. We exclude blank lines and comments, and count the number of times the model produces exactly the masked out line. This benchmark requires working solutions for problems, which MultiPL-E does not have. (A text2code benchmark like MultiPL-E only needs hidden tests.) Instead, of writing solutions by hand, we use solutions generated by a code generation model, which is the approach of Athiwaratkun et al. (2022). Speci\ufb01cally, we use working solutions produced by code-davinci-002 at temperature 0.8. Note that this approach does not produce solutions to every problem, since not all problems are solvable. Moreover, for uniformity, we use this approach for Python as well, even though hand- written Python solutions exist for our benchmarks. We only report \ufb01ll-in-the-middle evaluations for the data \ufb01ltering ablations. 6 RESULTS 6.1 ABLATIONS For the architecture ablations, we report the results on text2code benchmarks in Table 5. For the data \ufb01ltering ablations, we"}, {"question": " At what similarity level have good duplicates been observed in the experiments mentioned?,        answer: 0.65    ", "ref_chunk": "Match (alphanumeric token sequence) Exact Match (sha-256) Exact Match (non-whitespace text) Exact Match (hash) Near-deduplication (MinHash) Exact Match (\u201dunique python \ufb01les\u201d) Table 4: Various deduplication methods adopted for different model training data. calculation leads to many false positives, around 20% at 0.85. Increasing the n-gram size reduces false positives, but also increases false negatives. This is an expected trade-off between precision and recall; 2) A lower threshold would cause more documents to be removed at the cost of processing time. In our experiments, we have observed good duplicates occur with a similarity as low as 0.65, even though the FP and FN rates don\u2019t change much. We \ufb01nd that combining 5-grams and a 0.7 threshold strikes a good balance between false positives and false negatives while removing an additional 16%\u201320% \ufb01les. In particular, the increased false negatives occur mostly among documents with lower real Jaccard similarity bounds, whereas doc- uments with higher similarities (> 0.85) even have a decreased false negative rate (from 35% to 24%). Due to time constraints, we apply such deduplication on the already deduplicated datasets using the Stack v1 hyperparameters. We will refer to the \ufb01nal results as more near-deduplication or near-deduplication alt. Unlike other data preprocessing or \ufb01ltering techniques that target one document at a time, near- deduplication requires a centralized index that can be prohibitive for large data processing. We have released the deduplication code used in this paper on GitHub10 and will release a distributed version soon. For reference, it takes about 10 hours to deduplicate 42 million Java documents using plain multiprocessing while it takes less than 40 minutes in a distributed (but comparable) environment. Tokenizer fertility Can we use the tokenizer to remove low-quality \ufb01les from the dataset? We experiment with \ufb01ltering \ufb01les with a low character-to-token ratio11. For each language, we \ufb01nd that \ufb01les with a ratio below the 5th percentile are usually of poor quality, but increasing the threshold may eliminate some good-quality \ufb01les. We therefore set the cutoff value for this ratio to the following values: 2.5 for Python, 2.9 for Java, and 2.6 for JavaScript. This \ufb01lters out roughly 4% to 5% of data. Note that these values depend highly on the tokenizer and the data. This \ufb01lter may also be biased against \ufb01les with non-English comments. 5.4 EVALUATION Text2code evaluation The text2code task involves generating the body of a function from a prompt that includes a function description, the function signature (its name and arguments), and optionally a handful of example inputs and outputs. Every problem is accompanied by a set of hidden test cases, which are used to determine if the generated function is correct. We use the MultiPL-E text2code benchmark Cassano et al. (2022), which is derived from HumanEval Chen et al. (2021) and MBPP Austin et al. (2021) (the \u201csanitized\u201d subset of MBPP.). Whereas the latter two benchmarks target Python, MultiPL-E has a suite of compilers that translate HumanEval and MBPP to 18 other programming languages. Since our models are only trained on Java, JavaScript, and Python, we only evaluate them on these three languages. We use the methodology of Chen et al. (2021) and we calculate pass@k rates for (k = 1, 10, 100) for every problem. Intuitively, pass@1 estimates the likelihood a model will generate a correct solution in a single attempt, whereas pass@10 and pass@100 estimate the likelihood that the model will generate a correct solution given 10 and 100 attempts respectively. Following the literature, 10https://github.com/bigcode-project/bigcode-dataset 11We slightly abuse the term tokenizer fertility in this work as it usually refers to the average number of subwords per token, where a token is determined by the true tokenizer of the programming language. See e.g. (Rust et al., 2021) 8 Preprint Language Attention FIM HumanEval MBPP Java Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.35 0.36 0.37 0.54 0.55 0.55 JavaScript Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.33 0.37 0.37 0.64 0.67 0.65 Python Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.36 0.38 0.39 0.67 0.70 0.68 Table 5: Pass@100 results for the architecture ablations on HumanEval and MBPP. Model Java JavaScript Python Baseline GitHub stars Comments-to-code More near deduplication Tokenizer fertility 0.64 0.54 0.62 0.66 0.67 0.61 0.57 0.59 0.57 0.65 0.42 0.37 0.44 0.45 0.45 Final 0.62 0.60 0.44 Table 6: Fill-in-the-middle results for the data \ufb01ltering ablations on MultiPL-HumanEval. Each number reports the fraction of lines where the model exactly reproduces a single line of code that is held out from the body of a function in a held out problem. we sample 200 completions at temperatures 0.2 and 0.8 and use 0.2 to estimate pass@1 and 0.8 for pass@10 and pass@100. Fill-in-the-middle evaluation To evaluate \ufb01ll-in-the-middle, we use the single-line exact match metric, which was introduced by Fried et al. (2022) and also employed by Bavarian et al. (2022). For every benchmark problem, we mask out a single line of text from the function body (i.e., not from the function description or signature), and prompt the model to \ufb01ll in that line of code. We exclude blank lines and comments, and count the number of times the model produces exactly the masked out line. This benchmark requires working solutions for problems, which MultiPL-E does not have. (A text2code benchmark like MultiPL-E only needs hidden tests.) Instead, of writing solutions by hand, we use solutions generated by a code generation model, which is the approach of Athiwaratkun et al. (2022). Speci\ufb01cally, we use working solutions produced by code-davinci-002 at temperature 0.8. Note that this approach does not produce solutions to every problem, since not all problems are solvable. Moreover, for uniformity, we use this approach for Python as well, even though hand- written Python solutions exist for our benchmarks. We only report \ufb01ll-in-the-middle evaluations for the data \ufb01ltering ablations. 6 RESULTS 6.1 ABLATIONS For the architecture ablations, we report the results on text2code benchmarks in Table 5. For the data \ufb01ltering ablations, we"}, {"question": " What combination of n-grams and threshold was found to strike a good balance between false positives and false negatives?,        answer: 5-grams and a 0.7 threshold    ", "ref_chunk": "Match (alphanumeric token sequence) Exact Match (sha-256) Exact Match (non-whitespace text) Exact Match (hash) Near-deduplication (MinHash) Exact Match (\u201dunique python \ufb01les\u201d) Table 4: Various deduplication methods adopted for different model training data. calculation leads to many false positives, around 20% at 0.85. Increasing the n-gram size reduces false positives, but also increases false negatives. This is an expected trade-off between precision and recall; 2) A lower threshold would cause more documents to be removed at the cost of processing time. In our experiments, we have observed good duplicates occur with a similarity as low as 0.65, even though the FP and FN rates don\u2019t change much. We \ufb01nd that combining 5-grams and a 0.7 threshold strikes a good balance between false positives and false negatives while removing an additional 16%\u201320% \ufb01les. In particular, the increased false negatives occur mostly among documents with lower real Jaccard similarity bounds, whereas doc- uments with higher similarities (> 0.85) even have a decreased false negative rate (from 35% to 24%). Due to time constraints, we apply such deduplication on the already deduplicated datasets using the Stack v1 hyperparameters. We will refer to the \ufb01nal results as more near-deduplication or near-deduplication alt. Unlike other data preprocessing or \ufb01ltering techniques that target one document at a time, near- deduplication requires a centralized index that can be prohibitive for large data processing. We have released the deduplication code used in this paper on GitHub10 and will release a distributed version soon. For reference, it takes about 10 hours to deduplicate 42 million Java documents using plain multiprocessing while it takes less than 40 minutes in a distributed (but comparable) environment. Tokenizer fertility Can we use the tokenizer to remove low-quality \ufb01les from the dataset? We experiment with \ufb01ltering \ufb01les with a low character-to-token ratio11. For each language, we \ufb01nd that \ufb01les with a ratio below the 5th percentile are usually of poor quality, but increasing the threshold may eliminate some good-quality \ufb01les. We therefore set the cutoff value for this ratio to the following values: 2.5 for Python, 2.9 for Java, and 2.6 for JavaScript. This \ufb01lters out roughly 4% to 5% of data. Note that these values depend highly on the tokenizer and the data. This \ufb01lter may also be biased against \ufb01les with non-English comments. 5.4 EVALUATION Text2code evaluation The text2code task involves generating the body of a function from a prompt that includes a function description, the function signature (its name and arguments), and optionally a handful of example inputs and outputs. Every problem is accompanied by a set of hidden test cases, which are used to determine if the generated function is correct. We use the MultiPL-E text2code benchmark Cassano et al. (2022), which is derived from HumanEval Chen et al. (2021) and MBPP Austin et al. (2021) (the \u201csanitized\u201d subset of MBPP.). Whereas the latter two benchmarks target Python, MultiPL-E has a suite of compilers that translate HumanEval and MBPP to 18 other programming languages. Since our models are only trained on Java, JavaScript, and Python, we only evaluate them on these three languages. We use the methodology of Chen et al. (2021) and we calculate pass@k rates for (k = 1, 10, 100) for every problem. Intuitively, pass@1 estimates the likelihood a model will generate a correct solution in a single attempt, whereas pass@10 and pass@100 estimate the likelihood that the model will generate a correct solution given 10 and 100 attempts respectively. Following the literature, 10https://github.com/bigcode-project/bigcode-dataset 11We slightly abuse the term tokenizer fertility in this work as it usually refers to the average number of subwords per token, where a token is determined by the true tokenizer of the programming language. See e.g. (Rust et al., 2021) 8 Preprint Language Attention FIM HumanEval MBPP Java Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.35 0.36 0.37 0.54 0.55 0.55 JavaScript Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.33 0.37 0.37 0.64 0.67 0.65 Python Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.36 0.38 0.39 0.67 0.70 0.68 Table 5: Pass@100 results for the architecture ablations on HumanEval and MBPP. Model Java JavaScript Python Baseline GitHub stars Comments-to-code More near deduplication Tokenizer fertility 0.64 0.54 0.62 0.66 0.67 0.61 0.57 0.59 0.57 0.65 0.42 0.37 0.44 0.45 0.45 Final 0.62 0.60 0.44 Table 6: Fill-in-the-middle results for the data \ufb01ltering ablations on MultiPL-HumanEval. Each number reports the fraction of lines where the model exactly reproduces a single line of code that is held out from the body of a function in a held out problem. we sample 200 completions at temperatures 0.2 and 0.8 and use 0.2 to estimate pass@1 and 0.8 for pass@10 and pass@100. Fill-in-the-middle evaluation To evaluate \ufb01ll-in-the-middle, we use the single-line exact match metric, which was introduced by Fried et al. (2022) and also employed by Bavarian et al. (2022). For every benchmark problem, we mask out a single line of text from the function body (i.e., not from the function description or signature), and prompt the model to \ufb01ll in that line of code. We exclude blank lines and comments, and count the number of times the model produces exactly the masked out line. This benchmark requires working solutions for problems, which MultiPL-E does not have. (A text2code benchmark like MultiPL-E only needs hidden tests.) Instead, of writing solutions by hand, we use solutions generated by a code generation model, which is the approach of Athiwaratkun et al. (2022). Speci\ufb01cally, we use working solutions produced by code-davinci-002 at temperature 0.8. Note that this approach does not produce solutions to every problem, since not all problems are solvable. Moreover, for uniformity, we use this approach for Python as well, even though hand- written Python solutions exist for our benchmarks. We only report \ufb01ll-in-the-middle evaluations for the data \ufb01ltering ablations. 6 RESULTS 6.1 ABLATIONS For the architecture ablations, we report the results on text2code benchmarks in Table 5. For the data \ufb01ltering ablations, we"}, {"question": " What type of documents mostly experience increased false negatives in the deduplication process?,        answer: Documents with lower real Jaccard similarity bounds    ", "ref_chunk": "Match (alphanumeric token sequence) Exact Match (sha-256) Exact Match (non-whitespace text) Exact Match (hash) Near-deduplication (MinHash) Exact Match (\u201dunique python \ufb01les\u201d) Table 4: Various deduplication methods adopted for different model training data. calculation leads to many false positives, around 20% at 0.85. Increasing the n-gram size reduces false positives, but also increases false negatives. This is an expected trade-off between precision and recall; 2) A lower threshold would cause more documents to be removed at the cost of processing time. In our experiments, we have observed good duplicates occur with a similarity as low as 0.65, even though the FP and FN rates don\u2019t change much. We \ufb01nd that combining 5-grams and a 0.7 threshold strikes a good balance between false positives and false negatives while removing an additional 16%\u201320% \ufb01les. In particular, the increased false negatives occur mostly among documents with lower real Jaccard similarity bounds, whereas doc- uments with higher similarities (> 0.85) even have a decreased false negative rate (from 35% to 24%). Due to time constraints, we apply such deduplication on the already deduplicated datasets using the Stack v1 hyperparameters. We will refer to the \ufb01nal results as more near-deduplication or near-deduplication alt. Unlike other data preprocessing or \ufb01ltering techniques that target one document at a time, near- deduplication requires a centralized index that can be prohibitive for large data processing. We have released the deduplication code used in this paper on GitHub10 and will release a distributed version soon. For reference, it takes about 10 hours to deduplicate 42 million Java documents using plain multiprocessing while it takes less than 40 minutes in a distributed (but comparable) environment. Tokenizer fertility Can we use the tokenizer to remove low-quality \ufb01les from the dataset? We experiment with \ufb01ltering \ufb01les with a low character-to-token ratio11. For each language, we \ufb01nd that \ufb01les with a ratio below the 5th percentile are usually of poor quality, but increasing the threshold may eliminate some good-quality \ufb01les. We therefore set the cutoff value for this ratio to the following values: 2.5 for Python, 2.9 for Java, and 2.6 for JavaScript. This \ufb01lters out roughly 4% to 5% of data. Note that these values depend highly on the tokenizer and the data. This \ufb01lter may also be biased against \ufb01les with non-English comments. 5.4 EVALUATION Text2code evaluation The text2code task involves generating the body of a function from a prompt that includes a function description, the function signature (its name and arguments), and optionally a handful of example inputs and outputs. Every problem is accompanied by a set of hidden test cases, which are used to determine if the generated function is correct. We use the MultiPL-E text2code benchmark Cassano et al. (2022), which is derived from HumanEval Chen et al. (2021) and MBPP Austin et al. (2021) (the \u201csanitized\u201d subset of MBPP.). Whereas the latter two benchmarks target Python, MultiPL-E has a suite of compilers that translate HumanEval and MBPP to 18 other programming languages. Since our models are only trained on Java, JavaScript, and Python, we only evaluate them on these three languages. We use the methodology of Chen et al. (2021) and we calculate pass@k rates for (k = 1, 10, 100) for every problem. Intuitively, pass@1 estimates the likelihood a model will generate a correct solution in a single attempt, whereas pass@10 and pass@100 estimate the likelihood that the model will generate a correct solution given 10 and 100 attempts respectively. Following the literature, 10https://github.com/bigcode-project/bigcode-dataset 11We slightly abuse the term tokenizer fertility in this work as it usually refers to the average number of subwords per token, where a token is determined by the true tokenizer of the programming language. See e.g. (Rust et al., 2021) 8 Preprint Language Attention FIM HumanEval MBPP Java Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.35 0.36 0.37 0.54 0.55 0.55 JavaScript Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.33 0.37 0.37 0.64 0.67 0.65 Python Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.36 0.38 0.39 0.67 0.70 0.68 Table 5: Pass@100 results for the architecture ablations on HumanEval and MBPP. Model Java JavaScript Python Baseline GitHub stars Comments-to-code More near deduplication Tokenizer fertility 0.64 0.54 0.62 0.66 0.67 0.61 0.57 0.59 0.57 0.65 0.42 0.37 0.44 0.45 0.45 Final 0.62 0.60 0.44 Table 6: Fill-in-the-middle results for the data \ufb01ltering ablations on MultiPL-HumanEval. Each number reports the fraction of lines where the model exactly reproduces a single line of code that is held out from the body of a function in a held out problem. we sample 200 completions at temperatures 0.2 and 0.8 and use 0.2 to estimate pass@1 and 0.8 for pass@10 and pass@100. Fill-in-the-middle evaluation To evaluate \ufb01ll-in-the-middle, we use the single-line exact match metric, which was introduced by Fried et al. (2022) and also employed by Bavarian et al. (2022). For every benchmark problem, we mask out a single line of text from the function body (i.e., not from the function description or signature), and prompt the model to \ufb01ll in that line of code. We exclude blank lines and comments, and count the number of times the model produces exactly the masked out line. This benchmark requires working solutions for problems, which MultiPL-E does not have. (A text2code benchmark like MultiPL-E only needs hidden tests.) Instead, of writing solutions by hand, we use solutions generated by a code generation model, which is the approach of Athiwaratkun et al. (2022). Speci\ufb01cally, we use working solutions produced by code-davinci-002 at temperature 0.8. Note that this approach does not produce solutions to every problem, since not all problems are solvable. Moreover, for uniformity, we use this approach for Python as well, even though hand- written Python solutions exist for our benchmarks. We only report \ufb01ll-in-the-middle evaluations for the data \ufb01ltering ablations. 6 RESULTS 6.1 ABLATIONS For the architecture ablations, we report the results on text2code benchmarks in Table 5. For the data \ufb01ltering ablations, we"}, {"question": " How does the time required for deduplication change when using plain multiprocessing versus a distributed environment?,        answer: Less than 40 minutes in a distributed environment compared to about 10 hours using plain multiprocessing    ", "ref_chunk": "Match (alphanumeric token sequence) Exact Match (sha-256) Exact Match (non-whitespace text) Exact Match (hash) Near-deduplication (MinHash) Exact Match (\u201dunique python \ufb01les\u201d) Table 4: Various deduplication methods adopted for different model training data. calculation leads to many false positives, around 20% at 0.85. Increasing the n-gram size reduces false positives, but also increases false negatives. This is an expected trade-off between precision and recall; 2) A lower threshold would cause more documents to be removed at the cost of processing time. In our experiments, we have observed good duplicates occur with a similarity as low as 0.65, even though the FP and FN rates don\u2019t change much. We \ufb01nd that combining 5-grams and a 0.7 threshold strikes a good balance between false positives and false negatives while removing an additional 16%\u201320% \ufb01les. In particular, the increased false negatives occur mostly among documents with lower real Jaccard similarity bounds, whereas doc- uments with higher similarities (> 0.85) even have a decreased false negative rate (from 35% to 24%). Due to time constraints, we apply such deduplication on the already deduplicated datasets using the Stack v1 hyperparameters. We will refer to the \ufb01nal results as more near-deduplication or near-deduplication alt. Unlike other data preprocessing or \ufb01ltering techniques that target one document at a time, near- deduplication requires a centralized index that can be prohibitive for large data processing. We have released the deduplication code used in this paper on GitHub10 and will release a distributed version soon. For reference, it takes about 10 hours to deduplicate 42 million Java documents using plain multiprocessing while it takes less than 40 minutes in a distributed (but comparable) environment. Tokenizer fertility Can we use the tokenizer to remove low-quality \ufb01les from the dataset? We experiment with \ufb01ltering \ufb01les with a low character-to-token ratio11. For each language, we \ufb01nd that \ufb01les with a ratio below the 5th percentile are usually of poor quality, but increasing the threshold may eliminate some good-quality \ufb01les. We therefore set the cutoff value for this ratio to the following values: 2.5 for Python, 2.9 for Java, and 2.6 for JavaScript. This \ufb01lters out roughly 4% to 5% of data. Note that these values depend highly on the tokenizer and the data. This \ufb01lter may also be biased against \ufb01les with non-English comments. 5.4 EVALUATION Text2code evaluation The text2code task involves generating the body of a function from a prompt that includes a function description, the function signature (its name and arguments), and optionally a handful of example inputs and outputs. Every problem is accompanied by a set of hidden test cases, which are used to determine if the generated function is correct. We use the MultiPL-E text2code benchmark Cassano et al. (2022), which is derived from HumanEval Chen et al. (2021) and MBPP Austin et al. (2021) (the \u201csanitized\u201d subset of MBPP.). Whereas the latter two benchmarks target Python, MultiPL-E has a suite of compilers that translate HumanEval and MBPP to 18 other programming languages. Since our models are only trained on Java, JavaScript, and Python, we only evaluate them on these three languages. We use the methodology of Chen et al. (2021) and we calculate pass@k rates for (k = 1, 10, 100) for every problem. Intuitively, pass@1 estimates the likelihood a model will generate a correct solution in a single attempt, whereas pass@10 and pass@100 estimate the likelihood that the model will generate a correct solution given 10 and 100 attempts respectively. Following the literature, 10https://github.com/bigcode-project/bigcode-dataset 11We slightly abuse the term tokenizer fertility in this work as it usually refers to the average number of subwords per token, where a token is determined by the true tokenizer of the programming language. See e.g. (Rust et al., 2021) 8 Preprint Language Attention FIM HumanEval MBPP Java Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.35 0.36 0.37 0.54 0.55 0.55 JavaScript Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.33 0.37 0.37 0.64 0.67 0.65 Python Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.36 0.38 0.39 0.67 0.70 0.68 Table 5: Pass@100 results for the architecture ablations on HumanEval and MBPP. Model Java JavaScript Python Baseline GitHub stars Comments-to-code More near deduplication Tokenizer fertility 0.64 0.54 0.62 0.66 0.67 0.61 0.57 0.59 0.57 0.65 0.42 0.37 0.44 0.45 0.45 Final 0.62 0.60 0.44 Table 6: Fill-in-the-middle results for the data \ufb01ltering ablations on MultiPL-HumanEval. Each number reports the fraction of lines where the model exactly reproduces a single line of code that is held out from the body of a function in a held out problem. we sample 200 completions at temperatures 0.2 and 0.8 and use 0.2 to estimate pass@1 and 0.8 for pass@10 and pass@100. Fill-in-the-middle evaluation To evaluate \ufb01ll-in-the-middle, we use the single-line exact match metric, which was introduced by Fried et al. (2022) and also employed by Bavarian et al. (2022). For every benchmark problem, we mask out a single line of text from the function body (i.e., not from the function description or signature), and prompt the model to \ufb01ll in that line of code. We exclude blank lines and comments, and count the number of times the model produces exactly the masked out line. This benchmark requires working solutions for problems, which MultiPL-E does not have. (A text2code benchmark like MultiPL-E only needs hidden tests.) Instead, of writing solutions by hand, we use solutions generated by a code generation model, which is the approach of Athiwaratkun et al. (2022). Speci\ufb01cally, we use working solutions produced by code-davinci-002 at temperature 0.8. Note that this approach does not produce solutions to every problem, since not all problems are solvable. Moreover, for uniformity, we use this approach for Python as well, even though hand- written Python solutions exist for our benchmarks. We only report \ufb01ll-in-the-middle evaluations for the data \ufb01ltering ablations. 6 RESULTS 6.1 ABLATIONS For the architecture ablations, we report the results on text2code benchmarks in Table 5. For the data \ufb01ltering ablations, we"}, {"question": " What is the purpose of using the tokenizer in the experiment mentioned?,        answer: To remove low-quality files from the dataset    ", "ref_chunk": "Match (alphanumeric token sequence) Exact Match (sha-256) Exact Match (non-whitespace text) Exact Match (hash) Near-deduplication (MinHash) Exact Match (\u201dunique python \ufb01les\u201d) Table 4: Various deduplication methods adopted for different model training data. calculation leads to many false positives, around 20% at 0.85. Increasing the n-gram size reduces false positives, but also increases false negatives. This is an expected trade-off between precision and recall; 2) A lower threshold would cause more documents to be removed at the cost of processing time. In our experiments, we have observed good duplicates occur with a similarity as low as 0.65, even though the FP and FN rates don\u2019t change much. We \ufb01nd that combining 5-grams and a 0.7 threshold strikes a good balance between false positives and false negatives while removing an additional 16%\u201320% \ufb01les. In particular, the increased false negatives occur mostly among documents with lower real Jaccard similarity bounds, whereas doc- uments with higher similarities (> 0.85) even have a decreased false negative rate (from 35% to 24%). Due to time constraints, we apply such deduplication on the already deduplicated datasets using the Stack v1 hyperparameters. We will refer to the \ufb01nal results as more near-deduplication or near-deduplication alt. Unlike other data preprocessing or \ufb01ltering techniques that target one document at a time, near- deduplication requires a centralized index that can be prohibitive for large data processing. We have released the deduplication code used in this paper on GitHub10 and will release a distributed version soon. For reference, it takes about 10 hours to deduplicate 42 million Java documents using plain multiprocessing while it takes less than 40 minutes in a distributed (but comparable) environment. Tokenizer fertility Can we use the tokenizer to remove low-quality \ufb01les from the dataset? We experiment with \ufb01ltering \ufb01les with a low character-to-token ratio11. For each language, we \ufb01nd that \ufb01les with a ratio below the 5th percentile are usually of poor quality, but increasing the threshold may eliminate some good-quality \ufb01les. We therefore set the cutoff value for this ratio to the following values: 2.5 for Python, 2.9 for Java, and 2.6 for JavaScript. This \ufb01lters out roughly 4% to 5% of data. Note that these values depend highly on the tokenizer and the data. This \ufb01lter may also be biased against \ufb01les with non-English comments. 5.4 EVALUATION Text2code evaluation The text2code task involves generating the body of a function from a prompt that includes a function description, the function signature (its name and arguments), and optionally a handful of example inputs and outputs. Every problem is accompanied by a set of hidden test cases, which are used to determine if the generated function is correct. We use the MultiPL-E text2code benchmark Cassano et al. (2022), which is derived from HumanEval Chen et al. (2021) and MBPP Austin et al. (2021) (the \u201csanitized\u201d subset of MBPP.). Whereas the latter two benchmarks target Python, MultiPL-E has a suite of compilers that translate HumanEval and MBPP to 18 other programming languages. Since our models are only trained on Java, JavaScript, and Python, we only evaluate them on these three languages. We use the methodology of Chen et al. (2021) and we calculate pass@k rates for (k = 1, 10, 100) for every problem. Intuitively, pass@1 estimates the likelihood a model will generate a correct solution in a single attempt, whereas pass@10 and pass@100 estimate the likelihood that the model will generate a correct solution given 10 and 100 attempts respectively. Following the literature, 10https://github.com/bigcode-project/bigcode-dataset 11We slightly abuse the term tokenizer fertility in this work as it usually refers to the average number of subwords per token, where a token is determined by the true tokenizer of the programming language. See e.g. (Rust et al., 2021) 8 Preprint Language Attention FIM HumanEval MBPP Java Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.35 0.36 0.37 0.54 0.55 0.55 JavaScript Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.33 0.37 0.37 0.64 0.67 0.65 Python Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.36 0.38 0.39 0.67 0.70 0.68 Table 5: Pass@100 results for the architecture ablations on HumanEval and MBPP. Model Java JavaScript Python Baseline GitHub stars Comments-to-code More near deduplication Tokenizer fertility 0.64 0.54 0.62 0.66 0.67 0.61 0.57 0.59 0.57 0.65 0.42 0.37 0.44 0.45 0.45 Final 0.62 0.60 0.44 Table 6: Fill-in-the-middle results for the data \ufb01ltering ablations on MultiPL-HumanEval. Each number reports the fraction of lines where the model exactly reproduces a single line of code that is held out from the body of a function in a held out problem. we sample 200 completions at temperatures 0.2 and 0.8 and use 0.2 to estimate pass@1 and 0.8 for pass@10 and pass@100. Fill-in-the-middle evaluation To evaluate \ufb01ll-in-the-middle, we use the single-line exact match metric, which was introduced by Fried et al. (2022) and also employed by Bavarian et al. (2022). For every benchmark problem, we mask out a single line of text from the function body (i.e., not from the function description or signature), and prompt the model to \ufb01ll in that line of code. We exclude blank lines and comments, and count the number of times the model produces exactly the masked out line. This benchmark requires working solutions for problems, which MultiPL-E does not have. (A text2code benchmark like MultiPL-E only needs hidden tests.) Instead, of writing solutions by hand, we use solutions generated by a code generation model, which is the approach of Athiwaratkun et al. (2022). Speci\ufb01cally, we use working solutions produced by code-davinci-002 at temperature 0.8. Note that this approach does not produce solutions to every problem, since not all problems are solvable. Moreover, for uniformity, we use this approach for Python as well, even though hand- written Python solutions exist for our benchmarks. We only report \ufb01ll-in-the-middle evaluations for the data \ufb01ltering ablations. 6 RESULTS 6.1 ABLATIONS For the architecture ablations, we report the results on text2code benchmarks in Table 5. For the data \ufb01ltering ablations, we"}, {"question": " What character-to-token ratio cutoff values were set for Python, Java, and JavaScript files in the experiment?,        answer: 2.5 for Python, 2.9 for Java, and 2.6 for JavaScript    ", "ref_chunk": "Match (alphanumeric token sequence) Exact Match (sha-256) Exact Match (non-whitespace text) Exact Match (hash) Near-deduplication (MinHash) Exact Match (\u201dunique python \ufb01les\u201d) Table 4: Various deduplication methods adopted for different model training data. calculation leads to many false positives, around 20% at 0.85. Increasing the n-gram size reduces false positives, but also increases false negatives. This is an expected trade-off between precision and recall; 2) A lower threshold would cause more documents to be removed at the cost of processing time. In our experiments, we have observed good duplicates occur with a similarity as low as 0.65, even though the FP and FN rates don\u2019t change much. We \ufb01nd that combining 5-grams and a 0.7 threshold strikes a good balance between false positives and false negatives while removing an additional 16%\u201320% \ufb01les. In particular, the increased false negatives occur mostly among documents with lower real Jaccard similarity bounds, whereas doc- uments with higher similarities (> 0.85) even have a decreased false negative rate (from 35% to 24%). Due to time constraints, we apply such deduplication on the already deduplicated datasets using the Stack v1 hyperparameters. We will refer to the \ufb01nal results as more near-deduplication or near-deduplication alt. Unlike other data preprocessing or \ufb01ltering techniques that target one document at a time, near- deduplication requires a centralized index that can be prohibitive for large data processing. We have released the deduplication code used in this paper on GitHub10 and will release a distributed version soon. For reference, it takes about 10 hours to deduplicate 42 million Java documents using plain multiprocessing while it takes less than 40 minutes in a distributed (but comparable) environment. Tokenizer fertility Can we use the tokenizer to remove low-quality \ufb01les from the dataset? We experiment with \ufb01ltering \ufb01les with a low character-to-token ratio11. For each language, we \ufb01nd that \ufb01les with a ratio below the 5th percentile are usually of poor quality, but increasing the threshold may eliminate some good-quality \ufb01les. We therefore set the cutoff value for this ratio to the following values: 2.5 for Python, 2.9 for Java, and 2.6 for JavaScript. This \ufb01lters out roughly 4% to 5% of data. Note that these values depend highly on the tokenizer and the data. This \ufb01lter may also be biased against \ufb01les with non-English comments. 5.4 EVALUATION Text2code evaluation The text2code task involves generating the body of a function from a prompt that includes a function description, the function signature (its name and arguments), and optionally a handful of example inputs and outputs. Every problem is accompanied by a set of hidden test cases, which are used to determine if the generated function is correct. We use the MultiPL-E text2code benchmark Cassano et al. (2022), which is derived from HumanEval Chen et al. (2021) and MBPP Austin et al. (2021) (the \u201csanitized\u201d subset of MBPP.). Whereas the latter two benchmarks target Python, MultiPL-E has a suite of compilers that translate HumanEval and MBPP to 18 other programming languages. Since our models are only trained on Java, JavaScript, and Python, we only evaluate them on these three languages. We use the methodology of Chen et al. (2021) and we calculate pass@k rates for (k = 1, 10, 100) for every problem. Intuitively, pass@1 estimates the likelihood a model will generate a correct solution in a single attempt, whereas pass@10 and pass@100 estimate the likelihood that the model will generate a correct solution given 10 and 100 attempts respectively. Following the literature, 10https://github.com/bigcode-project/bigcode-dataset 11We slightly abuse the term tokenizer fertility in this work as it usually refers to the average number of subwords per token, where a token is determined by the true tokenizer of the programming language. See e.g. (Rust et al., 2021) 8 Preprint Language Attention FIM HumanEval MBPP Java Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.35 0.36 0.37 0.54 0.55 0.55 JavaScript Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.33 0.37 0.37 0.64 0.67 0.65 Python Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.36 0.38 0.39 0.67 0.70 0.68 Table 5: Pass@100 results for the architecture ablations on HumanEval and MBPP. Model Java JavaScript Python Baseline GitHub stars Comments-to-code More near deduplication Tokenizer fertility 0.64 0.54 0.62 0.66 0.67 0.61 0.57 0.59 0.57 0.65 0.42 0.37 0.44 0.45 0.45 Final 0.62 0.60 0.44 Table 6: Fill-in-the-middle results for the data \ufb01ltering ablations on MultiPL-HumanEval. Each number reports the fraction of lines where the model exactly reproduces a single line of code that is held out from the body of a function in a held out problem. we sample 200 completions at temperatures 0.2 and 0.8 and use 0.2 to estimate pass@1 and 0.8 for pass@10 and pass@100. Fill-in-the-middle evaluation To evaluate \ufb01ll-in-the-middle, we use the single-line exact match metric, which was introduced by Fried et al. (2022) and also employed by Bavarian et al. (2022). For every benchmark problem, we mask out a single line of text from the function body (i.e., not from the function description or signature), and prompt the model to \ufb01ll in that line of code. We exclude blank lines and comments, and count the number of times the model produces exactly the masked out line. This benchmark requires working solutions for problems, which MultiPL-E does not have. (A text2code benchmark like MultiPL-E only needs hidden tests.) Instead, of writing solutions by hand, we use solutions generated by a code generation model, which is the approach of Athiwaratkun et al. (2022). Speci\ufb01cally, we use working solutions produced by code-davinci-002 at temperature 0.8. Note that this approach does not produce solutions to every problem, since not all problems are solvable. Moreover, for uniformity, we use this approach for Python as well, even though hand- written Python solutions exist for our benchmarks. We only report \ufb01ll-in-the-middle evaluations for the data \ufb01ltering ablations. 6 RESULTS 6.1 ABLATIONS For the architecture ablations, we report the results on text2code benchmarks in Table 5. For the data \ufb01ltering ablations, we"}, {"question": " Approximately what percentage of data is filtered out by the character-to-token ratio filter?,        answer: Roughly 4% to 5%    ", "ref_chunk": "Match (alphanumeric token sequence) Exact Match (sha-256) Exact Match (non-whitespace text) Exact Match (hash) Near-deduplication (MinHash) Exact Match (\u201dunique python \ufb01les\u201d) Table 4: Various deduplication methods adopted for different model training data. calculation leads to many false positives, around 20% at 0.85. Increasing the n-gram size reduces false positives, but also increases false negatives. This is an expected trade-off between precision and recall; 2) A lower threshold would cause more documents to be removed at the cost of processing time. In our experiments, we have observed good duplicates occur with a similarity as low as 0.65, even though the FP and FN rates don\u2019t change much. We \ufb01nd that combining 5-grams and a 0.7 threshold strikes a good balance between false positives and false negatives while removing an additional 16%\u201320% \ufb01les. In particular, the increased false negatives occur mostly among documents with lower real Jaccard similarity bounds, whereas doc- uments with higher similarities (> 0.85) even have a decreased false negative rate (from 35% to 24%). Due to time constraints, we apply such deduplication on the already deduplicated datasets using the Stack v1 hyperparameters. We will refer to the \ufb01nal results as more near-deduplication or near-deduplication alt. Unlike other data preprocessing or \ufb01ltering techniques that target one document at a time, near- deduplication requires a centralized index that can be prohibitive for large data processing. We have released the deduplication code used in this paper on GitHub10 and will release a distributed version soon. For reference, it takes about 10 hours to deduplicate 42 million Java documents using plain multiprocessing while it takes less than 40 minutes in a distributed (but comparable) environment. Tokenizer fertility Can we use the tokenizer to remove low-quality \ufb01les from the dataset? We experiment with \ufb01ltering \ufb01les with a low character-to-token ratio11. For each language, we \ufb01nd that \ufb01les with a ratio below the 5th percentile are usually of poor quality, but increasing the threshold may eliminate some good-quality \ufb01les. We therefore set the cutoff value for this ratio to the following values: 2.5 for Python, 2.9 for Java, and 2.6 for JavaScript. This \ufb01lters out roughly 4% to 5% of data. Note that these values depend highly on the tokenizer and the data. This \ufb01lter may also be biased against \ufb01les with non-English comments. 5.4 EVALUATION Text2code evaluation The text2code task involves generating the body of a function from a prompt that includes a function description, the function signature (its name and arguments), and optionally a handful of example inputs and outputs. Every problem is accompanied by a set of hidden test cases, which are used to determine if the generated function is correct. We use the MultiPL-E text2code benchmark Cassano et al. (2022), which is derived from HumanEval Chen et al. (2021) and MBPP Austin et al. (2021) (the \u201csanitized\u201d subset of MBPP.). Whereas the latter two benchmarks target Python, MultiPL-E has a suite of compilers that translate HumanEval and MBPP to 18 other programming languages. Since our models are only trained on Java, JavaScript, and Python, we only evaluate them on these three languages. We use the methodology of Chen et al. (2021) and we calculate pass@k rates for (k = 1, 10, 100) for every problem. Intuitively, pass@1 estimates the likelihood a model will generate a correct solution in a single attempt, whereas pass@10 and pass@100 estimate the likelihood that the model will generate a correct solution given 10 and 100 attempts respectively. Following the literature, 10https://github.com/bigcode-project/bigcode-dataset 11We slightly abuse the term tokenizer fertility in this work as it usually refers to the average number of subwords per token, where a token is determined by the true tokenizer of the programming language. See e.g. (Rust et al., 2021) 8 Preprint Language Attention FIM HumanEval MBPP Java Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.35 0.36 0.37 0.54 0.55 0.55 JavaScript Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.33 0.37 0.37 0.64 0.67 0.65 Python Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.36 0.38 0.39 0.67 0.70 0.68 Table 5: Pass@100 results for the architecture ablations on HumanEval and MBPP. Model Java JavaScript Python Baseline GitHub stars Comments-to-code More near deduplication Tokenizer fertility 0.64 0.54 0.62 0.66 0.67 0.61 0.57 0.59 0.57 0.65 0.42 0.37 0.44 0.45 0.45 Final 0.62 0.60 0.44 Table 6: Fill-in-the-middle results for the data \ufb01ltering ablations on MultiPL-HumanEval. Each number reports the fraction of lines where the model exactly reproduces a single line of code that is held out from the body of a function in a held out problem. we sample 200 completions at temperatures 0.2 and 0.8 and use 0.2 to estimate pass@1 and 0.8 for pass@10 and pass@100. Fill-in-the-middle evaluation To evaluate \ufb01ll-in-the-middle, we use the single-line exact match metric, which was introduced by Fried et al. (2022) and also employed by Bavarian et al. (2022). For every benchmark problem, we mask out a single line of text from the function body (i.e., not from the function description or signature), and prompt the model to \ufb01ll in that line of code. We exclude blank lines and comments, and count the number of times the model produces exactly the masked out line. This benchmark requires working solutions for problems, which MultiPL-E does not have. (A text2code benchmark like MultiPL-E only needs hidden tests.) Instead, of writing solutions by hand, we use solutions generated by a code generation model, which is the approach of Athiwaratkun et al. (2022). Speci\ufb01cally, we use working solutions produced by code-davinci-002 at temperature 0.8. Note that this approach does not produce solutions to every problem, since not all problems are solvable. Moreover, for uniformity, we use this approach for Python as well, even though hand- written Python solutions exist for our benchmarks. We only report \ufb01ll-in-the-middle evaluations for the data \ufb01ltering ablations. 6 RESULTS 6.1 ABLATIONS For the architecture ablations, we report the results on text2code benchmarks in Table 5. For the data \ufb01ltering ablations, we"}, {"question": " What languages are the models trained on and evaluated for in the text2code task?,        answer: Java, JavaScript, and Python    ", "ref_chunk": "Match (alphanumeric token sequence) Exact Match (sha-256) Exact Match (non-whitespace text) Exact Match (hash) Near-deduplication (MinHash) Exact Match (\u201dunique python \ufb01les\u201d) Table 4: Various deduplication methods adopted for different model training data. calculation leads to many false positives, around 20% at 0.85. Increasing the n-gram size reduces false positives, but also increases false negatives. This is an expected trade-off between precision and recall; 2) A lower threshold would cause more documents to be removed at the cost of processing time. In our experiments, we have observed good duplicates occur with a similarity as low as 0.65, even though the FP and FN rates don\u2019t change much. We \ufb01nd that combining 5-grams and a 0.7 threshold strikes a good balance between false positives and false negatives while removing an additional 16%\u201320% \ufb01les. In particular, the increased false negatives occur mostly among documents with lower real Jaccard similarity bounds, whereas doc- uments with higher similarities (> 0.85) even have a decreased false negative rate (from 35% to 24%). Due to time constraints, we apply such deduplication on the already deduplicated datasets using the Stack v1 hyperparameters. We will refer to the \ufb01nal results as more near-deduplication or near-deduplication alt. Unlike other data preprocessing or \ufb01ltering techniques that target one document at a time, near- deduplication requires a centralized index that can be prohibitive for large data processing. We have released the deduplication code used in this paper on GitHub10 and will release a distributed version soon. For reference, it takes about 10 hours to deduplicate 42 million Java documents using plain multiprocessing while it takes less than 40 minutes in a distributed (but comparable) environment. Tokenizer fertility Can we use the tokenizer to remove low-quality \ufb01les from the dataset? We experiment with \ufb01ltering \ufb01les with a low character-to-token ratio11. For each language, we \ufb01nd that \ufb01les with a ratio below the 5th percentile are usually of poor quality, but increasing the threshold may eliminate some good-quality \ufb01les. We therefore set the cutoff value for this ratio to the following values: 2.5 for Python, 2.9 for Java, and 2.6 for JavaScript. This \ufb01lters out roughly 4% to 5% of data. Note that these values depend highly on the tokenizer and the data. This \ufb01lter may also be biased against \ufb01les with non-English comments. 5.4 EVALUATION Text2code evaluation The text2code task involves generating the body of a function from a prompt that includes a function description, the function signature (its name and arguments), and optionally a handful of example inputs and outputs. Every problem is accompanied by a set of hidden test cases, which are used to determine if the generated function is correct. We use the MultiPL-E text2code benchmark Cassano et al. (2022), which is derived from HumanEval Chen et al. (2021) and MBPP Austin et al. (2021) (the \u201csanitized\u201d subset of MBPP.). Whereas the latter two benchmarks target Python, MultiPL-E has a suite of compilers that translate HumanEval and MBPP to 18 other programming languages. Since our models are only trained on Java, JavaScript, and Python, we only evaluate them on these three languages. We use the methodology of Chen et al. (2021) and we calculate pass@k rates for (k = 1, 10, 100) for every problem. Intuitively, pass@1 estimates the likelihood a model will generate a correct solution in a single attempt, whereas pass@10 and pass@100 estimate the likelihood that the model will generate a correct solution given 10 and 100 attempts respectively. Following the literature, 10https://github.com/bigcode-project/bigcode-dataset 11We slightly abuse the term tokenizer fertility in this work as it usually refers to the average number of subwords per token, where a token is determined by the true tokenizer of the programming language. See e.g. (Rust et al., 2021) 8 Preprint Language Attention FIM HumanEval MBPP Java Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.35 0.36 0.37 0.54 0.55 0.55 JavaScript Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.33 0.37 0.37 0.64 0.67 0.65 Python Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.36 0.38 0.39 0.67 0.70 0.68 Table 5: Pass@100 results for the architecture ablations on HumanEval and MBPP. Model Java JavaScript Python Baseline GitHub stars Comments-to-code More near deduplication Tokenizer fertility 0.64 0.54 0.62 0.66 0.67 0.61 0.57 0.59 0.57 0.65 0.42 0.37 0.44 0.45 0.45 Final 0.62 0.60 0.44 Table 6: Fill-in-the-middle results for the data \ufb01ltering ablations on MultiPL-HumanEval. Each number reports the fraction of lines where the model exactly reproduces a single line of code that is held out from the body of a function in a held out problem. we sample 200 completions at temperatures 0.2 and 0.8 and use 0.2 to estimate pass@1 and 0.8 for pass@10 and pass@100. Fill-in-the-middle evaluation To evaluate \ufb01ll-in-the-middle, we use the single-line exact match metric, which was introduced by Fried et al. (2022) and also employed by Bavarian et al. (2022). For every benchmark problem, we mask out a single line of text from the function body (i.e., not from the function description or signature), and prompt the model to \ufb01ll in that line of code. We exclude blank lines and comments, and count the number of times the model produces exactly the masked out line. This benchmark requires working solutions for problems, which MultiPL-E does not have. (A text2code benchmark like MultiPL-E only needs hidden tests.) Instead, of writing solutions by hand, we use solutions generated by a code generation model, which is the approach of Athiwaratkun et al. (2022). Speci\ufb01cally, we use working solutions produced by code-davinci-002 at temperature 0.8. Note that this approach does not produce solutions to every problem, since not all problems are solvable. Moreover, for uniformity, we use this approach for Python as well, even though hand- written Python solutions exist for our benchmarks. We only report \ufb01ll-in-the-middle evaluations for the data \ufb01ltering ablations. 6 RESULTS 6.1 ABLATIONS For the architecture ablations, we report the results on text2code benchmarks in Table 5. For the data \ufb01ltering ablations, we"}, {"question": " What does the fill-in-the-middle evaluation metric measure in the text2code task?,        answer: The number of times the model produces exactly the masked out line of code    ", "ref_chunk": "Match (alphanumeric token sequence) Exact Match (sha-256) Exact Match (non-whitespace text) Exact Match (hash) Near-deduplication (MinHash) Exact Match (\u201dunique python \ufb01les\u201d) Table 4: Various deduplication methods adopted for different model training data. calculation leads to many false positives, around 20% at 0.85. Increasing the n-gram size reduces false positives, but also increases false negatives. This is an expected trade-off between precision and recall; 2) A lower threshold would cause more documents to be removed at the cost of processing time. In our experiments, we have observed good duplicates occur with a similarity as low as 0.65, even though the FP and FN rates don\u2019t change much. We \ufb01nd that combining 5-grams and a 0.7 threshold strikes a good balance between false positives and false negatives while removing an additional 16%\u201320% \ufb01les. In particular, the increased false negatives occur mostly among documents with lower real Jaccard similarity bounds, whereas doc- uments with higher similarities (> 0.85) even have a decreased false negative rate (from 35% to 24%). Due to time constraints, we apply such deduplication on the already deduplicated datasets using the Stack v1 hyperparameters. We will refer to the \ufb01nal results as more near-deduplication or near-deduplication alt. Unlike other data preprocessing or \ufb01ltering techniques that target one document at a time, near- deduplication requires a centralized index that can be prohibitive for large data processing. We have released the deduplication code used in this paper on GitHub10 and will release a distributed version soon. For reference, it takes about 10 hours to deduplicate 42 million Java documents using plain multiprocessing while it takes less than 40 minutes in a distributed (but comparable) environment. Tokenizer fertility Can we use the tokenizer to remove low-quality \ufb01les from the dataset? We experiment with \ufb01ltering \ufb01les with a low character-to-token ratio11. For each language, we \ufb01nd that \ufb01les with a ratio below the 5th percentile are usually of poor quality, but increasing the threshold may eliminate some good-quality \ufb01les. We therefore set the cutoff value for this ratio to the following values: 2.5 for Python, 2.9 for Java, and 2.6 for JavaScript. This \ufb01lters out roughly 4% to 5% of data. Note that these values depend highly on the tokenizer and the data. This \ufb01lter may also be biased against \ufb01les with non-English comments. 5.4 EVALUATION Text2code evaluation The text2code task involves generating the body of a function from a prompt that includes a function description, the function signature (its name and arguments), and optionally a handful of example inputs and outputs. Every problem is accompanied by a set of hidden test cases, which are used to determine if the generated function is correct. We use the MultiPL-E text2code benchmark Cassano et al. (2022), which is derived from HumanEval Chen et al. (2021) and MBPP Austin et al. (2021) (the \u201csanitized\u201d subset of MBPP.). Whereas the latter two benchmarks target Python, MultiPL-E has a suite of compilers that translate HumanEval and MBPP to 18 other programming languages. Since our models are only trained on Java, JavaScript, and Python, we only evaluate them on these three languages. We use the methodology of Chen et al. (2021) and we calculate pass@k rates for (k = 1, 10, 100) for every problem. Intuitively, pass@1 estimates the likelihood a model will generate a correct solution in a single attempt, whereas pass@10 and pass@100 estimate the likelihood that the model will generate a correct solution given 10 and 100 attempts respectively. Following the literature, 10https://github.com/bigcode-project/bigcode-dataset 11We slightly abuse the term tokenizer fertility in this work as it usually refers to the average number of subwords per token, where a token is determined by the true tokenizer of the programming language. See e.g. (Rust et al., 2021) 8 Preprint Language Attention FIM HumanEval MBPP Java Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.35 0.36 0.37 0.54 0.55 0.55 JavaScript Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.33 0.37 0.37 0.64 0.67 0.65 Python Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.36 0.38 0.39 0.67 0.70 0.68 Table 5: Pass@100 results for the architecture ablations on HumanEval and MBPP. Model Java JavaScript Python Baseline GitHub stars Comments-to-code More near deduplication Tokenizer fertility 0.64 0.54 0.62 0.66 0.67 0.61 0.57 0.59 0.57 0.65 0.42 0.37 0.44 0.45 0.45 Final 0.62 0.60 0.44 Table 6: Fill-in-the-middle results for the data \ufb01ltering ablations on MultiPL-HumanEval. Each number reports the fraction of lines where the model exactly reproduces a single line of code that is held out from the body of a function in a held out problem. we sample 200 completions at temperatures 0.2 and 0.8 and use 0.2 to estimate pass@1 and 0.8 for pass@10 and pass@100. Fill-in-the-middle evaluation To evaluate \ufb01ll-in-the-middle, we use the single-line exact match metric, which was introduced by Fried et al. (2022) and also employed by Bavarian et al. (2022). For every benchmark problem, we mask out a single line of text from the function body (i.e., not from the function description or signature), and prompt the model to \ufb01ll in that line of code. We exclude blank lines and comments, and count the number of times the model produces exactly the masked out line. This benchmark requires working solutions for problems, which MultiPL-E does not have. (A text2code benchmark like MultiPL-E only needs hidden tests.) Instead, of writing solutions by hand, we use solutions generated by a code generation model, which is the approach of Athiwaratkun et al. (2022). Speci\ufb01cally, we use working solutions produced by code-davinci-002 at temperature 0.8. Note that this approach does not produce solutions to every problem, since not all problems are solvable. Moreover, for uniformity, we use this approach for Python as well, even though hand- written Python solutions exist for our benchmarks. We only report \ufb01ll-in-the-middle evaluations for the data \ufb01ltering ablations. 6 RESULTS 6.1 ABLATIONS For the architecture ablations, we report the results on text2code benchmarks in Table 5. For the data \ufb01ltering ablations, we"}], "doc_text": "Match (alphanumeric token sequence) Exact Match (sha-256) Exact Match (non-whitespace text) Exact Match (hash) Near-deduplication (MinHash) Exact Match (\u201dunique python \ufb01les\u201d) Table 4: Various deduplication methods adopted for different model training data. calculation leads to many false positives, around 20% at 0.85. Increasing the n-gram size reduces false positives, but also increases false negatives. This is an expected trade-off between precision and recall; 2) A lower threshold would cause more documents to be removed at the cost of processing time. In our experiments, we have observed good duplicates occur with a similarity as low as 0.65, even though the FP and FN rates don\u2019t change much. We \ufb01nd that combining 5-grams and a 0.7 threshold strikes a good balance between false positives and false negatives while removing an additional 16%\u201320% \ufb01les. In particular, the increased false negatives occur mostly among documents with lower real Jaccard similarity bounds, whereas doc- uments with higher similarities (> 0.85) even have a decreased false negative rate (from 35% to 24%). Due to time constraints, we apply such deduplication on the already deduplicated datasets using the Stack v1 hyperparameters. We will refer to the \ufb01nal results as more near-deduplication or near-deduplication alt. Unlike other data preprocessing or \ufb01ltering techniques that target one document at a time, near- deduplication requires a centralized index that can be prohibitive for large data processing. We have released the deduplication code used in this paper on GitHub10 and will release a distributed version soon. For reference, it takes about 10 hours to deduplicate 42 million Java documents using plain multiprocessing while it takes less than 40 minutes in a distributed (but comparable) environment. Tokenizer fertility Can we use the tokenizer to remove low-quality \ufb01les from the dataset? We experiment with \ufb01ltering \ufb01les with a low character-to-token ratio11. For each language, we \ufb01nd that \ufb01les with a ratio below the 5th percentile are usually of poor quality, but increasing the threshold may eliminate some good-quality \ufb01les. We therefore set the cutoff value for this ratio to the following values: 2.5 for Python, 2.9 for Java, and 2.6 for JavaScript. This \ufb01lters out roughly 4% to 5% of data. Note that these values depend highly on the tokenizer and the data. This \ufb01lter may also be biased against \ufb01les with non-English comments. 5.4 EVALUATION Text2code evaluation The text2code task involves generating the body of a function from a prompt that includes a function description, the function signature (its name and arguments), and optionally a handful of example inputs and outputs. Every problem is accompanied by a set of hidden test cases, which are used to determine if the generated function is correct. We use the MultiPL-E text2code benchmark Cassano et al. (2022), which is derived from HumanEval Chen et al. (2021) and MBPP Austin et al. (2021) (the \u201csanitized\u201d subset of MBPP.). Whereas the latter two benchmarks target Python, MultiPL-E has a suite of compilers that translate HumanEval and MBPP to 18 other programming languages. Since our models are only trained on Java, JavaScript, and Python, we only evaluate them on these three languages. We use the methodology of Chen et al. (2021) and we calculate pass@k rates for (k = 1, 10, 100) for every problem. Intuitively, pass@1 estimates the likelihood a model will generate a correct solution in a single attempt, whereas pass@10 and pass@100 estimate the likelihood that the model will generate a correct solution given 10 and 100 attempts respectively. Following the literature, 10https://github.com/bigcode-project/bigcode-dataset 11We slightly abuse the term tokenizer fertility in this work as it usually refers to the average number of subwords per token, where a token is determined by the true tokenizer of the programming language. See e.g. (Rust et al., 2021) 8 Preprint Language Attention FIM HumanEval MBPP Java Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.35 0.36 0.37 0.54 0.55 0.55 JavaScript Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.33 0.37 0.37 0.64 0.67 0.65 Python Multi Query Attention Multi Head Attention Multi Query Attention (cid:51) (cid:51) (cid:55) 0.36 0.38 0.39 0.67 0.70 0.68 Table 5: Pass@100 results for the architecture ablations on HumanEval and MBPP. Model Java JavaScript Python Baseline GitHub stars Comments-to-code More near deduplication Tokenizer fertility 0.64 0.54 0.62 0.66 0.67 0.61 0.57 0.59 0.57 0.65 0.42 0.37 0.44 0.45 0.45 Final 0.62 0.60 0.44 Table 6: Fill-in-the-middle results for the data \ufb01ltering ablations on MultiPL-HumanEval. Each number reports the fraction of lines where the model exactly reproduces a single line of code that is held out from the body of a function in a held out problem. we sample 200 completions at temperatures 0.2 and 0.8 and use 0.2 to estimate pass@1 and 0.8 for pass@10 and pass@100. Fill-in-the-middle evaluation To evaluate \ufb01ll-in-the-middle, we use the single-line exact match metric, which was introduced by Fried et al. (2022) and also employed by Bavarian et al. (2022). For every benchmark problem, we mask out a single line of text from the function body (i.e., not from the function description or signature), and prompt the model to \ufb01ll in that line of code. We exclude blank lines and comments, and count the number of times the model produces exactly the masked out line. This benchmark requires working solutions for problems, which MultiPL-E does not have. (A text2code benchmark like MultiPL-E only needs hidden tests.) Instead, of writing solutions by hand, we use solutions generated by a code generation model, which is the approach of Athiwaratkun et al. (2022). Speci\ufb01cally, we use working solutions produced by code-davinci-002 at temperature 0.8. Note that this approach does not produce solutions to every problem, since not all problems are solvable. Moreover, for uniformity, we use this approach for Python as well, even though hand- written Python solutions exist for our benchmarks. We only report \ufb01ll-in-the-middle evaluations for the data \ufb01ltering ablations. 6 RESULTS 6.1 ABLATIONS For the architecture ablations, we report the results on text2code benchmarks in Table 5. For the data \ufb01ltering ablations, we"}