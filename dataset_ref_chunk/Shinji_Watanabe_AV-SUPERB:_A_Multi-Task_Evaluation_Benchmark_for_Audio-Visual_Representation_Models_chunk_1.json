{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_AV-SUPERB:_A_Multi-Task_Evaluation_Benchmark_for_Audio-Visual_Representation_Models_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the AV-SUPERB benchmark and why was it proposed?", "answer": " The AV-SUPERB benchmark was proposed to enable general-purpose evaluation of unimodal audio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing.", "ref_chunk": "3 2 0 2 p e S 9 1 ] S A . s s e e [ 1 v 7 8 7 0 1 . 9 0 3 2 : v i X r a AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS Yuan Tseng1, Layne Berry2\u2217, Yi-Ting Chen3\u2217, I-Hsiang Chiu1\u2217, Hsuan-Hao Lin1\u2217, Max Liu1\u2217, Puyuan Peng2\u2217, Yi-Jen Shih1\u2217, Hung-Yu Wang1\u2217, Haibin Wu1\u2217, Po-Yao Huang4, Chun-Mao Lai1, Shang-Wen Li4, David Harwath2, Yu Tsao3, Shinji Watanabe5, Abdelrahman Mohamed6, Chi-Luen Feng1, Hung-yi Lee1 1 National Taiwan University, Taiwan 2 University of Texas at Austin, USA 3 Academia Sinica, Taiwan 4 Meta AI 5 Carnegie Mellon University, USA 6 Rembrand r11942082@ntu.edu.tw ABSTRACT Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned repre- sentations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal au- dio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong in- termediate task. We release our benchmark with evaluation code1 and a model submission platform2 to encourage further research in audio-visual learning. human perception, which synergistically integrates auditory and vi- sual cues [12, 13]. While audio-visual representation learning has made significant progress [14, 15, 16, 17, 18], the assessment of these models tends to be task-specific, leaving the broader gener- alization capabilities across various audio-visual challenges less un- derstood. This complicates comparitive analysis of different models and training strategies, impeding the development of more robust and versatile audio-visual representation learning approaches. To address this issue, we propose AV-SUPERB, a standardized benchmark for comprehensively evaluating representations across seven distinct datasets involving five speech and audio processing tasks. AV-SUPERB comprises of three tracks to assess audio, video, and audio-visual fusion representations. We envision that these dis- tinct tracks will allow researchers in speech, audio, and video repre- sentation learning alike to compare learning strategies across models and modalities, enabling broader analysis of their effectiveness. Index Terms\u2014 Audio-Visual Learning, Representation Learn- ing, Evaluation, Self-Supervised Learning 1. INTRODUCTION Emulating the seamless integration of multiple tasks in human cognition, such as spoken language comprehension, sound event detection, and visual object recognition has been a long-standing goal of computational research. Prior research demonstrates that the pretrain-then-finetune paradigm is an effective and scalable method of building multitasking algorithmic systems for speech [1, 2], au- dio [3, 4], and vision [5, 6]. In the pretraining stage, models can often learn meaningful representations from unlabelled data alone through optimization of contrastive, masked prediction, or other self-supervised loss functions. These pretrained representations can then be applied to diverse tasks just by fine-tuning minimal additional parameters. In order to better measure progress in representation learning, previous works have established multitask benchmarks in speech [7, 8], audio [9], and vision [10, 11]. However, these benchmark pre- dominantly evaluate performance in isolation within single modal- ities. This approach overlooks the inherent multimodal nature of \u2217 Equal contribution; sorted alphabetically 1https://github.com/roger-tseng/av-superb 2https://av.superbbenchmark.org Our contributions are four-fold: (1) Diverse-domain evalua- tion: We propose the first audio-visual learning benchmark that en- compasses multiple datasets and tasks, covering both speech and au- dio domains. (2) Easy and reproducible benchmarking: We re- lease evaluation code and a dedicated model submission platform that ensures reproducible evaluation on dynamic Youtube datasets (3) Intermediate-task and reduces computational entry barriers. fine-tuning: Our work emphasizes the potential benefits of full fine- tuning on intermediate tasks for improving performance on out-of- domain downstream tasks. (4) Layer-wise analysis: We show that different layers contribute variably to task performance, suggesting that simply using representations of the final layer is suboptimal, motivating the weighted-sum evaluation approach. 2. RELATED WORK Recognizing how the close relation between audition and vision fa- cilitates multimodal human perception, many audio-visual datasets have been gathered for action recognition [19, 15, 20, 21], speech recognition [22, 23, 24], speaker recognition [25, 26], and a vari- ety of other tasks to study audio-visual learning. However, most models are trained and evaluated on different datasets with different experiment settings, which increases comparison difficulty and ob- fuscates the broad applicability of proposed methods. Hence in the AV-SUPERB benchmark, we select a diverse set of datasets from multiple tasks to comprehensively compare works in audio-visual representation learning. Previous multitask benchmarks in speech [7, 8], audio [9], and Fig. 1. We consider three evaluation scenarios: extracting fea- tures using inputs from one or both modalities. Following [7], the weighted-sum of features from Transformer layers (if applicable) are used as input for fine-tuning a small downstream model for each in- dividual task. Details of selected tasks are given in Section 3.1. video representation learning [27, 10, 11] allow for fairer com- parison of different models and promote research towards general approaches that are applicable to a variety of real-world tasks. SUPERB [7] and SUPERB-SG [8] evaluate speech representation models on a wide range of downstream tasks covering content, speaker, and other different aspects of speech. Additionally, the HEAR benchmark [9] evaluates audio representations on diverse domains beyond speech, such as music and environmental sounds. For video representations, the SEVERE-benchmark [10] evaluates video self-supervised learning models on a diverse set of datasets to measure model sensitivity to different properties of downstream tasks. Feichtenhofer et al. [27] extend 4 image self-supervised learning methods to video representations and compare their effi- cacy on several downstream datasets, while Kumar et al. [11] focus on the effects of different factors in self-supervised video pretrain- ing. However, these works focus on individual domains, and cannot make use of the relationship between paired audio/visual inputs. Previous multi-task multimodal benchmarks either focus on ego- centric videos [28],"}, {"question": " How many tracks does the AV-SUPERB benchmark comprise of and what are they designed for?", "answer": " The AV-SUPERB benchmark comprises of three tracks to assess audio, video, and audio-visual fusion representations, designed to allow researchers to compare learning strategies across models and modalities.", "ref_chunk": "3 2 0 2 p e S 9 1 ] S A . s s e e [ 1 v 7 8 7 0 1 . 9 0 3 2 : v i X r a AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS Yuan Tseng1, Layne Berry2\u2217, Yi-Ting Chen3\u2217, I-Hsiang Chiu1\u2217, Hsuan-Hao Lin1\u2217, Max Liu1\u2217, Puyuan Peng2\u2217, Yi-Jen Shih1\u2217, Hung-Yu Wang1\u2217, Haibin Wu1\u2217, Po-Yao Huang4, Chun-Mao Lai1, Shang-Wen Li4, David Harwath2, Yu Tsao3, Shinji Watanabe5, Abdelrahman Mohamed6, Chi-Luen Feng1, Hung-yi Lee1 1 National Taiwan University, Taiwan 2 University of Texas at Austin, USA 3 Academia Sinica, Taiwan 4 Meta AI 5 Carnegie Mellon University, USA 6 Rembrand r11942082@ntu.edu.tw ABSTRACT Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned repre- sentations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal au- dio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong in- termediate task. We release our benchmark with evaluation code1 and a model submission platform2 to encourage further research in audio-visual learning. human perception, which synergistically integrates auditory and vi- sual cues [12, 13]. While audio-visual representation learning has made significant progress [14, 15, 16, 17, 18], the assessment of these models tends to be task-specific, leaving the broader gener- alization capabilities across various audio-visual challenges less un- derstood. This complicates comparitive analysis of different models and training strategies, impeding the development of more robust and versatile audio-visual representation learning approaches. To address this issue, we propose AV-SUPERB, a standardized benchmark for comprehensively evaluating representations across seven distinct datasets involving five speech and audio processing tasks. AV-SUPERB comprises of three tracks to assess audio, video, and audio-visual fusion representations. We envision that these dis- tinct tracks will allow researchers in speech, audio, and video repre- sentation learning alike to compare learning strategies across models and modalities, enabling broader analysis of their effectiveness. Index Terms\u2014 Audio-Visual Learning, Representation Learn- ing, Evaluation, Self-Supervised Learning 1. INTRODUCTION Emulating the seamless integration of multiple tasks in human cognition, such as spoken language comprehension, sound event detection, and visual object recognition has been a long-standing goal of computational research. Prior research demonstrates that the pretrain-then-finetune paradigm is an effective and scalable method of building multitasking algorithmic systems for speech [1, 2], au- dio [3, 4], and vision [5, 6]. In the pretraining stage, models can often learn meaningful representations from unlabelled data alone through optimization of contrastive, masked prediction, or other self-supervised loss functions. These pretrained representations can then be applied to diverse tasks just by fine-tuning minimal additional parameters. In order to better measure progress in representation learning, previous works have established multitask benchmarks in speech [7, 8], audio [9], and vision [10, 11]. However, these benchmark pre- dominantly evaluate performance in isolation within single modal- ities. This approach overlooks the inherent multimodal nature of \u2217 Equal contribution; sorted alphabetically 1https://github.com/roger-tseng/av-superb 2https://av.superbbenchmark.org Our contributions are four-fold: (1) Diverse-domain evalua- tion: We propose the first audio-visual learning benchmark that en- compasses multiple datasets and tasks, covering both speech and au- dio domains. (2) Easy and reproducible benchmarking: We re- lease evaluation code and a dedicated model submission platform that ensures reproducible evaluation on dynamic Youtube datasets (3) Intermediate-task and reduces computational entry barriers. fine-tuning: Our work emphasizes the potential benefits of full fine- tuning on intermediate tasks for improving performance on out-of- domain downstream tasks. (4) Layer-wise analysis: We show that different layers contribute variably to task performance, suggesting that simply using representations of the final layer is suboptimal, motivating the weighted-sum evaluation approach. 2. RELATED WORK Recognizing how the close relation between audition and vision fa- cilitates multimodal human perception, many audio-visual datasets have been gathered for action recognition [19, 15, 20, 21], speech recognition [22, 23, 24], speaker recognition [25, 26], and a vari- ety of other tasks to study audio-visual learning. However, most models are trained and evaluated on different datasets with different experiment settings, which increases comparison difficulty and ob- fuscates the broad applicability of proposed methods. Hence in the AV-SUPERB benchmark, we select a diverse set of datasets from multiple tasks to comprehensively compare works in audio-visual representation learning. Previous multitask benchmarks in speech [7, 8], audio [9], and Fig. 1. We consider three evaluation scenarios: extracting fea- tures using inputs from one or both modalities. Following [7], the weighted-sum of features from Transformer layers (if applicable) are used as input for fine-tuning a small downstream model for each in- dividual task. Details of selected tasks are given in Section 3.1. video representation learning [27, 10, 11] allow for fairer com- parison of different models and promote research towards general approaches that are applicable to a variety of real-world tasks. SUPERB [7] and SUPERB-SG [8] evaluate speech representation models on a wide range of downstream tasks covering content, speaker, and other different aspects of speech. Additionally, the HEAR benchmark [9] evaluates audio representations on diverse domains beyond speech, such as music and environmental sounds. For video representations, the SEVERE-benchmark [10] evaluates video self-supervised learning models on a diverse set of datasets to measure model sensitivity to different properties of downstream tasks. Feichtenhofer et al. [27] extend 4 image self-supervised learning methods to video representations and compare their effi- cacy on several downstream datasets, while Kumar et al. [11] focus on the effects of different factors in self-supervised video pretrain- ing. However, these works focus on individual domains, and cannot make use of the relationship between paired audio/visual inputs. Previous multi-task multimodal benchmarks either focus on ego- centric videos [28],"}, {"question": " What does the AV-SUPERB benchmark aim to evaluate?", "answer": " The AV-SUPERB benchmark aims to evaluate representations across seven distinct datasets involving five speech and audio processing tasks.", "ref_chunk": "3 2 0 2 p e S 9 1 ] S A . s s e e [ 1 v 7 8 7 0 1 . 9 0 3 2 : v i X r a AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS Yuan Tseng1, Layne Berry2\u2217, Yi-Ting Chen3\u2217, I-Hsiang Chiu1\u2217, Hsuan-Hao Lin1\u2217, Max Liu1\u2217, Puyuan Peng2\u2217, Yi-Jen Shih1\u2217, Hung-Yu Wang1\u2217, Haibin Wu1\u2217, Po-Yao Huang4, Chun-Mao Lai1, Shang-Wen Li4, David Harwath2, Yu Tsao3, Shinji Watanabe5, Abdelrahman Mohamed6, Chi-Luen Feng1, Hung-yi Lee1 1 National Taiwan University, Taiwan 2 University of Texas at Austin, USA 3 Academia Sinica, Taiwan 4 Meta AI 5 Carnegie Mellon University, USA 6 Rembrand r11942082@ntu.edu.tw ABSTRACT Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned repre- sentations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal au- dio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong in- termediate task. We release our benchmark with evaluation code1 and a model submission platform2 to encourage further research in audio-visual learning. human perception, which synergistically integrates auditory and vi- sual cues [12, 13]. While audio-visual representation learning has made significant progress [14, 15, 16, 17, 18], the assessment of these models tends to be task-specific, leaving the broader gener- alization capabilities across various audio-visual challenges less un- derstood. This complicates comparitive analysis of different models and training strategies, impeding the development of more robust and versatile audio-visual representation learning approaches. To address this issue, we propose AV-SUPERB, a standardized benchmark for comprehensively evaluating representations across seven distinct datasets involving five speech and audio processing tasks. AV-SUPERB comprises of three tracks to assess audio, video, and audio-visual fusion representations. We envision that these dis- tinct tracks will allow researchers in speech, audio, and video repre- sentation learning alike to compare learning strategies across models and modalities, enabling broader analysis of their effectiveness. Index Terms\u2014 Audio-Visual Learning, Representation Learn- ing, Evaluation, Self-Supervised Learning 1. INTRODUCTION Emulating the seamless integration of multiple tasks in human cognition, such as spoken language comprehension, sound event detection, and visual object recognition has been a long-standing goal of computational research. Prior research demonstrates that the pretrain-then-finetune paradigm is an effective and scalable method of building multitasking algorithmic systems for speech [1, 2], au- dio [3, 4], and vision [5, 6]. In the pretraining stage, models can often learn meaningful representations from unlabelled data alone through optimization of contrastive, masked prediction, or other self-supervised loss functions. These pretrained representations can then be applied to diverse tasks just by fine-tuning minimal additional parameters. In order to better measure progress in representation learning, previous works have established multitask benchmarks in speech [7, 8], audio [9], and vision [10, 11]. However, these benchmark pre- dominantly evaluate performance in isolation within single modal- ities. This approach overlooks the inherent multimodal nature of \u2217 Equal contribution; sorted alphabetically 1https://github.com/roger-tseng/av-superb 2https://av.superbbenchmark.org Our contributions are four-fold: (1) Diverse-domain evalua- tion: We propose the first audio-visual learning benchmark that en- compasses multiple datasets and tasks, covering both speech and au- dio domains. (2) Easy and reproducible benchmarking: We re- lease evaluation code and a dedicated model submission platform that ensures reproducible evaluation on dynamic Youtube datasets (3) Intermediate-task and reduces computational entry barriers. fine-tuning: Our work emphasizes the potential benefits of full fine- tuning on intermediate tasks for improving performance on out-of- domain downstream tasks. (4) Layer-wise analysis: We show that different layers contribute variably to task performance, suggesting that simply using representations of the final layer is suboptimal, motivating the weighted-sum evaluation approach. 2. RELATED WORK Recognizing how the close relation between audition and vision fa- cilitates multimodal human perception, many audio-visual datasets have been gathered for action recognition [19, 15, 20, 21], speech recognition [22, 23, 24], speaker recognition [25, 26], and a vari- ety of other tasks to study audio-visual learning. However, most models are trained and evaluated on different datasets with different experiment settings, which increases comparison difficulty and ob- fuscates the broad applicability of proposed methods. Hence in the AV-SUPERB benchmark, we select a diverse set of datasets from multiple tasks to comprehensively compare works in audio-visual representation learning. Previous multitask benchmarks in speech [7, 8], audio [9], and Fig. 1. We consider three evaluation scenarios: extracting fea- tures using inputs from one or both modalities. Following [7], the weighted-sum of features from Transformer layers (if applicable) are used as input for fine-tuning a small downstream model for each in- dividual task. Details of selected tasks are given in Section 3.1. video representation learning [27, 10, 11] allow for fairer com- parison of different models and promote research towards general approaches that are applicable to a variety of real-world tasks. SUPERB [7] and SUPERB-SG [8] evaluate speech representation models on a wide range of downstream tasks covering content, speaker, and other different aspects of speech. Additionally, the HEAR benchmark [9] evaluates audio representations on diverse domains beyond speech, such as music and environmental sounds. For video representations, the SEVERE-benchmark [10] evaluates video self-supervised learning models on a diverse set of datasets to measure model sensitivity to different properties of downstream tasks. Feichtenhofer et al. [27] extend 4 image self-supervised learning methods to video representations and compare their effi- cacy on several downstream datasets, while Kumar et al. [11] focus on the effects of different factors in self-supervised video pretrain- ing. However, these works focus on individual domains, and cannot make use of the relationship between paired audio/visual inputs. Previous multi-task multimodal benchmarks either focus on ego- centric videos [28],"}, {"question": " What approach is considered effective for building multitasking algorithmic systems in speech, audio, and vision domains?", "answer": " The pretrain-then-finetune paradigm is considered effective for building multitasking algorithmic systems in speech, audio, and vision domains.", "ref_chunk": "3 2 0 2 p e S 9 1 ] S A . s s e e [ 1 v 7 8 7 0 1 . 9 0 3 2 : v i X r a AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS Yuan Tseng1, Layne Berry2\u2217, Yi-Ting Chen3\u2217, I-Hsiang Chiu1\u2217, Hsuan-Hao Lin1\u2217, Max Liu1\u2217, Puyuan Peng2\u2217, Yi-Jen Shih1\u2217, Hung-Yu Wang1\u2217, Haibin Wu1\u2217, Po-Yao Huang4, Chun-Mao Lai1, Shang-Wen Li4, David Harwath2, Yu Tsao3, Shinji Watanabe5, Abdelrahman Mohamed6, Chi-Luen Feng1, Hung-yi Lee1 1 National Taiwan University, Taiwan 2 University of Texas at Austin, USA 3 Academia Sinica, Taiwan 4 Meta AI 5 Carnegie Mellon University, USA 6 Rembrand r11942082@ntu.edu.tw ABSTRACT Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned repre- sentations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal au- dio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong in- termediate task. We release our benchmark with evaluation code1 and a model submission platform2 to encourage further research in audio-visual learning. human perception, which synergistically integrates auditory and vi- sual cues [12, 13]. While audio-visual representation learning has made significant progress [14, 15, 16, 17, 18], the assessment of these models tends to be task-specific, leaving the broader gener- alization capabilities across various audio-visual challenges less un- derstood. This complicates comparitive analysis of different models and training strategies, impeding the development of more robust and versatile audio-visual representation learning approaches. To address this issue, we propose AV-SUPERB, a standardized benchmark for comprehensively evaluating representations across seven distinct datasets involving five speech and audio processing tasks. AV-SUPERB comprises of three tracks to assess audio, video, and audio-visual fusion representations. We envision that these dis- tinct tracks will allow researchers in speech, audio, and video repre- sentation learning alike to compare learning strategies across models and modalities, enabling broader analysis of their effectiveness. Index Terms\u2014 Audio-Visual Learning, Representation Learn- ing, Evaluation, Self-Supervised Learning 1. INTRODUCTION Emulating the seamless integration of multiple tasks in human cognition, such as spoken language comprehension, sound event detection, and visual object recognition has been a long-standing goal of computational research. Prior research demonstrates that the pretrain-then-finetune paradigm is an effective and scalable method of building multitasking algorithmic systems for speech [1, 2], au- dio [3, 4], and vision [5, 6]. In the pretraining stage, models can often learn meaningful representations from unlabelled data alone through optimization of contrastive, masked prediction, or other self-supervised loss functions. These pretrained representations can then be applied to diverse tasks just by fine-tuning minimal additional parameters. In order to better measure progress in representation learning, previous works have established multitask benchmarks in speech [7, 8], audio [9], and vision [10, 11]. However, these benchmark pre- dominantly evaluate performance in isolation within single modal- ities. This approach overlooks the inherent multimodal nature of \u2217 Equal contribution; sorted alphabetically 1https://github.com/roger-tseng/av-superb 2https://av.superbbenchmark.org Our contributions are four-fold: (1) Diverse-domain evalua- tion: We propose the first audio-visual learning benchmark that en- compasses multiple datasets and tasks, covering both speech and au- dio domains. (2) Easy and reproducible benchmarking: We re- lease evaluation code and a dedicated model submission platform that ensures reproducible evaluation on dynamic Youtube datasets (3) Intermediate-task and reduces computational entry barriers. fine-tuning: Our work emphasizes the potential benefits of full fine- tuning on intermediate tasks for improving performance on out-of- domain downstream tasks. (4) Layer-wise analysis: We show that different layers contribute variably to task performance, suggesting that simply using representations of the final layer is suboptimal, motivating the weighted-sum evaluation approach. 2. RELATED WORK Recognizing how the close relation between audition and vision fa- cilitates multimodal human perception, many audio-visual datasets have been gathered for action recognition [19, 15, 20, 21], speech recognition [22, 23, 24], speaker recognition [25, 26], and a vari- ety of other tasks to study audio-visual learning. However, most models are trained and evaluated on different datasets with different experiment settings, which increases comparison difficulty and ob- fuscates the broad applicability of proposed methods. Hence in the AV-SUPERB benchmark, we select a diverse set of datasets from multiple tasks to comprehensively compare works in audio-visual representation learning. Previous multitask benchmarks in speech [7, 8], audio [9], and Fig. 1. We consider three evaluation scenarios: extracting fea- tures using inputs from one or both modalities. Following [7], the weighted-sum of features from Transformer layers (if applicable) are used as input for fine-tuning a small downstream model for each in- dividual task. Details of selected tasks are given in Section 3.1. video representation learning [27, 10, 11] allow for fairer com- parison of different models and promote research towards general approaches that are applicable to a variety of real-world tasks. SUPERB [7] and SUPERB-SG [8] evaluate speech representation models on a wide range of downstream tasks covering content, speaker, and other different aspects of speech. Additionally, the HEAR benchmark [9] evaluates audio representations on diverse domains beyond speech, such as music and environmental sounds. For video representations, the SEVERE-benchmark [10] evaluates video self-supervised learning models on a diverse set of datasets to measure model sensitivity to different properties of downstream tasks. Feichtenhofer et al. [27] extend 4 image self-supervised learning methods to video representations and compare their effi- cacy on several downstream datasets, while Kumar et al. [11] focus on the effects of different factors in self-supervised video pretrain- ing. However, these works focus on individual domains, and cannot make use of the relationship between paired audio/visual inputs. Previous multi-task multimodal benchmarks either focus on ego- centric videos [28],"}, {"question": " What do pretrained representations obtained through self-supervised loss functions allow models to do?", "answer": " Pretrained representations obtained through self-supervised loss functions allow models to be applied to diverse tasks by fine-tuning minimal additional parameters.", "ref_chunk": "3 2 0 2 p e S 9 1 ] S A . s s e e [ 1 v 7 8 7 0 1 . 9 0 3 2 : v i X r a AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS Yuan Tseng1, Layne Berry2\u2217, Yi-Ting Chen3\u2217, I-Hsiang Chiu1\u2217, Hsuan-Hao Lin1\u2217, Max Liu1\u2217, Puyuan Peng2\u2217, Yi-Jen Shih1\u2217, Hung-Yu Wang1\u2217, Haibin Wu1\u2217, Po-Yao Huang4, Chun-Mao Lai1, Shang-Wen Li4, David Harwath2, Yu Tsao3, Shinji Watanabe5, Abdelrahman Mohamed6, Chi-Luen Feng1, Hung-yi Lee1 1 National Taiwan University, Taiwan 2 University of Texas at Austin, USA 3 Academia Sinica, Taiwan 4 Meta AI 5 Carnegie Mellon University, USA 6 Rembrand r11942082@ntu.edu.tw ABSTRACT Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned repre- sentations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal au- dio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong in- termediate task. We release our benchmark with evaluation code1 and a model submission platform2 to encourage further research in audio-visual learning. human perception, which synergistically integrates auditory and vi- sual cues [12, 13]. While audio-visual representation learning has made significant progress [14, 15, 16, 17, 18], the assessment of these models tends to be task-specific, leaving the broader gener- alization capabilities across various audio-visual challenges less un- derstood. This complicates comparitive analysis of different models and training strategies, impeding the development of more robust and versatile audio-visual representation learning approaches. To address this issue, we propose AV-SUPERB, a standardized benchmark for comprehensively evaluating representations across seven distinct datasets involving five speech and audio processing tasks. AV-SUPERB comprises of three tracks to assess audio, video, and audio-visual fusion representations. We envision that these dis- tinct tracks will allow researchers in speech, audio, and video repre- sentation learning alike to compare learning strategies across models and modalities, enabling broader analysis of their effectiveness. Index Terms\u2014 Audio-Visual Learning, Representation Learn- ing, Evaluation, Self-Supervised Learning 1. INTRODUCTION Emulating the seamless integration of multiple tasks in human cognition, such as spoken language comprehension, sound event detection, and visual object recognition has been a long-standing goal of computational research. Prior research demonstrates that the pretrain-then-finetune paradigm is an effective and scalable method of building multitasking algorithmic systems for speech [1, 2], au- dio [3, 4], and vision [5, 6]. In the pretraining stage, models can often learn meaningful representations from unlabelled data alone through optimization of contrastive, masked prediction, or other self-supervised loss functions. These pretrained representations can then be applied to diverse tasks just by fine-tuning minimal additional parameters. In order to better measure progress in representation learning, previous works have established multitask benchmarks in speech [7, 8], audio [9], and vision [10, 11]. However, these benchmark pre- dominantly evaluate performance in isolation within single modal- ities. This approach overlooks the inherent multimodal nature of \u2217 Equal contribution; sorted alphabetically 1https://github.com/roger-tseng/av-superb 2https://av.superbbenchmark.org Our contributions are four-fold: (1) Diverse-domain evalua- tion: We propose the first audio-visual learning benchmark that en- compasses multiple datasets and tasks, covering both speech and au- dio domains. (2) Easy and reproducible benchmarking: We re- lease evaluation code and a dedicated model submission platform that ensures reproducible evaluation on dynamic Youtube datasets (3) Intermediate-task and reduces computational entry barriers. fine-tuning: Our work emphasizes the potential benefits of full fine- tuning on intermediate tasks for improving performance on out-of- domain downstream tasks. (4) Layer-wise analysis: We show that different layers contribute variably to task performance, suggesting that simply using representations of the final layer is suboptimal, motivating the weighted-sum evaluation approach. 2. RELATED WORK Recognizing how the close relation between audition and vision fa- cilitates multimodal human perception, many audio-visual datasets have been gathered for action recognition [19, 15, 20, 21], speech recognition [22, 23, 24], speaker recognition [25, 26], and a vari- ety of other tasks to study audio-visual learning. However, most models are trained and evaluated on different datasets with different experiment settings, which increases comparison difficulty and ob- fuscates the broad applicability of proposed methods. Hence in the AV-SUPERB benchmark, we select a diverse set of datasets from multiple tasks to comprehensively compare works in audio-visual representation learning. Previous multitask benchmarks in speech [7, 8], audio [9], and Fig. 1. We consider three evaluation scenarios: extracting fea- tures using inputs from one or both modalities. Following [7], the weighted-sum of features from Transformer layers (if applicable) are used as input for fine-tuning a small downstream model for each in- dividual task. Details of selected tasks are given in Section 3.1. video representation learning [27, 10, 11] allow for fairer com- parison of different models and promote research towards general approaches that are applicable to a variety of real-world tasks. SUPERB [7] and SUPERB-SG [8] evaluate speech representation models on a wide range of downstream tasks covering content, speaker, and other different aspects of speech. Additionally, the HEAR benchmark [9] evaluates audio representations on diverse domains beyond speech, such as music and environmental sounds. For video representations, the SEVERE-benchmark [10] evaluates video self-supervised learning models on a diverse set of datasets to measure model sensitivity to different properties of downstream tasks. Feichtenhofer et al. [27] extend 4 image self-supervised learning methods to video representations and compare their effi- cacy on several downstream datasets, while Kumar et al. [11] focus on the effects of different factors in self-supervised video pretrain- ing. However, these works focus on individual domains, and cannot make use of the relationship between paired audio/visual inputs. Previous multi-task multimodal benchmarks either focus on ego- centric videos [28],"}, {"question": " What are the contributions of the AV-SUPERB benchmark according to the text?", "answer": " The contributions of the AV-SUPERB benchmark include diverse-domain evaluation, easy and reproducible benchmarking, intermediate-task fine-tuning, and layer-wise analysis of contributions to task performance.", "ref_chunk": "3 2 0 2 p e S 9 1 ] S A . s s e e [ 1 v 7 8 7 0 1 . 9 0 3 2 : v i X r a AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS Yuan Tseng1, Layne Berry2\u2217, Yi-Ting Chen3\u2217, I-Hsiang Chiu1\u2217, Hsuan-Hao Lin1\u2217, Max Liu1\u2217, Puyuan Peng2\u2217, Yi-Jen Shih1\u2217, Hung-Yu Wang1\u2217, Haibin Wu1\u2217, Po-Yao Huang4, Chun-Mao Lai1, Shang-Wen Li4, David Harwath2, Yu Tsao3, Shinji Watanabe5, Abdelrahman Mohamed6, Chi-Luen Feng1, Hung-yi Lee1 1 National Taiwan University, Taiwan 2 University of Texas at Austin, USA 3 Academia Sinica, Taiwan 4 Meta AI 5 Carnegie Mellon University, USA 6 Rembrand r11942082@ntu.edu.tw ABSTRACT Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned repre- sentations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal au- dio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong in- termediate task. We release our benchmark with evaluation code1 and a model submission platform2 to encourage further research in audio-visual learning. human perception, which synergistically integrates auditory and vi- sual cues [12, 13]. While audio-visual representation learning has made significant progress [14, 15, 16, 17, 18], the assessment of these models tends to be task-specific, leaving the broader gener- alization capabilities across various audio-visual challenges less un- derstood. This complicates comparitive analysis of different models and training strategies, impeding the development of more robust and versatile audio-visual representation learning approaches. To address this issue, we propose AV-SUPERB, a standardized benchmark for comprehensively evaluating representations across seven distinct datasets involving five speech and audio processing tasks. AV-SUPERB comprises of three tracks to assess audio, video, and audio-visual fusion representations. We envision that these dis- tinct tracks will allow researchers in speech, audio, and video repre- sentation learning alike to compare learning strategies across models and modalities, enabling broader analysis of their effectiveness. Index Terms\u2014 Audio-Visual Learning, Representation Learn- ing, Evaluation, Self-Supervised Learning 1. INTRODUCTION Emulating the seamless integration of multiple tasks in human cognition, such as spoken language comprehension, sound event detection, and visual object recognition has been a long-standing goal of computational research. Prior research demonstrates that the pretrain-then-finetune paradigm is an effective and scalable method of building multitasking algorithmic systems for speech [1, 2], au- dio [3, 4], and vision [5, 6]. In the pretraining stage, models can often learn meaningful representations from unlabelled data alone through optimization of contrastive, masked prediction, or other self-supervised loss functions. These pretrained representations can then be applied to diverse tasks just by fine-tuning minimal additional parameters. In order to better measure progress in representation learning, previous works have established multitask benchmarks in speech [7, 8], audio [9], and vision [10, 11]. However, these benchmark pre- dominantly evaluate performance in isolation within single modal- ities. This approach overlooks the inherent multimodal nature of \u2217 Equal contribution; sorted alphabetically 1https://github.com/roger-tseng/av-superb 2https://av.superbbenchmark.org Our contributions are four-fold: (1) Diverse-domain evalua- tion: We propose the first audio-visual learning benchmark that en- compasses multiple datasets and tasks, covering both speech and au- dio domains. (2) Easy and reproducible benchmarking: We re- lease evaluation code and a dedicated model submission platform that ensures reproducible evaluation on dynamic Youtube datasets (3) Intermediate-task and reduces computational entry barriers. fine-tuning: Our work emphasizes the potential benefits of full fine- tuning on intermediate tasks for improving performance on out-of- domain downstream tasks. (4) Layer-wise analysis: We show that different layers contribute variably to task performance, suggesting that simply using representations of the final layer is suboptimal, motivating the weighted-sum evaluation approach. 2. RELATED WORK Recognizing how the close relation between audition and vision fa- cilitates multimodal human perception, many audio-visual datasets have been gathered for action recognition [19, 15, 20, 21], speech recognition [22, 23, 24], speaker recognition [25, 26], and a vari- ety of other tasks to study audio-visual learning. However, most models are trained and evaluated on different datasets with different experiment settings, which increases comparison difficulty and ob- fuscates the broad applicability of proposed methods. Hence in the AV-SUPERB benchmark, we select a diverse set of datasets from multiple tasks to comprehensively compare works in audio-visual representation learning. Previous multitask benchmarks in speech [7, 8], audio [9], and Fig. 1. We consider three evaluation scenarios: extracting fea- tures using inputs from one or both modalities. Following [7], the weighted-sum of features from Transformer layers (if applicable) are used as input for fine-tuning a small downstream model for each in- dividual task. Details of selected tasks are given in Section 3.1. video representation learning [27, 10, 11] allow for fairer com- parison of different models and promote research towards general approaches that are applicable to a variety of real-world tasks. SUPERB [7] and SUPERB-SG [8] evaluate speech representation models on a wide range of downstream tasks covering content, speaker, and other different aspects of speech. Additionally, the HEAR benchmark [9] evaluates audio representations on diverse domains beyond speech, such as music and environmental sounds. For video representations, the SEVERE-benchmark [10] evaluates video self-supervised learning models on a diverse set of datasets to measure model sensitivity to different properties of downstream tasks. Feichtenhofer et al. [27] extend 4 image self-supervised learning methods to video representations and compare their effi- cacy on several downstream datasets, while Kumar et al. [11] focus on the effects of different factors in self-supervised video pretrain- ing. However, these works focus on individual domains, and cannot make use of the relationship between paired audio/visual inputs. Previous multi-task multimodal benchmarks either focus on ego- centric videos [28],"}, {"question": " How do previous multitask benchmarks in speech, audio, and video representation learning promote fairer comparison of different models?", "answer": " Previous multitask benchmarks in speech, audio, and video representation learning evaluate models across different tasks to promote research towards general approaches applicable to various real-world tasks.", "ref_chunk": "3 2 0 2 p e S 9 1 ] S A . s s e e [ 1 v 7 8 7 0 1 . 9 0 3 2 : v i X r a AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS Yuan Tseng1, Layne Berry2\u2217, Yi-Ting Chen3\u2217, I-Hsiang Chiu1\u2217, Hsuan-Hao Lin1\u2217, Max Liu1\u2217, Puyuan Peng2\u2217, Yi-Jen Shih1\u2217, Hung-Yu Wang1\u2217, Haibin Wu1\u2217, Po-Yao Huang4, Chun-Mao Lai1, Shang-Wen Li4, David Harwath2, Yu Tsao3, Shinji Watanabe5, Abdelrahman Mohamed6, Chi-Luen Feng1, Hung-yi Lee1 1 National Taiwan University, Taiwan 2 University of Texas at Austin, USA 3 Academia Sinica, Taiwan 4 Meta AI 5 Carnegie Mellon University, USA 6 Rembrand r11942082@ntu.edu.tw ABSTRACT Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned repre- sentations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal au- dio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong in- termediate task. We release our benchmark with evaluation code1 and a model submission platform2 to encourage further research in audio-visual learning. human perception, which synergistically integrates auditory and vi- sual cues [12, 13]. While audio-visual representation learning has made significant progress [14, 15, 16, 17, 18], the assessment of these models tends to be task-specific, leaving the broader gener- alization capabilities across various audio-visual challenges less un- derstood. This complicates comparitive analysis of different models and training strategies, impeding the development of more robust and versatile audio-visual representation learning approaches. To address this issue, we propose AV-SUPERB, a standardized benchmark for comprehensively evaluating representations across seven distinct datasets involving five speech and audio processing tasks. AV-SUPERB comprises of three tracks to assess audio, video, and audio-visual fusion representations. We envision that these dis- tinct tracks will allow researchers in speech, audio, and video repre- sentation learning alike to compare learning strategies across models and modalities, enabling broader analysis of their effectiveness. Index Terms\u2014 Audio-Visual Learning, Representation Learn- ing, Evaluation, Self-Supervised Learning 1. INTRODUCTION Emulating the seamless integration of multiple tasks in human cognition, such as spoken language comprehension, sound event detection, and visual object recognition has been a long-standing goal of computational research. Prior research demonstrates that the pretrain-then-finetune paradigm is an effective and scalable method of building multitasking algorithmic systems for speech [1, 2], au- dio [3, 4], and vision [5, 6]. In the pretraining stage, models can often learn meaningful representations from unlabelled data alone through optimization of contrastive, masked prediction, or other self-supervised loss functions. These pretrained representations can then be applied to diverse tasks just by fine-tuning minimal additional parameters. In order to better measure progress in representation learning, previous works have established multitask benchmarks in speech [7, 8], audio [9], and vision [10, 11]. However, these benchmark pre- dominantly evaluate performance in isolation within single modal- ities. This approach overlooks the inherent multimodal nature of \u2217 Equal contribution; sorted alphabetically 1https://github.com/roger-tseng/av-superb 2https://av.superbbenchmark.org Our contributions are four-fold: (1) Diverse-domain evalua- tion: We propose the first audio-visual learning benchmark that en- compasses multiple datasets and tasks, covering both speech and au- dio domains. (2) Easy and reproducible benchmarking: We re- lease evaluation code and a dedicated model submission platform that ensures reproducible evaluation on dynamic Youtube datasets (3) Intermediate-task and reduces computational entry barriers. fine-tuning: Our work emphasizes the potential benefits of full fine- tuning on intermediate tasks for improving performance on out-of- domain downstream tasks. (4) Layer-wise analysis: We show that different layers contribute variably to task performance, suggesting that simply using representations of the final layer is suboptimal, motivating the weighted-sum evaluation approach. 2. RELATED WORK Recognizing how the close relation between audition and vision fa- cilitates multimodal human perception, many audio-visual datasets have been gathered for action recognition [19, 15, 20, 21], speech recognition [22, 23, 24], speaker recognition [25, 26], and a vari- ety of other tasks to study audio-visual learning. However, most models are trained and evaluated on different datasets with different experiment settings, which increases comparison difficulty and ob- fuscates the broad applicability of proposed methods. Hence in the AV-SUPERB benchmark, we select a diverse set of datasets from multiple tasks to comprehensively compare works in audio-visual representation learning. Previous multitask benchmarks in speech [7, 8], audio [9], and Fig. 1. We consider three evaluation scenarios: extracting fea- tures using inputs from one or both modalities. Following [7], the weighted-sum of features from Transformer layers (if applicable) are used as input for fine-tuning a small downstream model for each in- dividual task. Details of selected tasks are given in Section 3.1. video representation learning [27, 10, 11] allow for fairer com- parison of different models and promote research towards general approaches that are applicable to a variety of real-world tasks. SUPERB [7] and SUPERB-SG [8] evaluate speech representation models on a wide range of downstream tasks covering content, speaker, and other different aspects of speech. Additionally, the HEAR benchmark [9] evaluates audio representations on diverse domains beyond speech, such as music and environmental sounds. For video representations, the SEVERE-benchmark [10] evaluates video self-supervised learning models on a diverse set of datasets to measure model sensitivity to different properties of downstream tasks. Feichtenhofer et al. [27] extend 4 image self-supervised learning methods to video representations and compare their effi- cacy on several downstream datasets, while Kumar et al. [11] focus on the effects of different factors in self-supervised video pretrain- ing. However, these works focus on individual domains, and cannot make use of the relationship between paired audio/visual inputs. Previous multi-task multimodal benchmarks either focus on ego- centric videos [28],"}, {"question": " What do the SUPERB and SUPERB-SG benchmarks evaluate?", "answer": " The SUPERB and SUPERB-SG benchmarks evaluate speech representation models on a wide range of downstream tasks covering content, speaker, and other aspects of speech.", "ref_chunk": "3 2 0 2 p e S 9 1 ] S A . s s e e [ 1 v 7 8 7 0 1 . 9 0 3 2 : v i X r a AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS Yuan Tseng1, Layne Berry2\u2217, Yi-Ting Chen3\u2217, I-Hsiang Chiu1\u2217, Hsuan-Hao Lin1\u2217, Max Liu1\u2217, Puyuan Peng2\u2217, Yi-Jen Shih1\u2217, Hung-Yu Wang1\u2217, Haibin Wu1\u2217, Po-Yao Huang4, Chun-Mao Lai1, Shang-Wen Li4, David Harwath2, Yu Tsao3, Shinji Watanabe5, Abdelrahman Mohamed6, Chi-Luen Feng1, Hung-yi Lee1 1 National Taiwan University, Taiwan 2 University of Texas at Austin, USA 3 Academia Sinica, Taiwan 4 Meta AI 5 Carnegie Mellon University, USA 6 Rembrand r11942082@ntu.edu.tw ABSTRACT Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned repre- sentations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal au- dio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong in- termediate task. We release our benchmark with evaluation code1 and a model submission platform2 to encourage further research in audio-visual learning. human perception, which synergistically integrates auditory and vi- sual cues [12, 13]. While audio-visual representation learning has made significant progress [14, 15, 16, 17, 18], the assessment of these models tends to be task-specific, leaving the broader gener- alization capabilities across various audio-visual challenges less un- derstood. This complicates comparitive analysis of different models and training strategies, impeding the development of more robust and versatile audio-visual representation learning approaches. To address this issue, we propose AV-SUPERB, a standardized benchmark for comprehensively evaluating representations across seven distinct datasets involving five speech and audio processing tasks. AV-SUPERB comprises of three tracks to assess audio, video, and audio-visual fusion representations. We envision that these dis- tinct tracks will allow researchers in speech, audio, and video repre- sentation learning alike to compare learning strategies across models and modalities, enabling broader analysis of their effectiveness. Index Terms\u2014 Audio-Visual Learning, Representation Learn- ing, Evaluation, Self-Supervised Learning 1. INTRODUCTION Emulating the seamless integration of multiple tasks in human cognition, such as spoken language comprehension, sound event detection, and visual object recognition has been a long-standing goal of computational research. Prior research demonstrates that the pretrain-then-finetune paradigm is an effective and scalable method of building multitasking algorithmic systems for speech [1, 2], au- dio [3, 4], and vision [5, 6]. In the pretraining stage, models can often learn meaningful representations from unlabelled data alone through optimization of contrastive, masked prediction, or other self-supervised loss functions. These pretrained representations can then be applied to diverse tasks just by fine-tuning minimal additional parameters. In order to better measure progress in representation learning, previous works have established multitask benchmarks in speech [7, 8], audio [9], and vision [10, 11]. However, these benchmark pre- dominantly evaluate performance in isolation within single modal- ities. This approach overlooks the inherent multimodal nature of \u2217 Equal contribution; sorted alphabetically 1https://github.com/roger-tseng/av-superb 2https://av.superbbenchmark.org Our contributions are four-fold: (1) Diverse-domain evalua- tion: We propose the first audio-visual learning benchmark that en- compasses multiple datasets and tasks, covering both speech and au- dio domains. (2) Easy and reproducible benchmarking: We re- lease evaluation code and a dedicated model submission platform that ensures reproducible evaluation on dynamic Youtube datasets (3) Intermediate-task and reduces computational entry barriers. fine-tuning: Our work emphasizes the potential benefits of full fine- tuning on intermediate tasks for improving performance on out-of- domain downstream tasks. (4) Layer-wise analysis: We show that different layers contribute variably to task performance, suggesting that simply using representations of the final layer is suboptimal, motivating the weighted-sum evaluation approach. 2. RELATED WORK Recognizing how the close relation between audition and vision fa- cilitates multimodal human perception, many audio-visual datasets have been gathered for action recognition [19, 15, 20, 21], speech recognition [22, 23, 24], speaker recognition [25, 26], and a vari- ety of other tasks to study audio-visual learning. However, most models are trained and evaluated on different datasets with different experiment settings, which increases comparison difficulty and ob- fuscates the broad applicability of proposed methods. Hence in the AV-SUPERB benchmark, we select a diverse set of datasets from multiple tasks to comprehensively compare works in audio-visual representation learning. Previous multitask benchmarks in speech [7, 8], audio [9], and Fig. 1. We consider three evaluation scenarios: extracting fea- tures using inputs from one or both modalities. Following [7], the weighted-sum of features from Transformer layers (if applicable) are used as input for fine-tuning a small downstream model for each in- dividual task. Details of selected tasks are given in Section 3.1. video representation learning [27, 10, 11] allow for fairer com- parison of different models and promote research towards general approaches that are applicable to a variety of real-world tasks. SUPERB [7] and SUPERB-SG [8] evaluate speech representation models on a wide range of downstream tasks covering content, speaker, and other different aspects of speech. Additionally, the HEAR benchmark [9] evaluates audio representations on diverse domains beyond speech, such as music and environmental sounds. For video representations, the SEVERE-benchmark [10] evaluates video self-supervised learning models on a diverse set of datasets to measure model sensitivity to different properties of downstream tasks. Feichtenhofer et al. [27] extend 4 image self-supervised learning methods to video representations and compare their effi- cacy on several downstream datasets, while Kumar et al. [11] focus on the effects of different factors in self-supervised video pretrain- ing. However, these works focus on individual domains, and cannot make use of the relationship between paired audio/visual inputs. Previous multi-task multimodal benchmarks either focus on ego- centric videos [28],"}, {"question": " What is the purpose of the HEAR benchmark?", "answer": " The HEAR benchmark evaluates audio representations on domains beyond speech, such as music and environmental sounds.", "ref_chunk": "3 2 0 2 p e S 9 1 ] S A . s s e e [ 1 v 7 8 7 0 1 . 9 0 3 2 : v i X r a AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS Yuan Tseng1, Layne Berry2\u2217, Yi-Ting Chen3\u2217, I-Hsiang Chiu1\u2217, Hsuan-Hao Lin1\u2217, Max Liu1\u2217, Puyuan Peng2\u2217, Yi-Jen Shih1\u2217, Hung-Yu Wang1\u2217, Haibin Wu1\u2217, Po-Yao Huang4, Chun-Mao Lai1, Shang-Wen Li4, David Harwath2, Yu Tsao3, Shinji Watanabe5, Abdelrahman Mohamed6, Chi-Luen Feng1, Hung-yi Lee1 1 National Taiwan University, Taiwan 2 University of Texas at Austin, USA 3 Academia Sinica, Taiwan 4 Meta AI 5 Carnegie Mellon University, USA 6 Rembrand r11942082@ntu.edu.tw ABSTRACT Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned repre- sentations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal au- dio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong in- termediate task. We release our benchmark with evaluation code1 and a model submission platform2 to encourage further research in audio-visual learning. human perception, which synergistically integrates auditory and vi- sual cues [12, 13]. While audio-visual representation learning has made significant progress [14, 15, 16, 17, 18], the assessment of these models tends to be task-specific, leaving the broader gener- alization capabilities across various audio-visual challenges less un- derstood. This complicates comparitive analysis of different models and training strategies, impeding the development of more robust and versatile audio-visual representation learning approaches. To address this issue, we propose AV-SUPERB, a standardized benchmark for comprehensively evaluating representations across seven distinct datasets involving five speech and audio processing tasks. AV-SUPERB comprises of three tracks to assess audio, video, and audio-visual fusion representations. We envision that these dis- tinct tracks will allow researchers in speech, audio, and video repre- sentation learning alike to compare learning strategies across models and modalities, enabling broader analysis of their effectiveness. Index Terms\u2014 Audio-Visual Learning, Representation Learn- ing, Evaluation, Self-Supervised Learning 1. INTRODUCTION Emulating the seamless integration of multiple tasks in human cognition, such as spoken language comprehension, sound event detection, and visual object recognition has been a long-standing goal of computational research. Prior research demonstrates that the pretrain-then-finetune paradigm is an effective and scalable method of building multitasking algorithmic systems for speech [1, 2], au- dio [3, 4], and vision [5, 6]. In the pretraining stage, models can often learn meaningful representations from unlabelled data alone through optimization of contrastive, masked prediction, or other self-supervised loss functions. These pretrained representations can then be applied to diverse tasks just by fine-tuning minimal additional parameters. In order to better measure progress in representation learning, previous works have established multitask benchmarks in speech [7, 8], audio [9], and vision [10, 11]. However, these benchmark pre- dominantly evaluate performance in isolation within single modal- ities. This approach overlooks the inherent multimodal nature of \u2217 Equal contribution; sorted alphabetically 1https://github.com/roger-tseng/av-superb 2https://av.superbbenchmark.org Our contributions are four-fold: (1) Diverse-domain evalua- tion: We propose the first audio-visual learning benchmark that en- compasses multiple datasets and tasks, covering both speech and au- dio domains. (2) Easy and reproducible benchmarking: We re- lease evaluation code and a dedicated model submission platform that ensures reproducible evaluation on dynamic Youtube datasets (3) Intermediate-task and reduces computational entry barriers. fine-tuning: Our work emphasizes the potential benefits of full fine- tuning on intermediate tasks for improving performance on out-of- domain downstream tasks. (4) Layer-wise analysis: We show that different layers contribute variably to task performance, suggesting that simply using representations of the final layer is suboptimal, motivating the weighted-sum evaluation approach. 2. RELATED WORK Recognizing how the close relation between audition and vision fa- cilitates multimodal human perception, many audio-visual datasets have been gathered for action recognition [19, 15, 20, 21], speech recognition [22, 23, 24], speaker recognition [25, 26], and a vari- ety of other tasks to study audio-visual learning. However, most models are trained and evaluated on different datasets with different experiment settings, which increases comparison difficulty and ob- fuscates the broad applicability of proposed methods. Hence in the AV-SUPERB benchmark, we select a diverse set of datasets from multiple tasks to comprehensively compare works in audio-visual representation learning. Previous multitask benchmarks in speech [7, 8], audio [9], and Fig. 1. We consider three evaluation scenarios: extracting fea- tures using inputs from one or both modalities. Following [7], the weighted-sum of features from Transformer layers (if applicable) are used as input for fine-tuning a small downstream model for each in- dividual task. Details of selected tasks are given in Section 3.1. video representation learning [27, 10, 11] allow for fairer com- parison of different models and promote research towards general approaches that are applicable to a variety of real-world tasks. SUPERB [7] and SUPERB-SG [8] evaluate speech representation models on a wide range of downstream tasks covering content, speaker, and other different aspects of speech. Additionally, the HEAR benchmark [9] evaluates audio representations on diverse domains beyond speech, such as music and environmental sounds. For video representations, the SEVERE-benchmark [10] evaluates video self-supervised learning models on a diverse set of datasets to measure model sensitivity to different properties of downstream tasks. Feichtenhofer et al. [27] extend 4 image self-supervised learning methods to video representations and compare their effi- cacy on several downstream datasets, while Kumar et al. [11] focus on the effects of different factors in self-supervised video pretrain- ing. However, these works focus on individual domains, and cannot make use of the relationship between paired audio/visual inputs. Previous multi-task multimodal benchmarks either focus on ego- centric videos [28],"}, {"question": " How do multi-task multimodal benchmarks contribute to research in audio-visual learning?", "answer": " Multi-task multimodal benchmarks allow for a focus on the relationship between paired audio/visual inputs, promoting research that leverages this connection.", "ref_chunk": "3 2 0 2 p e S 9 1 ] S A . s s e e [ 1 v 7 8 7 0 1 . 9 0 3 2 : v i X r a AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS Yuan Tseng1, Layne Berry2\u2217, Yi-Ting Chen3\u2217, I-Hsiang Chiu1\u2217, Hsuan-Hao Lin1\u2217, Max Liu1\u2217, Puyuan Peng2\u2217, Yi-Jen Shih1\u2217, Hung-Yu Wang1\u2217, Haibin Wu1\u2217, Po-Yao Huang4, Chun-Mao Lai1, Shang-Wen Li4, David Harwath2, Yu Tsao3, Shinji Watanabe5, Abdelrahman Mohamed6, Chi-Luen Feng1, Hung-yi Lee1 1 National Taiwan University, Taiwan 2 University of Texas at Austin, USA 3 Academia Sinica, Taiwan 4 Meta AI 5 Carnegie Mellon University, USA 6 Rembrand r11942082@ntu.edu.tw ABSTRACT Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned repre- sentations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal au- dio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong in- termediate task. We release our benchmark with evaluation code1 and a model submission platform2 to encourage further research in audio-visual learning. human perception, which synergistically integrates auditory and vi- sual cues [12, 13]. While audio-visual representation learning has made significant progress [14, 15, 16, 17, 18], the assessment of these models tends to be task-specific, leaving the broader gener- alization capabilities across various audio-visual challenges less un- derstood. This complicates comparitive analysis of different models and training strategies, impeding the development of more robust and versatile audio-visual representation learning approaches. To address this issue, we propose AV-SUPERB, a standardized benchmark for comprehensively evaluating representations across seven distinct datasets involving five speech and audio processing tasks. AV-SUPERB comprises of three tracks to assess audio, video, and audio-visual fusion representations. We envision that these dis- tinct tracks will allow researchers in speech, audio, and video repre- sentation learning alike to compare learning strategies across models and modalities, enabling broader analysis of their effectiveness. Index Terms\u2014 Audio-Visual Learning, Representation Learn- ing, Evaluation, Self-Supervised Learning 1. INTRODUCTION Emulating the seamless integration of multiple tasks in human cognition, such as spoken language comprehension, sound event detection, and visual object recognition has been a long-standing goal of computational research. Prior research demonstrates that the pretrain-then-finetune paradigm is an effective and scalable method of building multitasking algorithmic systems for speech [1, 2], au- dio [3, 4], and vision [5, 6]. In the pretraining stage, models can often learn meaningful representations from unlabelled data alone through optimization of contrastive, masked prediction, or other self-supervised loss functions. These pretrained representations can then be applied to diverse tasks just by fine-tuning minimal additional parameters. In order to better measure progress in representation learning, previous works have established multitask benchmarks in speech [7, 8], audio [9], and vision [10, 11]. However, these benchmark pre- dominantly evaluate performance in isolation within single modal- ities. This approach overlooks the inherent multimodal nature of \u2217 Equal contribution; sorted alphabetically 1https://github.com/roger-tseng/av-superb 2https://av.superbbenchmark.org Our contributions are four-fold: (1) Diverse-domain evalua- tion: We propose the first audio-visual learning benchmark that en- compasses multiple datasets and tasks, covering both speech and au- dio domains. (2) Easy and reproducible benchmarking: We re- lease evaluation code and a dedicated model submission platform that ensures reproducible evaluation on dynamic Youtube datasets (3) Intermediate-task and reduces computational entry barriers. fine-tuning: Our work emphasizes the potential benefits of full fine- tuning on intermediate tasks for improving performance on out-of- domain downstream tasks. (4) Layer-wise analysis: We show that different layers contribute variably to task performance, suggesting that simply using representations of the final layer is suboptimal, motivating the weighted-sum evaluation approach. 2. RELATED WORK Recognizing how the close relation between audition and vision fa- cilitates multimodal human perception, many audio-visual datasets have been gathered for action recognition [19, 15, 20, 21], speech recognition [22, 23, 24], speaker recognition [25, 26], and a vari- ety of other tasks to study audio-visual learning. However, most models are trained and evaluated on different datasets with different experiment settings, which increases comparison difficulty and ob- fuscates the broad applicability of proposed methods. Hence in the AV-SUPERB benchmark, we select a diverse set of datasets from multiple tasks to comprehensively compare works in audio-visual representation learning. Previous multitask benchmarks in speech [7, 8], audio [9], and Fig. 1. We consider three evaluation scenarios: extracting fea- tures using inputs from one or both modalities. Following [7], the weighted-sum of features from Transformer layers (if applicable) are used as input for fine-tuning a small downstream model for each in- dividual task. Details of selected tasks are given in Section 3.1. video representation learning [27, 10, 11] allow for fairer com- parison of different models and promote research towards general approaches that are applicable to a variety of real-world tasks. SUPERB [7] and SUPERB-SG [8] evaluate speech representation models on a wide range of downstream tasks covering content, speaker, and other different aspects of speech. Additionally, the HEAR benchmark [9] evaluates audio representations on diverse domains beyond speech, such as music and environmental sounds. For video representations, the SEVERE-benchmark [10] evaluates video self-supervised learning models on a diverse set of datasets to measure model sensitivity to different properties of downstream tasks. Feichtenhofer et al. [27] extend 4 image self-supervised learning methods to video representations and compare their effi- cacy on several downstream datasets, while Kumar et al. [11] focus on the effects of different factors in self-supervised video pretrain- ing. However, these works focus on individual domains, and cannot make use of the relationship between paired audio/visual inputs. Previous multi-task multimodal benchmarks either focus on ego- centric videos [28],"}], "doc_text": "3 2 0 2 p e S 9 1 ] S A . s s e e [ 1 v 7 8 7 0 1 . 9 0 3 2 : v i X r a AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS Yuan Tseng1, Layne Berry2\u2217, Yi-Ting Chen3\u2217, I-Hsiang Chiu1\u2217, Hsuan-Hao Lin1\u2217, Max Liu1\u2217, Puyuan Peng2\u2217, Yi-Jen Shih1\u2217, Hung-Yu Wang1\u2217, Haibin Wu1\u2217, Po-Yao Huang4, Chun-Mao Lai1, Shang-Wen Li4, David Harwath2, Yu Tsao3, Shinji Watanabe5, Abdelrahman Mohamed6, Chi-Luen Feng1, Hung-yi Lee1 1 National Taiwan University, Taiwan 2 University of Texas at Austin, USA 3 Academia Sinica, Taiwan 4 Meta AI 5 Carnegie Mellon University, USA 6 Rembrand r11942082@ntu.edu.tw ABSTRACT Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned repre- sentations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal au- dio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong in- termediate task. We release our benchmark with evaluation code1 and a model submission platform2 to encourage further research in audio-visual learning. human perception, which synergistically integrates auditory and vi- sual cues [12, 13]. While audio-visual representation learning has made significant progress [14, 15, 16, 17, 18], the assessment of these models tends to be task-specific, leaving the broader gener- alization capabilities across various audio-visual challenges less un- derstood. This complicates comparitive analysis of different models and training strategies, impeding the development of more robust and versatile audio-visual representation learning approaches. To address this issue, we propose AV-SUPERB, a standardized benchmark for comprehensively evaluating representations across seven distinct datasets involving five speech and audio processing tasks. AV-SUPERB comprises of three tracks to assess audio, video, and audio-visual fusion representations. We envision that these dis- tinct tracks will allow researchers in speech, audio, and video repre- sentation learning alike to compare learning strategies across models and modalities, enabling broader analysis of their effectiveness. Index Terms\u2014 Audio-Visual Learning, Representation Learn- ing, Evaluation, Self-Supervised Learning 1. INTRODUCTION Emulating the seamless integration of multiple tasks in human cognition, such as spoken language comprehension, sound event detection, and visual object recognition has been a long-standing goal of computational research. Prior research demonstrates that the pretrain-then-finetune paradigm is an effective and scalable method of building multitasking algorithmic systems for speech [1, 2], au- dio [3, 4], and vision [5, 6]. In the pretraining stage, models can often learn meaningful representations from unlabelled data alone through optimization of contrastive, masked prediction, or other self-supervised loss functions. These pretrained representations can then be applied to diverse tasks just by fine-tuning minimal additional parameters. In order to better measure progress in representation learning, previous works have established multitask benchmarks in speech [7, 8], audio [9], and vision [10, 11]. However, these benchmark pre- dominantly evaluate performance in isolation within single modal- ities. This approach overlooks the inherent multimodal nature of \u2217 Equal contribution; sorted alphabetically 1https://github.com/roger-tseng/av-superb 2https://av.superbbenchmark.org Our contributions are four-fold: (1) Diverse-domain evalua- tion: We propose the first audio-visual learning benchmark that en- compasses multiple datasets and tasks, covering both speech and au- dio domains. (2) Easy and reproducible benchmarking: We re- lease evaluation code and a dedicated model submission platform that ensures reproducible evaluation on dynamic Youtube datasets (3) Intermediate-task and reduces computational entry barriers. fine-tuning: Our work emphasizes the potential benefits of full fine- tuning on intermediate tasks for improving performance on out-of- domain downstream tasks. (4) Layer-wise analysis: We show that different layers contribute variably to task performance, suggesting that simply using representations of the final layer is suboptimal, motivating the weighted-sum evaluation approach. 2. RELATED WORK Recognizing how the close relation between audition and vision fa- cilitates multimodal human perception, many audio-visual datasets have been gathered for action recognition [19, 15, 20, 21], speech recognition [22, 23, 24], speaker recognition [25, 26], and a vari- ety of other tasks to study audio-visual learning. However, most models are trained and evaluated on different datasets with different experiment settings, which increases comparison difficulty and ob- fuscates the broad applicability of proposed methods. Hence in the AV-SUPERB benchmark, we select a diverse set of datasets from multiple tasks to comprehensively compare works in audio-visual representation learning. Previous multitask benchmarks in speech [7, 8], audio [9], and Fig. 1. We consider three evaluation scenarios: extracting fea- tures using inputs from one or both modalities. Following [7], the weighted-sum of features from Transformer layers (if applicable) are used as input for fine-tuning a small downstream model for each in- dividual task. Details of selected tasks are given in Section 3.1. video representation learning [27, 10, 11] allow for fairer com- parison of different models and promote research towards general approaches that are applicable to a variety of real-world tasks. SUPERB [7] and SUPERB-SG [8] evaluate speech representation models on a wide range of downstream tasks covering content, speaker, and other different aspects of speech. Additionally, the HEAR benchmark [9] evaluates audio representations on diverse domains beyond speech, such as music and environmental sounds. For video representations, the SEVERE-benchmark [10] evaluates video self-supervised learning models on a diverse set of datasets to measure model sensitivity to different properties of downstream tasks. Feichtenhofer et al. [27] extend 4 image self-supervised learning methods to video representations and compare their effi- cacy on several downstream datasets, while Kumar et al. [11] focus on the effects of different factors in self-supervised video pretrain- ing. However, these works focus on individual domains, and cannot make use of the relationship between paired audio/visual inputs. Previous multi-task multimodal benchmarks either focus on ego- centric videos [28],"}