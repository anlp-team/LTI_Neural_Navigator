{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_AudioGPT:_Understanding_and_Generating_Speech,_Music,_Sound,_and_Talking_Head_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is passed into the prompt manager M to generate argument an?", "answer": " The selected audio foundation model Pp and its corresponding task-related arguments hPp", "ref_chunk": "is passed into the prompt manager M to generate argument an, including the selected audio foundation model Pp and its corresponding task-related arguments hPp , where p is the index of the selected audio model from the audio model set {Pi}P n , ..., q(sk) } from q(cid:48) n n i=1. (Pp, hPp ) = L(M(H(q(cid:48) n), q(cid:48)(d) n ), C), where H(q(cid:48) n) is the task family selected by the task handler H. Noted that, for an audio/image-input task family, hPp may also contain the necessary resources (e.g., audio or images) from the previous context C. As aforementioned, the task family is determined through the task handler H by considering the I/O modality. To be speci\ufb01c, the families are: Audio-to-Text \u2013 Speech Recognition: Transcribe human speech 4 (1) (2) (3) (4) Table 1: Supported Tasks in AudioGPT Task Input Output Domain Model Speech Recognition Speech Translation Audio Audio Text Text Speech Speech Whisper (Radford et al., 2022) MultiDecoder (Dalmia et al., 2021) Style Transfer Speech Enhancement Speech Separation Mono-to-Binaural Audio Inpainting Sound Extraction Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Speech Speech Speech Speech Sound Sound GenerSpeech (Huang et al., 2022b) ConvTasNet (Luo & Mesgarani, 2019) TF-GridNet (Wang et al., 2022) NeuralWarp (Grabocka et al., 2018) Make-An-Audio (Huang et al., 2023a) LASSNet (Liu et al., 2022b) Sound Detection Audio Event Sound Pyramid Transformer (Xin et al., 2022) Talking Head Synthesis Audio Video Talking Head GeneFace (Ye et al., 2023) Text-to-Speech Text-to-Audio Text Text Audio Audio Speech Sound FastSpeech 2 (Ren et al., 2020) Make-An-Audio (Huang et al., 2023a) Audio-to-Text Audio Text Sound MAAC (Ye et al., 2021) Image-to-Audio Image Audio Sound Make-An-Audio (Huang et al., 2023a) Singing Synthesis Musical Score Audio Music DiffSinger (Liu et al., 2022a) VISinger (Zhang et al., 2022b) \u2013 Speech Translation: Translate human speech into another language \u2013 Audio Caption: Describe audio in text Audio-to-Audio \u2013 Style Transfer: Generate human speech with styles derived from a reference \u2013 Speech Enhancement: Improve the speech quality by reducing background noise \u2013 Speech Separation: Separate mix-speech of different speakers \u2013 Mono-to-Binaural: Generate binaural audio given mono one \u2013 Audio Impainting: Inpaint audio given user input mask Audio-to-Event \u2013 Sound Extraction: Selectly extract a part of audio based on description \u2013 Sound Detection: Predict the event timelines in audio Audio-to-Video \u2013 Talking Head Synthesis: Generate a talking human portrait video given input audio Text-to-Audio \u2013 Text-to-Speech: Generate human speech given user input text \u2013 Text-to-Audio: Generate general audio given user description Image-to-Audio \u2013 Image-to-Audio: Generate audio from image Score-to-Audio \u2013 Singing Synthesis: Generate singing voice given input text, note and duration Sequence 3.4 Model Assignment Given the selected model Pp and its corresponding arguments hPp , this step assigns the related resources to the model and executes the model Pp to get the task output oPp : oPp = Pp({q(s1) n , q(s2) n , ..., q(sk) n }, hPp ). To keep the ef\ufb01ciency of AudioGPT, we conduct the audio model initialization during either environ- mental setups or server initialization. 5 (5) 3.5 Response Generation The response generation is highly related to the select task Pp and its output oPp . Speci\ufb01cally, for audio generation tasks, AudioGPT shows both the waveform in an image and the corresponding audio \ufb01le for downloading/playing; for tasks that generate text, the model directly returns the transcribed text; for the video generation task, the output video and some related image frames are shown; for classi\ufb01cation tasks, a posteriorgram of categories is shown over the time span. 4 Evaluating Multi-Modal LLMs 4.1 Overview The rapid development of multi-modal LLMs (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b) has signi\ufb01cantly increased the research demand for evaluating its performance and behavior in understanding human intention, performing complex reasoning, and organizing the cooperation of multiple audio foundation models. In this section, we outline the design principles and process of evaluating multi-modal LLMs (i.e., AudioGPT). Speci\ufb01cally, we evaluate the LLMs in the following three aspects: 1) Consistency, which measures whether the LLMs properly understand the intention of a user, and assigns the audio foundation models closely aligned with human cognition and problem-solving; 2) Capabilitity, which measures the performance of audio foundation models in handling complex audio tasks, understanding and generating speech, music, sound, and talking head in a zero-shot fashion; and 3) Robustness, which measures the ability of LLMs deals with special cases. 4.2 Consistency Figure 2: A high-level overview of consistency evaluation. Audio ModelsRespond Does the response align with human intention faithfully? AudioGPT\u2022Can you synthesize a voice audio with the text content?\u2022Please convert written text into natural-sounding speech.\u2022Convert text into natural-sounding speech.\u2022Transform your written words into spoken audio.\u2022Bring your written content to life with speech synthesis. {\u201cPlease generate a voice from text\u201d, Text to Speech} LLMs Table 2: Ratings that have been used in the evaluation of query-answer consistency. Rating Consistency De\ufb01nition 20 40 60 80 100 Completely inconsistent Very annoying and objectionable inconsistency. Annoying but not objectionable inconsistency. Perceptible and slightly annoying inconsistency Just perceptible but not annoying inconsistency. Imperceptible inconsistency Mostly inconsistent somewhat consistent Mostly consistent Completely consistent In the consistency evaluation for the zero-shot setting, models are directly evaluated on the questions without being provided any prior examples of the speci\ufb01c tasks, which evaluate whether multi-modal LLMS could reason and solve problems without explicit training. More speci\ufb01cally, as shown in Figure 2, the consistency evaluation is carried out in three steps for each task in the benchmark. In the \ufb01rst step, we request human annotators to provide prompts for each task in a format of {prompts, task_name}. This allows us to evaluate the model\u2019s ability to comprehend complex tasks and identify the essential prompts needed for successful task assignments. In the second step, we leverage the outperformed language generation capacity of LLMs to produce descriptions with the same semantic meanings while having different expressions, enabling a comprehensive 6 evaluation of whether LLMs understands the intention of a broader amount of user. Finally, we use crowd-sourced"}, {"question": " How is the task family determined for an audio/image-input task?", "answer": " Through the task handler H by considering the I/O modality", "ref_chunk": "is passed into the prompt manager M to generate argument an, including the selected audio foundation model Pp and its corresponding task-related arguments hPp , where p is the index of the selected audio model from the audio model set {Pi}P n , ..., q(sk) } from q(cid:48) n n i=1. (Pp, hPp ) = L(M(H(q(cid:48) n), q(cid:48)(d) n ), C), where H(q(cid:48) n) is the task family selected by the task handler H. Noted that, for an audio/image-input task family, hPp may also contain the necessary resources (e.g., audio or images) from the previous context C. As aforementioned, the task family is determined through the task handler H by considering the I/O modality. To be speci\ufb01c, the families are: Audio-to-Text \u2013 Speech Recognition: Transcribe human speech 4 (1) (2) (3) (4) Table 1: Supported Tasks in AudioGPT Task Input Output Domain Model Speech Recognition Speech Translation Audio Audio Text Text Speech Speech Whisper (Radford et al., 2022) MultiDecoder (Dalmia et al., 2021) Style Transfer Speech Enhancement Speech Separation Mono-to-Binaural Audio Inpainting Sound Extraction Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Speech Speech Speech Speech Sound Sound GenerSpeech (Huang et al., 2022b) ConvTasNet (Luo & Mesgarani, 2019) TF-GridNet (Wang et al., 2022) NeuralWarp (Grabocka et al., 2018) Make-An-Audio (Huang et al., 2023a) LASSNet (Liu et al., 2022b) Sound Detection Audio Event Sound Pyramid Transformer (Xin et al., 2022) Talking Head Synthesis Audio Video Talking Head GeneFace (Ye et al., 2023) Text-to-Speech Text-to-Audio Text Text Audio Audio Speech Sound FastSpeech 2 (Ren et al., 2020) Make-An-Audio (Huang et al., 2023a) Audio-to-Text Audio Text Sound MAAC (Ye et al., 2021) Image-to-Audio Image Audio Sound Make-An-Audio (Huang et al., 2023a) Singing Synthesis Musical Score Audio Music DiffSinger (Liu et al., 2022a) VISinger (Zhang et al., 2022b) \u2013 Speech Translation: Translate human speech into another language \u2013 Audio Caption: Describe audio in text Audio-to-Audio \u2013 Style Transfer: Generate human speech with styles derived from a reference \u2013 Speech Enhancement: Improve the speech quality by reducing background noise \u2013 Speech Separation: Separate mix-speech of different speakers \u2013 Mono-to-Binaural: Generate binaural audio given mono one \u2013 Audio Impainting: Inpaint audio given user input mask Audio-to-Event \u2013 Sound Extraction: Selectly extract a part of audio based on description \u2013 Sound Detection: Predict the event timelines in audio Audio-to-Video \u2013 Talking Head Synthesis: Generate a talking human portrait video given input audio Text-to-Audio \u2013 Text-to-Speech: Generate human speech given user input text \u2013 Text-to-Audio: Generate general audio given user description Image-to-Audio \u2013 Image-to-Audio: Generate audio from image Score-to-Audio \u2013 Singing Synthesis: Generate singing voice given input text, note and duration Sequence 3.4 Model Assignment Given the selected model Pp and its corresponding arguments hPp , this step assigns the related resources to the model and executes the model Pp to get the task output oPp : oPp = Pp({q(s1) n , q(s2) n , ..., q(sk) n }, hPp ). To keep the ef\ufb01ciency of AudioGPT, we conduct the audio model initialization during either environ- mental setups or server initialization. 5 (5) 3.5 Response Generation The response generation is highly related to the select task Pp and its output oPp . Speci\ufb01cally, for audio generation tasks, AudioGPT shows both the waveform in an image and the corresponding audio \ufb01le for downloading/playing; for tasks that generate text, the model directly returns the transcribed text; for the video generation task, the output video and some related image frames are shown; for classi\ufb01cation tasks, a posteriorgram of categories is shown over the time span. 4 Evaluating Multi-Modal LLMs 4.1 Overview The rapid development of multi-modal LLMs (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b) has signi\ufb01cantly increased the research demand for evaluating its performance and behavior in understanding human intention, performing complex reasoning, and organizing the cooperation of multiple audio foundation models. In this section, we outline the design principles and process of evaluating multi-modal LLMs (i.e., AudioGPT). Speci\ufb01cally, we evaluate the LLMs in the following three aspects: 1) Consistency, which measures whether the LLMs properly understand the intention of a user, and assigns the audio foundation models closely aligned with human cognition and problem-solving; 2) Capabilitity, which measures the performance of audio foundation models in handling complex audio tasks, understanding and generating speech, music, sound, and talking head in a zero-shot fashion; and 3) Robustness, which measures the ability of LLMs deals with special cases. 4.2 Consistency Figure 2: A high-level overview of consistency evaluation. Audio ModelsRespond Does the response align with human intention faithfully? AudioGPT\u2022Can you synthesize a voice audio with the text content?\u2022Please convert written text into natural-sounding speech.\u2022Convert text into natural-sounding speech.\u2022Transform your written words into spoken audio.\u2022Bring your written content to life with speech synthesis. {\u201cPlease generate a voice from text\u201d, Text to Speech} LLMs Table 2: Ratings that have been used in the evaluation of query-answer consistency. Rating Consistency De\ufb01nition 20 40 60 80 100 Completely inconsistent Very annoying and objectionable inconsistency. Annoying but not objectionable inconsistency. Perceptible and slightly annoying inconsistency Just perceptible but not annoying inconsistency. Imperceptible inconsistency Mostly inconsistent somewhat consistent Mostly consistent Completely consistent In the consistency evaluation for the zero-shot setting, models are directly evaluated on the questions without being provided any prior examples of the speci\ufb01c tasks, which evaluate whether multi-modal LLMS could reason and solve problems without explicit training. More speci\ufb01cally, as shown in Figure 2, the consistency evaluation is carried out in three steps for each task in the benchmark. In the \ufb01rst step, we request human annotators to provide prompts for each task in a format of {prompts, task_name}. This allows us to evaluate the model\u2019s ability to comprehend complex tasks and identify the essential prompts needed for successful task assignments. In the second step, we leverage the outperformed language generation capacity of LLMs to produce descriptions with the same semantic meanings while having different expressions, enabling a comprehensive 6 evaluation of whether LLMs understands the intention of a broader amount of user. Finally, we use crowd-sourced"}, {"question": " List three supported tasks in AudioGPT according to Table 1.", "answer": " Speech Recognition, Speech Translation, Audio Caption", "ref_chunk": "is passed into the prompt manager M to generate argument an, including the selected audio foundation model Pp and its corresponding task-related arguments hPp , where p is the index of the selected audio model from the audio model set {Pi}P n , ..., q(sk) } from q(cid:48) n n i=1. (Pp, hPp ) = L(M(H(q(cid:48) n), q(cid:48)(d) n ), C), where H(q(cid:48) n) is the task family selected by the task handler H. Noted that, for an audio/image-input task family, hPp may also contain the necessary resources (e.g., audio or images) from the previous context C. As aforementioned, the task family is determined through the task handler H by considering the I/O modality. To be speci\ufb01c, the families are: Audio-to-Text \u2013 Speech Recognition: Transcribe human speech 4 (1) (2) (3) (4) Table 1: Supported Tasks in AudioGPT Task Input Output Domain Model Speech Recognition Speech Translation Audio Audio Text Text Speech Speech Whisper (Radford et al., 2022) MultiDecoder (Dalmia et al., 2021) Style Transfer Speech Enhancement Speech Separation Mono-to-Binaural Audio Inpainting Sound Extraction Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Speech Speech Speech Speech Sound Sound GenerSpeech (Huang et al., 2022b) ConvTasNet (Luo & Mesgarani, 2019) TF-GridNet (Wang et al., 2022) NeuralWarp (Grabocka et al., 2018) Make-An-Audio (Huang et al., 2023a) LASSNet (Liu et al., 2022b) Sound Detection Audio Event Sound Pyramid Transformer (Xin et al., 2022) Talking Head Synthesis Audio Video Talking Head GeneFace (Ye et al., 2023) Text-to-Speech Text-to-Audio Text Text Audio Audio Speech Sound FastSpeech 2 (Ren et al., 2020) Make-An-Audio (Huang et al., 2023a) Audio-to-Text Audio Text Sound MAAC (Ye et al., 2021) Image-to-Audio Image Audio Sound Make-An-Audio (Huang et al., 2023a) Singing Synthesis Musical Score Audio Music DiffSinger (Liu et al., 2022a) VISinger (Zhang et al., 2022b) \u2013 Speech Translation: Translate human speech into another language \u2013 Audio Caption: Describe audio in text Audio-to-Audio \u2013 Style Transfer: Generate human speech with styles derived from a reference \u2013 Speech Enhancement: Improve the speech quality by reducing background noise \u2013 Speech Separation: Separate mix-speech of different speakers \u2013 Mono-to-Binaural: Generate binaural audio given mono one \u2013 Audio Impainting: Inpaint audio given user input mask Audio-to-Event \u2013 Sound Extraction: Selectly extract a part of audio based on description \u2013 Sound Detection: Predict the event timelines in audio Audio-to-Video \u2013 Talking Head Synthesis: Generate a talking human portrait video given input audio Text-to-Audio \u2013 Text-to-Speech: Generate human speech given user input text \u2013 Text-to-Audio: Generate general audio given user description Image-to-Audio \u2013 Image-to-Audio: Generate audio from image Score-to-Audio \u2013 Singing Synthesis: Generate singing voice given input text, note and duration Sequence 3.4 Model Assignment Given the selected model Pp and its corresponding arguments hPp , this step assigns the related resources to the model and executes the model Pp to get the task output oPp : oPp = Pp({q(s1) n , q(s2) n , ..., q(sk) n }, hPp ). To keep the ef\ufb01ciency of AudioGPT, we conduct the audio model initialization during either environ- mental setups or server initialization. 5 (5) 3.5 Response Generation The response generation is highly related to the select task Pp and its output oPp . Speci\ufb01cally, for audio generation tasks, AudioGPT shows both the waveform in an image and the corresponding audio \ufb01le for downloading/playing; for tasks that generate text, the model directly returns the transcribed text; for the video generation task, the output video and some related image frames are shown; for classi\ufb01cation tasks, a posteriorgram of categories is shown over the time span. 4 Evaluating Multi-Modal LLMs 4.1 Overview The rapid development of multi-modal LLMs (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b) has signi\ufb01cantly increased the research demand for evaluating its performance and behavior in understanding human intention, performing complex reasoning, and organizing the cooperation of multiple audio foundation models. In this section, we outline the design principles and process of evaluating multi-modal LLMs (i.e., AudioGPT). Speci\ufb01cally, we evaluate the LLMs in the following three aspects: 1) Consistency, which measures whether the LLMs properly understand the intention of a user, and assigns the audio foundation models closely aligned with human cognition and problem-solving; 2) Capabilitity, which measures the performance of audio foundation models in handling complex audio tasks, understanding and generating speech, music, sound, and talking head in a zero-shot fashion; and 3) Robustness, which measures the ability of LLMs deals with special cases. 4.2 Consistency Figure 2: A high-level overview of consistency evaluation. Audio ModelsRespond Does the response align with human intention faithfully? AudioGPT\u2022Can you synthesize a voice audio with the text content?\u2022Please convert written text into natural-sounding speech.\u2022Convert text into natural-sounding speech.\u2022Transform your written words into spoken audio.\u2022Bring your written content to life with speech synthesis. {\u201cPlease generate a voice from text\u201d, Text to Speech} LLMs Table 2: Ratings that have been used in the evaluation of query-answer consistency. Rating Consistency De\ufb01nition 20 40 60 80 100 Completely inconsistent Very annoying and objectionable inconsistency. Annoying but not objectionable inconsistency. Perceptible and slightly annoying inconsistency Just perceptible but not annoying inconsistency. Imperceptible inconsistency Mostly inconsistent somewhat consistent Mostly consistent Completely consistent In the consistency evaluation for the zero-shot setting, models are directly evaluated on the questions without being provided any prior examples of the speci\ufb01c tasks, which evaluate whether multi-modal LLMS could reason and solve problems without explicit training. More speci\ufb01cally, as shown in Figure 2, the consistency evaluation is carried out in three steps for each task in the benchmark. In the \ufb01rst step, we request human annotators to provide prompts for each task in a format of {prompts, task_name}. This allows us to evaluate the model\u2019s ability to comprehend complex tasks and identify the essential prompts needed for successful task assignments. In the second step, we leverage the outperformed language generation capacity of LLMs to produce descriptions with the same semantic meanings while having different expressions, enabling a comprehensive 6 evaluation of whether LLMs understands the intention of a broader amount of user. Finally, we use crowd-sourced"}, {"question": " What does the model assignment step do?", "answer": " Assigns the related resources to the model and executes the model to get the task output", "ref_chunk": "is passed into the prompt manager M to generate argument an, including the selected audio foundation model Pp and its corresponding task-related arguments hPp , where p is the index of the selected audio model from the audio model set {Pi}P n , ..., q(sk) } from q(cid:48) n n i=1. (Pp, hPp ) = L(M(H(q(cid:48) n), q(cid:48)(d) n ), C), where H(q(cid:48) n) is the task family selected by the task handler H. Noted that, for an audio/image-input task family, hPp may also contain the necessary resources (e.g., audio or images) from the previous context C. As aforementioned, the task family is determined through the task handler H by considering the I/O modality. To be speci\ufb01c, the families are: Audio-to-Text \u2013 Speech Recognition: Transcribe human speech 4 (1) (2) (3) (4) Table 1: Supported Tasks in AudioGPT Task Input Output Domain Model Speech Recognition Speech Translation Audio Audio Text Text Speech Speech Whisper (Radford et al., 2022) MultiDecoder (Dalmia et al., 2021) Style Transfer Speech Enhancement Speech Separation Mono-to-Binaural Audio Inpainting Sound Extraction Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Speech Speech Speech Speech Sound Sound GenerSpeech (Huang et al., 2022b) ConvTasNet (Luo & Mesgarani, 2019) TF-GridNet (Wang et al., 2022) NeuralWarp (Grabocka et al., 2018) Make-An-Audio (Huang et al., 2023a) LASSNet (Liu et al., 2022b) Sound Detection Audio Event Sound Pyramid Transformer (Xin et al., 2022) Talking Head Synthesis Audio Video Talking Head GeneFace (Ye et al., 2023) Text-to-Speech Text-to-Audio Text Text Audio Audio Speech Sound FastSpeech 2 (Ren et al., 2020) Make-An-Audio (Huang et al., 2023a) Audio-to-Text Audio Text Sound MAAC (Ye et al., 2021) Image-to-Audio Image Audio Sound Make-An-Audio (Huang et al., 2023a) Singing Synthesis Musical Score Audio Music DiffSinger (Liu et al., 2022a) VISinger (Zhang et al., 2022b) \u2013 Speech Translation: Translate human speech into another language \u2013 Audio Caption: Describe audio in text Audio-to-Audio \u2013 Style Transfer: Generate human speech with styles derived from a reference \u2013 Speech Enhancement: Improve the speech quality by reducing background noise \u2013 Speech Separation: Separate mix-speech of different speakers \u2013 Mono-to-Binaural: Generate binaural audio given mono one \u2013 Audio Impainting: Inpaint audio given user input mask Audio-to-Event \u2013 Sound Extraction: Selectly extract a part of audio based on description \u2013 Sound Detection: Predict the event timelines in audio Audio-to-Video \u2013 Talking Head Synthesis: Generate a talking human portrait video given input audio Text-to-Audio \u2013 Text-to-Speech: Generate human speech given user input text \u2013 Text-to-Audio: Generate general audio given user description Image-to-Audio \u2013 Image-to-Audio: Generate audio from image Score-to-Audio \u2013 Singing Synthesis: Generate singing voice given input text, note and duration Sequence 3.4 Model Assignment Given the selected model Pp and its corresponding arguments hPp , this step assigns the related resources to the model and executes the model Pp to get the task output oPp : oPp = Pp({q(s1) n , q(s2) n , ..., q(sk) n }, hPp ). To keep the ef\ufb01ciency of AudioGPT, we conduct the audio model initialization during either environ- mental setups or server initialization. 5 (5) 3.5 Response Generation The response generation is highly related to the select task Pp and its output oPp . Speci\ufb01cally, for audio generation tasks, AudioGPT shows both the waveform in an image and the corresponding audio \ufb01le for downloading/playing; for tasks that generate text, the model directly returns the transcribed text; for the video generation task, the output video and some related image frames are shown; for classi\ufb01cation tasks, a posteriorgram of categories is shown over the time span. 4 Evaluating Multi-Modal LLMs 4.1 Overview The rapid development of multi-modal LLMs (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b) has signi\ufb01cantly increased the research demand for evaluating its performance and behavior in understanding human intention, performing complex reasoning, and organizing the cooperation of multiple audio foundation models. In this section, we outline the design principles and process of evaluating multi-modal LLMs (i.e., AudioGPT). Speci\ufb01cally, we evaluate the LLMs in the following three aspects: 1) Consistency, which measures whether the LLMs properly understand the intention of a user, and assigns the audio foundation models closely aligned with human cognition and problem-solving; 2) Capabilitity, which measures the performance of audio foundation models in handling complex audio tasks, understanding and generating speech, music, sound, and talking head in a zero-shot fashion; and 3) Robustness, which measures the ability of LLMs deals with special cases. 4.2 Consistency Figure 2: A high-level overview of consistency evaluation. Audio ModelsRespond Does the response align with human intention faithfully? AudioGPT\u2022Can you synthesize a voice audio with the text content?\u2022Please convert written text into natural-sounding speech.\u2022Convert text into natural-sounding speech.\u2022Transform your written words into spoken audio.\u2022Bring your written content to life with speech synthesis. {\u201cPlease generate a voice from text\u201d, Text to Speech} LLMs Table 2: Ratings that have been used in the evaluation of query-answer consistency. Rating Consistency De\ufb01nition 20 40 60 80 100 Completely inconsistent Very annoying and objectionable inconsistency. Annoying but not objectionable inconsistency. Perceptible and slightly annoying inconsistency Just perceptible but not annoying inconsistency. Imperceptible inconsistency Mostly inconsistent somewhat consistent Mostly consistent Completely consistent In the consistency evaluation for the zero-shot setting, models are directly evaluated on the questions without being provided any prior examples of the speci\ufb01c tasks, which evaluate whether multi-modal LLMS could reason and solve problems without explicit training. More speci\ufb01cally, as shown in Figure 2, the consistency evaluation is carried out in three steps for each task in the benchmark. In the \ufb01rst step, we request human annotators to provide prompts for each task in a format of {prompts, task_name}. This allows us to evaluate the model\u2019s ability to comprehend complex tasks and identify the essential prompts needed for successful task assignments. In the second step, we leverage the outperformed language generation capacity of LLMs to produce descriptions with the same semantic meanings while having different expressions, enabling a comprehensive 6 evaluation of whether LLMs understands the intention of a broader amount of user. Finally, we use crowd-sourced"}, {"question": " What is the response generation highly related to?", "answer": " The selected task Pp and its output oPp", "ref_chunk": "is passed into the prompt manager M to generate argument an, including the selected audio foundation model Pp and its corresponding task-related arguments hPp , where p is the index of the selected audio model from the audio model set {Pi}P n , ..., q(sk) } from q(cid:48) n n i=1. (Pp, hPp ) = L(M(H(q(cid:48) n), q(cid:48)(d) n ), C), where H(q(cid:48) n) is the task family selected by the task handler H. Noted that, for an audio/image-input task family, hPp may also contain the necessary resources (e.g., audio or images) from the previous context C. As aforementioned, the task family is determined through the task handler H by considering the I/O modality. To be speci\ufb01c, the families are: Audio-to-Text \u2013 Speech Recognition: Transcribe human speech 4 (1) (2) (3) (4) Table 1: Supported Tasks in AudioGPT Task Input Output Domain Model Speech Recognition Speech Translation Audio Audio Text Text Speech Speech Whisper (Radford et al., 2022) MultiDecoder (Dalmia et al., 2021) Style Transfer Speech Enhancement Speech Separation Mono-to-Binaural Audio Inpainting Sound Extraction Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Speech Speech Speech Speech Sound Sound GenerSpeech (Huang et al., 2022b) ConvTasNet (Luo & Mesgarani, 2019) TF-GridNet (Wang et al., 2022) NeuralWarp (Grabocka et al., 2018) Make-An-Audio (Huang et al., 2023a) LASSNet (Liu et al., 2022b) Sound Detection Audio Event Sound Pyramid Transformer (Xin et al., 2022) Talking Head Synthesis Audio Video Talking Head GeneFace (Ye et al., 2023) Text-to-Speech Text-to-Audio Text Text Audio Audio Speech Sound FastSpeech 2 (Ren et al., 2020) Make-An-Audio (Huang et al., 2023a) Audio-to-Text Audio Text Sound MAAC (Ye et al., 2021) Image-to-Audio Image Audio Sound Make-An-Audio (Huang et al., 2023a) Singing Synthesis Musical Score Audio Music DiffSinger (Liu et al., 2022a) VISinger (Zhang et al., 2022b) \u2013 Speech Translation: Translate human speech into another language \u2013 Audio Caption: Describe audio in text Audio-to-Audio \u2013 Style Transfer: Generate human speech with styles derived from a reference \u2013 Speech Enhancement: Improve the speech quality by reducing background noise \u2013 Speech Separation: Separate mix-speech of different speakers \u2013 Mono-to-Binaural: Generate binaural audio given mono one \u2013 Audio Impainting: Inpaint audio given user input mask Audio-to-Event \u2013 Sound Extraction: Selectly extract a part of audio based on description \u2013 Sound Detection: Predict the event timelines in audio Audio-to-Video \u2013 Talking Head Synthesis: Generate a talking human portrait video given input audio Text-to-Audio \u2013 Text-to-Speech: Generate human speech given user input text \u2013 Text-to-Audio: Generate general audio given user description Image-to-Audio \u2013 Image-to-Audio: Generate audio from image Score-to-Audio \u2013 Singing Synthesis: Generate singing voice given input text, note and duration Sequence 3.4 Model Assignment Given the selected model Pp and its corresponding arguments hPp , this step assigns the related resources to the model and executes the model Pp to get the task output oPp : oPp = Pp({q(s1) n , q(s2) n , ..., q(sk) n }, hPp ). To keep the ef\ufb01ciency of AudioGPT, we conduct the audio model initialization during either environ- mental setups or server initialization. 5 (5) 3.5 Response Generation The response generation is highly related to the select task Pp and its output oPp . Speci\ufb01cally, for audio generation tasks, AudioGPT shows both the waveform in an image and the corresponding audio \ufb01le for downloading/playing; for tasks that generate text, the model directly returns the transcribed text; for the video generation task, the output video and some related image frames are shown; for classi\ufb01cation tasks, a posteriorgram of categories is shown over the time span. 4 Evaluating Multi-Modal LLMs 4.1 Overview The rapid development of multi-modal LLMs (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b) has signi\ufb01cantly increased the research demand for evaluating its performance and behavior in understanding human intention, performing complex reasoning, and organizing the cooperation of multiple audio foundation models. In this section, we outline the design principles and process of evaluating multi-modal LLMs (i.e., AudioGPT). Speci\ufb01cally, we evaluate the LLMs in the following three aspects: 1) Consistency, which measures whether the LLMs properly understand the intention of a user, and assigns the audio foundation models closely aligned with human cognition and problem-solving; 2) Capabilitity, which measures the performance of audio foundation models in handling complex audio tasks, understanding and generating speech, music, sound, and talking head in a zero-shot fashion; and 3) Robustness, which measures the ability of LLMs deals with special cases. 4.2 Consistency Figure 2: A high-level overview of consistency evaluation. Audio ModelsRespond Does the response align with human intention faithfully? AudioGPT\u2022Can you synthesize a voice audio with the text content?\u2022Please convert written text into natural-sounding speech.\u2022Convert text into natural-sounding speech.\u2022Transform your written words into spoken audio.\u2022Bring your written content to life with speech synthesis. {\u201cPlease generate a voice from text\u201d, Text to Speech} LLMs Table 2: Ratings that have been used in the evaluation of query-answer consistency. Rating Consistency De\ufb01nition 20 40 60 80 100 Completely inconsistent Very annoying and objectionable inconsistency. Annoying but not objectionable inconsistency. Perceptible and slightly annoying inconsistency Just perceptible but not annoying inconsistency. Imperceptible inconsistency Mostly inconsistent somewhat consistent Mostly consistent Completely consistent In the consistency evaluation for the zero-shot setting, models are directly evaluated on the questions without being provided any prior examples of the speci\ufb01c tasks, which evaluate whether multi-modal LLMS could reason and solve problems without explicit training. More speci\ufb01cally, as shown in Figure 2, the consistency evaluation is carried out in three steps for each task in the benchmark. In the \ufb01rst step, we request human annotators to provide prompts for each task in a format of {prompts, task_name}. This allows us to evaluate the model\u2019s ability to comprehend complex tasks and identify the essential prompts needed for successful task assignments. In the second step, we leverage the outperformed language generation capacity of LLMs to produce descriptions with the same semantic meanings while having different expressions, enabling a comprehensive 6 evaluation of whether LLMs understands the intention of a broader amount of user. Finally, we use crowd-sourced"}, {"question": " Name three aspects in which multi-modal LLMs are evaluated.", "answer": " Consistency, Capability, Robustness", "ref_chunk": "is passed into the prompt manager M to generate argument an, including the selected audio foundation model Pp and its corresponding task-related arguments hPp , where p is the index of the selected audio model from the audio model set {Pi}P n , ..., q(sk) } from q(cid:48) n n i=1. (Pp, hPp ) = L(M(H(q(cid:48) n), q(cid:48)(d) n ), C), where H(q(cid:48) n) is the task family selected by the task handler H. Noted that, for an audio/image-input task family, hPp may also contain the necessary resources (e.g., audio or images) from the previous context C. As aforementioned, the task family is determined through the task handler H by considering the I/O modality. To be speci\ufb01c, the families are: Audio-to-Text \u2013 Speech Recognition: Transcribe human speech 4 (1) (2) (3) (4) Table 1: Supported Tasks in AudioGPT Task Input Output Domain Model Speech Recognition Speech Translation Audio Audio Text Text Speech Speech Whisper (Radford et al., 2022) MultiDecoder (Dalmia et al., 2021) Style Transfer Speech Enhancement Speech Separation Mono-to-Binaural Audio Inpainting Sound Extraction Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Speech Speech Speech Speech Sound Sound GenerSpeech (Huang et al., 2022b) ConvTasNet (Luo & Mesgarani, 2019) TF-GridNet (Wang et al., 2022) NeuralWarp (Grabocka et al., 2018) Make-An-Audio (Huang et al., 2023a) LASSNet (Liu et al., 2022b) Sound Detection Audio Event Sound Pyramid Transformer (Xin et al., 2022) Talking Head Synthesis Audio Video Talking Head GeneFace (Ye et al., 2023) Text-to-Speech Text-to-Audio Text Text Audio Audio Speech Sound FastSpeech 2 (Ren et al., 2020) Make-An-Audio (Huang et al., 2023a) Audio-to-Text Audio Text Sound MAAC (Ye et al., 2021) Image-to-Audio Image Audio Sound Make-An-Audio (Huang et al., 2023a) Singing Synthesis Musical Score Audio Music DiffSinger (Liu et al., 2022a) VISinger (Zhang et al., 2022b) \u2013 Speech Translation: Translate human speech into another language \u2013 Audio Caption: Describe audio in text Audio-to-Audio \u2013 Style Transfer: Generate human speech with styles derived from a reference \u2013 Speech Enhancement: Improve the speech quality by reducing background noise \u2013 Speech Separation: Separate mix-speech of different speakers \u2013 Mono-to-Binaural: Generate binaural audio given mono one \u2013 Audio Impainting: Inpaint audio given user input mask Audio-to-Event \u2013 Sound Extraction: Selectly extract a part of audio based on description \u2013 Sound Detection: Predict the event timelines in audio Audio-to-Video \u2013 Talking Head Synthesis: Generate a talking human portrait video given input audio Text-to-Audio \u2013 Text-to-Speech: Generate human speech given user input text \u2013 Text-to-Audio: Generate general audio given user description Image-to-Audio \u2013 Image-to-Audio: Generate audio from image Score-to-Audio \u2013 Singing Synthesis: Generate singing voice given input text, note and duration Sequence 3.4 Model Assignment Given the selected model Pp and its corresponding arguments hPp , this step assigns the related resources to the model and executes the model Pp to get the task output oPp : oPp = Pp({q(s1) n , q(s2) n , ..., q(sk) n }, hPp ). To keep the ef\ufb01ciency of AudioGPT, we conduct the audio model initialization during either environ- mental setups or server initialization. 5 (5) 3.5 Response Generation The response generation is highly related to the select task Pp and its output oPp . Speci\ufb01cally, for audio generation tasks, AudioGPT shows both the waveform in an image and the corresponding audio \ufb01le for downloading/playing; for tasks that generate text, the model directly returns the transcribed text; for the video generation task, the output video and some related image frames are shown; for classi\ufb01cation tasks, a posteriorgram of categories is shown over the time span. 4 Evaluating Multi-Modal LLMs 4.1 Overview The rapid development of multi-modal LLMs (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b) has signi\ufb01cantly increased the research demand for evaluating its performance and behavior in understanding human intention, performing complex reasoning, and organizing the cooperation of multiple audio foundation models. In this section, we outline the design principles and process of evaluating multi-modal LLMs (i.e., AudioGPT). Speci\ufb01cally, we evaluate the LLMs in the following three aspects: 1) Consistency, which measures whether the LLMs properly understand the intention of a user, and assigns the audio foundation models closely aligned with human cognition and problem-solving; 2) Capabilitity, which measures the performance of audio foundation models in handling complex audio tasks, understanding and generating speech, music, sound, and talking head in a zero-shot fashion; and 3) Robustness, which measures the ability of LLMs deals with special cases. 4.2 Consistency Figure 2: A high-level overview of consistency evaluation. Audio ModelsRespond Does the response align with human intention faithfully? AudioGPT\u2022Can you synthesize a voice audio with the text content?\u2022Please convert written text into natural-sounding speech.\u2022Convert text into natural-sounding speech.\u2022Transform your written words into spoken audio.\u2022Bring your written content to life with speech synthesis. {\u201cPlease generate a voice from text\u201d, Text to Speech} LLMs Table 2: Ratings that have been used in the evaluation of query-answer consistency. Rating Consistency De\ufb01nition 20 40 60 80 100 Completely inconsistent Very annoying and objectionable inconsistency. Annoying but not objectionable inconsistency. Perceptible and slightly annoying inconsistency Just perceptible but not annoying inconsistency. Imperceptible inconsistency Mostly inconsistent somewhat consistent Mostly consistent Completely consistent In the consistency evaluation for the zero-shot setting, models are directly evaluated on the questions without being provided any prior examples of the speci\ufb01c tasks, which evaluate whether multi-modal LLMS could reason and solve problems without explicit training. More speci\ufb01cally, as shown in Figure 2, the consistency evaluation is carried out in three steps for each task in the benchmark. In the \ufb01rst step, we request human annotators to provide prompts for each task in a format of {prompts, task_name}. This allows us to evaluate the model\u2019s ability to comprehend complex tasks and identify the essential prompts needed for successful task assignments. In the second step, we leverage the outperformed language generation capacity of LLMs to produce descriptions with the same semantic meanings while having different expressions, enabling a comprehensive 6 evaluation of whether LLMs understands the intention of a broader amount of user. Finally, we use crowd-sourced"}, {"question": " What does consistency measure in evaluating multi-modal LLMs?", "answer": " Whether the LLMs properly understand the intention of a user", "ref_chunk": "is passed into the prompt manager M to generate argument an, including the selected audio foundation model Pp and its corresponding task-related arguments hPp , where p is the index of the selected audio model from the audio model set {Pi}P n , ..., q(sk) } from q(cid:48) n n i=1. (Pp, hPp ) = L(M(H(q(cid:48) n), q(cid:48)(d) n ), C), where H(q(cid:48) n) is the task family selected by the task handler H. Noted that, for an audio/image-input task family, hPp may also contain the necessary resources (e.g., audio or images) from the previous context C. As aforementioned, the task family is determined through the task handler H by considering the I/O modality. To be speci\ufb01c, the families are: Audio-to-Text \u2013 Speech Recognition: Transcribe human speech 4 (1) (2) (3) (4) Table 1: Supported Tasks in AudioGPT Task Input Output Domain Model Speech Recognition Speech Translation Audio Audio Text Text Speech Speech Whisper (Radford et al., 2022) MultiDecoder (Dalmia et al., 2021) Style Transfer Speech Enhancement Speech Separation Mono-to-Binaural Audio Inpainting Sound Extraction Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Speech Speech Speech Speech Sound Sound GenerSpeech (Huang et al., 2022b) ConvTasNet (Luo & Mesgarani, 2019) TF-GridNet (Wang et al., 2022) NeuralWarp (Grabocka et al., 2018) Make-An-Audio (Huang et al., 2023a) LASSNet (Liu et al., 2022b) Sound Detection Audio Event Sound Pyramid Transformer (Xin et al., 2022) Talking Head Synthesis Audio Video Talking Head GeneFace (Ye et al., 2023) Text-to-Speech Text-to-Audio Text Text Audio Audio Speech Sound FastSpeech 2 (Ren et al., 2020) Make-An-Audio (Huang et al., 2023a) Audio-to-Text Audio Text Sound MAAC (Ye et al., 2021) Image-to-Audio Image Audio Sound Make-An-Audio (Huang et al., 2023a) Singing Synthesis Musical Score Audio Music DiffSinger (Liu et al., 2022a) VISinger (Zhang et al., 2022b) \u2013 Speech Translation: Translate human speech into another language \u2013 Audio Caption: Describe audio in text Audio-to-Audio \u2013 Style Transfer: Generate human speech with styles derived from a reference \u2013 Speech Enhancement: Improve the speech quality by reducing background noise \u2013 Speech Separation: Separate mix-speech of different speakers \u2013 Mono-to-Binaural: Generate binaural audio given mono one \u2013 Audio Impainting: Inpaint audio given user input mask Audio-to-Event \u2013 Sound Extraction: Selectly extract a part of audio based on description \u2013 Sound Detection: Predict the event timelines in audio Audio-to-Video \u2013 Talking Head Synthesis: Generate a talking human portrait video given input audio Text-to-Audio \u2013 Text-to-Speech: Generate human speech given user input text \u2013 Text-to-Audio: Generate general audio given user description Image-to-Audio \u2013 Image-to-Audio: Generate audio from image Score-to-Audio \u2013 Singing Synthesis: Generate singing voice given input text, note and duration Sequence 3.4 Model Assignment Given the selected model Pp and its corresponding arguments hPp , this step assigns the related resources to the model and executes the model Pp to get the task output oPp : oPp = Pp({q(s1) n , q(s2) n , ..., q(sk) n }, hPp ). To keep the ef\ufb01ciency of AudioGPT, we conduct the audio model initialization during either environ- mental setups or server initialization. 5 (5) 3.5 Response Generation The response generation is highly related to the select task Pp and its output oPp . Speci\ufb01cally, for audio generation tasks, AudioGPT shows both the waveform in an image and the corresponding audio \ufb01le for downloading/playing; for tasks that generate text, the model directly returns the transcribed text; for the video generation task, the output video and some related image frames are shown; for classi\ufb01cation tasks, a posteriorgram of categories is shown over the time span. 4 Evaluating Multi-Modal LLMs 4.1 Overview The rapid development of multi-modal LLMs (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b) has signi\ufb01cantly increased the research demand for evaluating its performance and behavior in understanding human intention, performing complex reasoning, and organizing the cooperation of multiple audio foundation models. In this section, we outline the design principles and process of evaluating multi-modal LLMs (i.e., AudioGPT). Speci\ufb01cally, we evaluate the LLMs in the following three aspects: 1) Consistency, which measures whether the LLMs properly understand the intention of a user, and assigns the audio foundation models closely aligned with human cognition and problem-solving; 2) Capabilitity, which measures the performance of audio foundation models in handling complex audio tasks, understanding and generating speech, music, sound, and talking head in a zero-shot fashion; and 3) Robustness, which measures the ability of LLMs deals with special cases. 4.2 Consistency Figure 2: A high-level overview of consistency evaluation. Audio ModelsRespond Does the response align with human intention faithfully? AudioGPT\u2022Can you synthesize a voice audio with the text content?\u2022Please convert written text into natural-sounding speech.\u2022Convert text into natural-sounding speech.\u2022Transform your written words into spoken audio.\u2022Bring your written content to life with speech synthesis. {\u201cPlease generate a voice from text\u201d, Text to Speech} LLMs Table 2: Ratings that have been used in the evaluation of query-answer consistency. Rating Consistency De\ufb01nition 20 40 60 80 100 Completely inconsistent Very annoying and objectionable inconsistency. Annoying but not objectionable inconsistency. Perceptible and slightly annoying inconsistency Just perceptible but not annoying inconsistency. Imperceptible inconsistency Mostly inconsistent somewhat consistent Mostly consistent Completely consistent In the consistency evaluation for the zero-shot setting, models are directly evaluated on the questions without being provided any prior examples of the speci\ufb01c tasks, which evaluate whether multi-modal LLMS could reason and solve problems without explicit training. More speci\ufb01cally, as shown in Figure 2, the consistency evaluation is carried out in three steps for each task in the benchmark. In the \ufb01rst step, we request human annotators to provide prompts for each task in a format of {prompts, task_name}. This allows us to evaluate the model\u2019s ability to comprehend complex tasks and identify the essential prompts needed for successful task assignments. In the second step, we leverage the outperformed language generation capacity of LLMs to produce descriptions with the same semantic meanings while having different expressions, enabling a comprehensive 6 evaluation of whether LLMs understands the intention of a broader amount of user. Finally, we use crowd-sourced"}, {"question": " What is evaluated in the zero-shot setting in the consistency evaluation?", "answer": " Models are directly evaluated on questions without being provided any prior examples of the specific tasks", "ref_chunk": "is passed into the prompt manager M to generate argument an, including the selected audio foundation model Pp and its corresponding task-related arguments hPp , where p is the index of the selected audio model from the audio model set {Pi}P n , ..., q(sk) } from q(cid:48) n n i=1. (Pp, hPp ) = L(M(H(q(cid:48) n), q(cid:48)(d) n ), C), where H(q(cid:48) n) is the task family selected by the task handler H. Noted that, for an audio/image-input task family, hPp may also contain the necessary resources (e.g., audio or images) from the previous context C. As aforementioned, the task family is determined through the task handler H by considering the I/O modality. To be speci\ufb01c, the families are: Audio-to-Text \u2013 Speech Recognition: Transcribe human speech 4 (1) (2) (3) (4) Table 1: Supported Tasks in AudioGPT Task Input Output Domain Model Speech Recognition Speech Translation Audio Audio Text Text Speech Speech Whisper (Radford et al., 2022) MultiDecoder (Dalmia et al., 2021) Style Transfer Speech Enhancement Speech Separation Mono-to-Binaural Audio Inpainting Sound Extraction Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Speech Speech Speech Speech Sound Sound GenerSpeech (Huang et al., 2022b) ConvTasNet (Luo & Mesgarani, 2019) TF-GridNet (Wang et al., 2022) NeuralWarp (Grabocka et al., 2018) Make-An-Audio (Huang et al., 2023a) LASSNet (Liu et al., 2022b) Sound Detection Audio Event Sound Pyramid Transformer (Xin et al., 2022) Talking Head Synthesis Audio Video Talking Head GeneFace (Ye et al., 2023) Text-to-Speech Text-to-Audio Text Text Audio Audio Speech Sound FastSpeech 2 (Ren et al., 2020) Make-An-Audio (Huang et al., 2023a) Audio-to-Text Audio Text Sound MAAC (Ye et al., 2021) Image-to-Audio Image Audio Sound Make-An-Audio (Huang et al., 2023a) Singing Synthesis Musical Score Audio Music DiffSinger (Liu et al., 2022a) VISinger (Zhang et al., 2022b) \u2013 Speech Translation: Translate human speech into another language \u2013 Audio Caption: Describe audio in text Audio-to-Audio \u2013 Style Transfer: Generate human speech with styles derived from a reference \u2013 Speech Enhancement: Improve the speech quality by reducing background noise \u2013 Speech Separation: Separate mix-speech of different speakers \u2013 Mono-to-Binaural: Generate binaural audio given mono one \u2013 Audio Impainting: Inpaint audio given user input mask Audio-to-Event \u2013 Sound Extraction: Selectly extract a part of audio based on description \u2013 Sound Detection: Predict the event timelines in audio Audio-to-Video \u2013 Talking Head Synthesis: Generate a talking human portrait video given input audio Text-to-Audio \u2013 Text-to-Speech: Generate human speech given user input text \u2013 Text-to-Audio: Generate general audio given user description Image-to-Audio \u2013 Image-to-Audio: Generate audio from image Score-to-Audio \u2013 Singing Synthesis: Generate singing voice given input text, note and duration Sequence 3.4 Model Assignment Given the selected model Pp and its corresponding arguments hPp , this step assigns the related resources to the model and executes the model Pp to get the task output oPp : oPp = Pp({q(s1) n , q(s2) n , ..., q(sk) n }, hPp ). To keep the ef\ufb01ciency of AudioGPT, we conduct the audio model initialization during either environ- mental setups or server initialization. 5 (5) 3.5 Response Generation The response generation is highly related to the select task Pp and its output oPp . Speci\ufb01cally, for audio generation tasks, AudioGPT shows both the waveform in an image and the corresponding audio \ufb01le for downloading/playing; for tasks that generate text, the model directly returns the transcribed text; for the video generation task, the output video and some related image frames are shown; for classi\ufb01cation tasks, a posteriorgram of categories is shown over the time span. 4 Evaluating Multi-Modal LLMs 4.1 Overview The rapid development of multi-modal LLMs (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b) has signi\ufb01cantly increased the research demand for evaluating its performance and behavior in understanding human intention, performing complex reasoning, and organizing the cooperation of multiple audio foundation models. In this section, we outline the design principles and process of evaluating multi-modal LLMs (i.e., AudioGPT). Speci\ufb01cally, we evaluate the LLMs in the following three aspects: 1) Consistency, which measures whether the LLMs properly understand the intention of a user, and assigns the audio foundation models closely aligned with human cognition and problem-solving; 2) Capabilitity, which measures the performance of audio foundation models in handling complex audio tasks, understanding and generating speech, music, sound, and talking head in a zero-shot fashion; and 3) Robustness, which measures the ability of LLMs deals with special cases. 4.2 Consistency Figure 2: A high-level overview of consistency evaluation. Audio ModelsRespond Does the response align with human intention faithfully? AudioGPT\u2022Can you synthesize a voice audio with the text content?\u2022Please convert written text into natural-sounding speech.\u2022Convert text into natural-sounding speech.\u2022Transform your written words into spoken audio.\u2022Bring your written content to life with speech synthesis. {\u201cPlease generate a voice from text\u201d, Text to Speech} LLMs Table 2: Ratings that have been used in the evaluation of query-answer consistency. Rating Consistency De\ufb01nition 20 40 60 80 100 Completely inconsistent Very annoying and objectionable inconsistency. Annoying but not objectionable inconsistency. Perceptible and slightly annoying inconsistency Just perceptible but not annoying inconsistency. Imperceptible inconsistency Mostly inconsistent somewhat consistent Mostly consistent Completely consistent In the consistency evaluation for the zero-shot setting, models are directly evaluated on the questions without being provided any prior examples of the speci\ufb01c tasks, which evaluate whether multi-modal LLMS could reason and solve problems without explicit training. More speci\ufb01cally, as shown in Figure 2, the consistency evaluation is carried out in three steps for each task in the benchmark. In the \ufb01rst step, we request human annotators to provide prompts for each task in a format of {prompts, task_name}. This allows us to evaluate the model\u2019s ability to comprehend complex tasks and identify the essential prompts needed for successful task assignments. In the second step, we leverage the outperformed language generation capacity of LLMs to produce descriptions with the same semantic meanings while having different expressions, enabling a comprehensive 6 evaluation of whether LLMs understands the intention of a broader amount of user. Finally, we use crowd-sourced"}, {"question": " What is requested from human annotators in the first step of the consistency evaluation?", "answer": " Prompts for each task in a format of {prompts, task_name}", "ref_chunk": "is passed into the prompt manager M to generate argument an, including the selected audio foundation model Pp and its corresponding task-related arguments hPp , where p is the index of the selected audio model from the audio model set {Pi}P n , ..., q(sk) } from q(cid:48) n n i=1. (Pp, hPp ) = L(M(H(q(cid:48) n), q(cid:48)(d) n ), C), where H(q(cid:48) n) is the task family selected by the task handler H. Noted that, for an audio/image-input task family, hPp may also contain the necessary resources (e.g., audio or images) from the previous context C. As aforementioned, the task family is determined through the task handler H by considering the I/O modality. To be speci\ufb01c, the families are: Audio-to-Text \u2013 Speech Recognition: Transcribe human speech 4 (1) (2) (3) (4) Table 1: Supported Tasks in AudioGPT Task Input Output Domain Model Speech Recognition Speech Translation Audio Audio Text Text Speech Speech Whisper (Radford et al., 2022) MultiDecoder (Dalmia et al., 2021) Style Transfer Speech Enhancement Speech Separation Mono-to-Binaural Audio Inpainting Sound Extraction Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Speech Speech Speech Speech Sound Sound GenerSpeech (Huang et al., 2022b) ConvTasNet (Luo & Mesgarani, 2019) TF-GridNet (Wang et al., 2022) NeuralWarp (Grabocka et al., 2018) Make-An-Audio (Huang et al., 2023a) LASSNet (Liu et al., 2022b) Sound Detection Audio Event Sound Pyramid Transformer (Xin et al., 2022) Talking Head Synthesis Audio Video Talking Head GeneFace (Ye et al., 2023) Text-to-Speech Text-to-Audio Text Text Audio Audio Speech Sound FastSpeech 2 (Ren et al., 2020) Make-An-Audio (Huang et al., 2023a) Audio-to-Text Audio Text Sound MAAC (Ye et al., 2021) Image-to-Audio Image Audio Sound Make-An-Audio (Huang et al., 2023a) Singing Synthesis Musical Score Audio Music DiffSinger (Liu et al., 2022a) VISinger (Zhang et al., 2022b) \u2013 Speech Translation: Translate human speech into another language \u2013 Audio Caption: Describe audio in text Audio-to-Audio \u2013 Style Transfer: Generate human speech with styles derived from a reference \u2013 Speech Enhancement: Improve the speech quality by reducing background noise \u2013 Speech Separation: Separate mix-speech of different speakers \u2013 Mono-to-Binaural: Generate binaural audio given mono one \u2013 Audio Impainting: Inpaint audio given user input mask Audio-to-Event \u2013 Sound Extraction: Selectly extract a part of audio based on description \u2013 Sound Detection: Predict the event timelines in audio Audio-to-Video \u2013 Talking Head Synthesis: Generate a talking human portrait video given input audio Text-to-Audio \u2013 Text-to-Speech: Generate human speech given user input text \u2013 Text-to-Audio: Generate general audio given user description Image-to-Audio \u2013 Image-to-Audio: Generate audio from image Score-to-Audio \u2013 Singing Synthesis: Generate singing voice given input text, note and duration Sequence 3.4 Model Assignment Given the selected model Pp and its corresponding arguments hPp , this step assigns the related resources to the model and executes the model Pp to get the task output oPp : oPp = Pp({q(s1) n , q(s2) n , ..., q(sk) n }, hPp ). To keep the ef\ufb01ciency of AudioGPT, we conduct the audio model initialization during either environ- mental setups or server initialization. 5 (5) 3.5 Response Generation The response generation is highly related to the select task Pp and its output oPp . Speci\ufb01cally, for audio generation tasks, AudioGPT shows both the waveform in an image and the corresponding audio \ufb01le for downloading/playing; for tasks that generate text, the model directly returns the transcribed text; for the video generation task, the output video and some related image frames are shown; for classi\ufb01cation tasks, a posteriorgram of categories is shown over the time span. 4 Evaluating Multi-Modal LLMs 4.1 Overview The rapid development of multi-modal LLMs (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b) has signi\ufb01cantly increased the research demand for evaluating its performance and behavior in understanding human intention, performing complex reasoning, and organizing the cooperation of multiple audio foundation models. In this section, we outline the design principles and process of evaluating multi-modal LLMs (i.e., AudioGPT). Speci\ufb01cally, we evaluate the LLMs in the following three aspects: 1) Consistency, which measures whether the LLMs properly understand the intention of a user, and assigns the audio foundation models closely aligned with human cognition and problem-solving; 2) Capabilitity, which measures the performance of audio foundation models in handling complex audio tasks, understanding and generating speech, music, sound, and talking head in a zero-shot fashion; and 3) Robustness, which measures the ability of LLMs deals with special cases. 4.2 Consistency Figure 2: A high-level overview of consistency evaluation. Audio ModelsRespond Does the response align with human intention faithfully? AudioGPT\u2022Can you synthesize a voice audio with the text content?\u2022Please convert written text into natural-sounding speech.\u2022Convert text into natural-sounding speech.\u2022Transform your written words into spoken audio.\u2022Bring your written content to life with speech synthesis. {\u201cPlease generate a voice from text\u201d, Text to Speech} LLMs Table 2: Ratings that have been used in the evaluation of query-answer consistency. Rating Consistency De\ufb01nition 20 40 60 80 100 Completely inconsistent Very annoying and objectionable inconsistency. Annoying but not objectionable inconsistency. Perceptible and slightly annoying inconsistency Just perceptible but not annoying inconsistency. Imperceptible inconsistency Mostly inconsistent somewhat consistent Mostly consistent Completely consistent In the consistency evaluation for the zero-shot setting, models are directly evaluated on the questions without being provided any prior examples of the speci\ufb01c tasks, which evaluate whether multi-modal LLMS could reason and solve problems without explicit training. More speci\ufb01cally, as shown in Figure 2, the consistency evaluation is carried out in three steps for each task in the benchmark. In the \ufb01rst step, we request human annotators to provide prompts for each task in a format of {prompts, task_name}. This allows us to evaluate the model\u2019s ability to comprehend complex tasks and identify the essential prompts needed for successful task assignments. In the second step, we leverage the outperformed language generation capacity of LLMs to produce descriptions with the same semantic meanings while having different expressions, enabling a comprehensive 6 evaluation of whether LLMs understands the intention of a broader amount of user. Finally, we use crowd-sourced"}, {"question": " How many steps are involved in the consistency evaluation for each task in the benchmark?", "answer": " Three steps", "ref_chunk": "is passed into the prompt manager M to generate argument an, including the selected audio foundation model Pp and its corresponding task-related arguments hPp , where p is the index of the selected audio model from the audio model set {Pi}P n , ..., q(sk) } from q(cid:48) n n i=1. (Pp, hPp ) = L(M(H(q(cid:48) n), q(cid:48)(d) n ), C), where H(q(cid:48) n) is the task family selected by the task handler H. Noted that, for an audio/image-input task family, hPp may also contain the necessary resources (e.g., audio or images) from the previous context C. As aforementioned, the task family is determined through the task handler H by considering the I/O modality. To be speci\ufb01c, the families are: Audio-to-Text \u2013 Speech Recognition: Transcribe human speech 4 (1) (2) (3) (4) Table 1: Supported Tasks in AudioGPT Task Input Output Domain Model Speech Recognition Speech Translation Audio Audio Text Text Speech Speech Whisper (Radford et al., 2022) MultiDecoder (Dalmia et al., 2021) Style Transfer Speech Enhancement Speech Separation Mono-to-Binaural Audio Inpainting Sound Extraction Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Speech Speech Speech Speech Sound Sound GenerSpeech (Huang et al., 2022b) ConvTasNet (Luo & Mesgarani, 2019) TF-GridNet (Wang et al., 2022) NeuralWarp (Grabocka et al., 2018) Make-An-Audio (Huang et al., 2023a) LASSNet (Liu et al., 2022b) Sound Detection Audio Event Sound Pyramid Transformer (Xin et al., 2022) Talking Head Synthesis Audio Video Talking Head GeneFace (Ye et al., 2023) Text-to-Speech Text-to-Audio Text Text Audio Audio Speech Sound FastSpeech 2 (Ren et al., 2020) Make-An-Audio (Huang et al., 2023a) Audio-to-Text Audio Text Sound MAAC (Ye et al., 2021) Image-to-Audio Image Audio Sound Make-An-Audio (Huang et al., 2023a) Singing Synthesis Musical Score Audio Music DiffSinger (Liu et al., 2022a) VISinger (Zhang et al., 2022b) \u2013 Speech Translation: Translate human speech into another language \u2013 Audio Caption: Describe audio in text Audio-to-Audio \u2013 Style Transfer: Generate human speech with styles derived from a reference \u2013 Speech Enhancement: Improve the speech quality by reducing background noise \u2013 Speech Separation: Separate mix-speech of different speakers \u2013 Mono-to-Binaural: Generate binaural audio given mono one \u2013 Audio Impainting: Inpaint audio given user input mask Audio-to-Event \u2013 Sound Extraction: Selectly extract a part of audio based on description \u2013 Sound Detection: Predict the event timelines in audio Audio-to-Video \u2013 Talking Head Synthesis: Generate a talking human portrait video given input audio Text-to-Audio \u2013 Text-to-Speech: Generate human speech given user input text \u2013 Text-to-Audio: Generate general audio given user description Image-to-Audio \u2013 Image-to-Audio: Generate audio from image Score-to-Audio \u2013 Singing Synthesis: Generate singing voice given input text, note and duration Sequence 3.4 Model Assignment Given the selected model Pp and its corresponding arguments hPp , this step assigns the related resources to the model and executes the model Pp to get the task output oPp : oPp = Pp({q(s1) n , q(s2) n , ..., q(sk) n }, hPp ). To keep the ef\ufb01ciency of AudioGPT, we conduct the audio model initialization during either environ- mental setups or server initialization. 5 (5) 3.5 Response Generation The response generation is highly related to the select task Pp and its output oPp . Speci\ufb01cally, for audio generation tasks, AudioGPT shows both the waveform in an image and the corresponding audio \ufb01le for downloading/playing; for tasks that generate text, the model directly returns the transcribed text; for the video generation task, the output video and some related image frames are shown; for classi\ufb01cation tasks, a posteriorgram of categories is shown over the time span. 4 Evaluating Multi-Modal LLMs 4.1 Overview The rapid development of multi-modal LLMs (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b) has signi\ufb01cantly increased the research demand for evaluating its performance and behavior in understanding human intention, performing complex reasoning, and organizing the cooperation of multiple audio foundation models. In this section, we outline the design principles and process of evaluating multi-modal LLMs (i.e., AudioGPT). Speci\ufb01cally, we evaluate the LLMs in the following three aspects: 1) Consistency, which measures whether the LLMs properly understand the intention of a user, and assigns the audio foundation models closely aligned with human cognition and problem-solving; 2) Capabilitity, which measures the performance of audio foundation models in handling complex audio tasks, understanding and generating speech, music, sound, and talking head in a zero-shot fashion; and 3) Robustness, which measures the ability of LLMs deals with special cases. 4.2 Consistency Figure 2: A high-level overview of consistency evaluation. Audio ModelsRespond Does the response align with human intention faithfully? AudioGPT\u2022Can you synthesize a voice audio with the text content?\u2022Please convert written text into natural-sounding speech.\u2022Convert text into natural-sounding speech.\u2022Transform your written words into spoken audio.\u2022Bring your written content to life with speech synthesis. {\u201cPlease generate a voice from text\u201d, Text to Speech} LLMs Table 2: Ratings that have been used in the evaluation of query-answer consistency. Rating Consistency De\ufb01nition 20 40 60 80 100 Completely inconsistent Very annoying and objectionable inconsistency. Annoying but not objectionable inconsistency. Perceptible and slightly annoying inconsistency Just perceptible but not annoying inconsistency. Imperceptible inconsistency Mostly inconsistent somewhat consistent Mostly consistent Completely consistent In the consistency evaluation for the zero-shot setting, models are directly evaluated on the questions without being provided any prior examples of the speci\ufb01c tasks, which evaluate whether multi-modal LLMS could reason and solve problems without explicit training. More speci\ufb01cally, as shown in Figure 2, the consistency evaluation is carried out in three steps for each task in the benchmark. In the \ufb01rst step, we request human annotators to provide prompts for each task in a format of {prompts, task_name}. This allows us to evaluate the model\u2019s ability to comprehend complex tasks and identify the essential prompts needed for successful task assignments. In the second step, we leverage the outperformed language generation capacity of LLMs to produce descriptions with the same semantic meanings while having different expressions, enabling a comprehensive 6 evaluation of whether LLMs understands the intention of a broader amount of user. Finally, we use crowd-sourced"}], "doc_text": "is passed into the prompt manager M to generate argument an, including the selected audio foundation model Pp and its corresponding task-related arguments hPp , where p is the index of the selected audio model from the audio model set {Pi}P n , ..., q(sk) } from q(cid:48) n n i=1. (Pp, hPp ) = L(M(H(q(cid:48) n), q(cid:48)(d) n ), C), where H(q(cid:48) n) is the task family selected by the task handler H. Noted that, for an audio/image-input task family, hPp may also contain the necessary resources (e.g., audio or images) from the previous context C. As aforementioned, the task family is determined through the task handler H by considering the I/O modality. To be speci\ufb01c, the families are: Audio-to-Text \u2013 Speech Recognition: Transcribe human speech 4 (1) (2) (3) (4) Table 1: Supported Tasks in AudioGPT Task Input Output Domain Model Speech Recognition Speech Translation Audio Audio Text Text Speech Speech Whisper (Radford et al., 2022) MultiDecoder (Dalmia et al., 2021) Style Transfer Speech Enhancement Speech Separation Mono-to-Binaural Audio Inpainting Sound Extraction Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Audio Speech Speech Speech Speech Sound Sound GenerSpeech (Huang et al., 2022b) ConvTasNet (Luo & Mesgarani, 2019) TF-GridNet (Wang et al., 2022) NeuralWarp (Grabocka et al., 2018) Make-An-Audio (Huang et al., 2023a) LASSNet (Liu et al., 2022b) Sound Detection Audio Event Sound Pyramid Transformer (Xin et al., 2022) Talking Head Synthesis Audio Video Talking Head GeneFace (Ye et al., 2023) Text-to-Speech Text-to-Audio Text Text Audio Audio Speech Sound FastSpeech 2 (Ren et al., 2020) Make-An-Audio (Huang et al., 2023a) Audio-to-Text Audio Text Sound MAAC (Ye et al., 2021) Image-to-Audio Image Audio Sound Make-An-Audio (Huang et al., 2023a) Singing Synthesis Musical Score Audio Music DiffSinger (Liu et al., 2022a) VISinger (Zhang et al., 2022b) \u2013 Speech Translation: Translate human speech into another language \u2013 Audio Caption: Describe audio in text Audio-to-Audio \u2013 Style Transfer: Generate human speech with styles derived from a reference \u2013 Speech Enhancement: Improve the speech quality by reducing background noise \u2013 Speech Separation: Separate mix-speech of different speakers \u2013 Mono-to-Binaural: Generate binaural audio given mono one \u2013 Audio Impainting: Inpaint audio given user input mask Audio-to-Event \u2013 Sound Extraction: Selectly extract a part of audio based on description \u2013 Sound Detection: Predict the event timelines in audio Audio-to-Video \u2013 Talking Head Synthesis: Generate a talking human portrait video given input audio Text-to-Audio \u2013 Text-to-Speech: Generate human speech given user input text \u2013 Text-to-Audio: Generate general audio given user description Image-to-Audio \u2013 Image-to-Audio: Generate audio from image Score-to-Audio \u2013 Singing Synthesis: Generate singing voice given input text, note and duration Sequence 3.4 Model Assignment Given the selected model Pp and its corresponding arguments hPp , this step assigns the related resources to the model and executes the model Pp to get the task output oPp : oPp = Pp({q(s1) n , q(s2) n , ..., q(sk) n }, hPp ). To keep the ef\ufb01ciency of AudioGPT, we conduct the audio model initialization during either environ- mental setups or server initialization. 5 (5) 3.5 Response Generation The response generation is highly related to the select task Pp and its output oPp . Speci\ufb01cally, for audio generation tasks, AudioGPT shows both the waveform in an image and the corresponding audio \ufb01le for downloading/playing; for tasks that generate text, the model directly returns the transcribed text; for the video generation task, the output video and some related image frames are shown; for classi\ufb01cation tasks, a posteriorgram of categories is shown over the time span. 4 Evaluating Multi-Modal LLMs 4.1 Overview The rapid development of multi-modal LLMs (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b) has signi\ufb01cantly increased the research demand for evaluating its performance and behavior in understanding human intention, performing complex reasoning, and organizing the cooperation of multiple audio foundation models. In this section, we outline the design principles and process of evaluating multi-modal LLMs (i.e., AudioGPT). Speci\ufb01cally, we evaluate the LLMs in the following three aspects: 1) Consistency, which measures whether the LLMs properly understand the intention of a user, and assigns the audio foundation models closely aligned with human cognition and problem-solving; 2) Capabilitity, which measures the performance of audio foundation models in handling complex audio tasks, understanding and generating speech, music, sound, and talking head in a zero-shot fashion; and 3) Robustness, which measures the ability of LLMs deals with special cases. 4.2 Consistency Figure 2: A high-level overview of consistency evaluation. Audio ModelsRespond Does the response align with human intention faithfully? AudioGPT\u2022Can you synthesize a voice audio with the text content?\u2022Please convert written text into natural-sounding speech.\u2022Convert text into natural-sounding speech.\u2022Transform your written words into spoken audio.\u2022Bring your written content to life with speech synthesis. {\u201cPlease generate a voice from text\u201d, Text to Speech} LLMs Table 2: Ratings that have been used in the evaluation of query-answer consistency. Rating Consistency De\ufb01nition 20 40 60 80 100 Completely inconsistent Very annoying and objectionable inconsistency. Annoying but not objectionable inconsistency. Perceptible and slightly annoying inconsistency Just perceptible but not annoying inconsistency. Imperceptible inconsistency Mostly inconsistent somewhat consistent Mostly consistent Completely consistent In the consistency evaluation for the zero-shot setting, models are directly evaluated on the questions without being provided any prior examples of the speci\ufb01c tasks, which evaluate whether multi-modal LLMS could reason and solve problems without explicit training. More speci\ufb01cally, as shown in Figure 2, the consistency evaluation is carried out in three steps for each task in the benchmark. In the \ufb01rst step, we request human annotators to provide prompts for each task in a format of {prompts, task_name}. This allows us to evaluate the model\u2019s ability to comprehend complex tasks and identify the essential prompts needed for successful task assignments. In the second step, we leverage the outperformed language generation capacity of LLMs to produce descriptions with the same semantic meanings while having different expressions, enabling a comprehensive 6 evaluation of whether LLMs understands the intention of a broader amount of user. Finally, we use crowd-sourced"}