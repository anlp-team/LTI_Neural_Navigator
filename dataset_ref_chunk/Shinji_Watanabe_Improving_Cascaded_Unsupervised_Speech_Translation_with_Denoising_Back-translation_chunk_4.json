{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Improving_Cascaded_Unsupervised_Speech_Translation_with_Denoising_Back-translation_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What was used to calculate the BLEU score of S2TT?", "answer": " sacreBLEU8", "ref_chunk": "used sacreBLEU8 to calculate the BLEU score of S2TT. For the \ufb01nal S2ST results, Whisper 9 (Radford et al., 2022), a supervised ASR model released by OpenAI, is adopted to transcribe the hypothesized audio and calculated the BLEU score. 4.5 Results We show our results in Table 1, including the results of ASR, S2TT, and S2ST. To compare our US2ST system with others\u2019 works, we collect some results from the previous studies on CoVoST 2 and CVSS ((a)\u2013(f), (h)\u2013(i)). To get better comparisons, both cascaded and direct systems are included. Next, we discuss the details of the methods in the table. (a) comes from Wang et al. (2020b); among all experiments in their paper, we only report the results of the cascaded S2TT system constructed 8https://github.com/mjpost/sacrebleu 9we used the cross-lingual large-v2 model released by https://github.com/openai/whisper Table 1: The results are evaluated on CoVoST 2 for S2TT, and CVSS for S2ST. C-T stands for cascaded S2TT; D-S for direct S2ST. Method Type Fr ASR \u2193 De Es S2TT (X\u2192En) \u2191 Es De Fr S2ST (X\u2192En) \u2191 De Fr Es SUPERVISED LEARNING (a) Wang et al. (2020b) (b) fairseq S2T (T-Sm) (Wang et al., 2020a) (c) fairseq S2T (Multi. T-Md) (d) XLS-R (2B) (Babu et al., 2021) (e) Translatotron (Jia et al., 2019) (f) Translatotron 2 (Jia et al., 2021) (g) Our upper bound UNSUPERVISED LEARNING (h) Wang et al. (2022) cascaded US2ST (i) Wang et al. (2022) end-to-end US2ST (j) Our cascaded US2ST C-S (k) Our cascaded US2ST with DBT (arti\ufb01cial noise) C-S C-T 18.3 D-T D-T D-T D-S D-S C-S - - - - 16.2 - 33.2 21.4 - - - - - 14.1 - 23.8 16.0 - - - - - 11.0 - 17.4 27.6 26.3 26.5 37.6 - - 29.5 24.4 24.2 20.0 20.8 21.0 17.1 17.5 33.6 - - 25.6 - 19.5 20.4 27.4 23.0 27.0 39.2 - - 31.0 23.4 24.0 23.8 24.5 - - - 15.5 28.3 23.8 21.6 21.2 13.4 14.4 - - - 6.9 19.7 21.8 - 13.8 14.7 - - - 14.1 23.5 26.2 21.2 20.1 16.7 17.4 by monolingual ASR and bilingual MT for fair comparison. According to the table, our US2TT performances in De\u2013En are just having small degra- dation from theirs ((j) and (k) vs (a)), indicating that our US2TT system can be comparable to some early supervised cascaded S2TT works in some language pairs. Rows (b) and (c) are the results of the direct S2TT systems from Wang et al. (2020a). They have developed a tool-kit for S2TT and demonstrated it on CoVoST 2 with different model backbones. We compare our US2TT results (row (j) and (k)) with their Transformer-based models. Our results have not only outperformed their small model ((b)) in De-En and Es-En tasks but also outperformed their large model in De-En (row (c)). Our works are comparable to some early direct S2TT systems ((b), (c)) except for the Fr-En pair. Row (d) is the SOTA direct S2TT model, which is a cross-lingual speech model based on wav2vec 2.0 architecture. Taking advantage of the large pretrained self-supervised model, the performance is signi\ufb01cantly better than all the other works in all language pairs, showing there is still a huge gap between SOTA supervised S2TT and our US2TT systems. Rows (h) and (i) come from the concurrent US2ST system (Wang et al., 2022); (h) is con- structed by concatenating UASR, UNMT, and UTTS, and (i) is an end2end S2ST systems trained on pseudo labels generated by (h). Our model ar- chitecture is similar to theirs, while they \ufb01ne-tune the wav2vec 2.0 with target languages in UASR to mitigate the domain mismatch between pretraining and downstream tasks. For Fr-En S2TT, their per- formance is superior to ours. This may be due to the dif\ufb01culty of French UASR, which has a higher error rate than other languages. This phenomenon was also observed in Wang et al. (2022), and we think that it might be more severe in our case since we did not \ufb01ne-tune the wav2vec 2.0 models on the target languages. Suffering from domain mismatch, our systems still outperform their works in Es-En S2TT. The performance of our S2ST systems drops even more than S2TT. However, this is because we did not utilize the data from CVSS when training our UTTS models for the fairness concern. Other studies ((e), (f), (h), (i)) directly trained the whole model or UTTS with the data from CVSS. Since our models are trained with LJSpeech (row (g), (j), (k)), they could have severe domain mismatch problems during inference on out-domain data. We had also investigated the cause of the performance drop in the later section and came out with the same conclusion. In spite of this, our unsupervised S2ST results still outperform Translatotron (row (j), (k) v.s. (e)) in De-En and Es-EN translation directions. Last but not least, Our cascaded system with DBT (k) outperforms (j) by 0.7 to 0.9 in both US2TT and US2ST, indicating a model can bet- ter translate the noisy input under the guidance of DBT, and thus mitigate the problem of error propa- gation in cascaded systems. 5 Analysis Stabilities of UASR cross different languages First of all, we found that the stabilities of our UASR models vary between languages. The mea- surement of the stability is by calculating the per- 6 centage of the converged rate among the models leveraging the same amount of text and speech data. We consider a UASR model is converged if its P ER < 50%. We summarize the discoveries in Table 2. According to our experiments, German and Spanish are easier to converge; while French usually can not converge well. However, we found that it might be more suitable for French UASR models to converge if we change the <SIL> token insertion rate from 0.25 to 0.5. Table 2: Stabilities of UASR across different lan- guages. Lang. <SIL> ins. rate Best PER (Viterbi) %-converged (PER < 50%) De 0.25 25.3% 66% Es 0.25 27.0% 50% Fr 0.25 0.50 49.2% 35.2% <10%"}, {"question": " Which model was adopted by OpenAI to transcribe the hypothesized audio and calculate the BLEU score?", "answer": " Whisper 9", "ref_chunk": "used sacreBLEU8 to calculate the BLEU score of S2TT. For the \ufb01nal S2ST results, Whisper 9 (Radford et al., 2022), a supervised ASR model released by OpenAI, is adopted to transcribe the hypothesized audio and calculated the BLEU score. 4.5 Results We show our results in Table 1, including the results of ASR, S2TT, and S2ST. To compare our US2ST system with others\u2019 works, we collect some results from the previous studies on CoVoST 2 and CVSS ((a)\u2013(f), (h)\u2013(i)). To get better comparisons, both cascaded and direct systems are included. Next, we discuss the details of the methods in the table. (a) comes from Wang et al. (2020b); among all experiments in their paper, we only report the results of the cascaded S2TT system constructed 8https://github.com/mjpost/sacrebleu 9we used the cross-lingual large-v2 model released by https://github.com/openai/whisper Table 1: The results are evaluated on CoVoST 2 for S2TT, and CVSS for S2ST. C-T stands for cascaded S2TT; D-S for direct S2ST. Method Type Fr ASR \u2193 De Es S2TT (X\u2192En) \u2191 Es De Fr S2ST (X\u2192En) \u2191 De Fr Es SUPERVISED LEARNING (a) Wang et al. (2020b) (b) fairseq S2T (T-Sm) (Wang et al., 2020a) (c) fairseq S2T (Multi. T-Md) (d) XLS-R (2B) (Babu et al., 2021) (e) Translatotron (Jia et al., 2019) (f) Translatotron 2 (Jia et al., 2021) (g) Our upper bound UNSUPERVISED LEARNING (h) Wang et al. (2022) cascaded US2ST (i) Wang et al. (2022) end-to-end US2ST (j) Our cascaded US2ST C-S (k) Our cascaded US2ST with DBT (arti\ufb01cial noise) C-S C-T 18.3 D-T D-T D-T D-S D-S C-S - - - - 16.2 - 33.2 21.4 - - - - - 14.1 - 23.8 16.0 - - - - - 11.0 - 17.4 27.6 26.3 26.5 37.6 - - 29.5 24.4 24.2 20.0 20.8 21.0 17.1 17.5 33.6 - - 25.6 - 19.5 20.4 27.4 23.0 27.0 39.2 - - 31.0 23.4 24.0 23.8 24.5 - - - 15.5 28.3 23.8 21.6 21.2 13.4 14.4 - - - 6.9 19.7 21.8 - 13.8 14.7 - - - 14.1 23.5 26.2 21.2 20.1 16.7 17.4 by monolingual ASR and bilingual MT for fair comparison. According to the table, our US2TT performances in De\u2013En are just having small degra- dation from theirs ((j) and (k) vs (a)), indicating that our US2TT system can be comparable to some early supervised cascaded S2TT works in some language pairs. Rows (b) and (c) are the results of the direct S2TT systems from Wang et al. (2020a). They have developed a tool-kit for S2TT and demonstrated it on CoVoST 2 with different model backbones. We compare our US2TT results (row (j) and (k)) with their Transformer-based models. Our results have not only outperformed their small model ((b)) in De-En and Es-En tasks but also outperformed their large model in De-En (row (c)). Our works are comparable to some early direct S2TT systems ((b), (c)) except for the Fr-En pair. Row (d) is the SOTA direct S2TT model, which is a cross-lingual speech model based on wav2vec 2.0 architecture. Taking advantage of the large pretrained self-supervised model, the performance is signi\ufb01cantly better than all the other works in all language pairs, showing there is still a huge gap between SOTA supervised S2TT and our US2TT systems. Rows (h) and (i) come from the concurrent US2ST system (Wang et al., 2022); (h) is con- structed by concatenating UASR, UNMT, and UTTS, and (i) is an end2end S2ST systems trained on pseudo labels generated by (h). Our model ar- chitecture is similar to theirs, while they \ufb01ne-tune the wav2vec 2.0 with target languages in UASR to mitigate the domain mismatch between pretraining and downstream tasks. For Fr-En S2TT, their per- formance is superior to ours. This may be due to the dif\ufb01culty of French UASR, which has a higher error rate than other languages. This phenomenon was also observed in Wang et al. (2022), and we think that it might be more severe in our case since we did not \ufb01ne-tune the wav2vec 2.0 models on the target languages. Suffering from domain mismatch, our systems still outperform their works in Es-En S2TT. The performance of our S2ST systems drops even more than S2TT. However, this is because we did not utilize the data from CVSS when training our UTTS models for the fairness concern. Other studies ((e), (f), (h), (i)) directly trained the whole model or UTTS with the data from CVSS. Since our models are trained with LJSpeech (row (g), (j), (k)), they could have severe domain mismatch problems during inference on out-domain data. We had also investigated the cause of the performance drop in the later section and came out with the same conclusion. In spite of this, our unsupervised S2ST results still outperform Translatotron (row (j), (k) v.s. (e)) in De-En and Es-EN translation directions. Last but not least, Our cascaded system with DBT (k) outperforms (j) by 0.7 to 0.9 in both US2TT and US2ST, indicating a model can bet- ter translate the noisy input under the guidance of DBT, and thus mitigate the problem of error propa- gation in cascaded systems. 5 Analysis Stabilities of UASR cross different languages First of all, we found that the stabilities of our UASR models vary between languages. The mea- surement of the stability is by calculating the per- 6 centage of the converged rate among the models leveraging the same amount of text and speech data. We consider a UASR model is converged if its P ER < 50%. We summarize the discoveries in Table 2. According to our experiments, German and Spanish are easier to converge; while French usually can not converge well. However, we found that it might be more suitable for French UASR models to converge if we change the <SIL> token insertion rate from 0.25 to 0.5. Table 2: Stabilities of UASR across different lan- guages. Lang. <SIL> ins. rate Best PER (Viterbi) %-converged (PER < 50%) De 0.25 25.3% 66% Es 0.25 27.0% 50% Fr 0.25 0.50 49.2% 35.2% <10%"}, {"question": " What does the abbreviation ASR stand for in the context of the text?", "answer": " Automatic Speech Recognition", "ref_chunk": "used sacreBLEU8 to calculate the BLEU score of S2TT. For the \ufb01nal S2ST results, Whisper 9 (Radford et al., 2022), a supervised ASR model released by OpenAI, is adopted to transcribe the hypothesized audio and calculated the BLEU score. 4.5 Results We show our results in Table 1, including the results of ASR, S2TT, and S2ST. To compare our US2ST system with others\u2019 works, we collect some results from the previous studies on CoVoST 2 and CVSS ((a)\u2013(f), (h)\u2013(i)). To get better comparisons, both cascaded and direct systems are included. Next, we discuss the details of the methods in the table. (a) comes from Wang et al. (2020b); among all experiments in their paper, we only report the results of the cascaded S2TT system constructed 8https://github.com/mjpost/sacrebleu 9we used the cross-lingual large-v2 model released by https://github.com/openai/whisper Table 1: The results are evaluated on CoVoST 2 for S2TT, and CVSS for S2ST. C-T stands for cascaded S2TT; D-S for direct S2ST. Method Type Fr ASR \u2193 De Es S2TT (X\u2192En) \u2191 Es De Fr S2ST (X\u2192En) \u2191 De Fr Es SUPERVISED LEARNING (a) Wang et al. (2020b) (b) fairseq S2T (T-Sm) (Wang et al., 2020a) (c) fairseq S2T (Multi. T-Md) (d) XLS-R (2B) (Babu et al., 2021) (e) Translatotron (Jia et al., 2019) (f) Translatotron 2 (Jia et al., 2021) (g) Our upper bound UNSUPERVISED LEARNING (h) Wang et al. (2022) cascaded US2ST (i) Wang et al. (2022) end-to-end US2ST (j) Our cascaded US2ST C-S (k) Our cascaded US2ST with DBT (arti\ufb01cial noise) C-S C-T 18.3 D-T D-T D-T D-S D-S C-S - - - - 16.2 - 33.2 21.4 - - - - - 14.1 - 23.8 16.0 - - - - - 11.0 - 17.4 27.6 26.3 26.5 37.6 - - 29.5 24.4 24.2 20.0 20.8 21.0 17.1 17.5 33.6 - - 25.6 - 19.5 20.4 27.4 23.0 27.0 39.2 - - 31.0 23.4 24.0 23.8 24.5 - - - 15.5 28.3 23.8 21.6 21.2 13.4 14.4 - - - 6.9 19.7 21.8 - 13.8 14.7 - - - 14.1 23.5 26.2 21.2 20.1 16.7 17.4 by monolingual ASR and bilingual MT for fair comparison. According to the table, our US2TT performances in De\u2013En are just having small degra- dation from theirs ((j) and (k) vs (a)), indicating that our US2TT system can be comparable to some early supervised cascaded S2TT works in some language pairs. Rows (b) and (c) are the results of the direct S2TT systems from Wang et al. (2020a). They have developed a tool-kit for S2TT and demonstrated it on CoVoST 2 with different model backbones. We compare our US2TT results (row (j) and (k)) with their Transformer-based models. Our results have not only outperformed their small model ((b)) in De-En and Es-En tasks but also outperformed their large model in De-En (row (c)). Our works are comparable to some early direct S2TT systems ((b), (c)) except for the Fr-En pair. Row (d) is the SOTA direct S2TT model, which is a cross-lingual speech model based on wav2vec 2.0 architecture. Taking advantage of the large pretrained self-supervised model, the performance is signi\ufb01cantly better than all the other works in all language pairs, showing there is still a huge gap between SOTA supervised S2TT and our US2TT systems. Rows (h) and (i) come from the concurrent US2ST system (Wang et al., 2022); (h) is con- structed by concatenating UASR, UNMT, and UTTS, and (i) is an end2end S2ST systems trained on pseudo labels generated by (h). Our model ar- chitecture is similar to theirs, while they \ufb01ne-tune the wav2vec 2.0 with target languages in UASR to mitigate the domain mismatch between pretraining and downstream tasks. For Fr-En S2TT, their per- formance is superior to ours. This may be due to the dif\ufb01culty of French UASR, which has a higher error rate than other languages. This phenomenon was also observed in Wang et al. (2022), and we think that it might be more severe in our case since we did not \ufb01ne-tune the wav2vec 2.0 models on the target languages. Suffering from domain mismatch, our systems still outperform their works in Es-En S2TT. The performance of our S2ST systems drops even more than S2TT. However, this is because we did not utilize the data from CVSS when training our UTTS models for the fairness concern. Other studies ((e), (f), (h), (i)) directly trained the whole model or UTTS with the data from CVSS. Since our models are trained with LJSpeech (row (g), (j), (k)), they could have severe domain mismatch problems during inference on out-domain data. We had also investigated the cause of the performance drop in the later section and came out with the same conclusion. In spite of this, our unsupervised S2ST results still outperform Translatotron (row (j), (k) v.s. (e)) in De-En and Es-EN translation directions. Last but not least, Our cascaded system with DBT (k) outperforms (j) by 0.7 to 0.9 in both US2TT and US2ST, indicating a model can bet- ter translate the noisy input under the guidance of DBT, and thus mitigate the problem of error propa- gation in cascaded systems. 5 Analysis Stabilities of UASR cross different languages First of all, we found that the stabilities of our UASR models vary between languages. The mea- surement of the stability is by calculating the per- 6 centage of the converged rate among the models leveraging the same amount of text and speech data. We consider a UASR model is converged if its P ER < 50%. We summarize the discoveries in Table 2. According to our experiments, German and Spanish are easier to converge; while French usually can not converge well. However, we found that it might be more suitable for French UASR models to converge if we change the <SIL> token insertion rate from 0.25 to 0.5. Table 2: Stabilities of UASR across different lan- guages. Lang. <SIL> ins. rate Best PER (Viterbi) %-converged (PER < 50%) De 0.25 25.3% 66% Es 0.25 27.0% 50% Fr 0.25 0.50 49.2% 35.2% <10%"}, {"question": " What does S2TT refer to in the results table?", "answer": " Speech-to-Text translation", "ref_chunk": "used sacreBLEU8 to calculate the BLEU score of S2TT. For the \ufb01nal S2ST results, Whisper 9 (Radford et al., 2022), a supervised ASR model released by OpenAI, is adopted to transcribe the hypothesized audio and calculated the BLEU score. 4.5 Results We show our results in Table 1, including the results of ASR, S2TT, and S2ST. To compare our US2ST system with others\u2019 works, we collect some results from the previous studies on CoVoST 2 and CVSS ((a)\u2013(f), (h)\u2013(i)). To get better comparisons, both cascaded and direct systems are included. Next, we discuss the details of the methods in the table. (a) comes from Wang et al. (2020b); among all experiments in their paper, we only report the results of the cascaded S2TT system constructed 8https://github.com/mjpost/sacrebleu 9we used the cross-lingual large-v2 model released by https://github.com/openai/whisper Table 1: The results are evaluated on CoVoST 2 for S2TT, and CVSS for S2ST. C-T stands for cascaded S2TT; D-S for direct S2ST. Method Type Fr ASR \u2193 De Es S2TT (X\u2192En) \u2191 Es De Fr S2ST (X\u2192En) \u2191 De Fr Es SUPERVISED LEARNING (a) Wang et al. (2020b) (b) fairseq S2T (T-Sm) (Wang et al., 2020a) (c) fairseq S2T (Multi. T-Md) (d) XLS-R (2B) (Babu et al., 2021) (e) Translatotron (Jia et al., 2019) (f) Translatotron 2 (Jia et al., 2021) (g) Our upper bound UNSUPERVISED LEARNING (h) Wang et al. (2022) cascaded US2ST (i) Wang et al. (2022) end-to-end US2ST (j) Our cascaded US2ST C-S (k) Our cascaded US2ST with DBT (arti\ufb01cial noise) C-S C-T 18.3 D-T D-T D-T D-S D-S C-S - - - - 16.2 - 33.2 21.4 - - - - - 14.1 - 23.8 16.0 - - - - - 11.0 - 17.4 27.6 26.3 26.5 37.6 - - 29.5 24.4 24.2 20.0 20.8 21.0 17.1 17.5 33.6 - - 25.6 - 19.5 20.4 27.4 23.0 27.0 39.2 - - 31.0 23.4 24.0 23.8 24.5 - - - 15.5 28.3 23.8 21.6 21.2 13.4 14.4 - - - 6.9 19.7 21.8 - 13.8 14.7 - - - 14.1 23.5 26.2 21.2 20.1 16.7 17.4 by monolingual ASR and bilingual MT for fair comparison. According to the table, our US2TT performances in De\u2013En are just having small degra- dation from theirs ((j) and (k) vs (a)), indicating that our US2TT system can be comparable to some early supervised cascaded S2TT works in some language pairs. Rows (b) and (c) are the results of the direct S2TT systems from Wang et al. (2020a). They have developed a tool-kit for S2TT and demonstrated it on CoVoST 2 with different model backbones. We compare our US2TT results (row (j) and (k)) with their Transformer-based models. Our results have not only outperformed their small model ((b)) in De-En and Es-En tasks but also outperformed their large model in De-En (row (c)). Our works are comparable to some early direct S2TT systems ((b), (c)) except for the Fr-En pair. Row (d) is the SOTA direct S2TT model, which is a cross-lingual speech model based on wav2vec 2.0 architecture. Taking advantage of the large pretrained self-supervised model, the performance is signi\ufb01cantly better than all the other works in all language pairs, showing there is still a huge gap between SOTA supervised S2TT and our US2TT systems. Rows (h) and (i) come from the concurrent US2ST system (Wang et al., 2022); (h) is con- structed by concatenating UASR, UNMT, and UTTS, and (i) is an end2end S2ST systems trained on pseudo labels generated by (h). Our model ar- chitecture is similar to theirs, while they \ufb01ne-tune the wav2vec 2.0 with target languages in UASR to mitigate the domain mismatch between pretraining and downstream tasks. For Fr-En S2TT, their per- formance is superior to ours. This may be due to the dif\ufb01culty of French UASR, which has a higher error rate than other languages. This phenomenon was also observed in Wang et al. (2022), and we think that it might be more severe in our case since we did not \ufb01ne-tune the wav2vec 2.0 models on the target languages. Suffering from domain mismatch, our systems still outperform their works in Es-En S2TT. The performance of our S2ST systems drops even more than S2TT. However, this is because we did not utilize the data from CVSS when training our UTTS models for the fairness concern. Other studies ((e), (f), (h), (i)) directly trained the whole model or UTTS with the data from CVSS. Since our models are trained with LJSpeech (row (g), (j), (k)), they could have severe domain mismatch problems during inference on out-domain data. We had also investigated the cause of the performance drop in the later section and came out with the same conclusion. In spite of this, our unsupervised S2ST results still outperform Translatotron (row (j), (k) v.s. (e)) in De-En and Es-EN translation directions. Last but not least, Our cascaded system with DBT (k) outperforms (j) by 0.7 to 0.9 in both US2TT and US2ST, indicating a model can bet- ter translate the noisy input under the guidance of DBT, and thus mitigate the problem of error propa- gation in cascaded systems. 5 Analysis Stabilities of UASR cross different languages First of all, we found that the stabilities of our UASR models vary between languages. The mea- surement of the stability is by calculating the per- 6 centage of the converged rate among the models leveraging the same amount of text and speech data. We consider a UASR model is converged if its P ER < 50%. We summarize the discoveries in Table 2. According to our experiments, German and Spanish are easier to converge; while French usually can not converge well. However, we found that it might be more suitable for French UASR models to converge if we change the <SIL> token insertion rate from 0.25 to 0.5. Table 2: Stabilities of UASR across different lan- guages. Lang. <SIL> ins. rate Best PER (Viterbi) %-converged (PER < 50%) De 0.25 25.3% 66% Es 0.25 27.0% 50% Fr 0.25 0.50 49.2% 35.2% <10%"}, {"question": " What does CVSS stand for in the context of the text?", "answer": " Corpus for Voice Speech Translation", "ref_chunk": "used sacreBLEU8 to calculate the BLEU score of S2TT. For the \ufb01nal S2ST results, Whisper 9 (Radford et al., 2022), a supervised ASR model released by OpenAI, is adopted to transcribe the hypothesized audio and calculated the BLEU score. 4.5 Results We show our results in Table 1, including the results of ASR, S2TT, and S2ST. To compare our US2ST system with others\u2019 works, we collect some results from the previous studies on CoVoST 2 and CVSS ((a)\u2013(f), (h)\u2013(i)). To get better comparisons, both cascaded and direct systems are included. Next, we discuss the details of the methods in the table. (a) comes from Wang et al. (2020b); among all experiments in their paper, we only report the results of the cascaded S2TT system constructed 8https://github.com/mjpost/sacrebleu 9we used the cross-lingual large-v2 model released by https://github.com/openai/whisper Table 1: The results are evaluated on CoVoST 2 for S2TT, and CVSS for S2ST. C-T stands for cascaded S2TT; D-S for direct S2ST. Method Type Fr ASR \u2193 De Es S2TT (X\u2192En) \u2191 Es De Fr S2ST (X\u2192En) \u2191 De Fr Es SUPERVISED LEARNING (a) Wang et al. (2020b) (b) fairseq S2T (T-Sm) (Wang et al., 2020a) (c) fairseq S2T (Multi. T-Md) (d) XLS-R (2B) (Babu et al., 2021) (e) Translatotron (Jia et al., 2019) (f) Translatotron 2 (Jia et al., 2021) (g) Our upper bound UNSUPERVISED LEARNING (h) Wang et al. (2022) cascaded US2ST (i) Wang et al. (2022) end-to-end US2ST (j) Our cascaded US2ST C-S (k) Our cascaded US2ST with DBT (arti\ufb01cial noise) C-S C-T 18.3 D-T D-T D-T D-S D-S C-S - - - - 16.2 - 33.2 21.4 - - - - - 14.1 - 23.8 16.0 - - - - - 11.0 - 17.4 27.6 26.3 26.5 37.6 - - 29.5 24.4 24.2 20.0 20.8 21.0 17.1 17.5 33.6 - - 25.6 - 19.5 20.4 27.4 23.0 27.0 39.2 - - 31.0 23.4 24.0 23.8 24.5 - - - 15.5 28.3 23.8 21.6 21.2 13.4 14.4 - - - 6.9 19.7 21.8 - 13.8 14.7 - - - 14.1 23.5 26.2 21.2 20.1 16.7 17.4 by monolingual ASR and bilingual MT for fair comparison. According to the table, our US2TT performances in De\u2013En are just having small degra- dation from theirs ((j) and (k) vs (a)), indicating that our US2TT system can be comparable to some early supervised cascaded S2TT works in some language pairs. Rows (b) and (c) are the results of the direct S2TT systems from Wang et al. (2020a). They have developed a tool-kit for S2TT and demonstrated it on CoVoST 2 with different model backbones. We compare our US2TT results (row (j) and (k)) with their Transformer-based models. Our results have not only outperformed their small model ((b)) in De-En and Es-En tasks but also outperformed their large model in De-En (row (c)). Our works are comparable to some early direct S2TT systems ((b), (c)) except for the Fr-En pair. Row (d) is the SOTA direct S2TT model, which is a cross-lingual speech model based on wav2vec 2.0 architecture. Taking advantage of the large pretrained self-supervised model, the performance is signi\ufb01cantly better than all the other works in all language pairs, showing there is still a huge gap between SOTA supervised S2TT and our US2TT systems. Rows (h) and (i) come from the concurrent US2ST system (Wang et al., 2022); (h) is con- structed by concatenating UASR, UNMT, and UTTS, and (i) is an end2end S2ST systems trained on pseudo labels generated by (h). Our model ar- chitecture is similar to theirs, while they \ufb01ne-tune the wav2vec 2.0 with target languages in UASR to mitigate the domain mismatch between pretraining and downstream tasks. For Fr-En S2TT, their per- formance is superior to ours. This may be due to the dif\ufb01culty of French UASR, which has a higher error rate than other languages. This phenomenon was also observed in Wang et al. (2022), and we think that it might be more severe in our case since we did not \ufb01ne-tune the wav2vec 2.0 models on the target languages. Suffering from domain mismatch, our systems still outperform their works in Es-En S2TT. The performance of our S2ST systems drops even more than S2TT. However, this is because we did not utilize the data from CVSS when training our UTTS models for the fairness concern. Other studies ((e), (f), (h), (i)) directly trained the whole model or UTTS with the data from CVSS. Since our models are trained with LJSpeech (row (g), (j), (k)), they could have severe domain mismatch problems during inference on out-domain data. We had also investigated the cause of the performance drop in the later section and came out with the same conclusion. In spite of this, our unsupervised S2ST results still outperform Translatotron (row (j), (k) v.s. (e)) in De-En and Es-EN translation directions. Last but not least, Our cascaded system with DBT (k) outperforms (j) by 0.7 to 0.9 in both US2TT and US2ST, indicating a model can bet- ter translate the noisy input under the guidance of DBT, and thus mitigate the problem of error propa- gation in cascaded systems. 5 Analysis Stabilities of UASR cross different languages First of all, we found that the stabilities of our UASR models vary between languages. The mea- surement of the stability is by calculating the per- 6 centage of the converged rate among the models leveraging the same amount of text and speech data. We consider a UASR model is converged if its P ER < 50%. We summarize the discoveries in Table 2. According to our experiments, German and Spanish are easier to converge; while French usually can not converge well. However, we found that it might be more suitable for French UASR models to converge if we change the <SIL> token insertion rate from 0.25 to 0.5. Table 2: Stabilities of UASR across different lan- guages. Lang. <SIL> ins. rate Best PER (Viterbi) %-converged (PER < 50%) De 0.25 25.3% 66% Es 0.25 27.0% 50% Fr 0.25 0.50 49.2% 35.2% <10%"}, {"question": " What is the abbreviation C-T used for in the table?", "answer": " Cascaded S2TT", "ref_chunk": "used sacreBLEU8 to calculate the BLEU score of S2TT. For the \ufb01nal S2ST results, Whisper 9 (Radford et al., 2022), a supervised ASR model released by OpenAI, is adopted to transcribe the hypothesized audio and calculated the BLEU score. 4.5 Results We show our results in Table 1, including the results of ASR, S2TT, and S2ST. To compare our US2ST system with others\u2019 works, we collect some results from the previous studies on CoVoST 2 and CVSS ((a)\u2013(f), (h)\u2013(i)). To get better comparisons, both cascaded and direct systems are included. Next, we discuss the details of the methods in the table. (a) comes from Wang et al. (2020b); among all experiments in their paper, we only report the results of the cascaded S2TT system constructed 8https://github.com/mjpost/sacrebleu 9we used the cross-lingual large-v2 model released by https://github.com/openai/whisper Table 1: The results are evaluated on CoVoST 2 for S2TT, and CVSS for S2ST. C-T stands for cascaded S2TT; D-S for direct S2ST. Method Type Fr ASR \u2193 De Es S2TT (X\u2192En) \u2191 Es De Fr S2ST (X\u2192En) \u2191 De Fr Es SUPERVISED LEARNING (a) Wang et al. (2020b) (b) fairseq S2T (T-Sm) (Wang et al., 2020a) (c) fairseq S2T (Multi. T-Md) (d) XLS-R (2B) (Babu et al., 2021) (e) Translatotron (Jia et al., 2019) (f) Translatotron 2 (Jia et al., 2021) (g) Our upper bound UNSUPERVISED LEARNING (h) Wang et al. (2022) cascaded US2ST (i) Wang et al. (2022) end-to-end US2ST (j) Our cascaded US2ST C-S (k) Our cascaded US2ST with DBT (arti\ufb01cial noise) C-S C-T 18.3 D-T D-T D-T D-S D-S C-S - - - - 16.2 - 33.2 21.4 - - - - - 14.1 - 23.8 16.0 - - - - - 11.0 - 17.4 27.6 26.3 26.5 37.6 - - 29.5 24.4 24.2 20.0 20.8 21.0 17.1 17.5 33.6 - - 25.6 - 19.5 20.4 27.4 23.0 27.0 39.2 - - 31.0 23.4 24.0 23.8 24.5 - - - 15.5 28.3 23.8 21.6 21.2 13.4 14.4 - - - 6.9 19.7 21.8 - 13.8 14.7 - - - 14.1 23.5 26.2 21.2 20.1 16.7 17.4 by monolingual ASR and bilingual MT for fair comparison. According to the table, our US2TT performances in De\u2013En are just having small degra- dation from theirs ((j) and (k) vs (a)), indicating that our US2TT system can be comparable to some early supervised cascaded S2TT works in some language pairs. Rows (b) and (c) are the results of the direct S2TT systems from Wang et al. (2020a). They have developed a tool-kit for S2TT and demonstrated it on CoVoST 2 with different model backbones. We compare our US2TT results (row (j) and (k)) with their Transformer-based models. Our results have not only outperformed their small model ((b)) in De-En and Es-En tasks but also outperformed their large model in De-En (row (c)). Our works are comparable to some early direct S2TT systems ((b), (c)) except for the Fr-En pair. Row (d) is the SOTA direct S2TT model, which is a cross-lingual speech model based on wav2vec 2.0 architecture. Taking advantage of the large pretrained self-supervised model, the performance is signi\ufb01cantly better than all the other works in all language pairs, showing there is still a huge gap between SOTA supervised S2TT and our US2TT systems. Rows (h) and (i) come from the concurrent US2ST system (Wang et al., 2022); (h) is con- structed by concatenating UASR, UNMT, and UTTS, and (i) is an end2end S2ST systems trained on pseudo labels generated by (h). Our model ar- chitecture is similar to theirs, while they \ufb01ne-tune the wav2vec 2.0 with target languages in UASR to mitigate the domain mismatch between pretraining and downstream tasks. For Fr-En S2TT, their per- formance is superior to ours. This may be due to the dif\ufb01culty of French UASR, which has a higher error rate than other languages. This phenomenon was also observed in Wang et al. (2022), and we think that it might be more severe in our case since we did not \ufb01ne-tune the wav2vec 2.0 models on the target languages. Suffering from domain mismatch, our systems still outperform their works in Es-En S2TT. The performance of our S2ST systems drops even more than S2TT. However, this is because we did not utilize the data from CVSS when training our UTTS models for the fairness concern. Other studies ((e), (f), (h), (i)) directly trained the whole model or UTTS with the data from CVSS. Since our models are trained with LJSpeech (row (g), (j), (k)), they could have severe domain mismatch problems during inference on out-domain data. We had also investigated the cause of the performance drop in the later section and came out with the same conclusion. In spite of this, our unsupervised S2ST results still outperform Translatotron (row (j), (k) v.s. (e)) in De-En and Es-EN translation directions. Last but not least, Our cascaded system with DBT (k) outperforms (j) by 0.7 to 0.9 in both US2TT and US2ST, indicating a model can bet- ter translate the noisy input under the guidance of DBT, and thus mitigate the problem of error propa- gation in cascaded systems. 5 Analysis Stabilities of UASR cross different languages First of all, we found that the stabilities of our UASR models vary between languages. The mea- surement of the stability is by calculating the per- 6 centage of the converged rate among the models leveraging the same amount of text and speech data. We consider a UASR model is converged if its P ER < 50%. We summarize the discoveries in Table 2. According to our experiments, German and Spanish are easier to converge; while French usually can not converge well. However, we found that it might be more suitable for French UASR models to converge if we change the <SIL> token insertion rate from 0.25 to 0.5. Table 2: Stabilities of UASR across different lan- guages. Lang. <SIL> ins. rate Best PER (Viterbi) %-converged (PER < 50%) De 0.25 25.3% 66% Es 0.25 27.0% 50% Fr 0.25 0.50 49.2% 35.2% <10%"}, {"question": " Which model is considered the SOTA direct S2TT model in the text?", "answer": " XLS-R (2B)", "ref_chunk": "used sacreBLEU8 to calculate the BLEU score of S2TT. For the \ufb01nal S2ST results, Whisper 9 (Radford et al., 2022), a supervised ASR model released by OpenAI, is adopted to transcribe the hypothesized audio and calculated the BLEU score. 4.5 Results We show our results in Table 1, including the results of ASR, S2TT, and S2ST. To compare our US2ST system with others\u2019 works, we collect some results from the previous studies on CoVoST 2 and CVSS ((a)\u2013(f), (h)\u2013(i)). To get better comparisons, both cascaded and direct systems are included. Next, we discuss the details of the methods in the table. (a) comes from Wang et al. (2020b); among all experiments in their paper, we only report the results of the cascaded S2TT system constructed 8https://github.com/mjpost/sacrebleu 9we used the cross-lingual large-v2 model released by https://github.com/openai/whisper Table 1: The results are evaluated on CoVoST 2 for S2TT, and CVSS for S2ST. C-T stands for cascaded S2TT; D-S for direct S2ST. Method Type Fr ASR \u2193 De Es S2TT (X\u2192En) \u2191 Es De Fr S2ST (X\u2192En) \u2191 De Fr Es SUPERVISED LEARNING (a) Wang et al. (2020b) (b) fairseq S2T (T-Sm) (Wang et al., 2020a) (c) fairseq S2T (Multi. T-Md) (d) XLS-R (2B) (Babu et al., 2021) (e) Translatotron (Jia et al., 2019) (f) Translatotron 2 (Jia et al., 2021) (g) Our upper bound UNSUPERVISED LEARNING (h) Wang et al. (2022) cascaded US2ST (i) Wang et al. (2022) end-to-end US2ST (j) Our cascaded US2ST C-S (k) Our cascaded US2ST with DBT (arti\ufb01cial noise) C-S C-T 18.3 D-T D-T D-T D-S D-S C-S - - - - 16.2 - 33.2 21.4 - - - - - 14.1 - 23.8 16.0 - - - - - 11.0 - 17.4 27.6 26.3 26.5 37.6 - - 29.5 24.4 24.2 20.0 20.8 21.0 17.1 17.5 33.6 - - 25.6 - 19.5 20.4 27.4 23.0 27.0 39.2 - - 31.0 23.4 24.0 23.8 24.5 - - - 15.5 28.3 23.8 21.6 21.2 13.4 14.4 - - - 6.9 19.7 21.8 - 13.8 14.7 - - - 14.1 23.5 26.2 21.2 20.1 16.7 17.4 by monolingual ASR and bilingual MT for fair comparison. According to the table, our US2TT performances in De\u2013En are just having small degra- dation from theirs ((j) and (k) vs (a)), indicating that our US2TT system can be comparable to some early supervised cascaded S2TT works in some language pairs. Rows (b) and (c) are the results of the direct S2TT systems from Wang et al. (2020a). They have developed a tool-kit for S2TT and demonstrated it on CoVoST 2 with different model backbones. We compare our US2TT results (row (j) and (k)) with their Transformer-based models. Our results have not only outperformed their small model ((b)) in De-En and Es-En tasks but also outperformed their large model in De-En (row (c)). Our works are comparable to some early direct S2TT systems ((b), (c)) except for the Fr-En pair. Row (d) is the SOTA direct S2TT model, which is a cross-lingual speech model based on wav2vec 2.0 architecture. Taking advantage of the large pretrained self-supervised model, the performance is signi\ufb01cantly better than all the other works in all language pairs, showing there is still a huge gap between SOTA supervised S2TT and our US2TT systems. Rows (h) and (i) come from the concurrent US2ST system (Wang et al., 2022); (h) is con- structed by concatenating UASR, UNMT, and UTTS, and (i) is an end2end S2ST systems trained on pseudo labels generated by (h). Our model ar- chitecture is similar to theirs, while they \ufb01ne-tune the wav2vec 2.0 with target languages in UASR to mitigate the domain mismatch between pretraining and downstream tasks. For Fr-En S2TT, their per- formance is superior to ours. This may be due to the dif\ufb01culty of French UASR, which has a higher error rate than other languages. This phenomenon was also observed in Wang et al. (2022), and we think that it might be more severe in our case since we did not \ufb01ne-tune the wav2vec 2.0 models on the target languages. Suffering from domain mismatch, our systems still outperform their works in Es-En S2TT. The performance of our S2ST systems drops even more than S2TT. However, this is because we did not utilize the data from CVSS when training our UTTS models for the fairness concern. Other studies ((e), (f), (h), (i)) directly trained the whole model or UTTS with the data from CVSS. Since our models are trained with LJSpeech (row (g), (j), (k)), they could have severe domain mismatch problems during inference on out-domain data. We had also investigated the cause of the performance drop in the later section and came out with the same conclusion. In spite of this, our unsupervised S2ST results still outperform Translatotron (row (j), (k) v.s. (e)) in De-En and Es-EN translation directions. Last but not least, Our cascaded system with DBT (k) outperforms (j) by 0.7 to 0.9 in both US2TT and US2ST, indicating a model can bet- ter translate the noisy input under the guidance of DBT, and thus mitigate the problem of error propa- gation in cascaded systems. 5 Analysis Stabilities of UASR cross different languages First of all, we found that the stabilities of our UASR models vary between languages. The mea- surement of the stability is by calculating the per- 6 centage of the converged rate among the models leveraging the same amount of text and speech data. We consider a UASR model is converged if its P ER < 50%. We summarize the discoveries in Table 2. According to our experiments, German and Spanish are easier to converge; while French usually can not converge well. However, we found that it might be more suitable for French UASR models to converge if we change the <SIL> token insertion rate from 0.25 to 0.5. Table 2: Stabilities of UASR across different lan- guages. Lang. <SIL> ins. rate Best PER (Viterbi) %-converged (PER < 50%) De 0.25 25.3% 66% Es 0.25 27.0% 50% Fr 0.25 0.50 49.2% 35.2% <10%"}, {"question": " What were the components of the concurrent US2ST system referred to as (h) in the text?", "answer": " UASR, UNMT, UTTS", "ref_chunk": "used sacreBLEU8 to calculate the BLEU score of S2TT. For the \ufb01nal S2ST results, Whisper 9 (Radford et al., 2022), a supervised ASR model released by OpenAI, is adopted to transcribe the hypothesized audio and calculated the BLEU score. 4.5 Results We show our results in Table 1, including the results of ASR, S2TT, and S2ST. To compare our US2ST system with others\u2019 works, we collect some results from the previous studies on CoVoST 2 and CVSS ((a)\u2013(f), (h)\u2013(i)). To get better comparisons, both cascaded and direct systems are included. Next, we discuss the details of the methods in the table. (a) comes from Wang et al. (2020b); among all experiments in their paper, we only report the results of the cascaded S2TT system constructed 8https://github.com/mjpost/sacrebleu 9we used the cross-lingual large-v2 model released by https://github.com/openai/whisper Table 1: The results are evaluated on CoVoST 2 for S2TT, and CVSS for S2ST. C-T stands for cascaded S2TT; D-S for direct S2ST. Method Type Fr ASR \u2193 De Es S2TT (X\u2192En) \u2191 Es De Fr S2ST (X\u2192En) \u2191 De Fr Es SUPERVISED LEARNING (a) Wang et al. (2020b) (b) fairseq S2T (T-Sm) (Wang et al., 2020a) (c) fairseq S2T (Multi. T-Md) (d) XLS-R (2B) (Babu et al., 2021) (e) Translatotron (Jia et al., 2019) (f) Translatotron 2 (Jia et al., 2021) (g) Our upper bound UNSUPERVISED LEARNING (h) Wang et al. (2022) cascaded US2ST (i) Wang et al. (2022) end-to-end US2ST (j) Our cascaded US2ST C-S (k) Our cascaded US2ST with DBT (arti\ufb01cial noise) C-S C-T 18.3 D-T D-T D-T D-S D-S C-S - - - - 16.2 - 33.2 21.4 - - - - - 14.1 - 23.8 16.0 - - - - - 11.0 - 17.4 27.6 26.3 26.5 37.6 - - 29.5 24.4 24.2 20.0 20.8 21.0 17.1 17.5 33.6 - - 25.6 - 19.5 20.4 27.4 23.0 27.0 39.2 - - 31.0 23.4 24.0 23.8 24.5 - - - 15.5 28.3 23.8 21.6 21.2 13.4 14.4 - - - 6.9 19.7 21.8 - 13.8 14.7 - - - 14.1 23.5 26.2 21.2 20.1 16.7 17.4 by monolingual ASR and bilingual MT for fair comparison. According to the table, our US2TT performances in De\u2013En are just having small degra- dation from theirs ((j) and (k) vs (a)), indicating that our US2TT system can be comparable to some early supervised cascaded S2TT works in some language pairs. Rows (b) and (c) are the results of the direct S2TT systems from Wang et al. (2020a). They have developed a tool-kit for S2TT and demonstrated it on CoVoST 2 with different model backbones. We compare our US2TT results (row (j) and (k)) with their Transformer-based models. Our results have not only outperformed their small model ((b)) in De-En and Es-En tasks but also outperformed their large model in De-En (row (c)). Our works are comparable to some early direct S2TT systems ((b), (c)) except for the Fr-En pair. Row (d) is the SOTA direct S2TT model, which is a cross-lingual speech model based on wav2vec 2.0 architecture. Taking advantage of the large pretrained self-supervised model, the performance is signi\ufb01cantly better than all the other works in all language pairs, showing there is still a huge gap between SOTA supervised S2TT and our US2TT systems. Rows (h) and (i) come from the concurrent US2ST system (Wang et al., 2022); (h) is con- structed by concatenating UASR, UNMT, and UTTS, and (i) is an end2end S2ST systems trained on pseudo labels generated by (h). Our model ar- chitecture is similar to theirs, while they \ufb01ne-tune the wav2vec 2.0 with target languages in UASR to mitigate the domain mismatch between pretraining and downstream tasks. For Fr-En S2TT, their per- formance is superior to ours. This may be due to the dif\ufb01culty of French UASR, which has a higher error rate than other languages. This phenomenon was also observed in Wang et al. (2022), and we think that it might be more severe in our case since we did not \ufb01ne-tune the wav2vec 2.0 models on the target languages. Suffering from domain mismatch, our systems still outperform their works in Es-En S2TT. The performance of our S2ST systems drops even more than S2TT. However, this is because we did not utilize the data from CVSS when training our UTTS models for the fairness concern. Other studies ((e), (f), (h), (i)) directly trained the whole model or UTTS with the data from CVSS. Since our models are trained with LJSpeech (row (g), (j), (k)), they could have severe domain mismatch problems during inference on out-domain data. We had also investigated the cause of the performance drop in the later section and came out with the same conclusion. In spite of this, our unsupervised S2ST results still outperform Translatotron (row (j), (k) v.s. (e)) in De-En and Es-EN translation directions. Last but not least, Our cascaded system with DBT (k) outperforms (j) by 0.7 to 0.9 in both US2TT and US2ST, indicating a model can bet- ter translate the noisy input under the guidance of DBT, and thus mitigate the problem of error propa- gation in cascaded systems. 5 Analysis Stabilities of UASR cross different languages First of all, we found that the stabilities of our UASR models vary between languages. The mea- surement of the stability is by calculating the per- 6 centage of the converged rate among the models leveraging the same amount of text and speech data. We consider a UASR model is converged if its P ER < 50%. We summarize the discoveries in Table 2. According to our experiments, German and Spanish are easier to converge; while French usually can not converge well. However, we found that it might be more suitable for French UASR models to converge if we change the <SIL> token insertion rate from 0.25 to 0.5. Table 2: Stabilities of UASR across different lan- guages. Lang. <SIL> ins. rate Best PER (Viterbi) %-converged (PER < 50%) De 0.25 25.3% 66% Es 0.25 27.0% 50% Fr 0.25 0.50 49.2% 35.2% <10%"}, {"question": " Why did the performance of the S2ST systems drop compared to S2TT systems according to the text?", "answer": " Due to not utilizing data from CVSS when training UTTS models", "ref_chunk": "used sacreBLEU8 to calculate the BLEU score of S2TT. For the \ufb01nal S2ST results, Whisper 9 (Radford et al., 2022), a supervised ASR model released by OpenAI, is adopted to transcribe the hypothesized audio and calculated the BLEU score. 4.5 Results We show our results in Table 1, including the results of ASR, S2TT, and S2ST. To compare our US2ST system with others\u2019 works, we collect some results from the previous studies on CoVoST 2 and CVSS ((a)\u2013(f), (h)\u2013(i)). To get better comparisons, both cascaded and direct systems are included. Next, we discuss the details of the methods in the table. (a) comes from Wang et al. (2020b); among all experiments in their paper, we only report the results of the cascaded S2TT system constructed 8https://github.com/mjpost/sacrebleu 9we used the cross-lingual large-v2 model released by https://github.com/openai/whisper Table 1: The results are evaluated on CoVoST 2 for S2TT, and CVSS for S2ST. C-T stands for cascaded S2TT; D-S for direct S2ST. Method Type Fr ASR \u2193 De Es S2TT (X\u2192En) \u2191 Es De Fr S2ST (X\u2192En) \u2191 De Fr Es SUPERVISED LEARNING (a) Wang et al. (2020b) (b) fairseq S2T (T-Sm) (Wang et al., 2020a) (c) fairseq S2T (Multi. T-Md) (d) XLS-R (2B) (Babu et al., 2021) (e) Translatotron (Jia et al., 2019) (f) Translatotron 2 (Jia et al., 2021) (g) Our upper bound UNSUPERVISED LEARNING (h) Wang et al. (2022) cascaded US2ST (i) Wang et al. (2022) end-to-end US2ST (j) Our cascaded US2ST C-S (k) Our cascaded US2ST with DBT (arti\ufb01cial noise) C-S C-T 18.3 D-T D-T D-T D-S D-S C-S - - - - 16.2 - 33.2 21.4 - - - - - 14.1 - 23.8 16.0 - - - - - 11.0 - 17.4 27.6 26.3 26.5 37.6 - - 29.5 24.4 24.2 20.0 20.8 21.0 17.1 17.5 33.6 - - 25.6 - 19.5 20.4 27.4 23.0 27.0 39.2 - - 31.0 23.4 24.0 23.8 24.5 - - - 15.5 28.3 23.8 21.6 21.2 13.4 14.4 - - - 6.9 19.7 21.8 - 13.8 14.7 - - - 14.1 23.5 26.2 21.2 20.1 16.7 17.4 by monolingual ASR and bilingual MT for fair comparison. According to the table, our US2TT performances in De\u2013En are just having small degra- dation from theirs ((j) and (k) vs (a)), indicating that our US2TT system can be comparable to some early supervised cascaded S2TT works in some language pairs. Rows (b) and (c) are the results of the direct S2TT systems from Wang et al. (2020a). They have developed a tool-kit for S2TT and demonstrated it on CoVoST 2 with different model backbones. We compare our US2TT results (row (j) and (k)) with their Transformer-based models. Our results have not only outperformed their small model ((b)) in De-En and Es-En tasks but also outperformed their large model in De-En (row (c)). Our works are comparable to some early direct S2TT systems ((b), (c)) except for the Fr-En pair. Row (d) is the SOTA direct S2TT model, which is a cross-lingual speech model based on wav2vec 2.0 architecture. Taking advantage of the large pretrained self-supervised model, the performance is signi\ufb01cantly better than all the other works in all language pairs, showing there is still a huge gap between SOTA supervised S2TT and our US2TT systems. Rows (h) and (i) come from the concurrent US2ST system (Wang et al., 2022); (h) is con- structed by concatenating UASR, UNMT, and UTTS, and (i) is an end2end S2ST systems trained on pseudo labels generated by (h). Our model ar- chitecture is similar to theirs, while they \ufb01ne-tune the wav2vec 2.0 with target languages in UASR to mitigate the domain mismatch between pretraining and downstream tasks. For Fr-En S2TT, their per- formance is superior to ours. This may be due to the dif\ufb01culty of French UASR, which has a higher error rate than other languages. This phenomenon was also observed in Wang et al. (2022), and we think that it might be more severe in our case since we did not \ufb01ne-tune the wav2vec 2.0 models on the target languages. Suffering from domain mismatch, our systems still outperform their works in Es-En S2TT. The performance of our S2ST systems drops even more than S2TT. However, this is because we did not utilize the data from CVSS when training our UTTS models for the fairness concern. Other studies ((e), (f), (h), (i)) directly trained the whole model or UTTS with the data from CVSS. Since our models are trained with LJSpeech (row (g), (j), (k)), they could have severe domain mismatch problems during inference on out-domain data. We had also investigated the cause of the performance drop in the later section and came out with the same conclusion. In spite of this, our unsupervised S2ST results still outperform Translatotron (row (j), (k) v.s. (e)) in De-En and Es-EN translation directions. Last but not least, Our cascaded system with DBT (k) outperforms (j) by 0.7 to 0.9 in both US2TT and US2ST, indicating a model can bet- ter translate the noisy input under the guidance of DBT, and thus mitigate the problem of error propa- gation in cascaded systems. 5 Analysis Stabilities of UASR cross different languages First of all, we found that the stabilities of our UASR models vary between languages. The mea- surement of the stability is by calculating the per- 6 centage of the converged rate among the models leveraging the same amount of text and speech data. We consider a UASR model is converged if its P ER < 50%. We summarize the discoveries in Table 2. According to our experiments, German and Spanish are easier to converge; while French usually can not converge well. However, we found that it might be more suitable for French UASR models to converge if we change the <SIL> token insertion rate from 0.25 to 0.5. Table 2: Stabilities of UASR across different lan- guages. Lang. <SIL> ins. rate Best PER (Viterbi) %-converged (PER < 50%) De 0.25 25.3% 66% Es 0.25 27.0% 50% Fr 0.25 0.50 49.2% 35.2% <10%"}, {"question": " How does the performance of the unsupervised S2ST systems compare to Translatotron in De-En and Es-En translation directions?", "answer": " Outperforms Translatotron", "ref_chunk": "used sacreBLEU8 to calculate the BLEU score of S2TT. For the \ufb01nal S2ST results, Whisper 9 (Radford et al., 2022), a supervised ASR model released by OpenAI, is adopted to transcribe the hypothesized audio and calculated the BLEU score. 4.5 Results We show our results in Table 1, including the results of ASR, S2TT, and S2ST. To compare our US2ST system with others\u2019 works, we collect some results from the previous studies on CoVoST 2 and CVSS ((a)\u2013(f), (h)\u2013(i)). To get better comparisons, both cascaded and direct systems are included. Next, we discuss the details of the methods in the table. (a) comes from Wang et al. (2020b); among all experiments in their paper, we only report the results of the cascaded S2TT system constructed 8https://github.com/mjpost/sacrebleu 9we used the cross-lingual large-v2 model released by https://github.com/openai/whisper Table 1: The results are evaluated on CoVoST 2 for S2TT, and CVSS for S2ST. C-T stands for cascaded S2TT; D-S for direct S2ST. Method Type Fr ASR \u2193 De Es S2TT (X\u2192En) \u2191 Es De Fr S2ST (X\u2192En) \u2191 De Fr Es SUPERVISED LEARNING (a) Wang et al. (2020b) (b) fairseq S2T (T-Sm) (Wang et al., 2020a) (c) fairseq S2T (Multi. T-Md) (d) XLS-R (2B) (Babu et al., 2021) (e) Translatotron (Jia et al., 2019) (f) Translatotron 2 (Jia et al., 2021) (g) Our upper bound UNSUPERVISED LEARNING (h) Wang et al. (2022) cascaded US2ST (i) Wang et al. (2022) end-to-end US2ST (j) Our cascaded US2ST C-S (k) Our cascaded US2ST with DBT (arti\ufb01cial noise) C-S C-T 18.3 D-T D-T D-T D-S D-S C-S - - - - 16.2 - 33.2 21.4 - - - - - 14.1 - 23.8 16.0 - - - - - 11.0 - 17.4 27.6 26.3 26.5 37.6 - - 29.5 24.4 24.2 20.0 20.8 21.0 17.1 17.5 33.6 - - 25.6 - 19.5 20.4 27.4 23.0 27.0 39.2 - - 31.0 23.4 24.0 23.8 24.5 - - - 15.5 28.3 23.8 21.6 21.2 13.4 14.4 - - - 6.9 19.7 21.8 - 13.8 14.7 - - - 14.1 23.5 26.2 21.2 20.1 16.7 17.4 by monolingual ASR and bilingual MT for fair comparison. According to the table, our US2TT performances in De\u2013En are just having small degra- dation from theirs ((j) and (k) vs (a)), indicating that our US2TT system can be comparable to some early supervised cascaded S2TT works in some language pairs. Rows (b) and (c) are the results of the direct S2TT systems from Wang et al. (2020a). They have developed a tool-kit for S2TT and demonstrated it on CoVoST 2 with different model backbones. We compare our US2TT results (row (j) and (k)) with their Transformer-based models. Our results have not only outperformed their small model ((b)) in De-En and Es-En tasks but also outperformed their large model in De-En (row (c)). Our works are comparable to some early direct S2TT systems ((b), (c)) except for the Fr-En pair. Row (d) is the SOTA direct S2TT model, which is a cross-lingual speech model based on wav2vec 2.0 architecture. Taking advantage of the large pretrained self-supervised model, the performance is signi\ufb01cantly better than all the other works in all language pairs, showing there is still a huge gap between SOTA supervised S2TT and our US2TT systems. Rows (h) and (i) come from the concurrent US2ST system (Wang et al., 2022); (h) is con- structed by concatenating UASR, UNMT, and UTTS, and (i) is an end2end S2ST systems trained on pseudo labels generated by (h). Our model ar- chitecture is similar to theirs, while they \ufb01ne-tune the wav2vec 2.0 with target languages in UASR to mitigate the domain mismatch between pretraining and downstream tasks. For Fr-En S2TT, their per- formance is superior to ours. This may be due to the dif\ufb01culty of French UASR, which has a higher error rate than other languages. This phenomenon was also observed in Wang et al. (2022), and we think that it might be more severe in our case since we did not \ufb01ne-tune the wav2vec 2.0 models on the target languages. Suffering from domain mismatch, our systems still outperform their works in Es-En S2TT. The performance of our S2ST systems drops even more than S2TT. However, this is because we did not utilize the data from CVSS when training our UTTS models for the fairness concern. Other studies ((e), (f), (h), (i)) directly trained the whole model or UTTS with the data from CVSS. Since our models are trained with LJSpeech (row (g), (j), (k)), they could have severe domain mismatch problems during inference on out-domain data. We had also investigated the cause of the performance drop in the later section and came out with the same conclusion. In spite of this, our unsupervised S2ST results still outperform Translatotron (row (j), (k) v.s. (e)) in De-En and Es-EN translation directions. Last but not least, Our cascaded system with DBT (k) outperforms (j) by 0.7 to 0.9 in both US2TT and US2ST, indicating a model can bet- ter translate the noisy input under the guidance of DBT, and thus mitigate the problem of error propa- gation in cascaded systems. 5 Analysis Stabilities of UASR cross different languages First of all, we found that the stabilities of our UASR models vary between languages. The mea- surement of the stability is by calculating the per- 6 centage of the converged rate among the models leveraging the same amount of text and speech data. We consider a UASR model is converged if its P ER < 50%. We summarize the discoveries in Table 2. According to our experiments, German and Spanish are easier to converge; while French usually can not converge well. However, we found that it might be more suitable for French UASR models to converge if we change the <SIL> token insertion rate from 0.25 to 0.5. Table 2: Stabilities of UASR across different lan- guages. Lang. <SIL> ins. rate Best PER (Viterbi) %-converged (PER < 50%) De 0.25 25.3% 66% Es 0.25 27.0% 50% Fr 0.25 0.50 49.2% 35.2% <10%"}], "doc_text": "used sacreBLEU8 to calculate the BLEU score of S2TT. For the \ufb01nal S2ST results, Whisper 9 (Radford et al., 2022), a supervised ASR model released by OpenAI, is adopted to transcribe the hypothesized audio and calculated the BLEU score. 4.5 Results We show our results in Table 1, including the results of ASR, S2TT, and S2ST. To compare our US2ST system with others\u2019 works, we collect some results from the previous studies on CoVoST 2 and CVSS ((a)\u2013(f), (h)\u2013(i)). To get better comparisons, both cascaded and direct systems are included. Next, we discuss the details of the methods in the table. (a) comes from Wang et al. (2020b); among all experiments in their paper, we only report the results of the cascaded S2TT system constructed 8https://github.com/mjpost/sacrebleu 9we used the cross-lingual large-v2 model released by https://github.com/openai/whisper Table 1: The results are evaluated on CoVoST 2 for S2TT, and CVSS for S2ST. C-T stands for cascaded S2TT; D-S for direct S2ST. Method Type Fr ASR \u2193 De Es S2TT (X\u2192En) \u2191 Es De Fr S2ST (X\u2192En) \u2191 De Fr Es SUPERVISED LEARNING (a) Wang et al. (2020b) (b) fairseq S2T (T-Sm) (Wang et al., 2020a) (c) fairseq S2T (Multi. T-Md) (d) XLS-R (2B) (Babu et al., 2021) (e) Translatotron (Jia et al., 2019) (f) Translatotron 2 (Jia et al., 2021) (g) Our upper bound UNSUPERVISED LEARNING (h) Wang et al. (2022) cascaded US2ST (i) Wang et al. (2022) end-to-end US2ST (j) Our cascaded US2ST C-S (k) Our cascaded US2ST with DBT (arti\ufb01cial noise) C-S C-T 18.3 D-T D-T D-T D-S D-S C-S - - - - 16.2 - 33.2 21.4 - - - - - 14.1 - 23.8 16.0 - - - - - 11.0 - 17.4 27.6 26.3 26.5 37.6 - - 29.5 24.4 24.2 20.0 20.8 21.0 17.1 17.5 33.6 - - 25.6 - 19.5 20.4 27.4 23.0 27.0 39.2 - - 31.0 23.4 24.0 23.8 24.5 - - - 15.5 28.3 23.8 21.6 21.2 13.4 14.4 - - - 6.9 19.7 21.8 - 13.8 14.7 - - - 14.1 23.5 26.2 21.2 20.1 16.7 17.4 by monolingual ASR and bilingual MT for fair comparison. According to the table, our US2TT performances in De\u2013En are just having small degra- dation from theirs ((j) and (k) vs (a)), indicating that our US2TT system can be comparable to some early supervised cascaded S2TT works in some language pairs. Rows (b) and (c) are the results of the direct S2TT systems from Wang et al. (2020a). They have developed a tool-kit for S2TT and demonstrated it on CoVoST 2 with different model backbones. We compare our US2TT results (row (j) and (k)) with their Transformer-based models. Our results have not only outperformed their small model ((b)) in De-En and Es-En tasks but also outperformed their large model in De-En (row (c)). Our works are comparable to some early direct S2TT systems ((b), (c)) except for the Fr-En pair. Row (d) is the SOTA direct S2TT model, which is a cross-lingual speech model based on wav2vec 2.0 architecture. Taking advantage of the large pretrained self-supervised model, the performance is signi\ufb01cantly better than all the other works in all language pairs, showing there is still a huge gap between SOTA supervised S2TT and our US2TT systems. Rows (h) and (i) come from the concurrent US2ST system (Wang et al., 2022); (h) is con- structed by concatenating UASR, UNMT, and UTTS, and (i) is an end2end S2ST systems trained on pseudo labels generated by (h). Our model ar- chitecture is similar to theirs, while they \ufb01ne-tune the wav2vec 2.0 with target languages in UASR to mitigate the domain mismatch between pretraining and downstream tasks. For Fr-En S2TT, their per- formance is superior to ours. This may be due to the dif\ufb01culty of French UASR, which has a higher error rate than other languages. This phenomenon was also observed in Wang et al. (2022), and we think that it might be more severe in our case since we did not \ufb01ne-tune the wav2vec 2.0 models on the target languages. Suffering from domain mismatch, our systems still outperform their works in Es-En S2TT. The performance of our S2ST systems drops even more than S2TT. However, this is because we did not utilize the data from CVSS when training our UTTS models for the fairness concern. Other studies ((e), (f), (h), (i)) directly trained the whole model or UTTS with the data from CVSS. Since our models are trained with LJSpeech (row (g), (j), (k)), they could have severe domain mismatch problems during inference on out-domain data. We had also investigated the cause of the performance drop in the later section and came out with the same conclusion. In spite of this, our unsupervised S2ST results still outperform Translatotron (row (j), (k) v.s. (e)) in De-En and Es-EN translation directions. Last but not least, Our cascaded system with DBT (k) outperforms (j) by 0.7 to 0.9 in both US2TT and US2ST, indicating a model can bet- ter translate the noisy input under the guidance of DBT, and thus mitigate the problem of error propa- gation in cascaded systems. 5 Analysis Stabilities of UASR cross different languages First of all, we found that the stabilities of our UASR models vary between languages. The mea- surement of the stability is by calculating the per- 6 centage of the converged rate among the models leveraging the same amount of text and speech data. We consider a UASR model is converged if its P ER < 50%. We summarize the discoveries in Table 2. According to our experiments, German and Spanish are easier to converge; while French usually can not converge well. However, we found that it might be more suitable for French UASR models to converge if we change the <SIL> token insertion rate from 0.25 to 0.5. Table 2: Stabilities of UASR across different lan- guages. Lang. <SIL> ins. rate Best PER (Viterbi) %-converged (PER < 50%) De 0.25 25.3% 66% Es 0.25 27.0% 50% Fr 0.25 0.50 49.2% 35.2% <10%"}