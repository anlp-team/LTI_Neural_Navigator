{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_ESPnet-ST-v2:_Multipurpose_Spoken_Language_Translation_Toolkit_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What type of encoders are used for encoding speech in the model?", "answer": " Conformer encoders", "ref_chunk": "above, following Yan et al. (2022). We use Con- former encoders with hierarchical CTC for encod- ing speech and Transformer encoders for encoding intermediate ASR text. We use Transformer de- coders for both ASR and ST. During inference, the ASR stage is decoded first and then the final MT/ST stage is decoded; both stages use label-synchronous joint CTC/attention beam search. 4.2 SST Models Time-Synchronous Blockwise CTC/Attention (TBCA) As shown in Figure 3, we adapt the aforementioned CTC/attention model for ST (\u00a74.1) Figure 4: Discrete Multi-Decoder (UnitY) for S2ST. to SST by replacing the Conformer encoder with a contextual block Conformer (Tsunoo et al., 2021). During inference, we initially followed Deng et al. (2022) and used the label-synchronous CTC/attention beam search originally proposed for ASR by Tsunoo et al. (2021). However, we found that label-synchrony results in overly conser- vative boundary block detection for SST. Therefore we opt instead for the time-synchronous variant which relies on CTC\u2019s more robust end-detection (Yan et al., 2023) to control boundary block de- tection; this change reduces latency without sacri- ficing quality. To perform incremental decoding without re-translation (as expected by SimulEval), hypotheses are pruned after processing all of the time steps for each encoder block. Blockwise Transducer (BT) As demonstrated by Xue et al. (2022), Transducers can be effec- tively applied to SST despite the monotonic na- ture of their underlying alignment model. We build Transducers for SST using contextual block Conformer encoders and unidirectional LSTM de- coders. We found that the aforementioned hier- archical CTC encoding (\u00a74.1) improves training stability and convergence rate. During inference, we found that the time-synchronous algorithm de- scribed by Saon et al. (2020) outperformed the original Graves decoding (Graves, 2012) and the later proposed alignment-synchronous algorithms (Saon et al., 2020). We also found that length nor- malization is required to avoid overly short outputs. Incremental decoding is applied in the same man- ner as for TBCA. 4.3 S2ST Models Spectral Multi-Decoder (Translatotron 2) Sim- ilar to the MCA model for ST (\u00a74.1), the spec- tral Multi-decoder (Jia et al., 2022a) decomposes S2ST into ST and TTS sub-tasks. The ST sub- TOOLKIT MODEL TYPE DE ES FR avg OFFLINE SPEECH TRANSLATION (ST) BLEU \u2191 Attentional Enc-Dec (AED) NeurST (Zhao et al., 2021) Fairseq (Wang et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v1 (Inaguma et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v2 (this work) Multi-Decoder CTC/Attn (MCA) 22.8 22.7 22.9 27.9 27.4 27.2 28.0 32.1 33.3 32.9 32.8 38.5 27.8 27.6 27.9 32.8 SIMULTANEOUS SPEECH TRANSLATION (SST) BLEU \u2191 / AL \u2193 Fairseq (Wang et al., 2020) ESPnet-ST-v2 (this work) Wait-K Attentional Enc-Dec (WAED) Time-Sync Blockwise CTC/Attn (TBCA) 18.6 / 6.8 23.5 / 2.3 22.9 / 6.9 29.2 / 2.4 28.5 / 6.7 32.7 / 2.3 23.3 / 6.8 28.5 / 2.3 OFFLINE SPEECH-TO-SPEECH TRANSLATION (S2ST) ASR-BLEU \u2191 Fairseq (Inaguma et al., 2022) ESPnet-ST-v2 (this work) Discrete Multi-Decoder (UnitY) Discrete Multi-Decoder (UnitY) 25.5 23.7 32.3 32.0 30.9 33.1 29.6 29.6 Table 2: Overview of ESPnet-ST-v2\u2019s ST, SST, and S2ST performances compared to other open-source toolkits. Results are presented on MuST-C-v1 (English-to-X) for ST/SST and on CVSS-C (X-to-English) for S2ST. task is modeled with an encoder-decoder network while the TTS sub-task is modeled with an auto- regressive synthesizer. The synthesizer attends over both the ST-encoder and ST-decoder hidden states. We use Transformers for the ST encoder-decoder and a Tacotron-style (Wang et al., 2017) decoder as the synthesizer. During inference, we first use beam search for the ST sub-task and then auto- regressively generate Mel-spectrograms. The final waveform speech is generated with a HiFi-GAN vocoder (Kong et al., 2020). MODEL HIERENC BLEU\u2191 Attn Enc-Dec (AED) Multi-Decoder Attn Enc-Dec (MAED) CTC/Attention (CA) Multi-Decoder CTC/Attn (MCA) Transducer (T) - \u2713 \u2713 \u2713 25.7 27.6 28.6 28.8 27.6 Table 3: Example ST models \u2013 results on MuST-C-v2 En-De tst-COMMON. 5.1 Experimental Setup Discrete Multi-Decoder (UnitY) The UnitY model (Inaguma et al., 2022) is similar to Trans- latotron 2, but critically predicts discrete units of speech SSL representations rather than spectral information in the final stage. In other words, UnitY is Multi-decoder consisting of a ST sub- task followed by a text-to-unit (T2U) sub-task (see Figure 4). We use Transformer-based encoder- decoders for both sub-tasks. During inference, the ST stage is first decoded and then followed by the T2U stage. Both stages use label synchronous beam search. The final speech is generated with a unit HiFi-GAN vocoder with Fastspeech-like du- ration prediction (Polyak et al., 2021; Lee et al., 2022a), which is separately trained in the Parallel- WaveGAN toolkit (Hayashi et al., 2020, 2021). Please refer to \u00a7A.1 for reproducibility details. The following is only a summary of our setup. Data We use MuST-C-v1 or MuST-C-v2 (Di Gangi et al., 2019) for ST/SST and CVSS-C for S2ST (Jia et al., 2022b). For IWSLT compar- isons, we combine MuST-C-v1, MuST-C-v2, and ST-TED (Niehues et al., 2018) for ST/SST. Models Unless otherwise indicated, we use a \"base\" setting for our models. Our base models have 40-80M trainable parameters across all tasks and are trained on a \u223c400h of single language pair data from a single corpus. For ST/SST, we also use a \"large\" setting for benchmarking against IWSLT submissions. Our large models have 150-200M trainable parameters and are trained on \u223c1000h of single language pair data from multiple corpora. 5 Performance Benchmarking In this section, we 1) compare open-source toolkits 2) compare our different example models and 3) compare our models with top IWSLT shared task systems and state-of-the-art prior works. Scoring For ST/SST, we evaluate detokenized case-sensitive BLEU (Post, 2018). For SST, we additionally evaluate Average Lagging (AL) (Ma et al., 2020a). For S2ST, we evaluate ASR-BLEU by transcribing the generated speech and then eval- uating the BLEU of this transcription. MODEL KD BT ENS BLEU\u2191 IWSLT\u201921 (Top 3 of 6) 1 Volctrans E2E\u2020 2 OPPO Cascade\u2020 3 Volctrans Cascade\u2020 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.3 22.6 22.2 ESPnet-ST-v2 A Base CA B Base MCA C Large CA D Large MCA - - - - -"}, {"question": " What type of encoders are used for encoding intermediate ASR text in the model?", "answer": " Transformer encoders", "ref_chunk": "above, following Yan et al. (2022). We use Con- former encoders with hierarchical CTC for encod- ing speech and Transformer encoders for encoding intermediate ASR text. We use Transformer de- coders for both ASR and ST. During inference, the ASR stage is decoded first and then the final MT/ST stage is decoded; both stages use label-synchronous joint CTC/attention beam search. 4.2 SST Models Time-Synchronous Blockwise CTC/Attention (TBCA) As shown in Figure 3, we adapt the aforementioned CTC/attention model for ST (\u00a74.1) Figure 4: Discrete Multi-Decoder (UnitY) for S2ST. to SST by replacing the Conformer encoder with a contextual block Conformer (Tsunoo et al., 2021). During inference, we initially followed Deng et al. (2022) and used the label-synchronous CTC/attention beam search originally proposed for ASR by Tsunoo et al. (2021). However, we found that label-synchrony results in overly conser- vative boundary block detection for SST. Therefore we opt instead for the time-synchronous variant which relies on CTC\u2019s more robust end-detection (Yan et al., 2023) to control boundary block de- tection; this change reduces latency without sacri- ficing quality. To perform incremental decoding without re-translation (as expected by SimulEval), hypotheses are pruned after processing all of the time steps for each encoder block. Blockwise Transducer (BT) As demonstrated by Xue et al. (2022), Transducers can be effec- tively applied to SST despite the monotonic na- ture of their underlying alignment model. We build Transducers for SST using contextual block Conformer encoders and unidirectional LSTM de- coders. We found that the aforementioned hier- archical CTC encoding (\u00a74.1) improves training stability and convergence rate. During inference, we found that the time-synchronous algorithm de- scribed by Saon et al. (2020) outperformed the original Graves decoding (Graves, 2012) and the later proposed alignment-synchronous algorithms (Saon et al., 2020). We also found that length nor- malization is required to avoid overly short outputs. Incremental decoding is applied in the same man- ner as for TBCA. 4.3 S2ST Models Spectral Multi-Decoder (Translatotron 2) Sim- ilar to the MCA model for ST (\u00a74.1), the spec- tral Multi-decoder (Jia et al., 2022a) decomposes S2ST into ST and TTS sub-tasks. The ST sub- TOOLKIT MODEL TYPE DE ES FR avg OFFLINE SPEECH TRANSLATION (ST) BLEU \u2191 Attentional Enc-Dec (AED) NeurST (Zhao et al., 2021) Fairseq (Wang et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v1 (Inaguma et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v2 (this work) Multi-Decoder CTC/Attn (MCA) 22.8 22.7 22.9 27.9 27.4 27.2 28.0 32.1 33.3 32.9 32.8 38.5 27.8 27.6 27.9 32.8 SIMULTANEOUS SPEECH TRANSLATION (SST) BLEU \u2191 / AL \u2193 Fairseq (Wang et al., 2020) ESPnet-ST-v2 (this work) Wait-K Attentional Enc-Dec (WAED) Time-Sync Blockwise CTC/Attn (TBCA) 18.6 / 6.8 23.5 / 2.3 22.9 / 6.9 29.2 / 2.4 28.5 / 6.7 32.7 / 2.3 23.3 / 6.8 28.5 / 2.3 OFFLINE SPEECH-TO-SPEECH TRANSLATION (S2ST) ASR-BLEU \u2191 Fairseq (Inaguma et al., 2022) ESPnet-ST-v2 (this work) Discrete Multi-Decoder (UnitY) Discrete Multi-Decoder (UnitY) 25.5 23.7 32.3 32.0 30.9 33.1 29.6 29.6 Table 2: Overview of ESPnet-ST-v2\u2019s ST, SST, and S2ST performances compared to other open-source toolkits. Results are presented on MuST-C-v1 (English-to-X) for ST/SST and on CVSS-C (X-to-English) for S2ST. task is modeled with an encoder-decoder network while the TTS sub-task is modeled with an auto- regressive synthesizer. The synthesizer attends over both the ST-encoder and ST-decoder hidden states. We use Transformers for the ST encoder-decoder and a Tacotron-style (Wang et al., 2017) decoder as the synthesizer. During inference, we first use beam search for the ST sub-task and then auto- regressively generate Mel-spectrograms. The final waveform speech is generated with a HiFi-GAN vocoder (Kong et al., 2020). MODEL HIERENC BLEU\u2191 Attn Enc-Dec (AED) Multi-Decoder Attn Enc-Dec (MAED) CTC/Attention (CA) Multi-Decoder CTC/Attn (MCA) Transducer (T) - \u2713 \u2713 \u2713 25.7 27.6 28.6 28.8 27.6 Table 3: Example ST models \u2013 results on MuST-C-v2 En-De tst-COMMON. 5.1 Experimental Setup Discrete Multi-Decoder (UnitY) The UnitY model (Inaguma et al., 2022) is similar to Trans- latotron 2, but critically predicts discrete units of speech SSL representations rather than spectral information in the final stage. In other words, UnitY is Multi-decoder consisting of a ST sub- task followed by a text-to-unit (T2U) sub-task (see Figure 4). We use Transformer-based encoder- decoders for both sub-tasks. During inference, the ST stage is first decoded and then followed by the T2U stage. Both stages use label synchronous beam search. The final speech is generated with a unit HiFi-GAN vocoder with Fastspeech-like du- ration prediction (Polyak et al., 2021; Lee et al., 2022a), which is separately trained in the Parallel- WaveGAN toolkit (Hayashi et al., 2020, 2021). Please refer to \u00a7A.1 for reproducibility details. The following is only a summary of our setup. Data We use MuST-C-v1 or MuST-C-v2 (Di Gangi et al., 2019) for ST/SST and CVSS-C for S2ST (Jia et al., 2022b). For IWSLT compar- isons, we combine MuST-C-v1, MuST-C-v2, and ST-TED (Niehues et al., 2018) for ST/SST. Models Unless otherwise indicated, we use a \"base\" setting for our models. Our base models have 40-80M trainable parameters across all tasks and are trained on a \u223c400h of single language pair data from a single corpus. For ST/SST, we also use a \"large\" setting for benchmarking against IWSLT submissions. Our large models have 150-200M trainable parameters and are trained on \u223c1000h of single language pair data from multiple corpora. 5 Performance Benchmarking In this section, we 1) compare open-source toolkits 2) compare our different example models and 3) compare our models with top IWSLT shared task systems and state-of-the-art prior works. Scoring For ST/SST, we evaluate detokenized case-sensitive BLEU (Post, 2018). For SST, we additionally evaluate Average Lagging (AL) (Ma et al., 2020a). For S2ST, we evaluate ASR-BLEU by transcribing the generated speech and then eval- uating the BLEU of this transcription. MODEL KD BT ENS BLEU\u2191 IWSLT\u201921 (Top 3 of 6) 1 Volctrans E2E\u2020 2 OPPO Cascade\u2020 3 Volctrans Cascade\u2020 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.3 22.6 22.2 ESPnet-ST-v2 A Base CA B Base MCA C Large CA D Large MCA - - - - -"}, {"question": " What type of decoders are used for both ASR and ST in the model?", "answer": " Transformer decoders", "ref_chunk": "above, following Yan et al. (2022). We use Con- former encoders with hierarchical CTC for encod- ing speech and Transformer encoders for encoding intermediate ASR text. We use Transformer de- coders for both ASR and ST. During inference, the ASR stage is decoded first and then the final MT/ST stage is decoded; both stages use label-synchronous joint CTC/attention beam search. 4.2 SST Models Time-Synchronous Blockwise CTC/Attention (TBCA) As shown in Figure 3, we adapt the aforementioned CTC/attention model for ST (\u00a74.1) Figure 4: Discrete Multi-Decoder (UnitY) for S2ST. to SST by replacing the Conformer encoder with a contextual block Conformer (Tsunoo et al., 2021). During inference, we initially followed Deng et al. (2022) and used the label-synchronous CTC/attention beam search originally proposed for ASR by Tsunoo et al. (2021). However, we found that label-synchrony results in overly conser- vative boundary block detection for SST. Therefore we opt instead for the time-synchronous variant which relies on CTC\u2019s more robust end-detection (Yan et al., 2023) to control boundary block de- tection; this change reduces latency without sacri- ficing quality. To perform incremental decoding without re-translation (as expected by SimulEval), hypotheses are pruned after processing all of the time steps for each encoder block. Blockwise Transducer (BT) As demonstrated by Xue et al. (2022), Transducers can be effec- tively applied to SST despite the monotonic na- ture of their underlying alignment model. We build Transducers for SST using contextual block Conformer encoders and unidirectional LSTM de- coders. We found that the aforementioned hier- archical CTC encoding (\u00a74.1) improves training stability and convergence rate. During inference, we found that the time-synchronous algorithm de- scribed by Saon et al. (2020) outperformed the original Graves decoding (Graves, 2012) and the later proposed alignment-synchronous algorithms (Saon et al., 2020). We also found that length nor- malization is required to avoid overly short outputs. Incremental decoding is applied in the same man- ner as for TBCA. 4.3 S2ST Models Spectral Multi-Decoder (Translatotron 2) Sim- ilar to the MCA model for ST (\u00a74.1), the spec- tral Multi-decoder (Jia et al., 2022a) decomposes S2ST into ST and TTS sub-tasks. The ST sub- TOOLKIT MODEL TYPE DE ES FR avg OFFLINE SPEECH TRANSLATION (ST) BLEU \u2191 Attentional Enc-Dec (AED) NeurST (Zhao et al., 2021) Fairseq (Wang et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v1 (Inaguma et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v2 (this work) Multi-Decoder CTC/Attn (MCA) 22.8 22.7 22.9 27.9 27.4 27.2 28.0 32.1 33.3 32.9 32.8 38.5 27.8 27.6 27.9 32.8 SIMULTANEOUS SPEECH TRANSLATION (SST) BLEU \u2191 / AL \u2193 Fairseq (Wang et al., 2020) ESPnet-ST-v2 (this work) Wait-K Attentional Enc-Dec (WAED) Time-Sync Blockwise CTC/Attn (TBCA) 18.6 / 6.8 23.5 / 2.3 22.9 / 6.9 29.2 / 2.4 28.5 / 6.7 32.7 / 2.3 23.3 / 6.8 28.5 / 2.3 OFFLINE SPEECH-TO-SPEECH TRANSLATION (S2ST) ASR-BLEU \u2191 Fairseq (Inaguma et al., 2022) ESPnet-ST-v2 (this work) Discrete Multi-Decoder (UnitY) Discrete Multi-Decoder (UnitY) 25.5 23.7 32.3 32.0 30.9 33.1 29.6 29.6 Table 2: Overview of ESPnet-ST-v2\u2019s ST, SST, and S2ST performances compared to other open-source toolkits. Results are presented on MuST-C-v1 (English-to-X) for ST/SST and on CVSS-C (X-to-English) for S2ST. task is modeled with an encoder-decoder network while the TTS sub-task is modeled with an auto- regressive synthesizer. The synthesizer attends over both the ST-encoder and ST-decoder hidden states. We use Transformers for the ST encoder-decoder and a Tacotron-style (Wang et al., 2017) decoder as the synthesizer. During inference, we first use beam search for the ST sub-task and then auto- regressively generate Mel-spectrograms. The final waveform speech is generated with a HiFi-GAN vocoder (Kong et al., 2020). MODEL HIERENC BLEU\u2191 Attn Enc-Dec (AED) Multi-Decoder Attn Enc-Dec (MAED) CTC/Attention (CA) Multi-Decoder CTC/Attn (MCA) Transducer (T) - \u2713 \u2713 \u2713 25.7 27.6 28.6 28.8 27.6 Table 3: Example ST models \u2013 results on MuST-C-v2 En-De tst-COMMON. 5.1 Experimental Setup Discrete Multi-Decoder (UnitY) The UnitY model (Inaguma et al., 2022) is similar to Trans- latotron 2, but critically predicts discrete units of speech SSL representations rather than spectral information in the final stage. In other words, UnitY is Multi-decoder consisting of a ST sub- task followed by a text-to-unit (T2U) sub-task (see Figure 4). We use Transformer-based encoder- decoders for both sub-tasks. During inference, the ST stage is first decoded and then followed by the T2U stage. Both stages use label synchronous beam search. The final speech is generated with a unit HiFi-GAN vocoder with Fastspeech-like du- ration prediction (Polyak et al., 2021; Lee et al., 2022a), which is separately trained in the Parallel- WaveGAN toolkit (Hayashi et al., 2020, 2021). Please refer to \u00a7A.1 for reproducibility details. The following is only a summary of our setup. Data We use MuST-C-v1 or MuST-C-v2 (Di Gangi et al., 2019) for ST/SST and CVSS-C for S2ST (Jia et al., 2022b). For IWSLT compar- isons, we combine MuST-C-v1, MuST-C-v2, and ST-TED (Niehues et al., 2018) for ST/SST. Models Unless otherwise indicated, we use a \"base\" setting for our models. Our base models have 40-80M trainable parameters across all tasks and are trained on a \u223c400h of single language pair data from a single corpus. For ST/SST, we also use a \"large\" setting for benchmarking against IWSLT submissions. Our large models have 150-200M trainable parameters and are trained on \u223c1000h of single language pair data from multiple corpora. 5 Performance Benchmarking In this section, we 1) compare open-source toolkits 2) compare our different example models and 3) compare our models with top IWSLT shared task systems and state-of-the-art prior works. Scoring For ST/SST, we evaluate detokenized case-sensitive BLEU (Post, 2018). For SST, we additionally evaluate Average Lagging (AL) (Ma et al., 2020a). For S2ST, we evaluate ASR-BLEU by transcribing the generated speech and then eval- uating the BLEU of this transcription. MODEL KD BT ENS BLEU\u2191 IWSLT\u201921 (Top 3 of 6) 1 Volctrans E2E\u2020 2 OPPO Cascade\u2020 3 Volctrans Cascade\u2020 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.3 22.6 22.2 ESPnet-ST-v2 A Base CA B Base MCA C Large CA D Large MCA - - - - -"}, {"question": " Why is the time-synchronous variant preferred over label-synchrony in the SST model?", "answer": " Label-synchrony results in overly conservative boundary block detection for SST.", "ref_chunk": "above, following Yan et al. (2022). We use Con- former encoders with hierarchical CTC for encod- ing speech and Transformer encoders for encoding intermediate ASR text. We use Transformer de- coders for both ASR and ST. During inference, the ASR stage is decoded first and then the final MT/ST stage is decoded; both stages use label-synchronous joint CTC/attention beam search. 4.2 SST Models Time-Synchronous Blockwise CTC/Attention (TBCA) As shown in Figure 3, we adapt the aforementioned CTC/attention model for ST (\u00a74.1) Figure 4: Discrete Multi-Decoder (UnitY) for S2ST. to SST by replacing the Conformer encoder with a contextual block Conformer (Tsunoo et al., 2021). During inference, we initially followed Deng et al. (2022) and used the label-synchronous CTC/attention beam search originally proposed for ASR by Tsunoo et al. (2021). However, we found that label-synchrony results in overly conser- vative boundary block detection for SST. Therefore we opt instead for the time-synchronous variant which relies on CTC\u2019s more robust end-detection (Yan et al., 2023) to control boundary block de- tection; this change reduces latency without sacri- ficing quality. To perform incremental decoding without re-translation (as expected by SimulEval), hypotheses are pruned after processing all of the time steps for each encoder block. Blockwise Transducer (BT) As demonstrated by Xue et al. (2022), Transducers can be effec- tively applied to SST despite the monotonic na- ture of their underlying alignment model. We build Transducers for SST using contextual block Conformer encoders and unidirectional LSTM de- coders. We found that the aforementioned hier- archical CTC encoding (\u00a74.1) improves training stability and convergence rate. During inference, we found that the time-synchronous algorithm de- scribed by Saon et al. (2020) outperformed the original Graves decoding (Graves, 2012) and the later proposed alignment-synchronous algorithms (Saon et al., 2020). We also found that length nor- malization is required to avoid overly short outputs. Incremental decoding is applied in the same man- ner as for TBCA. 4.3 S2ST Models Spectral Multi-Decoder (Translatotron 2) Sim- ilar to the MCA model for ST (\u00a74.1), the spec- tral Multi-decoder (Jia et al., 2022a) decomposes S2ST into ST and TTS sub-tasks. The ST sub- TOOLKIT MODEL TYPE DE ES FR avg OFFLINE SPEECH TRANSLATION (ST) BLEU \u2191 Attentional Enc-Dec (AED) NeurST (Zhao et al., 2021) Fairseq (Wang et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v1 (Inaguma et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v2 (this work) Multi-Decoder CTC/Attn (MCA) 22.8 22.7 22.9 27.9 27.4 27.2 28.0 32.1 33.3 32.9 32.8 38.5 27.8 27.6 27.9 32.8 SIMULTANEOUS SPEECH TRANSLATION (SST) BLEU \u2191 / AL \u2193 Fairseq (Wang et al., 2020) ESPnet-ST-v2 (this work) Wait-K Attentional Enc-Dec (WAED) Time-Sync Blockwise CTC/Attn (TBCA) 18.6 / 6.8 23.5 / 2.3 22.9 / 6.9 29.2 / 2.4 28.5 / 6.7 32.7 / 2.3 23.3 / 6.8 28.5 / 2.3 OFFLINE SPEECH-TO-SPEECH TRANSLATION (S2ST) ASR-BLEU \u2191 Fairseq (Inaguma et al., 2022) ESPnet-ST-v2 (this work) Discrete Multi-Decoder (UnitY) Discrete Multi-Decoder (UnitY) 25.5 23.7 32.3 32.0 30.9 33.1 29.6 29.6 Table 2: Overview of ESPnet-ST-v2\u2019s ST, SST, and S2ST performances compared to other open-source toolkits. Results are presented on MuST-C-v1 (English-to-X) for ST/SST and on CVSS-C (X-to-English) for S2ST. task is modeled with an encoder-decoder network while the TTS sub-task is modeled with an auto- regressive synthesizer. The synthesizer attends over both the ST-encoder and ST-decoder hidden states. We use Transformers for the ST encoder-decoder and a Tacotron-style (Wang et al., 2017) decoder as the synthesizer. During inference, we first use beam search for the ST sub-task and then auto- regressively generate Mel-spectrograms. The final waveform speech is generated with a HiFi-GAN vocoder (Kong et al., 2020). MODEL HIERENC BLEU\u2191 Attn Enc-Dec (AED) Multi-Decoder Attn Enc-Dec (MAED) CTC/Attention (CA) Multi-Decoder CTC/Attn (MCA) Transducer (T) - \u2713 \u2713 \u2713 25.7 27.6 28.6 28.8 27.6 Table 3: Example ST models \u2013 results on MuST-C-v2 En-De tst-COMMON. 5.1 Experimental Setup Discrete Multi-Decoder (UnitY) The UnitY model (Inaguma et al., 2022) is similar to Trans- latotron 2, but critically predicts discrete units of speech SSL representations rather than spectral information in the final stage. In other words, UnitY is Multi-decoder consisting of a ST sub- task followed by a text-to-unit (T2U) sub-task (see Figure 4). We use Transformer-based encoder- decoders for both sub-tasks. During inference, the ST stage is first decoded and then followed by the T2U stage. Both stages use label synchronous beam search. The final speech is generated with a unit HiFi-GAN vocoder with Fastspeech-like du- ration prediction (Polyak et al., 2021; Lee et al., 2022a), which is separately trained in the Parallel- WaveGAN toolkit (Hayashi et al., 2020, 2021). Please refer to \u00a7A.1 for reproducibility details. The following is only a summary of our setup. Data We use MuST-C-v1 or MuST-C-v2 (Di Gangi et al., 2019) for ST/SST and CVSS-C for S2ST (Jia et al., 2022b). For IWSLT compar- isons, we combine MuST-C-v1, MuST-C-v2, and ST-TED (Niehues et al., 2018) for ST/SST. Models Unless otherwise indicated, we use a \"base\" setting for our models. Our base models have 40-80M trainable parameters across all tasks and are trained on a \u223c400h of single language pair data from a single corpus. For ST/SST, we also use a \"large\" setting for benchmarking against IWSLT submissions. Our large models have 150-200M trainable parameters and are trained on \u223c1000h of single language pair data from multiple corpora. 5 Performance Benchmarking In this section, we 1) compare open-source toolkits 2) compare our different example models and 3) compare our models with top IWSLT shared task systems and state-of-the-art prior works. Scoring For ST/SST, we evaluate detokenized case-sensitive BLEU (Post, 2018). For SST, we additionally evaluate Average Lagging (AL) (Ma et al., 2020a). For S2ST, we evaluate ASR-BLEU by transcribing the generated speech and then eval- uating the BLEU of this transcription. MODEL KD BT ENS BLEU\u2191 IWSLT\u201921 (Top 3 of 6) 1 Volctrans E2E\u2020 2 OPPO Cascade\u2020 3 Volctrans Cascade\u2020 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.3 22.6 22.2 ESPnet-ST-v2 A Base CA B Base MCA C Large CA D Large MCA - - - - -"}, {"question": " How is incremental decoding performed in the model?", "answer": " Hypotheses are pruned after processing all of the time steps for each encoder block.", "ref_chunk": "above, following Yan et al. (2022). We use Con- former encoders with hierarchical CTC for encod- ing speech and Transformer encoders for encoding intermediate ASR text. We use Transformer de- coders for both ASR and ST. During inference, the ASR stage is decoded first and then the final MT/ST stage is decoded; both stages use label-synchronous joint CTC/attention beam search. 4.2 SST Models Time-Synchronous Blockwise CTC/Attention (TBCA) As shown in Figure 3, we adapt the aforementioned CTC/attention model for ST (\u00a74.1) Figure 4: Discrete Multi-Decoder (UnitY) for S2ST. to SST by replacing the Conformer encoder with a contextual block Conformer (Tsunoo et al., 2021). During inference, we initially followed Deng et al. (2022) and used the label-synchronous CTC/attention beam search originally proposed for ASR by Tsunoo et al. (2021). However, we found that label-synchrony results in overly conser- vative boundary block detection for SST. Therefore we opt instead for the time-synchronous variant which relies on CTC\u2019s more robust end-detection (Yan et al., 2023) to control boundary block de- tection; this change reduces latency without sacri- ficing quality. To perform incremental decoding without re-translation (as expected by SimulEval), hypotheses are pruned after processing all of the time steps for each encoder block. Blockwise Transducer (BT) As demonstrated by Xue et al. (2022), Transducers can be effec- tively applied to SST despite the monotonic na- ture of their underlying alignment model. We build Transducers for SST using contextual block Conformer encoders and unidirectional LSTM de- coders. We found that the aforementioned hier- archical CTC encoding (\u00a74.1) improves training stability and convergence rate. During inference, we found that the time-synchronous algorithm de- scribed by Saon et al. (2020) outperformed the original Graves decoding (Graves, 2012) and the later proposed alignment-synchronous algorithms (Saon et al., 2020). We also found that length nor- malization is required to avoid overly short outputs. Incremental decoding is applied in the same man- ner as for TBCA. 4.3 S2ST Models Spectral Multi-Decoder (Translatotron 2) Sim- ilar to the MCA model for ST (\u00a74.1), the spec- tral Multi-decoder (Jia et al., 2022a) decomposes S2ST into ST and TTS sub-tasks. The ST sub- TOOLKIT MODEL TYPE DE ES FR avg OFFLINE SPEECH TRANSLATION (ST) BLEU \u2191 Attentional Enc-Dec (AED) NeurST (Zhao et al., 2021) Fairseq (Wang et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v1 (Inaguma et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v2 (this work) Multi-Decoder CTC/Attn (MCA) 22.8 22.7 22.9 27.9 27.4 27.2 28.0 32.1 33.3 32.9 32.8 38.5 27.8 27.6 27.9 32.8 SIMULTANEOUS SPEECH TRANSLATION (SST) BLEU \u2191 / AL \u2193 Fairseq (Wang et al., 2020) ESPnet-ST-v2 (this work) Wait-K Attentional Enc-Dec (WAED) Time-Sync Blockwise CTC/Attn (TBCA) 18.6 / 6.8 23.5 / 2.3 22.9 / 6.9 29.2 / 2.4 28.5 / 6.7 32.7 / 2.3 23.3 / 6.8 28.5 / 2.3 OFFLINE SPEECH-TO-SPEECH TRANSLATION (S2ST) ASR-BLEU \u2191 Fairseq (Inaguma et al., 2022) ESPnet-ST-v2 (this work) Discrete Multi-Decoder (UnitY) Discrete Multi-Decoder (UnitY) 25.5 23.7 32.3 32.0 30.9 33.1 29.6 29.6 Table 2: Overview of ESPnet-ST-v2\u2019s ST, SST, and S2ST performances compared to other open-source toolkits. Results are presented on MuST-C-v1 (English-to-X) for ST/SST and on CVSS-C (X-to-English) for S2ST. task is modeled with an encoder-decoder network while the TTS sub-task is modeled with an auto- regressive synthesizer. The synthesizer attends over both the ST-encoder and ST-decoder hidden states. We use Transformers for the ST encoder-decoder and a Tacotron-style (Wang et al., 2017) decoder as the synthesizer. During inference, we first use beam search for the ST sub-task and then auto- regressively generate Mel-spectrograms. The final waveform speech is generated with a HiFi-GAN vocoder (Kong et al., 2020). MODEL HIERENC BLEU\u2191 Attn Enc-Dec (AED) Multi-Decoder Attn Enc-Dec (MAED) CTC/Attention (CA) Multi-Decoder CTC/Attn (MCA) Transducer (T) - \u2713 \u2713 \u2713 25.7 27.6 28.6 28.8 27.6 Table 3: Example ST models \u2013 results on MuST-C-v2 En-De tst-COMMON. 5.1 Experimental Setup Discrete Multi-Decoder (UnitY) The UnitY model (Inaguma et al., 2022) is similar to Trans- latotron 2, but critically predicts discrete units of speech SSL representations rather than spectral information in the final stage. In other words, UnitY is Multi-decoder consisting of a ST sub- task followed by a text-to-unit (T2U) sub-task (see Figure 4). We use Transformer-based encoder- decoders for both sub-tasks. During inference, the ST stage is first decoded and then followed by the T2U stage. Both stages use label synchronous beam search. The final speech is generated with a unit HiFi-GAN vocoder with Fastspeech-like du- ration prediction (Polyak et al., 2021; Lee et al., 2022a), which is separately trained in the Parallel- WaveGAN toolkit (Hayashi et al., 2020, 2021). Please refer to \u00a7A.1 for reproducibility details. The following is only a summary of our setup. Data We use MuST-C-v1 or MuST-C-v2 (Di Gangi et al., 2019) for ST/SST and CVSS-C for S2ST (Jia et al., 2022b). For IWSLT compar- isons, we combine MuST-C-v1, MuST-C-v2, and ST-TED (Niehues et al., 2018) for ST/SST. Models Unless otherwise indicated, we use a \"base\" setting for our models. Our base models have 40-80M trainable parameters across all tasks and are trained on a \u223c400h of single language pair data from a single corpus. For ST/SST, we also use a \"large\" setting for benchmarking against IWSLT submissions. Our large models have 150-200M trainable parameters and are trained on \u223c1000h of single language pair data from multiple corpora. 5 Performance Benchmarking In this section, we 1) compare open-source toolkits 2) compare our different example models and 3) compare our models with top IWSLT shared task systems and state-of-the-art prior works. Scoring For ST/SST, we evaluate detokenized case-sensitive BLEU (Post, 2018). For SST, we additionally evaluate Average Lagging (AL) (Ma et al., 2020a). For S2ST, we evaluate ASR-BLEU by transcribing the generated speech and then eval- uating the BLEU of this transcription. MODEL KD BT ENS BLEU\u2191 IWSLT\u201921 (Top 3 of 6) 1 Volctrans E2E\u2020 2 OPPO Cascade\u2020 3 Volctrans Cascade\u2020 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.3 22.6 22.2 ESPnet-ST-v2 A Base CA B Base MCA C Large CA D Large MCA - - - - -"}, {"question": " What type of model is Translatotron 2?", "answer": " Spectral Multi-Decoder for S2ST", "ref_chunk": "above, following Yan et al. (2022). We use Con- former encoders with hierarchical CTC for encod- ing speech and Transformer encoders for encoding intermediate ASR text. We use Transformer de- coders for both ASR and ST. During inference, the ASR stage is decoded first and then the final MT/ST stage is decoded; both stages use label-synchronous joint CTC/attention beam search. 4.2 SST Models Time-Synchronous Blockwise CTC/Attention (TBCA) As shown in Figure 3, we adapt the aforementioned CTC/attention model for ST (\u00a74.1) Figure 4: Discrete Multi-Decoder (UnitY) for S2ST. to SST by replacing the Conformer encoder with a contextual block Conformer (Tsunoo et al., 2021). During inference, we initially followed Deng et al. (2022) and used the label-synchronous CTC/attention beam search originally proposed for ASR by Tsunoo et al. (2021). However, we found that label-synchrony results in overly conser- vative boundary block detection for SST. Therefore we opt instead for the time-synchronous variant which relies on CTC\u2019s more robust end-detection (Yan et al., 2023) to control boundary block de- tection; this change reduces latency without sacri- ficing quality. To perform incremental decoding without re-translation (as expected by SimulEval), hypotheses are pruned after processing all of the time steps for each encoder block. Blockwise Transducer (BT) As demonstrated by Xue et al. (2022), Transducers can be effec- tively applied to SST despite the monotonic na- ture of their underlying alignment model. We build Transducers for SST using contextual block Conformer encoders and unidirectional LSTM de- coders. We found that the aforementioned hier- archical CTC encoding (\u00a74.1) improves training stability and convergence rate. During inference, we found that the time-synchronous algorithm de- scribed by Saon et al. (2020) outperformed the original Graves decoding (Graves, 2012) and the later proposed alignment-synchronous algorithms (Saon et al., 2020). We also found that length nor- malization is required to avoid overly short outputs. Incremental decoding is applied in the same man- ner as for TBCA. 4.3 S2ST Models Spectral Multi-Decoder (Translatotron 2) Sim- ilar to the MCA model for ST (\u00a74.1), the spec- tral Multi-decoder (Jia et al., 2022a) decomposes S2ST into ST and TTS sub-tasks. The ST sub- TOOLKIT MODEL TYPE DE ES FR avg OFFLINE SPEECH TRANSLATION (ST) BLEU \u2191 Attentional Enc-Dec (AED) NeurST (Zhao et al., 2021) Fairseq (Wang et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v1 (Inaguma et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v2 (this work) Multi-Decoder CTC/Attn (MCA) 22.8 22.7 22.9 27.9 27.4 27.2 28.0 32.1 33.3 32.9 32.8 38.5 27.8 27.6 27.9 32.8 SIMULTANEOUS SPEECH TRANSLATION (SST) BLEU \u2191 / AL \u2193 Fairseq (Wang et al., 2020) ESPnet-ST-v2 (this work) Wait-K Attentional Enc-Dec (WAED) Time-Sync Blockwise CTC/Attn (TBCA) 18.6 / 6.8 23.5 / 2.3 22.9 / 6.9 29.2 / 2.4 28.5 / 6.7 32.7 / 2.3 23.3 / 6.8 28.5 / 2.3 OFFLINE SPEECH-TO-SPEECH TRANSLATION (S2ST) ASR-BLEU \u2191 Fairseq (Inaguma et al., 2022) ESPnet-ST-v2 (this work) Discrete Multi-Decoder (UnitY) Discrete Multi-Decoder (UnitY) 25.5 23.7 32.3 32.0 30.9 33.1 29.6 29.6 Table 2: Overview of ESPnet-ST-v2\u2019s ST, SST, and S2ST performances compared to other open-source toolkits. Results are presented on MuST-C-v1 (English-to-X) for ST/SST and on CVSS-C (X-to-English) for S2ST. task is modeled with an encoder-decoder network while the TTS sub-task is modeled with an auto- regressive synthesizer. The synthesizer attends over both the ST-encoder and ST-decoder hidden states. We use Transformers for the ST encoder-decoder and a Tacotron-style (Wang et al., 2017) decoder as the synthesizer. During inference, we first use beam search for the ST sub-task and then auto- regressively generate Mel-spectrograms. The final waveform speech is generated with a HiFi-GAN vocoder (Kong et al., 2020). MODEL HIERENC BLEU\u2191 Attn Enc-Dec (AED) Multi-Decoder Attn Enc-Dec (MAED) CTC/Attention (CA) Multi-Decoder CTC/Attn (MCA) Transducer (T) - \u2713 \u2713 \u2713 25.7 27.6 28.6 28.8 27.6 Table 3: Example ST models \u2013 results on MuST-C-v2 En-De tst-COMMON. 5.1 Experimental Setup Discrete Multi-Decoder (UnitY) The UnitY model (Inaguma et al., 2022) is similar to Trans- latotron 2, but critically predicts discrete units of speech SSL representations rather than spectral information in the final stage. In other words, UnitY is Multi-decoder consisting of a ST sub- task followed by a text-to-unit (T2U) sub-task (see Figure 4). We use Transformer-based encoder- decoders for both sub-tasks. During inference, the ST stage is first decoded and then followed by the T2U stage. Both stages use label synchronous beam search. The final speech is generated with a unit HiFi-GAN vocoder with Fastspeech-like du- ration prediction (Polyak et al., 2021; Lee et al., 2022a), which is separately trained in the Parallel- WaveGAN toolkit (Hayashi et al., 2020, 2021). Please refer to \u00a7A.1 for reproducibility details. The following is only a summary of our setup. Data We use MuST-C-v1 or MuST-C-v2 (Di Gangi et al., 2019) for ST/SST and CVSS-C for S2ST (Jia et al., 2022b). For IWSLT compar- isons, we combine MuST-C-v1, MuST-C-v2, and ST-TED (Niehues et al., 2018) for ST/SST. Models Unless otherwise indicated, we use a \"base\" setting for our models. Our base models have 40-80M trainable parameters across all tasks and are trained on a \u223c400h of single language pair data from a single corpus. For ST/SST, we also use a \"large\" setting for benchmarking against IWSLT submissions. Our large models have 150-200M trainable parameters and are trained on \u223c1000h of single language pair data from multiple corpora. 5 Performance Benchmarking In this section, we 1) compare open-source toolkits 2) compare our different example models and 3) compare our models with top IWSLT shared task systems and state-of-the-art prior works. Scoring For ST/SST, we evaluate detokenized case-sensitive BLEU (Post, 2018). For SST, we additionally evaluate Average Lagging (AL) (Ma et al., 2020a). For S2ST, we evaluate ASR-BLEU by transcribing the generated speech and then eval- uating the BLEU of this transcription. MODEL KD BT ENS BLEU\u2191 IWSLT\u201921 (Top 3 of 6) 1 Volctrans E2E\u2020 2 OPPO Cascade\u2020 3 Volctrans Cascade\u2020 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.3 22.6 22.2 ESPnet-ST-v2 A Base CA B Base MCA C Large CA D Large MCA - - - - -"}, {"question": " What is the purpose of the TTS sub-task in the S2ST model?", "answer": " To model speech synthesis", "ref_chunk": "above, following Yan et al. (2022). We use Con- former encoders with hierarchical CTC for encod- ing speech and Transformer encoders for encoding intermediate ASR text. We use Transformer de- coders for both ASR and ST. During inference, the ASR stage is decoded first and then the final MT/ST stage is decoded; both stages use label-synchronous joint CTC/attention beam search. 4.2 SST Models Time-Synchronous Blockwise CTC/Attention (TBCA) As shown in Figure 3, we adapt the aforementioned CTC/attention model for ST (\u00a74.1) Figure 4: Discrete Multi-Decoder (UnitY) for S2ST. to SST by replacing the Conformer encoder with a contextual block Conformer (Tsunoo et al., 2021). During inference, we initially followed Deng et al. (2022) and used the label-synchronous CTC/attention beam search originally proposed for ASR by Tsunoo et al. (2021). However, we found that label-synchrony results in overly conser- vative boundary block detection for SST. Therefore we opt instead for the time-synchronous variant which relies on CTC\u2019s more robust end-detection (Yan et al., 2023) to control boundary block de- tection; this change reduces latency without sacri- ficing quality. To perform incremental decoding without re-translation (as expected by SimulEval), hypotheses are pruned after processing all of the time steps for each encoder block. Blockwise Transducer (BT) As demonstrated by Xue et al. (2022), Transducers can be effec- tively applied to SST despite the monotonic na- ture of their underlying alignment model. We build Transducers for SST using contextual block Conformer encoders and unidirectional LSTM de- coders. We found that the aforementioned hier- archical CTC encoding (\u00a74.1) improves training stability and convergence rate. During inference, we found that the time-synchronous algorithm de- scribed by Saon et al. (2020) outperformed the original Graves decoding (Graves, 2012) and the later proposed alignment-synchronous algorithms (Saon et al., 2020). We also found that length nor- malization is required to avoid overly short outputs. Incremental decoding is applied in the same man- ner as for TBCA. 4.3 S2ST Models Spectral Multi-Decoder (Translatotron 2) Sim- ilar to the MCA model for ST (\u00a74.1), the spec- tral Multi-decoder (Jia et al., 2022a) decomposes S2ST into ST and TTS sub-tasks. The ST sub- TOOLKIT MODEL TYPE DE ES FR avg OFFLINE SPEECH TRANSLATION (ST) BLEU \u2191 Attentional Enc-Dec (AED) NeurST (Zhao et al., 2021) Fairseq (Wang et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v1 (Inaguma et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v2 (this work) Multi-Decoder CTC/Attn (MCA) 22.8 22.7 22.9 27.9 27.4 27.2 28.0 32.1 33.3 32.9 32.8 38.5 27.8 27.6 27.9 32.8 SIMULTANEOUS SPEECH TRANSLATION (SST) BLEU \u2191 / AL \u2193 Fairseq (Wang et al., 2020) ESPnet-ST-v2 (this work) Wait-K Attentional Enc-Dec (WAED) Time-Sync Blockwise CTC/Attn (TBCA) 18.6 / 6.8 23.5 / 2.3 22.9 / 6.9 29.2 / 2.4 28.5 / 6.7 32.7 / 2.3 23.3 / 6.8 28.5 / 2.3 OFFLINE SPEECH-TO-SPEECH TRANSLATION (S2ST) ASR-BLEU \u2191 Fairseq (Inaguma et al., 2022) ESPnet-ST-v2 (this work) Discrete Multi-Decoder (UnitY) Discrete Multi-Decoder (UnitY) 25.5 23.7 32.3 32.0 30.9 33.1 29.6 29.6 Table 2: Overview of ESPnet-ST-v2\u2019s ST, SST, and S2ST performances compared to other open-source toolkits. Results are presented on MuST-C-v1 (English-to-X) for ST/SST and on CVSS-C (X-to-English) for S2ST. task is modeled with an encoder-decoder network while the TTS sub-task is modeled with an auto- regressive synthesizer. The synthesizer attends over both the ST-encoder and ST-decoder hidden states. We use Transformers for the ST encoder-decoder and a Tacotron-style (Wang et al., 2017) decoder as the synthesizer. During inference, we first use beam search for the ST sub-task and then auto- regressively generate Mel-spectrograms. The final waveform speech is generated with a HiFi-GAN vocoder (Kong et al., 2020). MODEL HIERENC BLEU\u2191 Attn Enc-Dec (AED) Multi-Decoder Attn Enc-Dec (MAED) CTC/Attention (CA) Multi-Decoder CTC/Attn (MCA) Transducer (T) - \u2713 \u2713 \u2713 25.7 27.6 28.6 28.8 27.6 Table 3: Example ST models \u2013 results on MuST-C-v2 En-De tst-COMMON. 5.1 Experimental Setup Discrete Multi-Decoder (UnitY) The UnitY model (Inaguma et al., 2022) is similar to Trans- latotron 2, but critically predicts discrete units of speech SSL representations rather than spectral information in the final stage. In other words, UnitY is Multi-decoder consisting of a ST sub- task followed by a text-to-unit (T2U) sub-task (see Figure 4). We use Transformer-based encoder- decoders for both sub-tasks. During inference, the ST stage is first decoded and then followed by the T2U stage. Both stages use label synchronous beam search. The final speech is generated with a unit HiFi-GAN vocoder with Fastspeech-like du- ration prediction (Polyak et al., 2021; Lee et al., 2022a), which is separately trained in the Parallel- WaveGAN toolkit (Hayashi et al., 2020, 2021). Please refer to \u00a7A.1 for reproducibility details. The following is only a summary of our setup. Data We use MuST-C-v1 or MuST-C-v2 (Di Gangi et al., 2019) for ST/SST and CVSS-C for S2ST (Jia et al., 2022b). For IWSLT compar- isons, we combine MuST-C-v1, MuST-C-v2, and ST-TED (Niehues et al., 2018) for ST/SST. Models Unless otherwise indicated, we use a \"base\" setting for our models. Our base models have 40-80M trainable parameters across all tasks and are trained on a \u223c400h of single language pair data from a single corpus. For ST/SST, we also use a \"large\" setting for benchmarking against IWSLT submissions. Our large models have 150-200M trainable parameters and are trained on \u223c1000h of single language pair data from multiple corpora. 5 Performance Benchmarking In this section, we 1) compare open-source toolkits 2) compare our different example models and 3) compare our models with top IWSLT shared task systems and state-of-the-art prior works. Scoring For ST/SST, we evaluate detokenized case-sensitive BLEU (Post, 2018). For SST, we additionally evaluate Average Lagging (AL) (Ma et al., 2020a). For S2ST, we evaluate ASR-BLEU by transcribing the generated speech and then eval- uating the BLEU of this transcription. MODEL KD BT ENS BLEU\u2191 IWSLT\u201921 (Top 3 of 6) 1 Volctrans E2E\u2020 2 OPPO Cascade\u2020 3 Volctrans Cascade\u2020 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.3 22.6 22.2 ESPnet-ST-v2 A Base CA B Base MCA C Large CA D Large MCA - - - - -"}, {"question": " What type of decoder is used in the synthesizer of the S2ST model?", "answer": " Tacotron-style decoder", "ref_chunk": "above, following Yan et al. (2022). We use Con- former encoders with hierarchical CTC for encod- ing speech and Transformer encoders for encoding intermediate ASR text. We use Transformer de- coders for both ASR and ST. During inference, the ASR stage is decoded first and then the final MT/ST stage is decoded; both stages use label-synchronous joint CTC/attention beam search. 4.2 SST Models Time-Synchronous Blockwise CTC/Attention (TBCA) As shown in Figure 3, we adapt the aforementioned CTC/attention model for ST (\u00a74.1) Figure 4: Discrete Multi-Decoder (UnitY) for S2ST. to SST by replacing the Conformer encoder with a contextual block Conformer (Tsunoo et al., 2021). During inference, we initially followed Deng et al. (2022) and used the label-synchronous CTC/attention beam search originally proposed for ASR by Tsunoo et al. (2021). However, we found that label-synchrony results in overly conser- vative boundary block detection for SST. Therefore we opt instead for the time-synchronous variant which relies on CTC\u2019s more robust end-detection (Yan et al., 2023) to control boundary block de- tection; this change reduces latency without sacri- ficing quality. To perform incremental decoding without re-translation (as expected by SimulEval), hypotheses are pruned after processing all of the time steps for each encoder block. Blockwise Transducer (BT) As demonstrated by Xue et al. (2022), Transducers can be effec- tively applied to SST despite the monotonic na- ture of their underlying alignment model. We build Transducers for SST using contextual block Conformer encoders and unidirectional LSTM de- coders. We found that the aforementioned hier- archical CTC encoding (\u00a74.1) improves training stability and convergence rate. During inference, we found that the time-synchronous algorithm de- scribed by Saon et al. (2020) outperformed the original Graves decoding (Graves, 2012) and the later proposed alignment-synchronous algorithms (Saon et al., 2020). We also found that length nor- malization is required to avoid overly short outputs. Incremental decoding is applied in the same man- ner as for TBCA. 4.3 S2ST Models Spectral Multi-Decoder (Translatotron 2) Sim- ilar to the MCA model for ST (\u00a74.1), the spec- tral Multi-decoder (Jia et al., 2022a) decomposes S2ST into ST and TTS sub-tasks. The ST sub- TOOLKIT MODEL TYPE DE ES FR avg OFFLINE SPEECH TRANSLATION (ST) BLEU \u2191 Attentional Enc-Dec (AED) NeurST (Zhao et al., 2021) Fairseq (Wang et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v1 (Inaguma et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v2 (this work) Multi-Decoder CTC/Attn (MCA) 22.8 22.7 22.9 27.9 27.4 27.2 28.0 32.1 33.3 32.9 32.8 38.5 27.8 27.6 27.9 32.8 SIMULTANEOUS SPEECH TRANSLATION (SST) BLEU \u2191 / AL \u2193 Fairseq (Wang et al., 2020) ESPnet-ST-v2 (this work) Wait-K Attentional Enc-Dec (WAED) Time-Sync Blockwise CTC/Attn (TBCA) 18.6 / 6.8 23.5 / 2.3 22.9 / 6.9 29.2 / 2.4 28.5 / 6.7 32.7 / 2.3 23.3 / 6.8 28.5 / 2.3 OFFLINE SPEECH-TO-SPEECH TRANSLATION (S2ST) ASR-BLEU \u2191 Fairseq (Inaguma et al., 2022) ESPnet-ST-v2 (this work) Discrete Multi-Decoder (UnitY) Discrete Multi-Decoder (UnitY) 25.5 23.7 32.3 32.0 30.9 33.1 29.6 29.6 Table 2: Overview of ESPnet-ST-v2\u2019s ST, SST, and S2ST performances compared to other open-source toolkits. Results are presented on MuST-C-v1 (English-to-X) for ST/SST and on CVSS-C (X-to-English) for S2ST. task is modeled with an encoder-decoder network while the TTS sub-task is modeled with an auto- regressive synthesizer. The synthesizer attends over both the ST-encoder and ST-decoder hidden states. We use Transformers for the ST encoder-decoder and a Tacotron-style (Wang et al., 2017) decoder as the synthesizer. During inference, we first use beam search for the ST sub-task and then auto- regressively generate Mel-spectrograms. The final waveform speech is generated with a HiFi-GAN vocoder (Kong et al., 2020). MODEL HIERENC BLEU\u2191 Attn Enc-Dec (AED) Multi-Decoder Attn Enc-Dec (MAED) CTC/Attention (CA) Multi-Decoder CTC/Attn (MCA) Transducer (T) - \u2713 \u2713 \u2713 25.7 27.6 28.6 28.8 27.6 Table 3: Example ST models \u2013 results on MuST-C-v2 En-De tst-COMMON. 5.1 Experimental Setup Discrete Multi-Decoder (UnitY) The UnitY model (Inaguma et al., 2022) is similar to Trans- latotron 2, but critically predicts discrete units of speech SSL representations rather than spectral information in the final stage. In other words, UnitY is Multi-decoder consisting of a ST sub- task followed by a text-to-unit (T2U) sub-task (see Figure 4). We use Transformer-based encoder- decoders for both sub-tasks. During inference, the ST stage is first decoded and then followed by the T2U stage. Both stages use label synchronous beam search. The final speech is generated with a unit HiFi-GAN vocoder with Fastspeech-like du- ration prediction (Polyak et al., 2021; Lee et al., 2022a), which is separately trained in the Parallel- WaveGAN toolkit (Hayashi et al., 2020, 2021). Please refer to \u00a7A.1 for reproducibility details. The following is only a summary of our setup. Data We use MuST-C-v1 or MuST-C-v2 (Di Gangi et al., 2019) for ST/SST and CVSS-C for S2ST (Jia et al., 2022b). For IWSLT compar- isons, we combine MuST-C-v1, MuST-C-v2, and ST-TED (Niehues et al., 2018) for ST/SST. Models Unless otherwise indicated, we use a \"base\" setting for our models. Our base models have 40-80M trainable parameters across all tasks and are trained on a \u223c400h of single language pair data from a single corpus. For ST/SST, we also use a \"large\" setting for benchmarking against IWSLT submissions. Our large models have 150-200M trainable parameters and are trained on \u223c1000h of single language pair data from multiple corpora. 5 Performance Benchmarking In this section, we 1) compare open-source toolkits 2) compare our different example models and 3) compare our models with top IWSLT shared task systems and state-of-the-art prior works. Scoring For ST/SST, we evaluate detokenized case-sensitive BLEU (Post, 2018). For SST, we additionally evaluate Average Lagging (AL) (Ma et al., 2020a). For S2ST, we evaluate ASR-BLEU by transcribing the generated speech and then eval- uating the BLEU of this transcription. MODEL KD BT ENS BLEU\u2191 IWSLT\u201921 (Top 3 of 6) 1 Volctrans E2E\u2020 2 OPPO Cascade\u2020 3 Volctrans Cascade\u2020 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.3 22.6 22.2 ESPnet-ST-v2 A Base CA B Base MCA C Large CA D Large MCA - - - - -"}, {"question": " What is the purpose of the Blockwise Transducer in the model?", "answer": " To improve training stability and convergence rate", "ref_chunk": "above, following Yan et al. (2022). We use Con- former encoders with hierarchical CTC for encod- ing speech and Transformer encoders for encoding intermediate ASR text. We use Transformer de- coders for both ASR and ST. During inference, the ASR stage is decoded first and then the final MT/ST stage is decoded; both stages use label-synchronous joint CTC/attention beam search. 4.2 SST Models Time-Synchronous Blockwise CTC/Attention (TBCA) As shown in Figure 3, we adapt the aforementioned CTC/attention model for ST (\u00a74.1) Figure 4: Discrete Multi-Decoder (UnitY) for S2ST. to SST by replacing the Conformer encoder with a contextual block Conformer (Tsunoo et al., 2021). During inference, we initially followed Deng et al. (2022) and used the label-synchronous CTC/attention beam search originally proposed for ASR by Tsunoo et al. (2021). However, we found that label-synchrony results in overly conser- vative boundary block detection for SST. Therefore we opt instead for the time-synchronous variant which relies on CTC\u2019s more robust end-detection (Yan et al., 2023) to control boundary block de- tection; this change reduces latency without sacri- ficing quality. To perform incremental decoding without re-translation (as expected by SimulEval), hypotheses are pruned after processing all of the time steps for each encoder block. Blockwise Transducer (BT) As demonstrated by Xue et al. (2022), Transducers can be effec- tively applied to SST despite the monotonic na- ture of their underlying alignment model. We build Transducers for SST using contextual block Conformer encoders and unidirectional LSTM de- coders. We found that the aforementioned hier- archical CTC encoding (\u00a74.1) improves training stability and convergence rate. During inference, we found that the time-synchronous algorithm de- scribed by Saon et al. (2020) outperformed the original Graves decoding (Graves, 2012) and the later proposed alignment-synchronous algorithms (Saon et al., 2020). We also found that length nor- malization is required to avoid overly short outputs. Incremental decoding is applied in the same man- ner as for TBCA. 4.3 S2ST Models Spectral Multi-Decoder (Translatotron 2) Sim- ilar to the MCA model for ST (\u00a74.1), the spec- tral Multi-decoder (Jia et al., 2022a) decomposes S2ST into ST and TTS sub-tasks. The ST sub- TOOLKIT MODEL TYPE DE ES FR avg OFFLINE SPEECH TRANSLATION (ST) BLEU \u2191 Attentional Enc-Dec (AED) NeurST (Zhao et al., 2021) Fairseq (Wang et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v1 (Inaguma et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v2 (this work) Multi-Decoder CTC/Attn (MCA) 22.8 22.7 22.9 27.9 27.4 27.2 28.0 32.1 33.3 32.9 32.8 38.5 27.8 27.6 27.9 32.8 SIMULTANEOUS SPEECH TRANSLATION (SST) BLEU \u2191 / AL \u2193 Fairseq (Wang et al., 2020) ESPnet-ST-v2 (this work) Wait-K Attentional Enc-Dec (WAED) Time-Sync Blockwise CTC/Attn (TBCA) 18.6 / 6.8 23.5 / 2.3 22.9 / 6.9 29.2 / 2.4 28.5 / 6.7 32.7 / 2.3 23.3 / 6.8 28.5 / 2.3 OFFLINE SPEECH-TO-SPEECH TRANSLATION (S2ST) ASR-BLEU \u2191 Fairseq (Inaguma et al., 2022) ESPnet-ST-v2 (this work) Discrete Multi-Decoder (UnitY) Discrete Multi-Decoder (UnitY) 25.5 23.7 32.3 32.0 30.9 33.1 29.6 29.6 Table 2: Overview of ESPnet-ST-v2\u2019s ST, SST, and S2ST performances compared to other open-source toolkits. Results are presented on MuST-C-v1 (English-to-X) for ST/SST and on CVSS-C (X-to-English) for S2ST. task is modeled with an encoder-decoder network while the TTS sub-task is modeled with an auto- regressive synthesizer. The synthesizer attends over both the ST-encoder and ST-decoder hidden states. We use Transformers for the ST encoder-decoder and a Tacotron-style (Wang et al., 2017) decoder as the synthesizer. During inference, we first use beam search for the ST sub-task and then auto- regressively generate Mel-spectrograms. The final waveform speech is generated with a HiFi-GAN vocoder (Kong et al., 2020). MODEL HIERENC BLEU\u2191 Attn Enc-Dec (AED) Multi-Decoder Attn Enc-Dec (MAED) CTC/Attention (CA) Multi-Decoder CTC/Attn (MCA) Transducer (T) - \u2713 \u2713 \u2713 25.7 27.6 28.6 28.8 27.6 Table 3: Example ST models \u2013 results on MuST-C-v2 En-De tst-COMMON. 5.1 Experimental Setup Discrete Multi-Decoder (UnitY) The UnitY model (Inaguma et al., 2022) is similar to Trans- latotron 2, but critically predicts discrete units of speech SSL representations rather than spectral information in the final stage. In other words, UnitY is Multi-decoder consisting of a ST sub- task followed by a text-to-unit (T2U) sub-task (see Figure 4). We use Transformer-based encoder- decoders for both sub-tasks. During inference, the ST stage is first decoded and then followed by the T2U stage. Both stages use label synchronous beam search. The final speech is generated with a unit HiFi-GAN vocoder with Fastspeech-like du- ration prediction (Polyak et al., 2021; Lee et al., 2022a), which is separately trained in the Parallel- WaveGAN toolkit (Hayashi et al., 2020, 2021). Please refer to \u00a7A.1 for reproducibility details. The following is only a summary of our setup. Data We use MuST-C-v1 or MuST-C-v2 (Di Gangi et al., 2019) for ST/SST and CVSS-C for S2ST (Jia et al., 2022b). For IWSLT compar- isons, we combine MuST-C-v1, MuST-C-v2, and ST-TED (Niehues et al., 2018) for ST/SST. Models Unless otherwise indicated, we use a \"base\" setting for our models. Our base models have 40-80M trainable parameters across all tasks and are trained on a \u223c400h of single language pair data from a single corpus. For ST/SST, we also use a \"large\" setting for benchmarking against IWSLT submissions. Our large models have 150-200M trainable parameters and are trained on \u223c1000h of single language pair data from multiple corpora. 5 Performance Benchmarking In this section, we 1) compare open-source toolkits 2) compare our different example models and 3) compare our models with top IWSLT shared task systems and state-of-the-art prior works. Scoring For ST/SST, we evaluate detokenized case-sensitive BLEU (Post, 2018). For SST, we additionally evaluate Average Lagging (AL) (Ma et al., 2020a). For S2ST, we evaluate ASR-BLEU by transcribing the generated speech and then eval- uating the BLEU of this transcription. MODEL KD BT ENS BLEU\u2191 IWSLT\u201921 (Top 3 of 6) 1 Volctrans E2E\u2020 2 OPPO Cascade\u2020 3 Volctrans Cascade\u2020 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.3 22.6 22.2 ESPnet-ST-v2 A Base CA B Base MCA C Large CA D Large MCA - - - - -"}, {"question": " How are the duration predictions handled in the UnitY model?", "answer": " Using Fastspeech-like duration prediction", "ref_chunk": "above, following Yan et al. (2022). We use Con- former encoders with hierarchical CTC for encod- ing speech and Transformer encoders for encoding intermediate ASR text. We use Transformer de- coders for both ASR and ST. During inference, the ASR stage is decoded first and then the final MT/ST stage is decoded; both stages use label-synchronous joint CTC/attention beam search. 4.2 SST Models Time-Synchronous Blockwise CTC/Attention (TBCA) As shown in Figure 3, we adapt the aforementioned CTC/attention model for ST (\u00a74.1) Figure 4: Discrete Multi-Decoder (UnitY) for S2ST. to SST by replacing the Conformer encoder with a contextual block Conformer (Tsunoo et al., 2021). During inference, we initially followed Deng et al. (2022) and used the label-synchronous CTC/attention beam search originally proposed for ASR by Tsunoo et al. (2021). However, we found that label-synchrony results in overly conser- vative boundary block detection for SST. Therefore we opt instead for the time-synchronous variant which relies on CTC\u2019s more robust end-detection (Yan et al., 2023) to control boundary block de- tection; this change reduces latency without sacri- ficing quality. To perform incremental decoding without re-translation (as expected by SimulEval), hypotheses are pruned after processing all of the time steps for each encoder block. Blockwise Transducer (BT) As demonstrated by Xue et al. (2022), Transducers can be effec- tively applied to SST despite the monotonic na- ture of their underlying alignment model. We build Transducers for SST using contextual block Conformer encoders and unidirectional LSTM de- coders. We found that the aforementioned hier- archical CTC encoding (\u00a74.1) improves training stability and convergence rate. During inference, we found that the time-synchronous algorithm de- scribed by Saon et al. (2020) outperformed the original Graves decoding (Graves, 2012) and the later proposed alignment-synchronous algorithms (Saon et al., 2020). We also found that length nor- malization is required to avoid overly short outputs. Incremental decoding is applied in the same man- ner as for TBCA. 4.3 S2ST Models Spectral Multi-Decoder (Translatotron 2) Sim- ilar to the MCA model for ST (\u00a74.1), the spec- tral Multi-decoder (Jia et al., 2022a) decomposes S2ST into ST and TTS sub-tasks. The ST sub- TOOLKIT MODEL TYPE DE ES FR avg OFFLINE SPEECH TRANSLATION (ST) BLEU \u2191 Attentional Enc-Dec (AED) NeurST (Zhao et al., 2021) Fairseq (Wang et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v1 (Inaguma et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v2 (this work) Multi-Decoder CTC/Attn (MCA) 22.8 22.7 22.9 27.9 27.4 27.2 28.0 32.1 33.3 32.9 32.8 38.5 27.8 27.6 27.9 32.8 SIMULTANEOUS SPEECH TRANSLATION (SST) BLEU \u2191 / AL \u2193 Fairseq (Wang et al., 2020) ESPnet-ST-v2 (this work) Wait-K Attentional Enc-Dec (WAED) Time-Sync Blockwise CTC/Attn (TBCA) 18.6 / 6.8 23.5 / 2.3 22.9 / 6.9 29.2 / 2.4 28.5 / 6.7 32.7 / 2.3 23.3 / 6.8 28.5 / 2.3 OFFLINE SPEECH-TO-SPEECH TRANSLATION (S2ST) ASR-BLEU \u2191 Fairseq (Inaguma et al., 2022) ESPnet-ST-v2 (this work) Discrete Multi-Decoder (UnitY) Discrete Multi-Decoder (UnitY) 25.5 23.7 32.3 32.0 30.9 33.1 29.6 29.6 Table 2: Overview of ESPnet-ST-v2\u2019s ST, SST, and S2ST performances compared to other open-source toolkits. Results are presented on MuST-C-v1 (English-to-X) for ST/SST and on CVSS-C (X-to-English) for S2ST. task is modeled with an encoder-decoder network while the TTS sub-task is modeled with an auto- regressive synthesizer. The synthesizer attends over both the ST-encoder and ST-decoder hidden states. We use Transformers for the ST encoder-decoder and a Tacotron-style (Wang et al., 2017) decoder as the synthesizer. During inference, we first use beam search for the ST sub-task and then auto- regressively generate Mel-spectrograms. The final waveform speech is generated with a HiFi-GAN vocoder (Kong et al., 2020). MODEL HIERENC BLEU\u2191 Attn Enc-Dec (AED) Multi-Decoder Attn Enc-Dec (MAED) CTC/Attention (CA) Multi-Decoder CTC/Attn (MCA) Transducer (T) - \u2713 \u2713 \u2713 25.7 27.6 28.6 28.8 27.6 Table 3: Example ST models \u2013 results on MuST-C-v2 En-De tst-COMMON. 5.1 Experimental Setup Discrete Multi-Decoder (UnitY) The UnitY model (Inaguma et al., 2022) is similar to Trans- latotron 2, but critically predicts discrete units of speech SSL representations rather than spectral information in the final stage. In other words, UnitY is Multi-decoder consisting of a ST sub- task followed by a text-to-unit (T2U) sub-task (see Figure 4). We use Transformer-based encoder- decoders for both sub-tasks. During inference, the ST stage is first decoded and then followed by the T2U stage. Both stages use label synchronous beam search. The final speech is generated with a unit HiFi-GAN vocoder with Fastspeech-like du- ration prediction (Polyak et al., 2021; Lee et al., 2022a), which is separately trained in the Parallel- WaveGAN toolkit (Hayashi et al., 2020, 2021). Please refer to \u00a7A.1 for reproducibility details. The following is only a summary of our setup. Data We use MuST-C-v1 or MuST-C-v2 (Di Gangi et al., 2019) for ST/SST and CVSS-C for S2ST (Jia et al., 2022b). For IWSLT compar- isons, we combine MuST-C-v1, MuST-C-v2, and ST-TED (Niehues et al., 2018) for ST/SST. Models Unless otherwise indicated, we use a \"base\" setting for our models. Our base models have 40-80M trainable parameters across all tasks and are trained on a \u223c400h of single language pair data from a single corpus. For ST/SST, we also use a \"large\" setting for benchmarking against IWSLT submissions. Our large models have 150-200M trainable parameters and are trained on \u223c1000h of single language pair data from multiple corpora. 5 Performance Benchmarking In this section, we 1) compare open-source toolkits 2) compare our different example models and 3) compare our models with top IWSLT shared task systems and state-of-the-art prior works. Scoring For ST/SST, we evaluate detokenized case-sensitive BLEU (Post, 2018). For SST, we additionally evaluate Average Lagging (AL) (Ma et al., 2020a). For S2ST, we evaluate ASR-BLEU by transcribing the generated speech and then eval- uating the BLEU of this transcription. MODEL KD BT ENS BLEU\u2191 IWSLT\u201921 (Top 3 of 6) 1 Volctrans E2E\u2020 2 OPPO Cascade\u2020 3 Volctrans Cascade\u2020 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.3 22.6 22.2 ESPnet-ST-v2 A Base CA B Base MCA C Large CA D Large MCA - - - - -"}], "doc_text": "above, following Yan et al. (2022). We use Con- former encoders with hierarchical CTC for encod- ing speech and Transformer encoders for encoding intermediate ASR text. We use Transformer de- coders for both ASR and ST. During inference, the ASR stage is decoded first and then the final MT/ST stage is decoded; both stages use label-synchronous joint CTC/attention beam search. 4.2 SST Models Time-Synchronous Blockwise CTC/Attention (TBCA) As shown in Figure 3, we adapt the aforementioned CTC/attention model for ST (\u00a74.1) Figure 4: Discrete Multi-Decoder (UnitY) for S2ST. to SST by replacing the Conformer encoder with a contextual block Conformer (Tsunoo et al., 2021). During inference, we initially followed Deng et al. (2022) and used the label-synchronous CTC/attention beam search originally proposed for ASR by Tsunoo et al. (2021). However, we found that label-synchrony results in overly conser- vative boundary block detection for SST. Therefore we opt instead for the time-synchronous variant which relies on CTC\u2019s more robust end-detection (Yan et al., 2023) to control boundary block de- tection; this change reduces latency without sacri- ficing quality. To perform incremental decoding without re-translation (as expected by SimulEval), hypotheses are pruned after processing all of the time steps for each encoder block. Blockwise Transducer (BT) As demonstrated by Xue et al. (2022), Transducers can be effec- tively applied to SST despite the monotonic na- ture of their underlying alignment model. We build Transducers for SST using contextual block Conformer encoders and unidirectional LSTM de- coders. We found that the aforementioned hier- archical CTC encoding (\u00a74.1) improves training stability and convergence rate. During inference, we found that the time-synchronous algorithm de- scribed by Saon et al. (2020) outperformed the original Graves decoding (Graves, 2012) and the later proposed alignment-synchronous algorithms (Saon et al., 2020). We also found that length nor- malization is required to avoid overly short outputs. Incremental decoding is applied in the same man- ner as for TBCA. 4.3 S2ST Models Spectral Multi-Decoder (Translatotron 2) Sim- ilar to the MCA model for ST (\u00a74.1), the spec- tral Multi-decoder (Jia et al., 2022a) decomposes S2ST into ST and TTS sub-tasks. The ST sub- TOOLKIT MODEL TYPE DE ES FR avg OFFLINE SPEECH TRANSLATION (ST) BLEU \u2191 Attentional Enc-Dec (AED) NeurST (Zhao et al., 2021) Fairseq (Wang et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v1 (Inaguma et al., 2020) Attentional Enc-Dec (AED) ESPnet-ST-v2 (this work) Multi-Decoder CTC/Attn (MCA) 22.8 22.7 22.9 27.9 27.4 27.2 28.0 32.1 33.3 32.9 32.8 38.5 27.8 27.6 27.9 32.8 SIMULTANEOUS SPEECH TRANSLATION (SST) BLEU \u2191 / AL \u2193 Fairseq (Wang et al., 2020) ESPnet-ST-v2 (this work) Wait-K Attentional Enc-Dec (WAED) Time-Sync Blockwise CTC/Attn (TBCA) 18.6 / 6.8 23.5 / 2.3 22.9 / 6.9 29.2 / 2.4 28.5 / 6.7 32.7 / 2.3 23.3 / 6.8 28.5 / 2.3 OFFLINE SPEECH-TO-SPEECH TRANSLATION (S2ST) ASR-BLEU \u2191 Fairseq (Inaguma et al., 2022) ESPnet-ST-v2 (this work) Discrete Multi-Decoder (UnitY) Discrete Multi-Decoder (UnitY) 25.5 23.7 32.3 32.0 30.9 33.1 29.6 29.6 Table 2: Overview of ESPnet-ST-v2\u2019s ST, SST, and S2ST performances compared to other open-source toolkits. Results are presented on MuST-C-v1 (English-to-X) for ST/SST and on CVSS-C (X-to-English) for S2ST. task is modeled with an encoder-decoder network while the TTS sub-task is modeled with an auto- regressive synthesizer. The synthesizer attends over both the ST-encoder and ST-decoder hidden states. We use Transformers for the ST encoder-decoder and a Tacotron-style (Wang et al., 2017) decoder as the synthesizer. During inference, we first use beam search for the ST sub-task and then auto- regressively generate Mel-spectrograms. The final waveform speech is generated with a HiFi-GAN vocoder (Kong et al., 2020). MODEL HIERENC BLEU\u2191 Attn Enc-Dec (AED) Multi-Decoder Attn Enc-Dec (MAED) CTC/Attention (CA) Multi-Decoder CTC/Attn (MCA) Transducer (T) - \u2713 \u2713 \u2713 25.7 27.6 28.6 28.8 27.6 Table 3: Example ST models \u2013 results on MuST-C-v2 En-De tst-COMMON. 5.1 Experimental Setup Discrete Multi-Decoder (UnitY) The UnitY model (Inaguma et al., 2022) is similar to Trans- latotron 2, but critically predicts discrete units of speech SSL representations rather than spectral information in the final stage. In other words, UnitY is Multi-decoder consisting of a ST sub- task followed by a text-to-unit (T2U) sub-task (see Figure 4). We use Transformer-based encoder- decoders for both sub-tasks. During inference, the ST stage is first decoded and then followed by the T2U stage. Both stages use label synchronous beam search. The final speech is generated with a unit HiFi-GAN vocoder with Fastspeech-like du- ration prediction (Polyak et al., 2021; Lee et al., 2022a), which is separately trained in the Parallel- WaveGAN toolkit (Hayashi et al., 2020, 2021). Please refer to \u00a7A.1 for reproducibility details. The following is only a summary of our setup. Data We use MuST-C-v1 or MuST-C-v2 (Di Gangi et al., 2019) for ST/SST and CVSS-C for S2ST (Jia et al., 2022b). For IWSLT compar- isons, we combine MuST-C-v1, MuST-C-v2, and ST-TED (Niehues et al., 2018) for ST/SST. Models Unless otherwise indicated, we use a \"base\" setting for our models. Our base models have 40-80M trainable parameters across all tasks and are trained on a \u223c400h of single language pair data from a single corpus. For ST/SST, we also use a \"large\" setting for benchmarking against IWSLT submissions. Our large models have 150-200M trainable parameters and are trained on \u223c1000h of single language pair data from multiple corpora. 5 Performance Benchmarking In this section, we 1) compare open-source toolkits 2) compare our different example models and 3) compare our models with top IWSLT shared task systems and state-of-the-art prior works. Scoring For ST/SST, we evaluate detokenized case-sensitive BLEU (Post, 2018). For SST, we additionally evaluate Average Lagging (AL) (Ma et al., 2020a). For S2ST, we evaluate ASR-BLEU by transcribing the generated speech and then eval- uating the BLEU of this transcription. MODEL KD BT ENS BLEU\u2191 IWSLT\u201921 (Top 3 of 6) 1 Volctrans E2E\u2020 2 OPPO Cascade\u2020 3 Volctrans Cascade\u2020 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.3 22.6 22.2 ESPnet-ST-v2 A Base CA B Base MCA C Large CA D Large MCA - - - - -"}