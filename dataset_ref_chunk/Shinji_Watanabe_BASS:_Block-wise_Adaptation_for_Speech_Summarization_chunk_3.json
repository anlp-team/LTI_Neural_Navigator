{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_BASS:_Block-wise_Adaptation_for_Speech_Summarization_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does Equation 6 describe?", "answer": " Equation 6 describes the process of identifying the most likely hypothesis summary \u02c6Y given the input X and semantic representation S.", "ref_chunk": "arg max Y \u2248 arg max Y (cid:88) P(Y, S|X) S max S P (Y, S|X) (6) Equation 6 describes the process of identifying the most likely hypothesis summary \u02c6Y given the input X and seman- tic representation S. From Figure 1, based on the condi- tional independence between the summary Y and acoustics X given semantic representation S, we can write P(Y, S|X) = P(Y |X)P(S|X). Thus, we can obtain the solution for Equa- tion 6 using the coordinate descent update shown in Equation 7. \u02c6S = arg max S \u02c6Y \u2248 arg max Y P(S|X) P(Y | \u02c6S) max S P(S|X) (7) From Equations 7 and 5, BASS estimates P(Y i|X 1:i, Y 1:i\u22121) using Equation 8. P(Y i|X 1:i) = P(Y i|Si)P(Si|S1:i\u22121, X i) (8) The encoder and decoder model the probabilities P(S|X) and P(Y |S) respectively. The updater uses the past semantic embeddings and the current encoder output to produce the cur- rent semantic embedding. Figure 2 shows three alternate struc- tures for our updater to aggregate semantic context from the prior and the current block: 1. Concatenation: Si = Concat(Si\u22121, Encoder(X i)) The current semantic embedding is obtained by concatenat- ing the embeddings from the previous and current blocks. 2. Gated Attention: Si = Encoder(X i) + w \u00b7 Attn(Si\u22121, Encoder(X i)) The current and previous semantic embeddings are combined using an attention mechanism and incorporated into the final embedding as a weighted sum. 3. Hierarchical Attention: Si = Attn([Attn(Si\u22121, Di), Attn(Encoder(X i)), Di]), Di) This method performs the context passing within each de- coder block, based on hierarchical attention [22]. We com- pute attention for the current decoder state Di with the previ- ous and current semantic embeddings independently. Then, we stack the two attention outputs and perform a second level of attention between this result and the decoder state. 3. Experimental Setup 3.1. Dataset Table 1: Statistics of the How-2 2,000h Dataset used for model training and evaluation. The maximum input length N (in frames), and maximum output length L (in tokens) are shown. Set Max N Max L #Videos Train Test 145,082 39,537 173 152 68,336 2,127 The How-2 Dataset [23] contains 2,000h of instructional videos with corresponding text transcripts, video, speech, trans- lations, and summaries. Abstractive summaries are generated based on user-provided descriptions of the videos. Table 1 high- lights the number of videos in the train and test partitions of the How2 data. The model features and reference summaries have been made public 1 by the authors of [7]. 3.2. Model Hyperparameters and Evaluation Models: Our models use ESPNet22 [24] and are first pre- trained on the ASR task and then fine-tuned for summariza- tion. The encoder consists of convolutional subsampling by factor 4, followed by 12 conformer [10] blocks with 8 attention heads and a hidden size of 2048. The decoder has 6 transformer blocks, with 4 attention heads and a hidden size of 2048. The total number of model parameters is 103M. Both the encoder and decoder use a dropout rate of 0.2. We use 43-dimensional filter bank and pitch features as input to the encoder. ASR: ASR models are trained with Connectionist Temporal Classification (CTC) and Cross-Entropy loss with CTC weight of 0.3. We use the Adam optimizer with peak lr=0.001, and a warmup scheduler for ASR pre-training. This takes 2 days on 8 V-100 32G GPUs SUMM: Our summarization models are trained with cross- entropy loss and label smoothing of 0.15. During inference, we use a beam size of 8. Model averaging was not performed as it was found to hurt summarization performance. Fine-tuning is run for a day on one A40 GPU. BASS: For BASS models, we use a block size of 1,000 input frames, corresponding to 10s of audio. We only use the seman- tic embedding from the previous block as context for the current block unless otherwise specified. Evaluation: We evaluate our models with ROUGE [25], ME- TEOR [26], and BERTScore [27], which are the most common automatic metrics for evaluating summarization models. 4. Experimental Results 4.1. Truncated Input Baselines First, we train end-to-end summarization baseline models on truncated inputs (Trunc) that are 10 seconds long and 30 sec- onds long. Table 2 reports the results of training on truncated inputs and evaluating recordings that are 10 seconds and 30 seconds long, compared to different state-of-the-art approaches referenced in prior work. We note that using the standard full multi-head attention provides significant gains over restricted self-attention, and therefore use the standard multi-head self- attention for our experiments. 1https://github.com/srvk/how2-dataset 2Code will be released in https://github.com/espnet/espnet Table 2: Performance of Block-wise Adaptation and Training Approaches compared to Truncated Baselines with different inference strategies using ROUGE, METEOR, and BERTScore metrics- higher scores indicate better performance Training Method Inference Pre-training Train Maxlen Inf. Maxlen ROUGE-1\u2191 ROUGE-2\u2191 ROUGE-L\u2191 METEOR\u2191 BERTScore\u2191 Trunc, Restricted Self-Attention [7] Trunc, Full Self-Attention [8] + TTS Augmentation [8] Trunc-Baseline Trunc-Baseline Trunc-Baseline BASS-Adapt BASS-Train Standard Standard Standard Standard Standard Standard Block Block X X X X X X 10s X 100s 100s 100s 10s 30s 60s 30s 30s 100s 100s 100s 10s 30s 60s 30s 30s 60.73 65.30 68.40 60.87 63.30 64.57 63.99 60.87 44.9 51.40 54.10 45.12 47.58 49.11 49.00 43.12 56.10 62.10 65.00 56.79 59.16 60.49 60.17 54.79 29.30 32.50 34.90 30.00 31.76 32.47 32.17 29.12 Table 3: Part-of-speech coverage between the Predicted Sum- mary and the Reference for Truncated 10s baseline and BASS- ADAPT 30s model Model Noun Verb Adj Adv Prop.Noun recordings comprising 10-second chunks can do as well as a model trained on 60-second recordings. The proposed approach is more computationally efficient than the baseline by a factor of 3 since the proposed approach uses 3x smaller inputs 3 times for quadratic self-attentions. Baseline 10-sec BASS-ADAPT 0.85 0.87 0.76 0.79 0.84 0.84 0.65 0.67 Table 4: Performance of BASS models with block level inference across different implementations of the semantic updater model. Models are pre-trained on 10s and fine-tuned on 30s. R-1, R-2, R-3 represent the ROUGE-1, ROUGE-2, and ROUGE-L"}, {"question": " How can we write P(Y, S|X) based on the conditional independence between summary Y and acoustics X given semantic representation S?", "answer": " P(Y, S|X) = P(Y |X)P(S|X)", "ref_chunk": "arg max Y \u2248 arg max Y (cid:88) P(Y, S|X) S max S P (Y, S|X) (6) Equation 6 describes the process of identifying the most likely hypothesis summary \u02c6Y given the input X and seman- tic representation S. From Figure 1, based on the condi- tional independence between the summary Y and acoustics X given semantic representation S, we can write P(Y, S|X) = P(Y |X)P(S|X). Thus, we can obtain the solution for Equa- tion 6 using the coordinate descent update shown in Equation 7. \u02c6S = arg max S \u02c6Y \u2248 arg max Y P(S|X) P(Y | \u02c6S) max S P(S|X) (7) From Equations 7 and 5, BASS estimates P(Y i|X 1:i, Y 1:i\u22121) using Equation 8. P(Y i|X 1:i) = P(Y i|Si)P(Si|S1:i\u22121, X i) (8) The encoder and decoder model the probabilities P(S|X) and P(Y |S) respectively. The updater uses the past semantic embeddings and the current encoder output to produce the cur- rent semantic embedding. Figure 2 shows three alternate struc- tures for our updater to aggregate semantic context from the prior and the current block: 1. Concatenation: Si = Concat(Si\u22121, Encoder(X i)) The current semantic embedding is obtained by concatenat- ing the embeddings from the previous and current blocks. 2. Gated Attention: Si = Encoder(X i) + w \u00b7 Attn(Si\u22121, Encoder(X i)) The current and previous semantic embeddings are combined using an attention mechanism and incorporated into the final embedding as a weighted sum. 3. Hierarchical Attention: Si = Attn([Attn(Si\u22121, Di), Attn(Encoder(X i)), Di]), Di) This method performs the context passing within each de- coder block, based on hierarchical attention [22]. We com- pute attention for the current decoder state Di with the previ- ous and current semantic embeddings independently. Then, we stack the two attention outputs and perform a second level of attention between this result and the decoder state. 3. Experimental Setup 3.1. Dataset Table 1: Statistics of the How-2 2,000h Dataset used for model training and evaluation. The maximum input length N (in frames), and maximum output length L (in tokens) are shown. Set Max N Max L #Videos Train Test 145,082 39,537 173 152 68,336 2,127 The How-2 Dataset [23] contains 2,000h of instructional videos with corresponding text transcripts, video, speech, trans- lations, and summaries. Abstractive summaries are generated based on user-provided descriptions of the videos. Table 1 high- lights the number of videos in the train and test partitions of the How2 data. The model features and reference summaries have been made public 1 by the authors of [7]. 3.2. Model Hyperparameters and Evaluation Models: Our models use ESPNet22 [24] and are first pre- trained on the ASR task and then fine-tuned for summariza- tion. The encoder consists of convolutional subsampling by factor 4, followed by 12 conformer [10] blocks with 8 attention heads and a hidden size of 2048. The decoder has 6 transformer blocks, with 4 attention heads and a hidden size of 2048. The total number of model parameters is 103M. Both the encoder and decoder use a dropout rate of 0.2. We use 43-dimensional filter bank and pitch features as input to the encoder. ASR: ASR models are trained with Connectionist Temporal Classification (CTC) and Cross-Entropy loss with CTC weight of 0.3. We use the Adam optimizer with peak lr=0.001, and a warmup scheduler for ASR pre-training. This takes 2 days on 8 V-100 32G GPUs SUMM: Our summarization models are trained with cross- entropy loss and label smoothing of 0.15. During inference, we use a beam size of 8. Model averaging was not performed as it was found to hurt summarization performance. Fine-tuning is run for a day on one A40 GPU. BASS: For BASS models, we use a block size of 1,000 input frames, corresponding to 10s of audio. We only use the seman- tic embedding from the previous block as context for the current block unless otherwise specified. Evaluation: We evaluate our models with ROUGE [25], ME- TEOR [26], and BERTScore [27], which are the most common automatic metrics for evaluating summarization models. 4. Experimental Results 4.1. Truncated Input Baselines First, we train end-to-end summarization baseline models on truncated inputs (Trunc) that are 10 seconds long and 30 sec- onds long. Table 2 reports the results of training on truncated inputs and evaluating recordings that are 10 seconds and 30 seconds long, compared to different state-of-the-art approaches referenced in prior work. We note that using the standard full multi-head attention provides significant gains over restricted self-attention, and therefore use the standard multi-head self- attention for our experiments. 1https://github.com/srvk/how2-dataset 2Code will be released in https://github.com/espnet/espnet Table 2: Performance of Block-wise Adaptation and Training Approaches compared to Truncated Baselines with different inference strategies using ROUGE, METEOR, and BERTScore metrics- higher scores indicate better performance Training Method Inference Pre-training Train Maxlen Inf. Maxlen ROUGE-1\u2191 ROUGE-2\u2191 ROUGE-L\u2191 METEOR\u2191 BERTScore\u2191 Trunc, Restricted Self-Attention [7] Trunc, Full Self-Attention [8] + TTS Augmentation [8] Trunc-Baseline Trunc-Baseline Trunc-Baseline BASS-Adapt BASS-Train Standard Standard Standard Standard Standard Standard Block Block X X X X X X 10s X 100s 100s 100s 10s 30s 60s 30s 30s 100s 100s 100s 10s 30s 60s 30s 30s 60.73 65.30 68.40 60.87 63.30 64.57 63.99 60.87 44.9 51.40 54.10 45.12 47.58 49.11 49.00 43.12 56.10 62.10 65.00 56.79 59.16 60.49 60.17 54.79 29.30 32.50 34.90 30.00 31.76 32.47 32.17 29.12 Table 3: Part-of-speech coverage between the Predicted Sum- mary and the Reference for Truncated 10s baseline and BASS- ADAPT 30s model Model Noun Verb Adj Adv Prop.Noun recordings comprising 10-second chunks can do as well as a model trained on 60-second recordings. The proposed approach is more computationally efficient than the baseline by a factor of 3 since the proposed approach uses 3x smaller inputs 3 times for quadratic self-attentions. Baseline 10-sec BASS-ADAPT 0.85 0.87 0.76 0.79 0.84 0.84 0.65 0.67 Table 4: Performance of BASS models with block level inference across different implementations of the semantic updater model. Models are pre-trained on 10s and fine-tuned on 30s. R-1, R-2, R-3 represent the ROUGE-1, ROUGE-2, and ROUGE-L"}, {"question": " What does Equation 8 estimate according to Equations 7 and 5?", "answer": " Equation 8 estimates P(Y i|X 1:i, Y 1:i\u22121) using Equations 7 and 5.", "ref_chunk": "arg max Y \u2248 arg max Y (cid:88) P(Y, S|X) S max S P (Y, S|X) (6) Equation 6 describes the process of identifying the most likely hypothesis summary \u02c6Y given the input X and seman- tic representation S. From Figure 1, based on the condi- tional independence between the summary Y and acoustics X given semantic representation S, we can write P(Y, S|X) = P(Y |X)P(S|X). Thus, we can obtain the solution for Equa- tion 6 using the coordinate descent update shown in Equation 7. \u02c6S = arg max S \u02c6Y \u2248 arg max Y P(S|X) P(Y | \u02c6S) max S P(S|X) (7) From Equations 7 and 5, BASS estimates P(Y i|X 1:i, Y 1:i\u22121) using Equation 8. P(Y i|X 1:i) = P(Y i|Si)P(Si|S1:i\u22121, X i) (8) The encoder and decoder model the probabilities P(S|X) and P(Y |S) respectively. The updater uses the past semantic embeddings and the current encoder output to produce the cur- rent semantic embedding. Figure 2 shows three alternate struc- tures for our updater to aggregate semantic context from the prior and the current block: 1. Concatenation: Si = Concat(Si\u22121, Encoder(X i)) The current semantic embedding is obtained by concatenat- ing the embeddings from the previous and current blocks. 2. Gated Attention: Si = Encoder(X i) + w \u00b7 Attn(Si\u22121, Encoder(X i)) The current and previous semantic embeddings are combined using an attention mechanism and incorporated into the final embedding as a weighted sum. 3. Hierarchical Attention: Si = Attn([Attn(Si\u22121, Di), Attn(Encoder(X i)), Di]), Di) This method performs the context passing within each de- coder block, based on hierarchical attention [22]. We com- pute attention for the current decoder state Di with the previ- ous and current semantic embeddings independently. Then, we stack the two attention outputs and perform a second level of attention between this result and the decoder state. 3. Experimental Setup 3.1. Dataset Table 1: Statistics of the How-2 2,000h Dataset used for model training and evaluation. The maximum input length N (in frames), and maximum output length L (in tokens) are shown. Set Max N Max L #Videos Train Test 145,082 39,537 173 152 68,336 2,127 The How-2 Dataset [23] contains 2,000h of instructional videos with corresponding text transcripts, video, speech, trans- lations, and summaries. Abstractive summaries are generated based on user-provided descriptions of the videos. Table 1 high- lights the number of videos in the train and test partitions of the How2 data. The model features and reference summaries have been made public 1 by the authors of [7]. 3.2. Model Hyperparameters and Evaluation Models: Our models use ESPNet22 [24] and are first pre- trained on the ASR task and then fine-tuned for summariza- tion. The encoder consists of convolutional subsampling by factor 4, followed by 12 conformer [10] blocks with 8 attention heads and a hidden size of 2048. The decoder has 6 transformer blocks, with 4 attention heads and a hidden size of 2048. The total number of model parameters is 103M. Both the encoder and decoder use a dropout rate of 0.2. We use 43-dimensional filter bank and pitch features as input to the encoder. ASR: ASR models are trained with Connectionist Temporal Classification (CTC) and Cross-Entropy loss with CTC weight of 0.3. We use the Adam optimizer with peak lr=0.001, and a warmup scheduler for ASR pre-training. This takes 2 days on 8 V-100 32G GPUs SUMM: Our summarization models are trained with cross- entropy loss and label smoothing of 0.15. During inference, we use a beam size of 8. Model averaging was not performed as it was found to hurt summarization performance. Fine-tuning is run for a day on one A40 GPU. BASS: For BASS models, we use a block size of 1,000 input frames, corresponding to 10s of audio. We only use the seman- tic embedding from the previous block as context for the current block unless otherwise specified. Evaluation: We evaluate our models with ROUGE [25], ME- TEOR [26], and BERTScore [27], which are the most common automatic metrics for evaluating summarization models. 4. Experimental Results 4.1. Truncated Input Baselines First, we train end-to-end summarization baseline models on truncated inputs (Trunc) that are 10 seconds long and 30 sec- onds long. Table 2 reports the results of training on truncated inputs and evaluating recordings that are 10 seconds and 30 seconds long, compared to different state-of-the-art approaches referenced in prior work. We note that using the standard full multi-head attention provides significant gains over restricted self-attention, and therefore use the standard multi-head self- attention for our experiments. 1https://github.com/srvk/how2-dataset 2Code will be released in https://github.com/espnet/espnet Table 2: Performance of Block-wise Adaptation and Training Approaches compared to Truncated Baselines with different inference strategies using ROUGE, METEOR, and BERTScore metrics- higher scores indicate better performance Training Method Inference Pre-training Train Maxlen Inf. Maxlen ROUGE-1\u2191 ROUGE-2\u2191 ROUGE-L\u2191 METEOR\u2191 BERTScore\u2191 Trunc, Restricted Self-Attention [7] Trunc, Full Self-Attention [8] + TTS Augmentation [8] Trunc-Baseline Trunc-Baseline Trunc-Baseline BASS-Adapt BASS-Train Standard Standard Standard Standard Standard Standard Block Block X X X X X X 10s X 100s 100s 100s 10s 30s 60s 30s 30s 100s 100s 100s 10s 30s 60s 30s 30s 60.73 65.30 68.40 60.87 63.30 64.57 63.99 60.87 44.9 51.40 54.10 45.12 47.58 49.11 49.00 43.12 56.10 62.10 65.00 56.79 59.16 60.49 60.17 54.79 29.30 32.50 34.90 30.00 31.76 32.47 32.17 29.12 Table 3: Part-of-speech coverage between the Predicted Sum- mary and the Reference for Truncated 10s baseline and BASS- ADAPT 30s model Model Noun Verb Adj Adv Prop.Noun recordings comprising 10-second chunks can do as well as a model trained on 60-second recordings. The proposed approach is more computationally efficient than the baseline by a factor of 3 since the proposed approach uses 3x smaller inputs 3 times for quadratic self-attentions. Baseline 10-sec BASS-ADAPT 0.85 0.87 0.76 0.79 0.84 0.84 0.65 0.67 Table 4: Performance of BASS models with block level inference across different implementations of the semantic updater model. Models are pre-trained on 10s and fine-tuned on 30s. R-1, R-2, R-3 represent the ROUGE-1, ROUGE-2, and ROUGE-L"}, {"question": " What do the encoder and decoder model the probabilities of?", "answer": " The encoder and decoder model the probabilities of P(S|X) and P(Y |S) respectively.", "ref_chunk": "arg max Y \u2248 arg max Y (cid:88) P(Y, S|X) S max S P (Y, S|X) (6) Equation 6 describes the process of identifying the most likely hypothesis summary \u02c6Y given the input X and seman- tic representation S. From Figure 1, based on the condi- tional independence between the summary Y and acoustics X given semantic representation S, we can write P(Y, S|X) = P(Y |X)P(S|X). Thus, we can obtain the solution for Equa- tion 6 using the coordinate descent update shown in Equation 7. \u02c6S = arg max S \u02c6Y \u2248 arg max Y P(S|X) P(Y | \u02c6S) max S P(S|X) (7) From Equations 7 and 5, BASS estimates P(Y i|X 1:i, Y 1:i\u22121) using Equation 8. P(Y i|X 1:i) = P(Y i|Si)P(Si|S1:i\u22121, X i) (8) The encoder and decoder model the probabilities P(S|X) and P(Y |S) respectively. The updater uses the past semantic embeddings and the current encoder output to produce the cur- rent semantic embedding. Figure 2 shows three alternate struc- tures for our updater to aggregate semantic context from the prior and the current block: 1. Concatenation: Si = Concat(Si\u22121, Encoder(X i)) The current semantic embedding is obtained by concatenat- ing the embeddings from the previous and current blocks. 2. Gated Attention: Si = Encoder(X i) + w \u00b7 Attn(Si\u22121, Encoder(X i)) The current and previous semantic embeddings are combined using an attention mechanism and incorporated into the final embedding as a weighted sum. 3. Hierarchical Attention: Si = Attn([Attn(Si\u22121, Di), Attn(Encoder(X i)), Di]), Di) This method performs the context passing within each de- coder block, based on hierarchical attention [22]. We com- pute attention for the current decoder state Di with the previ- ous and current semantic embeddings independently. Then, we stack the two attention outputs and perform a second level of attention between this result and the decoder state. 3. Experimental Setup 3.1. Dataset Table 1: Statistics of the How-2 2,000h Dataset used for model training and evaluation. The maximum input length N (in frames), and maximum output length L (in tokens) are shown. Set Max N Max L #Videos Train Test 145,082 39,537 173 152 68,336 2,127 The How-2 Dataset [23] contains 2,000h of instructional videos with corresponding text transcripts, video, speech, trans- lations, and summaries. Abstractive summaries are generated based on user-provided descriptions of the videos. Table 1 high- lights the number of videos in the train and test partitions of the How2 data. The model features and reference summaries have been made public 1 by the authors of [7]. 3.2. Model Hyperparameters and Evaluation Models: Our models use ESPNet22 [24] and are first pre- trained on the ASR task and then fine-tuned for summariza- tion. The encoder consists of convolutional subsampling by factor 4, followed by 12 conformer [10] blocks with 8 attention heads and a hidden size of 2048. The decoder has 6 transformer blocks, with 4 attention heads and a hidden size of 2048. The total number of model parameters is 103M. Both the encoder and decoder use a dropout rate of 0.2. We use 43-dimensional filter bank and pitch features as input to the encoder. ASR: ASR models are trained with Connectionist Temporal Classification (CTC) and Cross-Entropy loss with CTC weight of 0.3. We use the Adam optimizer with peak lr=0.001, and a warmup scheduler for ASR pre-training. This takes 2 days on 8 V-100 32G GPUs SUMM: Our summarization models are trained with cross- entropy loss and label smoothing of 0.15. During inference, we use a beam size of 8. Model averaging was not performed as it was found to hurt summarization performance. Fine-tuning is run for a day on one A40 GPU. BASS: For BASS models, we use a block size of 1,000 input frames, corresponding to 10s of audio. We only use the seman- tic embedding from the previous block as context for the current block unless otherwise specified. Evaluation: We evaluate our models with ROUGE [25], ME- TEOR [26], and BERTScore [27], which are the most common automatic metrics for evaluating summarization models. 4. Experimental Results 4.1. Truncated Input Baselines First, we train end-to-end summarization baseline models on truncated inputs (Trunc) that are 10 seconds long and 30 sec- onds long. Table 2 reports the results of training on truncated inputs and evaluating recordings that are 10 seconds and 30 seconds long, compared to different state-of-the-art approaches referenced in prior work. We note that using the standard full multi-head attention provides significant gains over restricted self-attention, and therefore use the standard multi-head self- attention for our experiments. 1https://github.com/srvk/how2-dataset 2Code will be released in https://github.com/espnet/espnet Table 2: Performance of Block-wise Adaptation and Training Approaches compared to Truncated Baselines with different inference strategies using ROUGE, METEOR, and BERTScore metrics- higher scores indicate better performance Training Method Inference Pre-training Train Maxlen Inf. Maxlen ROUGE-1\u2191 ROUGE-2\u2191 ROUGE-L\u2191 METEOR\u2191 BERTScore\u2191 Trunc, Restricted Self-Attention [7] Trunc, Full Self-Attention [8] + TTS Augmentation [8] Trunc-Baseline Trunc-Baseline Trunc-Baseline BASS-Adapt BASS-Train Standard Standard Standard Standard Standard Standard Block Block X X X X X X 10s X 100s 100s 100s 10s 30s 60s 30s 30s 100s 100s 100s 10s 30s 60s 30s 30s 60.73 65.30 68.40 60.87 63.30 64.57 63.99 60.87 44.9 51.40 54.10 45.12 47.58 49.11 49.00 43.12 56.10 62.10 65.00 56.79 59.16 60.49 60.17 54.79 29.30 32.50 34.90 30.00 31.76 32.47 32.17 29.12 Table 3: Part-of-speech coverage between the Predicted Sum- mary and the Reference for Truncated 10s baseline and BASS- ADAPT 30s model Model Noun Verb Adj Adv Prop.Noun recordings comprising 10-second chunks can do as well as a model trained on 60-second recordings. The proposed approach is more computationally efficient than the baseline by a factor of 3 since the proposed approach uses 3x smaller inputs 3 times for quadratic self-attentions. Baseline 10-sec BASS-ADAPT 0.85 0.87 0.76 0.79 0.84 0.84 0.65 0.67 Table 4: Performance of BASS models with block level inference across different implementations of the semantic updater model. Models are pre-trained on 10s and fine-tuned on 30s. R-1, R-2, R-3 represent the ROUGE-1, ROUGE-2, and ROUGE-L"}, {"question": " What is the purpose of the updater in the model?", "answer": " The updater uses the past semantic embeddings and the current encoder output to produce the current semantic embedding.", "ref_chunk": "arg max Y \u2248 arg max Y (cid:88) P(Y, S|X) S max S P (Y, S|X) (6) Equation 6 describes the process of identifying the most likely hypothesis summary \u02c6Y given the input X and seman- tic representation S. From Figure 1, based on the condi- tional independence between the summary Y and acoustics X given semantic representation S, we can write P(Y, S|X) = P(Y |X)P(S|X). Thus, we can obtain the solution for Equa- tion 6 using the coordinate descent update shown in Equation 7. \u02c6S = arg max S \u02c6Y \u2248 arg max Y P(S|X) P(Y | \u02c6S) max S P(S|X) (7) From Equations 7 and 5, BASS estimates P(Y i|X 1:i, Y 1:i\u22121) using Equation 8. P(Y i|X 1:i) = P(Y i|Si)P(Si|S1:i\u22121, X i) (8) The encoder and decoder model the probabilities P(S|X) and P(Y |S) respectively. The updater uses the past semantic embeddings and the current encoder output to produce the cur- rent semantic embedding. Figure 2 shows three alternate struc- tures for our updater to aggregate semantic context from the prior and the current block: 1. Concatenation: Si = Concat(Si\u22121, Encoder(X i)) The current semantic embedding is obtained by concatenat- ing the embeddings from the previous and current blocks. 2. Gated Attention: Si = Encoder(X i) + w \u00b7 Attn(Si\u22121, Encoder(X i)) The current and previous semantic embeddings are combined using an attention mechanism and incorporated into the final embedding as a weighted sum. 3. Hierarchical Attention: Si = Attn([Attn(Si\u22121, Di), Attn(Encoder(X i)), Di]), Di) This method performs the context passing within each de- coder block, based on hierarchical attention [22]. We com- pute attention for the current decoder state Di with the previ- ous and current semantic embeddings independently. Then, we stack the two attention outputs and perform a second level of attention between this result and the decoder state. 3. Experimental Setup 3.1. Dataset Table 1: Statistics of the How-2 2,000h Dataset used for model training and evaluation. The maximum input length N (in frames), and maximum output length L (in tokens) are shown. Set Max N Max L #Videos Train Test 145,082 39,537 173 152 68,336 2,127 The How-2 Dataset [23] contains 2,000h of instructional videos with corresponding text transcripts, video, speech, trans- lations, and summaries. Abstractive summaries are generated based on user-provided descriptions of the videos. Table 1 high- lights the number of videos in the train and test partitions of the How2 data. The model features and reference summaries have been made public 1 by the authors of [7]. 3.2. Model Hyperparameters and Evaluation Models: Our models use ESPNet22 [24] and are first pre- trained on the ASR task and then fine-tuned for summariza- tion. The encoder consists of convolutional subsampling by factor 4, followed by 12 conformer [10] blocks with 8 attention heads and a hidden size of 2048. The decoder has 6 transformer blocks, with 4 attention heads and a hidden size of 2048. The total number of model parameters is 103M. Both the encoder and decoder use a dropout rate of 0.2. We use 43-dimensional filter bank and pitch features as input to the encoder. ASR: ASR models are trained with Connectionist Temporal Classification (CTC) and Cross-Entropy loss with CTC weight of 0.3. We use the Adam optimizer with peak lr=0.001, and a warmup scheduler for ASR pre-training. This takes 2 days on 8 V-100 32G GPUs SUMM: Our summarization models are trained with cross- entropy loss and label smoothing of 0.15. During inference, we use a beam size of 8. Model averaging was not performed as it was found to hurt summarization performance. Fine-tuning is run for a day on one A40 GPU. BASS: For BASS models, we use a block size of 1,000 input frames, corresponding to 10s of audio. We only use the seman- tic embedding from the previous block as context for the current block unless otherwise specified. Evaluation: We evaluate our models with ROUGE [25], ME- TEOR [26], and BERTScore [27], which are the most common automatic metrics for evaluating summarization models. 4. Experimental Results 4.1. Truncated Input Baselines First, we train end-to-end summarization baseline models on truncated inputs (Trunc) that are 10 seconds long and 30 sec- onds long. Table 2 reports the results of training on truncated inputs and evaluating recordings that are 10 seconds and 30 seconds long, compared to different state-of-the-art approaches referenced in prior work. We note that using the standard full multi-head attention provides significant gains over restricted self-attention, and therefore use the standard multi-head self- attention for our experiments. 1https://github.com/srvk/how2-dataset 2Code will be released in https://github.com/espnet/espnet Table 2: Performance of Block-wise Adaptation and Training Approaches compared to Truncated Baselines with different inference strategies using ROUGE, METEOR, and BERTScore metrics- higher scores indicate better performance Training Method Inference Pre-training Train Maxlen Inf. Maxlen ROUGE-1\u2191 ROUGE-2\u2191 ROUGE-L\u2191 METEOR\u2191 BERTScore\u2191 Trunc, Restricted Self-Attention [7] Trunc, Full Self-Attention [8] + TTS Augmentation [8] Trunc-Baseline Trunc-Baseline Trunc-Baseline BASS-Adapt BASS-Train Standard Standard Standard Standard Standard Standard Block Block X X X X X X 10s X 100s 100s 100s 10s 30s 60s 30s 30s 100s 100s 100s 10s 30s 60s 30s 30s 60.73 65.30 68.40 60.87 63.30 64.57 63.99 60.87 44.9 51.40 54.10 45.12 47.58 49.11 49.00 43.12 56.10 62.10 65.00 56.79 59.16 60.49 60.17 54.79 29.30 32.50 34.90 30.00 31.76 32.47 32.17 29.12 Table 3: Part-of-speech coverage between the Predicted Sum- mary and the Reference for Truncated 10s baseline and BASS- ADAPT 30s model Model Noun Verb Adj Adv Prop.Noun recordings comprising 10-second chunks can do as well as a model trained on 60-second recordings. The proposed approach is more computationally efficient than the baseline by a factor of 3 since the proposed approach uses 3x smaller inputs 3 times for quadratic self-attentions. Baseline 10-sec BASS-ADAPT 0.85 0.87 0.76 0.79 0.84 0.84 0.65 0.67 Table 4: Performance of BASS models with block level inference across different implementations of the semantic updater model. Models are pre-trained on 10s and fine-tuned on 30s. R-1, R-2, R-3 represent the ROUGE-1, ROUGE-2, and ROUGE-L"}, {"question": " What is the maximum output length in tokens in the How-2 Dataset?", "answer": " The maximum output length in tokens in the How-2 Dataset is 152.", "ref_chunk": "arg max Y \u2248 arg max Y (cid:88) P(Y, S|X) S max S P (Y, S|X) (6) Equation 6 describes the process of identifying the most likely hypothesis summary \u02c6Y given the input X and seman- tic representation S. From Figure 1, based on the condi- tional independence between the summary Y and acoustics X given semantic representation S, we can write P(Y, S|X) = P(Y |X)P(S|X). Thus, we can obtain the solution for Equa- tion 6 using the coordinate descent update shown in Equation 7. \u02c6S = arg max S \u02c6Y \u2248 arg max Y P(S|X) P(Y | \u02c6S) max S P(S|X) (7) From Equations 7 and 5, BASS estimates P(Y i|X 1:i, Y 1:i\u22121) using Equation 8. P(Y i|X 1:i) = P(Y i|Si)P(Si|S1:i\u22121, X i) (8) The encoder and decoder model the probabilities P(S|X) and P(Y |S) respectively. The updater uses the past semantic embeddings and the current encoder output to produce the cur- rent semantic embedding. Figure 2 shows three alternate struc- tures for our updater to aggregate semantic context from the prior and the current block: 1. Concatenation: Si = Concat(Si\u22121, Encoder(X i)) The current semantic embedding is obtained by concatenat- ing the embeddings from the previous and current blocks. 2. Gated Attention: Si = Encoder(X i) + w \u00b7 Attn(Si\u22121, Encoder(X i)) The current and previous semantic embeddings are combined using an attention mechanism and incorporated into the final embedding as a weighted sum. 3. Hierarchical Attention: Si = Attn([Attn(Si\u22121, Di), Attn(Encoder(X i)), Di]), Di) This method performs the context passing within each de- coder block, based on hierarchical attention [22]. We com- pute attention for the current decoder state Di with the previ- ous and current semantic embeddings independently. Then, we stack the two attention outputs and perform a second level of attention between this result and the decoder state. 3. Experimental Setup 3.1. Dataset Table 1: Statistics of the How-2 2,000h Dataset used for model training and evaluation. The maximum input length N (in frames), and maximum output length L (in tokens) are shown. Set Max N Max L #Videos Train Test 145,082 39,537 173 152 68,336 2,127 The How-2 Dataset [23] contains 2,000h of instructional videos with corresponding text transcripts, video, speech, trans- lations, and summaries. Abstractive summaries are generated based on user-provided descriptions of the videos. Table 1 high- lights the number of videos in the train and test partitions of the How2 data. The model features and reference summaries have been made public 1 by the authors of [7]. 3.2. Model Hyperparameters and Evaluation Models: Our models use ESPNet22 [24] and are first pre- trained on the ASR task and then fine-tuned for summariza- tion. The encoder consists of convolutional subsampling by factor 4, followed by 12 conformer [10] blocks with 8 attention heads and a hidden size of 2048. The decoder has 6 transformer blocks, with 4 attention heads and a hidden size of 2048. The total number of model parameters is 103M. Both the encoder and decoder use a dropout rate of 0.2. We use 43-dimensional filter bank and pitch features as input to the encoder. ASR: ASR models are trained with Connectionist Temporal Classification (CTC) and Cross-Entropy loss with CTC weight of 0.3. We use the Adam optimizer with peak lr=0.001, and a warmup scheduler for ASR pre-training. This takes 2 days on 8 V-100 32G GPUs SUMM: Our summarization models are trained with cross- entropy loss and label smoothing of 0.15. During inference, we use a beam size of 8. Model averaging was not performed as it was found to hurt summarization performance. Fine-tuning is run for a day on one A40 GPU. BASS: For BASS models, we use a block size of 1,000 input frames, corresponding to 10s of audio. We only use the seman- tic embedding from the previous block as context for the current block unless otherwise specified. Evaluation: We evaluate our models with ROUGE [25], ME- TEOR [26], and BERTScore [27], which are the most common automatic metrics for evaluating summarization models. 4. Experimental Results 4.1. Truncated Input Baselines First, we train end-to-end summarization baseline models on truncated inputs (Trunc) that are 10 seconds long and 30 sec- onds long. Table 2 reports the results of training on truncated inputs and evaluating recordings that are 10 seconds and 30 seconds long, compared to different state-of-the-art approaches referenced in prior work. We note that using the standard full multi-head attention provides significant gains over restricted self-attention, and therefore use the standard multi-head self- attention for our experiments. 1https://github.com/srvk/how2-dataset 2Code will be released in https://github.com/espnet/espnet Table 2: Performance of Block-wise Adaptation and Training Approaches compared to Truncated Baselines with different inference strategies using ROUGE, METEOR, and BERTScore metrics- higher scores indicate better performance Training Method Inference Pre-training Train Maxlen Inf. Maxlen ROUGE-1\u2191 ROUGE-2\u2191 ROUGE-L\u2191 METEOR\u2191 BERTScore\u2191 Trunc, Restricted Self-Attention [7] Trunc, Full Self-Attention [8] + TTS Augmentation [8] Trunc-Baseline Trunc-Baseline Trunc-Baseline BASS-Adapt BASS-Train Standard Standard Standard Standard Standard Standard Block Block X X X X X X 10s X 100s 100s 100s 10s 30s 60s 30s 30s 100s 100s 100s 10s 30s 60s 30s 30s 60.73 65.30 68.40 60.87 63.30 64.57 63.99 60.87 44.9 51.40 54.10 45.12 47.58 49.11 49.00 43.12 56.10 62.10 65.00 56.79 59.16 60.49 60.17 54.79 29.30 32.50 34.90 30.00 31.76 32.47 32.17 29.12 Table 3: Part-of-speech coverage between the Predicted Sum- mary and the Reference for Truncated 10s baseline and BASS- ADAPT 30s model Model Noun Verb Adj Adv Prop.Noun recordings comprising 10-second chunks can do as well as a model trained on 60-second recordings. The proposed approach is more computationally efficient than the baseline by a factor of 3 since the proposed approach uses 3x smaller inputs 3 times for quadratic self-attentions. Baseline 10-sec BASS-ADAPT 0.85 0.87 0.76 0.79 0.84 0.84 0.65 0.67 Table 4: Performance of BASS models with block level inference across different implementations of the semantic updater model. Models are pre-trained on 10s and fine-tuned on 30s. R-1, R-2, R-3 represent the ROUGE-1, ROUGE-2, and ROUGE-L"}, {"question": " What is the total number of model parameters in the encoder and decoder of the models used?", "answer": " The total number of model parameters is 103 million.", "ref_chunk": "arg max Y \u2248 arg max Y (cid:88) P(Y, S|X) S max S P (Y, S|X) (6) Equation 6 describes the process of identifying the most likely hypothesis summary \u02c6Y given the input X and seman- tic representation S. From Figure 1, based on the condi- tional independence between the summary Y and acoustics X given semantic representation S, we can write P(Y, S|X) = P(Y |X)P(S|X). Thus, we can obtain the solution for Equa- tion 6 using the coordinate descent update shown in Equation 7. \u02c6S = arg max S \u02c6Y \u2248 arg max Y P(S|X) P(Y | \u02c6S) max S P(S|X) (7) From Equations 7 and 5, BASS estimates P(Y i|X 1:i, Y 1:i\u22121) using Equation 8. P(Y i|X 1:i) = P(Y i|Si)P(Si|S1:i\u22121, X i) (8) The encoder and decoder model the probabilities P(S|X) and P(Y |S) respectively. The updater uses the past semantic embeddings and the current encoder output to produce the cur- rent semantic embedding. Figure 2 shows three alternate struc- tures for our updater to aggregate semantic context from the prior and the current block: 1. Concatenation: Si = Concat(Si\u22121, Encoder(X i)) The current semantic embedding is obtained by concatenat- ing the embeddings from the previous and current blocks. 2. Gated Attention: Si = Encoder(X i) + w \u00b7 Attn(Si\u22121, Encoder(X i)) The current and previous semantic embeddings are combined using an attention mechanism and incorporated into the final embedding as a weighted sum. 3. Hierarchical Attention: Si = Attn([Attn(Si\u22121, Di), Attn(Encoder(X i)), Di]), Di) This method performs the context passing within each de- coder block, based on hierarchical attention [22]. We com- pute attention for the current decoder state Di with the previ- ous and current semantic embeddings independently. Then, we stack the two attention outputs and perform a second level of attention between this result and the decoder state. 3. Experimental Setup 3.1. Dataset Table 1: Statistics of the How-2 2,000h Dataset used for model training and evaluation. The maximum input length N (in frames), and maximum output length L (in tokens) are shown. Set Max N Max L #Videos Train Test 145,082 39,537 173 152 68,336 2,127 The How-2 Dataset [23] contains 2,000h of instructional videos with corresponding text transcripts, video, speech, trans- lations, and summaries. Abstractive summaries are generated based on user-provided descriptions of the videos. Table 1 high- lights the number of videos in the train and test partitions of the How2 data. The model features and reference summaries have been made public 1 by the authors of [7]. 3.2. Model Hyperparameters and Evaluation Models: Our models use ESPNet22 [24] and are first pre- trained on the ASR task and then fine-tuned for summariza- tion. The encoder consists of convolutional subsampling by factor 4, followed by 12 conformer [10] blocks with 8 attention heads and a hidden size of 2048. The decoder has 6 transformer blocks, with 4 attention heads and a hidden size of 2048. The total number of model parameters is 103M. Both the encoder and decoder use a dropout rate of 0.2. We use 43-dimensional filter bank and pitch features as input to the encoder. ASR: ASR models are trained with Connectionist Temporal Classification (CTC) and Cross-Entropy loss with CTC weight of 0.3. We use the Adam optimizer with peak lr=0.001, and a warmup scheduler for ASR pre-training. This takes 2 days on 8 V-100 32G GPUs SUMM: Our summarization models are trained with cross- entropy loss and label smoothing of 0.15. During inference, we use a beam size of 8. Model averaging was not performed as it was found to hurt summarization performance. Fine-tuning is run for a day on one A40 GPU. BASS: For BASS models, we use a block size of 1,000 input frames, corresponding to 10s of audio. We only use the seman- tic embedding from the previous block as context for the current block unless otherwise specified. Evaluation: We evaluate our models with ROUGE [25], ME- TEOR [26], and BERTScore [27], which are the most common automatic metrics for evaluating summarization models. 4. Experimental Results 4.1. Truncated Input Baselines First, we train end-to-end summarization baseline models on truncated inputs (Trunc) that are 10 seconds long and 30 sec- onds long. Table 2 reports the results of training on truncated inputs and evaluating recordings that are 10 seconds and 30 seconds long, compared to different state-of-the-art approaches referenced in prior work. We note that using the standard full multi-head attention provides significant gains over restricted self-attention, and therefore use the standard multi-head self- attention for our experiments. 1https://github.com/srvk/how2-dataset 2Code will be released in https://github.com/espnet/espnet Table 2: Performance of Block-wise Adaptation and Training Approaches compared to Truncated Baselines with different inference strategies using ROUGE, METEOR, and BERTScore metrics- higher scores indicate better performance Training Method Inference Pre-training Train Maxlen Inf. Maxlen ROUGE-1\u2191 ROUGE-2\u2191 ROUGE-L\u2191 METEOR\u2191 BERTScore\u2191 Trunc, Restricted Self-Attention [7] Trunc, Full Self-Attention [8] + TTS Augmentation [8] Trunc-Baseline Trunc-Baseline Trunc-Baseline BASS-Adapt BASS-Train Standard Standard Standard Standard Standard Standard Block Block X X X X X X 10s X 100s 100s 100s 10s 30s 60s 30s 30s 100s 100s 100s 10s 30s 60s 30s 30s 60.73 65.30 68.40 60.87 63.30 64.57 63.99 60.87 44.9 51.40 54.10 45.12 47.58 49.11 49.00 43.12 56.10 62.10 65.00 56.79 59.16 60.49 60.17 54.79 29.30 32.50 34.90 30.00 31.76 32.47 32.17 29.12 Table 3: Part-of-speech coverage between the Predicted Sum- mary and the Reference for Truncated 10s baseline and BASS- ADAPT 30s model Model Noun Verb Adj Adv Prop.Noun recordings comprising 10-second chunks can do as well as a model trained on 60-second recordings. The proposed approach is more computationally efficient than the baseline by a factor of 3 since the proposed approach uses 3x smaller inputs 3 times for quadratic self-attentions. Baseline 10-sec BASS-ADAPT 0.85 0.87 0.76 0.79 0.84 0.84 0.65 0.67 Table 4: Performance of BASS models with block level inference across different implementations of the semantic updater model. Models are pre-trained on 10s and fine-tuned on 30s. R-1, R-2, R-3 represent the ROUGE-1, ROUGE-2, and ROUGE-L"}, {"question": " What optimizer is used for ASR pre-training and with what peak learning rate?", "answer": " The Adam optimizer is used with a peak learning rate of 0.001 for ASR pre-training.", "ref_chunk": "arg max Y \u2248 arg max Y (cid:88) P(Y, S|X) S max S P (Y, S|X) (6) Equation 6 describes the process of identifying the most likely hypothesis summary \u02c6Y given the input X and seman- tic representation S. From Figure 1, based on the condi- tional independence between the summary Y and acoustics X given semantic representation S, we can write P(Y, S|X) = P(Y |X)P(S|X). Thus, we can obtain the solution for Equa- tion 6 using the coordinate descent update shown in Equation 7. \u02c6S = arg max S \u02c6Y \u2248 arg max Y P(S|X) P(Y | \u02c6S) max S P(S|X) (7) From Equations 7 and 5, BASS estimates P(Y i|X 1:i, Y 1:i\u22121) using Equation 8. P(Y i|X 1:i) = P(Y i|Si)P(Si|S1:i\u22121, X i) (8) The encoder and decoder model the probabilities P(S|X) and P(Y |S) respectively. The updater uses the past semantic embeddings and the current encoder output to produce the cur- rent semantic embedding. Figure 2 shows three alternate struc- tures for our updater to aggregate semantic context from the prior and the current block: 1. Concatenation: Si = Concat(Si\u22121, Encoder(X i)) The current semantic embedding is obtained by concatenat- ing the embeddings from the previous and current blocks. 2. Gated Attention: Si = Encoder(X i) + w \u00b7 Attn(Si\u22121, Encoder(X i)) The current and previous semantic embeddings are combined using an attention mechanism and incorporated into the final embedding as a weighted sum. 3. Hierarchical Attention: Si = Attn([Attn(Si\u22121, Di), Attn(Encoder(X i)), Di]), Di) This method performs the context passing within each de- coder block, based on hierarchical attention [22]. We com- pute attention for the current decoder state Di with the previ- ous and current semantic embeddings independently. Then, we stack the two attention outputs and perform a second level of attention between this result and the decoder state. 3. Experimental Setup 3.1. Dataset Table 1: Statistics of the How-2 2,000h Dataset used for model training and evaluation. The maximum input length N (in frames), and maximum output length L (in tokens) are shown. Set Max N Max L #Videos Train Test 145,082 39,537 173 152 68,336 2,127 The How-2 Dataset [23] contains 2,000h of instructional videos with corresponding text transcripts, video, speech, trans- lations, and summaries. Abstractive summaries are generated based on user-provided descriptions of the videos. Table 1 high- lights the number of videos in the train and test partitions of the How2 data. The model features and reference summaries have been made public 1 by the authors of [7]. 3.2. Model Hyperparameters and Evaluation Models: Our models use ESPNet22 [24] and are first pre- trained on the ASR task and then fine-tuned for summariza- tion. The encoder consists of convolutional subsampling by factor 4, followed by 12 conformer [10] blocks with 8 attention heads and a hidden size of 2048. The decoder has 6 transformer blocks, with 4 attention heads and a hidden size of 2048. The total number of model parameters is 103M. Both the encoder and decoder use a dropout rate of 0.2. We use 43-dimensional filter bank and pitch features as input to the encoder. ASR: ASR models are trained with Connectionist Temporal Classification (CTC) and Cross-Entropy loss with CTC weight of 0.3. We use the Adam optimizer with peak lr=0.001, and a warmup scheduler for ASR pre-training. This takes 2 days on 8 V-100 32G GPUs SUMM: Our summarization models are trained with cross- entropy loss and label smoothing of 0.15. During inference, we use a beam size of 8. Model averaging was not performed as it was found to hurt summarization performance. Fine-tuning is run for a day on one A40 GPU. BASS: For BASS models, we use a block size of 1,000 input frames, corresponding to 10s of audio. We only use the seman- tic embedding from the previous block as context for the current block unless otherwise specified. Evaluation: We evaluate our models with ROUGE [25], ME- TEOR [26], and BERTScore [27], which are the most common automatic metrics for evaluating summarization models. 4. Experimental Results 4.1. Truncated Input Baselines First, we train end-to-end summarization baseline models on truncated inputs (Trunc) that are 10 seconds long and 30 sec- onds long. Table 2 reports the results of training on truncated inputs and evaluating recordings that are 10 seconds and 30 seconds long, compared to different state-of-the-art approaches referenced in prior work. We note that using the standard full multi-head attention provides significant gains over restricted self-attention, and therefore use the standard multi-head self- attention for our experiments. 1https://github.com/srvk/how2-dataset 2Code will be released in https://github.com/espnet/espnet Table 2: Performance of Block-wise Adaptation and Training Approaches compared to Truncated Baselines with different inference strategies using ROUGE, METEOR, and BERTScore metrics- higher scores indicate better performance Training Method Inference Pre-training Train Maxlen Inf. Maxlen ROUGE-1\u2191 ROUGE-2\u2191 ROUGE-L\u2191 METEOR\u2191 BERTScore\u2191 Trunc, Restricted Self-Attention [7] Trunc, Full Self-Attention [8] + TTS Augmentation [8] Trunc-Baseline Trunc-Baseline Trunc-Baseline BASS-Adapt BASS-Train Standard Standard Standard Standard Standard Standard Block Block X X X X X X 10s X 100s 100s 100s 10s 30s 60s 30s 30s 100s 100s 100s 10s 30s 60s 30s 30s 60.73 65.30 68.40 60.87 63.30 64.57 63.99 60.87 44.9 51.40 54.10 45.12 47.58 49.11 49.00 43.12 56.10 62.10 65.00 56.79 59.16 60.49 60.17 54.79 29.30 32.50 34.90 30.00 31.76 32.47 32.17 29.12 Table 3: Part-of-speech coverage between the Predicted Sum- mary and the Reference for Truncated 10s baseline and BASS- ADAPT 30s model Model Noun Verb Adj Adv Prop.Noun recordings comprising 10-second chunks can do as well as a model trained on 60-second recordings. The proposed approach is more computationally efficient than the baseline by a factor of 3 since the proposed approach uses 3x smaller inputs 3 times for quadratic self-attentions. Baseline 10-sec BASS-ADAPT 0.85 0.87 0.76 0.79 0.84 0.84 0.65 0.67 Table 4: Performance of BASS models with block level inference across different implementations of the semantic updater model. Models are pre-trained on 10s and fine-tuned on 30s. R-1, R-2, R-3 represent the ROUGE-1, ROUGE-2, and ROUGE-L"}, {"question": " How long does fine-tuning take for the summarization models on one A40 GPU?", "answer": " Fine-tuning is run for a day on one A40 GPU.", "ref_chunk": "arg max Y \u2248 arg max Y (cid:88) P(Y, S|X) S max S P (Y, S|X) (6) Equation 6 describes the process of identifying the most likely hypothesis summary \u02c6Y given the input X and seman- tic representation S. From Figure 1, based on the condi- tional independence between the summary Y and acoustics X given semantic representation S, we can write P(Y, S|X) = P(Y |X)P(S|X). Thus, we can obtain the solution for Equa- tion 6 using the coordinate descent update shown in Equation 7. \u02c6S = arg max S \u02c6Y \u2248 arg max Y P(S|X) P(Y | \u02c6S) max S P(S|X) (7) From Equations 7 and 5, BASS estimates P(Y i|X 1:i, Y 1:i\u22121) using Equation 8. P(Y i|X 1:i) = P(Y i|Si)P(Si|S1:i\u22121, X i) (8) The encoder and decoder model the probabilities P(S|X) and P(Y |S) respectively. The updater uses the past semantic embeddings and the current encoder output to produce the cur- rent semantic embedding. Figure 2 shows three alternate struc- tures for our updater to aggregate semantic context from the prior and the current block: 1. Concatenation: Si = Concat(Si\u22121, Encoder(X i)) The current semantic embedding is obtained by concatenat- ing the embeddings from the previous and current blocks. 2. Gated Attention: Si = Encoder(X i) + w \u00b7 Attn(Si\u22121, Encoder(X i)) The current and previous semantic embeddings are combined using an attention mechanism and incorporated into the final embedding as a weighted sum. 3. Hierarchical Attention: Si = Attn([Attn(Si\u22121, Di), Attn(Encoder(X i)), Di]), Di) This method performs the context passing within each de- coder block, based on hierarchical attention [22]. We com- pute attention for the current decoder state Di with the previ- ous and current semantic embeddings independently. Then, we stack the two attention outputs and perform a second level of attention between this result and the decoder state. 3. Experimental Setup 3.1. Dataset Table 1: Statistics of the How-2 2,000h Dataset used for model training and evaluation. The maximum input length N (in frames), and maximum output length L (in tokens) are shown. Set Max N Max L #Videos Train Test 145,082 39,537 173 152 68,336 2,127 The How-2 Dataset [23] contains 2,000h of instructional videos with corresponding text transcripts, video, speech, trans- lations, and summaries. Abstractive summaries are generated based on user-provided descriptions of the videos. Table 1 high- lights the number of videos in the train and test partitions of the How2 data. The model features and reference summaries have been made public 1 by the authors of [7]. 3.2. Model Hyperparameters and Evaluation Models: Our models use ESPNet22 [24] and are first pre- trained on the ASR task and then fine-tuned for summariza- tion. The encoder consists of convolutional subsampling by factor 4, followed by 12 conformer [10] blocks with 8 attention heads and a hidden size of 2048. The decoder has 6 transformer blocks, with 4 attention heads and a hidden size of 2048. The total number of model parameters is 103M. Both the encoder and decoder use a dropout rate of 0.2. We use 43-dimensional filter bank and pitch features as input to the encoder. ASR: ASR models are trained with Connectionist Temporal Classification (CTC) and Cross-Entropy loss with CTC weight of 0.3. We use the Adam optimizer with peak lr=0.001, and a warmup scheduler for ASR pre-training. This takes 2 days on 8 V-100 32G GPUs SUMM: Our summarization models are trained with cross- entropy loss and label smoothing of 0.15. During inference, we use a beam size of 8. Model averaging was not performed as it was found to hurt summarization performance. Fine-tuning is run for a day on one A40 GPU. BASS: For BASS models, we use a block size of 1,000 input frames, corresponding to 10s of audio. We only use the seman- tic embedding from the previous block as context for the current block unless otherwise specified. Evaluation: We evaluate our models with ROUGE [25], ME- TEOR [26], and BERTScore [27], which are the most common automatic metrics for evaluating summarization models. 4. Experimental Results 4.1. Truncated Input Baselines First, we train end-to-end summarization baseline models on truncated inputs (Trunc) that are 10 seconds long and 30 sec- onds long. Table 2 reports the results of training on truncated inputs and evaluating recordings that are 10 seconds and 30 seconds long, compared to different state-of-the-art approaches referenced in prior work. We note that using the standard full multi-head attention provides significant gains over restricted self-attention, and therefore use the standard multi-head self- attention for our experiments. 1https://github.com/srvk/how2-dataset 2Code will be released in https://github.com/espnet/espnet Table 2: Performance of Block-wise Adaptation and Training Approaches compared to Truncated Baselines with different inference strategies using ROUGE, METEOR, and BERTScore metrics- higher scores indicate better performance Training Method Inference Pre-training Train Maxlen Inf. Maxlen ROUGE-1\u2191 ROUGE-2\u2191 ROUGE-L\u2191 METEOR\u2191 BERTScore\u2191 Trunc, Restricted Self-Attention [7] Trunc, Full Self-Attention [8] + TTS Augmentation [8] Trunc-Baseline Trunc-Baseline Trunc-Baseline BASS-Adapt BASS-Train Standard Standard Standard Standard Standard Standard Block Block X X X X X X 10s X 100s 100s 100s 10s 30s 60s 30s 30s 100s 100s 100s 10s 30s 60s 30s 30s 60.73 65.30 68.40 60.87 63.30 64.57 63.99 60.87 44.9 51.40 54.10 45.12 47.58 49.11 49.00 43.12 56.10 62.10 65.00 56.79 59.16 60.49 60.17 54.79 29.30 32.50 34.90 30.00 31.76 32.47 32.17 29.12 Table 3: Part-of-speech coverage between the Predicted Sum- mary and the Reference for Truncated 10s baseline and BASS- ADAPT 30s model Model Noun Verb Adj Adv Prop.Noun recordings comprising 10-second chunks can do as well as a model trained on 60-second recordings. The proposed approach is more computationally efficient than the baseline by a factor of 3 since the proposed approach uses 3x smaller inputs 3 times for quadratic self-attentions. Baseline 10-sec BASS-ADAPT 0.85 0.87 0.76 0.79 0.84 0.84 0.65 0.67 Table 4: Performance of BASS models with block level inference across different implementations of the semantic updater model. Models are pre-trained on 10s and fine-tuned on 30s. R-1, R-2, R-3 represent the ROUGE-1, ROUGE-2, and ROUGE-L"}, {"question": " What metrics are used to evaluate the models in the experiment?", "answer": " The models are evaluated with ROUGE, METEOR, and BERTScore.", "ref_chunk": "arg max Y \u2248 arg max Y (cid:88) P(Y, S|X) S max S P (Y, S|X) (6) Equation 6 describes the process of identifying the most likely hypothesis summary \u02c6Y given the input X and seman- tic representation S. From Figure 1, based on the condi- tional independence between the summary Y and acoustics X given semantic representation S, we can write P(Y, S|X) = P(Y |X)P(S|X). Thus, we can obtain the solution for Equa- tion 6 using the coordinate descent update shown in Equation 7. \u02c6S = arg max S \u02c6Y \u2248 arg max Y P(S|X) P(Y | \u02c6S) max S P(S|X) (7) From Equations 7 and 5, BASS estimates P(Y i|X 1:i, Y 1:i\u22121) using Equation 8. P(Y i|X 1:i) = P(Y i|Si)P(Si|S1:i\u22121, X i) (8) The encoder and decoder model the probabilities P(S|X) and P(Y |S) respectively. The updater uses the past semantic embeddings and the current encoder output to produce the cur- rent semantic embedding. Figure 2 shows three alternate struc- tures for our updater to aggregate semantic context from the prior and the current block: 1. Concatenation: Si = Concat(Si\u22121, Encoder(X i)) The current semantic embedding is obtained by concatenat- ing the embeddings from the previous and current blocks. 2. Gated Attention: Si = Encoder(X i) + w \u00b7 Attn(Si\u22121, Encoder(X i)) The current and previous semantic embeddings are combined using an attention mechanism and incorporated into the final embedding as a weighted sum. 3. Hierarchical Attention: Si = Attn([Attn(Si\u22121, Di), Attn(Encoder(X i)), Di]), Di) This method performs the context passing within each de- coder block, based on hierarchical attention [22]. We com- pute attention for the current decoder state Di with the previ- ous and current semantic embeddings independently. Then, we stack the two attention outputs and perform a second level of attention between this result and the decoder state. 3. Experimental Setup 3.1. Dataset Table 1: Statistics of the How-2 2,000h Dataset used for model training and evaluation. The maximum input length N (in frames), and maximum output length L (in tokens) are shown. Set Max N Max L #Videos Train Test 145,082 39,537 173 152 68,336 2,127 The How-2 Dataset [23] contains 2,000h of instructional videos with corresponding text transcripts, video, speech, trans- lations, and summaries. Abstractive summaries are generated based on user-provided descriptions of the videos. Table 1 high- lights the number of videos in the train and test partitions of the How2 data. The model features and reference summaries have been made public 1 by the authors of [7]. 3.2. Model Hyperparameters and Evaluation Models: Our models use ESPNet22 [24] and are first pre- trained on the ASR task and then fine-tuned for summariza- tion. The encoder consists of convolutional subsampling by factor 4, followed by 12 conformer [10] blocks with 8 attention heads and a hidden size of 2048. The decoder has 6 transformer blocks, with 4 attention heads and a hidden size of 2048. The total number of model parameters is 103M. Both the encoder and decoder use a dropout rate of 0.2. We use 43-dimensional filter bank and pitch features as input to the encoder. ASR: ASR models are trained with Connectionist Temporal Classification (CTC) and Cross-Entropy loss with CTC weight of 0.3. We use the Adam optimizer with peak lr=0.001, and a warmup scheduler for ASR pre-training. This takes 2 days on 8 V-100 32G GPUs SUMM: Our summarization models are trained with cross- entropy loss and label smoothing of 0.15. During inference, we use a beam size of 8. Model averaging was not performed as it was found to hurt summarization performance. Fine-tuning is run for a day on one A40 GPU. BASS: For BASS models, we use a block size of 1,000 input frames, corresponding to 10s of audio. We only use the seman- tic embedding from the previous block as context for the current block unless otherwise specified. Evaluation: We evaluate our models with ROUGE [25], ME- TEOR [26], and BERTScore [27], which are the most common automatic metrics for evaluating summarization models. 4. Experimental Results 4.1. Truncated Input Baselines First, we train end-to-end summarization baseline models on truncated inputs (Trunc) that are 10 seconds long and 30 sec- onds long. Table 2 reports the results of training on truncated inputs and evaluating recordings that are 10 seconds and 30 seconds long, compared to different state-of-the-art approaches referenced in prior work. We note that using the standard full multi-head attention provides significant gains over restricted self-attention, and therefore use the standard multi-head self- attention for our experiments. 1https://github.com/srvk/how2-dataset 2Code will be released in https://github.com/espnet/espnet Table 2: Performance of Block-wise Adaptation and Training Approaches compared to Truncated Baselines with different inference strategies using ROUGE, METEOR, and BERTScore metrics- higher scores indicate better performance Training Method Inference Pre-training Train Maxlen Inf. Maxlen ROUGE-1\u2191 ROUGE-2\u2191 ROUGE-L\u2191 METEOR\u2191 BERTScore\u2191 Trunc, Restricted Self-Attention [7] Trunc, Full Self-Attention [8] + TTS Augmentation [8] Trunc-Baseline Trunc-Baseline Trunc-Baseline BASS-Adapt BASS-Train Standard Standard Standard Standard Standard Standard Block Block X X X X X X 10s X 100s 100s 100s 10s 30s 60s 30s 30s 100s 100s 100s 10s 30s 60s 30s 30s 60.73 65.30 68.40 60.87 63.30 64.57 63.99 60.87 44.9 51.40 54.10 45.12 47.58 49.11 49.00 43.12 56.10 62.10 65.00 56.79 59.16 60.49 60.17 54.79 29.30 32.50 34.90 30.00 31.76 32.47 32.17 29.12 Table 3: Part-of-speech coverage between the Predicted Sum- mary and the Reference for Truncated 10s baseline and BASS- ADAPT 30s model Model Noun Verb Adj Adv Prop.Noun recordings comprising 10-second chunks can do as well as a model trained on 60-second recordings. The proposed approach is more computationally efficient than the baseline by a factor of 3 since the proposed approach uses 3x smaller inputs 3 times for quadratic self-attentions. Baseline 10-sec BASS-ADAPT 0.85 0.87 0.76 0.79 0.84 0.84 0.65 0.67 Table 4: Performance of BASS models with block level inference across different implementations of the semantic updater model. Models are pre-trained on 10s and fine-tuned on 30s. R-1, R-2, R-3 represent the ROUGE-1, ROUGE-2, and ROUGE-L"}], "doc_text": "arg max Y \u2248 arg max Y (cid:88) P(Y, S|X) S max S P (Y, S|X) (6) Equation 6 describes the process of identifying the most likely hypothesis summary \u02c6Y given the input X and seman- tic representation S. From Figure 1, based on the condi- tional independence between the summary Y and acoustics X given semantic representation S, we can write P(Y, S|X) = P(Y |X)P(S|X). Thus, we can obtain the solution for Equa- tion 6 using the coordinate descent update shown in Equation 7. \u02c6S = arg max S \u02c6Y \u2248 arg max Y P(S|X) P(Y | \u02c6S) max S P(S|X) (7) From Equations 7 and 5, BASS estimates P(Y i|X 1:i, Y 1:i\u22121) using Equation 8. P(Y i|X 1:i) = P(Y i|Si)P(Si|S1:i\u22121, X i) (8) The encoder and decoder model the probabilities P(S|X) and P(Y |S) respectively. The updater uses the past semantic embeddings and the current encoder output to produce the cur- rent semantic embedding. Figure 2 shows three alternate struc- tures for our updater to aggregate semantic context from the prior and the current block: 1. Concatenation: Si = Concat(Si\u22121, Encoder(X i)) The current semantic embedding is obtained by concatenat- ing the embeddings from the previous and current blocks. 2. Gated Attention: Si = Encoder(X i) + w \u00b7 Attn(Si\u22121, Encoder(X i)) The current and previous semantic embeddings are combined using an attention mechanism and incorporated into the final embedding as a weighted sum. 3. Hierarchical Attention: Si = Attn([Attn(Si\u22121, Di), Attn(Encoder(X i)), Di]), Di) This method performs the context passing within each de- coder block, based on hierarchical attention [22]. We com- pute attention for the current decoder state Di with the previ- ous and current semantic embeddings independently. Then, we stack the two attention outputs and perform a second level of attention between this result and the decoder state. 3. Experimental Setup 3.1. Dataset Table 1: Statistics of the How-2 2,000h Dataset used for model training and evaluation. The maximum input length N (in frames), and maximum output length L (in tokens) are shown. Set Max N Max L #Videos Train Test 145,082 39,537 173 152 68,336 2,127 The How-2 Dataset [23] contains 2,000h of instructional videos with corresponding text transcripts, video, speech, trans- lations, and summaries. Abstractive summaries are generated based on user-provided descriptions of the videos. Table 1 high- lights the number of videos in the train and test partitions of the How2 data. The model features and reference summaries have been made public 1 by the authors of [7]. 3.2. Model Hyperparameters and Evaluation Models: Our models use ESPNet22 [24] and are first pre- trained on the ASR task and then fine-tuned for summariza- tion. The encoder consists of convolutional subsampling by factor 4, followed by 12 conformer [10] blocks with 8 attention heads and a hidden size of 2048. The decoder has 6 transformer blocks, with 4 attention heads and a hidden size of 2048. The total number of model parameters is 103M. Both the encoder and decoder use a dropout rate of 0.2. We use 43-dimensional filter bank and pitch features as input to the encoder. ASR: ASR models are trained with Connectionist Temporal Classification (CTC) and Cross-Entropy loss with CTC weight of 0.3. We use the Adam optimizer with peak lr=0.001, and a warmup scheduler for ASR pre-training. This takes 2 days on 8 V-100 32G GPUs SUMM: Our summarization models are trained with cross- entropy loss and label smoothing of 0.15. During inference, we use a beam size of 8. Model averaging was not performed as it was found to hurt summarization performance. Fine-tuning is run for a day on one A40 GPU. BASS: For BASS models, we use a block size of 1,000 input frames, corresponding to 10s of audio. We only use the seman- tic embedding from the previous block as context for the current block unless otherwise specified. Evaluation: We evaluate our models with ROUGE [25], ME- TEOR [26], and BERTScore [27], which are the most common automatic metrics for evaluating summarization models. 4. Experimental Results 4.1. Truncated Input Baselines First, we train end-to-end summarization baseline models on truncated inputs (Trunc) that are 10 seconds long and 30 sec- onds long. Table 2 reports the results of training on truncated inputs and evaluating recordings that are 10 seconds and 30 seconds long, compared to different state-of-the-art approaches referenced in prior work. We note that using the standard full multi-head attention provides significant gains over restricted self-attention, and therefore use the standard multi-head self- attention for our experiments. 1https://github.com/srvk/how2-dataset 2Code will be released in https://github.com/espnet/espnet Table 2: Performance of Block-wise Adaptation and Training Approaches compared to Truncated Baselines with different inference strategies using ROUGE, METEOR, and BERTScore metrics- higher scores indicate better performance Training Method Inference Pre-training Train Maxlen Inf. Maxlen ROUGE-1\u2191 ROUGE-2\u2191 ROUGE-L\u2191 METEOR\u2191 BERTScore\u2191 Trunc, Restricted Self-Attention [7] Trunc, Full Self-Attention [8] + TTS Augmentation [8] Trunc-Baseline Trunc-Baseline Trunc-Baseline BASS-Adapt BASS-Train Standard Standard Standard Standard Standard Standard Block Block X X X X X X 10s X 100s 100s 100s 10s 30s 60s 30s 30s 100s 100s 100s 10s 30s 60s 30s 30s 60.73 65.30 68.40 60.87 63.30 64.57 63.99 60.87 44.9 51.40 54.10 45.12 47.58 49.11 49.00 43.12 56.10 62.10 65.00 56.79 59.16 60.49 60.17 54.79 29.30 32.50 34.90 30.00 31.76 32.47 32.17 29.12 Table 3: Part-of-speech coverage between the Predicted Sum- mary and the Reference for Truncated 10s baseline and BASS- ADAPT 30s model Model Noun Verb Adj Adv Prop.Noun recordings comprising 10-second chunks can do as well as a model trained on 60-second recordings. The proposed approach is more computationally efficient than the baseline by a factor of 3 since the proposed approach uses 3x smaller inputs 3 times for quadratic self-attentions. Baseline 10-sec BASS-ADAPT 0.85 0.87 0.76 0.79 0.84 0.84 0.65 0.67 Table 4: Performance of BASS models with block level inference across different implementations of the semantic updater model. Models are pre-trained on 10s and fine-tuned on 30s. R-1, R-2, R-3 represent the ROUGE-1, ROUGE-2, and ROUGE-L"}