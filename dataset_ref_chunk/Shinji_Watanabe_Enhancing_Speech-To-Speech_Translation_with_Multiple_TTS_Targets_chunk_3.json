{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Enhancing_Speech-To-Speech_Translation_with_Multiple_TTS_Targets_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What optimizer is used for training S2UT models?", "answer": " AdamW optimizer", "ref_chunk": "a dura- tion predictor that is jointly trained with unit-based HFG. The same of [11], the encoder is initialized from a Conformer-based wav2vec 2.0, while the decoder is initialized from the mBART decoder [33] released in [11]. To keep the consistency over different settings, we do not tune the hyper-parameters, but use the settings as [11] for different systems. TTS models: As mentioned in Sec. 2.1, we utilize three different TTS systems (i.e., TT2, FS2, and VITS) and three different vocoders (i.e., PWG, HFG, and SWG). To keep the reproducibility of the ex- periments, all the models are from public-available checkpoints in ESPnet-TTS [34, 35], an open-source framework for TTS. 3.2.2. Training and decoding For the training of S2UT models, we use the AdamW optimizer [36] with a learning rate of 0.0005. The scheduler is applied with a warmup policy that starts the learning rate from 1e-7 and reaches the maximum learning rate at 20k steps. We accumulate the gradients for every 120 steps to simulate a large batch size, which has shown to be effective for S2ST learning. During the training, we follow the \u201cLNA-D\u201d policy introduced in [11], which does not \ufb01ne-tune all the parameters in the pre-trained mBART decoder but only the Layer- Norm and self-attention parameters. For the decoding of S2UT, we apply beam search with a beam size of 10. For NAR TTS models, the duration can be tuned with a speed factor, resulting in the duration control to the generation. In our experiments, apart from the TTS models we discussed in Sec. 2.1, we also apply different speed factors including 0.95, 1.0, and 1.05. 3.2.3. Experiments design The experiments generally include three folds: Single TTS systems: To compare the effect of different TTS sys- tems, we directly train the S2UT model with discrete units generated from a single TTS system. Simple combination of TTS systems: As discussed in Sec. 2.2, multiple TTS systems could potentially improve the S2ST, by nor- malizing the noise from unit sequences. To systematically inves- tigate the effects of different systems, we carry out experiments based on the modeling properties of different TTS systems, includ- ing AR versus NAR, different vocoders, different speed factors, and text2Mel versus text2wav. Multi-task framework for multiple TTS targets: We follow the design of the experiments in the simple combination of TTS systems, but change from the data combination into the multi-task way of training, as introduced in Sec. 2.2. We use the wav2vec 2.0-based ASR model to compute the CER mentioned in Sec. 2.2. Due to the requirements of large GPU memory to train models with more than three additional branches, we limit the number of TTS systems to less than four. Table 1. S2ST Performances on different TTS systems. The \u201cData ID\u201d column stands for the target speech units generated from the TTS system. The CER and BLEU are calculated as discussed in Sec. 3.2.4. The acoustic models (AM) and vocoders are introduced in Sec. 2.1. Data ID AM Vocoder CER(\u2193) BLEU(\u2191) A B C TT2 PWG HFG SWG 9.1 8.9 8.7 37.3 37.3 37.3 Avg. TT2 / 8.9 37.3 D E F FS2 PWG HFG SWG 9.4 8.3 8.7 37.5 37.6 37.5 Avg. FS2 / 8.8 37.5 G VITS 8.4 38.3 3.2.4. Evaluation metric The TTS quality is \ufb01rst evaluated by inputting the synthesized speech to the ASR model and calculating the character error rate (CER) between the ASR prediction and the reference text. For the ASR model, we employ the open-source ASR model that is trained over wav2vec 2.03. For evaluation of the translation quality, we \ufb01rst utilize the same ASR model to get the transcription of the S2ST system (i.e., predicted units + code-HFG vocoder) and then compute BLEU score with the reference text using SacreBLEU [37]. Noted that the reference text is also tokenized and converted to lowercase without punctuation for BLEU calculation. 3.3. Results and Discussion We conduct experiments with target speech discrete units generated from a single TTS system. The best system was obtained from syn- thesized speech using VITS (data G), and the results are shown in Table 1. We observe that there was no signi\ufb01cant effect on S2ST performance when different vocoders were applied to models A-C and models D-F. However, the difference in acoustic models could affect the S2ST results, with the best system using target speech syn- thesized from VITS and the worst from TT2. We also \ufb01nd that the CER in each TTS acoustic model roughly correlated with their S2ST performance. An interesting \ufb01nding from Table 1 is that systems with differ- ent vocoders achieved similar performances despite having different CERs from the ASR model. We assume that this is due to the Hu- BERT units helping to normalize the vocoder differences in speech synthesis. To verify this hypothesis, we conduct a Pearson coef\ufb01- cient analysis over HuBERT unit distribution on FS2 with different vocoders and found that the Pearson scores were all above 0.98. Furthermore, we \ufb01nd that VITS usually had a higher CER than TT2 and FS2 for spoken words (e.g., \u201dHMM\u201d, \u201dHUM\u201d). This could be due to the data domain mismatch between the read speech used for TTS training (i.e., LJSpeech) and ASR training (i.e., Librispeech) and the conversational speech used for S2ST training (i.e., Fisher). Table 2 presents the effect of different speed factors on NAR TTS systems. The results show that a smaller speed factor can im- prove the performance of the S2ST system. However, when mea- 3https://huggingface.co/facebook/ wav2vec2-large-960h-lv60-self Table 2. S2ST Performances on different speed factors. The Fast- speech2 (FS) model is combined with the HFG vocoder for TTS as default. The TTS models are introduced in Sec. 2.1. Data ID TTS Speed Factor CER(\u2193) BLEU(\u2191) H E I FS2 0.95 1.0 1.05 8.7 8.3 9.5 38.0 37.5 37.4 J G K VITS 0.95 1.0 1.05 8.6 8.4 8.5 38.7 38.3 38.3 Table 3. S2ST Performances on multiple TTS targets. We follow the categories listed in Sec. 3.2.3 to conduct experiments The models with (cid:51)in the"}, {"question": " What is the learning rate used for training S2UT models?", "answer": " 0.0005", "ref_chunk": "a dura- tion predictor that is jointly trained with unit-based HFG. The same of [11], the encoder is initialized from a Conformer-based wav2vec 2.0, while the decoder is initialized from the mBART decoder [33] released in [11]. To keep the consistency over different settings, we do not tune the hyper-parameters, but use the settings as [11] for different systems. TTS models: As mentioned in Sec. 2.1, we utilize three different TTS systems (i.e., TT2, FS2, and VITS) and three different vocoders (i.e., PWG, HFG, and SWG). To keep the reproducibility of the ex- periments, all the models are from public-available checkpoints in ESPnet-TTS [34, 35], an open-source framework for TTS. 3.2.2. Training and decoding For the training of S2UT models, we use the AdamW optimizer [36] with a learning rate of 0.0005. The scheduler is applied with a warmup policy that starts the learning rate from 1e-7 and reaches the maximum learning rate at 20k steps. We accumulate the gradients for every 120 steps to simulate a large batch size, which has shown to be effective for S2ST learning. During the training, we follow the \u201cLNA-D\u201d policy introduced in [11], which does not \ufb01ne-tune all the parameters in the pre-trained mBART decoder but only the Layer- Norm and self-attention parameters. For the decoding of S2UT, we apply beam search with a beam size of 10. For NAR TTS models, the duration can be tuned with a speed factor, resulting in the duration control to the generation. In our experiments, apart from the TTS models we discussed in Sec. 2.1, we also apply different speed factors including 0.95, 1.0, and 1.05. 3.2.3. Experiments design The experiments generally include three folds: Single TTS systems: To compare the effect of different TTS sys- tems, we directly train the S2UT model with discrete units generated from a single TTS system. Simple combination of TTS systems: As discussed in Sec. 2.2, multiple TTS systems could potentially improve the S2ST, by nor- malizing the noise from unit sequences. To systematically inves- tigate the effects of different systems, we carry out experiments based on the modeling properties of different TTS systems, includ- ing AR versus NAR, different vocoders, different speed factors, and text2Mel versus text2wav. Multi-task framework for multiple TTS targets: We follow the design of the experiments in the simple combination of TTS systems, but change from the data combination into the multi-task way of training, as introduced in Sec. 2.2. We use the wav2vec 2.0-based ASR model to compute the CER mentioned in Sec. 2.2. Due to the requirements of large GPU memory to train models with more than three additional branches, we limit the number of TTS systems to less than four. Table 1. S2ST Performances on different TTS systems. The \u201cData ID\u201d column stands for the target speech units generated from the TTS system. The CER and BLEU are calculated as discussed in Sec. 3.2.4. The acoustic models (AM) and vocoders are introduced in Sec. 2.1. Data ID AM Vocoder CER(\u2193) BLEU(\u2191) A B C TT2 PWG HFG SWG 9.1 8.9 8.7 37.3 37.3 37.3 Avg. TT2 / 8.9 37.3 D E F FS2 PWG HFG SWG 9.4 8.3 8.7 37.5 37.6 37.5 Avg. FS2 / 8.8 37.5 G VITS 8.4 38.3 3.2.4. Evaluation metric The TTS quality is \ufb01rst evaluated by inputting the synthesized speech to the ASR model and calculating the character error rate (CER) between the ASR prediction and the reference text. For the ASR model, we employ the open-source ASR model that is trained over wav2vec 2.03. For evaluation of the translation quality, we \ufb01rst utilize the same ASR model to get the transcription of the S2ST system (i.e., predicted units + code-HFG vocoder) and then compute BLEU score with the reference text using SacreBLEU [37]. Noted that the reference text is also tokenized and converted to lowercase without punctuation for BLEU calculation. 3.3. Results and Discussion We conduct experiments with target speech discrete units generated from a single TTS system. The best system was obtained from syn- thesized speech using VITS (data G), and the results are shown in Table 1. We observe that there was no signi\ufb01cant effect on S2ST performance when different vocoders were applied to models A-C and models D-F. However, the difference in acoustic models could affect the S2ST results, with the best system using target speech syn- thesized from VITS and the worst from TT2. We also \ufb01nd that the CER in each TTS acoustic model roughly correlated with their S2ST performance. An interesting \ufb01nding from Table 1 is that systems with differ- ent vocoders achieved similar performances despite having different CERs from the ASR model. We assume that this is due to the Hu- BERT units helping to normalize the vocoder differences in speech synthesis. To verify this hypothesis, we conduct a Pearson coef\ufb01- cient analysis over HuBERT unit distribution on FS2 with different vocoders and found that the Pearson scores were all above 0.98. Furthermore, we \ufb01nd that VITS usually had a higher CER than TT2 and FS2 for spoken words (e.g., \u201dHMM\u201d, \u201dHUM\u201d). This could be due to the data domain mismatch between the read speech used for TTS training (i.e., LJSpeech) and ASR training (i.e., Librispeech) and the conversational speech used for S2ST training (i.e., Fisher). Table 2 presents the effect of different speed factors on NAR TTS systems. The results show that a smaller speed factor can im- prove the performance of the S2ST system. However, when mea- 3https://huggingface.co/facebook/ wav2vec2-large-960h-lv60-self Table 2. S2ST Performances on different speed factors. The Fast- speech2 (FS) model is combined with the HFG vocoder for TTS as default. The TTS models are introduced in Sec. 2.1. Data ID TTS Speed Factor CER(\u2193) BLEU(\u2191) H E I FS2 0.95 1.0 1.05 8.7 8.3 9.5 38.0 37.5 37.4 J G K VITS 0.95 1.0 1.05 8.6 8.4 8.5 38.7 38.3 38.3 Table 3. S2ST Performances on multiple TTS targets. We follow the categories listed in Sec. 3.2.3 to conduct experiments The models with (cid:51)in the"}, {"question": " How many steps does it take for the learning rate to reach the maximum value during training?", "answer": " 20k steps", "ref_chunk": "a dura- tion predictor that is jointly trained with unit-based HFG. The same of [11], the encoder is initialized from a Conformer-based wav2vec 2.0, while the decoder is initialized from the mBART decoder [33] released in [11]. To keep the consistency over different settings, we do not tune the hyper-parameters, but use the settings as [11] for different systems. TTS models: As mentioned in Sec. 2.1, we utilize three different TTS systems (i.e., TT2, FS2, and VITS) and three different vocoders (i.e., PWG, HFG, and SWG). To keep the reproducibility of the ex- periments, all the models are from public-available checkpoints in ESPnet-TTS [34, 35], an open-source framework for TTS. 3.2.2. Training and decoding For the training of S2UT models, we use the AdamW optimizer [36] with a learning rate of 0.0005. The scheduler is applied with a warmup policy that starts the learning rate from 1e-7 and reaches the maximum learning rate at 20k steps. We accumulate the gradients for every 120 steps to simulate a large batch size, which has shown to be effective for S2ST learning. During the training, we follow the \u201cLNA-D\u201d policy introduced in [11], which does not \ufb01ne-tune all the parameters in the pre-trained mBART decoder but only the Layer- Norm and self-attention parameters. For the decoding of S2UT, we apply beam search with a beam size of 10. For NAR TTS models, the duration can be tuned with a speed factor, resulting in the duration control to the generation. In our experiments, apart from the TTS models we discussed in Sec. 2.1, we also apply different speed factors including 0.95, 1.0, and 1.05. 3.2.3. Experiments design The experiments generally include three folds: Single TTS systems: To compare the effect of different TTS sys- tems, we directly train the S2UT model with discrete units generated from a single TTS system. Simple combination of TTS systems: As discussed in Sec. 2.2, multiple TTS systems could potentially improve the S2ST, by nor- malizing the noise from unit sequences. To systematically inves- tigate the effects of different systems, we carry out experiments based on the modeling properties of different TTS systems, includ- ing AR versus NAR, different vocoders, different speed factors, and text2Mel versus text2wav. Multi-task framework for multiple TTS targets: We follow the design of the experiments in the simple combination of TTS systems, but change from the data combination into the multi-task way of training, as introduced in Sec. 2.2. We use the wav2vec 2.0-based ASR model to compute the CER mentioned in Sec. 2.2. Due to the requirements of large GPU memory to train models with more than three additional branches, we limit the number of TTS systems to less than four. Table 1. S2ST Performances on different TTS systems. The \u201cData ID\u201d column stands for the target speech units generated from the TTS system. The CER and BLEU are calculated as discussed in Sec. 3.2.4. The acoustic models (AM) and vocoders are introduced in Sec. 2.1. Data ID AM Vocoder CER(\u2193) BLEU(\u2191) A B C TT2 PWG HFG SWG 9.1 8.9 8.7 37.3 37.3 37.3 Avg. TT2 / 8.9 37.3 D E F FS2 PWG HFG SWG 9.4 8.3 8.7 37.5 37.6 37.5 Avg. FS2 / 8.8 37.5 G VITS 8.4 38.3 3.2.4. Evaluation metric The TTS quality is \ufb01rst evaluated by inputting the synthesized speech to the ASR model and calculating the character error rate (CER) between the ASR prediction and the reference text. For the ASR model, we employ the open-source ASR model that is trained over wav2vec 2.03. For evaluation of the translation quality, we \ufb01rst utilize the same ASR model to get the transcription of the S2ST system (i.e., predicted units + code-HFG vocoder) and then compute BLEU score with the reference text using SacreBLEU [37]. Noted that the reference text is also tokenized and converted to lowercase without punctuation for BLEU calculation. 3.3. Results and Discussion We conduct experiments with target speech discrete units generated from a single TTS system. The best system was obtained from syn- thesized speech using VITS (data G), and the results are shown in Table 1. We observe that there was no signi\ufb01cant effect on S2ST performance when different vocoders were applied to models A-C and models D-F. However, the difference in acoustic models could affect the S2ST results, with the best system using target speech syn- thesized from VITS and the worst from TT2. We also \ufb01nd that the CER in each TTS acoustic model roughly correlated with their S2ST performance. An interesting \ufb01nding from Table 1 is that systems with differ- ent vocoders achieved similar performances despite having different CERs from the ASR model. We assume that this is due to the Hu- BERT units helping to normalize the vocoder differences in speech synthesis. To verify this hypothesis, we conduct a Pearson coef\ufb01- cient analysis over HuBERT unit distribution on FS2 with different vocoders and found that the Pearson scores were all above 0.98. Furthermore, we \ufb01nd that VITS usually had a higher CER than TT2 and FS2 for spoken words (e.g., \u201dHMM\u201d, \u201dHUM\u201d). This could be due to the data domain mismatch between the read speech used for TTS training (i.e., LJSpeech) and ASR training (i.e., Librispeech) and the conversational speech used for S2ST training (i.e., Fisher). Table 2 presents the effect of different speed factors on NAR TTS systems. The results show that a smaller speed factor can im- prove the performance of the S2ST system. However, when mea- 3https://huggingface.co/facebook/ wav2vec2-large-960h-lv60-self Table 2. S2ST Performances on different speed factors. The Fast- speech2 (FS) model is combined with the HFG vocoder for TTS as default. The TTS models are introduced in Sec. 2.1. Data ID TTS Speed Factor CER(\u2193) BLEU(\u2191) H E I FS2 0.95 1.0 1.05 8.7 8.3 9.5 38.0 37.5 37.4 J G K VITS 0.95 1.0 1.05 8.6 8.4 8.5 38.7 38.3 38.3 Table 3. S2ST Performances on multiple TTS targets. We follow the categories listed in Sec. 3.2.3 to conduct experiments The models with (cid:51)in the"}, {"question": " What policy is followed during training that does not fine-tune all parameters in the pre-trained mBART decoder?", "answer": " LNA-D policy", "ref_chunk": "a dura- tion predictor that is jointly trained with unit-based HFG. The same of [11], the encoder is initialized from a Conformer-based wav2vec 2.0, while the decoder is initialized from the mBART decoder [33] released in [11]. To keep the consistency over different settings, we do not tune the hyper-parameters, but use the settings as [11] for different systems. TTS models: As mentioned in Sec. 2.1, we utilize three different TTS systems (i.e., TT2, FS2, and VITS) and three different vocoders (i.e., PWG, HFG, and SWG). To keep the reproducibility of the ex- periments, all the models are from public-available checkpoints in ESPnet-TTS [34, 35], an open-source framework for TTS. 3.2.2. Training and decoding For the training of S2UT models, we use the AdamW optimizer [36] with a learning rate of 0.0005. The scheduler is applied with a warmup policy that starts the learning rate from 1e-7 and reaches the maximum learning rate at 20k steps. We accumulate the gradients for every 120 steps to simulate a large batch size, which has shown to be effective for S2ST learning. During the training, we follow the \u201cLNA-D\u201d policy introduced in [11], which does not \ufb01ne-tune all the parameters in the pre-trained mBART decoder but only the Layer- Norm and self-attention parameters. For the decoding of S2UT, we apply beam search with a beam size of 10. For NAR TTS models, the duration can be tuned with a speed factor, resulting in the duration control to the generation. In our experiments, apart from the TTS models we discussed in Sec. 2.1, we also apply different speed factors including 0.95, 1.0, and 1.05. 3.2.3. Experiments design The experiments generally include three folds: Single TTS systems: To compare the effect of different TTS sys- tems, we directly train the S2UT model with discrete units generated from a single TTS system. Simple combination of TTS systems: As discussed in Sec. 2.2, multiple TTS systems could potentially improve the S2ST, by nor- malizing the noise from unit sequences. To systematically inves- tigate the effects of different systems, we carry out experiments based on the modeling properties of different TTS systems, includ- ing AR versus NAR, different vocoders, different speed factors, and text2Mel versus text2wav. Multi-task framework for multiple TTS targets: We follow the design of the experiments in the simple combination of TTS systems, but change from the data combination into the multi-task way of training, as introduced in Sec. 2.2. We use the wav2vec 2.0-based ASR model to compute the CER mentioned in Sec. 2.2. Due to the requirements of large GPU memory to train models with more than three additional branches, we limit the number of TTS systems to less than four. Table 1. S2ST Performances on different TTS systems. The \u201cData ID\u201d column stands for the target speech units generated from the TTS system. The CER and BLEU are calculated as discussed in Sec. 3.2.4. The acoustic models (AM) and vocoders are introduced in Sec. 2.1. Data ID AM Vocoder CER(\u2193) BLEU(\u2191) A B C TT2 PWG HFG SWG 9.1 8.9 8.7 37.3 37.3 37.3 Avg. TT2 / 8.9 37.3 D E F FS2 PWG HFG SWG 9.4 8.3 8.7 37.5 37.6 37.5 Avg. FS2 / 8.8 37.5 G VITS 8.4 38.3 3.2.4. Evaluation metric The TTS quality is \ufb01rst evaluated by inputting the synthesized speech to the ASR model and calculating the character error rate (CER) between the ASR prediction and the reference text. For the ASR model, we employ the open-source ASR model that is trained over wav2vec 2.03. For evaluation of the translation quality, we \ufb01rst utilize the same ASR model to get the transcription of the S2ST system (i.e., predicted units + code-HFG vocoder) and then compute BLEU score with the reference text using SacreBLEU [37]. Noted that the reference text is also tokenized and converted to lowercase without punctuation for BLEU calculation. 3.3. Results and Discussion We conduct experiments with target speech discrete units generated from a single TTS system. The best system was obtained from syn- thesized speech using VITS (data G), and the results are shown in Table 1. We observe that there was no signi\ufb01cant effect on S2ST performance when different vocoders were applied to models A-C and models D-F. However, the difference in acoustic models could affect the S2ST results, with the best system using target speech syn- thesized from VITS and the worst from TT2. We also \ufb01nd that the CER in each TTS acoustic model roughly correlated with their S2ST performance. An interesting \ufb01nding from Table 1 is that systems with differ- ent vocoders achieved similar performances despite having different CERs from the ASR model. We assume that this is due to the Hu- BERT units helping to normalize the vocoder differences in speech synthesis. To verify this hypothesis, we conduct a Pearson coef\ufb01- cient analysis over HuBERT unit distribution on FS2 with different vocoders and found that the Pearson scores were all above 0.98. Furthermore, we \ufb01nd that VITS usually had a higher CER than TT2 and FS2 for spoken words (e.g., \u201dHMM\u201d, \u201dHUM\u201d). This could be due to the data domain mismatch between the read speech used for TTS training (i.e., LJSpeech) and ASR training (i.e., Librispeech) and the conversational speech used for S2ST training (i.e., Fisher). Table 2 presents the effect of different speed factors on NAR TTS systems. The results show that a smaller speed factor can im- prove the performance of the S2ST system. However, when mea- 3https://huggingface.co/facebook/ wav2vec2-large-960h-lv60-self Table 2. S2ST Performances on different speed factors. The Fast- speech2 (FS) model is combined with the HFG vocoder for TTS as default. The TTS models are introduced in Sec. 2.1. Data ID TTS Speed Factor CER(\u2193) BLEU(\u2191) H E I FS2 0.95 1.0 1.05 8.7 8.3 9.5 38.0 37.5 37.4 J G K VITS 0.95 1.0 1.05 8.6 8.4 8.5 38.7 38.3 38.3 Table 3. S2ST Performances on multiple TTS targets. We follow the categories listed in Sec. 3.2.3 to conduct experiments The models with (cid:51)in the"}, {"question": " What decoding technique is applied for S2UT?", "answer": " Beam search with a beam size of 10", "ref_chunk": "a dura- tion predictor that is jointly trained with unit-based HFG. The same of [11], the encoder is initialized from a Conformer-based wav2vec 2.0, while the decoder is initialized from the mBART decoder [33] released in [11]. To keep the consistency over different settings, we do not tune the hyper-parameters, but use the settings as [11] for different systems. TTS models: As mentioned in Sec. 2.1, we utilize three different TTS systems (i.e., TT2, FS2, and VITS) and three different vocoders (i.e., PWG, HFG, and SWG). To keep the reproducibility of the ex- periments, all the models are from public-available checkpoints in ESPnet-TTS [34, 35], an open-source framework for TTS. 3.2.2. Training and decoding For the training of S2UT models, we use the AdamW optimizer [36] with a learning rate of 0.0005. The scheduler is applied with a warmup policy that starts the learning rate from 1e-7 and reaches the maximum learning rate at 20k steps. We accumulate the gradients for every 120 steps to simulate a large batch size, which has shown to be effective for S2ST learning. During the training, we follow the \u201cLNA-D\u201d policy introduced in [11], which does not \ufb01ne-tune all the parameters in the pre-trained mBART decoder but only the Layer- Norm and self-attention parameters. For the decoding of S2UT, we apply beam search with a beam size of 10. For NAR TTS models, the duration can be tuned with a speed factor, resulting in the duration control to the generation. In our experiments, apart from the TTS models we discussed in Sec. 2.1, we also apply different speed factors including 0.95, 1.0, and 1.05. 3.2.3. Experiments design The experiments generally include three folds: Single TTS systems: To compare the effect of different TTS sys- tems, we directly train the S2UT model with discrete units generated from a single TTS system. Simple combination of TTS systems: As discussed in Sec. 2.2, multiple TTS systems could potentially improve the S2ST, by nor- malizing the noise from unit sequences. To systematically inves- tigate the effects of different systems, we carry out experiments based on the modeling properties of different TTS systems, includ- ing AR versus NAR, different vocoders, different speed factors, and text2Mel versus text2wav. Multi-task framework for multiple TTS targets: We follow the design of the experiments in the simple combination of TTS systems, but change from the data combination into the multi-task way of training, as introduced in Sec. 2.2. We use the wav2vec 2.0-based ASR model to compute the CER mentioned in Sec. 2.2. Due to the requirements of large GPU memory to train models with more than three additional branches, we limit the number of TTS systems to less than four. Table 1. S2ST Performances on different TTS systems. The \u201cData ID\u201d column stands for the target speech units generated from the TTS system. The CER and BLEU are calculated as discussed in Sec. 3.2.4. The acoustic models (AM) and vocoders are introduced in Sec. 2.1. Data ID AM Vocoder CER(\u2193) BLEU(\u2191) A B C TT2 PWG HFG SWG 9.1 8.9 8.7 37.3 37.3 37.3 Avg. TT2 / 8.9 37.3 D E F FS2 PWG HFG SWG 9.4 8.3 8.7 37.5 37.6 37.5 Avg. FS2 / 8.8 37.5 G VITS 8.4 38.3 3.2.4. Evaluation metric The TTS quality is \ufb01rst evaluated by inputting the synthesized speech to the ASR model and calculating the character error rate (CER) between the ASR prediction and the reference text. For the ASR model, we employ the open-source ASR model that is trained over wav2vec 2.03. For evaluation of the translation quality, we \ufb01rst utilize the same ASR model to get the transcription of the S2ST system (i.e., predicted units + code-HFG vocoder) and then compute BLEU score with the reference text using SacreBLEU [37]. Noted that the reference text is also tokenized and converted to lowercase without punctuation for BLEU calculation. 3.3. Results and Discussion We conduct experiments with target speech discrete units generated from a single TTS system. The best system was obtained from syn- thesized speech using VITS (data G), and the results are shown in Table 1. We observe that there was no signi\ufb01cant effect on S2ST performance when different vocoders were applied to models A-C and models D-F. However, the difference in acoustic models could affect the S2ST results, with the best system using target speech syn- thesized from VITS and the worst from TT2. We also \ufb01nd that the CER in each TTS acoustic model roughly correlated with their S2ST performance. An interesting \ufb01nding from Table 1 is that systems with differ- ent vocoders achieved similar performances despite having different CERs from the ASR model. We assume that this is due to the Hu- BERT units helping to normalize the vocoder differences in speech synthesis. To verify this hypothesis, we conduct a Pearson coef\ufb01- cient analysis over HuBERT unit distribution on FS2 with different vocoders and found that the Pearson scores were all above 0.98. Furthermore, we \ufb01nd that VITS usually had a higher CER than TT2 and FS2 for spoken words (e.g., \u201dHMM\u201d, \u201dHUM\u201d). This could be due to the data domain mismatch between the read speech used for TTS training (i.e., LJSpeech) and ASR training (i.e., Librispeech) and the conversational speech used for S2ST training (i.e., Fisher). Table 2 presents the effect of different speed factors on NAR TTS systems. The results show that a smaller speed factor can im- prove the performance of the S2ST system. However, when mea- 3https://huggingface.co/facebook/ wav2vec2-large-960h-lv60-self Table 2. S2ST Performances on different speed factors. The Fast- speech2 (FS) model is combined with the HFG vocoder for TTS as default. The TTS models are introduced in Sec. 2.1. Data ID TTS Speed Factor CER(\u2193) BLEU(\u2191) H E I FS2 0.95 1.0 1.05 8.7 8.3 9.5 38.0 37.5 37.4 J G K VITS 0.95 1.0 1.05 8.6 8.4 8.5 38.7 38.3 38.3 Table 3. S2ST Performances on multiple TTS targets. We follow the categories listed in Sec. 3.2.3 to conduct experiments The models with (cid:51)in the"}, {"question": " How can the duration in NAR TTS models be controlled?", "answer": " By tuning with a speed factor", "ref_chunk": "a dura- tion predictor that is jointly trained with unit-based HFG. The same of [11], the encoder is initialized from a Conformer-based wav2vec 2.0, while the decoder is initialized from the mBART decoder [33] released in [11]. To keep the consistency over different settings, we do not tune the hyper-parameters, but use the settings as [11] for different systems. TTS models: As mentioned in Sec. 2.1, we utilize three different TTS systems (i.e., TT2, FS2, and VITS) and three different vocoders (i.e., PWG, HFG, and SWG). To keep the reproducibility of the ex- periments, all the models are from public-available checkpoints in ESPnet-TTS [34, 35], an open-source framework for TTS. 3.2.2. Training and decoding For the training of S2UT models, we use the AdamW optimizer [36] with a learning rate of 0.0005. The scheduler is applied with a warmup policy that starts the learning rate from 1e-7 and reaches the maximum learning rate at 20k steps. We accumulate the gradients for every 120 steps to simulate a large batch size, which has shown to be effective for S2ST learning. During the training, we follow the \u201cLNA-D\u201d policy introduced in [11], which does not \ufb01ne-tune all the parameters in the pre-trained mBART decoder but only the Layer- Norm and self-attention parameters. For the decoding of S2UT, we apply beam search with a beam size of 10. For NAR TTS models, the duration can be tuned with a speed factor, resulting in the duration control to the generation. In our experiments, apart from the TTS models we discussed in Sec. 2.1, we also apply different speed factors including 0.95, 1.0, and 1.05. 3.2.3. Experiments design The experiments generally include three folds: Single TTS systems: To compare the effect of different TTS sys- tems, we directly train the S2UT model with discrete units generated from a single TTS system. Simple combination of TTS systems: As discussed in Sec. 2.2, multiple TTS systems could potentially improve the S2ST, by nor- malizing the noise from unit sequences. To systematically inves- tigate the effects of different systems, we carry out experiments based on the modeling properties of different TTS systems, includ- ing AR versus NAR, different vocoders, different speed factors, and text2Mel versus text2wav. Multi-task framework for multiple TTS targets: We follow the design of the experiments in the simple combination of TTS systems, but change from the data combination into the multi-task way of training, as introduced in Sec. 2.2. We use the wav2vec 2.0-based ASR model to compute the CER mentioned in Sec. 2.2. Due to the requirements of large GPU memory to train models with more than three additional branches, we limit the number of TTS systems to less than four. Table 1. S2ST Performances on different TTS systems. The \u201cData ID\u201d column stands for the target speech units generated from the TTS system. The CER and BLEU are calculated as discussed in Sec. 3.2.4. The acoustic models (AM) and vocoders are introduced in Sec. 2.1. Data ID AM Vocoder CER(\u2193) BLEU(\u2191) A B C TT2 PWG HFG SWG 9.1 8.9 8.7 37.3 37.3 37.3 Avg. TT2 / 8.9 37.3 D E F FS2 PWG HFG SWG 9.4 8.3 8.7 37.5 37.6 37.5 Avg. FS2 / 8.8 37.5 G VITS 8.4 38.3 3.2.4. Evaluation metric The TTS quality is \ufb01rst evaluated by inputting the synthesized speech to the ASR model and calculating the character error rate (CER) between the ASR prediction and the reference text. For the ASR model, we employ the open-source ASR model that is trained over wav2vec 2.03. For evaluation of the translation quality, we \ufb01rst utilize the same ASR model to get the transcription of the S2ST system (i.e., predicted units + code-HFG vocoder) and then compute BLEU score with the reference text using SacreBLEU [37]. Noted that the reference text is also tokenized and converted to lowercase without punctuation for BLEU calculation. 3.3. Results and Discussion We conduct experiments with target speech discrete units generated from a single TTS system. The best system was obtained from syn- thesized speech using VITS (data G), and the results are shown in Table 1. We observe that there was no signi\ufb01cant effect on S2ST performance when different vocoders were applied to models A-C and models D-F. However, the difference in acoustic models could affect the S2ST results, with the best system using target speech syn- thesized from VITS and the worst from TT2. We also \ufb01nd that the CER in each TTS acoustic model roughly correlated with their S2ST performance. An interesting \ufb01nding from Table 1 is that systems with differ- ent vocoders achieved similar performances despite having different CERs from the ASR model. We assume that this is due to the Hu- BERT units helping to normalize the vocoder differences in speech synthesis. To verify this hypothesis, we conduct a Pearson coef\ufb01- cient analysis over HuBERT unit distribution on FS2 with different vocoders and found that the Pearson scores were all above 0.98. Furthermore, we \ufb01nd that VITS usually had a higher CER than TT2 and FS2 for spoken words (e.g., \u201dHMM\u201d, \u201dHUM\u201d). This could be due to the data domain mismatch between the read speech used for TTS training (i.e., LJSpeech) and ASR training (i.e., Librispeech) and the conversational speech used for S2ST training (i.e., Fisher). Table 2 presents the effect of different speed factors on NAR TTS systems. The results show that a smaller speed factor can im- prove the performance of the S2ST system. However, when mea- 3https://huggingface.co/facebook/ wav2vec2-large-960h-lv60-self Table 2. S2ST Performances on different speed factors. The Fast- speech2 (FS) model is combined with the HFG vocoder for TTS as default. The TTS models are introduced in Sec. 2.1. Data ID TTS Speed Factor CER(\u2193) BLEU(\u2191) H E I FS2 0.95 1.0 1.05 8.7 8.3 9.5 38.0 37.5 37.4 J G K VITS 0.95 1.0 1.05 8.6 8.4 8.5 38.7 38.3 38.3 Table 3. S2ST Performances on multiple TTS targets. We follow the categories listed in Sec. 3.2.3 to conduct experiments The models with (cid:51)in the"}, {"question": " What limits the number of TTS systems to less than four in the multi-task framework experiments?", "answer": " Large GPU memory requirements", "ref_chunk": "a dura- tion predictor that is jointly trained with unit-based HFG. The same of [11], the encoder is initialized from a Conformer-based wav2vec 2.0, while the decoder is initialized from the mBART decoder [33] released in [11]. To keep the consistency over different settings, we do not tune the hyper-parameters, but use the settings as [11] for different systems. TTS models: As mentioned in Sec. 2.1, we utilize three different TTS systems (i.e., TT2, FS2, and VITS) and three different vocoders (i.e., PWG, HFG, and SWG). To keep the reproducibility of the ex- periments, all the models are from public-available checkpoints in ESPnet-TTS [34, 35], an open-source framework for TTS. 3.2.2. Training and decoding For the training of S2UT models, we use the AdamW optimizer [36] with a learning rate of 0.0005. The scheduler is applied with a warmup policy that starts the learning rate from 1e-7 and reaches the maximum learning rate at 20k steps. We accumulate the gradients for every 120 steps to simulate a large batch size, which has shown to be effective for S2ST learning. During the training, we follow the \u201cLNA-D\u201d policy introduced in [11], which does not \ufb01ne-tune all the parameters in the pre-trained mBART decoder but only the Layer- Norm and self-attention parameters. For the decoding of S2UT, we apply beam search with a beam size of 10. For NAR TTS models, the duration can be tuned with a speed factor, resulting in the duration control to the generation. In our experiments, apart from the TTS models we discussed in Sec. 2.1, we also apply different speed factors including 0.95, 1.0, and 1.05. 3.2.3. Experiments design The experiments generally include three folds: Single TTS systems: To compare the effect of different TTS sys- tems, we directly train the S2UT model with discrete units generated from a single TTS system. Simple combination of TTS systems: As discussed in Sec. 2.2, multiple TTS systems could potentially improve the S2ST, by nor- malizing the noise from unit sequences. To systematically inves- tigate the effects of different systems, we carry out experiments based on the modeling properties of different TTS systems, includ- ing AR versus NAR, different vocoders, different speed factors, and text2Mel versus text2wav. Multi-task framework for multiple TTS targets: We follow the design of the experiments in the simple combination of TTS systems, but change from the data combination into the multi-task way of training, as introduced in Sec. 2.2. We use the wav2vec 2.0-based ASR model to compute the CER mentioned in Sec. 2.2. Due to the requirements of large GPU memory to train models with more than three additional branches, we limit the number of TTS systems to less than four. Table 1. S2ST Performances on different TTS systems. The \u201cData ID\u201d column stands for the target speech units generated from the TTS system. The CER and BLEU are calculated as discussed in Sec. 3.2.4. The acoustic models (AM) and vocoders are introduced in Sec. 2.1. Data ID AM Vocoder CER(\u2193) BLEU(\u2191) A B C TT2 PWG HFG SWG 9.1 8.9 8.7 37.3 37.3 37.3 Avg. TT2 / 8.9 37.3 D E F FS2 PWG HFG SWG 9.4 8.3 8.7 37.5 37.6 37.5 Avg. FS2 / 8.8 37.5 G VITS 8.4 38.3 3.2.4. Evaluation metric The TTS quality is \ufb01rst evaluated by inputting the synthesized speech to the ASR model and calculating the character error rate (CER) between the ASR prediction and the reference text. For the ASR model, we employ the open-source ASR model that is trained over wav2vec 2.03. For evaluation of the translation quality, we \ufb01rst utilize the same ASR model to get the transcription of the S2ST system (i.e., predicted units + code-HFG vocoder) and then compute BLEU score with the reference text using SacreBLEU [37]. Noted that the reference text is also tokenized and converted to lowercase without punctuation for BLEU calculation. 3.3. Results and Discussion We conduct experiments with target speech discrete units generated from a single TTS system. The best system was obtained from syn- thesized speech using VITS (data G), and the results are shown in Table 1. We observe that there was no signi\ufb01cant effect on S2ST performance when different vocoders were applied to models A-C and models D-F. However, the difference in acoustic models could affect the S2ST results, with the best system using target speech syn- thesized from VITS and the worst from TT2. We also \ufb01nd that the CER in each TTS acoustic model roughly correlated with their S2ST performance. An interesting \ufb01nding from Table 1 is that systems with differ- ent vocoders achieved similar performances despite having different CERs from the ASR model. We assume that this is due to the Hu- BERT units helping to normalize the vocoder differences in speech synthesis. To verify this hypothesis, we conduct a Pearson coef\ufb01- cient analysis over HuBERT unit distribution on FS2 with different vocoders and found that the Pearson scores were all above 0.98. Furthermore, we \ufb01nd that VITS usually had a higher CER than TT2 and FS2 for spoken words (e.g., \u201dHMM\u201d, \u201dHUM\u201d). This could be due to the data domain mismatch between the read speech used for TTS training (i.e., LJSpeech) and ASR training (i.e., Librispeech) and the conversational speech used for S2ST training (i.e., Fisher). Table 2 presents the effect of different speed factors on NAR TTS systems. The results show that a smaller speed factor can im- prove the performance of the S2ST system. However, when mea- 3https://huggingface.co/facebook/ wav2vec2-large-960h-lv60-self Table 2. S2ST Performances on different speed factors. The Fast- speech2 (FS) model is combined with the HFG vocoder for TTS as default. The TTS models are introduced in Sec. 2.1. Data ID TTS Speed Factor CER(\u2193) BLEU(\u2191) H E I FS2 0.95 1.0 1.05 8.7 8.3 9.5 38.0 37.5 37.4 J G K VITS 0.95 1.0 1.05 8.6 8.4 8.5 38.7 38.3 38.3 Table 3. S2ST Performances on multiple TTS targets. We follow the categories listed in Sec. 3.2.3 to conduct experiments The models with (cid:51)in the"}, {"question": " How is the TTS quality evaluated first?", "answer": " By calculating the character error rate (CER) between the ASR prediction and the reference text", "ref_chunk": "a dura- tion predictor that is jointly trained with unit-based HFG. The same of [11], the encoder is initialized from a Conformer-based wav2vec 2.0, while the decoder is initialized from the mBART decoder [33] released in [11]. To keep the consistency over different settings, we do not tune the hyper-parameters, but use the settings as [11] for different systems. TTS models: As mentioned in Sec. 2.1, we utilize three different TTS systems (i.e., TT2, FS2, and VITS) and three different vocoders (i.e., PWG, HFG, and SWG). To keep the reproducibility of the ex- periments, all the models are from public-available checkpoints in ESPnet-TTS [34, 35], an open-source framework for TTS. 3.2.2. Training and decoding For the training of S2UT models, we use the AdamW optimizer [36] with a learning rate of 0.0005. The scheduler is applied with a warmup policy that starts the learning rate from 1e-7 and reaches the maximum learning rate at 20k steps. We accumulate the gradients for every 120 steps to simulate a large batch size, which has shown to be effective for S2ST learning. During the training, we follow the \u201cLNA-D\u201d policy introduced in [11], which does not \ufb01ne-tune all the parameters in the pre-trained mBART decoder but only the Layer- Norm and self-attention parameters. For the decoding of S2UT, we apply beam search with a beam size of 10. For NAR TTS models, the duration can be tuned with a speed factor, resulting in the duration control to the generation. In our experiments, apart from the TTS models we discussed in Sec. 2.1, we also apply different speed factors including 0.95, 1.0, and 1.05. 3.2.3. Experiments design The experiments generally include three folds: Single TTS systems: To compare the effect of different TTS sys- tems, we directly train the S2UT model with discrete units generated from a single TTS system. Simple combination of TTS systems: As discussed in Sec. 2.2, multiple TTS systems could potentially improve the S2ST, by nor- malizing the noise from unit sequences. To systematically inves- tigate the effects of different systems, we carry out experiments based on the modeling properties of different TTS systems, includ- ing AR versus NAR, different vocoders, different speed factors, and text2Mel versus text2wav. Multi-task framework for multiple TTS targets: We follow the design of the experiments in the simple combination of TTS systems, but change from the data combination into the multi-task way of training, as introduced in Sec. 2.2. We use the wav2vec 2.0-based ASR model to compute the CER mentioned in Sec. 2.2. Due to the requirements of large GPU memory to train models with more than three additional branches, we limit the number of TTS systems to less than four. Table 1. S2ST Performances on different TTS systems. The \u201cData ID\u201d column stands for the target speech units generated from the TTS system. The CER and BLEU are calculated as discussed in Sec. 3.2.4. The acoustic models (AM) and vocoders are introduced in Sec. 2.1. Data ID AM Vocoder CER(\u2193) BLEU(\u2191) A B C TT2 PWG HFG SWG 9.1 8.9 8.7 37.3 37.3 37.3 Avg. TT2 / 8.9 37.3 D E F FS2 PWG HFG SWG 9.4 8.3 8.7 37.5 37.6 37.5 Avg. FS2 / 8.8 37.5 G VITS 8.4 38.3 3.2.4. Evaluation metric The TTS quality is \ufb01rst evaluated by inputting the synthesized speech to the ASR model and calculating the character error rate (CER) between the ASR prediction and the reference text. For the ASR model, we employ the open-source ASR model that is trained over wav2vec 2.03. For evaluation of the translation quality, we \ufb01rst utilize the same ASR model to get the transcription of the S2ST system (i.e., predicted units + code-HFG vocoder) and then compute BLEU score with the reference text using SacreBLEU [37]. Noted that the reference text is also tokenized and converted to lowercase without punctuation for BLEU calculation. 3.3. Results and Discussion We conduct experiments with target speech discrete units generated from a single TTS system. The best system was obtained from syn- thesized speech using VITS (data G), and the results are shown in Table 1. We observe that there was no signi\ufb01cant effect on S2ST performance when different vocoders were applied to models A-C and models D-F. However, the difference in acoustic models could affect the S2ST results, with the best system using target speech syn- thesized from VITS and the worst from TT2. We also \ufb01nd that the CER in each TTS acoustic model roughly correlated with their S2ST performance. An interesting \ufb01nding from Table 1 is that systems with differ- ent vocoders achieved similar performances despite having different CERs from the ASR model. We assume that this is due to the Hu- BERT units helping to normalize the vocoder differences in speech synthesis. To verify this hypothesis, we conduct a Pearson coef\ufb01- cient analysis over HuBERT unit distribution on FS2 with different vocoders and found that the Pearson scores were all above 0.98. Furthermore, we \ufb01nd that VITS usually had a higher CER than TT2 and FS2 for spoken words (e.g., \u201dHMM\u201d, \u201dHUM\u201d). This could be due to the data domain mismatch between the read speech used for TTS training (i.e., LJSpeech) and ASR training (i.e., Librispeech) and the conversational speech used for S2ST training (i.e., Fisher). Table 2 presents the effect of different speed factors on NAR TTS systems. The results show that a smaller speed factor can im- prove the performance of the S2ST system. However, when mea- 3https://huggingface.co/facebook/ wav2vec2-large-960h-lv60-self Table 2. S2ST Performances on different speed factors. The Fast- speech2 (FS) model is combined with the HFG vocoder for TTS as default. The TTS models are introduced in Sec. 2.1. Data ID TTS Speed Factor CER(\u2193) BLEU(\u2191) H E I FS2 0.95 1.0 1.05 8.7 8.3 9.5 38.0 37.5 37.4 J G K VITS 0.95 1.0 1.05 8.6 8.4 8.5 38.7 38.3 38.3 Table 3. S2ST Performances on multiple TTS targets. We follow the categories listed in Sec. 3.2.3 to conduct experiments The models with (cid:51)in the"}, {"question": " What model is employed for evaluation of the translation quality?", "answer": " Open-source ASR model trained over wav2vec 2.0", "ref_chunk": "a dura- tion predictor that is jointly trained with unit-based HFG. The same of [11], the encoder is initialized from a Conformer-based wav2vec 2.0, while the decoder is initialized from the mBART decoder [33] released in [11]. To keep the consistency over different settings, we do not tune the hyper-parameters, but use the settings as [11] for different systems. TTS models: As mentioned in Sec. 2.1, we utilize three different TTS systems (i.e., TT2, FS2, and VITS) and three different vocoders (i.e., PWG, HFG, and SWG). To keep the reproducibility of the ex- periments, all the models are from public-available checkpoints in ESPnet-TTS [34, 35], an open-source framework for TTS. 3.2.2. Training and decoding For the training of S2UT models, we use the AdamW optimizer [36] with a learning rate of 0.0005. The scheduler is applied with a warmup policy that starts the learning rate from 1e-7 and reaches the maximum learning rate at 20k steps. We accumulate the gradients for every 120 steps to simulate a large batch size, which has shown to be effective for S2ST learning. During the training, we follow the \u201cLNA-D\u201d policy introduced in [11], which does not \ufb01ne-tune all the parameters in the pre-trained mBART decoder but only the Layer- Norm and self-attention parameters. For the decoding of S2UT, we apply beam search with a beam size of 10. For NAR TTS models, the duration can be tuned with a speed factor, resulting in the duration control to the generation. In our experiments, apart from the TTS models we discussed in Sec. 2.1, we also apply different speed factors including 0.95, 1.0, and 1.05. 3.2.3. Experiments design The experiments generally include three folds: Single TTS systems: To compare the effect of different TTS sys- tems, we directly train the S2UT model with discrete units generated from a single TTS system. Simple combination of TTS systems: As discussed in Sec. 2.2, multiple TTS systems could potentially improve the S2ST, by nor- malizing the noise from unit sequences. To systematically inves- tigate the effects of different systems, we carry out experiments based on the modeling properties of different TTS systems, includ- ing AR versus NAR, different vocoders, different speed factors, and text2Mel versus text2wav. Multi-task framework for multiple TTS targets: We follow the design of the experiments in the simple combination of TTS systems, but change from the data combination into the multi-task way of training, as introduced in Sec. 2.2. We use the wav2vec 2.0-based ASR model to compute the CER mentioned in Sec. 2.2. Due to the requirements of large GPU memory to train models with more than three additional branches, we limit the number of TTS systems to less than four. Table 1. S2ST Performances on different TTS systems. The \u201cData ID\u201d column stands for the target speech units generated from the TTS system. The CER and BLEU are calculated as discussed in Sec. 3.2.4. The acoustic models (AM) and vocoders are introduced in Sec. 2.1. Data ID AM Vocoder CER(\u2193) BLEU(\u2191) A B C TT2 PWG HFG SWG 9.1 8.9 8.7 37.3 37.3 37.3 Avg. TT2 / 8.9 37.3 D E F FS2 PWG HFG SWG 9.4 8.3 8.7 37.5 37.6 37.5 Avg. FS2 / 8.8 37.5 G VITS 8.4 38.3 3.2.4. Evaluation metric The TTS quality is \ufb01rst evaluated by inputting the synthesized speech to the ASR model and calculating the character error rate (CER) between the ASR prediction and the reference text. For the ASR model, we employ the open-source ASR model that is trained over wav2vec 2.03. For evaluation of the translation quality, we \ufb01rst utilize the same ASR model to get the transcription of the S2ST system (i.e., predicted units + code-HFG vocoder) and then compute BLEU score with the reference text using SacreBLEU [37]. Noted that the reference text is also tokenized and converted to lowercase without punctuation for BLEU calculation. 3.3. Results and Discussion We conduct experiments with target speech discrete units generated from a single TTS system. The best system was obtained from syn- thesized speech using VITS (data G), and the results are shown in Table 1. We observe that there was no signi\ufb01cant effect on S2ST performance when different vocoders were applied to models A-C and models D-F. However, the difference in acoustic models could affect the S2ST results, with the best system using target speech syn- thesized from VITS and the worst from TT2. We also \ufb01nd that the CER in each TTS acoustic model roughly correlated with their S2ST performance. An interesting \ufb01nding from Table 1 is that systems with differ- ent vocoders achieved similar performances despite having different CERs from the ASR model. We assume that this is due to the Hu- BERT units helping to normalize the vocoder differences in speech synthesis. To verify this hypothesis, we conduct a Pearson coef\ufb01- cient analysis over HuBERT unit distribution on FS2 with different vocoders and found that the Pearson scores were all above 0.98. Furthermore, we \ufb01nd that VITS usually had a higher CER than TT2 and FS2 for spoken words (e.g., \u201dHMM\u201d, \u201dHUM\u201d). This could be due to the data domain mismatch between the read speech used for TTS training (i.e., LJSpeech) and ASR training (i.e., Librispeech) and the conversational speech used for S2ST training (i.e., Fisher). Table 2 presents the effect of different speed factors on NAR TTS systems. The results show that a smaller speed factor can im- prove the performance of the S2ST system. However, when mea- 3https://huggingface.co/facebook/ wav2vec2-large-960h-lv60-self Table 2. S2ST Performances on different speed factors. The Fast- speech2 (FS) model is combined with the HFG vocoder for TTS as default. The TTS models are introduced in Sec. 2.1. Data ID TTS Speed Factor CER(\u2193) BLEU(\u2191) H E I FS2 0.95 1.0 1.05 8.7 8.3 9.5 38.0 37.5 37.4 J G K VITS 0.95 1.0 1.05 8.6 8.4 8.5 38.7 38.3 38.3 Table 3. S2ST Performances on multiple TTS targets. We follow the categories listed in Sec. 3.2.3 to conduct experiments The models with (cid:51)in the"}, {"question": " What is used to compute BLEU score with the reference text for translation quality evaluation?", "answer": " SacreBLEU", "ref_chunk": "a dura- tion predictor that is jointly trained with unit-based HFG. The same of [11], the encoder is initialized from a Conformer-based wav2vec 2.0, while the decoder is initialized from the mBART decoder [33] released in [11]. To keep the consistency over different settings, we do not tune the hyper-parameters, but use the settings as [11] for different systems. TTS models: As mentioned in Sec. 2.1, we utilize three different TTS systems (i.e., TT2, FS2, and VITS) and three different vocoders (i.e., PWG, HFG, and SWG). To keep the reproducibility of the ex- periments, all the models are from public-available checkpoints in ESPnet-TTS [34, 35], an open-source framework for TTS. 3.2.2. Training and decoding For the training of S2UT models, we use the AdamW optimizer [36] with a learning rate of 0.0005. The scheduler is applied with a warmup policy that starts the learning rate from 1e-7 and reaches the maximum learning rate at 20k steps. We accumulate the gradients for every 120 steps to simulate a large batch size, which has shown to be effective for S2ST learning. During the training, we follow the \u201cLNA-D\u201d policy introduced in [11], which does not \ufb01ne-tune all the parameters in the pre-trained mBART decoder but only the Layer- Norm and self-attention parameters. For the decoding of S2UT, we apply beam search with a beam size of 10. For NAR TTS models, the duration can be tuned with a speed factor, resulting in the duration control to the generation. In our experiments, apart from the TTS models we discussed in Sec. 2.1, we also apply different speed factors including 0.95, 1.0, and 1.05. 3.2.3. Experiments design The experiments generally include three folds: Single TTS systems: To compare the effect of different TTS sys- tems, we directly train the S2UT model with discrete units generated from a single TTS system. Simple combination of TTS systems: As discussed in Sec. 2.2, multiple TTS systems could potentially improve the S2ST, by nor- malizing the noise from unit sequences. To systematically inves- tigate the effects of different systems, we carry out experiments based on the modeling properties of different TTS systems, includ- ing AR versus NAR, different vocoders, different speed factors, and text2Mel versus text2wav. Multi-task framework for multiple TTS targets: We follow the design of the experiments in the simple combination of TTS systems, but change from the data combination into the multi-task way of training, as introduced in Sec. 2.2. We use the wav2vec 2.0-based ASR model to compute the CER mentioned in Sec. 2.2. Due to the requirements of large GPU memory to train models with more than three additional branches, we limit the number of TTS systems to less than four. Table 1. S2ST Performances on different TTS systems. The \u201cData ID\u201d column stands for the target speech units generated from the TTS system. The CER and BLEU are calculated as discussed in Sec. 3.2.4. The acoustic models (AM) and vocoders are introduced in Sec. 2.1. Data ID AM Vocoder CER(\u2193) BLEU(\u2191) A B C TT2 PWG HFG SWG 9.1 8.9 8.7 37.3 37.3 37.3 Avg. TT2 / 8.9 37.3 D E F FS2 PWG HFG SWG 9.4 8.3 8.7 37.5 37.6 37.5 Avg. FS2 / 8.8 37.5 G VITS 8.4 38.3 3.2.4. Evaluation metric The TTS quality is \ufb01rst evaluated by inputting the synthesized speech to the ASR model and calculating the character error rate (CER) between the ASR prediction and the reference text. For the ASR model, we employ the open-source ASR model that is trained over wav2vec 2.03. For evaluation of the translation quality, we \ufb01rst utilize the same ASR model to get the transcription of the S2ST system (i.e., predicted units + code-HFG vocoder) and then compute BLEU score with the reference text using SacreBLEU [37]. Noted that the reference text is also tokenized and converted to lowercase without punctuation for BLEU calculation. 3.3. Results and Discussion We conduct experiments with target speech discrete units generated from a single TTS system. The best system was obtained from syn- thesized speech using VITS (data G), and the results are shown in Table 1. We observe that there was no signi\ufb01cant effect on S2ST performance when different vocoders were applied to models A-C and models D-F. However, the difference in acoustic models could affect the S2ST results, with the best system using target speech syn- thesized from VITS and the worst from TT2. We also \ufb01nd that the CER in each TTS acoustic model roughly correlated with their S2ST performance. An interesting \ufb01nding from Table 1 is that systems with differ- ent vocoders achieved similar performances despite having different CERs from the ASR model. We assume that this is due to the Hu- BERT units helping to normalize the vocoder differences in speech synthesis. To verify this hypothesis, we conduct a Pearson coef\ufb01- cient analysis over HuBERT unit distribution on FS2 with different vocoders and found that the Pearson scores were all above 0.98. Furthermore, we \ufb01nd that VITS usually had a higher CER than TT2 and FS2 for spoken words (e.g., \u201dHMM\u201d, \u201dHUM\u201d). This could be due to the data domain mismatch between the read speech used for TTS training (i.e., LJSpeech) and ASR training (i.e., Librispeech) and the conversational speech used for S2ST training (i.e., Fisher). Table 2 presents the effect of different speed factors on NAR TTS systems. The results show that a smaller speed factor can im- prove the performance of the S2ST system. However, when mea- 3https://huggingface.co/facebook/ wav2vec2-large-960h-lv60-self Table 2. S2ST Performances on different speed factors. The Fast- speech2 (FS) model is combined with the HFG vocoder for TTS as default. The TTS models are introduced in Sec. 2.1. Data ID TTS Speed Factor CER(\u2193) BLEU(\u2191) H E I FS2 0.95 1.0 1.05 8.7 8.3 9.5 38.0 37.5 37.4 J G K VITS 0.95 1.0 1.05 8.6 8.4 8.5 38.7 38.3 38.3 Table 3. S2ST Performances on multiple TTS targets. We follow the categories listed in Sec. 3.2.3 to conduct experiments The models with (cid:51)in the"}], "doc_text": "a dura- tion predictor that is jointly trained with unit-based HFG. The same of [11], the encoder is initialized from a Conformer-based wav2vec 2.0, while the decoder is initialized from the mBART decoder [33] released in [11]. To keep the consistency over different settings, we do not tune the hyper-parameters, but use the settings as [11] for different systems. TTS models: As mentioned in Sec. 2.1, we utilize three different TTS systems (i.e., TT2, FS2, and VITS) and three different vocoders (i.e., PWG, HFG, and SWG). To keep the reproducibility of the ex- periments, all the models are from public-available checkpoints in ESPnet-TTS [34, 35], an open-source framework for TTS. 3.2.2. Training and decoding For the training of S2UT models, we use the AdamW optimizer [36] with a learning rate of 0.0005. The scheduler is applied with a warmup policy that starts the learning rate from 1e-7 and reaches the maximum learning rate at 20k steps. We accumulate the gradients for every 120 steps to simulate a large batch size, which has shown to be effective for S2ST learning. During the training, we follow the \u201cLNA-D\u201d policy introduced in [11], which does not \ufb01ne-tune all the parameters in the pre-trained mBART decoder but only the Layer- Norm and self-attention parameters. For the decoding of S2UT, we apply beam search with a beam size of 10. For NAR TTS models, the duration can be tuned with a speed factor, resulting in the duration control to the generation. In our experiments, apart from the TTS models we discussed in Sec. 2.1, we also apply different speed factors including 0.95, 1.0, and 1.05. 3.2.3. Experiments design The experiments generally include three folds: Single TTS systems: To compare the effect of different TTS sys- tems, we directly train the S2UT model with discrete units generated from a single TTS system. Simple combination of TTS systems: As discussed in Sec. 2.2, multiple TTS systems could potentially improve the S2ST, by nor- malizing the noise from unit sequences. To systematically inves- tigate the effects of different systems, we carry out experiments based on the modeling properties of different TTS systems, includ- ing AR versus NAR, different vocoders, different speed factors, and text2Mel versus text2wav. Multi-task framework for multiple TTS targets: We follow the design of the experiments in the simple combination of TTS systems, but change from the data combination into the multi-task way of training, as introduced in Sec. 2.2. We use the wav2vec 2.0-based ASR model to compute the CER mentioned in Sec. 2.2. Due to the requirements of large GPU memory to train models with more than three additional branches, we limit the number of TTS systems to less than four. Table 1. S2ST Performances on different TTS systems. The \u201cData ID\u201d column stands for the target speech units generated from the TTS system. The CER and BLEU are calculated as discussed in Sec. 3.2.4. The acoustic models (AM) and vocoders are introduced in Sec. 2.1. Data ID AM Vocoder CER(\u2193) BLEU(\u2191) A B C TT2 PWG HFG SWG 9.1 8.9 8.7 37.3 37.3 37.3 Avg. TT2 / 8.9 37.3 D E F FS2 PWG HFG SWG 9.4 8.3 8.7 37.5 37.6 37.5 Avg. FS2 / 8.8 37.5 G VITS 8.4 38.3 3.2.4. Evaluation metric The TTS quality is \ufb01rst evaluated by inputting the synthesized speech to the ASR model and calculating the character error rate (CER) between the ASR prediction and the reference text. For the ASR model, we employ the open-source ASR model that is trained over wav2vec 2.03. For evaluation of the translation quality, we \ufb01rst utilize the same ASR model to get the transcription of the S2ST system (i.e., predicted units + code-HFG vocoder) and then compute BLEU score with the reference text using SacreBLEU [37]. Noted that the reference text is also tokenized and converted to lowercase without punctuation for BLEU calculation. 3.3. Results and Discussion We conduct experiments with target speech discrete units generated from a single TTS system. The best system was obtained from syn- thesized speech using VITS (data G), and the results are shown in Table 1. We observe that there was no signi\ufb01cant effect on S2ST performance when different vocoders were applied to models A-C and models D-F. However, the difference in acoustic models could affect the S2ST results, with the best system using target speech syn- thesized from VITS and the worst from TT2. We also \ufb01nd that the CER in each TTS acoustic model roughly correlated with their S2ST performance. An interesting \ufb01nding from Table 1 is that systems with differ- ent vocoders achieved similar performances despite having different CERs from the ASR model. We assume that this is due to the Hu- BERT units helping to normalize the vocoder differences in speech synthesis. To verify this hypothesis, we conduct a Pearson coef\ufb01- cient analysis over HuBERT unit distribution on FS2 with different vocoders and found that the Pearson scores were all above 0.98. Furthermore, we \ufb01nd that VITS usually had a higher CER than TT2 and FS2 for spoken words (e.g., \u201dHMM\u201d, \u201dHUM\u201d). This could be due to the data domain mismatch between the read speech used for TTS training (i.e., LJSpeech) and ASR training (i.e., Librispeech) and the conversational speech used for S2ST training (i.e., Fisher). Table 2 presents the effect of different speed factors on NAR TTS systems. The results show that a smaller speed factor can im- prove the performance of the S2ST system. However, when mea- 3https://huggingface.co/facebook/ wav2vec2-large-960h-lv60-self Table 2. S2ST Performances on different speed factors. The Fast- speech2 (FS) model is combined with the HFG vocoder for TTS as default. The TTS models are introduced in Sec. 2.1. Data ID TTS Speed Factor CER(\u2193) BLEU(\u2191) H E I FS2 0.95 1.0 1.05 8.7 8.3 9.5 38.0 37.5 37.4 J G K VITS 0.95 1.0 1.05 8.6 8.4 8.5 38.7 38.3 38.3 Table 3. S2ST Performances on multiple TTS targets. We follow the categories listed in Sec. 3.2.3 to conduct experiments The models with (cid:51)in the"}