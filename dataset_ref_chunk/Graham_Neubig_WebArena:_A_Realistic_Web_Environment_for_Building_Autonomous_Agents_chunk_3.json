{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_WebArena:_A_Realistic_Web_Environment_for_Building_Autonomous_Agents_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What type of environment does WebArena provide to users?", "answer": " WebArena provides the environment as dockers and provides scripts to reset the environment to a deterministic initial state.", "ref_chunk": "by including both popular projects with many issues and pull requests and smaller, personal projects. Details of all websites in WebArena can be found in Appendix A.1. We deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state (See Appendix A.2). 2.3 OBSERVATION SPACE We design the observation space to roughly mimic the web browser experience: a web page URL, the opened tabs , and the web page content of the focused tab. WebArena is the first web environment to consider multi-tab web-based tasks to promote tool usage, direct comparisons and references across tabs, and other functionalities. The multi-tab functionality offers a more authentic replication of human web browsing habits compared to maintaining everything in a single tab. We provide flexible configuration to render the page content in many modes: (see Figure 3 for an example): (1) the raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used in past work (Shi et al., 2017; Deng et al., 2023; Li et al., 2020); (2) a screenshot, a pixel-based representation that represents the current web page as an RGB array and (3) the accessibility tree of the web page.2 The accessibility tree is a subset of the DOM tree with elements that are relevant and useful for displaying the contents of a web page. Every element is represented as its role (e.g., a link), its text content, and its properties (e.g., whether it is focusable). Accessibility trees largely retain the structured information of a web page while being more compact than the DOM representation. We provide an option to limit the content to the contents within a viewport for all modes. This ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements. 2.4 ACTION SPACE Following previous work on navigation and operation in web and embodied environments (Shi et al., 2017; Liu et al., 2018), we design a compound action space that emulates the keyboard and mouse operations available on web pages. Figure 4 lists all the available actions categorized into three distinct groups. The first group includes element operations such as clicking, hovering, typing, and key combination pressing. The second comprises tab-related actions such as opening, closing, and switching between tabs. The third category consists of URL navigation actions, such as visiting a specific URL or navigating forward and backward in the browsing history. Building on these actions, WebArena provides agents with the flexibility to refer to elements for operation in different ways. An element can be selected by its on-screen coordinates, (x, y), or by a unique element ID that is prepended to each element. This ID is generated when traversing the Document Object Model (DOM) or accessibility tree. With element IDs, the element selection is transformed into an n-way classification problem, thereby eliminating any disambiguation efforts required from the agent or the underlying implementation. For example, issuing the action click 1https://gitlab.com/gitlab-org/gitlab 2https://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree 4 Under review [1582] clicks the button given the observation of [1582] Add to Cart. This flexible element selection allows WebArena to support agents designed in various ways (e.g., accepting input from different modalities) without compromising fair comparison metrics such as step count. User Role Simulation Users of the same website often have disparate experiences due to their distinct roles, permissions, and interaction histories. We emulate this scenario by generating unique user profiles on each platform. The details can be found in Appendix A.3. 3 BENCHMARK SUITE OF WEB-BASED TASKS We provide a benchmark with 812 test examples on grounding high-level natural language instructions to interactions in WebArena. Each example has a metric to evaluate the functional correctness of the task execution. In this section, we first formally define the task of controlling an autonomous agent through natural language. Then we introduce the annotation process of our benchmark. 3.1 INTENT COLLECTION We focus on curating realistic intents to carry out complex and creative tasks within WebArena. To start with, our annotators were guided to spend a few minutes exploring the websites to familiarize themselves with the websites\u2019 content and functionalities. As most of our websites are virtually identical to their open-web counterparts, despite having sampled data, most annotators can quickly comprehend the websites. Next, we instructed the annotators to formulate intents based on the following criteria: (1) The intent should be abstract and high-level, implying that the task cannot be fulfilled with merely one or two actions. As an example, instead of \u201cclick the science subreddit\u201d, we encouraged annotators to come up with something more complex like \u201cpost a greeting message on science subreddit\u201d, which involves performing multiple actions. (2) The intent should be creative. Common tasks such as account creation can be easily thought of. We encouraged the annotators to add constraints (e.g., \u201ccreate a Reddit account identical to my GitLab one\u201d) to make the intents more unique. (3) The intent should be formulated as a template by making replaceable elements as variables. The annotators were also responsible for developing several instantiations for each variable. For example, the intent \u201ccreate a Reddit account identical to my GitLab one\u201d can be converted into \u201ccreate a {{site1}} account identical to my {{site2}} one\u201d, with an instantiation like \u201c{site1: Reddit, site2: GitLab}\u201d and another like \u201c{site1: GitLab, site2: OneStopShopping}\u201d. Notably, tasks derived from the same template can have distinct execution traces. The similarity resides primarily in the high-level semantics rather than the specific implementation. We also provided a prompt for the annotators to use with ChatGPT3 for inspiration, that contains an overview of each website and instructs the model to describe potential tasks to be performed on these sites. Furthermore, we offered a curated list of examples for annotators to reference. Intent Analysis template is instantiated to 3.3 examples. The intent distribution is shown in Figure 6. In total, we curated 241 templates and 812 instantiated intents. On average, each Furthermore, we classify the intents into three primary categories with examples"}, {"question": " What is the purpose of designing the observation space in WebArena?", "answer": " The observation space in WebArena is designed to roughly mimic the web browser experience, including a web page URL, the opened tabs, and the web page content of the focused tab.", "ref_chunk": "by including both popular projects with many issues and pull requests and smaller, personal projects. Details of all websites in WebArena can be found in Appendix A.1. We deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state (See Appendix A.2). 2.3 OBSERVATION SPACE We design the observation space to roughly mimic the web browser experience: a web page URL, the opened tabs , and the web page content of the focused tab. WebArena is the first web environment to consider multi-tab web-based tasks to promote tool usage, direct comparisons and references across tabs, and other functionalities. The multi-tab functionality offers a more authentic replication of human web browsing habits compared to maintaining everything in a single tab. We provide flexible configuration to render the page content in many modes: (see Figure 3 for an example): (1) the raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used in past work (Shi et al., 2017; Deng et al., 2023; Li et al., 2020); (2) a screenshot, a pixel-based representation that represents the current web page as an RGB array and (3) the accessibility tree of the web page.2 The accessibility tree is a subset of the DOM tree with elements that are relevant and useful for displaying the contents of a web page. Every element is represented as its role (e.g., a link), its text content, and its properties (e.g., whether it is focusable). Accessibility trees largely retain the structured information of a web page while being more compact than the DOM representation. We provide an option to limit the content to the contents within a viewport for all modes. This ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements. 2.4 ACTION SPACE Following previous work on navigation and operation in web and embodied environments (Shi et al., 2017; Liu et al., 2018), we design a compound action space that emulates the keyboard and mouse operations available on web pages. Figure 4 lists all the available actions categorized into three distinct groups. The first group includes element operations such as clicking, hovering, typing, and key combination pressing. The second comprises tab-related actions such as opening, closing, and switching between tabs. The third category consists of URL navigation actions, such as visiting a specific URL or navigating forward and backward in the browsing history. Building on these actions, WebArena provides agents with the flexibility to refer to elements for operation in different ways. An element can be selected by its on-screen coordinates, (x, y), or by a unique element ID that is prepended to each element. This ID is generated when traversing the Document Object Model (DOM) or accessibility tree. With element IDs, the element selection is transformed into an n-way classification problem, thereby eliminating any disambiguation efforts required from the agent or the underlying implementation. For example, issuing the action click 1https://gitlab.com/gitlab-org/gitlab 2https://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree 4 Under review [1582] clicks the button given the observation of [1582] Add to Cart. This flexible element selection allows WebArena to support agents designed in various ways (e.g., accepting input from different modalities) without compromising fair comparison metrics such as step count. User Role Simulation Users of the same website often have disparate experiences due to their distinct roles, permissions, and interaction histories. We emulate this scenario by generating unique user profiles on each platform. The details can be found in Appendix A.3. 3 BENCHMARK SUITE OF WEB-BASED TASKS We provide a benchmark with 812 test examples on grounding high-level natural language instructions to interactions in WebArena. Each example has a metric to evaluate the functional correctness of the task execution. In this section, we first formally define the task of controlling an autonomous agent through natural language. Then we introduce the annotation process of our benchmark. 3.1 INTENT COLLECTION We focus on curating realistic intents to carry out complex and creative tasks within WebArena. To start with, our annotators were guided to spend a few minutes exploring the websites to familiarize themselves with the websites\u2019 content and functionalities. As most of our websites are virtually identical to their open-web counterparts, despite having sampled data, most annotators can quickly comprehend the websites. Next, we instructed the annotators to formulate intents based on the following criteria: (1) The intent should be abstract and high-level, implying that the task cannot be fulfilled with merely one or two actions. As an example, instead of \u201cclick the science subreddit\u201d, we encouraged annotators to come up with something more complex like \u201cpost a greeting message on science subreddit\u201d, which involves performing multiple actions. (2) The intent should be creative. Common tasks such as account creation can be easily thought of. We encouraged the annotators to add constraints (e.g., \u201ccreate a Reddit account identical to my GitLab one\u201d) to make the intents more unique. (3) The intent should be formulated as a template by making replaceable elements as variables. The annotators were also responsible for developing several instantiations for each variable. For example, the intent \u201ccreate a Reddit account identical to my GitLab one\u201d can be converted into \u201ccreate a {{site1}} account identical to my {{site2}} one\u201d, with an instantiation like \u201c{site1: Reddit, site2: GitLab}\u201d and another like \u201c{site1: GitLab, site2: OneStopShopping}\u201d. Notably, tasks derived from the same template can have distinct execution traces. The similarity resides primarily in the high-level semantics rather than the specific implementation. We also provided a prompt for the annotators to use with ChatGPT3 for inspiration, that contains an overview of each website and instructs the model to describe potential tasks to be performed on these sites. Furthermore, we offered a curated list of examples for annotators to reference. Intent Analysis template is instantiated to 3.3 examples. The intent distribution is shown in Figure 6. In total, we curated 241 templates and 812 instantiated intents. On average, each Furthermore, we classify the intents into three primary categories with examples"}, {"question": " What is the multi-tab functionality designed to achieve in WebArena?", "answer": " The multi-tab functionality in WebArena aims to offer a more authentic replication of human web browsing habits compared to maintaining everything in a single tab.", "ref_chunk": "by including both popular projects with many issues and pull requests and smaller, personal projects. Details of all websites in WebArena can be found in Appendix A.1. We deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state (See Appendix A.2). 2.3 OBSERVATION SPACE We design the observation space to roughly mimic the web browser experience: a web page URL, the opened tabs , and the web page content of the focused tab. WebArena is the first web environment to consider multi-tab web-based tasks to promote tool usage, direct comparisons and references across tabs, and other functionalities. The multi-tab functionality offers a more authentic replication of human web browsing habits compared to maintaining everything in a single tab. We provide flexible configuration to render the page content in many modes: (see Figure 3 for an example): (1) the raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used in past work (Shi et al., 2017; Deng et al., 2023; Li et al., 2020); (2) a screenshot, a pixel-based representation that represents the current web page as an RGB array and (3) the accessibility tree of the web page.2 The accessibility tree is a subset of the DOM tree with elements that are relevant and useful for displaying the contents of a web page. Every element is represented as its role (e.g., a link), its text content, and its properties (e.g., whether it is focusable). Accessibility trees largely retain the structured information of a web page while being more compact than the DOM representation. We provide an option to limit the content to the contents within a viewport for all modes. This ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements. 2.4 ACTION SPACE Following previous work on navigation and operation in web and embodied environments (Shi et al., 2017; Liu et al., 2018), we design a compound action space that emulates the keyboard and mouse operations available on web pages. Figure 4 lists all the available actions categorized into three distinct groups. The first group includes element operations such as clicking, hovering, typing, and key combination pressing. The second comprises tab-related actions such as opening, closing, and switching between tabs. The third category consists of URL navigation actions, such as visiting a specific URL or navigating forward and backward in the browsing history. Building on these actions, WebArena provides agents with the flexibility to refer to elements for operation in different ways. An element can be selected by its on-screen coordinates, (x, y), or by a unique element ID that is prepended to each element. This ID is generated when traversing the Document Object Model (DOM) or accessibility tree. With element IDs, the element selection is transformed into an n-way classification problem, thereby eliminating any disambiguation efforts required from the agent or the underlying implementation. For example, issuing the action click 1https://gitlab.com/gitlab-org/gitlab 2https://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree 4 Under review [1582] clicks the button given the observation of [1582] Add to Cart. This flexible element selection allows WebArena to support agents designed in various ways (e.g., accepting input from different modalities) without compromising fair comparison metrics such as step count. User Role Simulation Users of the same website often have disparate experiences due to their distinct roles, permissions, and interaction histories. We emulate this scenario by generating unique user profiles on each platform. The details can be found in Appendix A.3. 3 BENCHMARK SUITE OF WEB-BASED TASKS We provide a benchmark with 812 test examples on grounding high-level natural language instructions to interactions in WebArena. Each example has a metric to evaluate the functional correctness of the task execution. In this section, we first formally define the task of controlling an autonomous agent through natural language. Then we introduce the annotation process of our benchmark. 3.1 INTENT COLLECTION We focus on curating realistic intents to carry out complex and creative tasks within WebArena. To start with, our annotators were guided to spend a few minutes exploring the websites to familiarize themselves with the websites\u2019 content and functionalities. As most of our websites are virtually identical to their open-web counterparts, despite having sampled data, most annotators can quickly comprehend the websites. Next, we instructed the annotators to formulate intents based on the following criteria: (1) The intent should be abstract and high-level, implying that the task cannot be fulfilled with merely one or two actions. As an example, instead of \u201cclick the science subreddit\u201d, we encouraged annotators to come up with something more complex like \u201cpost a greeting message on science subreddit\u201d, which involves performing multiple actions. (2) The intent should be creative. Common tasks such as account creation can be easily thought of. We encouraged the annotators to add constraints (e.g., \u201ccreate a Reddit account identical to my GitLab one\u201d) to make the intents more unique. (3) The intent should be formulated as a template by making replaceable elements as variables. The annotators were also responsible for developing several instantiations for each variable. For example, the intent \u201ccreate a Reddit account identical to my GitLab one\u201d can be converted into \u201ccreate a {{site1}} account identical to my {{site2}} one\u201d, with an instantiation like \u201c{site1: Reddit, site2: GitLab}\u201d and another like \u201c{site1: GitLab, site2: OneStopShopping}\u201d. Notably, tasks derived from the same template can have distinct execution traces. The similarity resides primarily in the high-level semantics rather than the specific implementation. We also provided a prompt for the annotators to use with ChatGPT3 for inspiration, that contains an overview of each website and instructs the model to describe potential tasks to be performed on these sites. Furthermore, we offered a curated list of examples for annotators to reference. Intent Analysis template is instantiated to 3.3 examples. The intent distribution is shown in Figure 6. In total, we curated 241 templates and 812 instantiated intents. On average, each Furthermore, we classify the intents into three primary categories with examples"}, {"question": " What are the three modes in which the page content can be rendered in WebArena?", "answer": " The three modes in which the page content can be rendered in WebArena are the raw web page HTML, a screenshot, and the accessibility tree of the web page.", "ref_chunk": "by including both popular projects with many issues and pull requests and smaller, personal projects. Details of all websites in WebArena can be found in Appendix A.1. We deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state (See Appendix A.2). 2.3 OBSERVATION SPACE We design the observation space to roughly mimic the web browser experience: a web page URL, the opened tabs , and the web page content of the focused tab. WebArena is the first web environment to consider multi-tab web-based tasks to promote tool usage, direct comparisons and references across tabs, and other functionalities. The multi-tab functionality offers a more authentic replication of human web browsing habits compared to maintaining everything in a single tab. We provide flexible configuration to render the page content in many modes: (see Figure 3 for an example): (1) the raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used in past work (Shi et al., 2017; Deng et al., 2023; Li et al., 2020); (2) a screenshot, a pixel-based representation that represents the current web page as an RGB array and (3) the accessibility tree of the web page.2 The accessibility tree is a subset of the DOM tree with elements that are relevant and useful for displaying the contents of a web page. Every element is represented as its role (e.g., a link), its text content, and its properties (e.g., whether it is focusable). Accessibility trees largely retain the structured information of a web page while being more compact than the DOM representation. We provide an option to limit the content to the contents within a viewport for all modes. This ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements. 2.4 ACTION SPACE Following previous work on navigation and operation in web and embodied environments (Shi et al., 2017; Liu et al., 2018), we design a compound action space that emulates the keyboard and mouse operations available on web pages. Figure 4 lists all the available actions categorized into three distinct groups. The first group includes element operations such as clicking, hovering, typing, and key combination pressing. The second comprises tab-related actions such as opening, closing, and switching between tabs. The third category consists of URL navigation actions, such as visiting a specific URL or navigating forward and backward in the browsing history. Building on these actions, WebArena provides agents with the flexibility to refer to elements for operation in different ways. An element can be selected by its on-screen coordinates, (x, y), or by a unique element ID that is prepended to each element. This ID is generated when traversing the Document Object Model (DOM) or accessibility tree. With element IDs, the element selection is transformed into an n-way classification problem, thereby eliminating any disambiguation efforts required from the agent or the underlying implementation. For example, issuing the action click 1https://gitlab.com/gitlab-org/gitlab 2https://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree 4 Under review [1582] clicks the button given the observation of [1582] Add to Cart. This flexible element selection allows WebArena to support agents designed in various ways (e.g., accepting input from different modalities) without compromising fair comparison metrics such as step count. User Role Simulation Users of the same website often have disparate experiences due to their distinct roles, permissions, and interaction histories. We emulate this scenario by generating unique user profiles on each platform. The details can be found in Appendix A.3. 3 BENCHMARK SUITE OF WEB-BASED TASKS We provide a benchmark with 812 test examples on grounding high-level natural language instructions to interactions in WebArena. Each example has a metric to evaluate the functional correctness of the task execution. In this section, we first formally define the task of controlling an autonomous agent through natural language. Then we introduce the annotation process of our benchmark. 3.1 INTENT COLLECTION We focus on curating realistic intents to carry out complex and creative tasks within WebArena. To start with, our annotators were guided to spend a few minutes exploring the websites to familiarize themselves with the websites\u2019 content and functionalities. As most of our websites are virtually identical to their open-web counterparts, despite having sampled data, most annotators can quickly comprehend the websites. Next, we instructed the annotators to formulate intents based on the following criteria: (1) The intent should be abstract and high-level, implying that the task cannot be fulfilled with merely one or two actions. As an example, instead of \u201cclick the science subreddit\u201d, we encouraged annotators to come up with something more complex like \u201cpost a greeting message on science subreddit\u201d, which involves performing multiple actions. (2) The intent should be creative. Common tasks such as account creation can be easily thought of. We encouraged the annotators to add constraints (e.g., \u201ccreate a Reddit account identical to my GitLab one\u201d) to make the intents more unique. (3) The intent should be formulated as a template by making replaceable elements as variables. The annotators were also responsible for developing several instantiations for each variable. For example, the intent \u201ccreate a Reddit account identical to my GitLab one\u201d can be converted into \u201ccreate a {{site1}} account identical to my {{site2}} one\u201d, with an instantiation like \u201c{site1: Reddit, site2: GitLab}\u201d and another like \u201c{site1: GitLab, site2: OneStopShopping}\u201d. Notably, tasks derived from the same template can have distinct execution traces. The similarity resides primarily in the high-level semantics rather than the specific implementation. We also provided a prompt for the annotators to use with ChatGPT3 for inspiration, that contains an overview of each website and instructs the model to describe potential tasks to be performed on these sites. Furthermore, we offered a curated list of examples for annotators to reference. Intent Analysis template is instantiated to 3.3 examples. The intent distribution is shown in Figure 6. In total, we curated 241 templates and 812 instantiated intents. On average, each Furthermore, we classify the intents into three primary categories with examples"}, {"question": " How does WebArena ensure that the observation can be input into different models?", "answer": " WebArena ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements by providing an option to limit the content to the contents within a viewport for all modes.", "ref_chunk": "by including both popular projects with many issues and pull requests and smaller, personal projects. Details of all websites in WebArena can be found in Appendix A.1. We deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state (See Appendix A.2). 2.3 OBSERVATION SPACE We design the observation space to roughly mimic the web browser experience: a web page URL, the opened tabs , and the web page content of the focused tab. WebArena is the first web environment to consider multi-tab web-based tasks to promote tool usage, direct comparisons and references across tabs, and other functionalities. The multi-tab functionality offers a more authentic replication of human web browsing habits compared to maintaining everything in a single tab. We provide flexible configuration to render the page content in many modes: (see Figure 3 for an example): (1) the raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used in past work (Shi et al., 2017; Deng et al., 2023; Li et al., 2020); (2) a screenshot, a pixel-based representation that represents the current web page as an RGB array and (3) the accessibility tree of the web page.2 The accessibility tree is a subset of the DOM tree with elements that are relevant and useful for displaying the contents of a web page. Every element is represented as its role (e.g., a link), its text content, and its properties (e.g., whether it is focusable). Accessibility trees largely retain the structured information of a web page while being more compact than the DOM representation. We provide an option to limit the content to the contents within a viewport for all modes. This ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements. 2.4 ACTION SPACE Following previous work on navigation and operation in web and embodied environments (Shi et al., 2017; Liu et al., 2018), we design a compound action space that emulates the keyboard and mouse operations available on web pages. Figure 4 lists all the available actions categorized into three distinct groups. The first group includes element operations such as clicking, hovering, typing, and key combination pressing. The second comprises tab-related actions such as opening, closing, and switching between tabs. The third category consists of URL navigation actions, such as visiting a specific URL or navigating forward and backward in the browsing history. Building on these actions, WebArena provides agents with the flexibility to refer to elements for operation in different ways. An element can be selected by its on-screen coordinates, (x, y), or by a unique element ID that is prepended to each element. This ID is generated when traversing the Document Object Model (DOM) or accessibility tree. With element IDs, the element selection is transformed into an n-way classification problem, thereby eliminating any disambiguation efforts required from the agent or the underlying implementation. For example, issuing the action click 1https://gitlab.com/gitlab-org/gitlab 2https://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree 4 Under review [1582] clicks the button given the observation of [1582] Add to Cart. This flexible element selection allows WebArena to support agents designed in various ways (e.g., accepting input from different modalities) without compromising fair comparison metrics such as step count. User Role Simulation Users of the same website often have disparate experiences due to their distinct roles, permissions, and interaction histories. We emulate this scenario by generating unique user profiles on each platform. The details can be found in Appendix A.3. 3 BENCHMARK SUITE OF WEB-BASED TASKS We provide a benchmark with 812 test examples on grounding high-level natural language instructions to interactions in WebArena. Each example has a metric to evaluate the functional correctness of the task execution. In this section, we first formally define the task of controlling an autonomous agent through natural language. Then we introduce the annotation process of our benchmark. 3.1 INTENT COLLECTION We focus on curating realistic intents to carry out complex and creative tasks within WebArena. To start with, our annotators were guided to spend a few minutes exploring the websites to familiarize themselves with the websites\u2019 content and functionalities. As most of our websites are virtually identical to their open-web counterparts, despite having sampled data, most annotators can quickly comprehend the websites. Next, we instructed the annotators to formulate intents based on the following criteria: (1) The intent should be abstract and high-level, implying that the task cannot be fulfilled with merely one or two actions. As an example, instead of \u201cclick the science subreddit\u201d, we encouraged annotators to come up with something more complex like \u201cpost a greeting message on science subreddit\u201d, which involves performing multiple actions. (2) The intent should be creative. Common tasks such as account creation can be easily thought of. We encouraged the annotators to add constraints (e.g., \u201ccreate a Reddit account identical to my GitLab one\u201d) to make the intents more unique. (3) The intent should be formulated as a template by making replaceable elements as variables. The annotators were also responsible for developing several instantiations for each variable. For example, the intent \u201ccreate a Reddit account identical to my GitLab one\u201d can be converted into \u201ccreate a {{site1}} account identical to my {{site2}} one\u201d, with an instantiation like \u201c{site1: Reddit, site2: GitLab}\u201d and another like \u201c{site1: GitLab, site2: OneStopShopping}\u201d. Notably, tasks derived from the same template can have distinct execution traces. The similarity resides primarily in the high-level semantics rather than the specific implementation. We also provided a prompt for the annotators to use with ChatGPT3 for inspiration, that contains an overview of each website and instructs the model to describe potential tasks to be performed on these sites. Furthermore, we offered a curated list of examples for annotators to reference. Intent Analysis template is instantiated to 3.3 examples. The intent distribution is shown in Figure 6. In total, we curated 241 templates and 812 instantiated intents. On average, each Furthermore, we classify the intents into three primary categories with examples"}, {"question": " What are the three distinct groups into which the available actions in WebArena are categorized?", "answer": " The available actions in WebArena are categorized into three distinct groups: element operations, tab-related actions, and URL navigation actions.", "ref_chunk": "by including both popular projects with many issues and pull requests and smaller, personal projects. Details of all websites in WebArena can be found in Appendix A.1. We deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state (See Appendix A.2). 2.3 OBSERVATION SPACE We design the observation space to roughly mimic the web browser experience: a web page URL, the opened tabs , and the web page content of the focused tab. WebArena is the first web environment to consider multi-tab web-based tasks to promote tool usage, direct comparisons and references across tabs, and other functionalities. The multi-tab functionality offers a more authentic replication of human web browsing habits compared to maintaining everything in a single tab. We provide flexible configuration to render the page content in many modes: (see Figure 3 for an example): (1) the raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used in past work (Shi et al., 2017; Deng et al., 2023; Li et al., 2020); (2) a screenshot, a pixel-based representation that represents the current web page as an RGB array and (3) the accessibility tree of the web page.2 The accessibility tree is a subset of the DOM tree with elements that are relevant and useful for displaying the contents of a web page. Every element is represented as its role (e.g., a link), its text content, and its properties (e.g., whether it is focusable). Accessibility trees largely retain the structured information of a web page while being more compact than the DOM representation. We provide an option to limit the content to the contents within a viewport for all modes. This ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements. 2.4 ACTION SPACE Following previous work on navigation and operation in web and embodied environments (Shi et al., 2017; Liu et al., 2018), we design a compound action space that emulates the keyboard and mouse operations available on web pages. Figure 4 lists all the available actions categorized into three distinct groups. The first group includes element operations such as clicking, hovering, typing, and key combination pressing. The second comprises tab-related actions such as opening, closing, and switching between tabs. The third category consists of URL navigation actions, such as visiting a specific URL or navigating forward and backward in the browsing history. Building on these actions, WebArena provides agents with the flexibility to refer to elements for operation in different ways. An element can be selected by its on-screen coordinates, (x, y), or by a unique element ID that is prepended to each element. This ID is generated when traversing the Document Object Model (DOM) or accessibility tree. With element IDs, the element selection is transformed into an n-way classification problem, thereby eliminating any disambiguation efforts required from the agent or the underlying implementation. For example, issuing the action click 1https://gitlab.com/gitlab-org/gitlab 2https://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree 4 Under review [1582] clicks the button given the observation of [1582] Add to Cart. This flexible element selection allows WebArena to support agents designed in various ways (e.g., accepting input from different modalities) without compromising fair comparison metrics such as step count. User Role Simulation Users of the same website often have disparate experiences due to their distinct roles, permissions, and interaction histories. We emulate this scenario by generating unique user profiles on each platform. The details can be found in Appendix A.3. 3 BENCHMARK SUITE OF WEB-BASED TASKS We provide a benchmark with 812 test examples on grounding high-level natural language instructions to interactions in WebArena. Each example has a metric to evaluate the functional correctness of the task execution. In this section, we first formally define the task of controlling an autonomous agent through natural language. Then we introduce the annotation process of our benchmark. 3.1 INTENT COLLECTION We focus on curating realistic intents to carry out complex and creative tasks within WebArena. To start with, our annotators were guided to spend a few minutes exploring the websites to familiarize themselves with the websites\u2019 content and functionalities. As most of our websites are virtually identical to their open-web counterparts, despite having sampled data, most annotators can quickly comprehend the websites. Next, we instructed the annotators to formulate intents based on the following criteria: (1) The intent should be abstract and high-level, implying that the task cannot be fulfilled with merely one or two actions. As an example, instead of \u201cclick the science subreddit\u201d, we encouraged annotators to come up with something more complex like \u201cpost a greeting message on science subreddit\u201d, which involves performing multiple actions. (2) The intent should be creative. Common tasks such as account creation can be easily thought of. We encouraged the annotators to add constraints (e.g., \u201ccreate a Reddit account identical to my GitLab one\u201d) to make the intents more unique. (3) The intent should be formulated as a template by making replaceable elements as variables. The annotators were also responsible for developing several instantiations for each variable. For example, the intent \u201ccreate a Reddit account identical to my GitLab one\u201d can be converted into \u201ccreate a {{site1}} account identical to my {{site2}} one\u201d, with an instantiation like \u201c{site1: Reddit, site2: GitLab}\u201d and another like \u201c{site1: GitLab, site2: OneStopShopping}\u201d. Notably, tasks derived from the same template can have distinct execution traces. The similarity resides primarily in the high-level semantics rather than the specific implementation. We also provided a prompt for the annotators to use with ChatGPT3 for inspiration, that contains an overview of each website and instructs the model to describe potential tasks to be performed on these sites. Furthermore, we offered a curated list of examples for annotators to reference. Intent Analysis template is instantiated to 3.3 examples. The intent distribution is shown in Figure 6. In total, we curated 241 templates and 812 instantiated intents. On average, each Furthermore, we classify the intents into three primary categories with examples"}, {"question": " How does WebArena support agents designed in various ways in terms of element selection?", "answer": " WebArena supports agents designed in various ways by allowing element selection either by on-screen coordinates or by a unique element ID that is generated when traversing the Document Object Model (DOM) or accessibility tree.", "ref_chunk": "by including both popular projects with many issues and pull requests and smaller, personal projects. Details of all websites in WebArena can be found in Appendix A.1. We deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state (See Appendix A.2). 2.3 OBSERVATION SPACE We design the observation space to roughly mimic the web browser experience: a web page URL, the opened tabs , and the web page content of the focused tab. WebArena is the first web environment to consider multi-tab web-based tasks to promote tool usage, direct comparisons and references across tabs, and other functionalities. The multi-tab functionality offers a more authentic replication of human web browsing habits compared to maintaining everything in a single tab. We provide flexible configuration to render the page content in many modes: (see Figure 3 for an example): (1) the raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used in past work (Shi et al., 2017; Deng et al., 2023; Li et al., 2020); (2) a screenshot, a pixel-based representation that represents the current web page as an RGB array and (3) the accessibility tree of the web page.2 The accessibility tree is a subset of the DOM tree with elements that are relevant and useful for displaying the contents of a web page. Every element is represented as its role (e.g., a link), its text content, and its properties (e.g., whether it is focusable). Accessibility trees largely retain the structured information of a web page while being more compact than the DOM representation. We provide an option to limit the content to the contents within a viewport for all modes. This ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements. 2.4 ACTION SPACE Following previous work on navigation and operation in web and embodied environments (Shi et al., 2017; Liu et al., 2018), we design a compound action space that emulates the keyboard and mouse operations available on web pages. Figure 4 lists all the available actions categorized into three distinct groups. The first group includes element operations such as clicking, hovering, typing, and key combination pressing. The second comprises tab-related actions such as opening, closing, and switching between tabs. The third category consists of URL navigation actions, such as visiting a specific URL or navigating forward and backward in the browsing history. Building on these actions, WebArena provides agents with the flexibility to refer to elements for operation in different ways. An element can be selected by its on-screen coordinates, (x, y), or by a unique element ID that is prepended to each element. This ID is generated when traversing the Document Object Model (DOM) or accessibility tree. With element IDs, the element selection is transformed into an n-way classification problem, thereby eliminating any disambiguation efforts required from the agent or the underlying implementation. For example, issuing the action click 1https://gitlab.com/gitlab-org/gitlab 2https://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree 4 Under review [1582] clicks the button given the observation of [1582] Add to Cart. This flexible element selection allows WebArena to support agents designed in various ways (e.g., accepting input from different modalities) without compromising fair comparison metrics such as step count. User Role Simulation Users of the same website often have disparate experiences due to their distinct roles, permissions, and interaction histories. We emulate this scenario by generating unique user profiles on each platform. The details can be found in Appendix A.3. 3 BENCHMARK SUITE OF WEB-BASED TASKS We provide a benchmark with 812 test examples on grounding high-level natural language instructions to interactions in WebArena. Each example has a metric to evaluate the functional correctness of the task execution. In this section, we first formally define the task of controlling an autonomous agent through natural language. Then we introduce the annotation process of our benchmark. 3.1 INTENT COLLECTION We focus on curating realistic intents to carry out complex and creative tasks within WebArena. To start with, our annotators were guided to spend a few minutes exploring the websites to familiarize themselves with the websites\u2019 content and functionalities. As most of our websites are virtually identical to their open-web counterparts, despite having sampled data, most annotators can quickly comprehend the websites. Next, we instructed the annotators to formulate intents based on the following criteria: (1) The intent should be abstract and high-level, implying that the task cannot be fulfilled with merely one or two actions. As an example, instead of \u201cclick the science subreddit\u201d, we encouraged annotators to come up with something more complex like \u201cpost a greeting message on science subreddit\u201d, which involves performing multiple actions. (2) The intent should be creative. Common tasks such as account creation can be easily thought of. We encouraged the annotators to add constraints (e.g., \u201ccreate a Reddit account identical to my GitLab one\u201d) to make the intents more unique. (3) The intent should be formulated as a template by making replaceable elements as variables. The annotators were also responsible for developing several instantiations for each variable. For example, the intent \u201ccreate a Reddit account identical to my GitLab one\u201d can be converted into \u201ccreate a {{site1}} account identical to my {{site2}} one\u201d, with an instantiation like \u201c{site1: Reddit, site2: GitLab}\u201d and another like \u201c{site1: GitLab, site2: OneStopShopping}\u201d. Notably, tasks derived from the same template can have distinct execution traces. The similarity resides primarily in the high-level semantics rather than the specific implementation. We also provided a prompt for the annotators to use with ChatGPT3 for inspiration, that contains an overview of each website and instructs the model to describe potential tasks to be performed on these sites. Furthermore, we offered a curated list of examples for annotators to reference. Intent Analysis template is instantiated to 3.3 examples. The intent distribution is shown in Figure 6. In total, we curated 241 templates and 812 instantiated intents. On average, each Furthermore, we classify the intents into three primary categories with examples"}, {"question": " Why does WebArena generate unique user profiles on each platform?", "answer": " WebArena generates unique user profiles on each platform to emulate the scenario where users of the same website often have disparate experiences due to their distinct roles, permissions, and interaction histories.", "ref_chunk": "by including both popular projects with many issues and pull requests and smaller, personal projects. Details of all websites in WebArena can be found in Appendix A.1. We deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state (See Appendix A.2). 2.3 OBSERVATION SPACE We design the observation space to roughly mimic the web browser experience: a web page URL, the opened tabs , and the web page content of the focused tab. WebArena is the first web environment to consider multi-tab web-based tasks to promote tool usage, direct comparisons and references across tabs, and other functionalities. The multi-tab functionality offers a more authentic replication of human web browsing habits compared to maintaining everything in a single tab. We provide flexible configuration to render the page content in many modes: (see Figure 3 for an example): (1) the raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used in past work (Shi et al., 2017; Deng et al., 2023; Li et al., 2020); (2) a screenshot, a pixel-based representation that represents the current web page as an RGB array and (3) the accessibility tree of the web page.2 The accessibility tree is a subset of the DOM tree with elements that are relevant and useful for displaying the contents of a web page. Every element is represented as its role (e.g., a link), its text content, and its properties (e.g., whether it is focusable). Accessibility trees largely retain the structured information of a web page while being more compact than the DOM representation. We provide an option to limit the content to the contents within a viewport for all modes. This ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements. 2.4 ACTION SPACE Following previous work on navigation and operation in web and embodied environments (Shi et al., 2017; Liu et al., 2018), we design a compound action space that emulates the keyboard and mouse operations available on web pages. Figure 4 lists all the available actions categorized into three distinct groups. The first group includes element operations such as clicking, hovering, typing, and key combination pressing. The second comprises tab-related actions such as opening, closing, and switching between tabs. The third category consists of URL navigation actions, such as visiting a specific URL or navigating forward and backward in the browsing history. Building on these actions, WebArena provides agents with the flexibility to refer to elements for operation in different ways. An element can be selected by its on-screen coordinates, (x, y), or by a unique element ID that is prepended to each element. This ID is generated when traversing the Document Object Model (DOM) or accessibility tree. With element IDs, the element selection is transformed into an n-way classification problem, thereby eliminating any disambiguation efforts required from the agent or the underlying implementation. For example, issuing the action click 1https://gitlab.com/gitlab-org/gitlab 2https://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree 4 Under review [1582] clicks the button given the observation of [1582] Add to Cart. This flexible element selection allows WebArena to support agents designed in various ways (e.g., accepting input from different modalities) without compromising fair comparison metrics such as step count. User Role Simulation Users of the same website often have disparate experiences due to their distinct roles, permissions, and interaction histories. We emulate this scenario by generating unique user profiles on each platform. The details can be found in Appendix A.3. 3 BENCHMARK SUITE OF WEB-BASED TASKS We provide a benchmark with 812 test examples on grounding high-level natural language instructions to interactions in WebArena. Each example has a metric to evaluate the functional correctness of the task execution. In this section, we first formally define the task of controlling an autonomous agent through natural language. Then we introduce the annotation process of our benchmark. 3.1 INTENT COLLECTION We focus on curating realistic intents to carry out complex and creative tasks within WebArena. To start with, our annotators were guided to spend a few minutes exploring the websites to familiarize themselves with the websites\u2019 content and functionalities. As most of our websites are virtually identical to their open-web counterparts, despite having sampled data, most annotators can quickly comprehend the websites. Next, we instructed the annotators to formulate intents based on the following criteria: (1) The intent should be abstract and high-level, implying that the task cannot be fulfilled with merely one or two actions. As an example, instead of \u201cclick the science subreddit\u201d, we encouraged annotators to come up with something more complex like \u201cpost a greeting message on science subreddit\u201d, which involves performing multiple actions. (2) The intent should be creative. Common tasks such as account creation can be easily thought of. We encouraged the annotators to add constraints (e.g., \u201ccreate a Reddit account identical to my GitLab one\u201d) to make the intents more unique. (3) The intent should be formulated as a template by making replaceable elements as variables. The annotators were also responsible for developing several instantiations for each variable. For example, the intent \u201ccreate a Reddit account identical to my GitLab one\u201d can be converted into \u201ccreate a {{site1}} account identical to my {{site2}} one\u201d, with an instantiation like \u201c{site1: Reddit, site2: GitLab}\u201d and another like \u201c{site1: GitLab, site2: OneStopShopping}\u201d. Notably, tasks derived from the same template can have distinct execution traces. The similarity resides primarily in the high-level semantics rather than the specific implementation. We also provided a prompt for the annotators to use with ChatGPT3 for inspiration, that contains an overview of each website and instructs the model to describe potential tasks to be performed on these sites. Furthermore, we offered a curated list of examples for annotators to reference. Intent Analysis template is instantiated to 3.3 examples. The intent distribution is shown in Figure 6. In total, we curated 241 templates and 812 instantiated intents. On average, each Furthermore, we classify the intents into three primary categories with examples"}, {"question": " How many test examples are provided in the benchmark suite of web-based tasks in WebArena?", "answer": " There are 812 test examples provided in the benchmark suite of web-based tasks in WebArena.", "ref_chunk": "by including both popular projects with many issues and pull requests and smaller, personal projects. Details of all websites in WebArena can be found in Appendix A.1. We deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state (See Appendix A.2). 2.3 OBSERVATION SPACE We design the observation space to roughly mimic the web browser experience: a web page URL, the opened tabs , and the web page content of the focused tab. WebArena is the first web environment to consider multi-tab web-based tasks to promote tool usage, direct comparisons and references across tabs, and other functionalities. The multi-tab functionality offers a more authentic replication of human web browsing habits compared to maintaining everything in a single tab. We provide flexible configuration to render the page content in many modes: (see Figure 3 for an example): (1) the raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used in past work (Shi et al., 2017; Deng et al., 2023; Li et al., 2020); (2) a screenshot, a pixel-based representation that represents the current web page as an RGB array and (3) the accessibility tree of the web page.2 The accessibility tree is a subset of the DOM tree with elements that are relevant and useful for displaying the contents of a web page. Every element is represented as its role (e.g., a link), its text content, and its properties (e.g., whether it is focusable). Accessibility trees largely retain the structured information of a web page while being more compact than the DOM representation. We provide an option to limit the content to the contents within a viewport for all modes. This ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements. 2.4 ACTION SPACE Following previous work on navigation and operation in web and embodied environments (Shi et al., 2017; Liu et al., 2018), we design a compound action space that emulates the keyboard and mouse operations available on web pages. Figure 4 lists all the available actions categorized into three distinct groups. The first group includes element operations such as clicking, hovering, typing, and key combination pressing. The second comprises tab-related actions such as opening, closing, and switching between tabs. The third category consists of URL navigation actions, such as visiting a specific URL or navigating forward and backward in the browsing history. Building on these actions, WebArena provides agents with the flexibility to refer to elements for operation in different ways. An element can be selected by its on-screen coordinates, (x, y), or by a unique element ID that is prepended to each element. This ID is generated when traversing the Document Object Model (DOM) or accessibility tree. With element IDs, the element selection is transformed into an n-way classification problem, thereby eliminating any disambiguation efforts required from the agent or the underlying implementation. For example, issuing the action click 1https://gitlab.com/gitlab-org/gitlab 2https://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree 4 Under review [1582] clicks the button given the observation of [1582] Add to Cart. This flexible element selection allows WebArena to support agents designed in various ways (e.g., accepting input from different modalities) without compromising fair comparison metrics such as step count. User Role Simulation Users of the same website often have disparate experiences due to their distinct roles, permissions, and interaction histories. We emulate this scenario by generating unique user profiles on each platform. The details can be found in Appendix A.3. 3 BENCHMARK SUITE OF WEB-BASED TASKS We provide a benchmark with 812 test examples on grounding high-level natural language instructions to interactions in WebArena. Each example has a metric to evaluate the functional correctness of the task execution. In this section, we first formally define the task of controlling an autonomous agent through natural language. Then we introduce the annotation process of our benchmark. 3.1 INTENT COLLECTION We focus on curating realistic intents to carry out complex and creative tasks within WebArena. To start with, our annotators were guided to spend a few minutes exploring the websites to familiarize themselves with the websites\u2019 content and functionalities. As most of our websites are virtually identical to their open-web counterparts, despite having sampled data, most annotators can quickly comprehend the websites. Next, we instructed the annotators to formulate intents based on the following criteria: (1) The intent should be abstract and high-level, implying that the task cannot be fulfilled with merely one or two actions. As an example, instead of \u201cclick the science subreddit\u201d, we encouraged annotators to come up with something more complex like \u201cpost a greeting message on science subreddit\u201d, which involves performing multiple actions. (2) The intent should be creative. Common tasks such as account creation can be easily thought of. We encouraged the annotators to add constraints (e.g., \u201ccreate a Reddit account identical to my GitLab one\u201d) to make the intents more unique. (3) The intent should be formulated as a template by making replaceable elements as variables. The annotators were also responsible for developing several instantiations for each variable. For example, the intent \u201ccreate a Reddit account identical to my GitLab one\u201d can be converted into \u201ccreate a {{site1}} account identical to my {{site2}} one\u201d, with an instantiation like \u201c{site1: Reddit, site2: GitLab}\u201d and another like \u201c{site1: GitLab, site2: OneStopShopping}\u201d. Notably, tasks derived from the same template can have distinct execution traces. The similarity resides primarily in the high-level semantics rather than the specific implementation. We also provided a prompt for the annotators to use with ChatGPT3 for inspiration, that contains an overview of each website and instructs the model to describe potential tasks to be performed on these sites. Furthermore, we offered a curated list of examples for annotators to reference. Intent Analysis template is instantiated to 3.3 examples. The intent distribution is shown in Figure 6. In total, we curated 241 templates and 812 instantiated intents. On average, each Furthermore, we classify the intents into three primary categories with examples"}, {"question": " What criteria were annotators guided by in formulating intents for the benchmark suite in WebArena?", "answer": " Annotators were guided to formulate abstract and high-level intents, ensure creativity in the intents, and formulate the intent as a template with replaceable elements as variables.", "ref_chunk": "by including both popular projects with many issues and pull requests and smaller, personal projects. Details of all websites in WebArena can be found in Appendix A.1. We deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state (See Appendix A.2). 2.3 OBSERVATION SPACE We design the observation space to roughly mimic the web browser experience: a web page URL, the opened tabs , and the web page content of the focused tab. WebArena is the first web environment to consider multi-tab web-based tasks to promote tool usage, direct comparisons and references across tabs, and other functionalities. The multi-tab functionality offers a more authentic replication of human web browsing habits compared to maintaining everything in a single tab. We provide flexible configuration to render the page content in many modes: (see Figure 3 for an example): (1) the raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used in past work (Shi et al., 2017; Deng et al., 2023; Li et al., 2020); (2) a screenshot, a pixel-based representation that represents the current web page as an RGB array and (3) the accessibility tree of the web page.2 The accessibility tree is a subset of the DOM tree with elements that are relevant and useful for displaying the contents of a web page. Every element is represented as its role (e.g., a link), its text content, and its properties (e.g., whether it is focusable). Accessibility trees largely retain the structured information of a web page while being more compact than the DOM representation. We provide an option to limit the content to the contents within a viewport for all modes. This ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements. 2.4 ACTION SPACE Following previous work on navigation and operation in web and embodied environments (Shi et al., 2017; Liu et al., 2018), we design a compound action space that emulates the keyboard and mouse operations available on web pages. Figure 4 lists all the available actions categorized into three distinct groups. The first group includes element operations such as clicking, hovering, typing, and key combination pressing. The second comprises tab-related actions such as opening, closing, and switching between tabs. The third category consists of URL navigation actions, such as visiting a specific URL or navigating forward and backward in the browsing history. Building on these actions, WebArena provides agents with the flexibility to refer to elements for operation in different ways. An element can be selected by its on-screen coordinates, (x, y), or by a unique element ID that is prepended to each element. This ID is generated when traversing the Document Object Model (DOM) or accessibility tree. With element IDs, the element selection is transformed into an n-way classification problem, thereby eliminating any disambiguation efforts required from the agent or the underlying implementation. For example, issuing the action click 1https://gitlab.com/gitlab-org/gitlab 2https://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree 4 Under review [1582] clicks the button given the observation of [1582] Add to Cart. This flexible element selection allows WebArena to support agents designed in various ways (e.g., accepting input from different modalities) without compromising fair comparison metrics such as step count. User Role Simulation Users of the same website often have disparate experiences due to their distinct roles, permissions, and interaction histories. We emulate this scenario by generating unique user profiles on each platform. The details can be found in Appendix A.3. 3 BENCHMARK SUITE OF WEB-BASED TASKS We provide a benchmark with 812 test examples on grounding high-level natural language instructions to interactions in WebArena. Each example has a metric to evaluate the functional correctness of the task execution. In this section, we first formally define the task of controlling an autonomous agent through natural language. Then we introduce the annotation process of our benchmark. 3.1 INTENT COLLECTION We focus on curating realistic intents to carry out complex and creative tasks within WebArena. To start with, our annotators were guided to spend a few minutes exploring the websites to familiarize themselves with the websites\u2019 content and functionalities. As most of our websites are virtually identical to their open-web counterparts, despite having sampled data, most annotators can quickly comprehend the websites. Next, we instructed the annotators to formulate intents based on the following criteria: (1) The intent should be abstract and high-level, implying that the task cannot be fulfilled with merely one or two actions. As an example, instead of \u201cclick the science subreddit\u201d, we encouraged annotators to come up with something more complex like \u201cpost a greeting message on science subreddit\u201d, which involves performing multiple actions. (2) The intent should be creative. Common tasks such as account creation can be easily thought of. We encouraged the annotators to add constraints (e.g., \u201ccreate a Reddit account identical to my GitLab one\u201d) to make the intents more unique. (3) The intent should be formulated as a template by making replaceable elements as variables. The annotators were also responsible for developing several instantiations for each variable. For example, the intent \u201ccreate a Reddit account identical to my GitLab one\u201d can be converted into \u201ccreate a {{site1}} account identical to my {{site2}} one\u201d, with an instantiation like \u201c{site1: Reddit, site2: GitLab}\u201d and another like \u201c{site1: GitLab, site2: OneStopShopping}\u201d. Notably, tasks derived from the same template can have distinct execution traces. The similarity resides primarily in the high-level semantics rather than the specific implementation. We also provided a prompt for the annotators to use with ChatGPT3 for inspiration, that contains an overview of each website and instructs the model to describe potential tasks to be performed on these sites. Furthermore, we offered a curated list of examples for annotators to reference. Intent Analysis template is instantiated to 3.3 examples. The intent distribution is shown in Figure 6. In total, we curated 241 templates and 812 instantiated intents. On average, each Furthermore, we classify the intents into three primary categories with examples"}], "doc_text": "by including both popular projects with many issues and pull requests and smaller, personal projects. Details of all websites in WebArena can be found in Appendix A.1. We deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state (See Appendix A.2). 2.3 OBSERVATION SPACE We design the observation space to roughly mimic the web browser experience: a web page URL, the opened tabs , and the web page content of the focused tab. WebArena is the first web environment to consider multi-tab web-based tasks to promote tool usage, direct comparisons and references across tabs, and other functionalities. The multi-tab functionality offers a more authentic replication of human web browsing habits compared to maintaining everything in a single tab. We provide flexible configuration to render the page content in many modes: (see Figure 3 for an example): (1) the raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used in past work (Shi et al., 2017; Deng et al., 2023; Li et al., 2020); (2) a screenshot, a pixel-based representation that represents the current web page as an RGB array and (3) the accessibility tree of the web page.2 The accessibility tree is a subset of the DOM tree with elements that are relevant and useful for displaying the contents of a web page. Every element is represented as its role (e.g., a link), its text content, and its properties (e.g., whether it is focusable). Accessibility trees largely retain the structured information of a web page while being more compact than the DOM representation. We provide an option to limit the content to the contents within a viewport for all modes. This ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements. 2.4 ACTION SPACE Following previous work on navigation and operation in web and embodied environments (Shi et al., 2017; Liu et al., 2018), we design a compound action space that emulates the keyboard and mouse operations available on web pages. Figure 4 lists all the available actions categorized into three distinct groups. The first group includes element operations such as clicking, hovering, typing, and key combination pressing. The second comprises tab-related actions such as opening, closing, and switching between tabs. The third category consists of URL navigation actions, such as visiting a specific URL or navigating forward and backward in the browsing history. Building on these actions, WebArena provides agents with the flexibility to refer to elements for operation in different ways. An element can be selected by its on-screen coordinates, (x, y), or by a unique element ID that is prepended to each element. This ID is generated when traversing the Document Object Model (DOM) or accessibility tree. With element IDs, the element selection is transformed into an n-way classification problem, thereby eliminating any disambiguation efforts required from the agent or the underlying implementation. For example, issuing the action click 1https://gitlab.com/gitlab-org/gitlab 2https://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree 4 Under review [1582] clicks the button given the observation of [1582] Add to Cart. This flexible element selection allows WebArena to support agents designed in various ways (e.g., accepting input from different modalities) without compromising fair comparison metrics such as step count. User Role Simulation Users of the same website often have disparate experiences due to their distinct roles, permissions, and interaction histories. We emulate this scenario by generating unique user profiles on each platform. The details can be found in Appendix A.3. 3 BENCHMARK SUITE OF WEB-BASED TASKS We provide a benchmark with 812 test examples on grounding high-level natural language instructions to interactions in WebArena. Each example has a metric to evaluate the functional correctness of the task execution. In this section, we first formally define the task of controlling an autonomous agent through natural language. Then we introduce the annotation process of our benchmark. 3.1 INTENT COLLECTION We focus on curating realistic intents to carry out complex and creative tasks within WebArena. To start with, our annotators were guided to spend a few minutes exploring the websites to familiarize themselves with the websites\u2019 content and functionalities. As most of our websites are virtually identical to their open-web counterparts, despite having sampled data, most annotators can quickly comprehend the websites. Next, we instructed the annotators to formulate intents based on the following criteria: (1) The intent should be abstract and high-level, implying that the task cannot be fulfilled with merely one or two actions. As an example, instead of \u201cclick the science subreddit\u201d, we encouraged annotators to come up with something more complex like \u201cpost a greeting message on science subreddit\u201d, which involves performing multiple actions. (2) The intent should be creative. Common tasks such as account creation can be easily thought of. We encouraged the annotators to add constraints (e.g., \u201ccreate a Reddit account identical to my GitLab one\u201d) to make the intents more unique. (3) The intent should be formulated as a template by making replaceable elements as variables. The annotators were also responsible for developing several instantiations for each variable. For example, the intent \u201ccreate a Reddit account identical to my GitLab one\u201d can be converted into \u201ccreate a {{site1}} account identical to my {{site2}} one\u201d, with an instantiation like \u201c{site1: Reddit, site2: GitLab}\u201d and another like \u201c{site1: GitLab, site2: OneStopShopping}\u201d. Notably, tasks derived from the same template can have distinct execution traces. The similarity resides primarily in the high-level semantics rather than the specific implementation. We also provided a prompt for the annotators to use with ChatGPT3 for inspiration, that contains an overview of each website and instructs the model to describe potential tasks to be performed on these sites. Furthermore, we offered a curated list of examples for annotators to reference. Intent Analysis template is instantiated to 3.3 examples. The intent distribution is shown in Figure 6. In total, we curated 241 templates and 812 instantiated intents. On average, each Furthermore, we classify the intents into three primary categories with examples"}