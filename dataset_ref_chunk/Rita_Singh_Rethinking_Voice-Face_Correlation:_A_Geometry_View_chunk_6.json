{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_Rethinking_Voice-Face_Correlation:_A_Geometry_View_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the format of the mesh with points for each face?,answer: 6790 points", "ref_chunk": "the mesh format with 6790 points for each face. The voice recordings are about 2 minutes long for each speaker. We reduce the influencing factors to the voice and face by (1) asking participants to speak a set of specified sentences, (2) asking participants to speak without emotion, (3) control the age of participants (roughly 18-28 years old). In addition, to prevent the models from taking the gender shortcuts, we split the dataset D by gender, and experiments are individually performed on male and fe- male subsets. For each subset, we adopt 7/1/1/1 splitting for D\ud835\udc61 /D\ud835\udc631 /D\ud835\udc632/D\ud835\udc52 . In training, the voice recordings are randomly trimmed to segments of 6 to 8 seconds, while we use the entire recordings in testing. The ground truth AMs are normalized to zero mean and unit variance. For voice features, we extract 64-dimensional log Mel- spectrograms using an analysis window of 25ms, with the hop of 10ms between frames. We perform mean and variance normalization of each Mel-frequency bin. 4.2 We leverage a backbone E to learn voice code \ud835\udc52 which is a sim- ple convolutional neural network. The detailed network structure is presented in the supplementary materials. \ud835\udc39\ud835\udc58 and \ud835\udc3a\ud835\udc58 share the backbone\u2019s learnable parameters but have individual parameters for their heads. We use a single layer fully-connected network for each head. For the variance head, we add an exponential activation to the last layer of \ud835\udc3a\ud835\udc58 for non-negative positive output. We follow the typical settings of stochastic gradient descent (SGD) for optimiza- tion. Minibatch size is 64. The momentum, learning rate, and weight decay values are 0.9, 0.1, and 0.0005, respectively. The training is completed at 5k iterations. Since the phonatory module requires a long training procedure, we first train it with the voice code encoder E for 60k steps on our training set D\ud835\udc61 . We follow the training setting in [18] to train the phonatory module. The other parameter setting follows [18]. We directly normalized the voice signal as input to the network instead of first converting it to Log-Mel spectrum. To ensure statistical significance, we perform N = 100 repeated experiments to compute the \ud835\udc36\ud835\udc3c\ud835\udc62 . For the experiments at phoneme level, we leverage Wav2Vec [2] to cut the long voice recordings into phonemes. Implementation Details Rethinking Voice-Face Correlation: A Geometry View 50% \u02c6\ud835\udc64 0.842\u00b10.030 0.879\u00b10.041 Table 1: Effect of the phonatory module. We measure the normal- ized mean squared error between predicted and ground-truth AM among all AMs with different confidence thresholds. 100% \u02c6\ud835\udc64 0.953\u00b1 0.009 0.952\u00b10.014 75% \u02c6\ud835\udc64 0.909\u00b10.024 0.927\u00b10.030 Phonation Module (cid:33) (cid:37) Female Male Figure 5: Visualization of the predictable AMs. Blue box: male, Red box: female. 4.3 Predictable AM Analysis For AM prediction, the estimation models are trained on D\ud835\udc61 and selected based on their performance on D\ud835\udc631 (hyperparameter tun- ing). For AM selection, the predictable AMs are selected based on the upper bound of the CI (\ud835\udc36\ud835\udc3c\ud835\udc62 ) on D\ud835\udc632. The performance can be evaluated by the mean error of each AM and its CI. Fig. 4 shows the results, including 20 AMs with highest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 and 4 AMs with lowest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 . The gray bars are the results on the entire validation set D\ud835\udc632, while the red and yellow ones are the results of 75% and 50% voice samples with lowest uncertainty \u02c6\ud835\udc64 on D\ud835\udc632, respectively. The self-constructed female subset has the same size as the male subset. Higher 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 indicates better results and the normalized error of 0 indicates the chance-level performance. As suggested by our hypothesis testing formulation, the AMs with 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0 are considered predictable from voice. In this sense, we have discovered a number of predictable female AMs (see the gray bars and their \ud835\udc36\ud835\udc3c\ud835\udc62 in Fig. 4 (b)). By filtering out the voice samples with high uncertainties, we achieve even higher 1 \u2212\ud835\udc36\ud835\udc3c\ud835\udc62 (see the red and yellow bars and their \ud835\udc36\ud835\udc3c s). The improved performance indicates that more AMs are discovered as predictable from voice. The complete results of all AMs are given in the appendix. The results empirically demonstrate that the information of 3D facial shape is indeed encoded in the voices and can be discovered by our analysis pipeline. To intuitively locate the predictable AMs on the 3D face, we visualize them in Fig. 5. We clearly observe that most of the pre- dictable AMs are around nose and mouth, and many of them are shared between male and female subsets. This is consistent with the fact that nose and mouth shapes affect pronunciation. We also notice that the performance of female subset is much better than that of the male subset. To investigate whether the im- provements come from the larger data scale (364 males \ud835\udc63.\ud835\udc60. 662 females), we perform another set of repeated experiments on a self- constructed female subset, which has the same size as the male Conference\u201923, July 2023, Ottawa, Canada Phonatory Module (cid:33) (cid:37) Predictable Unpredictable 0.990\u00b10.032 0.628\u00b10.021 1.002\u00b10.031 0.730\u00b10.048 Table 2: Effect of phonatory module for predictable and unpre- dictable AMs. We measure the normalized mean squared error between predicted and ground-truth AM among all predictable and unpredictable AMs. Interestingly, we find phonatory mod- ule only improves predictable AMs. subset,i.e., 364 females. Surprisingly, the results on the new subset are still better than those on the male subset, as shown in Fig. 4 (c). This is possible because the female subjects have higher nasalance scores on the nasal sentences [42] among other things, which pro- vides useful information for predicting the AMs around the nose. Here we note that our experiments have revealed that measurements around the nose are highly correlated to voice. More investigations are left for future work. On the other hand, some AMs have not been shown to be pre- dictable from voice. This observation suggests that voices may only associate with a few specific regions of the 3D facial shape, like the nose and mouth. For the AMs with higher errors than chance level,"}, {"question": " How long are the voice recordings for each speaker?,answer: About 2 minutes", "ref_chunk": "the mesh format with 6790 points for each face. The voice recordings are about 2 minutes long for each speaker. We reduce the influencing factors to the voice and face by (1) asking participants to speak a set of specified sentences, (2) asking participants to speak without emotion, (3) control the age of participants (roughly 18-28 years old). In addition, to prevent the models from taking the gender shortcuts, we split the dataset D by gender, and experiments are individually performed on male and fe- male subsets. For each subset, we adopt 7/1/1/1 splitting for D\ud835\udc61 /D\ud835\udc631 /D\ud835\udc632/D\ud835\udc52 . In training, the voice recordings are randomly trimmed to segments of 6 to 8 seconds, while we use the entire recordings in testing. The ground truth AMs are normalized to zero mean and unit variance. For voice features, we extract 64-dimensional log Mel- spectrograms using an analysis window of 25ms, with the hop of 10ms between frames. We perform mean and variance normalization of each Mel-frequency bin. 4.2 We leverage a backbone E to learn voice code \ud835\udc52 which is a sim- ple convolutional neural network. The detailed network structure is presented in the supplementary materials. \ud835\udc39\ud835\udc58 and \ud835\udc3a\ud835\udc58 share the backbone\u2019s learnable parameters but have individual parameters for their heads. We use a single layer fully-connected network for each head. For the variance head, we add an exponential activation to the last layer of \ud835\udc3a\ud835\udc58 for non-negative positive output. We follow the typical settings of stochastic gradient descent (SGD) for optimiza- tion. Minibatch size is 64. The momentum, learning rate, and weight decay values are 0.9, 0.1, and 0.0005, respectively. The training is completed at 5k iterations. Since the phonatory module requires a long training procedure, we first train it with the voice code encoder E for 60k steps on our training set D\ud835\udc61 . We follow the training setting in [18] to train the phonatory module. The other parameter setting follows [18]. We directly normalized the voice signal as input to the network instead of first converting it to Log-Mel spectrum. To ensure statistical significance, we perform N = 100 repeated experiments to compute the \ud835\udc36\ud835\udc3c\ud835\udc62 . For the experiments at phoneme level, we leverage Wav2Vec [2] to cut the long voice recordings into phonemes. Implementation Details Rethinking Voice-Face Correlation: A Geometry View 50% \u02c6\ud835\udc64 0.842\u00b10.030 0.879\u00b10.041 Table 1: Effect of the phonatory module. We measure the normal- ized mean squared error between predicted and ground-truth AM among all AMs with different confidence thresholds. 100% \u02c6\ud835\udc64 0.953\u00b1 0.009 0.952\u00b10.014 75% \u02c6\ud835\udc64 0.909\u00b10.024 0.927\u00b10.030 Phonation Module (cid:33) (cid:37) Female Male Figure 5: Visualization of the predictable AMs. Blue box: male, Red box: female. 4.3 Predictable AM Analysis For AM prediction, the estimation models are trained on D\ud835\udc61 and selected based on their performance on D\ud835\udc631 (hyperparameter tun- ing). For AM selection, the predictable AMs are selected based on the upper bound of the CI (\ud835\udc36\ud835\udc3c\ud835\udc62 ) on D\ud835\udc632. The performance can be evaluated by the mean error of each AM and its CI. Fig. 4 shows the results, including 20 AMs with highest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 and 4 AMs with lowest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 . The gray bars are the results on the entire validation set D\ud835\udc632, while the red and yellow ones are the results of 75% and 50% voice samples with lowest uncertainty \u02c6\ud835\udc64 on D\ud835\udc632, respectively. The self-constructed female subset has the same size as the male subset. Higher 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 indicates better results and the normalized error of 0 indicates the chance-level performance. As suggested by our hypothesis testing formulation, the AMs with 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0 are considered predictable from voice. In this sense, we have discovered a number of predictable female AMs (see the gray bars and their \ud835\udc36\ud835\udc3c\ud835\udc62 in Fig. 4 (b)). By filtering out the voice samples with high uncertainties, we achieve even higher 1 \u2212\ud835\udc36\ud835\udc3c\ud835\udc62 (see the red and yellow bars and their \ud835\udc36\ud835\udc3c s). The improved performance indicates that more AMs are discovered as predictable from voice. The complete results of all AMs are given in the appendix. The results empirically demonstrate that the information of 3D facial shape is indeed encoded in the voices and can be discovered by our analysis pipeline. To intuitively locate the predictable AMs on the 3D face, we visualize them in Fig. 5. We clearly observe that most of the pre- dictable AMs are around nose and mouth, and many of them are shared between male and female subsets. This is consistent with the fact that nose and mouth shapes affect pronunciation. We also notice that the performance of female subset is much better than that of the male subset. To investigate whether the im- provements come from the larger data scale (364 males \ud835\udc63.\ud835\udc60. 662 females), we perform another set of repeated experiments on a self- constructed female subset, which has the same size as the male Conference\u201923, July 2023, Ottawa, Canada Phonatory Module (cid:33) (cid:37) Predictable Unpredictable 0.990\u00b10.032 0.628\u00b10.021 1.002\u00b10.031 0.730\u00b10.048 Table 2: Effect of phonatory module for predictable and unpre- dictable AMs. We measure the normalized mean squared error between predicted and ground-truth AM among all predictable and unpredictable AMs. Interestingly, we find phonatory mod- ule only improves predictable AMs. subset,i.e., 364 females. Surprisingly, the results on the new subset are still better than those on the male subset, as shown in Fig. 4 (c). This is possible because the female subjects have higher nasalance scores on the nasal sentences [42] among other things, which pro- vides useful information for predicting the AMs around the nose. Here we note that our experiments have revealed that measurements around the nose are highly correlated to voice. More investigations are left for future work. On the other hand, some AMs have not been shown to be pre- dictable from voice. This observation suggests that voices may only associate with a few specific regions of the 3D facial shape, like the nose and mouth. For the AMs with higher errors than chance level,"}, {"question": " How did the researchers reduce the influencing factors to the voice and face?,answer: By asking participants to speak specified sentences, speak without emotion, and controlling the age of participants", "ref_chunk": "the mesh format with 6790 points for each face. The voice recordings are about 2 minutes long for each speaker. We reduce the influencing factors to the voice and face by (1) asking participants to speak a set of specified sentences, (2) asking participants to speak without emotion, (3) control the age of participants (roughly 18-28 years old). In addition, to prevent the models from taking the gender shortcuts, we split the dataset D by gender, and experiments are individually performed on male and fe- male subsets. For each subset, we adopt 7/1/1/1 splitting for D\ud835\udc61 /D\ud835\udc631 /D\ud835\udc632/D\ud835\udc52 . In training, the voice recordings are randomly trimmed to segments of 6 to 8 seconds, while we use the entire recordings in testing. The ground truth AMs are normalized to zero mean and unit variance. For voice features, we extract 64-dimensional log Mel- spectrograms using an analysis window of 25ms, with the hop of 10ms between frames. We perform mean and variance normalization of each Mel-frequency bin. 4.2 We leverage a backbone E to learn voice code \ud835\udc52 which is a sim- ple convolutional neural network. The detailed network structure is presented in the supplementary materials. \ud835\udc39\ud835\udc58 and \ud835\udc3a\ud835\udc58 share the backbone\u2019s learnable parameters but have individual parameters for their heads. We use a single layer fully-connected network for each head. For the variance head, we add an exponential activation to the last layer of \ud835\udc3a\ud835\udc58 for non-negative positive output. We follow the typical settings of stochastic gradient descent (SGD) for optimiza- tion. Minibatch size is 64. The momentum, learning rate, and weight decay values are 0.9, 0.1, and 0.0005, respectively. The training is completed at 5k iterations. Since the phonatory module requires a long training procedure, we first train it with the voice code encoder E for 60k steps on our training set D\ud835\udc61 . We follow the training setting in [18] to train the phonatory module. The other parameter setting follows [18]. We directly normalized the voice signal as input to the network instead of first converting it to Log-Mel spectrum. To ensure statistical significance, we perform N = 100 repeated experiments to compute the \ud835\udc36\ud835\udc3c\ud835\udc62 . For the experiments at phoneme level, we leverage Wav2Vec [2] to cut the long voice recordings into phonemes. Implementation Details Rethinking Voice-Face Correlation: A Geometry View 50% \u02c6\ud835\udc64 0.842\u00b10.030 0.879\u00b10.041 Table 1: Effect of the phonatory module. We measure the normal- ized mean squared error between predicted and ground-truth AM among all AMs with different confidence thresholds. 100% \u02c6\ud835\udc64 0.953\u00b1 0.009 0.952\u00b10.014 75% \u02c6\ud835\udc64 0.909\u00b10.024 0.927\u00b10.030 Phonation Module (cid:33) (cid:37) Female Male Figure 5: Visualization of the predictable AMs. Blue box: male, Red box: female. 4.3 Predictable AM Analysis For AM prediction, the estimation models are trained on D\ud835\udc61 and selected based on their performance on D\ud835\udc631 (hyperparameter tun- ing). For AM selection, the predictable AMs are selected based on the upper bound of the CI (\ud835\udc36\ud835\udc3c\ud835\udc62 ) on D\ud835\udc632. The performance can be evaluated by the mean error of each AM and its CI. Fig. 4 shows the results, including 20 AMs with highest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 and 4 AMs with lowest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 . The gray bars are the results on the entire validation set D\ud835\udc632, while the red and yellow ones are the results of 75% and 50% voice samples with lowest uncertainty \u02c6\ud835\udc64 on D\ud835\udc632, respectively. The self-constructed female subset has the same size as the male subset. Higher 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 indicates better results and the normalized error of 0 indicates the chance-level performance. As suggested by our hypothesis testing formulation, the AMs with 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0 are considered predictable from voice. In this sense, we have discovered a number of predictable female AMs (see the gray bars and their \ud835\udc36\ud835\udc3c\ud835\udc62 in Fig. 4 (b)). By filtering out the voice samples with high uncertainties, we achieve even higher 1 \u2212\ud835\udc36\ud835\udc3c\ud835\udc62 (see the red and yellow bars and their \ud835\udc36\ud835\udc3c s). The improved performance indicates that more AMs are discovered as predictable from voice. The complete results of all AMs are given in the appendix. The results empirically demonstrate that the information of 3D facial shape is indeed encoded in the voices and can be discovered by our analysis pipeline. To intuitively locate the predictable AMs on the 3D face, we visualize them in Fig. 5. We clearly observe that most of the pre- dictable AMs are around nose and mouth, and many of them are shared between male and female subsets. This is consistent with the fact that nose and mouth shapes affect pronunciation. We also notice that the performance of female subset is much better than that of the male subset. To investigate whether the im- provements come from the larger data scale (364 males \ud835\udc63.\ud835\udc60. 662 females), we perform another set of repeated experiments on a self- constructed female subset, which has the same size as the male Conference\u201923, July 2023, Ottawa, Canada Phonatory Module (cid:33) (cid:37) Predictable Unpredictable 0.990\u00b10.032 0.628\u00b10.021 1.002\u00b10.031 0.730\u00b10.048 Table 2: Effect of phonatory module for predictable and unpre- dictable AMs. We measure the normalized mean squared error between predicted and ground-truth AM among all predictable and unpredictable AMs. Interestingly, we find phonatory mod- ule only improves predictable AMs. subset,i.e., 364 females. Surprisingly, the results on the new subset are still better than those on the male subset, as shown in Fig. 4 (c). This is possible because the female subjects have higher nasalance scores on the nasal sentences [42] among other things, which pro- vides useful information for predicting the AMs around the nose. Here we note that our experiments have revealed that measurements around the nose are highly correlated to voice. More investigations are left for future work. On the other hand, some AMs have not been shown to be pre- dictable from voice. This observation suggests that voices may only associate with a few specific regions of the 3D facial shape, like the nose and mouth. For the AMs with higher errors than chance level,"}, {"question": " How was the dataset split to prevent gender shortcuts?,answer: D was split by gender, and experiments were individually performed on male and female subsets", "ref_chunk": "the mesh format with 6790 points for each face. The voice recordings are about 2 minutes long for each speaker. We reduce the influencing factors to the voice and face by (1) asking participants to speak a set of specified sentences, (2) asking participants to speak without emotion, (3) control the age of participants (roughly 18-28 years old). In addition, to prevent the models from taking the gender shortcuts, we split the dataset D by gender, and experiments are individually performed on male and fe- male subsets. For each subset, we adopt 7/1/1/1 splitting for D\ud835\udc61 /D\ud835\udc631 /D\ud835\udc632/D\ud835\udc52 . In training, the voice recordings are randomly trimmed to segments of 6 to 8 seconds, while we use the entire recordings in testing. The ground truth AMs are normalized to zero mean and unit variance. For voice features, we extract 64-dimensional log Mel- spectrograms using an analysis window of 25ms, with the hop of 10ms between frames. We perform mean and variance normalization of each Mel-frequency bin. 4.2 We leverage a backbone E to learn voice code \ud835\udc52 which is a sim- ple convolutional neural network. The detailed network structure is presented in the supplementary materials. \ud835\udc39\ud835\udc58 and \ud835\udc3a\ud835\udc58 share the backbone\u2019s learnable parameters but have individual parameters for their heads. We use a single layer fully-connected network for each head. For the variance head, we add an exponential activation to the last layer of \ud835\udc3a\ud835\udc58 for non-negative positive output. We follow the typical settings of stochastic gradient descent (SGD) for optimiza- tion. Minibatch size is 64. The momentum, learning rate, and weight decay values are 0.9, 0.1, and 0.0005, respectively. The training is completed at 5k iterations. Since the phonatory module requires a long training procedure, we first train it with the voice code encoder E for 60k steps on our training set D\ud835\udc61 . We follow the training setting in [18] to train the phonatory module. The other parameter setting follows [18]. We directly normalized the voice signal as input to the network instead of first converting it to Log-Mel spectrum. To ensure statistical significance, we perform N = 100 repeated experiments to compute the \ud835\udc36\ud835\udc3c\ud835\udc62 . For the experiments at phoneme level, we leverage Wav2Vec [2] to cut the long voice recordings into phonemes. Implementation Details Rethinking Voice-Face Correlation: A Geometry View 50% \u02c6\ud835\udc64 0.842\u00b10.030 0.879\u00b10.041 Table 1: Effect of the phonatory module. We measure the normal- ized mean squared error between predicted and ground-truth AM among all AMs with different confidence thresholds. 100% \u02c6\ud835\udc64 0.953\u00b1 0.009 0.952\u00b10.014 75% \u02c6\ud835\udc64 0.909\u00b10.024 0.927\u00b10.030 Phonation Module (cid:33) (cid:37) Female Male Figure 5: Visualization of the predictable AMs. Blue box: male, Red box: female. 4.3 Predictable AM Analysis For AM prediction, the estimation models are trained on D\ud835\udc61 and selected based on their performance on D\ud835\udc631 (hyperparameter tun- ing). For AM selection, the predictable AMs are selected based on the upper bound of the CI (\ud835\udc36\ud835\udc3c\ud835\udc62 ) on D\ud835\udc632. The performance can be evaluated by the mean error of each AM and its CI. Fig. 4 shows the results, including 20 AMs with highest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 and 4 AMs with lowest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 . The gray bars are the results on the entire validation set D\ud835\udc632, while the red and yellow ones are the results of 75% and 50% voice samples with lowest uncertainty \u02c6\ud835\udc64 on D\ud835\udc632, respectively. The self-constructed female subset has the same size as the male subset. Higher 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 indicates better results and the normalized error of 0 indicates the chance-level performance. As suggested by our hypothesis testing formulation, the AMs with 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0 are considered predictable from voice. In this sense, we have discovered a number of predictable female AMs (see the gray bars and their \ud835\udc36\ud835\udc3c\ud835\udc62 in Fig. 4 (b)). By filtering out the voice samples with high uncertainties, we achieve even higher 1 \u2212\ud835\udc36\ud835\udc3c\ud835\udc62 (see the red and yellow bars and their \ud835\udc36\ud835\udc3c s). The improved performance indicates that more AMs are discovered as predictable from voice. The complete results of all AMs are given in the appendix. The results empirically demonstrate that the information of 3D facial shape is indeed encoded in the voices and can be discovered by our analysis pipeline. To intuitively locate the predictable AMs on the 3D face, we visualize them in Fig. 5. We clearly observe that most of the pre- dictable AMs are around nose and mouth, and many of them are shared between male and female subsets. This is consistent with the fact that nose and mouth shapes affect pronunciation. We also notice that the performance of female subset is much better than that of the male subset. To investigate whether the im- provements come from the larger data scale (364 males \ud835\udc63.\ud835\udc60. 662 females), we perform another set of repeated experiments on a self- constructed female subset, which has the same size as the male Conference\u201923, July 2023, Ottawa, Canada Phonatory Module (cid:33) (cid:37) Predictable Unpredictable 0.990\u00b10.032 0.628\u00b10.021 1.002\u00b10.031 0.730\u00b10.048 Table 2: Effect of phonatory module for predictable and unpre- dictable AMs. We measure the normalized mean squared error between predicted and ground-truth AM among all predictable and unpredictable AMs. Interestingly, we find phonatory mod- ule only improves predictable AMs. subset,i.e., 364 females. Surprisingly, the results on the new subset are still better than those on the male subset, as shown in Fig. 4 (c). This is possible because the female subjects have higher nasalance scores on the nasal sentences [42] among other things, which pro- vides useful information for predicting the AMs around the nose. Here we note that our experiments have revealed that measurements around the nose are highly correlated to voice. More investigations are left for future work. On the other hand, some AMs have not been shown to be pre- dictable from voice. This observation suggests that voices may only associate with a few specific regions of the 3D facial shape, like the nose and mouth. For the AMs with higher errors than chance level,"}, {"question": " What was the size of the minibatch used in training?,answer: 64", "ref_chunk": "the mesh format with 6790 points for each face. The voice recordings are about 2 minutes long for each speaker. We reduce the influencing factors to the voice and face by (1) asking participants to speak a set of specified sentences, (2) asking participants to speak without emotion, (3) control the age of participants (roughly 18-28 years old). In addition, to prevent the models from taking the gender shortcuts, we split the dataset D by gender, and experiments are individually performed on male and fe- male subsets. For each subset, we adopt 7/1/1/1 splitting for D\ud835\udc61 /D\ud835\udc631 /D\ud835\udc632/D\ud835\udc52 . In training, the voice recordings are randomly trimmed to segments of 6 to 8 seconds, while we use the entire recordings in testing. The ground truth AMs are normalized to zero mean and unit variance. For voice features, we extract 64-dimensional log Mel- spectrograms using an analysis window of 25ms, with the hop of 10ms between frames. We perform mean and variance normalization of each Mel-frequency bin. 4.2 We leverage a backbone E to learn voice code \ud835\udc52 which is a sim- ple convolutional neural network. The detailed network structure is presented in the supplementary materials. \ud835\udc39\ud835\udc58 and \ud835\udc3a\ud835\udc58 share the backbone\u2019s learnable parameters but have individual parameters for their heads. We use a single layer fully-connected network for each head. For the variance head, we add an exponential activation to the last layer of \ud835\udc3a\ud835\udc58 for non-negative positive output. We follow the typical settings of stochastic gradient descent (SGD) for optimiza- tion. Minibatch size is 64. The momentum, learning rate, and weight decay values are 0.9, 0.1, and 0.0005, respectively. The training is completed at 5k iterations. Since the phonatory module requires a long training procedure, we first train it with the voice code encoder E for 60k steps on our training set D\ud835\udc61 . We follow the training setting in [18] to train the phonatory module. The other parameter setting follows [18]. We directly normalized the voice signal as input to the network instead of first converting it to Log-Mel spectrum. To ensure statistical significance, we perform N = 100 repeated experiments to compute the \ud835\udc36\ud835\udc3c\ud835\udc62 . For the experiments at phoneme level, we leverage Wav2Vec [2] to cut the long voice recordings into phonemes. Implementation Details Rethinking Voice-Face Correlation: A Geometry View 50% \u02c6\ud835\udc64 0.842\u00b10.030 0.879\u00b10.041 Table 1: Effect of the phonatory module. We measure the normal- ized mean squared error between predicted and ground-truth AM among all AMs with different confidence thresholds. 100% \u02c6\ud835\udc64 0.953\u00b1 0.009 0.952\u00b10.014 75% \u02c6\ud835\udc64 0.909\u00b10.024 0.927\u00b10.030 Phonation Module (cid:33) (cid:37) Female Male Figure 5: Visualization of the predictable AMs. Blue box: male, Red box: female. 4.3 Predictable AM Analysis For AM prediction, the estimation models are trained on D\ud835\udc61 and selected based on their performance on D\ud835\udc631 (hyperparameter tun- ing). For AM selection, the predictable AMs are selected based on the upper bound of the CI (\ud835\udc36\ud835\udc3c\ud835\udc62 ) on D\ud835\udc632. The performance can be evaluated by the mean error of each AM and its CI. Fig. 4 shows the results, including 20 AMs with highest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 and 4 AMs with lowest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 . The gray bars are the results on the entire validation set D\ud835\udc632, while the red and yellow ones are the results of 75% and 50% voice samples with lowest uncertainty \u02c6\ud835\udc64 on D\ud835\udc632, respectively. The self-constructed female subset has the same size as the male subset. Higher 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 indicates better results and the normalized error of 0 indicates the chance-level performance. As suggested by our hypothesis testing formulation, the AMs with 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0 are considered predictable from voice. In this sense, we have discovered a number of predictable female AMs (see the gray bars and their \ud835\udc36\ud835\udc3c\ud835\udc62 in Fig. 4 (b)). By filtering out the voice samples with high uncertainties, we achieve even higher 1 \u2212\ud835\udc36\ud835\udc3c\ud835\udc62 (see the red and yellow bars and their \ud835\udc36\ud835\udc3c s). The improved performance indicates that more AMs are discovered as predictable from voice. The complete results of all AMs are given in the appendix. The results empirically demonstrate that the information of 3D facial shape is indeed encoded in the voices and can be discovered by our analysis pipeline. To intuitively locate the predictable AMs on the 3D face, we visualize them in Fig. 5. We clearly observe that most of the pre- dictable AMs are around nose and mouth, and many of them are shared between male and female subsets. This is consistent with the fact that nose and mouth shapes affect pronunciation. We also notice that the performance of female subset is much better than that of the male subset. To investigate whether the im- provements come from the larger data scale (364 males \ud835\udc63.\ud835\udc60. 662 females), we perform another set of repeated experiments on a self- constructed female subset, which has the same size as the male Conference\u201923, July 2023, Ottawa, Canada Phonatory Module (cid:33) (cid:37) Predictable Unpredictable 0.990\u00b10.032 0.628\u00b10.021 1.002\u00b10.031 0.730\u00b10.048 Table 2: Effect of phonatory module for predictable and unpre- dictable AMs. We measure the normalized mean squared error between predicted and ground-truth AM among all predictable and unpredictable AMs. Interestingly, we find phonatory mod- ule only improves predictable AMs. subset,i.e., 364 females. Surprisingly, the results on the new subset are still better than those on the male subset, as shown in Fig. 4 (c). This is possible because the female subjects have higher nasalance scores on the nasal sentences [42] among other things, which pro- vides useful information for predicting the AMs around the nose. Here we note that our experiments have revealed that measurements around the nose are highly correlated to voice. More investigations are left for future work. On the other hand, some AMs have not been shown to be pre- dictable from voice. This observation suggests that voices may only associate with a few specific regions of the 3D facial shape, like the nose and mouth. For the AMs with higher errors than chance level,"}, {"question": " How many steps was the phonatory module trained for on the training set D\ud835\udc61?,answer: 60k steps", "ref_chunk": "the mesh format with 6790 points for each face. The voice recordings are about 2 minutes long for each speaker. We reduce the influencing factors to the voice and face by (1) asking participants to speak a set of specified sentences, (2) asking participants to speak without emotion, (3) control the age of participants (roughly 18-28 years old). In addition, to prevent the models from taking the gender shortcuts, we split the dataset D by gender, and experiments are individually performed on male and fe- male subsets. For each subset, we adopt 7/1/1/1 splitting for D\ud835\udc61 /D\ud835\udc631 /D\ud835\udc632/D\ud835\udc52 . In training, the voice recordings are randomly trimmed to segments of 6 to 8 seconds, while we use the entire recordings in testing. The ground truth AMs are normalized to zero mean and unit variance. For voice features, we extract 64-dimensional log Mel- spectrograms using an analysis window of 25ms, with the hop of 10ms between frames. We perform mean and variance normalization of each Mel-frequency bin. 4.2 We leverage a backbone E to learn voice code \ud835\udc52 which is a sim- ple convolutional neural network. The detailed network structure is presented in the supplementary materials. \ud835\udc39\ud835\udc58 and \ud835\udc3a\ud835\udc58 share the backbone\u2019s learnable parameters but have individual parameters for their heads. We use a single layer fully-connected network for each head. For the variance head, we add an exponential activation to the last layer of \ud835\udc3a\ud835\udc58 for non-negative positive output. We follow the typical settings of stochastic gradient descent (SGD) for optimiza- tion. Minibatch size is 64. The momentum, learning rate, and weight decay values are 0.9, 0.1, and 0.0005, respectively. The training is completed at 5k iterations. Since the phonatory module requires a long training procedure, we first train it with the voice code encoder E for 60k steps on our training set D\ud835\udc61 . We follow the training setting in [18] to train the phonatory module. The other parameter setting follows [18]. We directly normalized the voice signal as input to the network instead of first converting it to Log-Mel spectrum. To ensure statistical significance, we perform N = 100 repeated experiments to compute the \ud835\udc36\ud835\udc3c\ud835\udc62 . For the experiments at phoneme level, we leverage Wav2Vec [2] to cut the long voice recordings into phonemes. Implementation Details Rethinking Voice-Face Correlation: A Geometry View 50% \u02c6\ud835\udc64 0.842\u00b10.030 0.879\u00b10.041 Table 1: Effect of the phonatory module. We measure the normal- ized mean squared error between predicted and ground-truth AM among all AMs with different confidence thresholds. 100% \u02c6\ud835\udc64 0.953\u00b1 0.009 0.952\u00b10.014 75% \u02c6\ud835\udc64 0.909\u00b10.024 0.927\u00b10.030 Phonation Module (cid:33) (cid:37) Female Male Figure 5: Visualization of the predictable AMs. Blue box: male, Red box: female. 4.3 Predictable AM Analysis For AM prediction, the estimation models are trained on D\ud835\udc61 and selected based on their performance on D\ud835\udc631 (hyperparameter tun- ing). For AM selection, the predictable AMs are selected based on the upper bound of the CI (\ud835\udc36\ud835\udc3c\ud835\udc62 ) on D\ud835\udc632. The performance can be evaluated by the mean error of each AM and its CI. Fig. 4 shows the results, including 20 AMs with highest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 and 4 AMs with lowest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 . The gray bars are the results on the entire validation set D\ud835\udc632, while the red and yellow ones are the results of 75% and 50% voice samples with lowest uncertainty \u02c6\ud835\udc64 on D\ud835\udc632, respectively. The self-constructed female subset has the same size as the male subset. Higher 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 indicates better results and the normalized error of 0 indicates the chance-level performance. As suggested by our hypothesis testing formulation, the AMs with 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0 are considered predictable from voice. In this sense, we have discovered a number of predictable female AMs (see the gray bars and their \ud835\udc36\ud835\udc3c\ud835\udc62 in Fig. 4 (b)). By filtering out the voice samples with high uncertainties, we achieve even higher 1 \u2212\ud835\udc36\ud835\udc3c\ud835\udc62 (see the red and yellow bars and their \ud835\udc36\ud835\udc3c s). The improved performance indicates that more AMs are discovered as predictable from voice. The complete results of all AMs are given in the appendix. The results empirically demonstrate that the information of 3D facial shape is indeed encoded in the voices and can be discovered by our analysis pipeline. To intuitively locate the predictable AMs on the 3D face, we visualize them in Fig. 5. We clearly observe that most of the pre- dictable AMs are around nose and mouth, and many of them are shared between male and female subsets. This is consistent with the fact that nose and mouth shapes affect pronunciation. We also notice that the performance of female subset is much better than that of the male subset. To investigate whether the im- provements come from the larger data scale (364 males \ud835\udc63.\ud835\udc60. 662 females), we perform another set of repeated experiments on a self- constructed female subset, which has the same size as the male Conference\u201923, July 2023, Ottawa, Canada Phonatory Module (cid:33) (cid:37) Predictable Unpredictable 0.990\u00b10.032 0.628\u00b10.021 1.002\u00b10.031 0.730\u00b10.048 Table 2: Effect of phonatory module for predictable and unpre- dictable AMs. We measure the normalized mean squared error between predicted and ground-truth AM among all predictable and unpredictable AMs. Interestingly, we find phonatory mod- ule only improves predictable AMs. subset,i.e., 364 females. Surprisingly, the results on the new subset are still better than those on the male subset, as shown in Fig. 4 (c). This is possible because the female subjects have higher nasalance scores on the nasal sentences [42] among other things, which pro- vides useful information for predicting the AMs around the nose. Here we note that our experiments have revealed that measurements around the nose are highly correlated to voice. More investigations are left for future work. On the other hand, some AMs have not been shown to be pre- dictable from voice. This observation suggests that voices may only associate with a few specific regions of the 3D facial shape, like the nose and mouth. For the AMs with higher errors than chance level,"}, {"question": " How many repeated experiments were performed to compute the \ud835\udc36\ud835\udc3c\ud835\udc62?,answer: N = 100", "ref_chunk": "the mesh format with 6790 points for each face. The voice recordings are about 2 minutes long for each speaker. We reduce the influencing factors to the voice and face by (1) asking participants to speak a set of specified sentences, (2) asking participants to speak without emotion, (3) control the age of participants (roughly 18-28 years old). In addition, to prevent the models from taking the gender shortcuts, we split the dataset D by gender, and experiments are individually performed on male and fe- male subsets. For each subset, we adopt 7/1/1/1 splitting for D\ud835\udc61 /D\ud835\udc631 /D\ud835\udc632/D\ud835\udc52 . In training, the voice recordings are randomly trimmed to segments of 6 to 8 seconds, while we use the entire recordings in testing. The ground truth AMs are normalized to zero mean and unit variance. For voice features, we extract 64-dimensional log Mel- spectrograms using an analysis window of 25ms, with the hop of 10ms between frames. We perform mean and variance normalization of each Mel-frequency bin. 4.2 We leverage a backbone E to learn voice code \ud835\udc52 which is a sim- ple convolutional neural network. The detailed network structure is presented in the supplementary materials. \ud835\udc39\ud835\udc58 and \ud835\udc3a\ud835\udc58 share the backbone\u2019s learnable parameters but have individual parameters for their heads. We use a single layer fully-connected network for each head. For the variance head, we add an exponential activation to the last layer of \ud835\udc3a\ud835\udc58 for non-negative positive output. We follow the typical settings of stochastic gradient descent (SGD) for optimiza- tion. Minibatch size is 64. The momentum, learning rate, and weight decay values are 0.9, 0.1, and 0.0005, respectively. The training is completed at 5k iterations. Since the phonatory module requires a long training procedure, we first train it with the voice code encoder E for 60k steps on our training set D\ud835\udc61 . We follow the training setting in [18] to train the phonatory module. The other parameter setting follows [18]. We directly normalized the voice signal as input to the network instead of first converting it to Log-Mel spectrum. To ensure statistical significance, we perform N = 100 repeated experiments to compute the \ud835\udc36\ud835\udc3c\ud835\udc62 . For the experiments at phoneme level, we leverage Wav2Vec [2] to cut the long voice recordings into phonemes. Implementation Details Rethinking Voice-Face Correlation: A Geometry View 50% \u02c6\ud835\udc64 0.842\u00b10.030 0.879\u00b10.041 Table 1: Effect of the phonatory module. We measure the normal- ized mean squared error between predicted and ground-truth AM among all AMs with different confidence thresholds. 100% \u02c6\ud835\udc64 0.953\u00b1 0.009 0.952\u00b10.014 75% \u02c6\ud835\udc64 0.909\u00b10.024 0.927\u00b10.030 Phonation Module (cid:33) (cid:37) Female Male Figure 5: Visualization of the predictable AMs. Blue box: male, Red box: female. 4.3 Predictable AM Analysis For AM prediction, the estimation models are trained on D\ud835\udc61 and selected based on their performance on D\ud835\udc631 (hyperparameter tun- ing). For AM selection, the predictable AMs are selected based on the upper bound of the CI (\ud835\udc36\ud835\udc3c\ud835\udc62 ) on D\ud835\udc632. The performance can be evaluated by the mean error of each AM and its CI. Fig. 4 shows the results, including 20 AMs with highest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 and 4 AMs with lowest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 . The gray bars are the results on the entire validation set D\ud835\udc632, while the red and yellow ones are the results of 75% and 50% voice samples with lowest uncertainty \u02c6\ud835\udc64 on D\ud835\udc632, respectively. The self-constructed female subset has the same size as the male subset. Higher 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 indicates better results and the normalized error of 0 indicates the chance-level performance. As suggested by our hypothesis testing formulation, the AMs with 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0 are considered predictable from voice. In this sense, we have discovered a number of predictable female AMs (see the gray bars and their \ud835\udc36\ud835\udc3c\ud835\udc62 in Fig. 4 (b)). By filtering out the voice samples with high uncertainties, we achieve even higher 1 \u2212\ud835\udc36\ud835\udc3c\ud835\udc62 (see the red and yellow bars and their \ud835\udc36\ud835\udc3c s). The improved performance indicates that more AMs are discovered as predictable from voice. The complete results of all AMs are given in the appendix. The results empirically demonstrate that the information of 3D facial shape is indeed encoded in the voices and can be discovered by our analysis pipeline. To intuitively locate the predictable AMs on the 3D face, we visualize them in Fig. 5. We clearly observe that most of the pre- dictable AMs are around nose and mouth, and many of them are shared between male and female subsets. This is consistent with the fact that nose and mouth shapes affect pronunciation. We also notice that the performance of female subset is much better than that of the male subset. To investigate whether the im- provements come from the larger data scale (364 males \ud835\udc63.\ud835\udc60. 662 females), we perform another set of repeated experiments on a self- constructed female subset, which has the same size as the male Conference\u201923, July 2023, Ottawa, Canada Phonatory Module (cid:33) (cid:37) Predictable Unpredictable 0.990\u00b10.032 0.628\u00b10.021 1.002\u00b10.031 0.730\u00b10.048 Table 2: Effect of phonatory module for predictable and unpre- dictable AMs. We measure the normalized mean squared error between predicted and ground-truth AM among all predictable and unpredictable AMs. Interestingly, we find phonatory mod- ule only improves predictable AMs. subset,i.e., 364 females. Surprisingly, the results on the new subset are still better than those on the male subset, as shown in Fig. 4 (c). This is possible because the female subjects have higher nasalance scores on the nasal sentences [42] among other things, which pro- vides useful information for predicting the AMs around the nose. Here we note that our experiments have revealed that measurements around the nose are highly correlated to voice. More investigations are left for future work. On the other hand, some AMs have not been shown to be pre- dictable from voice. This observation suggests that voices may only associate with a few specific regions of the 3D facial shape, like the nose and mouth. For the AMs with higher errors than chance level,"}, {"question": " What is the effect of the phonatory module on predictable and unpredictable AMs, according to Table 2?,answer: The phonatory module only improves predictable AMs", "ref_chunk": "the mesh format with 6790 points for each face. The voice recordings are about 2 minutes long for each speaker. We reduce the influencing factors to the voice and face by (1) asking participants to speak a set of specified sentences, (2) asking participants to speak without emotion, (3) control the age of participants (roughly 18-28 years old). In addition, to prevent the models from taking the gender shortcuts, we split the dataset D by gender, and experiments are individually performed on male and fe- male subsets. For each subset, we adopt 7/1/1/1 splitting for D\ud835\udc61 /D\ud835\udc631 /D\ud835\udc632/D\ud835\udc52 . In training, the voice recordings are randomly trimmed to segments of 6 to 8 seconds, while we use the entire recordings in testing. The ground truth AMs are normalized to zero mean and unit variance. For voice features, we extract 64-dimensional log Mel- spectrograms using an analysis window of 25ms, with the hop of 10ms between frames. We perform mean and variance normalization of each Mel-frequency bin. 4.2 We leverage a backbone E to learn voice code \ud835\udc52 which is a sim- ple convolutional neural network. The detailed network structure is presented in the supplementary materials. \ud835\udc39\ud835\udc58 and \ud835\udc3a\ud835\udc58 share the backbone\u2019s learnable parameters but have individual parameters for their heads. We use a single layer fully-connected network for each head. For the variance head, we add an exponential activation to the last layer of \ud835\udc3a\ud835\udc58 for non-negative positive output. We follow the typical settings of stochastic gradient descent (SGD) for optimiza- tion. Minibatch size is 64. The momentum, learning rate, and weight decay values are 0.9, 0.1, and 0.0005, respectively. The training is completed at 5k iterations. Since the phonatory module requires a long training procedure, we first train it with the voice code encoder E for 60k steps on our training set D\ud835\udc61 . We follow the training setting in [18] to train the phonatory module. The other parameter setting follows [18]. We directly normalized the voice signal as input to the network instead of first converting it to Log-Mel spectrum. To ensure statistical significance, we perform N = 100 repeated experiments to compute the \ud835\udc36\ud835\udc3c\ud835\udc62 . For the experiments at phoneme level, we leverage Wav2Vec [2] to cut the long voice recordings into phonemes. Implementation Details Rethinking Voice-Face Correlation: A Geometry View 50% \u02c6\ud835\udc64 0.842\u00b10.030 0.879\u00b10.041 Table 1: Effect of the phonatory module. We measure the normal- ized mean squared error between predicted and ground-truth AM among all AMs with different confidence thresholds. 100% \u02c6\ud835\udc64 0.953\u00b1 0.009 0.952\u00b10.014 75% \u02c6\ud835\udc64 0.909\u00b10.024 0.927\u00b10.030 Phonation Module (cid:33) (cid:37) Female Male Figure 5: Visualization of the predictable AMs. Blue box: male, Red box: female. 4.3 Predictable AM Analysis For AM prediction, the estimation models are trained on D\ud835\udc61 and selected based on their performance on D\ud835\udc631 (hyperparameter tun- ing). For AM selection, the predictable AMs are selected based on the upper bound of the CI (\ud835\udc36\ud835\udc3c\ud835\udc62 ) on D\ud835\udc632. The performance can be evaluated by the mean error of each AM and its CI. Fig. 4 shows the results, including 20 AMs with highest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 and 4 AMs with lowest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 . The gray bars are the results on the entire validation set D\ud835\udc632, while the red and yellow ones are the results of 75% and 50% voice samples with lowest uncertainty \u02c6\ud835\udc64 on D\ud835\udc632, respectively. The self-constructed female subset has the same size as the male subset. Higher 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 indicates better results and the normalized error of 0 indicates the chance-level performance. As suggested by our hypothesis testing formulation, the AMs with 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0 are considered predictable from voice. In this sense, we have discovered a number of predictable female AMs (see the gray bars and their \ud835\udc36\ud835\udc3c\ud835\udc62 in Fig. 4 (b)). By filtering out the voice samples with high uncertainties, we achieve even higher 1 \u2212\ud835\udc36\ud835\udc3c\ud835\udc62 (see the red and yellow bars and their \ud835\udc36\ud835\udc3c s). The improved performance indicates that more AMs are discovered as predictable from voice. The complete results of all AMs are given in the appendix. The results empirically demonstrate that the information of 3D facial shape is indeed encoded in the voices and can be discovered by our analysis pipeline. To intuitively locate the predictable AMs on the 3D face, we visualize them in Fig. 5. We clearly observe that most of the pre- dictable AMs are around nose and mouth, and many of them are shared between male and female subsets. This is consistent with the fact that nose and mouth shapes affect pronunciation. We also notice that the performance of female subset is much better than that of the male subset. To investigate whether the im- provements come from the larger data scale (364 males \ud835\udc63.\ud835\udc60. 662 females), we perform another set of repeated experiments on a self- constructed female subset, which has the same size as the male Conference\u201923, July 2023, Ottawa, Canada Phonatory Module (cid:33) (cid:37) Predictable Unpredictable 0.990\u00b10.032 0.628\u00b10.021 1.002\u00b10.031 0.730\u00b10.048 Table 2: Effect of phonatory module for predictable and unpre- dictable AMs. We measure the normalized mean squared error between predicted and ground-truth AM among all predictable and unpredictable AMs. Interestingly, we find phonatory mod- ule only improves predictable AMs. subset,i.e., 364 females. Surprisingly, the results on the new subset are still better than those on the male subset, as shown in Fig. 4 (c). This is possible because the female subjects have higher nasalance scores on the nasal sentences [42] among other things, which pro- vides useful information for predicting the AMs around the nose. Here we note that our experiments have revealed that measurements around the nose are highly correlated to voice. More investigations are left for future work. On the other hand, some AMs have not been shown to be pre- dictable from voice. This observation suggests that voices may only associate with a few specific regions of the 3D facial shape, like the nose and mouth. For the AMs with higher errors than chance level,"}, {"question": " Where are most of the predictable AMs located on the 3D face?,answer: Around nose and mouth", "ref_chunk": "the mesh format with 6790 points for each face. The voice recordings are about 2 minutes long for each speaker. We reduce the influencing factors to the voice and face by (1) asking participants to speak a set of specified sentences, (2) asking participants to speak without emotion, (3) control the age of participants (roughly 18-28 years old). In addition, to prevent the models from taking the gender shortcuts, we split the dataset D by gender, and experiments are individually performed on male and fe- male subsets. For each subset, we adopt 7/1/1/1 splitting for D\ud835\udc61 /D\ud835\udc631 /D\ud835\udc632/D\ud835\udc52 . In training, the voice recordings are randomly trimmed to segments of 6 to 8 seconds, while we use the entire recordings in testing. The ground truth AMs are normalized to zero mean and unit variance. For voice features, we extract 64-dimensional log Mel- spectrograms using an analysis window of 25ms, with the hop of 10ms between frames. We perform mean and variance normalization of each Mel-frequency bin. 4.2 We leverage a backbone E to learn voice code \ud835\udc52 which is a sim- ple convolutional neural network. The detailed network structure is presented in the supplementary materials. \ud835\udc39\ud835\udc58 and \ud835\udc3a\ud835\udc58 share the backbone\u2019s learnable parameters but have individual parameters for their heads. We use a single layer fully-connected network for each head. For the variance head, we add an exponential activation to the last layer of \ud835\udc3a\ud835\udc58 for non-negative positive output. We follow the typical settings of stochastic gradient descent (SGD) for optimiza- tion. Minibatch size is 64. The momentum, learning rate, and weight decay values are 0.9, 0.1, and 0.0005, respectively. The training is completed at 5k iterations. Since the phonatory module requires a long training procedure, we first train it with the voice code encoder E for 60k steps on our training set D\ud835\udc61 . We follow the training setting in [18] to train the phonatory module. The other parameter setting follows [18]. We directly normalized the voice signal as input to the network instead of first converting it to Log-Mel spectrum. To ensure statistical significance, we perform N = 100 repeated experiments to compute the \ud835\udc36\ud835\udc3c\ud835\udc62 . For the experiments at phoneme level, we leverage Wav2Vec [2] to cut the long voice recordings into phonemes. Implementation Details Rethinking Voice-Face Correlation: A Geometry View 50% \u02c6\ud835\udc64 0.842\u00b10.030 0.879\u00b10.041 Table 1: Effect of the phonatory module. We measure the normal- ized mean squared error between predicted and ground-truth AM among all AMs with different confidence thresholds. 100% \u02c6\ud835\udc64 0.953\u00b1 0.009 0.952\u00b10.014 75% \u02c6\ud835\udc64 0.909\u00b10.024 0.927\u00b10.030 Phonation Module (cid:33) (cid:37) Female Male Figure 5: Visualization of the predictable AMs. Blue box: male, Red box: female. 4.3 Predictable AM Analysis For AM prediction, the estimation models are trained on D\ud835\udc61 and selected based on their performance on D\ud835\udc631 (hyperparameter tun- ing). For AM selection, the predictable AMs are selected based on the upper bound of the CI (\ud835\udc36\ud835\udc3c\ud835\udc62 ) on D\ud835\udc632. The performance can be evaluated by the mean error of each AM and its CI. Fig. 4 shows the results, including 20 AMs with highest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 and 4 AMs with lowest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 . The gray bars are the results on the entire validation set D\ud835\udc632, while the red and yellow ones are the results of 75% and 50% voice samples with lowest uncertainty \u02c6\ud835\udc64 on D\ud835\udc632, respectively. The self-constructed female subset has the same size as the male subset. Higher 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 indicates better results and the normalized error of 0 indicates the chance-level performance. As suggested by our hypothesis testing formulation, the AMs with 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0 are considered predictable from voice. In this sense, we have discovered a number of predictable female AMs (see the gray bars and their \ud835\udc36\ud835\udc3c\ud835\udc62 in Fig. 4 (b)). By filtering out the voice samples with high uncertainties, we achieve even higher 1 \u2212\ud835\udc36\ud835\udc3c\ud835\udc62 (see the red and yellow bars and their \ud835\udc36\ud835\udc3c s). The improved performance indicates that more AMs are discovered as predictable from voice. The complete results of all AMs are given in the appendix. The results empirically demonstrate that the information of 3D facial shape is indeed encoded in the voices and can be discovered by our analysis pipeline. To intuitively locate the predictable AMs on the 3D face, we visualize them in Fig. 5. We clearly observe that most of the pre- dictable AMs are around nose and mouth, and many of them are shared between male and female subsets. This is consistent with the fact that nose and mouth shapes affect pronunciation. We also notice that the performance of female subset is much better than that of the male subset. To investigate whether the im- provements come from the larger data scale (364 males \ud835\udc63.\ud835\udc60. 662 females), we perform another set of repeated experiments on a self- constructed female subset, which has the same size as the male Conference\u201923, July 2023, Ottawa, Canada Phonatory Module (cid:33) (cid:37) Predictable Unpredictable 0.990\u00b10.032 0.628\u00b10.021 1.002\u00b10.031 0.730\u00b10.048 Table 2: Effect of phonatory module for predictable and unpre- dictable AMs. We measure the normalized mean squared error between predicted and ground-truth AM among all predictable and unpredictable AMs. Interestingly, we find phonatory mod- ule only improves predictable AMs. subset,i.e., 364 females. Surprisingly, the results on the new subset are still better than those on the male subset, as shown in Fig. 4 (c). This is possible because the female subjects have higher nasalance scores on the nasal sentences [42] among other things, which pro- vides useful information for predicting the AMs around the nose. Here we note that our experiments have revealed that measurements around the nose are highly correlated to voice. More investigations are left for future work. On the other hand, some AMs have not been shown to be pre- dictable from voice. This observation suggests that voices may only associate with a few specific regions of the 3D facial shape, like the nose and mouth. For the AMs with higher errors than chance level,"}, {"question": " What provides useful information for predicting the AMs around the nose, according to the text?,answer: Higher nasalance scores on the nasal sentences among other things", "ref_chunk": "the mesh format with 6790 points for each face. The voice recordings are about 2 minutes long for each speaker. We reduce the influencing factors to the voice and face by (1) asking participants to speak a set of specified sentences, (2) asking participants to speak without emotion, (3) control the age of participants (roughly 18-28 years old). In addition, to prevent the models from taking the gender shortcuts, we split the dataset D by gender, and experiments are individually performed on male and fe- male subsets. For each subset, we adopt 7/1/1/1 splitting for D\ud835\udc61 /D\ud835\udc631 /D\ud835\udc632/D\ud835\udc52 . In training, the voice recordings are randomly trimmed to segments of 6 to 8 seconds, while we use the entire recordings in testing. The ground truth AMs are normalized to zero mean and unit variance. For voice features, we extract 64-dimensional log Mel- spectrograms using an analysis window of 25ms, with the hop of 10ms between frames. We perform mean and variance normalization of each Mel-frequency bin. 4.2 We leverage a backbone E to learn voice code \ud835\udc52 which is a sim- ple convolutional neural network. The detailed network structure is presented in the supplementary materials. \ud835\udc39\ud835\udc58 and \ud835\udc3a\ud835\udc58 share the backbone\u2019s learnable parameters but have individual parameters for their heads. We use a single layer fully-connected network for each head. For the variance head, we add an exponential activation to the last layer of \ud835\udc3a\ud835\udc58 for non-negative positive output. We follow the typical settings of stochastic gradient descent (SGD) for optimiza- tion. Minibatch size is 64. The momentum, learning rate, and weight decay values are 0.9, 0.1, and 0.0005, respectively. The training is completed at 5k iterations. Since the phonatory module requires a long training procedure, we first train it with the voice code encoder E for 60k steps on our training set D\ud835\udc61 . We follow the training setting in [18] to train the phonatory module. The other parameter setting follows [18]. We directly normalized the voice signal as input to the network instead of first converting it to Log-Mel spectrum. To ensure statistical significance, we perform N = 100 repeated experiments to compute the \ud835\udc36\ud835\udc3c\ud835\udc62 . For the experiments at phoneme level, we leverage Wav2Vec [2] to cut the long voice recordings into phonemes. Implementation Details Rethinking Voice-Face Correlation: A Geometry View 50% \u02c6\ud835\udc64 0.842\u00b10.030 0.879\u00b10.041 Table 1: Effect of the phonatory module. We measure the normal- ized mean squared error between predicted and ground-truth AM among all AMs with different confidence thresholds. 100% \u02c6\ud835\udc64 0.953\u00b1 0.009 0.952\u00b10.014 75% \u02c6\ud835\udc64 0.909\u00b10.024 0.927\u00b10.030 Phonation Module (cid:33) (cid:37) Female Male Figure 5: Visualization of the predictable AMs. Blue box: male, Red box: female. 4.3 Predictable AM Analysis For AM prediction, the estimation models are trained on D\ud835\udc61 and selected based on their performance on D\ud835\udc631 (hyperparameter tun- ing). For AM selection, the predictable AMs are selected based on the upper bound of the CI (\ud835\udc36\ud835\udc3c\ud835\udc62 ) on D\ud835\udc632. The performance can be evaluated by the mean error of each AM and its CI. Fig. 4 shows the results, including 20 AMs with highest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 and 4 AMs with lowest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 . The gray bars are the results on the entire validation set D\ud835\udc632, while the red and yellow ones are the results of 75% and 50% voice samples with lowest uncertainty \u02c6\ud835\udc64 on D\ud835\udc632, respectively. The self-constructed female subset has the same size as the male subset. Higher 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 indicates better results and the normalized error of 0 indicates the chance-level performance. As suggested by our hypothesis testing formulation, the AMs with 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0 are considered predictable from voice. In this sense, we have discovered a number of predictable female AMs (see the gray bars and their \ud835\udc36\ud835\udc3c\ud835\udc62 in Fig. 4 (b)). By filtering out the voice samples with high uncertainties, we achieve even higher 1 \u2212\ud835\udc36\ud835\udc3c\ud835\udc62 (see the red and yellow bars and their \ud835\udc36\ud835\udc3c s). The improved performance indicates that more AMs are discovered as predictable from voice. The complete results of all AMs are given in the appendix. The results empirically demonstrate that the information of 3D facial shape is indeed encoded in the voices and can be discovered by our analysis pipeline. To intuitively locate the predictable AMs on the 3D face, we visualize them in Fig. 5. We clearly observe that most of the pre- dictable AMs are around nose and mouth, and many of them are shared between male and female subsets. This is consistent with the fact that nose and mouth shapes affect pronunciation. We also notice that the performance of female subset is much better than that of the male subset. To investigate whether the im- provements come from the larger data scale (364 males \ud835\udc63.\ud835\udc60. 662 females), we perform another set of repeated experiments on a self- constructed female subset, which has the same size as the male Conference\u201923, July 2023, Ottawa, Canada Phonatory Module (cid:33) (cid:37) Predictable Unpredictable 0.990\u00b10.032 0.628\u00b10.021 1.002\u00b10.031 0.730\u00b10.048 Table 2: Effect of phonatory module for predictable and unpre- dictable AMs. We measure the normalized mean squared error between predicted and ground-truth AM among all predictable and unpredictable AMs. Interestingly, we find phonatory mod- ule only improves predictable AMs. subset,i.e., 364 females. Surprisingly, the results on the new subset are still better than those on the male subset, as shown in Fig. 4 (c). This is possible because the female subjects have higher nasalance scores on the nasal sentences [42] among other things, which pro- vides useful information for predicting the AMs around the nose. Here we note that our experiments have revealed that measurements around the nose are highly correlated to voice. More investigations are left for future work. On the other hand, some AMs have not been shown to be pre- dictable from voice. This observation suggests that voices may only associate with a few specific regions of the 3D facial shape, like the nose and mouth. For the AMs with higher errors than chance level,"}], "doc_text": "the mesh format with 6790 points for each face. The voice recordings are about 2 minutes long for each speaker. We reduce the influencing factors to the voice and face by (1) asking participants to speak a set of specified sentences, (2) asking participants to speak without emotion, (3) control the age of participants (roughly 18-28 years old). In addition, to prevent the models from taking the gender shortcuts, we split the dataset D by gender, and experiments are individually performed on male and fe- male subsets. For each subset, we adopt 7/1/1/1 splitting for D\ud835\udc61 /D\ud835\udc631 /D\ud835\udc632/D\ud835\udc52 . In training, the voice recordings are randomly trimmed to segments of 6 to 8 seconds, while we use the entire recordings in testing. The ground truth AMs are normalized to zero mean and unit variance. For voice features, we extract 64-dimensional log Mel- spectrograms using an analysis window of 25ms, with the hop of 10ms between frames. We perform mean and variance normalization of each Mel-frequency bin. 4.2 We leverage a backbone E to learn voice code \ud835\udc52 which is a sim- ple convolutional neural network. The detailed network structure is presented in the supplementary materials. \ud835\udc39\ud835\udc58 and \ud835\udc3a\ud835\udc58 share the backbone\u2019s learnable parameters but have individual parameters for their heads. We use a single layer fully-connected network for each head. For the variance head, we add an exponential activation to the last layer of \ud835\udc3a\ud835\udc58 for non-negative positive output. We follow the typical settings of stochastic gradient descent (SGD) for optimiza- tion. Minibatch size is 64. The momentum, learning rate, and weight decay values are 0.9, 0.1, and 0.0005, respectively. The training is completed at 5k iterations. Since the phonatory module requires a long training procedure, we first train it with the voice code encoder E for 60k steps on our training set D\ud835\udc61 . We follow the training setting in [18] to train the phonatory module. The other parameter setting follows [18]. We directly normalized the voice signal as input to the network instead of first converting it to Log-Mel spectrum. To ensure statistical significance, we perform N = 100 repeated experiments to compute the \ud835\udc36\ud835\udc3c\ud835\udc62 . For the experiments at phoneme level, we leverage Wav2Vec [2] to cut the long voice recordings into phonemes. Implementation Details Rethinking Voice-Face Correlation: A Geometry View 50% \u02c6\ud835\udc64 0.842\u00b10.030 0.879\u00b10.041 Table 1: Effect of the phonatory module. We measure the normal- ized mean squared error between predicted and ground-truth AM among all AMs with different confidence thresholds. 100% \u02c6\ud835\udc64 0.953\u00b1 0.009 0.952\u00b10.014 75% \u02c6\ud835\udc64 0.909\u00b10.024 0.927\u00b10.030 Phonation Module (cid:33) (cid:37) Female Male Figure 5: Visualization of the predictable AMs. Blue box: male, Red box: female. 4.3 Predictable AM Analysis For AM prediction, the estimation models are trained on D\ud835\udc61 and selected based on their performance on D\ud835\udc631 (hyperparameter tun- ing). For AM selection, the predictable AMs are selected based on the upper bound of the CI (\ud835\udc36\ud835\udc3c\ud835\udc62 ) on D\ud835\udc632. The performance can be evaluated by the mean error of each AM and its CI. Fig. 4 shows the results, including 20 AMs with highest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 and 4 AMs with lowest 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 . The gray bars are the results on the entire validation set D\ud835\udc632, while the red and yellow ones are the results of 75% and 50% voice samples with lowest uncertainty \u02c6\ud835\udc64 on D\ud835\udc632, respectively. The self-constructed female subset has the same size as the male subset. Higher 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 indicates better results and the normalized error of 0 indicates the chance-level performance. As suggested by our hypothesis testing formulation, the AMs with 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0 are considered predictable from voice. In this sense, we have discovered a number of predictable female AMs (see the gray bars and their \ud835\udc36\ud835\udc3c\ud835\udc62 in Fig. 4 (b)). By filtering out the voice samples with high uncertainties, we achieve even higher 1 \u2212\ud835\udc36\ud835\udc3c\ud835\udc62 (see the red and yellow bars and their \ud835\udc36\ud835\udc3c s). The improved performance indicates that more AMs are discovered as predictable from voice. The complete results of all AMs are given in the appendix. The results empirically demonstrate that the information of 3D facial shape is indeed encoded in the voices and can be discovered by our analysis pipeline. To intuitively locate the predictable AMs on the 3D face, we visualize them in Fig. 5. We clearly observe that most of the pre- dictable AMs are around nose and mouth, and many of them are shared between male and female subsets. This is consistent with the fact that nose and mouth shapes affect pronunciation. We also notice that the performance of female subset is much better than that of the male subset. To investigate whether the im- provements come from the larger data scale (364 males \ud835\udc63.\ud835\udc60. 662 females), we perform another set of repeated experiments on a self- constructed female subset, which has the same size as the male Conference\u201923, July 2023, Ottawa, Canada Phonatory Module (cid:33) (cid:37) Predictable Unpredictable 0.990\u00b10.032 0.628\u00b10.021 1.002\u00b10.031 0.730\u00b10.048 Table 2: Effect of phonatory module for predictable and unpre- dictable AMs. We measure the normalized mean squared error between predicted and ground-truth AM among all predictable and unpredictable AMs. Interestingly, we find phonatory mod- ule only improves predictable AMs. subset,i.e., 364 females. Surprisingly, the results on the new subset are still better than those on the male subset, as shown in Fig. 4 (c). This is possible because the female subjects have higher nasalance scores on the nasal sentences [42] among other things, which pro- vides useful information for predicting the AMs around the nose. Here we note that our experiments have revealed that measurements around the nose are highly correlated to voice. More investigations are left for future work. On the other hand, some AMs have not been shown to be pre- dictable from voice. This observation suggests that voices may only associate with a few specific regions of the 3D facial shape, like the nose and mouth. For the AMs with higher errors than chance level,"}