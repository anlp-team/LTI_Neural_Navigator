{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Fernando_Diaz_Fairness_Through_Domain_Awareness:_Mitigating_Popularity_Bias_For_Music_Discovery_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the first step in defining the popularity setting?", "answer": " Counting the number of times each song track appears within the playlist or user training interactions.", "ref_chunk": "leaving some bins empty. Finally, we select 10 bins based on the distribution of the datasets and the formulation of our BOOST methodology (see Section 3.4). We use the following steps to define our popularity setting. First, we count the number of times each song track, \ud835\udc61\ud835\udc56 appears within the playlist (or user) training interactions such that for each \ud835\udc61\ud835\udc56 , \ud835\udc4e\ud835\udc61\ud835\udc56 = \u2223 {\ud835\udc5d\ud835\udc56 \u2236 \ud835\udc61\ud835\udc56 \u2208 \ud835\udc5d\ud835\udc56 } \u2223. Then, we apply the base 10 logarithmic smoothing = log10 (\ud835\udc4e\ud835\udc61\ud835\udc56 ). Finally, to these values such that for each \ud835\udc61\ud835\udc56 , pop\ud835\udc61\ud835\udc56 we apply binning onto these values to split them into 10 groups such that for each \ud835\udc61\ud835\udc56 , pop_bin (\ud835\udc61\ud835\udc56 ) \u2208 {0, . . . , 9} where bin 9 has a higher popularity value than bin 0. The visualization of this binning procedure and its comparison with the long tail method can be seen in Figure 2. As demonstrated by our visualizations, transforming the raw values into the logarithmic space shows that the bins are filled in relatively even intervals, where, as the popularity increases, so does the number of songs included in a bin. We showcase the gains that our method has over the canonical long tail model in Figure 2 where we compare the positioning of our binning methodology with the classic long tail model. Furthermore, as we later show Figures 5 and 6 our formulation of popularity is able to elucidate crucial differences among both the datasets and baseline model performances on these datasets. 3.3.2 Popularity Bias and Music Discovery In addition we formalize the inverse relationship between music discovery and popularity bias. For each song track, \ud835\udc61\ud835\udc56 \u2208 \ud835\udc47\ud835\udc42\ud835\udc3a , we generate a counterfactual example song, \ud835\udc61 \u2217 \ud835\udc56 \u2208 \ud835\udc47\ud835\udc36\ud835\udc39 , where everything about the features is exactly the same and the only difference is that \ud835\udc61\ud835\udc56 has a high degree while \ud835\udc61 \u2217 \ud835\udc56 has a degree of one. We calculate the distance between an original song node, \ud835\udc61\ud835\udc56 and its counterfactual duplicate, \ud835\udc61 \u2217 \ud835\udc56 . A system with high potential for musical discovery will have a low distance between the songs, showing a low popularity bias and an understanding of musical similarity. We will return to this formulation in Section 5.1, showing that a node\u2019s placement and degree in the graph can exacerbate the presence of popularity bias, reflecting itself in the node\u2019s learned representation. 3.4 Mitigating Popularity Bias Through Individual Fairness Ranking-based individual fairness. REDRESS is an individual fairness framework proposed by Dong et al. for learning fair repre- sentations in single node graphs. We extend this framework to the Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi bipartite recommendation setting and integrate it into our popu- larity bias mitigation approach. In the REDRESS setting, individual fairness requires that nodes which were similar in their initial fea- ture space should remain similar in their learned representation embeddings [25]. More concretely, for each song node, \ud835\udc61\ud835\udc56 , and node pair \ud835\udc61\ud835\udc62, \ud835\udc61\ud835\udc63 in a graph \ud835\udc3a, similarity is defined on the basis of the cosine \ud835\udc51 similarity metric, \ud835\udc60(\u22c5, \u22c5), as applied to either a feature \ud835\udc4b [\ud835\udc63] \u2208 R , \ud835\udc5a or learned embedding set, \ud835\udc4d [\ud835\udc63] \u2208 R . Applying this procedure in a pairwise fashion produces two similarity matrices. The first, or apriori similarity, \ud835\udc46\ud835\udc3a , in which similarity is calculated on input fea- tures and the second, or learned similarity, \ud835\udc46\ud835\udc4d , in which similarity is calculated between learned embeddings generated by some GNN model, \ud835\udc40. The purpose of REDRESS is to formulate this as a ranking on the basis of similarity and differentiate on the disparity between differences in rankings of the two representational spaces. Thus, drawing on principles from learn to rank [9], each entry in these similarity matrices is re-cast as the probability that node \ud835\udc61\ud835\udc56 is more similar to node \ud835\udc61\ud835\udc62 than \ud835\udc61\ud835\udc63 and transformed into an apriori proba- bility tensor, \ud835\udc43\ud835\udc3a \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223, and a learned probability tensor, \ud835\udc43\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223. Having defined these two probability tensors, for each individual node the fairness loss, \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ), can be treated the cross entropy loss applied to these probability distributions such that for an individual node, \ud835\udc61\ud835\udc56 : \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) = \u2212\ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56] log \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56] \u2212 (1 \u2212 \ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56]) log(1 \u2212 \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56]) and aggregated over all nodes \ud835\udc61\ud835\udc56 \u2208 \ud835\udc49 as: \ud835\udc3ffairness = \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc56 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc62 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc63 \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) Individually fair music discovery. The original formulation of individual fairness requires some form of domain expertise [25] to determine how (dis-)similar two items are. For the music dis- covery domain, we use music features (see Section 3.1 for exact details) as the basis for calculating cosine similarity. Thus, our apriori similarity, \ud835\udc46\ud835\udc3a , is defined as the cosine similarity between the musical features, \ud835\udc4b [\ud835\udc63] \u2208 R\u2223\ud835\udc47 \u2223\u00d79, associated with song nodes. Meanwhile, our learned similarity contains the song-level embed- dings, \ud835\udc46\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\ud835\udc5a , learned by PinSage. In this way, REDRESS acts as a regularizer that ensures that rank-based similarity between songs is preserved between the input and embedding space. Thus, our similarity notion is domain-aware and grounded in the essence of musical experiences: acoustics. To learn the embedding of songs, we follow the learning par- adigm of PinSage [67] and make a few deviations. Unlike Ying et al., we use uniform random sampling to avoid the computational burden of calculating negative samples on our large graph and compensate for the potential loss of information by using focal loss [43], rather than the margin loss, to train the network. It is important to note that since the potential benefits or drawbacks of PinSage as a general recommender system are out of the scope of this paper, we do not focus on the performance gains that such a change might provide and leave the addition of various negative sampling techniques to future"}, {"question": " How are the values transformed in the popularity setting?", "answer": " The values are transformed into logarithmic space using base 10 logarithmic smoothing.", "ref_chunk": "leaving some bins empty. Finally, we select 10 bins based on the distribution of the datasets and the formulation of our BOOST methodology (see Section 3.4). We use the following steps to define our popularity setting. First, we count the number of times each song track, \ud835\udc61\ud835\udc56 appears within the playlist (or user) training interactions such that for each \ud835\udc61\ud835\udc56 , \ud835\udc4e\ud835\udc61\ud835\udc56 = \u2223 {\ud835\udc5d\ud835\udc56 \u2236 \ud835\udc61\ud835\udc56 \u2208 \ud835\udc5d\ud835\udc56 } \u2223. Then, we apply the base 10 logarithmic smoothing = log10 (\ud835\udc4e\ud835\udc61\ud835\udc56 ). Finally, to these values such that for each \ud835\udc61\ud835\udc56 , pop\ud835\udc61\ud835\udc56 we apply binning onto these values to split them into 10 groups such that for each \ud835\udc61\ud835\udc56 , pop_bin (\ud835\udc61\ud835\udc56 ) \u2208 {0, . . . , 9} where bin 9 has a higher popularity value than bin 0. The visualization of this binning procedure and its comparison with the long tail method can be seen in Figure 2. As demonstrated by our visualizations, transforming the raw values into the logarithmic space shows that the bins are filled in relatively even intervals, where, as the popularity increases, so does the number of songs included in a bin. We showcase the gains that our method has over the canonical long tail model in Figure 2 where we compare the positioning of our binning methodology with the classic long tail model. Furthermore, as we later show Figures 5 and 6 our formulation of popularity is able to elucidate crucial differences among both the datasets and baseline model performances on these datasets. 3.3.2 Popularity Bias and Music Discovery In addition we formalize the inverse relationship between music discovery and popularity bias. For each song track, \ud835\udc61\ud835\udc56 \u2208 \ud835\udc47\ud835\udc42\ud835\udc3a , we generate a counterfactual example song, \ud835\udc61 \u2217 \ud835\udc56 \u2208 \ud835\udc47\ud835\udc36\ud835\udc39 , where everything about the features is exactly the same and the only difference is that \ud835\udc61\ud835\udc56 has a high degree while \ud835\udc61 \u2217 \ud835\udc56 has a degree of one. We calculate the distance between an original song node, \ud835\udc61\ud835\udc56 and its counterfactual duplicate, \ud835\udc61 \u2217 \ud835\udc56 . A system with high potential for musical discovery will have a low distance between the songs, showing a low popularity bias and an understanding of musical similarity. We will return to this formulation in Section 5.1, showing that a node\u2019s placement and degree in the graph can exacerbate the presence of popularity bias, reflecting itself in the node\u2019s learned representation. 3.4 Mitigating Popularity Bias Through Individual Fairness Ranking-based individual fairness. REDRESS is an individual fairness framework proposed by Dong et al. for learning fair repre- sentations in single node graphs. We extend this framework to the Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi bipartite recommendation setting and integrate it into our popu- larity bias mitigation approach. In the REDRESS setting, individual fairness requires that nodes which were similar in their initial fea- ture space should remain similar in their learned representation embeddings [25]. More concretely, for each song node, \ud835\udc61\ud835\udc56 , and node pair \ud835\udc61\ud835\udc62, \ud835\udc61\ud835\udc63 in a graph \ud835\udc3a, similarity is defined on the basis of the cosine \ud835\udc51 similarity metric, \ud835\udc60(\u22c5, \u22c5), as applied to either a feature \ud835\udc4b [\ud835\udc63] \u2208 R , \ud835\udc5a or learned embedding set, \ud835\udc4d [\ud835\udc63] \u2208 R . Applying this procedure in a pairwise fashion produces two similarity matrices. The first, or apriori similarity, \ud835\udc46\ud835\udc3a , in which similarity is calculated on input fea- tures and the second, or learned similarity, \ud835\udc46\ud835\udc4d , in which similarity is calculated between learned embeddings generated by some GNN model, \ud835\udc40. The purpose of REDRESS is to formulate this as a ranking on the basis of similarity and differentiate on the disparity between differences in rankings of the two representational spaces. Thus, drawing on principles from learn to rank [9], each entry in these similarity matrices is re-cast as the probability that node \ud835\udc61\ud835\udc56 is more similar to node \ud835\udc61\ud835\udc62 than \ud835\udc61\ud835\udc63 and transformed into an apriori proba- bility tensor, \ud835\udc43\ud835\udc3a \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223, and a learned probability tensor, \ud835\udc43\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223. Having defined these two probability tensors, for each individual node the fairness loss, \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ), can be treated the cross entropy loss applied to these probability distributions such that for an individual node, \ud835\udc61\ud835\udc56 : \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) = \u2212\ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56] log \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56] \u2212 (1 \u2212 \ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56]) log(1 \u2212 \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56]) and aggregated over all nodes \ud835\udc61\ud835\udc56 \u2208 \ud835\udc49 as: \ud835\udc3ffairness = \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc56 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc62 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc63 \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) Individually fair music discovery. The original formulation of individual fairness requires some form of domain expertise [25] to determine how (dis-)similar two items are. For the music dis- covery domain, we use music features (see Section 3.1 for exact details) as the basis for calculating cosine similarity. Thus, our apriori similarity, \ud835\udc46\ud835\udc3a , is defined as the cosine similarity between the musical features, \ud835\udc4b [\ud835\udc63] \u2208 R\u2223\ud835\udc47 \u2223\u00d79, associated with song nodes. Meanwhile, our learned similarity contains the song-level embed- dings, \ud835\udc46\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\ud835\udc5a , learned by PinSage. In this way, REDRESS acts as a regularizer that ensures that rank-based similarity between songs is preserved between the input and embedding space. Thus, our similarity notion is domain-aware and grounded in the essence of musical experiences: acoustics. To learn the embedding of songs, we follow the learning par- adigm of PinSage [67] and make a few deviations. Unlike Ying et al., we use uniform random sampling to avoid the computational burden of calculating negative samples on our large graph and compensate for the potential loss of information by using focal loss [43], rather than the margin loss, to train the network. It is important to note that since the potential benefits or drawbacks of PinSage as a general recommender system are out of the scope of this paper, we do not focus on the performance gains that such a change might provide and leave the addition of various negative sampling techniques to future"}, {"question": " How many bins are the values split into during the binning process?", "answer": " The values are split into 10 groups or bins.", "ref_chunk": "leaving some bins empty. Finally, we select 10 bins based on the distribution of the datasets and the formulation of our BOOST methodology (see Section 3.4). We use the following steps to define our popularity setting. First, we count the number of times each song track, \ud835\udc61\ud835\udc56 appears within the playlist (or user) training interactions such that for each \ud835\udc61\ud835\udc56 , \ud835\udc4e\ud835\udc61\ud835\udc56 = \u2223 {\ud835\udc5d\ud835\udc56 \u2236 \ud835\udc61\ud835\udc56 \u2208 \ud835\udc5d\ud835\udc56 } \u2223. Then, we apply the base 10 logarithmic smoothing = log10 (\ud835\udc4e\ud835\udc61\ud835\udc56 ). Finally, to these values such that for each \ud835\udc61\ud835\udc56 , pop\ud835\udc61\ud835\udc56 we apply binning onto these values to split them into 10 groups such that for each \ud835\udc61\ud835\udc56 , pop_bin (\ud835\udc61\ud835\udc56 ) \u2208 {0, . . . , 9} where bin 9 has a higher popularity value than bin 0. The visualization of this binning procedure and its comparison with the long tail method can be seen in Figure 2. As demonstrated by our visualizations, transforming the raw values into the logarithmic space shows that the bins are filled in relatively even intervals, where, as the popularity increases, so does the number of songs included in a bin. We showcase the gains that our method has over the canonical long tail model in Figure 2 where we compare the positioning of our binning methodology with the classic long tail model. Furthermore, as we later show Figures 5 and 6 our formulation of popularity is able to elucidate crucial differences among both the datasets and baseline model performances on these datasets. 3.3.2 Popularity Bias and Music Discovery In addition we formalize the inverse relationship between music discovery and popularity bias. For each song track, \ud835\udc61\ud835\udc56 \u2208 \ud835\udc47\ud835\udc42\ud835\udc3a , we generate a counterfactual example song, \ud835\udc61 \u2217 \ud835\udc56 \u2208 \ud835\udc47\ud835\udc36\ud835\udc39 , where everything about the features is exactly the same and the only difference is that \ud835\udc61\ud835\udc56 has a high degree while \ud835\udc61 \u2217 \ud835\udc56 has a degree of one. We calculate the distance between an original song node, \ud835\udc61\ud835\udc56 and its counterfactual duplicate, \ud835\udc61 \u2217 \ud835\udc56 . A system with high potential for musical discovery will have a low distance between the songs, showing a low popularity bias and an understanding of musical similarity. We will return to this formulation in Section 5.1, showing that a node\u2019s placement and degree in the graph can exacerbate the presence of popularity bias, reflecting itself in the node\u2019s learned representation. 3.4 Mitigating Popularity Bias Through Individual Fairness Ranking-based individual fairness. REDRESS is an individual fairness framework proposed by Dong et al. for learning fair repre- sentations in single node graphs. We extend this framework to the Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi bipartite recommendation setting and integrate it into our popu- larity bias mitigation approach. In the REDRESS setting, individual fairness requires that nodes which were similar in their initial fea- ture space should remain similar in their learned representation embeddings [25]. More concretely, for each song node, \ud835\udc61\ud835\udc56 , and node pair \ud835\udc61\ud835\udc62, \ud835\udc61\ud835\udc63 in a graph \ud835\udc3a, similarity is defined on the basis of the cosine \ud835\udc51 similarity metric, \ud835\udc60(\u22c5, \u22c5), as applied to either a feature \ud835\udc4b [\ud835\udc63] \u2208 R , \ud835\udc5a or learned embedding set, \ud835\udc4d [\ud835\udc63] \u2208 R . Applying this procedure in a pairwise fashion produces two similarity matrices. The first, or apriori similarity, \ud835\udc46\ud835\udc3a , in which similarity is calculated on input fea- tures and the second, or learned similarity, \ud835\udc46\ud835\udc4d , in which similarity is calculated between learned embeddings generated by some GNN model, \ud835\udc40. The purpose of REDRESS is to formulate this as a ranking on the basis of similarity and differentiate on the disparity between differences in rankings of the two representational spaces. Thus, drawing on principles from learn to rank [9], each entry in these similarity matrices is re-cast as the probability that node \ud835\udc61\ud835\udc56 is more similar to node \ud835\udc61\ud835\udc62 than \ud835\udc61\ud835\udc63 and transformed into an apriori proba- bility tensor, \ud835\udc43\ud835\udc3a \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223, and a learned probability tensor, \ud835\udc43\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223. Having defined these two probability tensors, for each individual node the fairness loss, \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ), can be treated the cross entropy loss applied to these probability distributions such that for an individual node, \ud835\udc61\ud835\udc56 : \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) = \u2212\ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56] log \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56] \u2212 (1 \u2212 \ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56]) log(1 \u2212 \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56]) and aggregated over all nodes \ud835\udc61\ud835\udc56 \u2208 \ud835\udc49 as: \ud835\udc3ffairness = \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc56 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc62 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc63 \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) Individually fair music discovery. The original formulation of individual fairness requires some form of domain expertise [25] to determine how (dis-)similar two items are. For the music dis- covery domain, we use music features (see Section 3.1 for exact details) as the basis for calculating cosine similarity. Thus, our apriori similarity, \ud835\udc46\ud835\udc3a , is defined as the cosine similarity between the musical features, \ud835\udc4b [\ud835\udc63] \u2208 R\u2223\ud835\udc47 \u2223\u00d79, associated with song nodes. Meanwhile, our learned similarity contains the song-level embed- dings, \ud835\udc46\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\ud835\udc5a , learned by PinSage. In this way, REDRESS acts as a regularizer that ensures that rank-based similarity between songs is preserved between the input and embedding space. Thus, our similarity notion is domain-aware and grounded in the essence of musical experiences: acoustics. To learn the embedding of songs, we follow the learning par- adigm of PinSage [67] and make a few deviations. Unlike Ying et al., we use uniform random sampling to avoid the computational burden of calculating negative samples on our large graph and compensate for the potential loss of information by using focal loss [43], rather than the margin loss, to train the network. It is important to note that since the potential benefits or drawbacks of PinSage as a general recommender system are out of the scope of this paper, we do not focus on the performance gains that such a change might provide and leave the addition of various negative sampling techniques to future"}, {"question": " What is the purpose of the binning procedure mentioned in the text?", "answer": " To split the values into different groups based on popularity levels.", "ref_chunk": "leaving some bins empty. Finally, we select 10 bins based on the distribution of the datasets and the formulation of our BOOST methodology (see Section 3.4). We use the following steps to define our popularity setting. First, we count the number of times each song track, \ud835\udc61\ud835\udc56 appears within the playlist (or user) training interactions such that for each \ud835\udc61\ud835\udc56 , \ud835\udc4e\ud835\udc61\ud835\udc56 = \u2223 {\ud835\udc5d\ud835\udc56 \u2236 \ud835\udc61\ud835\udc56 \u2208 \ud835\udc5d\ud835\udc56 } \u2223. Then, we apply the base 10 logarithmic smoothing = log10 (\ud835\udc4e\ud835\udc61\ud835\udc56 ). Finally, to these values such that for each \ud835\udc61\ud835\udc56 , pop\ud835\udc61\ud835\udc56 we apply binning onto these values to split them into 10 groups such that for each \ud835\udc61\ud835\udc56 , pop_bin (\ud835\udc61\ud835\udc56 ) \u2208 {0, . . . , 9} where bin 9 has a higher popularity value than bin 0. The visualization of this binning procedure and its comparison with the long tail method can be seen in Figure 2. As demonstrated by our visualizations, transforming the raw values into the logarithmic space shows that the bins are filled in relatively even intervals, where, as the popularity increases, so does the number of songs included in a bin. We showcase the gains that our method has over the canonical long tail model in Figure 2 where we compare the positioning of our binning methodology with the classic long tail model. Furthermore, as we later show Figures 5 and 6 our formulation of popularity is able to elucidate crucial differences among both the datasets and baseline model performances on these datasets. 3.3.2 Popularity Bias and Music Discovery In addition we formalize the inverse relationship between music discovery and popularity bias. For each song track, \ud835\udc61\ud835\udc56 \u2208 \ud835\udc47\ud835\udc42\ud835\udc3a , we generate a counterfactual example song, \ud835\udc61 \u2217 \ud835\udc56 \u2208 \ud835\udc47\ud835\udc36\ud835\udc39 , where everything about the features is exactly the same and the only difference is that \ud835\udc61\ud835\udc56 has a high degree while \ud835\udc61 \u2217 \ud835\udc56 has a degree of one. We calculate the distance between an original song node, \ud835\udc61\ud835\udc56 and its counterfactual duplicate, \ud835\udc61 \u2217 \ud835\udc56 . A system with high potential for musical discovery will have a low distance between the songs, showing a low popularity bias and an understanding of musical similarity. We will return to this formulation in Section 5.1, showing that a node\u2019s placement and degree in the graph can exacerbate the presence of popularity bias, reflecting itself in the node\u2019s learned representation. 3.4 Mitigating Popularity Bias Through Individual Fairness Ranking-based individual fairness. REDRESS is an individual fairness framework proposed by Dong et al. for learning fair repre- sentations in single node graphs. We extend this framework to the Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi bipartite recommendation setting and integrate it into our popu- larity bias mitigation approach. In the REDRESS setting, individual fairness requires that nodes which were similar in their initial fea- ture space should remain similar in their learned representation embeddings [25]. More concretely, for each song node, \ud835\udc61\ud835\udc56 , and node pair \ud835\udc61\ud835\udc62, \ud835\udc61\ud835\udc63 in a graph \ud835\udc3a, similarity is defined on the basis of the cosine \ud835\udc51 similarity metric, \ud835\udc60(\u22c5, \u22c5), as applied to either a feature \ud835\udc4b [\ud835\udc63] \u2208 R , \ud835\udc5a or learned embedding set, \ud835\udc4d [\ud835\udc63] \u2208 R . Applying this procedure in a pairwise fashion produces two similarity matrices. The first, or apriori similarity, \ud835\udc46\ud835\udc3a , in which similarity is calculated on input fea- tures and the second, or learned similarity, \ud835\udc46\ud835\udc4d , in which similarity is calculated between learned embeddings generated by some GNN model, \ud835\udc40. The purpose of REDRESS is to formulate this as a ranking on the basis of similarity and differentiate on the disparity between differences in rankings of the two representational spaces. Thus, drawing on principles from learn to rank [9], each entry in these similarity matrices is re-cast as the probability that node \ud835\udc61\ud835\udc56 is more similar to node \ud835\udc61\ud835\udc62 than \ud835\udc61\ud835\udc63 and transformed into an apriori proba- bility tensor, \ud835\udc43\ud835\udc3a \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223, and a learned probability tensor, \ud835\udc43\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223. Having defined these two probability tensors, for each individual node the fairness loss, \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ), can be treated the cross entropy loss applied to these probability distributions such that for an individual node, \ud835\udc61\ud835\udc56 : \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) = \u2212\ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56] log \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56] \u2212 (1 \u2212 \ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56]) log(1 \u2212 \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56]) and aggregated over all nodes \ud835\udc61\ud835\udc56 \u2208 \ud835\udc49 as: \ud835\udc3ffairness = \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc56 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc62 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc63 \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) Individually fair music discovery. The original formulation of individual fairness requires some form of domain expertise [25] to determine how (dis-)similar two items are. For the music dis- covery domain, we use music features (see Section 3.1 for exact details) as the basis for calculating cosine similarity. Thus, our apriori similarity, \ud835\udc46\ud835\udc3a , is defined as the cosine similarity between the musical features, \ud835\udc4b [\ud835\udc63] \u2208 R\u2223\ud835\udc47 \u2223\u00d79, associated with song nodes. Meanwhile, our learned similarity contains the song-level embed- dings, \ud835\udc46\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\ud835\udc5a , learned by PinSage. In this way, REDRESS acts as a regularizer that ensures that rank-based similarity between songs is preserved between the input and embedding space. Thus, our similarity notion is domain-aware and grounded in the essence of musical experiences: acoustics. To learn the embedding of songs, we follow the learning par- adigm of PinSage [67] and make a few deviations. Unlike Ying et al., we use uniform random sampling to avoid the computational burden of calculating negative samples on our large graph and compensate for the potential loss of information by using focal loss [43], rather than the margin loss, to train the network. It is important to note that since the potential benefits or drawbacks of PinSage as a general recommender system are out of the scope of this paper, we do not focus on the performance gains that such a change might provide and leave the addition of various negative sampling techniques to future"}, {"question": " What is the relationship between popularity and the number of songs included in a bin?", "answer": " As popularity increases, the number of songs included in a bin also increases.", "ref_chunk": "leaving some bins empty. Finally, we select 10 bins based on the distribution of the datasets and the formulation of our BOOST methodology (see Section 3.4). We use the following steps to define our popularity setting. First, we count the number of times each song track, \ud835\udc61\ud835\udc56 appears within the playlist (or user) training interactions such that for each \ud835\udc61\ud835\udc56 , \ud835\udc4e\ud835\udc61\ud835\udc56 = \u2223 {\ud835\udc5d\ud835\udc56 \u2236 \ud835\udc61\ud835\udc56 \u2208 \ud835\udc5d\ud835\udc56 } \u2223. Then, we apply the base 10 logarithmic smoothing = log10 (\ud835\udc4e\ud835\udc61\ud835\udc56 ). Finally, to these values such that for each \ud835\udc61\ud835\udc56 , pop\ud835\udc61\ud835\udc56 we apply binning onto these values to split them into 10 groups such that for each \ud835\udc61\ud835\udc56 , pop_bin (\ud835\udc61\ud835\udc56 ) \u2208 {0, . . . , 9} where bin 9 has a higher popularity value than bin 0. The visualization of this binning procedure and its comparison with the long tail method can be seen in Figure 2. As demonstrated by our visualizations, transforming the raw values into the logarithmic space shows that the bins are filled in relatively even intervals, where, as the popularity increases, so does the number of songs included in a bin. We showcase the gains that our method has over the canonical long tail model in Figure 2 where we compare the positioning of our binning methodology with the classic long tail model. Furthermore, as we later show Figures 5 and 6 our formulation of popularity is able to elucidate crucial differences among both the datasets and baseline model performances on these datasets. 3.3.2 Popularity Bias and Music Discovery In addition we formalize the inverse relationship between music discovery and popularity bias. For each song track, \ud835\udc61\ud835\udc56 \u2208 \ud835\udc47\ud835\udc42\ud835\udc3a , we generate a counterfactual example song, \ud835\udc61 \u2217 \ud835\udc56 \u2208 \ud835\udc47\ud835\udc36\ud835\udc39 , where everything about the features is exactly the same and the only difference is that \ud835\udc61\ud835\udc56 has a high degree while \ud835\udc61 \u2217 \ud835\udc56 has a degree of one. We calculate the distance between an original song node, \ud835\udc61\ud835\udc56 and its counterfactual duplicate, \ud835\udc61 \u2217 \ud835\udc56 . A system with high potential for musical discovery will have a low distance between the songs, showing a low popularity bias and an understanding of musical similarity. We will return to this formulation in Section 5.1, showing that a node\u2019s placement and degree in the graph can exacerbate the presence of popularity bias, reflecting itself in the node\u2019s learned representation. 3.4 Mitigating Popularity Bias Through Individual Fairness Ranking-based individual fairness. REDRESS is an individual fairness framework proposed by Dong et al. for learning fair repre- sentations in single node graphs. We extend this framework to the Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi bipartite recommendation setting and integrate it into our popu- larity bias mitigation approach. In the REDRESS setting, individual fairness requires that nodes which were similar in their initial fea- ture space should remain similar in their learned representation embeddings [25]. More concretely, for each song node, \ud835\udc61\ud835\udc56 , and node pair \ud835\udc61\ud835\udc62, \ud835\udc61\ud835\udc63 in a graph \ud835\udc3a, similarity is defined on the basis of the cosine \ud835\udc51 similarity metric, \ud835\udc60(\u22c5, \u22c5), as applied to either a feature \ud835\udc4b [\ud835\udc63] \u2208 R , \ud835\udc5a or learned embedding set, \ud835\udc4d [\ud835\udc63] \u2208 R . Applying this procedure in a pairwise fashion produces two similarity matrices. The first, or apriori similarity, \ud835\udc46\ud835\udc3a , in which similarity is calculated on input fea- tures and the second, or learned similarity, \ud835\udc46\ud835\udc4d , in which similarity is calculated between learned embeddings generated by some GNN model, \ud835\udc40. The purpose of REDRESS is to formulate this as a ranking on the basis of similarity and differentiate on the disparity between differences in rankings of the two representational spaces. Thus, drawing on principles from learn to rank [9], each entry in these similarity matrices is re-cast as the probability that node \ud835\udc61\ud835\udc56 is more similar to node \ud835\udc61\ud835\udc62 than \ud835\udc61\ud835\udc63 and transformed into an apriori proba- bility tensor, \ud835\udc43\ud835\udc3a \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223, and a learned probability tensor, \ud835\udc43\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223. Having defined these two probability tensors, for each individual node the fairness loss, \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ), can be treated the cross entropy loss applied to these probability distributions such that for an individual node, \ud835\udc61\ud835\udc56 : \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) = \u2212\ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56] log \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56] \u2212 (1 \u2212 \ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56]) log(1 \u2212 \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56]) and aggregated over all nodes \ud835\udc61\ud835\udc56 \u2208 \ud835\udc49 as: \ud835\udc3ffairness = \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc56 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc62 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc63 \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) Individually fair music discovery. The original formulation of individual fairness requires some form of domain expertise [25] to determine how (dis-)similar two items are. For the music dis- covery domain, we use music features (see Section 3.1 for exact details) as the basis for calculating cosine similarity. Thus, our apriori similarity, \ud835\udc46\ud835\udc3a , is defined as the cosine similarity between the musical features, \ud835\udc4b [\ud835\udc63] \u2208 R\u2223\ud835\udc47 \u2223\u00d79, associated with song nodes. Meanwhile, our learned similarity contains the song-level embed- dings, \ud835\udc46\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\ud835\udc5a , learned by PinSage. In this way, REDRESS acts as a regularizer that ensures that rank-based similarity between songs is preserved between the input and embedding space. Thus, our similarity notion is domain-aware and grounded in the essence of musical experiences: acoustics. To learn the embedding of songs, we follow the learning par- adigm of PinSage [67] and make a few deviations. Unlike Ying et al., we use uniform random sampling to avoid the computational burden of calculating negative samples on our large graph and compensate for the potential loss of information by using focal loss [43], rather than the margin loss, to train the network. It is important to note that since the potential benefits or drawbacks of PinSage as a general recommender system are out of the scope of this paper, we do not focus on the performance gains that such a change might provide and leave the addition of various negative sampling techniques to future"}, {"question": " What method is compared with the binning methodology?", "answer": " The long tail model.", "ref_chunk": "leaving some bins empty. Finally, we select 10 bins based on the distribution of the datasets and the formulation of our BOOST methodology (see Section 3.4). We use the following steps to define our popularity setting. First, we count the number of times each song track, \ud835\udc61\ud835\udc56 appears within the playlist (or user) training interactions such that for each \ud835\udc61\ud835\udc56 , \ud835\udc4e\ud835\udc61\ud835\udc56 = \u2223 {\ud835\udc5d\ud835\udc56 \u2236 \ud835\udc61\ud835\udc56 \u2208 \ud835\udc5d\ud835\udc56 } \u2223. Then, we apply the base 10 logarithmic smoothing = log10 (\ud835\udc4e\ud835\udc61\ud835\udc56 ). Finally, to these values such that for each \ud835\udc61\ud835\udc56 , pop\ud835\udc61\ud835\udc56 we apply binning onto these values to split them into 10 groups such that for each \ud835\udc61\ud835\udc56 , pop_bin (\ud835\udc61\ud835\udc56 ) \u2208 {0, . . . , 9} where bin 9 has a higher popularity value than bin 0. The visualization of this binning procedure and its comparison with the long tail method can be seen in Figure 2. As demonstrated by our visualizations, transforming the raw values into the logarithmic space shows that the bins are filled in relatively even intervals, where, as the popularity increases, so does the number of songs included in a bin. We showcase the gains that our method has over the canonical long tail model in Figure 2 where we compare the positioning of our binning methodology with the classic long tail model. Furthermore, as we later show Figures 5 and 6 our formulation of popularity is able to elucidate crucial differences among both the datasets and baseline model performances on these datasets. 3.3.2 Popularity Bias and Music Discovery In addition we formalize the inverse relationship between music discovery and popularity bias. For each song track, \ud835\udc61\ud835\udc56 \u2208 \ud835\udc47\ud835\udc42\ud835\udc3a , we generate a counterfactual example song, \ud835\udc61 \u2217 \ud835\udc56 \u2208 \ud835\udc47\ud835\udc36\ud835\udc39 , where everything about the features is exactly the same and the only difference is that \ud835\udc61\ud835\udc56 has a high degree while \ud835\udc61 \u2217 \ud835\udc56 has a degree of one. We calculate the distance between an original song node, \ud835\udc61\ud835\udc56 and its counterfactual duplicate, \ud835\udc61 \u2217 \ud835\udc56 . A system with high potential for musical discovery will have a low distance between the songs, showing a low popularity bias and an understanding of musical similarity. We will return to this formulation in Section 5.1, showing that a node\u2019s placement and degree in the graph can exacerbate the presence of popularity bias, reflecting itself in the node\u2019s learned representation. 3.4 Mitigating Popularity Bias Through Individual Fairness Ranking-based individual fairness. REDRESS is an individual fairness framework proposed by Dong et al. for learning fair repre- sentations in single node graphs. We extend this framework to the Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi bipartite recommendation setting and integrate it into our popu- larity bias mitigation approach. In the REDRESS setting, individual fairness requires that nodes which were similar in their initial fea- ture space should remain similar in their learned representation embeddings [25]. More concretely, for each song node, \ud835\udc61\ud835\udc56 , and node pair \ud835\udc61\ud835\udc62, \ud835\udc61\ud835\udc63 in a graph \ud835\udc3a, similarity is defined on the basis of the cosine \ud835\udc51 similarity metric, \ud835\udc60(\u22c5, \u22c5), as applied to either a feature \ud835\udc4b [\ud835\udc63] \u2208 R , \ud835\udc5a or learned embedding set, \ud835\udc4d [\ud835\udc63] \u2208 R . Applying this procedure in a pairwise fashion produces two similarity matrices. The first, or apriori similarity, \ud835\udc46\ud835\udc3a , in which similarity is calculated on input fea- tures and the second, or learned similarity, \ud835\udc46\ud835\udc4d , in which similarity is calculated between learned embeddings generated by some GNN model, \ud835\udc40. The purpose of REDRESS is to formulate this as a ranking on the basis of similarity and differentiate on the disparity between differences in rankings of the two representational spaces. Thus, drawing on principles from learn to rank [9], each entry in these similarity matrices is re-cast as the probability that node \ud835\udc61\ud835\udc56 is more similar to node \ud835\udc61\ud835\udc62 than \ud835\udc61\ud835\udc63 and transformed into an apriori proba- bility tensor, \ud835\udc43\ud835\udc3a \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223, and a learned probability tensor, \ud835\udc43\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223. Having defined these two probability tensors, for each individual node the fairness loss, \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ), can be treated the cross entropy loss applied to these probability distributions such that for an individual node, \ud835\udc61\ud835\udc56 : \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) = \u2212\ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56] log \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56] \u2212 (1 \u2212 \ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56]) log(1 \u2212 \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56]) and aggregated over all nodes \ud835\udc61\ud835\udc56 \u2208 \ud835\udc49 as: \ud835\udc3ffairness = \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc56 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc62 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc63 \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) Individually fair music discovery. The original formulation of individual fairness requires some form of domain expertise [25] to determine how (dis-)similar two items are. For the music dis- covery domain, we use music features (see Section 3.1 for exact details) as the basis for calculating cosine similarity. Thus, our apriori similarity, \ud835\udc46\ud835\udc3a , is defined as the cosine similarity between the musical features, \ud835\udc4b [\ud835\udc63] \u2208 R\u2223\ud835\udc47 \u2223\u00d79, associated with song nodes. Meanwhile, our learned similarity contains the song-level embed- dings, \ud835\udc46\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\ud835\udc5a , learned by PinSage. In this way, REDRESS acts as a regularizer that ensures that rank-based similarity between songs is preserved between the input and embedding space. Thus, our similarity notion is domain-aware and grounded in the essence of musical experiences: acoustics. To learn the embedding of songs, we follow the learning par- adigm of PinSage [67] and make a few deviations. Unlike Ying et al., we use uniform random sampling to avoid the computational burden of calculating negative samples on our large graph and compensate for the potential loss of information by using focal loss [43], rather than the margin loss, to train the network. It is important to note that since the potential benefits or drawbacks of PinSage as a general recommender system are out of the scope of this paper, we do not focus on the performance gains that such a change might provide and leave the addition of various negative sampling techniques to future"}, {"question": " How does the text describe the gains of the binning methodology over the long tail model?", "answer": " It showcases the gains by comparing the positioning of the methodologies.", "ref_chunk": "leaving some bins empty. Finally, we select 10 bins based on the distribution of the datasets and the formulation of our BOOST methodology (see Section 3.4). We use the following steps to define our popularity setting. First, we count the number of times each song track, \ud835\udc61\ud835\udc56 appears within the playlist (or user) training interactions such that for each \ud835\udc61\ud835\udc56 , \ud835\udc4e\ud835\udc61\ud835\udc56 = \u2223 {\ud835\udc5d\ud835\udc56 \u2236 \ud835\udc61\ud835\udc56 \u2208 \ud835\udc5d\ud835\udc56 } \u2223. Then, we apply the base 10 logarithmic smoothing = log10 (\ud835\udc4e\ud835\udc61\ud835\udc56 ). Finally, to these values such that for each \ud835\udc61\ud835\udc56 , pop\ud835\udc61\ud835\udc56 we apply binning onto these values to split them into 10 groups such that for each \ud835\udc61\ud835\udc56 , pop_bin (\ud835\udc61\ud835\udc56 ) \u2208 {0, . . . , 9} where bin 9 has a higher popularity value than bin 0. The visualization of this binning procedure and its comparison with the long tail method can be seen in Figure 2. As demonstrated by our visualizations, transforming the raw values into the logarithmic space shows that the bins are filled in relatively even intervals, where, as the popularity increases, so does the number of songs included in a bin. We showcase the gains that our method has over the canonical long tail model in Figure 2 where we compare the positioning of our binning methodology with the classic long tail model. Furthermore, as we later show Figures 5 and 6 our formulation of popularity is able to elucidate crucial differences among both the datasets and baseline model performances on these datasets. 3.3.2 Popularity Bias and Music Discovery In addition we formalize the inverse relationship between music discovery and popularity bias. For each song track, \ud835\udc61\ud835\udc56 \u2208 \ud835\udc47\ud835\udc42\ud835\udc3a , we generate a counterfactual example song, \ud835\udc61 \u2217 \ud835\udc56 \u2208 \ud835\udc47\ud835\udc36\ud835\udc39 , where everything about the features is exactly the same and the only difference is that \ud835\udc61\ud835\udc56 has a high degree while \ud835\udc61 \u2217 \ud835\udc56 has a degree of one. We calculate the distance between an original song node, \ud835\udc61\ud835\udc56 and its counterfactual duplicate, \ud835\udc61 \u2217 \ud835\udc56 . A system with high potential for musical discovery will have a low distance between the songs, showing a low popularity bias and an understanding of musical similarity. We will return to this formulation in Section 5.1, showing that a node\u2019s placement and degree in the graph can exacerbate the presence of popularity bias, reflecting itself in the node\u2019s learned representation. 3.4 Mitigating Popularity Bias Through Individual Fairness Ranking-based individual fairness. REDRESS is an individual fairness framework proposed by Dong et al. for learning fair repre- sentations in single node graphs. We extend this framework to the Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi bipartite recommendation setting and integrate it into our popu- larity bias mitigation approach. In the REDRESS setting, individual fairness requires that nodes which were similar in their initial fea- ture space should remain similar in their learned representation embeddings [25]. More concretely, for each song node, \ud835\udc61\ud835\udc56 , and node pair \ud835\udc61\ud835\udc62, \ud835\udc61\ud835\udc63 in a graph \ud835\udc3a, similarity is defined on the basis of the cosine \ud835\udc51 similarity metric, \ud835\udc60(\u22c5, \u22c5), as applied to either a feature \ud835\udc4b [\ud835\udc63] \u2208 R , \ud835\udc5a or learned embedding set, \ud835\udc4d [\ud835\udc63] \u2208 R . Applying this procedure in a pairwise fashion produces two similarity matrices. The first, or apriori similarity, \ud835\udc46\ud835\udc3a , in which similarity is calculated on input fea- tures and the second, or learned similarity, \ud835\udc46\ud835\udc4d , in which similarity is calculated between learned embeddings generated by some GNN model, \ud835\udc40. The purpose of REDRESS is to formulate this as a ranking on the basis of similarity and differentiate on the disparity between differences in rankings of the two representational spaces. Thus, drawing on principles from learn to rank [9], each entry in these similarity matrices is re-cast as the probability that node \ud835\udc61\ud835\udc56 is more similar to node \ud835\udc61\ud835\udc62 than \ud835\udc61\ud835\udc63 and transformed into an apriori proba- bility tensor, \ud835\udc43\ud835\udc3a \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223, and a learned probability tensor, \ud835\udc43\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223. Having defined these two probability tensors, for each individual node the fairness loss, \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ), can be treated the cross entropy loss applied to these probability distributions such that for an individual node, \ud835\udc61\ud835\udc56 : \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) = \u2212\ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56] log \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56] \u2212 (1 \u2212 \ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56]) log(1 \u2212 \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56]) and aggregated over all nodes \ud835\udc61\ud835\udc56 \u2208 \ud835\udc49 as: \ud835\udc3ffairness = \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc56 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc62 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc63 \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) Individually fair music discovery. The original formulation of individual fairness requires some form of domain expertise [25] to determine how (dis-)similar two items are. For the music dis- covery domain, we use music features (see Section 3.1 for exact details) as the basis for calculating cosine similarity. Thus, our apriori similarity, \ud835\udc46\ud835\udc3a , is defined as the cosine similarity between the musical features, \ud835\udc4b [\ud835\udc63] \u2208 R\u2223\ud835\udc47 \u2223\u00d79, associated with song nodes. Meanwhile, our learned similarity contains the song-level embed- dings, \ud835\udc46\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\ud835\udc5a , learned by PinSage. In this way, REDRESS acts as a regularizer that ensures that rank-based similarity between songs is preserved between the input and embedding space. Thus, our similarity notion is domain-aware and grounded in the essence of musical experiences: acoustics. To learn the embedding of songs, we follow the learning par- adigm of PinSage [67] and make a few deviations. Unlike Ying et al., we use uniform random sampling to avoid the computational burden of calculating negative samples on our large graph and compensate for the potential loss of information by using focal loss [43], rather than the margin loss, to train the network. It is important to note that since the potential benefits or drawbacks of PinSage as a general recommender system are out of the scope of this paper, we do not focus on the performance gains that such a change might provide and leave the addition of various negative sampling techniques to future"}, {"question": " What is the counterfactual example song generated for each song track?", "answer": " A song with the same features but a degree of one.", "ref_chunk": "leaving some bins empty. Finally, we select 10 bins based on the distribution of the datasets and the formulation of our BOOST methodology (see Section 3.4). We use the following steps to define our popularity setting. First, we count the number of times each song track, \ud835\udc61\ud835\udc56 appears within the playlist (or user) training interactions such that for each \ud835\udc61\ud835\udc56 , \ud835\udc4e\ud835\udc61\ud835\udc56 = \u2223 {\ud835\udc5d\ud835\udc56 \u2236 \ud835\udc61\ud835\udc56 \u2208 \ud835\udc5d\ud835\udc56 } \u2223. Then, we apply the base 10 logarithmic smoothing = log10 (\ud835\udc4e\ud835\udc61\ud835\udc56 ). Finally, to these values such that for each \ud835\udc61\ud835\udc56 , pop\ud835\udc61\ud835\udc56 we apply binning onto these values to split them into 10 groups such that for each \ud835\udc61\ud835\udc56 , pop_bin (\ud835\udc61\ud835\udc56 ) \u2208 {0, . . . , 9} where bin 9 has a higher popularity value than bin 0. The visualization of this binning procedure and its comparison with the long tail method can be seen in Figure 2. As demonstrated by our visualizations, transforming the raw values into the logarithmic space shows that the bins are filled in relatively even intervals, where, as the popularity increases, so does the number of songs included in a bin. We showcase the gains that our method has over the canonical long tail model in Figure 2 where we compare the positioning of our binning methodology with the classic long tail model. Furthermore, as we later show Figures 5 and 6 our formulation of popularity is able to elucidate crucial differences among both the datasets and baseline model performances on these datasets. 3.3.2 Popularity Bias and Music Discovery In addition we formalize the inverse relationship between music discovery and popularity bias. For each song track, \ud835\udc61\ud835\udc56 \u2208 \ud835\udc47\ud835\udc42\ud835\udc3a , we generate a counterfactual example song, \ud835\udc61 \u2217 \ud835\udc56 \u2208 \ud835\udc47\ud835\udc36\ud835\udc39 , where everything about the features is exactly the same and the only difference is that \ud835\udc61\ud835\udc56 has a high degree while \ud835\udc61 \u2217 \ud835\udc56 has a degree of one. We calculate the distance between an original song node, \ud835\udc61\ud835\udc56 and its counterfactual duplicate, \ud835\udc61 \u2217 \ud835\udc56 . A system with high potential for musical discovery will have a low distance between the songs, showing a low popularity bias and an understanding of musical similarity. We will return to this formulation in Section 5.1, showing that a node\u2019s placement and degree in the graph can exacerbate the presence of popularity bias, reflecting itself in the node\u2019s learned representation. 3.4 Mitigating Popularity Bias Through Individual Fairness Ranking-based individual fairness. REDRESS is an individual fairness framework proposed by Dong et al. for learning fair repre- sentations in single node graphs. We extend this framework to the Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi bipartite recommendation setting and integrate it into our popu- larity bias mitigation approach. In the REDRESS setting, individual fairness requires that nodes which were similar in their initial fea- ture space should remain similar in their learned representation embeddings [25]. More concretely, for each song node, \ud835\udc61\ud835\udc56 , and node pair \ud835\udc61\ud835\udc62, \ud835\udc61\ud835\udc63 in a graph \ud835\udc3a, similarity is defined on the basis of the cosine \ud835\udc51 similarity metric, \ud835\udc60(\u22c5, \u22c5), as applied to either a feature \ud835\udc4b [\ud835\udc63] \u2208 R , \ud835\udc5a or learned embedding set, \ud835\udc4d [\ud835\udc63] \u2208 R . Applying this procedure in a pairwise fashion produces two similarity matrices. The first, or apriori similarity, \ud835\udc46\ud835\udc3a , in which similarity is calculated on input fea- tures and the second, or learned similarity, \ud835\udc46\ud835\udc4d , in which similarity is calculated between learned embeddings generated by some GNN model, \ud835\udc40. The purpose of REDRESS is to formulate this as a ranking on the basis of similarity and differentiate on the disparity between differences in rankings of the two representational spaces. Thus, drawing on principles from learn to rank [9], each entry in these similarity matrices is re-cast as the probability that node \ud835\udc61\ud835\udc56 is more similar to node \ud835\udc61\ud835\udc62 than \ud835\udc61\ud835\udc63 and transformed into an apriori proba- bility tensor, \ud835\udc43\ud835\udc3a \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223, and a learned probability tensor, \ud835\udc43\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223. Having defined these two probability tensors, for each individual node the fairness loss, \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ), can be treated the cross entropy loss applied to these probability distributions such that for an individual node, \ud835\udc61\ud835\udc56 : \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) = \u2212\ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56] log \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56] \u2212 (1 \u2212 \ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56]) log(1 \u2212 \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56]) and aggregated over all nodes \ud835\udc61\ud835\udc56 \u2208 \ud835\udc49 as: \ud835\udc3ffairness = \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc56 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc62 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc63 \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) Individually fair music discovery. The original formulation of individual fairness requires some form of domain expertise [25] to determine how (dis-)similar two items are. For the music dis- covery domain, we use music features (see Section 3.1 for exact details) as the basis for calculating cosine similarity. Thus, our apriori similarity, \ud835\udc46\ud835\udc3a , is defined as the cosine similarity between the musical features, \ud835\udc4b [\ud835\udc63] \u2208 R\u2223\ud835\udc47 \u2223\u00d79, associated with song nodes. Meanwhile, our learned similarity contains the song-level embed- dings, \ud835\udc46\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\ud835\udc5a , learned by PinSage. In this way, REDRESS acts as a regularizer that ensures that rank-based similarity between songs is preserved between the input and embedding space. Thus, our similarity notion is domain-aware and grounded in the essence of musical experiences: acoustics. To learn the embedding of songs, we follow the learning par- adigm of PinSage [67] and make a few deviations. Unlike Ying et al., we use uniform random sampling to avoid the computational burden of calculating negative samples on our large graph and compensate for the potential loss of information by using focal loss [43], rather than the margin loss, to train the network. It is important to note that since the potential benefits or drawbacks of PinSage as a general recommender system are out of the scope of this paper, we do not focus on the performance gains that such a change might provide and leave the addition of various negative sampling techniques to future"}, {"question": " What is REDRESS in the context of mitigating popularity bias?", "answer": " A framework for learning fair representations in single node graphs.", "ref_chunk": "leaving some bins empty. Finally, we select 10 bins based on the distribution of the datasets and the formulation of our BOOST methodology (see Section 3.4). We use the following steps to define our popularity setting. First, we count the number of times each song track, \ud835\udc61\ud835\udc56 appears within the playlist (or user) training interactions such that for each \ud835\udc61\ud835\udc56 , \ud835\udc4e\ud835\udc61\ud835\udc56 = \u2223 {\ud835\udc5d\ud835\udc56 \u2236 \ud835\udc61\ud835\udc56 \u2208 \ud835\udc5d\ud835\udc56 } \u2223. Then, we apply the base 10 logarithmic smoothing = log10 (\ud835\udc4e\ud835\udc61\ud835\udc56 ). Finally, to these values such that for each \ud835\udc61\ud835\udc56 , pop\ud835\udc61\ud835\udc56 we apply binning onto these values to split them into 10 groups such that for each \ud835\udc61\ud835\udc56 , pop_bin (\ud835\udc61\ud835\udc56 ) \u2208 {0, . . . , 9} where bin 9 has a higher popularity value than bin 0. The visualization of this binning procedure and its comparison with the long tail method can be seen in Figure 2. As demonstrated by our visualizations, transforming the raw values into the logarithmic space shows that the bins are filled in relatively even intervals, where, as the popularity increases, so does the number of songs included in a bin. We showcase the gains that our method has over the canonical long tail model in Figure 2 where we compare the positioning of our binning methodology with the classic long tail model. Furthermore, as we later show Figures 5 and 6 our formulation of popularity is able to elucidate crucial differences among both the datasets and baseline model performances on these datasets. 3.3.2 Popularity Bias and Music Discovery In addition we formalize the inverse relationship between music discovery and popularity bias. For each song track, \ud835\udc61\ud835\udc56 \u2208 \ud835\udc47\ud835\udc42\ud835\udc3a , we generate a counterfactual example song, \ud835\udc61 \u2217 \ud835\udc56 \u2208 \ud835\udc47\ud835\udc36\ud835\udc39 , where everything about the features is exactly the same and the only difference is that \ud835\udc61\ud835\udc56 has a high degree while \ud835\udc61 \u2217 \ud835\udc56 has a degree of one. We calculate the distance between an original song node, \ud835\udc61\ud835\udc56 and its counterfactual duplicate, \ud835\udc61 \u2217 \ud835\udc56 . A system with high potential for musical discovery will have a low distance between the songs, showing a low popularity bias and an understanding of musical similarity. We will return to this formulation in Section 5.1, showing that a node\u2019s placement and degree in the graph can exacerbate the presence of popularity bias, reflecting itself in the node\u2019s learned representation. 3.4 Mitigating Popularity Bias Through Individual Fairness Ranking-based individual fairness. REDRESS is an individual fairness framework proposed by Dong et al. for learning fair repre- sentations in single node graphs. We extend this framework to the Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi bipartite recommendation setting and integrate it into our popu- larity bias mitigation approach. In the REDRESS setting, individual fairness requires that nodes which were similar in their initial fea- ture space should remain similar in their learned representation embeddings [25]. More concretely, for each song node, \ud835\udc61\ud835\udc56 , and node pair \ud835\udc61\ud835\udc62, \ud835\udc61\ud835\udc63 in a graph \ud835\udc3a, similarity is defined on the basis of the cosine \ud835\udc51 similarity metric, \ud835\udc60(\u22c5, \u22c5), as applied to either a feature \ud835\udc4b [\ud835\udc63] \u2208 R , \ud835\udc5a or learned embedding set, \ud835\udc4d [\ud835\udc63] \u2208 R . Applying this procedure in a pairwise fashion produces two similarity matrices. The first, or apriori similarity, \ud835\udc46\ud835\udc3a , in which similarity is calculated on input fea- tures and the second, or learned similarity, \ud835\udc46\ud835\udc4d , in which similarity is calculated between learned embeddings generated by some GNN model, \ud835\udc40. The purpose of REDRESS is to formulate this as a ranking on the basis of similarity and differentiate on the disparity between differences in rankings of the two representational spaces. Thus, drawing on principles from learn to rank [9], each entry in these similarity matrices is re-cast as the probability that node \ud835\udc61\ud835\udc56 is more similar to node \ud835\udc61\ud835\udc62 than \ud835\udc61\ud835\udc63 and transformed into an apriori proba- bility tensor, \ud835\udc43\ud835\udc3a \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223, and a learned probability tensor, \ud835\udc43\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223. Having defined these two probability tensors, for each individual node the fairness loss, \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ), can be treated the cross entropy loss applied to these probability distributions such that for an individual node, \ud835\udc61\ud835\udc56 : \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) = \u2212\ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56] log \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56] \u2212 (1 \u2212 \ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56]) log(1 \u2212 \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56]) and aggregated over all nodes \ud835\udc61\ud835\udc56 \u2208 \ud835\udc49 as: \ud835\udc3ffairness = \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc56 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc62 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc63 \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) Individually fair music discovery. The original formulation of individual fairness requires some form of domain expertise [25] to determine how (dis-)similar two items are. For the music dis- covery domain, we use music features (see Section 3.1 for exact details) as the basis for calculating cosine similarity. Thus, our apriori similarity, \ud835\udc46\ud835\udc3a , is defined as the cosine similarity between the musical features, \ud835\udc4b [\ud835\udc63] \u2208 R\u2223\ud835\udc47 \u2223\u00d79, associated with song nodes. Meanwhile, our learned similarity contains the song-level embed- dings, \ud835\udc46\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\ud835\udc5a , learned by PinSage. In this way, REDRESS acts as a regularizer that ensures that rank-based similarity between songs is preserved between the input and embedding space. Thus, our similarity notion is domain-aware and grounded in the essence of musical experiences: acoustics. To learn the embedding of songs, we follow the learning par- adigm of PinSage [67] and make a few deviations. Unlike Ying et al., we use uniform random sampling to avoid the computational burden of calculating negative samples on our large graph and compensate for the potential loss of information by using focal loss [43], rather than the margin loss, to train the network. It is important to note that since the potential benefits or drawbacks of PinSage as a general recommender system are out of the scope of this paper, we do not focus on the performance gains that such a change might provide and leave the addition of various negative sampling techniques to future"}, {"question": " How is individual fairness defined in the context of music discovery?", "answer": " It requires nodes similar in the initial feature space to remain similar in their learned representation embeddings.", "ref_chunk": "leaving some bins empty. Finally, we select 10 bins based on the distribution of the datasets and the formulation of our BOOST methodology (see Section 3.4). We use the following steps to define our popularity setting. First, we count the number of times each song track, \ud835\udc61\ud835\udc56 appears within the playlist (or user) training interactions such that for each \ud835\udc61\ud835\udc56 , \ud835\udc4e\ud835\udc61\ud835\udc56 = \u2223 {\ud835\udc5d\ud835\udc56 \u2236 \ud835\udc61\ud835\udc56 \u2208 \ud835\udc5d\ud835\udc56 } \u2223. Then, we apply the base 10 logarithmic smoothing = log10 (\ud835\udc4e\ud835\udc61\ud835\udc56 ). Finally, to these values such that for each \ud835\udc61\ud835\udc56 , pop\ud835\udc61\ud835\udc56 we apply binning onto these values to split them into 10 groups such that for each \ud835\udc61\ud835\udc56 , pop_bin (\ud835\udc61\ud835\udc56 ) \u2208 {0, . . . , 9} where bin 9 has a higher popularity value than bin 0. The visualization of this binning procedure and its comparison with the long tail method can be seen in Figure 2. As demonstrated by our visualizations, transforming the raw values into the logarithmic space shows that the bins are filled in relatively even intervals, where, as the popularity increases, so does the number of songs included in a bin. We showcase the gains that our method has over the canonical long tail model in Figure 2 where we compare the positioning of our binning methodology with the classic long tail model. Furthermore, as we later show Figures 5 and 6 our formulation of popularity is able to elucidate crucial differences among both the datasets and baseline model performances on these datasets. 3.3.2 Popularity Bias and Music Discovery In addition we formalize the inverse relationship between music discovery and popularity bias. For each song track, \ud835\udc61\ud835\udc56 \u2208 \ud835\udc47\ud835\udc42\ud835\udc3a , we generate a counterfactual example song, \ud835\udc61 \u2217 \ud835\udc56 \u2208 \ud835\udc47\ud835\udc36\ud835\udc39 , where everything about the features is exactly the same and the only difference is that \ud835\udc61\ud835\udc56 has a high degree while \ud835\udc61 \u2217 \ud835\udc56 has a degree of one. We calculate the distance between an original song node, \ud835\udc61\ud835\udc56 and its counterfactual duplicate, \ud835\udc61 \u2217 \ud835\udc56 . A system with high potential for musical discovery will have a low distance between the songs, showing a low popularity bias and an understanding of musical similarity. We will return to this formulation in Section 5.1, showing that a node\u2019s placement and degree in the graph can exacerbate the presence of popularity bias, reflecting itself in the node\u2019s learned representation. 3.4 Mitigating Popularity Bias Through Individual Fairness Ranking-based individual fairness. REDRESS is an individual fairness framework proposed by Dong et al. for learning fair repre- sentations in single node graphs. We extend this framework to the Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi bipartite recommendation setting and integrate it into our popu- larity bias mitigation approach. In the REDRESS setting, individual fairness requires that nodes which were similar in their initial fea- ture space should remain similar in their learned representation embeddings [25]. More concretely, for each song node, \ud835\udc61\ud835\udc56 , and node pair \ud835\udc61\ud835\udc62, \ud835\udc61\ud835\udc63 in a graph \ud835\udc3a, similarity is defined on the basis of the cosine \ud835\udc51 similarity metric, \ud835\udc60(\u22c5, \u22c5), as applied to either a feature \ud835\udc4b [\ud835\udc63] \u2208 R , \ud835\udc5a or learned embedding set, \ud835\udc4d [\ud835\udc63] \u2208 R . Applying this procedure in a pairwise fashion produces two similarity matrices. The first, or apriori similarity, \ud835\udc46\ud835\udc3a , in which similarity is calculated on input fea- tures and the second, or learned similarity, \ud835\udc46\ud835\udc4d , in which similarity is calculated between learned embeddings generated by some GNN model, \ud835\udc40. The purpose of REDRESS is to formulate this as a ranking on the basis of similarity and differentiate on the disparity between differences in rankings of the two representational spaces. Thus, drawing on principles from learn to rank [9], each entry in these similarity matrices is re-cast as the probability that node \ud835\udc61\ud835\udc56 is more similar to node \ud835\udc61\ud835\udc62 than \ud835\udc61\ud835\udc63 and transformed into an apriori proba- bility tensor, \ud835\udc43\ud835\udc3a \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223, and a learned probability tensor, \ud835\udc43\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223. Having defined these two probability tensors, for each individual node the fairness loss, \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ), can be treated the cross entropy loss applied to these probability distributions such that for an individual node, \ud835\udc61\ud835\udc56 : \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) = \u2212\ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56] log \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56] \u2212 (1 \u2212 \ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56]) log(1 \u2212 \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56]) and aggregated over all nodes \ud835\udc61\ud835\udc56 \u2208 \ud835\udc49 as: \ud835\udc3ffairness = \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc56 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc62 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc63 \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) Individually fair music discovery. The original formulation of individual fairness requires some form of domain expertise [25] to determine how (dis-)similar two items are. For the music dis- covery domain, we use music features (see Section 3.1 for exact details) as the basis for calculating cosine similarity. Thus, our apriori similarity, \ud835\udc46\ud835\udc3a , is defined as the cosine similarity between the musical features, \ud835\udc4b [\ud835\udc63] \u2208 R\u2223\ud835\udc47 \u2223\u00d79, associated with song nodes. Meanwhile, our learned similarity contains the song-level embed- dings, \ud835\udc46\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\ud835\udc5a , learned by PinSage. In this way, REDRESS acts as a regularizer that ensures that rank-based similarity between songs is preserved between the input and embedding space. Thus, our similarity notion is domain-aware and grounded in the essence of musical experiences: acoustics. To learn the embedding of songs, we follow the learning par- adigm of PinSage [67] and make a few deviations. Unlike Ying et al., we use uniform random sampling to avoid the computational burden of calculating negative samples on our large graph and compensate for the potential loss of information by using focal loss [43], rather than the margin loss, to train the network. It is important to note that since the potential benefits or drawbacks of PinSage as a general recommender system are out of the scope of this paper, we do not focus on the performance gains that such a change might provide and leave the addition of various negative sampling techniques to future"}], "doc_text": "leaving some bins empty. Finally, we select 10 bins based on the distribution of the datasets and the formulation of our BOOST methodology (see Section 3.4). We use the following steps to define our popularity setting. First, we count the number of times each song track, \ud835\udc61\ud835\udc56 appears within the playlist (or user) training interactions such that for each \ud835\udc61\ud835\udc56 , \ud835\udc4e\ud835\udc61\ud835\udc56 = \u2223 {\ud835\udc5d\ud835\udc56 \u2236 \ud835\udc61\ud835\udc56 \u2208 \ud835\udc5d\ud835\udc56 } \u2223. Then, we apply the base 10 logarithmic smoothing = log10 (\ud835\udc4e\ud835\udc61\ud835\udc56 ). Finally, to these values such that for each \ud835\udc61\ud835\udc56 , pop\ud835\udc61\ud835\udc56 we apply binning onto these values to split them into 10 groups such that for each \ud835\udc61\ud835\udc56 , pop_bin (\ud835\udc61\ud835\udc56 ) \u2208 {0, . . . , 9} where bin 9 has a higher popularity value than bin 0. The visualization of this binning procedure and its comparison with the long tail method can be seen in Figure 2. As demonstrated by our visualizations, transforming the raw values into the logarithmic space shows that the bins are filled in relatively even intervals, where, as the popularity increases, so does the number of songs included in a bin. We showcase the gains that our method has over the canonical long tail model in Figure 2 where we compare the positioning of our binning methodology with the classic long tail model. Furthermore, as we later show Figures 5 and 6 our formulation of popularity is able to elucidate crucial differences among both the datasets and baseline model performances on these datasets. 3.3.2 Popularity Bias and Music Discovery In addition we formalize the inverse relationship between music discovery and popularity bias. For each song track, \ud835\udc61\ud835\udc56 \u2208 \ud835\udc47\ud835\udc42\ud835\udc3a , we generate a counterfactual example song, \ud835\udc61 \u2217 \ud835\udc56 \u2208 \ud835\udc47\ud835\udc36\ud835\udc39 , where everything about the features is exactly the same and the only difference is that \ud835\udc61\ud835\udc56 has a high degree while \ud835\udc61 \u2217 \ud835\udc56 has a degree of one. We calculate the distance between an original song node, \ud835\udc61\ud835\udc56 and its counterfactual duplicate, \ud835\udc61 \u2217 \ud835\udc56 . A system with high potential for musical discovery will have a low distance between the songs, showing a low popularity bias and an understanding of musical similarity. We will return to this formulation in Section 5.1, showing that a node\u2019s placement and degree in the graph can exacerbate the presence of popularity bias, reflecting itself in the node\u2019s learned representation. 3.4 Mitigating Popularity Bias Through Individual Fairness Ranking-based individual fairness. REDRESS is an individual fairness framework proposed by Dong et al. for learning fair repre- sentations in single node graphs. We extend this framework to the Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi bipartite recommendation setting and integrate it into our popu- larity bias mitigation approach. In the REDRESS setting, individual fairness requires that nodes which were similar in their initial fea- ture space should remain similar in their learned representation embeddings [25]. More concretely, for each song node, \ud835\udc61\ud835\udc56 , and node pair \ud835\udc61\ud835\udc62, \ud835\udc61\ud835\udc63 in a graph \ud835\udc3a, similarity is defined on the basis of the cosine \ud835\udc51 similarity metric, \ud835\udc60(\u22c5, \u22c5), as applied to either a feature \ud835\udc4b [\ud835\udc63] \u2208 R , \ud835\udc5a or learned embedding set, \ud835\udc4d [\ud835\udc63] \u2208 R . Applying this procedure in a pairwise fashion produces two similarity matrices. The first, or apriori similarity, \ud835\udc46\ud835\udc3a , in which similarity is calculated on input fea- tures and the second, or learned similarity, \ud835\udc46\ud835\udc4d , in which similarity is calculated between learned embeddings generated by some GNN model, \ud835\udc40. The purpose of REDRESS is to formulate this as a ranking on the basis of similarity and differentiate on the disparity between differences in rankings of the two representational spaces. Thus, drawing on principles from learn to rank [9], each entry in these similarity matrices is re-cast as the probability that node \ud835\udc61\ud835\udc56 is more similar to node \ud835\udc61\ud835\udc62 than \ud835\udc61\ud835\udc63 and transformed into an apriori proba- bility tensor, \ud835\udc43\ud835\udc3a \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223, and a learned probability tensor, \ud835\udc43\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223\u00d7\u2223\ud835\udc47 \u2223. Having defined these two probability tensors, for each individual node the fairness loss, \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ), can be treated the cross entropy loss applied to these probability distributions such that for an individual node, \ud835\udc61\ud835\udc56 : \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) = \u2212\ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56] log \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56] \u2212 (1 \u2212 \ud835\udc43\ud835\udc3a [\ud835\udc62, \ud835\udc63, \ud835\udc56]) log(1 \u2212 \ud835\udc43\ud835\udc4d [\ud835\udc62, \ud835\udc63, \ud835\udc56]) and aggregated over all nodes \ud835\udc61\ud835\udc56 \u2208 \ud835\udc49 as: \ud835\udc3ffairness = \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc56 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc62 \u2223\ud835\udc47 \u2223 \u2211 \ud835\udc63 \ud835\udc3f\ud835\udc61\ud835\udc62,\ud835\udc61\ud835\udc63 (\ud835\udc61\ud835\udc56 ) Individually fair music discovery. The original formulation of individual fairness requires some form of domain expertise [25] to determine how (dis-)similar two items are. For the music dis- covery domain, we use music features (see Section 3.1 for exact details) as the basis for calculating cosine similarity. Thus, our apriori similarity, \ud835\udc46\ud835\udc3a , is defined as the cosine similarity between the musical features, \ud835\udc4b [\ud835\udc63] \u2208 R\u2223\ud835\udc47 \u2223\u00d79, associated with song nodes. Meanwhile, our learned similarity contains the song-level embed- dings, \ud835\udc46\ud835\udc4d \u2208 R\u2223\ud835\udc47 \u2223\u00d7\ud835\udc5a , learned by PinSage. In this way, REDRESS acts as a regularizer that ensures that rank-based similarity between songs is preserved between the input and embedding space. Thus, our similarity notion is domain-aware and grounded in the essence of musical experiences: acoustics. To learn the embedding of songs, we follow the learning par- adigm of PinSage [67] and make a few deviations. Unlike Ying et al., we use uniform random sampling to avoid the computational burden of calculating negative samples on our large graph and compensate for the potential loss of information by using focal loss [43], rather than the margin loss, to train the network. It is important to note that since the potential benefits or drawbacks of PinSage as a general recommender system are out of the scope of this paper, we do not focus on the performance gains that such a change might provide and leave the addition of various negative sampling techniques to future"}