{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Improving_Audio_Captioning_Models_with_Fine-grained_Audio_Features,_Text_Embedding_Supervision,_and_LLM_Mix-up_Augmentation_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the goal of Automated audio captioning (AAC)?", "answer": " The goal of Automated audio captioning (AAC) is to describe an input audio clip using text, allowing for free-form sentences.", "ref_chunk": "4 2 0 2 n a J 0 1 ] D S . s c [ 2 v 2 5 3 7 1 . 9 0 3 2 : v i X r a IMPROVING AUDIO CAPTIONING MODELS WITH FINE-GRAINED AUDIO FEATURES, TEXT EMBEDDING SUPERVISION, AND LLM MIX-UP AUGMENTATION Shih-Lun Wu1, Xuankai Chang1, Gordon Wichern2, Jee-weon Jung1, Franc\u00b8ois Germain2, Jonathan Le Roux2, Shinji Watanabe1 1 Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA 2 Mitsubishi Electric Research Labs (MERL), Cambridge, MA, USA ABSTRACT Automated audio captioning (AAC) aims to generate informative de- scriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state- of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Fol- lowing the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large lan- guage models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ INSTRUCTOR LLM to fetch text embeddings of captions, and infuse their language- modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmen- tation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state- of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge. Index Terms\u2014 AAC, BEATs, LLM, mix-up, InfoNCE 1. INTRODUCTION Automated audio captioning (AAC) is a multimodal task whose goal is to describe an input audio clip using text. The descriptions are not restricted to a fixed set of class labels or tags, but are free- form sentences [1], which allow better flexibility and expressivity. Research progress on AAC has accelerated in recent years thanks to the yearly DCASE challenges, the impressive performance of Transformer-based language models, and the release of the open au- dio captioning datasets Clotho [2] and AudioCaps [3]. Recent lead- ing works in AAC [4\u20139] all used the sequence-to-sequence (seq2seq) modeling framework, where an audio encoder is used to extract fea- tures from the input audio, and a text decoder learns to generate the caption autoregressively based on the extracted audio features. feature extraction, auxiliary training objective, and data augmen- tation. We begin with revamping the audio encoder (Section 2.1). While PANN [10], a convolution-based audio feature extractor, has long been the tried-and-true choice for AAC research [4\u20136], many recently proposed audio encoders [11\u201313] have the potential to fur- ther improve AAC performance due to more advanced architectures, pretraining objectives, and finer-grained output features. Specifi- cally, we choose Bidirectional Encoder representation from Audio Transformers (BEATs) [13], a state-of-the-art multi-label audio tag- ging model pretrained on AudioSet [14], as our audio encoder. Next, witnessing the tremendous success of large language mod- els (LLMs) in representing and generating text [15, 16], we use text embeddings from the INSTRUCTOR Transformer [17] to provide ad- ditional supervision (Section 2.2), and employ ChatGPT [18] to per- form a novel mix-up data augmentation (Section 2.3). In previous AAC studies, to help linking audio features with concepts in text, which is the output space of AAC tasks, [5] pretrained the PANN en- coder with an audio-caption InfoNCE [19] contrastive loss, while [6] used multitask learning to predict keywords in the caption. In our work, we combine the benefits of both representation learning and multitask training, and also leverage the LLM\u2019s rich knowledge of text. Particularly, we use INSTRUCTOR to obtain text embeddings for ground-truth captions, and apply an auxiliary InfoNCE loss on a Conformer [20, 21] postencoder to refine/summarize the BEATs audio features and align them with INSTRUCTOR text embeddings. For data augmentation, other than SpecAugment [22] used com- monly in audio-related tasks, researchers have leveraged the original mix-up [23] to linearly combine audio/text embeddings from two unrelated samples [8,24], synonym substitution on ground-truth cap- tions [7], and caption concatenation [7]. As for LLM-based efforts, ChatGPT has been used to compile the large-scale WavCaps [7,8,25] AAC dataset by rewriting tags or fragmented descriptions, which are often associated with audio files on the web, to coherent sentences. Integrating the ideas of mix-up and LLM augmentation methods, we prompt ChatGPT to mix-up the captions of two audio clips, which produces more natural combined captions than simple text concate- nation [7]. The text mix-ups, when paired with audio mix-ups (i.e., summations of waveforms), increase the amount, complexity, and diversity of our training data. With all the techniques above, we also discover that nucleus sampling decoding [26] followed by hybrid reranking (Section 2.4) leads to a further performance boost. While our work is also seq2seq-based, we extensively leverage machine learning models that are pretrained on large-scale datasets to improve AAC performance from multiple aspects, namely, audio This work used Bridges2-PSC and Delta-NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosys- tem: Services & Support (ACCESS) program, supported by NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. Our AAC model attains a state-of-the-art SPIDEr-FL score of 32.6 (on Clotho V2 [2] evaluation set) and is the winner of the 2023 DCASE AAC Challenge.1 Despite the numerous components in- troduced to our model, we show in our ablation study (Section 3.4) 1DCASE Challenge 2023 technical report: https://dcase.community/ documents/challenge2023/technical reports/DCASE2023 Wu 31 t6a.pdf. (17Hz) \" man walks on pebbles <eos> \" (50Hz) InfoNCE loss Conv1D downsample audio 12 negativepairs attention negativepairs BARTdecoder BEATsencoder Global mean pool ross c NLL loss repel \"a man walks on pebbles\" audioembedding caption \" a man walks on pebbles \" attract captionembedding Instructor embedder Conformer Given that the BEATs module is frozen, to enable further train- ing on the audio features (more details in Section 2.2),"}, {"question": " What backbone do state-of-the-art AAC systems rely on?", "answer": " State-of-the-art AAC systems rely on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers.", "ref_chunk": "4 2 0 2 n a J 0 1 ] D S . s c [ 2 v 2 5 3 7 1 . 9 0 3 2 : v i X r a IMPROVING AUDIO CAPTIONING MODELS WITH FINE-GRAINED AUDIO FEATURES, TEXT EMBEDDING SUPERVISION, AND LLM MIX-UP AUGMENTATION Shih-Lun Wu1, Xuankai Chang1, Gordon Wichern2, Jee-weon Jung1, Franc\u00b8ois Germain2, Jonathan Le Roux2, Shinji Watanabe1 1 Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA 2 Mitsubishi Electric Research Labs (MERL), Cambridge, MA, USA ABSTRACT Automated audio captioning (AAC) aims to generate informative de- scriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state- of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Fol- lowing the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large lan- guage models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ INSTRUCTOR LLM to fetch text embeddings of captions, and infuse their language- modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmen- tation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state- of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge. Index Terms\u2014 AAC, BEATs, LLM, mix-up, InfoNCE 1. INTRODUCTION Automated audio captioning (AAC) is a multimodal task whose goal is to describe an input audio clip using text. The descriptions are not restricted to a fixed set of class labels or tags, but are free- form sentences [1], which allow better flexibility and expressivity. Research progress on AAC has accelerated in recent years thanks to the yearly DCASE challenges, the impressive performance of Transformer-based language models, and the release of the open au- dio captioning datasets Clotho [2] and AudioCaps [3]. Recent lead- ing works in AAC [4\u20139] all used the sequence-to-sequence (seq2seq) modeling framework, where an audio encoder is used to extract fea- tures from the input audio, and a text decoder learns to generate the caption autoregressively based on the extracted audio features. feature extraction, auxiliary training objective, and data augmen- tation. We begin with revamping the audio encoder (Section 2.1). While PANN [10], a convolution-based audio feature extractor, has long been the tried-and-true choice for AAC research [4\u20136], many recently proposed audio encoders [11\u201313] have the potential to fur- ther improve AAC performance due to more advanced architectures, pretraining objectives, and finer-grained output features. Specifi- cally, we choose Bidirectional Encoder representation from Audio Transformers (BEATs) [13], a state-of-the-art multi-label audio tag- ging model pretrained on AudioSet [14], as our audio encoder. Next, witnessing the tremendous success of large language mod- els (LLMs) in representing and generating text [15, 16], we use text embeddings from the INSTRUCTOR Transformer [17] to provide ad- ditional supervision (Section 2.2), and employ ChatGPT [18] to per- form a novel mix-up data augmentation (Section 2.3). In previous AAC studies, to help linking audio features with concepts in text, which is the output space of AAC tasks, [5] pretrained the PANN en- coder with an audio-caption InfoNCE [19] contrastive loss, while [6] used multitask learning to predict keywords in the caption. In our work, we combine the benefits of both representation learning and multitask training, and also leverage the LLM\u2019s rich knowledge of text. Particularly, we use INSTRUCTOR to obtain text embeddings for ground-truth captions, and apply an auxiliary InfoNCE loss on a Conformer [20, 21] postencoder to refine/summarize the BEATs audio features and align them with INSTRUCTOR text embeddings. For data augmentation, other than SpecAugment [22] used com- monly in audio-related tasks, researchers have leveraged the original mix-up [23] to linearly combine audio/text embeddings from two unrelated samples [8,24], synonym substitution on ground-truth cap- tions [7], and caption concatenation [7]. As for LLM-based efforts, ChatGPT has been used to compile the large-scale WavCaps [7,8,25] AAC dataset by rewriting tags or fragmented descriptions, which are often associated with audio files on the web, to coherent sentences. Integrating the ideas of mix-up and LLM augmentation methods, we prompt ChatGPT to mix-up the captions of two audio clips, which produces more natural combined captions than simple text concate- nation [7]. The text mix-ups, when paired with audio mix-ups (i.e., summations of waveforms), increase the amount, complexity, and diversity of our training data. With all the techniques above, we also discover that nucleus sampling decoding [26] followed by hybrid reranking (Section 2.4) leads to a further performance boost. While our work is also seq2seq-based, we extensively leverage machine learning models that are pretrained on large-scale datasets to improve AAC performance from multiple aspects, namely, audio This work used Bridges2-PSC and Delta-NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosys- tem: Services & Support (ACCESS) program, supported by NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. Our AAC model attains a state-of-the-art SPIDEr-FL score of 32.6 (on Clotho V2 [2] evaluation set) and is the winner of the 2023 DCASE AAC Challenge.1 Despite the numerous components in- troduced to our model, we show in our ablation study (Section 3.4) 1DCASE Challenge 2023 technical report: https://dcase.community/ documents/challenge2023/technical reports/DCASE2023 Wu 31 t6a.pdf. (17Hz) \" man walks on pebbles <eos> \" (50Hz) InfoNCE loss Conv1D downsample audio 12 negativepairs attention negativepairs BARTdecoder BEATsencoder Global mean pool ross c NLL loss repel \"a man walks on pebbles\" audioembedding caption \" a man walks on pebbles \" attract captionembedding Instructor embedder Conformer Given that the BEATs module is frozen, to enable further train- ing on the audio features (more details in Section 2.2),"}, {"question": " What are some methods used to improve the performance of seq2seq AAC models?", "answer": " Some methods to improve the performance of seq2seq AAC models include leveraging pretrained models, large language models (LLMs), fine-grained audio features, text embedding supervision, and data augmentation.", "ref_chunk": "4 2 0 2 n a J 0 1 ] D S . s c [ 2 v 2 5 3 7 1 . 9 0 3 2 : v i X r a IMPROVING AUDIO CAPTIONING MODELS WITH FINE-GRAINED AUDIO FEATURES, TEXT EMBEDDING SUPERVISION, AND LLM MIX-UP AUGMENTATION Shih-Lun Wu1, Xuankai Chang1, Gordon Wichern2, Jee-weon Jung1, Franc\u00b8ois Germain2, Jonathan Le Roux2, Shinji Watanabe1 1 Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA 2 Mitsubishi Electric Research Labs (MERL), Cambridge, MA, USA ABSTRACT Automated audio captioning (AAC) aims to generate informative de- scriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state- of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Fol- lowing the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large lan- guage models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ INSTRUCTOR LLM to fetch text embeddings of captions, and infuse their language- modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmen- tation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state- of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge. Index Terms\u2014 AAC, BEATs, LLM, mix-up, InfoNCE 1. INTRODUCTION Automated audio captioning (AAC) is a multimodal task whose goal is to describe an input audio clip using text. The descriptions are not restricted to a fixed set of class labels or tags, but are free- form sentences [1], which allow better flexibility and expressivity. Research progress on AAC has accelerated in recent years thanks to the yearly DCASE challenges, the impressive performance of Transformer-based language models, and the release of the open au- dio captioning datasets Clotho [2] and AudioCaps [3]. Recent lead- ing works in AAC [4\u20139] all used the sequence-to-sequence (seq2seq) modeling framework, where an audio encoder is used to extract fea- tures from the input audio, and a text decoder learns to generate the caption autoregressively based on the extracted audio features. feature extraction, auxiliary training objective, and data augmen- tation. We begin with revamping the audio encoder (Section 2.1). While PANN [10], a convolution-based audio feature extractor, has long been the tried-and-true choice for AAC research [4\u20136], many recently proposed audio encoders [11\u201313] have the potential to fur- ther improve AAC performance due to more advanced architectures, pretraining objectives, and finer-grained output features. Specifi- cally, we choose Bidirectional Encoder representation from Audio Transformers (BEATs) [13], a state-of-the-art multi-label audio tag- ging model pretrained on AudioSet [14], as our audio encoder. Next, witnessing the tremendous success of large language mod- els (LLMs) in representing and generating text [15, 16], we use text embeddings from the INSTRUCTOR Transformer [17] to provide ad- ditional supervision (Section 2.2), and employ ChatGPT [18] to per- form a novel mix-up data augmentation (Section 2.3). In previous AAC studies, to help linking audio features with concepts in text, which is the output space of AAC tasks, [5] pretrained the PANN en- coder with an audio-caption InfoNCE [19] contrastive loss, while [6] used multitask learning to predict keywords in the caption. In our work, we combine the benefits of both representation learning and multitask training, and also leverage the LLM\u2019s rich knowledge of text. Particularly, we use INSTRUCTOR to obtain text embeddings for ground-truth captions, and apply an auxiliary InfoNCE loss on a Conformer [20, 21] postencoder to refine/summarize the BEATs audio features and align them with INSTRUCTOR text embeddings. For data augmentation, other than SpecAugment [22] used com- monly in audio-related tasks, researchers have leveraged the original mix-up [23] to linearly combine audio/text embeddings from two unrelated samples [8,24], synonym substitution on ground-truth cap- tions [7], and caption concatenation [7]. As for LLM-based efforts, ChatGPT has been used to compile the large-scale WavCaps [7,8,25] AAC dataset by rewriting tags or fragmented descriptions, which are often associated with audio files on the web, to coherent sentences. Integrating the ideas of mix-up and LLM augmentation methods, we prompt ChatGPT to mix-up the captions of two audio clips, which produces more natural combined captions than simple text concate- nation [7]. The text mix-ups, when paired with audio mix-ups (i.e., summations of waveforms), increase the amount, complexity, and diversity of our training data. With all the techniques above, we also discover that nucleus sampling decoding [26] followed by hybrid reranking (Section 2.4) leads to a further performance boost. While our work is also seq2seq-based, we extensively leverage machine learning models that are pretrained on large-scale datasets to improve AAC performance from multiple aspects, namely, audio This work used Bridges2-PSC and Delta-NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosys- tem: Services & Support (ACCESS) program, supported by NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. Our AAC model attains a state-of-the-art SPIDEr-FL score of 32.6 (on Clotho V2 [2] evaluation set) and is the winner of the 2023 DCASE AAC Challenge.1 Despite the numerous components in- troduced to our model, we show in our ablation study (Section 3.4) 1DCASE Challenge 2023 technical report: https://dcase.community/ documents/challenge2023/technical reports/DCASE2023 Wu 31 t6a.pdf. (17Hz) \" man walks on pebbles <eos> \" (50Hz) InfoNCE loss Conv1D downsample audio 12 negativepairs attention negativepairs BARTdecoder BEATsencoder Global mean pool ross c NLL loss repel \"a man walks on pebbles\" audioembedding caption \" a man walks on pebbles \" attract captionembedding Instructor embedder Conformer Given that the BEATs module is frozen, to enable further train- ing on the audio features (more details in Section 2.2),"}, {"question": " What is BEATs used for in the context of AAC models?", "answer": " BEATs is used to extract fine-grained audio features in the context of AAC models.", "ref_chunk": "4 2 0 2 n a J 0 1 ] D S . s c [ 2 v 2 5 3 7 1 . 9 0 3 2 : v i X r a IMPROVING AUDIO CAPTIONING MODELS WITH FINE-GRAINED AUDIO FEATURES, TEXT EMBEDDING SUPERVISION, AND LLM MIX-UP AUGMENTATION Shih-Lun Wu1, Xuankai Chang1, Gordon Wichern2, Jee-weon Jung1, Franc\u00b8ois Germain2, Jonathan Le Roux2, Shinji Watanabe1 1 Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA 2 Mitsubishi Electric Research Labs (MERL), Cambridge, MA, USA ABSTRACT Automated audio captioning (AAC) aims to generate informative de- scriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state- of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Fol- lowing the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large lan- guage models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ INSTRUCTOR LLM to fetch text embeddings of captions, and infuse their language- modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmen- tation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state- of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge. Index Terms\u2014 AAC, BEATs, LLM, mix-up, InfoNCE 1. INTRODUCTION Automated audio captioning (AAC) is a multimodal task whose goal is to describe an input audio clip using text. The descriptions are not restricted to a fixed set of class labels or tags, but are free- form sentences [1], which allow better flexibility and expressivity. Research progress on AAC has accelerated in recent years thanks to the yearly DCASE challenges, the impressive performance of Transformer-based language models, and the release of the open au- dio captioning datasets Clotho [2] and AudioCaps [3]. Recent lead- ing works in AAC [4\u20139] all used the sequence-to-sequence (seq2seq) modeling framework, where an audio encoder is used to extract fea- tures from the input audio, and a text decoder learns to generate the caption autoregressively based on the extracted audio features. feature extraction, auxiliary training objective, and data augmen- tation. We begin with revamping the audio encoder (Section 2.1). While PANN [10], a convolution-based audio feature extractor, has long been the tried-and-true choice for AAC research [4\u20136], many recently proposed audio encoders [11\u201313] have the potential to fur- ther improve AAC performance due to more advanced architectures, pretraining objectives, and finer-grained output features. Specifi- cally, we choose Bidirectional Encoder representation from Audio Transformers (BEATs) [13], a state-of-the-art multi-label audio tag- ging model pretrained on AudioSet [14], as our audio encoder. Next, witnessing the tremendous success of large language mod- els (LLMs) in representing and generating text [15, 16], we use text embeddings from the INSTRUCTOR Transformer [17] to provide ad- ditional supervision (Section 2.2), and employ ChatGPT [18] to per- form a novel mix-up data augmentation (Section 2.3). In previous AAC studies, to help linking audio features with concepts in text, which is the output space of AAC tasks, [5] pretrained the PANN en- coder with an audio-caption InfoNCE [19] contrastive loss, while [6] used multitask learning to predict keywords in the caption. In our work, we combine the benefits of both representation learning and multitask training, and also leverage the LLM\u2019s rich knowledge of text. Particularly, we use INSTRUCTOR to obtain text embeddings for ground-truth captions, and apply an auxiliary InfoNCE loss on a Conformer [20, 21] postencoder to refine/summarize the BEATs audio features and align them with INSTRUCTOR text embeddings. For data augmentation, other than SpecAugment [22] used com- monly in audio-related tasks, researchers have leveraged the original mix-up [23] to linearly combine audio/text embeddings from two unrelated samples [8,24], synonym substitution on ground-truth cap- tions [7], and caption concatenation [7]. As for LLM-based efforts, ChatGPT has been used to compile the large-scale WavCaps [7,8,25] AAC dataset by rewriting tags or fragmented descriptions, which are often associated with audio files on the web, to coherent sentences. Integrating the ideas of mix-up and LLM augmentation methods, we prompt ChatGPT to mix-up the captions of two audio clips, which produces more natural combined captions than simple text concate- nation [7]. The text mix-ups, when paired with audio mix-ups (i.e., summations of waveforms), increase the amount, complexity, and diversity of our training data. With all the techniques above, we also discover that nucleus sampling decoding [26] followed by hybrid reranking (Section 2.4) leads to a further performance boost. While our work is also seq2seq-based, we extensively leverage machine learning models that are pretrained on large-scale datasets to improve AAC performance from multiple aspects, namely, audio This work used Bridges2-PSC and Delta-NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosys- tem: Services & Support (ACCESS) program, supported by NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. Our AAC model attains a state-of-the-art SPIDEr-FL score of 32.6 (on Clotho V2 [2] evaluation set) and is the winner of the 2023 DCASE AAC Challenge.1 Despite the numerous components in- troduced to our model, we show in our ablation study (Section 3.4) 1DCASE Challenge 2023 technical report: https://dcase.community/ documents/challenge2023/technical reports/DCASE2023 Wu 31 t6a.pdf. (17Hz) \" man walks on pebbles <eos> \" (50Hz) InfoNCE loss Conv1D downsample audio 12 negativepairs attention negativepairs BARTdecoder BEATsencoder Global mean pool ross c NLL loss repel \"a man walks on pebbles\" audioembedding caption \" a man walks on pebbles \" attract captionembedding Instructor embedder Conformer Given that the BEATs module is frozen, to enable further train- ing on the audio features (more details in Section 2.2),"}, {"question": " How are text embeddings from the INSTRUCTOR Transformer used in AAC models?", "answer": " Text embeddings from the INSTRUCTOR Transformer are used to provide additional supervision and infuse language-modality knowledge into BEATs audio features.", "ref_chunk": "4 2 0 2 n a J 0 1 ] D S . s c [ 2 v 2 5 3 7 1 . 9 0 3 2 : v i X r a IMPROVING AUDIO CAPTIONING MODELS WITH FINE-GRAINED AUDIO FEATURES, TEXT EMBEDDING SUPERVISION, AND LLM MIX-UP AUGMENTATION Shih-Lun Wu1, Xuankai Chang1, Gordon Wichern2, Jee-weon Jung1, Franc\u00b8ois Germain2, Jonathan Le Roux2, Shinji Watanabe1 1 Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA 2 Mitsubishi Electric Research Labs (MERL), Cambridge, MA, USA ABSTRACT Automated audio captioning (AAC) aims to generate informative de- scriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state- of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Fol- lowing the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large lan- guage models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ INSTRUCTOR LLM to fetch text embeddings of captions, and infuse their language- modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmen- tation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state- of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge. Index Terms\u2014 AAC, BEATs, LLM, mix-up, InfoNCE 1. INTRODUCTION Automated audio captioning (AAC) is a multimodal task whose goal is to describe an input audio clip using text. The descriptions are not restricted to a fixed set of class labels or tags, but are free- form sentences [1], which allow better flexibility and expressivity. Research progress on AAC has accelerated in recent years thanks to the yearly DCASE challenges, the impressive performance of Transformer-based language models, and the release of the open au- dio captioning datasets Clotho [2] and AudioCaps [3]. Recent lead- ing works in AAC [4\u20139] all used the sequence-to-sequence (seq2seq) modeling framework, where an audio encoder is used to extract fea- tures from the input audio, and a text decoder learns to generate the caption autoregressively based on the extracted audio features. feature extraction, auxiliary training objective, and data augmen- tation. We begin with revamping the audio encoder (Section 2.1). While PANN [10], a convolution-based audio feature extractor, has long been the tried-and-true choice for AAC research [4\u20136], many recently proposed audio encoders [11\u201313] have the potential to fur- ther improve AAC performance due to more advanced architectures, pretraining objectives, and finer-grained output features. Specifi- cally, we choose Bidirectional Encoder representation from Audio Transformers (BEATs) [13], a state-of-the-art multi-label audio tag- ging model pretrained on AudioSet [14], as our audio encoder. Next, witnessing the tremendous success of large language mod- els (LLMs) in representing and generating text [15, 16], we use text embeddings from the INSTRUCTOR Transformer [17] to provide ad- ditional supervision (Section 2.2), and employ ChatGPT [18] to per- form a novel mix-up data augmentation (Section 2.3). In previous AAC studies, to help linking audio features with concepts in text, which is the output space of AAC tasks, [5] pretrained the PANN en- coder with an audio-caption InfoNCE [19] contrastive loss, while [6] used multitask learning to predict keywords in the caption. In our work, we combine the benefits of both representation learning and multitask training, and also leverage the LLM\u2019s rich knowledge of text. Particularly, we use INSTRUCTOR to obtain text embeddings for ground-truth captions, and apply an auxiliary InfoNCE loss on a Conformer [20, 21] postencoder to refine/summarize the BEATs audio features and align them with INSTRUCTOR text embeddings. For data augmentation, other than SpecAugment [22] used com- monly in audio-related tasks, researchers have leveraged the original mix-up [23] to linearly combine audio/text embeddings from two unrelated samples [8,24], synonym substitution on ground-truth cap- tions [7], and caption concatenation [7]. As for LLM-based efforts, ChatGPT has been used to compile the large-scale WavCaps [7,8,25] AAC dataset by rewriting tags or fragmented descriptions, which are often associated with audio files on the web, to coherent sentences. Integrating the ideas of mix-up and LLM augmentation methods, we prompt ChatGPT to mix-up the captions of two audio clips, which produces more natural combined captions than simple text concate- nation [7]. The text mix-ups, when paired with audio mix-ups (i.e., summations of waveforms), increase the amount, complexity, and diversity of our training data. With all the techniques above, we also discover that nucleus sampling decoding [26] followed by hybrid reranking (Section 2.4) leads to a further performance boost. While our work is also seq2seq-based, we extensively leverage machine learning models that are pretrained on large-scale datasets to improve AAC performance from multiple aspects, namely, audio This work used Bridges2-PSC and Delta-NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosys- tem: Services & Support (ACCESS) program, supported by NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. Our AAC model attains a state-of-the-art SPIDEr-FL score of 32.6 (on Clotho V2 [2] evaluation set) and is the winner of the 2023 DCASE AAC Challenge.1 Despite the numerous components in- troduced to our model, we show in our ablation study (Section 3.4) 1DCASE Challenge 2023 technical report: https://dcase.community/ documents/challenge2023/technical reports/DCASE2023 Wu 31 t6a.pdf. (17Hz) \" man walks on pebbles <eos> \" (50Hz) InfoNCE loss Conv1D downsample audio 12 negativepairs attention negativepairs BARTdecoder BEATsencoder Global mean pool ross c NLL loss repel \"a man walks on pebbles\" audioembedding caption \" a man walks on pebbles \" attract captionembedding Instructor embedder Conformer Given that the BEATs module is frozen, to enable further train- ing on the audio features (more details in Section 2.2),"}, {"question": " What is the purpose of the novel data augmentation method using ChatGPT in AAC models?", "answer": " The novel data augmentation method using ChatGPT produces caption mix-ups to increase the complexity, diversity, and amount of training data in AAC models.", "ref_chunk": "4 2 0 2 n a J 0 1 ] D S . s c [ 2 v 2 5 3 7 1 . 9 0 3 2 : v i X r a IMPROVING AUDIO CAPTIONING MODELS WITH FINE-GRAINED AUDIO FEATURES, TEXT EMBEDDING SUPERVISION, AND LLM MIX-UP AUGMENTATION Shih-Lun Wu1, Xuankai Chang1, Gordon Wichern2, Jee-weon Jung1, Franc\u00b8ois Germain2, Jonathan Le Roux2, Shinji Watanabe1 1 Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA 2 Mitsubishi Electric Research Labs (MERL), Cambridge, MA, USA ABSTRACT Automated audio captioning (AAC) aims to generate informative de- scriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state- of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Fol- lowing the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large lan- guage models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ INSTRUCTOR LLM to fetch text embeddings of captions, and infuse their language- modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmen- tation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state- of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge. Index Terms\u2014 AAC, BEATs, LLM, mix-up, InfoNCE 1. INTRODUCTION Automated audio captioning (AAC) is a multimodal task whose goal is to describe an input audio clip using text. The descriptions are not restricted to a fixed set of class labels or tags, but are free- form sentences [1], which allow better flexibility and expressivity. Research progress on AAC has accelerated in recent years thanks to the yearly DCASE challenges, the impressive performance of Transformer-based language models, and the release of the open au- dio captioning datasets Clotho [2] and AudioCaps [3]. Recent lead- ing works in AAC [4\u20139] all used the sequence-to-sequence (seq2seq) modeling framework, where an audio encoder is used to extract fea- tures from the input audio, and a text decoder learns to generate the caption autoregressively based on the extracted audio features. feature extraction, auxiliary training objective, and data augmen- tation. We begin with revamping the audio encoder (Section 2.1). While PANN [10], a convolution-based audio feature extractor, has long been the tried-and-true choice for AAC research [4\u20136], many recently proposed audio encoders [11\u201313] have the potential to fur- ther improve AAC performance due to more advanced architectures, pretraining objectives, and finer-grained output features. Specifi- cally, we choose Bidirectional Encoder representation from Audio Transformers (BEATs) [13], a state-of-the-art multi-label audio tag- ging model pretrained on AudioSet [14], as our audio encoder. Next, witnessing the tremendous success of large language mod- els (LLMs) in representing and generating text [15, 16], we use text embeddings from the INSTRUCTOR Transformer [17] to provide ad- ditional supervision (Section 2.2), and employ ChatGPT [18] to per- form a novel mix-up data augmentation (Section 2.3). In previous AAC studies, to help linking audio features with concepts in text, which is the output space of AAC tasks, [5] pretrained the PANN en- coder with an audio-caption InfoNCE [19] contrastive loss, while [6] used multitask learning to predict keywords in the caption. In our work, we combine the benefits of both representation learning and multitask training, and also leverage the LLM\u2019s rich knowledge of text. Particularly, we use INSTRUCTOR to obtain text embeddings for ground-truth captions, and apply an auxiliary InfoNCE loss on a Conformer [20, 21] postencoder to refine/summarize the BEATs audio features and align them with INSTRUCTOR text embeddings. For data augmentation, other than SpecAugment [22] used com- monly in audio-related tasks, researchers have leveraged the original mix-up [23] to linearly combine audio/text embeddings from two unrelated samples [8,24], synonym substitution on ground-truth cap- tions [7], and caption concatenation [7]. As for LLM-based efforts, ChatGPT has been used to compile the large-scale WavCaps [7,8,25] AAC dataset by rewriting tags or fragmented descriptions, which are often associated with audio files on the web, to coherent sentences. Integrating the ideas of mix-up and LLM augmentation methods, we prompt ChatGPT to mix-up the captions of two audio clips, which produces more natural combined captions than simple text concate- nation [7]. The text mix-ups, when paired with audio mix-ups (i.e., summations of waveforms), increase the amount, complexity, and diversity of our training data. With all the techniques above, we also discover that nucleus sampling decoding [26] followed by hybrid reranking (Section 2.4) leads to a further performance boost. While our work is also seq2seq-based, we extensively leverage machine learning models that are pretrained on large-scale datasets to improve AAC performance from multiple aspects, namely, audio This work used Bridges2-PSC and Delta-NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosys- tem: Services & Support (ACCESS) program, supported by NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. Our AAC model attains a state-of-the-art SPIDEr-FL score of 32.6 (on Clotho V2 [2] evaluation set) and is the winner of the 2023 DCASE AAC Challenge.1 Despite the numerous components in- troduced to our model, we show in our ablation study (Section 3.4) 1DCASE Challenge 2023 technical report: https://dcase.community/ documents/challenge2023/technical reports/DCASE2023 Wu 31 t6a.pdf. (17Hz) \" man walks on pebbles <eos> \" (50Hz) InfoNCE loss Conv1D downsample audio 12 negativepairs attention negativepairs BARTdecoder BEATsencoder Global mean pool ross c NLL loss repel \"a man walks on pebbles\" audioembedding caption \" a man walks on pebbles \" attract captionembedding Instructor embedder Conformer Given that the BEATs module is frozen, to enable further train- ing on the audio features (more details in Section 2.2),"}, {"question": " What decoding technique is proposed for inference in AAC models?", "answer": " Nucleus sampling and a hybrid reranking algorithm are proposed for inference in AAC models.", "ref_chunk": "4 2 0 2 n a J 0 1 ] D S . s c [ 2 v 2 5 3 7 1 . 9 0 3 2 : v i X r a IMPROVING AUDIO CAPTIONING MODELS WITH FINE-GRAINED AUDIO FEATURES, TEXT EMBEDDING SUPERVISION, AND LLM MIX-UP AUGMENTATION Shih-Lun Wu1, Xuankai Chang1, Gordon Wichern2, Jee-weon Jung1, Franc\u00b8ois Germain2, Jonathan Le Roux2, Shinji Watanabe1 1 Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA 2 Mitsubishi Electric Research Labs (MERL), Cambridge, MA, USA ABSTRACT Automated audio captioning (AAC) aims to generate informative de- scriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state- of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Fol- lowing the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large lan- guage models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ INSTRUCTOR LLM to fetch text embeddings of captions, and infuse their language- modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmen- tation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state- of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge. Index Terms\u2014 AAC, BEATs, LLM, mix-up, InfoNCE 1. INTRODUCTION Automated audio captioning (AAC) is a multimodal task whose goal is to describe an input audio clip using text. The descriptions are not restricted to a fixed set of class labels or tags, but are free- form sentences [1], which allow better flexibility and expressivity. Research progress on AAC has accelerated in recent years thanks to the yearly DCASE challenges, the impressive performance of Transformer-based language models, and the release of the open au- dio captioning datasets Clotho [2] and AudioCaps [3]. Recent lead- ing works in AAC [4\u20139] all used the sequence-to-sequence (seq2seq) modeling framework, where an audio encoder is used to extract fea- tures from the input audio, and a text decoder learns to generate the caption autoregressively based on the extracted audio features. feature extraction, auxiliary training objective, and data augmen- tation. We begin with revamping the audio encoder (Section 2.1). While PANN [10], a convolution-based audio feature extractor, has long been the tried-and-true choice for AAC research [4\u20136], many recently proposed audio encoders [11\u201313] have the potential to fur- ther improve AAC performance due to more advanced architectures, pretraining objectives, and finer-grained output features. Specifi- cally, we choose Bidirectional Encoder representation from Audio Transformers (BEATs) [13], a state-of-the-art multi-label audio tag- ging model pretrained on AudioSet [14], as our audio encoder. Next, witnessing the tremendous success of large language mod- els (LLMs) in representing and generating text [15, 16], we use text embeddings from the INSTRUCTOR Transformer [17] to provide ad- ditional supervision (Section 2.2), and employ ChatGPT [18] to per- form a novel mix-up data augmentation (Section 2.3). In previous AAC studies, to help linking audio features with concepts in text, which is the output space of AAC tasks, [5] pretrained the PANN en- coder with an audio-caption InfoNCE [19] contrastive loss, while [6] used multitask learning to predict keywords in the caption. In our work, we combine the benefits of both representation learning and multitask training, and also leverage the LLM\u2019s rich knowledge of text. Particularly, we use INSTRUCTOR to obtain text embeddings for ground-truth captions, and apply an auxiliary InfoNCE loss on a Conformer [20, 21] postencoder to refine/summarize the BEATs audio features and align them with INSTRUCTOR text embeddings. For data augmentation, other than SpecAugment [22] used com- monly in audio-related tasks, researchers have leveraged the original mix-up [23] to linearly combine audio/text embeddings from two unrelated samples [8,24], synonym substitution on ground-truth cap- tions [7], and caption concatenation [7]. As for LLM-based efforts, ChatGPT has been used to compile the large-scale WavCaps [7,8,25] AAC dataset by rewriting tags or fragmented descriptions, which are often associated with audio files on the web, to coherent sentences. Integrating the ideas of mix-up and LLM augmentation methods, we prompt ChatGPT to mix-up the captions of two audio clips, which produces more natural combined captions than simple text concate- nation [7]. The text mix-ups, when paired with audio mix-ups (i.e., summations of waveforms), increase the amount, complexity, and diversity of our training data. With all the techniques above, we also discover that nucleus sampling decoding [26] followed by hybrid reranking (Section 2.4) leads to a further performance boost. While our work is also seq2seq-based, we extensively leverage machine learning models that are pretrained on large-scale datasets to improve AAC performance from multiple aspects, namely, audio This work used Bridges2-PSC and Delta-NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosys- tem: Services & Support (ACCESS) program, supported by NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. Our AAC model attains a state-of-the-art SPIDEr-FL score of 32.6 (on Clotho V2 [2] evaluation set) and is the winner of the 2023 DCASE AAC Challenge.1 Despite the numerous components in- troduced to our model, we show in our ablation study (Section 3.4) 1DCASE Challenge 2023 technical report: https://dcase.community/ documents/challenge2023/technical reports/DCASE2023 Wu 31 t6a.pdf. (17Hz) \" man walks on pebbles <eos> \" (50Hz) InfoNCE loss Conv1D downsample audio 12 negativepairs attention negativepairs BARTdecoder BEATsencoder Global mean pool ross c NLL loss repel \"a man walks on pebbles\" audioembedding caption \" a man walks on pebbles \" attract captionembedding Instructor embedder Conformer Given that the BEATs module is frozen, to enable further train- ing on the audio features (more details in Section 2.2),"}, {"question": " What are some examples of data augmentation methods used in AAC research?", "answer": " Examples of data augmentation methods used in AAC research include original mix-up, synonym substitution, and caption concatenation.", "ref_chunk": "4 2 0 2 n a J 0 1 ] D S . s c [ 2 v 2 5 3 7 1 . 9 0 3 2 : v i X r a IMPROVING AUDIO CAPTIONING MODELS WITH FINE-GRAINED AUDIO FEATURES, TEXT EMBEDDING SUPERVISION, AND LLM MIX-UP AUGMENTATION Shih-Lun Wu1, Xuankai Chang1, Gordon Wichern2, Jee-weon Jung1, Franc\u00b8ois Germain2, Jonathan Le Roux2, Shinji Watanabe1 1 Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA 2 Mitsubishi Electric Research Labs (MERL), Cambridge, MA, USA ABSTRACT Automated audio captioning (AAC) aims to generate informative de- scriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state- of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Fol- lowing the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large lan- guage models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ INSTRUCTOR LLM to fetch text embeddings of captions, and infuse their language- modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmen- tation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state- of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge. Index Terms\u2014 AAC, BEATs, LLM, mix-up, InfoNCE 1. INTRODUCTION Automated audio captioning (AAC) is a multimodal task whose goal is to describe an input audio clip using text. The descriptions are not restricted to a fixed set of class labels or tags, but are free- form sentences [1], which allow better flexibility and expressivity. Research progress on AAC has accelerated in recent years thanks to the yearly DCASE challenges, the impressive performance of Transformer-based language models, and the release of the open au- dio captioning datasets Clotho [2] and AudioCaps [3]. Recent lead- ing works in AAC [4\u20139] all used the sequence-to-sequence (seq2seq) modeling framework, where an audio encoder is used to extract fea- tures from the input audio, and a text decoder learns to generate the caption autoregressively based on the extracted audio features. feature extraction, auxiliary training objective, and data augmen- tation. We begin with revamping the audio encoder (Section 2.1). While PANN [10], a convolution-based audio feature extractor, has long been the tried-and-true choice for AAC research [4\u20136], many recently proposed audio encoders [11\u201313] have the potential to fur- ther improve AAC performance due to more advanced architectures, pretraining objectives, and finer-grained output features. Specifi- cally, we choose Bidirectional Encoder representation from Audio Transformers (BEATs) [13], a state-of-the-art multi-label audio tag- ging model pretrained on AudioSet [14], as our audio encoder. Next, witnessing the tremendous success of large language mod- els (LLMs) in representing and generating text [15, 16], we use text embeddings from the INSTRUCTOR Transformer [17] to provide ad- ditional supervision (Section 2.2), and employ ChatGPT [18] to per- form a novel mix-up data augmentation (Section 2.3). In previous AAC studies, to help linking audio features with concepts in text, which is the output space of AAC tasks, [5] pretrained the PANN en- coder with an audio-caption InfoNCE [19] contrastive loss, while [6] used multitask learning to predict keywords in the caption. In our work, we combine the benefits of both representation learning and multitask training, and also leverage the LLM\u2019s rich knowledge of text. Particularly, we use INSTRUCTOR to obtain text embeddings for ground-truth captions, and apply an auxiliary InfoNCE loss on a Conformer [20, 21] postencoder to refine/summarize the BEATs audio features and align them with INSTRUCTOR text embeddings. For data augmentation, other than SpecAugment [22] used com- monly in audio-related tasks, researchers have leveraged the original mix-up [23] to linearly combine audio/text embeddings from two unrelated samples [8,24], synonym substitution on ground-truth cap- tions [7], and caption concatenation [7]. As for LLM-based efforts, ChatGPT has been used to compile the large-scale WavCaps [7,8,25] AAC dataset by rewriting tags or fragmented descriptions, which are often associated with audio files on the web, to coherent sentences. Integrating the ideas of mix-up and LLM augmentation methods, we prompt ChatGPT to mix-up the captions of two audio clips, which produces more natural combined captions than simple text concate- nation [7]. The text mix-ups, when paired with audio mix-ups (i.e., summations of waveforms), increase the amount, complexity, and diversity of our training data. With all the techniques above, we also discover that nucleus sampling decoding [26] followed by hybrid reranking (Section 2.4) leads to a further performance boost. While our work is also seq2seq-based, we extensively leverage machine learning models that are pretrained on large-scale datasets to improve AAC performance from multiple aspects, namely, audio This work used Bridges2-PSC and Delta-NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosys- tem: Services & Support (ACCESS) program, supported by NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. Our AAC model attains a state-of-the-art SPIDEr-FL score of 32.6 (on Clotho V2 [2] evaluation set) and is the winner of the 2023 DCASE AAC Challenge.1 Despite the numerous components in- troduced to our model, we show in our ablation study (Section 3.4) 1DCASE Challenge 2023 technical report: https://dcase.community/ documents/challenge2023/technical reports/DCASE2023 Wu 31 t6a.pdf. (17Hz) \" man walks on pebbles <eos> \" (50Hz) InfoNCE loss Conv1D downsample audio 12 negativepairs attention negativepairs BARTdecoder BEATsencoder Global mean pool ross c NLL loss repel \"a man walks on pebbles\" audioembedding caption \" a man walks on pebbles \" attract captionembedding Instructor embedder Conformer Given that the BEATs module is frozen, to enable further train- ing on the audio features (more details in Section 2.2),"}, {"question": " How do researchers combine audio and text mix-ups in AAC models?", "answer": " Researchers prompt ChatGPT to mix-up the captions of two audio clips, producing natural combined captions that increase the amount, complexity, and diversity of training data.", "ref_chunk": "4 2 0 2 n a J 0 1 ] D S . s c [ 2 v 2 5 3 7 1 . 9 0 3 2 : v i X r a IMPROVING AUDIO CAPTIONING MODELS WITH FINE-GRAINED AUDIO FEATURES, TEXT EMBEDDING SUPERVISION, AND LLM MIX-UP AUGMENTATION Shih-Lun Wu1, Xuankai Chang1, Gordon Wichern2, Jee-weon Jung1, Franc\u00b8ois Germain2, Jonathan Le Roux2, Shinji Watanabe1 1 Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA 2 Mitsubishi Electric Research Labs (MERL), Cambridge, MA, USA ABSTRACT Automated audio captioning (AAC) aims to generate informative de- scriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state- of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Fol- lowing the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large lan- guage models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ INSTRUCTOR LLM to fetch text embeddings of captions, and infuse their language- modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmen- tation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state- of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge. Index Terms\u2014 AAC, BEATs, LLM, mix-up, InfoNCE 1. INTRODUCTION Automated audio captioning (AAC) is a multimodal task whose goal is to describe an input audio clip using text. The descriptions are not restricted to a fixed set of class labels or tags, but are free- form sentences [1], which allow better flexibility and expressivity. Research progress on AAC has accelerated in recent years thanks to the yearly DCASE challenges, the impressive performance of Transformer-based language models, and the release of the open au- dio captioning datasets Clotho [2] and AudioCaps [3]. Recent lead- ing works in AAC [4\u20139] all used the sequence-to-sequence (seq2seq) modeling framework, where an audio encoder is used to extract fea- tures from the input audio, and a text decoder learns to generate the caption autoregressively based on the extracted audio features. feature extraction, auxiliary training objective, and data augmen- tation. We begin with revamping the audio encoder (Section 2.1). While PANN [10], a convolution-based audio feature extractor, has long been the tried-and-true choice for AAC research [4\u20136], many recently proposed audio encoders [11\u201313] have the potential to fur- ther improve AAC performance due to more advanced architectures, pretraining objectives, and finer-grained output features. Specifi- cally, we choose Bidirectional Encoder representation from Audio Transformers (BEATs) [13], a state-of-the-art multi-label audio tag- ging model pretrained on AudioSet [14], as our audio encoder. Next, witnessing the tremendous success of large language mod- els (LLMs) in representing and generating text [15, 16], we use text embeddings from the INSTRUCTOR Transformer [17] to provide ad- ditional supervision (Section 2.2), and employ ChatGPT [18] to per- form a novel mix-up data augmentation (Section 2.3). In previous AAC studies, to help linking audio features with concepts in text, which is the output space of AAC tasks, [5] pretrained the PANN en- coder with an audio-caption InfoNCE [19] contrastive loss, while [6] used multitask learning to predict keywords in the caption. In our work, we combine the benefits of both representation learning and multitask training, and also leverage the LLM\u2019s rich knowledge of text. Particularly, we use INSTRUCTOR to obtain text embeddings for ground-truth captions, and apply an auxiliary InfoNCE loss on a Conformer [20, 21] postencoder to refine/summarize the BEATs audio features and align them with INSTRUCTOR text embeddings. For data augmentation, other than SpecAugment [22] used com- monly in audio-related tasks, researchers have leveraged the original mix-up [23] to linearly combine audio/text embeddings from two unrelated samples [8,24], synonym substitution on ground-truth cap- tions [7], and caption concatenation [7]. As for LLM-based efforts, ChatGPT has been used to compile the large-scale WavCaps [7,8,25] AAC dataset by rewriting tags or fragmented descriptions, which are often associated with audio files on the web, to coherent sentences. Integrating the ideas of mix-up and LLM augmentation methods, we prompt ChatGPT to mix-up the captions of two audio clips, which produces more natural combined captions than simple text concate- nation [7]. The text mix-ups, when paired with audio mix-ups (i.e., summations of waveforms), increase the amount, complexity, and diversity of our training data. With all the techniques above, we also discover that nucleus sampling decoding [26] followed by hybrid reranking (Section 2.4) leads to a further performance boost. While our work is also seq2seq-based, we extensively leverage machine learning models that are pretrained on large-scale datasets to improve AAC performance from multiple aspects, namely, audio This work used Bridges2-PSC and Delta-NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosys- tem: Services & Support (ACCESS) program, supported by NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. Our AAC model attains a state-of-the-art SPIDEr-FL score of 32.6 (on Clotho V2 [2] evaluation set) and is the winner of the 2023 DCASE AAC Challenge.1 Despite the numerous components in- troduced to our model, we show in our ablation study (Section 3.4) 1DCASE Challenge 2023 technical report: https://dcase.community/ documents/challenge2023/technical reports/DCASE2023 Wu 31 t6a.pdf. (17Hz) \" man walks on pebbles <eos> \" (50Hz) InfoNCE loss Conv1D downsample audio 12 negativepairs attention negativepairs BARTdecoder BEATsencoder Global mean pool ross c NLL loss repel \"a man walks on pebbles\" audioembedding caption \" a man walks on pebbles \" attract captionembedding Instructor embedder Conformer Given that the BEATs module is frozen, to enable further train- ing on the audio features (more details in Section 2.2),"}, {"question": " What led to a performance boost in AAC models according to the text?", "answer": " Nucleus sampling decoding followed by hybrid reranking led to a performance boost in AAC models.", "ref_chunk": "4 2 0 2 n a J 0 1 ] D S . s c [ 2 v 2 5 3 7 1 . 9 0 3 2 : v i X r a IMPROVING AUDIO CAPTIONING MODELS WITH FINE-GRAINED AUDIO FEATURES, TEXT EMBEDDING SUPERVISION, AND LLM MIX-UP AUGMENTATION Shih-Lun Wu1, Xuankai Chang1, Gordon Wichern2, Jee-weon Jung1, Franc\u00b8ois Germain2, Jonathan Le Roux2, Shinji Watanabe1 1 Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA 2 Mitsubishi Electric Research Labs (MERL), Cambridge, MA, USA ABSTRACT Automated audio captioning (AAC) aims to generate informative de- scriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state- of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Fol- lowing the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large lan- guage models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ INSTRUCTOR LLM to fetch text embeddings of captions, and infuse their language- modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmen- tation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state- of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge. Index Terms\u2014 AAC, BEATs, LLM, mix-up, InfoNCE 1. INTRODUCTION Automated audio captioning (AAC) is a multimodal task whose goal is to describe an input audio clip using text. The descriptions are not restricted to a fixed set of class labels or tags, but are free- form sentences [1], which allow better flexibility and expressivity. Research progress on AAC has accelerated in recent years thanks to the yearly DCASE challenges, the impressive performance of Transformer-based language models, and the release of the open au- dio captioning datasets Clotho [2] and AudioCaps [3]. Recent lead- ing works in AAC [4\u20139] all used the sequence-to-sequence (seq2seq) modeling framework, where an audio encoder is used to extract fea- tures from the input audio, and a text decoder learns to generate the caption autoregressively based on the extracted audio features. feature extraction, auxiliary training objective, and data augmen- tation. We begin with revamping the audio encoder (Section 2.1). While PANN [10], a convolution-based audio feature extractor, has long been the tried-and-true choice for AAC research [4\u20136], many recently proposed audio encoders [11\u201313] have the potential to fur- ther improve AAC performance due to more advanced architectures, pretraining objectives, and finer-grained output features. Specifi- cally, we choose Bidirectional Encoder representation from Audio Transformers (BEATs) [13], a state-of-the-art multi-label audio tag- ging model pretrained on AudioSet [14], as our audio encoder. Next, witnessing the tremendous success of large language mod- els (LLMs) in representing and generating text [15, 16], we use text embeddings from the INSTRUCTOR Transformer [17] to provide ad- ditional supervision (Section 2.2), and employ ChatGPT [18] to per- form a novel mix-up data augmentation (Section 2.3). In previous AAC studies, to help linking audio features with concepts in text, which is the output space of AAC tasks, [5] pretrained the PANN en- coder with an audio-caption InfoNCE [19] contrastive loss, while [6] used multitask learning to predict keywords in the caption. In our work, we combine the benefits of both representation learning and multitask training, and also leverage the LLM\u2019s rich knowledge of text. Particularly, we use INSTRUCTOR to obtain text embeddings for ground-truth captions, and apply an auxiliary InfoNCE loss on a Conformer [20, 21] postencoder to refine/summarize the BEATs audio features and align them with INSTRUCTOR text embeddings. For data augmentation, other than SpecAugment [22] used com- monly in audio-related tasks, researchers have leveraged the original mix-up [23] to linearly combine audio/text embeddings from two unrelated samples [8,24], synonym substitution on ground-truth cap- tions [7], and caption concatenation [7]. As for LLM-based efforts, ChatGPT has been used to compile the large-scale WavCaps [7,8,25] AAC dataset by rewriting tags or fragmented descriptions, which are often associated with audio files on the web, to coherent sentences. Integrating the ideas of mix-up and LLM augmentation methods, we prompt ChatGPT to mix-up the captions of two audio clips, which produces more natural combined captions than simple text concate- nation [7]. The text mix-ups, when paired with audio mix-ups (i.e., summations of waveforms), increase the amount, complexity, and diversity of our training data. With all the techniques above, we also discover that nucleus sampling decoding [26] followed by hybrid reranking (Section 2.4) leads to a further performance boost. While our work is also seq2seq-based, we extensively leverage machine learning models that are pretrained on large-scale datasets to improve AAC performance from multiple aspects, namely, audio This work used Bridges2-PSC and Delta-NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosys- tem: Services & Support (ACCESS) program, supported by NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. Our AAC model attains a state-of-the-art SPIDEr-FL score of 32.6 (on Clotho V2 [2] evaluation set) and is the winner of the 2023 DCASE AAC Challenge.1 Despite the numerous components in- troduced to our model, we show in our ablation study (Section 3.4) 1DCASE Challenge 2023 technical report: https://dcase.community/ documents/challenge2023/technical reports/DCASE2023 Wu 31 t6a.pdf. (17Hz) \" man walks on pebbles <eos> \" (50Hz) InfoNCE loss Conv1D downsample audio 12 negativepairs attention negativepairs BARTdecoder BEATsencoder Global mean pool ross c NLL loss repel \"a man walks on pebbles\" audioembedding caption \" a man walks on pebbles \" attract captionembedding Instructor embedder Conformer Given that the BEATs module is frozen, to enable further train- ing on the audio features (more details in Section 2.2),"}], "doc_text": "4 2 0 2 n a J 0 1 ] D S . s c [ 2 v 2 5 3 7 1 . 9 0 3 2 : v i X r a IMPROVING AUDIO CAPTIONING MODELS WITH FINE-GRAINED AUDIO FEATURES, TEXT EMBEDDING SUPERVISION, AND LLM MIX-UP AUGMENTATION Shih-Lun Wu1, Xuankai Chang1, Gordon Wichern2, Jee-weon Jung1, Franc\u00b8ois Germain2, Jonathan Le Roux2, Shinji Watanabe1 1 Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA 2 Mitsubishi Electric Research Labs (MERL), Cambridge, MA, USA ABSTRACT Automated audio captioning (AAC) aims to generate informative de- scriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state- of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Fol- lowing the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large lan- guage models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ INSTRUCTOR LLM to fetch text embeddings of captions, and infuse their language- modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmen- tation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state- of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge. Index Terms\u2014 AAC, BEATs, LLM, mix-up, InfoNCE 1. INTRODUCTION Automated audio captioning (AAC) is a multimodal task whose goal is to describe an input audio clip using text. The descriptions are not restricted to a fixed set of class labels or tags, but are free- form sentences [1], which allow better flexibility and expressivity. Research progress on AAC has accelerated in recent years thanks to the yearly DCASE challenges, the impressive performance of Transformer-based language models, and the release of the open au- dio captioning datasets Clotho [2] and AudioCaps [3]. Recent lead- ing works in AAC [4\u20139] all used the sequence-to-sequence (seq2seq) modeling framework, where an audio encoder is used to extract fea- tures from the input audio, and a text decoder learns to generate the caption autoregressively based on the extracted audio features. feature extraction, auxiliary training objective, and data augmen- tation. We begin with revamping the audio encoder (Section 2.1). While PANN [10], a convolution-based audio feature extractor, has long been the tried-and-true choice for AAC research [4\u20136], many recently proposed audio encoders [11\u201313] have the potential to fur- ther improve AAC performance due to more advanced architectures, pretraining objectives, and finer-grained output features. Specifi- cally, we choose Bidirectional Encoder representation from Audio Transformers (BEATs) [13], a state-of-the-art multi-label audio tag- ging model pretrained on AudioSet [14], as our audio encoder. Next, witnessing the tremendous success of large language mod- els (LLMs) in representing and generating text [15, 16], we use text embeddings from the INSTRUCTOR Transformer [17] to provide ad- ditional supervision (Section 2.2), and employ ChatGPT [18] to per- form a novel mix-up data augmentation (Section 2.3). In previous AAC studies, to help linking audio features with concepts in text, which is the output space of AAC tasks, [5] pretrained the PANN en- coder with an audio-caption InfoNCE [19] contrastive loss, while [6] used multitask learning to predict keywords in the caption. In our work, we combine the benefits of both representation learning and multitask training, and also leverage the LLM\u2019s rich knowledge of text. Particularly, we use INSTRUCTOR to obtain text embeddings for ground-truth captions, and apply an auxiliary InfoNCE loss on a Conformer [20, 21] postencoder to refine/summarize the BEATs audio features and align them with INSTRUCTOR text embeddings. For data augmentation, other than SpecAugment [22] used com- monly in audio-related tasks, researchers have leveraged the original mix-up [23] to linearly combine audio/text embeddings from two unrelated samples [8,24], synonym substitution on ground-truth cap- tions [7], and caption concatenation [7]. As for LLM-based efforts, ChatGPT has been used to compile the large-scale WavCaps [7,8,25] AAC dataset by rewriting tags or fragmented descriptions, which are often associated with audio files on the web, to coherent sentences. Integrating the ideas of mix-up and LLM augmentation methods, we prompt ChatGPT to mix-up the captions of two audio clips, which produces more natural combined captions than simple text concate- nation [7]. The text mix-ups, when paired with audio mix-ups (i.e., summations of waveforms), increase the amount, complexity, and diversity of our training data. With all the techniques above, we also discover that nucleus sampling decoding [26] followed by hybrid reranking (Section 2.4) leads to a further performance boost. While our work is also seq2seq-based, we extensively leverage machine learning models that are pretrained on large-scale datasets to improve AAC performance from multiple aspects, namely, audio This work used Bridges2-PSC and Delta-NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosys- tem: Services & Support (ACCESS) program, supported by NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. Our AAC model attains a state-of-the-art SPIDEr-FL score of 32.6 (on Clotho V2 [2] evaluation set) and is the winner of the 2023 DCASE AAC Challenge.1 Despite the numerous components in- troduced to our model, we show in our ablation study (Section 3.4) 1DCASE Challenge 2023 technical report: https://dcase.community/ documents/challenge2023/technical reports/DCASE2023 Wu 31 t6a.pdf. (17Hz) \" man walks on pebbles <eos> \" (50Hz) InfoNCE loss Conv1D downsample audio 12 negativepairs attention negativepairs BARTdecoder BEATsencoder Global mean pool ross c NLL loss repel \"a man walks on pebbles\" audioembedding caption \" a man walks on pebbles \" attract captionembedding Instructor embedder Conformer Given that the BEATs module is frozen, to enable further train- ing on the audio features (more details in Section 2.2),"}