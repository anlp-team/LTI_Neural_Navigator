{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Data-efficient_Active_Learning_for_Structured_Prediction_with_Partial_Annotation_and_Self-Training_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What type of model is adopted for estimating error rate in the text?", "answer": " One-dimensional logistic regression model", "ref_chunk": "error rate can- not be directly obtained, requiring estimation.2 We adopt a simple one-dimensional logistic regression model for this purpose. The input to the model is the uncertainty score3 and the output is a binary pre- diction of whether its prediction is confidently cor- rect4 or not. The estimator is trained using all the sub-structures together with their correctness on the development set5 and then applied to the queried candidates. For each candidate sub-structure s, the estimator will give it a correctness probability. We 2Directly using uncertainty is another option, but the main trouble is that the model is not well-calibrated. We also tried model calibration by temperature scaling (Guo et al., 2017), but did not find better results. 3We transform the input with a logarithm, which leads to better estimation according to preliminary experiments. 4The specific criterion is that the arg max prediction matches the gold one and its margin is greater than 0.5. Since neural models are usually over-confident, it is hard to decide a confidence threshold. Nevertheless, we find 0.5 a reasonable value for the ratio decision here. 5We re-use the development set for the task model training. estimate the overall error rate as one minus the aver- age correctness probability over all the candidates in the query set Q (all sub-structures in the selected sentences), and set the selection ratio r as this error rate: r = 1 \u2212 1 n (cid:88) s\u2208Q p(correct = 1|s) In this way, the selection ratio can be set adap- tively according to the current model\u2019s capability. If the model is weak and makes many mistakes, we will have a larger ratio which can lead to more dense annotations and richer training signals. As the model is trained with more data and makes fewer errors, the ratio will be tuned down corre- spondingly to avoid wasting annotation budget on already-correctly-predicted sub-structures. As we will see in later experiments, this adaptive scheme is suitable for AL (\u00a73.3). 2.3 Self-training Better utilization of unlabeled data is a promising direction to further enhance model training in AL since unlabeled data are usually freely available from the unlabeled pool. In this work, we adopt self-training (Yarowsky, 1995) for this purpose. The main idea of self-training is to enhance the model training with pseudo labels that are predicted by the current model on the unlabeled data. It has been shown effective for various NLP tasks (Yarowsky, 1995; McClosky et al., 2006; He et al., 2020; Du et al., 2021). For the training of AL mod- els, self-training can be seamlessly incorporated. For FA, the application of self-training is no dif- ferent than that in the conventional scenarios by applying the current model to all the un-annotated instances in the unlabeled pool. The more inter- esting case is on the partially annotated instances in the PA regime. The same motivation from the adaptive ratio scheme (\u00a72.2) also applies here: We select the highly-uncertain sub-structures that are error-prone and the remaining un-selected parts are likely to be correctly predicted; therefore we can trust the predictions on the un-selected sub- structures and include them for training. One more enhancement to apply here is that we could further perform re-inference by incorporating the updated annotations over the selected sub-structures, which can enhance the predictions of un-annotated sub- structures through output dependencies. In this work, we adopt a soft version of self- training through knowledge distillation (KD; Hin- ton et al., 2015). This choice is because we want to avoid the potential negative influences of ambigu- ous predictions (mostly in completely unlabeled instances). One way to mitigate this is to set an uncertainty threshold and only utilize the highly- confident sub-structures. However, it is unclear how to set a proper value, similar to the scenarios in query selection. Therefore, we take the model\u2019s full output predictions as the training targets with- out further processing. Specifically, our self-training objective function is the cross-entropy between the output distribu- tions predicted by the previous model m\u2032 before training and the current model m being trained: L = \u2212 (cid:88) pm\u2032(y|x) log pm(y|x) y\u2208Y Several points are notable here: 1) The previous model is kept unchanged, and we can simply cache its predictions before training; 2) Over the instances that have partial annotations, the predictions should reflect these annotations by incorporating corre- sponding constraints at inference time; 3) For tasks with CRF based models, the output space Y is usu- ally exponentially large and infeasible to explicitly enumerate; we utilize special algorithms (Wang et al., 2021) to deal with this, and more details are presented in Appendix C. Finally, we find it beneficial to include both the pseudo labels and the real annotated gold labels for the model training. With the gold data, the original training loss is adopted, while the KD objective is utilized with the pseudo labels. We simply mix these two types of data with a ratio of 1:1 in the training process, which we find works well. 3 Experiments 3.1 Main Settings Tasks and data. Our experiments6 are conducted over four English tasks. The first two are named entity recognition (NER) and dependency parsing (DPAR), which are representative structured pre- diction tasks for predicting sequence and tree struc- tures. We adopt the CoNLL-2003 English dataset (Tjong Kim Sang and De Meulder, 2003) for NER and the English Web Treebank (EWT) from Uni- versal Dependencies v2.10 (Nivre et al., 2020) for DPAR. Moreover, we explore two more complex IE tasks: Event extraction and relation extraction. 6Our implementation is available at https://github. com/zzsfornlp/zmsp/. Each task involves two pipelined sub-tasks: The first aims to extract the event trigger and/or entity mentions, and the second predicts links between these mentions as event arguments or entity rela- tions. We utilize the ACE05 dataset (Walker et al., 2006) for these IE tasks. AL. For the AL procedure, we adopt settings fol- lowing conventional practices. We use the original training set as the unlabeled data pool to select in- stances. Unless otherwise noted, we set the AL"}, {"question": " What is the input to the logistic regression model for error rate estimation?", "answer": " Uncertainty score", "ref_chunk": "error rate can- not be directly obtained, requiring estimation.2 We adopt a simple one-dimensional logistic regression model for this purpose. The input to the model is the uncertainty score3 and the output is a binary pre- diction of whether its prediction is confidently cor- rect4 or not. The estimator is trained using all the sub-structures together with their correctness on the development set5 and then applied to the queried candidates. For each candidate sub-structure s, the estimator will give it a correctness probability. We 2Directly using uncertainty is another option, but the main trouble is that the model is not well-calibrated. We also tried model calibration by temperature scaling (Guo et al., 2017), but did not find better results. 3We transform the input with a logarithm, which leads to better estimation according to preliminary experiments. 4The specific criterion is that the arg max prediction matches the gold one and its margin is greater than 0.5. Since neural models are usually over-confident, it is hard to decide a confidence threshold. Nevertheless, we find 0.5 a reasonable value for the ratio decision here. 5We re-use the development set for the task model training. estimate the overall error rate as one minus the aver- age correctness probability over all the candidates in the query set Q (all sub-structures in the selected sentences), and set the selection ratio r as this error rate: r = 1 \u2212 1 n (cid:88) s\u2208Q p(correct = 1|s) In this way, the selection ratio can be set adap- tively according to the current model\u2019s capability. If the model is weak and makes many mistakes, we will have a larger ratio which can lead to more dense annotations and richer training signals. As the model is trained with more data and makes fewer errors, the ratio will be tuned down corre- spondingly to avoid wasting annotation budget on already-correctly-predicted sub-structures. As we will see in later experiments, this adaptive scheme is suitable for AL (\u00a73.3). 2.3 Self-training Better utilization of unlabeled data is a promising direction to further enhance model training in AL since unlabeled data are usually freely available from the unlabeled pool. In this work, we adopt self-training (Yarowsky, 1995) for this purpose. The main idea of self-training is to enhance the model training with pseudo labels that are predicted by the current model on the unlabeled data. It has been shown effective for various NLP tasks (Yarowsky, 1995; McClosky et al., 2006; He et al., 2020; Du et al., 2021). For the training of AL mod- els, self-training can be seamlessly incorporated. For FA, the application of self-training is no dif- ferent than that in the conventional scenarios by applying the current model to all the un-annotated instances in the unlabeled pool. The more inter- esting case is on the partially annotated instances in the PA regime. The same motivation from the adaptive ratio scheme (\u00a72.2) also applies here: We select the highly-uncertain sub-structures that are error-prone and the remaining un-selected parts are likely to be correctly predicted; therefore we can trust the predictions on the un-selected sub- structures and include them for training. One more enhancement to apply here is that we could further perform re-inference by incorporating the updated annotations over the selected sub-structures, which can enhance the predictions of un-annotated sub- structures through output dependencies. In this work, we adopt a soft version of self- training through knowledge distillation (KD; Hin- ton et al., 2015). This choice is because we want to avoid the potential negative influences of ambigu- ous predictions (mostly in completely unlabeled instances). One way to mitigate this is to set an uncertainty threshold and only utilize the highly- confident sub-structures. However, it is unclear how to set a proper value, similar to the scenarios in query selection. Therefore, we take the model\u2019s full output predictions as the training targets with- out further processing. Specifically, our self-training objective function is the cross-entropy between the output distribu- tions predicted by the previous model m\u2032 before training and the current model m being trained: L = \u2212 (cid:88) pm\u2032(y|x) log pm(y|x) y\u2208Y Several points are notable here: 1) The previous model is kept unchanged, and we can simply cache its predictions before training; 2) Over the instances that have partial annotations, the predictions should reflect these annotations by incorporating corre- sponding constraints at inference time; 3) For tasks with CRF based models, the output space Y is usu- ally exponentially large and infeasible to explicitly enumerate; we utilize special algorithms (Wang et al., 2021) to deal with this, and more details are presented in Appendix C. Finally, we find it beneficial to include both the pseudo labels and the real annotated gold labels for the model training. With the gold data, the original training loss is adopted, while the KD objective is utilized with the pseudo labels. We simply mix these two types of data with a ratio of 1:1 in the training process, which we find works well. 3 Experiments 3.1 Main Settings Tasks and data. Our experiments6 are conducted over four English tasks. The first two are named entity recognition (NER) and dependency parsing (DPAR), which are representative structured pre- diction tasks for predicting sequence and tree struc- tures. We adopt the CoNLL-2003 English dataset (Tjong Kim Sang and De Meulder, 2003) for NER and the English Web Treebank (EWT) from Uni- versal Dependencies v2.10 (Nivre et al., 2020) for DPAR. Moreover, we explore two more complex IE tasks: Event extraction and relation extraction. 6Our implementation is available at https://github. com/zzsfornlp/zmsp/. Each task involves two pipelined sub-tasks: The first aims to extract the event trigger and/or entity mentions, and the second predicts links between these mentions as event arguments or entity rela- tions. We utilize the ACE05 dataset (Walker et al., 2006) for these IE tasks. AL. For the AL procedure, we adopt settings fol- lowing conventional practices. We use the original training set as the unlabeled data pool to select in- stances. Unless otherwise noted, we set the AL"}, {"question": " How is the correctness of a prediction determined in the model?", "answer": " Binary prediction of whether its prediction is confidently correct or not", "ref_chunk": "error rate can- not be directly obtained, requiring estimation.2 We adopt a simple one-dimensional logistic regression model for this purpose. The input to the model is the uncertainty score3 and the output is a binary pre- diction of whether its prediction is confidently cor- rect4 or not. The estimator is trained using all the sub-structures together with their correctness on the development set5 and then applied to the queried candidates. For each candidate sub-structure s, the estimator will give it a correctness probability. We 2Directly using uncertainty is another option, but the main trouble is that the model is not well-calibrated. We also tried model calibration by temperature scaling (Guo et al., 2017), but did not find better results. 3We transform the input with a logarithm, which leads to better estimation according to preliminary experiments. 4The specific criterion is that the arg max prediction matches the gold one and its margin is greater than 0.5. Since neural models are usually over-confident, it is hard to decide a confidence threshold. Nevertheless, we find 0.5 a reasonable value for the ratio decision here. 5We re-use the development set for the task model training. estimate the overall error rate as one minus the aver- age correctness probability over all the candidates in the query set Q (all sub-structures in the selected sentences), and set the selection ratio r as this error rate: r = 1 \u2212 1 n (cid:88) s\u2208Q p(correct = 1|s) In this way, the selection ratio can be set adap- tively according to the current model\u2019s capability. If the model is weak and makes many mistakes, we will have a larger ratio which can lead to more dense annotations and richer training signals. As the model is trained with more data and makes fewer errors, the ratio will be tuned down corre- spondingly to avoid wasting annotation budget on already-correctly-predicted sub-structures. As we will see in later experiments, this adaptive scheme is suitable for AL (\u00a73.3). 2.3 Self-training Better utilization of unlabeled data is a promising direction to further enhance model training in AL since unlabeled data are usually freely available from the unlabeled pool. In this work, we adopt self-training (Yarowsky, 1995) for this purpose. The main idea of self-training is to enhance the model training with pseudo labels that are predicted by the current model on the unlabeled data. It has been shown effective for various NLP tasks (Yarowsky, 1995; McClosky et al., 2006; He et al., 2020; Du et al., 2021). For the training of AL mod- els, self-training can be seamlessly incorporated. For FA, the application of self-training is no dif- ferent than that in the conventional scenarios by applying the current model to all the un-annotated instances in the unlabeled pool. The more inter- esting case is on the partially annotated instances in the PA regime. The same motivation from the adaptive ratio scheme (\u00a72.2) also applies here: We select the highly-uncertain sub-structures that are error-prone and the remaining un-selected parts are likely to be correctly predicted; therefore we can trust the predictions on the un-selected sub- structures and include them for training. One more enhancement to apply here is that we could further perform re-inference by incorporating the updated annotations over the selected sub-structures, which can enhance the predictions of un-annotated sub- structures through output dependencies. In this work, we adopt a soft version of self- training through knowledge distillation (KD; Hin- ton et al., 2015). This choice is because we want to avoid the potential negative influences of ambigu- ous predictions (mostly in completely unlabeled instances). One way to mitigate this is to set an uncertainty threshold and only utilize the highly- confident sub-structures. However, it is unclear how to set a proper value, similar to the scenarios in query selection. Therefore, we take the model\u2019s full output predictions as the training targets with- out further processing. Specifically, our self-training objective function is the cross-entropy between the output distribu- tions predicted by the previous model m\u2032 before training and the current model m being trained: L = \u2212 (cid:88) pm\u2032(y|x) log pm(y|x) y\u2208Y Several points are notable here: 1) The previous model is kept unchanged, and we can simply cache its predictions before training; 2) Over the instances that have partial annotations, the predictions should reflect these annotations by incorporating corre- sponding constraints at inference time; 3) For tasks with CRF based models, the output space Y is usu- ally exponentially large and infeasible to explicitly enumerate; we utilize special algorithms (Wang et al., 2021) to deal with this, and more details are presented in Appendix C. Finally, we find it beneficial to include both the pseudo labels and the real annotated gold labels for the model training. With the gold data, the original training loss is adopted, while the KD objective is utilized with the pseudo labels. We simply mix these two types of data with a ratio of 1:1 in the training process, which we find works well. 3 Experiments 3.1 Main Settings Tasks and data. Our experiments6 are conducted over four English tasks. The first two are named entity recognition (NER) and dependency parsing (DPAR), which are representative structured pre- diction tasks for predicting sequence and tree struc- tures. We adopt the CoNLL-2003 English dataset (Tjong Kim Sang and De Meulder, 2003) for NER and the English Web Treebank (EWT) from Uni- versal Dependencies v2.10 (Nivre et al., 2020) for DPAR. Moreover, we explore two more complex IE tasks: Event extraction and relation extraction. 6Our implementation is available at https://github. com/zzsfornlp/zmsp/. Each task involves two pipelined sub-tasks: The first aims to extract the event trigger and/or entity mentions, and the second predicts links between these mentions as event arguments or entity rela- tions. We utilize the ACE05 dataset (Walker et al., 2006) for these IE tasks. AL. For the AL procedure, we adopt settings fol- lowing conventional practices. We use the original training set as the unlabeled data pool to select in- stances. Unless otherwise noted, we set the AL"}, {"question": " What transformation is applied to the input of the model according to preliminary experiments?", "answer": " Transformed with a logarithm", "ref_chunk": "error rate can- not be directly obtained, requiring estimation.2 We adopt a simple one-dimensional logistic regression model for this purpose. The input to the model is the uncertainty score3 and the output is a binary pre- diction of whether its prediction is confidently cor- rect4 or not. The estimator is trained using all the sub-structures together with their correctness on the development set5 and then applied to the queried candidates. For each candidate sub-structure s, the estimator will give it a correctness probability. We 2Directly using uncertainty is another option, but the main trouble is that the model is not well-calibrated. We also tried model calibration by temperature scaling (Guo et al., 2017), but did not find better results. 3We transform the input with a logarithm, which leads to better estimation according to preliminary experiments. 4The specific criterion is that the arg max prediction matches the gold one and its margin is greater than 0.5. Since neural models are usually over-confident, it is hard to decide a confidence threshold. Nevertheless, we find 0.5 a reasonable value for the ratio decision here. 5We re-use the development set for the task model training. estimate the overall error rate as one minus the aver- age correctness probability over all the candidates in the query set Q (all sub-structures in the selected sentences), and set the selection ratio r as this error rate: r = 1 \u2212 1 n (cid:88) s\u2208Q p(correct = 1|s) In this way, the selection ratio can be set adap- tively according to the current model\u2019s capability. If the model is weak and makes many mistakes, we will have a larger ratio which can lead to more dense annotations and richer training signals. As the model is trained with more data and makes fewer errors, the ratio will be tuned down corre- spondingly to avoid wasting annotation budget on already-correctly-predicted sub-structures. As we will see in later experiments, this adaptive scheme is suitable for AL (\u00a73.3). 2.3 Self-training Better utilization of unlabeled data is a promising direction to further enhance model training in AL since unlabeled data are usually freely available from the unlabeled pool. In this work, we adopt self-training (Yarowsky, 1995) for this purpose. The main idea of self-training is to enhance the model training with pseudo labels that are predicted by the current model on the unlabeled data. It has been shown effective for various NLP tasks (Yarowsky, 1995; McClosky et al., 2006; He et al., 2020; Du et al., 2021). For the training of AL mod- els, self-training can be seamlessly incorporated. For FA, the application of self-training is no dif- ferent than that in the conventional scenarios by applying the current model to all the un-annotated instances in the unlabeled pool. The more inter- esting case is on the partially annotated instances in the PA regime. The same motivation from the adaptive ratio scheme (\u00a72.2) also applies here: We select the highly-uncertain sub-structures that are error-prone and the remaining un-selected parts are likely to be correctly predicted; therefore we can trust the predictions on the un-selected sub- structures and include them for training. One more enhancement to apply here is that we could further perform re-inference by incorporating the updated annotations over the selected sub-structures, which can enhance the predictions of un-annotated sub- structures through output dependencies. In this work, we adopt a soft version of self- training through knowledge distillation (KD; Hin- ton et al., 2015). This choice is because we want to avoid the potential negative influences of ambigu- ous predictions (mostly in completely unlabeled instances). One way to mitigate this is to set an uncertainty threshold and only utilize the highly- confident sub-structures. However, it is unclear how to set a proper value, similar to the scenarios in query selection. Therefore, we take the model\u2019s full output predictions as the training targets with- out further processing. Specifically, our self-training objective function is the cross-entropy between the output distribu- tions predicted by the previous model m\u2032 before training and the current model m being trained: L = \u2212 (cid:88) pm\u2032(y|x) log pm(y|x) y\u2208Y Several points are notable here: 1) The previous model is kept unchanged, and we can simply cache its predictions before training; 2) Over the instances that have partial annotations, the predictions should reflect these annotations by incorporating corre- sponding constraints at inference time; 3) For tasks with CRF based models, the output space Y is usu- ally exponentially large and infeasible to explicitly enumerate; we utilize special algorithms (Wang et al., 2021) to deal with this, and more details are presented in Appendix C. Finally, we find it beneficial to include both the pseudo labels and the real annotated gold labels for the model training. With the gold data, the original training loss is adopted, while the KD objective is utilized with the pseudo labels. We simply mix these two types of data with a ratio of 1:1 in the training process, which we find works well. 3 Experiments 3.1 Main Settings Tasks and data. Our experiments6 are conducted over four English tasks. The first two are named entity recognition (NER) and dependency parsing (DPAR), which are representative structured pre- diction tasks for predicting sequence and tree struc- tures. We adopt the CoNLL-2003 English dataset (Tjong Kim Sang and De Meulder, 2003) for NER and the English Web Treebank (EWT) from Uni- versal Dependencies v2.10 (Nivre et al., 2020) for DPAR. Moreover, we explore two more complex IE tasks: Event extraction and relation extraction. 6Our implementation is available at https://github. com/zzsfornlp/zmsp/. Each task involves two pipelined sub-tasks: The first aims to extract the event trigger and/or entity mentions, and the second predicts links between these mentions as event arguments or entity rela- tions. We utilize the ACE05 dataset (Walker et al., 2006) for these IE tasks. AL. For the AL procedure, we adopt settings fol- lowing conventional practices. We use the original training set as the unlabeled data pool to select in- stances. Unless otherwise noted, we set the AL"}, {"question": " How is the overall error rate estimated in the text?", "answer": " As one minus the average correctness probability over all the candidates in the query set", "ref_chunk": "error rate can- not be directly obtained, requiring estimation.2 We adopt a simple one-dimensional logistic regression model for this purpose. The input to the model is the uncertainty score3 and the output is a binary pre- diction of whether its prediction is confidently cor- rect4 or not. The estimator is trained using all the sub-structures together with their correctness on the development set5 and then applied to the queried candidates. For each candidate sub-structure s, the estimator will give it a correctness probability. We 2Directly using uncertainty is another option, but the main trouble is that the model is not well-calibrated. We also tried model calibration by temperature scaling (Guo et al., 2017), but did not find better results. 3We transform the input with a logarithm, which leads to better estimation according to preliminary experiments. 4The specific criterion is that the arg max prediction matches the gold one and its margin is greater than 0.5. Since neural models are usually over-confident, it is hard to decide a confidence threshold. Nevertheless, we find 0.5 a reasonable value for the ratio decision here. 5We re-use the development set for the task model training. estimate the overall error rate as one minus the aver- age correctness probability over all the candidates in the query set Q (all sub-structures in the selected sentences), and set the selection ratio r as this error rate: r = 1 \u2212 1 n (cid:88) s\u2208Q p(correct = 1|s) In this way, the selection ratio can be set adap- tively according to the current model\u2019s capability. If the model is weak and makes many mistakes, we will have a larger ratio which can lead to more dense annotations and richer training signals. As the model is trained with more data and makes fewer errors, the ratio will be tuned down corre- spondingly to avoid wasting annotation budget on already-correctly-predicted sub-structures. As we will see in later experiments, this adaptive scheme is suitable for AL (\u00a73.3). 2.3 Self-training Better utilization of unlabeled data is a promising direction to further enhance model training in AL since unlabeled data are usually freely available from the unlabeled pool. In this work, we adopt self-training (Yarowsky, 1995) for this purpose. The main idea of self-training is to enhance the model training with pseudo labels that are predicted by the current model on the unlabeled data. It has been shown effective for various NLP tasks (Yarowsky, 1995; McClosky et al., 2006; He et al., 2020; Du et al., 2021). For the training of AL mod- els, self-training can be seamlessly incorporated. For FA, the application of self-training is no dif- ferent than that in the conventional scenarios by applying the current model to all the un-annotated instances in the unlabeled pool. The more inter- esting case is on the partially annotated instances in the PA regime. The same motivation from the adaptive ratio scheme (\u00a72.2) also applies here: We select the highly-uncertain sub-structures that are error-prone and the remaining un-selected parts are likely to be correctly predicted; therefore we can trust the predictions on the un-selected sub- structures and include them for training. One more enhancement to apply here is that we could further perform re-inference by incorporating the updated annotations over the selected sub-structures, which can enhance the predictions of un-annotated sub- structures through output dependencies. In this work, we adopt a soft version of self- training through knowledge distillation (KD; Hin- ton et al., 2015). This choice is because we want to avoid the potential negative influences of ambigu- ous predictions (mostly in completely unlabeled instances). One way to mitigate this is to set an uncertainty threshold and only utilize the highly- confident sub-structures. However, it is unclear how to set a proper value, similar to the scenarios in query selection. Therefore, we take the model\u2019s full output predictions as the training targets with- out further processing. Specifically, our self-training objective function is the cross-entropy between the output distribu- tions predicted by the previous model m\u2032 before training and the current model m being trained: L = \u2212 (cid:88) pm\u2032(y|x) log pm(y|x) y\u2208Y Several points are notable here: 1) The previous model is kept unchanged, and we can simply cache its predictions before training; 2) Over the instances that have partial annotations, the predictions should reflect these annotations by incorporating corre- sponding constraints at inference time; 3) For tasks with CRF based models, the output space Y is usu- ally exponentially large and infeasible to explicitly enumerate; we utilize special algorithms (Wang et al., 2021) to deal with this, and more details are presented in Appendix C. Finally, we find it beneficial to include both the pseudo labels and the real annotated gold labels for the model training. With the gold data, the original training loss is adopted, while the KD objective is utilized with the pseudo labels. We simply mix these two types of data with a ratio of 1:1 in the training process, which we find works well. 3 Experiments 3.1 Main Settings Tasks and data. Our experiments6 are conducted over four English tasks. The first two are named entity recognition (NER) and dependency parsing (DPAR), which are representative structured pre- diction tasks for predicting sequence and tree struc- tures. We adopt the CoNLL-2003 English dataset (Tjong Kim Sang and De Meulder, 2003) for NER and the English Web Treebank (EWT) from Uni- versal Dependencies v2.10 (Nivre et al., 2020) for DPAR. Moreover, we explore two more complex IE tasks: Event extraction and relation extraction. 6Our implementation is available at https://github. com/zzsfornlp/zmsp/. Each task involves two pipelined sub-tasks: The first aims to extract the event trigger and/or entity mentions, and the second predicts links between these mentions as event arguments or entity rela- tions. We utilize the ACE05 dataset (Walker et al., 2006) for these IE tasks. AL. For the AL procedure, we adopt settings fol- lowing conventional practices. We use the original training set as the unlabeled data pool to select in- stances. Unless otherwise noted, we set the AL"}, {"question": " What is the application of self-training in AL?", "answer": " Enhancing model training with pseudo labels predicted by the current model on unlabeled data", "ref_chunk": "error rate can- not be directly obtained, requiring estimation.2 We adopt a simple one-dimensional logistic regression model for this purpose. The input to the model is the uncertainty score3 and the output is a binary pre- diction of whether its prediction is confidently cor- rect4 or not. The estimator is trained using all the sub-structures together with their correctness on the development set5 and then applied to the queried candidates. For each candidate sub-structure s, the estimator will give it a correctness probability. We 2Directly using uncertainty is another option, but the main trouble is that the model is not well-calibrated. We also tried model calibration by temperature scaling (Guo et al., 2017), but did not find better results. 3We transform the input with a logarithm, which leads to better estimation according to preliminary experiments. 4The specific criterion is that the arg max prediction matches the gold one and its margin is greater than 0.5. Since neural models are usually over-confident, it is hard to decide a confidence threshold. Nevertheless, we find 0.5 a reasonable value for the ratio decision here. 5We re-use the development set for the task model training. estimate the overall error rate as one minus the aver- age correctness probability over all the candidates in the query set Q (all sub-structures in the selected sentences), and set the selection ratio r as this error rate: r = 1 \u2212 1 n (cid:88) s\u2208Q p(correct = 1|s) In this way, the selection ratio can be set adap- tively according to the current model\u2019s capability. If the model is weak and makes many mistakes, we will have a larger ratio which can lead to more dense annotations and richer training signals. As the model is trained with more data and makes fewer errors, the ratio will be tuned down corre- spondingly to avoid wasting annotation budget on already-correctly-predicted sub-structures. As we will see in later experiments, this adaptive scheme is suitable for AL (\u00a73.3). 2.3 Self-training Better utilization of unlabeled data is a promising direction to further enhance model training in AL since unlabeled data are usually freely available from the unlabeled pool. In this work, we adopt self-training (Yarowsky, 1995) for this purpose. The main idea of self-training is to enhance the model training with pseudo labels that are predicted by the current model on the unlabeled data. It has been shown effective for various NLP tasks (Yarowsky, 1995; McClosky et al., 2006; He et al., 2020; Du et al., 2021). For the training of AL mod- els, self-training can be seamlessly incorporated. For FA, the application of self-training is no dif- ferent than that in the conventional scenarios by applying the current model to all the un-annotated instances in the unlabeled pool. The more inter- esting case is on the partially annotated instances in the PA regime. The same motivation from the adaptive ratio scheme (\u00a72.2) also applies here: We select the highly-uncertain sub-structures that are error-prone and the remaining un-selected parts are likely to be correctly predicted; therefore we can trust the predictions on the un-selected sub- structures and include them for training. One more enhancement to apply here is that we could further perform re-inference by incorporating the updated annotations over the selected sub-structures, which can enhance the predictions of un-annotated sub- structures through output dependencies. In this work, we adopt a soft version of self- training through knowledge distillation (KD; Hin- ton et al., 2015). This choice is because we want to avoid the potential negative influences of ambigu- ous predictions (mostly in completely unlabeled instances). One way to mitigate this is to set an uncertainty threshold and only utilize the highly- confident sub-structures. However, it is unclear how to set a proper value, similar to the scenarios in query selection. Therefore, we take the model\u2019s full output predictions as the training targets with- out further processing. Specifically, our self-training objective function is the cross-entropy between the output distribu- tions predicted by the previous model m\u2032 before training and the current model m being trained: L = \u2212 (cid:88) pm\u2032(y|x) log pm(y|x) y\u2208Y Several points are notable here: 1) The previous model is kept unchanged, and we can simply cache its predictions before training; 2) Over the instances that have partial annotations, the predictions should reflect these annotations by incorporating corre- sponding constraints at inference time; 3) For tasks with CRF based models, the output space Y is usu- ally exponentially large and infeasible to explicitly enumerate; we utilize special algorithms (Wang et al., 2021) to deal with this, and more details are presented in Appendix C. Finally, we find it beneficial to include both the pseudo labels and the real annotated gold labels for the model training. With the gold data, the original training loss is adopted, while the KD objective is utilized with the pseudo labels. We simply mix these two types of data with a ratio of 1:1 in the training process, which we find works well. 3 Experiments 3.1 Main Settings Tasks and data. Our experiments6 are conducted over four English tasks. The first two are named entity recognition (NER) and dependency parsing (DPAR), which are representative structured pre- diction tasks for predicting sequence and tree struc- tures. We adopt the CoNLL-2003 English dataset (Tjong Kim Sang and De Meulder, 2003) for NER and the English Web Treebank (EWT) from Uni- versal Dependencies v2.10 (Nivre et al., 2020) for DPAR. Moreover, we explore two more complex IE tasks: Event extraction and relation extraction. 6Our implementation is available at https://github. com/zzsfornlp/zmsp/. Each task involves two pipelined sub-tasks: The first aims to extract the event trigger and/or entity mentions, and the second predicts links between these mentions as event arguments or entity rela- tions. We utilize the ACE05 dataset (Walker et al., 2006) for these IE tasks. AL. For the AL procedure, we adopt settings fol- lowing conventional practices. We use the original training set as the unlabeled data pool to select in- stances. Unless otherwise noted, we set the AL"}, {"question": " How are the highly-uncertain sub-structures selected for training in the PA regime?", "answer": " Selecting the error-prone sub-structures and including the remaining un-selected parts for training", "ref_chunk": "error rate can- not be directly obtained, requiring estimation.2 We adopt a simple one-dimensional logistic regression model for this purpose. The input to the model is the uncertainty score3 and the output is a binary pre- diction of whether its prediction is confidently cor- rect4 or not. The estimator is trained using all the sub-structures together with their correctness on the development set5 and then applied to the queried candidates. For each candidate sub-structure s, the estimator will give it a correctness probability. We 2Directly using uncertainty is another option, but the main trouble is that the model is not well-calibrated. We also tried model calibration by temperature scaling (Guo et al., 2017), but did not find better results. 3We transform the input with a logarithm, which leads to better estimation according to preliminary experiments. 4The specific criterion is that the arg max prediction matches the gold one and its margin is greater than 0.5. Since neural models are usually over-confident, it is hard to decide a confidence threshold. Nevertheless, we find 0.5 a reasonable value for the ratio decision here. 5We re-use the development set for the task model training. estimate the overall error rate as one minus the aver- age correctness probability over all the candidates in the query set Q (all sub-structures in the selected sentences), and set the selection ratio r as this error rate: r = 1 \u2212 1 n (cid:88) s\u2208Q p(correct = 1|s) In this way, the selection ratio can be set adap- tively according to the current model\u2019s capability. If the model is weak and makes many mistakes, we will have a larger ratio which can lead to more dense annotations and richer training signals. As the model is trained with more data and makes fewer errors, the ratio will be tuned down corre- spondingly to avoid wasting annotation budget on already-correctly-predicted sub-structures. As we will see in later experiments, this adaptive scheme is suitable for AL (\u00a73.3). 2.3 Self-training Better utilization of unlabeled data is a promising direction to further enhance model training in AL since unlabeled data are usually freely available from the unlabeled pool. In this work, we adopt self-training (Yarowsky, 1995) for this purpose. The main idea of self-training is to enhance the model training with pseudo labels that are predicted by the current model on the unlabeled data. It has been shown effective for various NLP tasks (Yarowsky, 1995; McClosky et al., 2006; He et al., 2020; Du et al., 2021). For the training of AL mod- els, self-training can be seamlessly incorporated. For FA, the application of self-training is no dif- ferent than that in the conventional scenarios by applying the current model to all the un-annotated instances in the unlabeled pool. The more inter- esting case is on the partially annotated instances in the PA regime. The same motivation from the adaptive ratio scheme (\u00a72.2) also applies here: We select the highly-uncertain sub-structures that are error-prone and the remaining un-selected parts are likely to be correctly predicted; therefore we can trust the predictions on the un-selected sub- structures and include them for training. One more enhancement to apply here is that we could further perform re-inference by incorporating the updated annotations over the selected sub-structures, which can enhance the predictions of un-annotated sub- structures through output dependencies. In this work, we adopt a soft version of self- training through knowledge distillation (KD; Hin- ton et al., 2015). This choice is because we want to avoid the potential negative influences of ambigu- ous predictions (mostly in completely unlabeled instances). One way to mitigate this is to set an uncertainty threshold and only utilize the highly- confident sub-structures. However, it is unclear how to set a proper value, similar to the scenarios in query selection. Therefore, we take the model\u2019s full output predictions as the training targets with- out further processing. Specifically, our self-training objective function is the cross-entropy between the output distribu- tions predicted by the previous model m\u2032 before training and the current model m being trained: L = \u2212 (cid:88) pm\u2032(y|x) log pm(y|x) y\u2208Y Several points are notable here: 1) The previous model is kept unchanged, and we can simply cache its predictions before training; 2) Over the instances that have partial annotations, the predictions should reflect these annotations by incorporating corre- sponding constraints at inference time; 3) For tasks with CRF based models, the output space Y is usu- ally exponentially large and infeasible to explicitly enumerate; we utilize special algorithms (Wang et al., 2021) to deal with this, and more details are presented in Appendix C. Finally, we find it beneficial to include both the pseudo labels and the real annotated gold labels for the model training. With the gold data, the original training loss is adopted, while the KD objective is utilized with the pseudo labels. We simply mix these two types of data with a ratio of 1:1 in the training process, which we find works well. 3 Experiments 3.1 Main Settings Tasks and data. Our experiments6 are conducted over four English tasks. The first two are named entity recognition (NER) and dependency parsing (DPAR), which are representative structured pre- diction tasks for predicting sequence and tree struc- tures. We adopt the CoNLL-2003 English dataset (Tjong Kim Sang and De Meulder, 2003) for NER and the English Web Treebank (EWT) from Uni- versal Dependencies v2.10 (Nivre et al., 2020) for DPAR. Moreover, we explore two more complex IE tasks: Event extraction and relation extraction. 6Our implementation is available at https://github. com/zzsfornlp/zmsp/. Each task involves two pipelined sub-tasks: The first aims to extract the event trigger and/or entity mentions, and the second predicts links between these mentions as event arguments or entity rela- tions. We utilize the ACE05 dataset (Walker et al., 2006) for these IE tasks. AL. For the AL procedure, we adopt settings fol- lowing conventional practices. We use the original training set as the unlabeled data pool to select in- stances. Unless otherwise noted, we set the AL"}, {"question": " What is the objective function for the soft version of self-training through knowledge distillation?", "answer": " The cross-entropy between the output distributions predicted by the previous model and the current model being trained", "ref_chunk": "error rate can- not be directly obtained, requiring estimation.2 We adopt a simple one-dimensional logistic regression model for this purpose. The input to the model is the uncertainty score3 and the output is a binary pre- diction of whether its prediction is confidently cor- rect4 or not. The estimator is trained using all the sub-structures together with their correctness on the development set5 and then applied to the queried candidates. For each candidate sub-structure s, the estimator will give it a correctness probability. We 2Directly using uncertainty is another option, but the main trouble is that the model is not well-calibrated. We also tried model calibration by temperature scaling (Guo et al., 2017), but did not find better results. 3We transform the input with a logarithm, which leads to better estimation according to preliminary experiments. 4The specific criterion is that the arg max prediction matches the gold one and its margin is greater than 0.5. Since neural models are usually over-confident, it is hard to decide a confidence threshold. Nevertheless, we find 0.5 a reasonable value for the ratio decision here. 5We re-use the development set for the task model training. estimate the overall error rate as one minus the aver- age correctness probability over all the candidates in the query set Q (all sub-structures in the selected sentences), and set the selection ratio r as this error rate: r = 1 \u2212 1 n (cid:88) s\u2208Q p(correct = 1|s) In this way, the selection ratio can be set adap- tively according to the current model\u2019s capability. If the model is weak and makes many mistakes, we will have a larger ratio which can lead to more dense annotations and richer training signals. As the model is trained with more data and makes fewer errors, the ratio will be tuned down corre- spondingly to avoid wasting annotation budget on already-correctly-predicted sub-structures. As we will see in later experiments, this adaptive scheme is suitable for AL (\u00a73.3). 2.3 Self-training Better utilization of unlabeled data is a promising direction to further enhance model training in AL since unlabeled data are usually freely available from the unlabeled pool. In this work, we adopt self-training (Yarowsky, 1995) for this purpose. The main idea of self-training is to enhance the model training with pseudo labels that are predicted by the current model on the unlabeled data. It has been shown effective for various NLP tasks (Yarowsky, 1995; McClosky et al., 2006; He et al., 2020; Du et al., 2021). For the training of AL mod- els, self-training can be seamlessly incorporated. For FA, the application of self-training is no dif- ferent than that in the conventional scenarios by applying the current model to all the un-annotated instances in the unlabeled pool. The more inter- esting case is on the partially annotated instances in the PA regime. The same motivation from the adaptive ratio scheme (\u00a72.2) also applies here: We select the highly-uncertain sub-structures that are error-prone and the remaining un-selected parts are likely to be correctly predicted; therefore we can trust the predictions on the un-selected sub- structures and include them for training. One more enhancement to apply here is that we could further perform re-inference by incorporating the updated annotations over the selected sub-structures, which can enhance the predictions of un-annotated sub- structures through output dependencies. In this work, we adopt a soft version of self- training through knowledge distillation (KD; Hin- ton et al., 2015). This choice is because we want to avoid the potential negative influences of ambigu- ous predictions (mostly in completely unlabeled instances). One way to mitigate this is to set an uncertainty threshold and only utilize the highly- confident sub-structures. However, it is unclear how to set a proper value, similar to the scenarios in query selection. Therefore, we take the model\u2019s full output predictions as the training targets with- out further processing. Specifically, our self-training objective function is the cross-entropy between the output distribu- tions predicted by the previous model m\u2032 before training and the current model m being trained: L = \u2212 (cid:88) pm\u2032(y|x) log pm(y|x) y\u2208Y Several points are notable here: 1) The previous model is kept unchanged, and we can simply cache its predictions before training; 2) Over the instances that have partial annotations, the predictions should reflect these annotations by incorporating corre- sponding constraints at inference time; 3) For tasks with CRF based models, the output space Y is usu- ally exponentially large and infeasible to explicitly enumerate; we utilize special algorithms (Wang et al., 2021) to deal with this, and more details are presented in Appendix C. Finally, we find it beneficial to include both the pseudo labels and the real annotated gold labels for the model training. With the gold data, the original training loss is adopted, while the KD objective is utilized with the pseudo labels. We simply mix these two types of data with a ratio of 1:1 in the training process, which we find works well. 3 Experiments 3.1 Main Settings Tasks and data. Our experiments6 are conducted over four English tasks. The first two are named entity recognition (NER) and dependency parsing (DPAR), which are representative structured pre- diction tasks for predicting sequence and tree struc- tures. We adopt the CoNLL-2003 English dataset (Tjong Kim Sang and De Meulder, 2003) for NER and the English Web Treebank (EWT) from Uni- versal Dependencies v2.10 (Nivre et al., 2020) for DPAR. Moreover, we explore two more complex IE tasks: Event extraction and relation extraction. 6Our implementation is available at https://github. com/zzsfornlp/zmsp/. Each task involves two pipelined sub-tasks: The first aims to extract the event trigger and/or entity mentions, and the second predicts links between these mentions as event arguments or entity rela- tions. We utilize the ACE05 dataset (Walker et al., 2006) for these IE tasks. AL. For the AL procedure, we adopt settings fol- lowing conventional practices. We use the original training set as the unlabeled data pool to select in- stances. Unless otherwise noted, we set the AL"}, {"question": " How are the pseudo labels and the real annotated gold labels mixed for the model training?", "answer": " With a ratio of 1:1 in the training process", "ref_chunk": "error rate can- not be directly obtained, requiring estimation.2 We adopt a simple one-dimensional logistic regression model for this purpose. The input to the model is the uncertainty score3 and the output is a binary pre- diction of whether its prediction is confidently cor- rect4 or not. The estimator is trained using all the sub-structures together with their correctness on the development set5 and then applied to the queried candidates. For each candidate sub-structure s, the estimator will give it a correctness probability. We 2Directly using uncertainty is another option, but the main trouble is that the model is not well-calibrated. We also tried model calibration by temperature scaling (Guo et al., 2017), but did not find better results. 3We transform the input with a logarithm, which leads to better estimation according to preliminary experiments. 4The specific criterion is that the arg max prediction matches the gold one and its margin is greater than 0.5. Since neural models are usually over-confident, it is hard to decide a confidence threshold. Nevertheless, we find 0.5 a reasonable value for the ratio decision here. 5We re-use the development set for the task model training. estimate the overall error rate as one minus the aver- age correctness probability over all the candidates in the query set Q (all sub-structures in the selected sentences), and set the selection ratio r as this error rate: r = 1 \u2212 1 n (cid:88) s\u2208Q p(correct = 1|s) In this way, the selection ratio can be set adap- tively according to the current model\u2019s capability. If the model is weak and makes many mistakes, we will have a larger ratio which can lead to more dense annotations and richer training signals. As the model is trained with more data and makes fewer errors, the ratio will be tuned down corre- spondingly to avoid wasting annotation budget on already-correctly-predicted sub-structures. As we will see in later experiments, this adaptive scheme is suitable for AL (\u00a73.3). 2.3 Self-training Better utilization of unlabeled data is a promising direction to further enhance model training in AL since unlabeled data are usually freely available from the unlabeled pool. In this work, we adopt self-training (Yarowsky, 1995) for this purpose. The main idea of self-training is to enhance the model training with pseudo labels that are predicted by the current model on the unlabeled data. It has been shown effective for various NLP tasks (Yarowsky, 1995; McClosky et al., 2006; He et al., 2020; Du et al., 2021). For the training of AL mod- els, self-training can be seamlessly incorporated. For FA, the application of self-training is no dif- ferent than that in the conventional scenarios by applying the current model to all the un-annotated instances in the unlabeled pool. The more inter- esting case is on the partially annotated instances in the PA regime. The same motivation from the adaptive ratio scheme (\u00a72.2) also applies here: We select the highly-uncertain sub-structures that are error-prone and the remaining un-selected parts are likely to be correctly predicted; therefore we can trust the predictions on the un-selected sub- structures and include them for training. One more enhancement to apply here is that we could further perform re-inference by incorporating the updated annotations over the selected sub-structures, which can enhance the predictions of un-annotated sub- structures through output dependencies. In this work, we adopt a soft version of self- training through knowledge distillation (KD; Hin- ton et al., 2015). This choice is because we want to avoid the potential negative influences of ambigu- ous predictions (mostly in completely unlabeled instances). One way to mitigate this is to set an uncertainty threshold and only utilize the highly- confident sub-structures. However, it is unclear how to set a proper value, similar to the scenarios in query selection. Therefore, we take the model\u2019s full output predictions as the training targets with- out further processing. Specifically, our self-training objective function is the cross-entropy between the output distribu- tions predicted by the previous model m\u2032 before training and the current model m being trained: L = \u2212 (cid:88) pm\u2032(y|x) log pm(y|x) y\u2208Y Several points are notable here: 1) The previous model is kept unchanged, and we can simply cache its predictions before training; 2) Over the instances that have partial annotations, the predictions should reflect these annotations by incorporating corre- sponding constraints at inference time; 3) For tasks with CRF based models, the output space Y is usu- ally exponentially large and infeasible to explicitly enumerate; we utilize special algorithms (Wang et al., 2021) to deal with this, and more details are presented in Appendix C. Finally, we find it beneficial to include both the pseudo labels and the real annotated gold labels for the model training. With the gold data, the original training loss is adopted, while the KD objective is utilized with the pseudo labels. We simply mix these two types of data with a ratio of 1:1 in the training process, which we find works well. 3 Experiments 3.1 Main Settings Tasks and data. Our experiments6 are conducted over four English tasks. The first two are named entity recognition (NER) and dependency parsing (DPAR), which are representative structured pre- diction tasks for predicting sequence and tree struc- tures. We adopt the CoNLL-2003 English dataset (Tjong Kim Sang and De Meulder, 2003) for NER and the English Web Treebank (EWT) from Uni- versal Dependencies v2.10 (Nivre et al., 2020) for DPAR. Moreover, we explore two more complex IE tasks: Event extraction and relation extraction. 6Our implementation is available at https://github. com/zzsfornlp/zmsp/. Each task involves two pipelined sub-tasks: The first aims to extract the event trigger and/or entity mentions, and the second predicts links between these mentions as event arguments or entity rela- tions. We utilize the ACE05 dataset (Walker et al., 2006) for these IE tasks. AL. For the AL procedure, we adopt settings fol- lowing conventional practices. We use the original training set as the unlabeled data pool to select in- stances. Unless otherwise noted, we set the AL"}, {"question": " What English tasks are involved in the experiments mentioned in the text?", "answer": " Named entity recognition (NER), dependency parsing (DPAR), event extraction, and relation extraction", "ref_chunk": "error rate can- not be directly obtained, requiring estimation.2 We adopt a simple one-dimensional logistic regression model for this purpose. The input to the model is the uncertainty score3 and the output is a binary pre- diction of whether its prediction is confidently cor- rect4 or not. The estimator is trained using all the sub-structures together with their correctness on the development set5 and then applied to the queried candidates. For each candidate sub-structure s, the estimator will give it a correctness probability. We 2Directly using uncertainty is another option, but the main trouble is that the model is not well-calibrated. We also tried model calibration by temperature scaling (Guo et al., 2017), but did not find better results. 3We transform the input with a logarithm, which leads to better estimation according to preliminary experiments. 4The specific criterion is that the arg max prediction matches the gold one and its margin is greater than 0.5. Since neural models are usually over-confident, it is hard to decide a confidence threshold. Nevertheless, we find 0.5 a reasonable value for the ratio decision here. 5We re-use the development set for the task model training. estimate the overall error rate as one minus the aver- age correctness probability over all the candidates in the query set Q (all sub-structures in the selected sentences), and set the selection ratio r as this error rate: r = 1 \u2212 1 n (cid:88) s\u2208Q p(correct = 1|s) In this way, the selection ratio can be set adap- tively according to the current model\u2019s capability. If the model is weak and makes many mistakes, we will have a larger ratio which can lead to more dense annotations and richer training signals. As the model is trained with more data and makes fewer errors, the ratio will be tuned down corre- spondingly to avoid wasting annotation budget on already-correctly-predicted sub-structures. As we will see in later experiments, this adaptive scheme is suitable for AL (\u00a73.3). 2.3 Self-training Better utilization of unlabeled data is a promising direction to further enhance model training in AL since unlabeled data are usually freely available from the unlabeled pool. In this work, we adopt self-training (Yarowsky, 1995) for this purpose. The main idea of self-training is to enhance the model training with pseudo labels that are predicted by the current model on the unlabeled data. It has been shown effective for various NLP tasks (Yarowsky, 1995; McClosky et al., 2006; He et al., 2020; Du et al., 2021). For the training of AL mod- els, self-training can be seamlessly incorporated. For FA, the application of self-training is no dif- ferent than that in the conventional scenarios by applying the current model to all the un-annotated instances in the unlabeled pool. The more inter- esting case is on the partially annotated instances in the PA regime. The same motivation from the adaptive ratio scheme (\u00a72.2) also applies here: We select the highly-uncertain sub-structures that are error-prone and the remaining un-selected parts are likely to be correctly predicted; therefore we can trust the predictions on the un-selected sub- structures and include them for training. One more enhancement to apply here is that we could further perform re-inference by incorporating the updated annotations over the selected sub-structures, which can enhance the predictions of un-annotated sub- structures through output dependencies. In this work, we adopt a soft version of self- training through knowledge distillation (KD; Hin- ton et al., 2015). This choice is because we want to avoid the potential negative influences of ambigu- ous predictions (mostly in completely unlabeled instances). One way to mitigate this is to set an uncertainty threshold and only utilize the highly- confident sub-structures. However, it is unclear how to set a proper value, similar to the scenarios in query selection. Therefore, we take the model\u2019s full output predictions as the training targets with- out further processing. Specifically, our self-training objective function is the cross-entropy between the output distribu- tions predicted by the previous model m\u2032 before training and the current model m being trained: L = \u2212 (cid:88) pm\u2032(y|x) log pm(y|x) y\u2208Y Several points are notable here: 1) The previous model is kept unchanged, and we can simply cache its predictions before training; 2) Over the instances that have partial annotations, the predictions should reflect these annotations by incorporating corre- sponding constraints at inference time; 3) For tasks with CRF based models, the output space Y is usu- ally exponentially large and infeasible to explicitly enumerate; we utilize special algorithms (Wang et al., 2021) to deal with this, and more details are presented in Appendix C. Finally, we find it beneficial to include both the pseudo labels and the real annotated gold labels for the model training. With the gold data, the original training loss is adopted, while the KD objective is utilized with the pseudo labels. We simply mix these two types of data with a ratio of 1:1 in the training process, which we find works well. 3 Experiments 3.1 Main Settings Tasks and data. Our experiments6 are conducted over four English tasks. The first two are named entity recognition (NER) and dependency parsing (DPAR), which are representative structured pre- diction tasks for predicting sequence and tree struc- tures. We adopt the CoNLL-2003 English dataset (Tjong Kim Sang and De Meulder, 2003) for NER and the English Web Treebank (EWT) from Uni- versal Dependencies v2.10 (Nivre et al., 2020) for DPAR. Moreover, we explore two more complex IE tasks: Event extraction and relation extraction. 6Our implementation is available at https://github. com/zzsfornlp/zmsp/. Each task involves two pipelined sub-tasks: The first aims to extract the event trigger and/or entity mentions, and the second predicts links between these mentions as event arguments or entity rela- tions. We utilize the ACE05 dataset (Walker et al., 2006) for these IE tasks. AL. For the AL procedure, we adopt settings fol- lowing conventional practices. We use the original training set as the unlabeled data pool to select in- stances. Unless otherwise noted, we set the AL"}], "doc_text": "error rate can- not be directly obtained, requiring estimation.2 We adopt a simple one-dimensional logistic regression model for this purpose. The input to the model is the uncertainty score3 and the output is a binary pre- diction of whether its prediction is confidently cor- rect4 or not. The estimator is trained using all the sub-structures together with their correctness on the development set5 and then applied to the queried candidates. For each candidate sub-structure s, the estimator will give it a correctness probability. We 2Directly using uncertainty is another option, but the main trouble is that the model is not well-calibrated. We also tried model calibration by temperature scaling (Guo et al., 2017), but did not find better results. 3We transform the input with a logarithm, which leads to better estimation according to preliminary experiments. 4The specific criterion is that the arg max prediction matches the gold one and its margin is greater than 0.5. Since neural models are usually over-confident, it is hard to decide a confidence threshold. Nevertheless, we find 0.5 a reasonable value for the ratio decision here. 5We re-use the development set for the task model training. estimate the overall error rate as one minus the aver- age correctness probability over all the candidates in the query set Q (all sub-structures in the selected sentences), and set the selection ratio r as this error rate: r = 1 \u2212 1 n (cid:88) s\u2208Q p(correct = 1|s) In this way, the selection ratio can be set adap- tively according to the current model\u2019s capability. If the model is weak and makes many mistakes, we will have a larger ratio which can lead to more dense annotations and richer training signals. As the model is trained with more data and makes fewer errors, the ratio will be tuned down corre- spondingly to avoid wasting annotation budget on already-correctly-predicted sub-structures. As we will see in later experiments, this adaptive scheme is suitable for AL (\u00a73.3). 2.3 Self-training Better utilization of unlabeled data is a promising direction to further enhance model training in AL since unlabeled data are usually freely available from the unlabeled pool. In this work, we adopt self-training (Yarowsky, 1995) for this purpose. The main idea of self-training is to enhance the model training with pseudo labels that are predicted by the current model on the unlabeled data. It has been shown effective for various NLP tasks (Yarowsky, 1995; McClosky et al., 2006; He et al., 2020; Du et al., 2021). For the training of AL mod- els, self-training can be seamlessly incorporated. For FA, the application of self-training is no dif- ferent than that in the conventional scenarios by applying the current model to all the un-annotated instances in the unlabeled pool. The more inter- esting case is on the partially annotated instances in the PA regime. The same motivation from the adaptive ratio scheme (\u00a72.2) also applies here: We select the highly-uncertain sub-structures that are error-prone and the remaining un-selected parts are likely to be correctly predicted; therefore we can trust the predictions on the un-selected sub- structures and include them for training. One more enhancement to apply here is that we could further perform re-inference by incorporating the updated annotations over the selected sub-structures, which can enhance the predictions of un-annotated sub- structures through output dependencies. In this work, we adopt a soft version of self- training through knowledge distillation (KD; Hin- ton et al., 2015). This choice is because we want to avoid the potential negative influences of ambigu- ous predictions (mostly in completely unlabeled instances). One way to mitigate this is to set an uncertainty threshold and only utilize the highly- confident sub-structures. However, it is unclear how to set a proper value, similar to the scenarios in query selection. Therefore, we take the model\u2019s full output predictions as the training targets with- out further processing. Specifically, our self-training objective function is the cross-entropy between the output distribu- tions predicted by the previous model m\u2032 before training and the current model m being trained: L = \u2212 (cid:88) pm\u2032(y|x) log pm(y|x) y\u2208Y Several points are notable here: 1) The previous model is kept unchanged, and we can simply cache its predictions before training; 2) Over the instances that have partial annotations, the predictions should reflect these annotations by incorporating corre- sponding constraints at inference time; 3) For tasks with CRF based models, the output space Y is usu- ally exponentially large and infeasible to explicitly enumerate; we utilize special algorithms (Wang et al., 2021) to deal with this, and more details are presented in Appendix C. Finally, we find it beneficial to include both the pseudo labels and the real annotated gold labels for the model training. With the gold data, the original training loss is adopted, while the KD objective is utilized with the pseudo labels. We simply mix these two types of data with a ratio of 1:1 in the training process, which we find works well. 3 Experiments 3.1 Main Settings Tasks and data. Our experiments6 are conducted over four English tasks. The first two are named entity recognition (NER) and dependency parsing (DPAR), which are representative structured pre- diction tasks for predicting sequence and tree struc- tures. We adopt the CoNLL-2003 English dataset (Tjong Kim Sang and De Meulder, 2003) for NER and the English Web Treebank (EWT) from Uni- versal Dependencies v2.10 (Nivre et al., 2020) for DPAR. Moreover, we explore two more complex IE tasks: Event extraction and relation extraction. 6Our implementation is available at https://github. com/zzsfornlp/zmsp/. Each task involves two pipelined sub-tasks: The first aims to extract the event trigger and/or entity mentions, and the second predicts links between these mentions as event arguments or entity rela- tions. We utilize the ACE05 dataset (Walker et al., 2006) for these IE tasks. AL. For the AL procedure, we adopt settings fol- lowing conventional practices. We use the original training set as the unlabeled data pool to select in- stances. Unless otherwise noted, we set the AL"}