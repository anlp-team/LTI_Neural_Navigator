{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Justine_Cassell_\"You_might_think_about_slightly_revising_the_title\u201d:_Identifying_Hedges_in_Peer-tutoring_Interactions.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What type of linguistic markers are used to soften the impact of instructions and negative feedback in peer-tutoring interactions?", "answer": " Hedges", "ref_chunk": "\"Youmightthinkaboutslightlyrevisingthetitle\":IdentifyingHedgesinPeer-tutoringInteractionsYannRaphalen1,Chlo\u00e9Clavel2,JustineCassell1,31InriaParis2LTCI,InstitutPolytechniquedeParis,Telecom-Paris3CarnegieMellonUniversityyann.raphalen.pro@gmail.com,justine@cs.cmu.edu,chloe.clavel@telecom-paris.frAbstractHedgesplayanimportantroleinthemanage-mentofconversationalinteraction.Inpeer-tutoring,theyarenotablyusedbytutorsindyads(pairsofinterlocutors)experiencinglowrapporttotonedowntheimpactofinstructionsandnegativefeedback.Pursuingtheobjectiveofbuildingatutoringagentthatmanagesrap-portwithstudentsinordertoimprovelearning,weusedamultimodalpeer-tutoringdatasettoconstructacomputationalframeworkforiden-tifyinghedges.Wecomparedapproachesre-lyingonpre-trainedresourceswithothersthatintegrateinsightsfromthesocialsciencelitera-ture.Ourbestperformanceinvolvedahybridapproachthatoutperformstheexistingbase-linewhilebeingeasiertointerpret.Weemployamodelexplainabilitytooltoexplorethefea-turesthatcharacterizehedgesinpeer-tutoringconversations,andweidentifysomenovelfea-tures,andthebenefitsofsuchahybridmodelapproach.1IntroductionRapport,mostsimplydefinedasthe\u201c...relativeharmonyandsmoothnessofrelationsbetweenpeo-ple...\u201d(Spencer-Oatey,2005),hasbeenshowntoplayaroleinthesuccessofactivitiesasvariedaspsychotherapy(Leach,2005)andsurveyinterview-ing(LuneandBerg,2017).Inpeer-tutoring,rap-port,asmeasuredbytheannotationofthinslicesofvideo,hasbeenshowntobebeneficialforlearningoutcomes(Zhaoetal.,2014;SinhaandCassell,2015).Thelevelofrapportrisesandfallswithconversationalstrategiesdeployedbytutorsandtuteesatappropriatetimes,andasafunctionofthecontentofpriorturns.Thesestrategiesincludeself-disclosure,referringtosharedexperience,and,onthepartoftutors,givinginstructionsinanindirectmanner.Someworkhasattemptedtoautomaticallydetectthesestrategiesintheserviceofintelligenttutors(Zhaoetal.,2016a),butonlyafewstrate-gieshavebeenattempted.Otherworkhascon-centratedona\"socialreasoningmodule\"(Romeroetal.,2017)todecidewhichstrategiesshouldbegeneratedinagivencontext,butindirectnesswasnotamongthestrategiestargeted.Inthispaper,wefocusontheautomaticclassificationofonespe-cificstrategythatisparticularlyimportantforthetutoringdomain,andthereforeimportantforintel-ligenttutors:hedging,asub-partofindirectnessthat\"softens\"whatwesay.Thisworkispartofalargerresearchprogramwiththelong-termgoalofautomaticallygeneratingindirectnessbehaviorsforatutoringagent.\nFigure1:Amockconversationdisplayingeachtypeofhedgedformulation.AccordingtoBrownandLevinson(1987),hedgesarepartofthelinguistictoolsthatinterlocu-torsusetoproducepoliteness,bylimitingthefacethreattotheinterlocutor(basicallybylimitingtheextenttowhichtheinterlocutormightexperienceembarrassmentbecauseofsomekindofpoorper-formance).Anexampleis\"that\u2019skindofawronganswer\".Hedgesarealsofoundwhenspeakerswishtoavoidlosingfacethemselves,forexam-plewhensaying(\"IthinkImighthavetoadd6.\").Madaioetal.(2017)foundthatinapeer-tutoringtask,whenrapportbetweeninterlocutorsislow,tu-teesattemptedmoreproblemsandcorrectlysolvedmoreproblemswhentheirtutorshedgedinstruc-\n2160 Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1: Long Papers, pages 2160 - 2174 May 22-27, 2022 c(cid:13)2022 Association for Computational Linguistics\n\n\ntions,whichlikewisepointstowardsa\"mitigationoffacethreat\"function.Hedgescanalsobeasso-ciatedwithanonverbalcomponent,forexampleavertedeyegazeduringcriticism(BurgoonandKoper,1984).Hedgesarenot,however,alwaysap-propriate,asin\"Ikindofthinkit\u2019srainingtoday.\"whentheinterlocutorscanbothseerain(althoughitmightbetakenashumorous).Thesefactsabouthedgesmotivateawaytoautomaticallydetectthemand,ultimately(althoughnotinthecurrentwork)alsogeneratethem.Inbothcaseswefirsthavetobeabletocharacterizethemusinginterpretablelinguisticfeatures,whichiswhatweaddressinthecurrentpaper.Thus,intheworkdescribedhere,basedonlinguisticdescriptionsofhedges(BrownandLevinson,1987;Fraser,2010),webuiltarule-basedclassifier.Weshowthatthisclassifierincombinationwithadditionalmultimodalinter-pretablecontext-dependentfeaturessignificantlyimprovestheperformanceofamachinelearningmodelforhedges,comparedtoalessinterpretabledeeplearningbaselinefromGoeletal.(2019)us-ingwordembeddings.Wealsoreliedonamachinelearningmodelexplanationtool(LundbergandLee,2017)toinvestigatethelinguisticfeaturesrelatedtohedgesinthecontextofpeer-tutoring,primarilytoseeifwecoulddiscoversurprisingfeaturesthattheclassificationmodelwouldassociatetohedgesinthiscontext,andwedescribethosebelow.Thecodeofthemodelsdescribedinthepaperisalsoprovided.12RelatedworkHedges:AccordingtoFraser(2010),hedgingisarhetoricalstrategythatattenuatesthestrengthofastatement.Onewaytoproduceahedgeisbyalteringthefullsemanticvalueofaparticu-larexpressionthroughPropositionalhedges(alsocalledApproximatorsinPrinceetal.(1982)),asin\"Youarekindofwrong,\"thatreduceprototypical-ity(i.eaccuracyofthecorrespondencebetweenthepropositionandtherealitythatthespeakerseekstodescribe).Propositionalhedgesarerelatedtofuzzylanguage(Lakoff,1975),andthereforetotheproductionofvagueness(Williamson,2002)anduncertainty(Vincze,2014).AsecondkindareRelationalHedges(alsocalledShieldsinPrinceetal.(1982)),suchas\u201cIthinkthatyouarewrong.\u201dor\u201cThedoctorwantsyoutostopsmoking.\u201d,conveyingthatthepropositionis\n1https://github.com/AnonymousHedges/HedgeDetectionconsideredbythespeakerassubjective.Inafurthersub-division,AttributionShields,asin\"Thedoc-torwantsyou...\",theinvolvementofthespeakerinthetruthvalueofthepropositionisnotmadeexplicit,whichallowsspeakersnottotakeastance.Asdescribedabove,Madaioetal.(2017)foundthattutorswhoshowedlowerrapportwiththeirtuteesusedmorehedgedinstructions(theyalsoemployedmorepositivefeedback),howeverthiswasonlythecasefortutorswithagreaterbeliefintheirabilitytotutor.Tuteesinthiscontextsolvedmoreproblemscorrectlywhentheirtutorshedgedinstructions.Noeffectofhedgingwasfoundfordyads(pairsofinterlocutors)withgreatersocialcloseness.However,theauthorsdidnotlookatthespecificlinguisticformstheseteenagersused.Rowland(2007)alsodescribestherolethathedg-ingplaysinthisagegroup,showingthatstudentsusebothrelational(\"IthinkthatJohnissmart.\")andpropositional(\"Johniskindofsmart.\")hedgesformuchthesameshieldingfunctionofdemon-stratinguncertainty,tosavethemfromtheriskofembarrassmentiftheyarewrong.TheauthorobservedthatteensusedfewAdaptors(kindof,somewhat)andpreferredtouseRounders(around,closeto).However,thisstudywasperformedwithanadultandtwochildren,possiblybiasingthere-sultsduetotheparticipationoftheadultinvestiga-tor.Hedgeshavebeenincludedinvirtualtutoringagentsbeforenow.(Howardetal.,2015)integratedhedgesinatutoragentforundergraduatesinCS,asawaytoencouragethestudenttotaketheinitiative.Hedgeshavealsobeenusedasawayofintegrat-ingBrownandLevinson\u2019spolitenessframework(Wangetal.,2008;Schneideretal.,2015)invir-tualtutoringagents.Resultswerenotbrokenoutbystrategy,butpolitenessingeneralwasshowntopositivelyinfluencemotivationandlearning,incertainconditions.Computationalmethodsforhedgedetection:Anumberofstudieshavetargetedthedetectionofhedgesanduncertaintyintext(MedlockandBriscoe,2007;GanterandStrube,2009;Tangetal.,2010;Velldal,2011;Szarvasetal.,2012),partic-ularlyfollowingtheCoNLL2010datasetrelease(Farkasetal.,2010).However,thisworkisnotasrelatedtohedgesinconversation,asitfocusesonaformalandacademiclanguageregister(Hy-land,1998;Varttala,1999).AsnotedbyProkofievaandHirschberg(2014),thefunctionsofhedgesaredomain-andgenre-dependent,thereforethisbias\n2161\n\n\ntowardsformalityimpliesthattheexistingworkmaynotadaptwelltothedetectionofhedgesinconversationbetweenteenagers.Aconsequenceisthattheexistingworkdoesnotconsidertermslike\"Ithink,\"sinceopinionsrarelyappearinanaca-demicwritingdataset.Instructionsarealsoalmostabsent(\"Ithinkyouhavetoaddtentobothsides.\"),astronglimitationforthestudyofconversationalhedgessinceitisinrequests(includingtutoringin-structions)thatindirectformulationsmostlyoccuraccordingtoBlum-Kulka(1987).ProkofievaandHirschberg(2014)alsonotethatitisdifficulttodetecthedgesbecausethewordpatternsassociatedwiththemhaveothersemanticandpragmaticfunc-tions:considering\"Ithinkthatyouhavetoaddxtobothsides.\"vs\"Ithinkthatyouareanidiot.\",itisnotclearthattheseconduseof\"Ithinkthat\"isanhedgemarker.Theyadvocateusingmachinelearningapproachestodealwiththeambiguityofthesemarkers.Workingonaconversationaldataset,Ulinskietal.(2018)builtacomputationalsystemtoassessspeakercommitment(i.e.atwhichpointthespeakerseemsconvincedbythetruthvalueofastatement),inparticularbyrelyingonarule-baseddetectionsystemforhedges.Comparedtothatwork,ourrule-basedclassificationmodelisdirectlydetectinghedgeclasses,andweemploythepredictionsoftherule-basedmodelasafeatureforstrongermachinelearningmodels,designedtolessentheimpactoftheimbalancebetweenclasses.Wealsoconsiderapologieswhentheyserveamit-igationfunction(wethencallthemApologizers),aswasdonebytheauthorsofourcorpus,andwealsousethetermsubjectivizersasdefinedbelow,tobeabletocomparedirectlywiththepreviousworkcarriedoutonthiscorpus.Asfarasweknow,onlyGoeletal.(2019)haveworkedwithapeer-tutoringdataset(thesameonethatwealsouse),andtheyachievedtheirbestclassificationresultbyemployinganAttention-CNNmodel,inspiredbyAdelandSch\u00fctze(2017).3ProblemstatementWeconsiderasetDofconversationsD=(c1,c2,...,c|D|),whereeachconversationiscom-posedofasequenceofindependentsyntacticclausesci=(u1,u2,...,uM),whereMisthenumberofclausesintheconversation.Notethattwoconsecutiveclausescanbeproducedbythesamespeaker.Eachclauseisassociatedwithauniquelabelcorrespondingtothediffer-enthedgeclassesdescribedinTable1:yi\u2208C={PropositionalHedges,Apologizers,Subjec-tivizers,Nothedged}.Finally,anutteranceuicanberepresentedasavectoroffeaturesX=(x1,x2,...,xN),whereNrepresentsthenumberoffeaturesweusedtodescribeaclause.Ourfirstgoalistodesignamodelthatcorrectlypredictsthelabelyiassociatedtoui.Itcanbeunderstoodasthefollowingresearchquestion:RQ1:\"Whichmodelsandfeaturescanbeusedtoautomaticallycharacterizehedgesinapeer-tutoringinteraction?\"Oursecondgoalistoidentify,foreachhedgeclass,thesetoffeaturesFclass={fk},k\u2208[1,N]sortedbyfeatureimportanceintheclassificationofclass.Itcorrespondstothefollowingresearchquestion:RQ2:\"Whatarethemostimportantlinguisticfeaturesthatcharacterizeourhedgeclassesinapeer-tutoringsetting?\"4Methodology4.1CorpusDatacollection:Thedialoguecorpususedherewascollectedaspartofalargerstudyontheeffectsofrapport-buildingonreciprocalpeertutoring.24Americanteenagers(meanage=13.5,min=12,max=15),halfmaleandhalffemale,cametoalabwherehalfoftheparticipantswerepairedwithasame-age,same-genderfriend,andtheotherhalfpairedwithastranger.Theparticipantswereassignedtoatotalof12dyadsinwhichthepar-ticipantsalternatedtutoringoneanotherinlinearalgebraequationsolvingfor5weeklyhour-longsessions,foratotalcorpusofnearly60hoursofface-to-faceinteractions.Eachsessionwasstruc-turedsuchthatthestudentsengagedinbriefsocialchitchatinthebeginning,thenoneofthestudentswasrandomlyassignedtotutortheotherfor20minutes.Theythenengagedinanothersocialpe-riod,andconcludedwithasecondtutoringperiodwheretheotherstudentwasassignedtheroleoftutor.Audioandvideodatawererecorded,tran-scribed,andsegmentedforclause-leveldialogueannotation,providingnearly24000clauses.Non-speechsegments(notablyfillersandlaughter)weremaintained.Becauseoftemporalmisalignmentforpartsofthecorpus,manyparaverbalphenomena,suchasprosody,wereunfortunatelynotavailabletous.SinceouraccesstothedatasetiscoveredbyaNon-DisclosureAgreement,itcannotbereleased\n2162\n\n\n2https://github.com/AnonymousHedges/HedgeDetectionclasses.4.2FeaturesLabelfromrule-basedclassifier(LabelRB):Weusetheclasslabelpredictedbytherule-basedclas-sifierdescribedinSection4.3asafeature.Ourhypothesisisthatthemachinelearningmodelcanusethisinformationtocounterbalancetheclassimbalance.Totakeintoaccountthefactthatsomerulesaremoreefficientthanothers,weweightedtheclasslabelresultingfromtherule-basedmodelbytheprecisionoftherulethatgeneratedit.Unigramandbigram:Wecountthenumberofoccurrencesofunigramsandbigramsofthecorpusineachclause.Weusedthelemmaofthewordsforunigramsandbigramsusingthenltklemmatizer(Loper,2002)andselectedunigramsandbigramsthatoccurredinthetrainingdatasetatleastfiftytimes.Thegoalwastoinvestigate,withabottom-upapproach,towhatextenttheuseofcertainwordscharacterizeshedgeclassesintutoring.InSection5weexaminetheoverlapbetweenthesewordsandthoseaprioriidentifiedbytherules.Part-of-speech(POS):Hedgeclassesseemtobeassociatedwithdifferentsyntacticpatterns:forex-ample,subjectivizersmostoftencontainapersonalpronounfollowedbyaverb,asin\"Iguess\",\"Ibelieve\",\"Ithink\".WethereforeconsideredthenumberofoccurrencesofPOS-Tagn-grams(n=1,2,3)asfeatures.WeusedthespaCyPOS-taggerandconsideredPOSunigrams,bigramsandtri-gramsthatoccuratleast10timesinthetrainingdataset.LIWC:LinguisticInquiryandWordCount(LIWC)(Pennebakeretal.,2015)isstandardsoft-wareforextractingthecountofwordsbelongingtospecificpsycho-socialcategories(e.g.,emotions,religion).Ithasbeensuccessfullyusedinthede-tectionofconversationalstrategies(Zhaoetal.,2016a).Wethereforecountthenumberofoccur-rencesofallthe73categoriesfromLIWC.Tutoringmoves(TM):Intelligenttutoringsys-temsrelyonspecifictutoringmovestosuccess-fullyconveycontent(asdohumantutors).Wethereforelookedatthelinkbetweenthetutoringmoves,asannotatedinMadaioetal.(2017),andhedges.Fortutors,thesemovesare(1)instruc-tionaldirectivesandsuggestions,(2)feedback,and(3)affirmations,mostlyexplicitreflectionsontheirpartners\u2019comprehension,whilefortutees,theyare(1)questions,(2)feedbacks,and(3)affirmations,\npublicly.Howevertheoriginalexperimenters\u2019In-stitutionalReviewBoard(IRB)approvalallowsustoview,annotate,andusethedatatotrainmodels.Thisalsoallowsustoprovidealinktoapixe-latedvideoexampleintheGitHubrepositoryoftheproject2.Dataannotation:Thedatasetwaspreviouslyan-notatedbyMadaioetal.(2017),followingananno-tationmanualthatusedhedgeclassesderivedfromRowland(2007)(seeTable1).Onlythetaskperi-odsoftheinteractionswereannotated.Comparingtheannotationswiththeclassesmentionedintherelatedworksection,SubjectivizerscorrespondtoRelationalhedges(Fraser,2010),PropositionalhedgesandExtenderscorrespondtoApproxima-tors(Princeetal.,1982)withtheadditionofsomediscoursemarkerssuchasjust.ApologizersarementionedaslinguistictoolsrelatedtonegativepolitenessinBrownandLevinson(1987).Krippen-dorff\u2019salphaobtainedforthiscorpusannotatedbyfourcoderswasover0.7forallclasses(denotinganacceptableinter-coderreliabilityaccordingtoKrippendorff(2004)).Thedatasetiswidelyim-balanced,withmorethan90%oftheutterancesbelongingtotheNothedgedclass.Inreviewingthecorpusandtheannotationman-ual,however,wenoticedtwoissues.First,theannotationoftheExtendersclasswasinconsis-tent,leadingtotheExtendersandPropositionalhedgesclassescarryingsimilarsemanticfunctions.WethereforemergedthetwoclassesandgroupedutteranceslabeledasExtendersandthoselabeledasPropositionalhedgesundertheheadingofPropositionalhedges.Second,theannotationofclausescontainingthetokens\"just\"and\"would\"(twotermsoccurringfrequentlyinthedatasetthatarekeycomponentsofPropositionalHedgesandSubjectivizersbutthatarenotinfacthedgesinallcases)wasalsoinconsistent,leadingtovirtuallyallclauseswiththosetwotokensbeingconsideredhedges.Wethereforere-consideredalltheclausesassociatedwithanyofthehedgeclasses,aswellasalltheclausesinthe\"Nothedged\"classthatcontained\"just\"or\"would\".There-annotationwascarriedoutbytwoannotatorswhoachievedaKrippendorff\u2019salphainter-raterreliabilityof.9orbetterforApologizers,Subjectivizers,andPropo-sitionalhedgesbeforeindependentlyre-annotatingtherelevantclauses.Anexampleofare-annotationwasremoving\"Iwouldkillyou!\"fromthehedge\n2163\n\n\nProp.hedgesApologizersSubjectivizersNothedgedTotal\nTable1:Definitionoftheclasses\nFeaturesnameAutomaticextractionVectorsize\nSubjectivizersWordsthatreduceintensityorcertainty\u201cSothenIwoulddividebytwo.\u201dApologizersApologiesusedtosoftendirectspeechacts\u201cOhsorrysixb.\u201dPropositionalhedgesQualifyingwordstoreduceintensityorcertaintyofutterances\u201cIt\u2019sactuallyeight.\u201dExtendersWordsusedtoindicateuncertaintybyreferringtovaguecategories\u201cIt\u2019llbethenumberxorwhatevervariableyouhave.\u201d\n12101286262119223156\nTable2:Distributionoftheclasses\nClassDefinitionExample\nRule-basedlabelYes4UnigramYes~250BigramYes~250POSYes~1200LIWCYes73NonverbalNo24TutoringmovesNo6Total~1800\nTable3:Listofautomaticallyextractedandmanuallyannotatedfeatureswiththeirsize.mostlytentativeanswers.Nonverbalandparaverbalbehaviors:AsinGoeletal.(2019),weincludedthenonverbalandpar-averbalbehaviorsthatarerelatedtohedges.Specif-ically,weconsiderlaughterandsmiles,thathavebeenshowntobeeffectivemethodsofmitiga-tion(Warner-Garcia,2014),cut-offsindicatingself-repairs,fillerslike\"Um\",gazeshifts(annotatedas\u2019GazeatPartner\u2019,\u2019GazeattheMathWorksheet\u2019,and\u2019Gazeelsewhere\u2019),andheadnods.Eachfea-turewaspresenttwiceinthefeaturevector,onetimeforeachinterlocutor.Inter-raterreliabilityfornonverbalbehaviorwas0.89(asmeasuredbyKrippendorff\u2019salpha)foreyegaze,0.75forsmilecount,0.64forsmiledurationand0.99forheadnod.Laughterisalsoreportedinthetranscriptatthewordlevel.Weseparatethetutor\u2019sbehaviorsfromthoseofthetutee.ThecollectionprocessforthesebehaviorsisdetailedfurtherinZhaoetal.(2016b).Theclause-levelfeaturevectorwasnormalizedbythelengthoftheclause(exceptfortherule-basedlabel).Thislengthwasalsoaddedasafeature.Table3presentsanoverviewofthefinalfeaturevector.4.3ClassificationmodelsTheclassificationmodelsusedarepresentedhereaccordingtotheirlevelofintegrationofexternallinguisticknowledge.Rule-basedmodel:OnthebasisoftheannotationmanualusedtoconstructthedatasetfromMadaioetal.(2017),andwithdescriptionsofhedgesfromRowland(2007),Fraser(2010)andBrownandLevinson(1987),weconstructedarule-basedclas-sifierthatmatchesregularexpressionsindicativeofhedges.TherulesaredetailedinTable7intheAppendix.LGBM:Sincehedgesareoftencharacterizedbyexplicitlexicalmarkers,wetestedtheassumptionthatamachinelearningmodelwithaknowledge-drivenrepresentationforclausescouldcompetewithaBERTmodelinperformance,whilebeingmuchmoreinterpretable.WereliedonLightGBM,anensembleofdecisiontreestrainedwithgradi-entboosting(Keetal.,2017).Thismodelwasselectedbecauseofitsperformancewithsmalltrainingdatasetsandbecauseitcanignoreunin-formativefeatures,butalsoforitstrainingspeedcomparedtoalternativeimplementationsofgradi-entboostingmethods.Multi-layerperceptron(MLP):Asasimplebase-line,webuiltamulti-layerperceptronusingthreesetsoffeatures:apre-trainedcontextualrepre-sentationoftheclause(SentBERT;ReimersandGurevych(2019));theconcatenationofthiscon-textualrepresentationoftheclauseandarule-basedlabel(notrelyingonthepreviousclauses);andfinallytheconcatenationofallthefeaturesmen-tionedinsection4.2,withoutthecontextualizedrepresentation.LSTMoverasequenceofclauses:Sinceweareworkingwithconversationaldata,wealsowantedtotestwhethertakingintoac-countthepreviousclauseshelpstodetectthetypeofhedgeclassinthenextclause.Formally,wewanttoinferyiusingyi=maxy\u2208ClassesP(y|X(ui),X(ui\u22121),...,X(ui\u2212K)),whereKisthenumberofpreviousclausesthatthemodelwilltakeintoaccount.The\n2164\n\n\nMLPmodelpresentedaboveinfersyiusingyi=maxy\u2208ClassesP(y|X(ui)),thereforeadifferenceofperformancebetweenthetwomodelswouldbeasignthatusinginformationfromthepreviousclausescouldhelptodetectthehedgedformulationinthecurrentclause.WetestedaLSTMmodelwiththesamerepresentationsforclausesasfortheMLPmodel.CNNwithattention:Goeletal.(2019)estab-lishedtheirbestperformanceonhedgedetec-tionusingaCNNmodelwithadditiveattentionoverword(andnotclause)embeddings.Con-trarytotheMLPandLSTMmodelsmentionedabove,thismodeltriestoinferyiusingyi=maxy\u2208ClassesP(y|g(w0),g(w1),...,g(wL)),withLrepresentingthemaximumclauselengthweal-low,andgrepresentingafunctionthatturnsthewordwj,j\u2208[0,L]intoavectorrepresentation(formoredetails,pleaseseeAdelandSch\u00fctze(2017)).BERT:Tobenefitfromdeepsemanticandcon-textualrepresentationsoftheutterances,wealsofine-tunedBERT(Devlinetal.,2019)onourclas-sificationtask.BERTisapre-trainedTransformersencoder(Vaswanietal.,2017)thathassignificantlyimprovedthestateoftheartonanumberofNLPtasks,includingsentimentanalysis.Itproducesacontextualrepresentationofeachwordinasen-tence,makingitcapableofdisambiguatingthemeaningofwordslike\"think\"or\"just\"thatarerepresentativeofcertainclassesofhedges.BERT,however,isnotablyhardtointerpret.4.4AnalysistoolsLookingatwhichfeaturesimprovetheperfor-manceofourclassificationmodelstellsuswhetherthesefeaturesareinformativeornot,butdoesnotexplainhowthesefeaturesareusedbythemod-elstomakeagivenprediction.Wethereforepro-ducedacomplementaryanalysisusinganinter-pretabilitytool.Asdemonstratedby(LundbergandLee,2017),LightGBMinternalfeatureimpor-tancescoresareinconsistentwithboththemodelbehaviorandhumanintuition,soweinsteadusedamodel-agnostictool.SHAP(LundbergandLee,2017)assignstoeachfeatureanimportancevalue(calledShapleyvalues)foraparticularpredictiondependingontheextentofitscontribution(ade-tailedintroductiontoShapleyvaluesandSHAPcanbefoundinMolnar(2020)).SHAPisamodel-agnosticframework,thereforethevaluesassoci-atedwithasetoffeaturescanbecomparedacrossmodels.ItshouldbenotedthatSHAPproducesexplanationsonacase-by-casebasis,thereforeitcanbothprovidelocalandglobalexplanations.FortheGradientBoostingmodel,weuseanadaptedversionofSHAP(Lundbergetal.,2018),calledTreeSHAP.5Experimentsandresults5.1ExperimentalsettingTodetectthebestsetoffeatures,weusedLight-GBMandproceededincrementally,byaddingthegroupoffeatureswethoughttobemostlikelyasso-ciatedwithhedges.Wedidnotconsidertheriskofrelyingonasub-optimalsetoffeaturesthroughthisprocedurebecauseofthestrongabilityofLight-GBMtoignoreuninformativefeatures.Weusethisincrementalapproachasawaytotestourintuitionabouttheperformativityofgroupsoffeatures(i.e.doesaddingafeatureimprovetheperformanceofthemodel)withregardtothetaskofclassifica-tion.Tocompareourmodels,wetrainedthemonthe4-classtask,andlookedattheaverageoftheweightedF1-scoresforthethreehedgeclasses(i.e.howwellthemodelsinferminorityclasses)thatwereporthereas\"3-classes\",andattheaverageoftheweightedF1-scoresforthe4classes,thatwereportas\"4-classes\".DetailsofthehyperparametersandexperimentalsettingsareprovidedinAppendixA.5.2ModelcomparisonandfeatureanalysisOverallresults:Table4presentstheresultsob-tainedbythe6modelspresentedinSection4.3forthemulti-classproblem.Bestperformance(F1-scoreof79.0)isobtainedwithLightGBMlever-agingalmostallthefeatures.Intheappendix(seeTable8andTable9)weindicatetheconfidenceintervalstorepresentthesignificanceofthediffer-encesbetweenthemodels.First,andperhapssurprisingly,wenoticethattheuseof\"Knowledge-Driven\"featuresbasedonrulesbuiltfromlinguisticknowledgeofhedgesintheLightGBMmodeloutperformstheuseofpre-trainedembeddingswithinafine-tunedBERTmodel(79.0vs.70.6),andintheneuralbaselinefrom(Goeletal.,2019)(79.0vs64.5).ThelowscoresobtainedbytheLGBM,LSTMandMLPmodelswithpre-trainedsentenceem-beddingsversusKnowledge-Drivenfeaturesmightsignalthatthewordpatternscharacterizinghedgesarenotsalientintheserepresentations(i.e.the\n2165\n\n\n68.5(1.6)35.8(3.1)64.8(1.1)Attention-CNN(3-classes)\n65.1(5.7)39.8(8.0)65.2(5.1)BERT(3-classes)\n\u220570.6(2.3)\u2205LGBM(3-classes)\n79.0(1.3)35.0(2.2)70.1(1.4)\nRule-based(3-classes)\n94.7\u2205\u2205MLP(4-classes)\n3Notethatthereisstrongredundancybetweensomefea-turesofLIWCandthespaCyPOStaggerthatbothproducea\"Pronoun\"category,usingalexiconinthefirstcase,andaneuralinferenceinthesecond.\n\u220594.9(0.4)\u2205LGBM(4-classes)\nRule-based(4-classes)\nTable4:AveragedweightedF1-scores(andstandarddeviation)forthethreeminorityclassesandforthe4classes,forallmodels.\"KD\"standsfor\"Knowledge-Driven\",meaningthatthefeaturesarederivedfromlexicon,n-grammodelsandannotations.distancebetween\"Ithinkyoushouldadd5.\"and\"Youshouldadd5.\"isshort.).KDFeaturesseemtoprovideabetterseparabilityoftheclasses.ThecombinationofKDfeaturesandPre-trainedem-beddingsdoesnotsignificantlyimprovetheperfor-manceofthemodelscomparedtotheKDFeaturesonly,whichsuggeststhattheinformationfromthePre-trainedembeddingsisredundantwiththeonefromtheKDFeatures.Thisresultmaybeduetothehighdimensionalityoftheinputvector(868withPCAontheKDFeatures;2500otherwise).Asecondfindingisthattheuseofgradientboost-ingmodelsontopofrule-basedclassifiersbettermodelsthehedgeclasses.Theothermachinelearn-ingmodelsdidnotprovetobeaseffective,exceptforBERT.FeatureanalysisusingLightGBM:Usingthebestperformingmodel,Table5showstheroleofeachfeaturesetinthepredictiontask.ThesignificanceofthedifferencesisshowninTable10andTable11.Comparedtotherule-basedmodel,theintroductionofn-gramssignificantlyimprovedtheperformanceofourclassifier,suggestingthatsomelexicalandsyntacticinformationdescribingthehedgeclasseswasnotpresentintherule-basedmodel.LookingatTable5,wedonotobservesignificantdifferencesbetweentheLGBMmodelusingonlythelabelrulebased+(1-gramsand2-grams)andthemodelsin-corporatingmorefeatures.Tooursurprise,neitherthetutoringmovesnorthenonverbalfeaturessig-nificantlyimprovedtheperformanceofthemodel.The2featureswereincludedtoindexthespecificpeertutoringcontextofthesehedges,sothisindi-catesthatinfutureworkwemightwishtoapplythecurrentmodeltoanothercontextofusetoseeifthismodelofhedgesismoregenerallyapplicablethanweoriginallythought.Bycombiningthisresultwiththeincreasedperformanceofthemodelus-ingKnowledge-Driven(i.e.explicit)featurescom-paredtopre-trainedembeddings,itwouldseemthathedgesareaboveallalexicalphenomenon(i.e.producedbyspecificlexicalelements).5.3In-depthanalysisoftheinformativefeaturesWetrainedtheSHAPexplanationmodelsonLight-GBMwithallfeatures.Themostinformativefea-tures(inabsolutevalue)foreachclassareshowninTable6,andtheplotsbyclassarepresentedintheAppendix.Themostimportantfeaturesseemtobetherule-basedlabels,whichappearinatleastthefourthpositionforthreeclasses(seeTable6),andinthefirstpositionforPropositionalHedgesandNothedgedclasses.Surprisingly,theRule-Basedlabeldoesnotappearinthetop20featuresforApologizers.However,giventhattheclassrarelyappearsinthedata,therulesseldomactivate,sothefeaturemaysimplybeinformativeforaverysmallnumberofclauses.Unigrams(Oh,Sorry,just,Would,andI)arealsopresentinthe5top-rankedfeatures.Thisconfirmsthefindingsmen-tionedinrelatedworkforthecharacterizationofthedifferenthedgeclasses(justwithPropositionalHedges,sorrywithApologizer,IwithSubjectiviz-ers).ThepresenceofOhalsohashighimportanceforthecharacterizationofApologizer(n=2),asillustratedinexamplessuchas\"Ohsorry,that\u2019snine.\".Wenotethattheoccurrencesof\"Ohsorry\"asastand-aloneclausewereexcludedbyourrule-basedmodelbecausetheydonotcorrespondtoanapologizer(theycannotmitigatethecontentofapropositionifthereisnopropositionassociated).Thisexampleillustratestheinterestofamachinelearningmodelapproachtodisambiguatethefunc-tionofconventionalnon-propositionalphraseslike\"Ohsorry\".Inaddition,SHAPhighlightstheimportanceofnovelfeatureswhosefunctionwasnotidentifiedinthehedgesliterature:(i)whatLIWCclassifiesasinformalwordsbutthataremostlyinterjectionslikeahandoharestronglyassociatedwithApol-ogizer,asaredisfluencies(n=12);(ii)theuseofPOStagsseemstobeveryrelevantforcharac-terizingthedifferentclasses(2-gramofPOStagfeatures3occurinthetop-rankedfeaturesofallthe\n\u220564.5(3.0)\u2205LSTM(3-classes)\nKDFeat.(KDF)Pre-TrainedEmb.(PTE)KDF+PTE\n\u220594.4(0.2)\u2205LSTM(4-classes)\n96.7(0.2)91.0(0.2)95.4(0.2)\n94.8(0.3)89.7(0.4)93.9(0.4)Attention-CNN(4-classes)\n93.9(1.4)89.1(1.4)94.1(1.2)BERT(4-classes)\n67.6\u2205\u2205MLP(3-classes)\nModels\n2166\n\n\n95.0(0.2)96.5(0.3)96.5(0.2)96.7(0.2)96.6(0.4)96.7(0.3)\n3-classes\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\n68.8(0.8)78.2(1.6)78.1(1.3)79.0(1.3)78.5(2.4)78.7(1.8)\nTable5:AveragedweightedF1-scoresforthethreeclassesofhedgesandthefourclasses,withanadditiveintegrationofKDFfeaturesintheLightGBMmodel.Thestandarddeviationiscomputedacrossfivefolds.\n4-classes\nModels\nRankApologizerSubjectivizersProp.HedgesNothedged\n1Functionwords(LIWC)\"I\"ClasslabelClasslabel2\"Oh\"(LIWC)\"Yeah\"\"Would\"\"Would\"3\"Sorry\"Noun(POS)\"Just\"\"Yeah\"4Affect(LIWC)ClasslabelFunctionword(LIWC)Noun(POS)5ClauselengthCognitiveprocess(LIWC)Netspeak(LIWC)Cognitiveprocess(LIWC)Table6:Mostimportantclause-levelfeaturesforLightGBMaccordingtotheSHAPanalysis.classes(seeFiguresintheAppendix).Itmeansthattherearesomerecurringsyntacticpatternsineachclass;(iii)Regardingtheutterancesize,aclauseshorterthanthemeanisweaklyassociatedwithdirectness(n=17)whilealongerclausesuggeststhatitcontainsaSubjectivizer(n=6).Apologizersarecharacterizedbyameanclauselength(n=5),withfewvariationsfromit;(iv)Tutoringmovesarenotstrongpredictorsofanyclasses:\"Affirma-tionfromtutor\"istheonlyfeatureappearingasapredictorofPropositionalhedges(n=20).ThisisconsistentwiththefeatureanalysisinTable5,suggestingthattutoringmovesdonotsignificantlyimprovetheperformanceoftheclassifier;(v)Non-verbalbehaviorsdonotappearasimportantfea-turesfortheclassification.Thisiscoherentwithresultsfrom(Goeletal.,2019).Notethatprosodymightplayaroleindetectinginstructionsthattrailoff,but,asdescribed,paraverbalfeatureswerenotavailable;(vi)Wouldplaysanimportantroleintheproductionofhedges,asitisstronglyassociatedtoPropositionalhedges(n=2).Itisinterestingtonotethat,whendesigningtherule-basedclassifier,wesawitdecreaseinperformancewhenwestartedtoincludewouldinourregularexpressionpatterns,probablybecausetheformishardtodisambiguateforadeterministicsystem.WhileexploringtheShapleyvaluesassociatedtoeachclause,weobservedthatfeaturesliketutoringmovesareextremelyinformativeforaverysmallnumberofclauses(thereforenotsignificantlyinflu-encingtheoverallperformanceoftheprediction),andmoreorlessnotinformativefortherest.Infer-ringtheglobalimportanceofafeatureasameanacrosstheshapleyvaluesinthedatasetmaynotbetheonlywaytoexplorethebehaviorofgradi-entboostingmethods.ItmightbemoreusefultoclusterclausesbasedontheimportancethatSHAPgivestothatfeatureinitsclassification,asthiscouldhelpdiscoversub-classesofhedgesthataredifferentiatedfromtherestbytheirinteractionwithaspecificfeature(inthewaythatsomeApologiz-ersarecharacterizedbyan\"oh\").Wealsonotethattheexplanationmodelissensitivetospuri-ouscorrelationsinthedataset,causedbythesmallrepresentationofsomeclass:forexample,\"nine\"(n=7)and\"four\"(n=20)arepositivepredictorsofApologizers.6ConclusionandfutureworkThroughourclassificationperformanceexperi-ments,weshowedthatitispossibletousema-chinelearningmethodstodiminishtheambigu-ityofhedges,andthatthehybridapproachofus-ingrule-basedlabelfeaturesderivedfromsocialscience(includinglinguistics)literaturewithinamachinelearningmodelhelpedsignificantlytoin-creasethemodel\u2019sperformance.Nonverbalbehav-iorsandtutoringmovesdidnotprovideinformationatthesentencelevel;boththeperformanceofthemodelandthefeaturecontributionanalysissug-gestedthattheirimpactonthemodeloutputwasnotstrong.ThisisconsistentwithresultsfromGoeletal.(2019).However,infutureworkwewouldliketoinvestigatethepotentialofmultimodalpat-ternswhenweareabletobettermodelsequentiality(e.g.,negativefeedbackfollowedbyasmile).Re-gardingtheSHAPanalysis,mostofthefeaturesthatareconsideredasimportantarecoherentwiththedefinitionoftheclasses(Iforsubjectivizers,sorryforapologizers,justforpropositionalhedges).However,wediscoveredthatfeatureslikeutterance\n2167\n\n\nsizecanalsoserveasindicatorsofcertainclassesofhedges.AlimitationofSHAPisthatitmakesafeatureindependenceassumption,whichpromptstheexplanatorymodeltounderestimatetheimpor-tanceofredundantfeatures(likepronounsinourwork).Inthefuturewewillexploreexplanatorymodelscapableoftakingintoaccountthecorre-lationbetweenfeaturesinthedatasetlikeSAGE(Covertetal.,2020),butsuitedforveryimbal-anceddatasets.Inthedomainofpeer-tutoring,wewouldliketobeabletofurthertestthelinkbe-tweenhedgesandrapport,andthelinkbetweenhedgesandlearninggainsinthesubjectbeingtu-tored.Asnotedabove,thiskindofstudyrequiresafine-grainedcontrolofthelanguageproducedbyoneoftheinterlocutors,whichisdifficulttoachieveinahuman-humanexperience.Wenotethatthehedgeclassifiercanbeusednotjusttoclassify,butalsotoworktowardsimprovingthegenerationofhedgesfortutoragents.Infutureworkwewillexploreusingtheclassifiertore-rankgenerationoutputs,takingadvantageoftherecur-ringsyntacticpatterns(see(ii)inSection5.3)toimprovethegenerationprocessofhedges,andre-generatingclausesthatdon\u2019tcontainoneofthesesyntacticpatterns.AcknowledgmentsManythankstomembersoftheArticuLaboatIN-RIAParisfortheirpreciousassistance.ThisworkwassupportedinpartbythetheFrenchgovern-mentundermanagementofAgenceNationaledelaRechercheaspartofthe\u201cInvestissementsd\u2019avenir\u201dprogram,referenceANR-19-P3IA-0001(PRAIRIE3IAInstitute).ReferencesHeikeAdelandHinrichSch\u00fctze.2017.Exploringdif-ferentdimensionsofattentionforuncertaintydetec-tion.InProceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputa-tionalLinguistics:Volume1,LongPapers,pages22\u201334,Valencia,Spain.AssociationforComputa-tionalLinguistics.ShoshanaBlum-Kulka.1987.Indirectnessandpolite-nessinrequests:Sameordifferent?Journalofpragmatics,11(2):131\u2013146.PenelopeBrownandStephenCLevinson.1987.Polite-ness:Someuniversalsinlanguageusage,volume4.Cambridgeuniversitypress.JudeeKBurgoonandRandallJKoper.1984.Nonverbalandrelationalcommunicationassociatedwithreti-cence.HumanCommunicationResearch,10(4):601\u2013626.IanCovert,ScottMLundberg,andSu-InLee.2020.Understandingglobalfeaturecontributionswithad-ditiveimportancemeasures.AdvancesinNeuralInformationProcessingSystems,33:17212\u201317223.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforCom-putationalLinguistics:HumanLanguageTechnolo-gies,Volume1(LongandShortPapers),pages4171\u20134186.Rich\u00e1rdFarkas,VeronikaVincze,Gy\u00f6rgyM\u00f3ra,J\u00e1nosCsirik,andGy\u00f6rgySzarvas.2010.Theconll-2010sharedtask:learningtodetecthedgesandtheirscopeinnaturallanguagetext.InProceedingsofthefour-teenthconferenceoncomputationalnaturallanguagelearning\u2013Sharedtask,pages1\u201312.BruceFraser.2010.Pragmaticcompetence:Thecaseofhedging.Newapproachestohedging,1534.ViolaGanterandMichaelStrube.2009.Findinghedgesbychasingweasels:Hedgedetectionusingwikipediatagsandshallowlinguisticfeatures.InProceedingsoftheACL-IJCNLP2009ConferenceShortPapers,pages173\u2013176.PranavGoel,YoichiMatsuyama,MichaelMadaio,andJustineCassell.2019.\u201cithinkitmighthelpifwemultiply,andnotadd\u201d:Detectingindirectnessincon-versation.In9thInternationalWorkshoponSpokenDialogueSystemTechnology,pages27\u201340.Springer.CynthiaHoward,PamelaW.Jordan,BarbaraMariaDiEugenio,andSandraKatz.2015.Shiftingtheload:apeerdialogueagentthatencouragesitshumancollab-oratortocontributemoretoproblemsolving.Interna-tionalJournalofArtificialIntelligenceinEducation,27:101\u2013129.KenHyland.1998.Hedginginscientificresearcharti-cles,volume54.JohnBenjaminsPublishing.GuolinKe,QiMeng,ThomasFinley,TaifengWang,WeiChen,WeidongMa,QiweiYe,andTie-YanLiu.2017.Lightgbm:Ahighlyefficientgradientboost-ingdecisiontree.Advancesinneuralinformationprocessingsystems,30:3146\u20133154.KlausKrippendorff.2004.Reliabilityincontentanaly-sis:Somecommonmisconceptionsandrecommen-dations.Humancommunicationresearch,30(3):411\u2013433.GeorgeLakoff.1975.Hedges:Astudyinmeaningcriteriaandthelogicoffuzzyconcepts.InContem-poraryresearchinphilosophicallogicandlinguisticsemantics,pages221\u2013271.Springer.\n2168\n\n\nMatthewLeach.2005.Rapport:Akeytotreatmentsuc-cess.Complementarytherapiesinclinicalpractice,11:262\u20135.IlyaLoshchilovandFrankHutter.2018.Decoupledweightdecayregularization.InInternationalConfer-enceonLearningRepresentations.ScottMLundberg,GabrielGErion,andSu-InLee.2018.Consistentindividualizedfeatureat-tributionfortreeensembles.arXivpreprintarXiv:1802.03888.ScottMLundbergandSu-InLee.2017.Aunifiedap-proachtointerpretingmodelpredictions.InProceed-ingsofthe31stinternationalconferenceonneuralinformationprocessingsystems,pages4768\u20134777.HowardLuneandBruceLBerg.2017.Qualitativeresearchmethodsforthesocialsciences.Pearson.MichaelMadaio,JustineCassell,andAmyOgan.2017.Theimpactofpeertutors\u2019useofindirectfeedbackandinstructions.Philadelphia,PA:InternationalSo-cietyoftheLearningSciences.BenMedlockandTedBriscoe.2007.Weaklysuper-visedlearningforhedgeclassificationinscientificliterature.InProceedingsofthe45thannualmeetingoftheassociationofcomputationallinguistics,pages992\u2013999.ChristophMolnar.2020.Interpretablemachinelearn-ing.Lulu.com.AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advancesinneuralinformationprocessingsystems,32:8026\u20138037.JamesWPennebaker,RyanLBoyd,KaylaJordan,andKateBlackburn.2015.Thedevelopmentandpsycho-metricpropertiesofliwc2015.Technicalreport.JeffreyPennington,RichardSocher,andChristopherDManning.2014.Glove:Globalvectorsforwordrep-resentation.InProceedingsofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing(EMNLP),pages1532\u20131543.EllenFPrince,JoelFrader,CharlesBosk,etal.1982.Onhedginginphysician-physiciandiscourse.Lin-guisticsandtheProfessions,8(1):83\u201397.AnnaProkofievaandJuliaHirschberg.2014.Hedgingandspeakercommitment.In5thIntl.WorkshoponEmotion,SocialSignals,Sentiment&LinkedOpenData,Reykjavik,Iceland.NilsReimersandIrynaGurevych.2019.Sentence-bert:Sentenceembeddingsusingsiamesebert-networks.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages3982\u20133992.OscarJRomero,RanZhao,andJustineCassell.2017.Cognitive-inspiredconversational-strategyreasonerforsocially-awareagents.InIJCAI,pages3807\u20133813.TimRowland.2007.\u2018wellmaybenotexactly,butit\u2019saroundfiftybasically?\u2019:Vaguelanguageinmath-ematicsclassrooms.InVaguelanguageexplored,pages79\u201396.Springer.SaschaSchneider,SteveNebel,SimonPradel,andG\u00fcn-terDanielRey.2015.Mindyourpsandqs!howpoliteinstructionsaffectlearningwithmultimedia.ComputersinHumanBehavior,51:546\u2013555.TanmaySinhaandJustineCassell.2015.Weclick,wealign,welearn:Impactofinfluenceandconvergenceprocessesonstudentlearningandrapportbuilding.InProceedingsofthe1stWorkshoponModelingINTERPERsonalSynchrONyAndInfLuence,INTER-PERSONAL\u201915,page13\u201320,NewYork,NY,USA.AssociationforComputingMachinery.HelenSpencer-Oatey.2005.(im)politeness,faceandperceptionsofrapport:Unpackagingtheirbasesandinterrelationships.1(1):95\u2013119.Gy\u00f6rgySzarvas,VeronikaVincze,Rich\u00e1rdFarkas,Gy\u00f6rgyM\u00f3ra,andIrynaGurevych.2012.Cross-genreandcross-domaindetectionofsemanticuncer-tainty.ComputationalLinguistics,38(2):335\u2013367.BuzhouTang,XiaolongWang,XuanWang,BoYuan,andShixiFan.2010.Acascademethodfordetectinghedgesandtheirscopeinnaturallanguagetext.InProceedingsoftheFourteenthConferenceonCom-putationalNaturalLanguageLearning\u2013SharedTask,pages13\u201317.MorganUlinski,SethBenjamin,andJuliaHirschberg.2018.Usinghedgedetectiontoimprovecommittedbelieftagging.InProceedingsoftheWorkshoponComputationalSemanticsbeyondEventsandRoles,pages1\u20135.TeppoVarttala.1999.Remarksonthecommunicativefunctionsofhedginginpopularscientificandspecial-istresearcharticlesonmedicine.Englishforspecificpurposes,18(2):177\u2013200.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,\u0141ukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.InAdvancesinneuralinformationpro-cessingsystems,pages5998\u20136008.ErikVelldal.2011.Predictingspeculation:asimpledis-ambiguationapproachtohedgedetectioninbiomed-icalliterature.JournalofBiomedicalSemantics,2(5):1\u201314.VeronikaVincze.2014.Uncertaintydetectioninnaturallanguagetexts.PhD,UniversityofSzeged,page141.\n2169\n\n\nNingWang,WLewisJohnson,RichardEMayer,PaolaRizzo,ErinShaw,andHeatherCollins.2008.Thepolitenesseffect:Pedagogicalagentsandlearningoutcomes.Internationaljournalofhuman-computerstudies,66(2):98\u2013112.ShawnWarner-Garcia.2014.Laughingwhennothing\u2019sfunny:Thepragmaticuseofcopinglaughterinthenegotiationofconversationaldisagreement.Prag-matics,24(1):157\u2013180.TimothyWilliamson.2002.Vagueness.Routledge.RanZhao,AlexandrosPapangelis,andJustineCassell.2014.Towardsadyadiccomputationalmodelofrap-portmanagementforhuman-virtualagentinteraction.InInternationalConferenceonIntelligentVirtualAgents,pages514\u2013527.Springer.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016a.Automaticrecognitionofconversa-tionalstrategiesintheserviceofasocially-awaredialogsystem.InProceedingsofthe17thAnnualMeetingoftheSpecialInterestGrouponDiscourseandDialogue,pages381\u2013392.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016b.Socially-awarevirtualagents:Au-tomaticallyassessingdyadicrapportfromtemporalpatternsofbehavior.InInternationalconferenceonintelligentvirtualagents,pages218\u2013233.Springer.AAdditionalinformationontheexperimentalsettingsWeusedPyTorch(Paszkeetal.,2019)toimple-menttheneuralmodels.Foreachsetoffeatures,hyperparameterswereselectedusingOptuna(Ak-iba,2019),aparametersearchframework.Were-implementedtheAttention-CNNwithGlove(Pen-ningtonetal.,2014)300-Dwordsembeddingsasthevectorrepresentation.Foreachmodels,theresultsarecross-validatedusing5folds(wechose5insteadof10toavoidhavingfoldswithtoofewsamplesperclass).Wecorrectedthelossfunctionforclassimbalancetoforcethemodeltoadaptmoretothelessfrequentclasses.Thestrengthofthiscorrectiondependedonthemodel,andwasselectedbecauseitprovidedasatisfyingcompro-misebetweenfavoringrecallandprecisionintheclassificationresultsofthatmodel.ForLightGBM,a\"squarerootofthesquarerootoftheinverseclassproportion\"correctionwasselected.Neu-ralmodelsweretrainedusingAdamWasanop-timizer(LoshchilovandHutter,2018),andusedareducedfeaturevector,obtainedwiththeap-plicationofPCA(dinit=1800;d=100;99.8%oftheinformationisconserved).Nosignifi-cantperformancedifferenceswereobservedbe-tweentheoriginalvectorandthereducedvectorfortrainingthemodels.TocomputetheSHAPvaluesmentionedinthepaper,wekeptonesplittoperformthe5-splitofthedataset,andleave1splittovalidateandearlystopthemodel,inor-dertoavoidoverfitting.Acompleteconfigura-tionofhyperparametersusedforeachmodelisre-portedintheGitHubrepositorywiththecodeofthepaper:https://github.com/YannRaphalen/Hedges-Detection.TheBERTmodelwasfine-tunedonaNvidiaQuadroRTX8000GPU.BTables\n2170\n\n\n?(whether|if|is|that|it|this)?.*Subj.\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nNoNoYesNoNoNoYesYesYesYesYesBERT(PTE)\nexceptionally|forthemostpart|inamannerofspeaking|\nNoYesNoNoNoYesNoNoYesYesNoMLP(KDF)\nsomethingbetween|essentially|only).*Prop.\nTable8:Significancetableforthe3-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesYesYesYesYesNoYesYesYesNoYesMLP(KDF+PTE)\ntechnically|typically|virtually|approximately|\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nmuch|somewhat|exactly|almost|littlebit|quite|\nClass\nrelatively|roughly|sotosay|strictlyspeaking|\nNoNoYesNoNoYesNoYesYesYesYesLSTM(KDF)\nYesNoNoNoNo+TM\nTable10:Significancetableforthe3-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoNoYesYesYesYesYesYesNoYesYes\n.*(it)(looks|seems|appears)[,]?.*\",\".*(or|and)(that|something|stuff|soforth)Table7:Regexprulesusedfortheclassifier.\nRule-based\nRule-based\n.*(you(might|may)(believe|think)).*Subj.\nYesYesYesYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nYesYesNoYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nNoNoYesNoNoYesNoNoYesYesYesLSTM(KDF)\nYesYesNoYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(i|i\u2019m|you|it\u2019s)(am|are)(apparently|surely)[,]?.*Prop.\nNoNoYesNoNoYesNoYesYesYesYesLSTM(PTE)\nNoNoYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nNoNoYesYesNoYesYesYesYesYesNoLGBM(KDF)\nNoNoYesYesYesYesYesYesYesYesNoLGBM(KDF)\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nYesNoYesYesYesYesYesYesNoYesYes\nYesYesYesYesYesYesYesYesYesYesYesMLP(KDF+PTE)\nregular|regularly|actually|almost|asitwere|basically|\nNoYesYesNoYesYesYesNoYesYesNoMLP(PTE)\nRule(regexp)\n.*(i|i\u2019m|we)?(was|am|wasn\u2019t)??(not)?(sure|certain).*Subj.\n.*(accordingto|presumably).*Subj.\n(?!.*(be|been|was)likeexcuseme)((excuseme|sorry)[w,\u2019]+|[w,\u2019]+(excuseme|sorry))Prop.\n.*(i|you|we)haveto(check|look|verify).*Subj.\n.*(i\u2019m|i|we\u2019re)(am|are)??(apologize|sorry).*Apol.\nonthetallside|parexcellence|particularly|\n(?!what).*(i|we)?(don\u2019t|didn\u2019t|did)??(not)?\ninarealsense|inasense|inaway|largely|literally|\nYesYesYesNoNoNoYesYesYesYesYesBERT(PTE)\nModels\nModels\nModels\nYesYesYesNoNoYesNoNoYesYesYesLSTM(PTE)\nYesNoNoNoNo\nYesYesYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nSubj.\nTable9:Significancetableforthe4-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoYesNoNoNoYesNoNoYesYesNoMLP(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(ifeellikeyou).*Subj.\nlooselyspeaking|kinda|moreorless|mostly|often|\n.*(unlessi).*Apol.\nprobably|canbeviewas|crypto-|especially|essentially|\n.*(ifi\u2019mnotwrong|ifi\u2019mright|ifthat\u2019strue).*Subj.\nYesNoNoNoNo+POS\n(guess|guessed|thought|think|believe|believed|suppose|supposed)\n.*(just|alittle|maybe|actually|sortof|kindof|pretty\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nNoYesYesNoYesYesNoNoYesYesYesMLP(KDF)\nprettymuch|principally|pseudo-|quintessentially|\n2171\n\n\nFigure2:Absoluteaveragedfeaturecontribution,asindicatedbySHAP.Thelongerthebarisforonecolor,themorethefeatureisassociatedwiththeclassrepresentedbythatcolor.\nFigure3:Averagedcontributionoffeaturestothedetectionofthe\"Notindirect\"class,asindicatedbySHAP.Eachdotcorrespondstoaclassifiedclause.Areddotindicatesthatthefeatureispresentintheclause,whileabluedotindicatesthatthefeatureisabsent.Thefartherontherightthedotis,themorethefeaturecontributedtoitsclassificationasahedge.\n2172\n\n\nFigure4:Averagedcontributionoffeaturestothedetectionof\"Apologizers\",asindicatedbySHAP.\nFigure5:Averagedcontributionoffeaturestothedetectionof\"Propositionalhedges\",asindicatedbySHAP.\n2173\n\n\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nYesNoNoNoNo+TM\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nFigure6:Averagedcontributionoffeaturestothedetectionof\"Subjectivizers\",asindicatedbySHAP.\nModels\nYesNoNoNoNo\nTable11:Significancetableforthe4-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesNoNoNoNo+POS\n2174"}, {"question": " What are some conversational strategies that help build rapport in peer-tutoring interactions?", "answer": " Self-disclosure, shared experiences, indirect instruction giving", "ref_chunk": "\"Youmightthinkaboutslightlyrevisingthetitle\":IdentifyingHedgesinPeer-tutoringInteractionsYannRaphalen1,Chlo\u00e9Clavel2,JustineCassell1,31InriaParis2LTCI,InstitutPolytechniquedeParis,Telecom-Paris3CarnegieMellonUniversityyann.raphalen.pro@gmail.com,justine@cs.cmu.edu,chloe.clavel@telecom-paris.frAbstractHedgesplayanimportantroleinthemanage-mentofconversationalinteraction.Inpeer-tutoring,theyarenotablyusedbytutorsindyads(pairsofinterlocutors)experiencinglowrapporttotonedowntheimpactofinstructionsandnegativefeedback.Pursuingtheobjectiveofbuildingatutoringagentthatmanagesrap-portwithstudentsinordertoimprovelearning,weusedamultimodalpeer-tutoringdatasettoconstructacomputationalframeworkforiden-tifyinghedges.Wecomparedapproachesre-lyingonpre-trainedresourceswithothersthatintegrateinsightsfromthesocialsciencelitera-ture.Ourbestperformanceinvolvedahybridapproachthatoutperformstheexistingbase-linewhilebeingeasiertointerpret.Weemployamodelexplainabilitytooltoexplorethefea-turesthatcharacterizehedgesinpeer-tutoringconversations,andweidentifysomenovelfea-tures,andthebenefitsofsuchahybridmodelapproach.1IntroductionRapport,mostsimplydefinedasthe\u201c...relativeharmonyandsmoothnessofrelationsbetweenpeo-ple...\u201d(Spencer-Oatey,2005),hasbeenshowntoplayaroleinthesuccessofactivitiesasvariedaspsychotherapy(Leach,2005)andsurveyinterview-ing(LuneandBerg,2017).Inpeer-tutoring,rap-port,asmeasuredbytheannotationofthinslicesofvideo,hasbeenshowntobebeneficialforlearningoutcomes(Zhaoetal.,2014;SinhaandCassell,2015).Thelevelofrapportrisesandfallswithconversationalstrategiesdeployedbytutorsandtuteesatappropriatetimes,andasafunctionofthecontentofpriorturns.Thesestrategiesincludeself-disclosure,referringtosharedexperience,and,onthepartoftutors,givinginstructionsinanindirectmanner.Someworkhasattemptedtoautomaticallydetectthesestrategiesintheserviceofintelligenttutors(Zhaoetal.,2016a),butonlyafewstrate-gieshavebeenattempted.Otherworkhascon-centratedona\"socialreasoningmodule\"(Romeroetal.,2017)todecidewhichstrategiesshouldbegeneratedinagivencontext,butindirectnesswasnotamongthestrategiestargeted.Inthispaper,wefocusontheautomaticclassificationofonespe-cificstrategythatisparticularlyimportantforthetutoringdomain,andthereforeimportantforintel-ligenttutors:hedging,asub-partofindirectnessthat\"softens\"whatwesay.Thisworkispartofalargerresearchprogramwiththelong-termgoalofautomaticallygeneratingindirectnessbehaviorsforatutoringagent.\nFigure1:Amockconversationdisplayingeachtypeofhedgedformulation.AccordingtoBrownandLevinson(1987),hedgesarepartofthelinguistictoolsthatinterlocu-torsusetoproducepoliteness,bylimitingthefacethreattotheinterlocutor(basicallybylimitingtheextenttowhichtheinterlocutormightexperienceembarrassmentbecauseofsomekindofpoorper-formance).Anexampleis\"that\u2019skindofawronganswer\".Hedgesarealsofoundwhenspeakerswishtoavoidlosingfacethemselves,forexam-plewhensaying(\"IthinkImighthavetoadd6.\").Madaioetal.(2017)foundthatinapeer-tutoringtask,whenrapportbetweeninterlocutorsislow,tu-teesattemptedmoreproblemsandcorrectlysolvedmoreproblemswhentheirtutorshedgedinstruc-\n2160 Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1: Long Papers, pages 2160 - 2174 May 22-27, 2022 c(cid:13)2022 Association for Computational Linguistics\n\n\ntions,whichlikewisepointstowardsa\"mitigationoffacethreat\"function.Hedgescanalsobeasso-ciatedwithanonverbalcomponent,forexampleavertedeyegazeduringcriticism(BurgoonandKoper,1984).Hedgesarenot,however,alwaysap-propriate,asin\"Ikindofthinkit\u2019srainingtoday.\"whentheinterlocutorscanbothseerain(althoughitmightbetakenashumorous).Thesefactsabouthedgesmotivateawaytoautomaticallydetectthemand,ultimately(althoughnotinthecurrentwork)alsogeneratethem.Inbothcaseswefirsthavetobeabletocharacterizethemusinginterpretablelinguisticfeatures,whichiswhatweaddressinthecurrentpaper.Thus,intheworkdescribedhere,basedonlinguisticdescriptionsofhedges(BrownandLevinson,1987;Fraser,2010),webuiltarule-basedclassifier.Weshowthatthisclassifierincombinationwithadditionalmultimodalinter-pretablecontext-dependentfeaturessignificantlyimprovestheperformanceofamachinelearningmodelforhedges,comparedtoalessinterpretabledeeplearningbaselinefromGoeletal.(2019)us-ingwordembeddings.Wealsoreliedonamachinelearningmodelexplanationtool(LundbergandLee,2017)toinvestigatethelinguisticfeaturesrelatedtohedgesinthecontextofpeer-tutoring,primarilytoseeifwecoulddiscoversurprisingfeaturesthattheclassificationmodelwouldassociatetohedgesinthiscontext,andwedescribethosebelow.Thecodeofthemodelsdescribedinthepaperisalsoprovided.12RelatedworkHedges:AccordingtoFraser(2010),hedgingisarhetoricalstrategythatattenuatesthestrengthofastatement.Onewaytoproduceahedgeisbyalteringthefullsemanticvalueofaparticu-larexpressionthroughPropositionalhedges(alsocalledApproximatorsinPrinceetal.(1982)),asin\"Youarekindofwrong,\"thatreduceprototypical-ity(i.eaccuracyofthecorrespondencebetweenthepropositionandtherealitythatthespeakerseekstodescribe).Propositionalhedgesarerelatedtofuzzylanguage(Lakoff,1975),andthereforetotheproductionofvagueness(Williamson,2002)anduncertainty(Vincze,2014).AsecondkindareRelationalHedges(alsocalledShieldsinPrinceetal.(1982)),suchas\u201cIthinkthatyouarewrong.\u201dor\u201cThedoctorwantsyoutostopsmoking.\u201d,conveyingthatthepropositionis\n1https://github.com/AnonymousHedges/HedgeDetectionconsideredbythespeakerassubjective.Inafurthersub-division,AttributionShields,asin\"Thedoc-torwantsyou...\",theinvolvementofthespeakerinthetruthvalueofthepropositionisnotmadeexplicit,whichallowsspeakersnottotakeastance.Asdescribedabove,Madaioetal.(2017)foundthattutorswhoshowedlowerrapportwiththeirtuteesusedmorehedgedinstructions(theyalsoemployedmorepositivefeedback),howeverthiswasonlythecasefortutorswithagreaterbeliefintheirabilitytotutor.Tuteesinthiscontextsolvedmoreproblemscorrectlywhentheirtutorshedgedinstructions.Noeffectofhedgingwasfoundfordyads(pairsofinterlocutors)withgreatersocialcloseness.However,theauthorsdidnotlookatthespecificlinguisticformstheseteenagersused.Rowland(2007)alsodescribestherolethathedg-ingplaysinthisagegroup,showingthatstudentsusebothrelational(\"IthinkthatJohnissmart.\")andpropositional(\"Johniskindofsmart.\")hedgesformuchthesameshieldingfunctionofdemon-stratinguncertainty,tosavethemfromtheriskofembarrassmentiftheyarewrong.TheauthorobservedthatteensusedfewAdaptors(kindof,somewhat)andpreferredtouseRounders(around,closeto).However,thisstudywasperformedwithanadultandtwochildren,possiblybiasingthere-sultsduetotheparticipationoftheadultinvestiga-tor.Hedgeshavebeenincludedinvirtualtutoringagentsbeforenow.(Howardetal.,2015)integratedhedgesinatutoragentforundergraduatesinCS,asawaytoencouragethestudenttotaketheinitiative.Hedgeshavealsobeenusedasawayofintegrat-ingBrownandLevinson\u2019spolitenessframework(Wangetal.,2008;Schneideretal.,2015)invir-tualtutoringagents.Resultswerenotbrokenoutbystrategy,butpolitenessingeneralwasshowntopositivelyinfluencemotivationandlearning,incertainconditions.Computationalmethodsforhedgedetection:Anumberofstudieshavetargetedthedetectionofhedgesanduncertaintyintext(MedlockandBriscoe,2007;GanterandStrube,2009;Tangetal.,2010;Velldal,2011;Szarvasetal.,2012),partic-ularlyfollowingtheCoNLL2010datasetrelease(Farkasetal.,2010).However,thisworkisnotasrelatedtohedgesinconversation,asitfocusesonaformalandacademiclanguageregister(Hy-land,1998;Varttala,1999).AsnotedbyProkofievaandHirschberg(2014),thefunctionsofhedgesaredomain-andgenre-dependent,thereforethisbias\n2161\n\n\ntowardsformalityimpliesthattheexistingworkmaynotadaptwelltothedetectionofhedgesinconversationbetweenteenagers.Aconsequenceisthattheexistingworkdoesnotconsidertermslike\"Ithink,\"sinceopinionsrarelyappearinanaca-demicwritingdataset.Instructionsarealsoalmostabsent(\"Ithinkyouhavetoaddtentobothsides.\"),astronglimitationforthestudyofconversationalhedgessinceitisinrequests(includingtutoringin-structions)thatindirectformulationsmostlyoccuraccordingtoBlum-Kulka(1987).ProkofievaandHirschberg(2014)alsonotethatitisdifficulttodetecthedgesbecausethewordpatternsassociatedwiththemhaveothersemanticandpragmaticfunc-tions:considering\"Ithinkthatyouhavetoaddxtobothsides.\"vs\"Ithinkthatyouareanidiot.\",itisnotclearthattheseconduseof\"Ithinkthat\"isanhedgemarker.Theyadvocateusingmachinelearningapproachestodealwiththeambiguityofthesemarkers.Workingonaconversationaldataset,Ulinskietal.(2018)builtacomputationalsystemtoassessspeakercommitment(i.e.atwhichpointthespeakerseemsconvincedbythetruthvalueofastatement),inparticularbyrelyingonarule-baseddetectionsystemforhedges.Comparedtothatwork,ourrule-basedclassificationmodelisdirectlydetectinghedgeclasses,andweemploythepredictionsoftherule-basedmodelasafeatureforstrongermachinelearningmodels,designedtolessentheimpactoftheimbalancebetweenclasses.Wealsoconsiderapologieswhentheyserveamit-igationfunction(wethencallthemApologizers),aswasdonebytheauthorsofourcorpus,andwealsousethetermsubjectivizersasdefinedbelow,tobeabletocomparedirectlywiththepreviousworkcarriedoutonthiscorpus.Asfarasweknow,onlyGoeletal.(2019)haveworkedwithapeer-tutoringdataset(thesameonethatwealsouse),andtheyachievedtheirbestclassificationresultbyemployinganAttention-CNNmodel,inspiredbyAdelandSch\u00fctze(2017).3ProblemstatementWeconsiderasetDofconversationsD=(c1,c2,...,c|D|),whereeachconversationiscom-posedofasequenceofindependentsyntacticclausesci=(u1,u2,...,uM),whereMisthenumberofclausesintheconversation.Notethattwoconsecutiveclausescanbeproducedbythesamespeaker.Eachclauseisassociatedwithauniquelabelcorrespondingtothediffer-enthedgeclassesdescribedinTable1:yi\u2208C={PropositionalHedges,Apologizers,Subjec-tivizers,Nothedged}.Finally,anutteranceuicanberepresentedasavectoroffeaturesX=(x1,x2,...,xN),whereNrepresentsthenumberoffeaturesweusedtodescribeaclause.Ourfirstgoalistodesignamodelthatcorrectlypredictsthelabelyiassociatedtoui.Itcanbeunderstoodasthefollowingresearchquestion:RQ1:\"Whichmodelsandfeaturescanbeusedtoautomaticallycharacterizehedgesinapeer-tutoringinteraction?\"Oursecondgoalistoidentify,foreachhedgeclass,thesetoffeaturesFclass={fk},k\u2208[1,N]sortedbyfeatureimportanceintheclassificationofclass.Itcorrespondstothefollowingresearchquestion:RQ2:\"Whatarethemostimportantlinguisticfeaturesthatcharacterizeourhedgeclassesinapeer-tutoringsetting?\"4Methodology4.1CorpusDatacollection:Thedialoguecorpususedherewascollectedaspartofalargerstudyontheeffectsofrapport-buildingonreciprocalpeertutoring.24Americanteenagers(meanage=13.5,min=12,max=15),halfmaleandhalffemale,cametoalabwherehalfoftheparticipantswerepairedwithasame-age,same-genderfriend,andtheotherhalfpairedwithastranger.Theparticipantswereassignedtoatotalof12dyadsinwhichthepar-ticipantsalternatedtutoringoneanotherinlinearalgebraequationsolvingfor5weeklyhour-longsessions,foratotalcorpusofnearly60hoursofface-to-faceinteractions.Eachsessionwasstruc-turedsuchthatthestudentsengagedinbriefsocialchitchatinthebeginning,thenoneofthestudentswasrandomlyassignedtotutortheotherfor20minutes.Theythenengagedinanothersocialpe-riod,andconcludedwithasecondtutoringperiodwheretheotherstudentwasassignedtheroleoftutor.Audioandvideodatawererecorded,tran-scribed,andsegmentedforclause-leveldialogueannotation,providingnearly24000clauses.Non-speechsegments(notablyfillersandlaughter)weremaintained.Becauseoftemporalmisalignmentforpartsofthecorpus,manyparaverbalphenomena,suchasprosody,wereunfortunatelynotavailabletous.SinceouraccesstothedatasetiscoveredbyaNon-DisclosureAgreement,itcannotbereleased\n2162\n\n\n2https://github.com/AnonymousHedges/HedgeDetectionclasses.4.2FeaturesLabelfromrule-basedclassifier(LabelRB):Weusetheclasslabelpredictedbytherule-basedclas-sifierdescribedinSection4.3asafeature.Ourhypothesisisthatthemachinelearningmodelcanusethisinformationtocounterbalancetheclassimbalance.Totakeintoaccountthefactthatsomerulesaremoreefficientthanothers,weweightedtheclasslabelresultingfromtherule-basedmodelbytheprecisionoftherulethatgeneratedit.Unigramandbigram:Wecountthenumberofoccurrencesofunigramsandbigramsofthecorpusineachclause.Weusedthelemmaofthewordsforunigramsandbigramsusingthenltklemmatizer(Loper,2002)andselectedunigramsandbigramsthatoccurredinthetrainingdatasetatleastfiftytimes.Thegoalwastoinvestigate,withabottom-upapproach,towhatextenttheuseofcertainwordscharacterizeshedgeclassesintutoring.InSection5weexaminetheoverlapbetweenthesewordsandthoseaprioriidentifiedbytherules.Part-of-speech(POS):Hedgeclassesseemtobeassociatedwithdifferentsyntacticpatterns:forex-ample,subjectivizersmostoftencontainapersonalpronounfollowedbyaverb,asin\"Iguess\",\"Ibelieve\",\"Ithink\".WethereforeconsideredthenumberofoccurrencesofPOS-Tagn-grams(n=1,2,3)asfeatures.WeusedthespaCyPOS-taggerandconsideredPOSunigrams,bigramsandtri-gramsthatoccuratleast10timesinthetrainingdataset.LIWC:LinguisticInquiryandWordCount(LIWC)(Pennebakeretal.,2015)isstandardsoft-wareforextractingthecountofwordsbelongingtospecificpsycho-socialcategories(e.g.,emotions,religion).Ithasbeensuccessfullyusedinthede-tectionofconversationalstrategies(Zhaoetal.,2016a).Wethereforecountthenumberofoccur-rencesofallthe73categoriesfromLIWC.Tutoringmoves(TM):Intelligenttutoringsys-temsrelyonspecifictutoringmovestosuccess-fullyconveycontent(asdohumantutors).Wethereforelookedatthelinkbetweenthetutoringmoves,asannotatedinMadaioetal.(2017),andhedges.Fortutors,thesemovesare(1)instruc-tionaldirectivesandsuggestions,(2)feedback,and(3)affirmations,mostlyexplicitreflectionsontheirpartners\u2019comprehension,whilefortutees,theyare(1)questions,(2)feedbacks,and(3)affirmations,\npublicly.Howevertheoriginalexperimenters\u2019In-stitutionalReviewBoard(IRB)approvalallowsustoview,annotate,andusethedatatotrainmodels.Thisalsoallowsustoprovidealinktoapixe-latedvideoexampleintheGitHubrepositoryoftheproject2.Dataannotation:Thedatasetwaspreviouslyan-notatedbyMadaioetal.(2017),followingananno-tationmanualthatusedhedgeclassesderivedfromRowland(2007)(seeTable1).Onlythetaskperi-odsoftheinteractionswereannotated.Comparingtheannotationswiththeclassesmentionedintherelatedworksection,SubjectivizerscorrespondtoRelationalhedges(Fraser,2010),PropositionalhedgesandExtenderscorrespondtoApproxima-tors(Princeetal.,1982)withtheadditionofsomediscoursemarkerssuchasjust.ApologizersarementionedaslinguistictoolsrelatedtonegativepolitenessinBrownandLevinson(1987).Krippen-dorff\u2019salphaobtainedforthiscorpusannotatedbyfourcoderswasover0.7forallclasses(denotinganacceptableinter-coderreliabilityaccordingtoKrippendorff(2004)).Thedatasetiswidelyim-balanced,withmorethan90%oftheutterancesbelongingtotheNothedgedclass.Inreviewingthecorpusandtheannotationman-ual,however,wenoticedtwoissues.First,theannotationoftheExtendersclasswasinconsis-tent,leadingtotheExtendersandPropositionalhedgesclassescarryingsimilarsemanticfunctions.WethereforemergedthetwoclassesandgroupedutteranceslabeledasExtendersandthoselabeledasPropositionalhedgesundertheheadingofPropositionalhedges.Second,theannotationofclausescontainingthetokens\"just\"and\"would\"(twotermsoccurringfrequentlyinthedatasetthatarekeycomponentsofPropositionalHedgesandSubjectivizersbutthatarenotinfacthedgesinallcases)wasalsoinconsistent,leadingtovirtuallyallclauseswiththosetwotokensbeingconsideredhedges.Wethereforere-consideredalltheclausesassociatedwithanyofthehedgeclasses,aswellasalltheclausesinthe\"Nothedged\"classthatcontained\"just\"or\"would\".There-annotationwascarriedoutbytwoannotatorswhoachievedaKrippendorff\u2019salphainter-raterreliabilityof.9orbetterforApologizers,Subjectivizers,andPropo-sitionalhedgesbeforeindependentlyre-annotatingtherelevantclauses.Anexampleofare-annotationwasremoving\"Iwouldkillyou!\"fromthehedge\n2163\n\n\nProp.hedgesApologizersSubjectivizersNothedgedTotal\nTable1:Definitionoftheclasses\nFeaturesnameAutomaticextractionVectorsize\nSubjectivizersWordsthatreduceintensityorcertainty\u201cSothenIwoulddividebytwo.\u201dApologizersApologiesusedtosoftendirectspeechacts\u201cOhsorrysixb.\u201dPropositionalhedgesQualifyingwordstoreduceintensityorcertaintyofutterances\u201cIt\u2019sactuallyeight.\u201dExtendersWordsusedtoindicateuncertaintybyreferringtovaguecategories\u201cIt\u2019llbethenumberxorwhatevervariableyouhave.\u201d\n12101286262119223156\nTable2:Distributionoftheclasses\nClassDefinitionExample\nRule-basedlabelYes4UnigramYes~250BigramYes~250POSYes~1200LIWCYes73NonverbalNo24TutoringmovesNo6Total~1800\nTable3:Listofautomaticallyextractedandmanuallyannotatedfeatureswiththeirsize.mostlytentativeanswers.Nonverbalandparaverbalbehaviors:AsinGoeletal.(2019),weincludedthenonverbalandpar-averbalbehaviorsthatarerelatedtohedges.Specif-ically,weconsiderlaughterandsmiles,thathavebeenshowntobeeffectivemethodsofmitiga-tion(Warner-Garcia,2014),cut-offsindicatingself-repairs,fillerslike\"Um\",gazeshifts(annotatedas\u2019GazeatPartner\u2019,\u2019GazeattheMathWorksheet\u2019,and\u2019Gazeelsewhere\u2019),andheadnods.Eachfea-turewaspresenttwiceinthefeaturevector,onetimeforeachinterlocutor.Inter-raterreliabilityfornonverbalbehaviorwas0.89(asmeasuredbyKrippendorff\u2019salpha)foreyegaze,0.75forsmilecount,0.64forsmiledurationand0.99forheadnod.Laughterisalsoreportedinthetranscriptatthewordlevel.Weseparatethetutor\u2019sbehaviorsfromthoseofthetutee.ThecollectionprocessforthesebehaviorsisdetailedfurtherinZhaoetal.(2016b).Theclause-levelfeaturevectorwasnormalizedbythelengthoftheclause(exceptfortherule-basedlabel).Thislengthwasalsoaddedasafeature.Table3presentsanoverviewofthefinalfeaturevector.4.3ClassificationmodelsTheclassificationmodelsusedarepresentedhereaccordingtotheirlevelofintegrationofexternallinguisticknowledge.Rule-basedmodel:OnthebasisoftheannotationmanualusedtoconstructthedatasetfromMadaioetal.(2017),andwithdescriptionsofhedgesfromRowland(2007),Fraser(2010)andBrownandLevinson(1987),weconstructedarule-basedclas-sifierthatmatchesregularexpressionsindicativeofhedges.TherulesaredetailedinTable7intheAppendix.LGBM:Sincehedgesareoftencharacterizedbyexplicitlexicalmarkers,wetestedtheassumptionthatamachinelearningmodelwithaknowledge-drivenrepresentationforclausescouldcompetewithaBERTmodelinperformance,whilebeingmuchmoreinterpretable.WereliedonLightGBM,anensembleofdecisiontreestrainedwithgradi-entboosting(Keetal.,2017).Thismodelwasselectedbecauseofitsperformancewithsmalltrainingdatasetsandbecauseitcanignoreunin-formativefeatures,butalsoforitstrainingspeedcomparedtoalternativeimplementationsofgradi-entboostingmethods.Multi-layerperceptron(MLP):Asasimplebase-line,webuiltamulti-layerperceptronusingthreesetsoffeatures:apre-trainedcontextualrepre-sentationoftheclause(SentBERT;ReimersandGurevych(2019));theconcatenationofthiscon-textualrepresentationoftheclauseandarule-basedlabel(notrelyingonthepreviousclauses);andfinallytheconcatenationofallthefeaturesmen-tionedinsection4.2,withoutthecontextualizedrepresentation.LSTMoverasequenceofclauses:Sinceweareworkingwithconversationaldata,wealsowantedtotestwhethertakingintoac-countthepreviousclauseshelpstodetectthetypeofhedgeclassinthenextclause.Formally,wewanttoinferyiusingyi=maxy\u2208ClassesP(y|X(ui),X(ui\u22121),...,X(ui\u2212K)),whereKisthenumberofpreviousclausesthatthemodelwilltakeintoaccount.The\n2164\n\n\nMLPmodelpresentedaboveinfersyiusingyi=maxy\u2208ClassesP(y|X(ui)),thereforeadifferenceofperformancebetweenthetwomodelswouldbeasignthatusinginformationfromthepreviousclausescouldhelptodetectthehedgedformulationinthecurrentclause.WetestedaLSTMmodelwiththesamerepresentationsforclausesasfortheMLPmodel.CNNwithattention:Goeletal.(2019)estab-lishedtheirbestperformanceonhedgedetec-tionusingaCNNmodelwithadditiveattentionoverword(andnotclause)embeddings.Con-trarytotheMLPandLSTMmodelsmentionedabove,thismodeltriestoinferyiusingyi=maxy\u2208ClassesP(y|g(w0),g(w1),...,g(wL)),withLrepresentingthemaximumclauselengthweal-low,andgrepresentingafunctionthatturnsthewordwj,j\u2208[0,L]intoavectorrepresentation(formoredetails,pleaseseeAdelandSch\u00fctze(2017)).BERT:Tobenefitfromdeepsemanticandcon-textualrepresentationsoftheutterances,wealsofine-tunedBERT(Devlinetal.,2019)onourclas-sificationtask.BERTisapre-trainedTransformersencoder(Vaswanietal.,2017)thathassignificantlyimprovedthestateoftheartonanumberofNLPtasks,includingsentimentanalysis.Itproducesacontextualrepresentationofeachwordinasen-tence,makingitcapableofdisambiguatingthemeaningofwordslike\"think\"or\"just\"thatarerepresentativeofcertainclassesofhedges.BERT,however,isnotablyhardtointerpret.4.4AnalysistoolsLookingatwhichfeaturesimprovetheperfor-manceofourclassificationmodelstellsuswhetherthesefeaturesareinformativeornot,butdoesnotexplainhowthesefeaturesareusedbythemod-elstomakeagivenprediction.Wethereforepro-ducedacomplementaryanalysisusinganinter-pretabilitytool.Asdemonstratedby(LundbergandLee,2017),LightGBMinternalfeatureimpor-tancescoresareinconsistentwithboththemodelbehaviorandhumanintuition,soweinsteadusedamodel-agnostictool.SHAP(LundbergandLee,2017)assignstoeachfeatureanimportancevalue(calledShapleyvalues)foraparticularpredictiondependingontheextentofitscontribution(ade-tailedintroductiontoShapleyvaluesandSHAPcanbefoundinMolnar(2020)).SHAPisamodel-agnosticframework,thereforethevaluesassoci-atedwithasetoffeaturescanbecomparedacrossmodels.ItshouldbenotedthatSHAPproducesexplanationsonacase-by-casebasis,thereforeitcanbothprovidelocalandglobalexplanations.FortheGradientBoostingmodel,weuseanadaptedversionofSHAP(Lundbergetal.,2018),calledTreeSHAP.5Experimentsandresults5.1ExperimentalsettingTodetectthebestsetoffeatures,weusedLight-GBMandproceededincrementally,byaddingthegroupoffeatureswethoughttobemostlikelyasso-ciatedwithhedges.Wedidnotconsidertheriskofrelyingonasub-optimalsetoffeaturesthroughthisprocedurebecauseofthestrongabilityofLight-GBMtoignoreuninformativefeatures.Weusethisincrementalapproachasawaytotestourintuitionabouttheperformativityofgroupsoffeatures(i.e.doesaddingafeatureimprovetheperformanceofthemodel)withregardtothetaskofclassifica-tion.Tocompareourmodels,wetrainedthemonthe4-classtask,andlookedattheaverageoftheweightedF1-scoresforthethreehedgeclasses(i.e.howwellthemodelsinferminorityclasses)thatwereporthereas\"3-classes\",andattheaverageoftheweightedF1-scoresforthe4classes,thatwereportas\"4-classes\".DetailsofthehyperparametersandexperimentalsettingsareprovidedinAppendixA.5.2ModelcomparisonandfeatureanalysisOverallresults:Table4presentstheresultsob-tainedbythe6modelspresentedinSection4.3forthemulti-classproblem.Bestperformance(F1-scoreof79.0)isobtainedwithLightGBMlever-agingalmostallthefeatures.Intheappendix(seeTable8andTable9)weindicatetheconfidenceintervalstorepresentthesignificanceofthediffer-encesbetweenthemodels.First,andperhapssurprisingly,wenoticethattheuseof\"Knowledge-Driven\"featuresbasedonrulesbuiltfromlinguisticknowledgeofhedgesintheLightGBMmodeloutperformstheuseofpre-trainedembeddingswithinafine-tunedBERTmodel(79.0vs.70.6),andintheneuralbaselinefrom(Goeletal.,2019)(79.0vs64.5).ThelowscoresobtainedbytheLGBM,LSTMandMLPmodelswithpre-trainedsentenceem-beddingsversusKnowledge-Drivenfeaturesmightsignalthatthewordpatternscharacterizinghedgesarenotsalientintheserepresentations(i.e.the\n2165\n\n\n68.5(1.6)35.8(3.1)64.8(1.1)Attention-CNN(3-classes)\n65.1(5.7)39.8(8.0)65.2(5.1)BERT(3-classes)\n\u220570.6(2.3)\u2205LGBM(3-classes)\n79.0(1.3)35.0(2.2)70.1(1.4)\nRule-based(3-classes)\n94.7\u2205\u2205MLP(4-classes)\n3Notethatthereisstrongredundancybetweensomefea-turesofLIWCandthespaCyPOStaggerthatbothproducea\"Pronoun\"category,usingalexiconinthefirstcase,andaneuralinferenceinthesecond.\n\u220594.9(0.4)\u2205LGBM(4-classes)\nRule-based(4-classes)\nTable4:AveragedweightedF1-scores(andstandarddeviation)forthethreeminorityclassesandforthe4classes,forallmodels.\"KD\"standsfor\"Knowledge-Driven\",meaningthatthefeaturesarederivedfromlexicon,n-grammodelsandannotations.distancebetween\"Ithinkyoushouldadd5.\"and\"Youshouldadd5.\"isshort.).KDFeaturesseemtoprovideabetterseparabilityoftheclasses.ThecombinationofKDfeaturesandPre-trainedem-beddingsdoesnotsignificantlyimprovetheperfor-manceofthemodelscomparedtotheKDFeaturesonly,whichsuggeststhattheinformationfromthePre-trainedembeddingsisredundantwiththeonefromtheKDFeatures.Thisresultmaybeduetothehighdimensionalityoftheinputvector(868withPCAontheKDFeatures;2500otherwise).Asecondfindingisthattheuseofgradientboost-ingmodelsontopofrule-basedclassifiersbettermodelsthehedgeclasses.Theothermachinelearn-ingmodelsdidnotprovetobeaseffective,exceptforBERT.FeatureanalysisusingLightGBM:Usingthebestperformingmodel,Table5showstheroleofeachfeaturesetinthepredictiontask.ThesignificanceofthedifferencesisshowninTable10andTable11.Comparedtotherule-basedmodel,theintroductionofn-gramssignificantlyimprovedtheperformanceofourclassifier,suggestingthatsomelexicalandsyntacticinformationdescribingthehedgeclasseswasnotpresentintherule-basedmodel.LookingatTable5,wedonotobservesignificantdifferencesbetweentheLGBMmodelusingonlythelabelrulebased+(1-gramsand2-grams)andthemodelsin-corporatingmorefeatures.Tooursurprise,neitherthetutoringmovesnorthenonverbalfeaturessig-nificantlyimprovedtheperformanceofthemodel.The2featureswereincludedtoindexthespecificpeertutoringcontextofthesehedges,sothisindi-catesthatinfutureworkwemightwishtoapplythecurrentmodeltoanothercontextofusetoseeifthismodelofhedgesismoregenerallyapplicablethanweoriginallythought.Bycombiningthisresultwiththeincreasedperformanceofthemodelus-ingKnowledge-Driven(i.e.explicit)featurescom-paredtopre-trainedembeddings,itwouldseemthathedgesareaboveallalexicalphenomenon(i.e.producedbyspecificlexicalelements).5.3In-depthanalysisoftheinformativefeaturesWetrainedtheSHAPexplanationmodelsonLight-GBMwithallfeatures.Themostinformativefea-tures(inabsolutevalue)foreachclassareshowninTable6,andtheplotsbyclassarepresentedintheAppendix.Themostimportantfeaturesseemtobetherule-basedlabels,whichappearinatleastthefourthpositionforthreeclasses(seeTable6),andinthefirstpositionforPropositionalHedgesandNothedgedclasses.Surprisingly,theRule-Basedlabeldoesnotappearinthetop20featuresforApologizers.However,giventhattheclassrarelyappearsinthedata,therulesseldomactivate,sothefeaturemaysimplybeinformativeforaverysmallnumberofclauses.Unigrams(Oh,Sorry,just,Would,andI)arealsopresentinthe5top-rankedfeatures.Thisconfirmsthefindingsmen-tionedinrelatedworkforthecharacterizationofthedifferenthedgeclasses(justwithPropositionalHedges,sorrywithApologizer,IwithSubjectiviz-ers).ThepresenceofOhalsohashighimportanceforthecharacterizationofApologizer(n=2),asillustratedinexamplessuchas\"Ohsorry,that\u2019snine.\".Wenotethattheoccurrencesof\"Ohsorry\"asastand-aloneclausewereexcludedbyourrule-basedmodelbecausetheydonotcorrespondtoanapologizer(theycannotmitigatethecontentofapropositionifthereisnopropositionassociated).Thisexampleillustratestheinterestofamachinelearningmodelapproachtodisambiguatethefunc-tionofconventionalnon-propositionalphraseslike\"Ohsorry\".Inaddition,SHAPhighlightstheimportanceofnovelfeatureswhosefunctionwasnotidentifiedinthehedgesliterature:(i)whatLIWCclassifiesasinformalwordsbutthataremostlyinterjectionslikeahandoharestronglyassociatedwithApol-ogizer,asaredisfluencies(n=12);(ii)theuseofPOStagsseemstobeveryrelevantforcharac-terizingthedifferentclasses(2-gramofPOStagfeatures3occurinthetop-rankedfeaturesofallthe\n\u220564.5(3.0)\u2205LSTM(3-classes)\nKDFeat.(KDF)Pre-TrainedEmb.(PTE)KDF+PTE\n\u220594.4(0.2)\u2205LSTM(4-classes)\n96.7(0.2)91.0(0.2)95.4(0.2)\n94.8(0.3)89.7(0.4)93.9(0.4)Attention-CNN(4-classes)\n93.9(1.4)89.1(1.4)94.1(1.2)BERT(4-classes)\n67.6\u2205\u2205MLP(3-classes)\nModels\n2166\n\n\n95.0(0.2)96.5(0.3)96.5(0.2)96.7(0.2)96.6(0.4)96.7(0.3)\n3-classes\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\n68.8(0.8)78.2(1.6)78.1(1.3)79.0(1.3)78.5(2.4)78.7(1.8)\nTable5:AveragedweightedF1-scoresforthethreeclassesofhedgesandthefourclasses,withanadditiveintegrationofKDFfeaturesintheLightGBMmodel.Thestandarddeviationiscomputedacrossfivefolds.\n4-classes\nModels\nRankApologizerSubjectivizersProp.HedgesNothedged\n1Functionwords(LIWC)\"I\"ClasslabelClasslabel2\"Oh\"(LIWC)\"Yeah\"\"Would\"\"Would\"3\"Sorry\"Noun(POS)\"Just\"\"Yeah\"4Affect(LIWC)ClasslabelFunctionword(LIWC)Noun(POS)5ClauselengthCognitiveprocess(LIWC)Netspeak(LIWC)Cognitiveprocess(LIWC)Table6:Mostimportantclause-levelfeaturesforLightGBMaccordingtotheSHAPanalysis.classes(seeFiguresintheAppendix).Itmeansthattherearesomerecurringsyntacticpatternsineachclass;(iii)Regardingtheutterancesize,aclauseshorterthanthemeanisweaklyassociatedwithdirectness(n=17)whilealongerclausesuggeststhatitcontainsaSubjectivizer(n=6).Apologizersarecharacterizedbyameanclauselength(n=5),withfewvariationsfromit;(iv)Tutoringmovesarenotstrongpredictorsofanyclasses:\"Affirma-tionfromtutor\"istheonlyfeatureappearingasapredictorofPropositionalhedges(n=20).ThisisconsistentwiththefeatureanalysisinTable5,suggestingthattutoringmovesdonotsignificantlyimprovetheperformanceoftheclassifier;(v)Non-verbalbehaviorsdonotappearasimportantfea-turesfortheclassification.Thisiscoherentwithresultsfrom(Goeletal.,2019).Notethatprosodymightplayaroleindetectinginstructionsthattrailoff,but,asdescribed,paraverbalfeatureswerenotavailable;(vi)Wouldplaysanimportantroleintheproductionofhedges,asitisstronglyassociatedtoPropositionalhedges(n=2).Itisinterestingtonotethat,whendesigningtherule-basedclassifier,wesawitdecreaseinperformancewhenwestartedtoincludewouldinourregularexpressionpatterns,probablybecausetheformishardtodisambiguateforadeterministicsystem.WhileexploringtheShapleyvaluesassociatedtoeachclause,weobservedthatfeaturesliketutoringmovesareextremelyinformativeforaverysmallnumberofclauses(thereforenotsignificantlyinflu-encingtheoverallperformanceoftheprediction),andmoreorlessnotinformativefortherest.Infer-ringtheglobalimportanceofafeatureasameanacrosstheshapleyvaluesinthedatasetmaynotbetheonlywaytoexplorethebehaviorofgradi-entboostingmethods.ItmightbemoreusefultoclusterclausesbasedontheimportancethatSHAPgivestothatfeatureinitsclassification,asthiscouldhelpdiscoversub-classesofhedgesthataredifferentiatedfromtherestbytheirinteractionwithaspecificfeature(inthewaythatsomeApologiz-ersarecharacterizedbyan\"oh\").Wealsonotethattheexplanationmodelissensitivetospuri-ouscorrelationsinthedataset,causedbythesmallrepresentationofsomeclass:forexample,\"nine\"(n=7)and\"four\"(n=20)arepositivepredictorsofApologizers.6ConclusionandfutureworkThroughourclassificationperformanceexperi-ments,weshowedthatitispossibletousema-chinelearningmethodstodiminishtheambigu-ityofhedges,andthatthehybridapproachofus-ingrule-basedlabelfeaturesderivedfromsocialscience(includinglinguistics)literaturewithinamachinelearningmodelhelpedsignificantlytoin-creasethemodel\u2019sperformance.Nonverbalbehav-iorsandtutoringmovesdidnotprovideinformationatthesentencelevel;boththeperformanceofthemodelandthefeaturecontributionanalysissug-gestedthattheirimpactonthemodeloutputwasnotstrong.ThisisconsistentwithresultsfromGoeletal.(2019).However,infutureworkwewouldliketoinvestigatethepotentialofmultimodalpat-ternswhenweareabletobettermodelsequentiality(e.g.,negativefeedbackfollowedbyasmile).Re-gardingtheSHAPanalysis,mostofthefeaturesthatareconsideredasimportantarecoherentwiththedefinitionoftheclasses(Iforsubjectivizers,sorryforapologizers,justforpropositionalhedges).However,wediscoveredthatfeatureslikeutterance\n2167\n\n\nsizecanalsoserveasindicatorsofcertainclassesofhedges.AlimitationofSHAPisthatitmakesafeatureindependenceassumption,whichpromptstheexplanatorymodeltounderestimatetheimpor-tanceofredundantfeatures(likepronounsinourwork).Inthefuturewewillexploreexplanatorymodelscapableoftakingintoaccountthecorre-lationbetweenfeaturesinthedatasetlikeSAGE(Covertetal.,2020),butsuitedforveryimbal-anceddatasets.Inthedomainofpeer-tutoring,wewouldliketobeabletofurthertestthelinkbe-tweenhedgesandrapport,andthelinkbetweenhedgesandlearninggainsinthesubjectbeingtu-tored.Asnotedabove,thiskindofstudyrequiresafine-grainedcontrolofthelanguageproducedbyoneoftheinterlocutors,whichisdifficulttoachieveinahuman-humanexperience.Wenotethatthehedgeclassifiercanbeusednotjusttoclassify,butalsotoworktowardsimprovingthegenerationofhedgesfortutoragents.Infutureworkwewillexploreusingtheclassifiertore-rankgenerationoutputs,takingadvantageoftherecur-ringsyntacticpatterns(see(ii)inSection5.3)toimprovethegenerationprocessofhedges,andre-generatingclausesthatdon\u2019tcontainoneofthesesyntacticpatterns.AcknowledgmentsManythankstomembersoftheArticuLaboatIN-RIAParisfortheirpreciousassistance.ThisworkwassupportedinpartbythetheFrenchgovern-mentundermanagementofAgenceNationaledelaRechercheaspartofthe\u201cInvestissementsd\u2019avenir\u201dprogram,referenceANR-19-P3IA-0001(PRAIRIE3IAInstitute).ReferencesHeikeAdelandHinrichSch\u00fctze.2017.Exploringdif-ferentdimensionsofattentionforuncertaintydetec-tion.InProceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputa-tionalLinguistics:Volume1,LongPapers,pages22\u201334,Valencia,Spain.AssociationforComputa-tionalLinguistics.ShoshanaBlum-Kulka.1987.Indirectnessandpolite-nessinrequests:Sameordifferent?Journalofpragmatics,11(2):131\u2013146.PenelopeBrownandStephenCLevinson.1987.Polite-ness:Someuniversalsinlanguageusage,volume4.Cambridgeuniversitypress.JudeeKBurgoonandRandallJKoper.1984.Nonverbalandrelationalcommunicationassociatedwithreti-cence.HumanCommunicationResearch,10(4):601\u2013626.IanCovert,ScottMLundberg,andSu-InLee.2020.Understandingglobalfeaturecontributionswithad-ditiveimportancemeasures.AdvancesinNeuralInformationProcessingSystems,33:17212\u201317223.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforCom-putationalLinguistics:HumanLanguageTechnolo-gies,Volume1(LongandShortPapers),pages4171\u20134186.Rich\u00e1rdFarkas,VeronikaVincze,Gy\u00f6rgyM\u00f3ra,J\u00e1nosCsirik,andGy\u00f6rgySzarvas.2010.Theconll-2010sharedtask:learningtodetecthedgesandtheirscopeinnaturallanguagetext.InProceedingsofthefour-teenthconferenceoncomputationalnaturallanguagelearning\u2013Sharedtask,pages1\u201312.BruceFraser.2010.Pragmaticcompetence:Thecaseofhedging.Newapproachestohedging,1534.ViolaGanterandMichaelStrube.2009.Findinghedgesbychasingweasels:Hedgedetectionusingwikipediatagsandshallowlinguisticfeatures.InProceedingsoftheACL-IJCNLP2009ConferenceShortPapers,pages173\u2013176.PranavGoel,YoichiMatsuyama,MichaelMadaio,andJustineCassell.2019.\u201cithinkitmighthelpifwemultiply,andnotadd\u201d:Detectingindirectnessincon-versation.In9thInternationalWorkshoponSpokenDialogueSystemTechnology,pages27\u201340.Springer.CynthiaHoward,PamelaW.Jordan,BarbaraMariaDiEugenio,andSandraKatz.2015.Shiftingtheload:apeerdialogueagentthatencouragesitshumancollab-oratortocontributemoretoproblemsolving.Interna-tionalJournalofArtificialIntelligenceinEducation,27:101\u2013129.KenHyland.1998.Hedginginscientificresearcharti-cles,volume54.JohnBenjaminsPublishing.GuolinKe,QiMeng,ThomasFinley,TaifengWang,WeiChen,WeidongMa,QiweiYe,andTie-YanLiu.2017.Lightgbm:Ahighlyefficientgradientboost-ingdecisiontree.Advancesinneuralinformationprocessingsystems,30:3146\u20133154.KlausKrippendorff.2004.Reliabilityincontentanaly-sis:Somecommonmisconceptionsandrecommen-dations.Humancommunicationresearch,30(3):411\u2013433.GeorgeLakoff.1975.Hedges:Astudyinmeaningcriteriaandthelogicoffuzzyconcepts.InContem-poraryresearchinphilosophicallogicandlinguisticsemantics,pages221\u2013271.Springer.\n2168\n\n\nMatthewLeach.2005.Rapport:Akeytotreatmentsuc-cess.Complementarytherapiesinclinicalpractice,11:262\u20135.IlyaLoshchilovandFrankHutter.2018.Decoupledweightdecayregularization.InInternationalConfer-enceonLearningRepresentations.ScottMLundberg,GabrielGErion,andSu-InLee.2018.Consistentindividualizedfeatureat-tributionfortreeensembles.arXivpreprintarXiv:1802.03888.ScottMLundbergandSu-InLee.2017.Aunifiedap-proachtointerpretingmodelpredictions.InProceed-ingsofthe31stinternationalconferenceonneuralinformationprocessingsystems,pages4768\u20134777.HowardLuneandBruceLBerg.2017.Qualitativeresearchmethodsforthesocialsciences.Pearson.MichaelMadaio,JustineCassell,andAmyOgan.2017.Theimpactofpeertutors\u2019useofindirectfeedbackandinstructions.Philadelphia,PA:InternationalSo-cietyoftheLearningSciences.BenMedlockandTedBriscoe.2007.Weaklysuper-visedlearningforhedgeclassificationinscientificliterature.InProceedingsofthe45thannualmeetingoftheassociationofcomputationallinguistics,pages992\u2013999.ChristophMolnar.2020.Interpretablemachinelearn-ing.Lulu.com.AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advancesinneuralinformationprocessingsystems,32:8026\u20138037.JamesWPennebaker,RyanLBoyd,KaylaJordan,andKateBlackburn.2015.Thedevelopmentandpsycho-metricpropertiesofliwc2015.Technicalreport.JeffreyPennington,RichardSocher,andChristopherDManning.2014.Glove:Globalvectorsforwordrep-resentation.InProceedingsofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing(EMNLP),pages1532\u20131543.EllenFPrince,JoelFrader,CharlesBosk,etal.1982.Onhedginginphysician-physiciandiscourse.Lin-guisticsandtheProfessions,8(1):83\u201397.AnnaProkofievaandJuliaHirschberg.2014.Hedgingandspeakercommitment.In5thIntl.WorkshoponEmotion,SocialSignals,Sentiment&LinkedOpenData,Reykjavik,Iceland.NilsReimersandIrynaGurevych.2019.Sentence-bert:Sentenceembeddingsusingsiamesebert-networks.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages3982\u20133992.OscarJRomero,RanZhao,andJustineCassell.2017.Cognitive-inspiredconversational-strategyreasonerforsocially-awareagents.InIJCAI,pages3807\u20133813.TimRowland.2007.\u2018wellmaybenotexactly,butit\u2019saroundfiftybasically?\u2019:Vaguelanguageinmath-ematicsclassrooms.InVaguelanguageexplored,pages79\u201396.Springer.SaschaSchneider,SteveNebel,SimonPradel,andG\u00fcn-terDanielRey.2015.Mindyourpsandqs!howpoliteinstructionsaffectlearningwithmultimedia.ComputersinHumanBehavior,51:546\u2013555.TanmaySinhaandJustineCassell.2015.Weclick,wealign,welearn:Impactofinfluenceandconvergenceprocessesonstudentlearningandrapportbuilding.InProceedingsofthe1stWorkshoponModelingINTERPERsonalSynchrONyAndInfLuence,INTER-PERSONAL\u201915,page13\u201320,NewYork,NY,USA.AssociationforComputingMachinery.HelenSpencer-Oatey.2005.(im)politeness,faceandperceptionsofrapport:Unpackagingtheirbasesandinterrelationships.1(1):95\u2013119.Gy\u00f6rgySzarvas,VeronikaVincze,Rich\u00e1rdFarkas,Gy\u00f6rgyM\u00f3ra,andIrynaGurevych.2012.Cross-genreandcross-domaindetectionofsemanticuncer-tainty.ComputationalLinguistics,38(2):335\u2013367.BuzhouTang,XiaolongWang,XuanWang,BoYuan,andShixiFan.2010.Acascademethodfordetectinghedgesandtheirscopeinnaturallanguagetext.InProceedingsoftheFourteenthConferenceonCom-putationalNaturalLanguageLearning\u2013SharedTask,pages13\u201317.MorganUlinski,SethBenjamin,andJuliaHirschberg.2018.Usinghedgedetectiontoimprovecommittedbelieftagging.InProceedingsoftheWorkshoponComputationalSemanticsbeyondEventsandRoles,pages1\u20135.TeppoVarttala.1999.Remarksonthecommunicativefunctionsofhedginginpopularscientificandspecial-istresearcharticlesonmedicine.Englishforspecificpurposes,18(2):177\u2013200.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,\u0141ukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.InAdvancesinneuralinformationpro-cessingsystems,pages5998\u20136008.ErikVelldal.2011.Predictingspeculation:asimpledis-ambiguationapproachtohedgedetectioninbiomed-icalliterature.JournalofBiomedicalSemantics,2(5):1\u201314.VeronikaVincze.2014.Uncertaintydetectioninnaturallanguagetexts.PhD,UniversityofSzeged,page141.\n2169\n\n\nNingWang,WLewisJohnson,RichardEMayer,PaolaRizzo,ErinShaw,andHeatherCollins.2008.Thepolitenesseffect:Pedagogicalagentsandlearningoutcomes.Internationaljournalofhuman-computerstudies,66(2):98\u2013112.ShawnWarner-Garcia.2014.Laughingwhennothing\u2019sfunny:Thepragmaticuseofcopinglaughterinthenegotiationofconversationaldisagreement.Prag-matics,24(1):157\u2013180.TimothyWilliamson.2002.Vagueness.Routledge.RanZhao,AlexandrosPapangelis,andJustineCassell.2014.Towardsadyadiccomputationalmodelofrap-portmanagementforhuman-virtualagentinteraction.InInternationalConferenceonIntelligentVirtualAgents,pages514\u2013527.Springer.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016a.Automaticrecognitionofconversa-tionalstrategiesintheserviceofasocially-awaredialogsystem.InProceedingsofthe17thAnnualMeetingoftheSpecialInterestGrouponDiscourseandDialogue,pages381\u2013392.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016b.Socially-awarevirtualagents:Au-tomaticallyassessingdyadicrapportfromtemporalpatternsofbehavior.InInternationalconferenceonintelligentvirtualagents,pages218\u2013233.Springer.AAdditionalinformationontheexperimentalsettingsWeusedPyTorch(Paszkeetal.,2019)toimple-menttheneuralmodels.Foreachsetoffeatures,hyperparameterswereselectedusingOptuna(Ak-iba,2019),aparametersearchframework.Were-implementedtheAttention-CNNwithGlove(Pen-ningtonetal.,2014)300-Dwordsembeddingsasthevectorrepresentation.Foreachmodels,theresultsarecross-validatedusing5folds(wechose5insteadof10toavoidhavingfoldswithtoofewsamplesperclass).Wecorrectedthelossfunctionforclassimbalancetoforcethemodeltoadaptmoretothelessfrequentclasses.Thestrengthofthiscorrectiondependedonthemodel,andwasselectedbecauseitprovidedasatisfyingcompro-misebetweenfavoringrecallandprecisionintheclassificationresultsofthatmodel.ForLightGBM,a\"squarerootofthesquarerootoftheinverseclassproportion\"correctionwasselected.Neu-ralmodelsweretrainedusingAdamWasanop-timizer(LoshchilovandHutter,2018),andusedareducedfeaturevector,obtainedwiththeap-plicationofPCA(dinit=1800;d=100;99.8%oftheinformationisconserved).Nosignifi-cantperformancedifferenceswereobservedbe-tweentheoriginalvectorandthereducedvectorfortrainingthemodels.TocomputetheSHAPvaluesmentionedinthepaper,wekeptonesplittoperformthe5-splitofthedataset,andleave1splittovalidateandearlystopthemodel,inor-dertoavoidoverfitting.Acompleteconfigura-tionofhyperparametersusedforeachmodelisre-portedintheGitHubrepositorywiththecodeofthepaper:https://github.com/YannRaphalen/Hedges-Detection.TheBERTmodelwasfine-tunedonaNvidiaQuadroRTX8000GPU.BTables\n2170\n\n\n?(whether|if|is|that|it|this)?.*Subj.\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nNoNoYesNoNoNoYesYesYesYesYesBERT(PTE)\nexceptionally|forthemostpart|inamannerofspeaking|\nNoYesNoNoNoYesNoNoYesYesNoMLP(KDF)\nsomethingbetween|essentially|only).*Prop.\nTable8:Significancetableforthe3-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesYesYesYesYesNoYesYesYesNoYesMLP(KDF+PTE)\ntechnically|typically|virtually|approximately|\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nmuch|somewhat|exactly|almost|littlebit|quite|\nClass\nrelatively|roughly|sotosay|strictlyspeaking|\nNoNoYesNoNoYesNoYesYesYesYesLSTM(KDF)\nYesNoNoNoNo+TM\nTable10:Significancetableforthe3-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoNoYesYesYesYesYesYesNoYesYes\n.*(it)(looks|seems|appears)[,]?.*\",\".*(or|and)(that|something|stuff|soforth)Table7:Regexprulesusedfortheclassifier.\nRule-based\nRule-based\n.*(you(might|may)(believe|think)).*Subj.\nYesYesYesYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nYesYesNoYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nNoNoYesNoNoYesNoNoYesYesYesLSTM(KDF)\nYesYesNoYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(i|i\u2019m|you|it\u2019s)(am|are)(apparently|surely)[,]?.*Prop.\nNoNoYesNoNoYesNoYesYesYesYesLSTM(PTE)\nNoNoYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nNoNoYesYesNoYesYesYesYesYesNoLGBM(KDF)\nNoNoYesYesYesYesYesYesYesYesNoLGBM(KDF)\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nYesNoYesYesYesYesYesYesNoYesYes\nYesYesYesYesYesYesYesYesYesYesYesMLP(KDF+PTE)\nregular|regularly|actually|almost|asitwere|basically|\nNoYesYesNoYesYesYesNoYesYesNoMLP(PTE)\nRule(regexp)\n.*(i|i\u2019m|we)?(was|am|wasn\u2019t)??(not)?(sure|certain).*Subj.\n.*(accordingto|presumably).*Subj.\n(?!.*(be|been|was)likeexcuseme)((excuseme|sorry)[w,\u2019]+|[w,\u2019]+(excuseme|sorry))Prop.\n.*(i|you|we)haveto(check|look|verify).*Subj.\n.*(i\u2019m|i|we\u2019re)(am|are)??(apologize|sorry).*Apol.\nonthetallside|parexcellence|particularly|\n(?!what).*(i|we)?(don\u2019t|didn\u2019t|did)??(not)?\ninarealsense|inasense|inaway|largely|literally|\nYesYesYesNoNoNoYesYesYesYesYesBERT(PTE)\nModels\nModels\nModels\nYesYesYesNoNoYesNoNoYesYesYesLSTM(PTE)\nYesNoNoNoNo\nYesYesYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nSubj.\nTable9:Significancetableforthe4-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoYesNoNoNoYesNoNoYesYesNoMLP(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(ifeellikeyou).*Subj.\nlooselyspeaking|kinda|moreorless|mostly|often|\n.*(unlessi).*Apol.\nprobably|canbeviewas|crypto-|especially|essentially|\n.*(ifi\u2019mnotwrong|ifi\u2019mright|ifthat\u2019strue).*Subj.\nYesNoNoNoNo+POS\n(guess|guessed|thought|think|believe|believed|suppose|supposed)\n.*(just|alittle|maybe|actually|sortof|kindof|pretty\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nNoYesYesNoYesYesNoNoYesYesYesMLP(KDF)\nprettymuch|principally|pseudo-|quintessentially|\n2171\n\n\nFigure2:Absoluteaveragedfeaturecontribution,asindicatedbySHAP.Thelongerthebarisforonecolor,themorethefeatureisassociatedwiththeclassrepresentedbythatcolor.\nFigure3:Averagedcontributionoffeaturestothedetectionofthe\"Notindirect\"class,asindicatedbySHAP.Eachdotcorrespondstoaclassifiedclause.Areddotindicatesthatthefeatureispresentintheclause,whileabluedotindicatesthatthefeatureisabsent.Thefartherontherightthedotis,themorethefeaturecontributedtoitsclassificationasahedge.\n2172\n\n\nFigure4:Averagedcontributionoffeaturestothedetectionof\"Apologizers\",asindicatedbySHAP.\nFigure5:Averagedcontributionoffeaturestothedetectionof\"Propositionalhedges\",asindicatedbySHAP.\n2173\n\n\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nYesNoNoNoNo+TM\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nFigure6:Averagedcontributionoffeaturestothedetectionof\"Subjectivizers\",asindicatedbySHAP.\nModels\nYesNoNoNoNo\nTable11:Significancetableforthe4-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesNoNoNoNo+POS\n2174"}, {"question": " How does rapport impact the success of peer-tutoring activities?", "answer": " It has been shown to be beneficial for learning outcomes", "ref_chunk": "\"Youmightthinkaboutslightlyrevisingthetitle\":IdentifyingHedgesinPeer-tutoringInteractionsYannRaphalen1,Chlo\u00e9Clavel2,JustineCassell1,31InriaParis2LTCI,InstitutPolytechniquedeParis,Telecom-Paris3CarnegieMellonUniversityyann.raphalen.pro@gmail.com,justine@cs.cmu.edu,chloe.clavel@telecom-paris.frAbstractHedgesplayanimportantroleinthemanage-mentofconversationalinteraction.Inpeer-tutoring,theyarenotablyusedbytutorsindyads(pairsofinterlocutors)experiencinglowrapporttotonedowntheimpactofinstructionsandnegativefeedback.Pursuingtheobjectiveofbuildingatutoringagentthatmanagesrap-portwithstudentsinordertoimprovelearning,weusedamultimodalpeer-tutoringdatasettoconstructacomputationalframeworkforiden-tifyinghedges.Wecomparedapproachesre-lyingonpre-trainedresourceswithothersthatintegrateinsightsfromthesocialsciencelitera-ture.Ourbestperformanceinvolvedahybridapproachthatoutperformstheexistingbase-linewhilebeingeasiertointerpret.Weemployamodelexplainabilitytooltoexplorethefea-turesthatcharacterizehedgesinpeer-tutoringconversations,andweidentifysomenovelfea-tures,andthebenefitsofsuchahybridmodelapproach.1IntroductionRapport,mostsimplydefinedasthe\u201c...relativeharmonyandsmoothnessofrelationsbetweenpeo-ple...\u201d(Spencer-Oatey,2005),hasbeenshowntoplayaroleinthesuccessofactivitiesasvariedaspsychotherapy(Leach,2005)andsurveyinterview-ing(LuneandBerg,2017).Inpeer-tutoring,rap-port,asmeasuredbytheannotationofthinslicesofvideo,hasbeenshowntobebeneficialforlearningoutcomes(Zhaoetal.,2014;SinhaandCassell,2015).Thelevelofrapportrisesandfallswithconversationalstrategiesdeployedbytutorsandtuteesatappropriatetimes,andasafunctionofthecontentofpriorturns.Thesestrategiesincludeself-disclosure,referringtosharedexperience,and,onthepartoftutors,givinginstructionsinanindirectmanner.Someworkhasattemptedtoautomaticallydetectthesestrategiesintheserviceofintelligenttutors(Zhaoetal.,2016a),butonlyafewstrate-gieshavebeenattempted.Otherworkhascon-centratedona\"socialreasoningmodule\"(Romeroetal.,2017)todecidewhichstrategiesshouldbegeneratedinagivencontext,butindirectnesswasnotamongthestrategiestargeted.Inthispaper,wefocusontheautomaticclassificationofonespe-cificstrategythatisparticularlyimportantforthetutoringdomain,andthereforeimportantforintel-ligenttutors:hedging,asub-partofindirectnessthat\"softens\"whatwesay.Thisworkispartofalargerresearchprogramwiththelong-termgoalofautomaticallygeneratingindirectnessbehaviorsforatutoringagent.\nFigure1:Amockconversationdisplayingeachtypeofhedgedformulation.AccordingtoBrownandLevinson(1987),hedgesarepartofthelinguistictoolsthatinterlocu-torsusetoproducepoliteness,bylimitingthefacethreattotheinterlocutor(basicallybylimitingtheextenttowhichtheinterlocutormightexperienceembarrassmentbecauseofsomekindofpoorper-formance).Anexampleis\"that\u2019skindofawronganswer\".Hedgesarealsofoundwhenspeakerswishtoavoidlosingfacethemselves,forexam-plewhensaying(\"IthinkImighthavetoadd6.\").Madaioetal.(2017)foundthatinapeer-tutoringtask,whenrapportbetweeninterlocutorsislow,tu-teesattemptedmoreproblemsandcorrectlysolvedmoreproblemswhentheirtutorshedgedinstruc-\n2160 Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1: Long Papers, pages 2160 - 2174 May 22-27, 2022 c(cid:13)2022 Association for Computational Linguistics\n\n\ntions,whichlikewisepointstowardsa\"mitigationoffacethreat\"function.Hedgescanalsobeasso-ciatedwithanonverbalcomponent,forexampleavertedeyegazeduringcriticism(BurgoonandKoper,1984).Hedgesarenot,however,alwaysap-propriate,asin\"Ikindofthinkit\u2019srainingtoday.\"whentheinterlocutorscanbothseerain(althoughitmightbetakenashumorous).Thesefactsabouthedgesmotivateawaytoautomaticallydetectthemand,ultimately(althoughnotinthecurrentwork)alsogeneratethem.Inbothcaseswefirsthavetobeabletocharacterizethemusinginterpretablelinguisticfeatures,whichiswhatweaddressinthecurrentpaper.Thus,intheworkdescribedhere,basedonlinguisticdescriptionsofhedges(BrownandLevinson,1987;Fraser,2010),webuiltarule-basedclassifier.Weshowthatthisclassifierincombinationwithadditionalmultimodalinter-pretablecontext-dependentfeaturessignificantlyimprovestheperformanceofamachinelearningmodelforhedges,comparedtoalessinterpretabledeeplearningbaselinefromGoeletal.(2019)us-ingwordembeddings.Wealsoreliedonamachinelearningmodelexplanationtool(LundbergandLee,2017)toinvestigatethelinguisticfeaturesrelatedtohedgesinthecontextofpeer-tutoring,primarilytoseeifwecoulddiscoversurprisingfeaturesthattheclassificationmodelwouldassociatetohedgesinthiscontext,andwedescribethosebelow.Thecodeofthemodelsdescribedinthepaperisalsoprovided.12RelatedworkHedges:AccordingtoFraser(2010),hedgingisarhetoricalstrategythatattenuatesthestrengthofastatement.Onewaytoproduceahedgeisbyalteringthefullsemanticvalueofaparticu-larexpressionthroughPropositionalhedges(alsocalledApproximatorsinPrinceetal.(1982)),asin\"Youarekindofwrong,\"thatreduceprototypical-ity(i.eaccuracyofthecorrespondencebetweenthepropositionandtherealitythatthespeakerseekstodescribe).Propositionalhedgesarerelatedtofuzzylanguage(Lakoff,1975),andthereforetotheproductionofvagueness(Williamson,2002)anduncertainty(Vincze,2014).AsecondkindareRelationalHedges(alsocalledShieldsinPrinceetal.(1982)),suchas\u201cIthinkthatyouarewrong.\u201dor\u201cThedoctorwantsyoutostopsmoking.\u201d,conveyingthatthepropositionis\n1https://github.com/AnonymousHedges/HedgeDetectionconsideredbythespeakerassubjective.Inafurthersub-division,AttributionShields,asin\"Thedoc-torwantsyou...\",theinvolvementofthespeakerinthetruthvalueofthepropositionisnotmadeexplicit,whichallowsspeakersnottotakeastance.Asdescribedabove,Madaioetal.(2017)foundthattutorswhoshowedlowerrapportwiththeirtuteesusedmorehedgedinstructions(theyalsoemployedmorepositivefeedback),howeverthiswasonlythecasefortutorswithagreaterbeliefintheirabilitytotutor.Tuteesinthiscontextsolvedmoreproblemscorrectlywhentheirtutorshedgedinstructions.Noeffectofhedgingwasfoundfordyads(pairsofinterlocutors)withgreatersocialcloseness.However,theauthorsdidnotlookatthespecificlinguisticformstheseteenagersused.Rowland(2007)alsodescribestherolethathedg-ingplaysinthisagegroup,showingthatstudentsusebothrelational(\"IthinkthatJohnissmart.\")andpropositional(\"Johniskindofsmart.\")hedgesformuchthesameshieldingfunctionofdemon-stratinguncertainty,tosavethemfromtheriskofembarrassmentiftheyarewrong.TheauthorobservedthatteensusedfewAdaptors(kindof,somewhat)andpreferredtouseRounders(around,closeto).However,thisstudywasperformedwithanadultandtwochildren,possiblybiasingthere-sultsduetotheparticipationoftheadultinvestiga-tor.Hedgeshavebeenincludedinvirtualtutoringagentsbeforenow.(Howardetal.,2015)integratedhedgesinatutoragentforundergraduatesinCS,asawaytoencouragethestudenttotaketheinitiative.Hedgeshavealsobeenusedasawayofintegrat-ingBrownandLevinson\u2019spolitenessframework(Wangetal.,2008;Schneideretal.,2015)invir-tualtutoringagents.Resultswerenotbrokenoutbystrategy,butpolitenessingeneralwasshowntopositivelyinfluencemotivationandlearning,incertainconditions.Computationalmethodsforhedgedetection:Anumberofstudieshavetargetedthedetectionofhedgesanduncertaintyintext(MedlockandBriscoe,2007;GanterandStrube,2009;Tangetal.,2010;Velldal,2011;Szarvasetal.,2012),partic-ularlyfollowingtheCoNLL2010datasetrelease(Farkasetal.,2010).However,thisworkisnotasrelatedtohedgesinconversation,asitfocusesonaformalandacademiclanguageregister(Hy-land,1998;Varttala,1999).AsnotedbyProkofievaandHirschberg(2014),thefunctionsofhedgesaredomain-andgenre-dependent,thereforethisbias\n2161\n\n\ntowardsformalityimpliesthattheexistingworkmaynotadaptwelltothedetectionofhedgesinconversationbetweenteenagers.Aconsequenceisthattheexistingworkdoesnotconsidertermslike\"Ithink,\"sinceopinionsrarelyappearinanaca-demicwritingdataset.Instructionsarealsoalmostabsent(\"Ithinkyouhavetoaddtentobothsides.\"),astronglimitationforthestudyofconversationalhedgessinceitisinrequests(includingtutoringin-structions)thatindirectformulationsmostlyoccuraccordingtoBlum-Kulka(1987).ProkofievaandHirschberg(2014)alsonotethatitisdifficulttodetecthedgesbecausethewordpatternsassociatedwiththemhaveothersemanticandpragmaticfunc-tions:considering\"Ithinkthatyouhavetoaddxtobothsides.\"vs\"Ithinkthatyouareanidiot.\",itisnotclearthattheseconduseof\"Ithinkthat\"isanhedgemarker.Theyadvocateusingmachinelearningapproachestodealwiththeambiguityofthesemarkers.Workingonaconversationaldataset,Ulinskietal.(2018)builtacomputationalsystemtoassessspeakercommitment(i.e.atwhichpointthespeakerseemsconvincedbythetruthvalueofastatement),inparticularbyrelyingonarule-baseddetectionsystemforhedges.Comparedtothatwork,ourrule-basedclassificationmodelisdirectlydetectinghedgeclasses,andweemploythepredictionsoftherule-basedmodelasafeatureforstrongermachinelearningmodels,designedtolessentheimpactoftheimbalancebetweenclasses.Wealsoconsiderapologieswhentheyserveamit-igationfunction(wethencallthemApologizers),aswasdonebytheauthorsofourcorpus,andwealsousethetermsubjectivizersasdefinedbelow,tobeabletocomparedirectlywiththepreviousworkcarriedoutonthiscorpus.Asfarasweknow,onlyGoeletal.(2019)haveworkedwithapeer-tutoringdataset(thesameonethatwealsouse),andtheyachievedtheirbestclassificationresultbyemployinganAttention-CNNmodel,inspiredbyAdelandSch\u00fctze(2017).3ProblemstatementWeconsiderasetDofconversationsD=(c1,c2,...,c|D|),whereeachconversationiscom-posedofasequenceofindependentsyntacticclausesci=(u1,u2,...,uM),whereMisthenumberofclausesintheconversation.Notethattwoconsecutiveclausescanbeproducedbythesamespeaker.Eachclauseisassociatedwithauniquelabelcorrespondingtothediffer-enthedgeclassesdescribedinTable1:yi\u2208C={PropositionalHedges,Apologizers,Subjec-tivizers,Nothedged}.Finally,anutteranceuicanberepresentedasavectoroffeaturesX=(x1,x2,...,xN),whereNrepresentsthenumberoffeaturesweusedtodescribeaclause.Ourfirstgoalistodesignamodelthatcorrectlypredictsthelabelyiassociatedtoui.Itcanbeunderstoodasthefollowingresearchquestion:RQ1:\"Whichmodelsandfeaturescanbeusedtoautomaticallycharacterizehedgesinapeer-tutoringinteraction?\"Oursecondgoalistoidentify,foreachhedgeclass,thesetoffeaturesFclass={fk},k\u2208[1,N]sortedbyfeatureimportanceintheclassificationofclass.Itcorrespondstothefollowingresearchquestion:RQ2:\"Whatarethemostimportantlinguisticfeaturesthatcharacterizeourhedgeclassesinapeer-tutoringsetting?\"4Methodology4.1CorpusDatacollection:Thedialoguecorpususedherewascollectedaspartofalargerstudyontheeffectsofrapport-buildingonreciprocalpeertutoring.24Americanteenagers(meanage=13.5,min=12,max=15),halfmaleandhalffemale,cametoalabwherehalfoftheparticipantswerepairedwithasame-age,same-genderfriend,andtheotherhalfpairedwithastranger.Theparticipantswereassignedtoatotalof12dyadsinwhichthepar-ticipantsalternatedtutoringoneanotherinlinearalgebraequationsolvingfor5weeklyhour-longsessions,foratotalcorpusofnearly60hoursofface-to-faceinteractions.Eachsessionwasstruc-turedsuchthatthestudentsengagedinbriefsocialchitchatinthebeginning,thenoneofthestudentswasrandomlyassignedtotutortheotherfor20minutes.Theythenengagedinanothersocialpe-riod,andconcludedwithasecondtutoringperiodwheretheotherstudentwasassignedtheroleoftutor.Audioandvideodatawererecorded,tran-scribed,andsegmentedforclause-leveldialogueannotation,providingnearly24000clauses.Non-speechsegments(notablyfillersandlaughter)weremaintained.Becauseoftemporalmisalignmentforpartsofthecorpus,manyparaverbalphenomena,suchasprosody,wereunfortunatelynotavailabletous.SinceouraccesstothedatasetiscoveredbyaNon-DisclosureAgreement,itcannotbereleased\n2162\n\n\n2https://github.com/AnonymousHedges/HedgeDetectionclasses.4.2FeaturesLabelfromrule-basedclassifier(LabelRB):Weusetheclasslabelpredictedbytherule-basedclas-sifierdescribedinSection4.3asafeature.Ourhypothesisisthatthemachinelearningmodelcanusethisinformationtocounterbalancetheclassimbalance.Totakeintoaccountthefactthatsomerulesaremoreefficientthanothers,weweightedtheclasslabelresultingfromtherule-basedmodelbytheprecisionoftherulethatgeneratedit.Unigramandbigram:Wecountthenumberofoccurrencesofunigramsandbigramsofthecorpusineachclause.Weusedthelemmaofthewordsforunigramsandbigramsusingthenltklemmatizer(Loper,2002)andselectedunigramsandbigramsthatoccurredinthetrainingdatasetatleastfiftytimes.Thegoalwastoinvestigate,withabottom-upapproach,towhatextenttheuseofcertainwordscharacterizeshedgeclassesintutoring.InSection5weexaminetheoverlapbetweenthesewordsandthoseaprioriidentifiedbytherules.Part-of-speech(POS):Hedgeclassesseemtobeassociatedwithdifferentsyntacticpatterns:forex-ample,subjectivizersmostoftencontainapersonalpronounfollowedbyaverb,asin\"Iguess\",\"Ibelieve\",\"Ithink\".WethereforeconsideredthenumberofoccurrencesofPOS-Tagn-grams(n=1,2,3)asfeatures.WeusedthespaCyPOS-taggerandconsideredPOSunigrams,bigramsandtri-gramsthatoccuratleast10timesinthetrainingdataset.LIWC:LinguisticInquiryandWordCount(LIWC)(Pennebakeretal.,2015)isstandardsoft-wareforextractingthecountofwordsbelongingtospecificpsycho-socialcategories(e.g.,emotions,religion).Ithasbeensuccessfullyusedinthede-tectionofconversationalstrategies(Zhaoetal.,2016a).Wethereforecountthenumberofoccur-rencesofallthe73categoriesfromLIWC.Tutoringmoves(TM):Intelligenttutoringsys-temsrelyonspecifictutoringmovestosuccess-fullyconveycontent(asdohumantutors).Wethereforelookedatthelinkbetweenthetutoringmoves,asannotatedinMadaioetal.(2017),andhedges.Fortutors,thesemovesare(1)instruc-tionaldirectivesandsuggestions,(2)feedback,and(3)affirmations,mostlyexplicitreflectionsontheirpartners\u2019comprehension,whilefortutees,theyare(1)questions,(2)feedbacks,and(3)affirmations,\npublicly.Howevertheoriginalexperimenters\u2019In-stitutionalReviewBoard(IRB)approvalallowsustoview,annotate,andusethedatatotrainmodels.Thisalsoallowsustoprovidealinktoapixe-latedvideoexampleintheGitHubrepositoryoftheproject2.Dataannotation:Thedatasetwaspreviouslyan-notatedbyMadaioetal.(2017),followingananno-tationmanualthatusedhedgeclassesderivedfromRowland(2007)(seeTable1).Onlythetaskperi-odsoftheinteractionswereannotated.Comparingtheannotationswiththeclassesmentionedintherelatedworksection,SubjectivizerscorrespondtoRelationalhedges(Fraser,2010),PropositionalhedgesandExtenderscorrespondtoApproxima-tors(Princeetal.,1982)withtheadditionofsomediscoursemarkerssuchasjust.ApologizersarementionedaslinguistictoolsrelatedtonegativepolitenessinBrownandLevinson(1987).Krippen-dorff\u2019salphaobtainedforthiscorpusannotatedbyfourcoderswasover0.7forallclasses(denotinganacceptableinter-coderreliabilityaccordingtoKrippendorff(2004)).Thedatasetiswidelyim-balanced,withmorethan90%oftheutterancesbelongingtotheNothedgedclass.Inreviewingthecorpusandtheannotationman-ual,however,wenoticedtwoissues.First,theannotationoftheExtendersclasswasinconsis-tent,leadingtotheExtendersandPropositionalhedgesclassescarryingsimilarsemanticfunctions.WethereforemergedthetwoclassesandgroupedutteranceslabeledasExtendersandthoselabeledasPropositionalhedgesundertheheadingofPropositionalhedges.Second,theannotationofclausescontainingthetokens\"just\"and\"would\"(twotermsoccurringfrequentlyinthedatasetthatarekeycomponentsofPropositionalHedgesandSubjectivizersbutthatarenotinfacthedgesinallcases)wasalsoinconsistent,leadingtovirtuallyallclauseswiththosetwotokensbeingconsideredhedges.Wethereforere-consideredalltheclausesassociatedwithanyofthehedgeclasses,aswellasalltheclausesinthe\"Nothedged\"classthatcontained\"just\"or\"would\".There-annotationwascarriedoutbytwoannotatorswhoachievedaKrippendorff\u2019salphainter-raterreliabilityof.9orbetterforApologizers,Subjectivizers,andPropo-sitionalhedgesbeforeindependentlyre-annotatingtherelevantclauses.Anexampleofare-annotationwasremoving\"Iwouldkillyou!\"fromthehedge\n2163\n\n\nProp.hedgesApologizersSubjectivizersNothedgedTotal\nTable1:Definitionoftheclasses\nFeaturesnameAutomaticextractionVectorsize\nSubjectivizersWordsthatreduceintensityorcertainty\u201cSothenIwoulddividebytwo.\u201dApologizersApologiesusedtosoftendirectspeechacts\u201cOhsorrysixb.\u201dPropositionalhedgesQualifyingwordstoreduceintensityorcertaintyofutterances\u201cIt\u2019sactuallyeight.\u201dExtendersWordsusedtoindicateuncertaintybyreferringtovaguecategories\u201cIt\u2019llbethenumberxorwhatevervariableyouhave.\u201d\n12101286262119223156\nTable2:Distributionoftheclasses\nClassDefinitionExample\nRule-basedlabelYes4UnigramYes~250BigramYes~250POSYes~1200LIWCYes73NonverbalNo24TutoringmovesNo6Total~1800\nTable3:Listofautomaticallyextractedandmanuallyannotatedfeatureswiththeirsize.mostlytentativeanswers.Nonverbalandparaverbalbehaviors:AsinGoeletal.(2019),weincludedthenonverbalandpar-averbalbehaviorsthatarerelatedtohedges.Specif-ically,weconsiderlaughterandsmiles,thathavebeenshowntobeeffectivemethodsofmitiga-tion(Warner-Garcia,2014),cut-offsindicatingself-repairs,fillerslike\"Um\",gazeshifts(annotatedas\u2019GazeatPartner\u2019,\u2019GazeattheMathWorksheet\u2019,and\u2019Gazeelsewhere\u2019),andheadnods.Eachfea-turewaspresenttwiceinthefeaturevector,onetimeforeachinterlocutor.Inter-raterreliabilityfornonverbalbehaviorwas0.89(asmeasuredbyKrippendorff\u2019salpha)foreyegaze,0.75forsmilecount,0.64forsmiledurationand0.99forheadnod.Laughterisalsoreportedinthetranscriptatthewordlevel.Weseparatethetutor\u2019sbehaviorsfromthoseofthetutee.ThecollectionprocessforthesebehaviorsisdetailedfurtherinZhaoetal.(2016b).Theclause-levelfeaturevectorwasnormalizedbythelengthoftheclause(exceptfortherule-basedlabel).Thislengthwasalsoaddedasafeature.Table3presentsanoverviewofthefinalfeaturevector.4.3ClassificationmodelsTheclassificationmodelsusedarepresentedhereaccordingtotheirlevelofintegrationofexternallinguisticknowledge.Rule-basedmodel:OnthebasisoftheannotationmanualusedtoconstructthedatasetfromMadaioetal.(2017),andwithdescriptionsofhedgesfromRowland(2007),Fraser(2010)andBrownandLevinson(1987),weconstructedarule-basedclas-sifierthatmatchesregularexpressionsindicativeofhedges.TherulesaredetailedinTable7intheAppendix.LGBM:Sincehedgesareoftencharacterizedbyexplicitlexicalmarkers,wetestedtheassumptionthatamachinelearningmodelwithaknowledge-drivenrepresentationforclausescouldcompetewithaBERTmodelinperformance,whilebeingmuchmoreinterpretable.WereliedonLightGBM,anensembleofdecisiontreestrainedwithgradi-entboosting(Keetal.,2017).Thismodelwasselectedbecauseofitsperformancewithsmalltrainingdatasetsandbecauseitcanignoreunin-formativefeatures,butalsoforitstrainingspeedcomparedtoalternativeimplementationsofgradi-entboostingmethods.Multi-layerperceptron(MLP):Asasimplebase-line,webuiltamulti-layerperceptronusingthreesetsoffeatures:apre-trainedcontextualrepre-sentationoftheclause(SentBERT;ReimersandGurevych(2019));theconcatenationofthiscon-textualrepresentationoftheclauseandarule-basedlabel(notrelyingonthepreviousclauses);andfinallytheconcatenationofallthefeaturesmen-tionedinsection4.2,withoutthecontextualizedrepresentation.LSTMoverasequenceofclauses:Sinceweareworkingwithconversationaldata,wealsowantedtotestwhethertakingintoac-countthepreviousclauseshelpstodetectthetypeofhedgeclassinthenextclause.Formally,wewanttoinferyiusingyi=maxy\u2208ClassesP(y|X(ui),X(ui\u22121),...,X(ui\u2212K)),whereKisthenumberofpreviousclausesthatthemodelwilltakeintoaccount.The\n2164\n\n\nMLPmodelpresentedaboveinfersyiusingyi=maxy\u2208ClassesP(y|X(ui)),thereforeadifferenceofperformancebetweenthetwomodelswouldbeasignthatusinginformationfromthepreviousclausescouldhelptodetectthehedgedformulationinthecurrentclause.WetestedaLSTMmodelwiththesamerepresentationsforclausesasfortheMLPmodel.CNNwithattention:Goeletal.(2019)estab-lishedtheirbestperformanceonhedgedetec-tionusingaCNNmodelwithadditiveattentionoverword(andnotclause)embeddings.Con-trarytotheMLPandLSTMmodelsmentionedabove,thismodeltriestoinferyiusingyi=maxy\u2208ClassesP(y|g(w0),g(w1),...,g(wL)),withLrepresentingthemaximumclauselengthweal-low,andgrepresentingafunctionthatturnsthewordwj,j\u2208[0,L]intoavectorrepresentation(formoredetails,pleaseseeAdelandSch\u00fctze(2017)).BERT:Tobenefitfromdeepsemanticandcon-textualrepresentationsoftheutterances,wealsofine-tunedBERT(Devlinetal.,2019)onourclas-sificationtask.BERTisapre-trainedTransformersencoder(Vaswanietal.,2017)thathassignificantlyimprovedthestateoftheartonanumberofNLPtasks,includingsentimentanalysis.Itproducesacontextualrepresentationofeachwordinasen-tence,makingitcapableofdisambiguatingthemeaningofwordslike\"think\"or\"just\"thatarerepresentativeofcertainclassesofhedges.BERT,however,isnotablyhardtointerpret.4.4AnalysistoolsLookingatwhichfeaturesimprovetheperfor-manceofourclassificationmodelstellsuswhetherthesefeaturesareinformativeornot,butdoesnotexplainhowthesefeaturesareusedbythemod-elstomakeagivenprediction.Wethereforepro-ducedacomplementaryanalysisusinganinter-pretabilitytool.Asdemonstratedby(LundbergandLee,2017),LightGBMinternalfeatureimpor-tancescoresareinconsistentwithboththemodelbehaviorandhumanintuition,soweinsteadusedamodel-agnostictool.SHAP(LundbergandLee,2017)assignstoeachfeatureanimportancevalue(calledShapleyvalues)foraparticularpredictiondependingontheextentofitscontribution(ade-tailedintroductiontoShapleyvaluesandSHAPcanbefoundinMolnar(2020)).SHAPisamodel-agnosticframework,thereforethevaluesassoci-atedwithasetoffeaturescanbecomparedacrossmodels.ItshouldbenotedthatSHAPproducesexplanationsonacase-by-casebasis,thereforeitcanbothprovidelocalandglobalexplanations.FortheGradientBoostingmodel,weuseanadaptedversionofSHAP(Lundbergetal.,2018),calledTreeSHAP.5Experimentsandresults5.1ExperimentalsettingTodetectthebestsetoffeatures,weusedLight-GBMandproceededincrementally,byaddingthegroupoffeatureswethoughttobemostlikelyasso-ciatedwithhedges.Wedidnotconsidertheriskofrelyingonasub-optimalsetoffeaturesthroughthisprocedurebecauseofthestrongabilityofLight-GBMtoignoreuninformativefeatures.Weusethisincrementalapproachasawaytotestourintuitionabouttheperformativityofgroupsoffeatures(i.e.doesaddingafeatureimprovetheperformanceofthemodel)withregardtothetaskofclassifica-tion.Tocompareourmodels,wetrainedthemonthe4-classtask,andlookedattheaverageoftheweightedF1-scoresforthethreehedgeclasses(i.e.howwellthemodelsinferminorityclasses)thatwereporthereas\"3-classes\",andattheaverageoftheweightedF1-scoresforthe4classes,thatwereportas\"4-classes\".DetailsofthehyperparametersandexperimentalsettingsareprovidedinAppendixA.5.2ModelcomparisonandfeatureanalysisOverallresults:Table4presentstheresultsob-tainedbythe6modelspresentedinSection4.3forthemulti-classproblem.Bestperformance(F1-scoreof79.0)isobtainedwithLightGBMlever-agingalmostallthefeatures.Intheappendix(seeTable8andTable9)weindicatetheconfidenceintervalstorepresentthesignificanceofthediffer-encesbetweenthemodels.First,andperhapssurprisingly,wenoticethattheuseof\"Knowledge-Driven\"featuresbasedonrulesbuiltfromlinguisticknowledgeofhedgesintheLightGBMmodeloutperformstheuseofpre-trainedembeddingswithinafine-tunedBERTmodel(79.0vs.70.6),andintheneuralbaselinefrom(Goeletal.,2019)(79.0vs64.5).ThelowscoresobtainedbytheLGBM,LSTMandMLPmodelswithpre-trainedsentenceem-beddingsversusKnowledge-Drivenfeaturesmightsignalthatthewordpatternscharacterizinghedgesarenotsalientintheserepresentations(i.e.the\n2165\n\n\n68.5(1.6)35.8(3.1)64.8(1.1)Attention-CNN(3-classes)\n65.1(5.7)39.8(8.0)65.2(5.1)BERT(3-classes)\n\u220570.6(2.3)\u2205LGBM(3-classes)\n79.0(1.3)35.0(2.2)70.1(1.4)\nRule-based(3-classes)\n94.7\u2205\u2205MLP(4-classes)\n3Notethatthereisstrongredundancybetweensomefea-turesofLIWCandthespaCyPOStaggerthatbothproducea\"Pronoun\"category,usingalexiconinthefirstcase,andaneuralinferenceinthesecond.\n\u220594.9(0.4)\u2205LGBM(4-classes)\nRule-based(4-classes)\nTable4:AveragedweightedF1-scores(andstandarddeviation)forthethreeminorityclassesandforthe4classes,forallmodels.\"KD\"standsfor\"Knowledge-Driven\",meaningthatthefeaturesarederivedfromlexicon,n-grammodelsandannotations.distancebetween\"Ithinkyoushouldadd5.\"and\"Youshouldadd5.\"isshort.).KDFeaturesseemtoprovideabetterseparabilityoftheclasses.ThecombinationofKDfeaturesandPre-trainedem-beddingsdoesnotsignificantlyimprovetheperfor-manceofthemodelscomparedtotheKDFeaturesonly,whichsuggeststhattheinformationfromthePre-trainedembeddingsisredundantwiththeonefromtheKDFeatures.Thisresultmaybeduetothehighdimensionalityoftheinputvector(868withPCAontheKDFeatures;2500otherwise).Asecondfindingisthattheuseofgradientboost-ingmodelsontopofrule-basedclassifiersbettermodelsthehedgeclasses.Theothermachinelearn-ingmodelsdidnotprovetobeaseffective,exceptforBERT.FeatureanalysisusingLightGBM:Usingthebestperformingmodel,Table5showstheroleofeachfeaturesetinthepredictiontask.ThesignificanceofthedifferencesisshowninTable10andTable11.Comparedtotherule-basedmodel,theintroductionofn-gramssignificantlyimprovedtheperformanceofourclassifier,suggestingthatsomelexicalandsyntacticinformationdescribingthehedgeclasseswasnotpresentintherule-basedmodel.LookingatTable5,wedonotobservesignificantdifferencesbetweentheLGBMmodelusingonlythelabelrulebased+(1-gramsand2-grams)andthemodelsin-corporatingmorefeatures.Tooursurprise,neitherthetutoringmovesnorthenonverbalfeaturessig-nificantlyimprovedtheperformanceofthemodel.The2featureswereincludedtoindexthespecificpeertutoringcontextofthesehedges,sothisindi-catesthatinfutureworkwemightwishtoapplythecurrentmodeltoanothercontextofusetoseeifthismodelofhedgesismoregenerallyapplicablethanweoriginallythought.Bycombiningthisresultwiththeincreasedperformanceofthemodelus-ingKnowledge-Driven(i.e.explicit)featurescom-paredtopre-trainedembeddings,itwouldseemthathedgesareaboveallalexicalphenomenon(i.e.producedbyspecificlexicalelements).5.3In-depthanalysisoftheinformativefeaturesWetrainedtheSHAPexplanationmodelsonLight-GBMwithallfeatures.Themostinformativefea-tures(inabsolutevalue)foreachclassareshowninTable6,andtheplotsbyclassarepresentedintheAppendix.Themostimportantfeaturesseemtobetherule-basedlabels,whichappearinatleastthefourthpositionforthreeclasses(seeTable6),andinthefirstpositionforPropositionalHedgesandNothedgedclasses.Surprisingly,theRule-Basedlabeldoesnotappearinthetop20featuresforApologizers.However,giventhattheclassrarelyappearsinthedata,therulesseldomactivate,sothefeaturemaysimplybeinformativeforaverysmallnumberofclauses.Unigrams(Oh,Sorry,just,Would,andI)arealsopresentinthe5top-rankedfeatures.Thisconfirmsthefindingsmen-tionedinrelatedworkforthecharacterizationofthedifferenthedgeclasses(justwithPropositionalHedges,sorrywithApologizer,IwithSubjectiviz-ers).ThepresenceofOhalsohashighimportanceforthecharacterizationofApologizer(n=2),asillustratedinexamplessuchas\"Ohsorry,that\u2019snine.\".Wenotethattheoccurrencesof\"Ohsorry\"asastand-aloneclausewereexcludedbyourrule-basedmodelbecausetheydonotcorrespondtoanapologizer(theycannotmitigatethecontentofapropositionifthereisnopropositionassociated).Thisexampleillustratestheinterestofamachinelearningmodelapproachtodisambiguatethefunc-tionofconventionalnon-propositionalphraseslike\"Ohsorry\".Inaddition,SHAPhighlightstheimportanceofnovelfeatureswhosefunctionwasnotidentifiedinthehedgesliterature:(i)whatLIWCclassifiesasinformalwordsbutthataremostlyinterjectionslikeahandoharestronglyassociatedwithApol-ogizer,asaredisfluencies(n=12);(ii)theuseofPOStagsseemstobeveryrelevantforcharac-terizingthedifferentclasses(2-gramofPOStagfeatures3occurinthetop-rankedfeaturesofallthe\n\u220564.5(3.0)\u2205LSTM(3-classes)\nKDFeat.(KDF)Pre-TrainedEmb.(PTE)KDF+PTE\n\u220594.4(0.2)\u2205LSTM(4-classes)\n96.7(0.2)91.0(0.2)95.4(0.2)\n94.8(0.3)89.7(0.4)93.9(0.4)Attention-CNN(4-classes)\n93.9(1.4)89.1(1.4)94.1(1.2)BERT(4-classes)\n67.6\u2205\u2205MLP(3-classes)\nModels\n2166\n\n\n95.0(0.2)96.5(0.3)96.5(0.2)96.7(0.2)96.6(0.4)96.7(0.3)\n3-classes\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\n68.8(0.8)78.2(1.6)78.1(1.3)79.0(1.3)78.5(2.4)78.7(1.8)\nTable5:AveragedweightedF1-scoresforthethreeclassesofhedgesandthefourclasses,withanadditiveintegrationofKDFfeaturesintheLightGBMmodel.Thestandarddeviationiscomputedacrossfivefolds.\n4-classes\nModels\nRankApologizerSubjectivizersProp.HedgesNothedged\n1Functionwords(LIWC)\"I\"ClasslabelClasslabel2\"Oh\"(LIWC)\"Yeah\"\"Would\"\"Would\"3\"Sorry\"Noun(POS)\"Just\"\"Yeah\"4Affect(LIWC)ClasslabelFunctionword(LIWC)Noun(POS)5ClauselengthCognitiveprocess(LIWC)Netspeak(LIWC)Cognitiveprocess(LIWC)Table6:Mostimportantclause-levelfeaturesforLightGBMaccordingtotheSHAPanalysis.classes(seeFiguresintheAppendix).Itmeansthattherearesomerecurringsyntacticpatternsineachclass;(iii)Regardingtheutterancesize,aclauseshorterthanthemeanisweaklyassociatedwithdirectness(n=17)whilealongerclausesuggeststhatitcontainsaSubjectivizer(n=6).Apologizersarecharacterizedbyameanclauselength(n=5),withfewvariationsfromit;(iv)Tutoringmovesarenotstrongpredictorsofanyclasses:\"Affirma-tionfromtutor\"istheonlyfeatureappearingasapredictorofPropositionalhedges(n=20).ThisisconsistentwiththefeatureanalysisinTable5,suggestingthattutoringmovesdonotsignificantlyimprovetheperformanceoftheclassifier;(v)Non-verbalbehaviorsdonotappearasimportantfea-turesfortheclassification.Thisiscoherentwithresultsfrom(Goeletal.,2019).Notethatprosodymightplayaroleindetectinginstructionsthattrailoff,but,asdescribed,paraverbalfeatureswerenotavailable;(vi)Wouldplaysanimportantroleintheproductionofhedges,asitisstronglyassociatedtoPropositionalhedges(n=2).Itisinterestingtonotethat,whendesigningtherule-basedclassifier,wesawitdecreaseinperformancewhenwestartedtoincludewouldinourregularexpressionpatterns,probablybecausetheformishardtodisambiguateforadeterministicsystem.WhileexploringtheShapleyvaluesassociatedtoeachclause,weobservedthatfeaturesliketutoringmovesareextremelyinformativeforaverysmallnumberofclauses(thereforenotsignificantlyinflu-encingtheoverallperformanceoftheprediction),andmoreorlessnotinformativefortherest.Infer-ringtheglobalimportanceofafeatureasameanacrosstheshapleyvaluesinthedatasetmaynotbetheonlywaytoexplorethebehaviorofgradi-entboostingmethods.ItmightbemoreusefultoclusterclausesbasedontheimportancethatSHAPgivestothatfeatureinitsclassification,asthiscouldhelpdiscoversub-classesofhedgesthataredifferentiatedfromtherestbytheirinteractionwithaspecificfeature(inthewaythatsomeApologiz-ersarecharacterizedbyan\"oh\").Wealsonotethattheexplanationmodelissensitivetospuri-ouscorrelationsinthedataset,causedbythesmallrepresentationofsomeclass:forexample,\"nine\"(n=7)and\"four\"(n=20)arepositivepredictorsofApologizers.6ConclusionandfutureworkThroughourclassificationperformanceexperi-ments,weshowedthatitispossibletousema-chinelearningmethodstodiminishtheambigu-ityofhedges,andthatthehybridapproachofus-ingrule-basedlabelfeaturesderivedfromsocialscience(includinglinguistics)literaturewithinamachinelearningmodelhelpedsignificantlytoin-creasethemodel\u2019sperformance.Nonverbalbehav-iorsandtutoringmovesdidnotprovideinformationatthesentencelevel;boththeperformanceofthemodelandthefeaturecontributionanalysissug-gestedthattheirimpactonthemodeloutputwasnotstrong.ThisisconsistentwithresultsfromGoeletal.(2019).However,infutureworkwewouldliketoinvestigatethepotentialofmultimodalpat-ternswhenweareabletobettermodelsequentiality(e.g.,negativefeedbackfollowedbyasmile).Re-gardingtheSHAPanalysis,mostofthefeaturesthatareconsideredasimportantarecoherentwiththedefinitionoftheclasses(Iforsubjectivizers,sorryforapologizers,justforpropositionalhedges).However,wediscoveredthatfeatureslikeutterance\n2167\n\n\nsizecanalsoserveasindicatorsofcertainclassesofhedges.AlimitationofSHAPisthatitmakesafeatureindependenceassumption,whichpromptstheexplanatorymodeltounderestimatetheimpor-tanceofredundantfeatures(likepronounsinourwork).Inthefuturewewillexploreexplanatorymodelscapableoftakingintoaccountthecorre-lationbetweenfeaturesinthedatasetlikeSAGE(Covertetal.,2020),butsuitedforveryimbal-anceddatasets.Inthedomainofpeer-tutoring,wewouldliketobeabletofurthertestthelinkbe-tweenhedgesandrapport,andthelinkbetweenhedgesandlearninggainsinthesubjectbeingtu-tored.Asnotedabove,thiskindofstudyrequiresafine-grainedcontrolofthelanguageproducedbyoneoftheinterlocutors,whichisdifficulttoachieveinahuman-humanexperience.Wenotethatthehedgeclassifiercanbeusednotjusttoclassify,butalsotoworktowardsimprovingthegenerationofhedgesfortutoragents.Infutureworkwewillexploreusingtheclassifiertore-rankgenerationoutputs,takingadvantageoftherecur-ringsyntacticpatterns(see(ii)inSection5.3)toimprovethegenerationprocessofhedges,andre-generatingclausesthatdon\u2019tcontainoneofthesesyntacticpatterns.AcknowledgmentsManythankstomembersoftheArticuLaboatIN-RIAParisfortheirpreciousassistance.ThisworkwassupportedinpartbythetheFrenchgovern-mentundermanagementofAgenceNationaledelaRechercheaspartofthe\u201cInvestissementsd\u2019avenir\u201dprogram,referenceANR-19-P3IA-0001(PRAIRIE3IAInstitute).ReferencesHeikeAdelandHinrichSch\u00fctze.2017.Exploringdif-ferentdimensionsofattentionforuncertaintydetec-tion.InProceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputa-tionalLinguistics:Volume1,LongPapers,pages22\u201334,Valencia,Spain.AssociationforComputa-tionalLinguistics.ShoshanaBlum-Kulka.1987.Indirectnessandpolite-nessinrequests:Sameordifferent?Journalofpragmatics,11(2):131\u2013146.PenelopeBrownandStephenCLevinson.1987.Polite-ness:Someuniversalsinlanguageusage,volume4.Cambridgeuniversitypress.JudeeKBurgoonandRandallJKoper.1984.Nonverbalandrelationalcommunicationassociatedwithreti-cence.HumanCommunicationResearch,10(4):601\u2013626.IanCovert,ScottMLundberg,andSu-InLee.2020.Understandingglobalfeaturecontributionswithad-ditiveimportancemeasures.AdvancesinNeuralInformationProcessingSystems,33:17212\u201317223.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforCom-putationalLinguistics:HumanLanguageTechnolo-gies,Volume1(LongandShortPapers),pages4171\u20134186.Rich\u00e1rdFarkas,VeronikaVincze,Gy\u00f6rgyM\u00f3ra,J\u00e1nosCsirik,andGy\u00f6rgySzarvas.2010.Theconll-2010sharedtask:learningtodetecthedgesandtheirscopeinnaturallanguagetext.InProceedingsofthefour-teenthconferenceoncomputationalnaturallanguagelearning\u2013Sharedtask,pages1\u201312.BruceFraser.2010.Pragmaticcompetence:Thecaseofhedging.Newapproachestohedging,1534.ViolaGanterandMichaelStrube.2009.Findinghedgesbychasingweasels:Hedgedetectionusingwikipediatagsandshallowlinguisticfeatures.InProceedingsoftheACL-IJCNLP2009ConferenceShortPapers,pages173\u2013176.PranavGoel,YoichiMatsuyama,MichaelMadaio,andJustineCassell.2019.\u201cithinkitmighthelpifwemultiply,andnotadd\u201d:Detectingindirectnessincon-versation.In9thInternationalWorkshoponSpokenDialogueSystemTechnology,pages27\u201340.Springer.CynthiaHoward,PamelaW.Jordan,BarbaraMariaDiEugenio,andSandraKatz.2015.Shiftingtheload:apeerdialogueagentthatencouragesitshumancollab-oratortocontributemoretoproblemsolving.Interna-tionalJournalofArtificialIntelligenceinEducation,27:101\u2013129.KenHyland.1998.Hedginginscientificresearcharti-cles,volume54.JohnBenjaminsPublishing.GuolinKe,QiMeng,ThomasFinley,TaifengWang,WeiChen,WeidongMa,QiweiYe,andTie-YanLiu.2017.Lightgbm:Ahighlyefficientgradientboost-ingdecisiontree.Advancesinneuralinformationprocessingsystems,30:3146\u20133154.KlausKrippendorff.2004.Reliabilityincontentanaly-sis:Somecommonmisconceptionsandrecommen-dations.Humancommunicationresearch,30(3):411\u2013433.GeorgeLakoff.1975.Hedges:Astudyinmeaningcriteriaandthelogicoffuzzyconcepts.InContem-poraryresearchinphilosophicallogicandlinguisticsemantics,pages221\u2013271.Springer.\n2168\n\n\nMatthewLeach.2005.Rapport:Akeytotreatmentsuc-cess.Complementarytherapiesinclinicalpractice,11:262\u20135.IlyaLoshchilovandFrankHutter.2018.Decoupledweightdecayregularization.InInternationalConfer-enceonLearningRepresentations.ScottMLundberg,GabrielGErion,andSu-InLee.2018.Consistentindividualizedfeatureat-tributionfortreeensembles.arXivpreprintarXiv:1802.03888.ScottMLundbergandSu-InLee.2017.Aunifiedap-proachtointerpretingmodelpredictions.InProceed-ingsofthe31stinternationalconferenceonneuralinformationprocessingsystems,pages4768\u20134777.HowardLuneandBruceLBerg.2017.Qualitativeresearchmethodsforthesocialsciences.Pearson.MichaelMadaio,JustineCassell,andAmyOgan.2017.Theimpactofpeertutors\u2019useofindirectfeedbackandinstructions.Philadelphia,PA:InternationalSo-cietyoftheLearningSciences.BenMedlockandTedBriscoe.2007.Weaklysuper-visedlearningforhedgeclassificationinscientificliterature.InProceedingsofthe45thannualmeetingoftheassociationofcomputationallinguistics,pages992\u2013999.ChristophMolnar.2020.Interpretablemachinelearn-ing.Lulu.com.AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advancesinneuralinformationprocessingsystems,32:8026\u20138037.JamesWPennebaker,RyanLBoyd,KaylaJordan,andKateBlackburn.2015.Thedevelopmentandpsycho-metricpropertiesofliwc2015.Technicalreport.JeffreyPennington,RichardSocher,andChristopherDManning.2014.Glove:Globalvectorsforwordrep-resentation.InProceedingsofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing(EMNLP),pages1532\u20131543.EllenFPrince,JoelFrader,CharlesBosk,etal.1982.Onhedginginphysician-physiciandiscourse.Lin-guisticsandtheProfessions,8(1):83\u201397.AnnaProkofievaandJuliaHirschberg.2014.Hedgingandspeakercommitment.In5thIntl.WorkshoponEmotion,SocialSignals,Sentiment&LinkedOpenData,Reykjavik,Iceland.NilsReimersandIrynaGurevych.2019.Sentence-bert:Sentenceembeddingsusingsiamesebert-networks.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages3982\u20133992.OscarJRomero,RanZhao,andJustineCassell.2017.Cognitive-inspiredconversational-strategyreasonerforsocially-awareagents.InIJCAI,pages3807\u20133813.TimRowland.2007.\u2018wellmaybenotexactly,butit\u2019saroundfiftybasically?\u2019:Vaguelanguageinmath-ematicsclassrooms.InVaguelanguageexplored,pages79\u201396.Springer.SaschaSchneider,SteveNebel,SimonPradel,andG\u00fcn-terDanielRey.2015.Mindyourpsandqs!howpoliteinstructionsaffectlearningwithmultimedia.ComputersinHumanBehavior,51:546\u2013555.TanmaySinhaandJustineCassell.2015.Weclick,wealign,welearn:Impactofinfluenceandconvergenceprocessesonstudentlearningandrapportbuilding.InProceedingsofthe1stWorkshoponModelingINTERPERsonalSynchrONyAndInfLuence,INTER-PERSONAL\u201915,page13\u201320,NewYork,NY,USA.AssociationforComputingMachinery.HelenSpencer-Oatey.2005.(im)politeness,faceandperceptionsofrapport:Unpackagingtheirbasesandinterrelationships.1(1):95\u2013119.Gy\u00f6rgySzarvas,VeronikaVincze,Rich\u00e1rdFarkas,Gy\u00f6rgyM\u00f3ra,andIrynaGurevych.2012.Cross-genreandcross-domaindetectionofsemanticuncer-tainty.ComputationalLinguistics,38(2):335\u2013367.BuzhouTang,XiaolongWang,XuanWang,BoYuan,andShixiFan.2010.Acascademethodfordetectinghedgesandtheirscopeinnaturallanguagetext.InProceedingsoftheFourteenthConferenceonCom-putationalNaturalLanguageLearning\u2013SharedTask,pages13\u201317.MorganUlinski,SethBenjamin,andJuliaHirschberg.2018.Usinghedgedetectiontoimprovecommittedbelieftagging.InProceedingsoftheWorkshoponComputationalSemanticsbeyondEventsandRoles,pages1\u20135.TeppoVarttala.1999.Remarksonthecommunicativefunctionsofhedginginpopularscientificandspecial-istresearcharticlesonmedicine.Englishforspecificpurposes,18(2):177\u2013200.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,\u0141ukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.InAdvancesinneuralinformationpro-cessingsystems,pages5998\u20136008.ErikVelldal.2011.Predictingspeculation:asimpledis-ambiguationapproachtohedgedetectioninbiomed-icalliterature.JournalofBiomedicalSemantics,2(5):1\u201314.VeronikaVincze.2014.Uncertaintydetectioninnaturallanguagetexts.PhD,UniversityofSzeged,page141.\n2169\n\n\nNingWang,WLewisJohnson,RichardEMayer,PaolaRizzo,ErinShaw,andHeatherCollins.2008.Thepolitenesseffect:Pedagogicalagentsandlearningoutcomes.Internationaljournalofhuman-computerstudies,66(2):98\u2013112.ShawnWarner-Garcia.2014.Laughingwhennothing\u2019sfunny:Thepragmaticuseofcopinglaughterinthenegotiationofconversationaldisagreement.Prag-matics,24(1):157\u2013180.TimothyWilliamson.2002.Vagueness.Routledge.RanZhao,AlexandrosPapangelis,andJustineCassell.2014.Towardsadyadiccomputationalmodelofrap-portmanagementforhuman-virtualagentinteraction.InInternationalConferenceonIntelligentVirtualAgents,pages514\u2013527.Springer.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016a.Automaticrecognitionofconversa-tionalstrategiesintheserviceofasocially-awaredialogsystem.InProceedingsofthe17thAnnualMeetingoftheSpecialInterestGrouponDiscourseandDialogue,pages381\u2013392.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016b.Socially-awarevirtualagents:Au-tomaticallyassessingdyadicrapportfromtemporalpatternsofbehavior.InInternationalconferenceonintelligentvirtualagents,pages218\u2013233.Springer.AAdditionalinformationontheexperimentalsettingsWeusedPyTorch(Paszkeetal.,2019)toimple-menttheneuralmodels.Foreachsetoffeatures,hyperparameterswereselectedusingOptuna(Ak-iba,2019),aparametersearchframework.Were-implementedtheAttention-CNNwithGlove(Pen-ningtonetal.,2014)300-Dwordsembeddingsasthevectorrepresentation.Foreachmodels,theresultsarecross-validatedusing5folds(wechose5insteadof10toavoidhavingfoldswithtoofewsamplesperclass).Wecorrectedthelossfunctionforclassimbalancetoforcethemodeltoadaptmoretothelessfrequentclasses.Thestrengthofthiscorrectiondependedonthemodel,andwasselectedbecauseitprovidedasatisfyingcompro-misebetweenfavoringrecallandprecisionintheclassificationresultsofthatmodel.ForLightGBM,a\"squarerootofthesquarerootoftheinverseclassproportion\"correctionwasselected.Neu-ralmodelsweretrainedusingAdamWasanop-timizer(LoshchilovandHutter,2018),andusedareducedfeaturevector,obtainedwiththeap-plicationofPCA(dinit=1800;d=100;99.8%oftheinformationisconserved).Nosignifi-cantperformancedifferenceswereobservedbe-tweentheoriginalvectorandthereducedvectorfortrainingthemodels.TocomputetheSHAPvaluesmentionedinthepaper,wekeptonesplittoperformthe5-splitofthedataset,andleave1splittovalidateandearlystopthemodel,inor-dertoavoidoverfitting.Acompleteconfigura-tionofhyperparametersusedforeachmodelisre-portedintheGitHubrepositorywiththecodeofthepaper:https://github.com/YannRaphalen/Hedges-Detection.TheBERTmodelwasfine-tunedonaNvidiaQuadroRTX8000GPU.BTables\n2170\n\n\n?(whether|if|is|that|it|this)?.*Subj.\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nNoNoYesNoNoNoYesYesYesYesYesBERT(PTE)\nexceptionally|forthemostpart|inamannerofspeaking|\nNoYesNoNoNoYesNoNoYesYesNoMLP(KDF)\nsomethingbetween|essentially|only).*Prop.\nTable8:Significancetableforthe3-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesYesYesYesYesNoYesYesYesNoYesMLP(KDF+PTE)\ntechnically|typically|virtually|approximately|\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nmuch|somewhat|exactly|almost|littlebit|quite|\nClass\nrelatively|roughly|sotosay|strictlyspeaking|\nNoNoYesNoNoYesNoYesYesYesYesLSTM(KDF)\nYesNoNoNoNo+TM\nTable10:Significancetableforthe3-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoNoYesYesYesYesYesYesNoYesYes\n.*(it)(looks|seems|appears)[,]?.*\",\".*(or|and)(that|something|stuff|soforth)Table7:Regexprulesusedfortheclassifier.\nRule-based\nRule-based\n.*(you(might|may)(believe|think)).*Subj.\nYesYesYesYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nYesYesNoYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nNoNoYesNoNoYesNoNoYesYesYesLSTM(KDF)\nYesYesNoYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(i|i\u2019m|you|it\u2019s)(am|are)(apparently|surely)[,]?.*Prop.\nNoNoYesNoNoYesNoYesYesYesYesLSTM(PTE)\nNoNoYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nNoNoYesYesNoYesYesYesYesYesNoLGBM(KDF)\nNoNoYesYesYesYesYesYesYesYesNoLGBM(KDF)\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nYesNoYesYesYesYesYesYesNoYesYes\nYesYesYesYesYesYesYesYesYesYesYesMLP(KDF+PTE)\nregular|regularly|actually|almost|asitwere|basically|\nNoYesYesNoYesYesYesNoYesYesNoMLP(PTE)\nRule(regexp)\n.*(i|i\u2019m|we)?(was|am|wasn\u2019t)??(not)?(sure|certain).*Subj.\n.*(accordingto|presumably).*Subj.\n(?!.*(be|been|was)likeexcuseme)((excuseme|sorry)[w,\u2019]+|[w,\u2019]+(excuseme|sorry))Prop.\n.*(i|you|we)haveto(check|look|verify).*Subj.\n.*(i\u2019m|i|we\u2019re)(am|are)??(apologize|sorry).*Apol.\nonthetallside|parexcellence|particularly|\n(?!what).*(i|we)?(don\u2019t|didn\u2019t|did)??(not)?\ninarealsense|inasense|inaway|largely|literally|\nYesYesYesNoNoNoYesYesYesYesYesBERT(PTE)\nModels\nModels\nModels\nYesYesYesNoNoYesNoNoYesYesYesLSTM(PTE)\nYesNoNoNoNo\nYesYesYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nSubj.\nTable9:Significancetableforthe4-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoYesNoNoNoYesNoNoYesYesNoMLP(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(ifeellikeyou).*Subj.\nlooselyspeaking|kinda|moreorless|mostly|often|\n.*(unlessi).*Apol.\nprobably|canbeviewas|crypto-|especially|essentially|\n.*(ifi\u2019mnotwrong|ifi\u2019mright|ifthat\u2019strue).*Subj.\nYesNoNoNoNo+POS\n(guess|guessed|thought|think|believe|believed|suppose|supposed)\n.*(just|alittle|maybe|actually|sortof|kindof|pretty\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nNoYesYesNoYesYesNoNoYesYesYesMLP(KDF)\nprettymuch|principally|pseudo-|quintessentially|\n2171\n\n\nFigure2:Absoluteaveragedfeaturecontribution,asindicatedbySHAP.Thelongerthebarisforonecolor,themorethefeatureisassociatedwiththeclassrepresentedbythatcolor.\nFigure3:Averagedcontributionoffeaturestothedetectionofthe\"Notindirect\"class,asindicatedbySHAP.Eachdotcorrespondstoaclassifiedclause.Areddotindicatesthatthefeatureispresentintheclause,whileabluedotindicatesthatthefeatureisabsent.Thefartherontherightthedotis,themorethefeaturecontributedtoitsclassificationasahedge.\n2172\n\n\nFigure4:Averagedcontributionoffeaturestothedetectionof\"Apologizers\",asindicatedbySHAP.\nFigure5:Averagedcontributionoffeaturestothedetectionof\"Propositionalhedges\",asindicatedbySHAP.\n2173\n\n\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nYesNoNoNoNo+TM\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nFigure6:Averagedcontributionoffeaturestothedetectionof\"Subjectivizers\",asindicatedbySHAP.\nModels\nYesNoNoNoNo\nTable11:Significancetableforthe4-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesNoNoNoNo+POS\n2174"}, {"question": " What specific strategy related to indirectness is important in the tutoring domain?", "answer": " Hedging", "ref_chunk": "\"Youmightthinkaboutslightlyrevisingthetitle\":IdentifyingHedgesinPeer-tutoringInteractionsYannRaphalen1,Chlo\u00e9Clavel2,JustineCassell1,31InriaParis2LTCI,InstitutPolytechniquedeParis,Telecom-Paris3CarnegieMellonUniversityyann.raphalen.pro@gmail.com,justine@cs.cmu.edu,chloe.clavel@telecom-paris.frAbstractHedgesplayanimportantroleinthemanage-mentofconversationalinteraction.Inpeer-tutoring,theyarenotablyusedbytutorsindyads(pairsofinterlocutors)experiencinglowrapporttotonedowntheimpactofinstructionsandnegativefeedback.Pursuingtheobjectiveofbuildingatutoringagentthatmanagesrap-portwithstudentsinordertoimprovelearning,weusedamultimodalpeer-tutoringdatasettoconstructacomputationalframeworkforiden-tifyinghedges.Wecomparedapproachesre-lyingonpre-trainedresourceswithothersthatintegrateinsightsfromthesocialsciencelitera-ture.Ourbestperformanceinvolvedahybridapproachthatoutperformstheexistingbase-linewhilebeingeasiertointerpret.Weemployamodelexplainabilitytooltoexplorethefea-turesthatcharacterizehedgesinpeer-tutoringconversations,andweidentifysomenovelfea-tures,andthebenefitsofsuchahybridmodelapproach.1IntroductionRapport,mostsimplydefinedasthe\u201c...relativeharmonyandsmoothnessofrelationsbetweenpeo-ple...\u201d(Spencer-Oatey,2005),hasbeenshowntoplayaroleinthesuccessofactivitiesasvariedaspsychotherapy(Leach,2005)andsurveyinterview-ing(LuneandBerg,2017).Inpeer-tutoring,rap-port,asmeasuredbytheannotationofthinslicesofvideo,hasbeenshowntobebeneficialforlearningoutcomes(Zhaoetal.,2014;SinhaandCassell,2015).Thelevelofrapportrisesandfallswithconversationalstrategiesdeployedbytutorsandtuteesatappropriatetimes,andasafunctionofthecontentofpriorturns.Thesestrategiesincludeself-disclosure,referringtosharedexperience,and,onthepartoftutors,givinginstructionsinanindirectmanner.Someworkhasattemptedtoautomaticallydetectthesestrategiesintheserviceofintelligenttutors(Zhaoetal.,2016a),butonlyafewstrate-gieshavebeenattempted.Otherworkhascon-centratedona\"socialreasoningmodule\"(Romeroetal.,2017)todecidewhichstrategiesshouldbegeneratedinagivencontext,butindirectnesswasnotamongthestrategiestargeted.Inthispaper,wefocusontheautomaticclassificationofonespe-cificstrategythatisparticularlyimportantforthetutoringdomain,andthereforeimportantforintel-ligenttutors:hedging,asub-partofindirectnessthat\"softens\"whatwesay.Thisworkispartofalargerresearchprogramwiththelong-termgoalofautomaticallygeneratingindirectnessbehaviorsforatutoringagent.\nFigure1:Amockconversationdisplayingeachtypeofhedgedformulation.AccordingtoBrownandLevinson(1987),hedgesarepartofthelinguistictoolsthatinterlocu-torsusetoproducepoliteness,bylimitingthefacethreattotheinterlocutor(basicallybylimitingtheextenttowhichtheinterlocutormightexperienceembarrassmentbecauseofsomekindofpoorper-formance).Anexampleis\"that\u2019skindofawronganswer\".Hedgesarealsofoundwhenspeakerswishtoavoidlosingfacethemselves,forexam-plewhensaying(\"IthinkImighthavetoadd6.\").Madaioetal.(2017)foundthatinapeer-tutoringtask,whenrapportbetweeninterlocutorsislow,tu-teesattemptedmoreproblemsandcorrectlysolvedmoreproblemswhentheirtutorshedgedinstruc-\n2160 Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1: Long Papers, pages 2160 - 2174 May 22-27, 2022 c(cid:13)2022 Association for Computational Linguistics\n\n\ntions,whichlikewisepointstowardsa\"mitigationoffacethreat\"function.Hedgescanalsobeasso-ciatedwithanonverbalcomponent,forexampleavertedeyegazeduringcriticism(BurgoonandKoper,1984).Hedgesarenot,however,alwaysap-propriate,asin\"Ikindofthinkit\u2019srainingtoday.\"whentheinterlocutorscanbothseerain(althoughitmightbetakenashumorous).Thesefactsabouthedgesmotivateawaytoautomaticallydetectthemand,ultimately(althoughnotinthecurrentwork)alsogeneratethem.Inbothcaseswefirsthavetobeabletocharacterizethemusinginterpretablelinguisticfeatures,whichiswhatweaddressinthecurrentpaper.Thus,intheworkdescribedhere,basedonlinguisticdescriptionsofhedges(BrownandLevinson,1987;Fraser,2010),webuiltarule-basedclassifier.Weshowthatthisclassifierincombinationwithadditionalmultimodalinter-pretablecontext-dependentfeaturessignificantlyimprovestheperformanceofamachinelearningmodelforhedges,comparedtoalessinterpretabledeeplearningbaselinefromGoeletal.(2019)us-ingwordembeddings.Wealsoreliedonamachinelearningmodelexplanationtool(LundbergandLee,2017)toinvestigatethelinguisticfeaturesrelatedtohedgesinthecontextofpeer-tutoring,primarilytoseeifwecoulddiscoversurprisingfeaturesthattheclassificationmodelwouldassociatetohedgesinthiscontext,andwedescribethosebelow.Thecodeofthemodelsdescribedinthepaperisalsoprovided.12RelatedworkHedges:AccordingtoFraser(2010),hedgingisarhetoricalstrategythatattenuatesthestrengthofastatement.Onewaytoproduceahedgeisbyalteringthefullsemanticvalueofaparticu-larexpressionthroughPropositionalhedges(alsocalledApproximatorsinPrinceetal.(1982)),asin\"Youarekindofwrong,\"thatreduceprototypical-ity(i.eaccuracyofthecorrespondencebetweenthepropositionandtherealitythatthespeakerseekstodescribe).Propositionalhedgesarerelatedtofuzzylanguage(Lakoff,1975),andthereforetotheproductionofvagueness(Williamson,2002)anduncertainty(Vincze,2014).AsecondkindareRelationalHedges(alsocalledShieldsinPrinceetal.(1982)),suchas\u201cIthinkthatyouarewrong.\u201dor\u201cThedoctorwantsyoutostopsmoking.\u201d,conveyingthatthepropositionis\n1https://github.com/AnonymousHedges/HedgeDetectionconsideredbythespeakerassubjective.Inafurthersub-division,AttributionShields,asin\"Thedoc-torwantsyou...\",theinvolvementofthespeakerinthetruthvalueofthepropositionisnotmadeexplicit,whichallowsspeakersnottotakeastance.Asdescribedabove,Madaioetal.(2017)foundthattutorswhoshowedlowerrapportwiththeirtuteesusedmorehedgedinstructions(theyalsoemployedmorepositivefeedback),howeverthiswasonlythecasefortutorswithagreaterbeliefintheirabilitytotutor.Tuteesinthiscontextsolvedmoreproblemscorrectlywhentheirtutorshedgedinstructions.Noeffectofhedgingwasfoundfordyads(pairsofinterlocutors)withgreatersocialcloseness.However,theauthorsdidnotlookatthespecificlinguisticformstheseteenagersused.Rowland(2007)alsodescribestherolethathedg-ingplaysinthisagegroup,showingthatstudentsusebothrelational(\"IthinkthatJohnissmart.\")andpropositional(\"Johniskindofsmart.\")hedgesformuchthesameshieldingfunctionofdemon-stratinguncertainty,tosavethemfromtheriskofembarrassmentiftheyarewrong.TheauthorobservedthatteensusedfewAdaptors(kindof,somewhat)andpreferredtouseRounders(around,closeto).However,thisstudywasperformedwithanadultandtwochildren,possiblybiasingthere-sultsduetotheparticipationoftheadultinvestiga-tor.Hedgeshavebeenincludedinvirtualtutoringagentsbeforenow.(Howardetal.,2015)integratedhedgesinatutoragentforundergraduatesinCS,asawaytoencouragethestudenttotaketheinitiative.Hedgeshavealsobeenusedasawayofintegrat-ingBrownandLevinson\u2019spolitenessframework(Wangetal.,2008;Schneideretal.,2015)invir-tualtutoringagents.Resultswerenotbrokenoutbystrategy,butpolitenessingeneralwasshowntopositivelyinfluencemotivationandlearning,incertainconditions.Computationalmethodsforhedgedetection:Anumberofstudieshavetargetedthedetectionofhedgesanduncertaintyintext(MedlockandBriscoe,2007;GanterandStrube,2009;Tangetal.,2010;Velldal,2011;Szarvasetal.,2012),partic-ularlyfollowingtheCoNLL2010datasetrelease(Farkasetal.,2010).However,thisworkisnotasrelatedtohedgesinconversation,asitfocusesonaformalandacademiclanguageregister(Hy-land,1998;Varttala,1999).AsnotedbyProkofievaandHirschberg(2014),thefunctionsofhedgesaredomain-andgenre-dependent,thereforethisbias\n2161\n\n\ntowardsformalityimpliesthattheexistingworkmaynotadaptwelltothedetectionofhedgesinconversationbetweenteenagers.Aconsequenceisthattheexistingworkdoesnotconsidertermslike\"Ithink,\"sinceopinionsrarelyappearinanaca-demicwritingdataset.Instructionsarealsoalmostabsent(\"Ithinkyouhavetoaddtentobothsides.\"),astronglimitationforthestudyofconversationalhedgessinceitisinrequests(includingtutoringin-structions)thatindirectformulationsmostlyoccuraccordingtoBlum-Kulka(1987).ProkofievaandHirschberg(2014)alsonotethatitisdifficulttodetecthedgesbecausethewordpatternsassociatedwiththemhaveothersemanticandpragmaticfunc-tions:considering\"Ithinkthatyouhavetoaddxtobothsides.\"vs\"Ithinkthatyouareanidiot.\",itisnotclearthattheseconduseof\"Ithinkthat\"isanhedgemarker.Theyadvocateusingmachinelearningapproachestodealwiththeambiguityofthesemarkers.Workingonaconversationaldataset,Ulinskietal.(2018)builtacomputationalsystemtoassessspeakercommitment(i.e.atwhichpointthespeakerseemsconvincedbythetruthvalueofastatement),inparticularbyrelyingonarule-baseddetectionsystemforhedges.Comparedtothatwork,ourrule-basedclassificationmodelisdirectlydetectinghedgeclasses,andweemploythepredictionsoftherule-basedmodelasafeatureforstrongermachinelearningmodels,designedtolessentheimpactoftheimbalancebetweenclasses.Wealsoconsiderapologieswhentheyserveamit-igationfunction(wethencallthemApologizers),aswasdonebytheauthorsofourcorpus,andwealsousethetermsubjectivizersasdefinedbelow,tobeabletocomparedirectlywiththepreviousworkcarriedoutonthiscorpus.Asfarasweknow,onlyGoeletal.(2019)haveworkedwithapeer-tutoringdataset(thesameonethatwealsouse),andtheyachievedtheirbestclassificationresultbyemployinganAttention-CNNmodel,inspiredbyAdelandSch\u00fctze(2017).3ProblemstatementWeconsiderasetDofconversationsD=(c1,c2,...,c|D|),whereeachconversationiscom-posedofasequenceofindependentsyntacticclausesci=(u1,u2,...,uM),whereMisthenumberofclausesintheconversation.Notethattwoconsecutiveclausescanbeproducedbythesamespeaker.Eachclauseisassociatedwithauniquelabelcorrespondingtothediffer-enthedgeclassesdescribedinTable1:yi\u2208C={PropositionalHedges,Apologizers,Subjec-tivizers,Nothedged}.Finally,anutteranceuicanberepresentedasavectoroffeaturesX=(x1,x2,...,xN),whereNrepresentsthenumberoffeaturesweusedtodescribeaclause.Ourfirstgoalistodesignamodelthatcorrectlypredictsthelabelyiassociatedtoui.Itcanbeunderstoodasthefollowingresearchquestion:RQ1:\"Whichmodelsandfeaturescanbeusedtoautomaticallycharacterizehedgesinapeer-tutoringinteraction?\"Oursecondgoalistoidentify,foreachhedgeclass,thesetoffeaturesFclass={fk},k\u2208[1,N]sortedbyfeatureimportanceintheclassificationofclass.Itcorrespondstothefollowingresearchquestion:RQ2:\"Whatarethemostimportantlinguisticfeaturesthatcharacterizeourhedgeclassesinapeer-tutoringsetting?\"4Methodology4.1CorpusDatacollection:Thedialoguecorpususedherewascollectedaspartofalargerstudyontheeffectsofrapport-buildingonreciprocalpeertutoring.24Americanteenagers(meanage=13.5,min=12,max=15),halfmaleandhalffemale,cametoalabwherehalfoftheparticipantswerepairedwithasame-age,same-genderfriend,andtheotherhalfpairedwithastranger.Theparticipantswereassignedtoatotalof12dyadsinwhichthepar-ticipantsalternatedtutoringoneanotherinlinearalgebraequationsolvingfor5weeklyhour-longsessions,foratotalcorpusofnearly60hoursofface-to-faceinteractions.Eachsessionwasstruc-turedsuchthatthestudentsengagedinbriefsocialchitchatinthebeginning,thenoneofthestudentswasrandomlyassignedtotutortheotherfor20minutes.Theythenengagedinanothersocialpe-riod,andconcludedwithasecondtutoringperiodwheretheotherstudentwasassignedtheroleoftutor.Audioandvideodatawererecorded,tran-scribed,andsegmentedforclause-leveldialogueannotation,providingnearly24000clauses.Non-speechsegments(notablyfillersandlaughter)weremaintained.Becauseoftemporalmisalignmentforpartsofthecorpus,manyparaverbalphenomena,suchasprosody,wereunfortunatelynotavailabletous.SinceouraccesstothedatasetiscoveredbyaNon-DisclosureAgreement,itcannotbereleased\n2162\n\n\n2https://github.com/AnonymousHedges/HedgeDetectionclasses.4.2FeaturesLabelfromrule-basedclassifier(LabelRB):Weusetheclasslabelpredictedbytherule-basedclas-sifierdescribedinSection4.3asafeature.Ourhypothesisisthatthemachinelearningmodelcanusethisinformationtocounterbalancetheclassimbalance.Totakeintoaccountthefactthatsomerulesaremoreefficientthanothers,weweightedtheclasslabelresultingfromtherule-basedmodelbytheprecisionoftherulethatgeneratedit.Unigramandbigram:Wecountthenumberofoccurrencesofunigramsandbigramsofthecorpusineachclause.Weusedthelemmaofthewordsforunigramsandbigramsusingthenltklemmatizer(Loper,2002)andselectedunigramsandbigramsthatoccurredinthetrainingdatasetatleastfiftytimes.Thegoalwastoinvestigate,withabottom-upapproach,towhatextenttheuseofcertainwordscharacterizeshedgeclassesintutoring.InSection5weexaminetheoverlapbetweenthesewordsandthoseaprioriidentifiedbytherules.Part-of-speech(POS):Hedgeclassesseemtobeassociatedwithdifferentsyntacticpatterns:forex-ample,subjectivizersmostoftencontainapersonalpronounfollowedbyaverb,asin\"Iguess\",\"Ibelieve\",\"Ithink\".WethereforeconsideredthenumberofoccurrencesofPOS-Tagn-grams(n=1,2,3)asfeatures.WeusedthespaCyPOS-taggerandconsideredPOSunigrams,bigramsandtri-gramsthatoccuratleast10timesinthetrainingdataset.LIWC:LinguisticInquiryandWordCount(LIWC)(Pennebakeretal.,2015)isstandardsoft-wareforextractingthecountofwordsbelongingtospecificpsycho-socialcategories(e.g.,emotions,religion).Ithasbeensuccessfullyusedinthede-tectionofconversationalstrategies(Zhaoetal.,2016a).Wethereforecountthenumberofoccur-rencesofallthe73categoriesfromLIWC.Tutoringmoves(TM):Intelligenttutoringsys-temsrelyonspecifictutoringmovestosuccess-fullyconveycontent(asdohumantutors).Wethereforelookedatthelinkbetweenthetutoringmoves,asannotatedinMadaioetal.(2017),andhedges.Fortutors,thesemovesare(1)instruc-tionaldirectivesandsuggestions,(2)feedback,and(3)affirmations,mostlyexplicitreflectionsontheirpartners\u2019comprehension,whilefortutees,theyare(1)questions,(2)feedbacks,and(3)affirmations,\npublicly.Howevertheoriginalexperimenters\u2019In-stitutionalReviewBoard(IRB)approvalallowsustoview,annotate,andusethedatatotrainmodels.Thisalsoallowsustoprovidealinktoapixe-latedvideoexampleintheGitHubrepositoryoftheproject2.Dataannotation:Thedatasetwaspreviouslyan-notatedbyMadaioetal.(2017),followingananno-tationmanualthatusedhedgeclassesderivedfromRowland(2007)(seeTable1).Onlythetaskperi-odsoftheinteractionswereannotated.Comparingtheannotationswiththeclassesmentionedintherelatedworksection,SubjectivizerscorrespondtoRelationalhedges(Fraser,2010),PropositionalhedgesandExtenderscorrespondtoApproxima-tors(Princeetal.,1982)withtheadditionofsomediscoursemarkerssuchasjust.ApologizersarementionedaslinguistictoolsrelatedtonegativepolitenessinBrownandLevinson(1987).Krippen-dorff\u2019salphaobtainedforthiscorpusannotatedbyfourcoderswasover0.7forallclasses(denotinganacceptableinter-coderreliabilityaccordingtoKrippendorff(2004)).Thedatasetiswidelyim-balanced,withmorethan90%oftheutterancesbelongingtotheNothedgedclass.Inreviewingthecorpusandtheannotationman-ual,however,wenoticedtwoissues.First,theannotationoftheExtendersclasswasinconsis-tent,leadingtotheExtendersandPropositionalhedgesclassescarryingsimilarsemanticfunctions.WethereforemergedthetwoclassesandgroupedutteranceslabeledasExtendersandthoselabeledasPropositionalhedgesundertheheadingofPropositionalhedges.Second,theannotationofclausescontainingthetokens\"just\"and\"would\"(twotermsoccurringfrequentlyinthedatasetthatarekeycomponentsofPropositionalHedgesandSubjectivizersbutthatarenotinfacthedgesinallcases)wasalsoinconsistent,leadingtovirtuallyallclauseswiththosetwotokensbeingconsideredhedges.Wethereforere-consideredalltheclausesassociatedwithanyofthehedgeclasses,aswellasalltheclausesinthe\"Nothedged\"classthatcontained\"just\"or\"would\".There-annotationwascarriedoutbytwoannotatorswhoachievedaKrippendorff\u2019salphainter-raterreliabilityof.9orbetterforApologizers,Subjectivizers,andPropo-sitionalhedgesbeforeindependentlyre-annotatingtherelevantclauses.Anexampleofare-annotationwasremoving\"Iwouldkillyou!\"fromthehedge\n2163\n\n\nProp.hedgesApologizersSubjectivizersNothedgedTotal\nTable1:Definitionoftheclasses\nFeaturesnameAutomaticextractionVectorsize\nSubjectivizersWordsthatreduceintensityorcertainty\u201cSothenIwoulddividebytwo.\u201dApologizersApologiesusedtosoftendirectspeechacts\u201cOhsorrysixb.\u201dPropositionalhedgesQualifyingwordstoreduceintensityorcertaintyofutterances\u201cIt\u2019sactuallyeight.\u201dExtendersWordsusedtoindicateuncertaintybyreferringtovaguecategories\u201cIt\u2019llbethenumberxorwhatevervariableyouhave.\u201d\n12101286262119223156\nTable2:Distributionoftheclasses\nClassDefinitionExample\nRule-basedlabelYes4UnigramYes~250BigramYes~250POSYes~1200LIWCYes73NonverbalNo24TutoringmovesNo6Total~1800\nTable3:Listofautomaticallyextractedandmanuallyannotatedfeatureswiththeirsize.mostlytentativeanswers.Nonverbalandparaverbalbehaviors:AsinGoeletal.(2019),weincludedthenonverbalandpar-averbalbehaviorsthatarerelatedtohedges.Specif-ically,weconsiderlaughterandsmiles,thathavebeenshowntobeeffectivemethodsofmitiga-tion(Warner-Garcia,2014),cut-offsindicatingself-repairs,fillerslike\"Um\",gazeshifts(annotatedas\u2019GazeatPartner\u2019,\u2019GazeattheMathWorksheet\u2019,and\u2019Gazeelsewhere\u2019),andheadnods.Eachfea-turewaspresenttwiceinthefeaturevector,onetimeforeachinterlocutor.Inter-raterreliabilityfornonverbalbehaviorwas0.89(asmeasuredbyKrippendorff\u2019salpha)foreyegaze,0.75forsmilecount,0.64forsmiledurationand0.99forheadnod.Laughterisalsoreportedinthetranscriptatthewordlevel.Weseparatethetutor\u2019sbehaviorsfromthoseofthetutee.ThecollectionprocessforthesebehaviorsisdetailedfurtherinZhaoetal.(2016b).Theclause-levelfeaturevectorwasnormalizedbythelengthoftheclause(exceptfortherule-basedlabel).Thislengthwasalsoaddedasafeature.Table3presentsanoverviewofthefinalfeaturevector.4.3ClassificationmodelsTheclassificationmodelsusedarepresentedhereaccordingtotheirlevelofintegrationofexternallinguisticknowledge.Rule-basedmodel:OnthebasisoftheannotationmanualusedtoconstructthedatasetfromMadaioetal.(2017),andwithdescriptionsofhedgesfromRowland(2007),Fraser(2010)andBrownandLevinson(1987),weconstructedarule-basedclas-sifierthatmatchesregularexpressionsindicativeofhedges.TherulesaredetailedinTable7intheAppendix.LGBM:Sincehedgesareoftencharacterizedbyexplicitlexicalmarkers,wetestedtheassumptionthatamachinelearningmodelwithaknowledge-drivenrepresentationforclausescouldcompetewithaBERTmodelinperformance,whilebeingmuchmoreinterpretable.WereliedonLightGBM,anensembleofdecisiontreestrainedwithgradi-entboosting(Keetal.,2017).Thismodelwasselectedbecauseofitsperformancewithsmalltrainingdatasetsandbecauseitcanignoreunin-formativefeatures,butalsoforitstrainingspeedcomparedtoalternativeimplementationsofgradi-entboostingmethods.Multi-layerperceptron(MLP):Asasimplebase-line,webuiltamulti-layerperceptronusingthreesetsoffeatures:apre-trainedcontextualrepre-sentationoftheclause(SentBERT;ReimersandGurevych(2019));theconcatenationofthiscon-textualrepresentationoftheclauseandarule-basedlabel(notrelyingonthepreviousclauses);andfinallytheconcatenationofallthefeaturesmen-tionedinsection4.2,withoutthecontextualizedrepresentation.LSTMoverasequenceofclauses:Sinceweareworkingwithconversationaldata,wealsowantedtotestwhethertakingintoac-countthepreviousclauseshelpstodetectthetypeofhedgeclassinthenextclause.Formally,wewanttoinferyiusingyi=maxy\u2208ClassesP(y|X(ui),X(ui\u22121),...,X(ui\u2212K)),whereKisthenumberofpreviousclausesthatthemodelwilltakeintoaccount.The\n2164\n\n\nMLPmodelpresentedaboveinfersyiusingyi=maxy\u2208ClassesP(y|X(ui)),thereforeadifferenceofperformancebetweenthetwomodelswouldbeasignthatusinginformationfromthepreviousclausescouldhelptodetectthehedgedformulationinthecurrentclause.WetestedaLSTMmodelwiththesamerepresentationsforclausesasfortheMLPmodel.CNNwithattention:Goeletal.(2019)estab-lishedtheirbestperformanceonhedgedetec-tionusingaCNNmodelwithadditiveattentionoverword(andnotclause)embeddings.Con-trarytotheMLPandLSTMmodelsmentionedabove,thismodeltriestoinferyiusingyi=maxy\u2208ClassesP(y|g(w0),g(w1),...,g(wL)),withLrepresentingthemaximumclauselengthweal-low,andgrepresentingafunctionthatturnsthewordwj,j\u2208[0,L]intoavectorrepresentation(formoredetails,pleaseseeAdelandSch\u00fctze(2017)).BERT:Tobenefitfromdeepsemanticandcon-textualrepresentationsoftheutterances,wealsofine-tunedBERT(Devlinetal.,2019)onourclas-sificationtask.BERTisapre-trainedTransformersencoder(Vaswanietal.,2017)thathassignificantlyimprovedthestateoftheartonanumberofNLPtasks,includingsentimentanalysis.Itproducesacontextualrepresentationofeachwordinasen-tence,makingitcapableofdisambiguatingthemeaningofwordslike\"think\"or\"just\"thatarerepresentativeofcertainclassesofhedges.BERT,however,isnotablyhardtointerpret.4.4AnalysistoolsLookingatwhichfeaturesimprovetheperfor-manceofourclassificationmodelstellsuswhetherthesefeaturesareinformativeornot,butdoesnotexplainhowthesefeaturesareusedbythemod-elstomakeagivenprediction.Wethereforepro-ducedacomplementaryanalysisusinganinter-pretabilitytool.Asdemonstratedby(LundbergandLee,2017),LightGBMinternalfeatureimpor-tancescoresareinconsistentwithboththemodelbehaviorandhumanintuition,soweinsteadusedamodel-agnostictool.SHAP(LundbergandLee,2017)assignstoeachfeatureanimportancevalue(calledShapleyvalues)foraparticularpredictiondependingontheextentofitscontribution(ade-tailedintroductiontoShapleyvaluesandSHAPcanbefoundinMolnar(2020)).SHAPisamodel-agnosticframework,thereforethevaluesassoci-atedwithasetoffeaturescanbecomparedacrossmodels.ItshouldbenotedthatSHAPproducesexplanationsonacase-by-casebasis,thereforeitcanbothprovidelocalandglobalexplanations.FortheGradientBoostingmodel,weuseanadaptedversionofSHAP(Lundbergetal.,2018),calledTreeSHAP.5Experimentsandresults5.1ExperimentalsettingTodetectthebestsetoffeatures,weusedLight-GBMandproceededincrementally,byaddingthegroupoffeatureswethoughttobemostlikelyasso-ciatedwithhedges.Wedidnotconsidertheriskofrelyingonasub-optimalsetoffeaturesthroughthisprocedurebecauseofthestrongabilityofLight-GBMtoignoreuninformativefeatures.Weusethisincrementalapproachasawaytotestourintuitionabouttheperformativityofgroupsoffeatures(i.e.doesaddingafeatureimprovetheperformanceofthemodel)withregardtothetaskofclassifica-tion.Tocompareourmodels,wetrainedthemonthe4-classtask,andlookedattheaverageoftheweightedF1-scoresforthethreehedgeclasses(i.e.howwellthemodelsinferminorityclasses)thatwereporthereas\"3-classes\",andattheaverageoftheweightedF1-scoresforthe4classes,thatwereportas\"4-classes\".DetailsofthehyperparametersandexperimentalsettingsareprovidedinAppendixA.5.2ModelcomparisonandfeatureanalysisOverallresults:Table4presentstheresultsob-tainedbythe6modelspresentedinSection4.3forthemulti-classproblem.Bestperformance(F1-scoreof79.0)isobtainedwithLightGBMlever-agingalmostallthefeatures.Intheappendix(seeTable8andTable9)weindicatetheconfidenceintervalstorepresentthesignificanceofthediffer-encesbetweenthemodels.First,andperhapssurprisingly,wenoticethattheuseof\"Knowledge-Driven\"featuresbasedonrulesbuiltfromlinguisticknowledgeofhedgesintheLightGBMmodeloutperformstheuseofpre-trainedembeddingswithinafine-tunedBERTmodel(79.0vs.70.6),andintheneuralbaselinefrom(Goeletal.,2019)(79.0vs64.5).ThelowscoresobtainedbytheLGBM,LSTMandMLPmodelswithpre-trainedsentenceem-beddingsversusKnowledge-Drivenfeaturesmightsignalthatthewordpatternscharacterizinghedgesarenotsalientintheserepresentations(i.e.the\n2165\n\n\n68.5(1.6)35.8(3.1)64.8(1.1)Attention-CNN(3-classes)\n65.1(5.7)39.8(8.0)65.2(5.1)BERT(3-classes)\n\u220570.6(2.3)\u2205LGBM(3-classes)\n79.0(1.3)35.0(2.2)70.1(1.4)\nRule-based(3-classes)\n94.7\u2205\u2205MLP(4-classes)\n3Notethatthereisstrongredundancybetweensomefea-turesofLIWCandthespaCyPOStaggerthatbothproducea\"Pronoun\"category,usingalexiconinthefirstcase,andaneuralinferenceinthesecond.\n\u220594.9(0.4)\u2205LGBM(4-classes)\nRule-based(4-classes)\nTable4:AveragedweightedF1-scores(andstandarddeviation)forthethreeminorityclassesandforthe4classes,forallmodels.\"KD\"standsfor\"Knowledge-Driven\",meaningthatthefeaturesarederivedfromlexicon,n-grammodelsandannotations.distancebetween\"Ithinkyoushouldadd5.\"and\"Youshouldadd5.\"isshort.).KDFeaturesseemtoprovideabetterseparabilityoftheclasses.ThecombinationofKDfeaturesandPre-trainedem-beddingsdoesnotsignificantlyimprovetheperfor-manceofthemodelscomparedtotheKDFeaturesonly,whichsuggeststhattheinformationfromthePre-trainedembeddingsisredundantwiththeonefromtheKDFeatures.Thisresultmaybeduetothehighdimensionalityoftheinputvector(868withPCAontheKDFeatures;2500otherwise).Asecondfindingisthattheuseofgradientboost-ingmodelsontopofrule-basedclassifiersbettermodelsthehedgeclasses.Theothermachinelearn-ingmodelsdidnotprovetobeaseffective,exceptforBERT.FeatureanalysisusingLightGBM:Usingthebestperformingmodel,Table5showstheroleofeachfeaturesetinthepredictiontask.ThesignificanceofthedifferencesisshowninTable10andTable11.Comparedtotherule-basedmodel,theintroductionofn-gramssignificantlyimprovedtheperformanceofourclassifier,suggestingthatsomelexicalandsyntacticinformationdescribingthehedgeclasseswasnotpresentintherule-basedmodel.LookingatTable5,wedonotobservesignificantdifferencesbetweentheLGBMmodelusingonlythelabelrulebased+(1-gramsand2-grams)andthemodelsin-corporatingmorefeatures.Tooursurprise,neitherthetutoringmovesnorthenonverbalfeaturessig-nificantlyimprovedtheperformanceofthemodel.The2featureswereincludedtoindexthespecificpeertutoringcontextofthesehedges,sothisindi-catesthatinfutureworkwemightwishtoapplythecurrentmodeltoanothercontextofusetoseeifthismodelofhedgesismoregenerallyapplicablethanweoriginallythought.Bycombiningthisresultwiththeincreasedperformanceofthemodelus-ingKnowledge-Driven(i.e.explicit)featurescom-paredtopre-trainedembeddings,itwouldseemthathedgesareaboveallalexicalphenomenon(i.e.producedbyspecificlexicalelements).5.3In-depthanalysisoftheinformativefeaturesWetrainedtheSHAPexplanationmodelsonLight-GBMwithallfeatures.Themostinformativefea-tures(inabsolutevalue)foreachclassareshowninTable6,andtheplotsbyclassarepresentedintheAppendix.Themostimportantfeaturesseemtobetherule-basedlabels,whichappearinatleastthefourthpositionforthreeclasses(seeTable6),andinthefirstpositionforPropositionalHedgesandNothedgedclasses.Surprisingly,theRule-Basedlabeldoesnotappearinthetop20featuresforApologizers.However,giventhattheclassrarelyappearsinthedata,therulesseldomactivate,sothefeaturemaysimplybeinformativeforaverysmallnumberofclauses.Unigrams(Oh,Sorry,just,Would,andI)arealsopresentinthe5top-rankedfeatures.Thisconfirmsthefindingsmen-tionedinrelatedworkforthecharacterizationofthedifferenthedgeclasses(justwithPropositionalHedges,sorrywithApologizer,IwithSubjectiviz-ers).ThepresenceofOhalsohashighimportanceforthecharacterizationofApologizer(n=2),asillustratedinexamplessuchas\"Ohsorry,that\u2019snine.\".Wenotethattheoccurrencesof\"Ohsorry\"asastand-aloneclausewereexcludedbyourrule-basedmodelbecausetheydonotcorrespondtoanapologizer(theycannotmitigatethecontentofapropositionifthereisnopropositionassociated).Thisexampleillustratestheinterestofamachinelearningmodelapproachtodisambiguatethefunc-tionofconventionalnon-propositionalphraseslike\"Ohsorry\".Inaddition,SHAPhighlightstheimportanceofnovelfeatureswhosefunctionwasnotidentifiedinthehedgesliterature:(i)whatLIWCclassifiesasinformalwordsbutthataremostlyinterjectionslikeahandoharestronglyassociatedwithApol-ogizer,asaredisfluencies(n=12);(ii)theuseofPOStagsseemstobeveryrelevantforcharac-terizingthedifferentclasses(2-gramofPOStagfeatures3occurinthetop-rankedfeaturesofallthe\n\u220564.5(3.0)\u2205LSTM(3-classes)\nKDFeat.(KDF)Pre-TrainedEmb.(PTE)KDF+PTE\n\u220594.4(0.2)\u2205LSTM(4-classes)\n96.7(0.2)91.0(0.2)95.4(0.2)\n94.8(0.3)89.7(0.4)93.9(0.4)Attention-CNN(4-classes)\n93.9(1.4)89.1(1.4)94.1(1.2)BERT(4-classes)\n67.6\u2205\u2205MLP(3-classes)\nModels\n2166\n\n\n95.0(0.2)96.5(0.3)96.5(0.2)96.7(0.2)96.6(0.4)96.7(0.3)\n3-classes\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\n68.8(0.8)78.2(1.6)78.1(1.3)79.0(1.3)78.5(2.4)78.7(1.8)\nTable5:AveragedweightedF1-scoresforthethreeclassesofhedgesandthefourclasses,withanadditiveintegrationofKDFfeaturesintheLightGBMmodel.Thestandarddeviationiscomputedacrossfivefolds.\n4-classes\nModels\nRankApologizerSubjectivizersProp.HedgesNothedged\n1Functionwords(LIWC)\"I\"ClasslabelClasslabel2\"Oh\"(LIWC)\"Yeah\"\"Would\"\"Would\"3\"Sorry\"Noun(POS)\"Just\"\"Yeah\"4Affect(LIWC)ClasslabelFunctionword(LIWC)Noun(POS)5ClauselengthCognitiveprocess(LIWC)Netspeak(LIWC)Cognitiveprocess(LIWC)Table6:Mostimportantclause-levelfeaturesforLightGBMaccordingtotheSHAPanalysis.classes(seeFiguresintheAppendix).Itmeansthattherearesomerecurringsyntacticpatternsineachclass;(iii)Regardingtheutterancesize,aclauseshorterthanthemeanisweaklyassociatedwithdirectness(n=17)whilealongerclausesuggeststhatitcontainsaSubjectivizer(n=6).Apologizersarecharacterizedbyameanclauselength(n=5),withfewvariationsfromit;(iv)Tutoringmovesarenotstrongpredictorsofanyclasses:\"Affirma-tionfromtutor\"istheonlyfeatureappearingasapredictorofPropositionalhedges(n=20).ThisisconsistentwiththefeatureanalysisinTable5,suggestingthattutoringmovesdonotsignificantlyimprovetheperformanceoftheclassifier;(v)Non-verbalbehaviorsdonotappearasimportantfea-turesfortheclassification.Thisiscoherentwithresultsfrom(Goeletal.,2019).Notethatprosodymightplayaroleindetectinginstructionsthattrailoff,but,asdescribed,paraverbalfeatureswerenotavailable;(vi)Wouldplaysanimportantroleintheproductionofhedges,asitisstronglyassociatedtoPropositionalhedges(n=2).Itisinterestingtonotethat,whendesigningtherule-basedclassifier,wesawitdecreaseinperformancewhenwestartedtoincludewouldinourregularexpressionpatterns,probablybecausetheformishardtodisambiguateforadeterministicsystem.WhileexploringtheShapleyvaluesassociatedtoeachclause,weobservedthatfeaturesliketutoringmovesareextremelyinformativeforaverysmallnumberofclauses(thereforenotsignificantlyinflu-encingtheoverallperformanceoftheprediction),andmoreorlessnotinformativefortherest.Infer-ringtheglobalimportanceofafeatureasameanacrosstheshapleyvaluesinthedatasetmaynotbetheonlywaytoexplorethebehaviorofgradi-entboostingmethods.ItmightbemoreusefultoclusterclausesbasedontheimportancethatSHAPgivestothatfeatureinitsclassification,asthiscouldhelpdiscoversub-classesofhedgesthataredifferentiatedfromtherestbytheirinteractionwithaspecificfeature(inthewaythatsomeApologiz-ersarecharacterizedbyan\"oh\").Wealsonotethattheexplanationmodelissensitivetospuri-ouscorrelationsinthedataset,causedbythesmallrepresentationofsomeclass:forexample,\"nine\"(n=7)and\"four\"(n=20)arepositivepredictorsofApologizers.6ConclusionandfutureworkThroughourclassificationperformanceexperi-ments,weshowedthatitispossibletousema-chinelearningmethodstodiminishtheambigu-ityofhedges,andthatthehybridapproachofus-ingrule-basedlabelfeaturesderivedfromsocialscience(includinglinguistics)literaturewithinamachinelearningmodelhelpedsignificantlytoin-creasethemodel\u2019sperformance.Nonverbalbehav-iorsandtutoringmovesdidnotprovideinformationatthesentencelevel;boththeperformanceofthemodelandthefeaturecontributionanalysissug-gestedthattheirimpactonthemodeloutputwasnotstrong.ThisisconsistentwithresultsfromGoeletal.(2019).However,infutureworkwewouldliketoinvestigatethepotentialofmultimodalpat-ternswhenweareabletobettermodelsequentiality(e.g.,negativefeedbackfollowedbyasmile).Re-gardingtheSHAPanalysis,mostofthefeaturesthatareconsideredasimportantarecoherentwiththedefinitionoftheclasses(Iforsubjectivizers,sorryforapologizers,justforpropositionalhedges).However,wediscoveredthatfeatureslikeutterance\n2167\n\n\nsizecanalsoserveasindicatorsofcertainclassesofhedges.AlimitationofSHAPisthatitmakesafeatureindependenceassumption,whichpromptstheexplanatorymodeltounderestimatetheimpor-tanceofredundantfeatures(likepronounsinourwork).Inthefuturewewillexploreexplanatorymodelscapableoftakingintoaccountthecorre-lationbetweenfeaturesinthedatasetlikeSAGE(Covertetal.,2020),butsuitedforveryimbal-anceddatasets.Inthedomainofpeer-tutoring,wewouldliketobeabletofurthertestthelinkbe-tweenhedgesandrapport,andthelinkbetweenhedgesandlearninggainsinthesubjectbeingtu-tored.Asnotedabove,thiskindofstudyrequiresafine-grainedcontrolofthelanguageproducedbyoneoftheinterlocutors,whichisdifficulttoachieveinahuman-humanexperience.Wenotethatthehedgeclassifiercanbeusednotjusttoclassify,butalsotoworktowardsimprovingthegenerationofhedgesfortutoragents.Infutureworkwewillexploreusingtheclassifiertore-rankgenerationoutputs,takingadvantageoftherecur-ringsyntacticpatterns(see(ii)inSection5.3)toimprovethegenerationprocessofhedges,andre-generatingclausesthatdon\u2019tcontainoneofthesesyntacticpatterns.AcknowledgmentsManythankstomembersoftheArticuLaboatIN-RIAParisfortheirpreciousassistance.ThisworkwassupportedinpartbythetheFrenchgovern-mentundermanagementofAgenceNationaledelaRechercheaspartofthe\u201cInvestissementsd\u2019avenir\u201dprogram,referenceANR-19-P3IA-0001(PRAIRIE3IAInstitute).ReferencesHeikeAdelandHinrichSch\u00fctze.2017.Exploringdif-ferentdimensionsofattentionforuncertaintydetec-tion.InProceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputa-tionalLinguistics:Volume1,LongPapers,pages22\u201334,Valencia,Spain.AssociationforComputa-tionalLinguistics.ShoshanaBlum-Kulka.1987.Indirectnessandpolite-nessinrequests:Sameordifferent?Journalofpragmatics,11(2):131\u2013146.PenelopeBrownandStephenCLevinson.1987.Polite-ness:Someuniversalsinlanguageusage,volume4.Cambridgeuniversitypress.JudeeKBurgoonandRandallJKoper.1984.Nonverbalandrelationalcommunicationassociatedwithreti-cence.HumanCommunicationResearch,10(4):601\u2013626.IanCovert,ScottMLundberg,andSu-InLee.2020.Understandingglobalfeaturecontributionswithad-ditiveimportancemeasures.AdvancesinNeuralInformationProcessingSystems,33:17212\u201317223.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforCom-putationalLinguistics:HumanLanguageTechnolo-gies,Volume1(LongandShortPapers),pages4171\u20134186.Rich\u00e1rdFarkas,VeronikaVincze,Gy\u00f6rgyM\u00f3ra,J\u00e1nosCsirik,andGy\u00f6rgySzarvas.2010.Theconll-2010sharedtask:learningtodetecthedgesandtheirscopeinnaturallanguagetext.InProceedingsofthefour-teenthconferenceoncomputationalnaturallanguagelearning\u2013Sharedtask,pages1\u201312.BruceFraser.2010.Pragmaticcompetence:Thecaseofhedging.Newapproachestohedging,1534.ViolaGanterandMichaelStrube.2009.Findinghedgesbychasingweasels:Hedgedetectionusingwikipediatagsandshallowlinguisticfeatures.InProceedingsoftheACL-IJCNLP2009ConferenceShortPapers,pages173\u2013176.PranavGoel,YoichiMatsuyama,MichaelMadaio,andJustineCassell.2019.\u201cithinkitmighthelpifwemultiply,andnotadd\u201d:Detectingindirectnessincon-versation.In9thInternationalWorkshoponSpokenDialogueSystemTechnology,pages27\u201340.Springer.CynthiaHoward,PamelaW.Jordan,BarbaraMariaDiEugenio,andSandraKatz.2015.Shiftingtheload:apeerdialogueagentthatencouragesitshumancollab-oratortocontributemoretoproblemsolving.Interna-tionalJournalofArtificialIntelligenceinEducation,27:101\u2013129.KenHyland.1998.Hedginginscientificresearcharti-cles,volume54.JohnBenjaminsPublishing.GuolinKe,QiMeng,ThomasFinley,TaifengWang,WeiChen,WeidongMa,QiweiYe,andTie-YanLiu.2017.Lightgbm:Ahighlyefficientgradientboost-ingdecisiontree.Advancesinneuralinformationprocessingsystems,30:3146\u20133154.KlausKrippendorff.2004.Reliabilityincontentanaly-sis:Somecommonmisconceptionsandrecommen-dations.Humancommunicationresearch,30(3):411\u2013433.GeorgeLakoff.1975.Hedges:Astudyinmeaningcriteriaandthelogicoffuzzyconcepts.InContem-poraryresearchinphilosophicallogicandlinguisticsemantics,pages221\u2013271.Springer.\n2168\n\n\nMatthewLeach.2005.Rapport:Akeytotreatmentsuc-cess.Complementarytherapiesinclinicalpractice,11:262\u20135.IlyaLoshchilovandFrankHutter.2018.Decoupledweightdecayregularization.InInternationalConfer-enceonLearningRepresentations.ScottMLundberg,GabrielGErion,andSu-InLee.2018.Consistentindividualizedfeatureat-tributionfortreeensembles.arXivpreprintarXiv:1802.03888.ScottMLundbergandSu-InLee.2017.Aunifiedap-proachtointerpretingmodelpredictions.InProceed-ingsofthe31stinternationalconferenceonneuralinformationprocessingsystems,pages4768\u20134777.HowardLuneandBruceLBerg.2017.Qualitativeresearchmethodsforthesocialsciences.Pearson.MichaelMadaio,JustineCassell,andAmyOgan.2017.Theimpactofpeertutors\u2019useofindirectfeedbackandinstructions.Philadelphia,PA:InternationalSo-cietyoftheLearningSciences.BenMedlockandTedBriscoe.2007.Weaklysuper-visedlearningforhedgeclassificationinscientificliterature.InProceedingsofthe45thannualmeetingoftheassociationofcomputationallinguistics,pages992\u2013999.ChristophMolnar.2020.Interpretablemachinelearn-ing.Lulu.com.AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advancesinneuralinformationprocessingsystems,32:8026\u20138037.JamesWPennebaker,RyanLBoyd,KaylaJordan,andKateBlackburn.2015.Thedevelopmentandpsycho-metricpropertiesofliwc2015.Technicalreport.JeffreyPennington,RichardSocher,andChristopherDManning.2014.Glove:Globalvectorsforwordrep-resentation.InProceedingsofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing(EMNLP),pages1532\u20131543.EllenFPrince,JoelFrader,CharlesBosk,etal.1982.Onhedginginphysician-physiciandiscourse.Lin-guisticsandtheProfessions,8(1):83\u201397.AnnaProkofievaandJuliaHirschberg.2014.Hedgingandspeakercommitment.In5thIntl.WorkshoponEmotion,SocialSignals,Sentiment&LinkedOpenData,Reykjavik,Iceland.NilsReimersandIrynaGurevych.2019.Sentence-bert:Sentenceembeddingsusingsiamesebert-networks.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages3982\u20133992.OscarJRomero,RanZhao,andJustineCassell.2017.Cognitive-inspiredconversational-strategyreasonerforsocially-awareagents.InIJCAI,pages3807\u20133813.TimRowland.2007.\u2018wellmaybenotexactly,butit\u2019saroundfiftybasically?\u2019:Vaguelanguageinmath-ematicsclassrooms.InVaguelanguageexplored,pages79\u201396.Springer.SaschaSchneider,SteveNebel,SimonPradel,andG\u00fcn-terDanielRey.2015.Mindyourpsandqs!howpoliteinstructionsaffectlearningwithmultimedia.ComputersinHumanBehavior,51:546\u2013555.TanmaySinhaandJustineCassell.2015.Weclick,wealign,welearn:Impactofinfluenceandconvergenceprocessesonstudentlearningandrapportbuilding.InProceedingsofthe1stWorkshoponModelingINTERPERsonalSynchrONyAndInfLuence,INTER-PERSONAL\u201915,page13\u201320,NewYork,NY,USA.AssociationforComputingMachinery.HelenSpencer-Oatey.2005.(im)politeness,faceandperceptionsofrapport:Unpackagingtheirbasesandinterrelationships.1(1):95\u2013119.Gy\u00f6rgySzarvas,VeronikaVincze,Rich\u00e1rdFarkas,Gy\u00f6rgyM\u00f3ra,andIrynaGurevych.2012.Cross-genreandcross-domaindetectionofsemanticuncer-tainty.ComputationalLinguistics,38(2):335\u2013367.BuzhouTang,XiaolongWang,XuanWang,BoYuan,andShixiFan.2010.Acascademethodfordetectinghedgesandtheirscopeinnaturallanguagetext.InProceedingsoftheFourteenthConferenceonCom-putationalNaturalLanguageLearning\u2013SharedTask,pages13\u201317.MorganUlinski,SethBenjamin,andJuliaHirschberg.2018.Usinghedgedetectiontoimprovecommittedbelieftagging.InProceedingsoftheWorkshoponComputationalSemanticsbeyondEventsandRoles,pages1\u20135.TeppoVarttala.1999.Remarksonthecommunicativefunctionsofhedginginpopularscientificandspecial-istresearcharticlesonmedicine.Englishforspecificpurposes,18(2):177\u2013200.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,\u0141ukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.InAdvancesinneuralinformationpro-cessingsystems,pages5998\u20136008.ErikVelldal.2011.Predictingspeculation:asimpledis-ambiguationapproachtohedgedetectioninbiomed-icalliterature.JournalofBiomedicalSemantics,2(5):1\u201314.VeronikaVincze.2014.Uncertaintydetectioninnaturallanguagetexts.PhD,UniversityofSzeged,page141.\n2169\n\n\nNingWang,WLewisJohnson,RichardEMayer,PaolaRizzo,ErinShaw,andHeatherCollins.2008.Thepolitenesseffect:Pedagogicalagentsandlearningoutcomes.Internationaljournalofhuman-computerstudies,66(2):98\u2013112.ShawnWarner-Garcia.2014.Laughingwhennothing\u2019sfunny:Thepragmaticuseofcopinglaughterinthenegotiationofconversationaldisagreement.Prag-matics,24(1):157\u2013180.TimothyWilliamson.2002.Vagueness.Routledge.RanZhao,AlexandrosPapangelis,andJustineCassell.2014.Towardsadyadiccomputationalmodelofrap-portmanagementforhuman-virtualagentinteraction.InInternationalConferenceonIntelligentVirtualAgents,pages514\u2013527.Springer.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016a.Automaticrecognitionofconversa-tionalstrategiesintheserviceofasocially-awaredialogsystem.InProceedingsofthe17thAnnualMeetingoftheSpecialInterestGrouponDiscourseandDialogue,pages381\u2013392.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016b.Socially-awarevirtualagents:Au-tomaticallyassessingdyadicrapportfromtemporalpatternsofbehavior.InInternationalconferenceonintelligentvirtualagents,pages218\u2013233.Springer.AAdditionalinformationontheexperimentalsettingsWeusedPyTorch(Paszkeetal.,2019)toimple-menttheneuralmodels.Foreachsetoffeatures,hyperparameterswereselectedusingOptuna(Ak-iba,2019),aparametersearchframework.Were-implementedtheAttention-CNNwithGlove(Pen-ningtonetal.,2014)300-Dwordsembeddingsasthevectorrepresentation.Foreachmodels,theresultsarecross-validatedusing5folds(wechose5insteadof10toavoidhavingfoldswithtoofewsamplesperclass).Wecorrectedthelossfunctionforclassimbalancetoforcethemodeltoadaptmoretothelessfrequentclasses.Thestrengthofthiscorrectiondependedonthemodel,andwasselectedbecauseitprovidedasatisfyingcompro-misebetweenfavoringrecallandprecisionintheclassificationresultsofthatmodel.ForLightGBM,a\"squarerootofthesquarerootoftheinverseclassproportion\"correctionwasselected.Neu-ralmodelsweretrainedusingAdamWasanop-timizer(LoshchilovandHutter,2018),andusedareducedfeaturevector,obtainedwiththeap-plicationofPCA(dinit=1800;d=100;99.8%oftheinformationisconserved).Nosignifi-cantperformancedifferenceswereobservedbe-tweentheoriginalvectorandthereducedvectorfortrainingthemodels.TocomputetheSHAPvaluesmentionedinthepaper,wekeptonesplittoperformthe5-splitofthedataset,andleave1splittovalidateandearlystopthemodel,inor-dertoavoidoverfitting.Acompleteconfigura-tionofhyperparametersusedforeachmodelisre-portedintheGitHubrepositorywiththecodeofthepaper:https://github.com/YannRaphalen/Hedges-Detection.TheBERTmodelwasfine-tunedonaNvidiaQuadroRTX8000GPU.BTables\n2170\n\n\n?(whether|if|is|that|it|this)?.*Subj.\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nNoNoYesNoNoNoYesYesYesYesYesBERT(PTE)\nexceptionally|forthemostpart|inamannerofspeaking|\nNoYesNoNoNoYesNoNoYesYesNoMLP(KDF)\nsomethingbetween|essentially|only).*Prop.\nTable8:Significancetableforthe3-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesYesYesYesYesNoYesYesYesNoYesMLP(KDF+PTE)\ntechnically|typically|virtually|approximately|\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nmuch|somewhat|exactly|almost|littlebit|quite|\nClass\nrelatively|roughly|sotosay|strictlyspeaking|\nNoNoYesNoNoYesNoYesYesYesYesLSTM(KDF)\nYesNoNoNoNo+TM\nTable10:Significancetableforthe3-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoNoYesYesYesYesYesYesNoYesYes\n.*(it)(looks|seems|appears)[,]?.*\",\".*(or|and)(that|something|stuff|soforth)Table7:Regexprulesusedfortheclassifier.\nRule-based\nRule-based\n.*(you(might|may)(believe|think)).*Subj.\nYesYesYesYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nYesYesNoYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nNoNoYesNoNoYesNoNoYesYesYesLSTM(KDF)\nYesYesNoYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(i|i\u2019m|you|it\u2019s)(am|are)(apparently|surely)[,]?.*Prop.\nNoNoYesNoNoYesNoYesYesYesYesLSTM(PTE)\nNoNoYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nNoNoYesYesNoYesYesYesYesYesNoLGBM(KDF)\nNoNoYesYesYesYesYesYesYesYesNoLGBM(KDF)\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nYesNoYesYesYesYesYesYesNoYesYes\nYesYesYesYesYesYesYesYesYesYesYesMLP(KDF+PTE)\nregular|regularly|actually|almost|asitwere|basically|\nNoYesYesNoYesYesYesNoYesYesNoMLP(PTE)\nRule(regexp)\n.*(i|i\u2019m|we)?(was|am|wasn\u2019t)??(not)?(sure|certain).*Subj.\n.*(accordingto|presumably).*Subj.\n(?!.*(be|been|was)likeexcuseme)((excuseme|sorry)[w,\u2019]+|[w,\u2019]+(excuseme|sorry))Prop.\n.*(i|you|we)haveto(check|look|verify).*Subj.\n.*(i\u2019m|i|we\u2019re)(am|are)??(apologize|sorry).*Apol.\nonthetallside|parexcellence|particularly|\n(?!what).*(i|we)?(don\u2019t|didn\u2019t|did)??(not)?\ninarealsense|inasense|inaway|largely|literally|\nYesYesYesNoNoNoYesYesYesYesYesBERT(PTE)\nModels\nModels\nModels\nYesYesYesNoNoYesNoNoYesYesYesLSTM(PTE)\nYesNoNoNoNo\nYesYesYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nSubj.\nTable9:Significancetableforthe4-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoYesNoNoNoYesNoNoYesYesNoMLP(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(ifeellikeyou).*Subj.\nlooselyspeaking|kinda|moreorless|mostly|often|\n.*(unlessi).*Apol.\nprobably|canbeviewas|crypto-|especially|essentially|\n.*(ifi\u2019mnotwrong|ifi\u2019mright|ifthat\u2019strue).*Subj.\nYesNoNoNoNo+POS\n(guess|guessed|thought|think|believe|believed|suppose|supposed)\n.*(just|alittle|maybe|actually|sortof|kindof|pretty\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nNoYesYesNoYesYesNoNoYesYesYesMLP(KDF)\nprettymuch|principally|pseudo-|quintessentially|\n2171\n\n\nFigure2:Absoluteaveragedfeaturecontribution,asindicatedbySHAP.Thelongerthebarisforonecolor,themorethefeatureisassociatedwiththeclassrepresentedbythatcolor.\nFigure3:Averagedcontributionoffeaturestothedetectionofthe\"Notindirect\"class,asindicatedbySHAP.Eachdotcorrespondstoaclassifiedclause.Areddotindicatesthatthefeatureispresentintheclause,whileabluedotindicatesthatthefeatureisabsent.Thefartherontherightthedotis,themorethefeaturecontributedtoitsclassificationasahedge.\n2172\n\n\nFigure4:Averagedcontributionoffeaturestothedetectionof\"Apologizers\",asindicatedbySHAP.\nFigure5:Averagedcontributionoffeaturestothedetectionof\"Propositionalhedges\",asindicatedbySHAP.\n2173\n\n\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nYesNoNoNoNo+TM\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nFigure6:Averagedcontributionoffeaturestothedetectionof\"Subjectivizers\",asindicatedbySHAP.\nModels\nYesNoNoNoNo\nTable11:Significancetableforthe4-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesNoNoNoNo+POS\n2174"}, {"question": " What is the long-term goal of the research mentioned in the text?", "answer": " Automatically generating indirectness behaviors for a tutoring agent", "ref_chunk": "\"Youmightthinkaboutslightlyrevisingthetitle\":IdentifyingHedgesinPeer-tutoringInteractionsYannRaphalen1,Chlo\u00e9Clavel2,JustineCassell1,31InriaParis2LTCI,InstitutPolytechniquedeParis,Telecom-Paris3CarnegieMellonUniversityyann.raphalen.pro@gmail.com,justine@cs.cmu.edu,chloe.clavel@telecom-paris.frAbstractHedgesplayanimportantroleinthemanage-mentofconversationalinteraction.Inpeer-tutoring,theyarenotablyusedbytutorsindyads(pairsofinterlocutors)experiencinglowrapporttotonedowntheimpactofinstructionsandnegativefeedback.Pursuingtheobjectiveofbuildingatutoringagentthatmanagesrap-portwithstudentsinordertoimprovelearning,weusedamultimodalpeer-tutoringdatasettoconstructacomputationalframeworkforiden-tifyinghedges.Wecomparedapproachesre-lyingonpre-trainedresourceswithothersthatintegrateinsightsfromthesocialsciencelitera-ture.Ourbestperformanceinvolvedahybridapproachthatoutperformstheexistingbase-linewhilebeingeasiertointerpret.Weemployamodelexplainabilitytooltoexplorethefea-turesthatcharacterizehedgesinpeer-tutoringconversations,andweidentifysomenovelfea-tures,andthebenefitsofsuchahybridmodelapproach.1IntroductionRapport,mostsimplydefinedasthe\u201c...relativeharmonyandsmoothnessofrelationsbetweenpeo-ple...\u201d(Spencer-Oatey,2005),hasbeenshowntoplayaroleinthesuccessofactivitiesasvariedaspsychotherapy(Leach,2005)andsurveyinterview-ing(LuneandBerg,2017).Inpeer-tutoring,rap-port,asmeasuredbytheannotationofthinslicesofvideo,hasbeenshowntobebeneficialforlearningoutcomes(Zhaoetal.,2014;SinhaandCassell,2015).Thelevelofrapportrisesandfallswithconversationalstrategiesdeployedbytutorsandtuteesatappropriatetimes,andasafunctionofthecontentofpriorturns.Thesestrategiesincludeself-disclosure,referringtosharedexperience,and,onthepartoftutors,givinginstructionsinanindirectmanner.Someworkhasattemptedtoautomaticallydetectthesestrategiesintheserviceofintelligenttutors(Zhaoetal.,2016a),butonlyafewstrate-gieshavebeenattempted.Otherworkhascon-centratedona\"socialreasoningmodule\"(Romeroetal.,2017)todecidewhichstrategiesshouldbegeneratedinagivencontext,butindirectnesswasnotamongthestrategiestargeted.Inthispaper,wefocusontheautomaticclassificationofonespe-cificstrategythatisparticularlyimportantforthetutoringdomain,andthereforeimportantforintel-ligenttutors:hedging,asub-partofindirectnessthat\"softens\"whatwesay.Thisworkispartofalargerresearchprogramwiththelong-termgoalofautomaticallygeneratingindirectnessbehaviorsforatutoringagent.\nFigure1:Amockconversationdisplayingeachtypeofhedgedformulation.AccordingtoBrownandLevinson(1987),hedgesarepartofthelinguistictoolsthatinterlocu-torsusetoproducepoliteness,bylimitingthefacethreattotheinterlocutor(basicallybylimitingtheextenttowhichtheinterlocutormightexperienceembarrassmentbecauseofsomekindofpoorper-formance).Anexampleis\"that\u2019skindofawronganswer\".Hedgesarealsofoundwhenspeakerswishtoavoidlosingfacethemselves,forexam-plewhensaying(\"IthinkImighthavetoadd6.\").Madaioetal.(2017)foundthatinapeer-tutoringtask,whenrapportbetweeninterlocutorsislow,tu-teesattemptedmoreproblemsandcorrectlysolvedmoreproblemswhentheirtutorshedgedinstruc-\n2160 Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1: Long Papers, pages 2160 - 2174 May 22-27, 2022 c(cid:13)2022 Association for Computational Linguistics\n\n\ntions,whichlikewisepointstowardsa\"mitigationoffacethreat\"function.Hedgescanalsobeasso-ciatedwithanonverbalcomponent,forexampleavertedeyegazeduringcriticism(BurgoonandKoper,1984).Hedgesarenot,however,alwaysap-propriate,asin\"Ikindofthinkit\u2019srainingtoday.\"whentheinterlocutorscanbothseerain(althoughitmightbetakenashumorous).Thesefactsabouthedgesmotivateawaytoautomaticallydetectthemand,ultimately(althoughnotinthecurrentwork)alsogeneratethem.Inbothcaseswefirsthavetobeabletocharacterizethemusinginterpretablelinguisticfeatures,whichiswhatweaddressinthecurrentpaper.Thus,intheworkdescribedhere,basedonlinguisticdescriptionsofhedges(BrownandLevinson,1987;Fraser,2010),webuiltarule-basedclassifier.Weshowthatthisclassifierincombinationwithadditionalmultimodalinter-pretablecontext-dependentfeaturessignificantlyimprovestheperformanceofamachinelearningmodelforhedges,comparedtoalessinterpretabledeeplearningbaselinefromGoeletal.(2019)us-ingwordembeddings.Wealsoreliedonamachinelearningmodelexplanationtool(LundbergandLee,2017)toinvestigatethelinguisticfeaturesrelatedtohedgesinthecontextofpeer-tutoring,primarilytoseeifwecoulddiscoversurprisingfeaturesthattheclassificationmodelwouldassociatetohedgesinthiscontext,andwedescribethosebelow.Thecodeofthemodelsdescribedinthepaperisalsoprovided.12RelatedworkHedges:AccordingtoFraser(2010),hedgingisarhetoricalstrategythatattenuatesthestrengthofastatement.Onewaytoproduceahedgeisbyalteringthefullsemanticvalueofaparticu-larexpressionthroughPropositionalhedges(alsocalledApproximatorsinPrinceetal.(1982)),asin\"Youarekindofwrong,\"thatreduceprototypical-ity(i.eaccuracyofthecorrespondencebetweenthepropositionandtherealitythatthespeakerseekstodescribe).Propositionalhedgesarerelatedtofuzzylanguage(Lakoff,1975),andthereforetotheproductionofvagueness(Williamson,2002)anduncertainty(Vincze,2014).AsecondkindareRelationalHedges(alsocalledShieldsinPrinceetal.(1982)),suchas\u201cIthinkthatyouarewrong.\u201dor\u201cThedoctorwantsyoutostopsmoking.\u201d,conveyingthatthepropositionis\n1https://github.com/AnonymousHedges/HedgeDetectionconsideredbythespeakerassubjective.Inafurthersub-division,AttributionShields,asin\"Thedoc-torwantsyou...\",theinvolvementofthespeakerinthetruthvalueofthepropositionisnotmadeexplicit,whichallowsspeakersnottotakeastance.Asdescribedabove,Madaioetal.(2017)foundthattutorswhoshowedlowerrapportwiththeirtuteesusedmorehedgedinstructions(theyalsoemployedmorepositivefeedback),howeverthiswasonlythecasefortutorswithagreaterbeliefintheirabilitytotutor.Tuteesinthiscontextsolvedmoreproblemscorrectlywhentheirtutorshedgedinstructions.Noeffectofhedgingwasfoundfordyads(pairsofinterlocutors)withgreatersocialcloseness.However,theauthorsdidnotlookatthespecificlinguisticformstheseteenagersused.Rowland(2007)alsodescribestherolethathedg-ingplaysinthisagegroup,showingthatstudentsusebothrelational(\"IthinkthatJohnissmart.\")andpropositional(\"Johniskindofsmart.\")hedgesformuchthesameshieldingfunctionofdemon-stratinguncertainty,tosavethemfromtheriskofembarrassmentiftheyarewrong.TheauthorobservedthatteensusedfewAdaptors(kindof,somewhat)andpreferredtouseRounders(around,closeto).However,thisstudywasperformedwithanadultandtwochildren,possiblybiasingthere-sultsduetotheparticipationoftheadultinvestiga-tor.Hedgeshavebeenincludedinvirtualtutoringagentsbeforenow.(Howardetal.,2015)integratedhedgesinatutoragentforundergraduatesinCS,asawaytoencouragethestudenttotaketheinitiative.Hedgeshavealsobeenusedasawayofintegrat-ingBrownandLevinson\u2019spolitenessframework(Wangetal.,2008;Schneideretal.,2015)invir-tualtutoringagents.Resultswerenotbrokenoutbystrategy,butpolitenessingeneralwasshowntopositivelyinfluencemotivationandlearning,incertainconditions.Computationalmethodsforhedgedetection:Anumberofstudieshavetargetedthedetectionofhedgesanduncertaintyintext(MedlockandBriscoe,2007;GanterandStrube,2009;Tangetal.,2010;Velldal,2011;Szarvasetal.,2012),partic-ularlyfollowingtheCoNLL2010datasetrelease(Farkasetal.,2010).However,thisworkisnotasrelatedtohedgesinconversation,asitfocusesonaformalandacademiclanguageregister(Hy-land,1998;Varttala,1999).AsnotedbyProkofievaandHirschberg(2014),thefunctionsofhedgesaredomain-andgenre-dependent,thereforethisbias\n2161\n\n\ntowardsformalityimpliesthattheexistingworkmaynotadaptwelltothedetectionofhedgesinconversationbetweenteenagers.Aconsequenceisthattheexistingworkdoesnotconsidertermslike\"Ithink,\"sinceopinionsrarelyappearinanaca-demicwritingdataset.Instructionsarealsoalmostabsent(\"Ithinkyouhavetoaddtentobothsides.\"),astronglimitationforthestudyofconversationalhedgessinceitisinrequests(includingtutoringin-structions)thatindirectformulationsmostlyoccuraccordingtoBlum-Kulka(1987).ProkofievaandHirschberg(2014)alsonotethatitisdifficulttodetecthedgesbecausethewordpatternsassociatedwiththemhaveothersemanticandpragmaticfunc-tions:considering\"Ithinkthatyouhavetoaddxtobothsides.\"vs\"Ithinkthatyouareanidiot.\",itisnotclearthattheseconduseof\"Ithinkthat\"isanhedgemarker.Theyadvocateusingmachinelearningapproachestodealwiththeambiguityofthesemarkers.Workingonaconversationaldataset,Ulinskietal.(2018)builtacomputationalsystemtoassessspeakercommitment(i.e.atwhichpointthespeakerseemsconvincedbythetruthvalueofastatement),inparticularbyrelyingonarule-baseddetectionsystemforhedges.Comparedtothatwork,ourrule-basedclassificationmodelisdirectlydetectinghedgeclasses,andweemploythepredictionsoftherule-basedmodelasafeatureforstrongermachinelearningmodels,designedtolessentheimpactoftheimbalancebetweenclasses.Wealsoconsiderapologieswhentheyserveamit-igationfunction(wethencallthemApologizers),aswasdonebytheauthorsofourcorpus,andwealsousethetermsubjectivizersasdefinedbelow,tobeabletocomparedirectlywiththepreviousworkcarriedoutonthiscorpus.Asfarasweknow,onlyGoeletal.(2019)haveworkedwithapeer-tutoringdataset(thesameonethatwealsouse),andtheyachievedtheirbestclassificationresultbyemployinganAttention-CNNmodel,inspiredbyAdelandSch\u00fctze(2017).3ProblemstatementWeconsiderasetDofconversationsD=(c1,c2,...,c|D|),whereeachconversationiscom-posedofasequenceofindependentsyntacticclausesci=(u1,u2,...,uM),whereMisthenumberofclausesintheconversation.Notethattwoconsecutiveclausescanbeproducedbythesamespeaker.Eachclauseisassociatedwithauniquelabelcorrespondingtothediffer-enthedgeclassesdescribedinTable1:yi\u2208C={PropositionalHedges,Apologizers,Subjec-tivizers,Nothedged}.Finally,anutteranceuicanberepresentedasavectoroffeaturesX=(x1,x2,...,xN),whereNrepresentsthenumberoffeaturesweusedtodescribeaclause.Ourfirstgoalistodesignamodelthatcorrectlypredictsthelabelyiassociatedtoui.Itcanbeunderstoodasthefollowingresearchquestion:RQ1:\"Whichmodelsandfeaturescanbeusedtoautomaticallycharacterizehedgesinapeer-tutoringinteraction?\"Oursecondgoalistoidentify,foreachhedgeclass,thesetoffeaturesFclass={fk},k\u2208[1,N]sortedbyfeatureimportanceintheclassificationofclass.Itcorrespondstothefollowingresearchquestion:RQ2:\"Whatarethemostimportantlinguisticfeaturesthatcharacterizeourhedgeclassesinapeer-tutoringsetting?\"4Methodology4.1CorpusDatacollection:Thedialoguecorpususedherewascollectedaspartofalargerstudyontheeffectsofrapport-buildingonreciprocalpeertutoring.24Americanteenagers(meanage=13.5,min=12,max=15),halfmaleandhalffemale,cametoalabwherehalfoftheparticipantswerepairedwithasame-age,same-genderfriend,andtheotherhalfpairedwithastranger.Theparticipantswereassignedtoatotalof12dyadsinwhichthepar-ticipantsalternatedtutoringoneanotherinlinearalgebraequationsolvingfor5weeklyhour-longsessions,foratotalcorpusofnearly60hoursofface-to-faceinteractions.Eachsessionwasstruc-turedsuchthatthestudentsengagedinbriefsocialchitchatinthebeginning,thenoneofthestudentswasrandomlyassignedtotutortheotherfor20minutes.Theythenengagedinanothersocialpe-riod,andconcludedwithasecondtutoringperiodwheretheotherstudentwasassignedtheroleoftutor.Audioandvideodatawererecorded,tran-scribed,andsegmentedforclause-leveldialogueannotation,providingnearly24000clauses.Non-speechsegments(notablyfillersandlaughter)weremaintained.Becauseoftemporalmisalignmentforpartsofthecorpus,manyparaverbalphenomena,suchasprosody,wereunfortunatelynotavailabletous.SinceouraccesstothedatasetiscoveredbyaNon-DisclosureAgreement,itcannotbereleased\n2162\n\n\n2https://github.com/AnonymousHedges/HedgeDetectionclasses.4.2FeaturesLabelfromrule-basedclassifier(LabelRB):Weusetheclasslabelpredictedbytherule-basedclas-sifierdescribedinSection4.3asafeature.Ourhypothesisisthatthemachinelearningmodelcanusethisinformationtocounterbalancetheclassimbalance.Totakeintoaccountthefactthatsomerulesaremoreefficientthanothers,weweightedtheclasslabelresultingfromtherule-basedmodelbytheprecisionoftherulethatgeneratedit.Unigramandbigram:Wecountthenumberofoccurrencesofunigramsandbigramsofthecorpusineachclause.Weusedthelemmaofthewordsforunigramsandbigramsusingthenltklemmatizer(Loper,2002)andselectedunigramsandbigramsthatoccurredinthetrainingdatasetatleastfiftytimes.Thegoalwastoinvestigate,withabottom-upapproach,towhatextenttheuseofcertainwordscharacterizeshedgeclassesintutoring.InSection5weexaminetheoverlapbetweenthesewordsandthoseaprioriidentifiedbytherules.Part-of-speech(POS):Hedgeclassesseemtobeassociatedwithdifferentsyntacticpatterns:forex-ample,subjectivizersmostoftencontainapersonalpronounfollowedbyaverb,asin\"Iguess\",\"Ibelieve\",\"Ithink\".WethereforeconsideredthenumberofoccurrencesofPOS-Tagn-grams(n=1,2,3)asfeatures.WeusedthespaCyPOS-taggerandconsideredPOSunigrams,bigramsandtri-gramsthatoccuratleast10timesinthetrainingdataset.LIWC:LinguisticInquiryandWordCount(LIWC)(Pennebakeretal.,2015)isstandardsoft-wareforextractingthecountofwordsbelongingtospecificpsycho-socialcategories(e.g.,emotions,religion).Ithasbeensuccessfullyusedinthede-tectionofconversationalstrategies(Zhaoetal.,2016a).Wethereforecountthenumberofoccur-rencesofallthe73categoriesfromLIWC.Tutoringmoves(TM):Intelligenttutoringsys-temsrelyonspecifictutoringmovestosuccess-fullyconveycontent(asdohumantutors).Wethereforelookedatthelinkbetweenthetutoringmoves,asannotatedinMadaioetal.(2017),andhedges.Fortutors,thesemovesare(1)instruc-tionaldirectivesandsuggestions,(2)feedback,and(3)affirmations,mostlyexplicitreflectionsontheirpartners\u2019comprehension,whilefortutees,theyare(1)questions,(2)feedbacks,and(3)affirmations,\npublicly.Howevertheoriginalexperimenters\u2019In-stitutionalReviewBoard(IRB)approvalallowsustoview,annotate,andusethedatatotrainmodels.Thisalsoallowsustoprovidealinktoapixe-latedvideoexampleintheGitHubrepositoryoftheproject2.Dataannotation:Thedatasetwaspreviouslyan-notatedbyMadaioetal.(2017),followingananno-tationmanualthatusedhedgeclassesderivedfromRowland(2007)(seeTable1).Onlythetaskperi-odsoftheinteractionswereannotated.Comparingtheannotationswiththeclassesmentionedintherelatedworksection,SubjectivizerscorrespondtoRelationalhedges(Fraser,2010),PropositionalhedgesandExtenderscorrespondtoApproxima-tors(Princeetal.,1982)withtheadditionofsomediscoursemarkerssuchasjust.ApologizersarementionedaslinguistictoolsrelatedtonegativepolitenessinBrownandLevinson(1987).Krippen-dorff\u2019salphaobtainedforthiscorpusannotatedbyfourcoderswasover0.7forallclasses(denotinganacceptableinter-coderreliabilityaccordingtoKrippendorff(2004)).Thedatasetiswidelyim-balanced,withmorethan90%oftheutterancesbelongingtotheNothedgedclass.Inreviewingthecorpusandtheannotationman-ual,however,wenoticedtwoissues.First,theannotationoftheExtendersclasswasinconsis-tent,leadingtotheExtendersandPropositionalhedgesclassescarryingsimilarsemanticfunctions.WethereforemergedthetwoclassesandgroupedutteranceslabeledasExtendersandthoselabeledasPropositionalhedgesundertheheadingofPropositionalhedges.Second,theannotationofclausescontainingthetokens\"just\"and\"would\"(twotermsoccurringfrequentlyinthedatasetthatarekeycomponentsofPropositionalHedgesandSubjectivizersbutthatarenotinfacthedgesinallcases)wasalsoinconsistent,leadingtovirtuallyallclauseswiththosetwotokensbeingconsideredhedges.Wethereforere-consideredalltheclausesassociatedwithanyofthehedgeclasses,aswellasalltheclausesinthe\"Nothedged\"classthatcontained\"just\"or\"would\".There-annotationwascarriedoutbytwoannotatorswhoachievedaKrippendorff\u2019salphainter-raterreliabilityof.9orbetterforApologizers,Subjectivizers,andPropo-sitionalhedgesbeforeindependentlyre-annotatingtherelevantclauses.Anexampleofare-annotationwasremoving\"Iwouldkillyou!\"fromthehedge\n2163\n\n\nProp.hedgesApologizersSubjectivizersNothedgedTotal\nTable1:Definitionoftheclasses\nFeaturesnameAutomaticextractionVectorsize\nSubjectivizersWordsthatreduceintensityorcertainty\u201cSothenIwoulddividebytwo.\u201dApologizersApologiesusedtosoftendirectspeechacts\u201cOhsorrysixb.\u201dPropositionalhedgesQualifyingwordstoreduceintensityorcertaintyofutterances\u201cIt\u2019sactuallyeight.\u201dExtendersWordsusedtoindicateuncertaintybyreferringtovaguecategories\u201cIt\u2019llbethenumberxorwhatevervariableyouhave.\u201d\n12101286262119223156\nTable2:Distributionoftheclasses\nClassDefinitionExample\nRule-basedlabelYes4UnigramYes~250BigramYes~250POSYes~1200LIWCYes73NonverbalNo24TutoringmovesNo6Total~1800\nTable3:Listofautomaticallyextractedandmanuallyannotatedfeatureswiththeirsize.mostlytentativeanswers.Nonverbalandparaverbalbehaviors:AsinGoeletal.(2019),weincludedthenonverbalandpar-averbalbehaviorsthatarerelatedtohedges.Specif-ically,weconsiderlaughterandsmiles,thathavebeenshowntobeeffectivemethodsofmitiga-tion(Warner-Garcia,2014),cut-offsindicatingself-repairs,fillerslike\"Um\",gazeshifts(annotatedas\u2019GazeatPartner\u2019,\u2019GazeattheMathWorksheet\u2019,and\u2019Gazeelsewhere\u2019),andheadnods.Eachfea-turewaspresenttwiceinthefeaturevector,onetimeforeachinterlocutor.Inter-raterreliabilityfornonverbalbehaviorwas0.89(asmeasuredbyKrippendorff\u2019salpha)foreyegaze,0.75forsmilecount,0.64forsmiledurationand0.99forheadnod.Laughterisalsoreportedinthetranscriptatthewordlevel.Weseparatethetutor\u2019sbehaviorsfromthoseofthetutee.ThecollectionprocessforthesebehaviorsisdetailedfurtherinZhaoetal.(2016b).Theclause-levelfeaturevectorwasnormalizedbythelengthoftheclause(exceptfortherule-basedlabel).Thislengthwasalsoaddedasafeature.Table3presentsanoverviewofthefinalfeaturevector.4.3ClassificationmodelsTheclassificationmodelsusedarepresentedhereaccordingtotheirlevelofintegrationofexternallinguisticknowledge.Rule-basedmodel:OnthebasisoftheannotationmanualusedtoconstructthedatasetfromMadaioetal.(2017),andwithdescriptionsofhedgesfromRowland(2007),Fraser(2010)andBrownandLevinson(1987),weconstructedarule-basedclas-sifierthatmatchesregularexpressionsindicativeofhedges.TherulesaredetailedinTable7intheAppendix.LGBM:Sincehedgesareoftencharacterizedbyexplicitlexicalmarkers,wetestedtheassumptionthatamachinelearningmodelwithaknowledge-drivenrepresentationforclausescouldcompetewithaBERTmodelinperformance,whilebeingmuchmoreinterpretable.WereliedonLightGBM,anensembleofdecisiontreestrainedwithgradi-entboosting(Keetal.,2017).Thismodelwasselectedbecauseofitsperformancewithsmalltrainingdatasetsandbecauseitcanignoreunin-formativefeatures,butalsoforitstrainingspeedcomparedtoalternativeimplementationsofgradi-entboostingmethods.Multi-layerperceptron(MLP):Asasimplebase-line,webuiltamulti-layerperceptronusingthreesetsoffeatures:apre-trainedcontextualrepre-sentationoftheclause(SentBERT;ReimersandGurevych(2019));theconcatenationofthiscon-textualrepresentationoftheclauseandarule-basedlabel(notrelyingonthepreviousclauses);andfinallytheconcatenationofallthefeaturesmen-tionedinsection4.2,withoutthecontextualizedrepresentation.LSTMoverasequenceofclauses:Sinceweareworkingwithconversationaldata,wealsowantedtotestwhethertakingintoac-countthepreviousclauseshelpstodetectthetypeofhedgeclassinthenextclause.Formally,wewanttoinferyiusingyi=maxy\u2208ClassesP(y|X(ui),X(ui\u22121),...,X(ui\u2212K)),whereKisthenumberofpreviousclausesthatthemodelwilltakeintoaccount.The\n2164\n\n\nMLPmodelpresentedaboveinfersyiusingyi=maxy\u2208ClassesP(y|X(ui)),thereforeadifferenceofperformancebetweenthetwomodelswouldbeasignthatusinginformationfromthepreviousclausescouldhelptodetectthehedgedformulationinthecurrentclause.WetestedaLSTMmodelwiththesamerepresentationsforclausesasfortheMLPmodel.CNNwithattention:Goeletal.(2019)estab-lishedtheirbestperformanceonhedgedetec-tionusingaCNNmodelwithadditiveattentionoverword(andnotclause)embeddings.Con-trarytotheMLPandLSTMmodelsmentionedabove,thismodeltriestoinferyiusingyi=maxy\u2208ClassesP(y|g(w0),g(w1),...,g(wL)),withLrepresentingthemaximumclauselengthweal-low,andgrepresentingafunctionthatturnsthewordwj,j\u2208[0,L]intoavectorrepresentation(formoredetails,pleaseseeAdelandSch\u00fctze(2017)).BERT:Tobenefitfromdeepsemanticandcon-textualrepresentationsoftheutterances,wealsofine-tunedBERT(Devlinetal.,2019)onourclas-sificationtask.BERTisapre-trainedTransformersencoder(Vaswanietal.,2017)thathassignificantlyimprovedthestateoftheartonanumberofNLPtasks,includingsentimentanalysis.Itproducesacontextualrepresentationofeachwordinasen-tence,makingitcapableofdisambiguatingthemeaningofwordslike\"think\"or\"just\"thatarerepresentativeofcertainclassesofhedges.BERT,however,isnotablyhardtointerpret.4.4AnalysistoolsLookingatwhichfeaturesimprovetheperfor-manceofourclassificationmodelstellsuswhetherthesefeaturesareinformativeornot,butdoesnotexplainhowthesefeaturesareusedbythemod-elstomakeagivenprediction.Wethereforepro-ducedacomplementaryanalysisusinganinter-pretabilitytool.Asdemonstratedby(LundbergandLee,2017),LightGBMinternalfeatureimpor-tancescoresareinconsistentwithboththemodelbehaviorandhumanintuition,soweinsteadusedamodel-agnostictool.SHAP(LundbergandLee,2017)assignstoeachfeatureanimportancevalue(calledShapleyvalues)foraparticularpredictiondependingontheextentofitscontribution(ade-tailedintroductiontoShapleyvaluesandSHAPcanbefoundinMolnar(2020)).SHAPisamodel-agnosticframework,thereforethevaluesassoci-atedwithasetoffeaturescanbecomparedacrossmodels.ItshouldbenotedthatSHAPproducesexplanationsonacase-by-casebasis,thereforeitcanbothprovidelocalandglobalexplanations.FortheGradientBoostingmodel,weuseanadaptedversionofSHAP(Lundbergetal.,2018),calledTreeSHAP.5Experimentsandresults5.1ExperimentalsettingTodetectthebestsetoffeatures,weusedLight-GBMandproceededincrementally,byaddingthegroupoffeatureswethoughttobemostlikelyasso-ciatedwithhedges.Wedidnotconsidertheriskofrelyingonasub-optimalsetoffeaturesthroughthisprocedurebecauseofthestrongabilityofLight-GBMtoignoreuninformativefeatures.Weusethisincrementalapproachasawaytotestourintuitionabouttheperformativityofgroupsoffeatures(i.e.doesaddingafeatureimprovetheperformanceofthemodel)withregardtothetaskofclassifica-tion.Tocompareourmodels,wetrainedthemonthe4-classtask,andlookedattheaverageoftheweightedF1-scoresforthethreehedgeclasses(i.e.howwellthemodelsinferminorityclasses)thatwereporthereas\"3-classes\",andattheaverageoftheweightedF1-scoresforthe4classes,thatwereportas\"4-classes\".DetailsofthehyperparametersandexperimentalsettingsareprovidedinAppendixA.5.2ModelcomparisonandfeatureanalysisOverallresults:Table4presentstheresultsob-tainedbythe6modelspresentedinSection4.3forthemulti-classproblem.Bestperformance(F1-scoreof79.0)isobtainedwithLightGBMlever-agingalmostallthefeatures.Intheappendix(seeTable8andTable9)weindicatetheconfidenceintervalstorepresentthesignificanceofthediffer-encesbetweenthemodels.First,andperhapssurprisingly,wenoticethattheuseof\"Knowledge-Driven\"featuresbasedonrulesbuiltfromlinguisticknowledgeofhedgesintheLightGBMmodeloutperformstheuseofpre-trainedembeddingswithinafine-tunedBERTmodel(79.0vs.70.6),andintheneuralbaselinefrom(Goeletal.,2019)(79.0vs64.5).ThelowscoresobtainedbytheLGBM,LSTMandMLPmodelswithpre-trainedsentenceem-beddingsversusKnowledge-Drivenfeaturesmightsignalthatthewordpatternscharacterizinghedgesarenotsalientintheserepresentations(i.e.the\n2165\n\n\n68.5(1.6)35.8(3.1)64.8(1.1)Attention-CNN(3-classes)\n65.1(5.7)39.8(8.0)65.2(5.1)BERT(3-classes)\n\u220570.6(2.3)\u2205LGBM(3-classes)\n79.0(1.3)35.0(2.2)70.1(1.4)\nRule-based(3-classes)\n94.7\u2205\u2205MLP(4-classes)\n3Notethatthereisstrongredundancybetweensomefea-turesofLIWCandthespaCyPOStaggerthatbothproducea\"Pronoun\"category,usingalexiconinthefirstcase,andaneuralinferenceinthesecond.\n\u220594.9(0.4)\u2205LGBM(4-classes)\nRule-based(4-classes)\nTable4:AveragedweightedF1-scores(andstandarddeviation)forthethreeminorityclassesandforthe4classes,forallmodels.\"KD\"standsfor\"Knowledge-Driven\",meaningthatthefeaturesarederivedfromlexicon,n-grammodelsandannotations.distancebetween\"Ithinkyoushouldadd5.\"and\"Youshouldadd5.\"isshort.).KDFeaturesseemtoprovideabetterseparabilityoftheclasses.ThecombinationofKDfeaturesandPre-trainedem-beddingsdoesnotsignificantlyimprovetheperfor-manceofthemodelscomparedtotheKDFeaturesonly,whichsuggeststhattheinformationfromthePre-trainedembeddingsisredundantwiththeonefromtheKDFeatures.Thisresultmaybeduetothehighdimensionalityoftheinputvector(868withPCAontheKDFeatures;2500otherwise).Asecondfindingisthattheuseofgradientboost-ingmodelsontopofrule-basedclassifiersbettermodelsthehedgeclasses.Theothermachinelearn-ingmodelsdidnotprovetobeaseffective,exceptforBERT.FeatureanalysisusingLightGBM:Usingthebestperformingmodel,Table5showstheroleofeachfeaturesetinthepredictiontask.ThesignificanceofthedifferencesisshowninTable10andTable11.Comparedtotherule-basedmodel,theintroductionofn-gramssignificantlyimprovedtheperformanceofourclassifier,suggestingthatsomelexicalandsyntacticinformationdescribingthehedgeclasseswasnotpresentintherule-basedmodel.LookingatTable5,wedonotobservesignificantdifferencesbetweentheLGBMmodelusingonlythelabelrulebased+(1-gramsand2-grams)andthemodelsin-corporatingmorefeatures.Tooursurprise,neitherthetutoringmovesnorthenonverbalfeaturessig-nificantlyimprovedtheperformanceofthemodel.The2featureswereincludedtoindexthespecificpeertutoringcontextofthesehedges,sothisindi-catesthatinfutureworkwemightwishtoapplythecurrentmodeltoanothercontextofusetoseeifthismodelofhedgesismoregenerallyapplicablethanweoriginallythought.Bycombiningthisresultwiththeincreasedperformanceofthemodelus-ingKnowledge-Driven(i.e.explicit)featurescom-paredtopre-trainedembeddings,itwouldseemthathedgesareaboveallalexicalphenomenon(i.e.producedbyspecificlexicalelements).5.3In-depthanalysisoftheinformativefeaturesWetrainedtheSHAPexplanationmodelsonLight-GBMwithallfeatures.Themostinformativefea-tures(inabsolutevalue)foreachclassareshowninTable6,andtheplotsbyclassarepresentedintheAppendix.Themostimportantfeaturesseemtobetherule-basedlabels,whichappearinatleastthefourthpositionforthreeclasses(seeTable6),andinthefirstpositionforPropositionalHedgesandNothedgedclasses.Surprisingly,theRule-Basedlabeldoesnotappearinthetop20featuresforApologizers.However,giventhattheclassrarelyappearsinthedata,therulesseldomactivate,sothefeaturemaysimplybeinformativeforaverysmallnumberofclauses.Unigrams(Oh,Sorry,just,Would,andI)arealsopresentinthe5top-rankedfeatures.Thisconfirmsthefindingsmen-tionedinrelatedworkforthecharacterizationofthedifferenthedgeclasses(justwithPropositionalHedges,sorrywithApologizer,IwithSubjectiviz-ers).ThepresenceofOhalsohashighimportanceforthecharacterizationofApologizer(n=2),asillustratedinexamplessuchas\"Ohsorry,that\u2019snine.\".Wenotethattheoccurrencesof\"Ohsorry\"asastand-aloneclausewereexcludedbyourrule-basedmodelbecausetheydonotcorrespondtoanapologizer(theycannotmitigatethecontentofapropositionifthereisnopropositionassociated).Thisexampleillustratestheinterestofamachinelearningmodelapproachtodisambiguatethefunc-tionofconventionalnon-propositionalphraseslike\"Ohsorry\".Inaddition,SHAPhighlightstheimportanceofnovelfeatureswhosefunctionwasnotidentifiedinthehedgesliterature:(i)whatLIWCclassifiesasinformalwordsbutthataremostlyinterjectionslikeahandoharestronglyassociatedwithApol-ogizer,asaredisfluencies(n=12);(ii)theuseofPOStagsseemstobeveryrelevantforcharac-terizingthedifferentclasses(2-gramofPOStagfeatures3occurinthetop-rankedfeaturesofallthe\n\u220564.5(3.0)\u2205LSTM(3-classes)\nKDFeat.(KDF)Pre-TrainedEmb.(PTE)KDF+PTE\n\u220594.4(0.2)\u2205LSTM(4-classes)\n96.7(0.2)91.0(0.2)95.4(0.2)\n94.8(0.3)89.7(0.4)93.9(0.4)Attention-CNN(4-classes)\n93.9(1.4)89.1(1.4)94.1(1.2)BERT(4-classes)\n67.6\u2205\u2205MLP(3-classes)\nModels\n2166\n\n\n95.0(0.2)96.5(0.3)96.5(0.2)96.7(0.2)96.6(0.4)96.7(0.3)\n3-classes\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\n68.8(0.8)78.2(1.6)78.1(1.3)79.0(1.3)78.5(2.4)78.7(1.8)\nTable5:AveragedweightedF1-scoresforthethreeclassesofhedgesandthefourclasses,withanadditiveintegrationofKDFfeaturesintheLightGBMmodel.Thestandarddeviationiscomputedacrossfivefolds.\n4-classes\nModels\nRankApologizerSubjectivizersProp.HedgesNothedged\n1Functionwords(LIWC)\"I\"ClasslabelClasslabel2\"Oh\"(LIWC)\"Yeah\"\"Would\"\"Would\"3\"Sorry\"Noun(POS)\"Just\"\"Yeah\"4Affect(LIWC)ClasslabelFunctionword(LIWC)Noun(POS)5ClauselengthCognitiveprocess(LIWC)Netspeak(LIWC)Cognitiveprocess(LIWC)Table6:Mostimportantclause-levelfeaturesforLightGBMaccordingtotheSHAPanalysis.classes(seeFiguresintheAppendix).Itmeansthattherearesomerecurringsyntacticpatternsineachclass;(iii)Regardingtheutterancesize,aclauseshorterthanthemeanisweaklyassociatedwithdirectness(n=17)whilealongerclausesuggeststhatitcontainsaSubjectivizer(n=6).Apologizersarecharacterizedbyameanclauselength(n=5),withfewvariationsfromit;(iv)Tutoringmovesarenotstrongpredictorsofanyclasses:\"Affirma-tionfromtutor\"istheonlyfeatureappearingasapredictorofPropositionalhedges(n=20).ThisisconsistentwiththefeatureanalysisinTable5,suggestingthattutoringmovesdonotsignificantlyimprovetheperformanceoftheclassifier;(v)Non-verbalbehaviorsdonotappearasimportantfea-turesfortheclassification.Thisiscoherentwithresultsfrom(Goeletal.,2019).Notethatprosodymightplayaroleindetectinginstructionsthattrailoff,but,asdescribed,paraverbalfeatureswerenotavailable;(vi)Wouldplaysanimportantroleintheproductionofhedges,asitisstronglyassociatedtoPropositionalhedges(n=2).Itisinterestingtonotethat,whendesigningtherule-basedclassifier,wesawitdecreaseinperformancewhenwestartedtoincludewouldinourregularexpressionpatterns,probablybecausetheformishardtodisambiguateforadeterministicsystem.WhileexploringtheShapleyvaluesassociatedtoeachclause,weobservedthatfeaturesliketutoringmovesareextremelyinformativeforaverysmallnumberofclauses(thereforenotsignificantlyinflu-encingtheoverallperformanceoftheprediction),andmoreorlessnotinformativefortherest.Infer-ringtheglobalimportanceofafeatureasameanacrosstheshapleyvaluesinthedatasetmaynotbetheonlywaytoexplorethebehaviorofgradi-entboostingmethods.ItmightbemoreusefultoclusterclausesbasedontheimportancethatSHAPgivestothatfeatureinitsclassification,asthiscouldhelpdiscoversub-classesofhedgesthataredifferentiatedfromtherestbytheirinteractionwithaspecificfeature(inthewaythatsomeApologiz-ersarecharacterizedbyan\"oh\").Wealsonotethattheexplanationmodelissensitivetospuri-ouscorrelationsinthedataset,causedbythesmallrepresentationofsomeclass:forexample,\"nine\"(n=7)and\"four\"(n=20)arepositivepredictorsofApologizers.6ConclusionandfutureworkThroughourclassificationperformanceexperi-ments,weshowedthatitispossibletousema-chinelearningmethodstodiminishtheambigu-ityofhedges,andthatthehybridapproachofus-ingrule-basedlabelfeaturesderivedfromsocialscience(includinglinguistics)literaturewithinamachinelearningmodelhelpedsignificantlytoin-creasethemodel\u2019sperformance.Nonverbalbehav-iorsandtutoringmovesdidnotprovideinformationatthesentencelevel;boththeperformanceofthemodelandthefeaturecontributionanalysissug-gestedthattheirimpactonthemodeloutputwasnotstrong.ThisisconsistentwithresultsfromGoeletal.(2019).However,infutureworkwewouldliketoinvestigatethepotentialofmultimodalpat-ternswhenweareabletobettermodelsequentiality(e.g.,negativefeedbackfollowedbyasmile).Re-gardingtheSHAPanalysis,mostofthefeaturesthatareconsideredasimportantarecoherentwiththedefinitionoftheclasses(Iforsubjectivizers,sorryforapologizers,justforpropositionalhedges).However,wediscoveredthatfeatureslikeutterance\n2167\n\n\nsizecanalsoserveasindicatorsofcertainclassesofhedges.AlimitationofSHAPisthatitmakesafeatureindependenceassumption,whichpromptstheexplanatorymodeltounderestimatetheimpor-tanceofredundantfeatures(likepronounsinourwork).Inthefuturewewillexploreexplanatorymodelscapableoftakingintoaccountthecorre-lationbetweenfeaturesinthedatasetlikeSAGE(Covertetal.,2020),butsuitedforveryimbal-anceddatasets.Inthedomainofpeer-tutoring,wewouldliketobeabletofurthertestthelinkbe-tweenhedgesandrapport,andthelinkbetweenhedgesandlearninggainsinthesubjectbeingtu-tored.Asnotedabove,thiskindofstudyrequiresafine-grainedcontrolofthelanguageproducedbyoneoftheinterlocutors,whichisdifficulttoachieveinahuman-humanexperience.Wenotethatthehedgeclassifiercanbeusednotjusttoclassify,butalsotoworktowardsimprovingthegenerationofhedgesfortutoragents.Infutureworkwewillexploreusingtheclassifiertore-rankgenerationoutputs,takingadvantageoftherecur-ringsyntacticpatterns(see(ii)inSection5.3)toimprovethegenerationprocessofhedges,andre-generatingclausesthatdon\u2019tcontainoneofthesesyntacticpatterns.AcknowledgmentsManythankstomembersoftheArticuLaboatIN-RIAParisfortheirpreciousassistance.ThisworkwassupportedinpartbythetheFrenchgovern-mentundermanagementofAgenceNationaledelaRechercheaspartofthe\u201cInvestissementsd\u2019avenir\u201dprogram,referenceANR-19-P3IA-0001(PRAIRIE3IAInstitute).ReferencesHeikeAdelandHinrichSch\u00fctze.2017.Exploringdif-ferentdimensionsofattentionforuncertaintydetec-tion.InProceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputa-tionalLinguistics:Volume1,LongPapers,pages22\u201334,Valencia,Spain.AssociationforComputa-tionalLinguistics.ShoshanaBlum-Kulka.1987.Indirectnessandpolite-nessinrequests:Sameordifferent?Journalofpragmatics,11(2):131\u2013146.PenelopeBrownandStephenCLevinson.1987.Polite-ness:Someuniversalsinlanguageusage,volume4.Cambridgeuniversitypress.JudeeKBurgoonandRandallJKoper.1984.Nonverbalandrelationalcommunicationassociatedwithreti-cence.HumanCommunicationResearch,10(4):601\u2013626.IanCovert,ScottMLundberg,andSu-InLee.2020.Understandingglobalfeaturecontributionswithad-ditiveimportancemeasures.AdvancesinNeuralInformationProcessingSystems,33:17212\u201317223.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforCom-putationalLinguistics:HumanLanguageTechnolo-gies,Volume1(LongandShortPapers),pages4171\u20134186.Rich\u00e1rdFarkas,VeronikaVincze,Gy\u00f6rgyM\u00f3ra,J\u00e1nosCsirik,andGy\u00f6rgySzarvas.2010.Theconll-2010sharedtask:learningtodetecthedgesandtheirscopeinnaturallanguagetext.InProceedingsofthefour-teenthconferenceoncomputationalnaturallanguagelearning\u2013Sharedtask,pages1\u201312.BruceFraser.2010.Pragmaticcompetence:Thecaseofhedging.Newapproachestohedging,1534.ViolaGanterandMichaelStrube.2009.Findinghedgesbychasingweasels:Hedgedetectionusingwikipediatagsandshallowlinguisticfeatures.InProceedingsoftheACL-IJCNLP2009ConferenceShortPapers,pages173\u2013176.PranavGoel,YoichiMatsuyama,MichaelMadaio,andJustineCassell.2019.\u201cithinkitmighthelpifwemultiply,andnotadd\u201d:Detectingindirectnessincon-versation.In9thInternationalWorkshoponSpokenDialogueSystemTechnology,pages27\u201340.Springer.CynthiaHoward,PamelaW.Jordan,BarbaraMariaDiEugenio,andSandraKatz.2015.Shiftingtheload:apeerdialogueagentthatencouragesitshumancollab-oratortocontributemoretoproblemsolving.Interna-tionalJournalofArtificialIntelligenceinEducation,27:101\u2013129.KenHyland.1998.Hedginginscientificresearcharti-cles,volume54.JohnBenjaminsPublishing.GuolinKe,QiMeng,ThomasFinley,TaifengWang,WeiChen,WeidongMa,QiweiYe,andTie-YanLiu.2017.Lightgbm:Ahighlyefficientgradientboost-ingdecisiontree.Advancesinneuralinformationprocessingsystems,30:3146\u20133154.KlausKrippendorff.2004.Reliabilityincontentanaly-sis:Somecommonmisconceptionsandrecommen-dations.Humancommunicationresearch,30(3):411\u2013433.GeorgeLakoff.1975.Hedges:Astudyinmeaningcriteriaandthelogicoffuzzyconcepts.InContem-poraryresearchinphilosophicallogicandlinguisticsemantics,pages221\u2013271.Springer.\n2168\n\n\nMatthewLeach.2005.Rapport:Akeytotreatmentsuc-cess.Complementarytherapiesinclinicalpractice,11:262\u20135.IlyaLoshchilovandFrankHutter.2018.Decoupledweightdecayregularization.InInternationalConfer-enceonLearningRepresentations.ScottMLundberg,GabrielGErion,andSu-InLee.2018.Consistentindividualizedfeatureat-tributionfortreeensembles.arXivpreprintarXiv:1802.03888.ScottMLundbergandSu-InLee.2017.Aunifiedap-proachtointerpretingmodelpredictions.InProceed-ingsofthe31stinternationalconferenceonneuralinformationprocessingsystems,pages4768\u20134777.HowardLuneandBruceLBerg.2017.Qualitativeresearchmethodsforthesocialsciences.Pearson.MichaelMadaio,JustineCassell,andAmyOgan.2017.Theimpactofpeertutors\u2019useofindirectfeedbackandinstructions.Philadelphia,PA:InternationalSo-cietyoftheLearningSciences.BenMedlockandTedBriscoe.2007.Weaklysuper-visedlearningforhedgeclassificationinscientificliterature.InProceedingsofthe45thannualmeetingoftheassociationofcomputationallinguistics,pages992\u2013999.ChristophMolnar.2020.Interpretablemachinelearn-ing.Lulu.com.AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advancesinneuralinformationprocessingsystems,32:8026\u20138037.JamesWPennebaker,RyanLBoyd,KaylaJordan,andKateBlackburn.2015.Thedevelopmentandpsycho-metricpropertiesofliwc2015.Technicalreport.JeffreyPennington,RichardSocher,andChristopherDManning.2014.Glove:Globalvectorsforwordrep-resentation.InProceedingsofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing(EMNLP),pages1532\u20131543.EllenFPrince,JoelFrader,CharlesBosk,etal.1982.Onhedginginphysician-physiciandiscourse.Lin-guisticsandtheProfessions,8(1):83\u201397.AnnaProkofievaandJuliaHirschberg.2014.Hedgingandspeakercommitment.In5thIntl.WorkshoponEmotion,SocialSignals,Sentiment&LinkedOpenData,Reykjavik,Iceland.NilsReimersandIrynaGurevych.2019.Sentence-bert:Sentenceembeddingsusingsiamesebert-networks.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages3982\u20133992.OscarJRomero,RanZhao,andJustineCassell.2017.Cognitive-inspiredconversational-strategyreasonerforsocially-awareagents.InIJCAI,pages3807\u20133813.TimRowland.2007.\u2018wellmaybenotexactly,butit\u2019saroundfiftybasically?\u2019:Vaguelanguageinmath-ematicsclassrooms.InVaguelanguageexplored,pages79\u201396.Springer.SaschaSchneider,SteveNebel,SimonPradel,andG\u00fcn-terDanielRey.2015.Mindyourpsandqs!howpoliteinstructionsaffectlearningwithmultimedia.ComputersinHumanBehavior,51:546\u2013555.TanmaySinhaandJustineCassell.2015.Weclick,wealign,welearn:Impactofinfluenceandconvergenceprocessesonstudentlearningandrapportbuilding.InProceedingsofthe1stWorkshoponModelingINTERPERsonalSynchrONyAndInfLuence,INTER-PERSONAL\u201915,page13\u201320,NewYork,NY,USA.AssociationforComputingMachinery.HelenSpencer-Oatey.2005.(im)politeness,faceandperceptionsofrapport:Unpackagingtheirbasesandinterrelationships.1(1):95\u2013119.Gy\u00f6rgySzarvas,VeronikaVincze,Rich\u00e1rdFarkas,Gy\u00f6rgyM\u00f3ra,andIrynaGurevych.2012.Cross-genreandcross-domaindetectionofsemanticuncer-tainty.ComputationalLinguistics,38(2):335\u2013367.BuzhouTang,XiaolongWang,XuanWang,BoYuan,andShixiFan.2010.Acascademethodfordetectinghedgesandtheirscopeinnaturallanguagetext.InProceedingsoftheFourteenthConferenceonCom-putationalNaturalLanguageLearning\u2013SharedTask,pages13\u201317.MorganUlinski,SethBenjamin,andJuliaHirschberg.2018.Usinghedgedetectiontoimprovecommittedbelieftagging.InProceedingsoftheWorkshoponComputationalSemanticsbeyondEventsandRoles,pages1\u20135.TeppoVarttala.1999.Remarksonthecommunicativefunctionsofhedginginpopularscientificandspecial-istresearcharticlesonmedicine.Englishforspecificpurposes,18(2):177\u2013200.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,\u0141ukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.InAdvancesinneuralinformationpro-cessingsystems,pages5998\u20136008.ErikVelldal.2011.Predictingspeculation:asimpledis-ambiguationapproachtohedgedetectioninbiomed-icalliterature.JournalofBiomedicalSemantics,2(5):1\u201314.VeronikaVincze.2014.Uncertaintydetectioninnaturallanguagetexts.PhD,UniversityofSzeged,page141.\n2169\n\n\nNingWang,WLewisJohnson,RichardEMayer,PaolaRizzo,ErinShaw,andHeatherCollins.2008.Thepolitenesseffect:Pedagogicalagentsandlearningoutcomes.Internationaljournalofhuman-computerstudies,66(2):98\u2013112.ShawnWarner-Garcia.2014.Laughingwhennothing\u2019sfunny:Thepragmaticuseofcopinglaughterinthenegotiationofconversationaldisagreement.Prag-matics,24(1):157\u2013180.TimothyWilliamson.2002.Vagueness.Routledge.RanZhao,AlexandrosPapangelis,andJustineCassell.2014.Towardsadyadiccomputationalmodelofrap-portmanagementforhuman-virtualagentinteraction.InInternationalConferenceonIntelligentVirtualAgents,pages514\u2013527.Springer.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016a.Automaticrecognitionofconversa-tionalstrategiesintheserviceofasocially-awaredialogsystem.InProceedingsofthe17thAnnualMeetingoftheSpecialInterestGrouponDiscourseandDialogue,pages381\u2013392.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016b.Socially-awarevirtualagents:Au-tomaticallyassessingdyadicrapportfromtemporalpatternsofbehavior.InInternationalconferenceonintelligentvirtualagents,pages218\u2013233.Springer.AAdditionalinformationontheexperimentalsettingsWeusedPyTorch(Paszkeetal.,2019)toimple-menttheneuralmodels.Foreachsetoffeatures,hyperparameterswereselectedusingOptuna(Ak-iba,2019),aparametersearchframework.Were-implementedtheAttention-CNNwithGlove(Pen-ningtonetal.,2014)300-Dwordsembeddingsasthevectorrepresentation.Foreachmodels,theresultsarecross-validatedusing5folds(wechose5insteadof10toavoidhavingfoldswithtoofewsamplesperclass).Wecorrectedthelossfunctionforclassimbalancetoforcethemodeltoadaptmoretothelessfrequentclasses.Thestrengthofthiscorrectiondependedonthemodel,andwasselectedbecauseitprovidedasatisfyingcompro-misebetweenfavoringrecallandprecisionintheclassificationresultsofthatmodel.ForLightGBM,a\"squarerootofthesquarerootoftheinverseclassproportion\"correctionwasselected.Neu-ralmodelsweretrainedusingAdamWasanop-timizer(LoshchilovandHutter,2018),andusedareducedfeaturevector,obtainedwiththeap-plicationofPCA(dinit=1800;d=100;99.8%oftheinformationisconserved).Nosignifi-cantperformancedifferenceswereobservedbe-tweentheoriginalvectorandthereducedvectorfortrainingthemodels.TocomputetheSHAPvaluesmentionedinthepaper,wekeptonesplittoperformthe5-splitofthedataset,andleave1splittovalidateandearlystopthemodel,inor-dertoavoidoverfitting.Acompleteconfigura-tionofhyperparametersusedforeachmodelisre-portedintheGitHubrepositorywiththecodeofthepaper:https://github.com/YannRaphalen/Hedges-Detection.TheBERTmodelwasfine-tunedonaNvidiaQuadroRTX8000GPU.BTables\n2170\n\n\n?(whether|if|is|that|it|this)?.*Subj.\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nNoNoYesNoNoNoYesYesYesYesYesBERT(PTE)\nexceptionally|forthemostpart|inamannerofspeaking|\nNoYesNoNoNoYesNoNoYesYesNoMLP(KDF)\nsomethingbetween|essentially|only).*Prop.\nTable8:Significancetableforthe3-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesYesYesYesYesNoYesYesYesNoYesMLP(KDF+PTE)\ntechnically|typically|virtually|approximately|\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nmuch|somewhat|exactly|almost|littlebit|quite|\nClass\nrelatively|roughly|sotosay|strictlyspeaking|\nNoNoYesNoNoYesNoYesYesYesYesLSTM(KDF)\nYesNoNoNoNo+TM\nTable10:Significancetableforthe3-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoNoYesYesYesYesYesYesNoYesYes\n.*(it)(looks|seems|appears)[,]?.*\",\".*(or|and)(that|something|stuff|soforth)Table7:Regexprulesusedfortheclassifier.\nRule-based\nRule-based\n.*(you(might|may)(believe|think)).*Subj.\nYesYesYesYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nYesYesNoYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nNoNoYesNoNoYesNoNoYesYesYesLSTM(KDF)\nYesYesNoYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(i|i\u2019m|you|it\u2019s)(am|are)(apparently|surely)[,]?.*Prop.\nNoNoYesNoNoYesNoYesYesYesYesLSTM(PTE)\nNoNoYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nNoNoYesYesNoYesYesYesYesYesNoLGBM(KDF)\nNoNoYesYesYesYesYesYesYesYesNoLGBM(KDF)\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nYesNoYesYesYesYesYesYesNoYesYes\nYesYesYesYesYesYesYesYesYesYesYesMLP(KDF+PTE)\nregular|regularly|actually|almost|asitwere|basically|\nNoYesYesNoYesYesYesNoYesYesNoMLP(PTE)\nRule(regexp)\n.*(i|i\u2019m|we)?(was|am|wasn\u2019t)??(not)?(sure|certain).*Subj.\n.*(accordingto|presumably).*Subj.\n(?!.*(be|been|was)likeexcuseme)((excuseme|sorry)[w,\u2019]+|[w,\u2019]+(excuseme|sorry))Prop.\n.*(i|you|we)haveto(check|look|verify).*Subj.\n.*(i\u2019m|i|we\u2019re)(am|are)??(apologize|sorry).*Apol.\nonthetallside|parexcellence|particularly|\n(?!what).*(i|we)?(don\u2019t|didn\u2019t|did)??(not)?\ninarealsense|inasense|inaway|largely|literally|\nYesYesYesNoNoNoYesYesYesYesYesBERT(PTE)\nModels\nModels\nModels\nYesYesYesNoNoYesNoNoYesYesYesLSTM(PTE)\nYesNoNoNoNo\nYesYesYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nSubj.\nTable9:Significancetableforthe4-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoYesNoNoNoYesNoNoYesYesNoMLP(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(ifeellikeyou).*Subj.\nlooselyspeaking|kinda|moreorless|mostly|often|\n.*(unlessi).*Apol.\nprobably|canbeviewas|crypto-|especially|essentially|\n.*(ifi\u2019mnotwrong|ifi\u2019mright|ifthat\u2019strue).*Subj.\nYesNoNoNoNo+POS\n(guess|guessed|thought|think|believe|believed|suppose|supposed)\n.*(just|alittle|maybe|actually|sortof|kindof|pretty\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nNoYesYesNoYesYesNoNoYesYesYesMLP(KDF)\nprettymuch|principally|pseudo-|quintessentially|\n2171\n\n\nFigure2:Absoluteaveragedfeaturecontribution,asindicatedbySHAP.Thelongerthebarisforonecolor,themorethefeatureisassociatedwiththeclassrepresentedbythatcolor.\nFigure3:Averagedcontributionoffeaturestothedetectionofthe\"Notindirect\"class,asindicatedbySHAP.Eachdotcorrespondstoaclassifiedclause.Areddotindicatesthatthefeatureispresentintheclause,whileabluedotindicatesthatthefeatureisabsent.Thefartherontherightthedotis,themorethefeaturecontributedtoitsclassificationasahedge.\n2172\n\n\nFigure4:Averagedcontributionoffeaturestothedetectionof\"Apologizers\",asindicatedbySHAP.\nFigure5:Averagedcontributionoffeaturestothedetectionof\"Propositionalhedges\",asindicatedbySHAP.\n2173\n\n\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nYesNoNoNoNo+TM\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nFigure6:Averagedcontributionoffeaturestothedetectionof\"Subjectivizers\",asindicatedbySHAP.\nModels\nYesNoNoNoNo\nTable11:Significancetableforthe4-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesNoNoNoNo+POS\n2174"}, {"question": " What features were found to significantly improve the performance of the hedge classification model?", "answer": " Knowledge-Driven features based on rules from linguistic literature", "ref_chunk": "\"Youmightthinkaboutslightlyrevisingthetitle\":IdentifyingHedgesinPeer-tutoringInteractionsYannRaphalen1,Chlo\u00e9Clavel2,JustineCassell1,31InriaParis2LTCI,InstitutPolytechniquedeParis,Telecom-Paris3CarnegieMellonUniversityyann.raphalen.pro@gmail.com,justine@cs.cmu.edu,chloe.clavel@telecom-paris.frAbstractHedgesplayanimportantroleinthemanage-mentofconversationalinteraction.Inpeer-tutoring,theyarenotablyusedbytutorsindyads(pairsofinterlocutors)experiencinglowrapporttotonedowntheimpactofinstructionsandnegativefeedback.Pursuingtheobjectiveofbuildingatutoringagentthatmanagesrap-portwithstudentsinordertoimprovelearning,weusedamultimodalpeer-tutoringdatasettoconstructacomputationalframeworkforiden-tifyinghedges.Wecomparedapproachesre-lyingonpre-trainedresourceswithothersthatintegrateinsightsfromthesocialsciencelitera-ture.Ourbestperformanceinvolvedahybridapproachthatoutperformstheexistingbase-linewhilebeingeasiertointerpret.Weemployamodelexplainabilitytooltoexplorethefea-turesthatcharacterizehedgesinpeer-tutoringconversations,andweidentifysomenovelfea-tures,andthebenefitsofsuchahybridmodelapproach.1IntroductionRapport,mostsimplydefinedasthe\u201c...relativeharmonyandsmoothnessofrelationsbetweenpeo-ple...\u201d(Spencer-Oatey,2005),hasbeenshowntoplayaroleinthesuccessofactivitiesasvariedaspsychotherapy(Leach,2005)andsurveyinterview-ing(LuneandBerg,2017).Inpeer-tutoring,rap-port,asmeasuredbytheannotationofthinslicesofvideo,hasbeenshowntobebeneficialforlearningoutcomes(Zhaoetal.,2014;SinhaandCassell,2015).Thelevelofrapportrisesandfallswithconversationalstrategiesdeployedbytutorsandtuteesatappropriatetimes,andasafunctionofthecontentofpriorturns.Thesestrategiesincludeself-disclosure,referringtosharedexperience,and,onthepartoftutors,givinginstructionsinanindirectmanner.Someworkhasattemptedtoautomaticallydetectthesestrategiesintheserviceofintelligenttutors(Zhaoetal.,2016a),butonlyafewstrate-gieshavebeenattempted.Otherworkhascon-centratedona\"socialreasoningmodule\"(Romeroetal.,2017)todecidewhichstrategiesshouldbegeneratedinagivencontext,butindirectnesswasnotamongthestrategiestargeted.Inthispaper,wefocusontheautomaticclassificationofonespe-cificstrategythatisparticularlyimportantforthetutoringdomain,andthereforeimportantforintel-ligenttutors:hedging,asub-partofindirectnessthat\"softens\"whatwesay.Thisworkispartofalargerresearchprogramwiththelong-termgoalofautomaticallygeneratingindirectnessbehaviorsforatutoringagent.\nFigure1:Amockconversationdisplayingeachtypeofhedgedformulation.AccordingtoBrownandLevinson(1987),hedgesarepartofthelinguistictoolsthatinterlocu-torsusetoproducepoliteness,bylimitingthefacethreattotheinterlocutor(basicallybylimitingtheextenttowhichtheinterlocutormightexperienceembarrassmentbecauseofsomekindofpoorper-formance).Anexampleis\"that\u2019skindofawronganswer\".Hedgesarealsofoundwhenspeakerswishtoavoidlosingfacethemselves,forexam-plewhensaying(\"IthinkImighthavetoadd6.\").Madaioetal.(2017)foundthatinapeer-tutoringtask,whenrapportbetweeninterlocutorsislow,tu-teesattemptedmoreproblemsandcorrectlysolvedmoreproblemswhentheirtutorshedgedinstruc-\n2160 Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1: Long Papers, pages 2160 - 2174 May 22-27, 2022 c(cid:13)2022 Association for Computational Linguistics\n\n\ntions,whichlikewisepointstowardsa\"mitigationoffacethreat\"function.Hedgescanalsobeasso-ciatedwithanonverbalcomponent,forexampleavertedeyegazeduringcriticism(BurgoonandKoper,1984).Hedgesarenot,however,alwaysap-propriate,asin\"Ikindofthinkit\u2019srainingtoday.\"whentheinterlocutorscanbothseerain(althoughitmightbetakenashumorous).Thesefactsabouthedgesmotivateawaytoautomaticallydetectthemand,ultimately(althoughnotinthecurrentwork)alsogeneratethem.Inbothcaseswefirsthavetobeabletocharacterizethemusinginterpretablelinguisticfeatures,whichiswhatweaddressinthecurrentpaper.Thus,intheworkdescribedhere,basedonlinguisticdescriptionsofhedges(BrownandLevinson,1987;Fraser,2010),webuiltarule-basedclassifier.Weshowthatthisclassifierincombinationwithadditionalmultimodalinter-pretablecontext-dependentfeaturessignificantlyimprovestheperformanceofamachinelearningmodelforhedges,comparedtoalessinterpretabledeeplearningbaselinefromGoeletal.(2019)us-ingwordembeddings.Wealsoreliedonamachinelearningmodelexplanationtool(LundbergandLee,2017)toinvestigatethelinguisticfeaturesrelatedtohedgesinthecontextofpeer-tutoring,primarilytoseeifwecoulddiscoversurprisingfeaturesthattheclassificationmodelwouldassociatetohedgesinthiscontext,andwedescribethosebelow.Thecodeofthemodelsdescribedinthepaperisalsoprovided.12RelatedworkHedges:AccordingtoFraser(2010),hedgingisarhetoricalstrategythatattenuatesthestrengthofastatement.Onewaytoproduceahedgeisbyalteringthefullsemanticvalueofaparticu-larexpressionthroughPropositionalhedges(alsocalledApproximatorsinPrinceetal.(1982)),asin\"Youarekindofwrong,\"thatreduceprototypical-ity(i.eaccuracyofthecorrespondencebetweenthepropositionandtherealitythatthespeakerseekstodescribe).Propositionalhedgesarerelatedtofuzzylanguage(Lakoff,1975),andthereforetotheproductionofvagueness(Williamson,2002)anduncertainty(Vincze,2014).AsecondkindareRelationalHedges(alsocalledShieldsinPrinceetal.(1982)),suchas\u201cIthinkthatyouarewrong.\u201dor\u201cThedoctorwantsyoutostopsmoking.\u201d,conveyingthatthepropositionis\n1https://github.com/AnonymousHedges/HedgeDetectionconsideredbythespeakerassubjective.Inafurthersub-division,AttributionShields,asin\"Thedoc-torwantsyou...\",theinvolvementofthespeakerinthetruthvalueofthepropositionisnotmadeexplicit,whichallowsspeakersnottotakeastance.Asdescribedabove,Madaioetal.(2017)foundthattutorswhoshowedlowerrapportwiththeirtuteesusedmorehedgedinstructions(theyalsoemployedmorepositivefeedback),howeverthiswasonlythecasefortutorswithagreaterbeliefintheirabilitytotutor.Tuteesinthiscontextsolvedmoreproblemscorrectlywhentheirtutorshedgedinstructions.Noeffectofhedgingwasfoundfordyads(pairsofinterlocutors)withgreatersocialcloseness.However,theauthorsdidnotlookatthespecificlinguisticformstheseteenagersused.Rowland(2007)alsodescribestherolethathedg-ingplaysinthisagegroup,showingthatstudentsusebothrelational(\"IthinkthatJohnissmart.\")andpropositional(\"Johniskindofsmart.\")hedgesformuchthesameshieldingfunctionofdemon-stratinguncertainty,tosavethemfromtheriskofembarrassmentiftheyarewrong.TheauthorobservedthatteensusedfewAdaptors(kindof,somewhat)andpreferredtouseRounders(around,closeto).However,thisstudywasperformedwithanadultandtwochildren,possiblybiasingthere-sultsduetotheparticipationoftheadultinvestiga-tor.Hedgeshavebeenincludedinvirtualtutoringagentsbeforenow.(Howardetal.,2015)integratedhedgesinatutoragentforundergraduatesinCS,asawaytoencouragethestudenttotaketheinitiative.Hedgeshavealsobeenusedasawayofintegrat-ingBrownandLevinson\u2019spolitenessframework(Wangetal.,2008;Schneideretal.,2015)invir-tualtutoringagents.Resultswerenotbrokenoutbystrategy,butpolitenessingeneralwasshowntopositivelyinfluencemotivationandlearning,incertainconditions.Computationalmethodsforhedgedetection:Anumberofstudieshavetargetedthedetectionofhedgesanduncertaintyintext(MedlockandBriscoe,2007;GanterandStrube,2009;Tangetal.,2010;Velldal,2011;Szarvasetal.,2012),partic-ularlyfollowingtheCoNLL2010datasetrelease(Farkasetal.,2010).However,thisworkisnotasrelatedtohedgesinconversation,asitfocusesonaformalandacademiclanguageregister(Hy-land,1998;Varttala,1999).AsnotedbyProkofievaandHirschberg(2014),thefunctionsofhedgesaredomain-andgenre-dependent,thereforethisbias\n2161\n\n\ntowardsformalityimpliesthattheexistingworkmaynotadaptwelltothedetectionofhedgesinconversationbetweenteenagers.Aconsequenceisthattheexistingworkdoesnotconsidertermslike\"Ithink,\"sinceopinionsrarelyappearinanaca-demicwritingdataset.Instructionsarealsoalmostabsent(\"Ithinkyouhavetoaddtentobothsides.\"),astronglimitationforthestudyofconversationalhedgessinceitisinrequests(includingtutoringin-structions)thatindirectformulationsmostlyoccuraccordingtoBlum-Kulka(1987).ProkofievaandHirschberg(2014)alsonotethatitisdifficulttodetecthedgesbecausethewordpatternsassociatedwiththemhaveothersemanticandpragmaticfunc-tions:considering\"Ithinkthatyouhavetoaddxtobothsides.\"vs\"Ithinkthatyouareanidiot.\",itisnotclearthattheseconduseof\"Ithinkthat\"isanhedgemarker.Theyadvocateusingmachinelearningapproachestodealwiththeambiguityofthesemarkers.Workingonaconversationaldataset,Ulinskietal.(2018)builtacomputationalsystemtoassessspeakercommitment(i.e.atwhichpointthespeakerseemsconvincedbythetruthvalueofastatement),inparticularbyrelyingonarule-baseddetectionsystemforhedges.Comparedtothatwork,ourrule-basedclassificationmodelisdirectlydetectinghedgeclasses,andweemploythepredictionsoftherule-basedmodelasafeatureforstrongermachinelearningmodels,designedtolessentheimpactoftheimbalancebetweenclasses.Wealsoconsiderapologieswhentheyserveamit-igationfunction(wethencallthemApologizers),aswasdonebytheauthorsofourcorpus,andwealsousethetermsubjectivizersasdefinedbelow,tobeabletocomparedirectlywiththepreviousworkcarriedoutonthiscorpus.Asfarasweknow,onlyGoeletal.(2019)haveworkedwithapeer-tutoringdataset(thesameonethatwealsouse),andtheyachievedtheirbestclassificationresultbyemployinganAttention-CNNmodel,inspiredbyAdelandSch\u00fctze(2017).3ProblemstatementWeconsiderasetDofconversationsD=(c1,c2,...,c|D|),whereeachconversationiscom-posedofasequenceofindependentsyntacticclausesci=(u1,u2,...,uM),whereMisthenumberofclausesintheconversation.Notethattwoconsecutiveclausescanbeproducedbythesamespeaker.Eachclauseisassociatedwithauniquelabelcorrespondingtothediffer-enthedgeclassesdescribedinTable1:yi\u2208C={PropositionalHedges,Apologizers,Subjec-tivizers,Nothedged}.Finally,anutteranceuicanberepresentedasavectoroffeaturesX=(x1,x2,...,xN),whereNrepresentsthenumberoffeaturesweusedtodescribeaclause.Ourfirstgoalistodesignamodelthatcorrectlypredictsthelabelyiassociatedtoui.Itcanbeunderstoodasthefollowingresearchquestion:RQ1:\"Whichmodelsandfeaturescanbeusedtoautomaticallycharacterizehedgesinapeer-tutoringinteraction?\"Oursecondgoalistoidentify,foreachhedgeclass,thesetoffeaturesFclass={fk},k\u2208[1,N]sortedbyfeatureimportanceintheclassificationofclass.Itcorrespondstothefollowingresearchquestion:RQ2:\"Whatarethemostimportantlinguisticfeaturesthatcharacterizeourhedgeclassesinapeer-tutoringsetting?\"4Methodology4.1CorpusDatacollection:Thedialoguecorpususedherewascollectedaspartofalargerstudyontheeffectsofrapport-buildingonreciprocalpeertutoring.24Americanteenagers(meanage=13.5,min=12,max=15),halfmaleandhalffemale,cametoalabwherehalfoftheparticipantswerepairedwithasame-age,same-genderfriend,andtheotherhalfpairedwithastranger.Theparticipantswereassignedtoatotalof12dyadsinwhichthepar-ticipantsalternatedtutoringoneanotherinlinearalgebraequationsolvingfor5weeklyhour-longsessions,foratotalcorpusofnearly60hoursofface-to-faceinteractions.Eachsessionwasstruc-turedsuchthatthestudentsengagedinbriefsocialchitchatinthebeginning,thenoneofthestudentswasrandomlyassignedtotutortheotherfor20minutes.Theythenengagedinanothersocialpe-riod,andconcludedwithasecondtutoringperiodwheretheotherstudentwasassignedtheroleoftutor.Audioandvideodatawererecorded,tran-scribed,andsegmentedforclause-leveldialogueannotation,providingnearly24000clauses.Non-speechsegments(notablyfillersandlaughter)weremaintained.Becauseoftemporalmisalignmentforpartsofthecorpus,manyparaverbalphenomena,suchasprosody,wereunfortunatelynotavailabletous.SinceouraccesstothedatasetiscoveredbyaNon-DisclosureAgreement,itcannotbereleased\n2162\n\n\n2https://github.com/AnonymousHedges/HedgeDetectionclasses.4.2FeaturesLabelfromrule-basedclassifier(LabelRB):Weusetheclasslabelpredictedbytherule-basedclas-sifierdescribedinSection4.3asafeature.Ourhypothesisisthatthemachinelearningmodelcanusethisinformationtocounterbalancetheclassimbalance.Totakeintoaccountthefactthatsomerulesaremoreefficientthanothers,weweightedtheclasslabelresultingfromtherule-basedmodelbytheprecisionoftherulethatgeneratedit.Unigramandbigram:Wecountthenumberofoccurrencesofunigramsandbigramsofthecorpusineachclause.Weusedthelemmaofthewordsforunigramsandbigramsusingthenltklemmatizer(Loper,2002)andselectedunigramsandbigramsthatoccurredinthetrainingdatasetatleastfiftytimes.Thegoalwastoinvestigate,withabottom-upapproach,towhatextenttheuseofcertainwordscharacterizeshedgeclassesintutoring.InSection5weexaminetheoverlapbetweenthesewordsandthoseaprioriidentifiedbytherules.Part-of-speech(POS):Hedgeclassesseemtobeassociatedwithdifferentsyntacticpatterns:forex-ample,subjectivizersmostoftencontainapersonalpronounfollowedbyaverb,asin\"Iguess\",\"Ibelieve\",\"Ithink\".WethereforeconsideredthenumberofoccurrencesofPOS-Tagn-grams(n=1,2,3)asfeatures.WeusedthespaCyPOS-taggerandconsideredPOSunigrams,bigramsandtri-gramsthatoccuratleast10timesinthetrainingdataset.LIWC:LinguisticInquiryandWordCount(LIWC)(Pennebakeretal.,2015)isstandardsoft-wareforextractingthecountofwordsbelongingtospecificpsycho-socialcategories(e.g.,emotions,religion).Ithasbeensuccessfullyusedinthede-tectionofconversationalstrategies(Zhaoetal.,2016a).Wethereforecountthenumberofoccur-rencesofallthe73categoriesfromLIWC.Tutoringmoves(TM):Intelligenttutoringsys-temsrelyonspecifictutoringmovestosuccess-fullyconveycontent(asdohumantutors).Wethereforelookedatthelinkbetweenthetutoringmoves,asannotatedinMadaioetal.(2017),andhedges.Fortutors,thesemovesare(1)instruc-tionaldirectivesandsuggestions,(2)feedback,and(3)affirmations,mostlyexplicitreflectionsontheirpartners\u2019comprehension,whilefortutees,theyare(1)questions,(2)feedbacks,and(3)affirmations,\npublicly.Howevertheoriginalexperimenters\u2019In-stitutionalReviewBoard(IRB)approvalallowsustoview,annotate,andusethedatatotrainmodels.Thisalsoallowsustoprovidealinktoapixe-latedvideoexampleintheGitHubrepositoryoftheproject2.Dataannotation:Thedatasetwaspreviouslyan-notatedbyMadaioetal.(2017),followingananno-tationmanualthatusedhedgeclassesderivedfromRowland(2007)(seeTable1).Onlythetaskperi-odsoftheinteractionswereannotated.Comparingtheannotationswiththeclassesmentionedintherelatedworksection,SubjectivizerscorrespondtoRelationalhedges(Fraser,2010),PropositionalhedgesandExtenderscorrespondtoApproxima-tors(Princeetal.,1982)withtheadditionofsomediscoursemarkerssuchasjust.ApologizersarementionedaslinguistictoolsrelatedtonegativepolitenessinBrownandLevinson(1987).Krippen-dorff\u2019salphaobtainedforthiscorpusannotatedbyfourcoderswasover0.7forallclasses(denotinganacceptableinter-coderreliabilityaccordingtoKrippendorff(2004)).Thedatasetiswidelyim-balanced,withmorethan90%oftheutterancesbelongingtotheNothedgedclass.Inreviewingthecorpusandtheannotationman-ual,however,wenoticedtwoissues.First,theannotationoftheExtendersclasswasinconsis-tent,leadingtotheExtendersandPropositionalhedgesclassescarryingsimilarsemanticfunctions.WethereforemergedthetwoclassesandgroupedutteranceslabeledasExtendersandthoselabeledasPropositionalhedgesundertheheadingofPropositionalhedges.Second,theannotationofclausescontainingthetokens\"just\"and\"would\"(twotermsoccurringfrequentlyinthedatasetthatarekeycomponentsofPropositionalHedgesandSubjectivizersbutthatarenotinfacthedgesinallcases)wasalsoinconsistent,leadingtovirtuallyallclauseswiththosetwotokensbeingconsideredhedges.Wethereforere-consideredalltheclausesassociatedwithanyofthehedgeclasses,aswellasalltheclausesinthe\"Nothedged\"classthatcontained\"just\"or\"would\".There-annotationwascarriedoutbytwoannotatorswhoachievedaKrippendorff\u2019salphainter-raterreliabilityof.9orbetterforApologizers,Subjectivizers,andPropo-sitionalhedgesbeforeindependentlyre-annotatingtherelevantclauses.Anexampleofare-annotationwasremoving\"Iwouldkillyou!\"fromthehedge\n2163\n\n\nProp.hedgesApologizersSubjectivizersNothedgedTotal\nTable1:Definitionoftheclasses\nFeaturesnameAutomaticextractionVectorsize\nSubjectivizersWordsthatreduceintensityorcertainty\u201cSothenIwoulddividebytwo.\u201dApologizersApologiesusedtosoftendirectspeechacts\u201cOhsorrysixb.\u201dPropositionalhedgesQualifyingwordstoreduceintensityorcertaintyofutterances\u201cIt\u2019sactuallyeight.\u201dExtendersWordsusedtoindicateuncertaintybyreferringtovaguecategories\u201cIt\u2019llbethenumberxorwhatevervariableyouhave.\u201d\n12101286262119223156\nTable2:Distributionoftheclasses\nClassDefinitionExample\nRule-basedlabelYes4UnigramYes~250BigramYes~250POSYes~1200LIWCYes73NonverbalNo24TutoringmovesNo6Total~1800\nTable3:Listofautomaticallyextractedandmanuallyannotatedfeatureswiththeirsize.mostlytentativeanswers.Nonverbalandparaverbalbehaviors:AsinGoeletal.(2019),weincludedthenonverbalandpar-averbalbehaviorsthatarerelatedtohedges.Specif-ically,weconsiderlaughterandsmiles,thathavebeenshowntobeeffectivemethodsofmitiga-tion(Warner-Garcia,2014),cut-offsindicatingself-repairs,fillerslike\"Um\",gazeshifts(annotatedas\u2019GazeatPartner\u2019,\u2019GazeattheMathWorksheet\u2019,and\u2019Gazeelsewhere\u2019),andheadnods.Eachfea-turewaspresenttwiceinthefeaturevector,onetimeforeachinterlocutor.Inter-raterreliabilityfornonverbalbehaviorwas0.89(asmeasuredbyKrippendorff\u2019salpha)foreyegaze,0.75forsmilecount,0.64forsmiledurationand0.99forheadnod.Laughterisalsoreportedinthetranscriptatthewordlevel.Weseparatethetutor\u2019sbehaviorsfromthoseofthetutee.ThecollectionprocessforthesebehaviorsisdetailedfurtherinZhaoetal.(2016b).Theclause-levelfeaturevectorwasnormalizedbythelengthoftheclause(exceptfortherule-basedlabel).Thislengthwasalsoaddedasafeature.Table3presentsanoverviewofthefinalfeaturevector.4.3ClassificationmodelsTheclassificationmodelsusedarepresentedhereaccordingtotheirlevelofintegrationofexternallinguisticknowledge.Rule-basedmodel:OnthebasisoftheannotationmanualusedtoconstructthedatasetfromMadaioetal.(2017),andwithdescriptionsofhedgesfromRowland(2007),Fraser(2010)andBrownandLevinson(1987),weconstructedarule-basedclas-sifierthatmatchesregularexpressionsindicativeofhedges.TherulesaredetailedinTable7intheAppendix.LGBM:Sincehedgesareoftencharacterizedbyexplicitlexicalmarkers,wetestedtheassumptionthatamachinelearningmodelwithaknowledge-drivenrepresentationforclausescouldcompetewithaBERTmodelinperformance,whilebeingmuchmoreinterpretable.WereliedonLightGBM,anensembleofdecisiontreestrainedwithgradi-entboosting(Keetal.,2017).Thismodelwasselectedbecauseofitsperformancewithsmalltrainingdatasetsandbecauseitcanignoreunin-formativefeatures,butalsoforitstrainingspeedcomparedtoalternativeimplementationsofgradi-entboostingmethods.Multi-layerperceptron(MLP):Asasimplebase-line,webuiltamulti-layerperceptronusingthreesetsoffeatures:apre-trainedcontextualrepre-sentationoftheclause(SentBERT;ReimersandGurevych(2019));theconcatenationofthiscon-textualrepresentationoftheclauseandarule-basedlabel(notrelyingonthepreviousclauses);andfinallytheconcatenationofallthefeaturesmen-tionedinsection4.2,withoutthecontextualizedrepresentation.LSTMoverasequenceofclauses:Sinceweareworkingwithconversationaldata,wealsowantedtotestwhethertakingintoac-countthepreviousclauseshelpstodetectthetypeofhedgeclassinthenextclause.Formally,wewanttoinferyiusingyi=maxy\u2208ClassesP(y|X(ui),X(ui\u22121),...,X(ui\u2212K)),whereKisthenumberofpreviousclausesthatthemodelwilltakeintoaccount.The\n2164\n\n\nMLPmodelpresentedaboveinfersyiusingyi=maxy\u2208ClassesP(y|X(ui)),thereforeadifferenceofperformancebetweenthetwomodelswouldbeasignthatusinginformationfromthepreviousclausescouldhelptodetectthehedgedformulationinthecurrentclause.WetestedaLSTMmodelwiththesamerepresentationsforclausesasfortheMLPmodel.CNNwithattention:Goeletal.(2019)estab-lishedtheirbestperformanceonhedgedetec-tionusingaCNNmodelwithadditiveattentionoverword(andnotclause)embeddings.Con-trarytotheMLPandLSTMmodelsmentionedabove,thismodeltriestoinferyiusingyi=maxy\u2208ClassesP(y|g(w0),g(w1),...,g(wL)),withLrepresentingthemaximumclauselengthweal-low,andgrepresentingafunctionthatturnsthewordwj,j\u2208[0,L]intoavectorrepresentation(formoredetails,pleaseseeAdelandSch\u00fctze(2017)).BERT:Tobenefitfromdeepsemanticandcon-textualrepresentationsoftheutterances,wealsofine-tunedBERT(Devlinetal.,2019)onourclas-sificationtask.BERTisapre-trainedTransformersencoder(Vaswanietal.,2017)thathassignificantlyimprovedthestateoftheartonanumberofNLPtasks,includingsentimentanalysis.Itproducesacontextualrepresentationofeachwordinasen-tence,makingitcapableofdisambiguatingthemeaningofwordslike\"think\"or\"just\"thatarerepresentativeofcertainclassesofhedges.BERT,however,isnotablyhardtointerpret.4.4AnalysistoolsLookingatwhichfeaturesimprovetheperfor-manceofourclassificationmodelstellsuswhetherthesefeaturesareinformativeornot,butdoesnotexplainhowthesefeaturesareusedbythemod-elstomakeagivenprediction.Wethereforepro-ducedacomplementaryanalysisusinganinter-pretabilitytool.Asdemonstratedby(LundbergandLee,2017),LightGBMinternalfeatureimpor-tancescoresareinconsistentwithboththemodelbehaviorandhumanintuition,soweinsteadusedamodel-agnostictool.SHAP(LundbergandLee,2017)assignstoeachfeatureanimportancevalue(calledShapleyvalues)foraparticularpredictiondependingontheextentofitscontribution(ade-tailedintroductiontoShapleyvaluesandSHAPcanbefoundinMolnar(2020)).SHAPisamodel-agnosticframework,thereforethevaluesassoci-atedwithasetoffeaturescanbecomparedacrossmodels.ItshouldbenotedthatSHAPproducesexplanationsonacase-by-casebasis,thereforeitcanbothprovidelocalandglobalexplanations.FortheGradientBoostingmodel,weuseanadaptedversionofSHAP(Lundbergetal.,2018),calledTreeSHAP.5Experimentsandresults5.1ExperimentalsettingTodetectthebestsetoffeatures,weusedLight-GBMandproceededincrementally,byaddingthegroupoffeatureswethoughttobemostlikelyasso-ciatedwithhedges.Wedidnotconsidertheriskofrelyingonasub-optimalsetoffeaturesthroughthisprocedurebecauseofthestrongabilityofLight-GBMtoignoreuninformativefeatures.Weusethisincrementalapproachasawaytotestourintuitionabouttheperformativityofgroupsoffeatures(i.e.doesaddingafeatureimprovetheperformanceofthemodel)withregardtothetaskofclassifica-tion.Tocompareourmodels,wetrainedthemonthe4-classtask,andlookedattheaverageoftheweightedF1-scoresforthethreehedgeclasses(i.e.howwellthemodelsinferminorityclasses)thatwereporthereas\"3-classes\",andattheaverageoftheweightedF1-scoresforthe4classes,thatwereportas\"4-classes\".DetailsofthehyperparametersandexperimentalsettingsareprovidedinAppendixA.5.2ModelcomparisonandfeatureanalysisOverallresults:Table4presentstheresultsob-tainedbythe6modelspresentedinSection4.3forthemulti-classproblem.Bestperformance(F1-scoreof79.0)isobtainedwithLightGBMlever-agingalmostallthefeatures.Intheappendix(seeTable8andTable9)weindicatetheconfidenceintervalstorepresentthesignificanceofthediffer-encesbetweenthemodels.First,andperhapssurprisingly,wenoticethattheuseof\"Knowledge-Driven\"featuresbasedonrulesbuiltfromlinguisticknowledgeofhedgesintheLightGBMmodeloutperformstheuseofpre-trainedembeddingswithinafine-tunedBERTmodel(79.0vs.70.6),andintheneuralbaselinefrom(Goeletal.,2019)(79.0vs64.5).ThelowscoresobtainedbytheLGBM,LSTMandMLPmodelswithpre-trainedsentenceem-beddingsversusKnowledge-Drivenfeaturesmightsignalthatthewordpatternscharacterizinghedgesarenotsalientintheserepresentations(i.e.the\n2165\n\n\n68.5(1.6)35.8(3.1)64.8(1.1)Attention-CNN(3-classes)\n65.1(5.7)39.8(8.0)65.2(5.1)BERT(3-classes)\n\u220570.6(2.3)\u2205LGBM(3-classes)\n79.0(1.3)35.0(2.2)70.1(1.4)\nRule-based(3-classes)\n94.7\u2205\u2205MLP(4-classes)\n3Notethatthereisstrongredundancybetweensomefea-turesofLIWCandthespaCyPOStaggerthatbothproducea\"Pronoun\"category,usingalexiconinthefirstcase,andaneuralinferenceinthesecond.\n\u220594.9(0.4)\u2205LGBM(4-classes)\nRule-based(4-classes)\nTable4:AveragedweightedF1-scores(andstandarddeviation)forthethreeminorityclassesandforthe4classes,forallmodels.\"KD\"standsfor\"Knowledge-Driven\",meaningthatthefeaturesarederivedfromlexicon,n-grammodelsandannotations.distancebetween\"Ithinkyoushouldadd5.\"and\"Youshouldadd5.\"isshort.).KDFeaturesseemtoprovideabetterseparabilityoftheclasses.ThecombinationofKDfeaturesandPre-trainedem-beddingsdoesnotsignificantlyimprovetheperfor-manceofthemodelscomparedtotheKDFeaturesonly,whichsuggeststhattheinformationfromthePre-trainedembeddingsisredundantwiththeonefromtheKDFeatures.Thisresultmaybeduetothehighdimensionalityoftheinputvector(868withPCAontheKDFeatures;2500otherwise).Asecondfindingisthattheuseofgradientboost-ingmodelsontopofrule-basedclassifiersbettermodelsthehedgeclasses.Theothermachinelearn-ingmodelsdidnotprovetobeaseffective,exceptforBERT.FeatureanalysisusingLightGBM:Usingthebestperformingmodel,Table5showstheroleofeachfeaturesetinthepredictiontask.ThesignificanceofthedifferencesisshowninTable10andTable11.Comparedtotherule-basedmodel,theintroductionofn-gramssignificantlyimprovedtheperformanceofourclassifier,suggestingthatsomelexicalandsyntacticinformationdescribingthehedgeclasseswasnotpresentintherule-basedmodel.LookingatTable5,wedonotobservesignificantdifferencesbetweentheLGBMmodelusingonlythelabelrulebased+(1-gramsand2-grams)andthemodelsin-corporatingmorefeatures.Tooursurprise,neitherthetutoringmovesnorthenonverbalfeaturessig-nificantlyimprovedtheperformanceofthemodel.The2featureswereincludedtoindexthespecificpeertutoringcontextofthesehedges,sothisindi-catesthatinfutureworkwemightwishtoapplythecurrentmodeltoanothercontextofusetoseeifthismodelofhedgesismoregenerallyapplicablethanweoriginallythought.Bycombiningthisresultwiththeincreasedperformanceofthemodelus-ingKnowledge-Driven(i.e.explicit)featurescom-paredtopre-trainedembeddings,itwouldseemthathedgesareaboveallalexicalphenomenon(i.e.producedbyspecificlexicalelements).5.3In-depthanalysisoftheinformativefeaturesWetrainedtheSHAPexplanationmodelsonLight-GBMwithallfeatures.Themostinformativefea-tures(inabsolutevalue)foreachclassareshowninTable6,andtheplotsbyclassarepresentedintheAppendix.Themostimportantfeaturesseemtobetherule-basedlabels,whichappearinatleastthefourthpositionforthreeclasses(seeTable6),andinthefirstpositionforPropositionalHedgesandNothedgedclasses.Surprisingly,theRule-Basedlabeldoesnotappearinthetop20featuresforApologizers.However,giventhattheclassrarelyappearsinthedata,therulesseldomactivate,sothefeaturemaysimplybeinformativeforaverysmallnumberofclauses.Unigrams(Oh,Sorry,just,Would,andI)arealsopresentinthe5top-rankedfeatures.Thisconfirmsthefindingsmen-tionedinrelatedworkforthecharacterizationofthedifferenthedgeclasses(justwithPropositionalHedges,sorrywithApologizer,IwithSubjectiviz-ers).ThepresenceofOhalsohashighimportanceforthecharacterizationofApologizer(n=2),asillustratedinexamplessuchas\"Ohsorry,that\u2019snine.\".Wenotethattheoccurrencesof\"Ohsorry\"asastand-aloneclausewereexcludedbyourrule-basedmodelbecausetheydonotcorrespondtoanapologizer(theycannotmitigatethecontentofapropositionifthereisnopropositionassociated).Thisexampleillustratestheinterestofamachinelearningmodelapproachtodisambiguatethefunc-tionofconventionalnon-propositionalphraseslike\"Ohsorry\".Inaddition,SHAPhighlightstheimportanceofnovelfeatureswhosefunctionwasnotidentifiedinthehedgesliterature:(i)whatLIWCclassifiesasinformalwordsbutthataremostlyinterjectionslikeahandoharestronglyassociatedwithApol-ogizer,asaredisfluencies(n=12);(ii)theuseofPOStagsseemstobeveryrelevantforcharac-terizingthedifferentclasses(2-gramofPOStagfeatures3occurinthetop-rankedfeaturesofallthe\n\u220564.5(3.0)\u2205LSTM(3-classes)\nKDFeat.(KDF)Pre-TrainedEmb.(PTE)KDF+PTE\n\u220594.4(0.2)\u2205LSTM(4-classes)\n96.7(0.2)91.0(0.2)95.4(0.2)\n94.8(0.3)89.7(0.4)93.9(0.4)Attention-CNN(4-classes)\n93.9(1.4)89.1(1.4)94.1(1.2)BERT(4-classes)\n67.6\u2205\u2205MLP(3-classes)\nModels\n2166\n\n\n95.0(0.2)96.5(0.3)96.5(0.2)96.7(0.2)96.6(0.4)96.7(0.3)\n3-classes\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\n68.8(0.8)78.2(1.6)78.1(1.3)79.0(1.3)78.5(2.4)78.7(1.8)\nTable5:AveragedweightedF1-scoresforthethreeclassesofhedgesandthefourclasses,withanadditiveintegrationofKDFfeaturesintheLightGBMmodel.Thestandarddeviationiscomputedacrossfivefolds.\n4-classes\nModels\nRankApologizerSubjectivizersProp.HedgesNothedged\n1Functionwords(LIWC)\"I\"ClasslabelClasslabel2\"Oh\"(LIWC)\"Yeah\"\"Would\"\"Would\"3\"Sorry\"Noun(POS)\"Just\"\"Yeah\"4Affect(LIWC)ClasslabelFunctionword(LIWC)Noun(POS)5ClauselengthCognitiveprocess(LIWC)Netspeak(LIWC)Cognitiveprocess(LIWC)Table6:Mostimportantclause-levelfeaturesforLightGBMaccordingtotheSHAPanalysis.classes(seeFiguresintheAppendix).Itmeansthattherearesomerecurringsyntacticpatternsineachclass;(iii)Regardingtheutterancesize,aclauseshorterthanthemeanisweaklyassociatedwithdirectness(n=17)whilealongerclausesuggeststhatitcontainsaSubjectivizer(n=6).Apologizersarecharacterizedbyameanclauselength(n=5),withfewvariationsfromit;(iv)Tutoringmovesarenotstrongpredictorsofanyclasses:\"Affirma-tionfromtutor\"istheonlyfeatureappearingasapredictorofPropositionalhedges(n=20).ThisisconsistentwiththefeatureanalysisinTable5,suggestingthattutoringmovesdonotsignificantlyimprovetheperformanceoftheclassifier;(v)Non-verbalbehaviorsdonotappearasimportantfea-turesfortheclassification.Thisiscoherentwithresultsfrom(Goeletal.,2019).Notethatprosodymightplayaroleindetectinginstructionsthattrailoff,but,asdescribed,paraverbalfeatureswerenotavailable;(vi)Wouldplaysanimportantroleintheproductionofhedges,asitisstronglyassociatedtoPropositionalhedges(n=2).Itisinterestingtonotethat,whendesigningtherule-basedclassifier,wesawitdecreaseinperformancewhenwestartedtoincludewouldinourregularexpressionpatterns,probablybecausetheformishardtodisambiguateforadeterministicsystem.WhileexploringtheShapleyvaluesassociatedtoeachclause,weobservedthatfeaturesliketutoringmovesareextremelyinformativeforaverysmallnumberofclauses(thereforenotsignificantlyinflu-encingtheoverallperformanceoftheprediction),andmoreorlessnotinformativefortherest.Infer-ringtheglobalimportanceofafeatureasameanacrosstheshapleyvaluesinthedatasetmaynotbetheonlywaytoexplorethebehaviorofgradi-entboostingmethods.ItmightbemoreusefultoclusterclausesbasedontheimportancethatSHAPgivestothatfeatureinitsclassification,asthiscouldhelpdiscoversub-classesofhedgesthataredifferentiatedfromtherestbytheirinteractionwithaspecificfeature(inthewaythatsomeApologiz-ersarecharacterizedbyan\"oh\").Wealsonotethattheexplanationmodelissensitivetospuri-ouscorrelationsinthedataset,causedbythesmallrepresentationofsomeclass:forexample,\"nine\"(n=7)and\"four\"(n=20)arepositivepredictorsofApologizers.6ConclusionandfutureworkThroughourclassificationperformanceexperi-ments,weshowedthatitispossibletousema-chinelearningmethodstodiminishtheambigu-ityofhedges,andthatthehybridapproachofus-ingrule-basedlabelfeaturesderivedfromsocialscience(includinglinguistics)literaturewithinamachinelearningmodelhelpedsignificantlytoin-creasethemodel\u2019sperformance.Nonverbalbehav-iorsandtutoringmovesdidnotprovideinformationatthesentencelevel;boththeperformanceofthemodelandthefeaturecontributionanalysissug-gestedthattheirimpactonthemodeloutputwasnotstrong.ThisisconsistentwithresultsfromGoeletal.(2019).However,infutureworkwewouldliketoinvestigatethepotentialofmultimodalpat-ternswhenweareabletobettermodelsequentiality(e.g.,negativefeedbackfollowedbyasmile).Re-gardingtheSHAPanalysis,mostofthefeaturesthatareconsideredasimportantarecoherentwiththedefinitionoftheclasses(Iforsubjectivizers,sorryforapologizers,justforpropositionalhedges).However,wediscoveredthatfeatureslikeutterance\n2167\n\n\nsizecanalsoserveasindicatorsofcertainclassesofhedges.AlimitationofSHAPisthatitmakesafeatureindependenceassumption,whichpromptstheexplanatorymodeltounderestimatetheimpor-tanceofredundantfeatures(likepronounsinourwork).Inthefuturewewillexploreexplanatorymodelscapableoftakingintoaccountthecorre-lationbetweenfeaturesinthedatasetlikeSAGE(Covertetal.,2020),butsuitedforveryimbal-anceddatasets.Inthedomainofpeer-tutoring,wewouldliketobeabletofurthertestthelinkbe-tweenhedgesandrapport,andthelinkbetweenhedgesandlearninggainsinthesubjectbeingtu-tored.Asnotedabove,thiskindofstudyrequiresafine-grainedcontrolofthelanguageproducedbyoneoftheinterlocutors,whichisdifficulttoachieveinahuman-humanexperience.Wenotethatthehedgeclassifiercanbeusednotjusttoclassify,butalsotoworktowardsimprovingthegenerationofhedgesfortutoragents.Infutureworkwewillexploreusingtheclassifiertore-rankgenerationoutputs,takingadvantageoftherecur-ringsyntacticpatterns(see(ii)inSection5.3)toimprovethegenerationprocessofhedges,andre-generatingclausesthatdon\u2019tcontainoneofthesesyntacticpatterns.AcknowledgmentsManythankstomembersoftheArticuLaboatIN-RIAParisfortheirpreciousassistance.ThisworkwassupportedinpartbythetheFrenchgovern-mentundermanagementofAgenceNationaledelaRechercheaspartofthe\u201cInvestissementsd\u2019avenir\u201dprogram,referenceANR-19-P3IA-0001(PRAIRIE3IAInstitute).ReferencesHeikeAdelandHinrichSch\u00fctze.2017.Exploringdif-ferentdimensionsofattentionforuncertaintydetec-tion.InProceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputa-tionalLinguistics:Volume1,LongPapers,pages22\u201334,Valencia,Spain.AssociationforComputa-tionalLinguistics.ShoshanaBlum-Kulka.1987.Indirectnessandpolite-nessinrequests:Sameordifferent?Journalofpragmatics,11(2):131\u2013146.PenelopeBrownandStephenCLevinson.1987.Polite-ness:Someuniversalsinlanguageusage,volume4.Cambridgeuniversitypress.JudeeKBurgoonandRandallJKoper.1984.Nonverbalandrelationalcommunicationassociatedwithreti-cence.HumanCommunicationResearch,10(4):601\u2013626.IanCovert,ScottMLundberg,andSu-InLee.2020.Understandingglobalfeaturecontributionswithad-ditiveimportancemeasures.AdvancesinNeuralInformationProcessingSystems,33:17212\u201317223.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforCom-putationalLinguistics:HumanLanguageTechnolo-gies,Volume1(LongandShortPapers),pages4171\u20134186.Rich\u00e1rdFarkas,VeronikaVincze,Gy\u00f6rgyM\u00f3ra,J\u00e1nosCsirik,andGy\u00f6rgySzarvas.2010.Theconll-2010sharedtask:learningtodetecthedgesandtheirscopeinnaturallanguagetext.InProceedingsofthefour-teenthconferenceoncomputationalnaturallanguagelearning\u2013Sharedtask,pages1\u201312.BruceFraser.2010.Pragmaticcompetence:Thecaseofhedging.Newapproachestohedging,1534.ViolaGanterandMichaelStrube.2009.Findinghedgesbychasingweasels:Hedgedetectionusingwikipediatagsandshallowlinguisticfeatures.InProceedingsoftheACL-IJCNLP2009ConferenceShortPapers,pages173\u2013176.PranavGoel,YoichiMatsuyama,MichaelMadaio,andJustineCassell.2019.\u201cithinkitmighthelpifwemultiply,andnotadd\u201d:Detectingindirectnessincon-versation.In9thInternationalWorkshoponSpokenDialogueSystemTechnology,pages27\u201340.Springer.CynthiaHoward,PamelaW.Jordan,BarbaraMariaDiEugenio,andSandraKatz.2015.Shiftingtheload:apeerdialogueagentthatencouragesitshumancollab-oratortocontributemoretoproblemsolving.Interna-tionalJournalofArtificialIntelligenceinEducation,27:101\u2013129.KenHyland.1998.Hedginginscientificresearcharti-cles,volume54.JohnBenjaminsPublishing.GuolinKe,QiMeng,ThomasFinley,TaifengWang,WeiChen,WeidongMa,QiweiYe,andTie-YanLiu.2017.Lightgbm:Ahighlyefficientgradientboost-ingdecisiontree.Advancesinneuralinformationprocessingsystems,30:3146\u20133154.KlausKrippendorff.2004.Reliabilityincontentanaly-sis:Somecommonmisconceptionsandrecommen-dations.Humancommunicationresearch,30(3):411\u2013433.GeorgeLakoff.1975.Hedges:Astudyinmeaningcriteriaandthelogicoffuzzyconcepts.InContem-poraryresearchinphilosophicallogicandlinguisticsemantics,pages221\u2013271.Springer.\n2168\n\n\nMatthewLeach.2005.Rapport:Akeytotreatmentsuc-cess.Complementarytherapiesinclinicalpractice,11:262\u20135.IlyaLoshchilovandFrankHutter.2018.Decoupledweightdecayregularization.InInternationalConfer-enceonLearningRepresentations.ScottMLundberg,GabrielGErion,andSu-InLee.2018.Consistentindividualizedfeatureat-tributionfortreeensembles.arXivpreprintarXiv:1802.03888.ScottMLundbergandSu-InLee.2017.Aunifiedap-proachtointerpretingmodelpredictions.InProceed-ingsofthe31stinternationalconferenceonneuralinformationprocessingsystems,pages4768\u20134777.HowardLuneandBruceLBerg.2017.Qualitativeresearchmethodsforthesocialsciences.Pearson.MichaelMadaio,JustineCassell,andAmyOgan.2017.Theimpactofpeertutors\u2019useofindirectfeedbackandinstructions.Philadelphia,PA:InternationalSo-cietyoftheLearningSciences.BenMedlockandTedBriscoe.2007.Weaklysuper-visedlearningforhedgeclassificationinscientificliterature.InProceedingsofthe45thannualmeetingoftheassociationofcomputationallinguistics,pages992\u2013999.ChristophMolnar.2020.Interpretablemachinelearn-ing.Lulu.com.AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advancesinneuralinformationprocessingsystems,32:8026\u20138037.JamesWPennebaker,RyanLBoyd,KaylaJordan,andKateBlackburn.2015.Thedevelopmentandpsycho-metricpropertiesofliwc2015.Technicalreport.JeffreyPennington,RichardSocher,andChristopherDManning.2014.Glove:Globalvectorsforwordrep-resentation.InProceedingsofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing(EMNLP),pages1532\u20131543.EllenFPrince,JoelFrader,CharlesBosk,etal.1982.Onhedginginphysician-physiciandiscourse.Lin-guisticsandtheProfessions,8(1):83\u201397.AnnaProkofievaandJuliaHirschberg.2014.Hedgingandspeakercommitment.In5thIntl.WorkshoponEmotion,SocialSignals,Sentiment&LinkedOpenData,Reykjavik,Iceland.NilsReimersandIrynaGurevych.2019.Sentence-bert:Sentenceembeddingsusingsiamesebert-networks.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages3982\u20133992.OscarJRomero,RanZhao,andJustineCassell.2017.Cognitive-inspiredconversational-strategyreasonerforsocially-awareagents.InIJCAI,pages3807\u20133813.TimRowland.2007.\u2018wellmaybenotexactly,butit\u2019saroundfiftybasically?\u2019:Vaguelanguageinmath-ematicsclassrooms.InVaguelanguageexplored,pages79\u201396.Springer.SaschaSchneider,SteveNebel,SimonPradel,andG\u00fcn-terDanielRey.2015.Mindyourpsandqs!howpoliteinstructionsaffectlearningwithmultimedia.ComputersinHumanBehavior,51:546\u2013555.TanmaySinhaandJustineCassell.2015.Weclick,wealign,welearn:Impactofinfluenceandconvergenceprocessesonstudentlearningandrapportbuilding.InProceedingsofthe1stWorkshoponModelingINTERPERsonalSynchrONyAndInfLuence,INTER-PERSONAL\u201915,page13\u201320,NewYork,NY,USA.AssociationforComputingMachinery.HelenSpencer-Oatey.2005.(im)politeness,faceandperceptionsofrapport:Unpackagingtheirbasesandinterrelationships.1(1):95\u2013119.Gy\u00f6rgySzarvas,VeronikaVincze,Rich\u00e1rdFarkas,Gy\u00f6rgyM\u00f3ra,andIrynaGurevych.2012.Cross-genreandcross-domaindetectionofsemanticuncer-tainty.ComputationalLinguistics,38(2):335\u2013367.BuzhouTang,XiaolongWang,XuanWang,BoYuan,andShixiFan.2010.Acascademethodfordetectinghedgesandtheirscopeinnaturallanguagetext.InProceedingsoftheFourteenthConferenceonCom-putationalNaturalLanguageLearning\u2013SharedTask,pages13\u201317.MorganUlinski,SethBenjamin,andJuliaHirschberg.2018.Usinghedgedetectiontoimprovecommittedbelieftagging.InProceedingsoftheWorkshoponComputationalSemanticsbeyondEventsandRoles,pages1\u20135.TeppoVarttala.1999.Remarksonthecommunicativefunctionsofhedginginpopularscientificandspecial-istresearcharticlesonmedicine.Englishforspecificpurposes,18(2):177\u2013200.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,\u0141ukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.InAdvancesinneuralinformationpro-cessingsystems,pages5998\u20136008.ErikVelldal.2011.Predictingspeculation:asimpledis-ambiguationapproachtohedgedetectioninbiomed-icalliterature.JournalofBiomedicalSemantics,2(5):1\u201314.VeronikaVincze.2014.Uncertaintydetectioninnaturallanguagetexts.PhD,UniversityofSzeged,page141.\n2169\n\n\nNingWang,WLewisJohnson,RichardEMayer,PaolaRizzo,ErinShaw,andHeatherCollins.2008.Thepolitenesseffect:Pedagogicalagentsandlearningoutcomes.Internationaljournalofhuman-computerstudies,66(2):98\u2013112.ShawnWarner-Garcia.2014.Laughingwhennothing\u2019sfunny:Thepragmaticuseofcopinglaughterinthenegotiationofconversationaldisagreement.Prag-matics,24(1):157\u2013180.TimothyWilliamson.2002.Vagueness.Routledge.RanZhao,AlexandrosPapangelis,andJustineCassell.2014.Towardsadyadiccomputationalmodelofrap-portmanagementforhuman-virtualagentinteraction.InInternationalConferenceonIntelligentVirtualAgents,pages514\u2013527.Springer.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016a.Automaticrecognitionofconversa-tionalstrategiesintheserviceofasocially-awaredialogsystem.InProceedingsofthe17thAnnualMeetingoftheSpecialInterestGrouponDiscourseandDialogue,pages381\u2013392.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016b.Socially-awarevirtualagents:Au-tomaticallyassessingdyadicrapportfromtemporalpatternsofbehavior.InInternationalconferenceonintelligentvirtualagents,pages218\u2013233.Springer.AAdditionalinformationontheexperimentalsettingsWeusedPyTorch(Paszkeetal.,2019)toimple-menttheneuralmodels.Foreachsetoffeatures,hyperparameterswereselectedusingOptuna(Ak-iba,2019),aparametersearchframework.Were-implementedtheAttention-CNNwithGlove(Pen-ningtonetal.,2014)300-Dwordsembeddingsasthevectorrepresentation.Foreachmodels,theresultsarecross-validatedusing5folds(wechose5insteadof10toavoidhavingfoldswithtoofewsamplesperclass).Wecorrectedthelossfunctionforclassimbalancetoforcethemodeltoadaptmoretothelessfrequentclasses.Thestrengthofthiscorrectiondependedonthemodel,andwasselectedbecauseitprovidedasatisfyingcompro-misebetweenfavoringrecallandprecisionintheclassificationresultsofthatmodel.ForLightGBM,a\"squarerootofthesquarerootoftheinverseclassproportion\"correctionwasselected.Neu-ralmodelsweretrainedusingAdamWasanop-timizer(LoshchilovandHutter,2018),andusedareducedfeaturevector,obtainedwiththeap-plicationofPCA(dinit=1800;d=100;99.8%oftheinformationisconserved).Nosignifi-cantperformancedifferenceswereobservedbe-tweentheoriginalvectorandthereducedvectorfortrainingthemodels.TocomputetheSHAPvaluesmentionedinthepaper,wekeptonesplittoperformthe5-splitofthedataset,andleave1splittovalidateandearlystopthemodel,inor-dertoavoidoverfitting.Acompleteconfigura-tionofhyperparametersusedforeachmodelisre-portedintheGitHubrepositorywiththecodeofthepaper:https://github.com/YannRaphalen/Hedges-Detection.TheBERTmodelwasfine-tunedonaNvidiaQuadroRTX8000GPU.BTables\n2170\n\n\n?(whether|if|is|that|it|this)?.*Subj.\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nNoNoYesNoNoNoYesYesYesYesYesBERT(PTE)\nexceptionally|forthemostpart|inamannerofspeaking|\nNoYesNoNoNoYesNoNoYesYesNoMLP(KDF)\nsomethingbetween|essentially|only).*Prop.\nTable8:Significancetableforthe3-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesYesYesYesYesNoYesYesYesNoYesMLP(KDF+PTE)\ntechnically|typically|virtually|approximately|\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nmuch|somewhat|exactly|almost|littlebit|quite|\nClass\nrelatively|roughly|sotosay|strictlyspeaking|\nNoNoYesNoNoYesNoYesYesYesYesLSTM(KDF)\nYesNoNoNoNo+TM\nTable10:Significancetableforthe3-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoNoYesYesYesYesYesYesNoYesYes\n.*(it)(looks|seems|appears)[,]?.*\",\".*(or|and)(that|something|stuff|soforth)Table7:Regexprulesusedfortheclassifier.\nRule-based\nRule-based\n.*(you(might|may)(believe|think)).*Subj.\nYesYesYesYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nYesYesNoYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nNoNoYesNoNoYesNoNoYesYesYesLSTM(KDF)\nYesYesNoYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(i|i\u2019m|you|it\u2019s)(am|are)(apparently|surely)[,]?.*Prop.\nNoNoYesNoNoYesNoYesYesYesYesLSTM(PTE)\nNoNoYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nNoNoYesYesNoYesYesYesYesYesNoLGBM(KDF)\nNoNoYesYesYesYesYesYesYesYesNoLGBM(KDF)\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nYesNoYesYesYesYesYesYesNoYesYes\nYesYesYesYesYesYesYesYesYesYesYesMLP(KDF+PTE)\nregular|regularly|actually|almost|asitwere|basically|\nNoYesYesNoYesYesYesNoYesYesNoMLP(PTE)\nRule(regexp)\n.*(i|i\u2019m|we)?(was|am|wasn\u2019t)??(not)?(sure|certain).*Subj.\n.*(accordingto|presumably).*Subj.\n(?!.*(be|been|was)likeexcuseme)((excuseme|sorry)[w,\u2019]+|[w,\u2019]+(excuseme|sorry))Prop.\n.*(i|you|we)haveto(check|look|verify).*Subj.\n.*(i\u2019m|i|we\u2019re)(am|are)??(apologize|sorry).*Apol.\nonthetallside|parexcellence|particularly|\n(?!what).*(i|we)?(don\u2019t|didn\u2019t|did)??(not)?\ninarealsense|inasense|inaway|largely|literally|\nYesYesYesNoNoNoYesYesYesYesYesBERT(PTE)\nModels\nModels\nModels\nYesYesYesNoNoYesNoNoYesYesYesLSTM(PTE)\nYesNoNoNoNo\nYesYesYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nSubj.\nTable9:Significancetableforthe4-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoYesNoNoNoYesNoNoYesYesNoMLP(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(ifeellikeyou).*Subj.\nlooselyspeaking|kinda|moreorless|mostly|often|\n.*(unlessi).*Apol.\nprobably|canbeviewas|crypto-|especially|essentially|\n.*(ifi\u2019mnotwrong|ifi\u2019mright|ifthat\u2019strue).*Subj.\nYesNoNoNoNo+POS\n(guess|guessed|thought|think|believe|believed|suppose|supposed)\n.*(just|alittle|maybe|actually|sortof|kindof|pretty\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nNoYesYesNoYesYesNoNoYesYesYesMLP(KDF)\nprettymuch|principally|pseudo-|quintessentially|\n2171\n\n\nFigure2:Absoluteaveragedfeaturecontribution,asindicatedbySHAP.Thelongerthebarisforonecolor,themorethefeatureisassociatedwiththeclassrepresentedbythatcolor.\nFigure3:Averagedcontributionoffeaturestothedetectionofthe\"Notindirect\"class,asindicatedbySHAP.Eachdotcorrespondstoaclassifiedclause.Areddotindicatesthatthefeatureispresentintheclause,whileabluedotindicatesthatthefeatureisabsent.Thefartherontherightthedotis,themorethefeaturecontributedtoitsclassificationasahedge.\n2172\n\n\nFigure4:Averagedcontributionoffeaturestothedetectionof\"Apologizers\",asindicatedbySHAP.\nFigure5:Averagedcontributionoffeaturestothedetectionof\"Propositionalhedges\",asindicatedbySHAP.\n2173\n\n\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nYesNoNoNoNo+TM\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nFigure6:Averagedcontributionoffeaturestothedetectionof\"Subjectivizers\",asindicatedbySHAP.\nModels\nYesNoNoNoNo\nTable11:Significancetableforthe4-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesNoNoNoNo+POS\n2174"}, {"question": " What types of features did not significantly improve the performance of the hedge classification model?", "answer": " Nonverbal behaviors and tutoring moves", "ref_chunk": "\"Youmightthinkaboutslightlyrevisingthetitle\":IdentifyingHedgesinPeer-tutoringInteractionsYannRaphalen1,Chlo\u00e9Clavel2,JustineCassell1,31InriaParis2LTCI,InstitutPolytechniquedeParis,Telecom-Paris3CarnegieMellonUniversityyann.raphalen.pro@gmail.com,justine@cs.cmu.edu,chloe.clavel@telecom-paris.frAbstractHedgesplayanimportantroleinthemanage-mentofconversationalinteraction.Inpeer-tutoring,theyarenotablyusedbytutorsindyads(pairsofinterlocutors)experiencinglowrapporttotonedowntheimpactofinstructionsandnegativefeedback.Pursuingtheobjectiveofbuildingatutoringagentthatmanagesrap-portwithstudentsinordertoimprovelearning,weusedamultimodalpeer-tutoringdatasettoconstructacomputationalframeworkforiden-tifyinghedges.Wecomparedapproachesre-lyingonpre-trainedresourceswithothersthatintegrateinsightsfromthesocialsciencelitera-ture.Ourbestperformanceinvolvedahybridapproachthatoutperformstheexistingbase-linewhilebeingeasiertointerpret.Weemployamodelexplainabilitytooltoexplorethefea-turesthatcharacterizehedgesinpeer-tutoringconversations,andweidentifysomenovelfea-tures,andthebenefitsofsuchahybridmodelapproach.1IntroductionRapport,mostsimplydefinedasthe\u201c...relativeharmonyandsmoothnessofrelationsbetweenpeo-ple...\u201d(Spencer-Oatey,2005),hasbeenshowntoplayaroleinthesuccessofactivitiesasvariedaspsychotherapy(Leach,2005)andsurveyinterview-ing(LuneandBerg,2017).Inpeer-tutoring,rap-port,asmeasuredbytheannotationofthinslicesofvideo,hasbeenshowntobebeneficialforlearningoutcomes(Zhaoetal.,2014;SinhaandCassell,2015).Thelevelofrapportrisesandfallswithconversationalstrategiesdeployedbytutorsandtuteesatappropriatetimes,andasafunctionofthecontentofpriorturns.Thesestrategiesincludeself-disclosure,referringtosharedexperience,and,onthepartoftutors,givinginstructionsinanindirectmanner.Someworkhasattemptedtoautomaticallydetectthesestrategiesintheserviceofintelligenttutors(Zhaoetal.,2016a),butonlyafewstrate-gieshavebeenattempted.Otherworkhascon-centratedona\"socialreasoningmodule\"(Romeroetal.,2017)todecidewhichstrategiesshouldbegeneratedinagivencontext,butindirectnesswasnotamongthestrategiestargeted.Inthispaper,wefocusontheautomaticclassificationofonespe-cificstrategythatisparticularlyimportantforthetutoringdomain,andthereforeimportantforintel-ligenttutors:hedging,asub-partofindirectnessthat\"softens\"whatwesay.Thisworkispartofalargerresearchprogramwiththelong-termgoalofautomaticallygeneratingindirectnessbehaviorsforatutoringagent.\nFigure1:Amockconversationdisplayingeachtypeofhedgedformulation.AccordingtoBrownandLevinson(1987),hedgesarepartofthelinguistictoolsthatinterlocu-torsusetoproducepoliteness,bylimitingthefacethreattotheinterlocutor(basicallybylimitingtheextenttowhichtheinterlocutormightexperienceembarrassmentbecauseofsomekindofpoorper-formance).Anexampleis\"that\u2019skindofawronganswer\".Hedgesarealsofoundwhenspeakerswishtoavoidlosingfacethemselves,forexam-plewhensaying(\"IthinkImighthavetoadd6.\").Madaioetal.(2017)foundthatinapeer-tutoringtask,whenrapportbetweeninterlocutorsislow,tu-teesattemptedmoreproblemsandcorrectlysolvedmoreproblemswhentheirtutorshedgedinstruc-\n2160 Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1: Long Papers, pages 2160 - 2174 May 22-27, 2022 c(cid:13)2022 Association for Computational Linguistics\n\n\ntions,whichlikewisepointstowardsa\"mitigationoffacethreat\"function.Hedgescanalsobeasso-ciatedwithanonverbalcomponent,forexampleavertedeyegazeduringcriticism(BurgoonandKoper,1984).Hedgesarenot,however,alwaysap-propriate,asin\"Ikindofthinkit\u2019srainingtoday.\"whentheinterlocutorscanbothseerain(althoughitmightbetakenashumorous).Thesefactsabouthedgesmotivateawaytoautomaticallydetectthemand,ultimately(althoughnotinthecurrentwork)alsogeneratethem.Inbothcaseswefirsthavetobeabletocharacterizethemusinginterpretablelinguisticfeatures,whichiswhatweaddressinthecurrentpaper.Thus,intheworkdescribedhere,basedonlinguisticdescriptionsofhedges(BrownandLevinson,1987;Fraser,2010),webuiltarule-basedclassifier.Weshowthatthisclassifierincombinationwithadditionalmultimodalinter-pretablecontext-dependentfeaturessignificantlyimprovestheperformanceofamachinelearningmodelforhedges,comparedtoalessinterpretabledeeplearningbaselinefromGoeletal.(2019)us-ingwordembeddings.Wealsoreliedonamachinelearningmodelexplanationtool(LundbergandLee,2017)toinvestigatethelinguisticfeaturesrelatedtohedgesinthecontextofpeer-tutoring,primarilytoseeifwecoulddiscoversurprisingfeaturesthattheclassificationmodelwouldassociatetohedgesinthiscontext,andwedescribethosebelow.Thecodeofthemodelsdescribedinthepaperisalsoprovided.12RelatedworkHedges:AccordingtoFraser(2010),hedgingisarhetoricalstrategythatattenuatesthestrengthofastatement.Onewaytoproduceahedgeisbyalteringthefullsemanticvalueofaparticu-larexpressionthroughPropositionalhedges(alsocalledApproximatorsinPrinceetal.(1982)),asin\"Youarekindofwrong,\"thatreduceprototypical-ity(i.eaccuracyofthecorrespondencebetweenthepropositionandtherealitythatthespeakerseekstodescribe).Propositionalhedgesarerelatedtofuzzylanguage(Lakoff,1975),andthereforetotheproductionofvagueness(Williamson,2002)anduncertainty(Vincze,2014).AsecondkindareRelationalHedges(alsocalledShieldsinPrinceetal.(1982)),suchas\u201cIthinkthatyouarewrong.\u201dor\u201cThedoctorwantsyoutostopsmoking.\u201d,conveyingthatthepropositionis\n1https://github.com/AnonymousHedges/HedgeDetectionconsideredbythespeakerassubjective.Inafurthersub-division,AttributionShields,asin\"Thedoc-torwantsyou...\",theinvolvementofthespeakerinthetruthvalueofthepropositionisnotmadeexplicit,whichallowsspeakersnottotakeastance.Asdescribedabove,Madaioetal.(2017)foundthattutorswhoshowedlowerrapportwiththeirtuteesusedmorehedgedinstructions(theyalsoemployedmorepositivefeedback),howeverthiswasonlythecasefortutorswithagreaterbeliefintheirabilitytotutor.Tuteesinthiscontextsolvedmoreproblemscorrectlywhentheirtutorshedgedinstructions.Noeffectofhedgingwasfoundfordyads(pairsofinterlocutors)withgreatersocialcloseness.However,theauthorsdidnotlookatthespecificlinguisticformstheseteenagersused.Rowland(2007)alsodescribestherolethathedg-ingplaysinthisagegroup,showingthatstudentsusebothrelational(\"IthinkthatJohnissmart.\")andpropositional(\"Johniskindofsmart.\")hedgesformuchthesameshieldingfunctionofdemon-stratinguncertainty,tosavethemfromtheriskofembarrassmentiftheyarewrong.TheauthorobservedthatteensusedfewAdaptors(kindof,somewhat)andpreferredtouseRounders(around,closeto).However,thisstudywasperformedwithanadultandtwochildren,possiblybiasingthere-sultsduetotheparticipationoftheadultinvestiga-tor.Hedgeshavebeenincludedinvirtualtutoringagentsbeforenow.(Howardetal.,2015)integratedhedgesinatutoragentforundergraduatesinCS,asawaytoencouragethestudenttotaketheinitiative.Hedgeshavealsobeenusedasawayofintegrat-ingBrownandLevinson\u2019spolitenessframework(Wangetal.,2008;Schneideretal.,2015)invir-tualtutoringagents.Resultswerenotbrokenoutbystrategy,butpolitenessingeneralwasshowntopositivelyinfluencemotivationandlearning,incertainconditions.Computationalmethodsforhedgedetection:Anumberofstudieshavetargetedthedetectionofhedgesanduncertaintyintext(MedlockandBriscoe,2007;GanterandStrube,2009;Tangetal.,2010;Velldal,2011;Szarvasetal.,2012),partic-ularlyfollowingtheCoNLL2010datasetrelease(Farkasetal.,2010).However,thisworkisnotasrelatedtohedgesinconversation,asitfocusesonaformalandacademiclanguageregister(Hy-land,1998;Varttala,1999).AsnotedbyProkofievaandHirschberg(2014),thefunctionsofhedgesaredomain-andgenre-dependent,thereforethisbias\n2161\n\n\ntowardsformalityimpliesthattheexistingworkmaynotadaptwelltothedetectionofhedgesinconversationbetweenteenagers.Aconsequenceisthattheexistingworkdoesnotconsidertermslike\"Ithink,\"sinceopinionsrarelyappearinanaca-demicwritingdataset.Instructionsarealsoalmostabsent(\"Ithinkyouhavetoaddtentobothsides.\"),astronglimitationforthestudyofconversationalhedgessinceitisinrequests(includingtutoringin-structions)thatindirectformulationsmostlyoccuraccordingtoBlum-Kulka(1987).ProkofievaandHirschberg(2014)alsonotethatitisdifficulttodetecthedgesbecausethewordpatternsassociatedwiththemhaveothersemanticandpragmaticfunc-tions:considering\"Ithinkthatyouhavetoaddxtobothsides.\"vs\"Ithinkthatyouareanidiot.\",itisnotclearthattheseconduseof\"Ithinkthat\"isanhedgemarker.Theyadvocateusingmachinelearningapproachestodealwiththeambiguityofthesemarkers.Workingonaconversationaldataset,Ulinskietal.(2018)builtacomputationalsystemtoassessspeakercommitment(i.e.atwhichpointthespeakerseemsconvincedbythetruthvalueofastatement),inparticularbyrelyingonarule-baseddetectionsystemforhedges.Comparedtothatwork,ourrule-basedclassificationmodelisdirectlydetectinghedgeclasses,andweemploythepredictionsoftherule-basedmodelasafeatureforstrongermachinelearningmodels,designedtolessentheimpactoftheimbalancebetweenclasses.Wealsoconsiderapologieswhentheyserveamit-igationfunction(wethencallthemApologizers),aswasdonebytheauthorsofourcorpus,andwealsousethetermsubjectivizersasdefinedbelow,tobeabletocomparedirectlywiththepreviousworkcarriedoutonthiscorpus.Asfarasweknow,onlyGoeletal.(2019)haveworkedwithapeer-tutoringdataset(thesameonethatwealsouse),andtheyachievedtheirbestclassificationresultbyemployinganAttention-CNNmodel,inspiredbyAdelandSch\u00fctze(2017).3ProblemstatementWeconsiderasetDofconversationsD=(c1,c2,...,c|D|),whereeachconversationiscom-posedofasequenceofindependentsyntacticclausesci=(u1,u2,...,uM),whereMisthenumberofclausesintheconversation.Notethattwoconsecutiveclausescanbeproducedbythesamespeaker.Eachclauseisassociatedwithauniquelabelcorrespondingtothediffer-enthedgeclassesdescribedinTable1:yi\u2208C={PropositionalHedges,Apologizers,Subjec-tivizers,Nothedged}.Finally,anutteranceuicanberepresentedasavectoroffeaturesX=(x1,x2,...,xN),whereNrepresentsthenumberoffeaturesweusedtodescribeaclause.Ourfirstgoalistodesignamodelthatcorrectlypredictsthelabelyiassociatedtoui.Itcanbeunderstoodasthefollowingresearchquestion:RQ1:\"Whichmodelsandfeaturescanbeusedtoautomaticallycharacterizehedgesinapeer-tutoringinteraction?\"Oursecondgoalistoidentify,foreachhedgeclass,thesetoffeaturesFclass={fk},k\u2208[1,N]sortedbyfeatureimportanceintheclassificationofclass.Itcorrespondstothefollowingresearchquestion:RQ2:\"Whatarethemostimportantlinguisticfeaturesthatcharacterizeourhedgeclassesinapeer-tutoringsetting?\"4Methodology4.1CorpusDatacollection:Thedialoguecorpususedherewascollectedaspartofalargerstudyontheeffectsofrapport-buildingonreciprocalpeertutoring.24Americanteenagers(meanage=13.5,min=12,max=15),halfmaleandhalffemale,cametoalabwherehalfoftheparticipantswerepairedwithasame-age,same-genderfriend,andtheotherhalfpairedwithastranger.Theparticipantswereassignedtoatotalof12dyadsinwhichthepar-ticipantsalternatedtutoringoneanotherinlinearalgebraequationsolvingfor5weeklyhour-longsessions,foratotalcorpusofnearly60hoursofface-to-faceinteractions.Eachsessionwasstruc-turedsuchthatthestudentsengagedinbriefsocialchitchatinthebeginning,thenoneofthestudentswasrandomlyassignedtotutortheotherfor20minutes.Theythenengagedinanothersocialpe-riod,andconcludedwithasecondtutoringperiodwheretheotherstudentwasassignedtheroleoftutor.Audioandvideodatawererecorded,tran-scribed,andsegmentedforclause-leveldialogueannotation,providingnearly24000clauses.Non-speechsegments(notablyfillersandlaughter)weremaintained.Becauseoftemporalmisalignmentforpartsofthecorpus,manyparaverbalphenomena,suchasprosody,wereunfortunatelynotavailabletous.SinceouraccesstothedatasetiscoveredbyaNon-DisclosureAgreement,itcannotbereleased\n2162\n\n\n2https://github.com/AnonymousHedges/HedgeDetectionclasses.4.2FeaturesLabelfromrule-basedclassifier(LabelRB):Weusetheclasslabelpredictedbytherule-basedclas-sifierdescribedinSection4.3asafeature.Ourhypothesisisthatthemachinelearningmodelcanusethisinformationtocounterbalancetheclassimbalance.Totakeintoaccountthefactthatsomerulesaremoreefficientthanothers,weweightedtheclasslabelresultingfromtherule-basedmodelbytheprecisionoftherulethatgeneratedit.Unigramandbigram:Wecountthenumberofoccurrencesofunigramsandbigramsofthecorpusineachclause.Weusedthelemmaofthewordsforunigramsandbigramsusingthenltklemmatizer(Loper,2002)andselectedunigramsandbigramsthatoccurredinthetrainingdatasetatleastfiftytimes.Thegoalwastoinvestigate,withabottom-upapproach,towhatextenttheuseofcertainwordscharacterizeshedgeclassesintutoring.InSection5weexaminetheoverlapbetweenthesewordsandthoseaprioriidentifiedbytherules.Part-of-speech(POS):Hedgeclassesseemtobeassociatedwithdifferentsyntacticpatterns:forex-ample,subjectivizersmostoftencontainapersonalpronounfollowedbyaverb,asin\"Iguess\",\"Ibelieve\",\"Ithink\".WethereforeconsideredthenumberofoccurrencesofPOS-Tagn-grams(n=1,2,3)asfeatures.WeusedthespaCyPOS-taggerandconsideredPOSunigrams,bigramsandtri-gramsthatoccuratleast10timesinthetrainingdataset.LIWC:LinguisticInquiryandWordCount(LIWC)(Pennebakeretal.,2015)isstandardsoft-wareforextractingthecountofwordsbelongingtospecificpsycho-socialcategories(e.g.,emotions,religion).Ithasbeensuccessfullyusedinthede-tectionofconversationalstrategies(Zhaoetal.,2016a).Wethereforecountthenumberofoccur-rencesofallthe73categoriesfromLIWC.Tutoringmoves(TM):Intelligenttutoringsys-temsrelyonspecifictutoringmovestosuccess-fullyconveycontent(asdohumantutors).Wethereforelookedatthelinkbetweenthetutoringmoves,asannotatedinMadaioetal.(2017),andhedges.Fortutors,thesemovesare(1)instruc-tionaldirectivesandsuggestions,(2)feedback,and(3)affirmations,mostlyexplicitreflectionsontheirpartners\u2019comprehension,whilefortutees,theyare(1)questions,(2)feedbacks,and(3)affirmations,\npublicly.Howevertheoriginalexperimenters\u2019In-stitutionalReviewBoard(IRB)approvalallowsustoview,annotate,andusethedatatotrainmodels.Thisalsoallowsustoprovidealinktoapixe-latedvideoexampleintheGitHubrepositoryoftheproject2.Dataannotation:Thedatasetwaspreviouslyan-notatedbyMadaioetal.(2017),followingananno-tationmanualthatusedhedgeclassesderivedfromRowland(2007)(seeTable1).Onlythetaskperi-odsoftheinteractionswereannotated.Comparingtheannotationswiththeclassesmentionedintherelatedworksection,SubjectivizerscorrespondtoRelationalhedges(Fraser,2010),PropositionalhedgesandExtenderscorrespondtoApproxima-tors(Princeetal.,1982)withtheadditionofsomediscoursemarkerssuchasjust.ApologizersarementionedaslinguistictoolsrelatedtonegativepolitenessinBrownandLevinson(1987).Krippen-dorff\u2019salphaobtainedforthiscorpusannotatedbyfourcoderswasover0.7forallclasses(denotinganacceptableinter-coderreliabilityaccordingtoKrippendorff(2004)).Thedatasetiswidelyim-balanced,withmorethan90%oftheutterancesbelongingtotheNothedgedclass.Inreviewingthecorpusandtheannotationman-ual,however,wenoticedtwoissues.First,theannotationoftheExtendersclasswasinconsis-tent,leadingtotheExtendersandPropositionalhedgesclassescarryingsimilarsemanticfunctions.WethereforemergedthetwoclassesandgroupedutteranceslabeledasExtendersandthoselabeledasPropositionalhedgesundertheheadingofPropositionalhedges.Second,theannotationofclausescontainingthetokens\"just\"and\"would\"(twotermsoccurringfrequentlyinthedatasetthatarekeycomponentsofPropositionalHedgesandSubjectivizersbutthatarenotinfacthedgesinallcases)wasalsoinconsistent,leadingtovirtuallyallclauseswiththosetwotokensbeingconsideredhedges.Wethereforere-consideredalltheclausesassociatedwithanyofthehedgeclasses,aswellasalltheclausesinthe\"Nothedged\"classthatcontained\"just\"or\"would\".There-annotationwascarriedoutbytwoannotatorswhoachievedaKrippendorff\u2019salphainter-raterreliabilityof.9orbetterforApologizers,Subjectivizers,andPropo-sitionalhedgesbeforeindependentlyre-annotatingtherelevantclauses.Anexampleofare-annotationwasremoving\"Iwouldkillyou!\"fromthehedge\n2163\n\n\nProp.hedgesApologizersSubjectivizersNothedgedTotal\nTable1:Definitionoftheclasses\nFeaturesnameAutomaticextractionVectorsize\nSubjectivizersWordsthatreduceintensityorcertainty\u201cSothenIwoulddividebytwo.\u201dApologizersApologiesusedtosoftendirectspeechacts\u201cOhsorrysixb.\u201dPropositionalhedgesQualifyingwordstoreduceintensityorcertaintyofutterances\u201cIt\u2019sactuallyeight.\u201dExtendersWordsusedtoindicateuncertaintybyreferringtovaguecategories\u201cIt\u2019llbethenumberxorwhatevervariableyouhave.\u201d\n12101286262119223156\nTable2:Distributionoftheclasses\nClassDefinitionExample\nRule-basedlabelYes4UnigramYes~250BigramYes~250POSYes~1200LIWCYes73NonverbalNo24TutoringmovesNo6Total~1800\nTable3:Listofautomaticallyextractedandmanuallyannotatedfeatureswiththeirsize.mostlytentativeanswers.Nonverbalandparaverbalbehaviors:AsinGoeletal.(2019),weincludedthenonverbalandpar-averbalbehaviorsthatarerelatedtohedges.Specif-ically,weconsiderlaughterandsmiles,thathavebeenshowntobeeffectivemethodsofmitiga-tion(Warner-Garcia,2014),cut-offsindicatingself-repairs,fillerslike\"Um\",gazeshifts(annotatedas\u2019GazeatPartner\u2019,\u2019GazeattheMathWorksheet\u2019,and\u2019Gazeelsewhere\u2019),andheadnods.Eachfea-turewaspresenttwiceinthefeaturevector,onetimeforeachinterlocutor.Inter-raterreliabilityfornonverbalbehaviorwas0.89(asmeasuredbyKrippendorff\u2019salpha)foreyegaze,0.75forsmilecount,0.64forsmiledurationand0.99forheadnod.Laughterisalsoreportedinthetranscriptatthewordlevel.Weseparatethetutor\u2019sbehaviorsfromthoseofthetutee.ThecollectionprocessforthesebehaviorsisdetailedfurtherinZhaoetal.(2016b).Theclause-levelfeaturevectorwasnormalizedbythelengthoftheclause(exceptfortherule-basedlabel).Thislengthwasalsoaddedasafeature.Table3presentsanoverviewofthefinalfeaturevector.4.3ClassificationmodelsTheclassificationmodelsusedarepresentedhereaccordingtotheirlevelofintegrationofexternallinguisticknowledge.Rule-basedmodel:OnthebasisoftheannotationmanualusedtoconstructthedatasetfromMadaioetal.(2017),andwithdescriptionsofhedgesfromRowland(2007),Fraser(2010)andBrownandLevinson(1987),weconstructedarule-basedclas-sifierthatmatchesregularexpressionsindicativeofhedges.TherulesaredetailedinTable7intheAppendix.LGBM:Sincehedgesareoftencharacterizedbyexplicitlexicalmarkers,wetestedtheassumptionthatamachinelearningmodelwithaknowledge-drivenrepresentationforclausescouldcompetewithaBERTmodelinperformance,whilebeingmuchmoreinterpretable.WereliedonLightGBM,anensembleofdecisiontreestrainedwithgradi-entboosting(Keetal.,2017).Thismodelwasselectedbecauseofitsperformancewithsmalltrainingdatasetsandbecauseitcanignoreunin-formativefeatures,butalsoforitstrainingspeedcomparedtoalternativeimplementationsofgradi-entboostingmethods.Multi-layerperceptron(MLP):Asasimplebase-line,webuiltamulti-layerperceptronusingthreesetsoffeatures:apre-trainedcontextualrepre-sentationoftheclause(SentBERT;ReimersandGurevych(2019));theconcatenationofthiscon-textualrepresentationoftheclauseandarule-basedlabel(notrelyingonthepreviousclauses);andfinallytheconcatenationofallthefeaturesmen-tionedinsection4.2,withoutthecontextualizedrepresentation.LSTMoverasequenceofclauses:Sinceweareworkingwithconversationaldata,wealsowantedtotestwhethertakingintoac-countthepreviousclauseshelpstodetectthetypeofhedgeclassinthenextclause.Formally,wewanttoinferyiusingyi=maxy\u2208ClassesP(y|X(ui),X(ui\u22121),...,X(ui\u2212K)),whereKisthenumberofpreviousclausesthatthemodelwilltakeintoaccount.The\n2164\n\n\nMLPmodelpresentedaboveinfersyiusingyi=maxy\u2208ClassesP(y|X(ui)),thereforeadifferenceofperformancebetweenthetwomodelswouldbeasignthatusinginformationfromthepreviousclausescouldhelptodetectthehedgedformulationinthecurrentclause.WetestedaLSTMmodelwiththesamerepresentationsforclausesasfortheMLPmodel.CNNwithattention:Goeletal.(2019)estab-lishedtheirbestperformanceonhedgedetec-tionusingaCNNmodelwithadditiveattentionoverword(andnotclause)embeddings.Con-trarytotheMLPandLSTMmodelsmentionedabove,thismodeltriestoinferyiusingyi=maxy\u2208ClassesP(y|g(w0),g(w1),...,g(wL)),withLrepresentingthemaximumclauselengthweal-low,andgrepresentingafunctionthatturnsthewordwj,j\u2208[0,L]intoavectorrepresentation(formoredetails,pleaseseeAdelandSch\u00fctze(2017)).BERT:Tobenefitfromdeepsemanticandcon-textualrepresentationsoftheutterances,wealsofine-tunedBERT(Devlinetal.,2019)onourclas-sificationtask.BERTisapre-trainedTransformersencoder(Vaswanietal.,2017)thathassignificantlyimprovedthestateoftheartonanumberofNLPtasks,includingsentimentanalysis.Itproducesacontextualrepresentationofeachwordinasen-tence,makingitcapableofdisambiguatingthemeaningofwordslike\"think\"or\"just\"thatarerepresentativeofcertainclassesofhedges.BERT,however,isnotablyhardtointerpret.4.4AnalysistoolsLookingatwhichfeaturesimprovetheperfor-manceofourclassificationmodelstellsuswhetherthesefeaturesareinformativeornot,butdoesnotexplainhowthesefeaturesareusedbythemod-elstomakeagivenprediction.Wethereforepro-ducedacomplementaryanalysisusinganinter-pretabilitytool.Asdemonstratedby(LundbergandLee,2017),LightGBMinternalfeatureimpor-tancescoresareinconsistentwithboththemodelbehaviorandhumanintuition,soweinsteadusedamodel-agnostictool.SHAP(LundbergandLee,2017)assignstoeachfeatureanimportancevalue(calledShapleyvalues)foraparticularpredictiondependingontheextentofitscontribution(ade-tailedintroductiontoShapleyvaluesandSHAPcanbefoundinMolnar(2020)).SHAPisamodel-agnosticframework,thereforethevaluesassoci-atedwithasetoffeaturescanbecomparedacrossmodels.ItshouldbenotedthatSHAPproducesexplanationsonacase-by-casebasis,thereforeitcanbothprovidelocalandglobalexplanations.FortheGradientBoostingmodel,weuseanadaptedversionofSHAP(Lundbergetal.,2018),calledTreeSHAP.5Experimentsandresults5.1ExperimentalsettingTodetectthebestsetoffeatures,weusedLight-GBMandproceededincrementally,byaddingthegroupoffeatureswethoughttobemostlikelyasso-ciatedwithhedges.Wedidnotconsidertheriskofrelyingonasub-optimalsetoffeaturesthroughthisprocedurebecauseofthestrongabilityofLight-GBMtoignoreuninformativefeatures.Weusethisincrementalapproachasawaytotestourintuitionabouttheperformativityofgroupsoffeatures(i.e.doesaddingafeatureimprovetheperformanceofthemodel)withregardtothetaskofclassifica-tion.Tocompareourmodels,wetrainedthemonthe4-classtask,andlookedattheaverageoftheweightedF1-scoresforthethreehedgeclasses(i.e.howwellthemodelsinferminorityclasses)thatwereporthereas\"3-classes\",andattheaverageoftheweightedF1-scoresforthe4classes,thatwereportas\"4-classes\".DetailsofthehyperparametersandexperimentalsettingsareprovidedinAppendixA.5.2ModelcomparisonandfeatureanalysisOverallresults:Table4presentstheresultsob-tainedbythe6modelspresentedinSection4.3forthemulti-classproblem.Bestperformance(F1-scoreof79.0)isobtainedwithLightGBMlever-agingalmostallthefeatures.Intheappendix(seeTable8andTable9)weindicatetheconfidenceintervalstorepresentthesignificanceofthediffer-encesbetweenthemodels.First,andperhapssurprisingly,wenoticethattheuseof\"Knowledge-Driven\"featuresbasedonrulesbuiltfromlinguisticknowledgeofhedgesintheLightGBMmodeloutperformstheuseofpre-trainedembeddingswithinafine-tunedBERTmodel(79.0vs.70.6),andintheneuralbaselinefrom(Goeletal.,2019)(79.0vs64.5).ThelowscoresobtainedbytheLGBM,LSTMandMLPmodelswithpre-trainedsentenceem-beddingsversusKnowledge-Drivenfeaturesmightsignalthatthewordpatternscharacterizinghedgesarenotsalientintheserepresentations(i.e.the\n2165\n\n\n68.5(1.6)35.8(3.1)64.8(1.1)Attention-CNN(3-classes)\n65.1(5.7)39.8(8.0)65.2(5.1)BERT(3-classes)\n\u220570.6(2.3)\u2205LGBM(3-classes)\n79.0(1.3)35.0(2.2)70.1(1.4)\nRule-based(3-classes)\n94.7\u2205\u2205MLP(4-classes)\n3Notethatthereisstrongredundancybetweensomefea-turesofLIWCandthespaCyPOStaggerthatbothproducea\"Pronoun\"category,usingalexiconinthefirstcase,andaneuralinferenceinthesecond.\n\u220594.9(0.4)\u2205LGBM(4-classes)\nRule-based(4-classes)\nTable4:AveragedweightedF1-scores(andstandarddeviation)forthethreeminorityclassesandforthe4classes,forallmodels.\"KD\"standsfor\"Knowledge-Driven\",meaningthatthefeaturesarederivedfromlexicon,n-grammodelsandannotations.distancebetween\"Ithinkyoushouldadd5.\"and\"Youshouldadd5.\"isshort.).KDFeaturesseemtoprovideabetterseparabilityoftheclasses.ThecombinationofKDfeaturesandPre-trainedem-beddingsdoesnotsignificantlyimprovetheperfor-manceofthemodelscomparedtotheKDFeaturesonly,whichsuggeststhattheinformationfromthePre-trainedembeddingsisredundantwiththeonefromtheKDFeatures.Thisresultmaybeduetothehighdimensionalityoftheinputvector(868withPCAontheKDFeatures;2500otherwise).Asecondfindingisthattheuseofgradientboost-ingmodelsontopofrule-basedclassifiersbettermodelsthehedgeclasses.Theothermachinelearn-ingmodelsdidnotprovetobeaseffective,exceptforBERT.FeatureanalysisusingLightGBM:Usingthebestperformingmodel,Table5showstheroleofeachfeaturesetinthepredictiontask.ThesignificanceofthedifferencesisshowninTable10andTable11.Comparedtotherule-basedmodel,theintroductionofn-gramssignificantlyimprovedtheperformanceofourclassifier,suggestingthatsomelexicalandsyntacticinformationdescribingthehedgeclasseswasnotpresentintherule-basedmodel.LookingatTable5,wedonotobservesignificantdifferencesbetweentheLGBMmodelusingonlythelabelrulebased+(1-gramsand2-grams)andthemodelsin-corporatingmorefeatures.Tooursurprise,neitherthetutoringmovesnorthenonverbalfeaturessig-nificantlyimprovedtheperformanceofthemodel.The2featureswereincludedtoindexthespecificpeertutoringcontextofthesehedges,sothisindi-catesthatinfutureworkwemightwishtoapplythecurrentmodeltoanothercontextofusetoseeifthismodelofhedgesismoregenerallyapplicablethanweoriginallythought.Bycombiningthisresultwiththeincreasedperformanceofthemodelus-ingKnowledge-Driven(i.e.explicit)featurescom-paredtopre-trainedembeddings,itwouldseemthathedgesareaboveallalexicalphenomenon(i.e.producedbyspecificlexicalelements).5.3In-depthanalysisoftheinformativefeaturesWetrainedtheSHAPexplanationmodelsonLight-GBMwithallfeatures.Themostinformativefea-tures(inabsolutevalue)foreachclassareshowninTable6,andtheplotsbyclassarepresentedintheAppendix.Themostimportantfeaturesseemtobetherule-basedlabels,whichappearinatleastthefourthpositionforthreeclasses(seeTable6),andinthefirstpositionforPropositionalHedgesandNothedgedclasses.Surprisingly,theRule-Basedlabeldoesnotappearinthetop20featuresforApologizers.However,giventhattheclassrarelyappearsinthedata,therulesseldomactivate,sothefeaturemaysimplybeinformativeforaverysmallnumberofclauses.Unigrams(Oh,Sorry,just,Would,andI)arealsopresentinthe5top-rankedfeatures.Thisconfirmsthefindingsmen-tionedinrelatedworkforthecharacterizationofthedifferenthedgeclasses(justwithPropositionalHedges,sorrywithApologizer,IwithSubjectiviz-ers).ThepresenceofOhalsohashighimportanceforthecharacterizationofApologizer(n=2),asillustratedinexamplessuchas\"Ohsorry,that\u2019snine.\".Wenotethattheoccurrencesof\"Ohsorry\"asastand-aloneclausewereexcludedbyourrule-basedmodelbecausetheydonotcorrespondtoanapologizer(theycannotmitigatethecontentofapropositionifthereisnopropositionassociated).Thisexampleillustratestheinterestofamachinelearningmodelapproachtodisambiguatethefunc-tionofconventionalnon-propositionalphraseslike\"Ohsorry\".Inaddition,SHAPhighlightstheimportanceofnovelfeatureswhosefunctionwasnotidentifiedinthehedgesliterature:(i)whatLIWCclassifiesasinformalwordsbutthataremostlyinterjectionslikeahandoharestronglyassociatedwithApol-ogizer,asaredisfluencies(n=12);(ii)theuseofPOStagsseemstobeveryrelevantforcharac-terizingthedifferentclasses(2-gramofPOStagfeatures3occurinthetop-rankedfeaturesofallthe\n\u220564.5(3.0)\u2205LSTM(3-classes)\nKDFeat.(KDF)Pre-TrainedEmb.(PTE)KDF+PTE\n\u220594.4(0.2)\u2205LSTM(4-classes)\n96.7(0.2)91.0(0.2)95.4(0.2)\n94.8(0.3)89.7(0.4)93.9(0.4)Attention-CNN(4-classes)\n93.9(1.4)89.1(1.4)94.1(1.2)BERT(4-classes)\n67.6\u2205\u2205MLP(3-classes)\nModels\n2166\n\n\n95.0(0.2)96.5(0.3)96.5(0.2)96.7(0.2)96.6(0.4)96.7(0.3)\n3-classes\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\n68.8(0.8)78.2(1.6)78.1(1.3)79.0(1.3)78.5(2.4)78.7(1.8)\nTable5:AveragedweightedF1-scoresforthethreeclassesofhedgesandthefourclasses,withanadditiveintegrationofKDFfeaturesintheLightGBMmodel.Thestandarddeviationiscomputedacrossfivefolds.\n4-classes\nModels\nRankApologizerSubjectivizersProp.HedgesNothedged\n1Functionwords(LIWC)\"I\"ClasslabelClasslabel2\"Oh\"(LIWC)\"Yeah\"\"Would\"\"Would\"3\"Sorry\"Noun(POS)\"Just\"\"Yeah\"4Affect(LIWC)ClasslabelFunctionword(LIWC)Noun(POS)5ClauselengthCognitiveprocess(LIWC)Netspeak(LIWC)Cognitiveprocess(LIWC)Table6:Mostimportantclause-levelfeaturesforLightGBMaccordingtotheSHAPanalysis.classes(seeFiguresintheAppendix).Itmeansthattherearesomerecurringsyntacticpatternsineachclass;(iii)Regardingtheutterancesize,aclauseshorterthanthemeanisweaklyassociatedwithdirectness(n=17)whilealongerclausesuggeststhatitcontainsaSubjectivizer(n=6).Apologizersarecharacterizedbyameanclauselength(n=5),withfewvariationsfromit;(iv)Tutoringmovesarenotstrongpredictorsofanyclasses:\"Affirma-tionfromtutor\"istheonlyfeatureappearingasapredictorofPropositionalhedges(n=20).ThisisconsistentwiththefeatureanalysisinTable5,suggestingthattutoringmovesdonotsignificantlyimprovetheperformanceoftheclassifier;(v)Non-verbalbehaviorsdonotappearasimportantfea-turesfortheclassification.Thisiscoherentwithresultsfrom(Goeletal.,2019).Notethatprosodymightplayaroleindetectinginstructionsthattrailoff,but,asdescribed,paraverbalfeatureswerenotavailable;(vi)Wouldplaysanimportantroleintheproductionofhedges,asitisstronglyassociatedtoPropositionalhedges(n=2).Itisinterestingtonotethat,whendesigningtherule-basedclassifier,wesawitdecreaseinperformancewhenwestartedtoincludewouldinourregularexpressionpatterns,probablybecausetheformishardtodisambiguateforadeterministicsystem.WhileexploringtheShapleyvaluesassociatedtoeachclause,weobservedthatfeaturesliketutoringmovesareextremelyinformativeforaverysmallnumberofclauses(thereforenotsignificantlyinflu-encingtheoverallperformanceoftheprediction),andmoreorlessnotinformativefortherest.Infer-ringtheglobalimportanceofafeatureasameanacrosstheshapleyvaluesinthedatasetmaynotbetheonlywaytoexplorethebehaviorofgradi-entboostingmethods.ItmightbemoreusefultoclusterclausesbasedontheimportancethatSHAPgivestothatfeatureinitsclassification,asthiscouldhelpdiscoversub-classesofhedgesthataredifferentiatedfromtherestbytheirinteractionwithaspecificfeature(inthewaythatsomeApologiz-ersarecharacterizedbyan\"oh\").Wealsonotethattheexplanationmodelissensitivetospuri-ouscorrelationsinthedataset,causedbythesmallrepresentationofsomeclass:forexample,\"nine\"(n=7)and\"four\"(n=20)arepositivepredictorsofApologizers.6ConclusionandfutureworkThroughourclassificationperformanceexperi-ments,weshowedthatitispossibletousema-chinelearningmethodstodiminishtheambigu-ityofhedges,andthatthehybridapproachofus-ingrule-basedlabelfeaturesderivedfromsocialscience(includinglinguistics)literaturewithinamachinelearningmodelhelpedsignificantlytoin-creasethemodel\u2019sperformance.Nonverbalbehav-iorsandtutoringmovesdidnotprovideinformationatthesentencelevel;boththeperformanceofthemodelandthefeaturecontributionanalysissug-gestedthattheirimpactonthemodeloutputwasnotstrong.ThisisconsistentwithresultsfromGoeletal.(2019).However,infutureworkwewouldliketoinvestigatethepotentialofmultimodalpat-ternswhenweareabletobettermodelsequentiality(e.g.,negativefeedbackfollowedbyasmile).Re-gardingtheSHAPanalysis,mostofthefeaturesthatareconsideredasimportantarecoherentwiththedefinitionoftheclasses(Iforsubjectivizers,sorryforapologizers,justforpropositionalhedges).However,wediscoveredthatfeatureslikeutterance\n2167\n\n\nsizecanalsoserveasindicatorsofcertainclassesofhedges.AlimitationofSHAPisthatitmakesafeatureindependenceassumption,whichpromptstheexplanatorymodeltounderestimatetheimpor-tanceofredundantfeatures(likepronounsinourwork).Inthefuturewewillexploreexplanatorymodelscapableoftakingintoaccountthecorre-lationbetweenfeaturesinthedatasetlikeSAGE(Covertetal.,2020),butsuitedforveryimbal-anceddatasets.Inthedomainofpeer-tutoring,wewouldliketobeabletofurthertestthelinkbe-tweenhedgesandrapport,andthelinkbetweenhedgesandlearninggainsinthesubjectbeingtu-tored.Asnotedabove,thiskindofstudyrequiresafine-grainedcontrolofthelanguageproducedbyoneoftheinterlocutors,whichisdifficulttoachieveinahuman-humanexperience.Wenotethatthehedgeclassifiercanbeusednotjusttoclassify,butalsotoworktowardsimprovingthegenerationofhedgesfortutoragents.Infutureworkwewillexploreusingtheclassifiertore-rankgenerationoutputs,takingadvantageoftherecur-ringsyntacticpatterns(see(ii)inSection5.3)toimprovethegenerationprocessofhedges,andre-generatingclausesthatdon\u2019tcontainoneofthesesyntacticpatterns.AcknowledgmentsManythankstomembersoftheArticuLaboatIN-RIAParisfortheirpreciousassistance.ThisworkwassupportedinpartbythetheFrenchgovern-mentundermanagementofAgenceNationaledelaRechercheaspartofthe\u201cInvestissementsd\u2019avenir\u201dprogram,referenceANR-19-P3IA-0001(PRAIRIE3IAInstitute).ReferencesHeikeAdelandHinrichSch\u00fctze.2017.Exploringdif-ferentdimensionsofattentionforuncertaintydetec-tion.InProceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputa-tionalLinguistics:Volume1,LongPapers,pages22\u201334,Valencia,Spain.AssociationforComputa-tionalLinguistics.ShoshanaBlum-Kulka.1987.Indirectnessandpolite-nessinrequests:Sameordifferent?Journalofpragmatics,11(2):131\u2013146.PenelopeBrownandStephenCLevinson.1987.Polite-ness:Someuniversalsinlanguageusage,volume4.Cambridgeuniversitypress.JudeeKBurgoonandRandallJKoper.1984.Nonverbalandrelationalcommunicationassociatedwithreti-cence.HumanCommunicationResearch,10(4):601\u2013626.IanCovert,ScottMLundberg,andSu-InLee.2020.Understandingglobalfeaturecontributionswithad-ditiveimportancemeasures.AdvancesinNeuralInformationProcessingSystems,33:17212\u201317223.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforCom-putationalLinguistics:HumanLanguageTechnolo-gies,Volume1(LongandShortPapers),pages4171\u20134186.Rich\u00e1rdFarkas,VeronikaVincze,Gy\u00f6rgyM\u00f3ra,J\u00e1nosCsirik,andGy\u00f6rgySzarvas.2010.Theconll-2010sharedtask:learningtodetecthedgesandtheirscopeinnaturallanguagetext.InProceedingsofthefour-teenthconferenceoncomputationalnaturallanguagelearning\u2013Sharedtask,pages1\u201312.BruceFraser.2010.Pragmaticcompetence:Thecaseofhedging.Newapproachestohedging,1534.ViolaGanterandMichaelStrube.2009.Findinghedgesbychasingweasels:Hedgedetectionusingwikipediatagsandshallowlinguisticfeatures.InProceedingsoftheACL-IJCNLP2009ConferenceShortPapers,pages173\u2013176.PranavGoel,YoichiMatsuyama,MichaelMadaio,andJustineCassell.2019.\u201cithinkitmighthelpifwemultiply,andnotadd\u201d:Detectingindirectnessincon-versation.In9thInternationalWorkshoponSpokenDialogueSystemTechnology,pages27\u201340.Springer.CynthiaHoward,PamelaW.Jordan,BarbaraMariaDiEugenio,andSandraKatz.2015.Shiftingtheload:apeerdialogueagentthatencouragesitshumancollab-oratortocontributemoretoproblemsolving.Interna-tionalJournalofArtificialIntelligenceinEducation,27:101\u2013129.KenHyland.1998.Hedginginscientificresearcharti-cles,volume54.JohnBenjaminsPublishing.GuolinKe,QiMeng,ThomasFinley,TaifengWang,WeiChen,WeidongMa,QiweiYe,andTie-YanLiu.2017.Lightgbm:Ahighlyefficientgradientboost-ingdecisiontree.Advancesinneuralinformationprocessingsystems,30:3146\u20133154.KlausKrippendorff.2004.Reliabilityincontentanaly-sis:Somecommonmisconceptionsandrecommen-dations.Humancommunicationresearch,30(3):411\u2013433.GeorgeLakoff.1975.Hedges:Astudyinmeaningcriteriaandthelogicoffuzzyconcepts.InContem-poraryresearchinphilosophicallogicandlinguisticsemantics,pages221\u2013271.Springer.\n2168\n\n\nMatthewLeach.2005.Rapport:Akeytotreatmentsuc-cess.Complementarytherapiesinclinicalpractice,11:262\u20135.IlyaLoshchilovandFrankHutter.2018.Decoupledweightdecayregularization.InInternationalConfer-enceonLearningRepresentations.ScottMLundberg,GabrielGErion,andSu-InLee.2018.Consistentindividualizedfeatureat-tributionfortreeensembles.arXivpreprintarXiv:1802.03888.ScottMLundbergandSu-InLee.2017.Aunifiedap-proachtointerpretingmodelpredictions.InProceed-ingsofthe31stinternationalconferenceonneuralinformationprocessingsystems,pages4768\u20134777.HowardLuneandBruceLBerg.2017.Qualitativeresearchmethodsforthesocialsciences.Pearson.MichaelMadaio,JustineCassell,andAmyOgan.2017.Theimpactofpeertutors\u2019useofindirectfeedbackandinstructions.Philadelphia,PA:InternationalSo-cietyoftheLearningSciences.BenMedlockandTedBriscoe.2007.Weaklysuper-visedlearningforhedgeclassificationinscientificliterature.InProceedingsofthe45thannualmeetingoftheassociationofcomputationallinguistics,pages992\u2013999.ChristophMolnar.2020.Interpretablemachinelearn-ing.Lulu.com.AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advancesinneuralinformationprocessingsystems,32:8026\u20138037.JamesWPennebaker,RyanLBoyd,KaylaJordan,andKateBlackburn.2015.Thedevelopmentandpsycho-metricpropertiesofliwc2015.Technicalreport.JeffreyPennington,RichardSocher,andChristopherDManning.2014.Glove:Globalvectorsforwordrep-resentation.InProceedingsofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing(EMNLP),pages1532\u20131543.EllenFPrince,JoelFrader,CharlesBosk,etal.1982.Onhedginginphysician-physiciandiscourse.Lin-guisticsandtheProfessions,8(1):83\u201397.AnnaProkofievaandJuliaHirschberg.2014.Hedgingandspeakercommitment.In5thIntl.WorkshoponEmotion,SocialSignals,Sentiment&LinkedOpenData,Reykjavik,Iceland.NilsReimersandIrynaGurevych.2019.Sentence-bert:Sentenceembeddingsusingsiamesebert-networks.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages3982\u20133992.OscarJRomero,RanZhao,andJustineCassell.2017.Cognitive-inspiredconversational-strategyreasonerforsocially-awareagents.InIJCAI,pages3807\u20133813.TimRowland.2007.\u2018wellmaybenotexactly,butit\u2019saroundfiftybasically?\u2019:Vaguelanguageinmath-ematicsclassrooms.InVaguelanguageexplored,pages79\u201396.Springer.SaschaSchneider,SteveNebel,SimonPradel,andG\u00fcn-terDanielRey.2015.Mindyourpsandqs!howpoliteinstructionsaffectlearningwithmultimedia.ComputersinHumanBehavior,51:546\u2013555.TanmaySinhaandJustineCassell.2015.Weclick,wealign,welearn:Impactofinfluenceandconvergenceprocessesonstudentlearningandrapportbuilding.InProceedingsofthe1stWorkshoponModelingINTERPERsonalSynchrONyAndInfLuence,INTER-PERSONAL\u201915,page13\u201320,NewYork,NY,USA.AssociationforComputingMachinery.HelenSpencer-Oatey.2005.(im)politeness,faceandperceptionsofrapport:Unpackagingtheirbasesandinterrelationships.1(1):95\u2013119.Gy\u00f6rgySzarvas,VeronikaVincze,Rich\u00e1rdFarkas,Gy\u00f6rgyM\u00f3ra,andIrynaGurevych.2012.Cross-genreandcross-domaindetectionofsemanticuncer-tainty.ComputationalLinguistics,38(2):335\u2013367.BuzhouTang,XiaolongWang,XuanWang,BoYuan,andShixiFan.2010.Acascademethodfordetectinghedgesandtheirscopeinnaturallanguagetext.InProceedingsoftheFourteenthConferenceonCom-putationalNaturalLanguageLearning\u2013SharedTask,pages13\u201317.MorganUlinski,SethBenjamin,andJuliaHirschberg.2018.Usinghedgedetectiontoimprovecommittedbelieftagging.InProceedingsoftheWorkshoponComputationalSemanticsbeyondEventsandRoles,pages1\u20135.TeppoVarttala.1999.Remarksonthecommunicativefunctionsofhedginginpopularscientificandspecial-istresearcharticlesonmedicine.Englishforspecificpurposes,18(2):177\u2013200.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,\u0141ukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.InAdvancesinneuralinformationpro-cessingsystems,pages5998\u20136008.ErikVelldal.2011.Predictingspeculation:asimpledis-ambiguationapproachtohedgedetectioninbiomed-icalliterature.JournalofBiomedicalSemantics,2(5):1\u201314.VeronikaVincze.2014.Uncertaintydetectioninnaturallanguagetexts.PhD,UniversityofSzeged,page141.\n2169\n\n\nNingWang,WLewisJohnson,RichardEMayer,PaolaRizzo,ErinShaw,andHeatherCollins.2008.Thepolitenesseffect:Pedagogicalagentsandlearningoutcomes.Internationaljournalofhuman-computerstudies,66(2):98\u2013112.ShawnWarner-Garcia.2014.Laughingwhennothing\u2019sfunny:Thepragmaticuseofcopinglaughterinthenegotiationofconversationaldisagreement.Prag-matics,24(1):157\u2013180.TimothyWilliamson.2002.Vagueness.Routledge.RanZhao,AlexandrosPapangelis,andJustineCassell.2014.Towardsadyadiccomputationalmodelofrap-portmanagementforhuman-virtualagentinteraction.InInternationalConferenceonIntelligentVirtualAgents,pages514\u2013527.Springer.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016a.Automaticrecognitionofconversa-tionalstrategiesintheserviceofasocially-awaredialogsystem.InProceedingsofthe17thAnnualMeetingoftheSpecialInterestGrouponDiscourseandDialogue,pages381\u2013392.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016b.Socially-awarevirtualagents:Au-tomaticallyassessingdyadicrapportfromtemporalpatternsofbehavior.InInternationalconferenceonintelligentvirtualagents,pages218\u2013233.Springer.AAdditionalinformationontheexperimentalsettingsWeusedPyTorch(Paszkeetal.,2019)toimple-menttheneuralmodels.Foreachsetoffeatures,hyperparameterswereselectedusingOptuna(Ak-iba,2019),aparametersearchframework.Were-implementedtheAttention-CNNwithGlove(Pen-ningtonetal.,2014)300-Dwordsembeddingsasthevectorrepresentation.Foreachmodels,theresultsarecross-validatedusing5folds(wechose5insteadof10toavoidhavingfoldswithtoofewsamplesperclass).Wecorrectedthelossfunctionforclassimbalancetoforcethemodeltoadaptmoretothelessfrequentclasses.Thestrengthofthiscorrectiondependedonthemodel,andwasselectedbecauseitprovidedasatisfyingcompro-misebetweenfavoringrecallandprecisionintheclassificationresultsofthatmodel.ForLightGBM,a\"squarerootofthesquarerootoftheinverseclassproportion\"correctionwasselected.Neu-ralmodelsweretrainedusingAdamWasanop-timizer(LoshchilovandHutter,2018),andusedareducedfeaturevector,obtainedwiththeap-plicationofPCA(dinit=1800;d=100;99.8%oftheinformationisconserved).Nosignifi-cantperformancedifferenceswereobservedbe-tweentheoriginalvectorandthereducedvectorfortrainingthemodels.TocomputetheSHAPvaluesmentionedinthepaper,wekeptonesplittoperformthe5-splitofthedataset,andleave1splittovalidateandearlystopthemodel,inor-dertoavoidoverfitting.Acompleteconfigura-tionofhyperparametersusedforeachmodelisre-portedintheGitHubrepositorywiththecodeofthepaper:https://github.com/YannRaphalen/Hedges-Detection.TheBERTmodelwasfine-tunedonaNvidiaQuadroRTX8000GPU.BTables\n2170\n\n\n?(whether|if|is|that|it|this)?.*Subj.\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nNoNoYesNoNoNoYesYesYesYesYesBERT(PTE)\nexceptionally|forthemostpart|inamannerofspeaking|\nNoYesNoNoNoYesNoNoYesYesNoMLP(KDF)\nsomethingbetween|essentially|only).*Prop.\nTable8:Significancetableforthe3-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesYesYesYesYesNoYesYesYesNoYesMLP(KDF+PTE)\ntechnically|typically|virtually|approximately|\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nmuch|somewhat|exactly|almost|littlebit|quite|\nClass\nrelatively|roughly|sotosay|strictlyspeaking|\nNoNoYesNoNoYesNoYesYesYesYesLSTM(KDF)\nYesNoNoNoNo+TM\nTable10:Significancetableforthe3-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoNoYesYesYesYesYesYesNoYesYes\n.*(it)(looks|seems|appears)[,]?.*\",\".*(or|and)(that|something|stuff|soforth)Table7:Regexprulesusedfortheclassifier.\nRule-based\nRule-based\n.*(you(might|may)(believe|think)).*Subj.\nYesYesYesYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nYesYesNoYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nNoNoYesNoNoYesNoNoYesYesYesLSTM(KDF)\nYesYesNoYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(i|i\u2019m|you|it\u2019s)(am|are)(apparently|surely)[,]?.*Prop.\nNoNoYesNoNoYesNoYesYesYesYesLSTM(PTE)\nNoNoYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nNoNoYesYesNoYesYesYesYesYesNoLGBM(KDF)\nNoNoYesYesYesYesYesYesYesYesNoLGBM(KDF)\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nYesNoYesYesYesYesYesYesNoYesYes\nYesYesYesYesYesYesYesYesYesYesYesMLP(KDF+PTE)\nregular|regularly|actually|almost|asitwere|basically|\nNoYesYesNoYesYesYesNoYesYesNoMLP(PTE)\nRule(regexp)\n.*(i|i\u2019m|we)?(was|am|wasn\u2019t)??(not)?(sure|certain).*Subj.\n.*(accordingto|presumably).*Subj.\n(?!.*(be|been|was)likeexcuseme)((excuseme|sorry)[w,\u2019]+|[w,\u2019]+(excuseme|sorry))Prop.\n.*(i|you|we)haveto(check|look|verify).*Subj.\n.*(i\u2019m|i|we\u2019re)(am|are)??(apologize|sorry).*Apol.\nonthetallside|parexcellence|particularly|\n(?!what).*(i|we)?(don\u2019t|didn\u2019t|did)??(not)?\ninarealsense|inasense|inaway|largely|literally|\nYesYesYesNoNoNoYesYesYesYesYesBERT(PTE)\nModels\nModels\nModels\nYesYesYesNoNoYesNoNoYesYesYesLSTM(PTE)\nYesNoNoNoNo\nYesYesYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nSubj.\nTable9:Significancetableforthe4-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoYesNoNoNoYesNoNoYesYesNoMLP(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(ifeellikeyou).*Subj.\nlooselyspeaking|kinda|moreorless|mostly|often|\n.*(unlessi).*Apol.\nprobably|canbeviewas|crypto-|especially|essentially|\n.*(ifi\u2019mnotwrong|ifi\u2019mright|ifthat\u2019strue).*Subj.\nYesNoNoNoNo+POS\n(guess|guessed|thought|think|believe|believed|suppose|supposed)\n.*(just|alittle|maybe|actually|sortof|kindof|pretty\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nNoYesYesNoYesYesNoNoYesYesYesMLP(KDF)\nprettymuch|principally|pseudo-|quintessentially|\n2171\n\n\nFigure2:Absoluteaveragedfeaturecontribution,asindicatedbySHAP.Thelongerthebarisforonecolor,themorethefeatureisassociatedwiththeclassrepresentedbythatcolor.\nFigure3:Averagedcontributionoffeaturestothedetectionofthe\"Notindirect\"class,asindicatedbySHAP.Eachdotcorrespondstoaclassifiedclause.Areddotindicatesthatthefeatureispresentintheclause,whileabluedotindicatesthatthefeatureisabsent.Thefartherontherightthedotis,themorethefeaturecontributedtoitsclassificationasahedge.\n2172\n\n\nFigure4:Averagedcontributionoffeaturestothedetectionof\"Apologizers\",asindicatedbySHAP.\nFigure5:Averagedcontributionoffeaturestothedetectionof\"Propositionalhedges\",asindicatedbySHAP.\n2173\n\n\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nYesNoNoNoNo+TM\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nFigure6:Averagedcontributionoffeaturestothedetectionof\"Subjectivizers\",asindicatedbySHAP.\nModels\nYesNoNoNoNo\nTable11:Significancetableforthe4-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesNoNoNoNo+POS\n2174"}, {"question": " Which machine learning model outperformed others in the hedge classification task and why?", "answer": " LightGBM, utilizing Knowledge-Driven features, showed the best performance", "ref_chunk": "\"Youmightthinkaboutslightlyrevisingthetitle\":IdentifyingHedgesinPeer-tutoringInteractionsYannRaphalen1,Chlo\u00e9Clavel2,JustineCassell1,31InriaParis2LTCI,InstitutPolytechniquedeParis,Telecom-Paris3CarnegieMellonUniversityyann.raphalen.pro@gmail.com,justine@cs.cmu.edu,chloe.clavel@telecom-paris.frAbstractHedgesplayanimportantroleinthemanage-mentofconversationalinteraction.Inpeer-tutoring,theyarenotablyusedbytutorsindyads(pairsofinterlocutors)experiencinglowrapporttotonedowntheimpactofinstructionsandnegativefeedback.Pursuingtheobjectiveofbuildingatutoringagentthatmanagesrap-portwithstudentsinordertoimprovelearning,weusedamultimodalpeer-tutoringdatasettoconstructacomputationalframeworkforiden-tifyinghedges.Wecomparedapproachesre-lyingonpre-trainedresourceswithothersthatintegrateinsightsfromthesocialsciencelitera-ture.Ourbestperformanceinvolvedahybridapproachthatoutperformstheexistingbase-linewhilebeingeasiertointerpret.Weemployamodelexplainabilitytooltoexplorethefea-turesthatcharacterizehedgesinpeer-tutoringconversations,andweidentifysomenovelfea-tures,andthebenefitsofsuchahybridmodelapproach.1IntroductionRapport,mostsimplydefinedasthe\u201c...relativeharmonyandsmoothnessofrelationsbetweenpeo-ple...\u201d(Spencer-Oatey,2005),hasbeenshowntoplayaroleinthesuccessofactivitiesasvariedaspsychotherapy(Leach,2005)andsurveyinterview-ing(LuneandBerg,2017).Inpeer-tutoring,rap-port,asmeasuredbytheannotationofthinslicesofvideo,hasbeenshowntobebeneficialforlearningoutcomes(Zhaoetal.,2014;SinhaandCassell,2015).Thelevelofrapportrisesandfallswithconversationalstrategiesdeployedbytutorsandtuteesatappropriatetimes,andasafunctionofthecontentofpriorturns.Thesestrategiesincludeself-disclosure,referringtosharedexperience,and,onthepartoftutors,givinginstructionsinanindirectmanner.Someworkhasattemptedtoautomaticallydetectthesestrategiesintheserviceofintelligenttutors(Zhaoetal.,2016a),butonlyafewstrate-gieshavebeenattempted.Otherworkhascon-centratedona\"socialreasoningmodule\"(Romeroetal.,2017)todecidewhichstrategiesshouldbegeneratedinagivencontext,butindirectnesswasnotamongthestrategiestargeted.Inthispaper,wefocusontheautomaticclassificationofonespe-cificstrategythatisparticularlyimportantforthetutoringdomain,andthereforeimportantforintel-ligenttutors:hedging,asub-partofindirectnessthat\"softens\"whatwesay.Thisworkispartofalargerresearchprogramwiththelong-termgoalofautomaticallygeneratingindirectnessbehaviorsforatutoringagent.\nFigure1:Amockconversationdisplayingeachtypeofhedgedformulation.AccordingtoBrownandLevinson(1987),hedgesarepartofthelinguistictoolsthatinterlocu-torsusetoproducepoliteness,bylimitingthefacethreattotheinterlocutor(basicallybylimitingtheextenttowhichtheinterlocutormightexperienceembarrassmentbecauseofsomekindofpoorper-formance).Anexampleis\"that\u2019skindofawronganswer\".Hedgesarealsofoundwhenspeakerswishtoavoidlosingfacethemselves,forexam-plewhensaying(\"IthinkImighthavetoadd6.\").Madaioetal.(2017)foundthatinapeer-tutoringtask,whenrapportbetweeninterlocutorsislow,tu-teesattemptedmoreproblemsandcorrectlysolvedmoreproblemswhentheirtutorshedgedinstruc-\n2160 Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1: Long Papers, pages 2160 - 2174 May 22-27, 2022 c(cid:13)2022 Association for Computational Linguistics\n\n\ntions,whichlikewisepointstowardsa\"mitigationoffacethreat\"function.Hedgescanalsobeasso-ciatedwithanonverbalcomponent,forexampleavertedeyegazeduringcriticism(BurgoonandKoper,1984).Hedgesarenot,however,alwaysap-propriate,asin\"Ikindofthinkit\u2019srainingtoday.\"whentheinterlocutorscanbothseerain(althoughitmightbetakenashumorous).Thesefactsabouthedgesmotivateawaytoautomaticallydetectthemand,ultimately(althoughnotinthecurrentwork)alsogeneratethem.Inbothcaseswefirsthavetobeabletocharacterizethemusinginterpretablelinguisticfeatures,whichiswhatweaddressinthecurrentpaper.Thus,intheworkdescribedhere,basedonlinguisticdescriptionsofhedges(BrownandLevinson,1987;Fraser,2010),webuiltarule-basedclassifier.Weshowthatthisclassifierincombinationwithadditionalmultimodalinter-pretablecontext-dependentfeaturessignificantlyimprovestheperformanceofamachinelearningmodelforhedges,comparedtoalessinterpretabledeeplearningbaselinefromGoeletal.(2019)us-ingwordembeddings.Wealsoreliedonamachinelearningmodelexplanationtool(LundbergandLee,2017)toinvestigatethelinguisticfeaturesrelatedtohedgesinthecontextofpeer-tutoring,primarilytoseeifwecoulddiscoversurprisingfeaturesthattheclassificationmodelwouldassociatetohedgesinthiscontext,andwedescribethosebelow.Thecodeofthemodelsdescribedinthepaperisalsoprovided.12RelatedworkHedges:AccordingtoFraser(2010),hedgingisarhetoricalstrategythatattenuatesthestrengthofastatement.Onewaytoproduceahedgeisbyalteringthefullsemanticvalueofaparticu-larexpressionthroughPropositionalhedges(alsocalledApproximatorsinPrinceetal.(1982)),asin\"Youarekindofwrong,\"thatreduceprototypical-ity(i.eaccuracyofthecorrespondencebetweenthepropositionandtherealitythatthespeakerseekstodescribe).Propositionalhedgesarerelatedtofuzzylanguage(Lakoff,1975),andthereforetotheproductionofvagueness(Williamson,2002)anduncertainty(Vincze,2014).AsecondkindareRelationalHedges(alsocalledShieldsinPrinceetal.(1982)),suchas\u201cIthinkthatyouarewrong.\u201dor\u201cThedoctorwantsyoutostopsmoking.\u201d,conveyingthatthepropositionis\n1https://github.com/AnonymousHedges/HedgeDetectionconsideredbythespeakerassubjective.Inafurthersub-division,AttributionShields,asin\"Thedoc-torwantsyou...\",theinvolvementofthespeakerinthetruthvalueofthepropositionisnotmadeexplicit,whichallowsspeakersnottotakeastance.Asdescribedabove,Madaioetal.(2017)foundthattutorswhoshowedlowerrapportwiththeirtuteesusedmorehedgedinstructions(theyalsoemployedmorepositivefeedback),howeverthiswasonlythecasefortutorswithagreaterbeliefintheirabilitytotutor.Tuteesinthiscontextsolvedmoreproblemscorrectlywhentheirtutorshedgedinstructions.Noeffectofhedgingwasfoundfordyads(pairsofinterlocutors)withgreatersocialcloseness.However,theauthorsdidnotlookatthespecificlinguisticformstheseteenagersused.Rowland(2007)alsodescribestherolethathedg-ingplaysinthisagegroup,showingthatstudentsusebothrelational(\"IthinkthatJohnissmart.\")andpropositional(\"Johniskindofsmart.\")hedgesformuchthesameshieldingfunctionofdemon-stratinguncertainty,tosavethemfromtheriskofembarrassmentiftheyarewrong.TheauthorobservedthatteensusedfewAdaptors(kindof,somewhat)andpreferredtouseRounders(around,closeto).However,thisstudywasperformedwithanadultandtwochildren,possiblybiasingthere-sultsduetotheparticipationoftheadultinvestiga-tor.Hedgeshavebeenincludedinvirtualtutoringagentsbeforenow.(Howardetal.,2015)integratedhedgesinatutoragentforundergraduatesinCS,asawaytoencouragethestudenttotaketheinitiative.Hedgeshavealsobeenusedasawayofintegrat-ingBrownandLevinson\u2019spolitenessframework(Wangetal.,2008;Schneideretal.,2015)invir-tualtutoringagents.Resultswerenotbrokenoutbystrategy,butpolitenessingeneralwasshowntopositivelyinfluencemotivationandlearning,incertainconditions.Computationalmethodsforhedgedetection:Anumberofstudieshavetargetedthedetectionofhedgesanduncertaintyintext(MedlockandBriscoe,2007;GanterandStrube,2009;Tangetal.,2010;Velldal,2011;Szarvasetal.,2012),partic-ularlyfollowingtheCoNLL2010datasetrelease(Farkasetal.,2010).However,thisworkisnotasrelatedtohedgesinconversation,asitfocusesonaformalandacademiclanguageregister(Hy-land,1998;Varttala,1999).AsnotedbyProkofievaandHirschberg(2014),thefunctionsofhedgesaredomain-andgenre-dependent,thereforethisbias\n2161\n\n\ntowardsformalityimpliesthattheexistingworkmaynotadaptwelltothedetectionofhedgesinconversationbetweenteenagers.Aconsequenceisthattheexistingworkdoesnotconsidertermslike\"Ithink,\"sinceopinionsrarelyappearinanaca-demicwritingdataset.Instructionsarealsoalmostabsent(\"Ithinkyouhavetoaddtentobothsides.\"),astronglimitationforthestudyofconversationalhedgessinceitisinrequests(includingtutoringin-structions)thatindirectformulationsmostlyoccuraccordingtoBlum-Kulka(1987).ProkofievaandHirschberg(2014)alsonotethatitisdifficulttodetecthedgesbecausethewordpatternsassociatedwiththemhaveothersemanticandpragmaticfunc-tions:considering\"Ithinkthatyouhavetoaddxtobothsides.\"vs\"Ithinkthatyouareanidiot.\",itisnotclearthattheseconduseof\"Ithinkthat\"isanhedgemarker.Theyadvocateusingmachinelearningapproachestodealwiththeambiguityofthesemarkers.Workingonaconversationaldataset,Ulinskietal.(2018)builtacomputationalsystemtoassessspeakercommitment(i.e.atwhichpointthespeakerseemsconvincedbythetruthvalueofastatement),inparticularbyrelyingonarule-baseddetectionsystemforhedges.Comparedtothatwork,ourrule-basedclassificationmodelisdirectlydetectinghedgeclasses,andweemploythepredictionsoftherule-basedmodelasafeatureforstrongermachinelearningmodels,designedtolessentheimpactoftheimbalancebetweenclasses.Wealsoconsiderapologieswhentheyserveamit-igationfunction(wethencallthemApologizers),aswasdonebytheauthorsofourcorpus,andwealsousethetermsubjectivizersasdefinedbelow,tobeabletocomparedirectlywiththepreviousworkcarriedoutonthiscorpus.Asfarasweknow,onlyGoeletal.(2019)haveworkedwithapeer-tutoringdataset(thesameonethatwealsouse),andtheyachievedtheirbestclassificationresultbyemployinganAttention-CNNmodel,inspiredbyAdelandSch\u00fctze(2017).3ProblemstatementWeconsiderasetDofconversationsD=(c1,c2,...,c|D|),whereeachconversationiscom-posedofasequenceofindependentsyntacticclausesci=(u1,u2,...,uM),whereMisthenumberofclausesintheconversation.Notethattwoconsecutiveclausescanbeproducedbythesamespeaker.Eachclauseisassociatedwithauniquelabelcorrespondingtothediffer-enthedgeclassesdescribedinTable1:yi\u2208C={PropositionalHedges,Apologizers,Subjec-tivizers,Nothedged}.Finally,anutteranceuicanberepresentedasavectoroffeaturesX=(x1,x2,...,xN),whereNrepresentsthenumberoffeaturesweusedtodescribeaclause.Ourfirstgoalistodesignamodelthatcorrectlypredictsthelabelyiassociatedtoui.Itcanbeunderstoodasthefollowingresearchquestion:RQ1:\"Whichmodelsandfeaturescanbeusedtoautomaticallycharacterizehedgesinapeer-tutoringinteraction?\"Oursecondgoalistoidentify,foreachhedgeclass,thesetoffeaturesFclass={fk},k\u2208[1,N]sortedbyfeatureimportanceintheclassificationofclass.Itcorrespondstothefollowingresearchquestion:RQ2:\"Whatarethemostimportantlinguisticfeaturesthatcharacterizeourhedgeclassesinapeer-tutoringsetting?\"4Methodology4.1CorpusDatacollection:Thedialoguecorpususedherewascollectedaspartofalargerstudyontheeffectsofrapport-buildingonreciprocalpeertutoring.24Americanteenagers(meanage=13.5,min=12,max=15),halfmaleandhalffemale,cametoalabwherehalfoftheparticipantswerepairedwithasame-age,same-genderfriend,andtheotherhalfpairedwithastranger.Theparticipantswereassignedtoatotalof12dyadsinwhichthepar-ticipantsalternatedtutoringoneanotherinlinearalgebraequationsolvingfor5weeklyhour-longsessions,foratotalcorpusofnearly60hoursofface-to-faceinteractions.Eachsessionwasstruc-turedsuchthatthestudentsengagedinbriefsocialchitchatinthebeginning,thenoneofthestudentswasrandomlyassignedtotutortheotherfor20minutes.Theythenengagedinanothersocialpe-riod,andconcludedwithasecondtutoringperiodwheretheotherstudentwasassignedtheroleoftutor.Audioandvideodatawererecorded,tran-scribed,andsegmentedforclause-leveldialogueannotation,providingnearly24000clauses.Non-speechsegments(notablyfillersandlaughter)weremaintained.Becauseoftemporalmisalignmentforpartsofthecorpus,manyparaverbalphenomena,suchasprosody,wereunfortunatelynotavailabletous.SinceouraccesstothedatasetiscoveredbyaNon-DisclosureAgreement,itcannotbereleased\n2162\n\n\n2https://github.com/AnonymousHedges/HedgeDetectionclasses.4.2FeaturesLabelfromrule-basedclassifier(LabelRB):Weusetheclasslabelpredictedbytherule-basedclas-sifierdescribedinSection4.3asafeature.Ourhypothesisisthatthemachinelearningmodelcanusethisinformationtocounterbalancetheclassimbalance.Totakeintoaccountthefactthatsomerulesaremoreefficientthanothers,weweightedtheclasslabelresultingfromtherule-basedmodelbytheprecisionoftherulethatgeneratedit.Unigramandbigram:Wecountthenumberofoccurrencesofunigramsandbigramsofthecorpusineachclause.Weusedthelemmaofthewordsforunigramsandbigramsusingthenltklemmatizer(Loper,2002)andselectedunigramsandbigramsthatoccurredinthetrainingdatasetatleastfiftytimes.Thegoalwastoinvestigate,withabottom-upapproach,towhatextenttheuseofcertainwordscharacterizeshedgeclassesintutoring.InSection5weexaminetheoverlapbetweenthesewordsandthoseaprioriidentifiedbytherules.Part-of-speech(POS):Hedgeclassesseemtobeassociatedwithdifferentsyntacticpatterns:forex-ample,subjectivizersmostoftencontainapersonalpronounfollowedbyaverb,asin\"Iguess\",\"Ibelieve\",\"Ithink\".WethereforeconsideredthenumberofoccurrencesofPOS-Tagn-grams(n=1,2,3)asfeatures.WeusedthespaCyPOS-taggerandconsideredPOSunigrams,bigramsandtri-gramsthatoccuratleast10timesinthetrainingdataset.LIWC:LinguisticInquiryandWordCount(LIWC)(Pennebakeretal.,2015)isstandardsoft-wareforextractingthecountofwordsbelongingtospecificpsycho-socialcategories(e.g.,emotions,religion).Ithasbeensuccessfullyusedinthede-tectionofconversationalstrategies(Zhaoetal.,2016a).Wethereforecountthenumberofoccur-rencesofallthe73categoriesfromLIWC.Tutoringmoves(TM):Intelligenttutoringsys-temsrelyonspecifictutoringmovestosuccess-fullyconveycontent(asdohumantutors).Wethereforelookedatthelinkbetweenthetutoringmoves,asannotatedinMadaioetal.(2017),andhedges.Fortutors,thesemovesare(1)instruc-tionaldirectivesandsuggestions,(2)feedback,and(3)affirmations,mostlyexplicitreflectionsontheirpartners\u2019comprehension,whilefortutees,theyare(1)questions,(2)feedbacks,and(3)affirmations,\npublicly.Howevertheoriginalexperimenters\u2019In-stitutionalReviewBoard(IRB)approvalallowsustoview,annotate,andusethedatatotrainmodels.Thisalsoallowsustoprovidealinktoapixe-latedvideoexampleintheGitHubrepositoryoftheproject2.Dataannotation:Thedatasetwaspreviouslyan-notatedbyMadaioetal.(2017),followingananno-tationmanualthatusedhedgeclassesderivedfromRowland(2007)(seeTable1).Onlythetaskperi-odsoftheinteractionswereannotated.Comparingtheannotationswiththeclassesmentionedintherelatedworksection,SubjectivizerscorrespondtoRelationalhedges(Fraser,2010),PropositionalhedgesandExtenderscorrespondtoApproxima-tors(Princeetal.,1982)withtheadditionofsomediscoursemarkerssuchasjust.ApologizersarementionedaslinguistictoolsrelatedtonegativepolitenessinBrownandLevinson(1987).Krippen-dorff\u2019salphaobtainedforthiscorpusannotatedbyfourcoderswasover0.7forallclasses(denotinganacceptableinter-coderreliabilityaccordingtoKrippendorff(2004)).Thedatasetiswidelyim-balanced,withmorethan90%oftheutterancesbelongingtotheNothedgedclass.Inreviewingthecorpusandtheannotationman-ual,however,wenoticedtwoissues.First,theannotationoftheExtendersclasswasinconsis-tent,leadingtotheExtendersandPropositionalhedgesclassescarryingsimilarsemanticfunctions.WethereforemergedthetwoclassesandgroupedutteranceslabeledasExtendersandthoselabeledasPropositionalhedgesundertheheadingofPropositionalhedges.Second,theannotationofclausescontainingthetokens\"just\"and\"would\"(twotermsoccurringfrequentlyinthedatasetthatarekeycomponentsofPropositionalHedgesandSubjectivizersbutthatarenotinfacthedgesinallcases)wasalsoinconsistent,leadingtovirtuallyallclauseswiththosetwotokensbeingconsideredhedges.Wethereforere-consideredalltheclausesassociatedwithanyofthehedgeclasses,aswellasalltheclausesinthe\"Nothedged\"classthatcontained\"just\"or\"would\".There-annotationwascarriedoutbytwoannotatorswhoachievedaKrippendorff\u2019salphainter-raterreliabilityof.9orbetterforApologizers,Subjectivizers,andPropo-sitionalhedgesbeforeindependentlyre-annotatingtherelevantclauses.Anexampleofare-annotationwasremoving\"Iwouldkillyou!\"fromthehedge\n2163\n\n\nProp.hedgesApologizersSubjectivizersNothedgedTotal\nTable1:Definitionoftheclasses\nFeaturesnameAutomaticextractionVectorsize\nSubjectivizersWordsthatreduceintensityorcertainty\u201cSothenIwoulddividebytwo.\u201dApologizersApologiesusedtosoftendirectspeechacts\u201cOhsorrysixb.\u201dPropositionalhedgesQualifyingwordstoreduceintensityorcertaintyofutterances\u201cIt\u2019sactuallyeight.\u201dExtendersWordsusedtoindicateuncertaintybyreferringtovaguecategories\u201cIt\u2019llbethenumberxorwhatevervariableyouhave.\u201d\n12101286262119223156\nTable2:Distributionoftheclasses\nClassDefinitionExample\nRule-basedlabelYes4UnigramYes~250BigramYes~250POSYes~1200LIWCYes73NonverbalNo24TutoringmovesNo6Total~1800\nTable3:Listofautomaticallyextractedandmanuallyannotatedfeatureswiththeirsize.mostlytentativeanswers.Nonverbalandparaverbalbehaviors:AsinGoeletal.(2019),weincludedthenonverbalandpar-averbalbehaviorsthatarerelatedtohedges.Specif-ically,weconsiderlaughterandsmiles,thathavebeenshowntobeeffectivemethodsofmitiga-tion(Warner-Garcia,2014),cut-offsindicatingself-repairs,fillerslike\"Um\",gazeshifts(annotatedas\u2019GazeatPartner\u2019,\u2019GazeattheMathWorksheet\u2019,and\u2019Gazeelsewhere\u2019),andheadnods.Eachfea-turewaspresenttwiceinthefeaturevector,onetimeforeachinterlocutor.Inter-raterreliabilityfornonverbalbehaviorwas0.89(asmeasuredbyKrippendorff\u2019salpha)foreyegaze,0.75forsmilecount,0.64forsmiledurationand0.99forheadnod.Laughterisalsoreportedinthetranscriptatthewordlevel.Weseparatethetutor\u2019sbehaviorsfromthoseofthetutee.ThecollectionprocessforthesebehaviorsisdetailedfurtherinZhaoetal.(2016b).Theclause-levelfeaturevectorwasnormalizedbythelengthoftheclause(exceptfortherule-basedlabel).Thislengthwasalsoaddedasafeature.Table3presentsanoverviewofthefinalfeaturevector.4.3ClassificationmodelsTheclassificationmodelsusedarepresentedhereaccordingtotheirlevelofintegrationofexternallinguisticknowledge.Rule-basedmodel:OnthebasisoftheannotationmanualusedtoconstructthedatasetfromMadaioetal.(2017),andwithdescriptionsofhedgesfromRowland(2007),Fraser(2010)andBrownandLevinson(1987),weconstructedarule-basedclas-sifierthatmatchesregularexpressionsindicativeofhedges.TherulesaredetailedinTable7intheAppendix.LGBM:Sincehedgesareoftencharacterizedbyexplicitlexicalmarkers,wetestedtheassumptionthatamachinelearningmodelwithaknowledge-drivenrepresentationforclausescouldcompetewithaBERTmodelinperformance,whilebeingmuchmoreinterpretable.WereliedonLightGBM,anensembleofdecisiontreestrainedwithgradi-entboosting(Keetal.,2017).Thismodelwasselectedbecauseofitsperformancewithsmalltrainingdatasetsandbecauseitcanignoreunin-formativefeatures,butalsoforitstrainingspeedcomparedtoalternativeimplementationsofgradi-entboostingmethods.Multi-layerperceptron(MLP):Asasimplebase-line,webuiltamulti-layerperceptronusingthreesetsoffeatures:apre-trainedcontextualrepre-sentationoftheclause(SentBERT;ReimersandGurevych(2019));theconcatenationofthiscon-textualrepresentationoftheclauseandarule-basedlabel(notrelyingonthepreviousclauses);andfinallytheconcatenationofallthefeaturesmen-tionedinsection4.2,withoutthecontextualizedrepresentation.LSTMoverasequenceofclauses:Sinceweareworkingwithconversationaldata,wealsowantedtotestwhethertakingintoac-countthepreviousclauseshelpstodetectthetypeofhedgeclassinthenextclause.Formally,wewanttoinferyiusingyi=maxy\u2208ClassesP(y|X(ui),X(ui\u22121),...,X(ui\u2212K)),whereKisthenumberofpreviousclausesthatthemodelwilltakeintoaccount.The\n2164\n\n\nMLPmodelpresentedaboveinfersyiusingyi=maxy\u2208ClassesP(y|X(ui)),thereforeadifferenceofperformancebetweenthetwomodelswouldbeasignthatusinginformationfromthepreviousclausescouldhelptodetectthehedgedformulationinthecurrentclause.WetestedaLSTMmodelwiththesamerepresentationsforclausesasfortheMLPmodel.CNNwithattention:Goeletal.(2019)estab-lishedtheirbestperformanceonhedgedetec-tionusingaCNNmodelwithadditiveattentionoverword(andnotclause)embeddings.Con-trarytotheMLPandLSTMmodelsmentionedabove,thismodeltriestoinferyiusingyi=maxy\u2208ClassesP(y|g(w0),g(w1),...,g(wL)),withLrepresentingthemaximumclauselengthweal-low,andgrepresentingafunctionthatturnsthewordwj,j\u2208[0,L]intoavectorrepresentation(formoredetails,pleaseseeAdelandSch\u00fctze(2017)).BERT:Tobenefitfromdeepsemanticandcon-textualrepresentationsoftheutterances,wealsofine-tunedBERT(Devlinetal.,2019)onourclas-sificationtask.BERTisapre-trainedTransformersencoder(Vaswanietal.,2017)thathassignificantlyimprovedthestateoftheartonanumberofNLPtasks,includingsentimentanalysis.Itproducesacontextualrepresentationofeachwordinasen-tence,makingitcapableofdisambiguatingthemeaningofwordslike\"think\"or\"just\"thatarerepresentativeofcertainclassesofhedges.BERT,however,isnotablyhardtointerpret.4.4AnalysistoolsLookingatwhichfeaturesimprovetheperfor-manceofourclassificationmodelstellsuswhetherthesefeaturesareinformativeornot,butdoesnotexplainhowthesefeaturesareusedbythemod-elstomakeagivenprediction.Wethereforepro-ducedacomplementaryanalysisusinganinter-pretabilitytool.Asdemonstratedby(LundbergandLee,2017),LightGBMinternalfeatureimpor-tancescoresareinconsistentwithboththemodelbehaviorandhumanintuition,soweinsteadusedamodel-agnostictool.SHAP(LundbergandLee,2017)assignstoeachfeatureanimportancevalue(calledShapleyvalues)foraparticularpredictiondependingontheextentofitscontribution(ade-tailedintroductiontoShapleyvaluesandSHAPcanbefoundinMolnar(2020)).SHAPisamodel-agnosticframework,thereforethevaluesassoci-atedwithasetoffeaturescanbecomparedacrossmodels.ItshouldbenotedthatSHAPproducesexplanationsonacase-by-casebasis,thereforeitcanbothprovidelocalandglobalexplanations.FortheGradientBoostingmodel,weuseanadaptedversionofSHAP(Lundbergetal.,2018),calledTreeSHAP.5Experimentsandresults5.1ExperimentalsettingTodetectthebestsetoffeatures,weusedLight-GBMandproceededincrementally,byaddingthegroupoffeatureswethoughttobemostlikelyasso-ciatedwithhedges.Wedidnotconsidertheriskofrelyingonasub-optimalsetoffeaturesthroughthisprocedurebecauseofthestrongabilityofLight-GBMtoignoreuninformativefeatures.Weusethisincrementalapproachasawaytotestourintuitionabouttheperformativityofgroupsoffeatures(i.e.doesaddingafeatureimprovetheperformanceofthemodel)withregardtothetaskofclassifica-tion.Tocompareourmodels,wetrainedthemonthe4-classtask,andlookedattheaverageoftheweightedF1-scoresforthethreehedgeclasses(i.e.howwellthemodelsinferminorityclasses)thatwereporthereas\"3-classes\",andattheaverageoftheweightedF1-scoresforthe4classes,thatwereportas\"4-classes\".DetailsofthehyperparametersandexperimentalsettingsareprovidedinAppendixA.5.2ModelcomparisonandfeatureanalysisOverallresults:Table4presentstheresultsob-tainedbythe6modelspresentedinSection4.3forthemulti-classproblem.Bestperformance(F1-scoreof79.0)isobtainedwithLightGBMlever-agingalmostallthefeatures.Intheappendix(seeTable8andTable9)weindicatetheconfidenceintervalstorepresentthesignificanceofthediffer-encesbetweenthemodels.First,andperhapssurprisingly,wenoticethattheuseof\"Knowledge-Driven\"featuresbasedonrulesbuiltfromlinguisticknowledgeofhedgesintheLightGBMmodeloutperformstheuseofpre-trainedembeddingswithinafine-tunedBERTmodel(79.0vs.70.6),andintheneuralbaselinefrom(Goeletal.,2019)(79.0vs64.5).ThelowscoresobtainedbytheLGBM,LSTMandMLPmodelswithpre-trainedsentenceem-beddingsversusKnowledge-Drivenfeaturesmightsignalthatthewordpatternscharacterizinghedgesarenotsalientintheserepresentations(i.e.the\n2165\n\n\n68.5(1.6)35.8(3.1)64.8(1.1)Attention-CNN(3-classes)\n65.1(5.7)39.8(8.0)65.2(5.1)BERT(3-classes)\n\u220570.6(2.3)\u2205LGBM(3-classes)\n79.0(1.3)35.0(2.2)70.1(1.4)\nRule-based(3-classes)\n94.7\u2205\u2205MLP(4-classes)\n3Notethatthereisstrongredundancybetweensomefea-turesofLIWCandthespaCyPOStaggerthatbothproducea\"Pronoun\"category,usingalexiconinthefirstcase,andaneuralinferenceinthesecond.\n\u220594.9(0.4)\u2205LGBM(4-classes)\nRule-based(4-classes)\nTable4:AveragedweightedF1-scores(andstandarddeviation)forthethreeminorityclassesandforthe4classes,forallmodels.\"KD\"standsfor\"Knowledge-Driven\",meaningthatthefeaturesarederivedfromlexicon,n-grammodelsandannotations.distancebetween\"Ithinkyoushouldadd5.\"and\"Youshouldadd5.\"isshort.).KDFeaturesseemtoprovideabetterseparabilityoftheclasses.ThecombinationofKDfeaturesandPre-trainedem-beddingsdoesnotsignificantlyimprovetheperfor-manceofthemodelscomparedtotheKDFeaturesonly,whichsuggeststhattheinformationfromthePre-trainedembeddingsisredundantwiththeonefromtheKDFeatures.Thisresultmaybeduetothehighdimensionalityoftheinputvector(868withPCAontheKDFeatures;2500otherwise).Asecondfindingisthattheuseofgradientboost-ingmodelsontopofrule-basedclassifiersbettermodelsthehedgeclasses.Theothermachinelearn-ingmodelsdidnotprovetobeaseffective,exceptforBERT.FeatureanalysisusingLightGBM:Usingthebestperformingmodel,Table5showstheroleofeachfeaturesetinthepredictiontask.ThesignificanceofthedifferencesisshowninTable10andTable11.Comparedtotherule-basedmodel,theintroductionofn-gramssignificantlyimprovedtheperformanceofourclassifier,suggestingthatsomelexicalandsyntacticinformationdescribingthehedgeclasseswasnotpresentintherule-basedmodel.LookingatTable5,wedonotobservesignificantdifferencesbetweentheLGBMmodelusingonlythelabelrulebased+(1-gramsand2-grams)andthemodelsin-corporatingmorefeatures.Tooursurprise,neitherthetutoringmovesnorthenonverbalfeaturessig-nificantlyimprovedtheperformanceofthemodel.The2featureswereincludedtoindexthespecificpeertutoringcontextofthesehedges,sothisindi-catesthatinfutureworkwemightwishtoapplythecurrentmodeltoanothercontextofusetoseeifthismodelofhedgesismoregenerallyapplicablethanweoriginallythought.Bycombiningthisresultwiththeincreasedperformanceofthemodelus-ingKnowledge-Driven(i.e.explicit)featurescom-paredtopre-trainedembeddings,itwouldseemthathedgesareaboveallalexicalphenomenon(i.e.producedbyspecificlexicalelements).5.3In-depthanalysisoftheinformativefeaturesWetrainedtheSHAPexplanationmodelsonLight-GBMwithallfeatures.Themostinformativefea-tures(inabsolutevalue)foreachclassareshowninTable6,andtheplotsbyclassarepresentedintheAppendix.Themostimportantfeaturesseemtobetherule-basedlabels,whichappearinatleastthefourthpositionforthreeclasses(seeTable6),andinthefirstpositionforPropositionalHedgesandNothedgedclasses.Surprisingly,theRule-Basedlabeldoesnotappearinthetop20featuresforApologizers.However,giventhattheclassrarelyappearsinthedata,therulesseldomactivate,sothefeaturemaysimplybeinformativeforaverysmallnumberofclauses.Unigrams(Oh,Sorry,just,Would,andI)arealsopresentinthe5top-rankedfeatures.Thisconfirmsthefindingsmen-tionedinrelatedworkforthecharacterizationofthedifferenthedgeclasses(justwithPropositionalHedges,sorrywithApologizer,IwithSubjectiviz-ers).ThepresenceofOhalsohashighimportanceforthecharacterizationofApologizer(n=2),asillustratedinexamplessuchas\"Ohsorry,that\u2019snine.\".Wenotethattheoccurrencesof\"Ohsorry\"asastand-aloneclausewereexcludedbyourrule-basedmodelbecausetheydonotcorrespondtoanapologizer(theycannotmitigatethecontentofapropositionifthereisnopropositionassociated).Thisexampleillustratestheinterestofamachinelearningmodelapproachtodisambiguatethefunc-tionofconventionalnon-propositionalphraseslike\"Ohsorry\".Inaddition,SHAPhighlightstheimportanceofnovelfeatureswhosefunctionwasnotidentifiedinthehedgesliterature:(i)whatLIWCclassifiesasinformalwordsbutthataremostlyinterjectionslikeahandoharestronglyassociatedwithApol-ogizer,asaredisfluencies(n=12);(ii)theuseofPOStagsseemstobeveryrelevantforcharac-terizingthedifferentclasses(2-gramofPOStagfeatures3occurinthetop-rankedfeaturesofallthe\n\u220564.5(3.0)\u2205LSTM(3-classes)\nKDFeat.(KDF)Pre-TrainedEmb.(PTE)KDF+PTE\n\u220594.4(0.2)\u2205LSTM(4-classes)\n96.7(0.2)91.0(0.2)95.4(0.2)\n94.8(0.3)89.7(0.4)93.9(0.4)Attention-CNN(4-classes)\n93.9(1.4)89.1(1.4)94.1(1.2)BERT(4-classes)\n67.6\u2205\u2205MLP(3-classes)\nModels\n2166\n\n\n95.0(0.2)96.5(0.3)96.5(0.2)96.7(0.2)96.6(0.4)96.7(0.3)\n3-classes\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\n68.8(0.8)78.2(1.6)78.1(1.3)79.0(1.3)78.5(2.4)78.7(1.8)\nTable5:AveragedweightedF1-scoresforthethreeclassesofhedgesandthefourclasses,withanadditiveintegrationofKDFfeaturesintheLightGBMmodel.Thestandarddeviationiscomputedacrossfivefolds.\n4-classes\nModels\nRankApologizerSubjectivizersProp.HedgesNothedged\n1Functionwords(LIWC)\"I\"ClasslabelClasslabel2\"Oh\"(LIWC)\"Yeah\"\"Would\"\"Would\"3\"Sorry\"Noun(POS)\"Just\"\"Yeah\"4Affect(LIWC)ClasslabelFunctionword(LIWC)Noun(POS)5ClauselengthCognitiveprocess(LIWC)Netspeak(LIWC)Cognitiveprocess(LIWC)Table6:Mostimportantclause-levelfeaturesforLightGBMaccordingtotheSHAPanalysis.classes(seeFiguresintheAppendix).Itmeansthattherearesomerecurringsyntacticpatternsineachclass;(iii)Regardingtheutterancesize,aclauseshorterthanthemeanisweaklyassociatedwithdirectness(n=17)whilealongerclausesuggeststhatitcontainsaSubjectivizer(n=6).Apologizersarecharacterizedbyameanclauselength(n=5),withfewvariationsfromit;(iv)Tutoringmovesarenotstrongpredictorsofanyclasses:\"Affirma-tionfromtutor\"istheonlyfeatureappearingasapredictorofPropositionalhedges(n=20).ThisisconsistentwiththefeatureanalysisinTable5,suggestingthattutoringmovesdonotsignificantlyimprovetheperformanceoftheclassifier;(v)Non-verbalbehaviorsdonotappearasimportantfea-turesfortheclassification.Thisiscoherentwithresultsfrom(Goeletal.,2019).Notethatprosodymightplayaroleindetectinginstructionsthattrailoff,but,asdescribed,paraverbalfeatureswerenotavailable;(vi)Wouldplaysanimportantroleintheproductionofhedges,asitisstronglyassociatedtoPropositionalhedges(n=2).Itisinterestingtonotethat,whendesigningtherule-basedclassifier,wesawitdecreaseinperformancewhenwestartedtoincludewouldinourregularexpressionpatterns,probablybecausetheformishardtodisambiguateforadeterministicsystem.WhileexploringtheShapleyvaluesassociatedtoeachclause,weobservedthatfeaturesliketutoringmovesareextremelyinformativeforaverysmallnumberofclauses(thereforenotsignificantlyinflu-encingtheoverallperformanceoftheprediction),andmoreorlessnotinformativefortherest.Infer-ringtheglobalimportanceofafeatureasameanacrosstheshapleyvaluesinthedatasetmaynotbetheonlywaytoexplorethebehaviorofgradi-entboostingmethods.ItmightbemoreusefultoclusterclausesbasedontheimportancethatSHAPgivestothatfeatureinitsclassification,asthiscouldhelpdiscoversub-classesofhedgesthataredifferentiatedfromtherestbytheirinteractionwithaspecificfeature(inthewaythatsomeApologiz-ersarecharacterizedbyan\"oh\").Wealsonotethattheexplanationmodelissensitivetospuri-ouscorrelationsinthedataset,causedbythesmallrepresentationofsomeclass:forexample,\"nine\"(n=7)and\"four\"(n=20)arepositivepredictorsofApologizers.6ConclusionandfutureworkThroughourclassificationperformanceexperi-ments,weshowedthatitispossibletousema-chinelearningmethodstodiminishtheambigu-ityofhedges,andthatthehybridapproachofus-ingrule-basedlabelfeaturesderivedfromsocialscience(includinglinguistics)literaturewithinamachinelearningmodelhelpedsignificantlytoin-creasethemodel\u2019sperformance.Nonverbalbehav-iorsandtutoringmovesdidnotprovideinformationatthesentencelevel;boththeperformanceofthemodelandthefeaturecontributionanalysissug-gestedthattheirimpactonthemodeloutputwasnotstrong.ThisisconsistentwithresultsfromGoeletal.(2019).However,infutureworkwewouldliketoinvestigatethepotentialofmultimodalpat-ternswhenweareabletobettermodelsequentiality(e.g.,negativefeedbackfollowedbyasmile).Re-gardingtheSHAPanalysis,mostofthefeaturesthatareconsideredasimportantarecoherentwiththedefinitionoftheclasses(Iforsubjectivizers,sorryforapologizers,justforpropositionalhedges).However,wediscoveredthatfeatureslikeutterance\n2167\n\n\nsizecanalsoserveasindicatorsofcertainclassesofhedges.AlimitationofSHAPisthatitmakesafeatureindependenceassumption,whichpromptstheexplanatorymodeltounderestimatetheimpor-tanceofredundantfeatures(likepronounsinourwork).Inthefuturewewillexploreexplanatorymodelscapableoftakingintoaccountthecorre-lationbetweenfeaturesinthedatasetlikeSAGE(Covertetal.,2020),butsuitedforveryimbal-anceddatasets.Inthedomainofpeer-tutoring,wewouldliketobeabletofurthertestthelinkbe-tweenhedgesandrapport,andthelinkbetweenhedgesandlearninggainsinthesubjectbeingtu-tored.Asnotedabove,thiskindofstudyrequiresafine-grainedcontrolofthelanguageproducedbyoneoftheinterlocutors,whichisdifficulttoachieveinahuman-humanexperience.Wenotethatthehedgeclassifiercanbeusednotjusttoclassify,butalsotoworktowardsimprovingthegenerationofhedgesfortutoragents.Infutureworkwewillexploreusingtheclassifiertore-rankgenerationoutputs,takingadvantageoftherecur-ringsyntacticpatterns(see(ii)inSection5.3)toimprovethegenerationprocessofhedges,andre-generatingclausesthatdon\u2019tcontainoneofthesesyntacticpatterns.AcknowledgmentsManythankstomembersoftheArticuLaboatIN-RIAParisfortheirpreciousassistance.ThisworkwassupportedinpartbythetheFrenchgovern-mentundermanagementofAgenceNationaledelaRechercheaspartofthe\u201cInvestissementsd\u2019avenir\u201dprogram,referenceANR-19-P3IA-0001(PRAIRIE3IAInstitute).ReferencesHeikeAdelandHinrichSch\u00fctze.2017.Exploringdif-ferentdimensionsofattentionforuncertaintydetec-tion.InProceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputa-tionalLinguistics:Volume1,LongPapers,pages22\u201334,Valencia,Spain.AssociationforComputa-tionalLinguistics.ShoshanaBlum-Kulka.1987.Indirectnessandpolite-nessinrequests:Sameordifferent?Journalofpragmatics,11(2):131\u2013146.PenelopeBrownandStephenCLevinson.1987.Polite-ness:Someuniversalsinlanguageusage,volume4.Cambridgeuniversitypress.JudeeKBurgoonandRandallJKoper.1984.Nonverbalandrelationalcommunicationassociatedwithreti-cence.HumanCommunicationResearch,10(4):601\u2013626.IanCovert,ScottMLundberg,andSu-InLee.2020.Understandingglobalfeaturecontributionswithad-ditiveimportancemeasures.AdvancesinNeuralInformationProcessingSystems,33:17212\u201317223.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforCom-putationalLinguistics:HumanLanguageTechnolo-gies,Volume1(LongandShortPapers),pages4171\u20134186.Rich\u00e1rdFarkas,VeronikaVincze,Gy\u00f6rgyM\u00f3ra,J\u00e1nosCsirik,andGy\u00f6rgySzarvas.2010.Theconll-2010sharedtask:learningtodetecthedgesandtheirscopeinnaturallanguagetext.InProceedingsofthefour-teenthconferenceoncomputationalnaturallanguagelearning\u2013Sharedtask,pages1\u201312.BruceFraser.2010.Pragmaticcompetence:Thecaseofhedging.Newapproachestohedging,1534.ViolaGanterandMichaelStrube.2009.Findinghedgesbychasingweasels:Hedgedetectionusingwikipediatagsandshallowlinguisticfeatures.InProceedingsoftheACL-IJCNLP2009ConferenceShortPapers,pages173\u2013176.PranavGoel,YoichiMatsuyama,MichaelMadaio,andJustineCassell.2019.\u201cithinkitmighthelpifwemultiply,andnotadd\u201d:Detectingindirectnessincon-versation.In9thInternationalWorkshoponSpokenDialogueSystemTechnology,pages27\u201340.Springer.CynthiaHoward,PamelaW.Jordan,BarbaraMariaDiEugenio,andSandraKatz.2015.Shiftingtheload:apeerdialogueagentthatencouragesitshumancollab-oratortocontributemoretoproblemsolving.Interna-tionalJournalofArtificialIntelligenceinEducation,27:101\u2013129.KenHyland.1998.Hedginginscientificresearcharti-cles,volume54.JohnBenjaminsPublishing.GuolinKe,QiMeng,ThomasFinley,TaifengWang,WeiChen,WeidongMa,QiweiYe,andTie-YanLiu.2017.Lightgbm:Ahighlyefficientgradientboost-ingdecisiontree.Advancesinneuralinformationprocessingsystems,30:3146\u20133154.KlausKrippendorff.2004.Reliabilityincontentanaly-sis:Somecommonmisconceptionsandrecommen-dations.Humancommunicationresearch,30(3):411\u2013433.GeorgeLakoff.1975.Hedges:Astudyinmeaningcriteriaandthelogicoffuzzyconcepts.InContem-poraryresearchinphilosophicallogicandlinguisticsemantics,pages221\u2013271.Springer.\n2168\n\n\nMatthewLeach.2005.Rapport:Akeytotreatmentsuc-cess.Complementarytherapiesinclinicalpractice,11:262\u20135.IlyaLoshchilovandFrankHutter.2018.Decoupledweightdecayregularization.InInternationalConfer-enceonLearningRepresentations.ScottMLundberg,GabrielGErion,andSu-InLee.2018.Consistentindividualizedfeatureat-tributionfortreeensembles.arXivpreprintarXiv:1802.03888.ScottMLundbergandSu-InLee.2017.Aunifiedap-proachtointerpretingmodelpredictions.InProceed-ingsofthe31stinternationalconferenceonneuralinformationprocessingsystems,pages4768\u20134777.HowardLuneandBruceLBerg.2017.Qualitativeresearchmethodsforthesocialsciences.Pearson.MichaelMadaio,JustineCassell,andAmyOgan.2017.Theimpactofpeertutors\u2019useofindirectfeedbackandinstructions.Philadelphia,PA:InternationalSo-cietyoftheLearningSciences.BenMedlockandTedBriscoe.2007.Weaklysuper-visedlearningforhedgeclassificationinscientificliterature.InProceedingsofthe45thannualmeetingoftheassociationofcomputationallinguistics,pages992\u2013999.ChristophMolnar.2020.Interpretablemachinelearn-ing.Lulu.com.AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advancesinneuralinformationprocessingsystems,32:8026\u20138037.JamesWPennebaker,RyanLBoyd,KaylaJordan,andKateBlackburn.2015.Thedevelopmentandpsycho-metricpropertiesofliwc2015.Technicalreport.JeffreyPennington,RichardSocher,andChristopherDManning.2014.Glove:Globalvectorsforwordrep-resentation.InProceedingsofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing(EMNLP),pages1532\u20131543.EllenFPrince,JoelFrader,CharlesBosk,etal.1982.Onhedginginphysician-physiciandiscourse.Lin-guisticsandtheProfessions,8(1):83\u201397.AnnaProkofievaandJuliaHirschberg.2014.Hedgingandspeakercommitment.In5thIntl.WorkshoponEmotion,SocialSignals,Sentiment&LinkedOpenData,Reykjavik,Iceland.NilsReimersandIrynaGurevych.2019.Sentence-bert:Sentenceembeddingsusingsiamesebert-networks.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages3982\u20133992.OscarJRomero,RanZhao,andJustineCassell.2017.Cognitive-inspiredconversational-strategyreasonerforsocially-awareagents.InIJCAI,pages3807\u20133813.TimRowland.2007.\u2018wellmaybenotexactly,butit\u2019saroundfiftybasically?\u2019:Vaguelanguageinmath-ematicsclassrooms.InVaguelanguageexplored,pages79\u201396.Springer.SaschaSchneider,SteveNebel,SimonPradel,andG\u00fcn-terDanielRey.2015.Mindyourpsandqs!howpoliteinstructionsaffectlearningwithmultimedia.ComputersinHumanBehavior,51:546\u2013555.TanmaySinhaandJustineCassell.2015.Weclick,wealign,welearn:Impactofinfluenceandconvergenceprocessesonstudentlearningandrapportbuilding.InProceedingsofthe1stWorkshoponModelingINTERPERsonalSynchrONyAndInfLuence,INTER-PERSONAL\u201915,page13\u201320,NewYork,NY,USA.AssociationforComputingMachinery.HelenSpencer-Oatey.2005.(im)politeness,faceandperceptionsofrapport:Unpackagingtheirbasesandinterrelationships.1(1):95\u2013119.Gy\u00f6rgySzarvas,VeronikaVincze,Rich\u00e1rdFarkas,Gy\u00f6rgyM\u00f3ra,andIrynaGurevych.2012.Cross-genreandcross-domaindetectionofsemanticuncer-tainty.ComputationalLinguistics,38(2):335\u2013367.BuzhouTang,XiaolongWang,XuanWang,BoYuan,andShixiFan.2010.Acascademethodfordetectinghedgesandtheirscopeinnaturallanguagetext.InProceedingsoftheFourteenthConferenceonCom-putationalNaturalLanguageLearning\u2013SharedTask,pages13\u201317.MorganUlinski,SethBenjamin,andJuliaHirschberg.2018.Usinghedgedetectiontoimprovecommittedbelieftagging.InProceedingsoftheWorkshoponComputationalSemanticsbeyondEventsandRoles,pages1\u20135.TeppoVarttala.1999.Remarksonthecommunicativefunctionsofhedginginpopularscientificandspecial-istresearcharticlesonmedicine.Englishforspecificpurposes,18(2):177\u2013200.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,\u0141ukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.InAdvancesinneuralinformationpro-cessingsystems,pages5998\u20136008.ErikVelldal.2011.Predictingspeculation:asimpledis-ambiguationapproachtohedgedetectioninbiomed-icalliterature.JournalofBiomedicalSemantics,2(5):1\u201314.VeronikaVincze.2014.Uncertaintydetectioninnaturallanguagetexts.PhD,UniversityofSzeged,page141.\n2169\n\n\nNingWang,WLewisJohnson,RichardEMayer,PaolaRizzo,ErinShaw,andHeatherCollins.2008.Thepolitenesseffect:Pedagogicalagentsandlearningoutcomes.Internationaljournalofhuman-computerstudies,66(2):98\u2013112.ShawnWarner-Garcia.2014.Laughingwhennothing\u2019sfunny:Thepragmaticuseofcopinglaughterinthenegotiationofconversationaldisagreement.Prag-matics,24(1):157\u2013180.TimothyWilliamson.2002.Vagueness.Routledge.RanZhao,AlexandrosPapangelis,andJustineCassell.2014.Towardsadyadiccomputationalmodelofrap-portmanagementforhuman-virtualagentinteraction.InInternationalConferenceonIntelligentVirtualAgents,pages514\u2013527.Springer.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016a.Automaticrecognitionofconversa-tionalstrategiesintheserviceofasocially-awaredialogsystem.InProceedingsofthe17thAnnualMeetingoftheSpecialInterestGrouponDiscourseandDialogue,pages381\u2013392.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016b.Socially-awarevirtualagents:Au-tomaticallyassessingdyadicrapportfromtemporalpatternsofbehavior.InInternationalconferenceonintelligentvirtualagents,pages218\u2013233.Springer.AAdditionalinformationontheexperimentalsettingsWeusedPyTorch(Paszkeetal.,2019)toimple-menttheneuralmodels.Foreachsetoffeatures,hyperparameterswereselectedusingOptuna(Ak-iba,2019),aparametersearchframework.Were-implementedtheAttention-CNNwithGlove(Pen-ningtonetal.,2014)300-Dwordsembeddingsasthevectorrepresentation.Foreachmodels,theresultsarecross-validatedusing5folds(wechose5insteadof10toavoidhavingfoldswithtoofewsamplesperclass).Wecorrectedthelossfunctionforclassimbalancetoforcethemodeltoadaptmoretothelessfrequentclasses.Thestrengthofthiscorrectiondependedonthemodel,andwasselectedbecauseitprovidedasatisfyingcompro-misebetweenfavoringrecallandprecisionintheclassificationresultsofthatmodel.ForLightGBM,a\"squarerootofthesquarerootoftheinverseclassproportion\"correctionwasselected.Neu-ralmodelsweretrainedusingAdamWasanop-timizer(LoshchilovandHutter,2018),andusedareducedfeaturevector,obtainedwiththeap-plicationofPCA(dinit=1800;d=100;99.8%oftheinformationisconserved).Nosignifi-cantperformancedifferenceswereobservedbe-tweentheoriginalvectorandthereducedvectorfortrainingthemodels.TocomputetheSHAPvaluesmentionedinthepaper,wekeptonesplittoperformthe5-splitofthedataset,andleave1splittovalidateandearlystopthemodel,inor-dertoavoidoverfitting.Acompleteconfigura-tionofhyperparametersusedforeachmodelisre-portedintheGitHubrepositorywiththecodeofthepaper:https://github.com/YannRaphalen/Hedges-Detection.TheBERTmodelwasfine-tunedonaNvidiaQuadroRTX8000GPU.BTables\n2170\n\n\n?(whether|if|is|that|it|this)?.*Subj.\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nNoNoYesNoNoNoYesYesYesYesYesBERT(PTE)\nexceptionally|forthemostpart|inamannerofspeaking|\nNoYesNoNoNoYesNoNoYesYesNoMLP(KDF)\nsomethingbetween|essentially|only).*Prop.\nTable8:Significancetableforthe3-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesYesYesYesYesNoYesYesYesNoYesMLP(KDF+PTE)\ntechnically|typically|virtually|approximately|\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nmuch|somewhat|exactly|almost|littlebit|quite|\nClass\nrelatively|roughly|sotosay|strictlyspeaking|\nNoNoYesNoNoYesNoYesYesYesYesLSTM(KDF)\nYesNoNoNoNo+TM\nTable10:Significancetableforthe3-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoNoYesYesYesYesYesYesNoYesYes\n.*(it)(looks|seems|appears)[,]?.*\",\".*(or|and)(that|something|stuff|soforth)Table7:Regexprulesusedfortheclassifier.\nRule-based\nRule-based\n.*(you(might|may)(believe|think)).*Subj.\nYesYesYesYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nYesYesNoYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nNoNoYesNoNoYesNoNoYesYesYesLSTM(KDF)\nYesYesNoYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(i|i\u2019m|you|it\u2019s)(am|are)(apparently|surely)[,]?.*Prop.\nNoNoYesNoNoYesNoYesYesYesYesLSTM(PTE)\nNoNoYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nNoNoYesYesNoYesYesYesYesYesNoLGBM(KDF)\nNoNoYesYesYesYesYesYesYesYesNoLGBM(KDF)\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nYesNoYesYesYesYesYesYesNoYesYes\nYesYesYesYesYesYesYesYesYesYesYesMLP(KDF+PTE)\nregular|regularly|actually|almost|asitwere|basically|\nNoYesYesNoYesYesYesNoYesYesNoMLP(PTE)\nRule(regexp)\n.*(i|i\u2019m|we)?(was|am|wasn\u2019t)??(not)?(sure|certain).*Subj.\n.*(accordingto|presumably).*Subj.\n(?!.*(be|been|was)likeexcuseme)((excuseme|sorry)[w,\u2019]+|[w,\u2019]+(excuseme|sorry))Prop.\n.*(i|you|we)haveto(check|look|verify).*Subj.\n.*(i\u2019m|i|we\u2019re)(am|are)??(apologize|sorry).*Apol.\nonthetallside|parexcellence|particularly|\n(?!what).*(i|we)?(don\u2019t|didn\u2019t|did)??(not)?\ninarealsense|inasense|inaway|largely|literally|\nYesYesYesNoNoNoYesYesYesYesYesBERT(PTE)\nModels\nModels\nModels\nYesYesYesNoNoYesNoNoYesYesYesLSTM(PTE)\nYesNoNoNoNo\nYesYesYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nSubj.\nTable9:Significancetableforthe4-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoYesNoNoNoYesNoNoYesYesNoMLP(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(ifeellikeyou).*Subj.\nlooselyspeaking|kinda|moreorless|mostly|often|\n.*(unlessi).*Apol.\nprobably|canbeviewas|crypto-|especially|essentially|\n.*(ifi\u2019mnotwrong|ifi\u2019mright|ifthat\u2019strue).*Subj.\nYesNoNoNoNo+POS\n(guess|guessed|thought|think|believe|believed|suppose|supposed)\n.*(just|alittle|maybe|actually|sortof|kindof|pretty\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nNoYesYesNoYesYesNoNoYesYesYesMLP(KDF)\nprettymuch|principally|pseudo-|quintessentially|\n2171\n\n\nFigure2:Absoluteaveragedfeaturecontribution,asindicatedbySHAP.Thelongerthebarisforonecolor,themorethefeatureisassociatedwiththeclassrepresentedbythatcolor.\nFigure3:Averagedcontributionoffeaturestothedetectionofthe\"Notindirect\"class,asindicatedbySHAP.Eachdotcorrespondstoaclassifiedclause.Areddotindicatesthatthefeatureispresentintheclause,whileabluedotindicatesthatthefeatureisabsent.Thefartherontherightthedotis,themorethefeaturecontributedtoitsclassificationasahedge.\n2172\n\n\nFigure4:Averagedcontributionoffeaturestothedetectionof\"Apologizers\",asindicatedbySHAP.\nFigure5:Averagedcontributionoffeaturestothedetectionof\"Propositionalhedges\",asindicatedbySHAP.\n2173\n\n\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nYesNoNoNoNo+TM\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nFigure6:Averagedcontributionoffeaturestothedetectionof\"Subjectivizers\",asindicatedbySHAP.\nModels\nYesNoNoNoNo\nTable11:Significancetableforthe4-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesNoNoNoNo+POS\n2174"}, {"question": " What linguistic tool was used to extract words belonging to specific psycho-social categories for hedge detection?", "answer": " Linguistic Inquiry and Word Count (LIWC)", "ref_chunk": "\"Youmightthinkaboutslightlyrevisingthetitle\":IdentifyingHedgesinPeer-tutoringInteractionsYannRaphalen1,Chlo\u00e9Clavel2,JustineCassell1,31InriaParis2LTCI,InstitutPolytechniquedeParis,Telecom-Paris3CarnegieMellonUniversityyann.raphalen.pro@gmail.com,justine@cs.cmu.edu,chloe.clavel@telecom-paris.frAbstractHedgesplayanimportantroleinthemanage-mentofconversationalinteraction.Inpeer-tutoring,theyarenotablyusedbytutorsindyads(pairsofinterlocutors)experiencinglowrapporttotonedowntheimpactofinstructionsandnegativefeedback.Pursuingtheobjectiveofbuildingatutoringagentthatmanagesrap-portwithstudentsinordertoimprovelearning,weusedamultimodalpeer-tutoringdatasettoconstructacomputationalframeworkforiden-tifyinghedges.Wecomparedapproachesre-lyingonpre-trainedresourceswithothersthatintegrateinsightsfromthesocialsciencelitera-ture.Ourbestperformanceinvolvedahybridapproachthatoutperformstheexistingbase-linewhilebeingeasiertointerpret.Weemployamodelexplainabilitytooltoexplorethefea-turesthatcharacterizehedgesinpeer-tutoringconversations,andweidentifysomenovelfea-tures,andthebenefitsofsuchahybridmodelapproach.1IntroductionRapport,mostsimplydefinedasthe\u201c...relativeharmonyandsmoothnessofrelationsbetweenpeo-ple...\u201d(Spencer-Oatey,2005),hasbeenshowntoplayaroleinthesuccessofactivitiesasvariedaspsychotherapy(Leach,2005)andsurveyinterview-ing(LuneandBerg,2017).Inpeer-tutoring,rap-port,asmeasuredbytheannotationofthinslicesofvideo,hasbeenshowntobebeneficialforlearningoutcomes(Zhaoetal.,2014;SinhaandCassell,2015).Thelevelofrapportrisesandfallswithconversationalstrategiesdeployedbytutorsandtuteesatappropriatetimes,andasafunctionofthecontentofpriorturns.Thesestrategiesincludeself-disclosure,referringtosharedexperience,and,onthepartoftutors,givinginstructionsinanindirectmanner.Someworkhasattemptedtoautomaticallydetectthesestrategiesintheserviceofintelligenttutors(Zhaoetal.,2016a),butonlyafewstrate-gieshavebeenattempted.Otherworkhascon-centratedona\"socialreasoningmodule\"(Romeroetal.,2017)todecidewhichstrategiesshouldbegeneratedinagivencontext,butindirectnesswasnotamongthestrategiestargeted.Inthispaper,wefocusontheautomaticclassificationofonespe-cificstrategythatisparticularlyimportantforthetutoringdomain,andthereforeimportantforintel-ligenttutors:hedging,asub-partofindirectnessthat\"softens\"whatwesay.Thisworkispartofalargerresearchprogramwiththelong-termgoalofautomaticallygeneratingindirectnessbehaviorsforatutoringagent.\nFigure1:Amockconversationdisplayingeachtypeofhedgedformulation.AccordingtoBrownandLevinson(1987),hedgesarepartofthelinguistictoolsthatinterlocu-torsusetoproducepoliteness,bylimitingthefacethreattotheinterlocutor(basicallybylimitingtheextenttowhichtheinterlocutormightexperienceembarrassmentbecauseofsomekindofpoorper-formance).Anexampleis\"that\u2019skindofawronganswer\".Hedgesarealsofoundwhenspeakerswishtoavoidlosingfacethemselves,forexam-plewhensaying(\"IthinkImighthavetoadd6.\").Madaioetal.(2017)foundthatinapeer-tutoringtask,whenrapportbetweeninterlocutorsislow,tu-teesattemptedmoreproblemsandcorrectlysolvedmoreproblemswhentheirtutorshedgedinstruc-\n2160 Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1: Long Papers, pages 2160 - 2174 May 22-27, 2022 c(cid:13)2022 Association for Computational Linguistics\n\n\ntions,whichlikewisepointstowardsa\"mitigationoffacethreat\"function.Hedgescanalsobeasso-ciatedwithanonverbalcomponent,forexampleavertedeyegazeduringcriticism(BurgoonandKoper,1984).Hedgesarenot,however,alwaysap-propriate,asin\"Ikindofthinkit\u2019srainingtoday.\"whentheinterlocutorscanbothseerain(althoughitmightbetakenashumorous).Thesefactsabouthedgesmotivateawaytoautomaticallydetectthemand,ultimately(althoughnotinthecurrentwork)alsogeneratethem.Inbothcaseswefirsthavetobeabletocharacterizethemusinginterpretablelinguisticfeatures,whichiswhatweaddressinthecurrentpaper.Thus,intheworkdescribedhere,basedonlinguisticdescriptionsofhedges(BrownandLevinson,1987;Fraser,2010),webuiltarule-basedclassifier.Weshowthatthisclassifierincombinationwithadditionalmultimodalinter-pretablecontext-dependentfeaturessignificantlyimprovestheperformanceofamachinelearningmodelforhedges,comparedtoalessinterpretabledeeplearningbaselinefromGoeletal.(2019)us-ingwordembeddings.Wealsoreliedonamachinelearningmodelexplanationtool(LundbergandLee,2017)toinvestigatethelinguisticfeaturesrelatedtohedgesinthecontextofpeer-tutoring,primarilytoseeifwecoulddiscoversurprisingfeaturesthattheclassificationmodelwouldassociatetohedgesinthiscontext,andwedescribethosebelow.Thecodeofthemodelsdescribedinthepaperisalsoprovided.12RelatedworkHedges:AccordingtoFraser(2010),hedgingisarhetoricalstrategythatattenuatesthestrengthofastatement.Onewaytoproduceahedgeisbyalteringthefullsemanticvalueofaparticu-larexpressionthroughPropositionalhedges(alsocalledApproximatorsinPrinceetal.(1982)),asin\"Youarekindofwrong,\"thatreduceprototypical-ity(i.eaccuracyofthecorrespondencebetweenthepropositionandtherealitythatthespeakerseekstodescribe).Propositionalhedgesarerelatedtofuzzylanguage(Lakoff,1975),andthereforetotheproductionofvagueness(Williamson,2002)anduncertainty(Vincze,2014).AsecondkindareRelationalHedges(alsocalledShieldsinPrinceetal.(1982)),suchas\u201cIthinkthatyouarewrong.\u201dor\u201cThedoctorwantsyoutostopsmoking.\u201d,conveyingthatthepropositionis\n1https://github.com/AnonymousHedges/HedgeDetectionconsideredbythespeakerassubjective.Inafurthersub-division,AttributionShields,asin\"Thedoc-torwantsyou...\",theinvolvementofthespeakerinthetruthvalueofthepropositionisnotmadeexplicit,whichallowsspeakersnottotakeastance.Asdescribedabove,Madaioetal.(2017)foundthattutorswhoshowedlowerrapportwiththeirtuteesusedmorehedgedinstructions(theyalsoemployedmorepositivefeedback),howeverthiswasonlythecasefortutorswithagreaterbeliefintheirabilitytotutor.Tuteesinthiscontextsolvedmoreproblemscorrectlywhentheirtutorshedgedinstructions.Noeffectofhedgingwasfoundfordyads(pairsofinterlocutors)withgreatersocialcloseness.However,theauthorsdidnotlookatthespecificlinguisticformstheseteenagersused.Rowland(2007)alsodescribestherolethathedg-ingplaysinthisagegroup,showingthatstudentsusebothrelational(\"IthinkthatJohnissmart.\")andpropositional(\"Johniskindofsmart.\")hedgesformuchthesameshieldingfunctionofdemon-stratinguncertainty,tosavethemfromtheriskofembarrassmentiftheyarewrong.TheauthorobservedthatteensusedfewAdaptors(kindof,somewhat)andpreferredtouseRounders(around,closeto).However,thisstudywasperformedwithanadultandtwochildren,possiblybiasingthere-sultsduetotheparticipationoftheadultinvestiga-tor.Hedgeshavebeenincludedinvirtualtutoringagentsbeforenow.(Howardetal.,2015)integratedhedgesinatutoragentforundergraduatesinCS,asawaytoencouragethestudenttotaketheinitiative.Hedgeshavealsobeenusedasawayofintegrat-ingBrownandLevinson\u2019spolitenessframework(Wangetal.,2008;Schneideretal.,2015)invir-tualtutoringagents.Resultswerenotbrokenoutbystrategy,butpolitenessingeneralwasshowntopositivelyinfluencemotivationandlearning,incertainconditions.Computationalmethodsforhedgedetection:Anumberofstudieshavetargetedthedetectionofhedgesanduncertaintyintext(MedlockandBriscoe,2007;GanterandStrube,2009;Tangetal.,2010;Velldal,2011;Szarvasetal.,2012),partic-ularlyfollowingtheCoNLL2010datasetrelease(Farkasetal.,2010).However,thisworkisnotasrelatedtohedgesinconversation,asitfocusesonaformalandacademiclanguageregister(Hy-land,1998;Varttala,1999).AsnotedbyProkofievaandHirschberg(2014),thefunctionsofhedgesaredomain-andgenre-dependent,thereforethisbias\n2161\n\n\ntowardsformalityimpliesthattheexistingworkmaynotadaptwelltothedetectionofhedgesinconversationbetweenteenagers.Aconsequenceisthattheexistingworkdoesnotconsidertermslike\"Ithink,\"sinceopinionsrarelyappearinanaca-demicwritingdataset.Instructionsarealsoalmostabsent(\"Ithinkyouhavetoaddtentobothsides.\"),astronglimitationforthestudyofconversationalhedgessinceitisinrequests(includingtutoringin-structions)thatindirectformulationsmostlyoccuraccordingtoBlum-Kulka(1987).ProkofievaandHirschberg(2014)alsonotethatitisdifficulttodetecthedgesbecausethewordpatternsassociatedwiththemhaveothersemanticandpragmaticfunc-tions:considering\"Ithinkthatyouhavetoaddxtobothsides.\"vs\"Ithinkthatyouareanidiot.\",itisnotclearthattheseconduseof\"Ithinkthat\"isanhedgemarker.Theyadvocateusingmachinelearningapproachestodealwiththeambiguityofthesemarkers.Workingonaconversationaldataset,Ulinskietal.(2018)builtacomputationalsystemtoassessspeakercommitment(i.e.atwhichpointthespeakerseemsconvincedbythetruthvalueofastatement),inparticularbyrelyingonarule-baseddetectionsystemforhedges.Comparedtothatwork,ourrule-basedclassificationmodelisdirectlydetectinghedgeclasses,andweemploythepredictionsoftherule-basedmodelasafeatureforstrongermachinelearningmodels,designedtolessentheimpactoftheimbalancebetweenclasses.Wealsoconsiderapologieswhentheyserveamit-igationfunction(wethencallthemApologizers),aswasdonebytheauthorsofourcorpus,andwealsousethetermsubjectivizersasdefinedbelow,tobeabletocomparedirectlywiththepreviousworkcarriedoutonthiscorpus.Asfarasweknow,onlyGoeletal.(2019)haveworkedwithapeer-tutoringdataset(thesameonethatwealsouse),andtheyachievedtheirbestclassificationresultbyemployinganAttention-CNNmodel,inspiredbyAdelandSch\u00fctze(2017).3ProblemstatementWeconsiderasetDofconversationsD=(c1,c2,...,c|D|),whereeachconversationiscom-posedofasequenceofindependentsyntacticclausesci=(u1,u2,...,uM),whereMisthenumberofclausesintheconversation.Notethattwoconsecutiveclausescanbeproducedbythesamespeaker.Eachclauseisassociatedwithauniquelabelcorrespondingtothediffer-enthedgeclassesdescribedinTable1:yi\u2208C={PropositionalHedges,Apologizers,Subjec-tivizers,Nothedged}.Finally,anutteranceuicanberepresentedasavectoroffeaturesX=(x1,x2,...,xN),whereNrepresentsthenumberoffeaturesweusedtodescribeaclause.Ourfirstgoalistodesignamodelthatcorrectlypredictsthelabelyiassociatedtoui.Itcanbeunderstoodasthefollowingresearchquestion:RQ1:\"Whichmodelsandfeaturescanbeusedtoautomaticallycharacterizehedgesinapeer-tutoringinteraction?\"Oursecondgoalistoidentify,foreachhedgeclass,thesetoffeaturesFclass={fk},k\u2208[1,N]sortedbyfeatureimportanceintheclassificationofclass.Itcorrespondstothefollowingresearchquestion:RQ2:\"Whatarethemostimportantlinguisticfeaturesthatcharacterizeourhedgeclassesinapeer-tutoringsetting?\"4Methodology4.1CorpusDatacollection:Thedialoguecorpususedherewascollectedaspartofalargerstudyontheeffectsofrapport-buildingonreciprocalpeertutoring.24Americanteenagers(meanage=13.5,min=12,max=15),halfmaleandhalffemale,cametoalabwherehalfoftheparticipantswerepairedwithasame-age,same-genderfriend,andtheotherhalfpairedwithastranger.Theparticipantswereassignedtoatotalof12dyadsinwhichthepar-ticipantsalternatedtutoringoneanotherinlinearalgebraequationsolvingfor5weeklyhour-longsessions,foratotalcorpusofnearly60hoursofface-to-faceinteractions.Eachsessionwasstruc-turedsuchthatthestudentsengagedinbriefsocialchitchatinthebeginning,thenoneofthestudentswasrandomlyassignedtotutortheotherfor20minutes.Theythenengagedinanothersocialpe-riod,andconcludedwithasecondtutoringperiodwheretheotherstudentwasassignedtheroleoftutor.Audioandvideodatawererecorded,tran-scribed,andsegmentedforclause-leveldialogueannotation,providingnearly24000clauses.Non-speechsegments(notablyfillersandlaughter)weremaintained.Becauseoftemporalmisalignmentforpartsofthecorpus,manyparaverbalphenomena,suchasprosody,wereunfortunatelynotavailabletous.SinceouraccesstothedatasetiscoveredbyaNon-DisclosureAgreement,itcannotbereleased\n2162\n\n\n2https://github.com/AnonymousHedges/HedgeDetectionclasses.4.2FeaturesLabelfromrule-basedclassifier(LabelRB):Weusetheclasslabelpredictedbytherule-basedclas-sifierdescribedinSection4.3asafeature.Ourhypothesisisthatthemachinelearningmodelcanusethisinformationtocounterbalancetheclassimbalance.Totakeintoaccountthefactthatsomerulesaremoreefficientthanothers,weweightedtheclasslabelresultingfromtherule-basedmodelbytheprecisionoftherulethatgeneratedit.Unigramandbigram:Wecountthenumberofoccurrencesofunigramsandbigramsofthecorpusineachclause.Weusedthelemmaofthewordsforunigramsandbigramsusingthenltklemmatizer(Loper,2002)andselectedunigramsandbigramsthatoccurredinthetrainingdatasetatleastfiftytimes.Thegoalwastoinvestigate,withabottom-upapproach,towhatextenttheuseofcertainwordscharacterizeshedgeclassesintutoring.InSection5weexaminetheoverlapbetweenthesewordsandthoseaprioriidentifiedbytherules.Part-of-speech(POS):Hedgeclassesseemtobeassociatedwithdifferentsyntacticpatterns:forex-ample,subjectivizersmostoftencontainapersonalpronounfollowedbyaverb,asin\"Iguess\",\"Ibelieve\",\"Ithink\".WethereforeconsideredthenumberofoccurrencesofPOS-Tagn-grams(n=1,2,3)asfeatures.WeusedthespaCyPOS-taggerandconsideredPOSunigrams,bigramsandtri-gramsthatoccuratleast10timesinthetrainingdataset.LIWC:LinguisticInquiryandWordCount(LIWC)(Pennebakeretal.,2015)isstandardsoft-wareforextractingthecountofwordsbelongingtospecificpsycho-socialcategories(e.g.,emotions,religion).Ithasbeensuccessfullyusedinthede-tectionofconversationalstrategies(Zhaoetal.,2016a).Wethereforecountthenumberofoccur-rencesofallthe73categoriesfromLIWC.Tutoringmoves(TM):Intelligenttutoringsys-temsrelyonspecifictutoringmovestosuccess-fullyconveycontent(asdohumantutors).Wethereforelookedatthelinkbetweenthetutoringmoves,asannotatedinMadaioetal.(2017),andhedges.Fortutors,thesemovesare(1)instruc-tionaldirectivesandsuggestions,(2)feedback,and(3)affirmations,mostlyexplicitreflectionsontheirpartners\u2019comprehension,whilefortutees,theyare(1)questions,(2)feedbacks,and(3)affirmations,\npublicly.Howevertheoriginalexperimenters\u2019In-stitutionalReviewBoard(IRB)approvalallowsustoview,annotate,andusethedatatotrainmodels.Thisalsoallowsustoprovidealinktoapixe-latedvideoexampleintheGitHubrepositoryoftheproject2.Dataannotation:Thedatasetwaspreviouslyan-notatedbyMadaioetal.(2017),followingananno-tationmanualthatusedhedgeclassesderivedfromRowland(2007)(seeTable1).Onlythetaskperi-odsoftheinteractionswereannotated.Comparingtheannotationswiththeclassesmentionedintherelatedworksection,SubjectivizerscorrespondtoRelationalhedges(Fraser,2010),PropositionalhedgesandExtenderscorrespondtoApproxima-tors(Princeetal.,1982)withtheadditionofsomediscoursemarkerssuchasjust.ApologizersarementionedaslinguistictoolsrelatedtonegativepolitenessinBrownandLevinson(1987).Krippen-dorff\u2019salphaobtainedforthiscorpusannotatedbyfourcoderswasover0.7forallclasses(denotinganacceptableinter-coderreliabilityaccordingtoKrippendorff(2004)).Thedatasetiswidelyim-balanced,withmorethan90%oftheutterancesbelongingtotheNothedgedclass.Inreviewingthecorpusandtheannotationman-ual,however,wenoticedtwoissues.First,theannotationoftheExtendersclasswasinconsis-tent,leadingtotheExtendersandPropositionalhedgesclassescarryingsimilarsemanticfunctions.WethereforemergedthetwoclassesandgroupedutteranceslabeledasExtendersandthoselabeledasPropositionalhedgesundertheheadingofPropositionalhedges.Second,theannotationofclausescontainingthetokens\"just\"and\"would\"(twotermsoccurringfrequentlyinthedatasetthatarekeycomponentsofPropositionalHedgesandSubjectivizersbutthatarenotinfacthedgesinallcases)wasalsoinconsistent,leadingtovirtuallyallclauseswiththosetwotokensbeingconsideredhedges.Wethereforere-consideredalltheclausesassociatedwithanyofthehedgeclasses,aswellasalltheclausesinthe\"Nothedged\"classthatcontained\"just\"or\"would\".There-annotationwascarriedoutbytwoannotatorswhoachievedaKrippendorff\u2019salphainter-raterreliabilityof.9orbetterforApologizers,Subjectivizers,andPropo-sitionalhedgesbeforeindependentlyre-annotatingtherelevantclauses.Anexampleofare-annotationwasremoving\"Iwouldkillyou!\"fromthehedge\n2163\n\n\nProp.hedgesApologizersSubjectivizersNothedgedTotal\nTable1:Definitionoftheclasses\nFeaturesnameAutomaticextractionVectorsize\nSubjectivizersWordsthatreduceintensityorcertainty\u201cSothenIwoulddividebytwo.\u201dApologizersApologiesusedtosoftendirectspeechacts\u201cOhsorrysixb.\u201dPropositionalhedgesQualifyingwordstoreduceintensityorcertaintyofutterances\u201cIt\u2019sactuallyeight.\u201dExtendersWordsusedtoindicateuncertaintybyreferringtovaguecategories\u201cIt\u2019llbethenumberxorwhatevervariableyouhave.\u201d\n12101286262119223156\nTable2:Distributionoftheclasses\nClassDefinitionExample\nRule-basedlabelYes4UnigramYes~250BigramYes~250POSYes~1200LIWCYes73NonverbalNo24TutoringmovesNo6Total~1800\nTable3:Listofautomaticallyextractedandmanuallyannotatedfeatureswiththeirsize.mostlytentativeanswers.Nonverbalandparaverbalbehaviors:AsinGoeletal.(2019),weincludedthenonverbalandpar-averbalbehaviorsthatarerelatedtohedges.Specif-ically,weconsiderlaughterandsmiles,thathavebeenshowntobeeffectivemethodsofmitiga-tion(Warner-Garcia,2014),cut-offsindicatingself-repairs,fillerslike\"Um\",gazeshifts(annotatedas\u2019GazeatPartner\u2019,\u2019GazeattheMathWorksheet\u2019,and\u2019Gazeelsewhere\u2019),andheadnods.Eachfea-turewaspresenttwiceinthefeaturevector,onetimeforeachinterlocutor.Inter-raterreliabilityfornonverbalbehaviorwas0.89(asmeasuredbyKrippendorff\u2019salpha)foreyegaze,0.75forsmilecount,0.64forsmiledurationand0.99forheadnod.Laughterisalsoreportedinthetranscriptatthewordlevel.Weseparatethetutor\u2019sbehaviorsfromthoseofthetutee.ThecollectionprocessforthesebehaviorsisdetailedfurtherinZhaoetal.(2016b).Theclause-levelfeaturevectorwasnormalizedbythelengthoftheclause(exceptfortherule-basedlabel).Thislengthwasalsoaddedasafeature.Table3presentsanoverviewofthefinalfeaturevector.4.3ClassificationmodelsTheclassificationmodelsusedarepresentedhereaccordingtotheirlevelofintegrationofexternallinguisticknowledge.Rule-basedmodel:OnthebasisoftheannotationmanualusedtoconstructthedatasetfromMadaioetal.(2017),andwithdescriptionsofhedgesfromRowland(2007),Fraser(2010)andBrownandLevinson(1987),weconstructedarule-basedclas-sifierthatmatchesregularexpressionsindicativeofhedges.TherulesaredetailedinTable7intheAppendix.LGBM:Sincehedgesareoftencharacterizedbyexplicitlexicalmarkers,wetestedtheassumptionthatamachinelearningmodelwithaknowledge-drivenrepresentationforclausescouldcompetewithaBERTmodelinperformance,whilebeingmuchmoreinterpretable.WereliedonLightGBM,anensembleofdecisiontreestrainedwithgradi-entboosting(Keetal.,2017).Thismodelwasselectedbecauseofitsperformancewithsmalltrainingdatasetsandbecauseitcanignoreunin-formativefeatures,butalsoforitstrainingspeedcomparedtoalternativeimplementationsofgradi-entboostingmethods.Multi-layerperceptron(MLP):Asasimplebase-line,webuiltamulti-layerperceptronusingthreesetsoffeatures:apre-trainedcontextualrepre-sentationoftheclause(SentBERT;ReimersandGurevych(2019));theconcatenationofthiscon-textualrepresentationoftheclauseandarule-basedlabel(notrelyingonthepreviousclauses);andfinallytheconcatenationofallthefeaturesmen-tionedinsection4.2,withoutthecontextualizedrepresentation.LSTMoverasequenceofclauses:Sinceweareworkingwithconversationaldata,wealsowantedtotestwhethertakingintoac-countthepreviousclauseshelpstodetectthetypeofhedgeclassinthenextclause.Formally,wewanttoinferyiusingyi=maxy\u2208ClassesP(y|X(ui),X(ui\u22121),...,X(ui\u2212K)),whereKisthenumberofpreviousclausesthatthemodelwilltakeintoaccount.The\n2164\n\n\nMLPmodelpresentedaboveinfersyiusingyi=maxy\u2208ClassesP(y|X(ui)),thereforeadifferenceofperformancebetweenthetwomodelswouldbeasignthatusinginformationfromthepreviousclausescouldhelptodetectthehedgedformulationinthecurrentclause.WetestedaLSTMmodelwiththesamerepresentationsforclausesasfortheMLPmodel.CNNwithattention:Goeletal.(2019)estab-lishedtheirbestperformanceonhedgedetec-tionusingaCNNmodelwithadditiveattentionoverword(andnotclause)embeddings.Con-trarytotheMLPandLSTMmodelsmentionedabove,thismodeltriestoinferyiusingyi=maxy\u2208ClassesP(y|g(w0),g(w1),...,g(wL)),withLrepresentingthemaximumclauselengthweal-low,andgrepresentingafunctionthatturnsthewordwj,j\u2208[0,L]intoavectorrepresentation(formoredetails,pleaseseeAdelandSch\u00fctze(2017)).BERT:Tobenefitfromdeepsemanticandcon-textualrepresentationsoftheutterances,wealsofine-tunedBERT(Devlinetal.,2019)onourclas-sificationtask.BERTisapre-trainedTransformersencoder(Vaswanietal.,2017)thathassignificantlyimprovedthestateoftheartonanumberofNLPtasks,includingsentimentanalysis.Itproducesacontextualrepresentationofeachwordinasen-tence,makingitcapableofdisambiguatingthemeaningofwordslike\"think\"or\"just\"thatarerepresentativeofcertainclassesofhedges.BERT,however,isnotablyhardtointerpret.4.4AnalysistoolsLookingatwhichfeaturesimprovetheperfor-manceofourclassificationmodelstellsuswhetherthesefeaturesareinformativeornot,butdoesnotexplainhowthesefeaturesareusedbythemod-elstomakeagivenprediction.Wethereforepro-ducedacomplementaryanalysisusinganinter-pretabilitytool.Asdemonstratedby(LundbergandLee,2017),LightGBMinternalfeatureimpor-tancescoresareinconsistentwithboththemodelbehaviorandhumanintuition,soweinsteadusedamodel-agnostictool.SHAP(LundbergandLee,2017)assignstoeachfeatureanimportancevalue(calledShapleyvalues)foraparticularpredictiondependingontheextentofitscontribution(ade-tailedintroductiontoShapleyvaluesandSHAPcanbefoundinMolnar(2020)).SHAPisamodel-agnosticframework,thereforethevaluesassoci-atedwithasetoffeaturescanbecomparedacrossmodels.ItshouldbenotedthatSHAPproducesexplanationsonacase-by-casebasis,thereforeitcanbothprovidelocalandglobalexplanations.FortheGradientBoostingmodel,weuseanadaptedversionofSHAP(Lundbergetal.,2018),calledTreeSHAP.5Experimentsandresults5.1ExperimentalsettingTodetectthebestsetoffeatures,weusedLight-GBMandproceededincrementally,byaddingthegroupoffeatureswethoughttobemostlikelyasso-ciatedwithhedges.Wedidnotconsidertheriskofrelyingonasub-optimalsetoffeaturesthroughthisprocedurebecauseofthestrongabilityofLight-GBMtoignoreuninformativefeatures.Weusethisincrementalapproachasawaytotestourintuitionabouttheperformativityofgroupsoffeatures(i.e.doesaddingafeatureimprovetheperformanceofthemodel)withregardtothetaskofclassifica-tion.Tocompareourmodels,wetrainedthemonthe4-classtask,andlookedattheaverageoftheweightedF1-scoresforthethreehedgeclasses(i.e.howwellthemodelsinferminorityclasses)thatwereporthereas\"3-classes\",andattheaverageoftheweightedF1-scoresforthe4classes,thatwereportas\"4-classes\".DetailsofthehyperparametersandexperimentalsettingsareprovidedinAppendixA.5.2ModelcomparisonandfeatureanalysisOverallresults:Table4presentstheresultsob-tainedbythe6modelspresentedinSection4.3forthemulti-classproblem.Bestperformance(F1-scoreof79.0)isobtainedwithLightGBMlever-agingalmostallthefeatures.Intheappendix(seeTable8andTable9)weindicatetheconfidenceintervalstorepresentthesignificanceofthediffer-encesbetweenthemodels.First,andperhapssurprisingly,wenoticethattheuseof\"Knowledge-Driven\"featuresbasedonrulesbuiltfromlinguisticknowledgeofhedgesintheLightGBMmodeloutperformstheuseofpre-trainedembeddingswithinafine-tunedBERTmodel(79.0vs.70.6),andintheneuralbaselinefrom(Goeletal.,2019)(79.0vs64.5).ThelowscoresobtainedbytheLGBM,LSTMandMLPmodelswithpre-trainedsentenceem-beddingsversusKnowledge-Drivenfeaturesmightsignalthatthewordpatternscharacterizinghedgesarenotsalientintheserepresentations(i.e.the\n2165\n\n\n68.5(1.6)35.8(3.1)64.8(1.1)Attention-CNN(3-classes)\n65.1(5.7)39.8(8.0)65.2(5.1)BERT(3-classes)\n\u220570.6(2.3)\u2205LGBM(3-classes)\n79.0(1.3)35.0(2.2)70.1(1.4)\nRule-based(3-classes)\n94.7\u2205\u2205MLP(4-classes)\n3Notethatthereisstrongredundancybetweensomefea-turesofLIWCandthespaCyPOStaggerthatbothproducea\"Pronoun\"category,usingalexiconinthefirstcase,andaneuralinferenceinthesecond.\n\u220594.9(0.4)\u2205LGBM(4-classes)\nRule-based(4-classes)\nTable4:AveragedweightedF1-scores(andstandarddeviation)forthethreeminorityclassesandforthe4classes,forallmodels.\"KD\"standsfor\"Knowledge-Driven\",meaningthatthefeaturesarederivedfromlexicon,n-grammodelsandannotations.distancebetween\"Ithinkyoushouldadd5.\"and\"Youshouldadd5.\"isshort.).KDFeaturesseemtoprovideabetterseparabilityoftheclasses.ThecombinationofKDfeaturesandPre-trainedem-beddingsdoesnotsignificantlyimprovetheperfor-manceofthemodelscomparedtotheKDFeaturesonly,whichsuggeststhattheinformationfromthePre-trainedembeddingsisredundantwiththeonefromtheKDFeatures.Thisresultmaybeduetothehighdimensionalityoftheinputvector(868withPCAontheKDFeatures;2500otherwise).Asecondfindingisthattheuseofgradientboost-ingmodelsontopofrule-basedclassifiersbettermodelsthehedgeclasses.Theothermachinelearn-ingmodelsdidnotprovetobeaseffective,exceptforBERT.FeatureanalysisusingLightGBM:Usingthebestperformingmodel,Table5showstheroleofeachfeaturesetinthepredictiontask.ThesignificanceofthedifferencesisshowninTable10andTable11.Comparedtotherule-basedmodel,theintroductionofn-gramssignificantlyimprovedtheperformanceofourclassifier,suggestingthatsomelexicalandsyntacticinformationdescribingthehedgeclasseswasnotpresentintherule-basedmodel.LookingatTable5,wedonotobservesignificantdifferencesbetweentheLGBMmodelusingonlythelabelrulebased+(1-gramsand2-grams)andthemodelsin-corporatingmorefeatures.Tooursurprise,neitherthetutoringmovesnorthenonverbalfeaturessig-nificantlyimprovedtheperformanceofthemodel.The2featureswereincludedtoindexthespecificpeertutoringcontextofthesehedges,sothisindi-catesthatinfutureworkwemightwishtoapplythecurrentmodeltoanothercontextofusetoseeifthismodelofhedgesismoregenerallyapplicablethanweoriginallythought.Bycombiningthisresultwiththeincreasedperformanceofthemodelus-ingKnowledge-Driven(i.e.explicit)featurescom-paredtopre-trainedembeddings,itwouldseemthathedgesareaboveallalexicalphenomenon(i.e.producedbyspecificlexicalelements).5.3In-depthanalysisoftheinformativefeaturesWetrainedtheSHAPexplanationmodelsonLight-GBMwithallfeatures.Themostinformativefea-tures(inabsolutevalue)foreachclassareshowninTable6,andtheplotsbyclassarepresentedintheAppendix.Themostimportantfeaturesseemtobetherule-basedlabels,whichappearinatleastthefourthpositionforthreeclasses(seeTable6),andinthefirstpositionforPropositionalHedgesandNothedgedclasses.Surprisingly,theRule-Basedlabeldoesnotappearinthetop20featuresforApologizers.However,giventhattheclassrarelyappearsinthedata,therulesseldomactivate,sothefeaturemaysimplybeinformativeforaverysmallnumberofclauses.Unigrams(Oh,Sorry,just,Would,andI)arealsopresentinthe5top-rankedfeatures.Thisconfirmsthefindingsmen-tionedinrelatedworkforthecharacterizationofthedifferenthedgeclasses(justwithPropositionalHedges,sorrywithApologizer,IwithSubjectiviz-ers).ThepresenceofOhalsohashighimportanceforthecharacterizationofApologizer(n=2),asillustratedinexamplessuchas\"Ohsorry,that\u2019snine.\".Wenotethattheoccurrencesof\"Ohsorry\"asastand-aloneclausewereexcludedbyourrule-basedmodelbecausetheydonotcorrespondtoanapologizer(theycannotmitigatethecontentofapropositionifthereisnopropositionassociated).Thisexampleillustratestheinterestofamachinelearningmodelapproachtodisambiguatethefunc-tionofconventionalnon-propositionalphraseslike\"Ohsorry\".Inaddition,SHAPhighlightstheimportanceofnovelfeatureswhosefunctionwasnotidentifiedinthehedgesliterature:(i)whatLIWCclassifiesasinformalwordsbutthataremostlyinterjectionslikeahandoharestronglyassociatedwithApol-ogizer,asaredisfluencies(n=12);(ii)theuseofPOStagsseemstobeveryrelevantforcharac-terizingthedifferentclasses(2-gramofPOStagfeatures3occurinthetop-rankedfeaturesofallthe\n\u220564.5(3.0)\u2205LSTM(3-classes)\nKDFeat.(KDF)Pre-TrainedEmb.(PTE)KDF+PTE\n\u220594.4(0.2)\u2205LSTM(4-classes)\n96.7(0.2)91.0(0.2)95.4(0.2)\n94.8(0.3)89.7(0.4)93.9(0.4)Attention-CNN(4-classes)\n93.9(1.4)89.1(1.4)94.1(1.2)BERT(4-classes)\n67.6\u2205\u2205MLP(3-classes)\nModels\n2166\n\n\n95.0(0.2)96.5(0.3)96.5(0.2)96.7(0.2)96.6(0.4)96.7(0.3)\n3-classes\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\n68.8(0.8)78.2(1.6)78.1(1.3)79.0(1.3)78.5(2.4)78.7(1.8)\nTable5:AveragedweightedF1-scoresforthethreeclassesofhedgesandthefourclasses,withanadditiveintegrationofKDFfeaturesintheLightGBMmodel.Thestandarddeviationiscomputedacrossfivefolds.\n4-classes\nModels\nRankApologizerSubjectivizersProp.HedgesNothedged\n1Functionwords(LIWC)\"I\"ClasslabelClasslabel2\"Oh\"(LIWC)\"Yeah\"\"Would\"\"Would\"3\"Sorry\"Noun(POS)\"Just\"\"Yeah\"4Affect(LIWC)ClasslabelFunctionword(LIWC)Noun(POS)5ClauselengthCognitiveprocess(LIWC)Netspeak(LIWC)Cognitiveprocess(LIWC)Table6:Mostimportantclause-levelfeaturesforLightGBMaccordingtotheSHAPanalysis.classes(seeFiguresintheAppendix).Itmeansthattherearesomerecurringsyntacticpatternsineachclass;(iii)Regardingtheutterancesize,aclauseshorterthanthemeanisweaklyassociatedwithdirectness(n=17)whilealongerclausesuggeststhatitcontainsaSubjectivizer(n=6).Apologizersarecharacterizedbyameanclauselength(n=5),withfewvariationsfromit;(iv)Tutoringmovesarenotstrongpredictorsofanyclasses:\"Affirma-tionfromtutor\"istheonlyfeatureappearingasapredictorofPropositionalhedges(n=20).ThisisconsistentwiththefeatureanalysisinTable5,suggestingthattutoringmovesdonotsignificantlyimprovetheperformanceoftheclassifier;(v)Non-verbalbehaviorsdonotappearasimportantfea-turesfortheclassification.Thisiscoherentwithresultsfrom(Goeletal.,2019).Notethatprosodymightplayaroleindetectinginstructionsthattrailoff,but,asdescribed,paraverbalfeatureswerenotavailable;(vi)Wouldplaysanimportantroleintheproductionofhedges,asitisstronglyassociatedtoPropositionalhedges(n=2).Itisinterestingtonotethat,whendesigningtherule-basedclassifier,wesawitdecreaseinperformancewhenwestartedtoincludewouldinourregularexpressionpatterns,probablybecausetheformishardtodisambiguateforadeterministicsystem.WhileexploringtheShapleyvaluesassociatedtoeachclause,weobservedthatfeaturesliketutoringmovesareextremelyinformativeforaverysmallnumberofclauses(thereforenotsignificantlyinflu-encingtheoverallperformanceoftheprediction),andmoreorlessnotinformativefortherest.Infer-ringtheglobalimportanceofafeatureasameanacrosstheshapleyvaluesinthedatasetmaynotbetheonlywaytoexplorethebehaviorofgradi-entboostingmethods.ItmightbemoreusefultoclusterclausesbasedontheimportancethatSHAPgivestothatfeatureinitsclassification,asthiscouldhelpdiscoversub-classesofhedgesthataredifferentiatedfromtherestbytheirinteractionwithaspecificfeature(inthewaythatsomeApologiz-ersarecharacterizedbyan\"oh\").Wealsonotethattheexplanationmodelissensitivetospuri-ouscorrelationsinthedataset,causedbythesmallrepresentationofsomeclass:forexample,\"nine\"(n=7)and\"four\"(n=20)arepositivepredictorsofApologizers.6ConclusionandfutureworkThroughourclassificationperformanceexperi-ments,weshowedthatitispossibletousema-chinelearningmethodstodiminishtheambigu-ityofhedges,andthatthehybridapproachofus-ingrule-basedlabelfeaturesderivedfromsocialscience(includinglinguistics)literaturewithinamachinelearningmodelhelpedsignificantlytoin-creasethemodel\u2019sperformance.Nonverbalbehav-iorsandtutoringmovesdidnotprovideinformationatthesentencelevel;boththeperformanceofthemodelandthefeaturecontributionanalysissug-gestedthattheirimpactonthemodeloutputwasnotstrong.ThisisconsistentwithresultsfromGoeletal.(2019).However,infutureworkwewouldliketoinvestigatethepotentialofmultimodalpat-ternswhenweareabletobettermodelsequentiality(e.g.,negativefeedbackfollowedbyasmile).Re-gardingtheSHAPanalysis,mostofthefeaturesthatareconsideredasimportantarecoherentwiththedefinitionoftheclasses(Iforsubjectivizers,sorryforapologizers,justforpropositionalhedges).However,wediscoveredthatfeatureslikeutterance\n2167\n\n\nsizecanalsoserveasindicatorsofcertainclassesofhedges.AlimitationofSHAPisthatitmakesafeatureindependenceassumption,whichpromptstheexplanatorymodeltounderestimatetheimpor-tanceofredundantfeatures(likepronounsinourwork).Inthefuturewewillexploreexplanatorymodelscapableoftakingintoaccountthecorre-lationbetweenfeaturesinthedatasetlikeSAGE(Covertetal.,2020),butsuitedforveryimbal-anceddatasets.Inthedomainofpeer-tutoring,wewouldliketobeabletofurthertestthelinkbe-tweenhedgesandrapport,andthelinkbetweenhedgesandlearninggainsinthesubjectbeingtu-tored.Asnotedabove,thiskindofstudyrequiresafine-grainedcontrolofthelanguageproducedbyoneoftheinterlocutors,whichisdifficulttoachieveinahuman-humanexperience.Wenotethatthehedgeclassifiercanbeusednotjusttoclassify,butalsotoworktowardsimprovingthegenerationofhedgesfortutoragents.Infutureworkwewillexploreusingtheclassifiertore-rankgenerationoutputs,takingadvantageoftherecur-ringsyntacticpatterns(see(ii)inSection5.3)toimprovethegenerationprocessofhedges,andre-generatingclausesthatdon\u2019tcontainoneofthesesyntacticpatterns.AcknowledgmentsManythankstomembersoftheArticuLaboatIN-RIAParisfortheirpreciousassistance.ThisworkwassupportedinpartbythetheFrenchgovern-mentundermanagementofAgenceNationaledelaRechercheaspartofthe\u201cInvestissementsd\u2019avenir\u201dprogram,referenceANR-19-P3IA-0001(PRAIRIE3IAInstitute).ReferencesHeikeAdelandHinrichSch\u00fctze.2017.Exploringdif-ferentdimensionsofattentionforuncertaintydetec-tion.InProceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputa-tionalLinguistics:Volume1,LongPapers,pages22\u201334,Valencia,Spain.AssociationforComputa-tionalLinguistics.ShoshanaBlum-Kulka.1987.Indirectnessandpolite-nessinrequests:Sameordifferent?Journalofpragmatics,11(2):131\u2013146.PenelopeBrownandStephenCLevinson.1987.Polite-ness:Someuniversalsinlanguageusage,volume4.Cambridgeuniversitypress.JudeeKBurgoonandRandallJKoper.1984.Nonverbalandrelationalcommunicationassociatedwithreti-cence.HumanCommunicationResearch,10(4):601\u2013626.IanCovert,ScottMLundberg,andSu-InLee.2020.Understandingglobalfeaturecontributionswithad-ditiveimportancemeasures.AdvancesinNeuralInformationProcessingSystems,33:17212\u201317223.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforCom-putationalLinguistics:HumanLanguageTechnolo-gies,Volume1(LongandShortPapers),pages4171\u20134186.Rich\u00e1rdFarkas,VeronikaVincze,Gy\u00f6rgyM\u00f3ra,J\u00e1nosCsirik,andGy\u00f6rgySzarvas.2010.Theconll-2010sharedtask:learningtodetecthedgesandtheirscopeinnaturallanguagetext.InProceedingsofthefour-teenthconferenceoncomputationalnaturallanguagelearning\u2013Sharedtask,pages1\u201312.BruceFraser.2010.Pragmaticcompetence:Thecaseofhedging.Newapproachestohedging,1534.ViolaGanterandMichaelStrube.2009.Findinghedgesbychasingweasels:Hedgedetectionusingwikipediatagsandshallowlinguisticfeatures.InProceedingsoftheACL-IJCNLP2009ConferenceShortPapers,pages173\u2013176.PranavGoel,YoichiMatsuyama,MichaelMadaio,andJustineCassell.2019.\u201cithinkitmighthelpifwemultiply,andnotadd\u201d:Detectingindirectnessincon-versation.In9thInternationalWorkshoponSpokenDialogueSystemTechnology,pages27\u201340.Springer.CynthiaHoward,PamelaW.Jordan,BarbaraMariaDiEugenio,andSandraKatz.2015.Shiftingtheload:apeerdialogueagentthatencouragesitshumancollab-oratortocontributemoretoproblemsolving.Interna-tionalJournalofArtificialIntelligenceinEducation,27:101\u2013129.KenHyland.1998.Hedginginscientificresearcharti-cles,volume54.JohnBenjaminsPublishing.GuolinKe,QiMeng,ThomasFinley,TaifengWang,WeiChen,WeidongMa,QiweiYe,andTie-YanLiu.2017.Lightgbm:Ahighlyefficientgradientboost-ingdecisiontree.Advancesinneuralinformationprocessingsystems,30:3146\u20133154.KlausKrippendorff.2004.Reliabilityincontentanaly-sis:Somecommonmisconceptionsandrecommen-dations.Humancommunicationresearch,30(3):411\u2013433.GeorgeLakoff.1975.Hedges:Astudyinmeaningcriteriaandthelogicoffuzzyconcepts.InContem-poraryresearchinphilosophicallogicandlinguisticsemantics,pages221\u2013271.Springer.\n2168\n\n\nMatthewLeach.2005.Rapport:Akeytotreatmentsuc-cess.Complementarytherapiesinclinicalpractice,11:262\u20135.IlyaLoshchilovandFrankHutter.2018.Decoupledweightdecayregularization.InInternationalConfer-enceonLearningRepresentations.ScottMLundberg,GabrielGErion,andSu-InLee.2018.Consistentindividualizedfeatureat-tributionfortreeensembles.arXivpreprintarXiv:1802.03888.ScottMLundbergandSu-InLee.2017.Aunifiedap-proachtointerpretingmodelpredictions.InProceed-ingsofthe31stinternationalconferenceonneuralinformationprocessingsystems,pages4768\u20134777.HowardLuneandBruceLBerg.2017.Qualitativeresearchmethodsforthesocialsciences.Pearson.MichaelMadaio,JustineCassell,andAmyOgan.2017.Theimpactofpeertutors\u2019useofindirectfeedbackandinstructions.Philadelphia,PA:InternationalSo-cietyoftheLearningSciences.BenMedlockandTedBriscoe.2007.Weaklysuper-visedlearningforhedgeclassificationinscientificliterature.InProceedingsofthe45thannualmeetingoftheassociationofcomputationallinguistics,pages992\u2013999.ChristophMolnar.2020.Interpretablemachinelearn-ing.Lulu.com.AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advancesinneuralinformationprocessingsystems,32:8026\u20138037.JamesWPennebaker,RyanLBoyd,KaylaJordan,andKateBlackburn.2015.Thedevelopmentandpsycho-metricpropertiesofliwc2015.Technicalreport.JeffreyPennington,RichardSocher,andChristopherDManning.2014.Glove:Globalvectorsforwordrep-resentation.InProceedingsofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing(EMNLP),pages1532\u20131543.EllenFPrince,JoelFrader,CharlesBosk,etal.1982.Onhedginginphysician-physiciandiscourse.Lin-guisticsandtheProfessions,8(1):83\u201397.AnnaProkofievaandJuliaHirschberg.2014.Hedgingandspeakercommitment.In5thIntl.WorkshoponEmotion,SocialSignals,Sentiment&LinkedOpenData,Reykjavik,Iceland.NilsReimersandIrynaGurevych.2019.Sentence-bert:Sentenceembeddingsusingsiamesebert-networks.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages3982\u20133992.OscarJRomero,RanZhao,andJustineCassell.2017.Cognitive-inspiredconversational-strategyreasonerforsocially-awareagents.InIJCAI,pages3807\u20133813.TimRowland.2007.\u2018wellmaybenotexactly,butit\u2019saroundfiftybasically?\u2019:Vaguelanguageinmath-ematicsclassrooms.InVaguelanguageexplored,pages79\u201396.Springer.SaschaSchneider,SteveNebel,SimonPradel,andG\u00fcn-terDanielRey.2015.Mindyourpsandqs!howpoliteinstructionsaffectlearningwithmultimedia.ComputersinHumanBehavior,51:546\u2013555.TanmaySinhaandJustineCassell.2015.Weclick,wealign,welearn:Impactofinfluenceandconvergenceprocessesonstudentlearningandrapportbuilding.InProceedingsofthe1stWorkshoponModelingINTERPERsonalSynchrONyAndInfLuence,INTER-PERSONAL\u201915,page13\u201320,NewYork,NY,USA.AssociationforComputingMachinery.HelenSpencer-Oatey.2005.(im)politeness,faceandperceptionsofrapport:Unpackagingtheirbasesandinterrelationships.1(1):95\u2013119.Gy\u00f6rgySzarvas,VeronikaVincze,Rich\u00e1rdFarkas,Gy\u00f6rgyM\u00f3ra,andIrynaGurevych.2012.Cross-genreandcross-domaindetectionofsemanticuncer-tainty.ComputationalLinguistics,38(2):335\u2013367.BuzhouTang,XiaolongWang,XuanWang,BoYuan,andShixiFan.2010.Acascademethodfordetectinghedgesandtheirscopeinnaturallanguagetext.InProceedingsoftheFourteenthConferenceonCom-putationalNaturalLanguageLearning\u2013SharedTask,pages13\u201317.MorganUlinski,SethBenjamin,andJuliaHirschberg.2018.Usinghedgedetectiontoimprovecommittedbelieftagging.InProceedingsoftheWorkshoponComputationalSemanticsbeyondEventsandRoles,pages1\u20135.TeppoVarttala.1999.Remarksonthecommunicativefunctionsofhedginginpopularscientificandspecial-istresearcharticlesonmedicine.Englishforspecificpurposes,18(2):177\u2013200.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,\u0141ukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.InAdvancesinneuralinformationpro-cessingsystems,pages5998\u20136008.ErikVelldal.2011.Predictingspeculation:asimpledis-ambiguationapproachtohedgedetectioninbiomed-icalliterature.JournalofBiomedicalSemantics,2(5):1\u201314.VeronikaVincze.2014.Uncertaintydetectioninnaturallanguagetexts.PhD,UniversityofSzeged,page141.\n2169\n\n\nNingWang,WLewisJohnson,RichardEMayer,PaolaRizzo,ErinShaw,andHeatherCollins.2008.Thepolitenesseffect:Pedagogicalagentsandlearningoutcomes.Internationaljournalofhuman-computerstudies,66(2):98\u2013112.ShawnWarner-Garcia.2014.Laughingwhennothing\u2019sfunny:Thepragmaticuseofcopinglaughterinthenegotiationofconversationaldisagreement.Prag-matics,24(1):157\u2013180.TimothyWilliamson.2002.Vagueness.Routledge.RanZhao,AlexandrosPapangelis,andJustineCassell.2014.Towardsadyadiccomputationalmodelofrap-portmanagementforhuman-virtualagentinteraction.InInternationalConferenceonIntelligentVirtualAgents,pages514\u2013527.Springer.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016a.Automaticrecognitionofconversa-tionalstrategiesintheserviceofasocially-awaredialogsystem.InProceedingsofthe17thAnnualMeetingoftheSpecialInterestGrouponDiscourseandDialogue,pages381\u2013392.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016b.Socially-awarevirtualagents:Au-tomaticallyassessingdyadicrapportfromtemporalpatternsofbehavior.InInternationalconferenceonintelligentvirtualagents,pages218\u2013233.Springer.AAdditionalinformationontheexperimentalsettingsWeusedPyTorch(Paszkeetal.,2019)toimple-menttheneuralmodels.Foreachsetoffeatures,hyperparameterswereselectedusingOptuna(Ak-iba,2019),aparametersearchframework.Were-implementedtheAttention-CNNwithGlove(Pen-ningtonetal.,2014)300-Dwordsembeddingsasthevectorrepresentation.Foreachmodels,theresultsarecross-validatedusing5folds(wechose5insteadof10toavoidhavingfoldswithtoofewsamplesperclass).Wecorrectedthelossfunctionforclassimbalancetoforcethemodeltoadaptmoretothelessfrequentclasses.Thestrengthofthiscorrectiondependedonthemodel,andwasselectedbecauseitprovidedasatisfyingcompro-misebetweenfavoringrecallandprecisionintheclassificationresultsofthatmodel.ForLightGBM,a\"squarerootofthesquarerootoftheinverseclassproportion\"correctionwasselected.Neu-ralmodelsweretrainedusingAdamWasanop-timizer(LoshchilovandHutter,2018),andusedareducedfeaturevector,obtainedwiththeap-plicationofPCA(dinit=1800;d=100;99.8%oftheinformationisconserved).Nosignifi-cantperformancedifferenceswereobservedbe-tweentheoriginalvectorandthereducedvectorfortrainingthemodels.TocomputetheSHAPvaluesmentionedinthepaper,wekeptonesplittoperformthe5-splitofthedataset,andleave1splittovalidateandearlystopthemodel,inor-dertoavoidoverfitting.Acompleteconfigura-tionofhyperparametersusedforeachmodelisre-portedintheGitHubrepositorywiththecodeofthepaper:https://github.com/YannRaphalen/Hedges-Detection.TheBERTmodelwasfine-tunedonaNvidiaQuadroRTX8000GPU.BTables\n2170\n\n\n?(whether|if|is|that|it|this)?.*Subj.\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nNoNoYesNoNoNoYesYesYesYesYesBERT(PTE)\nexceptionally|forthemostpart|inamannerofspeaking|\nNoYesNoNoNoYesNoNoYesYesNoMLP(KDF)\nsomethingbetween|essentially|only).*Prop.\nTable8:Significancetableforthe3-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesYesYesYesYesNoYesYesYesNoYesMLP(KDF+PTE)\ntechnically|typically|virtually|approximately|\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nmuch|somewhat|exactly|almost|littlebit|quite|\nClass\nrelatively|roughly|sotosay|strictlyspeaking|\nNoNoYesNoNoYesNoYesYesYesYesLSTM(KDF)\nYesNoNoNoNo+TM\nTable10:Significancetableforthe3-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoNoYesYesYesYesYesYesNoYesYes\n.*(it)(looks|seems|appears)[,]?.*\",\".*(or|and)(that|something|stuff|soforth)Table7:Regexprulesusedfortheclassifier.\nRule-based\nRule-based\n.*(you(might|may)(believe|think)).*Subj.\nYesYesYesYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nYesYesNoYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nNoNoYesNoNoYesNoNoYesYesYesLSTM(KDF)\nYesYesNoYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(i|i\u2019m|you|it\u2019s)(am|are)(apparently|surely)[,]?.*Prop.\nNoNoYesNoNoYesNoYesYesYesYesLSTM(PTE)\nNoNoYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nNoNoYesYesNoYesYesYesYesYesNoLGBM(KDF)\nNoNoYesYesYesYesYesYesYesYesNoLGBM(KDF)\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nYesNoYesYesYesYesYesYesNoYesYes\nYesYesYesYesYesYesYesYesYesYesYesMLP(KDF+PTE)\nregular|regularly|actually|almost|asitwere|basically|\nNoYesYesNoYesYesYesNoYesYesNoMLP(PTE)\nRule(regexp)\n.*(i|i\u2019m|we)?(was|am|wasn\u2019t)??(not)?(sure|certain).*Subj.\n.*(accordingto|presumably).*Subj.\n(?!.*(be|been|was)likeexcuseme)((excuseme|sorry)[w,\u2019]+|[w,\u2019]+(excuseme|sorry))Prop.\n.*(i|you|we)haveto(check|look|verify).*Subj.\n.*(i\u2019m|i|we\u2019re)(am|are)??(apologize|sorry).*Apol.\nonthetallside|parexcellence|particularly|\n(?!what).*(i|we)?(don\u2019t|didn\u2019t|did)??(not)?\ninarealsense|inasense|inaway|largely|literally|\nYesYesYesNoNoNoYesYesYesYesYesBERT(PTE)\nModels\nModels\nModels\nYesYesYesNoNoYesNoNoYesYesYesLSTM(PTE)\nYesNoNoNoNo\nYesYesYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nSubj.\nTable9:Significancetableforthe4-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoYesNoNoNoYesNoNoYesYesNoMLP(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(ifeellikeyou).*Subj.\nlooselyspeaking|kinda|moreorless|mostly|often|\n.*(unlessi).*Apol.\nprobably|canbeviewas|crypto-|especially|essentially|\n.*(ifi\u2019mnotwrong|ifi\u2019mright|ifthat\u2019strue).*Subj.\nYesNoNoNoNo+POS\n(guess|guessed|thought|think|believe|believed|suppose|supposed)\n.*(just|alittle|maybe|actually|sortof|kindof|pretty\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nNoYesYesNoYesYesNoNoYesYesYesMLP(KDF)\nprettymuch|principally|pseudo-|quintessentially|\n2171\n\n\nFigure2:Absoluteaveragedfeaturecontribution,asindicatedbySHAP.Thelongerthebarisforonecolor,themorethefeatureisassociatedwiththeclassrepresentedbythatcolor.\nFigure3:Averagedcontributionoffeaturestothedetectionofthe\"Notindirect\"class,asindicatedbySHAP.Eachdotcorrespondstoaclassifiedclause.Areddotindicatesthatthefeatureispresentintheclause,whileabluedotindicatesthatthefeatureisabsent.Thefartherontherightthedotis,themorethefeaturecontributedtoitsclassificationasahedge.\n2172\n\n\nFigure4:Averagedcontributionoffeaturestothedetectionof\"Apologizers\",asindicatedbySHAP.\nFigure5:Averagedcontributionoffeaturestothedetectionof\"Propositionalhedges\",asindicatedbySHAP.\n2173\n\n\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nYesNoNoNoNo+TM\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nFigure6:Averagedcontributionoffeaturestothedetectionof\"Subjectivizers\",asindicatedbySHAP.\nModels\nYesNoNoNoNo\nTable11:Significancetableforthe4-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesNoNoNoNo+POS\n2174"}, {"question": " How did the study correct the loss function for class imbalance in training the models?", "answer": " By adjusting the correction strength based on the imbalance level of the classes", "ref_chunk": "\"Youmightthinkaboutslightlyrevisingthetitle\":IdentifyingHedgesinPeer-tutoringInteractionsYannRaphalen1,Chlo\u00e9Clavel2,JustineCassell1,31InriaParis2LTCI,InstitutPolytechniquedeParis,Telecom-Paris3CarnegieMellonUniversityyann.raphalen.pro@gmail.com,justine@cs.cmu.edu,chloe.clavel@telecom-paris.frAbstractHedgesplayanimportantroleinthemanage-mentofconversationalinteraction.Inpeer-tutoring,theyarenotablyusedbytutorsindyads(pairsofinterlocutors)experiencinglowrapporttotonedowntheimpactofinstructionsandnegativefeedback.Pursuingtheobjectiveofbuildingatutoringagentthatmanagesrap-portwithstudentsinordertoimprovelearning,weusedamultimodalpeer-tutoringdatasettoconstructacomputationalframeworkforiden-tifyinghedges.Wecomparedapproachesre-lyingonpre-trainedresourceswithothersthatintegrateinsightsfromthesocialsciencelitera-ture.Ourbestperformanceinvolvedahybridapproachthatoutperformstheexistingbase-linewhilebeingeasiertointerpret.Weemployamodelexplainabilitytooltoexplorethefea-turesthatcharacterizehedgesinpeer-tutoringconversations,andweidentifysomenovelfea-tures,andthebenefitsofsuchahybridmodelapproach.1IntroductionRapport,mostsimplydefinedasthe\u201c...relativeharmonyandsmoothnessofrelationsbetweenpeo-ple...\u201d(Spencer-Oatey,2005),hasbeenshowntoplayaroleinthesuccessofactivitiesasvariedaspsychotherapy(Leach,2005)andsurveyinterview-ing(LuneandBerg,2017).Inpeer-tutoring,rap-port,asmeasuredbytheannotationofthinslicesofvideo,hasbeenshowntobebeneficialforlearningoutcomes(Zhaoetal.,2014;SinhaandCassell,2015).Thelevelofrapportrisesandfallswithconversationalstrategiesdeployedbytutorsandtuteesatappropriatetimes,andasafunctionofthecontentofpriorturns.Thesestrategiesincludeself-disclosure,referringtosharedexperience,and,onthepartoftutors,givinginstructionsinanindirectmanner.Someworkhasattemptedtoautomaticallydetectthesestrategiesintheserviceofintelligenttutors(Zhaoetal.,2016a),butonlyafewstrate-gieshavebeenattempted.Otherworkhascon-centratedona\"socialreasoningmodule\"(Romeroetal.,2017)todecidewhichstrategiesshouldbegeneratedinagivencontext,butindirectnesswasnotamongthestrategiestargeted.Inthispaper,wefocusontheautomaticclassificationofonespe-cificstrategythatisparticularlyimportantforthetutoringdomain,andthereforeimportantforintel-ligenttutors:hedging,asub-partofindirectnessthat\"softens\"whatwesay.Thisworkispartofalargerresearchprogramwiththelong-termgoalofautomaticallygeneratingindirectnessbehaviorsforatutoringagent.\nFigure1:Amockconversationdisplayingeachtypeofhedgedformulation.AccordingtoBrownandLevinson(1987),hedgesarepartofthelinguistictoolsthatinterlocu-torsusetoproducepoliteness,bylimitingthefacethreattotheinterlocutor(basicallybylimitingtheextenttowhichtheinterlocutormightexperienceembarrassmentbecauseofsomekindofpoorper-formance).Anexampleis\"that\u2019skindofawronganswer\".Hedgesarealsofoundwhenspeakerswishtoavoidlosingfacethemselves,forexam-plewhensaying(\"IthinkImighthavetoadd6.\").Madaioetal.(2017)foundthatinapeer-tutoringtask,whenrapportbetweeninterlocutorsislow,tu-teesattemptedmoreproblemsandcorrectlysolvedmoreproblemswhentheirtutorshedgedinstruc-\n2160 Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1: Long Papers, pages 2160 - 2174 May 22-27, 2022 c(cid:13)2022 Association for Computational Linguistics\n\n\ntions,whichlikewisepointstowardsa\"mitigationoffacethreat\"function.Hedgescanalsobeasso-ciatedwithanonverbalcomponent,forexampleavertedeyegazeduringcriticism(BurgoonandKoper,1984).Hedgesarenot,however,alwaysap-propriate,asin\"Ikindofthinkit\u2019srainingtoday.\"whentheinterlocutorscanbothseerain(althoughitmightbetakenashumorous).Thesefactsabouthedgesmotivateawaytoautomaticallydetectthemand,ultimately(althoughnotinthecurrentwork)alsogeneratethem.Inbothcaseswefirsthavetobeabletocharacterizethemusinginterpretablelinguisticfeatures,whichiswhatweaddressinthecurrentpaper.Thus,intheworkdescribedhere,basedonlinguisticdescriptionsofhedges(BrownandLevinson,1987;Fraser,2010),webuiltarule-basedclassifier.Weshowthatthisclassifierincombinationwithadditionalmultimodalinter-pretablecontext-dependentfeaturessignificantlyimprovestheperformanceofamachinelearningmodelforhedges,comparedtoalessinterpretabledeeplearningbaselinefromGoeletal.(2019)us-ingwordembeddings.Wealsoreliedonamachinelearningmodelexplanationtool(LundbergandLee,2017)toinvestigatethelinguisticfeaturesrelatedtohedgesinthecontextofpeer-tutoring,primarilytoseeifwecoulddiscoversurprisingfeaturesthattheclassificationmodelwouldassociatetohedgesinthiscontext,andwedescribethosebelow.Thecodeofthemodelsdescribedinthepaperisalsoprovided.12RelatedworkHedges:AccordingtoFraser(2010),hedgingisarhetoricalstrategythatattenuatesthestrengthofastatement.Onewaytoproduceahedgeisbyalteringthefullsemanticvalueofaparticu-larexpressionthroughPropositionalhedges(alsocalledApproximatorsinPrinceetal.(1982)),asin\"Youarekindofwrong,\"thatreduceprototypical-ity(i.eaccuracyofthecorrespondencebetweenthepropositionandtherealitythatthespeakerseekstodescribe).Propositionalhedgesarerelatedtofuzzylanguage(Lakoff,1975),andthereforetotheproductionofvagueness(Williamson,2002)anduncertainty(Vincze,2014).AsecondkindareRelationalHedges(alsocalledShieldsinPrinceetal.(1982)),suchas\u201cIthinkthatyouarewrong.\u201dor\u201cThedoctorwantsyoutostopsmoking.\u201d,conveyingthatthepropositionis\n1https://github.com/AnonymousHedges/HedgeDetectionconsideredbythespeakerassubjective.Inafurthersub-division,AttributionShields,asin\"Thedoc-torwantsyou...\",theinvolvementofthespeakerinthetruthvalueofthepropositionisnotmadeexplicit,whichallowsspeakersnottotakeastance.Asdescribedabove,Madaioetal.(2017)foundthattutorswhoshowedlowerrapportwiththeirtuteesusedmorehedgedinstructions(theyalsoemployedmorepositivefeedback),howeverthiswasonlythecasefortutorswithagreaterbeliefintheirabilitytotutor.Tuteesinthiscontextsolvedmoreproblemscorrectlywhentheirtutorshedgedinstructions.Noeffectofhedgingwasfoundfordyads(pairsofinterlocutors)withgreatersocialcloseness.However,theauthorsdidnotlookatthespecificlinguisticformstheseteenagersused.Rowland(2007)alsodescribestherolethathedg-ingplaysinthisagegroup,showingthatstudentsusebothrelational(\"IthinkthatJohnissmart.\")andpropositional(\"Johniskindofsmart.\")hedgesformuchthesameshieldingfunctionofdemon-stratinguncertainty,tosavethemfromtheriskofembarrassmentiftheyarewrong.TheauthorobservedthatteensusedfewAdaptors(kindof,somewhat)andpreferredtouseRounders(around,closeto).However,thisstudywasperformedwithanadultandtwochildren,possiblybiasingthere-sultsduetotheparticipationoftheadultinvestiga-tor.Hedgeshavebeenincludedinvirtualtutoringagentsbeforenow.(Howardetal.,2015)integratedhedgesinatutoragentforundergraduatesinCS,asawaytoencouragethestudenttotaketheinitiative.Hedgeshavealsobeenusedasawayofintegrat-ingBrownandLevinson\u2019spolitenessframework(Wangetal.,2008;Schneideretal.,2015)invir-tualtutoringagents.Resultswerenotbrokenoutbystrategy,butpolitenessingeneralwasshowntopositivelyinfluencemotivationandlearning,incertainconditions.Computationalmethodsforhedgedetection:Anumberofstudieshavetargetedthedetectionofhedgesanduncertaintyintext(MedlockandBriscoe,2007;GanterandStrube,2009;Tangetal.,2010;Velldal,2011;Szarvasetal.,2012),partic-ularlyfollowingtheCoNLL2010datasetrelease(Farkasetal.,2010).However,thisworkisnotasrelatedtohedgesinconversation,asitfocusesonaformalandacademiclanguageregister(Hy-land,1998;Varttala,1999).AsnotedbyProkofievaandHirschberg(2014),thefunctionsofhedgesaredomain-andgenre-dependent,thereforethisbias\n2161\n\n\ntowardsformalityimpliesthattheexistingworkmaynotadaptwelltothedetectionofhedgesinconversationbetweenteenagers.Aconsequenceisthattheexistingworkdoesnotconsidertermslike\"Ithink,\"sinceopinionsrarelyappearinanaca-demicwritingdataset.Instructionsarealsoalmostabsent(\"Ithinkyouhavetoaddtentobothsides.\"),astronglimitationforthestudyofconversationalhedgessinceitisinrequests(includingtutoringin-structions)thatindirectformulationsmostlyoccuraccordingtoBlum-Kulka(1987).ProkofievaandHirschberg(2014)alsonotethatitisdifficulttodetecthedgesbecausethewordpatternsassociatedwiththemhaveothersemanticandpragmaticfunc-tions:considering\"Ithinkthatyouhavetoaddxtobothsides.\"vs\"Ithinkthatyouareanidiot.\",itisnotclearthattheseconduseof\"Ithinkthat\"isanhedgemarker.Theyadvocateusingmachinelearningapproachestodealwiththeambiguityofthesemarkers.Workingonaconversationaldataset,Ulinskietal.(2018)builtacomputationalsystemtoassessspeakercommitment(i.e.atwhichpointthespeakerseemsconvincedbythetruthvalueofastatement),inparticularbyrelyingonarule-baseddetectionsystemforhedges.Comparedtothatwork,ourrule-basedclassificationmodelisdirectlydetectinghedgeclasses,andweemploythepredictionsoftherule-basedmodelasafeatureforstrongermachinelearningmodels,designedtolessentheimpactoftheimbalancebetweenclasses.Wealsoconsiderapologieswhentheyserveamit-igationfunction(wethencallthemApologizers),aswasdonebytheauthorsofourcorpus,andwealsousethetermsubjectivizersasdefinedbelow,tobeabletocomparedirectlywiththepreviousworkcarriedoutonthiscorpus.Asfarasweknow,onlyGoeletal.(2019)haveworkedwithapeer-tutoringdataset(thesameonethatwealsouse),andtheyachievedtheirbestclassificationresultbyemployinganAttention-CNNmodel,inspiredbyAdelandSch\u00fctze(2017).3ProblemstatementWeconsiderasetDofconversationsD=(c1,c2,...,c|D|),whereeachconversationiscom-posedofasequenceofindependentsyntacticclausesci=(u1,u2,...,uM),whereMisthenumberofclausesintheconversation.Notethattwoconsecutiveclausescanbeproducedbythesamespeaker.Eachclauseisassociatedwithauniquelabelcorrespondingtothediffer-enthedgeclassesdescribedinTable1:yi\u2208C={PropositionalHedges,Apologizers,Subjec-tivizers,Nothedged}.Finally,anutteranceuicanberepresentedasavectoroffeaturesX=(x1,x2,...,xN),whereNrepresentsthenumberoffeaturesweusedtodescribeaclause.Ourfirstgoalistodesignamodelthatcorrectlypredictsthelabelyiassociatedtoui.Itcanbeunderstoodasthefollowingresearchquestion:RQ1:\"Whichmodelsandfeaturescanbeusedtoautomaticallycharacterizehedgesinapeer-tutoringinteraction?\"Oursecondgoalistoidentify,foreachhedgeclass,thesetoffeaturesFclass={fk},k\u2208[1,N]sortedbyfeatureimportanceintheclassificationofclass.Itcorrespondstothefollowingresearchquestion:RQ2:\"Whatarethemostimportantlinguisticfeaturesthatcharacterizeourhedgeclassesinapeer-tutoringsetting?\"4Methodology4.1CorpusDatacollection:Thedialoguecorpususedherewascollectedaspartofalargerstudyontheeffectsofrapport-buildingonreciprocalpeertutoring.24Americanteenagers(meanage=13.5,min=12,max=15),halfmaleandhalffemale,cametoalabwherehalfoftheparticipantswerepairedwithasame-age,same-genderfriend,andtheotherhalfpairedwithastranger.Theparticipantswereassignedtoatotalof12dyadsinwhichthepar-ticipantsalternatedtutoringoneanotherinlinearalgebraequationsolvingfor5weeklyhour-longsessions,foratotalcorpusofnearly60hoursofface-to-faceinteractions.Eachsessionwasstruc-turedsuchthatthestudentsengagedinbriefsocialchitchatinthebeginning,thenoneofthestudentswasrandomlyassignedtotutortheotherfor20minutes.Theythenengagedinanothersocialpe-riod,andconcludedwithasecondtutoringperiodwheretheotherstudentwasassignedtheroleoftutor.Audioandvideodatawererecorded,tran-scribed,andsegmentedforclause-leveldialogueannotation,providingnearly24000clauses.Non-speechsegments(notablyfillersandlaughter)weremaintained.Becauseoftemporalmisalignmentforpartsofthecorpus,manyparaverbalphenomena,suchasprosody,wereunfortunatelynotavailabletous.SinceouraccesstothedatasetiscoveredbyaNon-DisclosureAgreement,itcannotbereleased\n2162\n\n\n2https://github.com/AnonymousHedges/HedgeDetectionclasses.4.2FeaturesLabelfromrule-basedclassifier(LabelRB):Weusetheclasslabelpredictedbytherule-basedclas-sifierdescribedinSection4.3asafeature.Ourhypothesisisthatthemachinelearningmodelcanusethisinformationtocounterbalancetheclassimbalance.Totakeintoaccountthefactthatsomerulesaremoreefficientthanothers,weweightedtheclasslabelresultingfromtherule-basedmodelbytheprecisionoftherulethatgeneratedit.Unigramandbigram:Wecountthenumberofoccurrencesofunigramsandbigramsofthecorpusineachclause.Weusedthelemmaofthewordsforunigramsandbigramsusingthenltklemmatizer(Loper,2002)andselectedunigramsandbigramsthatoccurredinthetrainingdatasetatleastfiftytimes.Thegoalwastoinvestigate,withabottom-upapproach,towhatextenttheuseofcertainwordscharacterizeshedgeclassesintutoring.InSection5weexaminetheoverlapbetweenthesewordsandthoseaprioriidentifiedbytherules.Part-of-speech(POS):Hedgeclassesseemtobeassociatedwithdifferentsyntacticpatterns:forex-ample,subjectivizersmostoftencontainapersonalpronounfollowedbyaverb,asin\"Iguess\",\"Ibelieve\",\"Ithink\".WethereforeconsideredthenumberofoccurrencesofPOS-Tagn-grams(n=1,2,3)asfeatures.WeusedthespaCyPOS-taggerandconsideredPOSunigrams,bigramsandtri-gramsthatoccuratleast10timesinthetrainingdataset.LIWC:LinguisticInquiryandWordCount(LIWC)(Pennebakeretal.,2015)isstandardsoft-wareforextractingthecountofwordsbelongingtospecificpsycho-socialcategories(e.g.,emotions,religion).Ithasbeensuccessfullyusedinthede-tectionofconversationalstrategies(Zhaoetal.,2016a).Wethereforecountthenumberofoccur-rencesofallthe73categoriesfromLIWC.Tutoringmoves(TM):Intelligenttutoringsys-temsrelyonspecifictutoringmovestosuccess-fullyconveycontent(asdohumantutors).Wethereforelookedatthelinkbetweenthetutoringmoves,asannotatedinMadaioetal.(2017),andhedges.Fortutors,thesemovesare(1)instruc-tionaldirectivesandsuggestions,(2)feedback,and(3)affirmations,mostlyexplicitreflectionsontheirpartners\u2019comprehension,whilefortutees,theyare(1)questions,(2)feedbacks,and(3)affirmations,\npublicly.Howevertheoriginalexperimenters\u2019In-stitutionalReviewBoard(IRB)approvalallowsustoview,annotate,andusethedatatotrainmodels.Thisalsoallowsustoprovidealinktoapixe-latedvideoexampleintheGitHubrepositoryoftheproject2.Dataannotation:Thedatasetwaspreviouslyan-notatedbyMadaioetal.(2017),followingananno-tationmanualthatusedhedgeclassesderivedfromRowland(2007)(seeTable1).Onlythetaskperi-odsoftheinteractionswereannotated.Comparingtheannotationswiththeclassesmentionedintherelatedworksection,SubjectivizerscorrespondtoRelationalhedges(Fraser,2010),PropositionalhedgesandExtenderscorrespondtoApproxima-tors(Princeetal.,1982)withtheadditionofsomediscoursemarkerssuchasjust.ApologizersarementionedaslinguistictoolsrelatedtonegativepolitenessinBrownandLevinson(1987).Krippen-dorff\u2019salphaobtainedforthiscorpusannotatedbyfourcoderswasover0.7forallclasses(denotinganacceptableinter-coderreliabilityaccordingtoKrippendorff(2004)).Thedatasetiswidelyim-balanced,withmorethan90%oftheutterancesbelongingtotheNothedgedclass.Inreviewingthecorpusandtheannotationman-ual,however,wenoticedtwoissues.First,theannotationoftheExtendersclasswasinconsis-tent,leadingtotheExtendersandPropositionalhedgesclassescarryingsimilarsemanticfunctions.WethereforemergedthetwoclassesandgroupedutteranceslabeledasExtendersandthoselabeledasPropositionalhedgesundertheheadingofPropositionalhedges.Second,theannotationofclausescontainingthetokens\"just\"and\"would\"(twotermsoccurringfrequentlyinthedatasetthatarekeycomponentsofPropositionalHedgesandSubjectivizersbutthatarenotinfacthedgesinallcases)wasalsoinconsistent,leadingtovirtuallyallclauseswiththosetwotokensbeingconsideredhedges.Wethereforere-consideredalltheclausesassociatedwithanyofthehedgeclasses,aswellasalltheclausesinthe\"Nothedged\"classthatcontained\"just\"or\"would\".There-annotationwascarriedoutbytwoannotatorswhoachievedaKrippendorff\u2019salphainter-raterreliabilityof.9orbetterforApologizers,Subjectivizers,andPropo-sitionalhedgesbeforeindependentlyre-annotatingtherelevantclauses.Anexampleofare-annotationwasremoving\"Iwouldkillyou!\"fromthehedge\n2163\n\n\nProp.hedgesApologizersSubjectivizersNothedgedTotal\nTable1:Definitionoftheclasses\nFeaturesnameAutomaticextractionVectorsize\nSubjectivizersWordsthatreduceintensityorcertainty\u201cSothenIwoulddividebytwo.\u201dApologizersApologiesusedtosoftendirectspeechacts\u201cOhsorrysixb.\u201dPropositionalhedgesQualifyingwordstoreduceintensityorcertaintyofutterances\u201cIt\u2019sactuallyeight.\u201dExtendersWordsusedtoindicateuncertaintybyreferringtovaguecategories\u201cIt\u2019llbethenumberxorwhatevervariableyouhave.\u201d\n12101286262119223156\nTable2:Distributionoftheclasses\nClassDefinitionExample\nRule-basedlabelYes4UnigramYes~250BigramYes~250POSYes~1200LIWCYes73NonverbalNo24TutoringmovesNo6Total~1800\nTable3:Listofautomaticallyextractedandmanuallyannotatedfeatureswiththeirsize.mostlytentativeanswers.Nonverbalandparaverbalbehaviors:AsinGoeletal.(2019),weincludedthenonverbalandpar-averbalbehaviorsthatarerelatedtohedges.Specif-ically,weconsiderlaughterandsmiles,thathavebeenshowntobeeffectivemethodsofmitiga-tion(Warner-Garcia,2014),cut-offsindicatingself-repairs,fillerslike\"Um\",gazeshifts(annotatedas\u2019GazeatPartner\u2019,\u2019GazeattheMathWorksheet\u2019,and\u2019Gazeelsewhere\u2019),andheadnods.Eachfea-turewaspresenttwiceinthefeaturevector,onetimeforeachinterlocutor.Inter-raterreliabilityfornonverbalbehaviorwas0.89(asmeasuredbyKrippendorff\u2019salpha)foreyegaze,0.75forsmilecount,0.64forsmiledurationand0.99forheadnod.Laughterisalsoreportedinthetranscriptatthewordlevel.Weseparatethetutor\u2019sbehaviorsfromthoseofthetutee.ThecollectionprocessforthesebehaviorsisdetailedfurtherinZhaoetal.(2016b).Theclause-levelfeaturevectorwasnormalizedbythelengthoftheclause(exceptfortherule-basedlabel).Thislengthwasalsoaddedasafeature.Table3presentsanoverviewofthefinalfeaturevector.4.3ClassificationmodelsTheclassificationmodelsusedarepresentedhereaccordingtotheirlevelofintegrationofexternallinguisticknowledge.Rule-basedmodel:OnthebasisoftheannotationmanualusedtoconstructthedatasetfromMadaioetal.(2017),andwithdescriptionsofhedgesfromRowland(2007),Fraser(2010)andBrownandLevinson(1987),weconstructedarule-basedclas-sifierthatmatchesregularexpressionsindicativeofhedges.TherulesaredetailedinTable7intheAppendix.LGBM:Sincehedgesareoftencharacterizedbyexplicitlexicalmarkers,wetestedtheassumptionthatamachinelearningmodelwithaknowledge-drivenrepresentationforclausescouldcompetewithaBERTmodelinperformance,whilebeingmuchmoreinterpretable.WereliedonLightGBM,anensembleofdecisiontreestrainedwithgradi-entboosting(Keetal.,2017).Thismodelwasselectedbecauseofitsperformancewithsmalltrainingdatasetsandbecauseitcanignoreunin-formativefeatures,butalsoforitstrainingspeedcomparedtoalternativeimplementationsofgradi-entboostingmethods.Multi-layerperceptron(MLP):Asasimplebase-line,webuiltamulti-layerperceptronusingthreesetsoffeatures:apre-trainedcontextualrepre-sentationoftheclause(SentBERT;ReimersandGurevych(2019));theconcatenationofthiscon-textualrepresentationoftheclauseandarule-basedlabel(notrelyingonthepreviousclauses);andfinallytheconcatenationofallthefeaturesmen-tionedinsection4.2,withoutthecontextualizedrepresentation.LSTMoverasequenceofclauses:Sinceweareworkingwithconversationaldata,wealsowantedtotestwhethertakingintoac-countthepreviousclauseshelpstodetectthetypeofhedgeclassinthenextclause.Formally,wewanttoinferyiusingyi=maxy\u2208ClassesP(y|X(ui),X(ui\u22121),...,X(ui\u2212K)),whereKisthenumberofpreviousclausesthatthemodelwilltakeintoaccount.The\n2164\n\n\nMLPmodelpresentedaboveinfersyiusingyi=maxy\u2208ClassesP(y|X(ui)),thereforeadifferenceofperformancebetweenthetwomodelswouldbeasignthatusinginformationfromthepreviousclausescouldhelptodetectthehedgedformulationinthecurrentclause.WetestedaLSTMmodelwiththesamerepresentationsforclausesasfortheMLPmodel.CNNwithattention:Goeletal.(2019)estab-lishedtheirbestperformanceonhedgedetec-tionusingaCNNmodelwithadditiveattentionoverword(andnotclause)embeddings.Con-trarytotheMLPandLSTMmodelsmentionedabove,thismodeltriestoinferyiusingyi=maxy\u2208ClassesP(y|g(w0),g(w1),...,g(wL)),withLrepresentingthemaximumclauselengthweal-low,andgrepresentingafunctionthatturnsthewordwj,j\u2208[0,L]intoavectorrepresentation(formoredetails,pleaseseeAdelandSch\u00fctze(2017)).BERT:Tobenefitfromdeepsemanticandcon-textualrepresentationsoftheutterances,wealsofine-tunedBERT(Devlinetal.,2019)onourclas-sificationtask.BERTisapre-trainedTransformersencoder(Vaswanietal.,2017)thathassignificantlyimprovedthestateoftheartonanumberofNLPtasks,includingsentimentanalysis.Itproducesacontextualrepresentationofeachwordinasen-tence,makingitcapableofdisambiguatingthemeaningofwordslike\"think\"or\"just\"thatarerepresentativeofcertainclassesofhedges.BERT,however,isnotablyhardtointerpret.4.4AnalysistoolsLookingatwhichfeaturesimprovetheperfor-manceofourclassificationmodelstellsuswhetherthesefeaturesareinformativeornot,butdoesnotexplainhowthesefeaturesareusedbythemod-elstomakeagivenprediction.Wethereforepro-ducedacomplementaryanalysisusinganinter-pretabilitytool.Asdemonstratedby(LundbergandLee,2017),LightGBMinternalfeatureimpor-tancescoresareinconsistentwithboththemodelbehaviorandhumanintuition,soweinsteadusedamodel-agnostictool.SHAP(LundbergandLee,2017)assignstoeachfeatureanimportancevalue(calledShapleyvalues)foraparticularpredictiondependingontheextentofitscontribution(ade-tailedintroductiontoShapleyvaluesandSHAPcanbefoundinMolnar(2020)).SHAPisamodel-agnosticframework,thereforethevaluesassoci-atedwithasetoffeaturescanbecomparedacrossmodels.ItshouldbenotedthatSHAPproducesexplanationsonacase-by-casebasis,thereforeitcanbothprovidelocalandglobalexplanations.FortheGradientBoostingmodel,weuseanadaptedversionofSHAP(Lundbergetal.,2018),calledTreeSHAP.5Experimentsandresults5.1ExperimentalsettingTodetectthebestsetoffeatures,weusedLight-GBMandproceededincrementally,byaddingthegroupoffeatureswethoughttobemostlikelyasso-ciatedwithhedges.Wedidnotconsidertheriskofrelyingonasub-optimalsetoffeaturesthroughthisprocedurebecauseofthestrongabilityofLight-GBMtoignoreuninformativefeatures.Weusethisincrementalapproachasawaytotestourintuitionabouttheperformativityofgroupsoffeatures(i.e.doesaddingafeatureimprovetheperformanceofthemodel)withregardtothetaskofclassifica-tion.Tocompareourmodels,wetrainedthemonthe4-classtask,andlookedattheaverageoftheweightedF1-scoresforthethreehedgeclasses(i.e.howwellthemodelsinferminorityclasses)thatwereporthereas\"3-classes\",andattheaverageoftheweightedF1-scoresforthe4classes,thatwereportas\"4-classes\".DetailsofthehyperparametersandexperimentalsettingsareprovidedinAppendixA.5.2ModelcomparisonandfeatureanalysisOverallresults:Table4presentstheresultsob-tainedbythe6modelspresentedinSection4.3forthemulti-classproblem.Bestperformance(F1-scoreof79.0)isobtainedwithLightGBMlever-agingalmostallthefeatures.Intheappendix(seeTable8andTable9)weindicatetheconfidenceintervalstorepresentthesignificanceofthediffer-encesbetweenthemodels.First,andperhapssurprisingly,wenoticethattheuseof\"Knowledge-Driven\"featuresbasedonrulesbuiltfromlinguisticknowledgeofhedgesintheLightGBMmodeloutperformstheuseofpre-trainedembeddingswithinafine-tunedBERTmodel(79.0vs.70.6),andintheneuralbaselinefrom(Goeletal.,2019)(79.0vs64.5).ThelowscoresobtainedbytheLGBM,LSTMandMLPmodelswithpre-trainedsentenceem-beddingsversusKnowledge-Drivenfeaturesmightsignalthatthewordpatternscharacterizinghedgesarenotsalientintheserepresentations(i.e.the\n2165\n\n\n68.5(1.6)35.8(3.1)64.8(1.1)Attention-CNN(3-classes)\n65.1(5.7)39.8(8.0)65.2(5.1)BERT(3-classes)\n\u220570.6(2.3)\u2205LGBM(3-classes)\n79.0(1.3)35.0(2.2)70.1(1.4)\nRule-based(3-classes)\n94.7\u2205\u2205MLP(4-classes)\n3Notethatthereisstrongredundancybetweensomefea-turesofLIWCandthespaCyPOStaggerthatbothproducea\"Pronoun\"category,usingalexiconinthefirstcase,andaneuralinferenceinthesecond.\n\u220594.9(0.4)\u2205LGBM(4-classes)\nRule-based(4-classes)\nTable4:AveragedweightedF1-scores(andstandarddeviation)forthethreeminorityclassesandforthe4classes,forallmodels.\"KD\"standsfor\"Knowledge-Driven\",meaningthatthefeaturesarederivedfromlexicon,n-grammodelsandannotations.distancebetween\"Ithinkyoushouldadd5.\"and\"Youshouldadd5.\"isshort.).KDFeaturesseemtoprovideabetterseparabilityoftheclasses.ThecombinationofKDfeaturesandPre-trainedem-beddingsdoesnotsignificantlyimprovetheperfor-manceofthemodelscomparedtotheKDFeaturesonly,whichsuggeststhattheinformationfromthePre-trainedembeddingsisredundantwiththeonefromtheKDFeatures.Thisresultmaybeduetothehighdimensionalityoftheinputvector(868withPCAontheKDFeatures;2500otherwise).Asecondfindingisthattheuseofgradientboost-ingmodelsontopofrule-basedclassifiersbettermodelsthehedgeclasses.Theothermachinelearn-ingmodelsdidnotprovetobeaseffective,exceptforBERT.FeatureanalysisusingLightGBM:Usingthebestperformingmodel,Table5showstheroleofeachfeaturesetinthepredictiontask.ThesignificanceofthedifferencesisshowninTable10andTable11.Comparedtotherule-basedmodel,theintroductionofn-gramssignificantlyimprovedtheperformanceofourclassifier,suggestingthatsomelexicalandsyntacticinformationdescribingthehedgeclasseswasnotpresentintherule-basedmodel.LookingatTable5,wedonotobservesignificantdifferencesbetweentheLGBMmodelusingonlythelabelrulebased+(1-gramsand2-grams)andthemodelsin-corporatingmorefeatures.Tooursurprise,neitherthetutoringmovesnorthenonverbalfeaturessig-nificantlyimprovedtheperformanceofthemodel.The2featureswereincludedtoindexthespecificpeertutoringcontextofthesehedges,sothisindi-catesthatinfutureworkwemightwishtoapplythecurrentmodeltoanothercontextofusetoseeifthismodelofhedgesismoregenerallyapplicablethanweoriginallythought.Bycombiningthisresultwiththeincreasedperformanceofthemodelus-ingKnowledge-Driven(i.e.explicit)featurescom-paredtopre-trainedembeddings,itwouldseemthathedgesareaboveallalexicalphenomenon(i.e.producedbyspecificlexicalelements).5.3In-depthanalysisoftheinformativefeaturesWetrainedtheSHAPexplanationmodelsonLight-GBMwithallfeatures.Themostinformativefea-tures(inabsolutevalue)foreachclassareshowninTable6,andtheplotsbyclassarepresentedintheAppendix.Themostimportantfeaturesseemtobetherule-basedlabels,whichappearinatleastthefourthpositionforthreeclasses(seeTable6),andinthefirstpositionforPropositionalHedgesandNothedgedclasses.Surprisingly,theRule-Basedlabeldoesnotappearinthetop20featuresforApologizers.However,giventhattheclassrarelyappearsinthedata,therulesseldomactivate,sothefeaturemaysimplybeinformativeforaverysmallnumberofclauses.Unigrams(Oh,Sorry,just,Would,andI)arealsopresentinthe5top-rankedfeatures.Thisconfirmsthefindingsmen-tionedinrelatedworkforthecharacterizationofthedifferenthedgeclasses(justwithPropositionalHedges,sorrywithApologizer,IwithSubjectiviz-ers).ThepresenceofOhalsohashighimportanceforthecharacterizationofApologizer(n=2),asillustratedinexamplessuchas\"Ohsorry,that\u2019snine.\".Wenotethattheoccurrencesof\"Ohsorry\"asastand-aloneclausewereexcludedbyourrule-basedmodelbecausetheydonotcorrespondtoanapologizer(theycannotmitigatethecontentofapropositionifthereisnopropositionassociated).Thisexampleillustratestheinterestofamachinelearningmodelapproachtodisambiguatethefunc-tionofconventionalnon-propositionalphraseslike\"Ohsorry\".Inaddition,SHAPhighlightstheimportanceofnovelfeatureswhosefunctionwasnotidentifiedinthehedgesliterature:(i)whatLIWCclassifiesasinformalwordsbutthataremostlyinterjectionslikeahandoharestronglyassociatedwithApol-ogizer,asaredisfluencies(n=12);(ii)theuseofPOStagsseemstobeveryrelevantforcharac-terizingthedifferentclasses(2-gramofPOStagfeatures3occurinthetop-rankedfeaturesofallthe\n\u220564.5(3.0)\u2205LSTM(3-classes)\nKDFeat.(KDF)Pre-TrainedEmb.(PTE)KDF+PTE\n\u220594.4(0.2)\u2205LSTM(4-classes)\n96.7(0.2)91.0(0.2)95.4(0.2)\n94.8(0.3)89.7(0.4)93.9(0.4)Attention-CNN(4-classes)\n93.9(1.4)89.1(1.4)94.1(1.2)BERT(4-classes)\n67.6\u2205\u2205MLP(3-classes)\nModels\n2166\n\n\n95.0(0.2)96.5(0.3)96.5(0.2)96.7(0.2)96.6(0.4)96.7(0.3)\n3-classes\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\n68.8(0.8)78.2(1.6)78.1(1.3)79.0(1.3)78.5(2.4)78.7(1.8)\nTable5:AveragedweightedF1-scoresforthethreeclassesofhedgesandthefourclasses,withanadditiveintegrationofKDFfeaturesintheLightGBMmodel.Thestandarddeviationiscomputedacrossfivefolds.\n4-classes\nModels\nRankApologizerSubjectivizersProp.HedgesNothedged\n1Functionwords(LIWC)\"I\"ClasslabelClasslabel2\"Oh\"(LIWC)\"Yeah\"\"Would\"\"Would\"3\"Sorry\"Noun(POS)\"Just\"\"Yeah\"4Affect(LIWC)ClasslabelFunctionword(LIWC)Noun(POS)5ClauselengthCognitiveprocess(LIWC)Netspeak(LIWC)Cognitiveprocess(LIWC)Table6:Mostimportantclause-levelfeaturesforLightGBMaccordingtotheSHAPanalysis.classes(seeFiguresintheAppendix).Itmeansthattherearesomerecurringsyntacticpatternsineachclass;(iii)Regardingtheutterancesize,aclauseshorterthanthemeanisweaklyassociatedwithdirectness(n=17)whilealongerclausesuggeststhatitcontainsaSubjectivizer(n=6).Apologizersarecharacterizedbyameanclauselength(n=5),withfewvariationsfromit;(iv)Tutoringmovesarenotstrongpredictorsofanyclasses:\"Affirma-tionfromtutor\"istheonlyfeatureappearingasapredictorofPropositionalhedges(n=20).ThisisconsistentwiththefeatureanalysisinTable5,suggestingthattutoringmovesdonotsignificantlyimprovetheperformanceoftheclassifier;(v)Non-verbalbehaviorsdonotappearasimportantfea-turesfortheclassification.Thisiscoherentwithresultsfrom(Goeletal.,2019).Notethatprosodymightplayaroleindetectinginstructionsthattrailoff,but,asdescribed,paraverbalfeatureswerenotavailable;(vi)Wouldplaysanimportantroleintheproductionofhedges,asitisstronglyassociatedtoPropositionalhedges(n=2).Itisinterestingtonotethat,whendesigningtherule-basedclassifier,wesawitdecreaseinperformancewhenwestartedtoincludewouldinourregularexpressionpatterns,probablybecausetheformishardtodisambiguateforadeterministicsystem.WhileexploringtheShapleyvaluesassociatedtoeachclause,weobservedthatfeaturesliketutoringmovesareextremelyinformativeforaverysmallnumberofclauses(thereforenotsignificantlyinflu-encingtheoverallperformanceoftheprediction),andmoreorlessnotinformativefortherest.Infer-ringtheglobalimportanceofafeatureasameanacrosstheshapleyvaluesinthedatasetmaynotbetheonlywaytoexplorethebehaviorofgradi-entboostingmethods.ItmightbemoreusefultoclusterclausesbasedontheimportancethatSHAPgivestothatfeatureinitsclassification,asthiscouldhelpdiscoversub-classesofhedgesthataredifferentiatedfromtherestbytheirinteractionwithaspecificfeature(inthewaythatsomeApologiz-ersarecharacterizedbyan\"oh\").Wealsonotethattheexplanationmodelissensitivetospuri-ouscorrelationsinthedataset,causedbythesmallrepresentationofsomeclass:forexample,\"nine\"(n=7)and\"four\"(n=20)arepositivepredictorsofApologizers.6ConclusionandfutureworkThroughourclassificationperformanceexperi-ments,weshowedthatitispossibletousema-chinelearningmethodstodiminishtheambigu-ityofhedges,andthatthehybridapproachofus-ingrule-basedlabelfeaturesderivedfromsocialscience(includinglinguistics)literaturewithinamachinelearningmodelhelpedsignificantlytoin-creasethemodel\u2019sperformance.Nonverbalbehav-iorsandtutoringmovesdidnotprovideinformationatthesentencelevel;boththeperformanceofthemodelandthefeaturecontributionanalysissug-gestedthattheirimpactonthemodeloutputwasnotstrong.ThisisconsistentwithresultsfromGoeletal.(2019).However,infutureworkwewouldliketoinvestigatethepotentialofmultimodalpat-ternswhenweareabletobettermodelsequentiality(e.g.,negativefeedbackfollowedbyasmile).Re-gardingtheSHAPanalysis,mostofthefeaturesthatareconsideredasimportantarecoherentwiththedefinitionoftheclasses(Iforsubjectivizers,sorryforapologizers,justforpropositionalhedges).However,wediscoveredthatfeatureslikeutterance\n2167\n\n\nsizecanalsoserveasindicatorsofcertainclassesofhedges.AlimitationofSHAPisthatitmakesafeatureindependenceassumption,whichpromptstheexplanatorymodeltounderestimatetheimpor-tanceofredundantfeatures(likepronounsinourwork).Inthefuturewewillexploreexplanatorymodelscapableoftakingintoaccountthecorre-lationbetweenfeaturesinthedatasetlikeSAGE(Covertetal.,2020),butsuitedforveryimbal-anceddatasets.Inthedomainofpeer-tutoring,wewouldliketobeabletofurthertestthelinkbe-tweenhedgesandrapport,andthelinkbetweenhedgesandlearninggainsinthesubjectbeingtu-tored.Asnotedabove,thiskindofstudyrequiresafine-grainedcontrolofthelanguageproducedbyoneoftheinterlocutors,whichisdifficulttoachieveinahuman-humanexperience.Wenotethatthehedgeclassifiercanbeusednotjusttoclassify,butalsotoworktowardsimprovingthegenerationofhedgesfortutoragents.Infutureworkwewillexploreusingtheclassifiertore-rankgenerationoutputs,takingadvantageoftherecur-ringsyntacticpatterns(see(ii)inSection5.3)toimprovethegenerationprocessofhedges,andre-generatingclausesthatdon\u2019tcontainoneofthesesyntacticpatterns.AcknowledgmentsManythankstomembersoftheArticuLaboatIN-RIAParisfortheirpreciousassistance.ThisworkwassupportedinpartbythetheFrenchgovern-mentundermanagementofAgenceNationaledelaRechercheaspartofthe\u201cInvestissementsd\u2019avenir\u201dprogram,referenceANR-19-P3IA-0001(PRAIRIE3IAInstitute).ReferencesHeikeAdelandHinrichSch\u00fctze.2017.Exploringdif-ferentdimensionsofattentionforuncertaintydetec-tion.InProceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputa-tionalLinguistics:Volume1,LongPapers,pages22\u201334,Valencia,Spain.AssociationforComputa-tionalLinguistics.ShoshanaBlum-Kulka.1987.Indirectnessandpolite-nessinrequests:Sameordifferent?Journalofpragmatics,11(2):131\u2013146.PenelopeBrownandStephenCLevinson.1987.Polite-ness:Someuniversalsinlanguageusage,volume4.Cambridgeuniversitypress.JudeeKBurgoonandRandallJKoper.1984.Nonverbalandrelationalcommunicationassociatedwithreti-cence.HumanCommunicationResearch,10(4):601\u2013626.IanCovert,ScottMLundberg,andSu-InLee.2020.Understandingglobalfeaturecontributionswithad-ditiveimportancemeasures.AdvancesinNeuralInformationProcessingSystems,33:17212\u201317223.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforCom-putationalLinguistics:HumanLanguageTechnolo-gies,Volume1(LongandShortPapers),pages4171\u20134186.Rich\u00e1rdFarkas,VeronikaVincze,Gy\u00f6rgyM\u00f3ra,J\u00e1nosCsirik,andGy\u00f6rgySzarvas.2010.Theconll-2010sharedtask:learningtodetecthedgesandtheirscopeinnaturallanguagetext.InProceedingsofthefour-teenthconferenceoncomputationalnaturallanguagelearning\u2013Sharedtask,pages1\u201312.BruceFraser.2010.Pragmaticcompetence:Thecaseofhedging.Newapproachestohedging,1534.ViolaGanterandMichaelStrube.2009.Findinghedgesbychasingweasels:Hedgedetectionusingwikipediatagsandshallowlinguisticfeatures.InProceedingsoftheACL-IJCNLP2009ConferenceShortPapers,pages173\u2013176.PranavGoel,YoichiMatsuyama,MichaelMadaio,andJustineCassell.2019.\u201cithinkitmighthelpifwemultiply,andnotadd\u201d:Detectingindirectnessincon-versation.In9thInternationalWorkshoponSpokenDialogueSystemTechnology,pages27\u201340.Springer.CynthiaHoward,PamelaW.Jordan,BarbaraMariaDiEugenio,andSandraKatz.2015.Shiftingtheload:apeerdialogueagentthatencouragesitshumancollab-oratortocontributemoretoproblemsolving.Interna-tionalJournalofArtificialIntelligenceinEducation,27:101\u2013129.KenHyland.1998.Hedginginscientificresearcharti-cles,volume54.JohnBenjaminsPublishing.GuolinKe,QiMeng,ThomasFinley,TaifengWang,WeiChen,WeidongMa,QiweiYe,andTie-YanLiu.2017.Lightgbm:Ahighlyefficientgradientboost-ingdecisiontree.Advancesinneuralinformationprocessingsystems,30:3146\u20133154.KlausKrippendorff.2004.Reliabilityincontentanaly-sis:Somecommonmisconceptionsandrecommen-dations.Humancommunicationresearch,30(3):411\u2013433.GeorgeLakoff.1975.Hedges:Astudyinmeaningcriteriaandthelogicoffuzzyconcepts.InContem-poraryresearchinphilosophicallogicandlinguisticsemantics,pages221\u2013271.Springer.\n2168\n\n\nMatthewLeach.2005.Rapport:Akeytotreatmentsuc-cess.Complementarytherapiesinclinicalpractice,11:262\u20135.IlyaLoshchilovandFrankHutter.2018.Decoupledweightdecayregularization.InInternationalConfer-enceonLearningRepresentations.ScottMLundberg,GabrielGErion,andSu-InLee.2018.Consistentindividualizedfeatureat-tributionfortreeensembles.arXivpreprintarXiv:1802.03888.ScottMLundbergandSu-InLee.2017.Aunifiedap-proachtointerpretingmodelpredictions.InProceed-ingsofthe31stinternationalconferenceonneuralinformationprocessingsystems,pages4768\u20134777.HowardLuneandBruceLBerg.2017.Qualitativeresearchmethodsforthesocialsciences.Pearson.MichaelMadaio,JustineCassell,andAmyOgan.2017.Theimpactofpeertutors\u2019useofindirectfeedbackandinstructions.Philadelphia,PA:InternationalSo-cietyoftheLearningSciences.BenMedlockandTedBriscoe.2007.Weaklysuper-visedlearningforhedgeclassificationinscientificliterature.InProceedingsofthe45thannualmeetingoftheassociationofcomputationallinguistics,pages992\u2013999.ChristophMolnar.2020.Interpretablemachinelearn-ing.Lulu.com.AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advancesinneuralinformationprocessingsystems,32:8026\u20138037.JamesWPennebaker,RyanLBoyd,KaylaJordan,andKateBlackburn.2015.Thedevelopmentandpsycho-metricpropertiesofliwc2015.Technicalreport.JeffreyPennington,RichardSocher,andChristopherDManning.2014.Glove:Globalvectorsforwordrep-resentation.InProceedingsofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing(EMNLP),pages1532\u20131543.EllenFPrince,JoelFrader,CharlesBosk,etal.1982.Onhedginginphysician-physiciandiscourse.Lin-guisticsandtheProfessions,8(1):83\u201397.AnnaProkofievaandJuliaHirschberg.2014.Hedgingandspeakercommitment.In5thIntl.WorkshoponEmotion,SocialSignals,Sentiment&LinkedOpenData,Reykjavik,Iceland.NilsReimersandIrynaGurevych.2019.Sentence-bert:Sentenceembeddingsusingsiamesebert-networks.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages3982\u20133992.OscarJRomero,RanZhao,andJustineCassell.2017.Cognitive-inspiredconversational-strategyreasonerforsocially-awareagents.InIJCAI,pages3807\u20133813.TimRowland.2007.\u2018wellmaybenotexactly,butit\u2019saroundfiftybasically?\u2019:Vaguelanguageinmath-ematicsclassrooms.InVaguelanguageexplored,pages79\u201396.Springer.SaschaSchneider,SteveNebel,SimonPradel,andG\u00fcn-terDanielRey.2015.Mindyourpsandqs!howpoliteinstructionsaffectlearningwithmultimedia.ComputersinHumanBehavior,51:546\u2013555.TanmaySinhaandJustineCassell.2015.Weclick,wealign,welearn:Impactofinfluenceandconvergenceprocessesonstudentlearningandrapportbuilding.InProceedingsofthe1stWorkshoponModelingINTERPERsonalSynchrONyAndInfLuence,INTER-PERSONAL\u201915,page13\u201320,NewYork,NY,USA.AssociationforComputingMachinery.HelenSpencer-Oatey.2005.(im)politeness,faceandperceptionsofrapport:Unpackagingtheirbasesandinterrelationships.1(1):95\u2013119.Gy\u00f6rgySzarvas,VeronikaVincze,Rich\u00e1rdFarkas,Gy\u00f6rgyM\u00f3ra,andIrynaGurevych.2012.Cross-genreandcross-domaindetectionofsemanticuncer-tainty.ComputationalLinguistics,38(2):335\u2013367.BuzhouTang,XiaolongWang,XuanWang,BoYuan,andShixiFan.2010.Acascademethodfordetectinghedgesandtheirscopeinnaturallanguagetext.InProceedingsoftheFourteenthConferenceonCom-putationalNaturalLanguageLearning\u2013SharedTask,pages13\u201317.MorganUlinski,SethBenjamin,andJuliaHirschberg.2018.Usinghedgedetectiontoimprovecommittedbelieftagging.InProceedingsoftheWorkshoponComputationalSemanticsbeyondEventsandRoles,pages1\u20135.TeppoVarttala.1999.Remarksonthecommunicativefunctionsofhedginginpopularscientificandspecial-istresearcharticlesonmedicine.Englishforspecificpurposes,18(2):177\u2013200.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,\u0141ukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.InAdvancesinneuralinformationpro-cessingsystems,pages5998\u20136008.ErikVelldal.2011.Predictingspeculation:asimpledis-ambiguationapproachtohedgedetectioninbiomed-icalliterature.JournalofBiomedicalSemantics,2(5):1\u201314.VeronikaVincze.2014.Uncertaintydetectioninnaturallanguagetexts.PhD,UniversityofSzeged,page141.\n2169\n\n\nNingWang,WLewisJohnson,RichardEMayer,PaolaRizzo,ErinShaw,andHeatherCollins.2008.Thepolitenesseffect:Pedagogicalagentsandlearningoutcomes.Internationaljournalofhuman-computerstudies,66(2):98\u2013112.ShawnWarner-Garcia.2014.Laughingwhennothing\u2019sfunny:Thepragmaticuseofcopinglaughterinthenegotiationofconversationaldisagreement.Prag-matics,24(1):157\u2013180.TimothyWilliamson.2002.Vagueness.Routledge.RanZhao,AlexandrosPapangelis,andJustineCassell.2014.Towardsadyadiccomputationalmodelofrap-portmanagementforhuman-virtualagentinteraction.InInternationalConferenceonIntelligentVirtualAgents,pages514\u2013527.Springer.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016a.Automaticrecognitionofconversa-tionalstrategiesintheserviceofasocially-awaredialogsystem.InProceedingsofthe17thAnnualMeetingoftheSpecialInterestGrouponDiscourseandDialogue,pages381\u2013392.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016b.Socially-awarevirtualagents:Au-tomaticallyassessingdyadicrapportfromtemporalpatternsofbehavior.InInternationalconferenceonintelligentvirtualagents,pages218\u2013233.Springer.AAdditionalinformationontheexperimentalsettingsWeusedPyTorch(Paszkeetal.,2019)toimple-menttheneuralmodels.Foreachsetoffeatures,hyperparameterswereselectedusingOptuna(Ak-iba,2019),aparametersearchframework.Were-implementedtheAttention-CNNwithGlove(Pen-ningtonetal.,2014)300-Dwordsembeddingsasthevectorrepresentation.Foreachmodels,theresultsarecross-validatedusing5folds(wechose5insteadof10toavoidhavingfoldswithtoofewsamplesperclass).Wecorrectedthelossfunctionforclassimbalancetoforcethemodeltoadaptmoretothelessfrequentclasses.Thestrengthofthiscorrectiondependedonthemodel,andwasselectedbecauseitprovidedasatisfyingcompro-misebetweenfavoringrecallandprecisionintheclassificationresultsofthatmodel.ForLightGBM,a\"squarerootofthesquarerootoftheinverseclassproportion\"correctionwasselected.Neu-ralmodelsweretrainedusingAdamWasanop-timizer(LoshchilovandHutter,2018),andusedareducedfeaturevector,obtainedwiththeap-plicationofPCA(dinit=1800;d=100;99.8%oftheinformationisconserved).Nosignifi-cantperformancedifferenceswereobservedbe-tweentheoriginalvectorandthereducedvectorfortrainingthemodels.TocomputetheSHAPvaluesmentionedinthepaper,wekeptonesplittoperformthe5-splitofthedataset,andleave1splittovalidateandearlystopthemodel,inor-dertoavoidoverfitting.Acompleteconfigura-tionofhyperparametersusedforeachmodelisre-portedintheGitHubrepositorywiththecodeofthepaper:https://github.com/YannRaphalen/Hedges-Detection.TheBERTmodelwasfine-tunedonaNvidiaQuadroRTX8000GPU.BTables\n2170\n\n\n?(whether|if|is|that|it|this)?.*Subj.\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nNoNoYesNoNoNoYesYesYesYesYesBERT(PTE)\nexceptionally|forthemostpart|inamannerofspeaking|\nNoYesNoNoNoYesNoNoYesYesNoMLP(KDF)\nsomethingbetween|essentially|only).*Prop.\nTable8:Significancetableforthe3-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesYesYesYesYesNoYesYesYesNoYesMLP(KDF+PTE)\ntechnically|typically|virtually|approximately|\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nmuch|somewhat|exactly|almost|littlebit|quite|\nClass\nrelatively|roughly|sotosay|strictlyspeaking|\nNoNoYesNoNoYesNoYesYesYesYesLSTM(KDF)\nYesNoNoNoNo+TM\nTable10:Significancetableforthe3-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoNoYesYesYesYesYesYesNoYesYes\n.*(it)(looks|seems|appears)[,]?.*\",\".*(or|and)(that|something|stuff|soforth)Table7:Regexprulesusedfortheclassifier.\nRule-based\nRule-based\n.*(you(might|may)(believe|think)).*Subj.\nYesYesYesYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nYesYesNoYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nNoNoYesNoNoYesNoNoYesYesYesLSTM(KDF)\nYesYesNoYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(i|i\u2019m|you|it\u2019s)(am|are)(apparently|surely)[,]?.*Prop.\nNoNoYesNoNoYesNoYesYesYesYesLSTM(PTE)\nNoNoYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nNoNoYesYesNoYesYesYesYesYesNoLGBM(KDF)\nNoNoYesYesYesYesYesYesYesYesNoLGBM(KDF)\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nYesNoYesYesYesYesYesYesNoYesYes\nYesYesYesYesYesYesYesYesYesYesYesMLP(KDF+PTE)\nregular|regularly|actually|almost|asitwere|basically|\nNoYesYesNoYesYesYesNoYesYesNoMLP(PTE)\nRule(regexp)\n.*(i|i\u2019m|we)?(was|am|wasn\u2019t)??(not)?(sure|certain).*Subj.\n.*(accordingto|presumably).*Subj.\n(?!.*(be|been|was)likeexcuseme)((excuseme|sorry)[w,\u2019]+|[w,\u2019]+(excuseme|sorry))Prop.\n.*(i|you|we)haveto(check|look|verify).*Subj.\n.*(i\u2019m|i|we\u2019re)(am|are)??(apologize|sorry).*Apol.\nonthetallside|parexcellence|particularly|\n(?!what).*(i|we)?(don\u2019t|didn\u2019t|did)??(not)?\ninarealsense|inasense|inaway|largely|literally|\nYesYesYesNoNoNoYesYesYesYesYesBERT(PTE)\nModels\nModels\nModels\nYesYesYesNoNoYesNoNoYesYesYesLSTM(PTE)\nYesNoNoNoNo\nYesYesYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nSubj.\nTable9:Significancetableforthe4-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoYesNoNoNoYesNoNoYesYesNoMLP(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(ifeellikeyou).*Subj.\nlooselyspeaking|kinda|moreorless|mostly|often|\n.*(unlessi).*Apol.\nprobably|canbeviewas|crypto-|especially|essentially|\n.*(ifi\u2019mnotwrong|ifi\u2019mright|ifthat\u2019strue).*Subj.\nYesNoNoNoNo+POS\n(guess|guessed|thought|think|believe|believed|suppose|supposed)\n.*(just|alittle|maybe|actually|sortof|kindof|pretty\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nNoYesYesNoYesYesNoNoYesYesYesMLP(KDF)\nprettymuch|principally|pseudo-|quintessentially|\n2171\n\n\nFigure2:Absoluteaveragedfeaturecontribution,asindicatedbySHAP.Thelongerthebarisforonecolor,themorethefeatureisassociatedwiththeclassrepresentedbythatcolor.\nFigure3:Averagedcontributionoffeaturestothedetectionofthe\"Notindirect\"class,asindicatedbySHAP.Eachdotcorrespondstoaclassifiedclause.Areddotindicatesthatthefeatureispresentintheclause,whileabluedotindicatesthatthefeatureisabsent.Thefartherontherightthedotis,themorethefeaturecontributedtoitsclassificationasahedge.\n2172\n\n\nFigure4:Averagedcontributionoffeaturestothedetectionof\"Apologizers\",asindicatedbySHAP.\nFigure5:Averagedcontributionoffeaturestothedetectionof\"Propositionalhedges\",asindicatedbySHAP.\n2173\n\n\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nYesNoNoNoNo+TM\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nFigure6:Averagedcontributionoffeaturestothedetectionof\"Subjectivizers\",asindicatedbySHAP.\nModels\nYesNoNoNoNo\nTable11:Significancetableforthe4-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesNoNoNoNo+POS\n2174"}], "doc_text": "\"Youmightthinkaboutslightlyrevisingthetitle\":IdentifyingHedgesinPeer-tutoringInteractionsYannRaphalen1,Chlo\u00e9Clavel2,JustineCassell1,31InriaParis2LTCI,InstitutPolytechniquedeParis,Telecom-Paris3CarnegieMellonUniversityyann.raphalen.pro@gmail.com,justine@cs.cmu.edu,chloe.clavel@telecom-paris.frAbstractHedgesplayanimportantroleinthemanage-mentofconversationalinteraction.Inpeer-tutoring,theyarenotablyusedbytutorsindyads(pairsofinterlocutors)experiencinglowrapporttotonedowntheimpactofinstructionsandnegativefeedback.Pursuingtheobjectiveofbuildingatutoringagentthatmanagesrap-portwithstudentsinordertoimprovelearning,weusedamultimodalpeer-tutoringdatasettoconstructacomputationalframeworkforiden-tifyinghedges.Wecomparedapproachesre-lyingonpre-trainedresourceswithothersthatintegrateinsightsfromthesocialsciencelitera-ture.Ourbestperformanceinvolvedahybridapproachthatoutperformstheexistingbase-linewhilebeingeasiertointerpret.Weemployamodelexplainabilitytooltoexplorethefea-turesthatcharacterizehedgesinpeer-tutoringconversations,andweidentifysomenovelfea-tures,andthebenefitsofsuchahybridmodelapproach.1IntroductionRapport,mostsimplydefinedasthe\u201c...relativeharmonyandsmoothnessofrelationsbetweenpeo-ple...\u201d(Spencer-Oatey,2005),hasbeenshowntoplayaroleinthesuccessofactivitiesasvariedaspsychotherapy(Leach,2005)andsurveyinterview-ing(LuneandBerg,2017).Inpeer-tutoring,rap-port,asmeasuredbytheannotationofthinslicesofvideo,hasbeenshowntobebeneficialforlearningoutcomes(Zhaoetal.,2014;SinhaandCassell,2015).Thelevelofrapportrisesandfallswithconversationalstrategiesdeployedbytutorsandtuteesatappropriatetimes,andasafunctionofthecontentofpriorturns.Thesestrategiesincludeself-disclosure,referringtosharedexperience,and,onthepartoftutors,givinginstructionsinanindirectmanner.Someworkhasattemptedtoautomaticallydetectthesestrategiesintheserviceofintelligenttutors(Zhaoetal.,2016a),butonlyafewstrate-gieshavebeenattempted.Otherworkhascon-centratedona\"socialreasoningmodule\"(Romeroetal.,2017)todecidewhichstrategiesshouldbegeneratedinagivencontext,butindirectnesswasnotamongthestrategiestargeted.Inthispaper,wefocusontheautomaticclassificationofonespe-cificstrategythatisparticularlyimportantforthetutoringdomain,andthereforeimportantforintel-ligenttutors:hedging,asub-partofindirectnessthat\"softens\"whatwesay.Thisworkispartofalargerresearchprogramwiththelong-termgoalofautomaticallygeneratingindirectnessbehaviorsforatutoringagent.\nFigure1:Amockconversationdisplayingeachtypeofhedgedformulation.AccordingtoBrownandLevinson(1987),hedgesarepartofthelinguistictoolsthatinterlocu-torsusetoproducepoliteness,bylimitingthefacethreattotheinterlocutor(basicallybylimitingtheextenttowhichtheinterlocutormightexperienceembarrassmentbecauseofsomekindofpoorper-formance).Anexampleis\"that\u2019skindofawronganswer\".Hedgesarealsofoundwhenspeakerswishtoavoidlosingfacethemselves,forexam-plewhensaying(\"IthinkImighthavetoadd6.\").Madaioetal.(2017)foundthatinapeer-tutoringtask,whenrapportbetweeninterlocutorsislow,tu-teesattemptedmoreproblemsandcorrectlysolvedmoreproblemswhentheirtutorshedgedinstruc-\n2160 Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1: Long Papers, pages 2160 - 2174 May 22-27, 2022 c(cid:13)2022 Association for Computational Linguistics\n\n\ntions,whichlikewisepointstowardsa\"mitigationoffacethreat\"function.Hedgescanalsobeasso-ciatedwithanonverbalcomponent,forexampleavertedeyegazeduringcriticism(BurgoonandKoper,1984).Hedgesarenot,however,alwaysap-propriate,asin\"Ikindofthinkit\u2019srainingtoday.\"whentheinterlocutorscanbothseerain(althoughitmightbetakenashumorous).Thesefactsabouthedgesmotivateawaytoautomaticallydetectthemand,ultimately(althoughnotinthecurrentwork)alsogeneratethem.Inbothcaseswefirsthavetobeabletocharacterizethemusinginterpretablelinguisticfeatures,whichiswhatweaddressinthecurrentpaper.Thus,intheworkdescribedhere,basedonlinguisticdescriptionsofhedges(BrownandLevinson,1987;Fraser,2010),webuiltarule-basedclassifier.Weshowthatthisclassifierincombinationwithadditionalmultimodalinter-pretablecontext-dependentfeaturessignificantlyimprovestheperformanceofamachinelearningmodelforhedges,comparedtoalessinterpretabledeeplearningbaselinefromGoeletal.(2019)us-ingwordembeddings.Wealsoreliedonamachinelearningmodelexplanationtool(LundbergandLee,2017)toinvestigatethelinguisticfeaturesrelatedtohedgesinthecontextofpeer-tutoring,primarilytoseeifwecoulddiscoversurprisingfeaturesthattheclassificationmodelwouldassociatetohedgesinthiscontext,andwedescribethosebelow.Thecodeofthemodelsdescribedinthepaperisalsoprovided.12RelatedworkHedges:AccordingtoFraser(2010),hedgingisarhetoricalstrategythatattenuatesthestrengthofastatement.Onewaytoproduceahedgeisbyalteringthefullsemanticvalueofaparticu-larexpressionthroughPropositionalhedges(alsocalledApproximatorsinPrinceetal.(1982)),asin\"Youarekindofwrong,\"thatreduceprototypical-ity(i.eaccuracyofthecorrespondencebetweenthepropositionandtherealitythatthespeakerseekstodescribe).Propositionalhedgesarerelatedtofuzzylanguage(Lakoff,1975),andthereforetotheproductionofvagueness(Williamson,2002)anduncertainty(Vincze,2014).AsecondkindareRelationalHedges(alsocalledShieldsinPrinceetal.(1982)),suchas\u201cIthinkthatyouarewrong.\u201dor\u201cThedoctorwantsyoutostopsmoking.\u201d,conveyingthatthepropositionis\n1https://github.com/AnonymousHedges/HedgeDetectionconsideredbythespeakerassubjective.Inafurthersub-division,AttributionShields,asin\"Thedoc-torwantsyou...\",theinvolvementofthespeakerinthetruthvalueofthepropositionisnotmadeexplicit,whichallowsspeakersnottotakeastance.Asdescribedabove,Madaioetal.(2017)foundthattutorswhoshowedlowerrapportwiththeirtuteesusedmorehedgedinstructions(theyalsoemployedmorepositivefeedback),howeverthiswasonlythecasefortutorswithagreaterbeliefintheirabilitytotutor.Tuteesinthiscontextsolvedmoreproblemscorrectlywhentheirtutorshedgedinstructions.Noeffectofhedgingwasfoundfordyads(pairsofinterlocutors)withgreatersocialcloseness.However,theauthorsdidnotlookatthespecificlinguisticformstheseteenagersused.Rowland(2007)alsodescribestherolethathedg-ingplaysinthisagegroup,showingthatstudentsusebothrelational(\"IthinkthatJohnissmart.\")andpropositional(\"Johniskindofsmart.\")hedgesformuchthesameshieldingfunctionofdemon-stratinguncertainty,tosavethemfromtheriskofembarrassmentiftheyarewrong.TheauthorobservedthatteensusedfewAdaptors(kindof,somewhat)andpreferredtouseRounders(around,closeto).However,thisstudywasperformedwithanadultandtwochildren,possiblybiasingthere-sultsduetotheparticipationoftheadultinvestiga-tor.Hedgeshavebeenincludedinvirtualtutoringagentsbeforenow.(Howardetal.,2015)integratedhedgesinatutoragentforundergraduatesinCS,asawaytoencouragethestudenttotaketheinitiative.Hedgeshavealsobeenusedasawayofintegrat-ingBrownandLevinson\u2019spolitenessframework(Wangetal.,2008;Schneideretal.,2015)invir-tualtutoringagents.Resultswerenotbrokenoutbystrategy,butpolitenessingeneralwasshowntopositivelyinfluencemotivationandlearning,incertainconditions.Computationalmethodsforhedgedetection:Anumberofstudieshavetargetedthedetectionofhedgesanduncertaintyintext(MedlockandBriscoe,2007;GanterandStrube,2009;Tangetal.,2010;Velldal,2011;Szarvasetal.,2012),partic-ularlyfollowingtheCoNLL2010datasetrelease(Farkasetal.,2010).However,thisworkisnotasrelatedtohedgesinconversation,asitfocusesonaformalandacademiclanguageregister(Hy-land,1998;Varttala,1999).AsnotedbyProkofievaandHirschberg(2014),thefunctionsofhedgesaredomain-andgenre-dependent,thereforethisbias\n2161\n\n\ntowardsformalityimpliesthattheexistingworkmaynotadaptwelltothedetectionofhedgesinconversationbetweenteenagers.Aconsequenceisthattheexistingworkdoesnotconsidertermslike\"Ithink,\"sinceopinionsrarelyappearinanaca-demicwritingdataset.Instructionsarealsoalmostabsent(\"Ithinkyouhavetoaddtentobothsides.\"),astronglimitationforthestudyofconversationalhedgessinceitisinrequests(includingtutoringin-structions)thatindirectformulationsmostlyoccuraccordingtoBlum-Kulka(1987).ProkofievaandHirschberg(2014)alsonotethatitisdifficulttodetecthedgesbecausethewordpatternsassociatedwiththemhaveothersemanticandpragmaticfunc-tions:considering\"Ithinkthatyouhavetoaddxtobothsides.\"vs\"Ithinkthatyouareanidiot.\",itisnotclearthattheseconduseof\"Ithinkthat\"isanhedgemarker.Theyadvocateusingmachinelearningapproachestodealwiththeambiguityofthesemarkers.Workingonaconversationaldataset,Ulinskietal.(2018)builtacomputationalsystemtoassessspeakercommitment(i.e.atwhichpointthespeakerseemsconvincedbythetruthvalueofastatement),inparticularbyrelyingonarule-baseddetectionsystemforhedges.Comparedtothatwork,ourrule-basedclassificationmodelisdirectlydetectinghedgeclasses,andweemploythepredictionsoftherule-basedmodelasafeatureforstrongermachinelearningmodels,designedtolessentheimpactoftheimbalancebetweenclasses.Wealsoconsiderapologieswhentheyserveamit-igationfunction(wethencallthemApologizers),aswasdonebytheauthorsofourcorpus,andwealsousethetermsubjectivizersasdefinedbelow,tobeabletocomparedirectlywiththepreviousworkcarriedoutonthiscorpus.Asfarasweknow,onlyGoeletal.(2019)haveworkedwithapeer-tutoringdataset(thesameonethatwealsouse),andtheyachievedtheirbestclassificationresultbyemployinganAttention-CNNmodel,inspiredbyAdelandSch\u00fctze(2017).3ProblemstatementWeconsiderasetDofconversationsD=(c1,c2,...,c|D|),whereeachconversationiscom-posedofasequenceofindependentsyntacticclausesci=(u1,u2,...,uM),whereMisthenumberofclausesintheconversation.Notethattwoconsecutiveclausescanbeproducedbythesamespeaker.Eachclauseisassociatedwithauniquelabelcorrespondingtothediffer-enthedgeclassesdescribedinTable1:yi\u2208C={PropositionalHedges,Apologizers,Subjec-tivizers,Nothedged}.Finally,anutteranceuicanberepresentedasavectoroffeaturesX=(x1,x2,...,xN),whereNrepresentsthenumberoffeaturesweusedtodescribeaclause.Ourfirstgoalistodesignamodelthatcorrectlypredictsthelabelyiassociatedtoui.Itcanbeunderstoodasthefollowingresearchquestion:RQ1:\"Whichmodelsandfeaturescanbeusedtoautomaticallycharacterizehedgesinapeer-tutoringinteraction?\"Oursecondgoalistoidentify,foreachhedgeclass,thesetoffeaturesFclass={fk},k\u2208[1,N]sortedbyfeatureimportanceintheclassificationofclass.Itcorrespondstothefollowingresearchquestion:RQ2:\"Whatarethemostimportantlinguisticfeaturesthatcharacterizeourhedgeclassesinapeer-tutoringsetting?\"4Methodology4.1CorpusDatacollection:Thedialoguecorpususedherewascollectedaspartofalargerstudyontheeffectsofrapport-buildingonreciprocalpeertutoring.24Americanteenagers(meanage=13.5,min=12,max=15),halfmaleandhalffemale,cametoalabwherehalfoftheparticipantswerepairedwithasame-age,same-genderfriend,andtheotherhalfpairedwithastranger.Theparticipantswereassignedtoatotalof12dyadsinwhichthepar-ticipantsalternatedtutoringoneanotherinlinearalgebraequationsolvingfor5weeklyhour-longsessions,foratotalcorpusofnearly60hoursofface-to-faceinteractions.Eachsessionwasstruc-turedsuchthatthestudentsengagedinbriefsocialchitchatinthebeginning,thenoneofthestudentswasrandomlyassignedtotutortheotherfor20minutes.Theythenengagedinanothersocialpe-riod,andconcludedwithasecondtutoringperiodwheretheotherstudentwasassignedtheroleoftutor.Audioandvideodatawererecorded,tran-scribed,andsegmentedforclause-leveldialogueannotation,providingnearly24000clauses.Non-speechsegments(notablyfillersandlaughter)weremaintained.Becauseoftemporalmisalignmentforpartsofthecorpus,manyparaverbalphenomena,suchasprosody,wereunfortunatelynotavailabletous.SinceouraccesstothedatasetiscoveredbyaNon-DisclosureAgreement,itcannotbereleased\n2162\n\n\n2https://github.com/AnonymousHedges/HedgeDetectionclasses.4.2FeaturesLabelfromrule-basedclassifier(LabelRB):Weusetheclasslabelpredictedbytherule-basedclas-sifierdescribedinSection4.3asafeature.Ourhypothesisisthatthemachinelearningmodelcanusethisinformationtocounterbalancetheclassimbalance.Totakeintoaccountthefactthatsomerulesaremoreefficientthanothers,weweightedtheclasslabelresultingfromtherule-basedmodelbytheprecisionoftherulethatgeneratedit.Unigramandbigram:Wecountthenumberofoccurrencesofunigramsandbigramsofthecorpusineachclause.Weusedthelemmaofthewordsforunigramsandbigramsusingthenltklemmatizer(Loper,2002)andselectedunigramsandbigramsthatoccurredinthetrainingdatasetatleastfiftytimes.Thegoalwastoinvestigate,withabottom-upapproach,towhatextenttheuseofcertainwordscharacterizeshedgeclassesintutoring.InSection5weexaminetheoverlapbetweenthesewordsandthoseaprioriidentifiedbytherules.Part-of-speech(POS):Hedgeclassesseemtobeassociatedwithdifferentsyntacticpatterns:forex-ample,subjectivizersmostoftencontainapersonalpronounfollowedbyaverb,asin\"Iguess\",\"Ibelieve\",\"Ithink\".WethereforeconsideredthenumberofoccurrencesofPOS-Tagn-grams(n=1,2,3)asfeatures.WeusedthespaCyPOS-taggerandconsideredPOSunigrams,bigramsandtri-gramsthatoccuratleast10timesinthetrainingdataset.LIWC:LinguisticInquiryandWordCount(LIWC)(Pennebakeretal.,2015)isstandardsoft-wareforextractingthecountofwordsbelongingtospecificpsycho-socialcategories(e.g.,emotions,religion).Ithasbeensuccessfullyusedinthede-tectionofconversationalstrategies(Zhaoetal.,2016a).Wethereforecountthenumberofoccur-rencesofallthe73categoriesfromLIWC.Tutoringmoves(TM):Intelligenttutoringsys-temsrelyonspecifictutoringmovestosuccess-fullyconveycontent(asdohumantutors).Wethereforelookedatthelinkbetweenthetutoringmoves,asannotatedinMadaioetal.(2017),andhedges.Fortutors,thesemovesare(1)instruc-tionaldirectivesandsuggestions,(2)feedback,and(3)affirmations,mostlyexplicitreflectionsontheirpartners\u2019comprehension,whilefortutees,theyare(1)questions,(2)feedbacks,and(3)affirmations,\npublicly.Howevertheoriginalexperimenters\u2019In-stitutionalReviewBoard(IRB)approvalallowsustoview,annotate,andusethedatatotrainmodels.Thisalsoallowsustoprovidealinktoapixe-latedvideoexampleintheGitHubrepositoryoftheproject2.Dataannotation:Thedatasetwaspreviouslyan-notatedbyMadaioetal.(2017),followingananno-tationmanualthatusedhedgeclassesderivedfromRowland(2007)(seeTable1).Onlythetaskperi-odsoftheinteractionswereannotated.Comparingtheannotationswiththeclassesmentionedintherelatedworksection,SubjectivizerscorrespondtoRelationalhedges(Fraser,2010),PropositionalhedgesandExtenderscorrespondtoApproxima-tors(Princeetal.,1982)withtheadditionofsomediscoursemarkerssuchasjust.ApologizersarementionedaslinguistictoolsrelatedtonegativepolitenessinBrownandLevinson(1987).Krippen-dorff\u2019salphaobtainedforthiscorpusannotatedbyfourcoderswasover0.7forallclasses(denotinganacceptableinter-coderreliabilityaccordingtoKrippendorff(2004)).Thedatasetiswidelyim-balanced,withmorethan90%oftheutterancesbelongingtotheNothedgedclass.Inreviewingthecorpusandtheannotationman-ual,however,wenoticedtwoissues.First,theannotationoftheExtendersclasswasinconsis-tent,leadingtotheExtendersandPropositionalhedgesclassescarryingsimilarsemanticfunctions.WethereforemergedthetwoclassesandgroupedutteranceslabeledasExtendersandthoselabeledasPropositionalhedgesundertheheadingofPropositionalhedges.Second,theannotationofclausescontainingthetokens\"just\"and\"would\"(twotermsoccurringfrequentlyinthedatasetthatarekeycomponentsofPropositionalHedgesandSubjectivizersbutthatarenotinfacthedgesinallcases)wasalsoinconsistent,leadingtovirtuallyallclauseswiththosetwotokensbeingconsideredhedges.Wethereforere-consideredalltheclausesassociatedwithanyofthehedgeclasses,aswellasalltheclausesinthe\"Nothedged\"classthatcontained\"just\"or\"would\".There-annotationwascarriedoutbytwoannotatorswhoachievedaKrippendorff\u2019salphainter-raterreliabilityof.9orbetterforApologizers,Subjectivizers,andPropo-sitionalhedgesbeforeindependentlyre-annotatingtherelevantclauses.Anexampleofare-annotationwasremoving\"Iwouldkillyou!\"fromthehedge\n2163\n\n\nProp.hedgesApologizersSubjectivizersNothedgedTotal\nTable1:Definitionoftheclasses\nFeaturesnameAutomaticextractionVectorsize\nSubjectivizersWordsthatreduceintensityorcertainty\u201cSothenIwoulddividebytwo.\u201dApologizersApologiesusedtosoftendirectspeechacts\u201cOhsorrysixb.\u201dPropositionalhedgesQualifyingwordstoreduceintensityorcertaintyofutterances\u201cIt\u2019sactuallyeight.\u201dExtendersWordsusedtoindicateuncertaintybyreferringtovaguecategories\u201cIt\u2019llbethenumberxorwhatevervariableyouhave.\u201d\n12101286262119223156\nTable2:Distributionoftheclasses\nClassDefinitionExample\nRule-basedlabelYes4UnigramYes~250BigramYes~250POSYes~1200LIWCYes73NonverbalNo24TutoringmovesNo6Total~1800\nTable3:Listofautomaticallyextractedandmanuallyannotatedfeatureswiththeirsize.mostlytentativeanswers.Nonverbalandparaverbalbehaviors:AsinGoeletal.(2019),weincludedthenonverbalandpar-averbalbehaviorsthatarerelatedtohedges.Specif-ically,weconsiderlaughterandsmiles,thathavebeenshowntobeeffectivemethodsofmitiga-tion(Warner-Garcia,2014),cut-offsindicatingself-repairs,fillerslike\"Um\",gazeshifts(annotatedas\u2019GazeatPartner\u2019,\u2019GazeattheMathWorksheet\u2019,and\u2019Gazeelsewhere\u2019),andheadnods.Eachfea-turewaspresenttwiceinthefeaturevector,onetimeforeachinterlocutor.Inter-raterreliabilityfornonverbalbehaviorwas0.89(asmeasuredbyKrippendorff\u2019salpha)foreyegaze,0.75forsmilecount,0.64forsmiledurationand0.99forheadnod.Laughterisalsoreportedinthetranscriptatthewordlevel.Weseparatethetutor\u2019sbehaviorsfromthoseofthetutee.ThecollectionprocessforthesebehaviorsisdetailedfurtherinZhaoetal.(2016b).Theclause-levelfeaturevectorwasnormalizedbythelengthoftheclause(exceptfortherule-basedlabel).Thislengthwasalsoaddedasafeature.Table3presentsanoverviewofthefinalfeaturevector.4.3ClassificationmodelsTheclassificationmodelsusedarepresentedhereaccordingtotheirlevelofintegrationofexternallinguisticknowledge.Rule-basedmodel:OnthebasisoftheannotationmanualusedtoconstructthedatasetfromMadaioetal.(2017),andwithdescriptionsofhedgesfromRowland(2007),Fraser(2010)andBrownandLevinson(1987),weconstructedarule-basedclas-sifierthatmatchesregularexpressionsindicativeofhedges.TherulesaredetailedinTable7intheAppendix.LGBM:Sincehedgesareoftencharacterizedbyexplicitlexicalmarkers,wetestedtheassumptionthatamachinelearningmodelwithaknowledge-drivenrepresentationforclausescouldcompetewithaBERTmodelinperformance,whilebeingmuchmoreinterpretable.WereliedonLightGBM,anensembleofdecisiontreestrainedwithgradi-entboosting(Keetal.,2017).Thismodelwasselectedbecauseofitsperformancewithsmalltrainingdatasetsandbecauseitcanignoreunin-formativefeatures,butalsoforitstrainingspeedcomparedtoalternativeimplementationsofgradi-entboostingmethods.Multi-layerperceptron(MLP):Asasimplebase-line,webuiltamulti-layerperceptronusingthreesetsoffeatures:apre-trainedcontextualrepre-sentationoftheclause(SentBERT;ReimersandGurevych(2019));theconcatenationofthiscon-textualrepresentationoftheclauseandarule-basedlabel(notrelyingonthepreviousclauses);andfinallytheconcatenationofallthefeaturesmen-tionedinsection4.2,withoutthecontextualizedrepresentation.LSTMoverasequenceofclauses:Sinceweareworkingwithconversationaldata,wealsowantedtotestwhethertakingintoac-countthepreviousclauseshelpstodetectthetypeofhedgeclassinthenextclause.Formally,wewanttoinferyiusingyi=maxy\u2208ClassesP(y|X(ui),X(ui\u22121),...,X(ui\u2212K)),whereKisthenumberofpreviousclausesthatthemodelwilltakeintoaccount.The\n2164\n\n\nMLPmodelpresentedaboveinfersyiusingyi=maxy\u2208ClassesP(y|X(ui)),thereforeadifferenceofperformancebetweenthetwomodelswouldbeasignthatusinginformationfromthepreviousclausescouldhelptodetectthehedgedformulationinthecurrentclause.WetestedaLSTMmodelwiththesamerepresentationsforclausesasfortheMLPmodel.CNNwithattention:Goeletal.(2019)estab-lishedtheirbestperformanceonhedgedetec-tionusingaCNNmodelwithadditiveattentionoverword(andnotclause)embeddings.Con-trarytotheMLPandLSTMmodelsmentionedabove,thismodeltriestoinferyiusingyi=maxy\u2208ClassesP(y|g(w0),g(w1),...,g(wL)),withLrepresentingthemaximumclauselengthweal-low,andgrepresentingafunctionthatturnsthewordwj,j\u2208[0,L]intoavectorrepresentation(formoredetails,pleaseseeAdelandSch\u00fctze(2017)).BERT:Tobenefitfromdeepsemanticandcon-textualrepresentationsoftheutterances,wealsofine-tunedBERT(Devlinetal.,2019)onourclas-sificationtask.BERTisapre-trainedTransformersencoder(Vaswanietal.,2017)thathassignificantlyimprovedthestateoftheartonanumberofNLPtasks,includingsentimentanalysis.Itproducesacontextualrepresentationofeachwordinasen-tence,makingitcapableofdisambiguatingthemeaningofwordslike\"think\"or\"just\"thatarerepresentativeofcertainclassesofhedges.BERT,however,isnotablyhardtointerpret.4.4AnalysistoolsLookingatwhichfeaturesimprovetheperfor-manceofourclassificationmodelstellsuswhetherthesefeaturesareinformativeornot,butdoesnotexplainhowthesefeaturesareusedbythemod-elstomakeagivenprediction.Wethereforepro-ducedacomplementaryanalysisusinganinter-pretabilitytool.Asdemonstratedby(LundbergandLee,2017),LightGBMinternalfeatureimpor-tancescoresareinconsistentwithboththemodelbehaviorandhumanintuition,soweinsteadusedamodel-agnostictool.SHAP(LundbergandLee,2017)assignstoeachfeatureanimportancevalue(calledShapleyvalues)foraparticularpredictiondependingontheextentofitscontribution(ade-tailedintroductiontoShapleyvaluesandSHAPcanbefoundinMolnar(2020)).SHAPisamodel-agnosticframework,thereforethevaluesassoci-atedwithasetoffeaturescanbecomparedacrossmodels.ItshouldbenotedthatSHAPproducesexplanationsonacase-by-casebasis,thereforeitcanbothprovidelocalandglobalexplanations.FortheGradientBoostingmodel,weuseanadaptedversionofSHAP(Lundbergetal.,2018),calledTreeSHAP.5Experimentsandresults5.1ExperimentalsettingTodetectthebestsetoffeatures,weusedLight-GBMandproceededincrementally,byaddingthegroupoffeatureswethoughttobemostlikelyasso-ciatedwithhedges.Wedidnotconsidertheriskofrelyingonasub-optimalsetoffeaturesthroughthisprocedurebecauseofthestrongabilityofLight-GBMtoignoreuninformativefeatures.Weusethisincrementalapproachasawaytotestourintuitionabouttheperformativityofgroupsoffeatures(i.e.doesaddingafeatureimprovetheperformanceofthemodel)withregardtothetaskofclassifica-tion.Tocompareourmodels,wetrainedthemonthe4-classtask,andlookedattheaverageoftheweightedF1-scoresforthethreehedgeclasses(i.e.howwellthemodelsinferminorityclasses)thatwereporthereas\"3-classes\",andattheaverageoftheweightedF1-scoresforthe4classes,thatwereportas\"4-classes\".DetailsofthehyperparametersandexperimentalsettingsareprovidedinAppendixA.5.2ModelcomparisonandfeatureanalysisOverallresults:Table4presentstheresultsob-tainedbythe6modelspresentedinSection4.3forthemulti-classproblem.Bestperformance(F1-scoreof79.0)isobtainedwithLightGBMlever-agingalmostallthefeatures.Intheappendix(seeTable8andTable9)weindicatetheconfidenceintervalstorepresentthesignificanceofthediffer-encesbetweenthemodels.First,andperhapssurprisingly,wenoticethattheuseof\"Knowledge-Driven\"featuresbasedonrulesbuiltfromlinguisticknowledgeofhedgesintheLightGBMmodeloutperformstheuseofpre-trainedembeddingswithinafine-tunedBERTmodel(79.0vs.70.6),andintheneuralbaselinefrom(Goeletal.,2019)(79.0vs64.5).ThelowscoresobtainedbytheLGBM,LSTMandMLPmodelswithpre-trainedsentenceem-beddingsversusKnowledge-Drivenfeaturesmightsignalthatthewordpatternscharacterizinghedgesarenotsalientintheserepresentations(i.e.the\n2165\n\n\n68.5(1.6)35.8(3.1)64.8(1.1)Attention-CNN(3-classes)\n65.1(5.7)39.8(8.0)65.2(5.1)BERT(3-classes)\n\u220570.6(2.3)\u2205LGBM(3-classes)\n79.0(1.3)35.0(2.2)70.1(1.4)\nRule-based(3-classes)\n94.7\u2205\u2205MLP(4-classes)\n3Notethatthereisstrongredundancybetweensomefea-turesofLIWCandthespaCyPOStaggerthatbothproducea\"Pronoun\"category,usingalexiconinthefirstcase,andaneuralinferenceinthesecond.\n\u220594.9(0.4)\u2205LGBM(4-classes)\nRule-based(4-classes)\nTable4:AveragedweightedF1-scores(andstandarddeviation)forthethreeminorityclassesandforthe4classes,forallmodels.\"KD\"standsfor\"Knowledge-Driven\",meaningthatthefeaturesarederivedfromlexicon,n-grammodelsandannotations.distancebetween\"Ithinkyoushouldadd5.\"and\"Youshouldadd5.\"isshort.).KDFeaturesseemtoprovideabetterseparabilityoftheclasses.ThecombinationofKDfeaturesandPre-trainedem-beddingsdoesnotsignificantlyimprovetheperfor-manceofthemodelscomparedtotheKDFeaturesonly,whichsuggeststhattheinformationfromthePre-trainedembeddingsisredundantwiththeonefromtheKDFeatures.Thisresultmaybeduetothehighdimensionalityoftheinputvector(868withPCAontheKDFeatures;2500otherwise).Asecondfindingisthattheuseofgradientboost-ingmodelsontopofrule-basedclassifiersbettermodelsthehedgeclasses.Theothermachinelearn-ingmodelsdidnotprovetobeaseffective,exceptforBERT.FeatureanalysisusingLightGBM:Usingthebestperformingmodel,Table5showstheroleofeachfeaturesetinthepredictiontask.ThesignificanceofthedifferencesisshowninTable10andTable11.Comparedtotherule-basedmodel,theintroductionofn-gramssignificantlyimprovedtheperformanceofourclassifier,suggestingthatsomelexicalandsyntacticinformationdescribingthehedgeclasseswasnotpresentintherule-basedmodel.LookingatTable5,wedonotobservesignificantdifferencesbetweentheLGBMmodelusingonlythelabelrulebased+(1-gramsand2-grams)andthemodelsin-corporatingmorefeatures.Tooursurprise,neitherthetutoringmovesnorthenonverbalfeaturessig-nificantlyimprovedtheperformanceofthemodel.The2featureswereincludedtoindexthespecificpeertutoringcontextofthesehedges,sothisindi-catesthatinfutureworkwemightwishtoapplythecurrentmodeltoanothercontextofusetoseeifthismodelofhedgesismoregenerallyapplicablethanweoriginallythought.Bycombiningthisresultwiththeincreasedperformanceofthemodelus-ingKnowledge-Driven(i.e.explicit)featurescom-paredtopre-trainedembeddings,itwouldseemthathedgesareaboveallalexicalphenomenon(i.e.producedbyspecificlexicalelements).5.3In-depthanalysisoftheinformativefeaturesWetrainedtheSHAPexplanationmodelsonLight-GBMwithallfeatures.Themostinformativefea-tures(inabsolutevalue)foreachclassareshowninTable6,andtheplotsbyclassarepresentedintheAppendix.Themostimportantfeaturesseemtobetherule-basedlabels,whichappearinatleastthefourthpositionforthreeclasses(seeTable6),andinthefirstpositionforPropositionalHedgesandNothedgedclasses.Surprisingly,theRule-Basedlabeldoesnotappearinthetop20featuresforApologizers.However,giventhattheclassrarelyappearsinthedata,therulesseldomactivate,sothefeaturemaysimplybeinformativeforaverysmallnumberofclauses.Unigrams(Oh,Sorry,just,Would,andI)arealsopresentinthe5top-rankedfeatures.Thisconfirmsthefindingsmen-tionedinrelatedworkforthecharacterizationofthedifferenthedgeclasses(justwithPropositionalHedges,sorrywithApologizer,IwithSubjectiviz-ers).ThepresenceofOhalsohashighimportanceforthecharacterizationofApologizer(n=2),asillustratedinexamplessuchas\"Ohsorry,that\u2019snine.\".Wenotethattheoccurrencesof\"Ohsorry\"asastand-aloneclausewereexcludedbyourrule-basedmodelbecausetheydonotcorrespondtoanapologizer(theycannotmitigatethecontentofapropositionifthereisnopropositionassociated).Thisexampleillustratestheinterestofamachinelearningmodelapproachtodisambiguatethefunc-tionofconventionalnon-propositionalphraseslike\"Ohsorry\".Inaddition,SHAPhighlightstheimportanceofnovelfeatureswhosefunctionwasnotidentifiedinthehedgesliterature:(i)whatLIWCclassifiesasinformalwordsbutthataremostlyinterjectionslikeahandoharestronglyassociatedwithApol-ogizer,asaredisfluencies(n=12);(ii)theuseofPOStagsseemstobeveryrelevantforcharac-terizingthedifferentclasses(2-gramofPOStagfeatures3occurinthetop-rankedfeaturesofallthe\n\u220564.5(3.0)\u2205LSTM(3-classes)\nKDFeat.(KDF)Pre-TrainedEmb.(PTE)KDF+PTE\n\u220594.4(0.2)\u2205LSTM(4-classes)\n96.7(0.2)91.0(0.2)95.4(0.2)\n94.8(0.3)89.7(0.4)93.9(0.4)Attention-CNN(4-classes)\n93.9(1.4)89.1(1.4)94.1(1.2)BERT(4-classes)\n67.6\u2205\u2205MLP(3-classes)\nModels\n2166\n\n\n95.0(0.2)96.5(0.3)96.5(0.2)96.7(0.2)96.6(0.4)96.7(0.3)\n3-classes\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\n68.8(0.8)78.2(1.6)78.1(1.3)79.0(1.3)78.5(2.4)78.7(1.8)\nTable5:AveragedweightedF1-scoresforthethreeclassesofhedgesandthefourclasses,withanadditiveintegrationofKDFfeaturesintheLightGBMmodel.Thestandarddeviationiscomputedacrossfivefolds.\n4-classes\nModels\nRankApologizerSubjectivizersProp.HedgesNothedged\n1Functionwords(LIWC)\"I\"ClasslabelClasslabel2\"Oh\"(LIWC)\"Yeah\"\"Would\"\"Would\"3\"Sorry\"Noun(POS)\"Just\"\"Yeah\"4Affect(LIWC)ClasslabelFunctionword(LIWC)Noun(POS)5ClauselengthCognitiveprocess(LIWC)Netspeak(LIWC)Cognitiveprocess(LIWC)Table6:Mostimportantclause-levelfeaturesforLightGBMaccordingtotheSHAPanalysis.classes(seeFiguresintheAppendix).Itmeansthattherearesomerecurringsyntacticpatternsineachclass;(iii)Regardingtheutterancesize,aclauseshorterthanthemeanisweaklyassociatedwithdirectness(n=17)whilealongerclausesuggeststhatitcontainsaSubjectivizer(n=6).Apologizersarecharacterizedbyameanclauselength(n=5),withfewvariationsfromit;(iv)Tutoringmovesarenotstrongpredictorsofanyclasses:\"Affirma-tionfromtutor\"istheonlyfeatureappearingasapredictorofPropositionalhedges(n=20).ThisisconsistentwiththefeatureanalysisinTable5,suggestingthattutoringmovesdonotsignificantlyimprovetheperformanceoftheclassifier;(v)Non-verbalbehaviorsdonotappearasimportantfea-turesfortheclassification.Thisiscoherentwithresultsfrom(Goeletal.,2019).Notethatprosodymightplayaroleindetectinginstructionsthattrailoff,but,asdescribed,paraverbalfeatureswerenotavailable;(vi)Wouldplaysanimportantroleintheproductionofhedges,asitisstronglyassociatedtoPropositionalhedges(n=2).Itisinterestingtonotethat,whendesigningtherule-basedclassifier,wesawitdecreaseinperformancewhenwestartedtoincludewouldinourregularexpressionpatterns,probablybecausetheformishardtodisambiguateforadeterministicsystem.WhileexploringtheShapleyvaluesassociatedtoeachclause,weobservedthatfeaturesliketutoringmovesareextremelyinformativeforaverysmallnumberofclauses(thereforenotsignificantlyinflu-encingtheoverallperformanceoftheprediction),andmoreorlessnotinformativefortherest.Infer-ringtheglobalimportanceofafeatureasameanacrosstheshapleyvaluesinthedatasetmaynotbetheonlywaytoexplorethebehaviorofgradi-entboostingmethods.ItmightbemoreusefultoclusterclausesbasedontheimportancethatSHAPgivestothatfeatureinitsclassification,asthiscouldhelpdiscoversub-classesofhedgesthataredifferentiatedfromtherestbytheirinteractionwithaspecificfeature(inthewaythatsomeApologiz-ersarecharacterizedbyan\"oh\").Wealsonotethattheexplanationmodelissensitivetospuri-ouscorrelationsinthedataset,causedbythesmallrepresentationofsomeclass:forexample,\"nine\"(n=7)and\"four\"(n=20)arepositivepredictorsofApologizers.6ConclusionandfutureworkThroughourclassificationperformanceexperi-ments,weshowedthatitispossibletousema-chinelearningmethodstodiminishtheambigu-ityofhedges,andthatthehybridapproachofus-ingrule-basedlabelfeaturesderivedfromsocialscience(includinglinguistics)literaturewithinamachinelearningmodelhelpedsignificantlytoin-creasethemodel\u2019sperformance.Nonverbalbehav-iorsandtutoringmovesdidnotprovideinformationatthesentencelevel;boththeperformanceofthemodelandthefeaturecontributionanalysissug-gestedthattheirimpactonthemodeloutputwasnotstrong.ThisisconsistentwithresultsfromGoeletal.(2019).However,infutureworkwewouldliketoinvestigatethepotentialofmultimodalpat-ternswhenweareabletobettermodelsequentiality(e.g.,negativefeedbackfollowedbyasmile).Re-gardingtheSHAPanalysis,mostofthefeaturesthatareconsideredasimportantarecoherentwiththedefinitionoftheclasses(Iforsubjectivizers,sorryforapologizers,justforpropositionalhedges).However,wediscoveredthatfeatureslikeutterance\n2167\n\n\nsizecanalsoserveasindicatorsofcertainclassesofhedges.AlimitationofSHAPisthatitmakesafeatureindependenceassumption,whichpromptstheexplanatorymodeltounderestimatetheimpor-tanceofredundantfeatures(likepronounsinourwork).Inthefuturewewillexploreexplanatorymodelscapableoftakingintoaccountthecorre-lationbetweenfeaturesinthedatasetlikeSAGE(Covertetal.,2020),butsuitedforveryimbal-anceddatasets.Inthedomainofpeer-tutoring,wewouldliketobeabletofurthertestthelinkbe-tweenhedgesandrapport,andthelinkbetweenhedgesandlearninggainsinthesubjectbeingtu-tored.Asnotedabove,thiskindofstudyrequiresafine-grainedcontrolofthelanguageproducedbyoneoftheinterlocutors,whichisdifficulttoachieveinahuman-humanexperience.Wenotethatthehedgeclassifiercanbeusednotjusttoclassify,butalsotoworktowardsimprovingthegenerationofhedgesfortutoragents.Infutureworkwewillexploreusingtheclassifiertore-rankgenerationoutputs,takingadvantageoftherecur-ringsyntacticpatterns(see(ii)inSection5.3)toimprovethegenerationprocessofhedges,andre-generatingclausesthatdon\u2019tcontainoneofthesesyntacticpatterns.AcknowledgmentsManythankstomembersoftheArticuLaboatIN-RIAParisfortheirpreciousassistance.ThisworkwassupportedinpartbythetheFrenchgovern-mentundermanagementofAgenceNationaledelaRechercheaspartofthe\u201cInvestissementsd\u2019avenir\u201dprogram,referenceANR-19-P3IA-0001(PRAIRIE3IAInstitute).ReferencesHeikeAdelandHinrichSch\u00fctze.2017.Exploringdif-ferentdimensionsofattentionforuncertaintydetec-tion.InProceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputa-tionalLinguistics:Volume1,LongPapers,pages22\u201334,Valencia,Spain.AssociationforComputa-tionalLinguistics.ShoshanaBlum-Kulka.1987.Indirectnessandpolite-nessinrequests:Sameordifferent?Journalofpragmatics,11(2):131\u2013146.PenelopeBrownandStephenCLevinson.1987.Polite-ness:Someuniversalsinlanguageusage,volume4.Cambridgeuniversitypress.JudeeKBurgoonandRandallJKoper.1984.Nonverbalandrelationalcommunicationassociatedwithreti-cence.HumanCommunicationResearch,10(4):601\u2013626.IanCovert,ScottMLundberg,andSu-InLee.2020.Understandingglobalfeaturecontributionswithad-ditiveimportancemeasures.AdvancesinNeuralInformationProcessingSystems,33:17212\u201317223.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforCom-putationalLinguistics:HumanLanguageTechnolo-gies,Volume1(LongandShortPapers),pages4171\u20134186.Rich\u00e1rdFarkas,VeronikaVincze,Gy\u00f6rgyM\u00f3ra,J\u00e1nosCsirik,andGy\u00f6rgySzarvas.2010.Theconll-2010sharedtask:learningtodetecthedgesandtheirscopeinnaturallanguagetext.InProceedingsofthefour-teenthconferenceoncomputationalnaturallanguagelearning\u2013Sharedtask,pages1\u201312.BruceFraser.2010.Pragmaticcompetence:Thecaseofhedging.Newapproachestohedging,1534.ViolaGanterandMichaelStrube.2009.Findinghedgesbychasingweasels:Hedgedetectionusingwikipediatagsandshallowlinguisticfeatures.InProceedingsoftheACL-IJCNLP2009ConferenceShortPapers,pages173\u2013176.PranavGoel,YoichiMatsuyama,MichaelMadaio,andJustineCassell.2019.\u201cithinkitmighthelpifwemultiply,andnotadd\u201d:Detectingindirectnessincon-versation.In9thInternationalWorkshoponSpokenDialogueSystemTechnology,pages27\u201340.Springer.CynthiaHoward,PamelaW.Jordan,BarbaraMariaDiEugenio,andSandraKatz.2015.Shiftingtheload:apeerdialogueagentthatencouragesitshumancollab-oratortocontributemoretoproblemsolving.Interna-tionalJournalofArtificialIntelligenceinEducation,27:101\u2013129.KenHyland.1998.Hedginginscientificresearcharti-cles,volume54.JohnBenjaminsPublishing.GuolinKe,QiMeng,ThomasFinley,TaifengWang,WeiChen,WeidongMa,QiweiYe,andTie-YanLiu.2017.Lightgbm:Ahighlyefficientgradientboost-ingdecisiontree.Advancesinneuralinformationprocessingsystems,30:3146\u20133154.KlausKrippendorff.2004.Reliabilityincontentanaly-sis:Somecommonmisconceptionsandrecommen-dations.Humancommunicationresearch,30(3):411\u2013433.GeorgeLakoff.1975.Hedges:Astudyinmeaningcriteriaandthelogicoffuzzyconcepts.InContem-poraryresearchinphilosophicallogicandlinguisticsemantics,pages221\u2013271.Springer.\n2168\n\n\nMatthewLeach.2005.Rapport:Akeytotreatmentsuc-cess.Complementarytherapiesinclinicalpractice,11:262\u20135.IlyaLoshchilovandFrankHutter.2018.Decoupledweightdecayregularization.InInternationalConfer-enceonLearningRepresentations.ScottMLundberg,GabrielGErion,andSu-InLee.2018.Consistentindividualizedfeatureat-tributionfortreeensembles.arXivpreprintarXiv:1802.03888.ScottMLundbergandSu-InLee.2017.Aunifiedap-proachtointerpretingmodelpredictions.InProceed-ingsofthe31stinternationalconferenceonneuralinformationprocessingsystems,pages4768\u20134777.HowardLuneandBruceLBerg.2017.Qualitativeresearchmethodsforthesocialsciences.Pearson.MichaelMadaio,JustineCassell,andAmyOgan.2017.Theimpactofpeertutors\u2019useofindirectfeedbackandinstructions.Philadelphia,PA:InternationalSo-cietyoftheLearningSciences.BenMedlockandTedBriscoe.2007.Weaklysuper-visedlearningforhedgeclassificationinscientificliterature.InProceedingsofthe45thannualmeetingoftheassociationofcomputationallinguistics,pages992\u2013999.ChristophMolnar.2020.Interpretablemachinelearn-ing.Lulu.com.AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advancesinneuralinformationprocessingsystems,32:8026\u20138037.JamesWPennebaker,RyanLBoyd,KaylaJordan,andKateBlackburn.2015.Thedevelopmentandpsycho-metricpropertiesofliwc2015.Technicalreport.JeffreyPennington,RichardSocher,andChristopherDManning.2014.Glove:Globalvectorsforwordrep-resentation.InProceedingsofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing(EMNLP),pages1532\u20131543.EllenFPrince,JoelFrader,CharlesBosk,etal.1982.Onhedginginphysician-physiciandiscourse.Lin-guisticsandtheProfessions,8(1):83\u201397.AnnaProkofievaandJuliaHirschberg.2014.Hedgingandspeakercommitment.In5thIntl.WorkshoponEmotion,SocialSignals,Sentiment&LinkedOpenData,Reykjavik,Iceland.NilsReimersandIrynaGurevych.2019.Sentence-bert:Sentenceembeddingsusingsiamesebert-networks.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages3982\u20133992.OscarJRomero,RanZhao,andJustineCassell.2017.Cognitive-inspiredconversational-strategyreasonerforsocially-awareagents.InIJCAI,pages3807\u20133813.TimRowland.2007.\u2018wellmaybenotexactly,butit\u2019saroundfiftybasically?\u2019:Vaguelanguageinmath-ematicsclassrooms.InVaguelanguageexplored,pages79\u201396.Springer.SaschaSchneider,SteveNebel,SimonPradel,andG\u00fcn-terDanielRey.2015.Mindyourpsandqs!howpoliteinstructionsaffectlearningwithmultimedia.ComputersinHumanBehavior,51:546\u2013555.TanmaySinhaandJustineCassell.2015.Weclick,wealign,welearn:Impactofinfluenceandconvergenceprocessesonstudentlearningandrapportbuilding.InProceedingsofthe1stWorkshoponModelingINTERPERsonalSynchrONyAndInfLuence,INTER-PERSONAL\u201915,page13\u201320,NewYork,NY,USA.AssociationforComputingMachinery.HelenSpencer-Oatey.2005.(im)politeness,faceandperceptionsofrapport:Unpackagingtheirbasesandinterrelationships.1(1):95\u2013119.Gy\u00f6rgySzarvas,VeronikaVincze,Rich\u00e1rdFarkas,Gy\u00f6rgyM\u00f3ra,andIrynaGurevych.2012.Cross-genreandcross-domaindetectionofsemanticuncer-tainty.ComputationalLinguistics,38(2):335\u2013367.BuzhouTang,XiaolongWang,XuanWang,BoYuan,andShixiFan.2010.Acascademethodfordetectinghedgesandtheirscopeinnaturallanguagetext.InProceedingsoftheFourteenthConferenceonCom-putationalNaturalLanguageLearning\u2013SharedTask,pages13\u201317.MorganUlinski,SethBenjamin,andJuliaHirschberg.2018.Usinghedgedetectiontoimprovecommittedbelieftagging.InProceedingsoftheWorkshoponComputationalSemanticsbeyondEventsandRoles,pages1\u20135.TeppoVarttala.1999.Remarksonthecommunicativefunctionsofhedginginpopularscientificandspecial-istresearcharticlesonmedicine.Englishforspecificpurposes,18(2):177\u2013200.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,\u0141ukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.InAdvancesinneuralinformationpro-cessingsystems,pages5998\u20136008.ErikVelldal.2011.Predictingspeculation:asimpledis-ambiguationapproachtohedgedetectioninbiomed-icalliterature.JournalofBiomedicalSemantics,2(5):1\u201314.VeronikaVincze.2014.Uncertaintydetectioninnaturallanguagetexts.PhD,UniversityofSzeged,page141.\n2169\n\n\nNingWang,WLewisJohnson,RichardEMayer,PaolaRizzo,ErinShaw,andHeatherCollins.2008.Thepolitenesseffect:Pedagogicalagentsandlearningoutcomes.Internationaljournalofhuman-computerstudies,66(2):98\u2013112.ShawnWarner-Garcia.2014.Laughingwhennothing\u2019sfunny:Thepragmaticuseofcopinglaughterinthenegotiationofconversationaldisagreement.Prag-matics,24(1):157\u2013180.TimothyWilliamson.2002.Vagueness.Routledge.RanZhao,AlexandrosPapangelis,andJustineCassell.2014.Towardsadyadiccomputationalmodelofrap-portmanagementforhuman-virtualagentinteraction.InInternationalConferenceonIntelligentVirtualAgents,pages514\u2013527.Springer.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016a.Automaticrecognitionofconversa-tionalstrategiesintheserviceofasocially-awaredialogsystem.InProceedingsofthe17thAnnualMeetingoftheSpecialInterestGrouponDiscourseandDialogue,pages381\u2013392.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016b.Socially-awarevirtualagents:Au-tomaticallyassessingdyadicrapportfromtemporalpatternsofbehavior.InInternationalconferenceonintelligentvirtualagents,pages218\u2013233.Springer.AAdditionalinformationontheexperimentalsettingsWeusedPyTorch(Paszkeetal.,2019)toimple-menttheneuralmodels.Foreachsetoffeatures,hyperparameterswereselectedusingOptuna(Ak-iba,2019),aparametersearchframework.Were-implementedtheAttention-CNNwithGlove(Pen-ningtonetal.,2014)300-Dwordsembeddingsasthevectorrepresentation.Foreachmodels,theresultsarecross-validatedusing5folds(wechose5insteadof10toavoidhavingfoldswithtoofewsamplesperclass).Wecorrectedthelossfunctionforclassimbalancetoforcethemodeltoadaptmoretothelessfrequentclasses.Thestrengthofthiscorrectiondependedonthemodel,andwasselectedbecauseitprovidedasatisfyingcompro-misebetweenfavoringrecallandprecisionintheclassificationresultsofthatmodel.ForLightGBM,a\"squarerootofthesquarerootoftheinverseclassproportion\"correctionwasselected.Neu-ralmodelsweretrainedusingAdamWasanop-timizer(LoshchilovandHutter,2018),andusedareducedfeaturevector,obtainedwiththeap-plicationofPCA(dinit=1800;d=100;99.8%oftheinformationisconserved).Nosignifi-cantperformancedifferenceswereobservedbe-tweentheoriginalvectorandthereducedvectorfortrainingthemodels.TocomputetheSHAPvaluesmentionedinthepaper,wekeptonesplittoperformthe5-splitofthedataset,andleave1splittovalidateandearlystopthemodel,inor-dertoavoidoverfitting.Acompleteconfigura-tionofhyperparametersusedforeachmodelisre-portedintheGitHubrepositorywiththecodeofthepaper:https://github.com/YannRaphalen/Hedges-Detection.TheBERTmodelwasfine-tunedonaNvidiaQuadroRTX8000GPU.BTables\n2170\n\n\n?(whether|if|is|that|it|this)?.*Subj.\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nRBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)\nNoNoYesNoNoNoYesYesYesYesYesBERT(PTE)\nexceptionally|forthemostpart|inamannerofspeaking|\nNoYesNoNoNoYesNoNoYesYesNoMLP(KDF)\nsomethingbetween|essentially|only).*Prop.\nTable8:Significancetableforthe3-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesYesYesYesYesNoYesYesYesNoYesMLP(KDF+PTE)\ntechnically|typically|virtually|approximately|\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nmuch|somewhat|exactly|almost|littlebit|quite|\nClass\nrelatively|roughly|sotosay|strictlyspeaking|\nNoNoYesNoNoYesNoYesYesYesYesLSTM(KDF)\nYesNoNoNoNo+TM\nTable10:Significancetableforthe3-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoNoYesYesYesYesYesYesNoYesYes\n.*(it)(looks|seems|appears)[,]?.*\",\".*(or|and)(that|something|stuff|soforth)Table7:Regexprulesusedfortheclassifier.\nRule-based\nRule-based\n.*(you(might|may)(believe|think)).*Subj.\nYesYesYesYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nYesYesNoYesYesYesYesYesYesYesYesLSTM(KDF+PTE)\nNoNoYesNoNoYesNoNoYesYesYesLSTM(KDF)\nYesYesNoYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(i|i\u2019m|you|it\u2019s)(am|are)(apparently|surely)[,]?.*Prop.\nNoNoYesNoNoYesNoYesYesYesYesLSTM(PTE)\nNoNoYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nNoNoYesYesNoYesYesYesYesYesNoLGBM(KDF)\nNoNoYesYesYesYesYesYesYesYesNoLGBM(KDF)\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nYesNoYesYesYesYesYesYesNoYesYes\nYesYesYesYesYesYesYesYesYesYesYesMLP(KDF+PTE)\nregular|regularly|actually|almost|asitwere|basically|\nNoYesYesNoYesYesYesNoYesYesNoMLP(PTE)\nRule(regexp)\n.*(i|i\u2019m|we)?(was|am|wasn\u2019t)??(not)?(sure|certain).*Subj.\n.*(accordingto|presumably).*Subj.\n(?!.*(be|been|was)likeexcuseme)((excuseme|sorry)[w,\u2019]+|[w,\u2019]+(excuseme|sorry))Prop.\n.*(i|you|we)haveto(check|look|verify).*Subj.\n.*(i\u2019m|i|we\u2019re)(am|are)??(apologize|sorry).*Apol.\nonthetallside|parexcellence|particularly|\n(?!what).*(i|we)?(don\u2019t|didn\u2019t|did)??(not)?\ninarealsense|inasense|inaway|largely|literally|\nYesYesYesNoNoNoYesYesYesYesYesBERT(PTE)\nModels\nModels\nModels\nYesYesYesNoNoYesNoNoYesYesYesLSTM(PTE)\nYesNoNoNoNo\nYesYesYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)\nSubj.\nTable9:Significancetableforthe4-classespartofTable4.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nNoYesNoNoNoYesNoNoYesYesNoMLP(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(KDF+PTE)\n.*(ifeellikeyou).*Subj.\nlooselyspeaking|kinda|moreorless|mostly|often|\n.*(unlessi).*Apol.\nprobably|canbeviewas|crypto-|especially|essentially|\n.*(ifi\u2019mnotwrong|ifi\u2019mright|ifthat\u2019strue).*Subj.\nYesNoNoNoNo+POS\n(guess|guessed|thought|think|believe|believed|suppose|supposed)\n.*(just|alittle|maybe|actually|sortof|kindof|pretty\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nYesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)\nNoYesYesNoYesYesNoNoYesYesYesMLP(KDF)\nprettymuch|principally|pseudo-|quintessentially|\n2171\n\n\nFigure2:Absoluteaveragedfeaturecontribution,asindicatedbySHAP.Thelongerthebarisforonecolor,themorethefeatureisassociatedwiththeclassrepresentedbythatcolor.\nFigure3:Averagedcontributionoffeaturestothedetectionofthe\"Notindirect\"class,asindicatedbySHAP.Eachdotcorrespondstoaclassifiedclause.Areddotindicatesthatthefeatureispresentintheclause,whileabluedotindicatesthatthefeatureisabsent.Thefartherontherightthedotis,themorethefeaturecontributedtoitsclassificationasahedge.\n2172\n\n\nFigure4:Averagedcontributionoffeaturestothedetectionof\"Apologizers\",asindicatedbySHAP.\nFigure5:Averagedcontributionoffeaturestothedetectionof\"Propositionalhedges\",asindicatedbySHAP.\n2173\n\n\nYesYesYesYesYes+1-gramand2-gram\nLabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal\nLabelRB\nYesNoNoNoNo+TM\nYesNoNoNoNo+Nonverbal\nYesNoNoNoNo+LIWC\nFigure6:Averagedcontributionoffeaturestothedetectionof\"Subjectivizers\",asindicatedbySHAP.\nModels\nYesNoNoNoNo\nTable11:Significancetableforthe4-classespartofTable5.\"Yes\"meansthatthedifferenceisstatisticallysignificant.\nYesNoNoNoNo+POS\n2174"}