{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Squeeze,_Recover_and_Relabel:_Dataset_Condensation_at_ImageNet_Scale_From_A_New_Perspective_chunk_8.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the resolution of images in the Tiny-ImageNet dataset?", "answer": " 64x64", "ref_chunk": "in the main text, including: Section A: Implementation Details. Section B: Low-Resolution Data. Section C: Feature Embedding Distribution. Section D: Theoretical Analysis. Section E: More Visualization of Synthetic Data. A Implementation Details A.1 Dataset Statistics Table 6 enumerates various permutations of ImageNet-1K training set, delineated according to their individual configurations. Tiny-ImageNet [12] incorporates 200 classes derived from ImageNet-1K, with each class comprising 500 images possessing a resolution of 64\u00d764. Ima- geNette/ImageWoof [37] (alternatively referred to as subsets of ImageNet) include 10 classes from analogous subcategories, with each image having a resolution of 112\u00d7112. The MTT [1] framework introduces additional 10-class subsets of ImageNet, encompassing ImageFruit, ImageSquawk, Image- Meow, ImageBlub, and ImageYellow. ImageNet-10/100 [38] samples 10/100 classes from ImageNet while maintaining an image resolution of 224\u00d7224. Downsampled ImageNet-1K rescales the entirety of ImageNet data to a resolution of 64\u00d764. In our experiments, we choose two standard datasets of relatively large scale: Tiny-ImageNet and the full ImageNet-1K. Training Dataset #Class #Img per class Resolution Method Tiny-ImageNet [12] ImageNette/ImageWoof [37] ImageNet-10/100 [38] Downsampled ImageNet-1K [13] Full ImageNet-1K [24] 200 10 10/100 1,000 1,000 500 \u223c1,000 \u223c1,200 \u223c1,200 \u223c1,200 MTT [1], FRePo [5], DM [10], SRe2L (Ours) 64\u00d764 112\u00d7112 MTT [1], FRePo [5] 224\u00d7224 IDC [7] 64\u00d764 TESLA [11], DM [10] SRe2L (Ours) 224\u00d7224 Table 6: Variants of ImageNet-1K training set with different configurations. A.2 Squeezing Details Data Augmentation. Table 2 in the main paper illustrates that the utilization of data augmentation approaches during the squeezing phase contributes to a decrease in the final accuracy of the recovered data. To summarize, the results on Tiny-ImageNet indicate that lengthening the training budget and the application of more data augmentations in the squeezing phase intensify the intricacy involved in data recovery from the compressed model, which is not desired. Parallel conclusions are inferred from the compressed models for the ImageNet-1K dataset. For our experimental setup, we aimed to extract data from a pre-trained ResNet-50 model with available V1 and V2 weights in the PyTorch model zoo. The results propose that the task of data extraction poses a greater challenge from the ResNet-50 model equipped with V2 weights as compared to the model incorporating V1 weights. This can be attributed to the fact that models utilizing V1 weights are trained employing a rudimentary recipe, whereas models with V2 weights encompass numerous training enhancements, such as more training budget and data augmentations, to achieve cutting-edge performance. It is observed that these additional complexities impede the data recovery process even the pre-trained models are stronger. Therefore, the pre-trained models we employ for the recovery of ImageNet-1K are those integrating V1 weights from the PyTorch model zoo. Hyper-parameter Setting. We provide hyper-parameter settings for the two datasets in detail. Tiny-ImageNet: We train modified ResNet-{18, 50} models on Tiny-ImageNet data with the parameter setting in Table 7a. The well-trained ResNet-{18, 50} models achieve Top-1 accuracy of {59.47%, 61.17%} under the 50 epoch training budget. 14 config value config value optimizer base learning rate weight decay optimizer momentum 0.9 256 batch size learning rate schedule cosine decay 100 training epoch RandomResizedCrop augmentation SGD 0.2 1e-4 optimizer base learning rate weight decay optimizer momentum \u03b21, \u03b22 = 0.9, 0.999 batch size learning rate schedule training epoch augmentation AdamW 0.001 0.01 1,024 cosine decay 300 RandomResizedCrop (a) Tiny-ImageNet squeezing setting. (b) ImageNet-1K validation setting. config value config value \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 1.0 Adam 0.1 100 cosine decay 1,000 RandomResizedCrop \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 0.01 Adam 0.25 100 cosine decay 2,000 RandomResizedCrop (c) Tiny-ImageNet recovering setting. (d) ImageNet-1K recovering setting. Table 7: Hyper-parameter settings in three stages. ImageNet-1K: We use PyTorch off-the-shelf ResNet-{18, 50} with V1 weights and Top-1 accuracy of {69.76%, 76.13%} as squeezed/condensed models. In the original training script [39], ResNet models are trained for 90 epochs with a SGD optimizer, learning rate of 0.1, momentum of 0.9 and weight decay of 1 \u00d7 10\u22124. BN-ViT Model Structure. To give a clear illustration of incorporating BN layers into ViT, we provide more details in this subsection. The definitions of vanilla ViT and BN-ViT are presented below to show the structure modification. The vanilla ViT can be formulated as: z\u2032 \u2113 = MHSA (LN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFN (LN (z\u2032 \u2113)) + z\u2032 \u2113 where z\u2032 \u2113 is the intermediate representation before Feed-forward Network (FFN), and z\u2113 is that after FFN and residual connection. FFN contains two linear layers with a GELU non-linearity in between them, i.e., FFN(z\u2032 \u2113) = (cid:0)GELU (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1) W 2 \u2113 + b2 \u2113 The newly constructed BN-ViT is: z\u2032 \u2113 = MHSA (BN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFNBN (BN (z\u2032 \u2113)) + z\u2032 \u2113 where we add one additional BN layer in-between two linear layers of FFN, i.e., FFNBN(z\u2032 \u2113) = (cid:0)GELU (cid:0)BN (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1)(cid:1) W 2 \u2113 + b2 \u2113 We follow the DeiT official training recipe to train a DeiT-Tiny-BN model for 300 epochs with an AdamW optimizer, cosine decayed learning rate of 5 \u00d7 10\u22124, weight decay of 0.05 and 5 warmup epochs. 15 Ablation Top-1 acc. (%) RT V R\u21132 Random Crop Tiny-ImageNet \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 29.87 29.92 30.11 30.30 37.88 ImageNet-1K 22.92 23.15 40.81 40.37 46.71 Table 8: Top-1 validation accuracy under regularization ablation settings. ResNet-18 is used in all three stages with the relabeling temperature \u03c4 = 20. A.3 Recovering Details Regularization Terms. We conduct a large number of ablation experiments under varying regular- ization term conditions, as illustrated in Table 8. The two image prior regularizers, \u21132 regularization and total variation (TV), are not anticipated to enhance validation accuracy as our primary focus is on information recovery rather than image smoothness. Consequently, we exclude these two regularization terms from our experiments."}, {"question": " How many classes are included in the ImageNette/ImageWoof dataset?", "answer": " 10", "ref_chunk": "in the main text, including: Section A: Implementation Details. Section B: Low-Resolution Data. Section C: Feature Embedding Distribution. Section D: Theoretical Analysis. Section E: More Visualization of Synthetic Data. A Implementation Details A.1 Dataset Statistics Table 6 enumerates various permutations of ImageNet-1K training set, delineated according to their individual configurations. Tiny-ImageNet [12] incorporates 200 classes derived from ImageNet-1K, with each class comprising 500 images possessing a resolution of 64\u00d764. Ima- geNette/ImageWoof [37] (alternatively referred to as subsets of ImageNet) include 10 classes from analogous subcategories, with each image having a resolution of 112\u00d7112. The MTT [1] framework introduces additional 10-class subsets of ImageNet, encompassing ImageFruit, ImageSquawk, Image- Meow, ImageBlub, and ImageYellow. ImageNet-10/100 [38] samples 10/100 classes from ImageNet while maintaining an image resolution of 224\u00d7224. Downsampled ImageNet-1K rescales the entirety of ImageNet data to a resolution of 64\u00d764. In our experiments, we choose two standard datasets of relatively large scale: Tiny-ImageNet and the full ImageNet-1K. Training Dataset #Class #Img per class Resolution Method Tiny-ImageNet [12] ImageNette/ImageWoof [37] ImageNet-10/100 [38] Downsampled ImageNet-1K [13] Full ImageNet-1K [24] 200 10 10/100 1,000 1,000 500 \u223c1,000 \u223c1,200 \u223c1,200 \u223c1,200 MTT [1], FRePo [5], DM [10], SRe2L (Ours) 64\u00d764 112\u00d7112 MTT [1], FRePo [5] 224\u00d7224 IDC [7] 64\u00d764 TESLA [11], DM [10] SRe2L (Ours) 224\u00d7224 Table 6: Variants of ImageNet-1K training set with different configurations. A.2 Squeezing Details Data Augmentation. Table 2 in the main paper illustrates that the utilization of data augmentation approaches during the squeezing phase contributes to a decrease in the final accuracy of the recovered data. To summarize, the results on Tiny-ImageNet indicate that lengthening the training budget and the application of more data augmentations in the squeezing phase intensify the intricacy involved in data recovery from the compressed model, which is not desired. Parallel conclusions are inferred from the compressed models for the ImageNet-1K dataset. For our experimental setup, we aimed to extract data from a pre-trained ResNet-50 model with available V1 and V2 weights in the PyTorch model zoo. The results propose that the task of data extraction poses a greater challenge from the ResNet-50 model equipped with V2 weights as compared to the model incorporating V1 weights. This can be attributed to the fact that models utilizing V1 weights are trained employing a rudimentary recipe, whereas models with V2 weights encompass numerous training enhancements, such as more training budget and data augmentations, to achieve cutting-edge performance. It is observed that these additional complexities impede the data recovery process even the pre-trained models are stronger. Therefore, the pre-trained models we employ for the recovery of ImageNet-1K are those integrating V1 weights from the PyTorch model zoo. Hyper-parameter Setting. We provide hyper-parameter settings for the two datasets in detail. Tiny-ImageNet: We train modified ResNet-{18, 50} models on Tiny-ImageNet data with the parameter setting in Table 7a. The well-trained ResNet-{18, 50} models achieve Top-1 accuracy of {59.47%, 61.17%} under the 50 epoch training budget. 14 config value config value optimizer base learning rate weight decay optimizer momentum 0.9 256 batch size learning rate schedule cosine decay 100 training epoch RandomResizedCrop augmentation SGD 0.2 1e-4 optimizer base learning rate weight decay optimizer momentum \u03b21, \u03b22 = 0.9, 0.999 batch size learning rate schedule training epoch augmentation AdamW 0.001 0.01 1,024 cosine decay 300 RandomResizedCrop (a) Tiny-ImageNet squeezing setting. (b) ImageNet-1K validation setting. config value config value \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 1.0 Adam 0.1 100 cosine decay 1,000 RandomResizedCrop \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 0.01 Adam 0.25 100 cosine decay 2,000 RandomResizedCrop (c) Tiny-ImageNet recovering setting. (d) ImageNet-1K recovering setting. Table 7: Hyper-parameter settings in three stages. ImageNet-1K: We use PyTorch off-the-shelf ResNet-{18, 50} with V1 weights and Top-1 accuracy of {69.76%, 76.13%} as squeezed/condensed models. In the original training script [39], ResNet models are trained for 90 epochs with a SGD optimizer, learning rate of 0.1, momentum of 0.9 and weight decay of 1 \u00d7 10\u22124. BN-ViT Model Structure. To give a clear illustration of incorporating BN layers into ViT, we provide more details in this subsection. The definitions of vanilla ViT and BN-ViT are presented below to show the structure modification. The vanilla ViT can be formulated as: z\u2032 \u2113 = MHSA (LN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFN (LN (z\u2032 \u2113)) + z\u2032 \u2113 where z\u2032 \u2113 is the intermediate representation before Feed-forward Network (FFN), and z\u2113 is that after FFN and residual connection. FFN contains two linear layers with a GELU non-linearity in between them, i.e., FFN(z\u2032 \u2113) = (cid:0)GELU (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1) W 2 \u2113 + b2 \u2113 The newly constructed BN-ViT is: z\u2032 \u2113 = MHSA (BN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFNBN (BN (z\u2032 \u2113)) + z\u2032 \u2113 where we add one additional BN layer in-between two linear layers of FFN, i.e., FFNBN(z\u2032 \u2113) = (cid:0)GELU (cid:0)BN (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1)(cid:1) W 2 \u2113 + b2 \u2113 We follow the DeiT official training recipe to train a DeiT-Tiny-BN model for 300 epochs with an AdamW optimizer, cosine decayed learning rate of 5 \u00d7 10\u22124, weight decay of 0.05 and 5 warmup epochs. 15 Ablation Top-1 acc. (%) RT V R\u21132 Random Crop Tiny-ImageNet \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 29.87 29.92 30.11 30.30 37.88 ImageNet-1K 22.92 23.15 40.81 40.37 46.71 Table 8: Top-1 validation accuracy under regularization ablation settings. ResNet-18 is used in all three stages with the relabeling temperature \u03c4 = 20. A.3 Recovering Details Regularization Terms. We conduct a large number of ablation experiments under varying regular- ization term conditions, as illustrated in Table 8. The two image prior regularizers, \u21132 regularization and total variation (TV), are not anticipated to enhance validation accuracy as our primary focus is on information recovery rather than image smoothness. Consequently, we exclude these two regularization terms from our experiments."}, {"question": " What are some of the additional subsets of ImageNet introduced by the MTT framework?", "answer": " ImageFruit, ImageSquawk, ImageMeow, ImageBlub, ImageYellow", "ref_chunk": "in the main text, including: Section A: Implementation Details. Section B: Low-Resolution Data. Section C: Feature Embedding Distribution. Section D: Theoretical Analysis. Section E: More Visualization of Synthetic Data. A Implementation Details A.1 Dataset Statistics Table 6 enumerates various permutations of ImageNet-1K training set, delineated according to their individual configurations. Tiny-ImageNet [12] incorporates 200 classes derived from ImageNet-1K, with each class comprising 500 images possessing a resolution of 64\u00d764. Ima- geNette/ImageWoof [37] (alternatively referred to as subsets of ImageNet) include 10 classes from analogous subcategories, with each image having a resolution of 112\u00d7112. The MTT [1] framework introduces additional 10-class subsets of ImageNet, encompassing ImageFruit, ImageSquawk, Image- Meow, ImageBlub, and ImageYellow. ImageNet-10/100 [38] samples 10/100 classes from ImageNet while maintaining an image resolution of 224\u00d7224. Downsampled ImageNet-1K rescales the entirety of ImageNet data to a resolution of 64\u00d764. In our experiments, we choose two standard datasets of relatively large scale: Tiny-ImageNet and the full ImageNet-1K. Training Dataset #Class #Img per class Resolution Method Tiny-ImageNet [12] ImageNette/ImageWoof [37] ImageNet-10/100 [38] Downsampled ImageNet-1K [13] Full ImageNet-1K [24] 200 10 10/100 1,000 1,000 500 \u223c1,000 \u223c1,200 \u223c1,200 \u223c1,200 MTT [1], FRePo [5], DM [10], SRe2L (Ours) 64\u00d764 112\u00d7112 MTT [1], FRePo [5] 224\u00d7224 IDC [7] 64\u00d764 TESLA [11], DM [10] SRe2L (Ours) 224\u00d7224 Table 6: Variants of ImageNet-1K training set with different configurations. A.2 Squeezing Details Data Augmentation. Table 2 in the main paper illustrates that the utilization of data augmentation approaches during the squeezing phase contributes to a decrease in the final accuracy of the recovered data. To summarize, the results on Tiny-ImageNet indicate that lengthening the training budget and the application of more data augmentations in the squeezing phase intensify the intricacy involved in data recovery from the compressed model, which is not desired. Parallel conclusions are inferred from the compressed models for the ImageNet-1K dataset. For our experimental setup, we aimed to extract data from a pre-trained ResNet-50 model with available V1 and V2 weights in the PyTorch model zoo. The results propose that the task of data extraction poses a greater challenge from the ResNet-50 model equipped with V2 weights as compared to the model incorporating V1 weights. This can be attributed to the fact that models utilizing V1 weights are trained employing a rudimentary recipe, whereas models with V2 weights encompass numerous training enhancements, such as more training budget and data augmentations, to achieve cutting-edge performance. It is observed that these additional complexities impede the data recovery process even the pre-trained models are stronger. Therefore, the pre-trained models we employ for the recovery of ImageNet-1K are those integrating V1 weights from the PyTorch model zoo. Hyper-parameter Setting. We provide hyper-parameter settings for the two datasets in detail. Tiny-ImageNet: We train modified ResNet-{18, 50} models on Tiny-ImageNet data with the parameter setting in Table 7a. The well-trained ResNet-{18, 50} models achieve Top-1 accuracy of {59.47%, 61.17%} under the 50 epoch training budget. 14 config value config value optimizer base learning rate weight decay optimizer momentum 0.9 256 batch size learning rate schedule cosine decay 100 training epoch RandomResizedCrop augmentation SGD 0.2 1e-4 optimizer base learning rate weight decay optimizer momentum \u03b21, \u03b22 = 0.9, 0.999 batch size learning rate schedule training epoch augmentation AdamW 0.001 0.01 1,024 cosine decay 300 RandomResizedCrop (a) Tiny-ImageNet squeezing setting. (b) ImageNet-1K validation setting. config value config value \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 1.0 Adam 0.1 100 cosine decay 1,000 RandomResizedCrop \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 0.01 Adam 0.25 100 cosine decay 2,000 RandomResizedCrop (c) Tiny-ImageNet recovering setting. (d) ImageNet-1K recovering setting. Table 7: Hyper-parameter settings in three stages. ImageNet-1K: We use PyTorch off-the-shelf ResNet-{18, 50} with V1 weights and Top-1 accuracy of {69.76%, 76.13%} as squeezed/condensed models. In the original training script [39], ResNet models are trained for 90 epochs with a SGD optimizer, learning rate of 0.1, momentum of 0.9 and weight decay of 1 \u00d7 10\u22124. BN-ViT Model Structure. To give a clear illustration of incorporating BN layers into ViT, we provide more details in this subsection. The definitions of vanilla ViT and BN-ViT are presented below to show the structure modification. The vanilla ViT can be formulated as: z\u2032 \u2113 = MHSA (LN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFN (LN (z\u2032 \u2113)) + z\u2032 \u2113 where z\u2032 \u2113 is the intermediate representation before Feed-forward Network (FFN), and z\u2113 is that after FFN and residual connection. FFN contains two linear layers with a GELU non-linearity in between them, i.e., FFN(z\u2032 \u2113) = (cid:0)GELU (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1) W 2 \u2113 + b2 \u2113 The newly constructed BN-ViT is: z\u2032 \u2113 = MHSA (BN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFNBN (BN (z\u2032 \u2113)) + z\u2032 \u2113 where we add one additional BN layer in-between two linear layers of FFN, i.e., FFNBN(z\u2032 \u2113) = (cid:0)GELU (cid:0)BN (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1)(cid:1) W 2 \u2113 + b2 \u2113 We follow the DeiT official training recipe to train a DeiT-Tiny-BN model for 300 epochs with an AdamW optimizer, cosine decayed learning rate of 5 \u00d7 10\u22124, weight decay of 0.05 and 5 warmup epochs. 15 Ablation Top-1 acc. (%) RT V R\u21132 Random Crop Tiny-ImageNet \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 29.87 29.92 30.11 30.30 37.88 ImageNet-1K 22.92 23.15 40.81 40.37 46.71 Table 8: Top-1 validation accuracy under regularization ablation settings. ResNet-18 is used in all three stages with the relabeling temperature \u03c4 = 20. A.3 Recovering Details Regularization Terms. We conduct a large number of ablation experiments under varying regular- ization term conditions, as illustrated in Table 8. The two image prior regularizers, \u21132 regularization and total variation (TV), are not anticipated to enhance validation accuracy as our primary focus is on information recovery rather than image smoothness. Consequently, we exclude these two regularization terms from our experiments."}, {"question": " What is the resolution of images in the Downsampled ImageNet-1K dataset?", "answer": " 64x64", "ref_chunk": "in the main text, including: Section A: Implementation Details. Section B: Low-Resolution Data. Section C: Feature Embedding Distribution. Section D: Theoretical Analysis. Section E: More Visualization of Synthetic Data. A Implementation Details A.1 Dataset Statistics Table 6 enumerates various permutations of ImageNet-1K training set, delineated according to their individual configurations. Tiny-ImageNet [12] incorporates 200 classes derived from ImageNet-1K, with each class comprising 500 images possessing a resolution of 64\u00d764. Ima- geNette/ImageWoof [37] (alternatively referred to as subsets of ImageNet) include 10 classes from analogous subcategories, with each image having a resolution of 112\u00d7112. The MTT [1] framework introduces additional 10-class subsets of ImageNet, encompassing ImageFruit, ImageSquawk, Image- Meow, ImageBlub, and ImageYellow. ImageNet-10/100 [38] samples 10/100 classes from ImageNet while maintaining an image resolution of 224\u00d7224. Downsampled ImageNet-1K rescales the entirety of ImageNet data to a resolution of 64\u00d764. In our experiments, we choose two standard datasets of relatively large scale: Tiny-ImageNet and the full ImageNet-1K. Training Dataset #Class #Img per class Resolution Method Tiny-ImageNet [12] ImageNette/ImageWoof [37] ImageNet-10/100 [38] Downsampled ImageNet-1K [13] Full ImageNet-1K [24] 200 10 10/100 1,000 1,000 500 \u223c1,000 \u223c1,200 \u223c1,200 \u223c1,200 MTT [1], FRePo [5], DM [10], SRe2L (Ours) 64\u00d764 112\u00d7112 MTT [1], FRePo [5] 224\u00d7224 IDC [7] 64\u00d764 TESLA [11], DM [10] SRe2L (Ours) 224\u00d7224 Table 6: Variants of ImageNet-1K training set with different configurations. A.2 Squeezing Details Data Augmentation. Table 2 in the main paper illustrates that the utilization of data augmentation approaches during the squeezing phase contributes to a decrease in the final accuracy of the recovered data. To summarize, the results on Tiny-ImageNet indicate that lengthening the training budget and the application of more data augmentations in the squeezing phase intensify the intricacy involved in data recovery from the compressed model, which is not desired. Parallel conclusions are inferred from the compressed models for the ImageNet-1K dataset. For our experimental setup, we aimed to extract data from a pre-trained ResNet-50 model with available V1 and V2 weights in the PyTorch model zoo. The results propose that the task of data extraction poses a greater challenge from the ResNet-50 model equipped with V2 weights as compared to the model incorporating V1 weights. This can be attributed to the fact that models utilizing V1 weights are trained employing a rudimentary recipe, whereas models with V2 weights encompass numerous training enhancements, such as more training budget and data augmentations, to achieve cutting-edge performance. It is observed that these additional complexities impede the data recovery process even the pre-trained models are stronger. Therefore, the pre-trained models we employ for the recovery of ImageNet-1K are those integrating V1 weights from the PyTorch model zoo. Hyper-parameter Setting. We provide hyper-parameter settings for the two datasets in detail. Tiny-ImageNet: We train modified ResNet-{18, 50} models on Tiny-ImageNet data with the parameter setting in Table 7a. The well-trained ResNet-{18, 50} models achieve Top-1 accuracy of {59.47%, 61.17%} under the 50 epoch training budget. 14 config value config value optimizer base learning rate weight decay optimizer momentum 0.9 256 batch size learning rate schedule cosine decay 100 training epoch RandomResizedCrop augmentation SGD 0.2 1e-4 optimizer base learning rate weight decay optimizer momentum \u03b21, \u03b22 = 0.9, 0.999 batch size learning rate schedule training epoch augmentation AdamW 0.001 0.01 1,024 cosine decay 300 RandomResizedCrop (a) Tiny-ImageNet squeezing setting. (b) ImageNet-1K validation setting. config value config value \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 1.0 Adam 0.1 100 cosine decay 1,000 RandomResizedCrop \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 0.01 Adam 0.25 100 cosine decay 2,000 RandomResizedCrop (c) Tiny-ImageNet recovering setting. (d) ImageNet-1K recovering setting. Table 7: Hyper-parameter settings in three stages. ImageNet-1K: We use PyTorch off-the-shelf ResNet-{18, 50} with V1 weights and Top-1 accuracy of {69.76%, 76.13%} as squeezed/condensed models. In the original training script [39], ResNet models are trained for 90 epochs with a SGD optimizer, learning rate of 0.1, momentum of 0.9 and weight decay of 1 \u00d7 10\u22124. BN-ViT Model Structure. To give a clear illustration of incorporating BN layers into ViT, we provide more details in this subsection. The definitions of vanilla ViT and BN-ViT are presented below to show the structure modification. The vanilla ViT can be formulated as: z\u2032 \u2113 = MHSA (LN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFN (LN (z\u2032 \u2113)) + z\u2032 \u2113 where z\u2032 \u2113 is the intermediate representation before Feed-forward Network (FFN), and z\u2113 is that after FFN and residual connection. FFN contains two linear layers with a GELU non-linearity in between them, i.e., FFN(z\u2032 \u2113) = (cid:0)GELU (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1) W 2 \u2113 + b2 \u2113 The newly constructed BN-ViT is: z\u2032 \u2113 = MHSA (BN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFNBN (BN (z\u2032 \u2113)) + z\u2032 \u2113 where we add one additional BN layer in-between two linear layers of FFN, i.e., FFNBN(z\u2032 \u2113) = (cid:0)GELU (cid:0)BN (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1)(cid:1) W 2 \u2113 + b2 \u2113 We follow the DeiT official training recipe to train a DeiT-Tiny-BN model for 300 epochs with an AdamW optimizer, cosine decayed learning rate of 5 \u00d7 10\u22124, weight decay of 0.05 and 5 warmup epochs. 15 Ablation Top-1 acc. (%) RT V R\u21132 Random Crop Tiny-ImageNet \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 29.87 29.92 30.11 30.30 37.88 ImageNet-1K 22.92 23.15 40.81 40.37 46.71 Table 8: Top-1 validation accuracy under regularization ablation settings. ResNet-18 is used in all three stages with the relabeling temperature \u03c4 = 20. A.3 Recovering Details Regularization Terms. We conduct a large number of ablation experiments under varying regular- ization term conditions, as illustrated in Table 8. The two image prior regularizers, \u21132 regularization and total variation (TV), are not anticipated to enhance validation accuracy as our primary focus is on information recovery rather than image smoothness. Consequently, we exclude these two regularization terms from our experiments."}, {"question": " In the experiments mentioned, which two standard datasets of relatively large scale were chosen?", "answer": " Tiny-ImageNet and the full ImageNet-1K", "ref_chunk": "in the main text, including: Section A: Implementation Details. Section B: Low-Resolution Data. Section C: Feature Embedding Distribution. Section D: Theoretical Analysis. Section E: More Visualization of Synthetic Data. A Implementation Details A.1 Dataset Statistics Table 6 enumerates various permutations of ImageNet-1K training set, delineated according to their individual configurations. Tiny-ImageNet [12] incorporates 200 classes derived from ImageNet-1K, with each class comprising 500 images possessing a resolution of 64\u00d764. Ima- geNette/ImageWoof [37] (alternatively referred to as subsets of ImageNet) include 10 classes from analogous subcategories, with each image having a resolution of 112\u00d7112. The MTT [1] framework introduces additional 10-class subsets of ImageNet, encompassing ImageFruit, ImageSquawk, Image- Meow, ImageBlub, and ImageYellow. ImageNet-10/100 [38] samples 10/100 classes from ImageNet while maintaining an image resolution of 224\u00d7224. Downsampled ImageNet-1K rescales the entirety of ImageNet data to a resolution of 64\u00d764. In our experiments, we choose two standard datasets of relatively large scale: Tiny-ImageNet and the full ImageNet-1K. Training Dataset #Class #Img per class Resolution Method Tiny-ImageNet [12] ImageNette/ImageWoof [37] ImageNet-10/100 [38] Downsampled ImageNet-1K [13] Full ImageNet-1K [24] 200 10 10/100 1,000 1,000 500 \u223c1,000 \u223c1,200 \u223c1,200 \u223c1,200 MTT [1], FRePo [5], DM [10], SRe2L (Ours) 64\u00d764 112\u00d7112 MTT [1], FRePo [5] 224\u00d7224 IDC [7] 64\u00d764 TESLA [11], DM [10] SRe2L (Ours) 224\u00d7224 Table 6: Variants of ImageNet-1K training set with different configurations. A.2 Squeezing Details Data Augmentation. Table 2 in the main paper illustrates that the utilization of data augmentation approaches during the squeezing phase contributes to a decrease in the final accuracy of the recovered data. To summarize, the results on Tiny-ImageNet indicate that lengthening the training budget and the application of more data augmentations in the squeezing phase intensify the intricacy involved in data recovery from the compressed model, which is not desired. Parallel conclusions are inferred from the compressed models for the ImageNet-1K dataset. For our experimental setup, we aimed to extract data from a pre-trained ResNet-50 model with available V1 and V2 weights in the PyTorch model zoo. The results propose that the task of data extraction poses a greater challenge from the ResNet-50 model equipped with V2 weights as compared to the model incorporating V1 weights. This can be attributed to the fact that models utilizing V1 weights are trained employing a rudimentary recipe, whereas models with V2 weights encompass numerous training enhancements, such as more training budget and data augmentations, to achieve cutting-edge performance. It is observed that these additional complexities impede the data recovery process even the pre-trained models are stronger. Therefore, the pre-trained models we employ for the recovery of ImageNet-1K are those integrating V1 weights from the PyTorch model zoo. Hyper-parameter Setting. We provide hyper-parameter settings for the two datasets in detail. Tiny-ImageNet: We train modified ResNet-{18, 50} models on Tiny-ImageNet data with the parameter setting in Table 7a. The well-trained ResNet-{18, 50} models achieve Top-1 accuracy of {59.47%, 61.17%} under the 50 epoch training budget. 14 config value config value optimizer base learning rate weight decay optimizer momentum 0.9 256 batch size learning rate schedule cosine decay 100 training epoch RandomResizedCrop augmentation SGD 0.2 1e-4 optimizer base learning rate weight decay optimizer momentum \u03b21, \u03b22 = 0.9, 0.999 batch size learning rate schedule training epoch augmentation AdamW 0.001 0.01 1,024 cosine decay 300 RandomResizedCrop (a) Tiny-ImageNet squeezing setting. (b) ImageNet-1K validation setting. config value config value \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 1.0 Adam 0.1 100 cosine decay 1,000 RandomResizedCrop \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 0.01 Adam 0.25 100 cosine decay 2,000 RandomResizedCrop (c) Tiny-ImageNet recovering setting. (d) ImageNet-1K recovering setting. Table 7: Hyper-parameter settings in three stages. ImageNet-1K: We use PyTorch off-the-shelf ResNet-{18, 50} with V1 weights and Top-1 accuracy of {69.76%, 76.13%} as squeezed/condensed models. In the original training script [39], ResNet models are trained for 90 epochs with a SGD optimizer, learning rate of 0.1, momentum of 0.9 and weight decay of 1 \u00d7 10\u22124. BN-ViT Model Structure. To give a clear illustration of incorporating BN layers into ViT, we provide more details in this subsection. The definitions of vanilla ViT and BN-ViT are presented below to show the structure modification. The vanilla ViT can be formulated as: z\u2032 \u2113 = MHSA (LN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFN (LN (z\u2032 \u2113)) + z\u2032 \u2113 where z\u2032 \u2113 is the intermediate representation before Feed-forward Network (FFN), and z\u2113 is that after FFN and residual connection. FFN contains two linear layers with a GELU non-linearity in between them, i.e., FFN(z\u2032 \u2113) = (cid:0)GELU (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1) W 2 \u2113 + b2 \u2113 The newly constructed BN-ViT is: z\u2032 \u2113 = MHSA (BN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFNBN (BN (z\u2032 \u2113)) + z\u2032 \u2113 where we add one additional BN layer in-between two linear layers of FFN, i.e., FFNBN(z\u2032 \u2113) = (cid:0)GELU (cid:0)BN (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1)(cid:1) W 2 \u2113 + b2 \u2113 We follow the DeiT official training recipe to train a DeiT-Tiny-BN model for 300 epochs with an AdamW optimizer, cosine decayed learning rate of 5 \u00d7 10\u22124, weight decay of 0.05 and 5 warmup epochs. 15 Ablation Top-1 acc. (%) RT V R\u21132 Random Crop Tiny-ImageNet \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 29.87 29.92 30.11 30.30 37.88 ImageNet-1K 22.92 23.15 40.81 40.37 46.71 Table 8: Top-1 validation accuracy under regularization ablation settings. ResNet-18 is used in all three stages with the relabeling temperature \u03c4 = 20. A.3 Recovering Details Regularization Terms. We conduct a large number of ablation experiments under varying regular- ization term conditions, as illustrated in Table 8. The two image prior regularizers, \u21132 regularization and total variation (TV), are not anticipated to enhance validation accuracy as our primary focus is on information recovery rather than image smoothness. Consequently, we exclude these two regularization terms from our experiments."}, {"question": " How does the utilization of data augmentation approaches during the squeezing phase affect the final accuracy of the recovered data?", "answer": " Contributes to a decrease in the final accuracy", "ref_chunk": "in the main text, including: Section A: Implementation Details. Section B: Low-Resolution Data. Section C: Feature Embedding Distribution. Section D: Theoretical Analysis. Section E: More Visualization of Synthetic Data. A Implementation Details A.1 Dataset Statistics Table 6 enumerates various permutations of ImageNet-1K training set, delineated according to their individual configurations. Tiny-ImageNet [12] incorporates 200 classes derived from ImageNet-1K, with each class comprising 500 images possessing a resolution of 64\u00d764. Ima- geNette/ImageWoof [37] (alternatively referred to as subsets of ImageNet) include 10 classes from analogous subcategories, with each image having a resolution of 112\u00d7112. The MTT [1] framework introduces additional 10-class subsets of ImageNet, encompassing ImageFruit, ImageSquawk, Image- Meow, ImageBlub, and ImageYellow. ImageNet-10/100 [38] samples 10/100 classes from ImageNet while maintaining an image resolution of 224\u00d7224. Downsampled ImageNet-1K rescales the entirety of ImageNet data to a resolution of 64\u00d764. In our experiments, we choose two standard datasets of relatively large scale: Tiny-ImageNet and the full ImageNet-1K. Training Dataset #Class #Img per class Resolution Method Tiny-ImageNet [12] ImageNette/ImageWoof [37] ImageNet-10/100 [38] Downsampled ImageNet-1K [13] Full ImageNet-1K [24] 200 10 10/100 1,000 1,000 500 \u223c1,000 \u223c1,200 \u223c1,200 \u223c1,200 MTT [1], FRePo [5], DM [10], SRe2L (Ours) 64\u00d764 112\u00d7112 MTT [1], FRePo [5] 224\u00d7224 IDC [7] 64\u00d764 TESLA [11], DM [10] SRe2L (Ours) 224\u00d7224 Table 6: Variants of ImageNet-1K training set with different configurations. A.2 Squeezing Details Data Augmentation. Table 2 in the main paper illustrates that the utilization of data augmentation approaches during the squeezing phase contributes to a decrease in the final accuracy of the recovered data. To summarize, the results on Tiny-ImageNet indicate that lengthening the training budget and the application of more data augmentations in the squeezing phase intensify the intricacy involved in data recovery from the compressed model, which is not desired. Parallel conclusions are inferred from the compressed models for the ImageNet-1K dataset. For our experimental setup, we aimed to extract data from a pre-trained ResNet-50 model with available V1 and V2 weights in the PyTorch model zoo. The results propose that the task of data extraction poses a greater challenge from the ResNet-50 model equipped with V2 weights as compared to the model incorporating V1 weights. This can be attributed to the fact that models utilizing V1 weights are trained employing a rudimentary recipe, whereas models with V2 weights encompass numerous training enhancements, such as more training budget and data augmentations, to achieve cutting-edge performance. It is observed that these additional complexities impede the data recovery process even the pre-trained models are stronger. Therefore, the pre-trained models we employ for the recovery of ImageNet-1K are those integrating V1 weights from the PyTorch model zoo. Hyper-parameter Setting. We provide hyper-parameter settings for the two datasets in detail. Tiny-ImageNet: We train modified ResNet-{18, 50} models on Tiny-ImageNet data with the parameter setting in Table 7a. The well-trained ResNet-{18, 50} models achieve Top-1 accuracy of {59.47%, 61.17%} under the 50 epoch training budget. 14 config value config value optimizer base learning rate weight decay optimizer momentum 0.9 256 batch size learning rate schedule cosine decay 100 training epoch RandomResizedCrop augmentation SGD 0.2 1e-4 optimizer base learning rate weight decay optimizer momentum \u03b21, \u03b22 = 0.9, 0.999 batch size learning rate schedule training epoch augmentation AdamW 0.001 0.01 1,024 cosine decay 300 RandomResizedCrop (a) Tiny-ImageNet squeezing setting. (b) ImageNet-1K validation setting. config value config value \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 1.0 Adam 0.1 100 cosine decay 1,000 RandomResizedCrop \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 0.01 Adam 0.25 100 cosine decay 2,000 RandomResizedCrop (c) Tiny-ImageNet recovering setting. (d) ImageNet-1K recovering setting. Table 7: Hyper-parameter settings in three stages. ImageNet-1K: We use PyTorch off-the-shelf ResNet-{18, 50} with V1 weights and Top-1 accuracy of {69.76%, 76.13%} as squeezed/condensed models. In the original training script [39], ResNet models are trained for 90 epochs with a SGD optimizer, learning rate of 0.1, momentum of 0.9 and weight decay of 1 \u00d7 10\u22124. BN-ViT Model Structure. To give a clear illustration of incorporating BN layers into ViT, we provide more details in this subsection. The definitions of vanilla ViT and BN-ViT are presented below to show the structure modification. The vanilla ViT can be formulated as: z\u2032 \u2113 = MHSA (LN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFN (LN (z\u2032 \u2113)) + z\u2032 \u2113 where z\u2032 \u2113 is the intermediate representation before Feed-forward Network (FFN), and z\u2113 is that after FFN and residual connection. FFN contains two linear layers with a GELU non-linearity in between them, i.e., FFN(z\u2032 \u2113) = (cid:0)GELU (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1) W 2 \u2113 + b2 \u2113 The newly constructed BN-ViT is: z\u2032 \u2113 = MHSA (BN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFNBN (BN (z\u2032 \u2113)) + z\u2032 \u2113 where we add one additional BN layer in-between two linear layers of FFN, i.e., FFNBN(z\u2032 \u2113) = (cid:0)GELU (cid:0)BN (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1)(cid:1) W 2 \u2113 + b2 \u2113 We follow the DeiT official training recipe to train a DeiT-Tiny-BN model for 300 epochs with an AdamW optimizer, cosine decayed learning rate of 5 \u00d7 10\u22124, weight decay of 0.05 and 5 warmup epochs. 15 Ablation Top-1 acc. (%) RT V R\u21132 Random Crop Tiny-ImageNet \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 29.87 29.92 30.11 30.30 37.88 ImageNet-1K 22.92 23.15 40.81 40.37 46.71 Table 8: Top-1 validation accuracy under regularization ablation settings. ResNet-18 is used in all three stages with the relabeling temperature \u03c4 = 20. A.3 Recovering Details Regularization Terms. We conduct a large number of ablation experiments under varying regular- ization term conditions, as illustrated in Table 8. The two image prior regularizers, \u21132 regularization and total variation (TV), are not anticipated to enhance validation accuracy as our primary focus is on information recovery rather than image smoothness. Consequently, we exclude these two regularization terms from our experiments."}, {"question": " Why do pre-trained models employing V1 weights pose a lesser challenge for data extraction compared to models with V2 weights?", "answer": " Models with V1 weights are trained employing a rudimentary recipe, whereas models with V2 weights encompass numerous training enhancements", "ref_chunk": "in the main text, including: Section A: Implementation Details. Section B: Low-Resolution Data. Section C: Feature Embedding Distribution. Section D: Theoretical Analysis. Section E: More Visualization of Synthetic Data. A Implementation Details A.1 Dataset Statistics Table 6 enumerates various permutations of ImageNet-1K training set, delineated according to their individual configurations. Tiny-ImageNet [12] incorporates 200 classes derived from ImageNet-1K, with each class comprising 500 images possessing a resolution of 64\u00d764. Ima- geNette/ImageWoof [37] (alternatively referred to as subsets of ImageNet) include 10 classes from analogous subcategories, with each image having a resolution of 112\u00d7112. The MTT [1] framework introduces additional 10-class subsets of ImageNet, encompassing ImageFruit, ImageSquawk, Image- Meow, ImageBlub, and ImageYellow. ImageNet-10/100 [38] samples 10/100 classes from ImageNet while maintaining an image resolution of 224\u00d7224. Downsampled ImageNet-1K rescales the entirety of ImageNet data to a resolution of 64\u00d764. In our experiments, we choose two standard datasets of relatively large scale: Tiny-ImageNet and the full ImageNet-1K. Training Dataset #Class #Img per class Resolution Method Tiny-ImageNet [12] ImageNette/ImageWoof [37] ImageNet-10/100 [38] Downsampled ImageNet-1K [13] Full ImageNet-1K [24] 200 10 10/100 1,000 1,000 500 \u223c1,000 \u223c1,200 \u223c1,200 \u223c1,200 MTT [1], FRePo [5], DM [10], SRe2L (Ours) 64\u00d764 112\u00d7112 MTT [1], FRePo [5] 224\u00d7224 IDC [7] 64\u00d764 TESLA [11], DM [10] SRe2L (Ours) 224\u00d7224 Table 6: Variants of ImageNet-1K training set with different configurations. A.2 Squeezing Details Data Augmentation. Table 2 in the main paper illustrates that the utilization of data augmentation approaches during the squeezing phase contributes to a decrease in the final accuracy of the recovered data. To summarize, the results on Tiny-ImageNet indicate that lengthening the training budget and the application of more data augmentations in the squeezing phase intensify the intricacy involved in data recovery from the compressed model, which is not desired. Parallel conclusions are inferred from the compressed models for the ImageNet-1K dataset. For our experimental setup, we aimed to extract data from a pre-trained ResNet-50 model with available V1 and V2 weights in the PyTorch model zoo. The results propose that the task of data extraction poses a greater challenge from the ResNet-50 model equipped with V2 weights as compared to the model incorporating V1 weights. This can be attributed to the fact that models utilizing V1 weights are trained employing a rudimentary recipe, whereas models with V2 weights encompass numerous training enhancements, such as more training budget and data augmentations, to achieve cutting-edge performance. It is observed that these additional complexities impede the data recovery process even the pre-trained models are stronger. Therefore, the pre-trained models we employ for the recovery of ImageNet-1K are those integrating V1 weights from the PyTorch model zoo. Hyper-parameter Setting. We provide hyper-parameter settings for the two datasets in detail. Tiny-ImageNet: We train modified ResNet-{18, 50} models on Tiny-ImageNet data with the parameter setting in Table 7a. The well-trained ResNet-{18, 50} models achieve Top-1 accuracy of {59.47%, 61.17%} under the 50 epoch training budget. 14 config value config value optimizer base learning rate weight decay optimizer momentum 0.9 256 batch size learning rate schedule cosine decay 100 training epoch RandomResizedCrop augmentation SGD 0.2 1e-4 optimizer base learning rate weight decay optimizer momentum \u03b21, \u03b22 = 0.9, 0.999 batch size learning rate schedule training epoch augmentation AdamW 0.001 0.01 1,024 cosine decay 300 RandomResizedCrop (a) Tiny-ImageNet squeezing setting. (b) ImageNet-1K validation setting. config value config value \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 1.0 Adam 0.1 100 cosine decay 1,000 RandomResizedCrop \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 0.01 Adam 0.25 100 cosine decay 2,000 RandomResizedCrop (c) Tiny-ImageNet recovering setting. (d) ImageNet-1K recovering setting. Table 7: Hyper-parameter settings in three stages. ImageNet-1K: We use PyTorch off-the-shelf ResNet-{18, 50} with V1 weights and Top-1 accuracy of {69.76%, 76.13%} as squeezed/condensed models. In the original training script [39], ResNet models are trained for 90 epochs with a SGD optimizer, learning rate of 0.1, momentum of 0.9 and weight decay of 1 \u00d7 10\u22124. BN-ViT Model Structure. To give a clear illustration of incorporating BN layers into ViT, we provide more details in this subsection. The definitions of vanilla ViT and BN-ViT are presented below to show the structure modification. The vanilla ViT can be formulated as: z\u2032 \u2113 = MHSA (LN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFN (LN (z\u2032 \u2113)) + z\u2032 \u2113 where z\u2032 \u2113 is the intermediate representation before Feed-forward Network (FFN), and z\u2113 is that after FFN and residual connection. FFN contains two linear layers with a GELU non-linearity in between them, i.e., FFN(z\u2032 \u2113) = (cid:0)GELU (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1) W 2 \u2113 + b2 \u2113 The newly constructed BN-ViT is: z\u2032 \u2113 = MHSA (BN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFNBN (BN (z\u2032 \u2113)) + z\u2032 \u2113 where we add one additional BN layer in-between two linear layers of FFN, i.e., FFNBN(z\u2032 \u2113) = (cid:0)GELU (cid:0)BN (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1)(cid:1) W 2 \u2113 + b2 \u2113 We follow the DeiT official training recipe to train a DeiT-Tiny-BN model for 300 epochs with an AdamW optimizer, cosine decayed learning rate of 5 \u00d7 10\u22124, weight decay of 0.05 and 5 warmup epochs. 15 Ablation Top-1 acc. (%) RT V R\u21132 Random Crop Tiny-ImageNet \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 29.87 29.92 30.11 30.30 37.88 ImageNet-1K 22.92 23.15 40.81 40.37 46.71 Table 8: Top-1 validation accuracy under regularization ablation settings. ResNet-18 is used in all three stages with the relabeling temperature \u03c4 = 20. A.3 Recovering Details Regularization Terms. We conduct a large number of ablation experiments under varying regular- ization term conditions, as illustrated in Table 8. The two image prior regularizers, \u21132 regularization and total variation (TV), are not anticipated to enhance validation accuracy as our primary focus is on information recovery rather than image smoothness. Consequently, we exclude these two regularization terms from our experiments."}, {"question": " What is the Top-1 accuracy achieved by the well-trained ResNet-18 model on Tiny-ImageNet data?", "answer": " 59.47%", "ref_chunk": "in the main text, including: Section A: Implementation Details. Section B: Low-Resolution Data. Section C: Feature Embedding Distribution. Section D: Theoretical Analysis. Section E: More Visualization of Synthetic Data. A Implementation Details A.1 Dataset Statistics Table 6 enumerates various permutations of ImageNet-1K training set, delineated according to their individual configurations. Tiny-ImageNet [12] incorporates 200 classes derived from ImageNet-1K, with each class comprising 500 images possessing a resolution of 64\u00d764. Ima- geNette/ImageWoof [37] (alternatively referred to as subsets of ImageNet) include 10 classes from analogous subcategories, with each image having a resolution of 112\u00d7112. The MTT [1] framework introduces additional 10-class subsets of ImageNet, encompassing ImageFruit, ImageSquawk, Image- Meow, ImageBlub, and ImageYellow. ImageNet-10/100 [38] samples 10/100 classes from ImageNet while maintaining an image resolution of 224\u00d7224. Downsampled ImageNet-1K rescales the entirety of ImageNet data to a resolution of 64\u00d764. In our experiments, we choose two standard datasets of relatively large scale: Tiny-ImageNet and the full ImageNet-1K. Training Dataset #Class #Img per class Resolution Method Tiny-ImageNet [12] ImageNette/ImageWoof [37] ImageNet-10/100 [38] Downsampled ImageNet-1K [13] Full ImageNet-1K [24] 200 10 10/100 1,000 1,000 500 \u223c1,000 \u223c1,200 \u223c1,200 \u223c1,200 MTT [1], FRePo [5], DM [10], SRe2L (Ours) 64\u00d764 112\u00d7112 MTT [1], FRePo [5] 224\u00d7224 IDC [7] 64\u00d764 TESLA [11], DM [10] SRe2L (Ours) 224\u00d7224 Table 6: Variants of ImageNet-1K training set with different configurations. A.2 Squeezing Details Data Augmentation. Table 2 in the main paper illustrates that the utilization of data augmentation approaches during the squeezing phase contributes to a decrease in the final accuracy of the recovered data. To summarize, the results on Tiny-ImageNet indicate that lengthening the training budget and the application of more data augmentations in the squeezing phase intensify the intricacy involved in data recovery from the compressed model, which is not desired. Parallel conclusions are inferred from the compressed models for the ImageNet-1K dataset. For our experimental setup, we aimed to extract data from a pre-trained ResNet-50 model with available V1 and V2 weights in the PyTorch model zoo. The results propose that the task of data extraction poses a greater challenge from the ResNet-50 model equipped with V2 weights as compared to the model incorporating V1 weights. This can be attributed to the fact that models utilizing V1 weights are trained employing a rudimentary recipe, whereas models with V2 weights encompass numerous training enhancements, such as more training budget and data augmentations, to achieve cutting-edge performance. It is observed that these additional complexities impede the data recovery process even the pre-trained models are stronger. Therefore, the pre-trained models we employ for the recovery of ImageNet-1K are those integrating V1 weights from the PyTorch model zoo. Hyper-parameter Setting. We provide hyper-parameter settings for the two datasets in detail. Tiny-ImageNet: We train modified ResNet-{18, 50} models on Tiny-ImageNet data with the parameter setting in Table 7a. The well-trained ResNet-{18, 50} models achieve Top-1 accuracy of {59.47%, 61.17%} under the 50 epoch training budget. 14 config value config value optimizer base learning rate weight decay optimizer momentum 0.9 256 batch size learning rate schedule cosine decay 100 training epoch RandomResizedCrop augmentation SGD 0.2 1e-4 optimizer base learning rate weight decay optimizer momentum \u03b21, \u03b22 = 0.9, 0.999 batch size learning rate schedule training epoch augmentation AdamW 0.001 0.01 1,024 cosine decay 300 RandomResizedCrop (a) Tiny-ImageNet squeezing setting. (b) ImageNet-1K validation setting. config value config value \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 1.0 Adam 0.1 100 cosine decay 1,000 RandomResizedCrop \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 0.01 Adam 0.25 100 cosine decay 2,000 RandomResizedCrop (c) Tiny-ImageNet recovering setting. (d) ImageNet-1K recovering setting. Table 7: Hyper-parameter settings in three stages. ImageNet-1K: We use PyTorch off-the-shelf ResNet-{18, 50} with V1 weights and Top-1 accuracy of {69.76%, 76.13%} as squeezed/condensed models. In the original training script [39], ResNet models are trained for 90 epochs with a SGD optimizer, learning rate of 0.1, momentum of 0.9 and weight decay of 1 \u00d7 10\u22124. BN-ViT Model Structure. To give a clear illustration of incorporating BN layers into ViT, we provide more details in this subsection. The definitions of vanilla ViT and BN-ViT are presented below to show the structure modification. The vanilla ViT can be formulated as: z\u2032 \u2113 = MHSA (LN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFN (LN (z\u2032 \u2113)) + z\u2032 \u2113 where z\u2032 \u2113 is the intermediate representation before Feed-forward Network (FFN), and z\u2113 is that after FFN and residual connection. FFN contains two linear layers with a GELU non-linearity in between them, i.e., FFN(z\u2032 \u2113) = (cid:0)GELU (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1) W 2 \u2113 + b2 \u2113 The newly constructed BN-ViT is: z\u2032 \u2113 = MHSA (BN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFNBN (BN (z\u2032 \u2113)) + z\u2032 \u2113 where we add one additional BN layer in-between two linear layers of FFN, i.e., FFNBN(z\u2032 \u2113) = (cid:0)GELU (cid:0)BN (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1)(cid:1) W 2 \u2113 + b2 \u2113 We follow the DeiT official training recipe to train a DeiT-Tiny-BN model for 300 epochs with an AdamW optimizer, cosine decayed learning rate of 5 \u00d7 10\u22124, weight decay of 0.05 and 5 warmup epochs. 15 Ablation Top-1 acc. (%) RT V R\u21132 Random Crop Tiny-ImageNet \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 29.87 29.92 30.11 30.30 37.88 ImageNet-1K 22.92 23.15 40.81 40.37 46.71 Table 8: Top-1 validation accuracy under regularization ablation settings. ResNet-18 is used in all three stages with the relabeling temperature \u03c4 = 20. A.3 Recovering Details Regularization Terms. We conduct a large number of ablation experiments under varying regular- ization term conditions, as illustrated in Table 8. The two image prior regularizers, \u21132 regularization and total variation (TV), are not anticipated to enhance validation accuracy as our primary focus is on information recovery rather than image smoothness. Consequently, we exclude these two regularization terms from our experiments."}, {"question": " What hyper-parameter setting is used for training modified ResNet-50 models on Tiny-ImageNet data?", "answer": " RandomResizedCrop augmentation, SGD optimizer, learning rate of 0.2, weight decay of 1e-4", "ref_chunk": "in the main text, including: Section A: Implementation Details. Section B: Low-Resolution Data. Section C: Feature Embedding Distribution. Section D: Theoretical Analysis. Section E: More Visualization of Synthetic Data. A Implementation Details A.1 Dataset Statistics Table 6 enumerates various permutations of ImageNet-1K training set, delineated according to their individual configurations. Tiny-ImageNet [12] incorporates 200 classes derived from ImageNet-1K, with each class comprising 500 images possessing a resolution of 64\u00d764. Ima- geNette/ImageWoof [37] (alternatively referred to as subsets of ImageNet) include 10 classes from analogous subcategories, with each image having a resolution of 112\u00d7112. The MTT [1] framework introduces additional 10-class subsets of ImageNet, encompassing ImageFruit, ImageSquawk, Image- Meow, ImageBlub, and ImageYellow. ImageNet-10/100 [38] samples 10/100 classes from ImageNet while maintaining an image resolution of 224\u00d7224. Downsampled ImageNet-1K rescales the entirety of ImageNet data to a resolution of 64\u00d764. In our experiments, we choose two standard datasets of relatively large scale: Tiny-ImageNet and the full ImageNet-1K. Training Dataset #Class #Img per class Resolution Method Tiny-ImageNet [12] ImageNette/ImageWoof [37] ImageNet-10/100 [38] Downsampled ImageNet-1K [13] Full ImageNet-1K [24] 200 10 10/100 1,000 1,000 500 \u223c1,000 \u223c1,200 \u223c1,200 \u223c1,200 MTT [1], FRePo [5], DM [10], SRe2L (Ours) 64\u00d764 112\u00d7112 MTT [1], FRePo [5] 224\u00d7224 IDC [7] 64\u00d764 TESLA [11], DM [10] SRe2L (Ours) 224\u00d7224 Table 6: Variants of ImageNet-1K training set with different configurations. A.2 Squeezing Details Data Augmentation. Table 2 in the main paper illustrates that the utilization of data augmentation approaches during the squeezing phase contributes to a decrease in the final accuracy of the recovered data. To summarize, the results on Tiny-ImageNet indicate that lengthening the training budget and the application of more data augmentations in the squeezing phase intensify the intricacy involved in data recovery from the compressed model, which is not desired. Parallel conclusions are inferred from the compressed models for the ImageNet-1K dataset. For our experimental setup, we aimed to extract data from a pre-trained ResNet-50 model with available V1 and V2 weights in the PyTorch model zoo. The results propose that the task of data extraction poses a greater challenge from the ResNet-50 model equipped with V2 weights as compared to the model incorporating V1 weights. This can be attributed to the fact that models utilizing V1 weights are trained employing a rudimentary recipe, whereas models with V2 weights encompass numerous training enhancements, such as more training budget and data augmentations, to achieve cutting-edge performance. It is observed that these additional complexities impede the data recovery process even the pre-trained models are stronger. Therefore, the pre-trained models we employ for the recovery of ImageNet-1K are those integrating V1 weights from the PyTorch model zoo. Hyper-parameter Setting. We provide hyper-parameter settings for the two datasets in detail. Tiny-ImageNet: We train modified ResNet-{18, 50} models on Tiny-ImageNet data with the parameter setting in Table 7a. The well-trained ResNet-{18, 50} models achieve Top-1 accuracy of {59.47%, 61.17%} under the 50 epoch training budget. 14 config value config value optimizer base learning rate weight decay optimizer momentum 0.9 256 batch size learning rate schedule cosine decay 100 training epoch RandomResizedCrop augmentation SGD 0.2 1e-4 optimizer base learning rate weight decay optimizer momentum \u03b21, \u03b22 = 0.9, 0.999 batch size learning rate schedule training epoch augmentation AdamW 0.001 0.01 1,024 cosine decay 300 RandomResizedCrop (a) Tiny-ImageNet squeezing setting. (b) ImageNet-1K validation setting. config value config value \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 1.0 Adam 0.1 100 cosine decay 1,000 RandomResizedCrop \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 0.01 Adam 0.25 100 cosine decay 2,000 RandomResizedCrop (c) Tiny-ImageNet recovering setting. (d) ImageNet-1K recovering setting. Table 7: Hyper-parameter settings in three stages. ImageNet-1K: We use PyTorch off-the-shelf ResNet-{18, 50} with V1 weights and Top-1 accuracy of {69.76%, 76.13%} as squeezed/condensed models. In the original training script [39], ResNet models are trained for 90 epochs with a SGD optimizer, learning rate of 0.1, momentum of 0.9 and weight decay of 1 \u00d7 10\u22124. BN-ViT Model Structure. To give a clear illustration of incorporating BN layers into ViT, we provide more details in this subsection. The definitions of vanilla ViT and BN-ViT are presented below to show the structure modification. The vanilla ViT can be formulated as: z\u2032 \u2113 = MHSA (LN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFN (LN (z\u2032 \u2113)) + z\u2032 \u2113 where z\u2032 \u2113 is the intermediate representation before Feed-forward Network (FFN), and z\u2113 is that after FFN and residual connection. FFN contains two linear layers with a GELU non-linearity in between them, i.e., FFN(z\u2032 \u2113) = (cid:0)GELU (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1) W 2 \u2113 + b2 \u2113 The newly constructed BN-ViT is: z\u2032 \u2113 = MHSA (BN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFNBN (BN (z\u2032 \u2113)) + z\u2032 \u2113 where we add one additional BN layer in-between two linear layers of FFN, i.e., FFNBN(z\u2032 \u2113) = (cid:0)GELU (cid:0)BN (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1)(cid:1) W 2 \u2113 + b2 \u2113 We follow the DeiT official training recipe to train a DeiT-Tiny-BN model for 300 epochs with an AdamW optimizer, cosine decayed learning rate of 5 \u00d7 10\u22124, weight decay of 0.05 and 5 warmup epochs. 15 Ablation Top-1 acc. (%) RT V R\u21132 Random Crop Tiny-ImageNet \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 29.87 29.92 30.11 30.30 37.88 ImageNet-1K 22.92 23.15 40.81 40.37 46.71 Table 8: Top-1 validation accuracy under regularization ablation settings. ResNet-18 is used in all three stages with the relabeling temperature \u03c4 = 20. A.3 Recovering Details Regularization Terms. We conduct a large number of ablation experiments under varying regular- ization term conditions, as illustrated in Table 8. The two image prior regularizers, \u21132 regularization and total variation (TV), are not anticipated to enhance validation accuracy as our primary focus is on information recovery rather than image smoothness. Consequently, we exclude these two regularization terms from our experiments."}, {"question": " Why are the \u21132 regularization and total variation (TV) regularization terms excluded from the experiments?", "answer": " Because the primary focus is on information recovery rather than image smoothness", "ref_chunk": "in the main text, including: Section A: Implementation Details. Section B: Low-Resolution Data. Section C: Feature Embedding Distribution. Section D: Theoretical Analysis. Section E: More Visualization of Synthetic Data. A Implementation Details A.1 Dataset Statistics Table 6 enumerates various permutations of ImageNet-1K training set, delineated according to their individual configurations. Tiny-ImageNet [12] incorporates 200 classes derived from ImageNet-1K, with each class comprising 500 images possessing a resolution of 64\u00d764. Ima- geNette/ImageWoof [37] (alternatively referred to as subsets of ImageNet) include 10 classes from analogous subcategories, with each image having a resolution of 112\u00d7112. The MTT [1] framework introduces additional 10-class subsets of ImageNet, encompassing ImageFruit, ImageSquawk, Image- Meow, ImageBlub, and ImageYellow. ImageNet-10/100 [38] samples 10/100 classes from ImageNet while maintaining an image resolution of 224\u00d7224. Downsampled ImageNet-1K rescales the entirety of ImageNet data to a resolution of 64\u00d764. In our experiments, we choose two standard datasets of relatively large scale: Tiny-ImageNet and the full ImageNet-1K. Training Dataset #Class #Img per class Resolution Method Tiny-ImageNet [12] ImageNette/ImageWoof [37] ImageNet-10/100 [38] Downsampled ImageNet-1K [13] Full ImageNet-1K [24] 200 10 10/100 1,000 1,000 500 \u223c1,000 \u223c1,200 \u223c1,200 \u223c1,200 MTT [1], FRePo [5], DM [10], SRe2L (Ours) 64\u00d764 112\u00d7112 MTT [1], FRePo [5] 224\u00d7224 IDC [7] 64\u00d764 TESLA [11], DM [10] SRe2L (Ours) 224\u00d7224 Table 6: Variants of ImageNet-1K training set with different configurations. A.2 Squeezing Details Data Augmentation. Table 2 in the main paper illustrates that the utilization of data augmentation approaches during the squeezing phase contributes to a decrease in the final accuracy of the recovered data. To summarize, the results on Tiny-ImageNet indicate that lengthening the training budget and the application of more data augmentations in the squeezing phase intensify the intricacy involved in data recovery from the compressed model, which is not desired. Parallel conclusions are inferred from the compressed models for the ImageNet-1K dataset. For our experimental setup, we aimed to extract data from a pre-trained ResNet-50 model with available V1 and V2 weights in the PyTorch model zoo. The results propose that the task of data extraction poses a greater challenge from the ResNet-50 model equipped with V2 weights as compared to the model incorporating V1 weights. This can be attributed to the fact that models utilizing V1 weights are trained employing a rudimentary recipe, whereas models with V2 weights encompass numerous training enhancements, such as more training budget and data augmentations, to achieve cutting-edge performance. It is observed that these additional complexities impede the data recovery process even the pre-trained models are stronger. Therefore, the pre-trained models we employ for the recovery of ImageNet-1K are those integrating V1 weights from the PyTorch model zoo. Hyper-parameter Setting. We provide hyper-parameter settings for the two datasets in detail. Tiny-ImageNet: We train modified ResNet-{18, 50} models on Tiny-ImageNet data with the parameter setting in Table 7a. The well-trained ResNet-{18, 50} models achieve Top-1 accuracy of {59.47%, 61.17%} under the 50 epoch training budget. 14 config value config value optimizer base learning rate weight decay optimizer momentum 0.9 256 batch size learning rate schedule cosine decay 100 training epoch RandomResizedCrop augmentation SGD 0.2 1e-4 optimizer base learning rate weight decay optimizer momentum \u03b21, \u03b22 = 0.9, 0.999 batch size learning rate schedule training epoch augmentation AdamW 0.001 0.01 1,024 cosine decay 300 RandomResizedCrop (a) Tiny-ImageNet squeezing setting. (b) ImageNet-1K validation setting. config value config value \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 1.0 Adam 0.1 100 cosine decay 1,000 RandomResizedCrop \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 0.01 Adam 0.25 100 cosine decay 2,000 RandomResizedCrop (c) Tiny-ImageNet recovering setting. (d) ImageNet-1K recovering setting. Table 7: Hyper-parameter settings in three stages. ImageNet-1K: We use PyTorch off-the-shelf ResNet-{18, 50} with V1 weights and Top-1 accuracy of {69.76%, 76.13%} as squeezed/condensed models. In the original training script [39], ResNet models are trained for 90 epochs with a SGD optimizer, learning rate of 0.1, momentum of 0.9 and weight decay of 1 \u00d7 10\u22124. BN-ViT Model Structure. To give a clear illustration of incorporating BN layers into ViT, we provide more details in this subsection. The definitions of vanilla ViT and BN-ViT are presented below to show the structure modification. The vanilla ViT can be formulated as: z\u2032 \u2113 = MHSA (LN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFN (LN (z\u2032 \u2113)) + z\u2032 \u2113 where z\u2032 \u2113 is the intermediate representation before Feed-forward Network (FFN), and z\u2113 is that after FFN and residual connection. FFN contains two linear layers with a GELU non-linearity in between them, i.e., FFN(z\u2032 \u2113) = (cid:0)GELU (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1) W 2 \u2113 + b2 \u2113 The newly constructed BN-ViT is: z\u2032 \u2113 = MHSA (BN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFNBN (BN (z\u2032 \u2113)) + z\u2032 \u2113 where we add one additional BN layer in-between two linear layers of FFN, i.e., FFNBN(z\u2032 \u2113) = (cid:0)GELU (cid:0)BN (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1)(cid:1) W 2 \u2113 + b2 \u2113 We follow the DeiT official training recipe to train a DeiT-Tiny-BN model for 300 epochs with an AdamW optimizer, cosine decayed learning rate of 5 \u00d7 10\u22124, weight decay of 0.05 and 5 warmup epochs. 15 Ablation Top-1 acc. (%) RT V R\u21132 Random Crop Tiny-ImageNet \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 29.87 29.92 30.11 30.30 37.88 ImageNet-1K 22.92 23.15 40.81 40.37 46.71 Table 8: Top-1 validation accuracy under regularization ablation settings. ResNet-18 is used in all three stages with the relabeling temperature \u03c4 = 20. A.3 Recovering Details Regularization Terms. We conduct a large number of ablation experiments under varying regular- ization term conditions, as illustrated in Table 8. The two image prior regularizers, \u21132 regularization and total variation (TV), are not anticipated to enhance validation accuracy as our primary focus is on information recovery rather than image smoothness. Consequently, we exclude these two regularization terms from our experiments."}], "doc_text": "in the main text, including: Section A: Implementation Details. Section B: Low-Resolution Data. Section C: Feature Embedding Distribution. Section D: Theoretical Analysis. Section E: More Visualization of Synthetic Data. A Implementation Details A.1 Dataset Statistics Table 6 enumerates various permutations of ImageNet-1K training set, delineated according to their individual configurations. Tiny-ImageNet [12] incorporates 200 classes derived from ImageNet-1K, with each class comprising 500 images possessing a resolution of 64\u00d764. Ima- geNette/ImageWoof [37] (alternatively referred to as subsets of ImageNet) include 10 classes from analogous subcategories, with each image having a resolution of 112\u00d7112. The MTT [1] framework introduces additional 10-class subsets of ImageNet, encompassing ImageFruit, ImageSquawk, Image- Meow, ImageBlub, and ImageYellow. ImageNet-10/100 [38] samples 10/100 classes from ImageNet while maintaining an image resolution of 224\u00d7224. Downsampled ImageNet-1K rescales the entirety of ImageNet data to a resolution of 64\u00d764. In our experiments, we choose two standard datasets of relatively large scale: Tiny-ImageNet and the full ImageNet-1K. Training Dataset #Class #Img per class Resolution Method Tiny-ImageNet [12] ImageNette/ImageWoof [37] ImageNet-10/100 [38] Downsampled ImageNet-1K [13] Full ImageNet-1K [24] 200 10 10/100 1,000 1,000 500 \u223c1,000 \u223c1,200 \u223c1,200 \u223c1,200 MTT [1], FRePo [5], DM [10], SRe2L (Ours) 64\u00d764 112\u00d7112 MTT [1], FRePo [5] 224\u00d7224 IDC [7] 64\u00d764 TESLA [11], DM [10] SRe2L (Ours) 224\u00d7224 Table 6: Variants of ImageNet-1K training set with different configurations. A.2 Squeezing Details Data Augmentation. Table 2 in the main paper illustrates that the utilization of data augmentation approaches during the squeezing phase contributes to a decrease in the final accuracy of the recovered data. To summarize, the results on Tiny-ImageNet indicate that lengthening the training budget and the application of more data augmentations in the squeezing phase intensify the intricacy involved in data recovery from the compressed model, which is not desired. Parallel conclusions are inferred from the compressed models for the ImageNet-1K dataset. For our experimental setup, we aimed to extract data from a pre-trained ResNet-50 model with available V1 and V2 weights in the PyTorch model zoo. The results propose that the task of data extraction poses a greater challenge from the ResNet-50 model equipped with V2 weights as compared to the model incorporating V1 weights. This can be attributed to the fact that models utilizing V1 weights are trained employing a rudimentary recipe, whereas models with V2 weights encompass numerous training enhancements, such as more training budget and data augmentations, to achieve cutting-edge performance. It is observed that these additional complexities impede the data recovery process even the pre-trained models are stronger. Therefore, the pre-trained models we employ for the recovery of ImageNet-1K are those integrating V1 weights from the PyTorch model zoo. Hyper-parameter Setting. We provide hyper-parameter settings for the two datasets in detail. Tiny-ImageNet: We train modified ResNet-{18, 50} models on Tiny-ImageNet data with the parameter setting in Table 7a. The well-trained ResNet-{18, 50} models achieve Top-1 accuracy of {59.47%, 61.17%} under the 50 epoch training budget. 14 config value config value optimizer base learning rate weight decay optimizer momentum 0.9 256 batch size learning rate schedule cosine decay 100 training epoch RandomResizedCrop augmentation SGD 0.2 1e-4 optimizer base learning rate weight decay optimizer momentum \u03b21, \u03b22 = 0.9, 0.999 batch size learning rate schedule training epoch augmentation AdamW 0.001 0.01 1,024 cosine decay 300 RandomResizedCrop (a) Tiny-ImageNet squeezing setting. (b) ImageNet-1K validation setting. config value config value \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 1.0 Adam 0.1 100 cosine decay 1,000 RandomResizedCrop \u03b1BN optimizer base learning rate optimizer momentum \u03b21, \u03b22 = 0.5, 0.9 batch size learning rate schedule recovering iteration augmentation 0.01 Adam 0.25 100 cosine decay 2,000 RandomResizedCrop (c) Tiny-ImageNet recovering setting. (d) ImageNet-1K recovering setting. Table 7: Hyper-parameter settings in three stages. ImageNet-1K: We use PyTorch off-the-shelf ResNet-{18, 50} with V1 weights and Top-1 accuracy of {69.76%, 76.13%} as squeezed/condensed models. In the original training script [39], ResNet models are trained for 90 epochs with a SGD optimizer, learning rate of 0.1, momentum of 0.9 and weight decay of 1 \u00d7 10\u22124. BN-ViT Model Structure. To give a clear illustration of incorporating BN layers into ViT, we provide more details in this subsection. The definitions of vanilla ViT and BN-ViT are presented below to show the structure modification. The vanilla ViT can be formulated as: z\u2032 \u2113 = MHSA (LN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFN (LN (z\u2032 \u2113)) + z\u2032 \u2113 where z\u2032 \u2113 is the intermediate representation before Feed-forward Network (FFN), and z\u2113 is that after FFN and residual connection. FFN contains two linear layers with a GELU non-linearity in between them, i.e., FFN(z\u2032 \u2113) = (cid:0)GELU (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1) W 2 \u2113 + b2 \u2113 The newly constructed BN-ViT is: z\u2032 \u2113 = MHSA (BN (z\u2113\u22121)) + z\u2113\u22121 z\u2113 = FFNBN (BN (z\u2032 \u2113)) + z\u2032 \u2113 where we add one additional BN layer in-between two linear layers of FFN, i.e., FFNBN(z\u2032 \u2113) = (cid:0)GELU (cid:0)BN (cid:0)z\u2032 \u2113W 1 \u2113 + b1 \u2113 (cid:1)(cid:1)(cid:1) W 2 \u2113 + b2 \u2113 We follow the DeiT official training recipe to train a DeiT-Tiny-BN model for 300 epochs with an AdamW optimizer, cosine decayed learning rate of 5 \u00d7 10\u22124, weight decay of 0.05 and 5 warmup epochs. 15 Ablation Top-1 acc. (%) RT V R\u21132 Random Crop Tiny-ImageNet \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 29.87 29.92 30.11 30.30 37.88 ImageNet-1K 22.92 23.15 40.81 40.37 46.71 Table 8: Top-1 validation accuracy under regularization ablation settings. ResNet-18 is used in all three stages with the relabeling temperature \u03c4 = 20. A.3 Recovering Details Regularization Terms. We conduct a large number of ablation experiments under varying regular- ization term conditions, as illustrated in Table 8. The two image prior regularizers, \u21132 regularization and total variation (TV), are not anticipated to enhance validation accuracy as our primary focus is on information recovery rather than image smoothness. Consequently, we exclude these two regularization terms from our experiments."}