{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Alexander_I._Rudnicky_Learning_to_Ask_Questions_for_Zero-shot_Dialogue_State_Tracking_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the importance of being able to classify questions as unanswerable in QA models for DST?", "answer": " It is crucial for maintaining good performance, as error propagation caused by misclassifications can lead to a rapid decrease in joint goal accuracy.", "ref_chunk": "each element is extracted from as its context paragraph. We use greedy decoding to generate each ques- tion. QA models for DST require the ability to classify questions as unanswerable, as each utterance will generally only mention a small fraction of slots. This is crucial for maintaining good perfor- mance, as error propagation caused by misclassifications can lead to a rapid decrease in joint goal accuracy. In the out-of-domain pre- training step, we use the unanswerable SQuAD question set. In the in-domain step, we apply each question \ud835\udc44\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 to the next dialogue in the training set, if the dialogue does not explicitly contain the corresponding answer \ud835\udc34\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 . We illustrate this strategy in Figure 1. 3.2 Slot-based Question Formulation (\ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 ) We will refer to any information in a slot-key that is not its domain as a slot type. For instance, given the slot-key hotel-pricerange, the domain is hotel and the type is pricerange. To study the impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 on model performance, we experiment with its formulation in a number of ways.The strategies include template-based, LLM-based, and handcrafted approaches. 3.2.1 Template. This approach relies on the following templates: \"what is the <slot key>?\" and \"what is the value of the slot <slot key> ?\" [7]\u2014referred to as what-is and simple, respectively. 3.2.2 LLM. We generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 by prompting a Large Language Model (LLM) so that a natural language question is composed for each slot type. This reduces the human bias inherent to manually generating slot questions [14] or descriptions [16, 26]. An immedi- ate pitfall lies in language models not explicitly containing domain knowledge. This may cause the questions to misrepresent slot se- mantics when those are not immediately clear from the type. We define two LLM prompt strategies for question generation: LLM, which simply prompts all questions to be generated, and Pronouns, in which the LLM prompt requires the pronouns in the question to be in the third person, and refer to \"the user\", whenever needed. 3.2.3 Handcrafted. We also manually created a set of questions that contain explicit domain knowledge, alongside explicitly enforcing semantically similar slots to have similar questions. The resulting set is composed of questions less abstract and more meaningful to the dialogue than those in template-based approaches [6]. Implementation Details. When using span-based QA mod- 3.2.4 els, we prepend { Yes, No, Dontcare } to each question [18]. This allows the model to respond to questions pertaining to service- based slots (namely, hotel-internet, hotel-parking) with the required \"yes\" and \"no\". Furthermore, this facilitates assigning the commonly 2https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan used \"dontcare\" value to questions. We initialize the embedding of the Dontcare token to the average of embeddings in \"do not care\". We format the input of generative models with the \"question: \ud835\udc5e\ud835\udc56 context: \ud835\udc37\" prompt. 3.3 Reading Comprehension Fine-Tuning We use the MultiWOZ 2.1 [5] dataset for inference and fine-tuning. We fine-tune our models following the TRADE [23] pre and post- processing methodology, as recommended by the MultiWOZ au- thors, to ensure our results are directly comparable to the literature. We train and evaluate following a cross-domain adaptation setting, meaning we train on 4 dataset domains, and evaluate on the unseen domain data, with the proposed reading comprehension approach. Given that MultiWOZ is composed of 5 domains, we train a total of 5 instances of each model. We evaluate using the joint goal ac- curacy metric on the target slots, and use the average of the five runs as the main comparison metric. We use RoBERTa-base as our backbone span-based QA model and T5-base as our backbone gen- erative model. We generate the LLM and Pronouns \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 using a single InstructGPT [19] input. Unless otherwise stated, we use the LLM \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategy in our experiments. 4 RESULTS AND DISCUSSION 4.1 Baselines We compare our performance with the following approaches: TRADE [23] and MA-DST [13], which use RNNs and slot names as input; Li et al. [14] manually write questions and use a GPT-2 model for decoding the slot values. T5-DST [16], which uses slot descriptions with a T5-small model\u2014T5-DST (QA), one of the presented base- lines in T5-DST, uses automatic question generation. This model is evaluated on MultiWOZ 2.0, which may not make it directly comparable to our work. TransferQA [15] pretrains a T5-large model using several QA datasets and applies it to the DST task, without any in-domain training; D3ST [26] uses slot descriptions and a T5 model. Finally, SDT [9] uses a T5-XXL model, and requires dialogue examples to be manually written from analyzing the data. SDT always contains the input of a handwritten example, making it explicitly a one-shot approach. Li et al. [14], T5-DST, D3ST, and SDT require more domain expertise and a higher developer workload to express new slots: not only do they require manually creating slot descriptions or examples, but also explicitly listing all values of categorical slots\u2014which may not be trivial when adding new slots. While generating all the descriptions for D3ST and examples for SDT (in the SGD dataset [22]) takes 1.5 and 2 hours, respectively [9], we generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 automatically from only the slot keys, the minimal representation of a slot; thus requiring less data. 4.2 Zero-shot cross-domain adaptation Table 1 presents the results for the zero-shot domain transfer task. Each column represents an instance of the model trained on all domains except one, then evaluated on that domain. The metric is the standard per-domain joint goal accuracy (JGA). We also present the average JGA over all domains, which we use as our main means of comparison. We obtain SOTA performance versus models with comparable amounts of information (slot keys only), consistently outperforming them in most domains, oftentimes by large margins. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Table 1: Zero-shot cross-domain adaptation results. The top approaches rely on simple features, such as slot names. The bottom approaches rely on more complex features, such as slot descriptions or full examples. Model Hotel Rest. Taxi Attr. Train Fully zero-shot"}, {"question": " What type of questions does each utterance generally only mention a small fraction of slots?", "answer": " Unanswerable questions.", "ref_chunk": "each element is extracted from as its context paragraph. We use greedy decoding to generate each ques- tion. QA models for DST require the ability to classify questions as unanswerable, as each utterance will generally only mention a small fraction of slots. This is crucial for maintaining good perfor- mance, as error propagation caused by misclassifications can lead to a rapid decrease in joint goal accuracy. In the out-of-domain pre- training step, we use the unanswerable SQuAD question set. In the in-domain step, we apply each question \ud835\udc44\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 to the next dialogue in the training set, if the dialogue does not explicitly contain the corresponding answer \ud835\udc34\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 . We illustrate this strategy in Figure 1. 3.2 Slot-based Question Formulation (\ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 ) We will refer to any information in a slot-key that is not its domain as a slot type. For instance, given the slot-key hotel-pricerange, the domain is hotel and the type is pricerange. To study the impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 on model performance, we experiment with its formulation in a number of ways.The strategies include template-based, LLM-based, and handcrafted approaches. 3.2.1 Template. This approach relies on the following templates: \"what is the <slot key>?\" and \"what is the value of the slot <slot key> ?\" [7]\u2014referred to as what-is and simple, respectively. 3.2.2 LLM. We generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 by prompting a Large Language Model (LLM) so that a natural language question is composed for each slot type. This reduces the human bias inherent to manually generating slot questions [14] or descriptions [16, 26]. An immedi- ate pitfall lies in language models not explicitly containing domain knowledge. This may cause the questions to misrepresent slot se- mantics when those are not immediately clear from the type. We define two LLM prompt strategies for question generation: LLM, which simply prompts all questions to be generated, and Pronouns, in which the LLM prompt requires the pronouns in the question to be in the third person, and refer to \"the user\", whenever needed. 3.2.3 Handcrafted. We also manually created a set of questions that contain explicit domain knowledge, alongside explicitly enforcing semantically similar slots to have similar questions. The resulting set is composed of questions less abstract and more meaningful to the dialogue than those in template-based approaches [6]. Implementation Details. When using span-based QA mod- 3.2.4 els, we prepend { Yes, No, Dontcare } to each question [18]. This allows the model to respond to questions pertaining to service- based slots (namely, hotel-internet, hotel-parking) with the required \"yes\" and \"no\". Furthermore, this facilitates assigning the commonly 2https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan used \"dontcare\" value to questions. We initialize the embedding of the Dontcare token to the average of embeddings in \"do not care\". We format the input of generative models with the \"question: \ud835\udc5e\ud835\udc56 context: \ud835\udc37\" prompt. 3.3 Reading Comprehension Fine-Tuning We use the MultiWOZ 2.1 [5] dataset for inference and fine-tuning. We fine-tune our models following the TRADE [23] pre and post- processing methodology, as recommended by the MultiWOZ au- thors, to ensure our results are directly comparable to the literature. We train and evaluate following a cross-domain adaptation setting, meaning we train on 4 dataset domains, and evaluate on the unseen domain data, with the proposed reading comprehension approach. Given that MultiWOZ is composed of 5 domains, we train a total of 5 instances of each model. We evaluate using the joint goal ac- curacy metric on the target slots, and use the average of the five runs as the main comparison metric. We use RoBERTa-base as our backbone span-based QA model and T5-base as our backbone gen- erative model. We generate the LLM and Pronouns \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 using a single InstructGPT [19] input. Unless otherwise stated, we use the LLM \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategy in our experiments. 4 RESULTS AND DISCUSSION 4.1 Baselines We compare our performance with the following approaches: TRADE [23] and MA-DST [13], which use RNNs and slot names as input; Li et al. [14] manually write questions and use a GPT-2 model for decoding the slot values. T5-DST [16], which uses slot descriptions with a T5-small model\u2014T5-DST (QA), one of the presented base- lines in T5-DST, uses automatic question generation. This model is evaluated on MultiWOZ 2.0, which may not make it directly comparable to our work. TransferQA [15] pretrains a T5-large model using several QA datasets and applies it to the DST task, without any in-domain training; D3ST [26] uses slot descriptions and a T5 model. Finally, SDT [9] uses a T5-XXL model, and requires dialogue examples to be manually written from analyzing the data. SDT always contains the input of a handwritten example, making it explicitly a one-shot approach. Li et al. [14], T5-DST, D3ST, and SDT require more domain expertise and a higher developer workload to express new slots: not only do they require manually creating slot descriptions or examples, but also explicitly listing all values of categorical slots\u2014which may not be trivial when adding new slots. While generating all the descriptions for D3ST and examples for SDT (in the SGD dataset [22]) takes 1.5 and 2 hours, respectively [9], we generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 automatically from only the slot keys, the minimal representation of a slot; thus requiring less data. 4.2 Zero-shot cross-domain adaptation Table 1 presents the results for the zero-shot domain transfer task. Each column represents an instance of the model trained on all domains except one, then evaluated on that domain. The metric is the standard per-domain joint goal accuracy (JGA). We also present the average JGA over all domains, which we use as our main means of comparison. We obtain SOTA performance versus models with comparable amounts of information (slot keys only), consistently outperforming them in most domains, oftentimes by large margins. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Table 1: Zero-shot cross-domain adaptation results. The top approaches rely on simple features, such as slot names. The bottom approaches rely on more complex features, such as slot descriptions or full examples. Model Hotel Rest. Taxi Attr. Train Fully zero-shot"}, {"question": " What set of questions is used in the out-of-domain pre-training step for DST?", "answer": " Unanswerable SQuAD question set.", "ref_chunk": "each element is extracted from as its context paragraph. We use greedy decoding to generate each ques- tion. QA models for DST require the ability to classify questions as unanswerable, as each utterance will generally only mention a small fraction of slots. This is crucial for maintaining good perfor- mance, as error propagation caused by misclassifications can lead to a rapid decrease in joint goal accuracy. In the out-of-domain pre- training step, we use the unanswerable SQuAD question set. In the in-domain step, we apply each question \ud835\udc44\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 to the next dialogue in the training set, if the dialogue does not explicitly contain the corresponding answer \ud835\udc34\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 . We illustrate this strategy in Figure 1. 3.2 Slot-based Question Formulation (\ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 ) We will refer to any information in a slot-key that is not its domain as a slot type. For instance, given the slot-key hotel-pricerange, the domain is hotel and the type is pricerange. To study the impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 on model performance, we experiment with its formulation in a number of ways.The strategies include template-based, LLM-based, and handcrafted approaches. 3.2.1 Template. This approach relies on the following templates: \"what is the <slot key>?\" and \"what is the value of the slot <slot key> ?\" [7]\u2014referred to as what-is and simple, respectively. 3.2.2 LLM. We generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 by prompting a Large Language Model (LLM) so that a natural language question is composed for each slot type. This reduces the human bias inherent to manually generating slot questions [14] or descriptions [16, 26]. An immedi- ate pitfall lies in language models not explicitly containing domain knowledge. This may cause the questions to misrepresent slot se- mantics when those are not immediately clear from the type. We define two LLM prompt strategies for question generation: LLM, which simply prompts all questions to be generated, and Pronouns, in which the LLM prompt requires the pronouns in the question to be in the third person, and refer to \"the user\", whenever needed. 3.2.3 Handcrafted. We also manually created a set of questions that contain explicit domain knowledge, alongside explicitly enforcing semantically similar slots to have similar questions. The resulting set is composed of questions less abstract and more meaningful to the dialogue than those in template-based approaches [6]. Implementation Details. When using span-based QA mod- 3.2.4 els, we prepend { Yes, No, Dontcare } to each question [18]. This allows the model to respond to questions pertaining to service- based slots (namely, hotel-internet, hotel-parking) with the required \"yes\" and \"no\". Furthermore, this facilitates assigning the commonly 2https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan used \"dontcare\" value to questions. We initialize the embedding of the Dontcare token to the average of embeddings in \"do not care\". We format the input of generative models with the \"question: \ud835\udc5e\ud835\udc56 context: \ud835\udc37\" prompt. 3.3 Reading Comprehension Fine-Tuning We use the MultiWOZ 2.1 [5] dataset for inference and fine-tuning. We fine-tune our models following the TRADE [23] pre and post- processing methodology, as recommended by the MultiWOZ au- thors, to ensure our results are directly comparable to the literature. We train and evaluate following a cross-domain adaptation setting, meaning we train on 4 dataset domains, and evaluate on the unseen domain data, with the proposed reading comprehension approach. Given that MultiWOZ is composed of 5 domains, we train a total of 5 instances of each model. We evaluate using the joint goal ac- curacy metric on the target slots, and use the average of the five runs as the main comparison metric. We use RoBERTa-base as our backbone span-based QA model and T5-base as our backbone gen- erative model. We generate the LLM and Pronouns \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 using a single InstructGPT [19] input. Unless otherwise stated, we use the LLM \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategy in our experiments. 4 RESULTS AND DISCUSSION 4.1 Baselines We compare our performance with the following approaches: TRADE [23] and MA-DST [13], which use RNNs and slot names as input; Li et al. [14] manually write questions and use a GPT-2 model for decoding the slot values. T5-DST [16], which uses slot descriptions with a T5-small model\u2014T5-DST (QA), one of the presented base- lines in T5-DST, uses automatic question generation. This model is evaluated on MultiWOZ 2.0, which may not make it directly comparable to our work. TransferQA [15] pretrains a T5-large model using several QA datasets and applies it to the DST task, without any in-domain training; D3ST [26] uses slot descriptions and a T5 model. Finally, SDT [9] uses a T5-XXL model, and requires dialogue examples to be manually written from analyzing the data. SDT always contains the input of a handwritten example, making it explicitly a one-shot approach. Li et al. [14], T5-DST, D3ST, and SDT require more domain expertise and a higher developer workload to express new slots: not only do they require manually creating slot descriptions or examples, but also explicitly listing all values of categorical slots\u2014which may not be trivial when adding new slots. While generating all the descriptions for D3ST and examples for SDT (in the SGD dataset [22]) takes 1.5 and 2 hours, respectively [9], we generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 automatically from only the slot keys, the minimal representation of a slot; thus requiring less data. 4.2 Zero-shot cross-domain adaptation Table 1 presents the results for the zero-shot domain transfer task. Each column represents an instance of the model trained on all domains except one, then evaluated on that domain. The metric is the standard per-domain joint goal accuracy (JGA). We also present the average JGA over all domains, which we use as our main means of comparison. We obtain SOTA performance versus models with comparable amounts of information (slot keys only), consistently outperforming them in most domains, oftentimes by large margins. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Table 1: Zero-shot cross-domain adaptation results. The top approaches rely on simple features, such as slot names. The bottom approaches rely on more complex features, such as slot descriptions or full examples. Model Hotel Rest. Taxi Attr. Train Fully zero-shot"}, {"question": " What strategies are used for slot-based question formulation?", "answer": " Template-based, LLM-based, and handcrafted approaches.", "ref_chunk": "each element is extracted from as its context paragraph. We use greedy decoding to generate each ques- tion. QA models for DST require the ability to classify questions as unanswerable, as each utterance will generally only mention a small fraction of slots. This is crucial for maintaining good perfor- mance, as error propagation caused by misclassifications can lead to a rapid decrease in joint goal accuracy. In the out-of-domain pre- training step, we use the unanswerable SQuAD question set. In the in-domain step, we apply each question \ud835\udc44\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 to the next dialogue in the training set, if the dialogue does not explicitly contain the corresponding answer \ud835\udc34\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 . We illustrate this strategy in Figure 1. 3.2 Slot-based Question Formulation (\ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 ) We will refer to any information in a slot-key that is not its domain as a slot type. For instance, given the slot-key hotel-pricerange, the domain is hotel and the type is pricerange. To study the impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 on model performance, we experiment with its formulation in a number of ways.The strategies include template-based, LLM-based, and handcrafted approaches. 3.2.1 Template. This approach relies on the following templates: \"what is the <slot key>?\" and \"what is the value of the slot <slot key> ?\" [7]\u2014referred to as what-is and simple, respectively. 3.2.2 LLM. We generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 by prompting a Large Language Model (LLM) so that a natural language question is composed for each slot type. This reduces the human bias inherent to manually generating slot questions [14] or descriptions [16, 26]. An immedi- ate pitfall lies in language models not explicitly containing domain knowledge. This may cause the questions to misrepresent slot se- mantics when those are not immediately clear from the type. We define two LLM prompt strategies for question generation: LLM, which simply prompts all questions to be generated, and Pronouns, in which the LLM prompt requires the pronouns in the question to be in the third person, and refer to \"the user\", whenever needed. 3.2.3 Handcrafted. We also manually created a set of questions that contain explicit domain knowledge, alongside explicitly enforcing semantically similar slots to have similar questions. The resulting set is composed of questions less abstract and more meaningful to the dialogue than those in template-based approaches [6]. Implementation Details. When using span-based QA mod- 3.2.4 els, we prepend { Yes, No, Dontcare } to each question [18]. This allows the model to respond to questions pertaining to service- based slots (namely, hotel-internet, hotel-parking) with the required \"yes\" and \"no\". Furthermore, this facilitates assigning the commonly 2https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan used \"dontcare\" value to questions. We initialize the embedding of the Dontcare token to the average of embeddings in \"do not care\". We format the input of generative models with the \"question: \ud835\udc5e\ud835\udc56 context: \ud835\udc37\" prompt. 3.3 Reading Comprehension Fine-Tuning We use the MultiWOZ 2.1 [5] dataset for inference and fine-tuning. We fine-tune our models following the TRADE [23] pre and post- processing methodology, as recommended by the MultiWOZ au- thors, to ensure our results are directly comparable to the literature. We train and evaluate following a cross-domain adaptation setting, meaning we train on 4 dataset domains, and evaluate on the unseen domain data, with the proposed reading comprehension approach. Given that MultiWOZ is composed of 5 domains, we train a total of 5 instances of each model. We evaluate using the joint goal ac- curacy metric on the target slots, and use the average of the five runs as the main comparison metric. We use RoBERTa-base as our backbone span-based QA model and T5-base as our backbone gen- erative model. We generate the LLM and Pronouns \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 using a single InstructGPT [19] input. Unless otherwise stated, we use the LLM \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategy in our experiments. 4 RESULTS AND DISCUSSION 4.1 Baselines We compare our performance with the following approaches: TRADE [23] and MA-DST [13], which use RNNs and slot names as input; Li et al. [14] manually write questions and use a GPT-2 model for decoding the slot values. T5-DST [16], which uses slot descriptions with a T5-small model\u2014T5-DST (QA), one of the presented base- lines in T5-DST, uses automatic question generation. This model is evaluated on MultiWOZ 2.0, which may not make it directly comparable to our work. TransferQA [15] pretrains a T5-large model using several QA datasets and applies it to the DST task, without any in-domain training; D3ST [26] uses slot descriptions and a T5 model. Finally, SDT [9] uses a T5-XXL model, and requires dialogue examples to be manually written from analyzing the data. SDT always contains the input of a handwritten example, making it explicitly a one-shot approach. Li et al. [14], T5-DST, D3ST, and SDT require more domain expertise and a higher developer workload to express new slots: not only do they require manually creating slot descriptions or examples, but also explicitly listing all values of categorical slots\u2014which may not be trivial when adding new slots. While generating all the descriptions for D3ST and examples for SDT (in the SGD dataset [22]) takes 1.5 and 2 hours, respectively [9], we generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 automatically from only the slot keys, the minimal representation of a slot; thus requiring less data. 4.2 Zero-shot cross-domain adaptation Table 1 presents the results for the zero-shot domain transfer task. Each column represents an instance of the model trained on all domains except one, then evaluated on that domain. The metric is the standard per-domain joint goal accuracy (JGA). We also present the average JGA over all domains, which we use as our main means of comparison. We obtain SOTA performance versus models with comparable amounts of information (slot keys only), consistently outperforming them in most domains, oftentimes by large margins. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Table 1: Zero-shot cross-domain adaptation results. The top approaches rely on simple features, such as slot names. The bottom approaches rely on more complex features, such as slot descriptions or full examples. Model Hotel Rest. Taxi Attr. Train Fully zero-shot"}, {"question": " In the LLM-based approach, what reduces the human bias in generating slot questions?", "answer": " Prompting a Large Language Model (LLM) to compose natural language questions for each slot type.", "ref_chunk": "each element is extracted from as its context paragraph. We use greedy decoding to generate each ques- tion. QA models for DST require the ability to classify questions as unanswerable, as each utterance will generally only mention a small fraction of slots. This is crucial for maintaining good perfor- mance, as error propagation caused by misclassifications can lead to a rapid decrease in joint goal accuracy. In the out-of-domain pre- training step, we use the unanswerable SQuAD question set. In the in-domain step, we apply each question \ud835\udc44\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 to the next dialogue in the training set, if the dialogue does not explicitly contain the corresponding answer \ud835\udc34\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 . We illustrate this strategy in Figure 1. 3.2 Slot-based Question Formulation (\ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 ) We will refer to any information in a slot-key that is not its domain as a slot type. For instance, given the slot-key hotel-pricerange, the domain is hotel and the type is pricerange. To study the impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 on model performance, we experiment with its formulation in a number of ways.The strategies include template-based, LLM-based, and handcrafted approaches. 3.2.1 Template. This approach relies on the following templates: \"what is the <slot key>?\" and \"what is the value of the slot <slot key> ?\" [7]\u2014referred to as what-is and simple, respectively. 3.2.2 LLM. We generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 by prompting a Large Language Model (LLM) so that a natural language question is composed for each slot type. This reduces the human bias inherent to manually generating slot questions [14] or descriptions [16, 26]. An immedi- ate pitfall lies in language models not explicitly containing domain knowledge. This may cause the questions to misrepresent slot se- mantics when those are not immediately clear from the type. We define two LLM prompt strategies for question generation: LLM, which simply prompts all questions to be generated, and Pronouns, in which the LLM prompt requires the pronouns in the question to be in the third person, and refer to \"the user\", whenever needed. 3.2.3 Handcrafted. We also manually created a set of questions that contain explicit domain knowledge, alongside explicitly enforcing semantically similar slots to have similar questions. The resulting set is composed of questions less abstract and more meaningful to the dialogue than those in template-based approaches [6]. Implementation Details. When using span-based QA mod- 3.2.4 els, we prepend { Yes, No, Dontcare } to each question [18]. This allows the model to respond to questions pertaining to service- based slots (namely, hotel-internet, hotel-parking) with the required \"yes\" and \"no\". Furthermore, this facilitates assigning the commonly 2https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan used \"dontcare\" value to questions. We initialize the embedding of the Dontcare token to the average of embeddings in \"do not care\". We format the input of generative models with the \"question: \ud835\udc5e\ud835\udc56 context: \ud835\udc37\" prompt. 3.3 Reading Comprehension Fine-Tuning We use the MultiWOZ 2.1 [5] dataset for inference and fine-tuning. We fine-tune our models following the TRADE [23] pre and post- processing methodology, as recommended by the MultiWOZ au- thors, to ensure our results are directly comparable to the literature. We train and evaluate following a cross-domain adaptation setting, meaning we train on 4 dataset domains, and evaluate on the unseen domain data, with the proposed reading comprehension approach. Given that MultiWOZ is composed of 5 domains, we train a total of 5 instances of each model. We evaluate using the joint goal ac- curacy metric on the target slots, and use the average of the five runs as the main comparison metric. We use RoBERTa-base as our backbone span-based QA model and T5-base as our backbone gen- erative model. We generate the LLM and Pronouns \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 using a single InstructGPT [19] input. Unless otherwise stated, we use the LLM \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategy in our experiments. 4 RESULTS AND DISCUSSION 4.1 Baselines We compare our performance with the following approaches: TRADE [23] and MA-DST [13], which use RNNs and slot names as input; Li et al. [14] manually write questions and use a GPT-2 model for decoding the slot values. T5-DST [16], which uses slot descriptions with a T5-small model\u2014T5-DST (QA), one of the presented base- lines in T5-DST, uses automatic question generation. This model is evaluated on MultiWOZ 2.0, which may not make it directly comparable to our work. TransferQA [15] pretrains a T5-large model using several QA datasets and applies it to the DST task, without any in-domain training; D3ST [26] uses slot descriptions and a T5 model. Finally, SDT [9] uses a T5-XXL model, and requires dialogue examples to be manually written from analyzing the data. SDT always contains the input of a handwritten example, making it explicitly a one-shot approach. Li et al. [14], T5-DST, D3ST, and SDT require more domain expertise and a higher developer workload to express new slots: not only do they require manually creating slot descriptions or examples, but also explicitly listing all values of categorical slots\u2014which may not be trivial when adding new slots. While generating all the descriptions for D3ST and examples for SDT (in the SGD dataset [22]) takes 1.5 and 2 hours, respectively [9], we generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 automatically from only the slot keys, the minimal representation of a slot; thus requiring less data. 4.2 Zero-shot cross-domain adaptation Table 1 presents the results for the zero-shot domain transfer task. Each column represents an instance of the model trained on all domains except one, then evaluated on that domain. The metric is the standard per-domain joint goal accuracy (JGA). We also present the average JGA over all domains, which we use as our main means of comparison. We obtain SOTA performance versus models with comparable amounts of information (slot keys only), consistently outperforming them in most domains, oftentimes by large margins. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Table 1: Zero-shot cross-domain adaptation results. The top approaches rely on simple features, such as slot names. The bottom approaches rely on more complex features, such as slot descriptions or full examples. Model Hotel Rest. Taxi Attr. Train Fully zero-shot"}, {"question": " How do models respond to questions pertaining to service-based slots in span-based QA?", "answer": " By prepending {Yes, No, Dontcare} to each question.", "ref_chunk": "each element is extracted from as its context paragraph. We use greedy decoding to generate each ques- tion. QA models for DST require the ability to classify questions as unanswerable, as each utterance will generally only mention a small fraction of slots. This is crucial for maintaining good perfor- mance, as error propagation caused by misclassifications can lead to a rapid decrease in joint goal accuracy. In the out-of-domain pre- training step, we use the unanswerable SQuAD question set. In the in-domain step, we apply each question \ud835\udc44\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 to the next dialogue in the training set, if the dialogue does not explicitly contain the corresponding answer \ud835\udc34\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 . We illustrate this strategy in Figure 1. 3.2 Slot-based Question Formulation (\ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 ) We will refer to any information in a slot-key that is not its domain as a slot type. For instance, given the slot-key hotel-pricerange, the domain is hotel and the type is pricerange. To study the impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 on model performance, we experiment with its formulation in a number of ways.The strategies include template-based, LLM-based, and handcrafted approaches. 3.2.1 Template. This approach relies on the following templates: \"what is the <slot key>?\" and \"what is the value of the slot <slot key> ?\" [7]\u2014referred to as what-is and simple, respectively. 3.2.2 LLM. We generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 by prompting a Large Language Model (LLM) so that a natural language question is composed for each slot type. This reduces the human bias inherent to manually generating slot questions [14] or descriptions [16, 26]. An immedi- ate pitfall lies in language models not explicitly containing domain knowledge. This may cause the questions to misrepresent slot se- mantics when those are not immediately clear from the type. We define two LLM prompt strategies for question generation: LLM, which simply prompts all questions to be generated, and Pronouns, in which the LLM prompt requires the pronouns in the question to be in the third person, and refer to \"the user\", whenever needed. 3.2.3 Handcrafted. We also manually created a set of questions that contain explicit domain knowledge, alongside explicitly enforcing semantically similar slots to have similar questions. The resulting set is composed of questions less abstract and more meaningful to the dialogue than those in template-based approaches [6]. Implementation Details. When using span-based QA mod- 3.2.4 els, we prepend { Yes, No, Dontcare } to each question [18]. This allows the model to respond to questions pertaining to service- based slots (namely, hotel-internet, hotel-parking) with the required \"yes\" and \"no\". Furthermore, this facilitates assigning the commonly 2https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan used \"dontcare\" value to questions. We initialize the embedding of the Dontcare token to the average of embeddings in \"do not care\". We format the input of generative models with the \"question: \ud835\udc5e\ud835\udc56 context: \ud835\udc37\" prompt. 3.3 Reading Comprehension Fine-Tuning We use the MultiWOZ 2.1 [5] dataset for inference and fine-tuning. We fine-tune our models following the TRADE [23] pre and post- processing methodology, as recommended by the MultiWOZ au- thors, to ensure our results are directly comparable to the literature. We train and evaluate following a cross-domain adaptation setting, meaning we train on 4 dataset domains, and evaluate on the unseen domain data, with the proposed reading comprehension approach. Given that MultiWOZ is composed of 5 domains, we train a total of 5 instances of each model. We evaluate using the joint goal ac- curacy metric on the target slots, and use the average of the five runs as the main comparison metric. We use RoBERTa-base as our backbone span-based QA model and T5-base as our backbone gen- erative model. We generate the LLM and Pronouns \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 using a single InstructGPT [19] input. Unless otherwise stated, we use the LLM \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategy in our experiments. 4 RESULTS AND DISCUSSION 4.1 Baselines We compare our performance with the following approaches: TRADE [23] and MA-DST [13], which use RNNs and slot names as input; Li et al. [14] manually write questions and use a GPT-2 model for decoding the slot values. T5-DST [16], which uses slot descriptions with a T5-small model\u2014T5-DST (QA), one of the presented base- lines in T5-DST, uses automatic question generation. This model is evaluated on MultiWOZ 2.0, which may not make it directly comparable to our work. TransferQA [15] pretrains a T5-large model using several QA datasets and applies it to the DST task, without any in-domain training; D3ST [26] uses slot descriptions and a T5 model. Finally, SDT [9] uses a T5-XXL model, and requires dialogue examples to be manually written from analyzing the data. SDT always contains the input of a handwritten example, making it explicitly a one-shot approach. Li et al. [14], T5-DST, D3ST, and SDT require more domain expertise and a higher developer workload to express new slots: not only do they require manually creating slot descriptions or examples, but also explicitly listing all values of categorical slots\u2014which may not be trivial when adding new slots. While generating all the descriptions for D3ST and examples for SDT (in the SGD dataset [22]) takes 1.5 and 2 hours, respectively [9], we generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 automatically from only the slot keys, the minimal representation of a slot; thus requiring less data. 4.2 Zero-shot cross-domain adaptation Table 1 presents the results for the zero-shot domain transfer task. Each column represents an instance of the model trained on all domains except one, then evaluated on that domain. The metric is the standard per-domain joint goal accuracy (JGA). We also present the average JGA over all domains, which we use as our main means of comparison. We obtain SOTA performance versus models with comparable amounts of information (slot keys only), consistently outperforming them in most domains, oftentimes by large margins. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Table 1: Zero-shot cross-domain adaptation results. The top approaches rely on simple features, such as slot names. The bottom approaches rely on more complex features, such as slot descriptions or full examples. Model Hotel Rest. Taxi Attr. Train Fully zero-shot"}, {"question": " What dataset is used for inference and fine-tuning in the Reading Comprehension Fine-Tuning step?", "answer": " MultiWOZ 2.1 dataset.", "ref_chunk": "each element is extracted from as its context paragraph. We use greedy decoding to generate each ques- tion. QA models for DST require the ability to classify questions as unanswerable, as each utterance will generally only mention a small fraction of slots. This is crucial for maintaining good perfor- mance, as error propagation caused by misclassifications can lead to a rapid decrease in joint goal accuracy. In the out-of-domain pre- training step, we use the unanswerable SQuAD question set. In the in-domain step, we apply each question \ud835\udc44\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 to the next dialogue in the training set, if the dialogue does not explicitly contain the corresponding answer \ud835\udc34\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 . We illustrate this strategy in Figure 1. 3.2 Slot-based Question Formulation (\ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 ) We will refer to any information in a slot-key that is not its domain as a slot type. For instance, given the slot-key hotel-pricerange, the domain is hotel and the type is pricerange. To study the impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 on model performance, we experiment with its formulation in a number of ways.The strategies include template-based, LLM-based, and handcrafted approaches. 3.2.1 Template. This approach relies on the following templates: \"what is the <slot key>?\" and \"what is the value of the slot <slot key> ?\" [7]\u2014referred to as what-is and simple, respectively. 3.2.2 LLM. We generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 by prompting a Large Language Model (LLM) so that a natural language question is composed for each slot type. This reduces the human bias inherent to manually generating slot questions [14] or descriptions [16, 26]. An immedi- ate pitfall lies in language models not explicitly containing domain knowledge. This may cause the questions to misrepresent slot se- mantics when those are not immediately clear from the type. We define two LLM prompt strategies for question generation: LLM, which simply prompts all questions to be generated, and Pronouns, in which the LLM prompt requires the pronouns in the question to be in the third person, and refer to \"the user\", whenever needed. 3.2.3 Handcrafted. We also manually created a set of questions that contain explicit domain knowledge, alongside explicitly enforcing semantically similar slots to have similar questions. The resulting set is composed of questions less abstract and more meaningful to the dialogue than those in template-based approaches [6]. Implementation Details. When using span-based QA mod- 3.2.4 els, we prepend { Yes, No, Dontcare } to each question [18]. This allows the model to respond to questions pertaining to service- based slots (namely, hotel-internet, hotel-parking) with the required \"yes\" and \"no\". Furthermore, this facilitates assigning the commonly 2https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan used \"dontcare\" value to questions. We initialize the embedding of the Dontcare token to the average of embeddings in \"do not care\". We format the input of generative models with the \"question: \ud835\udc5e\ud835\udc56 context: \ud835\udc37\" prompt. 3.3 Reading Comprehension Fine-Tuning We use the MultiWOZ 2.1 [5] dataset for inference and fine-tuning. We fine-tune our models following the TRADE [23] pre and post- processing methodology, as recommended by the MultiWOZ au- thors, to ensure our results are directly comparable to the literature. We train and evaluate following a cross-domain adaptation setting, meaning we train on 4 dataset domains, and evaluate on the unseen domain data, with the proposed reading comprehension approach. Given that MultiWOZ is composed of 5 domains, we train a total of 5 instances of each model. We evaluate using the joint goal ac- curacy metric on the target slots, and use the average of the five runs as the main comparison metric. We use RoBERTa-base as our backbone span-based QA model and T5-base as our backbone gen- erative model. We generate the LLM and Pronouns \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 using a single InstructGPT [19] input. Unless otherwise stated, we use the LLM \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategy in our experiments. 4 RESULTS AND DISCUSSION 4.1 Baselines We compare our performance with the following approaches: TRADE [23] and MA-DST [13], which use RNNs and slot names as input; Li et al. [14] manually write questions and use a GPT-2 model for decoding the slot values. T5-DST [16], which uses slot descriptions with a T5-small model\u2014T5-DST (QA), one of the presented base- lines in T5-DST, uses automatic question generation. This model is evaluated on MultiWOZ 2.0, which may not make it directly comparable to our work. TransferQA [15] pretrains a T5-large model using several QA datasets and applies it to the DST task, without any in-domain training; D3ST [26] uses slot descriptions and a T5 model. Finally, SDT [9] uses a T5-XXL model, and requires dialogue examples to be manually written from analyzing the data. SDT always contains the input of a handwritten example, making it explicitly a one-shot approach. Li et al. [14], T5-DST, D3ST, and SDT require more domain expertise and a higher developer workload to express new slots: not only do they require manually creating slot descriptions or examples, but also explicitly listing all values of categorical slots\u2014which may not be trivial when adding new slots. While generating all the descriptions for D3ST and examples for SDT (in the SGD dataset [22]) takes 1.5 and 2 hours, respectively [9], we generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 automatically from only the slot keys, the minimal representation of a slot; thus requiring less data. 4.2 Zero-shot cross-domain adaptation Table 1 presents the results for the zero-shot domain transfer task. Each column represents an instance of the model trained on all domains except one, then evaluated on that domain. The metric is the standard per-domain joint goal accuracy (JGA). We also present the average JGA over all domains, which we use as our main means of comparison. We obtain SOTA performance versus models with comparable amounts of information (slot keys only), consistently outperforming them in most domains, oftentimes by large margins. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Table 1: Zero-shot cross-domain adaptation results. The top approaches rely on simple features, such as slot names. The bottom approaches rely on more complex features, such as slot descriptions or full examples. Model Hotel Rest. Taxi Attr. Train Fully zero-shot"}, {"question": " What is the backbone span-based QA model used in the experiments?", "answer": " RoBERTa-base.", "ref_chunk": "each element is extracted from as its context paragraph. We use greedy decoding to generate each ques- tion. QA models for DST require the ability to classify questions as unanswerable, as each utterance will generally only mention a small fraction of slots. This is crucial for maintaining good perfor- mance, as error propagation caused by misclassifications can lead to a rapid decrease in joint goal accuracy. In the out-of-domain pre- training step, we use the unanswerable SQuAD question set. In the in-domain step, we apply each question \ud835\udc44\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 to the next dialogue in the training set, if the dialogue does not explicitly contain the corresponding answer \ud835\udc34\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 . We illustrate this strategy in Figure 1. 3.2 Slot-based Question Formulation (\ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 ) We will refer to any information in a slot-key that is not its domain as a slot type. For instance, given the slot-key hotel-pricerange, the domain is hotel and the type is pricerange. To study the impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 on model performance, we experiment with its formulation in a number of ways.The strategies include template-based, LLM-based, and handcrafted approaches. 3.2.1 Template. This approach relies on the following templates: \"what is the <slot key>?\" and \"what is the value of the slot <slot key> ?\" [7]\u2014referred to as what-is and simple, respectively. 3.2.2 LLM. We generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 by prompting a Large Language Model (LLM) so that a natural language question is composed for each slot type. This reduces the human bias inherent to manually generating slot questions [14] or descriptions [16, 26]. An immedi- ate pitfall lies in language models not explicitly containing domain knowledge. This may cause the questions to misrepresent slot se- mantics when those are not immediately clear from the type. We define two LLM prompt strategies for question generation: LLM, which simply prompts all questions to be generated, and Pronouns, in which the LLM prompt requires the pronouns in the question to be in the third person, and refer to \"the user\", whenever needed. 3.2.3 Handcrafted. We also manually created a set of questions that contain explicit domain knowledge, alongside explicitly enforcing semantically similar slots to have similar questions. The resulting set is composed of questions less abstract and more meaningful to the dialogue than those in template-based approaches [6]. Implementation Details. When using span-based QA mod- 3.2.4 els, we prepend { Yes, No, Dontcare } to each question [18]. This allows the model to respond to questions pertaining to service- based slots (namely, hotel-internet, hotel-parking) with the required \"yes\" and \"no\". Furthermore, this facilitates assigning the commonly 2https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan used \"dontcare\" value to questions. We initialize the embedding of the Dontcare token to the average of embeddings in \"do not care\". We format the input of generative models with the \"question: \ud835\udc5e\ud835\udc56 context: \ud835\udc37\" prompt. 3.3 Reading Comprehension Fine-Tuning We use the MultiWOZ 2.1 [5] dataset for inference and fine-tuning. We fine-tune our models following the TRADE [23] pre and post- processing methodology, as recommended by the MultiWOZ au- thors, to ensure our results are directly comparable to the literature. We train and evaluate following a cross-domain adaptation setting, meaning we train on 4 dataset domains, and evaluate on the unseen domain data, with the proposed reading comprehension approach. Given that MultiWOZ is composed of 5 domains, we train a total of 5 instances of each model. We evaluate using the joint goal ac- curacy metric on the target slots, and use the average of the five runs as the main comparison metric. We use RoBERTa-base as our backbone span-based QA model and T5-base as our backbone gen- erative model. We generate the LLM and Pronouns \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 using a single InstructGPT [19] input. Unless otherwise stated, we use the LLM \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategy in our experiments. 4 RESULTS AND DISCUSSION 4.1 Baselines We compare our performance with the following approaches: TRADE [23] and MA-DST [13], which use RNNs and slot names as input; Li et al. [14] manually write questions and use a GPT-2 model for decoding the slot values. T5-DST [16], which uses slot descriptions with a T5-small model\u2014T5-DST (QA), one of the presented base- lines in T5-DST, uses automatic question generation. This model is evaluated on MultiWOZ 2.0, which may not make it directly comparable to our work. TransferQA [15] pretrains a T5-large model using several QA datasets and applies it to the DST task, without any in-domain training; D3ST [26] uses slot descriptions and a T5 model. Finally, SDT [9] uses a T5-XXL model, and requires dialogue examples to be manually written from analyzing the data. SDT always contains the input of a handwritten example, making it explicitly a one-shot approach. Li et al. [14], T5-DST, D3ST, and SDT require more domain expertise and a higher developer workload to express new slots: not only do they require manually creating slot descriptions or examples, but also explicitly listing all values of categorical slots\u2014which may not be trivial when adding new slots. While generating all the descriptions for D3ST and examples for SDT (in the SGD dataset [22]) takes 1.5 and 2 hours, respectively [9], we generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 automatically from only the slot keys, the minimal representation of a slot; thus requiring less data. 4.2 Zero-shot cross-domain adaptation Table 1 presents the results for the zero-shot domain transfer task. Each column represents an instance of the model trained on all domains except one, then evaluated on that domain. The metric is the standard per-domain joint goal accuracy (JGA). We also present the average JGA over all domains, which we use as our main means of comparison. We obtain SOTA performance versus models with comparable amounts of information (slot keys only), consistently outperforming them in most domains, oftentimes by large margins. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Table 1: Zero-shot cross-domain adaptation results. The top approaches rely on simple features, such as slot names. The bottom approaches rely on more complex features, such as slot descriptions or full examples. Model Hotel Rest. Taxi Attr. Train Fully zero-shot"}, {"question": " Which approach requires more domain expertise and a higher developer workload when expressing new slots?", "answer": " Li et al., T5-DST, D3ST, and SDT.", "ref_chunk": "each element is extracted from as its context paragraph. We use greedy decoding to generate each ques- tion. QA models for DST require the ability to classify questions as unanswerable, as each utterance will generally only mention a small fraction of slots. This is crucial for maintaining good perfor- mance, as error propagation caused by misclassifications can lead to a rapid decrease in joint goal accuracy. In the out-of-domain pre- training step, we use the unanswerable SQuAD question set. In the in-domain step, we apply each question \ud835\udc44\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 to the next dialogue in the training set, if the dialogue does not explicitly contain the corresponding answer \ud835\udc34\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 . We illustrate this strategy in Figure 1. 3.2 Slot-based Question Formulation (\ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 ) We will refer to any information in a slot-key that is not its domain as a slot type. For instance, given the slot-key hotel-pricerange, the domain is hotel and the type is pricerange. To study the impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 on model performance, we experiment with its formulation in a number of ways.The strategies include template-based, LLM-based, and handcrafted approaches. 3.2.1 Template. This approach relies on the following templates: \"what is the <slot key>?\" and \"what is the value of the slot <slot key> ?\" [7]\u2014referred to as what-is and simple, respectively. 3.2.2 LLM. We generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 by prompting a Large Language Model (LLM) so that a natural language question is composed for each slot type. This reduces the human bias inherent to manually generating slot questions [14] or descriptions [16, 26]. An immedi- ate pitfall lies in language models not explicitly containing domain knowledge. This may cause the questions to misrepresent slot se- mantics when those are not immediately clear from the type. We define two LLM prompt strategies for question generation: LLM, which simply prompts all questions to be generated, and Pronouns, in which the LLM prompt requires the pronouns in the question to be in the third person, and refer to \"the user\", whenever needed. 3.2.3 Handcrafted. We also manually created a set of questions that contain explicit domain knowledge, alongside explicitly enforcing semantically similar slots to have similar questions. The resulting set is composed of questions less abstract and more meaningful to the dialogue than those in template-based approaches [6]. Implementation Details. When using span-based QA mod- 3.2.4 els, we prepend { Yes, No, Dontcare } to each question [18]. This allows the model to respond to questions pertaining to service- based slots (namely, hotel-internet, hotel-parking) with the required \"yes\" and \"no\". Furthermore, this facilitates assigning the commonly 2https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan used \"dontcare\" value to questions. We initialize the embedding of the Dontcare token to the average of embeddings in \"do not care\". We format the input of generative models with the \"question: \ud835\udc5e\ud835\udc56 context: \ud835\udc37\" prompt. 3.3 Reading Comprehension Fine-Tuning We use the MultiWOZ 2.1 [5] dataset for inference and fine-tuning. We fine-tune our models following the TRADE [23] pre and post- processing methodology, as recommended by the MultiWOZ au- thors, to ensure our results are directly comparable to the literature. We train and evaluate following a cross-domain adaptation setting, meaning we train on 4 dataset domains, and evaluate on the unseen domain data, with the proposed reading comprehension approach. Given that MultiWOZ is composed of 5 domains, we train a total of 5 instances of each model. We evaluate using the joint goal ac- curacy metric on the target slots, and use the average of the five runs as the main comparison metric. We use RoBERTa-base as our backbone span-based QA model and T5-base as our backbone gen- erative model. We generate the LLM and Pronouns \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 using a single InstructGPT [19] input. Unless otherwise stated, we use the LLM \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategy in our experiments. 4 RESULTS AND DISCUSSION 4.1 Baselines We compare our performance with the following approaches: TRADE [23] and MA-DST [13], which use RNNs and slot names as input; Li et al. [14] manually write questions and use a GPT-2 model for decoding the slot values. T5-DST [16], which uses slot descriptions with a T5-small model\u2014T5-DST (QA), one of the presented base- lines in T5-DST, uses automatic question generation. This model is evaluated on MultiWOZ 2.0, which may not make it directly comparable to our work. TransferQA [15] pretrains a T5-large model using several QA datasets and applies it to the DST task, without any in-domain training; D3ST [26] uses slot descriptions and a T5 model. Finally, SDT [9] uses a T5-XXL model, and requires dialogue examples to be manually written from analyzing the data. SDT always contains the input of a handwritten example, making it explicitly a one-shot approach. Li et al. [14], T5-DST, D3ST, and SDT require more domain expertise and a higher developer workload to express new slots: not only do they require manually creating slot descriptions or examples, but also explicitly listing all values of categorical slots\u2014which may not be trivial when adding new slots. While generating all the descriptions for D3ST and examples for SDT (in the SGD dataset [22]) takes 1.5 and 2 hours, respectively [9], we generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 automatically from only the slot keys, the minimal representation of a slot; thus requiring less data. 4.2 Zero-shot cross-domain adaptation Table 1 presents the results for the zero-shot domain transfer task. Each column represents an instance of the model trained on all domains except one, then evaluated on that domain. The metric is the standard per-domain joint goal accuracy (JGA). We also present the average JGA over all domains, which we use as our main means of comparison. We obtain SOTA performance versus models with comparable amounts of information (slot keys only), consistently outperforming them in most domains, oftentimes by large margins. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Table 1: Zero-shot cross-domain adaptation results. The top approaches rely on simple features, such as slot names. The bottom approaches rely on more complex features, such as slot descriptions or full examples. Model Hotel Rest. Taxi Attr. Train Fully zero-shot"}, {"question": " What is the main means of comparison in the zero-shot cross-domain adaptation task?", "answer": " The average Joint Goal Accuracy (JGA) over all domains.", "ref_chunk": "each element is extracted from as its context paragraph. We use greedy decoding to generate each ques- tion. QA models for DST require the ability to classify questions as unanswerable, as each utterance will generally only mention a small fraction of slots. This is crucial for maintaining good perfor- mance, as error propagation caused by misclassifications can lead to a rapid decrease in joint goal accuracy. In the out-of-domain pre- training step, we use the unanswerable SQuAD question set. In the in-domain step, we apply each question \ud835\udc44\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 to the next dialogue in the training set, if the dialogue does not explicitly contain the corresponding answer \ud835\udc34\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 . We illustrate this strategy in Figure 1. 3.2 Slot-based Question Formulation (\ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 ) We will refer to any information in a slot-key that is not its domain as a slot type. For instance, given the slot-key hotel-pricerange, the domain is hotel and the type is pricerange. To study the impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 on model performance, we experiment with its formulation in a number of ways.The strategies include template-based, LLM-based, and handcrafted approaches. 3.2.1 Template. This approach relies on the following templates: \"what is the <slot key>?\" and \"what is the value of the slot <slot key> ?\" [7]\u2014referred to as what-is and simple, respectively. 3.2.2 LLM. We generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 by prompting a Large Language Model (LLM) so that a natural language question is composed for each slot type. This reduces the human bias inherent to manually generating slot questions [14] or descriptions [16, 26]. An immedi- ate pitfall lies in language models not explicitly containing domain knowledge. This may cause the questions to misrepresent slot se- mantics when those are not immediately clear from the type. We define two LLM prompt strategies for question generation: LLM, which simply prompts all questions to be generated, and Pronouns, in which the LLM prompt requires the pronouns in the question to be in the third person, and refer to \"the user\", whenever needed. 3.2.3 Handcrafted. We also manually created a set of questions that contain explicit domain knowledge, alongside explicitly enforcing semantically similar slots to have similar questions. The resulting set is composed of questions less abstract and more meaningful to the dialogue than those in template-based approaches [6]. Implementation Details. When using span-based QA mod- 3.2.4 els, we prepend { Yes, No, Dontcare } to each question [18]. This allows the model to respond to questions pertaining to service- based slots (namely, hotel-internet, hotel-parking) with the required \"yes\" and \"no\". Furthermore, this facilitates assigning the commonly 2https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan used \"dontcare\" value to questions. We initialize the embedding of the Dontcare token to the average of embeddings in \"do not care\". We format the input of generative models with the \"question: \ud835\udc5e\ud835\udc56 context: \ud835\udc37\" prompt. 3.3 Reading Comprehension Fine-Tuning We use the MultiWOZ 2.1 [5] dataset for inference and fine-tuning. We fine-tune our models following the TRADE [23] pre and post- processing methodology, as recommended by the MultiWOZ au- thors, to ensure our results are directly comparable to the literature. We train and evaluate following a cross-domain adaptation setting, meaning we train on 4 dataset domains, and evaluate on the unseen domain data, with the proposed reading comprehension approach. Given that MultiWOZ is composed of 5 domains, we train a total of 5 instances of each model. We evaluate using the joint goal ac- curacy metric on the target slots, and use the average of the five runs as the main comparison metric. We use RoBERTa-base as our backbone span-based QA model and T5-base as our backbone gen- erative model. We generate the LLM and Pronouns \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 using a single InstructGPT [19] input. Unless otherwise stated, we use the LLM \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategy in our experiments. 4 RESULTS AND DISCUSSION 4.1 Baselines We compare our performance with the following approaches: TRADE [23] and MA-DST [13], which use RNNs and slot names as input; Li et al. [14] manually write questions and use a GPT-2 model for decoding the slot values. T5-DST [16], which uses slot descriptions with a T5-small model\u2014T5-DST (QA), one of the presented base- lines in T5-DST, uses automatic question generation. This model is evaluated on MultiWOZ 2.0, which may not make it directly comparable to our work. TransferQA [15] pretrains a T5-large model using several QA datasets and applies it to the DST task, without any in-domain training; D3ST [26] uses slot descriptions and a T5 model. Finally, SDT [9] uses a T5-XXL model, and requires dialogue examples to be manually written from analyzing the data. SDT always contains the input of a handwritten example, making it explicitly a one-shot approach. Li et al. [14], T5-DST, D3ST, and SDT require more domain expertise and a higher developer workload to express new slots: not only do they require manually creating slot descriptions or examples, but also explicitly listing all values of categorical slots\u2014which may not be trivial when adding new slots. While generating all the descriptions for D3ST and examples for SDT (in the SGD dataset [22]) takes 1.5 and 2 hours, respectively [9], we generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 automatically from only the slot keys, the minimal representation of a slot; thus requiring less data. 4.2 Zero-shot cross-domain adaptation Table 1 presents the results for the zero-shot domain transfer task. Each column represents an instance of the model trained on all domains except one, then evaluated on that domain. The metric is the standard per-domain joint goal accuracy (JGA). We also present the average JGA over all domains, which we use as our main means of comparison. We obtain SOTA performance versus models with comparable amounts of information (slot keys only), consistently outperforming them in most domains, oftentimes by large margins. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Table 1: Zero-shot cross-domain adaptation results. The top approaches rely on simple features, such as slot names. The bottom approaches rely on more complex features, such as slot descriptions or full examples. Model Hotel Rest. Taxi Attr. Train Fully zero-shot"}], "doc_text": "each element is extracted from as its context paragraph. We use greedy decoding to generate each ques- tion. QA models for DST require the ability to classify questions as unanswerable, as each utterance will generally only mention a small fraction of slots. This is crucial for maintaining good perfor- mance, as error propagation caused by misclassifications can lead to a rapid decrease in joint goal accuracy. In the out-of-domain pre- training step, we use the unanswerable SQuAD question set. In the in-domain step, we apply each question \ud835\udc44\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 to the next dialogue in the training set, if the dialogue does not explicitly contain the corresponding answer \ud835\udc34\ud835\udc56 \ud835\udc5d\ud835\udc5f\ud835\udc52 . We illustrate this strategy in Figure 1. 3.2 Slot-based Question Formulation (\ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 ) We will refer to any information in a slot-key that is not its domain as a slot type. For instance, given the slot-key hotel-pricerange, the domain is hotel and the type is pricerange. To study the impact of \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 on model performance, we experiment with its formulation in a number of ways.The strategies include template-based, LLM-based, and handcrafted approaches. 3.2.1 Template. This approach relies on the following templates: \"what is the <slot key>?\" and \"what is the value of the slot <slot key> ?\" [7]\u2014referred to as what-is and simple, respectively. 3.2.2 LLM. We generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 by prompting a Large Language Model (LLM) so that a natural language question is composed for each slot type. This reduces the human bias inherent to manually generating slot questions [14] or descriptions [16, 26]. An immedi- ate pitfall lies in language models not explicitly containing domain knowledge. This may cause the questions to misrepresent slot se- mantics when those are not immediately clear from the type. We define two LLM prompt strategies for question generation: LLM, which simply prompts all questions to be generated, and Pronouns, in which the LLM prompt requires the pronouns in the question to be in the third person, and refer to \"the user\", whenever needed. 3.2.3 Handcrafted. We also manually created a set of questions that contain explicit domain knowledge, alongside explicitly enforcing semantically similar slots to have similar questions. The resulting set is composed of questions less abstract and more meaningful to the dialogue than those in template-based approaches [6]. Implementation Details. When using span-based QA mod- 3.2.4 els, we prepend { Yes, No, Dontcare } to each question [18]. This allows the model to respond to questions pertaining to service- based slots (namely, hotel-internet, hotel-parking) with the required \"yes\" and \"no\". Furthermore, this facilitates assigning the commonly 2https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan used \"dontcare\" value to questions. We initialize the embedding of the Dontcare token to the average of embeddings in \"do not care\". We format the input of generative models with the \"question: \ud835\udc5e\ud835\udc56 context: \ud835\udc37\" prompt. 3.3 Reading Comprehension Fine-Tuning We use the MultiWOZ 2.1 [5] dataset for inference and fine-tuning. We fine-tune our models following the TRADE [23] pre and post- processing methodology, as recommended by the MultiWOZ au- thors, to ensure our results are directly comparable to the literature. We train and evaluate following a cross-domain adaptation setting, meaning we train on 4 dataset domains, and evaluate on the unseen domain data, with the proposed reading comprehension approach. Given that MultiWOZ is composed of 5 domains, we train a total of 5 instances of each model. We evaluate using the joint goal ac- curacy metric on the target slots, and use the average of the five runs as the main comparison metric. We use RoBERTa-base as our backbone span-based QA model and T5-base as our backbone gen- erative model. We generate the LLM and Pronouns \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 using a single InstructGPT [19] input. Unless otherwise stated, we use the LLM \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 strategy in our experiments. 4 RESULTS AND DISCUSSION 4.1 Baselines We compare our performance with the following approaches: TRADE [23] and MA-DST [13], which use RNNs and slot names as input; Li et al. [14] manually write questions and use a GPT-2 model for decoding the slot values. T5-DST [16], which uses slot descriptions with a T5-small model\u2014T5-DST (QA), one of the presented base- lines in T5-DST, uses automatic question generation. This model is evaluated on MultiWOZ 2.0, which may not make it directly comparable to our work. TransferQA [15] pretrains a T5-large model using several QA datasets and applies it to the DST task, without any in-domain training; D3ST [26] uses slot descriptions and a T5 model. Finally, SDT [9] uses a T5-XXL model, and requires dialogue examples to be manually written from analyzing the data. SDT always contains the input of a handwritten example, making it explicitly a one-shot approach. Li et al. [14], T5-DST, D3ST, and SDT require more domain expertise and a higher developer workload to express new slots: not only do they require manually creating slot descriptions or examples, but also explicitly listing all values of categorical slots\u2014which may not be trivial when adding new slots. While generating all the descriptions for D3ST and examples for SDT (in the SGD dataset [22]) takes 1.5 and 2 hours, respectively [9], we generate \ud835\udc44\ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc61 automatically from only the slot keys, the minimal representation of a slot; thus requiring less data. 4.2 Zero-shot cross-domain adaptation Table 1 presents the results for the zero-shot domain transfer task. Each column represents an instance of the model trained on all domains except one, then evaluated on that domain. The metric is the standard per-domain joint goal accuracy (JGA). We also present the average JGA over all domains, which we use as our main means of comparison. We obtain SOTA performance versus models with comparable amounts of information (slot keys only), consistently outperforming them in most domains, oftentimes by large margins. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan Table 1: Zero-shot cross-domain adaptation results. The top approaches rely on simple features, such as slot names. The bottom approaches rely on more complex features, such as slot descriptions or full examples. Model Hotel Rest. Taxi Attr. Train Fully zero-shot"}