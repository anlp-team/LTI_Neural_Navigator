{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Learning_Performance-Improving_Code_Edits_chunk_9.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How many new programs were attempted to be generated through the prompting strategy?", "answer": " 10,000", "ref_chunk": "promising rates of novelty. We found that after attempting to generate 10,000 new programs through the prompting strategy, 6,553 were not in the training/validation/test set of PIE. We keep track of equivalent programs of the ones generated, and of these 6,553 generations we found 3,314 equivalence sets. In total, this required executing over 1.4 million binary input pairs. Parallelized on a 24-core Intel 13900k processor with 64GB of RAM, this took less than 72 hours to complete. A.5 ABLATION OF RETRIEVAL-BASED FEW-SHOT PROMPTING CONFIGURATION For our retrieval-based prompting experiment we tried multiple configurations for the number of retrieved prompts where of K = {1, 2, 4} of the K closest retrieved prompts. Table 7: Retrieval-based few-shot prompting experiments: Results for various models and datasets configu- rations. *We note that for GPT4, we report only with Best@4, given resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Dynamic Retrieval, K=1 CodeLlama7B Dynamic Retrieval, K=1 CodeLlama13B Dynamic Retrieval, K=1 CodeLlama34B 3.39% 5.62% 10.51% 1.09 1.17 1.26 17.29% 22.59% 32.06% 16.33% 23.22% 35.67% 1.52 1.74 2.26 Dynamic Retrieval, K=2 CodeLlama7B Dynamic Retrieval, K=2 CodeLlama13B Dynamic Retrieval, K=2 CodeLlama34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=4 CodeLlama7B Dynamic Retrieval, K=4 CodeLlama13B Dynamic Retrieval, K=4 CodeLlama34B 5.54% 9.61% 12.12% 1.19 1.30 1.35 23.72% 27.69% 31.45% 18.81% 29.05% 43.68% 1.63 2.07 2.47 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 48.16% 80.82% 69.03%* 2.14 3.56* 18 Correct 52.49% 65.43% 72.61% 56.74% 66.25% 77.92% 60.08% 64.26% 75.44% 97.96% 95.90%* Preprint. Under review. using namespace std; typedef long long ll; inline void getInt(int* p); const int maxn=1000010; const int inf=0x3f3f3f3f; ll n; ll dp[maxn]; ll a[maxn]; int main() { gbtb; cin>>n; repd(i,1,n) { cin>>a[i]; } dp[1]=0; dp[0]=0; dp[2]=abs(a[2]-a[1]); repd(i,3,n) { dp[i]=min(dp[i-2]+abs(a[i]-a[i-2]),dp[i-1]+abs(a[i]-a[i-1])); } cout<<dp[n]; return 0; } inline void getInt(int* p) { char ch; do { ch = getchar(); } while (ch == ' ' ch == '\\n'); if (ch == '-') { p = -(getchar() - '0'); while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 - ch + '0'; } } else { p = ch - '0'; while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 + ch - '0'; } } } Figure 8: An example of a C++ program we found multiple submissions for as it is template code. Across these submissions, we found variance in the reported CPU runtime despite the code and competitive pro- gramming environment being identical. A.6 TRAINING DETAILS We fine-tuned the 7B and 13B variants using the HuggingFace Transformers library with FSDP to distribute the training process across 8\u00d7 48GB GPUs (NVIDIA RTX A6000/NVIDIA L40). For our high-quality dataset, which consists of approximately 4,000 examples, the models were fine-tuned until convergence was achieved, which can be done under 12 hours with 8 GPUs. For tasks related to full data fine-tuning and performance-conditioned fine-tuning, we only train for 1 epoch, which takes 24 to 36 hours, depending on the model of GPU used. All experiments were conducted using the AdamW optimizer (Loshchilov & Hutter, 2019). For the 7B and 13B variants of CODELLAMA, we used a batch size of 32 and a learning rate of 1e\u22125 for all of the experiments. 19 Preprint. Under review. Description 1: {description_1} Code 1: {code_1} Description 2: {description_2} Code 2: {code_2} Now, can you generate a program that takes that same input as Code 2 in Code 3 but produces different outputs? Write it to be as novel as possible. (cid:44)\u2192 (cid:44)\u2192 Code 3: Figure 9: The prompt template used for prompting GPT-3.5 for generating synthetic data for self-play. A.7 LORA RESULTS We show results using low-rank adaptors for finetuning in Table 8. Table 8: LoRA Experiments: Results for fine-tuning CODELLAMA with low rank adapters. A LoRA rank of 32 and LoRA alpha of 16 is used for all experiments listed. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 1.12% 0.41% 1.01 1.01 45.82% 59.47% 9.57% 9.67% 1.17 1.15 HQ CODELLAMA 13B 0.92% 1.02 59.57% 10.69% 1.17 A.8 PROMPTS Given the program below, improve its performance: ### Program: {src_code} ### Optimized Version: Figure 10: Instruction-prompting for adapting LLMs. The model is provided with direct instructions to improve the performance of the given program. 20 Correct 87.47% 90.94% 91.04% Preprint. Under review. slow1 \u2192 fast1 || slow2 \u2192 fast2 || slow3 \u2192 fast3 || ... || slowN \u2192 (cid:44)\u2192 fastN ### Program: {src_code} ### Optimized Version: Figure 11: Few-shot prompting for in-context learning. The format \"slow \u2192 fast\" is used for adaptation. A test program is appended for inference. ### slower program: {src_code} Give a potential strategy improving the program: LLM reasoning: The current program calculates the sum of integers from 1 to n-1 using a loop. This iterative approach results in a time complexity of O(n). The sum of the first n natural numbers follows an arithmetic progression, and by leveraging Gauss\u2019s formula, n(n-1)/2, we can compute the sum directly. This formula-based approach allows us to calculate the sum in constant time, O(1), eliminating the need for iteration. ### optimized version of the same code with the strategy mentioned (cid:44)\u2192 above: LLM Response: optimized code Figure 12: Chain-of-thought prompting. The model\u2019s intermediate response and final program are high- lighted in blue, indicating they are produced by the LLM. similar_slow1 \u2192 similar_fast1 || similar_slow2 \u2192 similar_fast2 || ... (cid:44)\u2192 || similar_slowN \u2192 similar_fastN ### Program: {src_code} ### Optimized Version: Figure 13: Retrieval-based few-shot prompting. By dynamically retrieving analogous program structures or challenges, the model is guided to better harness patterns in PIE. 21 Preprint. Under review. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. ### Program: {src_code} ### Program: {src_code} ### Optimized Version: {tgt_code} ### Optimized Version: (a) Training Prompt. (b) Inference Prompt. Figure"}, {"question": " Out of the 6,553 generated programs that were not in the training/validation/test set of PIE, how many equivalence sets were found?", "answer": " 3,314", "ref_chunk": "promising rates of novelty. We found that after attempting to generate 10,000 new programs through the prompting strategy, 6,553 were not in the training/validation/test set of PIE. We keep track of equivalent programs of the ones generated, and of these 6,553 generations we found 3,314 equivalence sets. In total, this required executing over 1.4 million binary input pairs. Parallelized on a 24-core Intel 13900k processor with 64GB of RAM, this took less than 72 hours to complete. A.5 ABLATION OF RETRIEVAL-BASED FEW-SHOT PROMPTING CONFIGURATION For our retrieval-based prompting experiment we tried multiple configurations for the number of retrieved prompts where of K = {1, 2, 4} of the K closest retrieved prompts. Table 7: Retrieval-based few-shot prompting experiments: Results for various models and datasets configu- rations. *We note that for GPT4, we report only with Best@4, given resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Dynamic Retrieval, K=1 CodeLlama7B Dynamic Retrieval, K=1 CodeLlama13B Dynamic Retrieval, K=1 CodeLlama34B 3.39% 5.62% 10.51% 1.09 1.17 1.26 17.29% 22.59% 32.06% 16.33% 23.22% 35.67% 1.52 1.74 2.26 Dynamic Retrieval, K=2 CodeLlama7B Dynamic Retrieval, K=2 CodeLlama13B Dynamic Retrieval, K=2 CodeLlama34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=4 CodeLlama7B Dynamic Retrieval, K=4 CodeLlama13B Dynamic Retrieval, K=4 CodeLlama34B 5.54% 9.61% 12.12% 1.19 1.30 1.35 23.72% 27.69% 31.45% 18.81% 29.05% 43.68% 1.63 2.07 2.47 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 48.16% 80.82% 69.03%* 2.14 3.56* 18 Correct 52.49% 65.43% 72.61% 56.74% 66.25% 77.92% 60.08% 64.26% 75.44% 97.96% 95.90%* Preprint. Under review. using namespace std; typedef long long ll; inline void getInt(int* p); const int maxn=1000010; const int inf=0x3f3f3f3f; ll n; ll dp[maxn]; ll a[maxn]; int main() { gbtb; cin>>n; repd(i,1,n) { cin>>a[i]; } dp[1]=0; dp[0]=0; dp[2]=abs(a[2]-a[1]); repd(i,3,n) { dp[i]=min(dp[i-2]+abs(a[i]-a[i-2]),dp[i-1]+abs(a[i]-a[i-1])); } cout<<dp[n]; return 0; } inline void getInt(int* p) { char ch; do { ch = getchar(); } while (ch == ' ' ch == '\\n'); if (ch == '-') { p = -(getchar() - '0'); while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 - ch + '0'; } } else { p = ch - '0'; while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 + ch - '0'; } } } Figure 8: An example of a C++ program we found multiple submissions for as it is template code. Across these submissions, we found variance in the reported CPU runtime despite the code and competitive pro- gramming environment being identical. A.6 TRAINING DETAILS We fine-tuned the 7B and 13B variants using the HuggingFace Transformers library with FSDP to distribute the training process across 8\u00d7 48GB GPUs (NVIDIA RTX A6000/NVIDIA L40). For our high-quality dataset, which consists of approximately 4,000 examples, the models were fine-tuned until convergence was achieved, which can be done under 12 hours with 8 GPUs. For tasks related to full data fine-tuning and performance-conditioned fine-tuning, we only train for 1 epoch, which takes 24 to 36 hours, depending on the model of GPU used. All experiments were conducted using the AdamW optimizer (Loshchilov & Hutter, 2019). For the 7B and 13B variants of CODELLAMA, we used a batch size of 32 and a learning rate of 1e\u22125 for all of the experiments. 19 Preprint. Under review. Description 1: {description_1} Code 1: {code_1} Description 2: {description_2} Code 2: {code_2} Now, can you generate a program that takes that same input as Code 2 in Code 3 but produces different outputs? Write it to be as novel as possible. (cid:44)\u2192 (cid:44)\u2192 Code 3: Figure 9: The prompt template used for prompting GPT-3.5 for generating synthetic data for self-play. A.7 LORA RESULTS We show results using low-rank adaptors for finetuning in Table 8. Table 8: LoRA Experiments: Results for fine-tuning CODELLAMA with low rank adapters. A LoRA rank of 32 and LoRA alpha of 16 is used for all experiments listed. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 1.12% 0.41% 1.01 1.01 45.82% 59.47% 9.57% 9.67% 1.17 1.15 HQ CODELLAMA 13B 0.92% 1.02 59.57% 10.69% 1.17 A.8 PROMPTS Given the program below, improve its performance: ### Program: {src_code} ### Optimized Version: Figure 10: Instruction-prompting for adapting LLMs. The model is provided with direct instructions to improve the performance of the given program. 20 Correct 87.47% 90.94% 91.04% Preprint. Under review. slow1 \u2192 fast1 || slow2 \u2192 fast2 || slow3 \u2192 fast3 || ... || slowN \u2192 (cid:44)\u2192 fastN ### Program: {src_code} ### Optimized Version: Figure 11: Few-shot prompting for in-context learning. The format \"slow \u2192 fast\" is used for adaptation. A test program is appended for inference. ### slower program: {src_code} Give a potential strategy improving the program: LLM reasoning: The current program calculates the sum of integers from 1 to n-1 using a loop. This iterative approach results in a time complexity of O(n). The sum of the first n natural numbers follows an arithmetic progression, and by leveraging Gauss\u2019s formula, n(n-1)/2, we can compute the sum directly. This formula-based approach allows us to calculate the sum in constant time, O(1), eliminating the need for iteration. ### optimized version of the same code with the strategy mentioned (cid:44)\u2192 above: LLM Response: optimized code Figure 12: Chain-of-thought prompting. The model\u2019s intermediate response and final program are high- lighted in blue, indicating they are produced by the LLM. similar_slow1 \u2192 similar_fast1 || similar_slow2 \u2192 similar_fast2 || ... (cid:44)\u2192 || similar_slowN \u2192 similar_fastN ### Program: {src_code} ### Optimized Version: Figure 13: Retrieval-based few-shot prompting. By dynamically retrieving analogous program structures or challenges, the model is guided to better harness patterns in PIE. 21 Preprint. Under review. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. ### Program: {src_code} ### Program: {src_code} ### Optimized Version: {tgt_code} ### Optimized Version: (a) Training Prompt. (b) Inference Prompt. Figure"}, {"question": " How many binary input pairs were executed in total for the generated programs?", "answer": " 1.4 million", "ref_chunk": "promising rates of novelty. We found that after attempting to generate 10,000 new programs through the prompting strategy, 6,553 were not in the training/validation/test set of PIE. We keep track of equivalent programs of the ones generated, and of these 6,553 generations we found 3,314 equivalence sets. In total, this required executing over 1.4 million binary input pairs. Parallelized on a 24-core Intel 13900k processor with 64GB of RAM, this took less than 72 hours to complete. A.5 ABLATION OF RETRIEVAL-BASED FEW-SHOT PROMPTING CONFIGURATION For our retrieval-based prompting experiment we tried multiple configurations for the number of retrieved prompts where of K = {1, 2, 4} of the K closest retrieved prompts. Table 7: Retrieval-based few-shot prompting experiments: Results for various models and datasets configu- rations. *We note that for GPT4, we report only with Best@4, given resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Dynamic Retrieval, K=1 CodeLlama7B Dynamic Retrieval, K=1 CodeLlama13B Dynamic Retrieval, K=1 CodeLlama34B 3.39% 5.62% 10.51% 1.09 1.17 1.26 17.29% 22.59% 32.06% 16.33% 23.22% 35.67% 1.52 1.74 2.26 Dynamic Retrieval, K=2 CodeLlama7B Dynamic Retrieval, K=2 CodeLlama13B Dynamic Retrieval, K=2 CodeLlama34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=4 CodeLlama7B Dynamic Retrieval, K=4 CodeLlama13B Dynamic Retrieval, K=4 CodeLlama34B 5.54% 9.61% 12.12% 1.19 1.30 1.35 23.72% 27.69% 31.45% 18.81% 29.05% 43.68% 1.63 2.07 2.47 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 48.16% 80.82% 69.03%* 2.14 3.56* 18 Correct 52.49% 65.43% 72.61% 56.74% 66.25% 77.92% 60.08% 64.26% 75.44% 97.96% 95.90%* Preprint. Under review. using namespace std; typedef long long ll; inline void getInt(int* p); const int maxn=1000010; const int inf=0x3f3f3f3f; ll n; ll dp[maxn]; ll a[maxn]; int main() { gbtb; cin>>n; repd(i,1,n) { cin>>a[i]; } dp[1]=0; dp[0]=0; dp[2]=abs(a[2]-a[1]); repd(i,3,n) { dp[i]=min(dp[i-2]+abs(a[i]-a[i-2]),dp[i-1]+abs(a[i]-a[i-1])); } cout<<dp[n]; return 0; } inline void getInt(int* p) { char ch; do { ch = getchar(); } while (ch == ' ' ch == '\\n'); if (ch == '-') { p = -(getchar() - '0'); while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 - ch + '0'; } } else { p = ch - '0'; while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 + ch - '0'; } } } Figure 8: An example of a C++ program we found multiple submissions for as it is template code. Across these submissions, we found variance in the reported CPU runtime despite the code and competitive pro- gramming environment being identical. A.6 TRAINING DETAILS We fine-tuned the 7B and 13B variants using the HuggingFace Transformers library with FSDP to distribute the training process across 8\u00d7 48GB GPUs (NVIDIA RTX A6000/NVIDIA L40). For our high-quality dataset, which consists of approximately 4,000 examples, the models were fine-tuned until convergence was achieved, which can be done under 12 hours with 8 GPUs. For tasks related to full data fine-tuning and performance-conditioned fine-tuning, we only train for 1 epoch, which takes 24 to 36 hours, depending on the model of GPU used. All experiments were conducted using the AdamW optimizer (Loshchilov & Hutter, 2019). For the 7B and 13B variants of CODELLAMA, we used a batch size of 32 and a learning rate of 1e\u22125 for all of the experiments. 19 Preprint. Under review. Description 1: {description_1} Code 1: {code_1} Description 2: {description_2} Code 2: {code_2} Now, can you generate a program that takes that same input as Code 2 in Code 3 but produces different outputs? Write it to be as novel as possible. (cid:44)\u2192 (cid:44)\u2192 Code 3: Figure 9: The prompt template used for prompting GPT-3.5 for generating synthetic data for self-play. A.7 LORA RESULTS We show results using low-rank adaptors for finetuning in Table 8. Table 8: LoRA Experiments: Results for fine-tuning CODELLAMA with low rank adapters. A LoRA rank of 32 and LoRA alpha of 16 is used for all experiments listed. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 1.12% 0.41% 1.01 1.01 45.82% 59.47% 9.57% 9.67% 1.17 1.15 HQ CODELLAMA 13B 0.92% 1.02 59.57% 10.69% 1.17 A.8 PROMPTS Given the program below, improve its performance: ### Program: {src_code} ### Optimized Version: Figure 10: Instruction-prompting for adapting LLMs. The model is provided with direct instructions to improve the performance of the given program. 20 Correct 87.47% 90.94% 91.04% Preprint. Under review. slow1 \u2192 fast1 || slow2 \u2192 fast2 || slow3 \u2192 fast3 || ... || slowN \u2192 (cid:44)\u2192 fastN ### Program: {src_code} ### Optimized Version: Figure 11: Few-shot prompting for in-context learning. The format \"slow \u2192 fast\" is used for adaptation. A test program is appended for inference. ### slower program: {src_code} Give a potential strategy improving the program: LLM reasoning: The current program calculates the sum of integers from 1 to n-1 using a loop. This iterative approach results in a time complexity of O(n). The sum of the first n natural numbers follows an arithmetic progression, and by leveraging Gauss\u2019s formula, n(n-1)/2, we can compute the sum directly. This formula-based approach allows us to calculate the sum in constant time, O(1), eliminating the need for iteration. ### optimized version of the same code with the strategy mentioned (cid:44)\u2192 above: LLM Response: optimized code Figure 12: Chain-of-thought prompting. The model\u2019s intermediate response and final program are high- lighted in blue, indicating they are produced by the LLM. similar_slow1 \u2192 similar_fast1 || similar_slow2 \u2192 similar_fast2 || ... (cid:44)\u2192 || similar_slowN \u2192 similar_fastN ### Program: {src_code} ### Optimized Version: Figure 13: Retrieval-based few-shot prompting. By dynamically retrieving analogous program structures or challenges, the model is guided to better harness patterns in PIE. 21 Preprint. Under review. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. ### Program: {src_code} ### Program: {src_code} ### Optimized Version: {tgt_code} ### Optimized Version: (a) Training Prompt. (b) Inference Prompt. Figure"}, {"question": " What type of processor was used for the execution, and how many cores did it have?", "answer": " 24-core Intel 13900k processor", "ref_chunk": "promising rates of novelty. We found that after attempting to generate 10,000 new programs through the prompting strategy, 6,553 were not in the training/validation/test set of PIE. We keep track of equivalent programs of the ones generated, and of these 6,553 generations we found 3,314 equivalence sets. In total, this required executing over 1.4 million binary input pairs. Parallelized on a 24-core Intel 13900k processor with 64GB of RAM, this took less than 72 hours to complete. A.5 ABLATION OF RETRIEVAL-BASED FEW-SHOT PROMPTING CONFIGURATION For our retrieval-based prompting experiment we tried multiple configurations for the number of retrieved prompts where of K = {1, 2, 4} of the K closest retrieved prompts. Table 7: Retrieval-based few-shot prompting experiments: Results for various models and datasets configu- rations. *We note that for GPT4, we report only with Best@4, given resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Dynamic Retrieval, K=1 CodeLlama7B Dynamic Retrieval, K=1 CodeLlama13B Dynamic Retrieval, K=1 CodeLlama34B 3.39% 5.62% 10.51% 1.09 1.17 1.26 17.29% 22.59% 32.06% 16.33% 23.22% 35.67% 1.52 1.74 2.26 Dynamic Retrieval, K=2 CodeLlama7B Dynamic Retrieval, K=2 CodeLlama13B Dynamic Retrieval, K=2 CodeLlama34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=4 CodeLlama7B Dynamic Retrieval, K=4 CodeLlama13B Dynamic Retrieval, K=4 CodeLlama34B 5.54% 9.61% 12.12% 1.19 1.30 1.35 23.72% 27.69% 31.45% 18.81% 29.05% 43.68% 1.63 2.07 2.47 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 48.16% 80.82% 69.03%* 2.14 3.56* 18 Correct 52.49% 65.43% 72.61% 56.74% 66.25% 77.92% 60.08% 64.26% 75.44% 97.96% 95.90%* Preprint. Under review. using namespace std; typedef long long ll; inline void getInt(int* p); const int maxn=1000010; const int inf=0x3f3f3f3f; ll n; ll dp[maxn]; ll a[maxn]; int main() { gbtb; cin>>n; repd(i,1,n) { cin>>a[i]; } dp[1]=0; dp[0]=0; dp[2]=abs(a[2]-a[1]); repd(i,3,n) { dp[i]=min(dp[i-2]+abs(a[i]-a[i-2]),dp[i-1]+abs(a[i]-a[i-1])); } cout<<dp[n]; return 0; } inline void getInt(int* p) { char ch; do { ch = getchar(); } while (ch == ' ' ch == '\\n'); if (ch == '-') { p = -(getchar() - '0'); while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 - ch + '0'; } } else { p = ch - '0'; while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 + ch - '0'; } } } Figure 8: An example of a C++ program we found multiple submissions for as it is template code. Across these submissions, we found variance in the reported CPU runtime despite the code and competitive pro- gramming environment being identical. A.6 TRAINING DETAILS We fine-tuned the 7B and 13B variants using the HuggingFace Transformers library with FSDP to distribute the training process across 8\u00d7 48GB GPUs (NVIDIA RTX A6000/NVIDIA L40). For our high-quality dataset, which consists of approximately 4,000 examples, the models were fine-tuned until convergence was achieved, which can be done under 12 hours with 8 GPUs. For tasks related to full data fine-tuning and performance-conditioned fine-tuning, we only train for 1 epoch, which takes 24 to 36 hours, depending on the model of GPU used. All experiments were conducted using the AdamW optimizer (Loshchilov & Hutter, 2019). For the 7B and 13B variants of CODELLAMA, we used a batch size of 32 and a learning rate of 1e\u22125 for all of the experiments. 19 Preprint. Under review. Description 1: {description_1} Code 1: {code_1} Description 2: {description_2} Code 2: {code_2} Now, can you generate a program that takes that same input as Code 2 in Code 3 but produces different outputs? Write it to be as novel as possible. (cid:44)\u2192 (cid:44)\u2192 Code 3: Figure 9: The prompt template used for prompting GPT-3.5 for generating synthetic data for self-play. A.7 LORA RESULTS We show results using low-rank adaptors for finetuning in Table 8. Table 8: LoRA Experiments: Results for fine-tuning CODELLAMA with low rank adapters. A LoRA rank of 32 and LoRA alpha of 16 is used for all experiments listed. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 1.12% 0.41% 1.01 1.01 45.82% 59.47% 9.57% 9.67% 1.17 1.15 HQ CODELLAMA 13B 0.92% 1.02 59.57% 10.69% 1.17 A.8 PROMPTS Given the program below, improve its performance: ### Program: {src_code} ### Optimized Version: Figure 10: Instruction-prompting for adapting LLMs. The model is provided with direct instructions to improve the performance of the given program. 20 Correct 87.47% 90.94% 91.04% Preprint. Under review. slow1 \u2192 fast1 || slow2 \u2192 fast2 || slow3 \u2192 fast3 || ... || slowN \u2192 (cid:44)\u2192 fastN ### Program: {src_code} ### Optimized Version: Figure 11: Few-shot prompting for in-context learning. The format \"slow \u2192 fast\" is used for adaptation. A test program is appended for inference. ### slower program: {src_code} Give a potential strategy improving the program: LLM reasoning: The current program calculates the sum of integers from 1 to n-1 using a loop. This iterative approach results in a time complexity of O(n). The sum of the first n natural numbers follows an arithmetic progression, and by leveraging Gauss\u2019s formula, n(n-1)/2, we can compute the sum directly. This formula-based approach allows us to calculate the sum in constant time, O(1), eliminating the need for iteration. ### optimized version of the same code with the strategy mentioned (cid:44)\u2192 above: LLM Response: optimized code Figure 12: Chain-of-thought prompting. The model\u2019s intermediate response and final program are high- lighted in blue, indicating they are produced by the LLM. similar_slow1 \u2192 similar_fast1 || similar_slow2 \u2192 similar_fast2 || ... (cid:44)\u2192 || similar_slowN \u2192 similar_fastN ### Program: {src_code} ### Optimized Version: Figure 13: Retrieval-based few-shot prompting. By dynamically retrieving analogous program structures or challenges, the model is guided to better harness patterns in PIE. 21 Preprint. Under review. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. ### Program: {src_code} ### Program: {src_code} ### Optimized Version: {tgt_code} ### Optimized Version: (a) Training Prompt. (b) Inference Prompt. Figure"}, {"question": " How long did it take to complete the execution on the 24-core Intel 13900k processor?", "answer": " less than 72 hours", "ref_chunk": "promising rates of novelty. We found that after attempting to generate 10,000 new programs through the prompting strategy, 6,553 were not in the training/validation/test set of PIE. We keep track of equivalent programs of the ones generated, and of these 6,553 generations we found 3,314 equivalence sets. In total, this required executing over 1.4 million binary input pairs. Parallelized on a 24-core Intel 13900k processor with 64GB of RAM, this took less than 72 hours to complete. A.5 ABLATION OF RETRIEVAL-BASED FEW-SHOT PROMPTING CONFIGURATION For our retrieval-based prompting experiment we tried multiple configurations for the number of retrieved prompts where of K = {1, 2, 4} of the K closest retrieved prompts. Table 7: Retrieval-based few-shot prompting experiments: Results for various models and datasets configu- rations. *We note that for GPT4, we report only with Best@4, given resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Dynamic Retrieval, K=1 CodeLlama7B Dynamic Retrieval, K=1 CodeLlama13B Dynamic Retrieval, K=1 CodeLlama34B 3.39% 5.62% 10.51% 1.09 1.17 1.26 17.29% 22.59% 32.06% 16.33% 23.22% 35.67% 1.52 1.74 2.26 Dynamic Retrieval, K=2 CodeLlama7B Dynamic Retrieval, K=2 CodeLlama13B Dynamic Retrieval, K=2 CodeLlama34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=4 CodeLlama7B Dynamic Retrieval, K=4 CodeLlama13B Dynamic Retrieval, K=4 CodeLlama34B 5.54% 9.61% 12.12% 1.19 1.30 1.35 23.72% 27.69% 31.45% 18.81% 29.05% 43.68% 1.63 2.07 2.47 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 48.16% 80.82% 69.03%* 2.14 3.56* 18 Correct 52.49% 65.43% 72.61% 56.74% 66.25% 77.92% 60.08% 64.26% 75.44% 97.96% 95.90%* Preprint. Under review. using namespace std; typedef long long ll; inline void getInt(int* p); const int maxn=1000010; const int inf=0x3f3f3f3f; ll n; ll dp[maxn]; ll a[maxn]; int main() { gbtb; cin>>n; repd(i,1,n) { cin>>a[i]; } dp[1]=0; dp[0]=0; dp[2]=abs(a[2]-a[1]); repd(i,3,n) { dp[i]=min(dp[i-2]+abs(a[i]-a[i-2]),dp[i-1]+abs(a[i]-a[i-1])); } cout<<dp[n]; return 0; } inline void getInt(int* p) { char ch; do { ch = getchar(); } while (ch == ' ' ch == '\\n'); if (ch == '-') { p = -(getchar() - '0'); while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 - ch + '0'; } } else { p = ch - '0'; while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 + ch - '0'; } } } Figure 8: An example of a C++ program we found multiple submissions for as it is template code. Across these submissions, we found variance in the reported CPU runtime despite the code and competitive pro- gramming environment being identical. A.6 TRAINING DETAILS We fine-tuned the 7B and 13B variants using the HuggingFace Transformers library with FSDP to distribute the training process across 8\u00d7 48GB GPUs (NVIDIA RTX A6000/NVIDIA L40). For our high-quality dataset, which consists of approximately 4,000 examples, the models were fine-tuned until convergence was achieved, which can be done under 12 hours with 8 GPUs. For tasks related to full data fine-tuning and performance-conditioned fine-tuning, we only train for 1 epoch, which takes 24 to 36 hours, depending on the model of GPU used. All experiments were conducted using the AdamW optimizer (Loshchilov & Hutter, 2019). For the 7B and 13B variants of CODELLAMA, we used a batch size of 32 and a learning rate of 1e\u22125 for all of the experiments. 19 Preprint. Under review. Description 1: {description_1} Code 1: {code_1} Description 2: {description_2} Code 2: {code_2} Now, can you generate a program that takes that same input as Code 2 in Code 3 but produces different outputs? Write it to be as novel as possible. (cid:44)\u2192 (cid:44)\u2192 Code 3: Figure 9: The prompt template used for prompting GPT-3.5 for generating synthetic data for self-play. A.7 LORA RESULTS We show results using low-rank adaptors for finetuning in Table 8. Table 8: LoRA Experiments: Results for fine-tuning CODELLAMA with low rank adapters. A LoRA rank of 32 and LoRA alpha of 16 is used for all experiments listed. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 1.12% 0.41% 1.01 1.01 45.82% 59.47% 9.57% 9.67% 1.17 1.15 HQ CODELLAMA 13B 0.92% 1.02 59.57% 10.69% 1.17 A.8 PROMPTS Given the program below, improve its performance: ### Program: {src_code} ### Optimized Version: Figure 10: Instruction-prompting for adapting LLMs. The model is provided with direct instructions to improve the performance of the given program. 20 Correct 87.47% 90.94% 91.04% Preprint. Under review. slow1 \u2192 fast1 || slow2 \u2192 fast2 || slow3 \u2192 fast3 || ... || slowN \u2192 (cid:44)\u2192 fastN ### Program: {src_code} ### Optimized Version: Figure 11: Few-shot prompting for in-context learning. The format \"slow \u2192 fast\" is used for adaptation. A test program is appended for inference. ### slower program: {src_code} Give a potential strategy improving the program: LLM reasoning: The current program calculates the sum of integers from 1 to n-1 using a loop. This iterative approach results in a time complexity of O(n). The sum of the first n natural numbers follows an arithmetic progression, and by leveraging Gauss\u2019s formula, n(n-1)/2, we can compute the sum directly. This formula-based approach allows us to calculate the sum in constant time, O(1), eliminating the need for iteration. ### optimized version of the same code with the strategy mentioned (cid:44)\u2192 above: LLM Response: optimized code Figure 12: Chain-of-thought prompting. The model\u2019s intermediate response and final program are high- lighted in blue, indicating they are produced by the LLM. similar_slow1 \u2192 similar_fast1 || similar_slow2 \u2192 similar_fast2 || ... (cid:44)\u2192 || similar_slowN \u2192 similar_fastN ### Program: {src_code} ### Optimized Version: Figure 13: Retrieval-based few-shot prompting. By dynamically retrieving analogous program structures or challenges, the model is guided to better harness patterns in PIE. 21 Preprint. Under review. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. ### Program: {src_code} ### Program: {src_code} ### Optimized Version: {tgt_code} ### Optimized Version: (a) Training Prompt. (b) Inference Prompt. Figure"}, {"question": " What were the different values of K tried for the retrieval-based prompting experiment?", "answer": " K = {1, 2, 4}", "ref_chunk": "promising rates of novelty. We found that after attempting to generate 10,000 new programs through the prompting strategy, 6,553 were not in the training/validation/test set of PIE. We keep track of equivalent programs of the ones generated, and of these 6,553 generations we found 3,314 equivalence sets. In total, this required executing over 1.4 million binary input pairs. Parallelized on a 24-core Intel 13900k processor with 64GB of RAM, this took less than 72 hours to complete. A.5 ABLATION OF RETRIEVAL-BASED FEW-SHOT PROMPTING CONFIGURATION For our retrieval-based prompting experiment we tried multiple configurations for the number of retrieved prompts where of K = {1, 2, 4} of the K closest retrieved prompts. Table 7: Retrieval-based few-shot prompting experiments: Results for various models and datasets configu- rations. *We note that for GPT4, we report only with Best@4, given resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Dynamic Retrieval, K=1 CodeLlama7B Dynamic Retrieval, K=1 CodeLlama13B Dynamic Retrieval, K=1 CodeLlama34B 3.39% 5.62% 10.51% 1.09 1.17 1.26 17.29% 22.59% 32.06% 16.33% 23.22% 35.67% 1.52 1.74 2.26 Dynamic Retrieval, K=2 CodeLlama7B Dynamic Retrieval, K=2 CodeLlama13B Dynamic Retrieval, K=2 CodeLlama34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=4 CodeLlama7B Dynamic Retrieval, K=4 CodeLlama13B Dynamic Retrieval, K=4 CodeLlama34B 5.54% 9.61% 12.12% 1.19 1.30 1.35 23.72% 27.69% 31.45% 18.81% 29.05% 43.68% 1.63 2.07 2.47 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 48.16% 80.82% 69.03%* 2.14 3.56* 18 Correct 52.49% 65.43% 72.61% 56.74% 66.25% 77.92% 60.08% 64.26% 75.44% 97.96% 95.90%* Preprint. Under review. using namespace std; typedef long long ll; inline void getInt(int* p); const int maxn=1000010; const int inf=0x3f3f3f3f; ll n; ll dp[maxn]; ll a[maxn]; int main() { gbtb; cin>>n; repd(i,1,n) { cin>>a[i]; } dp[1]=0; dp[0]=0; dp[2]=abs(a[2]-a[1]); repd(i,3,n) { dp[i]=min(dp[i-2]+abs(a[i]-a[i-2]),dp[i-1]+abs(a[i]-a[i-1])); } cout<<dp[n]; return 0; } inline void getInt(int* p) { char ch; do { ch = getchar(); } while (ch == ' ' ch == '\\n'); if (ch == '-') { p = -(getchar() - '0'); while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 - ch + '0'; } } else { p = ch - '0'; while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 + ch - '0'; } } } Figure 8: An example of a C++ program we found multiple submissions for as it is template code. Across these submissions, we found variance in the reported CPU runtime despite the code and competitive pro- gramming environment being identical. A.6 TRAINING DETAILS We fine-tuned the 7B and 13B variants using the HuggingFace Transformers library with FSDP to distribute the training process across 8\u00d7 48GB GPUs (NVIDIA RTX A6000/NVIDIA L40). For our high-quality dataset, which consists of approximately 4,000 examples, the models were fine-tuned until convergence was achieved, which can be done under 12 hours with 8 GPUs. For tasks related to full data fine-tuning and performance-conditioned fine-tuning, we only train for 1 epoch, which takes 24 to 36 hours, depending on the model of GPU used. All experiments were conducted using the AdamW optimizer (Loshchilov & Hutter, 2019). For the 7B and 13B variants of CODELLAMA, we used a batch size of 32 and a learning rate of 1e\u22125 for all of the experiments. 19 Preprint. Under review. Description 1: {description_1} Code 1: {code_1} Description 2: {description_2} Code 2: {code_2} Now, can you generate a program that takes that same input as Code 2 in Code 3 but produces different outputs? Write it to be as novel as possible. (cid:44)\u2192 (cid:44)\u2192 Code 3: Figure 9: The prompt template used for prompting GPT-3.5 for generating synthetic data for self-play. A.7 LORA RESULTS We show results using low-rank adaptors for finetuning in Table 8. Table 8: LoRA Experiments: Results for fine-tuning CODELLAMA with low rank adapters. A LoRA rank of 32 and LoRA alpha of 16 is used for all experiments listed. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 1.12% 0.41% 1.01 1.01 45.82% 59.47% 9.57% 9.67% 1.17 1.15 HQ CODELLAMA 13B 0.92% 1.02 59.57% 10.69% 1.17 A.8 PROMPTS Given the program below, improve its performance: ### Program: {src_code} ### Optimized Version: Figure 10: Instruction-prompting for adapting LLMs. The model is provided with direct instructions to improve the performance of the given program. 20 Correct 87.47% 90.94% 91.04% Preprint. Under review. slow1 \u2192 fast1 || slow2 \u2192 fast2 || slow3 \u2192 fast3 || ... || slowN \u2192 (cid:44)\u2192 fastN ### Program: {src_code} ### Optimized Version: Figure 11: Few-shot prompting for in-context learning. The format \"slow \u2192 fast\" is used for adaptation. A test program is appended for inference. ### slower program: {src_code} Give a potential strategy improving the program: LLM reasoning: The current program calculates the sum of integers from 1 to n-1 using a loop. This iterative approach results in a time complexity of O(n). The sum of the first n natural numbers follows an arithmetic progression, and by leveraging Gauss\u2019s formula, n(n-1)/2, we can compute the sum directly. This formula-based approach allows us to calculate the sum in constant time, O(1), eliminating the need for iteration. ### optimized version of the same code with the strategy mentioned (cid:44)\u2192 above: LLM Response: optimized code Figure 12: Chain-of-thought prompting. The model\u2019s intermediate response and final program are high- lighted in blue, indicating they are produced by the LLM. similar_slow1 \u2192 similar_fast1 || similar_slow2 \u2192 similar_fast2 || ... (cid:44)\u2192 || similar_slowN \u2192 similar_fastN ### Program: {src_code} ### Optimized Version: Figure 13: Retrieval-based few-shot prompting. By dynamically retrieving analogous program structures or challenges, the model is guided to better harness patterns in PIE. 21 Preprint. Under review. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. ### Program: {src_code} ### Program: {src_code} ### Optimized Version: {tgt_code} ### Optimized Version: (a) Training Prompt. (b) Inference Prompt. Figure"}, {"question": " For GPT4, which value of K was reported given the resource constraints?", "answer": " Best@4", "ref_chunk": "promising rates of novelty. We found that after attempting to generate 10,000 new programs through the prompting strategy, 6,553 were not in the training/validation/test set of PIE. We keep track of equivalent programs of the ones generated, and of these 6,553 generations we found 3,314 equivalence sets. In total, this required executing over 1.4 million binary input pairs. Parallelized on a 24-core Intel 13900k processor with 64GB of RAM, this took less than 72 hours to complete. A.5 ABLATION OF RETRIEVAL-BASED FEW-SHOT PROMPTING CONFIGURATION For our retrieval-based prompting experiment we tried multiple configurations for the number of retrieved prompts where of K = {1, 2, 4} of the K closest retrieved prompts. Table 7: Retrieval-based few-shot prompting experiments: Results for various models and datasets configu- rations. *We note that for GPT4, we report only with Best@4, given resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Dynamic Retrieval, K=1 CodeLlama7B Dynamic Retrieval, K=1 CodeLlama13B Dynamic Retrieval, K=1 CodeLlama34B 3.39% 5.62% 10.51% 1.09 1.17 1.26 17.29% 22.59% 32.06% 16.33% 23.22% 35.67% 1.52 1.74 2.26 Dynamic Retrieval, K=2 CodeLlama7B Dynamic Retrieval, K=2 CodeLlama13B Dynamic Retrieval, K=2 CodeLlama34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=4 CodeLlama7B Dynamic Retrieval, K=4 CodeLlama13B Dynamic Retrieval, K=4 CodeLlama34B 5.54% 9.61% 12.12% 1.19 1.30 1.35 23.72% 27.69% 31.45% 18.81% 29.05% 43.68% 1.63 2.07 2.47 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 48.16% 80.82% 69.03%* 2.14 3.56* 18 Correct 52.49% 65.43% 72.61% 56.74% 66.25% 77.92% 60.08% 64.26% 75.44% 97.96% 95.90%* Preprint. Under review. using namespace std; typedef long long ll; inline void getInt(int* p); const int maxn=1000010; const int inf=0x3f3f3f3f; ll n; ll dp[maxn]; ll a[maxn]; int main() { gbtb; cin>>n; repd(i,1,n) { cin>>a[i]; } dp[1]=0; dp[0]=0; dp[2]=abs(a[2]-a[1]); repd(i,3,n) { dp[i]=min(dp[i-2]+abs(a[i]-a[i-2]),dp[i-1]+abs(a[i]-a[i-1])); } cout<<dp[n]; return 0; } inline void getInt(int* p) { char ch; do { ch = getchar(); } while (ch == ' ' ch == '\\n'); if (ch == '-') { p = -(getchar() - '0'); while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 - ch + '0'; } } else { p = ch - '0'; while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 + ch - '0'; } } } Figure 8: An example of a C++ program we found multiple submissions for as it is template code. Across these submissions, we found variance in the reported CPU runtime despite the code and competitive pro- gramming environment being identical. A.6 TRAINING DETAILS We fine-tuned the 7B and 13B variants using the HuggingFace Transformers library with FSDP to distribute the training process across 8\u00d7 48GB GPUs (NVIDIA RTX A6000/NVIDIA L40). For our high-quality dataset, which consists of approximately 4,000 examples, the models were fine-tuned until convergence was achieved, which can be done under 12 hours with 8 GPUs. For tasks related to full data fine-tuning and performance-conditioned fine-tuning, we only train for 1 epoch, which takes 24 to 36 hours, depending on the model of GPU used. All experiments were conducted using the AdamW optimizer (Loshchilov & Hutter, 2019). For the 7B and 13B variants of CODELLAMA, we used a batch size of 32 and a learning rate of 1e\u22125 for all of the experiments. 19 Preprint. Under review. Description 1: {description_1} Code 1: {code_1} Description 2: {description_2} Code 2: {code_2} Now, can you generate a program that takes that same input as Code 2 in Code 3 but produces different outputs? Write it to be as novel as possible. (cid:44)\u2192 (cid:44)\u2192 Code 3: Figure 9: The prompt template used for prompting GPT-3.5 for generating synthetic data for self-play. A.7 LORA RESULTS We show results using low-rank adaptors for finetuning in Table 8. Table 8: LoRA Experiments: Results for fine-tuning CODELLAMA with low rank adapters. A LoRA rank of 32 and LoRA alpha of 16 is used for all experiments listed. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 1.12% 0.41% 1.01 1.01 45.82% 59.47% 9.57% 9.67% 1.17 1.15 HQ CODELLAMA 13B 0.92% 1.02 59.57% 10.69% 1.17 A.8 PROMPTS Given the program below, improve its performance: ### Program: {src_code} ### Optimized Version: Figure 10: Instruction-prompting for adapting LLMs. The model is provided with direct instructions to improve the performance of the given program. 20 Correct 87.47% 90.94% 91.04% Preprint. Under review. slow1 \u2192 fast1 || slow2 \u2192 fast2 || slow3 \u2192 fast3 || ... || slowN \u2192 (cid:44)\u2192 fastN ### Program: {src_code} ### Optimized Version: Figure 11: Few-shot prompting for in-context learning. The format \"slow \u2192 fast\" is used for adaptation. A test program is appended for inference. ### slower program: {src_code} Give a potential strategy improving the program: LLM reasoning: The current program calculates the sum of integers from 1 to n-1 using a loop. This iterative approach results in a time complexity of O(n). The sum of the first n natural numbers follows an arithmetic progression, and by leveraging Gauss\u2019s formula, n(n-1)/2, we can compute the sum directly. This formula-based approach allows us to calculate the sum in constant time, O(1), eliminating the need for iteration. ### optimized version of the same code with the strategy mentioned (cid:44)\u2192 above: LLM Response: optimized code Figure 12: Chain-of-thought prompting. The model\u2019s intermediate response and final program are high- lighted in blue, indicating they are produced by the LLM. similar_slow1 \u2192 similar_fast1 || similar_slow2 \u2192 similar_fast2 || ... (cid:44)\u2192 || similar_slowN \u2192 similar_fastN ### Program: {src_code} ### Optimized Version: Figure 13: Retrieval-based few-shot prompting. By dynamically retrieving analogous program structures or challenges, the model is guided to better harness patterns in PIE. 21 Preprint. Under review. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. ### Program: {src_code} ### Program: {src_code} ### Optimized Version: {tgt_code} ### Optimized Version: (a) Training Prompt. (b) Inference Prompt. Figure"}, {"question": " What optimizer was used for all experiments conducted with the 7B and 13B variants of CODELLAMA?", "answer": " AdamW optimizer", "ref_chunk": "promising rates of novelty. We found that after attempting to generate 10,000 new programs through the prompting strategy, 6,553 were not in the training/validation/test set of PIE. We keep track of equivalent programs of the ones generated, and of these 6,553 generations we found 3,314 equivalence sets. In total, this required executing over 1.4 million binary input pairs. Parallelized on a 24-core Intel 13900k processor with 64GB of RAM, this took less than 72 hours to complete. A.5 ABLATION OF RETRIEVAL-BASED FEW-SHOT PROMPTING CONFIGURATION For our retrieval-based prompting experiment we tried multiple configurations for the number of retrieved prompts where of K = {1, 2, 4} of the K closest retrieved prompts. Table 7: Retrieval-based few-shot prompting experiments: Results for various models and datasets configu- rations. *We note that for GPT4, we report only with Best@4, given resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Dynamic Retrieval, K=1 CodeLlama7B Dynamic Retrieval, K=1 CodeLlama13B Dynamic Retrieval, K=1 CodeLlama34B 3.39% 5.62% 10.51% 1.09 1.17 1.26 17.29% 22.59% 32.06% 16.33% 23.22% 35.67% 1.52 1.74 2.26 Dynamic Retrieval, K=2 CodeLlama7B Dynamic Retrieval, K=2 CodeLlama13B Dynamic Retrieval, K=2 CodeLlama34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=4 CodeLlama7B Dynamic Retrieval, K=4 CodeLlama13B Dynamic Retrieval, K=4 CodeLlama34B 5.54% 9.61% 12.12% 1.19 1.30 1.35 23.72% 27.69% 31.45% 18.81% 29.05% 43.68% 1.63 2.07 2.47 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 48.16% 80.82% 69.03%* 2.14 3.56* 18 Correct 52.49% 65.43% 72.61% 56.74% 66.25% 77.92% 60.08% 64.26% 75.44% 97.96% 95.90%* Preprint. Under review. using namespace std; typedef long long ll; inline void getInt(int* p); const int maxn=1000010; const int inf=0x3f3f3f3f; ll n; ll dp[maxn]; ll a[maxn]; int main() { gbtb; cin>>n; repd(i,1,n) { cin>>a[i]; } dp[1]=0; dp[0]=0; dp[2]=abs(a[2]-a[1]); repd(i,3,n) { dp[i]=min(dp[i-2]+abs(a[i]-a[i-2]),dp[i-1]+abs(a[i]-a[i-1])); } cout<<dp[n]; return 0; } inline void getInt(int* p) { char ch; do { ch = getchar(); } while (ch == ' ' ch == '\\n'); if (ch == '-') { p = -(getchar() - '0'); while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 - ch + '0'; } } else { p = ch - '0'; while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 + ch - '0'; } } } Figure 8: An example of a C++ program we found multiple submissions for as it is template code. Across these submissions, we found variance in the reported CPU runtime despite the code and competitive pro- gramming environment being identical. A.6 TRAINING DETAILS We fine-tuned the 7B and 13B variants using the HuggingFace Transformers library with FSDP to distribute the training process across 8\u00d7 48GB GPUs (NVIDIA RTX A6000/NVIDIA L40). For our high-quality dataset, which consists of approximately 4,000 examples, the models were fine-tuned until convergence was achieved, which can be done under 12 hours with 8 GPUs. For tasks related to full data fine-tuning and performance-conditioned fine-tuning, we only train for 1 epoch, which takes 24 to 36 hours, depending on the model of GPU used. All experiments were conducted using the AdamW optimizer (Loshchilov & Hutter, 2019). For the 7B and 13B variants of CODELLAMA, we used a batch size of 32 and a learning rate of 1e\u22125 for all of the experiments. 19 Preprint. Under review. Description 1: {description_1} Code 1: {code_1} Description 2: {description_2} Code 2: {code_2} Now, can you generate a program that takes that same input as Code 2 in Code 3 but produces different outputs? Write it to be as novel as possible. (cid:44)\u2192 (cid:44)\u2192 Code 3: Figure 9: The prompt template used for prompting GPT-3.5 for generating synthetic data for self-play. A.7 LORA RESULTS We show results using low-rank adaptors for finetuning in Table 8. Table 8: LoRA Experiments: Results for fine-tuning CODELLAMA with low rank adapters. A LoRA rank of 32 and LoRA alpha of 16 is used for all experiments listed. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 1.12% 0.41% 1.01 1.01 45.82% 59.47% 9.57% 9.67% 1.17 1.15 HQ CODELLAMA 13B 0.92% 1.02 59.57% 10.69% 1.17 A.8 PROMPTS Given the program below, improve its performance: ### Program: {src_code} ### Optimized Version: Figure 10: Instruction-prompting for adapting LLMs. The model is provided with direct instructions to improve the performance of the given program. 20 Correct 87.47% 90.94% 91.04% Preprint. Under review. slow1 \u2192 fast1 || slow2 \u2192 fast2 || slow3 \u2192 fast3 || ... || slowN \u2192 (cid:44)\u2192 fastN ### Program: {src_code} ### Optimized Version: Figure 11: Few-shot prompting for in-context learning. The format \"slow \u2192 fast\" is used for adaptation. A test program is appended for inference. ### slower program: {src_code} Give a potential strategy improving the program: LLM reasoning: The current program calculates the sum of integers from 1 to n-1 using a loop. This iterative approach results in a time complexity of O(n). The sum of the first n natural numbers follows an arithmetic progression, and by leveraging Gauss\u2019s formula, n(n-1)/2, we can compute the sum directly. This formula-based approach allows us to calculate the sum in constant time, O(1), eliminating the need for iteration. ### optimized version of the same code with the strategy mentioned (cid:44)\u2192 above: LLM Response: optimized code Figure 12: Chain-of-thought prompting. The model\u2019s intermediate response and final program are high- lighted in blue, indicating they are produced by the LLM. similar_slow1 \u2192 similar_fast1 || similar_slow2 \u2192 similar_fast2 || ... (cid:44)\u2192 || similar_slowN \u2192 similar_fastN ### Program: {src_code} ### Optimized Version: Figure 13: Retrieval-based few-shot prompting. By dynamically retrieving analogous program structures or challenges, the model is guided to better harness patterns in PIE. 21 Preprint. Under review. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. ### Program: {src_code} ### Program: {src_code} ### Optimized Version: {tgt_code} ### Optimized Version: (a) Training Prompt. (b) Inference Prompt. Figure"}, {"question": " What batch size and learning rate were used for the 7B and 13B variants of CODELLAMA?", "answer": " Batch size of 32 and learning rate of 1e\u22125", "ref_chunk": "promising rates of novelty. We found that after attempting to generate 10,000 new programs through the prompting strategy, 6,553 were not in the training/validation/test set of PIE. We keep track of equivalent programs of the ones generated, and of these 6,553 generations we found 3,314 equivalence sets. In total, this required executing over 1.4 million binary input pairs. Parallelized on a 24-core Intel 13900k processor with 64GB of RAM, this took less than 72 hours to complete. A.5 ABLATION OF RETRIEVAL-BASED FEW-SHOT PROMPTING CONFIGURATION For our retrieval-based prompting experiment we tried multiple configurations for the number of retrieved prompts where of K = {1, 2, 4} of the K closest retrieved prompts. Table 7: Retrieval-based few-shot prompting experiments: Results for various models and datasets configu- rations. *We note that for GPT4, we report only with Best@4, given resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Dynamic Retrieval, K=1 CodeLlama7B Dynamic Retrieval, K=1 CodeLlama13B Dynamic Retrieval, K=1 CodeLlama34B 3.39% 5.62% 10.51% 1.09 1.17 1.26 17.29% 22.59% 32.06% 16.33% 23.22% 35.67% 1.52 1.74 2.26 Dynamic Retrieval, K=2 CodeLlama7B Dynamic Retrieval, K=2 CodeLlama13B Dynamic Retrieval, K=2 CodeLlama34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=4 CodeLlama7B Dynamic Retrieval, K=4 CodeLlama13B Dynamic Retrieval, K=4 CodeLlama34B 5.54% 9.61% 12.12% 1.19 1.30 1.35 23.72% 27.69% 31.45% 18.81% 29.05% 43.68% 1.63 2.07 2.47 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 48.16% 80.82% 69.03%* 2.14 3.56* 18 Correct 52.49% 65.43% 72.61% 56.74% 66.25% 77.92% 60.08% 64.26% 75.44% 97.96% 95.90%* Preprint. Under review. using namespace std; typedef long long ll; inline void getInt(int* p); const int maxn=1000010; const int inf=0x3f3f3f3f; ll n; ll dp[maxn]; ll a[maxn]; int main() { gbtb; cin>>n; repd(i,1,n) { cin>>a[i]; } dp[1]=0; dp[0]=0; dp[2]=abs(a[2]-a[1]); repd(i,3,n) { dp[i]=min(dp[i-2]+abs(a[i]-a[i-2]),dp[i-1]+abs(a[i]-a[i-1])); } cout<<dp[n]; return 0; } inline void getInt(int* p) { char ch; do { ch = getchar(); } while (ch == ' ' ch == '\\n'); if (ch == '-') { p = -(getchar() - '0'); while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 - ch + '0'; } } else { p = ch - '0'; while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 + ch - '0'; } } } Figure 8: An example of a C++ program we found multiple submissions for as it is template code. Across these submissions, we found variance in the reported CPU runtime despite the code and competitive pro- gramming environment being identical. A.6 TRAINING DETAILS We fine-tuned the 7B and 13B variants using the HuggingFace Transformers library with FSDP to distribute the training process across 8\u00d7 48GB GPUs (NVIDIA RTX A6000/NVIDIA L40). For our high-quality dataset, which consists of approximately 4,000 examples, the models were fine-tuned until convergence was achieved, which can be done under 12 hours with 8 GPUs. For tasks related to full data fine-tuning and performance-conditioned fine-tuning, we only train for 1 epoch, which takes 24 to 36 hours, depending on the model of GPU used. All experiments were conducted using the AdamW optimizer (Loshchilov & Hutter, 2019). For the 7B and 13B variants of CODELLAMA, we used a batch size of 32 and a learning rate of 1e\u22125 for all of the experiments. 19 Preprint. Under review. Description 1: {description_1} Code 1: {code_1} Description 2: {description_2} Code 2: {code_2} Now, can you generate a program that takes that same input as Code 2 in Code 3 but produces different outputs? Write it to be as novel as possible. (cid:44)\u2192 (cid:44)\u2192 Code 3: Figure 9: The prompt template used for prompting GPT-3.5 for generating synthetic data for self-play. A.7 LORA RESULTS We show results using low-rank adaptors for finetuning in Table 8. Table 8: LoRA Experiments: Results for fine-tuning CODELLAMA with low rank adapters. A LoRA rank of 32 and LoRA alpha of 16 is used for all experiments listed. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 1.12% 0.41% 1.01 1.01 45.82% 59.47% 9.57% 9.67% 1.17 1.15 HQ CODELLAMA 13B 0.92% 1.02 59.57% 10.69% 1.17 A.8 PROMPTS Given the program below, improve its performance: ### Program: {src_code} ### Optimized Version: Figure 10: Instruction-prompting for adapting LLMs. The model is provided with direct instructions to improve the performance of the given program. 20 Correct 87.47% 90.94% 91.04% Preprint. Under review. slow1 \u2192 fast1 || slow2 \u2192 fast2 || slow3 \u2192 fast3 || ... || slowN \u2192 (cid:44)\u2192 fastN ### Program: {src_code} ### Optimized Version: Figure 11: Few-shot prompting for in-context learning. The format \"slow \u2192 fast\" is used for adaptation. A test program is appended for inference. ### slower program: {src_code} Give a potential strategy improving the program: LLM reasoning: The current program calculates the sum of integers from 1 to n-1 using a loop. This iterative approach results in a time complexity of O(n). The sum of the first n natural numbers follows an arithmetic progression, and by leveraging Gauss\u2019s formula, n(n-1)/2, we can compute the sum directly. This formula-based approach allows us to calculate the sum in constant time, O(1), eliminating the need for iteration. ### optimized version of the same code with the strategy mentioned (cid:44)\u2192 above: LLM Response: optimized code Figure 12: Chain-of-thought prompting. The model\u2019s intermediate response and final program are high- lighted in blue, indicating they are produced by the LLM. similar_slow1 \u2192 similar_fast1 || similar_slow2 \u2192 similar_fast2 || ... (cid:44)\u2192 || similar_slowN \u2192 similar_fastN ### Program: {src_code} ### Optimized Version: Figure 13: Retrieval-based few-shot prompting. By dynamically retrieving analogous program structures or challenges, the model is guided to better harness patterns in PIE. 21 Preprint. Under review. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. ### Program: {src_code} ### Program: {src_code} ### Optimized Version: {tgt_code} ### Optimized Version: (a) Training Prompt. (b) Inference Prompt. Figure"}, {"question": " How many examples were there in the high-quality dataset for fine-tuning the models?", "answer": " approximately 4,000 examples", "ref_chunk": "promising rates of novelty. We found that after attempting to generate 10,000 new programs through the prompting strategy, 6,553 were not in the training/validation/test set of PIE. We keep track of equivalent programs of the ones generated, and of these 6,553 generations we found 3,314 equivalence sets. In total, this required executing over 1.4 million binary input pairs. Parallelized on a 24-core Intel 13900k processor with 64GB of RAM, this took less than 72 hours to complete. A.5 ABLATION OF RETRIEVAL-BASED FEW-SHOT PROMPTING CONFIGURATION For our retrieval-based prompting experiment we tried multiple configurations for the number of retrieved prompts where of K = {1, 2, 4} of the K closest retrieved prompts. Table 7: Retrieval-based few-shot prompting experiments: Results for various models and datasets configu- rations. *We note that for GPT4, we report only with Best@4, given resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Dynamic Retrieval, K=1 CodeLlama7B Dynamic Retrieval, K=1 CodeLlama13B Dynamic Retrieval, K=1 CodeLlama34B 3.39% 5.62% 10.51% 1.09 1.17 1.26 17.29% 22.59% 32.06% 16.33% 23.22% 35.67% 1.52 1.74 2.26 Dynamic Retrieval, K=2 CodeLlama7B Dynamic Retrieval, K=2 CodeLlama13B Dynamic Retrieval, K=2 CodeLlama34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=4 CodeLlama7B Dynamic Retrieval, K=4 CodeLlama13B Dynamic Retrieval, K=4 CodeLlama34B 5.54% 9.61% 12.12% 1.19 1.30 1.35 23.72% 27.69% 31.45% 18.81% 29.05% 43.68% 1.63 2.07 2.47 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 48.16% 80.82% 69.03%* 2.14 3.56* 18 Correct 52.49% 65.43% 72.61% 56.74% 66.25% 77.92% 60.08% 64.26% 75.44% 97.96% 95.90%* Preprint. Under review. using namespace std; typedef long long ll; inline void getInt(int* p); const int maxn=1000010; const int inf=0x3f3f3f3f; ll n; ll dp[maxn]; ll a[maxn]; int main() { gbtb; cin>>n; repd(i,1,n) { cin>>a[i]; } dp[1]=0; dp[0]=0; dp[2]=abs(a[2]-a[1]); repd(i,3,n) { dp[i]=min(dp[i-2]+abs(a[i]-a[i-2]),dp[i-1]+abs(a[i]-a[i-1])); } cout<<dp[n]; return 0; } inline void getInt(int* p) { char ch; do { ch = getchar(); } while (ch == ' ' ch == '\\n'); if (ch == '-') { p = -(getchar() - '0'); while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 - ch + '0'; } } else { p = ch - '0'; while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 + ch - '0'; } } } Figure 8: An example of a C++ program we found multiple submissions for as it is template code. Across these submissions, we found variance in the reported CPU runtime despite the code and competitive pro- gramming environment being identical. A.6 TRAINING DETAILS We fine-tuned the 7B and 13B variants using the HuggingFace Transformers library with FSDP to distribute the training process across 8\u00d7 48GB GPUs (NVIDIA RTX A6000/NVIDIA L40). For our high-quality dataset, which consists of approximately 4,000 examples, the models were fine-tuned until convergence was achieved, which can be done under 12 hours with 8 GPUs. For tasks related to full data fine-tuning and performance-conditioned fine-tuning, we only train for 1 epoch, which takes 24 to 36 hours, depending on the model of GPU used. All experiments were conducted using the AdamW optimizer (Loshchilov & Hutter, 2019). For the 7B and 13B variants of CODELLAMA, we used a batch size of 32 and a learning rate of 1e\u22125 for all of the experiments. 19 Preprint. Under review. Description 1: {description_1} Code 1: {code_1} Description 2: {description_2} Code 2: {code_2} Now, can you generate a program that takes that same input as Code 2 in Code 3 but produces different outputs? Write it to be as novel as possible. (cid:44)\u2192 (cid:44)\u2192 Code 3: Figure 9: The prompt template used for prompting GPT-3.5 for generating synthetic data for self-play. A.7 LORA RESULTS We show results using low-rank adaptors for finetuning in Table 8. Table 8: LoRA Experiments: Results for fine-tuning CODELLAMA with low rank adapters. A LoRA rank of 32 and LoRA alpha of 16 is used for all experiments listed. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 1.12% 0.41% 1.01 1.01 45.82% 59.47% 9.57% 9.67% 1.17 1.15 HQ CODELLAMA 13B 0.92% 1.02 59.57% 10.69% 1.17 A.8 PROMPTS Given the program below, improve its performance: ### Program: {src_code} ### Optimized Version: Figure 10: Instruction-prompting for adapting LLMs. The model is provided with direct instructions to improve the performance of the given program. 20 Correct 87.47% 90.94% 91.04% Preprint. Under review. slow1 \u2192 fast1 || slow2 \u2192 fast2 || slow3 \u2192 fast3 || ... || slowN \u2192 (cid:44)\u2192 fastN ### Program: {src_code} ### Optimized Version: Figure 11: Few-shot prompting for in-context learning. The format \"slow \u2192 fast\" is used for adaptation. A test program is appended for inference. ### slower program: {src_code} Give a potential strategy improving the program: LLM reasoning: The current program calculates the sum of integers from 1 to n-1 using a loop. This iterative approach results in a time complexity of O(n). The sum of the first n natural numbers follows an arithmetic progression, and by leveraging Gauss\u2019s formula, n(n-1)/2, we can compute the sum directly. This formula-based approach allows us to calculate the sum in constant time, O(1), eliminating the need for iteration. ### optimized version of the same code with the strategy mentioned (cid:44)\u2192 above: LLM Response: optimized code Figure 12: Chain-of-thought prompting. The model\u2019s intermediate response and final program are high- lighted in blue, indicating they are produced by the LLM. similar_slow1 \u2192 similar_fast1 || similar_slow2 \u2192 similar_fast2 || ... (cid:44)\u2192 || similar_slowN \u2192 similar_fastN ### Program: {src_code} ### Optimized Version: Figure 13: Retrieval-based few-shot prompting. By dynamically retrieving analogous program structures or challenges, the model is guided to better harness patterns in PIE. 21 Preprint. Under review. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. ### Program: {src_code} ### Program: {src_code} ### Optimized Version: {tgt_code} ### Optimized Version: (a) Training Prompt. (b) Inference Prompt. Figure"}], "doc_text": "promising rates of novelty. We found that after attempting to generate 10,000 new programs through the prompting strategy, 6,553 were not in the training/validation/test set of PIE. We keep track of equivalent programs of the ones generated, and of these 6,553 generations we found 3,314 equivalence sets. In total, this required executing over 1.4 million binary input pairs. Parallelized on a 24-core Intel 13900k processor with 64GB of RAM, this took less than 72 hours to complete. A.5 ABLATION OF RETRIEVAL-BASED FEW-SHOT PROMPTING CONFIGURATION For our retrieval-based prompting experiment we tried multiple configurations for the number of retrieved prompts where of K = {1, 2, 4} of the K closest retrieved prompts. Table 7: Retrieval-based few-shot prompting experiments: Results for various models and datasets configu- rations. *We note that for GPT4, we report only with Best@4, given resource constraints. Best@1 Best@8 Method Model %Opt Speedup Correct %Opt Speedup Dynamic Retrieval, K=1 CodeLlama7B Dynamic Retrieval, K=1 CodeLlama13B Dynamic Retrieval, K=1 CodeLlama34B 3.39% 5.62% 10.51% 1.09 1.17 1.26 17.29% 22.59% 32.06% 16.33% 23.22% 35.67% 1.52 1.74 2.26 Dynamic Retrieval, K=2 CodeLlama7B Dynamic Retrieval, K=2 CodeLlama13B Dynamic Retrieval, K=2 CodeLlama34B 4.60% 9.40% 12.67% 1.14 1.36 1.33 21.21% 29.47% 31.87% 17.35% 28.74% 42.16% 1.52 1.99 2.57 Dynamic Retrieval, K=4 CodeLlama7B Dynamic Retrieval, K=4 CodeLlama13B Dynamic Retrieval, K=4 CodeLlama34B 5.54% 9.61% 12.12% 1.19 1.30 1.35 23.72% 27.69% 31.45% 18.81% 29.05% 43.68% 1.63 2.07 2.47 Dynamic Retrieval, K=2 Dynamic Retrieval, K=2 GPT3.5 GPT4 26.28% 50.15% 1.58 2.61 80.47% 48.16% 80.82% 69.03%* 2.14 3.56* 18 Correct 52.49% 65.43% 72.61% 56.74% 66.25% 77.92% 60.08% 64.26% 75.44% 97.96% 95.90%* Preprint. Under review. using namespace std; typedef long long ll; inline void getInt(int* p); const int maxn=1000010; const int inf=0x3f3f3f3f; ll n; ll dp[maxn]; ll a[maxn]; int main() { gbtb; cin>>n; repd(i,1,n) { cin>>a[i]; } dp[1]=0; dp[0]=0; dp[2]=abs(a[2]-a[1]); repd(i,3,n) { dp[i]=min(dp[i-2]+abs(a[i]-a[i-2]),dp[i-1]+abs(a[i]-a[i-1])); } cout<<dp[n]; return 0; } inline void getInt(int* p) { char ch; do { ch = getchar(); } while (ch == ' ' ch == '\\n'); if (ch == '-') { p = -(getchar() - '0'); while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 - ch + '0'; } } else { p = ch - '0'; while ((ch = getchar()) >= '0' && ch <= '9') { p = *p * 10 + ch - '0'; } } } Figure 8: An example of a C++ program we found multiple submissions for as it is template code. Across these submissions, we found variance in the reported CPU runtime despite the code and competitive pro- gramming environment being identical. A.6 TRAINING DETAILS We fine-tuned the 7B and 13B variants using the HuggingFace Transformers library with FSDP to distribute the training process across 8\u00d7 48GB GPUs (NVIDIA RTX A6000/NVIDIA L40). For our high-quality dataset, which consists of approximately 4,000 examples, the models were fine-tuned until convergence was achieved, which can be done under 12 hours with 8 GPUs. For tasks related to full data fine-tuning and performance-conditioned fine-tuning, we only train for 1 epoch, which takes 24 to 36 hours, depending on the model of GPU used. All experiments were conducted using the AdamW optimizer (Loshchilov & Hutter, 2019). For the 7B and 13B variants of CODELLAMA, we used a batch size of 32 and a learning rate of 1e\u22125 for all of the experiments. 19 Preprint. Under review. Description 1: {description_1} Code 1: {code_1} Description 2: {description_2} Code 2: {code_2} Now, can you generate a program that takes that same input as Code 2 in Code 3 but produces different outputs? Write it to be as novel as possible. (cid:44)\u2192 (cid:44)\u2192 Code 3: Figure 9: The prompt template used for prompting GPT-3.5 for generating synthetic data for self-play. A.7 LORA RESULTS We show results using low-rank adaptors for finetuning in Table 8. Table 8: LoRA Experiments: Results for fine-tuning CODELLAMA with low rank adapters. A LoRA rank of 32 and LoRA alpha of 16 is used for all experiments listed. Best@1 Best@8 Dataset Model %Opt Speedup Correct %Opt Speedup All All CODELLAMA 7B CODELLAMA 13B 1.12% 0.41% 1.01 1.01 45.82% 59.47% 9.57% 9.67% 1.17 1.15 HQ CODELLAMA 13B 0.92% 1.02 59.57% 10.69% 1.17 A.8 PROMPTS Given the program below, improve its performance: ### Program: {src_code} ### Optimized Version: Figure 10: Instruction-prompting for adapting LLMs. The model is provided with direct instructions to improve the performance of the given program. 20 Correct 87.47% 90.94% 91.04% Preprint. Under review. slow1 \u2192 fast1 || slow2 \u2192 fast2 || slow3 \u2192 fast3 || ... || slowN \u2192 (cid:44)\u2192 fastN ### Program: {src_code} ### Optimized Version: Figure 11: Few-shot prompting for in-context learning. The format \"slow \u2192 fast\" is used for adaptation. A test program is appended for inference. ### slower program: {src_code} Give a potential strategy improving the program: LLM reasoning: The current program calculates the sum of integers from 1 to n-1 using a loop. This iterative approach results in a time complexity of O(n). The sum of the first n natural numbers follows an arithmetic progression, and by leveraging Gauss\u2019s formula, n(n-1)/2, we can compute the sum directly. This formula-based approach allows us to calculate the sum in constant time, O(1), eliminating the need for iteration. ### optimized version of the same code with the strategy mentioned (cid:44)\u2192 above: LLM Response: optimized code Figure 12: Chain-of-thought prompting. The model\u2019s intermediate response and final program are high- lighted in blue, indicating they are produced by the LLM. similar_slow1 \u2192 similar_fast1 || similar_slow2 \u2192 similar_fast2 || ... (cid:44)\u2192 || similar_slowN \u2192 similar_fastN ### Program: {src_code} ### Optimized Version: Figure 13: Retrieval-based few-shot prompting. By dynamically retrieving analogous program structures or challenges, the model is guided to better harness patterns in PIE. 21 Preprint. Under review. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. Below is a program. Optimize the (cid:44)\u2192 (cid:44)\u2192 program and provide a more efficient version. ### Program: {src_code} ### Program: {src_code} ### Optimized Version: {tgt_code} ### Optimized Version: (a) Training Prompt. (b) Inference Prompt. Figure"}