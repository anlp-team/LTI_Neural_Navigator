{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/T._Mitamura_Hierarchical_Event_Grounding_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is MaP in the context of evaluating models?", "answer": " MaP = 1/N \u2211 i=1^N |Ei \u2229 \u02c6Ei| / | \u02c6Ei|", "ref_chunk": "Ei MaP = 1 N N X i=1 |Ei \u2229 \u02c6Ei| | \u02c6Ei| , MaR = 1 N N X i=1 |Ei \u2229 \u02c6Ei| |Ei| Macro F1 = 2MaP \u00b7 MaR MaP + MaR N MiP = P i=1 |Ei \u2229 \u02c6Ei| i=1 | \u02c6Ei| P N , MiR = P N i=1 |Ei \u2229 \u02c6Ei| N i=1 |Ei| P Micro F1 = 2MiP \u00b7 MiR MiP + MiR We additionally evaluate the strict accuracy of cross- encoders where we report the top-x reranked candidates as predictions with x being the number of gold events linked with the mention. This is the same condition as evaluating the bi-encoder on Recall@min. It enables a direct compar- ison of strict accuracy to Recall@min such that to assess if cross-encoders make improvements on bi-encoders. We de- note the strict accuracy calculated under this condition as strict accuracy (top min). (6) (7) (8) (9) (10) Hierarchical Relation Extraction: As de\ufb01ned in \u00a73.2, we evaluate the proposed hierarchical relation extraction method on whether it could identify the parent for a given child event. In particular, given an event with a ranked list of candidate parents (generated by the proposed method), we measure Recall@k for the gold parent in the list. Since Recall@k is ill-de\ufb01ned for events without a parent, we only calculate it for the non-root events within hierarchies of dev and test set. For those events that have parents but are not linked to any mentions by the bi-encoder, they are added as miss at every k. Such evaluation with Recall@k measure is similar to the HIT@k evaluation in the KB link prediction literature (Bordes et al. 2011). 6.2 Bi-encoder Models As discussed in \u00a75, we evaluate the baseline bi-encoder (Baseline) and three hierarchy-aware con\ufb01gurations: Hierar- chy Pretraining (Baseline + HP); Hierarchy Joint Learning (Baseline + HJL); Hierarchy Pretraining & Hierarchy Joint Learning (Baseline + HP + HJL). 6.3 Cross-encoder Models Given the top-k retrieval results from each of the aforemen- tioned bi-encoders, we train and evaluate a unique crossen- coder respectively. The value of k used for all cross-encoder experiments is selected to balance retrieval qualities (i.e. bi- encoder Recall@k on dev set) and computation throughput (\u00a77.1). In case some of the gold events are not retrieved among top-k candidates for the corresponding mention in the training set, we substitute each missing gold event for the negative candidates with the current lowest probability and repeat this process until all the missing gold events are added. At inference time, we apply a threshold \u03c4c to the reranked event candidates and emit those with score \u2265 \u03c4c as \ufb01nal predictions. If there is no event yielded, we add a NULL event to the prediction. Hierarchical Relation Extraction: We apply the pro- posed method to top-4 retrieval results from the best- performing bi-encoder to perform hierarchical relation ex- traction. 7 Result and Analysis 7.1 Bi-encoder Bi-encoder retrieval results on the dev split for both multi- and cross-lingual tasks are illustrated in Figure 3 and Fig- ure 4 respectively. Since the gain in Recall@k is relatively minor when doubling k from 8 to 16 across all con\ufb01gura- tions and tasks, the cross-encoder is trained with the top 8 retrieved candidates, with the consideration of computa- tion ef\ufb01ciency. It is also shown that all con\ufb01gurations attain better performance when evaluated by retrieving the most atomic event only (set of dense dots vs line plots), which re\ufb02ects the bene\ufb01ts of following the gold hierarchies and in- dicates the performance upper-bound for current models that try to learn these hierarchies. We further report the quantitative results of bi-encoder Recall@min on dev and test set of both tasks in Table 2. Among all the hierarchy-integration strategies, hierarchical 100 90 k @ l l a c e R 80 70 baseline + pre-train + joint + pre-train & joint 60 1 4 8 16 k: # retrieved events Figure 3: Multilingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. 90 80 k @ l l a c e R 70 60 baseline + pre-train + joint + pre-train & joint 50 1 4 8 16 k: # retrieved events Figure 4: Crosslingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. pretraining offers consistent improvements on both tasks compared with the baseline. On the other hand, hierarchi- cal joint learning presents a mixture of effects. In particu- lar, it attains the best performance on the crosslingual test set when applied in conjunction with hierarchical pretrain- ing while contributing negatively in all other scenarios. In terms of task languages, all the multilingual con\ufb01gura- tions attain higher performance than their crosslingual coun- terparts, indicating that in general crosslingual task is more challenging than the multilingual task, which is similar to the single event linking scenario. As described in Section 6.1, we further report bi-encoder results under Recall@K (fraction) in Appendix D.1. 7.2 Cross-encoder Cross-encoder reranking results on both tasks are also shown in Table 2. On the multilingual task, all the cross-encoders Bi-encoder Cross-encoder Methods Recall@min Strict Acc Strict Acc (Top Min) Macro F1 Micro F1 Multilingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 65.4 / 54.8 71.3 / 58.1 63.3 / 51.4 67.6 / 55.2 34.4 / 37.6 40.4 / 39.2 43.6 / 40.2 38.2 / 39.9 57.8 / 59.5 61.7 / 60.3 63.6 / 60.8 60.4 / 60.6 56.4 / 62.8 60.8 / 62.0 62.1 / 60.1 57.6 / 61.4 53.0 / 58.3 57.5 / 58.7 59.2 / 57.5 54.7 / 57.8 Crosslingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 53.8 / 32.8 59.7 / 37.0 51.6 / 33.3 55.8 / 38.8 8.5 / 11.9 8.6 / 10.9 9.7 / 12.0 9.6 / 13.1 21.2 / 27.5 18.6 / 25.7 22.3 / 26.1 23.0 /"}, {"question": " How is Macro F1 calculated?", "answer": " Macro F1 = 2 * MaP * MaR / (MaP + MaR)", "ref_chunk": "Ei MaP = 1 N N X i=1 |Ei \u2229 \u02c6Ei| | \u02c6Ei| , MaR = 1 N N X i=1 |Ei \u2229 \u02c6Ei| |Ei| Macro F1 = 2MaP \u00b7 MaR MaP + MaR N MiP = P i=1 |Ei \u2229 \u02c6Ei| i=1 | \u02c6Ei| P N , MiR = P N i=1 |Ei \u2229 \u02c6Ei| N i=1 |Ei| P Micro F1 = 2MiP \u00b7 MiR MiP + MiR We additionally evaluate the strict accuracy of cross- encoders where we report the top-x reranked candidates as predictions with x being the number of gold events linked with the mention. This is the same condition as evaluating the bi-encoder on Recall@min. It enables a direct compar- ison of strict accuracy to Recall@min such that to assess if cross-encoders make improvements on bi-encoders. We de- note the strict accuracy calculated under this condition as strict accuracy (top min). (6) (7) (8) (9) (10) Hierarchical Relation Extraction: As de\ufb01ned in \u00a73.2, we evaluate the proposed hierarchical relation extraction method on whether it could identify the parent for a given child event. In particular, given an event with a ranked list of candidate parents (generated by the proposed method), we measure Recall@k for the gold parent in the list. Since Recall@k is ill-de\ufb01ned for events without a parent, we only calculate it for the non-root events within hierarchies of dev and test set. For those events that have parents but are not linked to any mentions by the bi-encoder, they are added as miss at every k. Such evaluation with Recall@k measure is similar to the HIT@k evaluation in the KB link prediction literature (Bordes et al. 2011). 6.2 Bi-encoder Models As discussed in \u00a75, we evaluate the baseline bi-encoder (Baseline) and three hierarchy-aware con\ufb01gurations: Hierar- chy Pretraining (Baseline + HP); Hierarchy Joint Learning (Baseline + HJL); Hierarchy Pretraining & Hierarchy Joint Learning (Baseline + HP + HJL). 6.3 Cross-encoder Models Given the top-k retrieval results from each of the aforemen- tioned bi-encoders, we train and evaluate a unique crossen- coder respectively. The value of k used for all cross-encoder experiments is selected to balance retrieval qualities (i.e. bi- encoder Recall@k on dev set) and computation throughput (\u00a77.1). In case some of the gold events are not retrieved among top-k candidates for the corresponding mention in the training set, we substitute each missing gold event for the negative candidates with the current lowest probability and repeat this process until all the missing gold events are added. At inference time, we apply a threshold \u03c4c to the reranked event candidates and emit those with score \u2265 \u03c4c as \ufb01nal predictions. If there is no event yielded, we add a NULL event to the prediction. Hierarchical Relation Extraction: We apply the pro- posed method to top-4 retrieval results from the best- performing bi-encoder to perform hierarchical relation ex- traction. 7 Result and Analysis 7.1 Bi-encoder Bi-encoder retrieval results on the dev split for both multi- and cross-lingual tasks are illustrated in Figure 3 and Fig- ure 4 respectively. Since the gain in Recall@k is relatively minor when doubling k from 8 to 16 across all con\ufb01gura- tions and tasks, the cross-encoder is trained with the top 8 retrieved candidates, with the consideration of computa- tion ef\ufb01ciency. It is also shown that all con\ufb01gurations attain better performance when evaluated by retrieving the most atomic event only (set of dense dots vs line plots), which re\ufb02ects the bene\ufb01ts of following the gold hierarchies and in- dicates the performance upper-bound for current models that try to learn these hierarchies. We further report the quantitative results of bi-encoder Recall@min on dev and test set of both tasks in Table 2. Among all the hierarchy-integration strategies, hierarchical 100 90 k @ l l a c e R 80 70 baseline + pre-train + joint + pre-train & joint 60 1 4 8 16 k: # retrieved events Figure 3: Multilingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. 90 80 k @ l l a c e R 70 60 baseline + pre-train + joint + pre-train & joint 50 1 4 8 16 k: # retrieved events Figure 4: Crosslingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. pretraining offers consistent improvements on both tasks compared with the baseline. On the other hand, hierarchi- cal joint learning presents a mixture of effects. In particu- lar, it attains the best performance on the crosslingual test set when applied in conjunction with hierarchical pretrain- ing while contributing negatively in all other scenarios. In terms of task languages, all the multilingual con\ufb01gura- tions attain higher performance than their crosslingual coun- terparts, indicating that in general crosslingual task is more challenging than the multilingual task, which is similar to the single event linking scenario. As described in Section 6.1, we further report bi-encoder results under Recall@K (fraction) in Appendix D.1. 7.2 Cross-encoder Cross-encoder reranking results on both tasks are also shown in Table 2. On the multilingual task, all the cross-encoders Bi-encoder Cross-encoder Methods Recall@min Strict Acc Strict Acc (Top Min) Macro F1 Micro F1 Multilingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 65.4 / 54.8 71.3 / 58.1 63.3 / 51.4 67.6 / 55.2 34.4 / 37.6 40.4 / 39.2 43.6 / 40.2 38.2 / 39.9 57.8 / 59.5 61.7 / 60.3 63.6 / 60.8 60.4 / 60.6 56.4 / 62.8 60.8 / 62.0 62.1 / 60.1 57.6 / 61.4 53.0 / 58.3 57.5 / 58.7 59.2 / 57.5 54.7 / 57.8 Crosslingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 53.8 / 32.8 59.7 / 37.0 51.6 / 33.3 55.8 / 38.8 8.5 / 11.9 8.6 / 10.9 9.7 / 12.0 9.6 / 13.1 21.2 / 27.5 18.6 / 25.7 22.3 / 26.1 23.0 /"}, {"question": " What is the purpose of evaluating the strict accuracy of cross-encoders?", "answer": " To compare their performance to bi-encoders and assess if they make improvements", "ref_chunk": "Ei MaP = 1 N N X i=1 |Ei \u2229 \u02c6Ei| | \u02c6Ei| , MaR = 1 N N X i=1 |Ei \u2229 \u02c6Ei| |Ei| Macro F1 = 2MaP \u00b7 MaR MaP + MaR N MiP = P i=1 |Ei \u2229 \u02c6Ei| i=1 | \u02c6Ei| P N , MiR = P N i=1 |Ei \u2229 \u02c6Ei| N i=1 |Ei| P Micro F1 = 2MiP \u00b7 MiR MiP + MiR We additionally evaluate the strict accuracy of cross- encoders where we report the top-x reranked candidates as predictions with x being the number of gold events linked with the mention. This is the same condition as evaluating the bi-encoder on Recall@min. It enables a direct compar- ison of strict accuracy to Recall@min such that to assess if cross-encoders make improvements on bi-encoders. We de- note the strict accuracy calculated under this condition as strict accuracy (top min). (6) (7) (8) (9) (10) Hierarchical Relation Extraction: As de\ufb01ned in \u00a73.2, we evaluate the proposed hierarchical relation extraction method on whether it could identify the parent for a given child event. In particular, given an event with a ranked list of candidate parents (generated by the proposed method), we measure Recall@k for the gold parent in the list. Since Recall@k is ill-de\ufb01ned for events without a parent, we only calculate it for the non-root events within hierarchies of dev and test set. For those events that have parents but are not linked to any mentions by the bi-encoder, they are added as miss at every k. Such evaluation with Recall@k measure is similar to the HIT@k evaluation in the KB link prediction literature (Bordes et al. 2011). 6.2 Bi-encoder Models As discussed in \u00a75, we evaluate the baseline bi-encoder (Baseline) and three hierarchy-aware con\ufb01gurations: Hierar- chy Pretraining (Baseline + HP); Hierarchy Joint Learning (Baseline + HJL); Hierarchy Pretraining & Hierarchy Joint Learning (Baseline + HP + HJL). 6.3 Cross-encoder Models Given the top-k retrieval results from each of the aforemen- tioned bi-encoders, we train and evaluate a unique crossen- coder respectively. The value of k used for all cross-encoder experiments is selected to balance retrieval qualities (i.e. bi- encoder Recall@k on dev set) and computation throughput (\u00a77.1). In case some of the gold events are not retrieved among top-k candidates for the corresponding mention in the training set, we substitute each missing gold event for the negative candidates with the current lowest probability and repeat this process until all the missing gold events are added. At inference time, we apply a threshold \u03c4c to the reranked event candidates and emit those with score \u2265 \u03c4c as \ufb01nal predictions. If there is no event yielded, we add a NULL event to the prediction. Hierarchical Relation Extraction: We apply the pro- posed method to top-4 retrieval results from the best- performing bi-encoder to perform hierarchical relation ex- traction. 7 Result and Analysis 7.1 Bi-encoder Bi-encoder retrieval results on the dev split for both multi- and cross-lingual tasks are illustrated in Figure 3 and Fig- ure 4 respectively. Since the gain in Recall@k is relatively minor when doubling k from 8 to 16 across all con\ufb01gura- tions and tasks, the cross-encoder is trained with the top 8 retrieved candidates, with the consideration of computa- tion ef\ufb01ciency. It is also shown that all con\ufb01gurations attain better performance when evaluated by retrieving the most atomic event only (set of dense dots vs line plots), which re\ufb02ects the bene\ufb01ts of following the gold hierarchies and in- dicates the performance upper-bound for current models that try to learn these hierarchies. We further report the quantitative results of bi-encoder Recall@min on dev and test set of both tasks in Table 2. Among all the hierarchy-integration strategies, hierarchical 100 90 k @ l l a c e R 80 70 baseline + pre-train + joint + pre-train & joint 60 1 4 8 16 k: # retrieved events Figure 3: Multilingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. 90 80 k @ l l a c e R 70 60 baseline + pre-train + joint + pre-train & joint 50 1 4 8 16 k: # retrieved events Figure 4: Crosslingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. pretraining offers consistent improvements on both tasks compared with the baseline. On the other hand, hierarchi- cal joint learning presents a mixture of effects. In particu- lar, it attains the best performance on the crosslingual test set when applied in conjunction with hierarchical pretrain- ing while contributing negatively in all other scenarios. In terms of task languages, all the multilingual con\ufb01gura- tions attain higher performance than their crosslingual coun- terparts, indicating that in general crosslingual task is more challenging than the multilingual task, which is similar to the single event linking scenario. As described in Section 6.1, we further report bi-encoder results under Recall@K (fraction) in Appendix D.1. 7.2 Cross-encoder Cross-encoder reranking results on both tasks are also shown in Table 2. On the multilingual task, all the cross-encoders Bi-encoder Cross-encoder Methods Recall@min Strict Acc Strict Acc (Top Min) Macro F1 Micro F1 Multilingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 65.4 / 54.8 71.3 / 58.1 63.3 / 51.4 67.6 / 55.2 34.4 / 37.6 40.4 / 39.2 43.6 / 40.2 38.2 / 39.9 57.8 / 59.5 61.7 / 60.3 63.6 / 60.8 60.4 / 60.6 56.4 / 62.8 60.8 / 62.0 62.1 / 60.1 57.6 / 61.4 53.0 / 58.3 57.5 / 58.7 59.2 / 57.5 54.7 / 57.8 Crosslingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 53.8 / 32.8 59.7 / 37.0 51.6 / 33.3 55.8 / 38.8 8.5 / 11.9 8.6 / 10.9 9.7 / 12.0 9.6 / 13.1 21.2 / 27.5 18.6 / 25.7 22.3 / 26.1 23.0 /"}, {"question": " How is Recall@k evaluated in the hierarchical relation extraction method?", "answer": " By measuring how well the method can identify the parent for a given child event", "ref_chunk": "Ei MaP = 1 N N X i=1 |Ei \u2229 \u02c6Ei| | \u02c6Ei| , MaR = 1 N N X i=1 |Ei \u2229 \u02c6Ei| |Ei| Macro F1 = 2MaP \u00b7 MaR MaP + MaR N MiP = P i=1 |Ei \u2229 \u02c6Ei| i=1 | \u02c6Ei| P N , MiR = P N i=1 |Ei \u2229 \u02c6Ei| N i=1 |Ei| P Micro F1 = 2MiP \u00b7 MiR MiP + MiR We additionally evaluate the strict accuracy of cross- encoders where we report the top-x reranked candidates as predictions with x being the number of gold events linked with the mention. This is the same condition as evaluating the bi-encoder on Recall@min. It enables a direct compar- ison of strict accuracy to Recall@min such that to assess if cross-encoders make improvements on bi-encoders. We de- note the strict accuracy calculated under this condition as strict accuracy (top min). (6) (7) (8) (9) (10) Hierarchical Relation Extraction: As de\ufb01ned in \u00a73.2, we evaluate the proposed hierarchical relation extraction method on whether it could identify the parent for a given child event. In particular, given an event with a ranked list of candidate parents (generated by the proposed method), we measure Recall@k for the gold parent in the list. Since Recall@k is ill-de\ufb01ned for events without a parent, we only calculate it for the non-root events within hierarchies of dev and test set. For those events that have parents but are not linked to any mentions by the bi-encoder, they are added as miss at every k. Such evaluation with Recall@k measure is similar to the HIT@k evaluation in the KB link prediction literature (Bordes et al. 2011). 6.2 Bi-encoder Models As discussed in \u00a75, we evaluate the baseline bi-encoder (Baseline) and three hierarchy-aware con\ufb01gurations: Hierar- chy Pretraining (Baseline + HP); Hierarchy Joint Learning (Baseline + HJL); Hierarchy Pretraining & Hierarchy Joint Learning (Baseline + HP + HJL). 6.3 Cross-encoder Models Given the top-k retrieval results from each of the aforemen- tioned bi-encoders, we train and evaluate a unique crossen- coder respectively. The value of k used for all cross-encoder experiments is selected to balance retrieval qualities (i.e. bi- encoder Recall@k on dev set) and computation throughput (\u00a77.1). In case some of the gold events are not retrieved among top-k candidates for the corresponding mention in the training set, we substitute each missing gold event for the negative candidates with the current lowest probability and repeat this process until all the missing gold events are added. At inference time, we apply a threshold \u03c4c to the reranked event candidates and emit those with score \u2265 \u03c4c as \ufb01nal predictions. If there is no event yielded, we add a NULL event to the prediction. Hierarchical Relation Extraction: We apply the pro- posed method to top-4 retrieval results from the best- performing bi-encoder to perform hierarchical relation ex- traction. 7 Result and Analysis 7.1 Bi-encoder Bi-encoder retrieval results on the dev split for both multi- and cross-lingual tasks are illustrated in Figure 3 and Fig- ure 4 respectively. Since the gain in Recall@k is relatively minor when doubling k from 8 to 16 across all con\ufb01gura- tions and tasks, the cross-encoder is trained with the top 8 retrieved candidates, with the consideration of computa- tion ef\ufb01ciency. It is also shown that all con\ufb01gurations attain better performance when evaluated by retrieving the most atomic event only (set of dense dots vs line plots), which re\ufb02ects the bene\ufb01ts of following the gold hierarchies and in- dicates the performance upper-bound for current models that try to learn these hierarchies. We further report the quantitative results of bi-encoder Recall@min on dev and test set of both tasks in Table 2. Among all the hierarchy-integration strategies, hierarchical 100 90 k @ l l a c e R 80 70 baseline + pre-train + joint + pre-train & joint 60 1 4 8 16 k: # retrieved events Figure 3: Multilingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. 90 80 k @ l l a c e R 70 60 baseline + pre-train + joint + pre-train & joint 50 1 4 8 16 k: # retrieved events Figure 4: Crosslingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. pretraining offers consistent improvements on both tasks compared with the baseline. On the other hand, hierarchi- cal joint learning presents a mixture of effects. In particu- lar, it attains the best performance on the crosslingual test set when applied in conjunction with hierarchical pretrain- ing while contributing negatively in all other scenarios. In terms of task languages, all the multilingual con\ufb01gura- tions attain higher performance than their crosslingual coun- terparts, indicating that in general crosslingual task is more challenging than the multilingual task, which is similar to the single event linking scenario. As described in Section 6.1, we further report bi-encoder results under Recall@K (fraction) in Appendix D.1. 7.2 Cross-encoder Cross-encoder reranking results on both tasks are also shown in Table 2. On the multilingual task, all the cross-encoders Bi-encoder Cross-encoder Methods Recall@min Strict Acc Strict Acc (Top Min) Macro F1 Micro F1 Multilingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 65.4 / 54.8 71.3 / 58.1 63.3 / 51.4 67.6 / 55.2 34.4 / 37.6 40.4 / 39.2 43.6 / 40.2 38.2 / 39.9 57.8 / 59.5 61.7 / 60.3 63.6 / 60.8 60.4 / 60.6 56.4 / 62.8 60.8 / 62.0 62.1 / 60.1 57.6 / 61.4 53.0 / 58.3 57.5 / 58.7 59.2 / 57.5 54.7 / 57.8 Crosslingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 53.8 / 32.8 59.7 / 37.0 51.6 / 33.3 55.8 / 38.8 8.5 / 11.9 8.6 / 10.9 9.7 / 12.0 9.6 / 13.1 21.2 / 27.5 18.6 / 25.7 22.3 / 26.1 23.0 /"}, {"question": " What are the three hierarchy-aware configurations for bi-encoder models?", "answer": " Hierarchy Pretraining (Baseline + HP); Hierarchy Joint Learning (Baseline + HJL); Hierarchy Pretraining & Hierarchy Joint Learning (Baseline + HP + HJL)", "ref_chunk": "Ei MaP = 1 N N X i=1 |Ei \u2229 \u02c6Ei| | \u02c6Ei| , MaR = 1 N N X i=1 |Ei \u2229 \u02c6Ei| |Ei| Macro F1 = 2MaP \u00b7 MaR MaP + MaR N MiP = P i=1 |Ei \u2229 \u02c6Ei| i=1 | \u02c6Ei| P N , MiR = P N i=1 |Ei \u2229 \u02c6Ei| N i=1 |Ei| P Micro F1 = 2MiP \u00b7 MiR MiP + MiR We additionally evaluate the strict accuracy of cross- encoders where we report the top-x reranked candidates as predictions with x being the number of gold events linked with the mention. This is the same condition as evaluating the bi-encoder on Recall@min. It enables a direct compar- ison of strict accuracy to Recall@min such that to assess if cross-encoders make improvements on bi-encoders. We de- note the strict accuracy calculated under this condition as strict accuracy (top min). (6) (7) (8) (9) (10) Hierarchical Relation Extraction: As de\ufb01ned in \u00a73.2, we evaluate the proposed hierarchical relation extraction method on whether it could identify the parent for a given child event. In particular, given an event with a ranked list of candidate parents (generated by the proposed method), we measure Recall@k for the gold parent in the list. Since Recall@k is ill-de\ufb01ned for events without a parent, we only calculate it for the non-root events within hierarchies of dev and test set. For those events that have parents but are not linked to any mentions by the bi-encoder, they are added as miss at every k. Such evaluation with Recall@k measure is similar to the HIT@k evaluation in the KB link prediction literature (Bordes et al. 2011). 6.2 Bi-encoder Models As discussed in \u00a75, we evaluate the baseline bi-encoder (Baseline) and three hierarchy-aware con\ufb01gurations: Hierar- chy Pretraining (Baseline + HP); Hierarchy Joint Learning (Baseline + HJL); Hierarchy Pretraining & Hierarchy Joint Learning (Baseline + HP + HJL). 6.3 Cross-encoder Models Given the top-k retrieval results from each of the aforemen- tioned bi-encoders, we train and evaluate a unique crossen- coder respectively. The value of k used for all cross-encoder experiments is selected to balance retrieval qualities (i.e. bi- encoder Recall@k on dev set) and computation throughput (\u00a77.1). In case some of the gold events are not retrieved among top-k candidates for the corresponding mention in the training set, we substitute each missing gold event for the negative candidates with the current lowest probability and repeat this process until all the missing gold events are added. At inference time, we apply a threshold \u03c4c to the reranked event candidates and emit those with score \u2265 \u03c4c as \ufb01nal predictions. If there is no event yielded, we add a NULL event to the prediction. Hierarchical Relation Extraction: We apply the pro- posed method to top-4 retrieval results from the best- performing bi-encoder to perform hierarchical relation ex- traction. 7 Result and Analysis 7.1 Bi-encoder Bi-encoder retrieval results on the dev split for both multi- and cross-lingual tasks are illustrated in Figure 3 and Fig- ure 4 respectively. Since the gain in Recall@k is relatively minor when doubling k from 8 to 16 across all con\ufb01gura- tions and tasks, the cross-encoder is trained with the top 8 retrieved candidates, with the consideration of computa- tion ef\ufb01ciency. It is also shown that all con\ufb01gurations attain better performance when evaluated by retrieving the most atomic event only (set of dense dots vs line plots), which re\ufb02ects the bene\ufb01ts of following the gold hierarchies and in- dicates the performance upper-bound for current models that try to learn these hierarchies. We further report the quantitative results of bi-encoder Recall@min on dev and test set of both tasks in Table 2. Among all the hierarchy-integration strategies, hierarchical 100 90 k @ l l a c e R 80 70 baseline + pre-train + joint + pre-train & joint 60 1 4 8 16 k: # retrieved events Figure 3: Multilingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. 90 80 k @ l l a c e R 70 60 baseline + pre-train + joint + pre-train & joint 50 1 4 8 16 k: # retrieved events Figure 4: Crosslingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. pretraining offers consistent improvements on both tasks compared with the baseline. On the other hand, hierarchi- cal joint learning presents a mixture of effects. In particu- lar, it attains the best performance on the crosslingual test set when applied in conjunction with hierarchical pretrain- ing while contributing negatively in all other scenarios. In terms of task languages, all the multilingual con\ufb01gura- tions attain higher performance than their crosslingual coun- terparts, indicating that in general crosslingual task is more challenging than the multilingual task, which is similar to the single event linking scenario. As described in Section 6.1, we further report bi-encoder results under Recall@K (fraction) in Appendix D.1. 7.2 Cross-encoder Cross-encoder reranking results on both tasks are also shown in Table 2. On the multilingual task, all the cross-encoders Bi-encoder Cross-encoder Methods Recall@min Strict Acc Strict Acc (Top Min) Macro F1 Micro F1 Multilingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 65.4 / 54.8 71.3 / 58.1 63.3 / 51.4 67.6 / 55.2 34.4 / 37.6 40.4 / 39.2 43.6 / 40.2 38.2 / 39.9 57.8 / 59.5 61.7 / 60.3 63.6 / 60.8 60.4 / 60.6 56.4 / 62.8 60.8 / 62.0 62.1 / 60.1 57.6 / 61.4 53.0 / 58.3 57.5 / 58.7 59.2 / 57.5 54.7 / 57.8 Crosslingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 53.8 / 32.8 59.7 / 37.0 51.6 / 33.3 55.8 / 38.8 8.5 / 11.9 8.6 / 10.9 9.7 / 12.0 9.6 / 13.1 21.2 / 27.5 18.6 / 25.7 22.3 / 26.1 23.0 /"}, {"question": " How are cross-encoder models trained and evaluated?", "answer": " Based on the top-k retrieval results from bi-encoders, with consideration of retrieval qualities and computation throughput", "ref_chunk": "Ei MaP = 1 N N X i=1 |Ei \u2229 \u02c6Ei| | \u02c6Ei| , MaR = 1 N N X i=1 |Ei \u2229 \u02c6Ei| |Ei| Macro F1 = 2MaP \u00b7 MaR MaP + MaR N MiP = P i=1 |Ei \u2229 \u02c6Ei| i=1 | \u02c6Ei| P N , MiR = P N i=1 |Ei \u2229 \u02c6Ei| N i=1 |Ei| P Micro F1 = 2MiP \u00b7 MiR MiP + MiR We additionally evaluate the strict accuracy of cross- encoders where we report the top-x reranked candidates as predictions with x being the number of gold events linked with the mention. This is the same condition as evaluating the bi-encoder on Recall@min. It enables a direct compar- ison of strict accuracy to Recall@min such that to assess if cross-encoders make improvements on bi-encoders. We de- note the strict accuracy calculated under this condition as strict accuracy (top min). (6) (7) (8) (9) (10) Hierarchical Relation Extraction: As de\ufb01ned in \u00a73.2, we evaluate the proposed hierarchical relation extraction method on whether it could identify the parent for a given child event. In particular, given an event with a ranked list of candidate parents (generated by the proposed method), we measure Recall@k for the gold parent in the list. Since Recall@k is ill-de\ufb01ned for events without a parent, we only calculate it for the non-root events within hierarchies of dev and test set. For those events that have parents but are not linked to any mentions by the bi-encoder, they are added as miss at every k. Such evaluation with Recall@k measure is similar to the HIT@k evaluation in the KB link prediction literature (Bordes et al. 2011). 6.2 Bi-encoder Models As discussed in \u00a75, we evaluate the baseline bi-encoder (Baseline) and three hierarchy-aware con\ufb01gurations: Hierar- chy Pretraining (Baseline + HP); Hierarchy Joint Learning (Baseline + HJL); Hierarchy Pretraining & Hierarchy Joint Learning (Baseline + HP + HJL). 6.3 Cross-encoder Models Given the top-k retrieval results from each of the aforemen- tioned bi-encoders, we train and evaluate a unique crossen- coder respectively. The value of k used for all cross-encoder experiments is selected to balance retrieval qualities (i.e. bi- encoder Recall@k on dev set) and computation throughput (\u00a77.1). In case some of the gold events are not retrieved among top-k candidates for the corresponding mention in the training set, we substitute each missing gold event for the negative candidates with the current lowest probability and repeat this process until all the missing gold events are added. At inference time, we apply a threshold \u03c4c to the reranked event candidates and emit those with score \u2265 \u03c4c as \ufb01nal predictions. If there is no event yielded, we add a NULL event to the prediction. Hierarchical Relation Extraction: We apply the pro- posed method to top-4 retrieval results from the best- performing bi-encoder to perform hierarchical relation ex- traction. 7 Result and Analysis 7.1 Bi-encoder Bi-encoder retrieval results on the dev split for both multi- and cross-lingual tasks are illustrated in Figure 3 and Fig- ure 4 respectively. Since the gain in Recall@k is relatively minor when doubling k from 8 to 16 across all con\ufb01gura- tions and tasks, the cross-encoder is trained with the top 8 retrieved candidates, with the consideration of computa- tion ef\ufb01ciency. It is also shown that all con\ufb01gurations attain better performance when evaluated by retrieving the most atomic event only (set of dense dots vs line plots), which re\ufb02ects the bene\ufb01ts of following the gold hierarchies and in- dicates the performance upper-bound for current models that try to learn these hierarchies. We further report the quantitative results of bi-encoder Recall@min on dev and test set of both tasks in Table 2. Among all the hierarchy-integration strategies, hierarchical 100 90 k @ l l a c e R 80 70 baseline + pre-train + joint + pre-train & joint 60 1 4 8 16 k: # retrieved events Figure 3: Multilingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. 90 80 k @ l l a c e R 70 60 baseline + pre-train + joint + pre-train & joint 50 1 4 8 16 k: # retrieved events Figure 4: Crosslingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. pretraining offers consistent improvements on both tasks compared with the baseline. On the other hand, hierarchi- cal joint learning presents a mixture of effects. In particu- lar, it attains the best performance on the crosslingual test set when applied in conjunction with hierarchical pretrain- ing while contributing negatively in all other scenarios. In terms of task languages, all the multilingual con\ufb01gura- tions attain higher performance than their crosslingual coun- terparts, indicating that in general crosslingual task is more challenging than the multilingual task, which is similar to the single event linking scenario. As described in Section 6.1, we further report bi-encoder results under Recall@K (fraction) in Appendix D.1. 7.2 Cross-encoder Cross-encoder reranking results on both tasks are also shown in Table 2. On the multilingual task, all the cross-encoders Bi-encoder Cross-encoder Methods Recall@min Strict Acc Strict Acc (Top Min) Macro F1 Micro F1 Multilingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 65.4 / 54.8 71.3 / 58.1 63.3 / 51.4 67.6 / 55.2 34.4 / 37.6 40.4 / 39.2 43.6 / 40.2 38.2 / 39.9 57.8 / 59.5 61.7 / 60.3 63.6 / 60.8 60.4 / 60.6 56.4 / 62.8 60.8 / 62.0 62.1 / 60.1 57.6 / 61.4 53.0 / 58.3 57.5 / 58.7 59.2 / 57.5 54.7 / 57.8 Crosslingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 53.8 / 32.8 59.7 / 37.0 51.6 / 33.3 55.8 / 38.8 8.5 / 11.9 8.6 / 10.9 9.7 / 12.0 9.6 / 13.1 21.2 / 27.5 18.6 / 25.7 22.3 / 26.1 23.0 /"}, {"question": " What is done when some gold events are not retrieved in the training set for cross-encoder models?", "answer": " The missing gold events are substituted with negative candidates with the lowest probability until all missing gold events are included", "ref_chunk": "Ei MaP = 1 N N X i=1 |Ei \u2229 \u02c6Ei| | \u02c6Ei| , MaR = 1 N N X i=1 |Ei \u2229 \u02c6Ei| |Ei| Macro F1 = 2MaP \u00b7 MaR MaP + MaR N MiP = P i=1 |Ei \u2229 \u02c6Ei| i=1 | \u02c6Ei| P N , MiR = P N i=1 |Ei \u2229 \u02c6Ei| N i=1 |Ei| P Micro F1 = 2MiP \u00b7 MiR MiP + MiR We additionally evaluate the strict accuracy of cross- encoders where we report the top-x reranked candidates as predictions with x being the number of gold events linked with the mention. This is the same condition as evaluating the bi-encoder on Recall@min. It enables a direct compar- ison of strict accuracy to Recall@min such that to assess if cross-encoders make improvements on bi-encoders. We de- note the strict accuracy calculated under this condition as strict accuracy (top min). (6) (7) (8) (9) (10) Hierarchical Relation Extraction: As de\ufb01ned in \u00a73.2, we evaluate the proposed hierarchical relation extraction method on whether it could identify the parent for a given child event. In particular, given an event with a ranked list of candidate parents (generated by the proposed method), we measure Recall@k for the gold parent in the list. Since Recall@k is ill-de\ufb01ned for events without a parent, we only calculate it for the non-root events within hierarchies of dev and test set. For those events that have parents but are not linked to any mentions by the bi-encoder, they are added as miss at every k. Such evaluation with Recall@k measure is similar to the HIT@k evaluation in the KB link prediction literature (Bordes et al. 2011). 6.2 Bi-encoder Models As discussed in \u00a75, we evaluate the baseline bi-encoder (Baseline) and three hierarchy-aware con\ufb01gurations: Hierar- chy Pretraining (Baseline + HP); Hierarchy Joint Learning (Baseline + HJL); Hierarchy Pretraining & Hierarchy Joint Learning (Baseline + HP + HJL). 6.3 Cross-encoder Models Given the top-k retrieval results from each of the aforemen- tioned bi-encoders, we train and evaluate a unique crossen- coder respectively. The value of k used for all cross-encoder experiments is selected to balance retrieval qualities (i.e. bi- encoder Recall@k on dev set) and computation throughput (\u00a77.1). In case some of the gold events are not retrieved among top-k candidates for the corresponding mention in the training set, we substitute each missing gold event for the negative candidates with the current lowest probability and repeat this process until all the missing gold events are added. At inference time, we apply a threshold \u03c4c to the reranked event candidates and emit those with score \u2265 \u03c4c as \ufb01nal predictions. If there is no event yielded, we add a NULL event to the prediction. Hierarchical Relation Extraction: We apply the pro- posed method to top-4 retrieval results from the best- performing bi-encoder to perform hierarchical relation ex- traction. 7 Result and Analysis 7.1 Bi-encoder Bi-encoder retrieval results on the dev split for both multi- and cross-lingual tasks are illustrated in Figure 3 and Fig- ure 4 respectively. Since the gain in Recall@k is relatively minor when doubling k from 8 to 16 across all con\ufb01gura- tions and tasks, the cross-encoder is trained with the top 8 retrieved candidates, with the consideration of computa- tion ef\ufb01ciency. It is also shown that all con\ufb01gurations attain better performance when evaluated by retrieving the most atomic event only (set of dense dots vs line plots), which re\ufb02ects the bene\ufb01ts of following the gold hierarchies and in- dicates the performance upper-bound for current models that try to learn these hierarchies. We further report the quantitative results of bi-encoder Recall@min on dev and test set of both tasks in Table 2. Among all the hierarchy-integration strategies, hierarchical 100 90 k @ l l a c e R 80 70 baseline + pre-train + joint + pre-train & joint 60 1 4 8 16 k: # retrieved events Figure 3: Multilingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. 90 80 k @ l l a c e R 70 60 baseline + pre-train + joint + pre-train & joint 50 1 4 8 16 k: # retrieved events Figure 4: Crosslingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. pretraining offers consistent improvements on both tasks compared with the baseline. On the other hand, hierarchi- cal joint learning presents a mixture of effects. In particu- lar, it attains the best performance on the crosslingual test set when applied in conjunction with hierarchical pretrain- ing while contributing negatively in all other scenarios. In terms of task languages, all the multilingual con\ufb01gura- tions attain higher performance than their crosslingual coun- terparts, indicating that in general crosslingual task is more challenging than the multilingual task, which is similar to the single event linking scenario. As described in Section 6.1, we further report bi-encoder results under Recall@K (fraction) in Appendix D.1. 7.2 Cross-encoder Cross-encoder reranking results on both tasks are also shown in Table 2. On the multilingual task, all the cross-encoders Bi-encoder Cross-encoder Methods Recall@min Strict Acc Strict Acc (Top Min) Macro F1 Micro F1 Multilingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 65.4 / 54.8 71.3 / 58.1 63.3 / 51.4 67.6 / 55.2 34.4 / 37.6 40.4 / 39.2 43.6 / 40.2 38.2 / 39.9 57.8 / 59.5 61.7 / 60.3 63.6 / 60.8 60.4 / 60.6 56.4 / 62.8 60.8 / 62.0 62.1 / 60.1 57.6 / 61.4 53.0 / 58.3 57.5 / 58.7 59.2 / 57.5 54.7 / 57.8 Crosslingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 53.8 / 32.8 59.7 / 37.0 51.6 / 33.3 55.8 / 38.8 8.5 / 11.9 8.6 / 10.9 9.7 / 12.0 9.6 / 13.1 21.2 / 27.5 18.6 / 25.7 22.3 / 26.1 23.0 /"}, {"question": " How is performance evaluated for bi-encoder models in terms of Recall@min?", "answer": " Quantitative results are reported on the dev and test set for both tasks", "ref_chunk": "Ei MaP = 1 N N X i=1 |Ei \u2229 \u02c6Ei| | \u02c6Ei| , MaR = 1 N N X i=1 |Ei \u2229 \u02c6Ei| |Ei| Macro F1 = 2MaP \u00b7 MaR MaP + MaR N MiP = P i=1 |Ei \u2229 \u02c6Ei| i=1 | \u02c6Ei| P N , MiR = P N i=1 |Ei \u2229 \u02c6Ei| N i=1 |Ei| P Micro F1 = 2MiP \u00b7 MiR MiP + MiR We additionally evaluate the strict accuracy of cross- encoders where we report the top-x reranked candidates as predictions with x being the number of gold events linked with the mention. This is the same condition as evaluating the bi-encoder on Recall@min. It enables a direct compar- ison of strict accuracy to Recall@min such that to assess if cross-encoders make improvements on bi-encoders. We de- note the strict accuracy calculated under this condition as strict accuracy (top min). (6) (7) (8) (9) (10) Hierarchical Relation Extraction: As de\ufb01ned in \u00a73.2, we evaluate the proposed hierarchical relation extraction method on whether it could identify the parent for a given child event. In particular, given an event with a ranked list of candidate parents (generated by the proposed method), we measure Recall@k for the gold parent in the list. Since Recall@k is ill-de\ufb01ned for events without a parent, we only calculate it for the non-root events within hierarchies of dev and test set. For those events that have parents but are not linked to any mentions by the bi-encoder, they are added as miss at every k. Such evaluation with Recall@k measure is similar to the HIT@k evaluation in the KB link prediction literature (Bordes et al. 2011). 6.2 Bi-encoder Models As discussed in \u00a75, we evaluate the baseline bi-encoder (Baseline) and three hierarchy-aware con\ufb01gurations: Hierar- chy Pretraining (Baseline + HP); Hierarchy Joint Learning (Baseline + HJL); Hierarchy Pretraining & Hierarchy Joint Learning (Baseline + HP + HJL). 6.3 Cross-encoder Models Given the top-k retrieval results from each of the aforemen- tioned bi-encoders, we train and evaluate a unique crossen- coder respectively. The value of k used for all cross-encoder experiments is selected to balance retrieval qualities (i.e. bi- encoder Recall@k on dev set) and computation throughput (\u00a77.1). In case some of the gold events are not retrieved among top-k candidates for the corresponding mention in the training set, we substitute each missing gold event for the negative candidates with the current lowest probability and repeat this process until all the missing gold events are added. At inference time, we apply a threshold \u03c4c to the reranked event candidates and emit those with score \u2265 \u03c4c as \ufb01nal predictions. If there is no event yielded, we add a NULL event to the prediction. Hierarchical Relation Extraction: We apply the pro- posed method to top-4 retrieval results from the best- performing bi-encoder to perform hierarchical relation ex- traction. 7 Result and Analysis 7.1 Bi-encoder Bi-encoder retrieval results on the dev split for both multi- and cross-lingual tasks are illustrated in Figure 3 and Fig- ure 4 respectively. Since the gain in Recall@k is relatively minor when doubling k from 8 to 16 across all con\ufb01gura- tions and tasks, the cross-encoder is trained with the top 8 retrieved candidates, with the consideration of computa- tion ef\ufb01ciency. It is also shown that all con\ufb01gurations attain better performance when evaluated by retrieving the most atomic event only (set of dense dots vs line plots), which re\ufb02ects the bene\ufb01ts of following the gold hierarchies and in- dicates the performance upper-bound for current models that try to learn these hierarchies. We further report the quantitative results of bi-encoder Recall@min on dev and test set of both tasks in Table 2. Among all the hierarchy-integration strategies, hierarchical 100 90 k @ l l a c e R 80 70 baseline + pre-train + joint + pre-train & joint 60 1 4 8 16 k: # retrieved events Figure 3: Multilingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. 90 80 k @ l l a c e R 70 60 baseline + pre-train + joint + pre-train & joint 50 1 4 8 16 k: # retrieved events Figure 4: Crosslingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. pretraining offers consistent improvements on both tasks compared with the baseline. On the other hand, hierarchi- cal joint learning presents a mixture of effects. In particu- lar, it attains the best performance on the crosslingual test set when applied in conjunction with hierarchical pretrain- ing while contributing negatively in all other scenarios. In terms of task languages, all the multilingual con\ufb01gura- tions attain higher performance than their crosslingual coun- terparts, indicating that in general crosslingual task is more challenging than the multilingual task, which is similar to the single event linking scenario. As described in Section 6.1, we further report bi-encoder results under Recall@K (fraction) in Appendix D.1. 7.2 Cross-encoder Cross-encoder reranking results on both tasks are also shown in Table 2. On the multilingual task, all the cross-encoders Bi-encoder Cross-encoder Methods Recall@min Strict Acc Strict Acc (Top Min) Macro F1 Micro F1 Multilingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 65.4 / 54.8 71.3 / 58.1 63.3 / 51.4 67.6 / 55.2 34.4 / 37.6 40.4 / 39.2 43.6 / 40.2 38.2 / 39.9 57.8 / 59.5 61.7 / 60.3 63.6 / 60.8 60.4 / 60.6 56.4 / 62.8 60.8 / 62.0 62.1 / 60.1 57.6 / 61.4 53.0 / 58.3 57.5 / 58.7 59.2 / 57.5 54.7 / 57.8 Crosslingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 53.8 / 32.8 59.7 / 37.0 51.6 / 33.3 55.8 / 38.8 8.5 / 11.9 8.6 / 10.9 9.7 / 12.0 9.6 / 13.1 21.2 / 27.5 18.6 / 25.7 22.3 / 26.1 23.0 /"}, {"question": " What does the dense dots vs line plots comparison indicate about bi-encoder performance?", "answer": " It shows that performance is better when evaluating the most atomic event only, reflecting the benefits of following gold hierarchies", "ref_chunk": "Ei MaP = 1 N N X i=1 |Ei \u2229 \u02c6Ei| | \u02c6Ei| , MaR = 1 N N X i=1 |Ei \u2229 \u02c6Ei| |Ei| Macro F1 = 2MaP \u00b7 MaR MaP + MaR N MiP = P i=1 |Ei \u2229 \u02c6Ei| i=1 | \u02c6Ei| P N , MiR = P N i=1 |Ei \u2229 \u02c6Ei| N i=1 |Ei| P Micro F1 = 2MiP \u00b7 MiR MiP + MiR We additionally evaluate the strict accuracy of cross- encoders where we report the top-x reranked candidates as predictions with x being the number of gold events linked with the mention. This is the same condition as evaluating the bi-encoder on Recall@min. It enables a direct compar- ison of strict accuracy to Recall@min such that to assess if cross-encoders make improvements on bi-encoders. We de- note the strict accuracy calculated under this condition as strict accuracy (top min). (6) (7) (8) (9) (10) Hierarchical Relation Extraction: As de\ufb01ned in \u00a73.2, we evaluate the proposed hierarchical relation extraction method on whether it could identify the parent for a given child event. In particular, given an event with a ranked list of candidate parents (generated by the proposed method), we measure Recall@k for the gold parent in the list. Since Recall@k is ill-de\ufb01ned for events without a parent, we only calculate it for the non-root events within hierarchies of dev and test set. For those events that have parents but are not linked to any mentions by the bi-encoder, they are added as miss at every k. Such evaluation with Recall@k measure is similar to the HIT@k evaluation in the KB link prediction literature (Bordes et al. 2011). 6.2 Bi-encoder Models As discussed in \u00a75, we evaluate the baseline bi-encoder (Baseline) and three hierarchy-aware con\ufb01gurations: Hierar- chy Pretraining (Baseline + HP); Hierarchy Joint Learning (Baseline + HJL); Hierarchy Pretraining & Hierarchy Joint Learning (Baseline + HP + HJL). 6.3 Cross-encoder Models Given the top-k retrieval results from each of the aforemen- tioned bi-encoders, we train and evaluate a unique crossen- coder respectively. The value of k used for all cross-encoder experiments is selected to balance retrieval qualities (i.e. bi- encoder Recall@k on dev set) and computation throughput (\u00a77.1). In case some of the gold events are not retrieved among top-k candidates for the corresponding mention in the training set, we substitute each missing gold event for the negative candidates with the current lowest probability and repeat this process until all the missing gold events are added. At inference time, we apply a threshold \u03c4c to the reranked event candidates and emit those with score \u2265 \u03c4c as \ufb01nal predictions. If there is no event yielded, we add a NULL event to the prediction. Hierarchical Relation Extraction: We apply the pro- posed method to top-4 retrieval results from the best- performing bi-encoder to perform hierarchical relation ex- traction. 7 Result and Analysis 7.1 Bi-encoder Bi-encoder retrieval results on the dev split for both multi- and cross-lingual tasks are illustrated in Figure 3 and Fig- ure 4 respectively. Since the gain in Recall@k is relatively minor when doubling k from 8 to 16 across all con\ufb01gura- tions and tasks, the cross-encoder is trained with the top 8 retrieved candidates, with the consideration of computa- tion ef\ufb01ciency. It is also shown that all con\ufb01gurations attain better performance when evaluated by retrieving the most atomic event only (set of dense dots vs line plots), which re\ufb02ects the bene\ufb01ts of following the gold hierarchies and in- dicates the performance upper-bound for current models that try to learn these hierarchies. We further report the quantitative results of bi-encoder Recall@min on dev and test set of both tasks in Table 2. Among all the hierarchy-integration strategies, hierarchical 100 90 k @ l l a c e R 80 70 baseline + pre-train + joint + pre-train & joint 60 1 4 8 16 k: # retrieved events Figure 3: Multilingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. 90 80 k @ l l a c e R 70 60 baseline + pre-train + joint + pre-train & joint 50 1 4 8 16 k: # retrieved events Figure 4: Crosslingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. pretraining offers consistent improvements on both tasks compared with the baseline. On the other hand, hierarchi- cal joint learning presents a mixture of effects. In particu- lar, it attains the best performance on the crosslingual test set when applied in conjunction with hierarchical pretrain- ing while contributing negatively in all other scenarios. In terms of task languages, all the multilingual con\ufb01gura- tions attain higher performance than their crosslingual coun- terparts, indicating that in general crosslingual task is more challenging than the multilingual task, which is similar to the single event linking scenario. As described in Section 6.1, we further report bi-encoder results under Recall@K (fraction) in Appendix D.1. 7.2 Cross-encoder Cross-encoder reranking results on both tasks are also shown in Table 2. On the multilingual task, all the cross-encoders Bi-encoder Cross-encoder Methods Recall@min Strict Acc Strict Acc (Top Min) Macro F1 Micro F1 Multilingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 65.4 / 54.8 71.3 / 58.1 63.3 / 51.4 67.6 / 55.2 34.4 / 37.6 40.4 / 39.2 43.6 / 40.2 38.2 / 39.9 57.8 / 59.5 61.7 / 60.3 63.6 / 60.8 60.4 / 60.6 56.4 / 62.8 60.8 / 62.0 62.1 / 60.1 57.6 / 61.4 53.0 / 58.3 57.5 / 58.7 59.2 / 57.5 54.7 / 57.8 Crosslingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 53.8 / 32.8 59.7 / 37.0 51.6 / 33.3 55.8 / 38.8 8.5 / 11.9 8.6 / 10.9 9.7 / 12.0 9.6 / 13.1 21.2 / 27.5 18.6 / 25.7 22.3 / 26.1 23.0 /"}, {"question": " What improvements can be observed when using hierarchical pretraining with bi-encoders?", "answer": " Consistent improvements on both tasks compared to the baseline", "ref_chunk": "Ei MaP = 1 N N X i=1 |Ei \u2229 \u02c6Ei| | \u02c6Ei| , MaR = 1 N N X i=1 |Ei \u2229 \u02c6Ei| |Ei| Macro F1 = 2MaP \u00b7 MaR MaP + MaR N MiP = P i=1 |Ei \u2229 \u02c6Ei| i=1 | \u02c6Ei| P N , MiR = P N i=1 |Ei \u2229 \u02c6Ei| N i=1 |Ei| P Micro F1 = 2MiP \u00b7 MiR MiP + MiR We additionally evaluate the strict accuracy of cross- encoders where we report the top-x reranked candidates as predictions with x being the number of gold events linked with the mention. This is the same condition as evaluating the bi-encoder on Recall@min. It enables a direct compar- ison of strict accuracy to Recall@min such that to assess if cross-encoders make improvements on bi-encoders. We de- note the strict accuracy calculated under this condition as strict accuracy (top min). (6) (7) (8) (9) (10) Hierarchical Relation Extraction: As de\ufb01ned in \u00a73.2, we evaluate the proposed hierarchical relation extraction method on whether it could identify the parent for a given child event. In particular, given an event with a ranked list of candidate parents (generated by the proposed method), we measure Recall@k for the gold parent in the list. Since Recall@k is ill-de\ufb01ned for events without a parent, we only calculate it for the non-root events within hierarchies of dev and test set. For those events that have parents but are not linked to any mentions by the bi-encoder, they are added as miss at every k. Such evaluation with Recall@k measure is similar to the HIT@k evaluation in the KB link prediction literature (Bordes et al. 2011). 6.2 Bi-encoder Models As discussed in \u00a75, we evaluate the baseline bi-encoder (Baseline) and three hierarchy-aware con\ufb01gurations: Hierar- chy Pretraining (Baseline + HP); Hierarchy Joint Learning (Baseline + HJL); Hierarchy Pretraining & Hierarchy Joint Learning (Baseline + HP + HJL). 6.3 Cross-encoder Models Given the top-k retrieval results from each of the aforemen- tioned bi-encoders, we train and evaluate a unique crossen- coder respectively. The value of k used for all cross-encoder experiments is selected to balance retrieval qualities (i.e. bi- encoder Recall@k on dev set) and computation throughput (\u00a77.1). In case some of the gold events are not retrieved among top-k candidates for the corresponding mention in the training set, we substitute each missing gold event for the negative candidates with the current lowest probability and repeat this process until all the missing gold events are added. At inference time, we apply a threshold \u03c4c to the reranked event candidates and emit those with score \u2265 \u03c4c as \ufb01nal predictions. If there is no event yielded, we add a NULL event to the prediction. Hierarchical Relation Extraction: We apply the pro- posed method to top-4 retrieval results from the best- performing bi-encoder to perform hierarchical relation ex- traction. 7 Result and Analysis 7.1 Bi-encoder Bi-encoder retrieval results on the dev split for both multi- and cross-lingual tasks are illustrated in Figure 3 and Fig- ure 4 respectively. Since the gain in Recall@k is relatively minor when doubling k from 8 to 16 across all con\ufb01gura- tions and tasks, the cross-encoder is trained with the top 8 retrieved candidates, with the consideration of computa- tion ef\ufb01ciency. It is also shown that all con\ufb01gurations attain better performance when evaluated by retrieving the most atomic event only (set of dense dots vs line plots), which re\ufb02ects the bene\ufb01ts of following the gold hierarchies and in- dicates the performance upper-bound for current models that try to learn these hierarchies. We further report the quantitative results of bi-encoder Recall@min on dev and test set of both tasks in Table 2. Among all the hierarchy-integration strategies, hierarchical 100 90 k @ l l a c e R 80 70 baseline + pre-train + joint + pre-train & joint 60 1 4 8 16 k: # retrieved events Figure 3: Multilingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. 90 80 k @ l l a c e R 70 60 baseline + pre-train + joint + pre-train & joint 50 1 4 8 16 k: # retrieved events Figure 4: Crosslingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. pretraining offers consistent improvements on both tasks compared with the baseline. On the other hand, hierarchi- cal joint learning presents a mixture of effects. In particu- lar, it attains the best performance on the crosslingual test set when applied in conjunction with hierarchical pretrain- ing while contributing negatively in all other scenarios. In terms of task languages, all the multilingual con\ufb01gura- tions attain higher performance than their crosslingual coun- terparts, indicating that in general crosslingual task is more challenging than the multilingual task, which is similar to the single event linking scenario. As described in Section 6.1, we further report bi-encoder results under Recall@K (fraction) in Appendix D.1. 7.2 Cross-encoder Cross-encoder reranking results on both tasks are also shown in Table 2. On the multilingual task, all the cross-encoders Bi-encoder Cross-encoder Methods Recall@min Strict Acc Strict Acc (Top Min) Macro F1 Micro F1 Multilingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 65.4 / 54.8 71.3 / 58.1 63.3 / 51.4 67.6 / 55.2 34.4 / 37.6 40.4 / 39.2 43.6 / 40.2 38.2 / 39.9 57.8 / 59.5 61.7 / 60.3 63.6 / 60.8 60.4 / 60.6 56.4 / 62.8 60.8 / 62.0 62.1 / 60.1 57.6 / 61.4 53.0 / 58.3 57.5 / 58.7 59.2 / 57.5 54.7 / 57.8 Crosslingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 53.8 / 32.8 59.7 / 37.0 51.6 / 33.3 55.8 / 38.8 8.5 / 11.9 8.6 / 10.9 9.7 / 12.0 9.6 / 13.1 21.2 / 27.5 18.6 / 25.7 22.3 / 26.1 23.0 /"}], "doc_text": "Ei MaP = 1 N N X i=1 |Ei \u2229 \u02c6Ei| | \u02c6Ei| , MaR = 1 N N X i=1 |Ei \u2229 \u02c6Ei| |Ei| Macro F1 = 2MaP \u00b7 MaR MaP + MaR N MiP = P i=1 |Ei \u2229 \u02c6Ei| i=1 | \u02c6Ei| P N , MiR = P N i=1 |Ei \u2229 \u02c6Ei| N i=1 |Ei| P Micro F1 = 2MiP \u00b7 MiR MiP + MiR We additionally evaluate the strict accuracy of cross- encoders where we report the top-x reranked candidates as predictions with x being the number of gold events linked with the mention. This is the same condition as evaluating the bi-encoder on Recall@min. It enables a direct compar- ison of strict accuracy to Recall@min such that to assess if cross-encoders make improvements on bi-encoders. We de- note the strict accuracy calculated under this condition as strict accuracy (top min). (6) (7) (8) (9) (10) Hierarchical Relation Extraction: As de\ufb01ned in \u00a73.2, we evaluate the proposed hierarchical relation extraction method on whether it could identify the parent for a given child event. In particular, given an event with a ranked list of candidate parents (generated by the proposed method), we measure Recall@k for the gold parent in the list. Since Recall@k is ill-de\ufb01ned for events without a parent, we only calculate it for the non-root events within hierarchies of dev and test set. For those events that have parents but are not linked to any mentions by the bi-encoder, they are added as miss at every k. Such evaluation with Recall@k measure is similar to the HIT@k evaluation in the KB link prediction literature (Bordes et al. 2011). 6.2 Bi-encoder Models As discussed in \u00a75, we evaluate the baseline bi-encoder (Baseline) and three hierarchy-aware con\ufb01gurations: Hierar- chy Pretraining (Baseline + HP); Hierarchy Joint Learning (Baseline + HJL); Hierarchy Pretraining & Hierarchy Joint Learning (Baseline + HP + HJL). 6.3 Cross-encoder Models Given the top-k retrieval results from each of the aforemen- tioned bi-encoders, we train and evaluate a unique crossen- coder respectively. The value of k used for all cross-encoder experiments is selected to balance retrieval qualities (i.e. bi- encoder Recall@k on dev set) and computation throughput (\u00a77.1). In case some of the gold events are not retrieved among top-k candidates for the corresponding mention in the training set, we substitute each missing gold event for the negative candidates with the current lowest probability and repeat this process until all the missing gold events are added. At inference time, we apply a threshold \u03c4c to the reranked event candidates and emit those with score \u2265 \u03c4c as \ufb01nal predictions. If there is no event yielded, we add a NULL event to the prediction. Hierarchical Relation Extraction: We apply the pro- posed method to top-4 retrieval results from the best- performing bi-encoder to perform hierarchical relation ex- traction. 7 Result and Analysis 7.1 Bi-encoder Bi-encoder retrieval results on the dev split for both multi- and cross-lingual tasks are illustrated in Figure 3 and Fig- ure 4 respectively. Since the gain in Recall@k is relatively minor when doubling k from 8 to 16 across all con\ufb01gura- tions and tasks, the cross-encoder is trained with the top 8 retrieved candidates, with the consideration of computa- tion ef\ufb01ciency. It is also shown that all con\ufb01gurations attain better performance when evaluated by retrieving the most atomic event only (set of dense dots vs line plots), which re\ufb02ects the bene\ufb01ts of following the gold hierarchies and in- dicates the performance upper-bound for current models that try to learn these hierarchies. We further report the quantitative results of bi-encoder Recall@min on dev and test set of both tasks in Table 2. Among all the hierarchy-integration strategies, hierarchical 100 90 k @ l l a c e R 80 70 baseline + pre-train + joint + pre-train & joint 60 1 4 8 16 k: # retrieved events Figure 3: Multilingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. 90 80 k @ l l a c e R 70 60 baseline + pre-train + joint + pre-train & joint 50 1 4 8 16 k: # retrieved events Figure 4: Crosslingual bi-encoder Recall@k on the dev set. The densely dotted plots ( ) denote the prediction scores for the atomic label, an upper bound for model performance. pretraining offers consistent improvements on both tasks compared with the baseline. On the other hand, hierarchi- cal joint learning presents a mixture of effects. In particu- lar, it attains the best performance on the crosslingual test set when applied in conjunction with hierarchical pretrain- ing while contributing negatively in all other scenarios. In terms of task languages, all the multilingual con\ufb01gura- tions attain higher performance than their crosslingual coun- terparts, indicating that in general crosslingual task is more challenging than the multilingual task, which is similar to the single event linking scenario. As described in Section 6.1, we further report bi-encoder results under Recall@K (fraction) in Appendix D.1. 7.2 Cross-encoder Cross-encoder reranking results on both tasks are also shown in Table 2. On the multilingual task, all the cross-encoders Bi-encoder Cross-encoder Methods Recall@min Strict Acc Strict Acc (Top Min) Macro F1 Micro F1 Multilingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 65.4 / 54.8 71.3 / 58.1 63.3 / 51.4 67.6 / 55.2 34.4 / 37.6 40.4 / 39.2 43.6 / 40.2 38.2 / 39.9 57.8 / 59.5 61.7 / 60.3 63.6 / 60.8 60.4 / 60.6 56.4 / 62.8 60.8 / 62.0 62.1 / 60.1 57.6 / 61.4 53.0 / 58.3 57.5 / 58.7 59.2 / 57.5 54.7 / 57.8 Crosslingual (a) (b) (c) (d) Baseline + HP + HJL + HP + HJL 53.8 / 32.8 59.7 / 37.0 51.6 / 33.3 55.8 / 38.8 8.5 / 11.9 8.6 / 10.9 9.7 / 12.0 9.6 / 13.1 21.2 / 27.5 18.6 / 25.7 22.3 / 26.1 23.0 /"}