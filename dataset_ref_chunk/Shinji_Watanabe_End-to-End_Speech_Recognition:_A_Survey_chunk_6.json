{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_End-to-End_Speech_Recognition:_A_Survey_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does a diagonal transition represent in the RNA model?", "answer": " Outputting a non-blank symbol.", "ref_chunk": "diagonal transitions correspond to outputting a non-blank symbol. The FSA (left) represents the set of valid alignments for the RNA model. Although the FSA is identical to the corresponding FSA for RNN-T in Figure 4, the semantics of the \u27e8b\u27e9 label are different in the two cases. symbol has a different interpretation in RNA and the RNN- T models: in RNN-T, outputting a blank symbol advances the model to the next frame; in RNA, however, the model advances to the next frame after outputting a single blank or non-blank label. Restricting the model to output a single non- blank label at each frame improves computational efficiency and simplifies the decoding process, by limiting the number of model expansions at each frame (in constrast to RNN-T decoding). For example, if T = 8, and C = (s, e, e), then A = (\u27e8b\u27e9 , s, \u27e8b\u27e9 , e, \u27e8b\u27e9 , \u27e8b\u27e9 , e, \u27e8b\u27e9) \u2208 ARNA (X,C) as illustrated in Figure 6. The RNA posterior probability, P (C|X), is defined as: PRNA(C|X) = (cid:88) P (A|H(X)) A\u2208ARNA (X,C) = (cid:88) T (cid:89) P (at|at\u22121, . . . , a1, H(X)) where, as before it denotes the number of non-blank sym- bols in the partial alignment sequence (a1, . . . , at\u22121), and qt\u22121 = NN(\u00b7|at\u22121, \u00b7 \u00b7 \u00b7 , a1) represents the output of a neu- ral network which summarizes the entire partial alignment sequence, where NN(\u00b7) represents a suitable neural network (an LSTM in [46]). Thus, RNA removes the one remaining conditional independence assumption of the RNN-T model, by conditioning on the sequence of previous non-blank labels as well as the alignment that generated them. However, this comes at a cost: the exact computation of the log-likelihood in Eq. (3) (and corresponding gradients) is intractable. Instead, RNA makes two simplifying assumption to ensure tractable training: by assuming that the model can only output a single label at each frame; and utilizing a straight-through estimator for the alignment [50]. The latter constraint \u2013 allowing only a single label (blank or non-blank) at each frame \u2013 has also been explored in the context of the monotonic RNN-T model [51]. Finally, we note that the work in [52] further generalizes the RNA model by employing two RNNs when defining the state: a slow RNN (which corresponds to the sequence of previously predicted non-blank labels), and a fast RNN (which also conditions on the frames at which the non-blank labels were output). C. Implicit Alignment Modeling Approaches One of the main benefits of the explicit alignment ap- proaches such as CTC, RNN-T, or RNA is that they result in ASR models that are easily amenable to frame-synchronous decoding6 In this section, we discuss the attention-based encoder-decoder (AED) models (also known as, listen-attend- and-spell (LAS)) [15], [16], [53], which employs the attention mechanism [43] to implicitly identify and model the portions of the input acoustics which are relevant to each output unit. These models were first popularized in the context of machine translation [54]. Unlike explicit alignment modeling approaches, attention-based encoder-decoder models use an attention mechanism [43] to learn a correspondence between the entire acoustic sequence and the individual labels. Such models support label-synchronous decoding, meaning that during inference, each hypothesis in the beam is expanded by 1 label. In the explicit alignment approaches presented in Sec- tion III-B, during inference, the model continues to output symbols until it has processed the final frame at which point the decoding process is complete; similarly, during training, the forward-backward algorithm aligns over all possible align- ment sequences. Since an AED model processes the entire acoustic sequence at once, the model needs a mechanism by which it can indicate that it is done emitting all output symbols. This is achieved by augmenting the set of outputs with an end-of-sentence symbol, \u27e8eos\u27e9, so that the output vocabulary consists of the set Ceos = C \u222a {\u27e8eos\u27e9}. Thus, the AED model, depicted in Figure 7, consists of an en- coder network \u2013 which encodes the input acoustic frame A\u2208ARNA (X,C) t=1 = (cid:88) A\u2208ARNA (X,C) T (cid:89) t=1 P (at|qt\u22121, ht) (4) 6 By frame-synchronous decoding, we refer to the ability of the model to produce output label for each input frame of speech. Models such as CTC, RNN-T, or RNA, support frame-synchronous decoding. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 6 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Attention Decoder Softmax Encoder H(X) e <eos> e Time s Fig. 8. Unlike models such as RNN-T or CTC, AED models do not have explicit alignment. However, it is possible to interpret the attention weights \u03b1t,i for a particular output symbol ci as an alignment weight which is represented above for the target sequence C = (s, e, e, \u27e8eos\u27e9). In this representation, the size of the circle and the darkness level are proportional to the corresponding attention weights; thus the total probability mass is the same for each row. As illustrated above, the first few frames correspond to the first symbol c1 = s, while the latter frames correspond to the second \u2018e\u2019: c3 = e. Fig. 7. An attention-based encoder decoder (AED) model [15], [16], [53]. The output distribution is conditioned on the decoder state, si (which summarizes the previously decoded symbols), and the context vector, vi (which summarizes the encoder output based on the decoder state). In the seminal work of Chan et al., [16], for example, this is accomplished by concatenating the two vectors, as denoted by the (cid:76) symbol in the figure. sequence, X = (x1, . . . , xT \u2032), into a higher-level representa- tion H(X) = (h1, . . . , hT ) \u2013 and an attention-based decoder which defines the probability distribution over the set of output symbols, Ceos. Thus,"}, {"question": " How do the semantics of the \u27e8b\u27e9 label differ between RNA and RNN-T models?", "answer": " In RNN-T, outputting a blank symbol advances the model to the next frame; in RNA, however, the model advances to the next frame after outputting a single blank or non-blank label.", "ref_chunk": "diagonal transitions correspond to outputting a non-blank symbol. The FSA (left) represents the set of valid alignments for the RNA model. Although the FSA is identical to the corresponding FSA for RNN-T in Figure 4, the semantics of the \u27e8b\u27e9 label are different in the two cases. symbol has a different interpretation in RNA and the RNN- T models: in RNN-T, outputting a blank symbol advances the model to the next frame; in RNA, however, the model advances to the next frame after outputting a single blank or non-blank label. Restricting the model to output a single non- blank label at each frame improves computational efficiency and simplifies the decoding process, by limiting the number of model expansions at each frame (in constrast to RNN-T decoding). For example, if T = 8, and C = (s, e, e), then A = (\u27e8b\u27e9 , s, \u27e8b\u27e9 , e, \u27e8b\u27e9 , \u27e8b\u27e9 , e, \u27e8b\u27e9) \u2208 ARNA (X,C) as illustrated in Figure 6. The RNA posterior probability, P (C|X), is defined as: PRNA(C|X) = (cid:88) P (A|H(X)) A\u2208ARNA (X,C) = (cid:88) T (cid:89) P (at|at\u22121, . . . , a1, H(X)) where, as before it denotes the number of non-blank sym- bols in the partial alignment sequence (a1, . . . , at\u22121), and qt\u22121 = NN(\u00b7|at\u22121, \u00b7 \u00b7 \u00b7 , a1) represents the output of a neu- ral network which summarizes the entire partial alignment sequence, where NN(\u00b7) represents a suitable neural network (an LSTM in [46]). Thus, RNA removes the one remaining conditional independence assumption of the RNN-T model, by conditioning on the sequence of previous non-blank labels as well as the alignment that generated them. However, this comes at a cost: the exact computation of the log-likelihood in Eq. (3) (and corresponding gradients) is intractable. Instead, RNA makes two simplifying assumption to ensure tractable training: by assuming that the model can only output a single label at each frame; and utilizing a straight-through estimator for the alignment [50]. The latter constraint \u2013 allowing only a single label (blank or non-blank) at each frame \u2013 has also been explored in the context of the monotonic RNN-T model [51]. Finally, we note that the work in [52] further generalizes the RNA model by employing two RNNs when defining the state: a slow RNN (which corresponds to the sequence of previously predicted non-blank labels), and a fast RNN (which also conditions on the frames at which the non-blank labels were output). C. Implicit Alignment Modeling Approaches One of the main benefits of the explicit alignment ap- proaches such as CTC, RNN-T, or RNA is that they result in ASR models that are easily amenable to frame-synchronous decoding6 In this section, we discuss the attention-based encoder-decoder (AED) models (also known as, listen-attend- and-spell (LAS)) [15], [16], [53], which employs the attention mechanism [43] to implicitly identify and model the portions of the input acoustics which are relevant to each output unit. These models were first popularized in the context of machine translation [54]. Unlike explicit alignment modeling approaches, attention-based encoder-decoder models use an attention mechanism [43] to learn a correspondence between the entire acoustic sequence and the individual labels. Such models support label-synchronous decoding, meaning that during inference, each hypothesis in the beam is expanded by 1 label. In the explicit alignment approaches presented in Sec- tion III-B, during inference, the model continues to output symbols until it has processed the final frame at which point the decoding process is complete; similarly, during training, the forward-backward algorithm aligns over all possible align- ment sequences. Since an AED model processes the entire acoustic sequence at once, the model needs a mechanism by which it can indicate that it is done emitting all output symbols. This is achieved by augmenting the set of outputs with an end-of-sentence symbol, \u27e8eos\u27e9, so that the output vocabulary consists of the set Ceos = C \u222a {\u27e8eos\u27e9}. Thus, the AED model, depicted in Figure 7, consists of an en- coder network \u2013 which encodes the input acoustic frame A\u2208ARNA (X,C) t=1 = (cid:88) A\u2208ARNA (X,C) T (cid:89) t=1 P (at|qt\u22121, ht) (4) 6 By frame-synchronous decoding, we refer to the ability of the model to produce output label for each input frame of speech. Models such as CTC, RNN-T, or RNA, support frame-synchronous decoding. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 6 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Attention Decoder Softmax Encoder H(X) e <eos> e Time s Fig. 8. Unlike models such as RNN-T or CTC, AED models do not have explicit alignment. However, it is possible to interpret the attention weights \u03b1t,i for a particular output symbol ci as an alignment weight which is represented above for the target sequence C = (s, e, e, \u27e8eos\u27e9). In this representation, the size of the circle and the darkness level are proportional to the corresponding attention weights; thus the total probability mass is the same for each row. As illustrated above, the first few frames correspond to the first symbol c1 = s, while the latter frames correspond to the second \u2018e\u2019: c3 = e. Fig. 7. An attention-based encoder decoder (AED) model [15], [16], [53]. The output distribution is conditioned on the decoder state, si (which summarizes the previously decoded symbols), and the context vector, vi (which summarizes the encoder output based on the decoder state). In the seminal work of Chan et al., [16], for example, this is accomplished by concatenating the two vectors, as denoted by the (cid:76) symbol in the figure. sequence, X = (x1, . . . , xT \u2032), into a higher-level representa- tion H(X) = (h1, . . . , hT ) \u2013 and an attention-based decoder which defines the probability distribution over the set of output symbols, Ceos. Thus,"}, {"question": " How does restricting the model to output a single non-blank label at each frame improve computational efficiency?", "answer": " It limits the number of model expansions at each frame.", "ref_chunk": "diagonal transitions correspond to outputting a non-blank symbol. The FSA (left) represents the set of valid alignments for the RNA model. Although the FSA is identical to the corresponding FSA for RNN-T in Figure 4, the semantics of the \u27e8b\u27e9 label are different in the two cases. symbol has a different interpretation in RNA and the RNN- T models: in RNN-T, outputting a blank symbol advances the model to the next frame; in RNA, however, the model advances to the next frame after outputting a single blank or non-blank label. Restricting the model to output a single non- blank label at each frame improves computational efficiency and simplifies the decoding process, by limiting the number of model expansions at each frame (in constrast to RNN-T decoding). For example, if T = 8, and C = (s, e, e), then A = (\u27e8b\u27e9 , s, \u27e8b\u27e9 , e, \u27e8b\u27e9 , \u27e8b\u27e9 , e, \u27e8b\u27e9) \u2208 ARNA (X,C) as illustrated in Figure 6. The RNA posterior probability, P (C|X), is defined as: PRNA(C|X) = (cid:88) P (A|H(X)) A\u2208ARNA (X,C) = (cid:88) T (cid:89) P (at|at\u22121, . . . , a1, H(X)) where, as before it denotes the number of non-blank sym- bols in the partial alignment sequence (a1, . . . , at\u22121), and qt\u22121 = NN(\u00b7|at\u22121, \u00b7 \u00b7 \u00b7 , a1) represents the output of a neu- ral network which summarizes the entire partial alignment sequence, where NN(\u00b7) represents a suitable neural network (an LSTM in [46]). Thus, RNA removes the one remaining conditional independence assumption of the RNN-T model, by conditioning on the sequence of previous non-blank labels as well as the alignment that generated them. However, this comes at a cost: the exact computation of the log-likelihood in Eq. (3) (and corresponding gradients) is intractable. Instead, RNA makes two simplifying assumption to ensure tractable training: by assuming that the model can only output a single label at each frame; and utilizing a straight-through estimator for the alignment [50]. The latter constraint \u2013 allowing only a single label (blank or non-blank) at each frame \u2013 has also been explored in the context of the monotonic RNN-T model [51]. Finally, we note that the work in [52] further generalizes the RNA model by employing two RNNs when defining the state: a slow RNN (which corresponds to the sequence of previously predicted non-blank labels), and a fast RNN (which also conditions on the frames at which the non-blank labels were output). C. Implicit Alignment Modeling Approaches One of the main benefits of the explicit alignment ap- proaches such as CTC, RNN-T, or RNA is that they result in ASR models that are easily amenable to frame-synchronous decoding6 In this section, we discuss the attention-based encoder-decoder (AED) models (also known as, listen-attend- and-spell (LAS)) [15], [16], [53], which employs the attention mechanism [43] to implicitly identify and model the portions of the input acoustics which are relevant to each output unit. These models were first popularized in the context of machine translation [54]. Unlike explicit alignment modeling approaches, attention-based encoder-decoder models use an attention mechanism [43] to learn a correspondence between the entire acoustic sequence and the individual labels. Such models support label-synchronous decoding, meaning that during inference, each hypothesis in the beam is expanded by 1 label. In the explicit alignment approaches presented in Sec- tion III-B, during inference, the model continues to output symbols until it has processed the final frame at which point the decoding process is complete; similarly, during training, the forward-backward algorithm aligns over all possible align- ment sequences. Since an AED model processes the entire acoustic sequence at once, the model needs a mechanism by which it can indicate that it is done emitting all output symbols. This is achieved by augmenting the set of outputs with an end-of-sentence symbol, \u27e8eos\u27e9, so that the output vocabulary consists of the set Ceos = C \u222a {\u27e8eos\u27e9}. Thus, the AED model, depicted in Figure 7, consists of an en- coder network \u2013 which encodes the input acoustic frame A\u2208ARNA (X,C) t=1 = (cid:88) A\u2208ARNA (X,C) T (cid:89) t=1 P (at|qt\u22121, ht) (4) 6 By frame-synchronous decoding, we refer to the ability of the model to produce output label for each input frame of speech. Models such as CTC, RNN-T, or RNA, support frame-synchronous decoding. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 6 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Attention Decoder Softmax Encoder H(X) e <eos> e Time s Fig. 8. Unlike models such as RNN-T or CTC, AED models do not have explicit alignment. However, it is possible to interpret the attention weights \u03b1t,i for a particular output symbol ci as an alignment weight which is represented above for the target sequence C = (s, e, e, \u27e8eos\u27e9). In this representation, the size of the circle and the darkness level are proportional to the corresponding attention weights; thus the total probability mass is the same for each row. As illustrated above, the first few frames correspond to the first symbol c1 = s, while the latter frames correspond to the second \u2018e\u2019: c3 = e. Fig. 7. An attention-based encoder decoder (AED) model [15], [16], [53]. The output distribution is conditioned on the decoder state, si (which summarizes the previously decoded symbols), and the context vector, vi (which summarizes the encoder output based on the decoder state). In the seminal work of Chan et al., [16], for example, this is accomplished by concatenating the two vectors, as denoted by the (cid:76) symbol in the figure. sequence, X = (x1, . . . , xT \u2032), into a higher-level representa- tion H(X) = (h1, . . . , hT ) \u2013 and an attention-based decoder which defines the probability distribution over the set of output symbols, Ceos. Thus,"}, {"question": " In the RNA model, how is the RNA posterior probability, P (C|X), defined?", "answer": " PRNA(C|X) = (\u2211 P (A|H(X))) where A\u2208ARNA (X,C)", "ref_chunk": "diagonal transitions correspond to outputting a non-blank symbol. The FSA (left) represents the set of valid alignments for the RNA model. Although the FSA is identical to the corresponding FSA for RNN-T in Figure 4, the semantics of the \u27e8b\u27e9 label are different in the two cases. symbol has a different interpretation in RNA and the RNN- T models: in RNN-T, outputting a blank symbol advances the model to the next frame; in RNA, however, the model advances to the next frame after outputting a single blank or non-blank label. Restricting the model to output a single non- blank label at each frame improves computational efficiency and simplifies the decoding process, by limiting the number of model expansions at each frame (in constrast to RNN-T decoding). For example, if T = 8, and C = (s, e, e), then A = (\u27e8b\u27e9 , s, \u27e8b\u27e9 , e, \u27e8b\u27e9 , \u27e8b\u27e9 , e, \u27e8b\u27e9) \u2208 ARNA (X,C) as illustrated in Figure 6. The RNA posterior probability, P (C|X), is defined as: PRNA(C|X) = (cid:88) P (A|H(X)) A\u2208ARNA (X,C) = (cid:88) T (cid:89) P (at|at\u22121, . . . , a1, H(X)) where, as before it denotes the number of non-blank sym- bols in the partial alignment sequence (a1, . . . , at\u22121), and qt\u22121 = NN(\u00b7|at\u22121, \u00b7 \u00b7 \u00b7 , a1) represents the output of a neu- ral network which summarizes the entire partial alignment sequence, where NN(\u00b7) represents a suitable neural network (an LSTM in [46]). Thus, RNA removes the one remaining conditional independence assumption of the RNN-T model, by conditioning on the sequence of previous non-blank labels as well as the alignment that generated them. However, this comes at a cost: the exact computation of the log-likelihood in Eq. (3) (and corresponding gradients) is intractable. Instead, RNA makes two simplifying assumption to ensure tractable training: by assuming that the model can only output a single label at each frame; and utilizing a straight-through estimator for the alignment [50]. The latter constraint \u2013 allowing only a single label (blank or non-blank) at each frame \u2013 has also been explored in the context of the monotonic RNN-T model [51]. Finally, we note that the work in [52] further generalizes the RNA model by employing two RNNs when defining the state: a slow RNN (which corresponds to the sequence of previously predicted non-blank labels), and a fast RNN (which also conditions on the frames at which the non-blank labels were output). C. Implicit Alignment Modeling Approaches One of the main benefits of the explicit alignment ap- proaches such as CTC, RNN-T, or RNA is that they result in ASR models that are easily amenable to frame-synchronous decoding6 In this section, we discuss the attention-based encoder-decoder (AED) models (also known as, listen-attend- and-spell (LAS)) [15], [16], [53], which employs the attention mechanism [43] to implicitly identify and model the portions of the input acoustics which are relevant to each output unit. These models were first popularized in the context of machine translation [54]. Unlike explicit alignment modeling approaches, attention-based encoder-decoder models use an attention mechanism [43] to learn a correspondence between the entire acoustic sequence and the individual labels. Such models support label-synchronous decoding, meaning that during inference, each hypothesis in the beam is expanded by 1 label. In the explicit alignment approaches presented in Sec- tion III-B, during inference, the model continues to output symbols until it has processed the final frame at which point the decoding process is complete; similarly, during training, the forward-backward algorithm aligns over all possible align- ment sequences. Since an AED model processes the entire acoustic sequence at once, the model needs a mechanism by which it can indicate that it is done emitting all output symbols. This is achieved by augmenting the set of outputs with an end-of-sentence symbol, \u27e8eos\u27e9, so that the output vocabulary consists of the set Ceos = C \u222a {\u27e8eos\u27e9}. Thus, the AED model, depicted in Figure 7, consists of an en- coder network \u2013 which encodes the input acoustic frame A\u2208ARNA (X,C) t=1 = (cid:88) A\u2208ARNA (X,C) T (cid:89) t=1 P (at|qt\u22121, ht) (4) 6 By frame-synchronous decoding, we refer to the ability of the model to produce output label for each input frame of speech. Models such as CTC, RNN-T, or RNA, support frame-synchronous decoding. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 6 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Attention Decoder Softmax Encoder H(X) e <eos> e Time s Fig. 8. Unlike models such as RNN-T or CTC, AED models do not have explicit alignment. However, it is possible to interpret the attention weights \u03b1t,i for a particular output symbol ci as an alignment weight which is represented above for the target sequence C = (s, e, e, \u27e8eos\u27e9). In this representation, the size of the circle and the darkness level are proportional to the corresponding attention weights; thus the total probability mass is the same for each row. As illustrated above, the first few frames correspond to the first symbol c1 = s, while the latter frames correspond to the second \u2018e\u2019: c3 = e. Fig. 7. An attention-based encoder decoder (AED) model [15], [16], [53]. The output distribution is conditioned on the decoder state, si (which summarizes the previously decoded symbols), and the context vector, vi (which summarizes the encoder output based on the decoder state). In the seminal work of Chan et al., [16], for example, this is accomplished by concatenating the two vectors, as denoted by the (cid:76) symbol in the figure. sequence, X = (x1, . . . , xT \u2032), into a higher-level representa- tion H(X) = (h1, . . . , hT ) \u2013 and an attention-based decoder which defines the probability distribution over the set of output symbols, Ceos. Thus,"}, {"question": " What are the simplifying assumptions made by RNA to ensure tractable training?", "answer": " Assuming that the model can only output a single label at each frame; and utilizing a straight-through estimator for the alignment.", "ref_chunk": "diagonal transitions correspond to outputting a non-blank symbol. The FSA (left) represents the set of valid alignments for the RNA model. Although the FSA is identical to the corresponding FSA for RNN-T in Figure 4, the semantics of the \u27e8b\u27e9 label are different in the two cases. symbol has a different interpretation in RNA and the RNN- T models: in RNN-T, outputting a blank symbol advances the model to the next frame; in RNA, however, the model advances to the next frame after outputting a single blank or non-blank label. Restricting the model to output a single non- blank label at each frame improves computational efficiency and simplifies the decoding process, by limiting the number of model expansions at each frame (in constrast to RNN-T decoding). For example, if T = 8, and C = (s, e, e), then A = (\u27e8b\u27e9 , s, \u27e8b\u27e9 , e, \u27e8b\u27e9 , \u27e8b\u27e9 , e, \u27e8b\u27e9) \u2208 ARNA (X,C) as illustrated in Figure 6. The RNA posterior probability, P (C|X), is defined as: PRNA(C|X) = (cid:88) P (A|H(X)) A\u2208ARNA (X,C) = (cid:88) T (cid:89) P (at|at\u22121, . . . , a1, H(X)) where, as before it denotes the number of non-blank sym- bols in the partial alignment sequence (a1, . . . , at\u22121), and qt\u22121 = NN(\u00b7|at\u22121, \u00b7 \u00b7 \u00b7 , a1) represents the output of a neu- ral network which summarizes the entire partial alignment sequence, where NN(\u00b7) represents a suitable neural network (an LSTM in [46]). Thus, RNA removes the one remaining conditional independence assumption of the RNN-T model, by conditioning on the sequence of previous non-blank labels as well as the alignment that generated them. However, this comes at a cost: the exact computation of the log-likelihood in Eq. (3) (and corresponding gradients) is intractable. Instead, RNA makes two simplifying assumption to ensure tractable training: by assuming that the model can only output a single label at each frame; and utilizing a straight-through estimator for the alignment [50]. The latter constraint \u2013 allowing only a single label (blank or non-blank) at each frame \u2013 has also been explored in the context of the monotonic RNN-T model [51]. Finally, we note that the work in [52] further generalizes the RNA model by employing two RNNs when defining the state: a slow RNN (which corresponds to the sequence of previously predicted non-blank labels), and a fast RNN (which also conditions on the frames at which the non-blank labels were output). C. Implicit Alignment Modeling Approaches One of the main benefits of the explicit alignment ap- proaches such as CTC, RNN-T, or RNA is that they result in ASR models that are easily amenable to frame-synchronous decoding6 In this section, we discuss the attention-based encoder-decoder (AED) models (also known as, listen-attend- and-spell (LAS)) [15], [16], [53], which employs the attention mechanism [43] to implicitly identify and model the portions of the input acoustics which are relevant to each output unit. These models were first popularized in the context of machine translation [54]. Unlike explicit alignment modeling approaches, attention-based encoder-decoder models use an attention mechanism [43] to learn a correspondence between the entire acoustic sequence and the individual labels. Such models support label-synchronous decoding, meaning that during inference, each hypothesis in the beam is expanded by 1 label. In the explicit alignment approaches presented in Sec- tion III-B, during inference, the model continues to output symbols until it has processed the final frame at which point the decoding process is complete; similarly, during training, the forward-backward algorithm aligns over all possible align- ment sequences. Since an AED model processes the entire acoustic sequence at once, the model needs a mechanism by which it can indicate that it is done emitting all output symbols. This is achieved by augmenting the set of outputs with an end-of-sentence symbol, \u27e8eos\u27e9, so that the output vocabulary consists of the set Ceos = C \u222a {\u27e8eos\u27e9}. Thus, the AED model, depicted in Figure 7, consists of an en- coder network \u2013 which encodes the input acoustic frame A\u2208ARNA (X,C) t=1 = (cid:88) A\u2208ARNA (X,C) T (cid:89) t=1 P (at|qt\u22121, ht) (4) 6 By frame-synchronous decoding, we refer to the ability of the model to produce output label for each input frame of speech. Models such as CTC, RNN-T, or RNA, support frame-synchronous decoding. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 6 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Attention Decoder Softmax Encoder H(X) e <eos> e Time s Fig. 8. Unlike models such as RNN-T or CTC, AED models do not have explicit alignment. However, it is possible to interpret the attention weights \u03b1t,i for a particular output symbol ci as an alignment weight which is represented above for the target sequence C = (s, e, e, \u27e8eos\u27e9). In this representation, the size of the circle and the darkness level are proportional to the corresponding attention weights; thus the total probability mass is the same for each row. As illustrated above, the first few frames correspond to the first symbol c1 = s, while the latter frames correspond to the second \u2018e\u2019: c3 = e. Fig. 7. An attention-based encoder decoder (AED) model [15], [16], [53]. The output distribution is conditioned on the decoder state, si (which summarizes the previously decoded symbols), and the context vector, vi (which summarizes the encoder output based on the decoder state). In the seminal work of Chan et al., [16], for example, this is accomplished by concatenating the two vectors, as denoted by the (cid:76) symbol in the figure. sequence, X = (x1, . . . , xT \u2032), into a higher-level representa- tion H(X) = (h1, . . . , hT ) \u2013 and an attention-based decoder which defines the probability distribution over the set of output symbols, Ceos. Thus,"}, {"question": " What is one of the main benefits of explicit alignment approaches like CTC, RNN-T, or RNA models?", "answer": " They result in ASR models that are easily amenable to frame-synchronous decoding.", "ref_chunk": "diagonal transitions correspond to outputting a non-blank symbol. The FSA (left) represents the set of valid alignments for the RNA model. Although the FSA is identical to the corresponding FSA for RNN-T in Figure 4, the semantics of the \u27e8b\u27e9 label are different in the two cases. symbol has a different interpretation in RNA and the RNN- T models: in RNN-T, outputting a blank symbol advances the model to the next frame; in RNA, however, the model advances to the next frame after outputting a single blank or non-blank label. Restricting the model to output a single non- blank label at each frame improves computational efficiency and simplifies the decoding process, by limiting the number of model expansions at each frame (in constrast to RNN-T decoding). For example, if T = 8, and C = (s, e, e), then A = (\u27e8b\u27e9 , s, \u27e8b\u27e9 , e, \u27e8b\u27e9 , \u27e8b\u27e9 , e, \u27e8b\u27e9) \u2208 ARNA (X,C) as illustrated in Figure 6. The RNA posterior probability, P (C|X), is defined as: PRNA(C|X) = (cid:88) P (A|H(X)) A\u2208ARNA (X,C) = (cid:88) T (cid:89) P (at|at\u22121, . . . , a1, H(X)) where, as before it denotes the number of non-blank sym- bols in the partial alignment sequence (a1, . . . , at\u22121), and qt\u22121 = NN(\u00b7|at\u22121, \u00b7 \u00b7 \u00b7 , a1) represents the output of a neu- ral network which summarizes the entire partial alignment sequence, where NN(\u00b7) represents a suitable neural network (an LSTM in [46]). Thus, RNA removes the one remaining conditional independence assumption of the RNN-T model, by conditioning on the sequence of previous non-blank labels as well as the alignment that generated them. However, this comes at a cost: the exact computation of the log-likelihood in Eq. (3) (and corresponding gradients) is intractable. Instead, RNA makes two simplifying assumption to ensure tractable training: by assuming that the model can only output a single label at each frame; and utilizing a straight-through estimator for the alignment [50]. The latter constraint \u2013 allowing only a single label (blank or non-blank) at each frame \u2013 has also been explored in the context of the monotonic RNN-T model [51]. Finally, we note that the work in [52] further generalizes the RNA model by employing two RNNs when defining the state: a slow RNN (which corresponds to the sequence of previously predicted non-blank labels), and a fast RNN (which also conditions on the frames at which the non-blank labels were output). C. Implicit Alignment Modeling Approaches One of the main benefits of the explicit alignment ap- proaches such as CTC, RNN-T, or RNA is that they result in ASR models that are easily amenable to frame-synchronous decoding6 In this section, we discuss the attention-based encoder-decoder (AED) models (also known as, listen-attend- and-spell (LAS)) [15], [16], [53], which employs the attention mechanism [43] to implicitly identify and model the portions of the input acoustics which are relevant to each output unit. These models were first popularized in the context of machine translation [54]. Unlike explicit alignment modeling approaches, attention-based encoder-decoder models use an attention mechanism [43] to learn a correspondence between the entire acoustic sequence and the individual labels. Such models support label-synchronous decoding, meaning that during inference, each hypothesis in the beam is expanded by 1 label. In the explicit alignment approaches presented in Sec- tion III-B, during inference, the model continues to output symbols until it has processed the final frame at which point the decoding process is complete; similarly, during training, the forward-backward algorithm aligns over all possible align- ment sequences. Since an AED model processes the entire acoustic sequence at once, the model needs a mechanism by which it can indicate that it is done emitting all output symbols. This is achieved by augmenting the set of outputs with an end-of-sentence symbol, \u27e8eos\u27e9, so that the output vocabulary consists of the set Ceos = C \u222a {\u27e8eos\u27e9}. Thus, the AED model, depicted in Figure 7, consists of an en- coder network \u2013 which encodes the input acoustic frame A\u2208ARNA (X,C) t=1 = (cid:88) A\u2208ARNA (X,C) T (cid:89) t=1 P (at|qt\u22121, ht) (4) 6 By frame-synchronous decoding, we refer to the ability of the model to produce output label for each input frame of speech. Models such as CTC, RNN-T, or RNA, support frame-synchronous decoding. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 6 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Attention Decoder Softmax Encoder H(X) e <eos> e Time s Fig. 8. Unlike models such as RNN-T or CTC, AED models do not have explicit alignment. However, it is possible to interpret the attention weights \u03b1t,i for a particular output symbol ci as an alignment weight which is represented above for the target sequence C = (s, e, e, \u27e8eos\u27e9). In this representation, the size of the circle and the darkness level are proportional to the corresponding attention weights; thus the total probability mass is the same for each row. As illustrated above, the first few frames correspond to the first symbol c1 = s, while the latter frames correspond to the second \u2018e\u2019: c3 = e. Fig. 7. An attention-based encoder decoder (AED) model [15], [16], [53]. The output distribution is conditioned on the decoder state, si (which summarizes the previously decoded symbols), and the context vector, vi (which summarizes the encoder output based on the decoder state). In the seminal work of Chan et al., [16], for example, this is accomplished by concatenating the two vectors, as denoted by the (cid:76) symbol in the figure. sequence, X = (x1, . . . , xT \u2032), into a higher-level representa- tion H(X) = (h1, . . . , hT ) \u2013 and an attention-based decoder which defines the probability distribution over the set of output symbols, Ceos. Thus,"}, {"question": " How do attention-based encoder-decoder (AED) models differ from explicit alignment modeling approaches?", "answer": " AED models use an attention mechanism to learn a correspondence between the entire acoustic sequence and the individual labels, while explicit alignment models directly align output symbols with input frames.", "ref_chunk": "diagonal transitions correspond to outputting a non-blank symbol. The FSA (left) represents the set of valid alignments for the RNA model. Although the FSA is identical to the corresponding FSA for RNN-T in Figure 4, the semantics of the \u27e8b\u27e9 label are different in the two cases. symbol has a different interpretation in RNA and the RNN- T models: in RNN-T, outputting a blank symbol advances the model to the next frame; in RNA, however, the model advances to the next frame after outputting a single blank or non-blank label. Restricting the model to output a single non- blank label at each frame improves computational efficiency and simplifies the decoding process, by limiting the number of model expansions at each frame (in constrast to RNN-T decoding). For example, if T = 8, and C = (s, e, e), then A = (\u27e8b\u27e9 , s, \u27e8b\u27e9 , e, \u27e8b\u27e9 , \u27e8b\u27e9 , e, \u27e8b\u27e9) \u2208 ARNA (X,C) as illustrated in Figure 6. The RNA posterior probability, P (C|X), is defined as: PRNA(C|X) = (cid:88) P (A|H(X)) A\u2208ARNA (X,C) = (cid:88) T (cid:89) P (at|at\u22121, . . . , a1, H(X)) where, as before it denotes the number of non-blank sym- bols in the partial alignment sequence (a1, . . . , at\u22121), and qt\u22121 = NN(\u00b7|at\u22121, \u00b7 \u00b7 \u00b7 , a1) represents the output of a neu- ral network which summarizes the entire partial alignment sequence, where NN(\u00b7) represents a suitable neural network (an LSTM in [46]). Thus, RNA removes the one remaining conditional independence assumption of the RNN-T model, by conditioning on the sequence of previous non-blank labels as well as the alignment that generated them. However, this comes at a cost: the exact computation of the log-likelihood in Eq. (3) (and corresponding gradients) is intractable. Instead, RNA makes two simplifying assumption to ensure tractable training: by assuming that the model can only output a single label at each frame; and utilizing a straight-through estimator for the alignment [50]. The latter constraint \u2013 allowing only a single label (blank or non-blank) at each frame \u2013 has also been explored in the context of the monotonic RNN-T model [51]. Finally, we note that the work in [52] further generalizes the RNA model by employing two RNNs when defining the state: a slow RNN (which corresponds to the sequence of previously predicted non-blank labels), and a fast RNN (which also conditions on the frames at which the non-blank labels were output). C. Implicit Alignment Modeling Approaches One of the main benefits of the explicit alignment ap- proaches such as CTC, RNN-T, or RNA is that they result in ASR models that are easily amenable to frame-synchronous decoding6 In this section, we discuss the attention-based encoder-decoder (AED) models (also known as, listen-attend- and-spell (LAS)) [15], [16], [53], which employs the attention mechanism [43] to implicitly identify and model the portions of the input acoustics which are relevant to each output unit. These models were first popularized in the context of machine translation [54]. Unlike explicit alignment modeling approaches, attention-based encoder-decoder models use an attention mechanism [43] to learn a correspondence between the entire acoustic sequence and the individual labels. Such models support label-synchronous decoding, meaning that during inference, each hypothesis in the beam is expanded by 1 label. In the explicit alignment approaches presented in Sec- tion III-B, during inference, the model continues to output symbols until it has processed the final frame at which point the decoding process is complete; similarly, during training, the forward-backward algorithm aligns over all possible align- ment sequences. Since an AED model processes the entire acoustic sequence at once, the model needs a mechanism by which it can indicate that it is done emitting all output symbols. This is achieved by augmenting the set of outputs with an end-of-sentence symbol, \u27e8eos\u27e9, so that the output vocabulary consists of the set Ceos = C \u222a {\u27e8eos\u27e9}. Thus, the AED model, depicted in Figure 7, consists of an en- coder network \u2013 which encodes the input acoustic frame A\u2208ARNA (X,C) t=1 = (cid:88) A\u2208ARNA (X,C) T (cid:89) t=1 P (at|qt\u22121, ht) (4) 6 By frame-synchronous decoding, we refer to the ability of the model to produce output label for each input frame of speech. Models such as CTC, RNN-T, or RNA, support frame-synchronous decoding. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 6 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Attention Decoder Softmax Encoder H(X) e <eos> e Time s Fig. 8. Unlike models such as RNN-T or CTC, AED models do not have explicit alignment. However, it is possible to interpret the attention weights \u03b1t,i for a particular output symbol ci as an alignment weight which is represented above for the target sequence C = (s, e, e, \u27e8eos\u27e9). In this representation, the size of the circle and the darkness level are proportional to the corresponding attention weights; thus the total probability mass is the same for each row. As illustrated above, the first few frames correspond to the first symbol c1 = s, while the latter frames correspond to the second \u2018e\u2019: c3 = e. Fig. 7. An attention-based encoder decoder (AED) model [15], [16], [53]. The output distribution is conditioned on the decoder state, si (which summarizes the previously decoded symbols), and the context vector, vi (which summarizes the encoder output based on the decoder state). In the seminal work of Chan et al., [16], for example, this is accomplished by concatenating the two vectors, as denoted by the (cid:76) symbol in the figure. sequence, X = (x1, . . . , xT \u2032), into a higher-level representa- tion H(X) = (h1, . . . , hT ) \u2013 and an attention-based decoder which defines the probability distribution over the set of output symbols, Ceos. Thus,"}, {"question": " What is used in AED models to indicate that all output symbols have been emitted?", "answer": " An end-of-sentence symbol, \u27e8eos\u27e9, is added to the output vocabulary.", "ref_chunk": "diagonal transitions correspond to outputting a non-blank symbol. The FSA (left) represents the set of valid alignments for the RNA model. Although the FSA is identical to the corresponding FSA for RNN-T in Figure 4, the semantics of the \u27e8b\u27e9 label are different in the two cases. symbol has a different interpretation in RNA and the RNN- T models: in RNN-T, outputting a blank symbol advances the model to the next frame; in RNA, however, the model advances to the next frame after outputting a single blank or non-blank label. Restricting the model to output a single non- blank label at each frame improves computational efficiency and simplifies the decoding process, by limiting the number of model expansions at each frame (in constrast to RNN-T decoding). For example, if T = 8, and C = (s, e, e), then A = (\u27e8b\u27e9 , s, \u27e8b\u27e9 , e, \u27e8b\u27e9 , \u27e8b\u27e9 , e, \u27e8b\u27e9) \u2208 ARNA (X,C) as illustrated in Figure 6. The RNA posterior probability, P (C|X), is defined as: PRNA(C|X) = (cid:88) P (A|H(X)) A\u2208ARNA (X,C) = (cid:88) T (cid:89) P (at|at\u22121, . . . , a1, H(X)) where, as before it denotes the number of non-blank sym- bols in the partial alignment sequence (a1, . . . , at\u22121), and qt\u22121 = NN(\u00b7|at\u22121, \u00b7 \u00b7 \u00b7 , a1) represents the output of a neu- ral network which summarizes the entire partial alignment sequence, where NN(\u00b7) represents a suitable neural network (an LSTM in [46]). Thus, RNA removes the one remaining conditional independence assumption of the RNN-T model, by conditioning on the sequence of previous non-blank labels as well as the alignment that generated them. However, this comes at a cost: the exact computation of the log-likelihood in Eq. (3) (and corresponding gradients) is intractable. Instead, RNA makes two simplifying assumption to ensure tractable training: by assuming that the model can only output a single label at each frame; and utilizing a straight-through estimator for the alignment [50]. The latter constraint \u2013 allowing only a single label (blank or non-blank) at each frame \u2013 has also been explored in the context of the monotonic RNN-T model [51]. Finally, we note that the work in [52] further generalizes the RNA model by employing two RNNs when defining the state: a slow RNN (which corresponds to the sequence of previously predicted non-blank labels), and a fast RNN (which also conditions on the frames at which the non-blank labels were output). C. Implicit Alignment Modeling Approaches One of the main benefits of the explicit alignment ap- proaches such as CTC, RNN-T, or RNA is that they result in ASR models that are easily amenable to frame-synchronous decoding6 In this section, we discuss the attention-based encoder-decoder (AED) models (also known as, listen-attend- and-spell (LAS)) [15], [16], [53], which employs the attention mechanism [43] to implicitly identify and model the portions of the input acoustics which are relevant to each output unit. These models were first popularized in the context of machine translation [54]. Unlike explicit alignment modeling approaches, attention-based encoder-decoder models use an attention mechanism [43] to learn a correspondence between the entire acoustic sequence and the individual labels. Such models support label-synchronous decoding, meaning that during inference, each hypothesis in the beam is expanded by 1 label. In the explicit alignment approaches presented in Sec- tion III-B, during inference, the model continues to output symbols until it has processed the final frame at which point the decoding process is complete; similarly, during training, the forward-backward algorithm aligns over all possible align- ment sequences. Since an AED model processes the entire acoustic sequence at once, the model needs a mechanism by which it can indicate that it is done emitting all output symbols. This is achieved by augmenting the set of outputs with an end-of-sentence symbol, \u27e8eos\u27e9, so that the output vocabulary consists of the set Ceos = C \u222a {\u27e8eos\u27e9}. Thus, the AED model, depicted in Figure 7, consists of an en- coder network \u2013 which encodes the input acoustic frame A\u2208ARNA (X,C) t=1 = (cid:88) A\u2208ARNA (X,C) T (cid:89) t=1 P (at|qt\u22121, ht) (4) 6 By frame-synchronous decoding, we refer to the ability of the model to produce output label for each input frame of speech. Models such as CTC, RNN-T, or RNA, support frame-synchronous decoding. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 6 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Attention Decoder Softmax Encoder H(X) e <eos> e Time s Fig. 8. Unlike models such as RNN-T or CTC, AED models do not have explicit alignment. However, it is possible to interpret the attention weights \u03b1t,i for a particular output symbol ci as an alignment weight which is represented above for the target sequence C = (s, e, e, \u27e8eos\u27e9). In this representation, the size of the circle and the darkness level are proportional to the corresponding attention weights; thus the total probability mass is the same for each row. As illustrated above, the first few frames correspond to the first symbol c1 = s, while the latter frames correspond to the second \u2018e\u2019: c3 = e. Fig. 7. An attention-based encoder decoder (AED) model [15], [16], [53]. The output distribution is conditioned on the decoder state, si (which summarizes the previously decoded symbols), and the context vector, vi (which summarizes the encoder output based on the decoder state). In the seminal work of Chan et al., [16], for example, this is accomplished by concatenating the two vectors, as denoted by the (cid:76) symbol in the figure. sequence, X = (x1, . . . , xT \u2032), into a higher-level representa- tion H(X) = (h1, . . . , hT ) \u2013 and an attention-based decoder which defines the probability distribution over the set of output symbols, Ceos. Thus,"}, {"question": " What does the attention mechanism in AED models allow the model to do during inference?", "answer": " It allows the model to learn which portions of the input acoustics are relevant to each output unit.", "ref_chunk": "diagonal transitions correspond to outputting a non-blank symbol. The FSA (left) represents the set of valid alignments for the RNA model. Although the FSA is identical to the corresponding FSA for RNN-T in Figure 4, the semantics of the \u27e8b\u27e9 label are different in the two cases. symbol has a different interpretation in RNA and the RNN- T models: in RNN-T, outputting a blank symbol advances the model to the next frame; in RNA, however, the model advances to the next frame after outputting a single blank or non-blank label. Restricting the model to output a single non- blank label at each frame improves computational efficiency and simplifies the decoding process, by limiting the number of model expansions at each frame (in constrast to RNN-T decoding). For example, if T = 8, and C = (s, e, e), then A = (\u27e8b\u27e9 , s, \u27e8b\u27e9 , e, \u27e8b\u27e9 , \u27e8b\u27e9 , e, \u27e8b\u27e9) \u2208 ARNA (X,C) as illustrated in Figure 6. The RNA posterior probability, P (C|X), is defined as: PRNA(C|X) = (cid:88) P (A|H(X)) A\u2208ARNA (X,C) = (cid:88) T (cid:89) P (at|at\u22121, . . . , a1, H(X)) where, as before it denotes the number of non-blank sym- bols in the partial alignment sequence (a1, . . . , at\u22121), and qt\u22121 = NN(\u00b7|at\u22121, \u00b7 \u00b7 \u00b7 , a1) represents the output of a neu- ral network which summarizes the entire partial alignment sequence, where NN(\u00b7) represents a suitable neural network (an LSTM in [46]). Thus, RNA removes the one remaining conditional independence assumption of the RNN-T model, by conditioning on the sequence of previous non-blank labels as well as the alignment that generated them. However, this comes at a cost: the exact computation of the log-likelihood in Eq. (3) (and corresponding gradients) is intractable. Instead, RNA makes two simplifying assumption to ensure tractable training: by assuming that the model can only output a single label at each frame; and utilizing a straight-through estimator for the alignment [50]. The latter constraint \u2013 allowing only a single label (blank or non-blank) at each frame \u2013 has also been explored in the context of the monotonic RNN-T model [51]. Finally, we note that the work in [52] further generalizes the RNA model by employing two RNNs when defining the state: a slow RNN (which corresponds to the sequence of previously predicted non-blank labels), and a fast RNN (which also conditions on the frames at which the non-blank labels were output). C. Implicit Alignment Modeling Approaches One of the main benefits of the explicit alignment ap- proaches such as CTC, RNN-T, or RNA is that they result in ASR models that are easily amenable to frame-synchronous decoding6 In this section, we discuss the attention-based encoder-decoder (AED) models (also known as, listen-attend- and-spell (LAS)) [15], [16], [53], which employs the attention mechanism [43] to implicitly identify and model the portions of the input acoustics which are relevant to each output unit. These models were first popularized in the context of machine translation [54]. Unlike explicit alignment modeling approaches, attention-based encoder-decoder models use an attention mechanism [43] to learn a correspondence between the entire acoustic sequence and the individual labels. Such models support label-synchronous decoding, meaning that during inference, each hypothesis in the beam is expanded by 1 label. In the explicit alignment approaches presented in Sec- tion III-B, during inference, the model continues to output symbols until it has processed the final frame at which point the decoding process is complete; similarly, during training, the forward-backward algorithm aligns over all possible align- ment sequences. Since an AED model processes the entire acoustic sequence at once, the model needs a mechanism by which it can indicate that it is done emitting all output symbols. This is achieved by augmenting the set of outputs with an end-of-sentence symbol, \u27e8eos\u27e9, so that the output vocabulary consists of the set Ceos = C \u222a {\u27e8eos\u27e9}. Thus, the AED model, depicted in Figure 7, consists of an en- coder network \u2013 which encodes the input acoustic frame A\u2208ARNA (X,C) t=1 = (cid:88) A\u2208ARNA (X,C) T (cid:89) t=1 P (at|qt\u22121, ht) (4) 6 By frame-synchronous decoding, we refer to the ability of the model to produce output label for each input frame of speech. Models such as CTC, RNN-T, or RNA, support frame-synchronous decoding. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 6 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Attention Decoder Softmax Encoder H(X) e <eos> e Time s Fig. 8. Unlike models such as RNN-T or CTC, AED models do not have explicit alignment. However, it is possible to interpret the attention weights \u03b1t,i for a particular output symbol ci as an alignment weight which is represented above for the target sequence C = (s, e, e, \u27e8eos\u27e9). In this representation, the size of the circle and the darkness level are proportional to the corresponding attention weights; thus the total probability mass is the same for each row. As illustrated above, the first few frames correspond to the first symbol c1 = s, while the latter frames correspond to the second \u2018e\u2019: c3 = e. Fig. 7. An attention-based encoder decoder (AED) model [15], [16], [53]. The output distribution is conditioned on the decoder state, si (which summarizes the previously decoded symbols), and the context vector, vi (which summarizes the encoder output based on the decoder state). In the seminal work of Chan et al., [16], for example, this is accomplished by concatenating the two vectors, as denoted by the (cid:76) symbol in the figure. sequence, X = (x1, . . . , xT \u2032), into a higher-level representa- tion H(X) = (h1, . . . , hT ) \u2013 and an attention-based decoder which defines the probability distribution over the set of output symbols, Ceos. Thus,"}, {"question": " How is the output distribution in AED models conditioned in relation to the decoder state and the context vector?", "answer": " The output distribution is conditioned on the decoder state (summarizes the previously decoded symbols) and the context vector (summarizes the encoder output based on the decoder state).", "ref_chunk": "diagonal transitions correspond to outputting a non-blank symbol. The FSA (left) represents the set of valid alignments for the RNA model. Although the FSA is identical to the corresponding FSA for RNN-T in Figure 4, the semantics of the \u27e8b\u27e9 label are different in the two cases. symbol has a different interpretation in RNA and the RNN- T models: in RNN-T, outputting a blank symbol advances the model to the next frame; in RNA, however, the model advances to the next frame after outputting a single blank or non-blank label. Restricting the model to output a single non- blank label at each frame improves computational efficiency and simplifies the decoding process, by limiting the number of model expansions at each frame (in constrast to RNN-T decoding). For example, if T = 8, and C = (s, e, e), then A = (\u27e8b\u27e9 , s, \u27e8b\u27e9 , e, \u27e8b\u27e9 , \u27e8b\u27e9 , e, \u27e8b\u27e9) \u2208 ARNA (X,C) as illustrated in Figure 6. The RNA posterior probability, P (C|X), is defined as: PRNA(C|X) = (cid:88) P (A|H(X)) A\u2208ARNA (X,C) = (cid:88) T (cid:89) P (at|at\u22121, . . . , a1, H(X)) where, as before it denotes the number of non-blank sym- bols in the partial alignment sequence (a1, . . . , at\u22121), and qt\u22121 = NN(\u00b7|at\u22121, \u00b7 \u00b7 \u00b7 , a1) represents the output of a neu- ral network which summarizes the entire partial alignment sequence, where NN(\u00b7) represents a suitable neural network (an LSTM in [46]). Thus, RNA removes the one remaining conditional independence assumption of the RNN-T model, by conditioning on the sequence of previous non-blank labels as well as the alignment that generated them. However, this comes at a cost: the exact computation of the log-likelihood in Eq. (3) (and corresponding gradients) is intractable. Instead, RNA makes two simplifying assumption to ensure tractable training: by assuming that the model can only output a single label at each frame; and utilizing a straight-through estimator for the alignment [50]. The latter constraint \u2013 allowing only a single label (blank or non-blank) at each frame \u2013 has also been explored in the context of the monotonic RNN-T model [51]. Finally, we note that the work in [52] further generalizes the RNA model by employing two RNNs when defining the state: a slow RNN (which corresponds to the sequence of previously predicted non-blank labels), and a fast RNN (which also conditions on the frames at which the non-blank labels were output). C. Implicit Alignment Modeling Approaches One of the main benefits of the explicit alignment ap- proaches such as CTC, RNN-T, or RNA is that they result in ASR models that are easily amenable to frame-synchronous decoding6 In this section, we discuss the attention-based encoder-decoder (AED) models (also known as, listen-attend- and-spell (LAS)) [15], [16], [53], which employs the attention mechanism [43] to implicitly identify and model the portions of the input acoustics which are relevant to each output unit. These models were first popularized in the context of machine translation [54]. Unlike explicit alignment modeling approaches, attention-based encoder-decoder models use an attention mechanism [43] to learn a correspondence between the entire acoustic sequence and the individual labels. Such models support label-synchronous decoding, meaning that during inference, each hypothesis in the beam is expanded by 1 label. In the explicit alignment approaches presented in Sec- tion III-B, during inference, the model continues to output symbols until it has processed the final frame at which point the decoding process is complete; similarly, during training, the forward-backward algorithm aligns over all possible align- ment sequences. Since an AED model processes the entire acoustic sequence at once, the model needs a mechanism by which it can indicate that it is done emitting all output symbols. This is achieved by augmenting the set of outputs with an end-of-sentence symbol, \u27e8eos\u27e9, so that the output vocabulary consists of the set Ceos = C \u222a {\u27e8eos\u27e9}. Thus, the AED model, depicted in Figure 7, consists of an en- coder network \u2013 which encodes the input acoustic frame A\u2208ARNA (X,C) t=1 = (cid:88) A\u2208ARNA (X,C) T (cid:89) t=1 P (at|qt\u22121, ht) (4) 6 By frame-synchronous decoding, we refer to the ability of the model to produce output label for each input frame of speech. Models such as CTC, RNN-T, or RNA, support frame-synchronous decoding. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 6 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Attention Decoder Softmax Encoder H(X) e <eos> e Time s Fig. 8. Unlike models such as RNN-T or CTC, AED models do not have explicit alignment. However, it is possible to interpret the attention weights \u03b1t,i for a particular output symbol ci as an alignment weight which is represented above for the target sequence C = (s, e, e, \u27e8eos\u27e9). In this representation, the size of the circle and the darkness level are proportional to the corresponding attention weights; thus the total probability mass is the same for each row. As illustrated above, the first few frames correspond to the first symbol c1 = s, while the latter frames correspond to the second \u2018e\u2019: c3 = e. Fig. 7. An attention-based encoder decoder (AED) model [15], [16], [53]. The output distribution is conditioned on the decoder state, si (which summarizes the previously decoded symbols), and the context vector, vi (which summarizes the encoder output based on the decoder state). In the seminal work of Chan et al., [16], for example, this is accomplished by concatenating the two vectors, as denoted by the (cid:76) symbol in the figure. sequence, X = (x1, . . . , xT \u2032), into a higher-level representa- tion H(X) = (h1, . . . , hT ) \u2013 and an attention-based decoder which defines the probability distribution over the set of output symbols, Ceos. Thus,"}], "doc_text": "diagonal transitions correspond to outputting a non-blank symbol. The FSA (left) represents the set of valid alignments for the RNA model. Although the FSA is identical to the corresponding FSA for RNN-T in Figure 4, the semantics of the \u27e8b\u27e9 label are different in the two cases. symbol has a different interpretation in RNA and the RNN- T models: in RNN-T, outputting a blank symbol advances the model to the next frame; in RNA, however, the model advances to the next frame after outputting a single blank or non-blank label. Restricting the model to output a single non- blank label at each frame improves computational efficiency and simplifies the decoding process, by limiting the number of model expansions at each frame (in constrast to RNN-T decoding). For example, if T = 8, and C = (s, e, e), then A = (\u27e8b\u27e9 , s, \u27e8b\u27e9 , e, \u27e8b\u27e9 , \u27e8b\u27e9 , e, \u27e8b\u27e9) \u2208 ARNA (X,C) as illustrated in Figure 6. The RNA posterior probability, P (C|X), is defined as: PRNA(C|X) = (cid:88) P (A|H(X)) A\u2208ARNA (X,C) = (cid:88) T (cid:89) P (at|at\u22121, . . . , a1, H(X)) where, as before it denotes the number of non-blank sym- bols in the partial alignment sequence (a1, . . . , at\u22121), and qt\u22121 = NN(\u00b7|at\u22121, \u00b7 \u00b7 \u00b7 , a1) represents the output of a neu- ral network which summarizes the entire partial alignment sequence, where NN(\u00b7) represents a suitable neural network (an LSTM in [46]). Thus, RNA removes the one remaining conditional independence assumption of the RNN-T model, by conditioning on the sequence of previous non-blank labels as well as the alignment that generated them. However, this comes at a cost: the exact computation of the log-likelihood in Eq. (3) (and corresponding gradients) is intractable. Instead, RNA makes two simplifying assumption to ensure tractable training: by assuming that the model can only output a single label at each frame; and utilizing a straight-through estimator for the alignment [50]. The latter constraint \u2013 allowing only a single label (blank or non-blank) at each frame \u2013 has also been explored in the context of the monotonic RNN-T model [51]. Finally, we note that the work in [52] further generalizes the RNA model by employing two RNNs when defining the state: a slow RNN (which corresponds to the sequence of previously predicted non-blank labels), and a fast RNN (which also conditions on the frames at which the non-blank labels were output). C. Implicit Alignment Modeling Approaches One of the main benefits of the explicit alignment ap- proaches such as CTC, RNN-T, or RNA is that they result in ASR models that are easily amenable to frame-synchronous decoding6 In this section, we discuss the attention-based encoder-decoder (AED) models (also known as, listen-attend- and-spell (LAS)) [15], [16], [53], which employs the attention mechanism [43] to implicitly identify and model the portions of the input acoustics which are relevant to each output unit. These models were first popularized in the context of machine translation [54]. Unlike explicit alignment modeling approaches, attention-based encoder-decoder models use an attention mechanism [43] to learn a correspondence between the entire acoustic sequence and the individual labels. Such models support label-synchronous decoding, meaning that during inference, each hypothesis in the beam is expanded by 1 label. In the explicit alignment approaches presented in Sec- tion III-B, during inference, the model continues to output symbols until it has processed the final frame at which point the decoding process is complete; similarly, during training, the forward-backward algorithm aligns over all possible align- ment sequences. Since an AED model processes the entire acoustic sequence at once, the model needs a mechanism by which it can indicate that it is done emitting all output symbols. This is achieved by augmenting the set of outputs with an end-of-sentence symbol, \u27e8eos\u27e9, so that the output vocabulary consists of the set Ceos = C \u222a {\u27e8eos\u27e9}. Thus, the AED model, depicted in Figure 7, consists of an en- coder network \u2013 which encodes the input acoustic frame A\u2208ARNA (X,C) t=1 = (cid:88) A\u2208ARNA (X,C) T (cid:89) t=1 P (at|qt\u22121, ht) (4) 6 By frame-synchronous decoding, we refer to the ability of the model to produce output label for each input frame of speech. Models such as CTC, RNN-T, or RNA, support frame-synchronous decoding. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 6 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 Attention Decoder Softmax Encoder H(X) e <eos> e Time s Fig. 8. Unlike models such as RNN-T or CTC, AED models do not have explicit alignment. However, it is possible to interpret the attention weights \u03b1t,i for a particular output symbol ci as an alignment weight which is represented above for the target sequence C = (s, e, e, \u27e8eos\u27e9). In this representation, the size of the circle and the darkness level are proportional to the corresponding attention weights; thus the total probability mass is the same for each row. As illustrated above, the first few frames correspond to the first symbol c1 = s, while the latter frames correspond to the second \u2018e\u2019: c3 = e. Fig. 7. An attention-based encoder decoder (AED) model [15], [16], [53]. The output distribution is conditioned on the decoder state, si (which summarizes the previously decoded symbols), and the context vector, vi (which summarizes the encoder output based on the decoder state). In the seminal work of Chan et al., [16], for example, this is accomplished by concatenating the two vectors, as denoted by the (cid:76) symbol in the figure. sequence, X = (x1, . . . , xT \u2032), into a higher-level representa- tion H(X) = (h1, . . . , hT ) \u2013 and an attention-based decoder which defines the probability distribution over the set of output symbols, Ceos. Thus,"}