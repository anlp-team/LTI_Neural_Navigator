{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yonatan_Bisk_Reasoning_about_the_Unseen_for_Efficient_Outdoor_Object_Navigation_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What can lead to suboptimal behaviors in planning within outdoor environments?", "answer": " Direct determinations by the LLM-based method solely on localized information.", "ref_chunk": "the sharpness of B. Reasoning about the uncertainty In the context of planning within intricate outdoor envi- ronments, direct determinations by the LLM-based method solely on localized information can lead to suboptimal be- haviors, such as inconsistency and short-sightedness. These behaviors arise not only from the inherent tendency of LLMs to hallucinate [33], but also the non-delineated nature of the outdoor environment. Such properties become particularly concerning when placing heavy reliance on singular output generations from LLMs [34]. To fortify against these vulnerabilities and enhance deci- sion robustness, we elected to incorporate the future informa- tion using expanding RRT strategy. By projecting multiple Fig. 5. The left image illustrates the expansion process where, at each step, N nodes are expanded (with N = 3 as depicted). The right image shows the agent\u2019s decision-making process with distance cost at each step. forward-looking imaginary branches through iterative queries of LLM Visionary and LLM Evaluator (Figure 3), mitigating the risks associated with single query outputs. Our choice of RRT was informed by its intrinsic properties, in contrast of the sequential sampling process used in MCTS[22][20], RRT provides a parallelizable framework to allow us score and expand the imaginary nodes all at the same time. In our integration of the RRT, the specifics and underlying mechanics are meticulously outlined in Algorithm 1 and vi- sually complemented in Figure 5. At its core, our adaptation hinges on the dual roles assumed by the LLM: as an agent, denoted as LLM Eval, and as an evaluator, represented by LLM Gen. Their specific responsibilities and interplay will be further discussed in the following sections. a) LLM Evaluator: denoted as V (G, S) \u2192 v, assesses the correlation between a scene description S and the pro- vided goal objects or instructions G. Its primary role is to guide the agent by offering a reference score, indicating the likelihood of achieving the goal based on the current scene. The scoring mechanism is structured on a Likert scale ranging from 1 to 5, where a score of 1 indicates a low likelihood of goal achievement, and a score of 5 signifies a high likelihood. This evaluative approach ensures that the agent can make informed decisions based on the contextual relevance of the scene to the goal. represented as L(St) \u2192 St+1, produces the next scene descriptor St+1 based on the current scene description St. Its objective is to enable the agent to anticipate or predict its future waypoint. This is achieved by prompting the agent to envision what it might encounter next. Comprehensive prompt templates and further details are available on our official website. b) LLM Visionary: In our adaptation of the RRT, the algorithm\u2019s mechanics are determined two hyperparameters: N: Dictates the action space dimension, signifying the range of feasible directions the agent can embark upon during its exploration. L: Denotes the length of individual simulations within the branch. This dictates the depth of the tree and how far the algorithm projects into potential future states. Algorithm 1 LLM-RRT 1: procedure LLM-RRT(K, St+1list) Ssample t+2 \u2190 EXPANSION(S\u2217 t+1) 2: vmean \u2190 SIMULATION(Ssample t+2 Q(S\u2217 t+1) \u2190BACK-PROPAGATION(vmean, depth) 4: 5: end procedure ) 3: C. Perceiving The agent captures N images during each exploration steps. These images are processed by a Vision Language Model (VLM). For the purposes of this study, we employed Kosmos-2 [35], a VLM fine-tuned with spatially-grounded image-text data. This model offers the distinct advantage of providing detailed object-level descriptions of scenes. Importantly, it is promptable, so we prompt it to describe not only the objects in the scene, but also the spatial relationship between objects as well as the backgrounds. D. Action on real robot Upon waypoint determination by the scoring function detailed in Section IV-A, our robot employs a straightforward PID controller to traverse the path navigating between the current waypoint and the subsequent one, with the localiza- tion from on board high resolution RTK-GPS and IMU. V. EXPERIMENTS Comprehensive evaluations are conducted across multiple platforms: 1. The AirSim [36] simulation environment: A photo-realistic outdoor simulation setting that consists of different semantic distinguishable areas in Downtown West environment. 2. A real-world robotic platform: Unitree Go1, equiped with a USB camera, high resolution RTK-GPS module, and Inertial measurement unit. A. A Compute Aware Metric for LLM-based Robotic Agents In the assessment of robotic agents in real-world settings, particularly for outdoor tasks like Search and Rescue, it is vital to strike a balance between navigation efficiency, com- putational overhead, and travel duration. The dominant met- rics in the space: Success Rate (SR) and Success Weighted by Path Length (SPL), ignore \u201ctime\u201d. Here, we specifically mean wallclock time, or the length of an experiment or episode. While always relevant in practical scenarios, the use of Large Foundation Models (e.g. over API) introduces a new computational trade-off. Specifically, the interplay between Computational Time (CT) and Travel Time (TT). An increase in computation that results in reduced travel time might lead to overall efficiency gains depending on the amount of computational time required. Colloquially, when is it faster to think before acting, versus acting on a hunch? a) Formulating the CASR Metric: This overall effi- ciency versus the maximum allowed episode length is simply a normalized sum of the two components: Compute (CT) and Travel (TT) time. The value is normalized to the range [0,1], aligning it for integration with the traditional SR metric: L1 L2 SR L3 L4 Avg L1 L2 OSR L3 L4 Avg L1 L2 SPL L3 L4 Avg L1 L2 CASR L3 L4 Avg LLM-as-Eval LLM-MCTS(10 iter) 0.17 0.09 0.00 0.00 0.06 0.54 0.43 0.59 0.33 0.47 0.17 0.27 0.00 0.00 0.06 0.88 0.76 0.63 0.69 0.74 0.13 0.04 0.00 0.00 0.04 0.37 0.31 0.38 0.23 0.32 0.10 0.06 0.00 0.00 0.04 0.11 0.09 0.11 0.07 0.10 Ours 0.59 0.49 0.63 0.32 0.51 0.88 0.82 0.71 0.88 0.82 0.44 0.32 0.43 0.22 0.35 0.51 0.42 0.46 0.28 0.42 TABLE I BASELINE COMPARISON FOR DIFFERENT TASK LEVELS IN SIMULATION (AIRSIM [36] ICT,T T = 1 \u2212 CT"}, {"question": " What are some properties of the outdoor environment that can lead to suboptimal behaviors?", "answer": " Hallucination by LLMs and the non-delineated nature of the outdoor environment.", "ref_chunk": "the sharpness of B. Reasoning about the uncertainty In the context of planning within intricate outdoor envi- ronments, direct determinations by the LLM-based method solely on localized information can lead to suboptimal be- haviors, such as inconsistency and short-sightedness. These behaviors arise not only from the inherent tendency of LLMs to hallucinate [33], but also the non-delineated nature of the outdoor environment. Such properties become particularly concerning when placing heavy reliance on singular output generations from LLMs [34]. To fortify against these vulnerabilities and enhance deci- sion robustness, we elected to incorporate the future informa- tion using expanding RRT strategy. By projecting multiple Fig. 5. The left image illustrates the expansion process where, at each step, N nodes are expanded (with N = 3 as depicted). The right image shows the agent\u2019s decision-making process with distance cost at each step. forward-looking imaginary branches through iterative queries of LLM Visionary and LLM Evaluator (Figure 3), mitigating the risks associated with single query outputs. Our choice of RRT was informed by its intrinsic properties, in contrast of the sequential sampling process used in MCTS[22][20], RRT provides a parallelizable framework to allow us score and expand the imaginary nodes all at the same time. In our integration of the RRT, the specifics and underlying mechanics are meticulously outlined in Algorithm 1 and vi- sually complemented in Figure 5. At its core, our adaptation hinges on the dual roles assumed by the LLM: as an agent, denoted as LLM Eval, and as an evaluator, represented by LLM Gen. Their specific responsibilities and interplay will be further discussed in the following sections. a) LLM Evaluator: denoted as V (G, S) \u2192 v, assesses the correlation between a scene description S and the pro- vided goal objects or instructions G. Its primary role is to guide the agent by offering a reference score, indicating the likelihood of achieving the goal based on the current scene. The scoring mechanism is structured on a Likert scale ranging from 1 to 5, where a score of 1 indicates a low likelihood of goal achievement, and a score of 5 signifies a high likelihood. This evaluative approach ensures that the agent can make informed decisions based on the contextual relevance of the scene to the goal. represented as L(St) \u2192 St+1, produces the next scene descriptor St+1 based on the current scene description St. Its objective is to enable the agent to anticipate or predict its future waypoint. This is achieved by prompting the agent to envision what it might encounter next. Comprehensive prompt templates and further details are available on our official website. b) LLM Visionary: In our adaptation of the RRT, the algorithm\u2019s mechanics are determined two hyperparameters: N: Dictates the action space dimension, signifying the range of feasible directions the agent can embark upon during its exploration. L: Denotes the length of individual simulations within the branch. This dictates the depth of the tree and how far the algorithm projects into potential future states. Algorithm 1 LLM-RRT 1: procedure LLM-RRT(K, St+1list) Ssample t+2 \u2190 EXPANSION(S\u2217 t+1) 2: vmean \u2190 SIMULATION(Ssample t+2 Q(S\u2217 t+1) \u2190BACK-PROPAGATION(vmean, depth) 4: 5: end procedure ) 3: C. Perceiving The agent captures N images during each exploration steps. These images are processed by a Vision Language Model (VLM). For the purposes of this study, we employed Kosmos-2 [35], a VLM fine-tuned with spatially-grounded image-text data. This model offers the distinct advantage of providing detailed object-level descriptions of scenes. Importantly, it is promptable, so we prompt it to describe not only the objects in the scene, but also the spatial relationship between objects as well as the backgrounds. D. Action on real robot Upon waypoint determination by the scoring function detailed in Section IV-A, our robot employs a straightforward PID controller to traverse the path navigating between the current waypoint and the subsequent one, with the localiza- tion from on board high resolution RTK-GPS and IMU. V. EXPERIMENTS Comprehensive evaluations are conducted across multiple platforms: 1. The AirSim [36] simulation environment: A photo-realistic outdoor simulation setting that consists of different semantic distinguishable areas in Downtown West environment. 2. A real-world robotic platform: Unitree Go1, equiped with a USB camera, high resolution RTK-GPS module, and Inertial measurement unit. A. A Compute Aware Metric for LLM-based Robotic Agents In the assessment of robotic agents in real-world settings, particularly for outdoor tasks like Search and Rescue, it is vital to strike a balance between navigation efficiency, com- putational overhead, and travel duration. The dominant met- rics in the space: Success Rate (SR) and Success Weighted by Path Length (SPL), ignore \u201ctime\u201d. Here, we specifically mean wallclock time, or the length of an experiment or episode. While always relevant in practical scenarios, the use of Large Foundation Models (e.g. over API) introduces a new computational trade-off. Specifically, the interplay between Computational Time (CT) and Travel Time (TT). An increase in computation that results in reduced travel time might lead to overall efficiency gains depending on the amount of computational time required. Colloquially, when is it faster to think before acting, versus acting on a hunch? a) Formulating the CASR Metric: This overall effi- ciency versus the maximum allowed episode length is simply a normalized sum of the two components: Compute (CT) and Travel (TT) time. The value is normalized to the range [0,1], aligning it for integration with the traditional SR metric: L1 L2 SR L3 L4 Avg L1 L2 OSR L3 L4 Avg L1 L2 SPL L3 L4 Avg L1 L2 CASR L3 L4 Avg LLM-as-Eval LLM-MCTS(10 iter) 0.17 0.09 0.00 0.00 0.06 0.54 0.43 0.59 0.33 0.47 0.17 0.27 0.00 0.00 0.06 0.88 0.76 0.63 0.69 0.74 0.13 0.04 0.00 0.00 0.04 0.37 0.31 0.38 0.23 0.32 0.10 0.06 0.00 0.00 0.04 0.11 0.09 0.11 0.07 0.10 Ours 0.59 0.49 0.63 0.32 0.51 0.88 0.82 0.71 0.88 0.82 0.44 0.32 0.43 0.22 0.35 0.51 0.42 0.46 0.28 0.42 TABLE I BASELINE COMPARISON FOR DIFFERENT TASK LEVELS IN SIMULATION (AIRSIM [36] ICT,T T = 1 \u2212 CT"}, {"question": " How does incorporating future information help fortify decision robustness?", "answer": " By projecting multiple forward-looking imaginary branches through iterative queries of LLM Visionary and LLM Evaluator.", "ref_chunk": "the sharpness of B. Reasoning about the uncertainty In the context of planning within intricate outdoor envi- ronments, direct determinations by the LLM-based method solely on localized information can lead to suboptimal be- haviors, such as inconsistency and short-sightedness. These behaviors arise not only from the inherent tendency of LLMs to hallucinate [33], but also the non-delineated nature of the outdoor environment. Such properties become particularly concerning when placing heavy reliance on singular output generations from LLMs [34]. To fortify against these vulnerabilities and enhance deci- sion robustness, we elected to incorporate the future informa- tion using expanding RRT strategy. By projecting multiple Fig. 5. The left image illustrates the expansion process where, at each step, N nodes are expanded (with N = 3 as depicted). The right image shows the agent\u2019s decision-making process with distance cost at each step. forward-looking imaginary branches through iterative queries of LLM Visionary and LLM Evaluator (Figure 3), mitigating the risks associated with single query outputs. Our choice of RRT was informed by its intrinsic properties, in contrast of the sequential sampling process used in MCTS[22][20], RRT provides a parallelizable framework to allow us score and expand the imaginary nodes all at the same time. In our integration of the RRT, the specifics and underlying mechanics are meticulously outlined in Algorithm 1 and vi- sually complemented in Figure 5. At its core, our adaptation hinges on the dual roles assumed by the LLM: as an agent, denoted as LLM Eval, and as an evaluator, represented by LLM Gen. Their specific responsibilities and interplay will be further discussed in the following sections. a) LLM Evaluator: denoted as V (G, S) \u2192 v, assesses the correlation between a scene description S and the pro- vided goal objects or instructions G. Its primary role is to guide the agent by offering a reference score, indicating the likelihood of achieving the goal based on the current scene. The scoring mechanism is structured on a Likert scale ranging from 1 to 5, where a score of 1 indicates a low likelihood of goal achievement, and a score of 5 signifies a high likelihood. This evaluative approach ensures that the agent can make informed decisions based on the contextual relevance of the scene to the goal. represented as L(St) \u2192 St+1, produces the next scene descriptor St+1 based on the current scene description St. Its objective is to enable the agent to anticipate or predict its future waypoint. This is achieved by prompting the agent to envision what it might encounter next. Comprehensive prompt templates and further details are available on our official website. b) LLM Visionary: In our adaptation of the RRT, the algorithm\u2019s mechanics are determined two hyperparameters: N: Dictates the action space dimension, signifying the range of feasible directions the agent can embark upon during its exploration. L: Denotes the length of individual simulations within the branch. This dictates the depth of the tree and how far the algorithm projects into potential future states. Algorithm 1 LLM-RRT 1: procedure LLM-RRT(K, St+1list) Ssample t+2 \u2190 EXPANSION(S\u2217 t+1) 2: vmean \u2190 SIMULATION(Ssample t+2 Q(S\u2217 t+1) \u2190BACK-PROPAGATION(vmean, depth) 4: 5: end procedure ) 3: C. Perceiving The agent captures N images during each exploration steps. These images are processed by a Vision Language Model (VLM). For the purposes of this study, we employed Kosmos-2 [35], a VLM fine-tuned with spatially-grounded image-text data. This model offers the distinct advantage of providing detailed object-level descriptions of scenes. Importantly, it is promptable, so we prompt it to describe not only the objects in the scene, but also the spatial relationship between objects as well as the backgrounds. D. Action on real robot Upon waypoint determination by the scoring function detailed in Section IV-A, our robot employs a straightforward PID controller to traverse the path navigating between the current waypoint and the subsequent one, with the localiza- tion from on board high resolution RTK-GPS and IMU. V. EXPERIMENTS Comprehensive evaluations are conducted across multiple platforms: 1. The AirSim [36] simulation environment: A photo-realistic outdoor simulation setting that consists of different semantic distinguishable areas in Downtown West environment. 2. A real-world robotic platform: Unitree Go1, equiped with a USB camera, high resolution RTK-GPS module, and Inertial measurement unit. A. A Compute Aware Metric for LLM-based Robotic Agents In the assessment of robotic agents in real-world settings, particularly for outdoor tasks like Search and Rescue, it is vital to strike a balance between navigation efficiency, com- putational overhead, and travel duration. The dominant met- rics in the space: Success Rate (SR) and Success Weighted by Path Length (SPL), ignore \u201ctime\u201d. Here, we specifically mean wallclock time, or the length of an experiment or episode. While always relevant in practical scenarios, the use of Large Foundation Models (e.g. over API) introduces a new computational trade-off. Specifically, the interplay between Computational Time (CT) and Travel Time (TT). An increase in computation that results in reduced travel time might lead to overall efficiency gains depending on the amount of computational time required. Colloquially, when is it faster to think before acting, versus acting on a hunch? a) Formulating the CASR Metric: This overall effi- ciency versus the maximum allowed episode length is simply a normalized sum of the two components: Compute (CT) and Travel (TT) time. The value is normalized to the range [0,1], aligning it for integration with the traditional SR metric: L1 L2 SR L3 L4 Avg L1 L2 OSR L3 L4 Avg L1 L2 SPL L3 L4 Avg L1 L2 CASR L3 L4 Avg LLM-as-Eval LLM-MCTS(10 iter) 0.17 0.09 0.00 0.00 0.06 0.54 0.43 0.59 0.33 0.47 0.17 0.27 0.00 0.00 0.06 0.88 0.76 0.63 0.69 0.74 0.13 0.04 0.00 0.00 0.04 0.37 0.31 0.38 0.23 0.32 0.10 0.06 0.00 0.00 0.04 0.11 0.09 0.11 0.07 0.10 Ours 0.59 0.49 0.63 0.32 0.51 0.88 0.82 0.71 0.88 0.82 0.44 0.32 0.43 0.22 0.35 0.51 0.42 0.46 0.28 0.42 TABLE I BASELINE COMPARISON FOR DIFFERENT TASK LEVELS IN SIMULATION (AIRSIM [36] ICT,T T = 1 \u2212 CT"}, {"question": " What advantages does RRT provide in decision-making over MCTS?", "answer": " RRT provides a parallelizable framework to score and expand imaginary nodes all at the same time.", "ref_chunk": "the sharpness of B. Reasoning about the uncertainty In the context of planning within intricate outdoor envi- ronments, direct determinations by the LLM-based method solely on localized information can lead to suboptimal be- haviors, such as inconsistency and short-sightedness. These behaviors arise not only from the inherent tendency of LLMs to hallucinate [33], but also the non-delineated nature of the outdoor environment. Such properties become particularly concerning when placing heavy reliance on singular output generations from LLMs [34]. To fortify against these vulnerabilities and enhance deci- sion robustness, we elected to incorporate the future informa- tion using expanding RRT strategy. By projecting multiple Fig. 5. The left image illustrates the expansion process where, at each step, N nodes are expanded (with N = 3 as depicted). The right image shows the agent\u2019s decision-making process with distance cost at each step. forward-looking imaginary branches through iterative queries of LLM Visionary and LLM Evaluator (Figure 3), mitigating the risks associated with single query outputs. Our choice of RRT was informed by its intrinsic properties, in contrast of the sequential sampling process used in MCTS[22][20], RRT provides a parallelizable framework to allow us score and expand the imaginary nodes all at the same time. In our integration of the RRT, the specifics and underlying mechanics are meticulously outlined in Algorithm 1 and vi- sually complemented in Figure 5. At its core, our adaptation hinges on the dual roles assumed by the LLM: as an agent, denoted as LLM Eval, and as an evaluator, represented by LLM Gen. Their specific responsibilities and interplay will be further discussed in the following sections. a) LLM Evaluator: denoted as V (G, S) \u2192 v, assesses the correlation between a scene description S and the pro- vided goal objects or instructions G. Its primary role is to guide the agent by offering a reference score, indicating the likelihood of achieving the goal based on the current scene. The scoring mechanism is structured on a Likert scale ranging from 1 to 5, where a score of 1 indicates a low likelihood of goal achievement, and a score of 5 signifies a high likelihood. This evaluative approach ensures that the agent can make informed decisions based on the contextual relevance of the scene to the goal. represented as L(St) \u2192 St+1, produces the next scene descriptor St+1 based on the current scene description St. Its objective is to enable the agent to anticipate or predict its future waypoint. This is achieved by prompting the agent to envision what it might encounter next. Comprehensive prompt templates and further details are available on our official website. b) LLM Visionary: In our adaptation of the RRT, the algorithm\u2019s mechanics are determined two hyperparameters: N: Dictates the action space dimension, signifying the range of feasible directions the agent can embark upon during its exploration. L: Denotes the length of individual simulations within the branch. This dictates the depth of the tree and how far the algorithm projects into potential future states. Algorithm 1 LLM-RRT 1: procedure LLM-RRT(K, St+1list) Ssample t+2 \u2190 EXPANSION(S\u2217 t+1) 2: vmean \u2190 SIMULATION(Ssample t+2 Q(S\u2217 t+1) \u2190BACK-PROPAGATION(vmean, depth) 4: 5: end procedure ) 3: C. Perceiving The agent captures N images during each exploration steps. These images are processed by a Vision Language Model (VLM). For the purposes of this study, we employed Kosmos-2 [35], a VLM fine-tuned with spatially-grounded image-text data. This model offers the distinct advantage of providing detailed object-level descriptions of scenes. Importantly, it is promptable, so we prompt it to describe not only the objects in the scene, but also the spatial relationship between objects as well as the backgrounds. D. Action on real robot Upon waypoint determination by the scoring function detailed in Section IV-A, our robot employs a straightforward PID controller to traverse the path navigating between the current waypoint and the subsequent one, with the localiza- tion from on board high resolution RTK-GPS and IMU. V. EXPERIMENTS Comprehensive evaluations are conducted across multiple platforms: 1. The AirSim [36] simulation environment: A photo-realistic outdoor simulation setting that consists of different semantic distinguishable areas in Downtown West environment. 2. A real-world robotic platform: Unitree Go1, equiped with a USB camera, high resolution RTK-GPS module, and Inertial measurement unit. A. A Compute Aware Metric for LLM-based Robotic Agents In the assessment of robotic agents in real-world settings, particularly for outdoor tasks like Search and Rescue, it is vital to strike a balance between navigation efficiency, com- putational overhead, and travel duration. The dominant met- rics in the space: Success Rate (SR) and Success Weighted by Path Length (SPL), ignore \u201ctime\u201d. Here, we specifically mean wallclock time, or the length of an experiment or episode. While always relevant in practical scenarios, the use of Large Foundation Models (e.g. over API) introduces a new computational trade-off. Specifically, the interplay between Computational Time (CT) and Travel Time (TT). An increase in computation that results in reduced travel time might lead to overall efficiency gains depending on the amount of computational time required. Colloquially, when is it faster to think before acting, versus acting on a hunch? a) Formulating the CASR Metric: This overall effi- ciency versus the maximum allowed episode length is simply a normalized sum of the two components: Compute (CT) and Travel (TT) time. The value is normalized to the range [0,1], aligning it for integration with the traditional SR metric: L1 L2 SR L3 L4 Avg L1 L2 OSR L3 L4 Avg L1 L2 SPL L3 L4 Avg L1 L2 CASR L3 L4 Avg LLM-as-Eval LLM-MCTS(10 iter) 0.17 0.09 0.00 0.00 0.06 0.54 0.43 0.59 0.33 0.47 0.17 0.27 0.00 0.00 0.06 0.88 0.76 0.63 0.69 0.74 0.13 0.04 0.00 0.00 0.04 0.37 0.31 0.38 0.23 0.32 0.10 0.06 0.00 0.00 0.04 0.11 0.09 0.11 0.07 0.10 Ours 0.59 0.49 0.63 0.32 0.51 0.88 0.82 0.71 0.88 0.82 0.44 0.32 0.43 0.22 0.35 0.51 0.42 0.46 0.28 0.42 TABLE I BASELINE COMPARISON FOR DIFFERENT TASK LEVELS IN SIMULATION (AIRSIM [36] ICT,T T = 1 \u2212 CT"}, {"question": " What are the dual roles assumed by the LLM in the adaptation discussed?", "answer": " LLM Eval as an agent and LLM Gen as an evaluator.", "ref_chunk": "the sharpness of B. Reasoning about the uncertainty In the context of planning within intricate outdoor envi- ronments, direct determinations by the LLM-based method solely on localized information can lead to suboptimal be- haviors, such as inconsistency and short-sightedness. These behaviors arise not only from the inherent tendency of LLMs to hallucinate [33], but also the non-delineated nature of the outdoor environment. Such properties become particularly concerning when placing heavy reliance on singular output generations from LLMs [34]. To fortify against these vulnerabilities and enhance deci- sion robustness, we elected to incorporate the future informa- tion using expanding RRT strategy. By projecting multiple Fig. 5. The left image illustrates the expansion process where, at each step, N nodes are expanded (with N = 3 as depicted). The right image shows the agent\u2019s decision-making process with distance cost at each step. forward-looking imaginary branches through iterative queries of LLM Visionary and LLM Evaluator (Figure 3), mitigating the risks associated with single query outputs. Our choice of RRT was informed by its intrinsic properties, in contrast of the sequential sampling process used in MCTS[22][20], RRT provides a parallelizable framework to allow us score and expand the imaginary nodes all at the same time. In our integration of the RRT, the specifics and underlying mechanics are meticulously outlined in Algorithm 1 and vi- sually complemented in Figure 5. At its core, our adaptation hinges on the dual roles assumed by the LLM: as an agent, denoted as LLM Eval, and as an evaluator, represented by LLM Gen. Their specific responsibilities and interplay will be further discussed in the following sections. a) LLM Evaluator: denoted as V (G, S) \u2192 v, assesses the correlation between a scene description S and the pro- vided goal objects or instructions G. Its primary role is to guide the agent by offering a reference score, indicating the likelihood of achieving the goal based on the current scene. The scoring mechanism is structured on a Likert scale ranging from 1 to 5, where a score of 1 indicates a low likelihood of goal achievement, and a score of 5 signifies a high likelihood. This evaluative approach ensures that the agent can make informed decisions based on the contextual relevance of the scene to the goal. represented as L(St) \u2192 St+1, produces the next scene descriptor St+1 based on the current scene description St. Its objective is to enable the agent to anticipate or predict its future waypoint. This is achieved by prompting the agent to envision what it might encounter next. Comprehensive prompt templates and further details are available on our official website. b) LLM Visionary: In our adaptation of the RRT, the algorithm\u2019s mechanics are determined two hyperparameters: N: Dictates the action space dimension, signifying the range of feasible directions the agent can embark upon during its exploration. L: Denotes the length of individual simulations within the branch. This dictates the depth of the tree and how far the algorithm projects into potential future states. Algorithm 1 LLM-RRT 1: procedure LLM-RRT(K, St+1list) Ssample t+2 \u2190 EXPANSION(S\u2217 t+1) 2: vmean \u2190 SIMULATION(Ssample t+2 Q(S\u2217 t+1) \u2190BACK-PROPAGATION(vmean, depth) 4: 5: end procedure ) 3: C. Perceiving The agent captures N images during each exploration steps. These images are processed by a Vision Language Model (VLM). For the purposes of this study, we employed Kosmos-2 [35], a VLM fine-tuned with spatially-grounded image-text data. This model offers the distinct advantage of providing detailed object-level descriptions of scenes. Importantly, it is promptable, so we prompt it to describe not only the objects in the scene, but also the spatial relationship between objects as well as the backgrounds. D. Action on real robot Upon waypoint determination by the scoring function detailed in Section IV-A, our robot employs a straightforward PID controller to traverse the path navigating between the current waypoint and the subsequent one, with the localiza- tion from on board high resolution RTK-GPS and IMU. V. EXPERIMENTS Comprehensive evaluations are conducted across multiple platforms: 1. The AirSim [36] simulation environment: A photo-realistic outdoor simulation setting that consists of different semantic distinguishable areas in Downtown West environment. 2. A real-world robotic platform: Unitree Go1, equiped with a USB camera, high resolution RTK-GPS module, and Inertial measurement unit. A. A Compute Aware Metric for LLM-based Robotic Agents In the assessment of robotic agents in real-world settings, particularly for outdoor tasks like Search and Rescue, it is vital to strike a balance between navigation efficiency, com- putational overhead, and travel duration. The dominant met- rics in the space: Success Rate (SR) and Success Weighted by Path Length (SPL), ignore \u201ctime\u201d. Here, we specifically mean wallclock time, or the length of an experiment or episode. While always relevant in practical scenarios, the use of Large Foundation Models (e.g. over API) introduces a new computational trade-off. Specifically, the interplay between Computational Time (CT) and Travel Time (TT). An increase in computation that results in reduced travel time might lead to overall efficiency gains depending on the amount of computational time required. Colloquially, when is it faster to think before acting, versus acting on a hunch? a) Formulating the CASR Metric: This overall effi- ciency versus the maximum allowed episode length is simply a normalized sum of the two components: Compute (CT) and Travel (TT) time. The value is normalized to the range [0,1], aligning it for integration with the traditional SR metric: L1 L2 SR L3 L4 Avg L1 L2 OSR L3 L4 Avg L1 L2 SPL L3 L4 Avg L1 L2 CASR L3 L4 Avg LLM-as-Eval LLM-MCTS(10 iter) 0.17 0.09 0.00 0.00 0.06 0.54 0.43 0.59 0.33 0.47 0.17 0.27 0.00 0.00 0.06 0.88 0.76 0.63 0.69 0.74 0.13 0.04 0.00 0.00 0.04 0.37 0.31 0.38 0.23 0.32 0.10 0.06 0.00 0.00 0.04 0.11 0.09 0.11 0.07 0.10 Ours 0.59 0.49 0.63 0.32 0.51 0.88 0.82 0.71 0.88 0.82 0.44 0.32 0.43 0.22 0.35 0.51 0.42 0.46 0.28 0.42 TABLE I BASELINE COMPARISON FOR DIFFERENT TASK LEVELS IN SIMULATION (AIRSIM [36] ICT,T T = 1 \u2212 CT"}, {"question": " What is the primary role of LLM Evaluator in guiding the agent?", "answer": " To offer a reference score indicating the likelihood of achieving the goal based on the current scene.", "ref_chunk": "the sharpness of B. Reasoning about the uncertainty In the context of planning within intricate outdoor envi- ronments, direct determinations by the LLM-based method solely on localized information can lead to suboptimal be- haviors, such as inconsistency and short-sightedness. These behaviors arise not only from the inherent tendency of LLMs to hallucinate [33], but also the non-delineated nature of the outdoor environment. Such properties become particularly concerning when placing heavy reliance on singular output generations from LLMs [34]. To fortify against these vulnerabilities and enhance deci- sion robustness, we elected to incorporate the future informa- tion using expanding RRT strategy. By projecting multiple Fig. 5. The left image illustrates the expansion process where, at each step, N nodes are expanded (with N = 3 as depicted). The right image shows the agent\u2019s decision-making process with distance cost at each step. forward-looking imaginary branches through iterative queries of LLM Visionary and LLM Evaluator (Figure 3), mitigating the risks associated with single query outputs. Our choice of RRT was informed by its intrinsic properties, in contrast of the sequential sampling process used in MCTS[22][20], RRT provides a parallelizable framework to allow us score and expand the imaginary nodes all at the same time. In our integration of the RRT, the specifics and underlying mechanics are meticulously outlined in Algorithm 1 and vi- sually complemented in Figure 5. At its core, our adaptation hinges on the dual roles assumed by the LLM: as an agent, denoted as LLM Eval, and as an evaluator, represented by LLM Gen. Their specific responsibilities and interplay will be further discussed in the following sections. a) LLM Evaluator: denoted as V (G, S) \u2192 v, assesses the correlation between a scene description S and the pro- vided goal objects or instructions G. Its primary role is to guide the agent by offering a reference score, indicating the likelihood of achieving the goal based on the current scene. The scoring mechanism is structured on a Likert scale ranging from 1 to 5, where a score of 1 indicates a low likelihood of goal achievement, and a score of 5 signifies a high likelihood. This evaluative approach ensures that the agent can make informed decisions based on the contextual relevance of the scene to the goal. represented as L(St) \u2192 St+1, produces the next scene descriptor St+1 based on the current scene description St. Its objective is to enable the agent to anticipate or predict its future waypoint. This is achieved by prompting the agent to envision what it might encounter next. Comprehensive prompt templates and further details are available on our official website. b) LLM Visionary: In our adaptation of the RRT, the algorithm\u2019s mechanics are determined two hyperparameters: N: Dictates the action space dimension, signifying the range of feasible directions the agent can embark upon during its exploration. L: Denotes the length of individual simulations within the branch. This dictates the depth of the tree and how far the algorithm projects into potential future states. Algorithm 1 LLM-RRT 1: procedure LLM-RRT(K, St+1list) Ssample t+2 \u2190 EXPANSION(S\u2217 t+1) 2: vmean \u2190 SIMULATION(Ssample t+2 Q(S\u2217 t+1) \u2190BACK-PROPAGATION(vmean, depth) 4: 5: end procedure ) 3: C. Perceiving The agent captures N images during each exploration steps. These images are processed by a Vision Language Model (VLM). For the purposes of this study, we employed Kosmos-2 [35], a VLM fine-tuned with spatially-grounded image-text data. This model offers the distinct advantage of providing detailed object-level descriptions of scenes. Importantly, it is promptable, so we prompt it to describe not only the objects in the scene, but also the spatial relationship between objects as well as the backgrounds. D. Action on real robot Upon waypoint determination by the scoring function detailed in Section IV-A, our robot employs a straightforward PID controller to traverse the path navigating between the current waypoint and the subsequent one, with the localiza- tion from on board high resolution RTK-GPS and IMU. V. EXPERIMENTS Comprehensive evaluations are conducted across multiple platforms: 1. The AirSim [36] simulation environment: A photo-realistic outdoor simulation setting that consists of different semantic distinguishable areas in Downtown West environment. 2. A real-world robotic platform: Unitree Go1, equiped with a USB camera, high resolution RTK-GPS module, and Inertial measurement unit. A. A Compute Aware Metric for LLM-based Robotic Agents In the assessment of robotic agents in real-world settings, particularly for outdoor tasks like Search and Rescue, it is vital to strike a balance between navigation efficiency, com- putational overhead, and travel duration. The dominant met- rics in the space: Success Rate (SR) and Success Weighted by Path Length (SPL), ignore \u201ctime\u201d. Here, we specifically mean wallclock time, or the length of an experiment or episode. While always relevant in practical scenarios, the use of Large Foundation Models (e.g. over API) introduces a new computational trade-off. Specifically, the interplay between Computational Time (CT) and Travel Time (TT). An increase in computation that results in reduced travel time might lead to overall efficiency gains depending on the amount of computational time required. Colloquially, when is it faster to think before acting, versus acting on a hunch? a) Formulating the CASR Metric: This overall effi- ciency versus the maximum allowed episode length is simply a normalized sum of the two components: Compute (CT) and Travel (TT) time. The value is normalized to the range [0,1], aligning it for integration with the traditional SR metric: L1 L2 SR L3 L4 Avg L1 L2 OSR L3 L4 Avg L1 L2 SPL L3 L4 Avg L1 L2 CASR L3 L4 Avg LLM-as-Eval LLM-MCTS(10 iter) 0.17 0.09 0.00 0.00 0.06 0.54 0.43 0.59 0.33 0.47 0.17 0.27 0.00 0.00 0.06 0.88 0.76 0.63 0.69 0.74 0.13 0.04 0.00 0.00 0.04 0.37 0.31 0.38 0.23 0.32 0.10 0.06 0.00 0.00 0.04 0.11 0.09 0.11 0.07 0.10 Ours 0.59 0.49 0.63 0.32 0.51 0.88 0.82 0.71 0.88 0.82 0.44 0.32 0.43 0.22 0.35 0.51 0.42 0.46 0.28 0.42 TABLE I BASELINE COMPARISON FOR DIFFERENT TASK LEVELS IN SIMULATION (AIRSIM [36] ICT,T T = 1 \u2212 CT"}, {"question": " What is the objective of LLM Gen in producing the next scene descriptor?", "answer": " To enable the agent to anticipate or predict its future waypoint.", "ref_chunk": "the sharpness of B. Reasoning about the uncertainty In the context of planning within intricate outdoor envi- ronments, direct determinations by the LLM-based method solely on localized information can lead to suboptimal be- haviors, such as inconsistency and short-sightedness. These behaviors arise not only from the inherent tendency of LLMs to hallucinate [33], but also the non-delineated nature of the outdoor environment. Such properties become particularly concerning when placing heavy reliance on singular output generations from LLMs [34]. To fortify against these vulnerabilities and enhance deci- sion robustness, we elected to incorporate the future informa- tion using expanding RRT strategy. By projecting multiple Fig. 5. The left image illustrates the expansion process where, at each step, N nodes are expanded (with N = 3 as depicted). The right image shows the agent\u2019s decision-making process with distance cost at each step. forward-looking imaginary branches through iterative queries of LLM Visionary and LLM Evaluator (Figure 3), mitigating the risks associated with single query outputs. Our choice of RRT was informed by its intrinsic properties, in contrast of the sequential sampling process used in MCTS[22][20], RRT provides a parallelizable framework to allow us score and expand the imaginary nodes all at the same time. In our integration of the RRT, the specifics and underlying mechanics are meticulously outlined in Algorithm 1 and vi- sually complemented in Figure 5. At its core, our adaptation hinges on the dual roles assumed by the LLM: as an agent, denoted as LLM Eval, and as an evaluator, represented by LLM Gen. Their specific responsibilities and interplay will be further discussed in the following sections. a) LLM Evaluator: denoted as V (G, S) \u2192 v, assesses the correlation between a scene description S and the pro- vided goal objects or instructions G. Its primary role is to guide the agent by offering a reference score, indicating the likelihood of achieving the goal based on the current scene. The scoring mechanism is structured on a Likert scale ranging from 1 to 5, where a score of 1 indicates a low likelihood of goal achievement, and a score of 5 signifies a high likelihood. This evaluative approach ensures that the agent can make informed decisions based on the contextual relevance of the scene to the goal. represented as L(St) \u2192 St+1, produces the next scene descriptor St+1 based on the current scene description St. Its objective is to enable the agent to anticipate or predict its future waypoint. This is achieved by prompting the agent to envision what it might encounter next. Comprehensive prompt templates and further details are available on our official website. b) LLM Visionary: In our adaptation of the RRT, the algorithm\u2019s mechanics are determined two hyperparameters: N: Dictates the action space dimension, signifying the range of feasible directions the agent can embark upon during its exploration. L: Denotes the length of individual simulations within the branch. This dictates the depth of the tree and how far the algorithm projects into potential future states. Algorithm 1 LLM-RRT 1: procedure LLM-RRT(K, St+1list) Ssample t+2 \u2190 EXPANSION(S\u2217 t+1) 2: vmean \u2190 SIMULATION(Ssample t+2 Q(S\u2217 t+1) \u2190BACK-PROPAGATION(vmean, depth) 4: 5: end procedure ) 3: C. Perceiving The agent captures N images during each exploration steps. These images are processed by a Vision Language Model (VLM). For the purposes of this study, we employed Kosmos-2 [35], a VLM fine-tuned with spatially-grounded image-text data. This model offers the distinct advantage of providing detailed object-level descriptions of scenes. Importantly, it is promptable, so we prompt it to describe not only the objects in the scene, but also the spatial relationship between objects as well as the backgrounds. D. Action on real robot Upon waypoint determination by the scoring function detailed in Section IV-A, our robot employs a straightforward PID controller to traverse the path navigating between the current waypoint and the subsequent one, with the localiza- tion from on board high resolution RTK-GPS and IMU. V. EXPERIMENTS Comprehensive evaluations are conducted across multiple platforms: 1. The AirSim [36] simulation environment: A photo-realistic outdoor simulation setting that consists of different semantic distinguishable areas in Downtown West environment. 2. A real-world robotic platform: Unitree Go1, equiped with a USB camera, high resolution RTK-GPS module, and Inertial measurement unit. A. A Compute Aware Metric for LLM-based Robotic Agents In the assessment of robotic agents in real-world settings, particularly for outdoor tasks like Search and Rescue, it is vital to strike a balance between navigation efficiency, com- putational overhead, and travel duration. The dominant met- rics in the space: Success Rate (SR) and Success Weighted by Path Length (SPL), ignore \u201ctime\u201d. Here, we specifically mean wallclock time, or the length of an experiment or episode. While always relevant in practical scenarios, the use of Large Foundation Models (e.g. over API) introduces a new computational trade-off. Specifically, the interplay between Computational Time (CT) and Travel Time (TT). An increase in computation that results in reduced travel time might lead to overall efficiency gains depending on the amount of computational time required. Colloquially, when is it faster to think before acting, versus acting on a hunch? a) Formulating the CASR Metric: This overall effi- ciency versus the maximum allowed episode length is simply a normalized sum of the two components: Compute (CT) and Travel (TT) time. The value is normalized to the range [0,1], aligning it for integration with the traditional SR metric: L1 L2 SR L3 L4 Avg L1 L2 OSR L3 L4 Avg L1 L2 SPL L3 L4 Avg L1 L2 CASR L3 L4 Avg LLM-as-Eval LLM-MCTS(10 iter) 0.17 0.09 0.00 0.00 0.06 0.54 0.43 0.59 0.33 0.47 0.17 0.27 0.00 0.00 0.06 0.88 0.76 0.63 0.69 0.74 0.13 0.04 0.00 0.00 0.04 0.37 0.31 0.38 0.23 0.32 0.10 0.06 0.00 0.00 0.04 0.11 0.09 0.11 0.07 0.10 Ours 0.59 0.49 0.63 0.32 0.51 0.88 0.82 0.71 0.88 0.82 0.44 0.32 0.43 0.22 0.35 0.51 0.42 0.46 0.28 0.42 TABLE I BASELINE COMPARISON FOR DIFFERENT TASK LEVELS IN SIMULATION (AIRSIM [36] ICT,T T = 1 \u2212 CT"}, {"question": " What is the role of Vision Language Model (VLM) in capturing images during exploration steps?", "answer": " To process the captured images and provide detailed object-level descriptions of scenes.", "ref_chunk": "the sharpness of B. Reasoning about the uncertainty In the context of planning within intricate outdoor envi- ronments, direct determinations by the LLM-based method solely on localized information can lead to suboptimal be- haviors, such as inconsistency and short-sightedness. These behaviors arise not only from the inherent tendency of LLMs to hallucinate [33], but also the non-delineated nature of the outdoor environment. Such properties become particularly concerning when placing heavy reliance on singular output generations from LLMs [34]. To fortify against these vulnerabilities and enhance deci- sion robustness, we elected to incorporate the future informa- tion using expanding RRT strategy. By projecting multiple Fig. 5. The left image illustrates the expansion process where, at each step, N nodes are expanded (with N = 3 as depicted). The right image shows the agent\u2019s decision-making process with distance cost at each step. forward-looking imaginary branches through iterative queries of LLM Visionary and LLM Evaluator (Figure 3), mitigating the risks associated with single query outputs. Our choice of RRT was informed by its intrinsic properties, in contrast of the sequential sampling process used in MCTS[22][20], RRT provides a parallelizable framework to allow us score and expand the imaginary nodes all at the same time. In our integration of the RRT, the specifics and underlying mechanics are meticulously outlined in Algorithm 1 and vi- sually complemented in Figure 5. At its core, our adaptation hinges on the dual roles assumed by the LLM: as an agent, denoted as LLM Eval, and as an evaluator, represented by LLM Gen. Their specific responsibilities and interplay will be further discussed in the following sections. a) LLM Evaluator: denoted as V (G, S) \u2192 v, assesses the correlation between a scene description S and the pro- vided goal objects or instructions G. Its primary role is to guide the agent by offering a reference score, indicating the likelihood of achieving the goal based on the current scene. The scoring mechanism is structured on a Likert scale ranging from 1 to 5, where a score of 1 indicates a low likelihood of goal achievement, and a score of 5 signifies a high likelihood. This evaluative approach ensures that the agent can make informed decisions based on the contextual relevance of the scene to the goal. represented as L(St) \u2192 St+1, produces the next scene descriptor St+1 based on the current scene description St. Its objective is to enable the agent to anticipate or predict its future waypoint. This is achieved by prompting the agent to envision what it might encounter next. Comprehensive prompt templates and further details are available on our official website. b) LLM Visionary: In our adaptation of the RRT, the algorithm\u2019s mechanics are determined two hyperparameters: N: Dictates the action space dimension, signifying the range of feasible directions the agent can embark upon during its exploration. L: Denotes the length of individual simulations within the branch. This dictates the depth of the tree and how far the algorithm projects into potential future states. Algorithm 1 LLM-RRT 1: procedure LLM-RRT(K, St+1list) Ssample t+2 \u2190 EXPANSION(S\u2217 t+1) 2: vmean \u2190 SIMULATION(Ssample t+2 Q(S\u2217 t+1) \u2190BACK-PROPAGATION(vmean, depth) 4: 5: end procedure ) 3: C. Perceiving The agent captures N images during each exploration steps. These images are processed by a Vision Language Model (VLM). For the purposes of this study, we employed Kosmos-2 [35], a VLM fine-tuned with spatially-grounded image-text data. This model offers the distinct advantage of providing detailed object-level descriptions of scenes. Importantly, it is promptable, so we prompt it to describe not only the objects in the scene, but also the spatial relationship between objects as well as the backgrounds. D. Action on real robot Upon waypoint determination by the scoring function detailed in Section IV-A, our robot employs a straightforward PID controller to traverse the path navigating between the current waypoint and the subsequent one, with the localiza- tion from on board high resolution RTK-GPS and IMU. V. EXPERIMENTS Comprehensive evaluations are conducted across multiple platforms: 1. The AirSim [36] simulation environment: A photo-realistic outdoor simulation setting that consists of different semantic distinguishable areas in Downtown West environment. 2. A real-world robotic platform: Unitree Go1, equiped with a USB camera, high resolution RTK-GPS module, and Inertial measurement unit. A. A Compute Aware Metric for LLM-based Robotic Agents In the assessment of robotic agents in real-world settings, particularly for outdoor tasks like Search and Rescue, it is vital to strike a balance between navigation efficiency, com- putational overhead, and travel duration. The dominant met- rics in the space: Success Rate (SR) and Success Weighted by Path Length (SPL), ignore \u201ctime\u201d. Here, we specifically mean wallclock time, or the length of an experiment or episode. While always relevant in practical scenarios, the use of Large Foundation Models (e.g. over API) introduces a new computational trade-off. Specifically, the interplay between Computational Time (CT) and Travel Time (TT). An increase in computation that results in reduced travel time might lead to overall efficiency gains depending on the amount of computational time required. Colloquially, when is it faster to think before acting, versus acting on a hunch? a) Formulating the CASR Metric: This overall effi- ciency versus the maximum allowed episode length is simply a normalized sum of the two components: Compute (CT) and Travel (TT) time. The value is normalized to the range [0,1], aligning it for integration with the traditional SR metric: L1 L2 SR L3 L4 Avg L1 L2 OSR L3 L4 Avg L1 L2 SPL L3 L4 Avg L1 L2 CASR L3 L4 Avg LLM-as-Eval LLM-MCTS(10 iter) 0.17 0.09 0.00 0.00 0.06 0.54 0.43 0.59 0.33 0.47 0.17 0.27 0.00 0.00 0.06 0.88 0.76 0.63 0.69 0.74 0.13 0.04 0.00 0.00 0.04 0.37 0.31 0.38 0.23 0.32 0.10 0.06 0.00 0.00 0.04 0.11 0.09 0.11 0.07 0.10 Ours 0.59 0.49 0.63 0.32 0.51 0.88 0.82 0.71 0.88 0.82 0.44 0.32 0.43 0.22 0.35 0.51 0.42 0.46 0.28 0.42 TABLE I BASELINE COMPARISON FOR DIFFERENT TASK LEVELS IN SIMULATION (AIRSIM [36] ICT,T T = 1 \u2212 CT"}, {"question": " How does the robot navigate between waypoints?", "answer": " It employs a PID controller with localization from on-board high resolution RTK-GPS and IMU.", "ref_chunk": "the sharpness of B. Reasoning about the uncertainty In the context of planning within intricate outdoor envi- ronments, direct determinations by the LLM-based method solely on localized information can lead to suboptimal be- haviors, such as inconsistency and short-sightedness. These behaviors arise not only from the inherent tendency of LLMs to hallucinate [33], but also the non-delineated nature of the outdoor environment. Such properties become particularly concerning when placing heavy reliance on singular output generations from LLMs [34]. To fortify against these vulnerabilities and enhance deci- sion robustness, we elected to incorporate the future informa- tion using expanding RRT strategy. By projecting multiple Fig. 5. The left image illustrates the expansion process where, at each step, N nodes are expanded (with N = 3 as depicted). The right image shows the agent\u2019s decision-making process with distance cost at each step. forward-looking imaginary branches through iterative queries of LLM Visionary and LLM Evaluator (Figure 3), mitigating the risks associated with single query outputs. Our choice of RRT was informed by its intrinsic properties, in contrast of the sequential sampling process used in MCTS[22][20], RRT provides a parallelizable framework to allow us score and expand the imaginary nodes all at the same time. In our integration of the RRT, the specifics and underlying mechanics are meticulously outlined in Algorithm 1 and vi- sually complemented in Figure 5. At its core, our adaptation hinges on the dual roles assumed by the LLM: as an agent, denoted as LLM Eval, and as an evaluator, represented by LLM Gen. Their specific responsibilities and interplay will be further discussed in the following sections. a) LLM Evaluator: denoted as V (G, S) \u2192 v, assesses the correlation between a scene description S and the pro- vided goal objects or instructions G. Its primary role is to guide the agent by offering a reference score, indicating the likelihood of achieving the goal based on the current scene. The scoring mechanism is structured on a Likert scale ranging from 1 to 5, where a score of 1 indicates a low likelihood of goal achievement, and a score of 5 signifies a high likelihood. This evaluative approach ensures that the agent can make informed decisions based on the contextual relevance of the scene to the goal. represented as L(St) \u2192 St+1, produces the next scene descriptor St+1 based on the current scene description St. Its objective is to enable the agent to anticipate or predict its future waypoint. This is achieved by prompting the agent to envision what it might encounter next. Comprehensive prompt templates and further details are available on our official website. b) LLM Visionary: In our adaptation of the RRT, the algorithm\u2019s mechanics are determined two hyperparameters: N: Dictates the action space dimension, signifying the range of feasible directions the agent can embark upon during its exploration. L: Denotes the length of individual simulations within the branch. This dictates the depth of the tree and how far the algorithm projects into potential future states. Algorithm 1 LLM-RRT 1: procedure LLM-RRT(K, St+1list) Ssample t+2 \u2190 EXPANSION(S\u2217 t+1) 2: vmean \u2190 SIMULATION(Ssample t+2 Q(S\u2217 t+1) \u2190BACK-PROPAGATION(vmean, depth) 4: 5: end procedure ) 3: C. Perceiving The agent captures N images during each exploration steps. These images are processed by a Vision Language Model (VLM). For the purposes of this study, we employed Kosmos-2 [35], a VLM fine-tuned with spatially-grounded image-text data. This model offers the distinct advantage of providing detailed object-level descriptions of scenes. Importantly, it is promptable, so we prompt it to describe not only the objects in the scene, but also the spatial relationship between objects as well as the backgrounds. D. Action on real robot Upon waypoint determination by the scoring function detailed in Section IV-A, our robot employs a straightforward PID controller to traverse the path navigating between the current waypoint and the subsequent one, with the localiza- tion from on board high resolution RTK-GPS and IMU. V. EXPERIMENTS Comprehensive evaluations are conducted across multiple platforms: 1. The AirSim [36] simulation environment: A photo-realistic outdoor simulation setting that consists of different semantic distinguishable areas in Downtown West environment. 2. A real-world robotic platform: Unitree Go1, equiped with a USB camera, high resolution RTK-GPS module, and Inertial measurement unit. A. A Compute Aware Metric for LLM-based Robotic Agents In the assessment of robotic agents in real-world settings, particularly for outdoor tasks like Search and Rescue, it is vital to strike a balance between navigation efficiency, com- putational overhead, and travel duration. The dominant met- rics in the space: Success Rate (SR) and Success Weighted by Path Length (SPL), ignore \u201ctime\u201d. Here, we specifically mean wallclock time, or the length of an experiment or episode. While always relevant in practical scenarios, the use of Large Foundation Models (e.g. over API) introduces a new computational trade-off. Specifically, the interplay between Computational Time (CT) and Travel Time (TT). An increase in computation that results in reduced travel time might lead to overall efficiency gains depending on the amount of computational time required. Colloquially, when is it faster to think before acting, versus acting on a hunch? a) Formulating the CASR Metric: This overall effi- ciency versus the maximum allowed episode length is simply a normalized sum of the two components: Compute (CT) and Travel (TT) time. The value is normalized to the range [0,1], aligning it for integration with the traditional SR metric: L1 L2 SR L3 L4 Avg L1 L2 OSR L3 L4 Avg L1 L2 SPL L3 L4 Avg L1 L2 CASR L3 L4 Avg LLM-as-Eval LLM-MCTS(10 iter) 0.17 0.09 0.00 0.00 0.06 0.54 0.43 0.59 0.33 0.47 0.17 0.27 0.00 0.00 0.06 0.88 0.76 0.63 0.69 0.74 0.13 0.04 0.00 0.00 0.04 0.37 0.31 0.38 0.23 0.32 0.10 0.06 0.00 0.00 0.04 0.11 0.09 0.11 0.07 0.10 Ours 0.59 0.49 0.63 0.32 0.51 0.88 0.82 0.71 0.88 0.82 0.44 0.32 0.43 0.22 0.35 0.51 0.42 0.46 0.28 0.42 TABLE I BASELINE COMPARISON FOR DIFFERENT TASK LEVELS IN SIMULATION (AIRSIM [36] ICT,T T = 1 \u2212 CT"}, {"question": " What is the CASR Metric used for in the assessment of robotic agents?", "answer": " To strike a balance between navigation efficiency, computational overhead, and travel duration.", "ref_chunk": "the sharpness of B. Reasoning about the uncertainty In the context of planning within intricate outdoor envi- ronments, direct determinations by the LLM-based method solely on localized information can lead to suboptimal be- haviors, such as inconsistency and short-sightedness. These behaviors arise not only from the inherent tendency of LLMs to hallucinate [33], but also the non-delineated nature of the outdoor environment. Such properties become particularly concerning when placing heavy reliance on singular output generations from LLMs [34]. To fortify against these vulnerabilities and enhance deci- sion robustness, we elected to incorporate the future informa- tion using expanding RRT strategy. By projecting multiple Fig. 5. The left image illustrates the expansion process where, at each step, N nodes are expanded (with N = 3 as depicted). The right image shows the agent\u2019s decision-making process with distance cost at each step. forward-looking imaginary branches through iterative queries of LLM Visionary and LLM Evaluator (Figure 3), mitigating the risks associated with single query outputs. Our choice of RRT was informed by its intrinsic properties, in contrast of the sequential sampling process used in MCTS[22][20], RRT provides a parallelizable framework to allow us score and expand the imaginary nodes all at the same time. In our integration of the RRT, the specifics and underlying mechanics are meticulously outlined in Algorithm 1 and vi- sually complemented in Figure 5. At its core, our adaptation hinges on the dual roles assumed by the LLM: as an agent, denoted as LLM Eval, and as an evaluator, represented by LLM Gen. Their specific responsibilities and interplay will be further discussed in the following sections. a) LLM Evaluator: denoted as V (G, S) \u2192 v, assesses the correlation between a scene description S and the pro- vided goal objects or instructions G. Its primary role is to guide the agent by offering a reference score, indicating the likelihood of achieving the goal based on the current scene. The scoring mechanism is structured on a Likert scale ranging from 1 to 5, where a score of 1 indicates a low likelihood of goal achievement, and a score of 5 signifies a high likelihood. This evaluative approach ensures that the agent can make informed decisions based on the contextual relevance of the scene to the goal. represented as L(St) \u2192 St+1, produces the next scene descriptor St+1 based on the current scene description St. Its objective is to enable the agent to anticipate or predict its future waypoint. This is achieved by prompting the agent to envision what it might encounter next. Comprehensive prompt templates and further details are available on our official website. b) LLM Visionary: In our adaptation of the RRT, the algorithm\u2019s mechanics are determined two hyperparameters: N: Dictates the action space dimension, signifying the range of feasible directions the agent can embark upon during its exploration. L: Denotes the length of individual simulations within the branch. This dictates the depth of the tree and how far the algorithm projects into potential future states. Algorithm 1 LLM-RRT 1: procedure LLM-RRT(K, St+1list) Ssample t+2 \u2190 EXPANSION(S\u2217 t+1) 2: vmean \u2190 SIMULATION(Ssample t+2 Q(S\u2217 t+1) \u2190BACK-PROPAGATION(vmean, depth) 4: 5: end procedure ) 3: C. Perceiving The agent captures N images during each exploration steps. These images are processed by a Vision Language Model (VLM). For the purposes of this study, we employed Kosmos-2 [35], a VLM fine-tuned with spatially-grounded image-text data. This model offers the distinct advantage of providing detailed object-level descriptions of scenes. Importantly, it is promptable, so we prompt it to describe not only the objects in the scene, but also the spatial relationship between objects as well as the backgrounds. D. Action on real robot Upon waypoint determination by the scoring function detailed in Section IV-A, our robot employs a straightforward PID controller to traverse the path navigating between the current waypoint and the subsequent one, with the localiza- tion from on board high resolution RTK-GPS and IMU. V. EXPERIMENTS Comprehensive evaluations are conducted across multiple platforms: 1. The AirSim [36] simulation environment: A photo-realistic outdoor simulation setting that consists of different semantic distinguishable areas in Downtown West environment. 2. A real-world robotic platform: Unitree Go1, equiped with a USB camera, high resolution RTK-GPS module, and Inertial measurement unit. A. A Compute Aware Metric for LLM-based Robotic Agents In the assessment of robotic agents in real-world settings, particularly for outdoor tasks like Search and Rescue, it is vital to strike a balance between navigation efficiency, com- putational overhead, and travel duration. The dominant met- rics in the space: Success Rate (SR) and Success Weighted by Path Length (SPL), ignore \u201ctime\u201d. Here, we specifically mean wallclock time, or the length of an experiment or episode. While always relevant in practical scenarios, the use of Large Foundation Models (e.g. over API) introduces a new computational trade-off. Specifically, the interplay between Computational Time (CT) and Travel Time (TT). An increase in computation that results in reduced travel time might lead to overall efficiency gains depending on the amount of computational time required. Colloquially, when is it faster to think before acting, versus acting on a hunch? a) Formulating the CASR Metric: This overall effi- ciency versus the maximum allowed episode length is simply a normalized sum of the two components: Compute (CT) and Travel (TT) time. The value is normalized to the range [0,1], aligning it for integration with the traditional SR metric: L1 L2 SR L3 L4 Avg L1 L2 OSR L3 L4 Avg L1 L2 SPL L3 L4 Avg L1 L2 CASR L3 L4 Avg LLM-as-Eval LLM-MCTS(10 iter) 0.17 0.09 0.00 0.00 0.06 0.54 0.43 0.59 0.33 0.47 0.17 0.27 0.00 0.00 0.06 0.88 0.76 0.63 0.69 0.74 0.13 0.04 0.00 0.00 0.04 0.37 0.31 0.38 0.23 0.32 0.10 0.06 0.00 0.00 0.04 0.11 0.09 0.11 0.07 0.10 Ours 0.59 0.49 0.63 0.32 0.51 0.88 0.82 0.71 0.88 0.82 0.44 0.32 0.43 0.22 0.35 0.51 0.42 0.46 0.28 0.42 TABLE I BASELINE COMPARISON FOR DIFFERENT TASK LEVELS IN SIMULATION (AIRSIM [36] ICT,T T = 1 \u2212 CT"}], "doc_text": "the sharpness of B. Reasoning about the uncertainty In the context of planning within intricate outdoor envi- ronments, direct determinations by the LLM-based method solely on localized information can lead to suboptimal be- haviors, such as inconsistency and short-sightedness. These behaviors arise not only from the inherent tendency of LLMs to hallucinate [33], but also the non-delineated nature of the outdoor environment. Such properties become particularly concerning when placing heavy reliance on singular output generations from LLMs [34]. To fortify against these vulnerabilities and enhance deci- sion robustness, we elected to incorporate the future informa- tion using expanding RRT strategy. By projecting multiple Fig. 5. The left image illustrates the expansion process where, at each step, N nodes are expanded (with N = 3 as depicted). The right image shows the agent\u2019s decision-making process with distance cost at each step. forward-looking imaginary branches through iterative queries of LLM Visionary and LLM Evaluator (Figure 3), mitigating the risks associated with single query outputs. Our choice of RRT was informed by its intrinsic properties, in contrast of the sequential sampling process used in MCTS[22][20], RRT provides a parallelizable framework to allow us score and expand the imaginary nodes all at the same time. In our integration of the RRT, the specifics and underlying mechanics are meticulously outlined in Algorithm 1 and vi- sually complemented in Figure 5. At its core, our adaptation hinges on the dual roles assumed by the LLM: as an agent, denoted as LLM Eval, and as an evaluator, represented by LLM Gen. Their specific responsibilities and interplay will be further discussed in the following sections. a) LLM Evaluator: denoted as V (G, S) \u2192 v, assesses the correlation between a scene description S and the pro- vided goal objects or instructions G. Its primary role is to guide the agent by offering a reference score, indicating the likelihood of achieving the goal based on the current scene. The scoring mechanism is structured on a Likert scale ranging from 1 to 5, where a score of 1 indicates a low likelihood of goal achievement, and a score of 5 signifies a high likelihood. This evaluative approach ensures that the agent can make informed decisions based on the contextual relevance of the scene to the goal. represented as L(St) \u2192 St+1, produces the next scene descriptor St+1 based on the current scene description St. Its objective is to enable the agent to anticipate or predict its future waypoint. This is achieved by prompting the agent to envision what it might encounter next. Comprehensive prompt templates and further details are available on our official website. b) LLM Visionary: In our adaptation of the RRT, the algorithm\u2019s mechanics are determined two hyperparameters: N: Dictates the action space dimension, signifying the range of feasible directions the agent can embark upon during its exploration. L: Denotes the length of individual simulations within the branch. This dictates the depth of the tree and how far the algorithm projects into potential future states. Algorithm 1 LLM-RRT 1: procedure LLM-RRT(K, St+1list) Ssample t+2 \u2190 EXPANSION(S\u2217 t+1) 2: vmean \u2190 SIMULATION(Ssample t+2 Q(S\u2217 t+1) \u2190BACK-PROPAGATION(vmean, depth) 4: 5: end procedure ) 3: C. Perceiving The agent captures N images during each exploration steps. These images are processed by a Vision Language Model (VLM). For the purposes of this study, we employed Kosmos-2 [35], a VLM fine-tuned with spatially-grounded image-text data. This model offers the distinct advantage of providing detailed object-level descriptions of scenes. Importantly, it is promptable, so we prompt it to describe not only the objects in the scene, but also the spatial relationship between objects as well as the backgrounds. D. Action on real robot Upon waypoint determination by the scoring function detailed in Section IV-A, our robot employs a straightforward PID controller to traverse the path navigating between the current waypoint and the subsequent one, with the localiza- tion from on board high resolution RTK-GPS and IMU. V. EXPERIMENTS Comprehensive evaluations are conducted across multiple platforms: 1. The AirSim [36] simulation environment: A photo-realistic outdoor simulation setting that consists of different semantic distinguishable areas in Downtown West environment. 2. A real-world robotic platform: Unitree Go1, equiped with a USB camera, high resolution RTK-GPS module, and Inertial measurement unit. A. A Compute Aware Metric for LLM-based Robotic Agents In the assessment of robotic agents in real-world settings, particularly for outdoor tasks like Search and Rescue, it is vital to strike a balance between navigation efficiency, com- putational overhead, and travel duration. The dominant met- rics in the space: Success Rate (SR) and Success Weighted by Path Length (SPL), ignore \u201ctime\u201d. Here, we specifically mean wallclock time, or the length of an experiment or episode. While always relevant in practical scenarios, the use of Large Foundation Models (e.g. over API) introduces a new computational trade-off. Specifically, the interplay between Computational Time (CT) and Travel Time (TT). An increase in computation that results in reduced travel time might lead to overall efficiency gains depending on the amount of computational time required. Colloquially, when is it faster to think before acting, versus acting on a hunch? a) Formulating the CASR Metric: This overall effi- ciency versus the maximum allowed episode length is simply a normalized sum of the two components: Compute (CT) and Travel (TT) time. The value is normalized to the range [0,1], aligning it for integration with the traditional SR metric: L1 L2 SR L3 L4 Avg L1 L2 OSR L3 L4 Avg L1 L2 SPL L3 L4 Avg L1 L2 CASR L3 L4 Avg LLM-as-Eval LLM-MCTS(10 iter) 0.17 0.09 0.00 0.00 0.06 0.54 0.43 0.59 0.33 0.47 0.17 0.27 0.00 0.00 0.06 0.88 0.76 0.63 0.69 0.74 0.13 0.04 0.00 0.00 0.04 0.37 0.31 0.38 0.23 0.32 0.10 0.06 0.00 0.00 0.04 0.11 0.09 0.11 0.07 0.10 Ours 0.59 0.49 0.63 0.32 0.51 0.88 0.82 0.71 0.88 0.82 0.44 0.32 0.43 0.22 0.35 0.51 0.42 0.46 0.28 0.42 TABLE I BASELINE COMPARISON FOR DIFFERENT TASK LEVELS IN SIMULATION (AIRSIM [36] ICT,T T = 1 \u2212 CT"}