{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_GlobalBench:_A_Benchmark_for_Global_Progress_in_Natural_Language_Processing_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of GlobalBench?,answer: To measure and encourage improvement in the quality of language technology systems.", "ref_chunk": "(a) Demographic global average (b) Linguistic global average Figure 5: A snaphsot capturing the increase in global averages for NER in the recent past, with the addition of new datasets. system outputs, please refer to Appendix \u00a7A.2. 5.3 Measuring improvement with GlobalBench Another major focus of GlobalBench is to mea- sure and encourage improvement in the quality of language technology systems. 5.3.1 How have we improved? GlobalBench keeps track of when each submission was made, making it possible to examine global progress in NLP over time. To give one example of this, in Figure 5 we show how dataset submis- sions have helped increase the global averages for NER in the recent past. Specifically, the earliest time point only covered English datasets, leaving both averages relatively low. In the second dat- apoint, systems for African languages from the MasakhaNER dataset were added (Adelani et al., 2021), significantly raising the linguistic average. In the third datapoint, systems from the XTREME benchmark (Hu et al., 2020) were added, cover- ing a more populous set of languages, significantly raising the demographic average. 5.3.2 Where can we improve? The variety of analysis in the previous section is, in a sense, backward-looking: it looks back at the progress that has already been made. In order to instead get a better idea of how we may improve our systems in the future, we use the methodology in Section 2.3 to identify most under-served lan- guages in GlobalBench, which are the languages with relatively high population and relatively low scores for each of the tasks. To display in Global- Bench, we choose a parameter of \u03c4 = 0.4, which allows us to moderate between considerations of serving all speakers in the world and serving all languages in the world. Task Lang 1 Lang 2 Lang 3 Named Entity Recognition Extractive QA Text Pair Classification Machine Translation Text Classification KG Prediction cmn por ben cmn cmn cmn pnb jpn por spa spa spa wuu urd ind ara ara ara Table 3: Most under-served languages for each task (by ISO 639-3 language code). We show the three most under-served languages for each task in Table 35. From these statistics we observe some interesting trends. First, for tasks where the most widely spoken non-English lan- guages in the world, Mandarin Chinese (cmn), Spanish (spa), and Arabic (ara) are not covered, these are selected the most under-served languages. 5Note that for the purpose of these statistics, we use the source language for the machine translation task. However, for the tasks with better language cov- erage such as NER, extractive QA, and text pair classification, the most under-served languages are ones with relatively high population that are nonetheless not covered well by existing multilin- gual datasets that have been included in Global- Bench. This indicates a need for more creation or incorporation of datasets for major languages such as Punjabi, Wu Chinese, and Portuguese, which have been overlooked by existing composite bench- marks. 6 Related Work From datasets to benchmarks Given the ubiq- uitous use of NLP technology in applications, it is imperative to track and maintain progress across a variety of NLP tasks. Evaluating and compar- ing systems on a single task can also be problem- atic; past work has identified issues with standard datasets (Artetxe et al., 2019; Gururangan et al., 2018). As the field has progressed, several bench- marks have been released to spur the development of generalizable NLU systems. GLUE (Wang et al., 2018) was one such benchmark with a collection of 9 diverse NLU tasks (sentiment analysis (Socher et al., 2013), natural language inference (Williams et al., 2017), etc.), contrary to prior benchmarks that focused on datasets for a single category of tasks (Conneau and Kiela, 2018). SuperGLUE (Wang et al., 2019) updates GLUE by introducing a new set of harder tasks like commonsense rea- soning (Zhang et al., 2018) and question answer- ing (Khashabi et al., 2018). The recently released BIG-bench (Srivastava et al., 2022) consists of a diverse set of 204 tasks, aimed at specifically evalu- ating the capabilities and limitations of large LMs. Finally, Dynabench (Kiela et al., 2021) is a human- and-model-in-the-loop platform, for dynamic data collection and benchmarking, which currently sup- ports ten tasks. Notably, none of these benchmarks provide utility/equity measures, or a reward struc- ture that incentivizes progress towards the most under-served languages. Moving beyond English While the aforemen- tioned benchmarks have driven progress in NLP for English, there have been several recent efforts made towards other languages as well. Multilin- gual composite benchmarks such as XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021), and XGLUE (Liang et al., 2020) are a collection of datasets in a variety of tasks and languages. XTREME includes 9 tasks across 40 languages, XTREME-R includes 10 tasks across 50 languages with 198 datasets, and X-GLUE improves GLUE (Wang et al., 2018) by including 11 cross-lingual tasks. However, all of these are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Beyond these, there have been directed efforts towards dataset curation, especially for the low [text] resource. MasakhaNER (Adelani et al., 2021) supports a large dataset for NER task of 10 African languages. IndoNLU (Wilie et al., 2020) is the first vast resource Indonesian bench- mark, and the KLUE benchmark (Park et al., 2021) focuses on 8 diverse tasks in Korean. In sum, while all of the above efforts have been impactful, none have had the goal to track global progress made by us as a research community, and design a reward system that incentivizes both data and model work, especially for the under-served languages. With GlobalBench, we propose a first step to bridge this gap and move towards inclusive, multi-faceted measurement of progress. 7 Conclusion In this paper, we introduce GlobalBench, an ever- expanding collection of datasets and models span- ning across all tasks and languages within NLP. Our aim is for the community to move towards a common goal of building NLP technology that equitably serves"}, {"question": " How does GlobalBench track global progress in NLP over time?,answer: It keeps track of when each submission was made, allowing examination of global progress.", "ref_chunk": "(a) Demographic global average (b) Linguistic global average Figure 5: A snaphsot capturing the increase in global averages for NER in the recent past, with the addition of new datasets. system outputs, please refer to Appendix \u00a7A.2. 5.3 Measuring improvement with GlobalBench Another major focus of GlobalBench is to mea- sure and encourage improvement in the quality of language technology systems. 5.3.1 How have we improved? GlobalBench keeps track of when each submission was made, making it possible to examine global progress in NLP over time. To give one example of this, in Figure 5 we show how dataset submis- sions have helped increase the global averages for NER in the recent past. Specifically, the earliest time point only covered English datasets, leaving both averages relatively low. In the second dat- apoint, systems for African languages from the MasakhaNER dataset were added (Adelani et al., 2021), significantly raising the linguistic average. In the third datapoint, systems from the XTREME benchmark (Hu et al., 2020) were added, cover- ing a more populous set of languages, significantly raising the demographic average. 5.3.2 Where can we improve? The variety of analysis in the previous section is, in a sense, backward-looking: it looks back at the progress that has already been made. In order to instead get a better idea of how we may improve our systems in the future, we use the methodology in Section 2.3 to identify most under-served lan- guages in GlobalBench, which are the languages with relatively high population and relatively low scores for each of the tasks. To display in Global- Bench, we choose a parameter of \u03c4 = 0.4, which allows us to moderate between considerations of serving all speakers in the world and serving all languages in the world. Task Lang 1 Lang 2 Lang 3 Named Entity Recognition Extractive QA Text Pair Classification Machine Translation Text Classification KG Prediction cmn por ben cmn cmn cmn pnb jpn por spa spa spa wuu urd ind ara ara ara Table 3: Most under-served languages for each task (by ISO 639-3 language code). We show the three most under-served languages for each task in Table 35. From these statistics we observe some interesting trends. First, for tasks where the most widely spoken non-English lan- guages in the world, Mandarin Chinese (cmn), Spanish (spa), and Arabic (ara) are not covered, these are selected the most under-served languages. 5Note that for the purpose of these statistics, we use the source language for the machine translation task. However, for the tasks with better language cov- erage such as NER, extractive QA, and text pair classification, the most under-served languages are ones with relatively high population that are nonetheless not covered well by existing multilin- gual datasets that have been included in Global- Bench. This indicates a need for more creation or incorporation of datasets for major languages such as Punjabi, Wu Chinese, and Portuguese, which have been overlooked by existing composite bench- marks. 6 Related Work From datasets to benchmarks Given the ubiq- uitous use of NLP technology in applications, it is imperative to track and maintain progress across a variety of NLP tasks. Evaluating and compar- ing systems on a single task can also be problem- atic; past work has identified issues with standard datasets (Artetxe et al., 2019; Gururangan et al., 2018). As the field has progressed, several bench- marks have been released to spur the development of generalizable NLU systems. GLUE (Wang et al., 2018) was one such benchmark with a collection of 9 diverse NLU tasks (sentiment analysis (Socher et al., 2013), natural language inference (Williams et al., 2017), etc.), contrary to prior benchmarks that focused on datasets for a single category of tasks (Conneau and Kiela, 2018). SuperGLUE (Wang et al., 2019) updates GLUE by introducing a new set of harder tasks like commonsense rea- soning (Zhang et al., 2018) and question answer- ing (Khashabi et al., 2018). The recently released BIG-bench (Srivastava et al., 2022) consists of a diverse set of 204 tasks, aimed at specifically evalu- ating the capabilities and limitations of large LMs. Finally, Dynabench (Kiela et al., 2021) is a human- and-model-in-the-loop platform, for dynamic data collection and benchmarking, which currently sup- ports ten tasks. Notably, none of these benchmarks provide utility/equity measures, or a reward struc- ture that incentivizes progress towards the most under-served languages. Moving beyond English While the aforemen- tioned benchmarks have driven progress in NLP for English, there have been several recent efforts made towards other languages as well. Multilin- gual composite benchmarks such as XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021), and XGLUE (Liang et al., 2020) are a collection of datasets in a variety of tasks and languages. XTREME includes 9 tasks across 40 languages, XTREME-R includes 10 tasks across 50 languages with 198 datasets, and X-GLUE improves GLUE (Wang et al., 2018) by including 11 cross-lingual tasks. However, all of these are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Beyond these, there have been directed efforts towards dataset curation, especially for the low [text] resource. MasakhaNER (Adelani et al., 2021) supports a large dataset for NER task of 10 African languages. IndoNLU (Wilie et al., 2020) is the first vast resource Indonesian bench- mark, and the KLUE benchmark (Park et al., 2021) focuses on 8 diverse tasks in Korean. In sum, while all of the above efforts have been impactful, none have had the goal to track global progress made by us as a research community, and design a reward system that incentivizes both data and model work, especially for the under-served languages. With GlobalBench, we propose a first step to bridge this gap and move towards inclusive, multi-faceted measurement of progress. 7 Conclusion In this paper, we introduce GlobalBench, an ever- expanding collection of datasets and models span- ning across all tasks and languages within NLP. Our aim is for the community to move towards a common goal of building NLP technology that equitably serves"}, {"question": " What impact did dataset submissions have on the global averages for Named Entity Recognition (NER) in the recent past?,answer: They helped increase the global averages, especially when datasets for African languages and more populous languages were added.", "ref_chunk": "(a) Demographic global average (b) Linguistic global average Figure 5: A snaphsot capturing the increase in global averages for NER in the recent past, with the addition of new datasets. system outputs, please refer to Appendix \u00a7A.2. 5.3 Measuring improvement with GlobalBench Another major focus of GlobalBench is to mea- sure and encourage improvement in the quality of language technology systems. 5.3.1 How have we improved? GlobalBench keeps track of when each submission was made, making it possible to examine global progress in NLP over time. To give one example of this, in Figure 5 we show how dataset submis- sions have helped increase the global averages for NER in the recent past. Specifically, the earliest time point only covered English datasets, leaving both averages relatively low. In the second dat- apoint, systems for African languages from the MasakhaNER dataset were added (Adelani et al., 2021), significantly raising the linguistic average. In the third datapoint, systems from the XTREME benchmark (Hu et al., 2020) were added, cover- ing a more populous set of languages, significantly raising the demographic average. 5.3.2 Where can we improve? The variety of analysis in the previous section is, in a sense, backward-looking: it looks back at the progress that has already been made. In order to instead get a better idea of how we may improve our systems in the future, we use the methodology in Section 2.3 to identify most under-served lan- guages in GlobalBench, which are the languages with relatively high population and relatively low scores for each of the tasks. To display in Global- Bench, we choose a parameter of \u03c4 = 0.4, which allows us to moderate between considerations of serving all speakers in the world and serving all languages in the world. Task Lang 1 Lang 2 Lang 3 Named Entity Recognition Extractive QA Text Pair Classification Machine Translation Text Classification KG Prediction cmn por ben cmn cmn cmn pnb jpn por spa spa spa wuu urd ind ara ara ara Table 3: Most under-served languages for each task (by ISO 639-3 language code). We show the three most under-served languages for each task in Table 35. From these statistics we observe some interesting trends. First, for tasks where the most widely spoken non-English lan- guages in the world, Mandarin Chinese (cmn), Spanish (spa), and Arabic (ara) are not covered, these are selected the most under-served languages. 5Note that for the purpose of these statistics, we use the source language for the machine translation task. However, for the tasks with better language cov- erage such as NER, extractive QA, and text pair classification, the most under-served languages are ones with relatively high population that are nonetheless not covered well by existing multilin- gual datasets that have been included in Global- Bench. This indicates a need for more creation or incorporation of datasets for major languages such as Punjabi, Wu Chinese, and Portuguese, which have been overlooked by existing composite bench- marks. 6 Related Work From datasets to benchmarks Given the ubiq- uitous use of NLP technology in applications, it is imperative to track and maintain progress across a variety of NLP tasks. Evaluating and compar- ing systems on a single task can also be problem- atic; past work has identified issues with standard datasets (Artetxe et al., 2019; Gururangan et al., 2018). As the field has progressed, several bench- marks have been released to spur the development of generalizable NLU systems. GLUE (Wang et al., 2018) was one such benchmark with a collection of 9 diverse NLU tasks (sentiment analysis (Socher et al., 2013), natural language inference (Williams et al., 2017), etc.), contrary to prior benchmarks that focused on datasets for a single category of tasks (Conneau and Kiela, 2018). SuperGLUE (Wang et al., 2019) updates GLUE by introducing a new set of harder tasks like commonsense rea- soning (Zhang et al., 2018) and question answer- ing (Khashabi et al., 2018). The recently released BIG-bench (Srivastava et al., 2022) consists of a diverse set of 204 tasks, aimed at specifically evalu- ating the capabilities and limitations of large LMs. Finally, Dynabench (Kiela et al., 2021) is a human- and-model-in-the-loop platform, for dynamic data collection and benchmarking, which currently sup- ports ten tasks. Notably, none of these benchmarks provide utility/equity measures, or a reward struc- ture that incentivizes progress towards the most under-served languages. Moving beyond English While the aforemen- tioned benchmarks have driven progress in NLP for English, there have been several recent efforts made towards other languages as well. Multilin- gual composite benchmarks such as XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021), and XGLUE (Liang et al., 2020) are a collection of datasets in a variety of tasks and languages. XTREME includes 9 tasks across 40 languages, XTREME-R includes 10 tasks across 50 languages with 198 datasets, and X-GLUE improves GLUE (Wang et al., 2018) by including 11 cross-lingual tasks. However, all of these are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Beyond these, there have been directed efforts towards dataset curation, especially for the low [text] resource. MasakhaNER (Adelani et al., 2021) supports a large dataset for NER task of 10 African languages. IndoNLU (Wilie et al., 2020) is the first vast resource Indonesian bench- mark, and the KLUE benchmark (Park et al., 2021) focuses on 8 diverse tasks in Korean. In sum, while all of the above efforts have been impactful, none have had the goal to track global progress made by us as a research community, and design a reward system that incentivizes both data and model work, especially for the under-served languages. With GlobalBench, we propose a first step to bridge this gap and move towards inclusive, multi-faceted measurement of progress. 7 Conclusion In this paper, we introduce GlobalBench, an ever- expanding collection of datasets and models span- ning across all tasks and languages within NLP. Our aim is for the community to move towards a common goal of building NLP technology that equitably serves"}, {"question": " How does GlobalBench identify under-served languages?,answer: It identifies languages with relatively high population and low scores for each task.", "ref_chunk": "(a) Demographic global average (b) Linguistic global average Figure 5: A snaphsot capturing the increase in global averages for NER in the recent past, with the addition of new datasets. system outputs, please refer to Appendix \u00a7A.2. 5.3 Measuring improvement with GlobalBench Another major focus of GlobalBench is to mea- sure and encourage improvement in the quality of language technology systems. 5.3.1 How have we improved? GlobalBench keeps track of when each submission was made, making it possible to examine global progress in NLP over time. To give one example of this, in Figure 5 we show how dataset submis- sions have helped increase the global averages for NER in the recent past. Specifically, the earliest time point only covered English datasets, leaving both averages relatively low. In the second dat- apoint, systems for African languages from the MasakhaNER dataset were added (Adelani et al., 2021), significantly raising the linguistic average. In the third datapoint, systems from the XTREME benchmark (Hu et al., 2020) were added, cover- ing a more populous set of languages, significantly raising the demographic average. 5.3.2 Where can we improve? The variety of analysis in the previous section is, in a sense, backward-looking: it looks back at the progress that has already been made. In order to instead get a better idea of how we may improve our systems in the future, we use the methodology in Section 2.3 to identify most under-served lan- guages in GlobalBench, which are the languages with relatively high population and relatively low scores for each of the tasks. To display in Global- Bench, we choose a parameter of \u03c4 = 0.4, which allows us to moderate between considerations of serving all speakers in the world and serving all languages in the world. Task Lang 1 Lang 2 Lang 3 Named Entity Recognition Extractive QA Text Pair Classification Machine Translation Text Classification KG Prediction cmn por ben cmn cmn cmn pnb jpn por spa spa spa wuu urd ind ara ara ara Table 3: Most under-served languages for each task (by ISO 639-3 language code). We show the three most under-served languages for each task in Table 35. From these statistics we observe some interesting trends. First, for tasks where the most widely spoken non-English lan- guages in the world, Mandarin Chinese (cmn), Spanish (spa), and Arabic (ara) are not covered, these are selected the most under-served languages. 5Note that for the purpose of these statistics, we use the source language for the machine translation task. However, for the tasks with better language cov- erage such as NER, extractive QA, and text pair classification, the most under-served languages are ones with relatively high population that are nonetheless not covered well by existing multilin- gual datasets that have been included in Global- Bench. This indicates a need for more creation or incorporation of datasets for major languages such as Punjabi, Wu Chinese, and Portuguese, which have been overlooked by existing composite bench- marks. 6 Related Work From datasets to benchmarks Given the ubiq- uitous use of NLP technology in applications, it is imperative to track and maintain progress across a variety of NLP tasks. Evaluating and compar- ing systems on a single task can also be problem- atic; past work has identified issues with standard datasets (Artetxe et al., 2019; Gururangan et al., 2018). As the field has progressed, several bench- marks have been released to spur the development of generalizable NLU systems. GLUE (Wang et al., 2018) was one such benchmark with a collection of 9 diverse NLU tasks (sentiment analysis (Socher et al., 2013), natural language inference (Williams et al., 2017), etc.), contrary to prior benchmarks that focused on datasets for a single category of tasks (Conneau and Kiela, 2018). SuperGLUE (Wang et al., 2019) updates GLUE by introducing a new set of harder tasks like commonsense rea- soning (Zhang et al., 2018) and question answer- ing (Khashabi et al., 2018). The recently released BIG-bench (Srivastava et al., 2022) consists of a diverse set of 204 tasks, aimed at specifically evalu- ating the capabilities and limitations of large LMs. Finally, Dynabench (Kiela et al., 2021) is a human- and-model-in-the-loop platform, for dynamic data collection and benchmarking, which currently sup- ports ten tasks. Notably, none of these benchmarks provide utility/equity measures, or a reward struc- ture that incentivizes progress towards the most under-served languages. Moving beyond English While the aforemen- tioned benchmarks have driven progress in NLP for English, there have been several recent efforts made towards other languages as well. Multilin- gual composite benchmarks such as XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021), and XGLUE (Liang et al., 2020) are a collection of datasets in a variety of tasks and languages. XTREME includes 9 tasks across 40 languages, XTREME-R includes 10 tasks across 50 languages with 198 datasets, and X-GLUE improves GLUE (Wang et al., 2018) by including 11 cross-lingual tasks. However, all of these are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Beyond these, there have been directed efforts towards dataset curation, especially for the low [text] resource. MasakhaNER (Adelani et al., 2021) supports a large dataset for NER task of 10 African languages. IndoNLU (Wilie et al., 2020) is the first vast resource Indonesian bench- mark, and the KLUE benchmark (Park et al., 2021) focuses on 8 diverse tasks in Korean. In sum, while all of the above efforts have been impactful, none have had the goal to track global progress made by us as a research community, and design a reward system that incentivizes both data and model work, especially for the under-served languages. With GlobalBench, we propose a first step to bridge this gap and move towards inclusive, multi-faceted measurement of progress. 7 Conclusion In this paper, we introduce GlobalBench, an ever- expanding collection of datasets and models span- ning across all tasks and languages within NLP. Our aim is for the community to move towards a common goal of building NLP technology that equitably serves"}, {"question": " What parameter is chosen to moderate between serving all speakers and serving all languages in the world in GlobalBench?,answer: A parameter of \u03c4 = 0.4 is chosen.", "ref_chunk": "(a) Demographic global average (b) Linguistic global average Figure 5: A snaphsot capturing the increase in global averages for NER in the recent past, with the addition of new datasets. system outputs, please refer to Appendix \u00a7A.2. 5.3 Measuring improvement with GlobalBench Another major focus of GlobalBench is to mea- sure and encourage improvement in the quality of language technology systems. 5.3.1 How have we improved? GlobalBench keeps track of when each submission was made, making it possible to examine global progress in NLP over time. To give one example of this, in Figure 5 we show how dataset submis- sions have helped increase the global averages for NER in the recent past. Specifically, the earliest time point only covered English datasets, leaving both averages relatively low. In the second dat- apoint, systems for African languages from the MasakhaNER dataset were added (Adelani et al., 2021), significantly raising the linguistic average. In the third datapoint, systems from the XTREME benchmark (Hu et al., 2020) were added, cover- ing a more populous set of languages, significantly raising the demographic average. 5.3.2 Where can we improve? The variety of analysis in the previous section is, in a sense, backward-looking: it looks back at the progress that has already been made. In order to instead get a better idea of how we may improve our systems in the future, we use the methodology in Section 2.3 to identify most under-served lan- guages in GlobalBench, which are the languages with relatively high population and relatively low scores for each of the tasks. To display in Global- Bench, we choose a parameter of \u03c4 = 0.4, which allows us to moderate between considerations of serving all speakers in the world and serving all languages in the world. Task Lang 1 Lang 2 Lang 3 Named Entity Recognition Extractive QA Text Pair Classification Machine Translation Text Classification KG Prediction cmn por ben cmn cmn cmn pnb jpn por spa spa spa wuu urd ind ara ara ara Table 3: Most under-served languages for each task (by ISO 639-3 language code). We show the three most under-served languages for each task in Table 35. From these statistics we observe some interesting trends. First, for tasks where the most widely spoken non-English lan- guages in the world, Mandarin Chinese (cmn), Spanish (spa), and Arabic (ara) are not covered, these are selected the most under-served languages. 5Note that for the purpose of these statistics, we use the source language for the machine translation task. However, for the tasks with better language cov- erage such as NER, extractive QA, and text pair classification, the most under-served languages are ones with relatively high population that are nonetheless not covered well by existing multilin- gual datasets that have been included in Global- Bench. This indicates a need for more creation or incorporation of datasets for major languages such as Punjabi, Wu Chinese, and Portuguese, which have been overlooked by existing composite bench- marks. 6 Related Work From datasets to benchmarks Given the ubiq- uitous use of NLP technology in applications, it is imperative to track and maintain progress across a variety of NLP tasks. Evaluating and compar- ing systems on a single task can also be problem- atic; past work has identified issues with standard datasets (Artetxe et al., 2019; Gururangan et al., 2018). As the field has progressed, several bench- marks have been released to spur the development of generalizable NLU systems. GLUE (Wang et al., 2018) was one such benchmark with a collection of 9 diverse NLU tasks (sentiment analysis (Socher et al., 2013), natural language inference (Williams et al., 2017), etc.), contrary to prior benchmarks that focused on datasets for a single category of tasks (Conneau and Kiela, 2018). SuperGLUE (Wang et al., 2019) updates GLUE by introducing a new set of harder tasks like commonsense rea- soning (Zhang et al., 2018) and question answer- ing (Khashabi et al., 2018). The recently released BIG-bench (Srivastava et al., 2022) consists of a diverse set of 204 tasks, aimed at specifically evalu- ating the capabilities and limitations of large LMs. Finally, Dynabench (Kiela et al., 2021) is a human- and-model-in-the-loop platform, for dynamic data collection and benchmarking, which currently sup- ports ten tasks. Notably, none of these benchmarks provide utility/equity measures, or a reward struc- ture that incentivizes progress towards the most under-served languages. Moving beyond English While the aforemen- tioned benchmarks have driven progress in NLP for English, there have been several recent efforts made towards other languages as well. Multilin- gual composite benchmarks such as XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021), and XGLUE (Liang et al., 2020) are a collection of datasets in a variety of tasks and languages. XTREME includes 9 tasks across 40 languages, XTREME-R includes 10 tasks across 50 languages with 198 datasets, and X-GLUE improves GLUE (Wang et al., 2018) by including 11 cross-lingual tasks. However, all of these are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Beyond these, there have been directed efforts towards dataset curation, especially for the low [text] resource. MasakhaNER (Adelani et al., 2021) supports a large dataset for NER task of 10 African languages. IndoNLU (Wilie et al., 2020) is the first vast resource Indonesian bench- mark, and the KLUE benchmark (Park et al., 2021) focuses on 8 diverse tasks in Korean. In sum, while all of the above efforts have been impactful, none have had the goal to track global progress made by us as a research community, and design a reward system that incentivizes both data and model work, especially for the under-served languages. With GlobalBench, we propose a first step to bridge this gap and move towards inclusive, multi-faceted measurement of progress. 7 Conclusion In this paper, we introduce GlobalBench, an ever- expanding collection of datasets and models span- ning across all tasks and languages within NLP. Our aim is for the community to move towards a common goal of building NLP technology that equitably serves"}, {"question": " Which are the three most under-served languages for tasks such as Named Entity Recognition, Extractive QA, and Text Pair Classification?,answer: Mandarin Chinese (cmn), Spanish (spa), and Arabic (ara).", "ref_chunk": "(a) Demographic global average (b) Linguistic global average Figure 5: A snaphsot capturing the increase in global averages for NER in the recent past, with the addition of new datasets. system outputs, please refer to Appendix \u00a7A.2. 5.3 Measuring improvement with GlobalBench Another major focus of GlobalBench is to mea- sure and encourage improvement in the quality of language technology systems. 5.3.1 How have we improved? GlobalBench keeps track of when each submission was made, making it possible to examine global progress in NLP over time. To give one example of this, in Figure 5 we show how dataset submis- sions have helped increase the global averages for NER in the recent past. Specifically, the earliest time point only covered English datasets, leaving both averages relatively low. In the second dat- apoint, systems for African languages from the MasakhaNER dataset were added (Adelani et al., 2021), significantly raising the linguistic average. In the third datapoint, systems from the XTREME benchmark (Hu et al., 2020) were added, cover- ing a more populous set of languages, significantly raising the demographic average. 5.3.2 Where can we improve? The variety of analysis in the previous section is, in a sense, backward-looking: it looks back at the progress that has already been made. In order to instead get a better idea of how we may improve our systems in the future, we use the methodology in Section 2.3 to identify most under-served lan- guages in GlobalBench, which are the languages with relatively high population and relatively low scores for each of the tasks. To display in Global- Bench, we choose a parameter of \u03c4 = 0.4, which allows us to moderate between considerations of serving all speakers in the world and serving all languages in the world. Task Lang 1 Lang 2 Lang 3 Named Entity Recognition Extractive QA Text Pair Classification Machine Translation Text Classification KG Prediction cmn por ben cmn cmn cmn pnb jpn por spa spa spa wuu urd ind ara ara ara Table 3: Most under-served languages for each task (by ISO 639-3 language code). We show the three most under-served languages for each task in Table 35. From these statistics we observe some interesting trends. First, for tasks where the most widely spoken non-English lan- guages in the world, Mandarin Chinese (cmn), Spanish (spa), and Arabic (ara) are not covered, these are selected the most under-served languages. 5Note that for the purpose of these statistics, we use the source language for the machine translation task. However, for the tasks with better language cov- erage such as NER, extractive QA, and text pair classification, the most under-served languages are ones with relatively high population that are nonetheless not covered well by existing multilin- gual datasets that have been included in Global- Bench. This indicates a need for more creation or incorporation of datasets for major languages such as Punjabi, Wu Chinese, and Portuguese, which have been overlooked by existing composite bench- marks. 6 Related Work From datasets to benchmarks Given the ubiq- uitous use of NLP technology in applications, it is imperative to track and maintain progress across a variety of NLP tasks. Evaluating and compar- ing systems on a single task can also be problem- atic; past work has identified issues with standard datasets (Artetxe et al., 2019; Gururangan et al., 2018). As the field has progressed, several bench- marks have been released to spur the development of generalizable NLU systems. GLUE (Wang et al., 2018) was one such benchmark with a collection of 9 diverse NLU tasks (sentiment analysis (Socher et al., 2013), natural language inference (Williams et al., 2017), etc.), contrary to prior benchmarks that focused on datasets for a single category of tasks (Conneau and Kiela, 2018). SuperGLUE (Wang et al., 2019) updates GLUE by introducing a new set of harder tasks like commonsense rea- soning (Zhang et al., 2018) and question answer- ing (Khashabi et al., 2018). The recently released BIG-bench (Srivastava et al., 2022) consists of a diverse set of 204 tasks, aimed at specifically evalu- ating the capabilities and limitations of large LMs. Finally, Dynabench (Kiela et al., 2021) is a human- and-model-in-the-loop platform, for dynamic data collection and benchmarking, which currently sup- ports ten tasks. Notably, none of these benchmarks provide utility/equity measures, or a reward struc- ture that incentivizes progress towards the most under-served languages. Moving beyond English While the aforemen- tioned benchmarks have driven progress in NLP for English, there have been several recent efforts made towards other languages as well. Multilin- gual composite benchmarks such as XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021), and XGLUE (Liang et al., 2020) are a collection of datasets in a variety of tasks and languages. XTREME includes 9 tasks across 40 languages, XTREME-R includes 10 tasks across 50 languages with 198 datasets, and X-GLUE improves GLUE (Wang et al., 2018) by including 11 cross-lingual tasks. However, all of these are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Beyond these, there have been directed efforts towards dataset curation, especially for the low [text] resource. MasakhaNER (Adelani et al., 2021) supports a large dataset for NER task of 10 African languages. IndoNLU (Wilie et al., 2020) is the first vast resource Indonesian bench- mark, and the KLUE benchmark (Park et al., 2021) focuses on 8 diverse tasks in Korean. In sum, while all of the above efforts have been impactful, none have had the goal to track global progress made by us as a research community, and design a reward system that incentivizes both data and model work, especially for the under-served languages. With GlobalBench, we propose a first step to bridge this gap and move towards inclusive, multi-faceted measurement of progress. 7 Conclusion In this paper, we introduce GlobalBench, an ever- expanding collection of datasets and models span- ning across all tasks and languages within NLP. Our aim is for the community to move towards a common goal of building NLP technology that equitably serves"}, {"question": " What issue does past work identify with evaluating and comparing systems on a single task?,answer: Issues with standard datasets have been identified.", "ref_chunk": "(a) Demographic global average (b) Linguistic global average Figure 5: A snaphsot capturing the increase in global averages for NER in the recent past, with the addition of new datasets. system outputs, please refer to Appendix \u00a7A.2. 5.3 Measuring improvement with GlobalBench Another major focus of GlobalBench is to mea- sure and encourage improvement in the quality of language technology systems. 5.3.1 How have we improved? GlobalBench keeps track of when each submission was made, making it possible to examine global progress in NLP over time. To give one example of this, in Figure 5 we show how dataset submis- sions have helped increase the global averages for NER in the recent past. Specifically, the earliest time point only covered English datasets, leaving both averages relatively low. In the second dat- apoint, systems for African languages from the MasakhaNER dataset were added (Adelani et al., 2021), significantly raising the linguistic average. In the third datapoint, systems from the XTREME benchmark (Hu et al., 2020) were added, cover- ing a more populous set of languages, significantly raising the demographic average. 5.3.2 Where can we improve? The variety of analysis in the previous section is, in a sense, backward-looking: it looks back at the progress that has already been made. In order to instead get a better idea of how we may improve our systems in the future, we use the methodology in Section 2.3 to identify most under-served lan- guages in GlobalBench, which are the languages with relatively high population and relatively low scores for each of the tasks. To display in Global- Bench, we choose a parameter of \u03c4 = 0.4, which allows us to moderate between considerations of serving all speakers in the world and serving all languages in the world. Task Lang 1 Lang 2 Lang 3 Named Entity Recognition Extractive QA Text Pair Classification Machine Translation Text Classification KG Prediction cmn por ben cmn cmn cmn pnb jpn por spa spa spa wuu urd ind ara ara ara Table 3: Most under-served languages for each task (by ISO 639-3 language code). We show the three most under-served languages for each task in Table 35. From these statistics we observe some interesting trends. First, for tasks where the most widely spoken non-English lan- guages in the world, Mandarin Chinese (cmn), Spanish (spa), and Arabic (ara) are not covered, these are selected the most under-served languages. 5Note that for the purpose of these statistics, we use the source language for the machine translation task. However, for the tasks with better language cov- erage such as NER, extractive QA, and text pair classification, the most under-served languages are ones with relatively high population that are nonetheless not covered well by existing multilin- gual datasets that have been included in Global- Bench. This indicates a need for more creation or incorporation of datasets for major languages such as Punjabi, Wu Chinese, and Portuguese, which have been overlooked by existing composite bench- marks. 6 Related Work From datasets to benchmarks Given the ubiq- uitous use of NLP technology in applications, it is imperative to track and maintain progress across a variety of NLP tasks. Evaluating and compar- ing systems on a single task can also be problem- atic; past work has identified issues with standard datasets (Artetxe et al., 2019; Gururangan et al., 2018). As the field has progressed, several bench- marks have been released to spur the development of generalizable NLU systems. GLUE (Wang et al., 2018) was one such benchmark with a collection of 9 diverse NLU tasks (sentiment analysis (Socher et al., 2013), natural language inference (Williams et al., 2017), etc.), contrary to prior benchmarks that focused on datasets for a single category of tasks (Conneau and Kiela, 2018). SuperGLUE (Wang et al., 2019) updates GLUE by introducing a new set of harder tasks like commonsense rea- soning (Zhang et al., 2018) and question answer- ing (Khashabi et al., 2018). The recently released BIG-bench (Srivastava et al., 2022) consists of a diverse set of 204 tasks, aimed at specifically evalu- ating the capabilities and limitations of large LMs. Finally, Dynabench (Kiela et al., 2021) is a human- and-model-in-the-loop platform, for dynamic data collection and benchmarking, which currently sup- ports ten tasks. Notably, none of these benchmarks provide utility/equity measures, or a reward struc- ture that incentivizes progress towards the most under-served languages. Moving beyond English While the aforemen- tioned benchmarks have driven progress in NLP for English, there have been several recent efforts made towards other languages as well. Multilin- gual composite benchmarks such as XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021), and XGLUE (Liang et al., 2020) are a collection of datasets in a variety of tasks and languages. XTREME includes 9 tasks across 40 languages, XTREME-R includes 10 tasks across 50 languages with 198 datasets, and X-GLUE improves GLUE (Wang et al., 2018) by including 11 cross-lingual tasks. However, all of these are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Beyond these, there have been directed efforts towards dataset curation, especially for the low [text] resource. MasakhaNER (Adelani et al., 2021) supports a large dataset for NER task of 10 African languages. IndoNLU (Wilie et al., 2020) is the first vast resource Indonesian bench- mark, and the KLUE benchmark (Park et al., 2021) focuses on 8 diverse tasks in Korean. In sum, while all of the above efforts have been impactful, none have had the goal to track global progress made by us as a research community, and design a reward system that incentivizes both data and model work, especially for the under-served languages. With GlobalBench, we propose a first step to bridge this gap and move towards inclusive, multi-faceted measurement of progress. 7 Conclusion In this paper, we introduce GlobalBench, an ever- expanding collection of datasets and models span- ning across all tasks and languages within NLP. Our aim is for the community to move towards a common goal of building NLP technology that equitably serves"}, {"question": " What is the goal of the benchmark BIG-bench?,answer: To evaluate the capabilities and limitations of large LMs.", "ref_chunk": "(a) Demographic global average (b) Linguistic global average Figure 5: A snaphsot capturing the increase in global averages for NER in the recent past, with the addition of new datasets. system outputs, please refer to Appendix \u00a7A.2. 5.3 Measuring improvement with GlobalBench Another major focus of GlobalBench is to mea- sure and encourage improvement in the quality of language technology systems. 5.3.1 How have we improved? GlobalBench keeps track of when each submission was made, making it possible to examine global progress in NLP over time. To give one example of this, in Figure 5 we show how dataset submis- sions have helped increase the global averages for NER in the recent past. Specifically, the earliest time point only covered English datasets, leaving both averages relatively low. In the second dat- apoint, systems for African languages from the MasakhaNER dataset were added (Adelani et al., 2021), significantly raising the linguistic average. In the third datapoint, systems from the XTREME benchmark (Hu et al., 2020) were added, cover- ing a more populous set of languages, significantly raising the demographic average. 5.3.2 Where can we improve? The variety of analysis in the previous section is, in a sense, backward-looking: it looks back at the progress that has already been made. In order to instead get a better idea of how we may improve our systems in the future, we use the methodology in Section 2.3 to identify most under-served lan- guages in GlobalBench, which are the languages with relatively high population and relatively low scores for each of the tasks. To display in Global- Bench, we choose a parameter of \u03c4 = 0.4, which allows us to moderate between considerations of serving all speakers in the world and serving all languages in the world. Task Lang 1 Lang 2 Lang 3 Named Entity Recognition Extractive QA Text Pair Classification Machine Translation Text Classification KG Prediction cmn por ben cmn cmn cmn pnb jpn por spa spa spa wuu urd ind ara ara ara Table 3: Most under-served languages for each task (by ISO 639-3 language code). We show the three most under-served languages for each task in Table 35. From these statistics we observe some interesting trends. First, for tasks where the most widely spoken non-English lan- guages in the world, Mandarin Chinese (cmn), Spanish (spa), and Arabic (ara) are not covered, these are selected the most under-served languages. 5Note that for the purpose of these statistics, we use the source language for the machine translation task. However, for the tasks with better language cov- erage such as NER, extractive QA, and text pair classification, the most under-served languages are ones with relatively high population that are nonetheless not covered well by existing multilin- gual datasets that have been included in Global- Bench. This indicates a need for more creation or incorporation of datasets for major languages such as Punjabi, Wu Chinese, and Portuguese, which have been overlooked by existing composite bench- marks. 6 Related Work From datasets to benchmarks Given the ubiq- uitous use of NLP technology in applications, it is imperative to track and maintain progress across a variety of NLP tasks. Evaluating and compar- ing systems on a single task can also be problem- atic; past work has identified issues with standard datasets (Artetxe et al., 2019; Gururangan et al., 2018). As the field has progressed, several bench- marks have been released to spur the development of generalizable NLU systems. GLUE (Wang et al., 2018) was one such benchmark with a collection of 9 diverse NLU tasks (sentiment analysis (Socher et al., 2013), natural language inference (Williams et al., 2017), etc.), contrary to prior benchmarks that focused on datasets for a single category of tasks (Conneau and Kiela, 2018). SuperGLUE (Wang et al., 2019) updates GLUE by introducing a new set of harder tasks like commonsense rea- soning (Zhang et al., 2018) and question answer- ing (Khashabi et al., 2018). The recently released BIG-bench (Srivastava et al., 2022) consists of a diverse set of 204 tasks, aimed at specifically evalu- ating the capabilities and limitations of large LMs. Finally, Dynabench (Kiela et al., 2021) is a human- and-model-in-the-loop platform, for dynamic data collection and benchmarking, which currently sup- ports ten tasks. Notably, none of these benchmarks provide utility/equity measures, or a reward struc- ture that incentivizes progress towards the most under-served languages. Moving beyond English While the aforemen- tioned benchmarks have driven progress in NLP for English, there have been several recent efforts made towards other languages as well. Multilin- gual composite benchmarks such as XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021), and XGLUE (Liang et al., 2020) are a collection of datasets in a variety of tasks and languages. XTREME includes 9 tasks across 40 languages, XTREME-R includes 10 tasks across 50 languages with 198 datasets, and X-GLUE improves GLUE (Wang et al., 2018) by including 11 cross-lingual tasks. However, all of these are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Beyond these, there have been directed efforts towards dataset curation, especially for the low [text] resource. MasakhaNER (Adelani et al., 2021) supports a large dataset for NER task of 10 African languages. IndoNLU (Wilie et al., 2020) is the first vast resource Indonesian bench- mark, and the KLUE benchmark (Park et al., 2021) focuses on 8 diverse tasks in Korean. In sum, while all of the above efforts have been impactful, none have had the goal to track global progress made by us as a research community, and design a reward system that incentivizes both data and model work, especially for the under-served languages. With GlobalBench, we propose a first step to bridge this gap and move towards inclusive, multi-faceted measurement of progress. 7 Conclusion In this paper, we introduce GlobalBench, an ever- expanding collection of datasets and models span- ning across all tasks and languages within NLP. Our aim is for the community to move towards a common goal of building NLP technology that equitably serves"}, {"question": " What are some examples of multilingual composite benchmarks mentioned in the text?,answer: XTREME, XTREME-R, and XGLUE.", "ref_chunk": "(a) Demographic global average (b) Linguistic global average Figure 5: A snaphsot capturing the increase in global averages for NER in the recent past, with the addition of new datasets. system outputs, please refer to Appendix \u00a7A.2. 5.3 Measuring improvement with GlobalBench Another major focus of GlobalBench is to mea- sure and encourage improvement in the quality of language technology systems. 5.3.1 How have we improved? GlobalBench keeps track of when each submission was made, making it possible to examine global progress in NLP over time. To give one example of this, in Figure 5 we show how dataset submis- sions have helped increase the global averages for NER in the recent past. Specifically, the earliest time point only covered English datasets, leaving both averages relatively low. In the second dat- apoint, systems for African languages from the MasakhaNER dataset were added (Adelani et al., 2021), significantly raising the linguistic average. In the third datapoint, systems from the XTREME benchmark (Hu et al., 2020) were added, cover- ing a more populous set of languages, significantly raising the demographic average. 5.3.2 Where can we improve? The variety of analysis in the previous section is, in a sense, backward-looking: it looks back at the progress that has already been made. In order to instead get a better idea of how we may improve our systems in the future, we use the methodology in Section 2.3 to identify most under-served lan- guages in GlobalBench, which are the languages with relatively high population and relatively low scores for each of the tasks. To display in Global- Bench, we choose a parameter of \u03c4 = 0.4, which allows us to moderate between considerations of serving all speakers in the world and serving all languages in the world. Task Lang 1 Lang 2 Lang 3 Named Entity Recognition Extractive QA Text Pair Classification Machine Translation Text Classification KG Prediction cmn por ben cmn cmn cmn pnb jpn por spa spa spa wuu urd ind ara ara ara Table 3: Most under-served languages for each task (by ISO 639-3 language code). We show the three most under-served languages for each task in Table 35. From these statistics we observe some interesting trends. First, for tasks where the most widely spoken non-English lan- guages in the world, Mandarin Chinese (cmn), Spanish (spa), and Arabic (ara) are not covered, these are selected the most under-served languages. 5Note that for the purpose of these statistics, we use the source language for the machine translation task. However, for the tasks with better language cov- erage such as NER, extractive QA, and text pair classification, the most under-served languages are ones with relatively high population that are nonetheless not covered well by existing multilin- gual datasets that have been included in Global- Bench. This indicates a need for more creation or incorporation of datasets for major languages such as Punjabi, Wu Chinese, and Portuguese, which have been overlooked by existing composite bench- marks. 6 Related Work From datasets to benchmarks Given the ubiq- uitous use of NLP technology in applications, it is imperative to track and maintain progress across a variety of NLP tasks. Evaluating and compar- ing systems on a single task can also be problem- atic; past work has identified issues with standard datasets (Artetxe et al., 2019; Gururangan et al., 2018). As the field has progressed, several bench- marks have been released to spur the development of generalizable NLU systems. GLUE (Wang et al., 2018) was one such benchmark with a collection of 9 diverse NLU tasks (sentiment analysis (Socher et al., 2013), natural language inference (Williams et al., 2017), etc.), contrary to prior benchmarks that focused on datasets for a single category of tasks (Conneau and Kiela, 2018). SuperGLUE (Wang et al., 2019) updates GLUE by introducing a new set of harder tasks like commonsense rea- soning (Zhang et al., 2018) and question answer- ing (Khashabi et al., 2018). The recently released BIG-bench (Srivastava et al., 2022) consists of a diverse set of 204 tasks, aimed at specifically evalu- ating the capabilities and limitations of large LMs. Finally, Dynabench (Kiela et al., 2021) is a human- and-model-in-the-loop platform, for dynamic data collection and benchmarking, which currently sup- ports ten tasks. Notably, none of these benchmarks provide utility/equity measures, or a reward struc- ture that incentivizes progress towards the most under-served languages. Moving beyond English While the aforemen- tioned benchmarks have driven progress in NLP for English, there have been several recent efforts made towards other languages as well. Multilin- gual composite benchmarks such as XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021), and XGLUE (Liang et al., 2020) are a collection of datasets in a variety of tasks and languages. XTREME includes 9 tasks across 40 languages, XTREME-R includes 10 tasks across 50 languages with 198 datasets, and X-GLUE improves GLUE (Wang et al., 2018) by including 11 cross-lingual tasks. However, all of these are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Beyond these, there have been directed efforts towards dataset curation, especially for the low [text] resource. MasakhaNER (Adelani et al., 2021) supports a large dataset for NER task of 10 African languages. IndoNLU (Wilie et al., 2020) is the first vast resource Indonesian bench- mark, and the KLUE benchmark (Park et al., 2021) focuses on 8 diverse tasks in Korean. In sum, while all of the above efforts have been impactful, none have had the goal to track global progress made by us as a research community, and design a reward system that incentivizes both data and model work, especially for the under-served languages. With GlobalBench, we propose a first step to bridge this gap and move towards inclusive, multi-faceted measurement of progress. 7 Conclusion In this paper, we introduce GlobalBench, an ever- expanding collection of datasets and models span- ning across all tasks and languages within NLP. Our aim is for the community to move towards a common goal of building NLP technology that equitably serves"}, {"question": " What is the purpose of MasakhaNER dataset?,answer: To support a large dataset for the NER task of 10 African languages.", "ref_chunk": "(a) Demographic global average (b) Linguistic global average Figure 5: A snaphsot capturing the increase in global averages for NER in the recent past, with the addition of new datasets. system outputs, please refer to Appendix \u00a7A.2. 5.3 Measuring improvement with GlobalBench Another major focus of GlobalBench is to mea- sure and encourage improvement in the quality of language technology systems. 5.3.1 How have we improved? GlobalBench keeps track of when each submission was made, making it possible to examine global progress in NLP over time. To give one example of this, in Figure 5 we show how dataset submis- sions have helped increase the global averages for NER in the recent past. Specifically, the earliest time point only covered English datasets, leaving both averages relatively low. In the second dat- apoint, systems for African languages from the MasakhaNER dataset were added (Adelani et al., 2021), significantly raising the linguistic average. In the third datapoint, systems from the XTREME benchmark (Hu et al., 2020) were added, cover- ing a more populous set of languages, significantly raising the demographic average. 5.3.2 Where can we improve? The variety of analysis in the previous section is, in a sense, backward-looking: it looks back at the progress that has already been made. In order to instead get a better idea of how we may improve our systems in the future, we use the methodology in Section 2.3 to identify most under-served lan- guages in GlobalBench, which are the languages with relatively high population and relatively low scores for each of the tasks. To display in Global- Bench, we choose a parameter of \u03c4 = 0.4, which allows us to moderate between considerations of serving all speakers in the world and serving all languages in the world. Task Lang 1 Lang 2 Lang 3 Named Entity Recognition Extractive QA Text Pair Classification Machine Translation Text Classification KG Prediction cmn por ben cmn cmn cmn pnb jpn por spa spa spa wuu urd ind ara ara ara Table 3: Most under-served languages for each task (by ISO 639-3 language code). We show the three most under-served languages for each task in Table 35. From these statistics we observe some interesting trends. First, for tasks where the most widely spoken non-English lan- guages in the world, Mandarin Chinese (cmn), Spanish (spa), and Arabic (ara) are not covered, these are selected the most under-served languages. 5Note that for the purpose of these statistics, we use the source language for the machine translation task. However, for the tasks with better language cov- erage such as NER, extractive QA, and text pair classification, the most under-served languages are ones with relatively high population that are nonetheless not covered well by existing multilin- gual datasets that have been included in Global- Bench. This indicates a need for more creation or incorporation of datasets for major languages such as Punjabi, Wu Chinese, and Portuguese, which have been overlooked by existing composite bench- marks. 6 Related Work From datasets to benchmarks Given the ubiq- uitous use of NLP technology in applications, it is imperative to track and maintain progress across a variety of NLP tasks. Evaluating and compar- ing systems on a single task can also be problem- atic; past work has identified issues with standard datasets (Artetxe et al., 2019; Gururangan et al., 2018). As the field has progressed, several bench- marks have been released to spur the development of generalizable NLU systems. GLUE (Wang et al., 2018) was one such benchmark with a collection of 9 diverse NLU tasks (sentiment analysis (Socher et al., 2013), natural language inference (Williams et al., 2017), etc.), contrary to prior benchmarks that focused on datasets for a single category of tasks (Conneau and Kiela, 2018). SuperGLUE (Wang et al., 2019) updates GLUE by introducing a new set of harder tasks like commonsense rea- soning (Zhang et al., 2018) and question answer- ing (Khashabi et al., 2018). The recently released BIG-bench (Srivastava et al., 2022) consists of a diverse set of 204 tasks, aimed at specifically evalu- ating the capabilities and limitations of large LMs. Finally, Dynabench (Kiela et al., 2021) is a human- and-model-in-the-loop platform, for dynamic data collection and benchmarking, which currently sup- ports ten tasks. Notably, none of these benchmarks provide utility/equity measures, or a reward struc- ture that incentivizes progress towards the most under-served languages. Moving beyond English While the aforemen- tioned benchmarks have driven progress in NLP for English, there have been several recent efforts made towards other languages as well. Multilin- gual composite benchmarks such as XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021), and XGLUE (Liang et al., 2020) are a collection of datasets in a variety of tasks and languages. XTREME includes 9 tasks across 40 languages, XTREME-R includes 10 tasks across 50 languages with 198 datasets, and X-GLUE improves GLUE (Wang et al., 2018) by including 11 cross-lingual tasks. However, all of these are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Beyond these, there have been directed efforts towards dataset curation, especially for the low [text] resource. MasakhaNER (Adelani et al., 2021) supports a large dataset for NER task of 10 African languages. IndoNLU (Wilie et al., 2020) is the first vast resource Indonesian bench- mark, and the KLUE benchmark (Park et al., 2021) focuses on 8 diverse tasks in Korean. In sum, while all of the above efforts have been impactful, none have had the goal to track global progress made by us as a research community, and design a reward system that incentivizes both data and model work, especially for the under-served languages. With GlobalBench, we propose a first step to bridge this gap and move towards inclusive, multi-faceted measurement of progress. 7 Conclusion In this paper, we introduce GlobalBench, an ever- expanding collection of datasets and models span- ning across all tasks and languages within NLP. Our aim is for the community to move towards a common goal of building NLP technology that equitably serves"}], "doc_text": "(a) Demographic global average (b) Linguistic global average Figure 5: A snaphsot capturing the increase in global averages for NER in the recent past, with the addition of new datasets. system outputs, please refer to Appendix \u00a7A.2. 5.3 Measuring improvement with GlobalBench Another major focus of GlobalBench is to mea- sure and encourage improvement in the quality of language technology systems. 5.3.1 How have we improved? GlobalBench keeps track of when each submission was made, making it possible to examine global progress in NLP over time. To give one example of this, in Figure 5 we show how dataset submis- sions have helped increase the global averages for NER in the recent past. Specifically, the earliest time point only covered English datasets, leaving both averages relatively low. In the second dat- apoint, systems for African languages from the MasakhaNER dataset were added (Adelani et al., 2021), significantly raising the linguistic average. In the third datapoint, systems from the XTREME benchmark (Hu et al., 2020) were added, cover- ing a more populous set of languages, significantly raising the demographic average. 5.3.2 Where can we improve? The variety of analysis in the previous section is, in a sense, backward-looking: it looks back at the progress that has already been made. In order to instead get a better idea of how we may improve our systems in the future, we use the methodology in Section 2.3 to identify most under-served lan- guages in GlobalBench, which are the languages with relatively high population and relatively low scores for each of the tasks. To display in Global- Bench, we choose a parameter of \u03c4 = 0.4, which allows us to moderate between considerations of serving all speakers in the world and serving all languages in the world. Task Lang 1 Lang 2 Lang 3 Named Entity Recognition Extractive QA Text Pair Classification Machine Translation Text Classification KG Prediction cmn por ben cmn cmn cmn pnb jpn por spa spa spa wuu urd ind ara ara ara Table 3: Most under-served languages for each task (by ISO 639-3 language code). We show the three most under-served languages for each task in Table 35. From these statistics we observe some interesting trends. First, for tasks where the most widely spoken non-English lan- guages in the world, Mandarin Chinese (cmn), Spanish (spa), and Arabic (ara) are not covered, these are selected the most under-served languages. 5Note that for the purpose of these statistics, we use the source language for the machine translation task. However, for the tasks with better language cov- erage such as NER, extractive QA, and text pair classification, the most under-served languages are ones with relatively high population that are nonetheless not covered well by existing multilin- gual datasets that have been included in Global- Bench. This indicates a need for more creation or incorporation of datasets for major languages such as Punjabi, Wu Chinese, and Portuguese, which have been overlooked by existing composite bench- marks. 6 Related Work From datasets to benchmarks Given the ubiq- uitous use of NLP technology in applications, it is imperative to track and maintain progress across a variety of NLP tasks. Evaluating and compar- ing systems on a single task can also be problem- atic; past work has identified issues with standard datasets (Artetxe et al., 2019; Gururangan et al., 2018). As the field has progressed, several bench- marks have been released to spur the development of generalizable NLU systems. GLUE (Wang et al., 2018) was one such benchmark with a collection of 9 diverse NLU tasks (sentiment analysis (Socher et al., 2013), natural language inference (Williams et al., 2017), etc.), contrary to prior benchmarks that focused on datasets for a single category of tasks (Conneau and Kiela, 2018). SuperGLUE (Wang et al., 2019) updates GLUE by introducing a new set of harder tasks like commonsense rea- soning (Zhang et al., 2018) and question answer- ing (Khashabi et al., 2018). The recently released BIG-bench (Srivastava et al., 2022) consists of a diverse set of 204 tasks, aimed at specifically evalu- ating the capabilities and limitations of large LMs. Finally, Dynabench (Kiela et al., 2021) is a human- and-model-in-the-loop platform, for dynamic data collection and benchmarking, which currently sup- ports ten tasks. Notably, none of these benchmarks provide utility/equity measures, or a reward struc- ture that incentivizes progress towards the most under-served languages. Moving beyond English While the aforemen- tioned benchmarks have driven progress in NLP for English, there have been several recent efforts made towards other languages as well. Multilin- gual composite benchmarks such as XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021), and XGLUE (Liang et al., 2020) are a collection of datasets in a variety of tasks and languages. XTREME includes 9 tasks across 40 languages, XTREME-R includes 10 tasks across 50 languages with 198 datasets, and X-GLUE improves GLUE (Wang et al., 2018) by including 11 cross-lingual tasks. However, all of these are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Beyond these, there have been directed efforts towards dataset curation, especially for the low [text] resource. MasakhaNER (Adelani et al., 2021) supports a large dataset for NER task of 10 African languages. IndoNLU (Wilie et al., 2020) is the first vast resource Indonesian bench- mark, and the KLUE benchmark (Park et al., 2021) focuses on 8 diverse tasks in Korean. In sum, while all of the above efforts have been impactful, none have had the goal to track global progress made by us as a research community, and design a reward system that incentivizes both data and model work, especially for the under-served languages. With GlobalBench, we propose a first step to bridge this gap and move towards inclusive, multi-faceted measurement of progress. 7 Conclusion In this paper, we introduce GlobalBench, an ever- expanding collection of datasets and models span- ning across all tasks and languages within NLP. Our aim is for the community to move towards a common goal of building NLP technology that equitably serves"}