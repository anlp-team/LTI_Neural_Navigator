{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Active_Retrieval_Augmented_Generation_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What approach is used to forward-looking active retrieval in the text?", "answer": " Confidence-based Active Retrieval", "ref_chunk": "with retrieval instructions (FLAREinstruct). It iteratively generates search queries (shown in gray italic) to retrieve relevant information to aid future generations. offer only API access, we elicit such behavior by few-shot prompting (Brown et al., 2020). Specifically, for a downstream task, we place the search-related instruction and exemplars at the beginning as skill 1, followed by the instruction and exemplars of the downstream task as skill 2. Given a test case, we ask LMs to combine skills 1 and 2 to generate search queries while performing the task. The structure of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3. Prompt 3.1: retrieval instructions Skill 1. An instruction to guide LMs to generate search queries. Several search-related exemplars. Skill 2. An instruction to guide LMs to perform a specific downstream task (e.g., multihop QA). Several task-related exemplars. An instruction to guide LMs to combine skills 1 and 2 for the test case. The input of the test case. As shown in Figure 2, when the LM generates \u201c[Search(query)]\u201d (shown in gray italic), we stop the generation and use the query terms to retrieve relevant documents, which are prepended before the user input to aid future generation until the next search query is generated or reaches the end. Additional implementation details are included in Appendix A. 3.2 Direct FLARE Since we cannot fine-tune black-box LMs, we found queries generated by FLAREinstruct through retrieval instructions might not be reliable. There- fore, we propose a more direct way of forward- looking active retrieval that uses the next sentence to decide when and what to retrieve. 3.2.1 Confidence-based Active Retrieval As shown in Figure 1, at step t, we first generate a temporary next sentence \u02c6st = LM([x, y<t]) with- out conditioning on retrieved documents. Then we decide whether to trigger retrieval and formulate queries based on \u02c6st. If the LM is confident about \u02c6st, we accept it without retrieving additional informa- tion; if not, we use \u02c6st to formulate search queries qt to retrieve relevant documents, and then regen- erate the next sentence st. The reason we utilize sentences as the basis of our iteration is due to their significance as semantic units that are neither too short nor too lengthy like phrases and paragraphs. However, our approach can also utilize phrases or paragraphs as the basis. Since LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022), we actively trigger retrieval if any token of \u02c6st has a probability lower than a threshold \u03b8 \u2208 [0, 1]. \u03b8 = 0 means retrieval is never triggered, while \u03b8 = 1 triggers retrieval every sentence. yt = (cid:40) \u02c6st st = LM([Dqt, x, y<t]) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise where the query qt is formulated based on \u02c6st. 3.2.2 Confidence-based Query Formulation One way to perform retrieval is to directly use the next sentence \u02c6st as the query qt. This shares a sim- ilar spirit with methods that use generated hypo- thetical titles or paragraphs from LMs as retrieval queries or evidences (Gao et al., 2022; Sun et al., 2022; Yu et al., 2022; Mao et al., 2021). We gen- eralize such techniques to long-form generation where active information access is essential. We found retrieving with the next sentence achieves significantly better results than with the previous context, as shown later in subsection 6.2. However, it has a risk of perpetuating errors con- tained in it. For example, if the LM produces the sentence \u201cJoe Biden attended the University of Pennsylvania\u201d instead of the correct fact that he attended the University of Delaware, using this er- roneous sentence as a query might retrieve mislead- What university did Joe Biden attend?What degree did Joe Biden earn? .Ask a question to which the answer is \u201cthe University of Pennsylvania\u201dAsk a question to which the answer is \u201ca law degree\u201d , where he earned a law degree Joe Biden attended the University of Pennsylvania implicit query by maskingexplicit query by question generation Joe Biden attended , where he earned .LM such as ChatGPT Figure 3: Implicit and explicit query formulation. To- kens with low probabilities are marked with underlines. ing information. We propose two simple methods to overcome this issue as illustrated in Figure 3. Masked sentences as implicit queries. The first method masks out low-confidence tokens in \u02c6st with probabilities below a threshold \u03b2 \u2208 [0, 1], where a higher \u03b2 results in more aggressive masking. This removes potential distractions from the sentence to improve retrieval accuracy. Generated questions as explicit queries. An- other method is to generate explicit questions that target the low-confident span in \u02c6st. For example, if the LM is uncertain about \u201cthe University of Penn- sylvania\u201d, a question like \u201cWhich university did Joe Biden attend?\u201d can help retrieve relevant in- formation. Self-ask (Press et al., 2022) achieved this by manually inserting follow-up questions into downstream task exemplars as shown later in Prompt D.2, which requires task-specific annota- tion efforts. Instead, we developed a universal ap- proach that generates questions for low-confidence spans without additional annotation. Specifically, We first extract all spans from \u02c6st with probabilities below \u03b2. For each extracted span z, we prompt gpt-3.5-turbo to generate a question qt,z that can be answered with the span: Prompt 3.2: zero-shot question generation User input x. Generated output so far y\u2264t. Given the above passage, ask a question to which the answer is the term/entity/phrase \u201cz\u201d. We retrieve using each generated question and interleave the returned documents into a single ranking list to aid future generations. In summary, queries qt are formulated based on \u02c6st as follows: qt = (cid:40) \u2205 mask(\u02c6st) or qgen(\u02c6st) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise 3.3 Base LM We validate our method on one of the most advanced GPT-3.5 LMs text-davinci-003 by iteratively querying their API.2 Implementation Details Document corpus and retrievers. Since we fo- cus"}, {"question": " How does the text propose to handle low confidence in generated sentences?", "answer": " By masking out low-confidence tokens or generating explicit questions", "ref_chunk": "with retrieval instructions (FLAREinstruct). It iteratively generates search queries (shown in gray italic) to retrieve relevant information to aid future generations. offer only API access, we elicit such behavior by few-shot prompting (Brown et al., 2020). Specifically, for a downstream task, we place the search-related instruction and exemplars at the beginning as skill 1, followed by the instruction and exemplars of the downstream task as skill 2. Given a test case, we ask LMs to combine skills 1 and 2 to generate search queries while performing the task. The structure of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3. Prompt 3.1: retrieval instructions Skill 1. An instruction to guide LMs to generate search queries. Several search-related exemplars. Skill 2. An instruction to guide LMs to perform a specific downstream task (e.g., multihop QA). Several task-related exemplars. An instruction to guide LMs to combine skills 1 and 2 for the test case. The input of the test case. As shown in Figure 2, when the LM generates \u201c[Search(query)]\u201d (shown in gray italic), we stop the generation and use the query terms to retrieve relevant documents, which are prepended before the user input to aid future generation until the next search query is generated or reaches the end. Additional implementation details are included in Appendix A. 3.2 Direct FLARE Since we cannot fine-tune black-box LMs, we found queries generated by FLAREinstruct through retrieval instructions might not be reliable. There- fore, we propose a more direct way of forward- looking active retrieval that uses the next sentence to decide when and what to retrieve. 3.2.1 Confidence-based Active Retrieval As shown in Figure 1, at step t, we first generate a temporary next sentence \u02c6st = LM([x, y<t]) with- out conditioning on retrieved documents. Then we decide whether to trigger retrieval and formulate queries based on \u02c6st. If the LM is confident about \u02c6st, we accept it without retrieving additional informa- tion; if not, we use \u02c6st to formulate search queries qt to retrieve relevant documents, and then regen- erate the next sentence st. The reason we utilize sentences as the basis of our iteration is due to their significance as semantic units that are neither too short nor too lengthy like phrases and paragraphs. However, our approach can also utilize phrases or paragraphs as the basis. Since LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022), we actively trigger retrieval if any token of \u02c6st has a probability lower than a threshold \u03b8 \u2208 [0, 1]. \u03b8 = 0 means retrieval is never triggered, while \u03b8 = 1 triggers retrieval every sentence. yt = (cid:40) \u02c6st st = LM([Dqt, x, y<t]) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise where the query qt is formulated based on \u02c6st. 3.2.2 Confidence-based Query Formulation One way to perform retrieval is to directly use the next sentence \u02c6st as the query qt. This shares a sim- ilar spirit with methods that use generated hypo- thetical titles or paragraphs from LMs as retrieval queries or evidences (Gao et al., 2022; Sun et al., 2022; Yu et al., 2022; Mao et al., 2021). We gen- eralize such techniques to long-form generation where active information access is essential. We found retrieving with the next sentence achieves significantly better results than with the previous context, as shown later in subsection 6.2. However, it has a risk of perpetuating errors con- tained in it. For example, if the LM produces the sentence \u201cJoe Biden attended the University of Pennsylvania\u201d instead of the correct fact that he attended the University of Delaware, using this er- roneous sentence as a query might retrieve mislead- What university did Joe Biden attend?What degree did Joe Biden earn? .Ask a question to which the answer is \u201cthe University of Pennsylvania\u201dAsk a question to which the answer is \u201ca law degree\u201d , where he earned a law degree Joe Biden attended the University of Pennsylvania implicit query by maskingexplicit query by question generation Joe Biden attended , where he earned .LM such as ChatGPT Figure 3: Implicit and explicit query formulation. To- kens with low probabilities are marked with underlines. ing information. We propose two simple methods to overcome this issue as illustrated in Figure 3. Masked sentences as implicit queries. The first method masks out low-confidence tokens in \u02c6st with probabilities below a threshold \u03b2 \u2208 [0, 1], where a higher \u03b2 results in more aggressive masking. This removes potential distractions from the sentence to improve retrieval accuracy. Generated questions as explicit queries. An- other method is to generate explicit questions that target the low-confident span in \u02c6st. For example, if the LM is uncertain about \u201cthe University of Penn- sylvania\u201d, a question like \u201cWhich university did Joe Biden attend?\u201d can help retrieve relevant in- formation. Self-ask (Press et al., 2022) achieved this by manually inserting follow-up questions into downstream task exemplars as shown later in Prompt D.2, which requires task-specific annota- tion efforts. Instead, we developed a universal ap- proach that generates questions for low-confidence spans without additional annotation. Specifically, We first extract all spans from \u02c6st with probabilities below \u03b2. For each extracted span z, we prompt gpt-3.5-turbo to generate a question qt,z that can be answered with the span: Prompt 3.2: zero-shot question generation User input x. Generated output so far y\u2264t. Given the above passage, ask a question to which the answer is the term/entity/phrase \u201cz\u201d. We retrieve using each generated question and interleave the returned documents into a single ranking list to aid future generations. In summary, queries qt are formulated based on \u02c6st as follows: qt = (cid:40) \u2205 mask(\u02c6st) or qgen(\u02c6st) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise 3.3 Base LM We validate our method on one of the most advanced GPT-3.5 LMs text-davinci-003 by iteratively querying their API.2 Implementation Details Document corpus and retrievers. Since we fo- cus"}, {"question": " Why are sentences used as the basis of iteration in the text?", "answer": " Due to their significance as semantic units that are neither too short nor too lengthy", "ref_chunk": "with retrieval instructions (FLAREinstruct). It iteratively generates search queries (shown in gray italic) to retrieve relevant information to aid future generations. offer only API access, we elicit such behavior by few-shot prompting (Brown et al., 2020). Specifically, for a downstream task, we place the search-related instruction and exemplars at the beginning as skill 1, followed by the instruction and exemplars of the downstream task as skill 2. Given a test case, we ask LMs to combine skills 1 and 2 to generate search queries while performing the task. The structure of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3. Prompt 3.1: retrieval instructions Skill 1. An instruction to guide LMs to generate search queries. Several search-related exemplars. Skill 2. An instruction to guide LMs to perform a specific downstream task (e.g., multihop QA). Several task-related exemplars. An instruction to guide LMs to combine skills 1 and 2 for the test case. The input of the test case. As shown in Figure 2, when the LM generates \u201c[Search(query)]\u201d (shown in gray italic), we stop the generation and use the query terms to retrieve relevant documents, which are prepended before the user input to aid future generation until the next search query is generated or reaches the end. Additional implementation details are included in Appendix A. 3.2 Direct FLARE Since we cannot fine-tune black-box LMs, we found queries generated by FLAREinstruct through retrieval instructions might not be reliable. There- fore, we propose a more direct way of forward- looking active retrieval that uses the next sentence to decide when and what to retrieve. 3.2.1 Confidence-based Active Retrieval As shown in Figure 1, at step t, we first generate a temporary next sentence \u02c6st = LM([x, y<t]) with- out conditioning on retrieved documents. Then we decide whether to trigger retrieval and formulate queries based on \u02c6st. If the LM is confident about \u02c6st, we accept it without retrieving additional informa- tion; if not, we use \u02c6st to formulate search queries qt to retrieve relevant documents, and then regen- erate the next sentence st. The reason we utilize sentences as the basis of our iteration is due to their significance as semantic units that are neither too short nor too lengthy like phrases and paragraphs. However, our approach can also utilize phrases or paragraphs as the basis. Since LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022), we actively trigger retrieval if any token of \u02c6st has a probability lower than a threshold \u03b8 \u2208 [0, 1]. \u03b8 = 0 means retrieval is never triggered, while \u03b8 = 1 triggers retrieval every sentence. yt = (cid:40) \u02c6st st = LM([Dqt, x, y<t]) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise where the query qt is formulated based on \u02c6st. 3.2.2 Confidence-based Query Formulation One way to perform retrieval is to directly use the next sentence \u02c6st as the query qt. This shares a sim- ilar spirit with methods that use generated hypo- thetical titles or paragraphs from LMs as retrieval queries or evidences (Gao et al., 2022; Sun et al., 2022; Yu et al., 2022; Mao et al., 2021). We gen- eralize such techniques to long-form generation where active information access is essential. We found retrieving with the next sentence achieves significantly better results than with the previous context, as shown later in subsection 6.2. However, it has a risk of perpetuating errors con- tained in it. For example, if the LM produces the sentence \u201cJoe Biden attended the University of Pennsylvania\u201d instead of the correct fact that he attended the University of Delaware, using this er- roneous sentence as a query might retrieve mislead- What university did Joe Biden attend?What degree did Joe Biden earn? .Ask a question to which the answer is \u201cthe University of Pennsylvania\u201dAsk a question to which the answer is \u201ca law degree\u201d , where he earned a law degree Joe Biden attended the University of Pennsylvania implicit query by maskingexplicit query by question generation Joe Biden attended , where he earned .LM such as ChatGPT Figure 3: Implicit and explicit query formulation. To- kens with low probabilities are marked with underlines. ing information. We propose two simple methods to overcome this issue as illustrated in Figure 3. Masked sentences as implicit queries. The first method masks out low-confidence tokens in \u02c6st with probabilities below a threshold \u03b2 \u2208 [0, 1], where a higher \u03b2 results in more aggressive masking. This removes potential distractions from the sentence to improve retrieval accuracy. Generated questions as explicit queries. An- other method is to generate explicit questions that target the low-confident span in \u02c6st. For example, if the LM is uncertain about \u201cthe University of Penn- sylvania\u201d, a question like \u201cWhich university did Joe Biden attend?\u201d can help retrieve relevant in- formation. Self-ask (Press et al., 2022) achieved this by manually inserting follow-up questions into downstream task exemplars as shown later in Prompt D.2, which requires task-specific annota- tion efforts. Instead, we developed a universal ap- proach that generates questions for low-confidence spans without additional annotation. Specifically, We first extract all spans from \u02c6st with probabilities below \u03b2. For each extracted span z, we prompt gpt-3.5-turbo to generate a question qt,z that can be answered with the span: Prompt 3.2: zero-shot question generation User input x. Generated output so far y\u2264t. Given the above passage, ask a question to which the answer is the term/entity/phrase \u201cz\u201d. We retrieve using each generated question and interleave the returned documents into a single ranking list to aid future generations. In summary, queries qt are formulated based on \u02c6st as follows: qt = (cid:40) \u2205 mask(\u02c6st) or qgen(\u02c6st) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise 3.3 Base LM We validate our method on one of the most advanced GPT-3.5 LMs text-davinci-003 by iteratively querying their API.2 Implementation Details Document corpus and retrievers. Since we fo- cus"}, {"question": " What is the purpose of utilizing phrases or paragraphs as the basis of iteration in the text?", "answer": " To trigger retrieval if any token of the sentence has a probability lower than a threshold", "ref_chunk": "with retrieval instructions (FLAREinstruct). It iteratively generates search queries (shown in gray italic) to retrieve relevant information to aid future generations. offer only API access, we elicit such behavior by few-shot prompting (Brown et al., 2020). Specifically, for a downstream task, we place the search-related instruction and exemplars at the beginning as skill 1, followed by the instruction and exemplars of the downstream task as skill 2. Given a test case, we ask LMs to combine skills 1 and 2 to generate search queries while performing the task. The structure of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3. Prompt 3.1: retrieval instructions Skill 1. An instruction to guide LMs to generate search queries. Several search-related exemplars. Skill 2. An instruction to guide LMs to perform a specific downstream task (e.g., multihop QA). Several task-related exemplars. An instruction to guide LMs to combine skills 1 and 2 for the test case. The input of the test case. As shown in Figure 2, when the LM generates \u201c[Search(query)]\u201d (shown in gray italic), we stop the generation and use the query terms to retrieve relevant documents, which are prepended before the user input to aid future generation until the next search query is generated or reaches the end. Additional implementation details are included in Appendix A. 3.2 Direct FLARE Since we cannot fine-tune black-box LMs, we found queries generated by FLAREinstruct through retrieval instructions might not be reliable. There- fore, we propose a more direct way of forward- looking active retrieval that uses the next sentence to decide when and what to retrieve. 3.2.1 Confidence-based Active Retrieval As shown in Figure 1, at step t, we first generate a temporary next sentence \u02c6st = LM([x, y<t]) with- out conditioning on retrieved documents. Then we decide whether to trigger retrieval and formulate queries based on \u02c6st. If the LM is confident about \u02c6st, we accept it without retrieving additional informa- tion; if not, we use \u02c6st to formulate search queries qt to retrieve relevant documents, and then regen- erate the next sentence st. The reason we utilize sentences as the basis of our iteration is due to their significance as semantic units that are neither too short nor too lengthy like phrases and paragraphs. However, our approach can also utilize phrases or paragraphs as the basis. Since LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022), we actively trigger retrieval if any token of \u02c6st has a probability lower than a threshold \u03b8 \u2208 [0, 1]. \u03b8 = 0 means retrieval is never triggered, while \u03b8 = 1 triggers retrieval every sentence. yt = (cid:40) \u02c6st st = LM([Dqt, x, y<t]) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise where the query qt is formulated based on \u02c6st. 3.2.2 Confidence-based Query Formulation One way to perform retrieval is to directly use the next sentence \u02c6st as the query qt. This shares a sim- ilar spirit with methods that use generated hypo- thetical titles or paragraphs from LMs as retrieval queries or evidences (Gao et al., 2022; Sun et al., 2022; Yu et al., 2022; Mao et al., 2021). We gen- eralize such techniques to long-form generation where active information access is essential. We found retrieving with the next sentence achieves significantly better results than with the previous context, as shown later in subsection 6.2. However, it has a risk of perpetuating errors con- tained in it. For example, if the LM produces the sentence \u201cJoe Biden attended the University of Pennsylvania\u201d instead of the correct fact that he attended the University of Delaware, using this er- roneous sentence as a query might retrieve mislead- What university did Joe Biden attend?What degree did Joe Biden earn? .Ask a question to which the answer is \u201cthe University of Pennsylvania\u201dAsk a question to which the answer is \u201ca law degree\u201d , where he earned a law degree Joe Biden attended the University of Pennsylvania implicit query by maskingexplicit query by question generation Joe Biden attended , where he earned .LM such as ChatGPT Figure 3: Implicit and explicit query formulation. To- kens with low probabilities are marked with underlines. ing information. We propose two simple methods to overcome this issue as illustrated in Figure 3. Masked sentences as implicit queries. The first method masks out low-confidence tokens in \u02c6st with probabilities below a threshold \u03b2 \u2208 [0, 1], where a higher \u03b2 results in more aggressive masking. This removes potential distractions from the sentence to improve retrieval accuracy. Generated questions as explicit queries. An- other method is to generate explicit questions that target the low-confident span in \u02c6st. For example, if the LM is uncertain about \u201cthe University of Penn- sylvania\u201d, a question like \u201cWhich university did Joe Biden attend?\u201d can help retrieve relevant in- formation. Self-ask (Press et al., 2022) achieved this by manually inserting follow-up questions into downstream task exemplars as shown later in Prompt D.2, which requires task-specific annota- tion efforts. Instead, we developed a universal ap- proach that generates questions for low-confidence spans without additional annotation. Specifically, We first extract all spans from \u02c6st with probabilities below \u03b2. For each extracted span z, we prompt gpt-3.5-turbo to generate a question qt,z that can be answered with the span: Prompt 3.2: zero-shot question generation User input x. Generated output so far y\u2264t. Given the above passage, ask a question to which the answer is the term/entity/phrase \u201cz\u201d. We retrieve using each generated question and interleave the returned documents into a single ranking list to aid future generations. In summary, queries qt are formulated based on \u02c6st as follows: qt = (cid:40) \u2205 mask(\u02c6st) or qgen(\u02c6st) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise 3.3 Base LM We validate our method on one of the most advanced GPT-3.5 LMs text-davinci-003 by iteratively querying their API.2 Implementation Details Document corpus and retrievers. Since we fo- cus"}, {"question": " What is the significance of low probability/confidence in Language Models according to the text?", "answer": " It often indicates a lack of knowledge", "ref_chunk": "with retrieval instructions (FLAREinstruct). It iteratively generates search queries (shown in gray italic) to retrieve relevant information to aid future generations. offer only API access, we elicit such behavior by few-shot prompting (Brown et al., 2020). Specifically, for a downstream task, we place the search-related instruction and exemplars at the beginning as skill 1, followed by the instruction and exemplars of the downstream task as skill 2. Given a test case, we ask LMs to combine skills 1 and 2 to generate search queries while performing the task. The structure of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3. Prompt 3.1: retrieval instructions Skill 1. An instruction to guide LMs to generate search queries. Several search-related exemplars. Skill 2. An instruction to guide LMs to perform a specific downstream task (e.g., multihop QA). Several task-related exemplars. An instruction to guide LMs to combine skills 1 and 2 for the test case. The input of the test case. As shown in Figure 2, when the LM generates \u201c[Search(query)]\u201d (shown in gray italic), we stop the generation and use the query terms to retrieve relevant documents, which are prepended before the user input to aid future generation until the next search query is generated or reaches the end. Additional implementation details are included in Appendix A. 3.2 Direct FLARE Since we cannot fine-tune black-box LMs, we found queries generated by FLAREinstruct through retrieval instructions might not be reliable. There- fore, we propose a more direct way of forward- looking active retrieval that uses the next sentence to decide when and what to retrieve. 3.2.1 Confidence-based Active Retrieval As shown in Figure 1, at step t, we first generate a temporary next sentence \u02c6st = LM([x, y<t]) with- out conditioning on retrieved documents. Then we decide whether to trigger retrieval and formulate queries based on \u02c6st. If the LM is confident about \u02c6st, we accept it without retrieving additional informa- tion; if not, we use \u02c6st to formulate search queries qt to retrieve relevant documents, and then regen- erate the next sentence st. The reason we utilize sentences as the basis of our iteration is due to their significance as semantic units that are neither too short nor too lengthy like phrases and paragraphs. However, our approach can also utilize phrases or paragraphs as the basis. Since LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022), we actively trigger retrieval if any token of \u02c6st has a probability lower than a threshold \u03b8 \u2208 [0, 1]. \u03b8 = 0 means retrieval is never triggered, while \u03b8 = 1 triggers retrieval every sentence. yt = (cid:40) \u02c6st st = LM([Dqt, x, y<t]) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise where the query qt is formulated based on \u02c6st. 3.2.2 Confidence-based Query Formulation One way to perform retrieval is to directly use the next sentence \u02c6st as the query qt. This shares a sim- ilar spirit with methods that use generated hypo- thetical titles or paragraphs from LMs as retrieval queries or evidences (Gao et al., 2022; Sun et al., 2022; Yu et al., 2022; Mao et al., 2021). We gen- eralize such techniques to long-form generation where active information access is essential. We found retrieving with the next sentence achieves significantly better results than with the previous context, as shown later in subsection 6.2. However, it has a risk of perpetuating errors con- tained in it. For example, if the LM produces the sentence \u201cJoe Biden attended the University of Pennsylvania\u201d instead of the correct fact that he attended the University of Delaware, using this er- roneous sentence as a query might retrieve mislead- What university did Joe Biden attend?What degree did Joe Biden earn? .Ask a question to which the answer is \u201cthe University of Pennsylvania\u201dAsk a question to which the answer is \u201ca law degree\u201d , where he earned a law degree Joe Biden attended the University of Pennsylvania implicit query by maskingexplicit query by question generation Joe Biden attended , where he earned .LM such as ChatGPT Figure 3: Implicit and explicit query formulation. To- kens with low probabilities are marked with underlines. ing information. We propose two simple methods to overcome this issue as illustrated in Figure 3. Masked sentences as implicit queries. The first method masks out low-confidence tokens in \u02c6st with probabilities below a threshold \u03b2 \u2208 [0, 1], where a higher \u03b2 results in more aggressive masking. This removes potential distractions from the sentence to improve retrieval accuracy. Generated questions as explicit queries. An- other method is to generate explicit questions that target the low-confident span in \u02c6st. For example, if the LM is uncertain about \u201cthe University of Penn- sylvania\u201d, a question like \u201cWhich university did Joe Biden attend?\u201d can help retrieve relevant in- formation. Self-ask (Press et al., 2022) achieved this by manually inserting follow-up questions into downstream task exemplars as shown later in Prompt D.2, which requires task-specific annota- tion efforts. Instead, we developed a universal ap- proach that generates questions for low-confidence spans without additional annotation. Specifically, We first extract all spans from \u02c6st with probabilities below \u03b2. For each extracted span z, we prompt gpt-3.5-turbo to generate a question qt,z that can be answered with the span: Prompt 3.2: zero-shot question generation User input x. Generated output so far y\u2264t. Given the above passage, ask a question to which the answer is the term/entity/phrase \u201cz\u201d. We retrieve using each generated question and interleave the returned documents into a single ranking list to aid future generations. In summary, queries qt are formulated based on \u02c6st as follows: qt = (cid:40) \u2205 mask(\u02c6st) or qgen(\u02c6st) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise 3.3 Base LM We validate our method on one of the most advanced GPT-3.5 LMs text-davinci-003 by iteratively querying their API.2 Implementation Details Document corpus and retrievers. Since we fo- cus"}, {"question": " What method does the text propose to perform retrieval directly using the next sentence as the query?", "answer": " Confidence-based Query Formulation", "ref_chunk": "with retrieval instructions (FLAREinstruct). It iteratively generates search queries (shown in gray italic) to retrieve relevant information to aid future generations. offer only API access, we elicit such behavior by few-shot prompting (Brown et al., 2020). Specifically, for a downstream task, we place the search-related instruction and exemplars at the beginning as skill 1, followed by the instruction and exemplars of the downstream task as skill 2. Given a test case, we ask LMs to combine skills 1 and 2 to generate search queries while performing the task. The structure of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3. Prompt 3.1: retrieval instructions Skill 1. An instruction to guide LMs to generate search queries. Several search-related exemplars. Skill 2. An instruction to guide LMs to perform a specific downstream task (e.g., multihop QA). Several task-related exemplars. An instruction to guide LMs to combine skills 1 and 2 for the test case. The input of the test case. As shown in Figure 2, when the LM generates \u201c[Search(query)]\u201d (shown in gray italic), we stop the generation and use the query terms to retrieve relevant documents, which are prepended before the user input to aid future generation until the next search query is generated or reaches the end. Additional implementation details are included in Appendix A. 3.2 Direct FLARE Since we cannot fine-tune black-box LMs, we found queries generated by FLAREinstruct through retrieval instructions might not be reliable. There- fore, we propose a more direct way of forward- looking active retrieval that uses the next sentence to decide when and what to retrieve. 3.2.1 Confidence-based Active Retrieval As shown in Figure 1, at step t, we first generate a temporary next sentence \u02c6st = LM([x, y<t]) with- out conditioning on retrieved documents. Then we decide whether to trigger retrieval and formulate queries based on \u02c6st. If the LM is confident about \u02c6st, we accept it without retrieving additional informa- tion; if not, we use \u02c6st to formulate search queries qt to retrieve relevant documents, and then regen- erate the next sentence st. The reason we utilize sentences as the basis of our iteration is due to their significance as semantic units that are neither too short nor too lengthy like phrases and paragraphs. However, our approach can also utilize phrases or paragraphs as the basis. Since LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022), we actively trigger retrieval if any token of \u02c6st has a probability lower than a threshold \u03b8 \u2208 [0, 1]. \u03b8 = 0 means retrieval is never triggered, while \u03b8 = 1 triggers retrieval every sentence. yt = (cid:40) \u02c6st st = LM([Dqt, x, y<t]) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise where the query qt is formulated based on \u02c6st. 3.2.2 Confidence-based Query Formulation One way to perform retrieval is to directly use the next sentence \u02c6st as the query qt. This shares a sim- ilar spirit with methods that use generated hypo- thetical titles or paragraphs from LMs as retrieval queries or evidences (Gao et al., 2022; Sun et al., 2022; Yu et al., 2022; Mao et al., 2021). We gen- eralize such techniques to long-form generation where active information access is essential. We found retrieving with the next sentence achieves significantly better results than with the previous context, as shown later in subsection 6.2. However, it has a risk of perpetuating errors con- tained in it. For example, if the LM produces the sentence \u201cJoe Biden attended the University of Pennsylvania\u201d instead of the correct fact that he attended the University of Delaware, using this er- roneous sentence as a query might retrieve mislead- What university did Joe Biden attend?What degree did Joe Biden earn? .Ask a question to which the answer is \u201cthe University of Pennsylvania\u201dAsk a question to which the answer is \u201ca law degree\u201d , where he earned a law degree Joe Biden attended the University of Pennsylvania implicit query by maskingexplicit query by question generation Joe Biden attended , where he earned .LM such as ChatGPT Figure 3: Implicit and explicit query formulation. To- kens with low probabilities are marked with underlines. ing information. We propose two simple methods to overcome this issue as illustrated in Figure 3. Masked sentences as implicit queries. The first method masks out low-confidence tokens in \u02c6st with probabilities below a threshold \u03b2 \u2208 [0, 1], where a higher \u03b2 results in more aggressive masking. This removes potential distractions from the sentence to improve retrieval accuracy. Generated questions as explicit queries. An- other method is to generate explicit questions that target the low-confident span in \u02c6st. For example, if the LM is uncertain about \u201cthe University of Penn- sylvania\u201d, a question like \u201cWhich university did Joe Biden attend?\u201d can help retrieve relevant in- formation. Self-ask (Press et al., 2022) achieved this by manually inserting follow-up questions into downstream task exemplars as shown later in Prompt D.2, which requires task-specific annota- tion efforts. Instead, we developed a universal ap- proach that generates questions for low-confidence spans without additional annotation. Specifically, We first extract all spans from \u02c6st with probabilities below \u03b2. For each extracted span z, we prompt gpt-3.5-turbo to generate a question qt,z that can be answered with the span: Prompt 3.2: zero-shot question generation User input x. Generated output so far y\u2264t. Given the above passage, ask a question to which the answer is the term/entity/phrase \u201cz\u201d. We retrieve using each generated question and interleave the returned documents into a single ranking list to aid future generations. In summary, queries qt are formulated based on \u02c6st as follows: qt = (cid:40) \u2205 mask(\u02c6st) or qgen(\u02c6st) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise 3.3 Base LM We validate our method on one of the most advanced GPT-3.5 LMs text-davinci-003 by iteratively querying their API.2 Implementation Details Document corpus and retrievers. Since we fo- cus"}, {"question": " Why does the text suggest generating explicit questions for low-confidence spans?", "answer": " To help retrieve relevant information and improve retrieval accuracy", "ref_chunk": "with retrieval instructions (FLAREinstruct). It iteratively generates search queries (shown in gray italic) to retrieve relevant information to aid future generations. offer only API access, we elicit such behavior by few-shot prompting (Brown et al., 2020). Specifically, for a downstream task, we place the search-related instruction and exemplars at the beginning as skill 1, followed by the instruction and exemplars of the downstream task as skill 2. Given a test case, we ask LMs to combine skills 1 and 2 to generate search queries while performing the task. The structure of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3. Prompt 3.1: retrieval instructions Skill 1. An instruction to guide LMs to generate search queries. Several search-related exemplars. Skill 2. An instruction to guide LMs to perform a specific downstream task (e.g., multihop QA). Several task-related exemplars. An instruction to guide LMs to combine skills 1 and 2 for the test case. The input of the test case. As shown in Figure 2, when the LM generates \u201c[Search(query)]\u201d (shown in gray italic), we stop the generation and use the query terms to retrieve relevant documents, which are prepended before the user input to aid future generation until the next search query is generated or reaches the end. Additional implementation details are included in Appendix A. 3.2 Direct FLARE Since we cannot fine-tune black-box LMs, we found queries generated by FLAREinstruct through retrieval instructions might not be reliable. There- fore, we propose a more direct way of forward- looking active retrieval that uses the next sentence to decide when and what to retrieve. 3.2.1 Confidence-based Active Retrieval As shown in Figure 1, at step t, we first generate a temporary next sentence \u02c6st = LM([x, y<t]) with- out conditioning on retrieved documents. Then we decide whether to trigger retrieval and formulate queries based on \u02c6st. If the LM is confident about \u02c6st, we accept it without retrieving additional informa- tion; if not, we use \u02c6st to formulate search queries qt to retrieve relevant documents, and then regen- erate the next sentence st. The reason we utilize sentences as the basis of our iteration is due to their significance as semantic units that are neither too short nor too lengthy like phrases and paragraphs. However, our approach can also utilize phrases or paragraphs as the basis. Since LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022), we actively trigger retrieval if any token of \u02c6st has a probability lower than a threshold \u03b8 \u2208 [0, 1]. \u03b8 = 0 means retrieval is never triggered, while \u03b8 = 1 triggers retrieval every sentence. yt = (cid:40) \u02c6st st = LM([Dqt, x, y<t]) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise where the query qt is formulated based on \u02c6st. 3.2.2 Confidence-based Query Formulation One way to perform retrieval is to directly use the next sentence \u02c6st as the query qt. This shares a sim- ilar spirit with methods that use generated hypo- thetical titles or paragraphs from LMs as retrieval queries or evidences (Gao et al., 2022; Sun et al., 2022; Yu et al., 2022; Mao et al., 2021). We gen- eralize such techniques to long-form generation where active information access is essential. We found retrieving with the next sentence achieves significantly better results than with the previous context, as shown later in subsection 6.2. However, it has a risk of perpetuating errors con- tained in it. For example, if the LM produces the sentence \u201cJoe Biden attended the University of Pennsylvania\u201d instead of the correct fact that he attended the University of Delaware, using this er- roneous sentence as a query might retrieve mislead- What university did Joe Biden attend?What degree did Joe Biden earn? .Ask a question to which the answer is \u201cthe University of Pennsylvania\u201dAsk a question to which the answer is \u201ca law degree\u201d , where he earned a law degree Joe Biden attended the University of Pennsylvania implicit query by maskingexplicit query by question generation Joe Biden attended , where he earned .LM such as ChatGPT Figure 3: Implicit and explicit query formulation. To- kens with low probabilities are marked with underlines. ing information. We propose two simple methods to overcome this issue as illustrated in Figure 3. Masked sentences as implicit queries. The first method masks out low-confidence tokens in \u02c6st with probabilities below a threshold \u03b2 \u2208 [0, 1], where a higher \u03b2 results in more aggressive masking. This removes potential distractions from the sentence to improve retrieval accuracy. Generated questions as explicit queries. An- other method is to generate explicit questions that target the low-confident span in \u02c6st. For example, if the LM is uncertain about \u201cthe University of Penn- sylvania\u201d, a question like \u201cWhich university did Joe Biden attend?\u201d can help retrieve relevant in- formation. Self-ask (Press et al., 2022) achieved this by manually inserting follow-up questions into downstream task exemplars as shown later in Prompt D.2, which requires task-specific annota- tion efforts. Instead, we developed a universal ap- proach that generates questions for low-confidence spans without additional annotation. Specifically, We first extract all spans from \u02c6st with probabilities below \u03b2. For each extracted span z, we prompt gpt-3.5-turbo to generate a question qt,z that can be answered with the span: Prompt 3.2: zero-shot question generation User input x. Generated output so far y\u2264t. Given the above passage, ask a question to which the answer is the term/entity/phrase \u201cz\u201d. We retrieve using each generated question and interleave the returned documents into a single ranking list to aid future generations. In summary, queries qt are formulated based on \u02c6st as follows: qt = (cid:40) \u2205 mask(\u02c6st) or qgen(\u02c6st) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise 3.3 Base LM We validate our method on one of the most advanced GPT-3.5 LMs text-davinci-003 by iteratively querying their API.2 Implementation Details Document corpus and retrievers. Since we fo- cus"}, {"question": " What is the purpose of the first method described to overcome errors in the text?", "answer": " To remove potential distractions from the sentence to improve retrieval accuracy", "ref_chunk": "with retrieval instructions (FLAREinstruct). It iteratively generates search queries (shown in gray italic) to retrieve relevant information to aid future generations. offer only API access, we elicit such behavior by few-shot prompting (Brown et al., 2020). Specifically, for a downstream task, we place the search-related instruction and exemplars at the beginning as skill 1, followed by the instruction and exemplars of the downstream task as skill 2. Given a test case, we ask LMs to combine skills 1 and 2 to generate search queries while performing the task. The structure of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3. Prompt 3.1: retrieval instructions Skill 1. An instruction to guide LMs to generate search queries. Several search-related exemplars. Skill 2. An instruction to guide LMs to perform a specific downstream task (e.g., multihop QA). Several task-related exemplars. An instruction to guide LMs to combine skills 1 and 2 for the test case. The input of the test case. As shown in Figure 2, when the LM generates \u201c[Search(query)]\u201d (shown in gray italic), we stop the generation and use the query terms to retrieve relevant documents, which are prepended before the user input to aid future generation until the next search query is generated or reaches the end. Additional implementation details are included in Appendix A. 3.2 Direct FLARE Since we cannot fine-tune black-box LMs, we found queries generated by FLAREinstruct through retrieval instructions might not be reliable. There- fore, we propose a more direct way of forward- looking active retrieval that uses the next sentence to decide when and what to retrieve. 3.2.1 Confidence-based Active Retrieval As shown in Figure 1, at step t, we first generate a temporary next sentence \u02c6st = LM([x, y<t]) with- out conditioning on retrieved documents. Then we decide whether to trigger retrieval and formulate queries based on \u02c6st. If the LM is confident about \u02c6st, we accept it without retrieving additional informa- tion; if not, we use \u02c6st to formulate search queries qt to retrieve relevant documents, and then regen- erate the next sentence st. The reason we utilize sentences as the basis of our iteration is due to their significance as semantic units that are neither too short nor too lengthy like phrases and paragraphs. However, our approach can also utilize phrases or paragraphs as the basis. Since LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022), we actively trigger retrieval if any token of \u02c6st has a probability lower than a threshold \u03b8 \u2208 [0, 1]. \u03b8 = 0 means retrieval is never triggered, while \u03b8 = 1 triggers retrieval every sentence. yt = (cid:40) \u02c6st st = LM([Dqt, x, y<t]) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise where the query qt is formulated based on \u02c6st. 3.2.2 Confidence-based Query Formulation One way to perform retrieval is to directly use the next sentence \u02c6st as the query qt. This shares a sim- ilar spirit with methods that use generated hypo- thetical titles or paragraphs from LMs as retrieval queries or evidences (Gao et al., 2022; Sun et al., 2022; Yu et al., 2022; Mao et al., 2021). We gen- eralize such techniques to long-form generation where active information access is essential. We found retrieving with the next sentence achieves significantly better results than with the previous context, as shown later in subsection 6.2. However, it has a risk of perpetuating errors con- tained in it. For example, if the LM produces the sentence \u201cJoe Biden attended the University of Pennsylvania\u201d instead of the correct fact that he attended the University of Delaware, using this er- roneous sentence as a query might retrieve mislead- What university did Joe Biden attend?What degree did Joe Biden earn? .Ask a question to which the answer is \u201cthe University of Pennsylvania\u201dAsk a question to which the answer is \u201ca law degree\u201d , where he earned a law degree Joe Biden attended the University of Pennsylvania implicit query by maskingexplicit query by question generation Joe Biden attended , where he earned .LM such as ChatGPT Figure 3: Implicit and explicit query formulation. To- kens with low probabilities are marked with underlines. ing information. We propose two simple methods to overcome this issue as illustrated in Figure 3. Masked sentences as implicit queries. The first method masks out low-confidence tokens in \u02c6st with probabilities below a threshold \u03b2 \u2208 [0, 1], where a higher \u03b2 results in more aggressive masking. This removes potential distractions from the sentence to improve retrieval accuracy. Generated questions as explicit queries. An- other method is to generate explicit questions that target the low-confident span in \u02c6st. For example, if the LM is uncertain about \u201cthe University of Penn- sylvania\u201d, a question like \u201cWhich university did Joe Biden attend?\u201d can help retrieve relevant in- formation. Self-ask (Press et al., 2022) achieved this by manually inserting follow-up questions into downstream task exemplars as shown later in Prompt D.2, which requires task-specific annota- tion efforts. Instead, we developed a universal ap- proach that generates questions for low-confidence spans without additional annotation. Specifically, We first extract all spans from \u02c6st with probabilities below \u03b2. For each extracted span z, we prompt gpt-3.5-turbo to generate a question qt,z that can be answered with the span: Prompt 3.2: zero-shot question generation User input x. Generated output so far y\u2264t. Given the above passage, ask a question to which the answer is the term/entity/phrase \u201cz\u201d. We retrieve using each generated question and interleave the returned documents into a single ranking list to aid future generations. In summary, queries qt are formulated based on \u02c6st as follows: qt = (cid:40) \u2205 mask(\u02c6st) or qgen(\u02c6st) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise 3.3 Base LM We validate our method on one of the most advanced GPT-3.5 LMs text-davinci-003 by iteratively querying their API.2 Implementation Details Document corpus and retrievers. Since we fo- cus"}, {"question": " What does the second method described to overcome errors involve in the text?", "answer": " Generating explicit questions that target the low-confident span", "ref_chunk": "with retrieval instructions (FLAREinstruct). It iteratively generates search queries (shown in gray italic) to retrieve relevant information to aid future generations. offer only API access, we elicit such behavior by few-shot prompting (Brown et al., 2020). Specifically, for a downstream task, we place the search-related instruction and exemplars at the beginning as skill 1, followed by the instruction and exemplars of the downstream task as skill 2. Given a test case, we ask LMs to combine skills 1 and 2 to generate search queries while performing the task. The structure of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3. Prompt 3.1: retrieval instructions Skill 1. An instruction to guide LMs to generate search queries. Several search-related exemplars. Skill 2. An instruction to guide LMs to perform a specific downstream task (e.g., multihop QA). Several task-related exemplars. An instruction to guide LMs to combine skills 1 and 2 for the test case. The input of the test case. As shown in Figure 2, when the LM generates \u201c[Search(query)]\u201d (shown in gray italic), we stop the generation and use the query terms to retrieve relevant documents, which are prepended before the user input to aid future generation until the next search query is generated or reaches the end. Additional implementation details are included in Appendix A. 3.2 Direct FLARE Since we cannot fine-tune black-box LMs, we found queries generated by FLAREinstruct through retrieval instructions might not be reliable. There- fore, we propose a more direct way of forward- looking active retrieval that uses the next sentence to decide when and what to retrieve. 3.2.1 Confidence-based Active Retrieval As shown in Figure 1, at step t, we first generate a temporary next sentence \u02c6st = LM([x, y<t]) with- out conditioning on retrieved documents. Then we decide whether to trigger retrieval and formulate queries based on \u02c6st. If the LM is confident about \u02c6st, we accept it without retrieving additional informa- tion; if not, we use \u02c6st to formulate search queries qt to retrieve relevant documents, and then regen- erate the next sentence st. The reason we utilize sentences as the basis of our iteration is due to their significance as semantic units that are neither too short nor too lengthy like phrases and paragraphs. However, our approach can also utilize phrases or paragraphs as the basis. Since LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022), we actively trigger retrieval if any token of \u02c6st has a probability lower than a threshold \u03b8 \u2208 [0, 1]. \u03b8 = 0 means retrieval is never triggered, while \u03b8 = 1 triggers retrieval every sentence. yt = (cid:40) \u02c6st st = LM([Dqt, x, y<t]) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise where the query qt is formulated based on \u02c6st. 3.2.2 Confidence-based Query Formulation One way to perform retrieval is to directly use the next sentence \u02c6st as the query qt. This shares a sim- ilar spirit with methods that use generated hypo- thetical titles or paragraphs from LMs as retrieval queries or evidences (Gao et al., 2022; Sun et al., 2022; Yu et al., 2022; Mao et al., 2021). We gen- eralize such techniques to long-form generation where active information access is essential. We found retrieving with the next sentence achieves significantly better results than with the previous context, as shown later in subsection 6.2. However, it has a risk of perpetuating errors con- tained in it. For example, if the LM produces the sentence \u201cJoe Biden attended the University of Pennsylvania\u201d instead of the correct fact that he attended the University of Delaware, using this er- roneous sentence as a query might retrieve mislead- What university did Joe Biden attend?What degree did Joe Biden earn? .Ask a question to which the answer is \u201cthe University of Pennsylvania\u201dAsk a question to which the answer is \u201ca law degree\u201d , where he earned a law degree Joe Biden attended the University of Pennsylvania implicit query by maskingexplicit query by question generation Joe Biden attended , where he earned .LM such as ChatGPT Figure 3: Implicit and explicit query formulation. To- kens with low probabilities are marked with underlines. ing information. We propose two simple methods to overcome this issue as illustrated in Figure 3. Masked sentences as implicit queries. The first method masks out low-confidence tokens in \u02c6st with probabilities below a threshold \u03b2 \u2208 [0, 1], where a higher \u03b2 results in more aggressive masking. This removes potential distractions from the sentence to improve retrieval accuracy. Generated questions as explicit queries. An- other method is to generate explicit questions that target the low-confident span in \u02c6st. For example, if the LM is uncertain about \u201cthe University of Penn- sylvania\u201d, a question like \u201cWhich university did Joe Biden attend?\u201d can help retrieve relevant in- formation. Self-ask (Press et al., 2022) achieved this by manually inserting follow-up questions into downstream task exemplars as shown later in Prompt D.2, which requires task-specific annota- tion efforts. Instead, we developed a universal ap- proach that generates questions for low-confidence spans without additional annotation. Specifically, We first extract all spans from \u02c6st with probabilities below \u03b2. For each extracted span z, we prompt gpt-3.5-turbo to generate a question qt,z that can be answered with the span: Prompt 3.2: zero-shot question generation User input x. Generated output so far y\u2264t. Given the above passage, ask a question to which the answer is the term/entity/phrase \u201cz\u201d. We retrieve using each generated question and interleave the returned documents into a single ranking list to aid future generations. In summary, queries qt are formulated based on \u02c6st as follows: qt = (cid:40) \u2205 mask(\u02c6st) or qgen(\u02c6st) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise 3.3 Base LM We validate our method on one of the most advanced GPT-3.5 LMs text-davinci-003 by iteratively querying their API.2 Implementation Details Document corpus and retrievers. Since we fo- cus"}, {"question": " Which LM was used for the validation method described in the text?", "answer": " GPT-3.5 LMs text-davinci-003", "ref_chunk": "with retrieval instructions (FLAREinstruct). It iteratively generates search queries (shown in gray italic) to retrieve relevant information to aid future generations. offer only API access, we elicit such behavior by few-shot prompting (Brown et al., 2020). Specifically, for a downstream task, we place the search-related instruction and exemplars at the beginning as skill 1, followed by the instruction and exemplars of the downstream task as skill 2. Given a test case, we ask LMs to combine skills 1 and 2 to generate search queries while performing the task. The structure of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3. Prompt 3.1: retrieval instructions Skill 1. An instruction to guide LMs to generate search queries. Several search-related exemplars. Skill 2. An instruction to guide LMs to perform a specific downstream task (e.g., multihop QA). Several task-related exemplars. An instruction to guide LMs to combine skills 1 and 2 for the test case. The input of the test case. As shown in Figure 2, when the LM generates \u201c[Search(query)]\u201d (shown in gray italic), we stop the generation and use the query terms to retrieve relevant documents, which are prepended before the user input to aid future generation until the next search query is generated or reaches the end. Additional implementation details are included in Appendix A. 3.2 Direct FLARE Since we cannot fine-tune black-box LMs, we found queries generated by FLAREinstruct through retrieval instructions might not be reliable. There- fore, we propose a more direct way of forward- looking active retrieval that uses the next sentence to decide when and what to retrieve. 3.2.1 Confidence-based Active Retrieval As shown in Figure 1, at step t, we first generate a temporary next sentence \u02c6st = LM([x, y<t]) with- out conditioning on retrieved documents. Then we decide whether to trigger retrieval and formulate queries based on \u02c6st. If the LM is confident about \u02c6st, we accept it without retrieving additional informa- tion; if not, we use \u02c6st to formulate search queries qt to retrieve relevant documents, and then regen- erate the next sentence st. The reason we utilize sentences as the basis of our iteration is due to their significance as semantic units that are neither too short nor too lengthy like phrases and paragraphs. However, our approach can also utilize phrases or paragraphs as the basis. Since LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022), we actively trigger retrieval if any token of \u02c6st has a probability lower than a threshold \u03b8 \u2208 [0, 1]. \u03b8 = 0 means retrieval is never triggered, while \u03b8 = 1 triggers retrieval every sentence. yt = (cid:40) \u02c6st st = LM([Dqt, x, y<t]) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise where the query qt is formulated based on \u02c6st. 3.2.2 Confidence-based Query Formulation One way to perform retrieval is to directly use the next sentence \u02c6st as the query qt. This shares a sim- ilar spirit with methods that use generated hypo- thetical titles or paragraphs from LMs as retrieval queries or evidences (Gao et al., 2022; Sun et al., 2022; Yu et al., 2022; Mao et al., 2021). We gen- eralize such techniques to long-form generation where active information access is essential. We found retrieving with the next sentence achieves significantly better results than with the previous context, as shown later in subsection 6.2. However, it has a risk of perpetuating errors con- tained in it. For example, if the LM produces the sentence \u201cJoe Biden attended the University of Pennsylvania\u201d instead of the correct fact that he attended the University of Delaware, using this er- roneous sentence as a query might retrieve mislead- What university did Joe Biden attend?What degree did Joe Biden earn? .Ask a question to which the answer is \u201cthe University of Pennsylvania\u201dAsk a question to which the answer is \u201ca law degree\u201d , where he earned a law degree Joe Biden attended the University of Pennsylvania implicit query by maskingexplicit query by question generation Joe Biden attended , where he earned .LM such as ChatGPT Figure 3: Implicit and explicit query formulation. To- kens with low probabilities are marked with underlines. ing information. We propose two simple methods to overcome this issue as illustrated in Figure 3. Masked sentences as implicit queries. The first method masks out low-confidence tokens in \u02c6st with probabilities below a threshold \u03b2 \u2208 [0, 1], where a higher \u03b2 results in more aggressive masking. This removes potential distractions from the sentence to improve retrieval accuracy. Generated questions as explicit queries. An- other method is to generate explicit questions that target the low-confident span in \u02c6st. For example, if the LM is uncertain about \u201cthe University of Penn- sylvania\u201d, a question like \u201cWhich university did Joe Biden attend?\u201d can help retrieve relevant in- formation. Self-ask (Press et al., 2022) achieved this by manually inserting follow-up questions into downstream task exemplars as shown later in Prompt D.2, which requires task-specific annota- tion efforts. Instead, we developed a universal ap- proach that generates questions for low-confidence spans without additional annotation. Specifically, We first extract all spans from \u02c6st with probabilities below \u03b2. For each extracted span z, we prompt gpt-3.5-turbo to generate a question qt,z that can be answered with the span: Prompt 3.2: zero-shot question generation User input x. Generated output so far y\u2264t. Given the above passage, ask a question to which the answer is the term/entity/phrase \u201cz\u201d. We retrieve using each generated question and interleave the returned documents into a single ranking list to aid future generations. In summary, queries qt are formulated based on \u02c6st as follows: qt = (cid:40) \u2205 mask(\u02c6st) or qgen(\u02c6st) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise 3.3 Base LM We validate our method on one of the most advanced GPT-3.5 LMs text-davinci-003 by iteratively querying their API.2 Implementation Details Document corpus and retrievers. Since we fo- cus"}], "doc_text": "with retrieval instructions (FLAREinstruct). It iteratively generates search queries (shown in gray italic) to retrieve relevant information to aid future generations. offer only API access, we elicit such behavior by few-shot prompting (Brown et al., 2020). Specifically, for a downstream task, we place the search-related instruction and exemplars at the beginning as skill 1, followed by the instruction and exemplars of the downstream task as skill 2. Given a test case, we ask LMs to combine skills 1 and 2 to generate search queries while performing the task. The structure of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3. Prompt 3.1: retrieval instructions Skill 1. An instruction to guide LMs to generate search queries. Several search-related exemplars. Skill 2. An instruction to guide LMs to perform a specific downstream task (e.g., multihop QA). Several task-related exemplars. An instruction to guide LMs to combine skills 1 and 2 for the test case. The input of the test case. As shown in Figure 2, when the LM generates \u201c[Search(query)]\u201d (shown in gray italic), we stop the generation and use the query terms to retrieve relevant documents, which are prepended before the user input to aid future generation until the next search query is generated or reaches the end. Additional implementation details are included in Appendix A. 3.2 Direct FLARE Since we cannot fine-tune black-box LMs, we found queries generated by FLAREinstruct through retrieval instructions might not be reliable. There- fore, we propose a more direct way of forward- looking active retrieval that uses the next sentence to decide when and what to retrieve. 3.2.1 Confidence-based Active Retrieval As shown in Figure 1, at step t, we first generate a temporary next sentence \u02c6st = LM([x, y<t]) with- out conditioning on retrieved documents. Then we decide whether to trigger retrieval and formulate queries based on \u02c6st. If the LM is confident about \u02c6st, we accept it without retrieving additional informa- tion; if not, we use \u02c6st to formulate search queries qt to retrieve relevant documents, and then regen- erate the next sentence st. The reason we utilize sentences as the basis of our iteration is due to their significance as semantic units that are neither too short nor too lengthy like phrases and paragraphs. However, our approach can also utilize phrases or paragraphs as the basis. Since LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022), we actively trigger retrieval if any token of \u02c6st has a probability lower than a threshold \u03b8 \u2208 [0, 1]. \u03b8 = 0 means retrieval is never triggered, while \u03b8 = 1 triggers retrieval every sentence. yt = (cid:40) \u02c6st st = LM([Dqt, x, y<t]) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise where the query qt is formulated based on \u02c6st. 3.2.2 Confidence-based Query Formulation One way to perform retrieval is to directly use the next sentence \u02c6st as the query qt. This shares a sim- ilar spirit with methods that use generated hypo- thetical titles or paragraphs from LMs as retrieval queries or evidences (Gao et al., 2022; Sun et al., 2022; Yu et al., 2022; Mao et al., 2021). We gen- eralize such techniques to long-form generation where active information access is essential. We found retrieving with the next sentence achieves significantly better results than with the previous context, as shown later in subsection 6.2. However, it has a risk of perpetuating errors con- tained in it. For example, if the LM produces the sentence \u201cJoe Biden attended the University of Pennsylvania\u201d instead of the correct fact that he attended the University of Delaware, using this er- roneous sentence as a query might retrieve mislead- What university did Joe Biden attend?What degree did Joe Biden earn? .Ask a question to which the answer is \u201cthe University of Pennsylvania\u201dAsk a question to which the answer is \u201ca law degree\u201d , where he earned a law degree Joe Biden attended the University of Pennsylvania implicit query by maskingexplicit query by question generation Joe Biden attended , where he earned .LM such as ChatGPT Figure 3: Implicit and explicit query formulation. To- kens with low probabilities are marked with underlines. ing information. We propose two simple methods to overcome this issue as illustrated in Figure 3. Masked sentences as implicit queries. The first method masks out low-confidence tokens in \u02c6st with probabilities below a threshold \u03b2 \u2208 [0, 1], where a higher \u03b2 results in more aggressive masking. This removes potential distractions from the sentence to improve retrieval accuracy. Generated questions as explicit queries. An- other method is to generate explicit questions that target the low-confident span in \u02c6st. For example, if the LM is uncertain about \u201cthe University of Penn- sylvania\u201d, a question like \u201cWhich university did Joe Biden attend?\u201d can help retrieve relevant in- formation. Self-ask (Press et al., 2022) achieved this by manually inserting follow-up questions into downstream task exemplars as shown later in Prompt D.2, which requires task-specific annota- tion efforts. Instead, we developed a universal ap- proach that generates questions for low-confidence spans without additional annotation. Specifically, We first extract all spans from \u02c6st with probabilities below \u03b2. For each extracted span z, we prompt gpt-3.5-turbo to generate a question qt,z that can be answered with the span: Prompt 3.2: zero-shot question generation User input x. Generated output so far y\u2264t. Given the above passage, ask a question to which the answer is the term/entity/phrase \u201cz\u201d. We retrieve using each generated question and interleave the returned documents into a single ranking list to aid future generations. In summary, queries qt are formulated based on \u02c6st as follows: qt = (cid:40) \u2205 mask(\u02c6st) or qgen(\u02c6st) if all tokens of \u02c6st have probs \u2265 \u03b8 otherwise 3.3 Base LM We validate our method on one of the most advanced GPT-3.5 LMs text-davinci-003 by iteratively querying their API.2 Implementation Details Document corpus and retrievers. Since we fo- cus"}