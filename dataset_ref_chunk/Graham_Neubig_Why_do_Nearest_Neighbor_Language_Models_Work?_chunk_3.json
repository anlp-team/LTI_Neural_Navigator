{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Why_do_Nearest_Neighbor_Language_Models_Work?_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are some of the additional components in the kNN-LM model mentioned in the text?", "answer": " M , mask-to-k, and \u2297", "ref_chunk": "model, with a few additional components: M , mask-to-k, and \u2297. More speci\ufb01cally, some of the design decisions that go into the kNN-LM, and parallels with standard parametric models are: 1. Size of Wds: In the standard parametric model, the size of Wsm is V embedding vectors, each with D dimensions. In the kNN-LM the size of Wds is very large: Nds, the size of the datastore, usually the number of tokens in the entire training corpus. 2. Input representation: In the parametric model, hsm is the output from the feedforward layer in the last transformer block, which we abbreviate \u201cffn\u201d. In contrast, Khandelwal et al. (2020b) rather use as hds the output from the multi-headed attention layer of the last transformer block (before running the representations through the feed-forward network, and after the LayerNorm (Ba et al., 2016)), which we abbreviate as \u201catt\u201d. 3. Similarity & Temperature: In the parametric model, the functional form of \u2297 is the inner product (abbreviated IP), whereas Khandelwal et al. (2020b) use negative squared L2 distance (abbreviated L2) as a similarity function between Wds and hds. As the similarity scores are turned into probability distributions with the softmax function, the choice of softmax temperature (\u03c4 ) can control the scaling of the similarity scores and thus the peakiness of the non-parametric component distribution. 4. Approximation & Sparsi\ufb01cation: In the parametric model, k = V , and no values are masked, but in the kNN-LM, k (cid:28) V , and most of the datastore entries are pruned out. The de\ufb01nition of the mask-to-k(\u00b7) function, i.e. how to select the important datastore embeddings to include in the similarity calculation (in kNN-LM\u2019s case the k nearest neighbors), is a crucial open design choice. In the following sections, we set out to better understand how each of these design decisions contributes to the improvement in accuracy due to the use of kNN-LMs. 3 Baseline kNN-LM Results First, we evaluate the kNN-LM baseline on the Wikitext-103 dataset (Merity et al., 2016), and examine the importance of two design choices: the input representation hds and the similarity function \u2297. In models examined in this paper, the parametric model is a transformer language model with mostly the same architecture as in Khandelwal et al. (2020b). However, We do make modi\ufb01cations to the original base LM (Baevski and Auli, 2018) to accommodate our experimentation need. We using BPE tokenization (Sennrich et al., 2015) to train a smaller vocabulary (33K) than the original (260K) on the training corpus of Wikitext-103, as subword tokenization is ubiquitous in many state-of-the-art language models (Radford et al., 2019; Devlin et al., 2018; Liu et al., 2019; Brown et al., 2020). Using subword tokenization also eliminates the need for adaptive softmax (Joulin et al., 2017). It makes the output layer more generalized, sharing more resemblance to the kNN component as described in Section 2, and facilitates the ablation studies in this paper.1 This base LM has 268M parameters. To get a perspective on how large the datastore is, it is built on the training data that contains nearly 150M BPE tokens, each paired with a context vector of size 1024. This datastore has a total memory consumption of about 300GB. At every retrieval step, we take the top 1024 nearest neighbors, i.e., k = 1024, following Khandelwal et al. (2020b). The interpolated perplexity is computed with optimal interpolation parameter \u03bb tuned according to the perplexity on the development set. \u03bb is \ufb01xed during the inference for all predictions, the same as the standard kNN-LM. 1By training our own version of the base LM from scratch with BPE tokenization and a standard output softmax layer, our LM\u2019s perplexity is worse than that used in the original kNN-LM paper. However, we observe similar relative gains from the additional kNN component. We argue that the base LM\u2019s performance is orthogonal to the study of the factors behind kNN-LM\u2019s improvements. 4 hds \u2297 +#params PPL Interp. PPL Oracle Base LM kNN-LM-L2 kNN-LM-IP kNN-LM-L2 kNN-LM-IP att att ffn ffn L2 Nds \u00d7 D IP Nds \u00d7 D L2 Nds \u00d7 D IP Nds \u00d7 D 0 21.750 \u221e \u221e \u221e \u221e 19.174 19.095 20.734 21.101 14.230 14.077 15.594 16.254 Table 1: Performance of the parametric language model and several kNN-LM variants. Results comparing multiple kNN-LM variants are shown in Table 1. The \ufb01rst row represents the base parametric language model\u2019s perplexity. The second is a formulation analogous to that of Khandelwal et al. (2020b), and in the remaining rows, we vary the input representation hds and distance function \u2297 from Equation 5. All of them use a large datastore with size Nds, approximately 5000 times the size of the vocabulary V , as also re\ufb02ected in \u201c+#params\u201d, the number of additional parameters other than the base LM. We report several important quantities with respect to each model. \u201cPPL\u201d shows the perplexity of only the kNN component of the model pkNN(). This is \u221e for all kNN- LM models in all cases, as when the kNN search does not retrieve any datastore entries corresponding to the true target word wt the probability of the target word will be zero. \u201cOracle\u201d shows the lower bound of the interpolation performance by choosing the best \u03bb for each token in the evaluation dataset, which will either be \u03bb = 0 or \u03bb = 1 depending on whether PLM (wt|ct) > Pknn(wt|ct) or not, respectively. From the table, we can see that: 1. Using the output of the multi-headed attention layer (\u201catt\u201d) as hds (instead of the standard \u201cffn\u201d layer) is crucial for better performance of kNN-LM. 2. In general, using negative squared L2 distance or inner product as a similarity function does not result in a large and consistent difference, although in our setting, IP provides slightly better performance when using the \u201catt\u201d inputs, and slightly worse when using \u201cffn\u201d inputs. 3. Interestingly, when using \u201cffn\u201d and \u201cIP\u201d, the same input and distance metric used in the parametric model, the results are the worst, indicating that"}, {"question": " How does the size of Wds in the kNN-LM model compare to the size of Wsm in the standard parametric model?", "answer": " The size of Wds in the kNN-LM model is very large: Nds, which is usually the number of tokens in the entire training corpus, while Wsm in the standard parametric model has V embedding vectors, each with D dimensions.", "ref_chunk": "model, with a few additional components: M , mask-to-k, and \u2297. More speci\ufb01cally, some of the design decisions that go into the kNN-LM, and parallels with standard parametric models are: 1. Size of Wds: In the standard parametric model, the size of Wsm is V embedding vectors, each with D dimensions. In the kNN-LM the size of Wds is very large: Nds, the size of the datastore, usually the number of tokens in the entire training corpus. 2. Input representation: In the parametric model, hsm is the output from the feedforward layer in the last transformer block, which we abbreviate \u201cffn\u201d. In contrast, Khandelwal et al. (2020b) rather use as hds the output from the multi-headed attention layer of the last transformer block (before running the representations through the feed-forward network, and after the LayerNorm (Ba et al., 2016)), which we abbreviate as \u201catt\u201d. 3. Similarity & Temperature: In the parametric model, the functional form of \u2297 is the inner product (abbreviated IP), whereas Khandelwal et al. (2020b) use negative squared L2 distance (abbreviated L2) as a similarity function between Wds and hds. As the similarity scores are turned into probability distributions with the softmax function, the choice of softmax temperature (\u03c4 ) can control the scaling of the similarity scores and thus the peakiness of the non-parametric component distribution. 4. Approximation & Sparsi\ufb01cation: In the parametric model, k = V , and no values are masked, but in the kNN-LM, k (cid:28) V , and most of the datastore entries are pruned out. The de\ufb01nition of the mask-to-k(\u00b7) function, i.e. how to select the important datastore embeddings to include in the similarity calculation (in kNN-LM\u2019s case the k nearest neighbors), is a crucial open design choice. In the following sections, we set out to better understand how each of these design decisions contributes to the improvement in accuracy due to the use of kNN-LMs. 3 Baseline kNN-LM Results First, we evaluate the kNN-LM baseline on the Wikitext-103 dataset (Merity et al., 2016), and examine the importance of two design choices: the input representation hds and the similarity function \u2297. In models examined in this paper, the parametric model is a transformer language model with mostly the same architecture as in Khandelwal et al. (2020b). However, We do make modi\ufb01cations to the original base LM (Baevski and Auli, 2018) to accommodate our experimentation need. We using BPE tokenization (Sennrich et al., 2015) to train a smaller vocabulary (33K) than the original (260K) on the training corpus of Wikitext-103, as subword tokenization is ubiquitous in many state-of-the-art language models (Radford et al., 2019; Devlin et al., 2018; Liu et al., 2019; Brown et al., 2020). Using subword tokenization also eliminates the need for adaptive softmax (Joulin et al., 2017). It makes the output layer more generalized, sharing more resemblance to the kNN component as described in Section 2, and facilitates the ablation studies in this paper.1 This base LM has 268M parameters. To get a perspective on how large the datastore is, it is built on the training data that contains nearly 150M BPE tokens, each paired with a context vector of size 1024. This datastore has a total memory consumption of about 300GB. At every retrieval step, we take the top 1024 nearest neighbors, i.e., k = 1024, following Khandelwal et al. (2020b). The interpolated perplexity is computed with optimal interpolation parameter \u03bb tuned according to the perplexity on the development set. \u03bb is \ufb01xed during the inference for all predictions, the same as the standard kNN-LM. 1By training our own version of the base LM from scratch with BPE tokenization and a standard output softmax layer, our LM\u2019s perplexity is worse than that used in the original kNN-LM paper. However, we observe similar relative gains from the additional kNN component. We argue that the base LM\u2019s performance is orthogonal to the study of the factors behind kNN-LM\u2019s improvements. 4 hds \u2297 +#params PPL Interp. PPL Oracle Base LM kNN-LM-L2 kNN-LM-IP kNN-LM-L2 kNN-LM-IP att att ffn ffn L2 Nds \u00d7 D IP Nds \u00d7 D L2 Nds \u00d7 D IP Nds \u00d7 D 0 21.750 \u221e \u221e \u221e \u221e 19.174 19.095 20.734 21.101 14.230 14.077 15.594 16.254 Table 1: Performance of the parametric language model and several kNN-LM variants. Results comparing multiple kNN-LM variants are shown in Table 1. The \ufb01rst row represents the base parametric language model\u2019s perplexity. The second is a formulation analogous to that of Khandelwal et al. (2020b), and in the remaining rows, we vary the input representation hds and distance function \u2297 from Equation 5. All of them use a large datastore with size Nds, approximately 5000 times the size of the vocabulary V , as also re\ufb02ected in \u201c+#params\u201d, the number of additional parameters other than the base LM. We report several important quantities with respect to each model. \u201cPPL\u201d shows the perplexity of only the kNN component of the model pkNN(). This is \u221e for all kNN- LM models in all cases, as when the kNN search does not retrieve any datastore entries corresponding to the true target word wt the probability of the target word will be zero. \u201cOracle\u201d shows the lower bound of the interpolation performance by choosing the best \u03bb for each token in the evaluation dataset, which will either be \u03bb = 0 or \u03bb = 1 depending on whether PLM (wt|ct) > Pknn(wt|ct) or not, respectively. From the table, we can see that: 1. Using the output of the multi-headed attention layer (\u201catt\u201d) as hds (instead of the standard \u201cffn\u201d layer) is crucial for better performance of kNN-LM. 2. In general, using negative squared L2 distance or inner product as a similarity function does not result in a large and consistent difference, although in our setting, IP provides slightly better performance when using the \u201catt\u201d inputs, and slightly worse when using \u201cffn\u201d inputs. 3. Interestingly, when using \u201cffn\u201d and \u201cIP\u201d, the same input and distance metric used in the parametric model, the results are the worst, indicating that"}, {"question": " What is the difference in the input representation between the parametric model and the kNN-LM model as discussed in the text?", "answer": " In the parametric model, hsm is the output from the feedforward layer in the last transformer block, while in the kNN-LM model, hds is the output from the multi-headed attention layer of the last transformer block.", "ref_chunk": "model, with a few additional components: M , mask-to-k, and \u2297. More speci\ufb01cally, some of the design decisions that go into the kNN-LM, and parallels with standard parametric models are: 1. Size of Wds: In the standard parametric model, the size of Wsm is V embedding vectors, each with D dimensions. In the kNN-LM the size of Wds is very large: Nds, the size of the datastore, usually the number of tokens in the entire training corpus. 2. Input representation: In the parametric model, hsm is the output from the feedforward layer in the last transformer block, which we abbreviate \u201cffn\u201d. In contrast, Khandelwal et al. (2020b) rather use as hds the output from the multi-headed attention layer of the last transformer block (before running the representations through the feed-forward network, and after the LayerNorm (Ba et al., 2016)), which we abbreviate as \u201catt\u201d. 3. Similarity & Temperature: In the parametric model, the functional form of \u2297 is the inner product (abbreviated IP), whereas Khandelwal et al. (2020b) use negative squared L2 distance (abbreviated L2) as a similarity function between Wds and hds. As the similarity scores are turned into probability distributions with the softmax function, the choice of softmax temperature (\u03c4 ) can control the scaling of the similarity scores and thus the peakiness of the non-parametric component distribution. 4. Approximation & Sparsi\ufb01cation: In the parametric model, k = V , and no values are masked, but in the kNN-LM, k (cid:28) V , and most of the datastore entries are pruned out. The de\ufb01nition of the mask-to-k(\u00b7) function, i.e. how to select the important datastore embeddings to include in the similarity calculation (in kNN-LM\u2019s case the k nearest neighbors), is a crucial open design choice. In the following sections, we set out to better understand how each of these design decisions contributes to the improvement in accuracy due to the use of kNN-LMs. 3 Baseline kNN-LM Results First, we evaluate the kNN-LM baseline on the Wikitext-103 dataset (Merity et al., 2016), and examine the importance of two design choices: the input representation hds and the similarity function \u2297. In models examined in this paper, the parametric model is a transformer language model with mostly the same architecture as in Khandelwal et al. (2020b). However, We do make modi\ufb01cations to the original base LM (Baevski and Auli, 2018) to accommodate our experimentation need. We using BPE tokenization (Sennrich et al., 2015) to train a smaller vocabulary (33K) than the original (260K) on the training corpus of Wikitext-103, as subword tokenization is ubiquitous in many state-of-the-art language models (Radford et al., 2019; Devlin et al., 2018; Liu et al., 2019; Brown et al., 2020). Using subword tokenization also eliminates the need for adaptive softmax (Joulin et al., 2017). It makes the output layer more generalized, sharing more resemblance to the kNN component as described in Section 2, and facilitates the ablation studies in this paper.1 This base LM has 268M parameters. To get a perspective on how large the datastore is, it is built on the training data that contains nearly 150M BPE tokens, each paired with a context vector of size 1024. This datastore has a total memory consumption of about 300GB. At every retrieval step, we take the top 1024 nearest neighbors, i.e., k = 1024, following Khandelwal et al. (2020b). The interpolated perplexity is computed with optimal interpolation parameter \u03bb tuned according to the perplexity on the development set. \u03bb is \ufb01xed during the inference for all predictions, the same as the standard kNN-LM. 1By training our own version of the base LM from scratch with BPE tokenization and a standard output softmax layer, our LM\u2019s perplexity is worse than that used in the original kNN-LM paper. However, we observe similar relative gains from the additional kNN component. We argue that the base LM\u2019s performance is orthogonal to the study of the factors behind kNN-LM\u2019s improvements. 4 hds \u2297 +#params PPL Interp. PPL Oracle Base LM kNN-LM-L2 kNN-LM-IP kNN-LM-L2 kNN-LM-IP att att ffn ffn L2 Nds \u00d7 D IP Nds \u00d7 D L2 Nds \u00d7 D IP Nds \u00d7 D 0 21.750 \u221e \u221e \u221e \u221e 19.174 19.095 20.734 21.101 14.230 14.077 15.594 16.254 Table 1: Performance of the parametric language model and several kNN-LM variants. Results comparing multiple kNN-LM variants are shown in Table 1. The \ufb01rst row represents the base parametric language model\u2019s perplexity. The second is a formulation analogous to that of Khandelwal et al. (2020b), and in the remaining rows, we vary the input representation hds and distance function \u2297 from Equation 5. All of them use a large datastore with size Nds, approximately 5000 times the size of the vocabulary V , as also re\ufb02ected in \u201c+#params\u201d, the number of additional parameters other than the base LM. We report several important quantities with respect to each model. \u201cPPL\u201d shows the perplexity of only the kNN component of the model pkNN(). This is \u221e for all kNN- LM models in all cases, as when the kNN search does not retrieve any datastore entries corresponding to the true target word wt the probability of the target word will be zero. \u201cOracle\u201d shows the lower bound of the interpolation performance by choosing the best \u03bb for each token in the evaluation dataset, which will either be \u03bb = 0 or \u03bb = 1 depending on whether PLM (wt|ct) > Pknn(wt|ct) or not, respectively. From the table, we can see that: 1. Using the output of the multi-headed attention layer (\u201catt\u201d) as hds (instead of the standard \u201cffn\u201d layer) is crucial for better performance of kNN-LM. 2. In general, using negative squared L2 distance or inner product as a similarity function does not result in a large and consistent difference, although in our setting, IP provides slightly better performance when using the \u201catt\u201d inputs, and slightly worse when using \u201cffn\u201d inputs. 3. Interestingly, when using \u201cffn\u201d and \u201cIP\u201d, the same input and distance metric used in the parametric model, the results are the worst, indicating that"}, {"question": " What function is used in the parametric model for similarity, and what function is used in the kNN-LM model?", "answer": " The parametric model uses the inner product (IP) function for similarity, whereas the kNN-LM model uses negative squared L2 distance (L2) as a similarity function.", "ref_chunk": "model, with a few additional components: M , mask-to-k, and \u2297. More speci\ufb01cally, some of the design decisions that go into the kNN-LM, and parallels with standard parametric models are: 1. Size of Wds: In the standard parametric model, the size of Wsm is V embedding vectors, each with D dimensions. In the kNN-LM the size of Wds is very large: Nds, the size of the datastore, usually the number of tokens in the entire training corpus. 2. Input representation: In the parametric model, hsm is the output from the feedforward layer in the last transformer block, which we abbreviate \u201cffn\u201d. In contrast, Khandelwal et al. (2020b) rather use as hds the output from the multi-headed attention layer of the last transformer block (before running the representations through the feed-forward network, and after the LayerNorm (Ba et al., 2016)), which we abbreviate as \u201catt\u201d. 3. Similarity & Temperature: In the parametric model, the functional form of \u2297 is the inner product (abbreviated IP), whereas Khandelwal et al. (2020b) use negative squared L2 distance (abbreviated L2) as a similarity function between Wds and hds. As the similarity scores are turned into probability distributions with the softmax function, the choice of softmax temperature (\u03c4 ) can control the scaling of the similarity scores and thus the peakiness of the non-parametric component distribution. 4. Approximation & Sparsi\ufb01cation: In the parametric model, k = V , and no values are masked, but in the kNN-LM, k (cid:28) V , and most of the datastore entries are pruned out. The de\ufb01nition of the mask-to-k(\u00b7) function, i.e. how to select the important datastore embeddings to include in the similarity calculation (in kNN-LM\u2019s case the k nearest neighbors), is a crucial open design choice. In the following sections, we set out to better understand how each of these design decisions contributes to the improvement in accuracy due to the use of kNN-LMs. 3 Baseline kNN-LM Results First, we evaluate the kNN-LM baseline on the Wikitext-103 dataset (Merity et al., 2016), and examine the importance of two design choices: the input representation hds and the similarity function \u2297. In models examined in this paper, the parametric model is a transformer language model with mostly the same architecture as in Khandelwal et al. (2020b). However, We do make modi\ufb01cations to the original base LM (Baevski and Auli, 2018) to accommodate our experimentation need. We using BPE tokenization (Sennrich et al., 2015) to train a smaller vocabulary (33K) than the original (260K) on the training corpus of Wikitext-103, as subword tokenization is ubiquitous in many state-of-the-art language models (Radford et al., 2019; Devlin et al., 2018; Liu et al., 2019; Brown et al., 2020). Using subword tokenization also eliminates the need for adaptive softmax (Joulin et al., 2017). It makes the output layer more generalized, sharing more resemblance to the kNN component as described in Section 2, and facilitates the ablation studies in this paper.1 This base LM has 268M parameters. To get a perspective on how large the datastore is, it is built on the training data that contains nearly 150M BPE tokens, each paired with a context vector of size 1024. This datastore has a total memory consumption of about 300GB. At every retrieval step, we take the top 1024 nearest neighbors, i.e., k = 1024, following Khandelwal et al. (2020b). The interpolated perplexity is computed with optimal interpolation parameter \u03bb tuned according to the perplexity on the development set. \u03bb is \ufb01xed during the inference for all predictions, the same as the standard kNN-LM. 1By training our own version of the base LM from scratch with BPE tokenization and a standard output softmax layer, our LM\u2019s perplexity is worse than that used in the original kNN-LM paper. However, we observe similar relative gains from the additional kNN component. We argue that the base LM\u2019s performance is orthogonal to the study of the factors behind kNN-LM\u2019s improvements. 4 hds \u2297 +#params PPL Interp. PPL Oracle Base LM kNN-LM-L2 kNN-LM-IP kNN-LM-L2 kNN-LM-IP att att ffn ffn L2 Nds \u00d7 D IP Nds \u00d7 D L2 Nds \u00d7 D IP Nds \u00d7 D 0 21.750 \u221e \u221e \u221e \u221e 19.174 19.095 20.734 21.101 14.230 14.077 15.594 16.254 Table 1: Performance of the parametric language model and several kNN-LM variants. Results comparing multiple kNN-LM variants are shown in Table 1. The \ufb01rst row represents the base parametric language model\u2019s perplexity. The second is a formulation analogous to that of Khandelwal et al. (2020b), and in the remaining rows, we vary the input representation hds and distance function \u2297 from Equation 5. All of them use a large datastore with size Nds, approximately 5000 times the size of the vocabulary V , as also re\ufb02ected in \u201c+#params\u201d, the number of additional parameters other than the base LM. We report several important quantities with respect to each model. \u201cPPL\u201d shows the perplexity of only the kNN component of the model pkNN(). This is \u221e for all kNN- LM models in all cases, as when the kNN search does not retrieve any datastore entries corresponding to the true target word wt the probability of the target word will be zero. \u201cOracle\u201d shows the lower bound of the interpolation performance by choosing the best \u03bb for each token in the evaluation dataset, which will either be \u03bb = 0 or \u03bb = 1 depending on whether PLM (wt|ct) > Pknn(wt|ct) or not, respectively. From the table, we can see that: 1. Using the output of the multi-headed attention layer (\u201catt\u201d) as hds (instead of the standard \u201cffn\u201d layer) is crucial for better performance of kNN-LM. 2. In general, using negative squared L2 distance or inner product as a similarity function does not result in a large and consistent difference, although in our setting, IP provides slightly better performance when using the \u201catt\u201d inputs, and slightly worse when using \u201cffn\u201d inputs. 3. Interestingly, when using \u201cffn\u201d and \u201cIP\u201d, the same input and distance metric used in the parametric model, the results are the worst, indicating that"}, {"question": " In the kNN-LM model, what happens to most of the datastore entries in comparison to the parametric model?", "answer": " In the kNN-LM model, most of the datastore entries are pruned out, while in the parametric model, no values are masked.", "ref_chunk": "model, with a few additional components: M , mask-to-k, and \u2297. More speci\ufb01cally, some of the design decisions that go into the kNN-LM, and parallels with standard parametric models are: 1. Size of Wds: In the standard parametric model, the size of Wsm is V embedding vectors, each with D dimensions. In the kNN-LM the size of Wds is very large: Nds, the size of the datastore, usually the number of tokens in the entire training corpus. 2. Input representation: In the parametric model, hsm is the output from the feedforward layer in the last transformer block, which we abbreviate \u201cffn\u201d. In contrast, Khandelwal et al. (2020b) rather use as hds the output from the multi-headed attention layer of the last transformer block (before running the representations through the feed-forward network, and after the LayerNorm (Ba et al., 2016)), which we abbreviate as \u201catt\u201d. 3. Similarity & Temperature: In the parametric model, the functional form of \u2297 is the inner product (abbreviated IP), whereas Khandelwal et al. (2020b) use negative squared L2 distance (abbreviated L2) as a similarity function between Wds and hds. As the similarity scores are turned into probability distributions with the softmax function, the choice of softmax temperature (\u03c4 ) can control the scaling of the similarity scores and thus the peakiness of the non-parametric component distribution. 4. Approximation & Sparsi\ufb01cation: In the parametric model, k = V , and no values are masked, but in the kNN-LM, k (cid:28) V , and most of the datastore entries are pruned out. The de\ufb01nition of the mask-to-k(\u00b7) function, i.e. how to select the important datastore embeddings to include in the similarity calculation (in kNN-LM\u2019s case the k nearest neighbors), is a crucial open design choice. In the following sections, we set out to better understand how each of these design decisions contributes to the improvement in accuracy due to the use of kNN-LMs. 3 Baseline kNN-LM Results First, we evaluate the kNN-LM baseline on the Wikitext-103 dataset (Merity et al., 2016), and examine the importance of two design choices: the input representation hds and the similarity function \u2297. In models examined in this paper, the parametric model is a transformer language model with mostly the same architecture as in Khandelwal et al. (2020b). However, We do make modi\ufb01cations to the original base LM (Baevski and Auli, 2018) to accommodate our experimentation need. We using BPE tokenization (Sennrich et al., 2015) to train a smaller vocabulary (33K) than the original (260K) on the training corpus of Wikitext-103, as subword tokenization is ubiquitous in many state-of-the-art language models (Radford et al., 2019; Devlin et al., 2018; Liu et al., 2019; Brown et al., 2020). Using subword tokenization also eliminates the need for adaptive softmax (Joulin et al., 2017). It makes the output layer more generalized, sharing more resemblance to the kNN component as described in Section 2, and facilitates the ablation studies in this paper.1 This base LM has 268M parameters. To get a perspective on how large the datastore is, it is built on the training data that contains nearly 150M BPE tokens, each paired with a context vector of size 1024. This datastore has a total memory consumption of about 300GB. At every retrieval step, we take the top 1024 nearest neighbors, i.e., k = 1024, following Khandelwal et al. (2020b). The interpolated perplexity is computed with optimal interpolation parameter \u03bb tuned according to the perplexity on the development set. \u03bb is \ufb01xed during the inference for all predictions, the same as the standard kNN-LM. 1By training our own version of the base LM from scratch with BPE tokenization and a standard output softmax layer, our LM\u2019s perplexity is worse than that used in the original kNN-LM paper. However, we observe similar relative gains from the additional kNN component. We argue that the base LM\u2019s performance is orthogonal to the study of the factors behind kNN-LM\u2019s improvements. 4 hds \u2297 +#params PPL Interp. PPL Oracle Base LM kNN-LM-L2 kNN-LM-IP kNN-LM-L2 kNN-LM-IP att att ffn ffn L2 Nds \u00d7 D IP Nds \u00d7 D L2 Nds \u00d7 D IP Nds \u00d7 D 0 21.750 \u221e \u221e \u221e \u221e 19.174 19.095 20.734 21.101 14.230 14.077 15.594 16.254 Table 1: Performance of the parametric language model and several kNN-LM variants. Results comparing multiple kNN-LM variants are shown in Table 1. The \ufb01rst row represents the base parametric language model\u2019s perplexity. The second is a formulation analogous to that of Khandelwal et al. (2020b), and in the remaining rows, we vary the input representation hds and distance function \u2297 from Equation 5. All of them use a large datastore with size Nds, approximately 5000 times the size of the vocabulary V , as also re\ufb02ected in \u201c+#params\u201d, the number of additional parameters other than the base LM. We report several important quantities with respect to each model. \u201cPPL\u201d shows the perplexity of only the kNN component of the model pkNN(). This is \u221e for all kNN- LM models in all cases, as when the kNN search does not retrieve any datastore entries corresponding to the true target word wt the probability of the target word will be zero. \u201cOracle\u201d shows the lower bound of the interpolation performance by choosing the best \u03bb for each token in the evaluation dataset, which will either be \u03bb = 0 or \u03bb = 1 depending on whether PLM (wt|ct) > Pknn(wt|ct) or not, respectively. From the table, we can see that: 1. Using the output of the multi-headed attention layer (\u201catt\u201d) as hds (instead of the standard \u201cffn\u201d layer) is crucial for better performance of kNN-LM. 2. In general, using negative squared L2 distance or inner product as a similarity function does not result in a large and consistent difference, although in our setting, IP provides slightly better performance when using the \u201catt\u201d inputs, and slightly worse when using \u201cffn\u201d inputs. 3. Interestingly, when using \u201cffn\u201d and \u201cIP\u201d, the same input and distance metric used in the parametric model, the results are the worst, indicating that"}, {"question": " What dataset is used to evaluate the kNN-LM baseline in the text?", "answer": " The Wikitext-103 dataset", "ref_chunk": "model, with a few additional components: M , mask-to-k, and \u2297. More speci\ufb01cally, some of the design decisions that go into the kNN-LM, and parallels with standard parametric models are: 1. Size of Wds: In the standard parametric model, the size of Wsm is V embedding vectors, each with D dimensions. In the kNN-LM the size of Wds is very large: Nds, the size of the datastore, usually the number of tokens in the entire training corpus. 2. Input representation: In the parametric model, hsm is the output from the feedforward layer in the last transformer block, which we abbreviate \u201cffn\u201d. In contrast, Khandelwal et al. (2020b) rather use as hds the output from the multi-headed attention layer of the last transformer block (before running the representations through the feed-forward network, and after the LayerNorm (Ba et al., 2016)), which we abbreviate as \u201catt\u201d. 3. Similarity & Temperature: In the parametric model, the functional form of \u2297 is the inner product (abbreviated IP), whereas Khandelwal et al. (2020b) use negative squared L2 distance (abbreviated L2) as a similarity function between Wds and hds. As the similarity scores are turned into probability distributions with the softmax function, the choice of softmax temperature (\u03c4 ) can control the scaling of the similarity scores and thus the peakiness of the non-parametric component distribution. 4. Approximation & Sparsi\ufb01cation: In the parametric model, k = V , and no values are masked, but in the kNN-LM, k (cid:28) V , and most of the datastore entries are pruned out. The de\ufb01nition of the mask-to-k(\u00b7) function, i.e. how to select the important datastore embeddings to include in the similarity calculation (in kNN-LM\u2019s case the k nearest neighbors), is a crucial open design choice. In the following sections, we set out to better understand how each of these design decisions contributes to the improvement in accuracy due to the use of kNN-LMs. 3 Baseline kNN-LM Results First, we evaluate the kNN-LM baseline on the Wikitext-103 dataset (Merity et al., 2016), and examine the importance of two design choices: the input representation hds and the similarity function \u2297. In models examined in this paper, the parametric model is a transformer language model with mostly the same architecture as in Khandelwal et al. (2020b). However, We do make modi\ufb01cations to the original base LM (Baevski and Auli, 2018) to accommodate our experimentation need. We using BPE tokenization (Sennrich et al., 2015) to train a smaller vocabulary (33K) than the original (260K) on the training corpus of Wikitext-103, as subword tokenization is ubiquitous in many state-of-the-art language models (Radford et al., 2019; Devlin et al., 2018; Liu et al., 2019; Brown et al., 2020). Using subword tokenization also eliminates the need for adaptive softmax (Joulin et al., 2017). It makes the output layer more generalized, sharing more resemblance to the kNN component as described in Section 2, and facilitates the ablation studies in this paper.1 This base LM has 268M parameters. To get a perspective on how large the datastore is, it is built on the training data that contains nearly 150M BPE tokens, each paired with a context vector of size 1024. This datastore has a total memory consumption of about 300GB. At every retrieval step, we take the top 1024 nearest neighbors, i.e., k = 1024, following Khandelwal et al. (2020b). The interpolated perplexity is computed with optimal interpolation parameter \u03bb tuned according to the perplexity on the development set. \u03bb is \ufb01xed during the inference for all predictions, the same as the standard kNN-LM. 1By training our own version of the base LM from scratch with BPE tokenization and a standard output softmax layer, our LM\u2019s perplexity is worse than that used in the original kNN-LM paper. However, we observe similar relative gains from the additional kNN component. We argue that the base LM\u2019s performance is orthogonal to the study of the factors behind kNN-LM\u2019s improvements. 4 hds \u2297 +#params PPL Interp. PPL Oracle Base LM kNN-LM-L2 kNN-LM-IP kNN-LM-L2 kNN-LM-IP att att ffn ffn L2 Nds \u00d7 D IP Nds \u00d7 D L2 Nds \u00d7 D IP Nds \u00d7 D 0 21.750 \u221e \u221e \u221e \u221e 19.174 19.095 20.734 21.101 14.230 14.077 15.594 16.254 Table 1: Performance of the parametric language model and several kNN-LM variants. Results comparing multiple kNN-LM variants are shown in Table 1. The \ufb01rst row represents the base parametric language model\u2019s perplexity. The second is a formulation analogous to that of Khandelwal et al. (2020b), and in the remaining rows, we vary the input representation hds and distance function \u2297 from Equation 5. All of them use a large datastore with size Nds, approximately 5000 times the size of the vocabulary V , as also re\ufb02ected in \u201c+#params\u201d, the number of additional parameters other than the base LM. We report several important quantities with respect to each model. \u201cPPL\u201d shows the perplexity of only the kNN component of the model pkNN(). This is \u221e for all kNN- LM models in all cases, as when the kNN search does not retrieve any datastore entries corresponding to the true target word wt the probability of the target word will be zero. \u201cOracle\u201d shows the lower bound of the interpolation performance by choosing the best \u03bb for each token in the evaluation dataset, which will either be \u03bb = 0 or \u03bb = 1 depending on whether PLM (wt|ct) > Pknn(wt|ct) or not, respectively. From the table, we can see that: 1. Using the output of the multi-headed attention layer (\u201catt\u201d) as hds (instead of the standard \u201cffn\u201d layer) is crucial for better performance of kNN-LM. 2. In general, using negative squared L2 distance or inner product as a similarity function does not result in a large and consistent difference, although in our setting, IP provides slightly better performance when using the \u201catt\u201d inputs, and slightly worse when using \u201cffn\u201d inputs. 3. Interestingly, when using \u201cffn\u201d and \u201cIP\u201d, the same input and distance metric used in the parametric model, the results are the worst, indicating that"}, {"question": " How many parameters does the base LM used in the text have?", "answer": " 268 million parameters", "ref_chunk": "model, with a few additional components: M , mask-to-k, and \u2297. More speci\ufb01cally, some of the design decisions that go into the kNN-LM, and parallels with standard parametric models are: 1. Size of Wds: In the standard parametric model, the size of Wsm is V embedding vectors, each with D dimensions. In the kNN-LM the size of Wds is very large: Nds, the size of the datastore, usually the number of tokens in the entire training corpus. 2. Input representation: In the parametric model, hsm is the output from the feedforward layer in the last transformer block, which we abbreviate \u201cffn\u201d. In contrast, Khandelwal et al. (2020b) rather use as hds the output from the multi-headed attention layer of the last transformer block (before running the representations through the feed-forward network, and after the LayerNorm (Ba et al., 2016)), which we abbreviate as \u201catt\u201d. 3. Similarity & Temperature: In the parametric model, the functional form of \u2297 is the inner product (abbreviated IP), whereas Khandelwal et al. (2020b) use negative squared L2 distance (abbreviated L2) as a similarity function between Wds and hds. As the similarity scores are turned into probability distributions with the softmax function, the choice of softmax temperature (\u03c4 ) can control the scaling of the similarity scores and thus the peakiness of the non-parametric component distribution. 4. Approximation & Sparsi\ufb01cation: In the parametric model, k = V , and no values are masked, but in the kNN-LM, k (cid:28) V , and most of the datastore entries are pruned out. The de\ufb01nition of the mask-to-k(\u00b7) function, i.e. how to select the important datastore embeddings to include in the similarity calculation (in kNN-LM\u2019s case the k nearest neighbors), is a crucial open design choice. In the following sections, we set out to better understand how each of these design decisions contributes to the improvement in accuracy due to the use of kNN-LMs. 3 Baseline kNN-LM Results First, we evaluate the kNN-LM baseline on the Wikitext-103 dataset (Merity et al., 2016), and examine the importance of two design choices: the input representation hds and the similarity function \u2297. In models examined in this paper, the parametric model is a transformer language model with mostly the same architecture as in Khandelwal et al. (2020b). However, We do make modi\ufb01cations to the original base LM (Baevski and Auli, 2018) to accommodate our experimentation need. We using BPE tokenization (Sennrich et al., 2015) to train a smaller vocabulary (33K) than the original (260K) on the training corpus of Wikitext-103, as subword tokenization is ubiquitous in many state-of-the-art language models (Radford et al., 2019; Devlin et al., 2018; Liu et al., 2019; Brown et al., 2020). Using subword tokenization also eliminates the need for adaptive softmax (Joulin et al., 2017). It makes the output layer more generalized, sharing more resemblance to the kNN component as described in Section 2, and facilitates the ablation studies in this paper.1 This base LM has 268M parameters. To get a perspective on how large the datastore is, it is built on the training data that contains nearly 150M BPE tokens, each paired with a context vector of size 1024. This datastore has a total memory consumption of about 300GB. At every retrieval step, we take the top 1024 nearest neighbors, i.e., k = 1024, following Khandelwal et al. (2020b). The interpolated perplexity is computed with optimal interpolation parameter \u03bb tuned according to the perplexity on the development set. \u03bb is \ufb01xed during the inference for all predictions, the same as the standard kNN-LM. 1By training our own version of the base LM from scratch with BPE tokenization and a standard output softmax layer, our LM\u2019s perplexity is worse than that used in the original kNN-LM paper. However, we observe similar relative gains from the additional kNN component. We argue that the base LM\u2019s performance is orthogonal to the study of the factors behind kNN-LM\u2019s improvements. 4 hds \u2297 +#params PPL Interp. PPL Oracle Base LM kNN-LM-L2 kNN-LM-IP kNN-LM-L2 kNN-LM-IP att att ffn ffn L2 Nds \u00d7 D IP Nds \u00d7 D L2 Nds \u00d7 D IP Nds \u00d7 D 0 21.750 \u221e \u221e \u221e \u221e 19.174 19.095 20.734 21.101 14.230 14.077 15.594 16.254 Table 1: Performance of the parametric language model and several kNN-LM variants. Results comparing multiple kNN-LM variants are shown in Table 1. The \ufb01rst row represents the base parametric language model\u2019s perplexity. The second is a formulation analogous to that of Khandelwal et al. (2020b), and in the remaining rows, we vary the input representation hds and distance function \u2297 from Equation 5. All of them use a large datastore with size Nds, approximately 5000 times the size of the vocabulary V , as also re\ufb02ected in \u201c+#params\u201d, the number of additional parameters other than the base LM. We report several important quantities with respect to each model. \u201cPPL\u201d shows the perplexity of only the kNN component of the model pkNN(). This is \u221e for all kNN- LM models in all cases, as when the kNN search does not retrieve any datastore entries corresponding to the true target word wt the probability of the target word will be zero. \u201cOracle\u201d shows the lower bound of the interpolation performance by choosing the best \u03bb for each token in the evaluation dataset, which will either be \u03bb = 0 or \u03bb = 1 depending on whether PLM (wt|ct) > Pknn(wt|ct) or not, respectively. From the table, we can see that: 1. Using the output of the multi-headed attention layer (\u201catt\u201d) as hds (instead of the standard \u201cffn\u201d layer) is crucial for better performance of kNN-LM. 2. In general, using negative squared L2 distance or inner product as a similarity function does not result in a large and consistent difference, although in our setting, IP provides slightly better performance when using the \u201catt\u201d inputs, and slightly worse when using \u201cffn\u201d inputs. 3. Interestingly, when using \u201cffn\u201d and \u201cIP\u201d, the same input and distance metric used in the parametric model, the results are the worst, indicating that"}, {"question": " What is the size of the datastore built on the training data mentioned in the text?", "answer": " The datastore contains nearly 150 million BPE tokens, each paired with a context vector of size 1024, with a total memory consumption of about 300GB.", "ref_chunk": "model, with a few additional components: M , mask-to-k, and \u2297. More speci\ufb01cally, some of the design decisions that go into the kNN-LM, and parallels with standard parametric models are: 1. Size of Wds: In the standard parametric model, the size of Wsm is V embedding vectors, each with D dimensions. In the kNN-LM the size of Wds is very large: Nds, the size of the datastore, usually the number of tokens in the entire training corpus. 2. Input representation: In the parametric model, hsm is the output from the feedforward layer in the last transformer block, which we abbreviate \u201cffn\u201d. In contrast, Khandelwal et al. (2020b) rather use as hds the output from the multi-headed attention layer of the last transformer block (before running the representations through the feed-forward network, and after the LayerNorm (Ba et al., 2016)), which we abbreviate as \u201catt\u201d. 3. Similarity & Temperature: In the parametric model, the functional form of \u2297 is the inner product (abbreviated IP), whereas Khandelwal et al. (2020b) use negative squared L2 distance (abbreviated L2) as a similarity function between Wds and hds. As the similarity scores are turned into probability distributions with the softmax function, the choice of softmax temperature (\u03c4 ) can control the scaling of the similarity scores and thus the peakiness of the non-parametric component distribution. 4. Approximation & Sparsi\ufb01cation: In the parametric model, k = V , and no values are masked, but in the kNN-LM, k (cid:28) V , and most of the datastore entries are pruned out. The de\ufb01nition of the mask-to-k(\u00b7) function, i.e. how to select the important datastore embeddings to include in the similarity calculation (in kNN-LM\u2019s case the k nearest neighbors), is a crucial open design choice. In the following sections, we set out to better understand how each of these design decisions contributes to the improvement in accuracy due to the use of kNN-LMs. 3 Baseline kNN-LM Results First, we evaluate the kNN-LM baseline on the Wikitext-103 dataset (Merity et al., 2016), and examine the importance of two design choices: the input representation hds and the similarity function \u2297. In models examined in this paper, the parametric model is a transformer language model with mostly the same architecture as in Khandelwal et al. (2020b). However, We do make modi\ufb01cations to the original base LM (Baevski and Auli, 2018) to accommodate our experimentation need. We using BPE tokenization (Sennrich et al., 2015) to train a smaller vocabulary (33K) than the original (260K) on the training corpus of Wikitext-103, as subword tokenization is ubiquitous in many state-of-the-art language models (Radford et al., 2019; Devlin et al., 2018; Liu et al., 2019; Brown et al., 2020). Using subword tokenization also eliminates the need for adaptive softmax (Joulin et al., 2017). It makes the output layer more generalized, sharing more resemblance to the kNN component as described in Section 2, and facilitates the ablation studies in this paper.1 This base LM has 268M parameters. To get a perspective on how large the datastore is, it is built on the training data that contains nearly 150M BPE tokens, each paired with a context vector of size 1024. This datastore has a total memory consumption of about 300GB. At every retrieval step, we take the top 1024 nearest neighbors, i.e., k = 1024, following Khandelwal et al. (2020b). The interpolated perplexity is computed with optimal interpolation parameter \u03bb tuned according to the perplexity on the development set. \u03bb is \ufb01xed during the inference for all predictions, the same as the standard kNN-LM. 1By training our own version of the base LM from scratch with BPE tokenization and a standard output softmax layer, our LM\u2019s perplexity is worse than that used in the original kNN-LM paper. However, we observe similar relative gains from the additional kNN component. We argue that the base LM\u2019s performance is orthogonal to the study of the factors behind kNN-LM\u2019s improvements. 4 hds \u2297 +#params PPL Interp. PPL Oracle Base LM kNN-LM-L2 kNN-LM-IP kNN-LM-L2 kNN-LM-IP att att ffn ffn L2 Nds \u00d7 D IP Nds \u00d7 D L2 Nds \u00d7 D IP Nds \u00d7 D 0 21.750 \u221e \u221e \u221e \u221e 19.174 19.095 20.734 21.101 14.230 14.077 15.594 16.254 Table 1: Performance of the parametric language model and several kNN-LM variants. Results comparing multiple kNN-LM variants are shown in Table 1. The \ufb01rst row represents the base parametric language model\u2019s perplexity. The second is a formulation analogous to that of Khandelwal et al. (2020b), and in the remaining rows, we vary the input representation hds and distance function \u2297 from Equation 5. All of them use a large datastore with size Nds, approximately 5000 times the size of the vocabulary V , as also re\ufb02ected in \u201c+#params\u201d, the number of additional parameters other than the base LM. We report several important quantities with respect to each model. \u201cPPL\u201d shows the perplexity of only the kNN component of the model pkNN(). This is \u221e for all kNN- LM models in all cases, as when the kNN search does not retrieve any datastore entries corresponding to the true target word wt the probability of the target word will be zero. \u201cOracle\u201d shows the lower bound of the interpolation performance by choosing the best \u03bb for each token in the evaluation dataset, which will either be \u03bb = 0 or \u03bb = 1 depending on whether PLM (wt|ct) > Pknn(wt|ct) or not, respectively. From the table, we can see that: 1. Using the output of the multi-headed attention layer (\u201catt\u201d) as hds (instead of the standard \u201cffn\u201d layer) is crucial for better performance of kNN-LM. 2. In general, using negative squared L2 distance or inner product as a similarity function does not result in a large and consistent difference, although in our setting, IP provides slightly better performance when using the \u201catt\u201d inputs, and slightly worse when using \u201cffn\u201d inputs. 3. Interestingly, when using \u201cffn\u201d and \u201cIP\u201d, the same input and distance metric used in the parametric model, the results are the worst, indicating that"}, {"question": " What is the value of k used in the kNN-LM model for taking the top nearest neighbors?", "answer": " k = 1024", "ref_chunk": "model, with a few additional components: M , mask-to-k, and \u2297. More speci\ufb01cally, some of the design decisions that go into the kNN-LM, and parallels with standard parametric models are: 1. Size of Wds: In the standard parametric model, the size of Wsm is V embedding vectors, each with D dimensions. In the kNN-LM the size of Wds is very large: Nds, the size of the datastore, usually the number of tokens in the entire training corpus. 2. Input representation: In the parametric model, hsm is the output from the feedforward layer in the last transformer block, which we abbreviate \u201cffn\u201d. In contrast, Khandelwal et al. (2020b) rather use as hds the output from the multi-headed attention layer of the last transformer block (before running the representations through the feed-forward network, and after the LayerNorm (Ba et al., 2016)), which we abbreviate as \u201catt\u201d. 3. Similarity & Temperature: In the parametric model, the functional form of \u2297 is the inner product (abbreviated IP), whereas Khandelwal et al. (2020b) use negative squared L2 distance (abbreviated L2) as a similarity function between Wds and hds. As the similarity scores are turned into probability distributions with the softmax function, the choice of softmax temperature (\u03c4 ) can control the scaling of the similarity scores and thus the peakiness of the non-parametric component distribution. 4. Approximation & Sparsi\ufb01cation: In the parametric model, k = V , and no values are masked, but in the kNN-LM, k (cid:28) V , and most of the datastore entries are pruned out. The de\ufb01nition of the mask-to-k(\u00b7) function, i.e. how to select the important datastore embeddings to include in the similarity calculation (in kNN-LM\u2019s case the k nearest neighbors), is a crucial open design choice. In the following sections, we set out to better understand how each of these design decisions contributes to the improvement in accuracy due to the use of kNN-LMs. 3 Baseline kNN-LM Results First, we evaluate the kNN-LM baseline on the Wikitext-103 dataset (Merity et al., 2016), and examine the importance of two design choices: the input representation hds and the similarity function \u2297. In models examined in this paper, the parametric model is a transformer language model with mostly the same architecture as in Khandelwal et al. (2020b). However, We do make modi\ufb01cations to the original base LM (Baevski and Auli, 2018) to accommodate our experimentation need. We using BPE tokenization (Sennrich et al., 2015) to train a smaller vocabulary (33K) than the original (260K) on the training corpus of Wikitext-103, as subword tokenization is ubiquitous in many state-of-the-art language models (Radford et al., 2019; Devlin et al., 2018; Liu et al., 2019; Brown et al., 2020). Using subword tokenization also eliminates the need for adaptive softmax (Joulin et al., 2017). It makes the output layer more generalized, sharing more resemblance to the kNN component as described in Section 2, and facilitates the ablation studies in this paper.1 This base LM has 268M parameters. To get a perspective on how large the datastore is, it is built on the training data that contains nearly 150M BPE tokens, each paired with a context vector of size 1024. This datastore has a total memory consumption of about 300GB. At every retrieval step, we take the top 1024 nearest neighbors, i.e., k = 1024, following Khandelwal et al. (2020b). The interpolated perplexity is computed with optimal interpolation parameter \u03bb tuned according to the perplexity on the development set. \u03bb is \ufb01xed during the inference for all predictions, the same as the standard kNN-LM. 1By training our own version of the base LM from scratch with BPE tokenization and a standard output softmax layer, our LM\u2019s perplexity is worse than that used in the original kNN-LM paper. However, we observe similar relative gains from the additional kNN component. We argue that the base LM\u2019s performance is orthogonal to the study of the factors behind kNN-LM\u2019s improvements. 4 hds \u2297 +#params PPL Interp. PPL Oracle Base LM kNN-LM-L2 kNN-LM-IP kNN-LM-L2 kNN-LM-IP att att ffn ffn L2 Nds \u00d7 D IP Nds \u00d7 D L2 Nds \u00d7 D IP Nds \u00d7 D 0 21.750 \u221e \u221e \u221e \u221e 19.174 19.095 20.734 21.101 14.230 14.077 15.594 16.254 Table 1: Performance of the parametric language model and several kNN-LM variants. Results comparing multiple kNN-LM variants are shown in Table 1. The \ufb01rst row represents the base parametric language model\u2019s perplexity. The second is a formulation analogous to that of Khandelwal et al. (2020b), and in the remaining rows, we vary the input representation hds and distance function \u2297 from Equation 5. All of them use a large datastore with size Nds, approximately 5000 times the size of the vocabulary V , as also re\ufb02ected in \u201c+#params\u201d, the number of additional parameters other than the base LM. We report several important quantities with respect to each model. \u201cPPL\u201d shows the perplexity of only the kNN component of the model pkNN(). This is \u221e for all kNN- LM models in all cases, as when the kNN search does not retrieve any datastore entries corresponding to the true target word wt the probability of the target word will be zero. \u201cOracle\u201d shows the lower bound of the interpolation performance by choosing the best \u03bb for each token in the evaluation dataset, which will either be \u03bb = 0 or \u03bb = 1 depending on whether PLM (wt|ct) > Pknn(wt|ct) or not, respectively. From the table, we can see that: 1. Using the output of the multi-headed attention layer (\u201catt\u201d) as hds (instead of the standard \u201cffn\u201d layer) is crucial for better performance of kNN-LM. 2. In general, using negative squared L2 distance or inner product as a similarity function does not result in a large and consistent difference, although in our setting, IP provides slightly better performance when using the \u201catt\u201d inputs, and slightly worse when using \u201cffn\u201d inputs. 3. Interestingly, when using \u201cffn\u201d and \u201cIP\u201d, the same input and distance metric used in the parametric model, the results are the worst, indicating that"}, {"question": " What is the purpose of using subword tokenization in the base LM mentioned in the text?", "answer": " Subword tokenization is used to train a smaller vocabulary and eliminate the need for adaptive softmax, making the output layer more generalized.", "ref_chunk": "model, with a few additional components: M , mask-to-k, and \u2297. More speci\ufb01cally, some of the design decisions that go into the kNN-LM, and parallels with standard parametric models are: 1. Size of Wds: In the standard parametric model, the size of Wsm is V embedding vectors, each with D dimensions. In the kNN-LM the size of Wds is very large: Nds, the size of the datastore, usually the number of tokens in the entire training corpus. 2. Input representation: In the parametric model, hsm is the output from the feedforward layer in the last transformer block, which we abbreviate \u201cffn\u201d. In contrast, Khandelwal et al. (2020b) rather use as hds the output from the multi-headed attention layer of the last transformer block (before running the representations through the feed-forward network, and after the LayerNorm (Ba et al., 2016)), which we abbreviate as \u201catt\u201d. 3. Similarity & Temperature: In the parametric model, the functional form of \u2297 is the inner product (abbreviated IP), whereas Khandelwal et al. (2020b) use negative squared L2 distance (abbreviated L2) as a similarity function between Wds and hds. As the similarity scores are turned into probability distributions with the softmax function, the choice of softmax temperature (\u03c4 ) can control the scaling of the similarity scores and thus the peakiness of the non-parametric component distribution. 4. Approximation & Sparsi\ufb01cation: In the parametric model, k = V , and no values are masked, but in the kNN-LM, k (cid:28) V , and most of the datastore entries are pruned out. The de\ufb01nition of the mask-to-k(\u00b7) function, i.e. how to select the important datastore embeddings to include in the similarity calculation (in kNN-LM\u2019s case the k nearest neighbors), is a crucial open design choice. In the following sections, we set out to better understand how each of these design decisions contributes to the improvement in accuracy due to the use of kNN-LMs. 3 Baseline kNN-LM Results First, we evaluate the kNN-LM baseline on the Wikitext-103 dataset (Merity et al., 2016), and examine the importance of two design choices: the input representation hds and the similarity function \u2297. In models examined in this paper, the parametric model is a transformer language model with mostly the same architecture as in Khandelwal et al. (2020b). However, We do make modi\ufb01cations to the original base LM (Baevski and Auli, 2018) to accommodate our experimentation need. We using BPE tokenization (Sennrich et al., 2015) to train a smaller vocabulary (33K) than the original (260K) on the training corpus of Wikitext-103, as subword tokenization is ubiquitous in many state-of-the-art language models (Radford et al., 2019; Devlin et al., 2018; Liu et al., 2019; Brown et al., 2020). Using subword tokenization also eliminates the need for adaptive softmax (Joulin et al., 2017). It makes the output layer more generalized, sharing more resemblance to the kNN component as described in Section 2, and facilitates the ablation studies in this paper.1 This base LM has 268M parameters. To get a perspective on how large the datastore is, it is built on the training data that contains nearly 150M BPE tokens, each paired with a context vector of size 1024. This datastore has a total memory consumption of about 300GB. At every retrieval step, we take the top 1024 nearest neighbors, i.e., k = 1024, following Khandelwal et al. (2020b). The interpolated perplexity is computed with optimal interpolation parameter \u03bb tuned according to the perplexity on the development set. \u03bb is \ufb01xed during the inference for all predictions, the same as the standard kNN-LM. 1By training our own version of the base LM from scratch with BPE tokenization and a standard output softmax layer, our LM\u2019s perplexity is worse than that used in the original kNN-LM paper. However, we observe similar relative gains from the additional kNN component. We argue that the base LM\u2019s performance is orthogonal to the study of the factors behind kNN-LM\u2019s improvements. 4 hds \u2297 +#params PPL Interp. PPL Oracle Base LM kNN-LM-L2 kNN-LM-IP kNN-LM-L2 kNN-LM-IP att att ffn ffn L2 Nds \u00d7 D IP Nds \u00d7 D L2 Nds \u00d7 D IP Nds \u00d7 D 0 21.750 \u221e \u221e \u221e \u221e 19.174 19.095 20.734 21.101 14.230 14.077 15.594 16.254 Table 1: Performance of the parametric language model and several kNN-LM variants. Results comparing multiple kNN-LM variants are shown in Table 1. The \ufb01rst row represents the base parametric language model\u2019s perplexity. The second is a formulation analogous to that of Khandelwal et al. (2020b), and in the remaining rows, we vary the input representation hds and distance function \u2297 from Equation 5. All of them use a large datastore with size Nds, approximately 5000 times the size of the vocabulary V , as also re\ufb02ected in \u201c+#params\u201d, the number of additional parameters other than the base LM. We report several important quantities with respect to each model. \u201cPPL\u201d shows the perplexity of only the kNN component of the model pkNN(). This is \u221e for all kNN- LM models in all cases, as when the kNN search does not retrieve any datastore entries corresponding to the true target word wt the probability of the target word will be zero. \u201cOracle\u201d shows the lower bound of the interpolation performance by choosing the best \u03bb for each token in the evaluation dataset, which will either be \u03bb = 0 or \u03bb = 1 depending on whether PLM (wt|ct) > Pknn(wt|ct) or not, respectively. From the table, we can see that: 1. Using the output of the multi-headed attention layer (\u201catt\u201d) as hds (instead of the standard \u201cffn\u201d layer) is crucial for better performance of kNN-LM. 2. In general, using negative squared L2 distance or inner product as a similarity function does not result in a large and consistent difference, although in our setting, IP provides slightly better performance when using the \u201catt\u201d inputs, and slightly worse when using \u201cffn\u201d inputs. 3. Interestingly, when using \u201cffn\u201d and \u201cIP\u201d, the same input and distance metric used in the parametric model, the results are the worst, indicating that"}], "doc_text": "model, with a few additional components: M , mask-to-k, and \u2297. More speci\ufb01cally, some of the design decisions that go into the kNN-LM, and parallels with standard parametric models are: 1. Size of Wds: In the standard parametric model, the size of Wsm is V embedding vectors, each with D dimensions. In the kNN-LM the size of Wds is very large: Nds, the size of the datastore, usually the number of tokens in the entire training corpus. 2. Input representation: In the parametric model, hsm is the output from the feedforward layer in the last transformer block, which we abbreviate \u201cffn\u201d. In contrast, Khandelwal et al. (2020b) rather use as hds the output from the multi-headed attention layer of the last transformer block (before running the representations through the feed-forward network, and after the LayerNorm (Ba et al., 2016)), which we abbreviate as \u201catt\u201d. 3. Similarity & Temperature: In the parametric model, the functional form of \u2297 is the inner product (abbreviated IP), whereas Khandelwal et al. (2020b) use negative squared L2 distance (abbreviated L2) as a similarity function between Wds and hds. As the similarity scores are turned into probability distributions with the softmax function, the choice of softmax temperature (\u03c4 ) can control the scaling of the similarity scores and thus the peakiness of the non-parametric component distribution. 4. Approximation & Sparsi\ufb01cation: In the parametric model, k = V , and no values are masked, but in the kNN-LM, k (cid:28) V , and most of the datastore entries are pruned out. The de\ufb01nition of the mask-to-k(\u00b7) function, i.e. how to select the important datastore embeddings to include in the similarity calculation (in kNN-LM\u2019s case the k nearest neighbors), is a crucial open design choice. In the following sections, we set out to better understand how each of these design decisions contributes to the improvement in accuracy due to the use of kNN-LMs. 3 Baseline kNN-LM Results First, we evaluate the kNN-LM baseline on the Wikitext-103 dataset (Merity et al., 2016), and examine the importance of two design choices: the input representation hds and the similarity function \u2297. In models examined in this paper, the parametric model is a transformer language model with mostly the same architecture as in Khandelwal et al. (2020b). However, We do make modi\ufb01cations to the original base LM (Baevski and Auli, 2018) to accommodate our experimentation need. We using BPE tokenization (Sennrich et al., 2015) to train a smaller vocabulary (33K) than the original (260K) on the training corpus of Wikitext-103, as subword tokenization is ubiquitous in many state-of-the-art language models (Radford et al., 2019; Devlin et al., 2018; Liu et al., 2019; Brown et al., 2020). Using subword tokenization also eliminates the need for adaptive softmax (Joulin et al., 2017). It makes the output layer more generalized, sharing more resemblance to the kNN component as described in Section 2, and facilitates the ablation studies in this paper.1 This base LM has 268M parameters. To get a perspective on how large the datastore is, it is built on the training data that contains nearly 150M BPE tokens, each paired with a context vector of size 1024. This datastore has a total memory consumption of about 300GB. At every retrieval step, we take the top 1024 nearest neighbors, i.e., k = 1024, following Khandelwal et al. (2020b). The interpolated perplexity is computed with optimal interpolation parameter \u03bb tuned according to the perplexity on the development set. \u03bb is \ufb01xed during the inference for all predictions, the same as the standard kNN-LM. 1By training our own version of the base LM from scratch with BPE tokenization and a standard output softmax layer, our LM\u2019s perplexity is worse than that used in the original kNN-LM paper. However, we observe similar relative gains from the additional kNN component. We argue that the base LM\u2019s performance is orthogonal to the study of the factors behind kNN-LM\u2019s improvements. 4 hds \u2297 +#params PPL Interp. PPL Oracle Base LM kNN-LM-L2 kNN-LM-IP kNN-LM-L2 kNN-LM-IP att att ffn ffn L2 Nds \u00d7 D IP Nds \u00d7 D L2 Nds \u00d7 D IP Nds \u00d7 D 0 21.750 \u221e \u221e \u221e \u221e 19.174 19.095 20.734 21.101 14.230 14.077 15.594 16.254 Table 1: Performance of the parametric language model and several kNN-LM variants. Results comparing multiple kNN-LM variants are shown in Table 1. The \ufb01rst row represents the base parametric language model\u2019s perplexity. The second is a formulation analogous to that of Khandelwal et al. (2020b), and in the remaining rows, we vary the input representation hds and distance function \u2297 from Equation 5. All of them use a large datastore with size Nds, approximately 5000 times the size of the vocabulary V , as also re\ufb02ected in \u201c+#params\u201d, the number of additional parameters other than the base LM. We report several important quantities with respect to each model. \u201cPPL\u201d shows the perplexity of only the kNN component of the model pkNN(). This is \u221e for all kNN- LM models in all cases, as when the kNN search does not retrieve any datastore entries corresponding to the true target word wt the probability of the target word will be zero. \u201cOracle\u201d shows the lower bound of the interpolation performance by choosing the best \u03bb for each token in the evaluation dataset, which will either be \u03bb = 0 or \u03bb = 1 depending on whether PLM (wt|ct) > Pknn(wt|ct) or not, respectively. From the table, we can see that: 1. Using the output of the multi-headed attention layer (\u201catt\u201d) as hds (instead of the standard \u201cffn\u201d layer) is crucial for better performance of kNN-LM. 2. In general, using negative squared L2 distance or inner product as a similarity function does not result in a large and consistent difference, although in our setting, IP provides slightly better performance when using the \u201catt\u201d inputs, and slightly worse when using \u201cffn\u201d inputs. 3. Interestingly, when using \u201cffn\u201d and \u201cIP\u201d, the same input and distance metric used in the parametric model, the results are the worst, indicating that"}