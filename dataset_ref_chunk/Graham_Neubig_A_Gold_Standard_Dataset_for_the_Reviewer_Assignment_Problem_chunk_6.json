{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_A_Gold_Standard_Dataset_for_the_Reviewer_Assignment_Problem_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What algorithm relies on general-purpose Embeddings from Language Models to compute textual similarities between submissions and reviewers\u2019 past papers?", "answer": " This algorithm relies on general-purpose Embeddings from Language Models (Peters et al., 2018) to compute textual similarities between submissions and reviewers\u2019 past papers.", "ref_chunk": "This algorithm relies on general-purpose Embeddings from Language Models (Peters et al., 2018) to compute textual similarities between submissions and reviewers\u2019 past papers. Specter. This algorithm employs more specialized document-level embeddings of scienti\ufb01c docu- ments (Cohan et al., 2020). Speci\ufb01cally, Specter explores the citation graph to construct embeddings that are useful for a variety downstream tasks, including the similarity computation we focus on in this work. Specter+MFR. Finally, Specter+MFR further enhances Specter (Chang and McCallum, 2021). Instead of constructing a single embedding of each paper, it constructs multiple embeddings that correspond to di\ufb00erent facets of the paper. These embeddings are then used to compute the similarity scores. We use implementations of these methods that are available on the OpenReview GitHub page1 and execute them with default parameters. ACL paper matching The Association for Computational Linguistics (ACL) is a community of re- searchers working on computational problems involving natural language. The association runs multiple publication venues, including the \ufb02agship ACL conference, and has its own method to compute expertise between papers and reviewers.2 This algorithm is trained on 45,309 abstracts of papers found in the ACL anthology (https://aclanthology.org) downloaded in November 2019. The key idea of the model training is to split each abstract into two non-contiguous parts and treat two parts of the same abstract as a positive example and highly similar parts of di\ufb00erent abstracts as negative examples. The algorithm uses ideas from the work of Wieting et al. (2019, 2022) to learn a simple and e\ufb03cient similarity function that separates positive examples from negative examples. This function is eventually used to compute similarity scores between papers and reviewers. More details on the ACL algorithm are provided in Appendix B. We note that the ACL algorithm is trained on the domain of computational linguistics and hence may su\ufb00er from a distribution shift when applied to the general machine learning domain. That said, in line with ELMo, Specter, and Specter+MFR, we use the ACL algorithm without additional \ufb01ne-tuning. The algorithm used by ACL is developed by a subset of authors of this paper (JW, GN) and we use this implementation in our experiments. Importantly, we note that JW and GN executed the algorithm without having access to the ground truth expertise, and the predicted similarities were independently evaluated by another author of this paper (IS). 6 Results In this section, we report the results of evaluation of algorithms described in Section 5.2. First, we juxtapose all algorithms on our data (Section 6.1). Second, we use the TPMS algorithm to explore various aspects of the similarity-computation problem (Section 6.2). 6.1 Comparison of the algorithms Our \ufb01rst set of results compares the performance of the existing similarity-computation algorithms. To run these algorithms on our data, we need to make some modeling choices faced by conference organizers in practice: Paper representation. First, in their inner-workings, similarity-computation algorithms operate with some representation of the paper content. Possible choices of representations include: (i) title of the paper, (ii) title and abstract, and (iii) full text of the paper. We choose option (ii) as this option is often used in real conferences and is supported by all algorithms we consider in this work. Thus, to predict 1https://github.com/openreview/openreview-expertise 2https://github.com/acl-org/reviewer-paper-matching 10 expertise, algorithms are provided with the title and abstract of each paper (both papers submitted to the conference and papers in reviewers\u2019 publication pro\ufb01les). Reviewer pro\ufb01les. The second important parameter is the choice of papers to include in reviewers\u2019 pro\ufb01les. In real conferences, this choice is often left to reviewers who can manually select the papers they \ufb01nd representative of their expertise. In our experiments, we construct reviewer pro\ufb01les automatically by using the 20 most recent papers from their Semantic Scholar pro\ufb01les. If a reviewer has less than 20 papers published, we include all of them in their pro\ufb01le. Our choice of the reviewer pro\ufb01le size is governed by the observation that the mean length of the reviewer pro\ufb01le in the NeurIPS 2022 conference is 16.5 papers. By setting the maximum number of papers to 20, we achieve the mean pro\ufb01le length of 14.8, thereby operating with the amount of information close to that available to algorithms in real conferences. Statistical aspects To build reviewer pro\ufb01les, we use publication years to order papers by recency, where we break ties uniformly at random. Thus, the content of reviewer pro\ufb01les depends on randomness. To average this randomness out, we repeat the procedure of pro\ufb01le construction and similarity prediction 10 times, and report the mean loss over these iterations. That said, we note that the observed variability due to the randomness in the construction of reviewer pro\ufb01les is negligible, with a standard deviation over all iterations is less than 0.005. The pointwise performance estimates obtained by the procedure above depend on the selection of par- ticipants who contributed to our dataset. To quantify the associated level of uncertainty, we compute 95% con\ufb01dence intervals as follows. For 1,000 iterations, we create a new reviewer pool by sampling participants with replacement and recomputing the loss of each algorithm on the bootstrapped set of reviewers. To save computation time, we do not reconstruct reviewer pro\ufb01les for each of these iterations as the uncertainty associated with the construction of reviewer pro\ufb01les is small. Instead, we reuse pro\ufb01les constructed to obtain pointwise estimates. Finally, we also build con\ufb01dence intervals for the di\ufb00erence in the performance of the algorithms. We do so in addition to the aforementioned procedure since even when the losses of the algorithms \ufb02uctuate with the choice of the bootstrapped dataset, the relative di\ufb00erence in performance of a pair of algorithms may be stable. Speci\ufb01cally, we use the procedure above to build con\ufb01dence intervals for the di\ufb00erence in performance between the TPMS algorithm and each of the more advanced algorithms. TPMS is chosen as a baseline for this comparison due to its simplicity. Results of the comparison Table 3 displays results of the comparison. The \ufb01rst pair of columns presents the loss of each algorithm on our dataset and the associated con\ufb01dence intervals."}, {"question": " What does the Specter algorithm employ for computing textual similarities between submissions?", "answer": " Specter algorithm employs more specialized document-level embeddings of scienti\ufb01c documents (Cohan et al., 2020).", "ref_chunk": "This algorithm relies on general-purpose Embeddings from Language Models (Peters et al., 2018) to compute textual similarities between submissions and reviewers\u2019 past papers. Specter. This algorithm employs more specialized document-level embeddings of scienti\ufb01c docu- ments (Cohan et al., 2020). Speci\ufb01cally, Specter explores the citation graph to construct embeddings that are useful for a variety downstream tasks, including the similarity computation we focus on in this work. Specter+MFR. Finally, Specter+MFR further enhances Specter (Chang and McCallum, 2021). Instead of constructing a single embedding of each paper, it constructs multiple embeddings that correspond to di\ufb00erent facets of the paper. These embeddings are then used to compute the similarity scores. We use implementations of these methods that are available on the OpenReview GitHub page1 and execute them with default parameters. ACL paper matching The Association for Computational Linguistics (ACL) is a community of re- searchers working on computational problems involving natural language. The association runs multiple publication venues, including the \ufb02agship ACL conference, and has its own method to compute expertise between papers and reviewers.2 This algorithm is trained on 45,309 abstracts of papers found in the ACL anthology (https://aclanthology.org) downloaded in November 2019. The key idea of the model training is to split each abstract into two non-contiguous parts and treat two parts of the same abstract as a positive example and highly similar parts of di\ufb00erent abstracts as negative examples. The algorithm uses ideas from the work of Wieting et al. (2019, 2022) to learn a simple and e\ufb03cient similarity function that separates positive examples from negative examples. This function is eventually used to compute similarity scores between papers and reviewers. More details on the ACL algorithm are provided in Appendix B. We note that the ACL algorithm is trained on the domain of computational linguistics and hence may su\ufb00er from a distribution shift when applied to the general machine learning domain. That said, in line with ELMo, Specter, and Specter+MFR, we use the ACL algorithm without additional \ufb01ne-tuning. The algorithm used by ACL is developed by a subset of authors of this paper (JW, GN) and we use this implementation in our experiments. Importantly, we note that JW and GN executed the algorithm without having access to the ground truth expertise, and the predicted similarities were independently evaluated by another author of this paper (IS). 6 Results In this section, we report the results of evaluation of algorithms described in Section 5.2. First, we juxtapose all algorithms on our data (Section 6.1). Second, we use the TPMS algorithm to explore various aspects of the similarity-computation problem (Section 6.2). 6.1 Comparison of the algorithms Our \ufb01rst set of results compares the performance of the existing similarity-computation algorithms. To run these algorithms on our data, we need to make some modeling choices faced by conference organizers in practice: Paper representation. First, in their inner-workings, similarity-computation algorithms operate with some representation of the paper content. Possible choices of representations include: (i) title of the paper, (ii) title and abstract, and (iii) full text of the paper. We choose option (ii) as this option is often used in real conferences and is supported by all algorithms we consider in this work. Thus, to predict 1https://github.com/openreview/openreview-expertise 2https://github.com/acl-org/reviewer-paper-matching 10 expertise, algorithms are provided with the title and abstract of each paper (both papers submitted to the conference and papers in reviewers\u2019 publication pro\ufb01les). Reviewer pro\ufb01les. The second important parameter is the choice of papers to include in reviewers\u2019 pro\ufb01les. In real conferences, this choice is often left to reviewers who can manually select the papers they \ufb01nd representative of their expertise. In our experiments, we construct reviewer pro\ufb01les automatically by using the 20 most recent papers from their Semantic Scholar pro\ufb01les. If a reviewer has less than 20 papers published, we include all of them in their pro\ufb01le. Our choice of the reviewer pro\ufb01le size is governed by the observation that the mean length of the reviewer pro\ufb01le in the NeurIPS 2022 conference is 16.5 papers. By setting the maximum number of papers to 20, we achieve the mean pro\ufb01le length of 14.8, thereby operating with the amount of information close to that available to algorithms in real conferences. Statistical aspects To build reviewer pro\ufb01les, we use publication years to order papers by recency, where we break ties uniformly at random. Thus, the content of reviewer pro\ufb01les depends on randomness. To average this randomness out, we repeat the procedure of pro\ufb01le construction and similarity prediction 10 times, and report the mean loss over these iterations. That said, we note that the observed variability due to the randomness in the construction of reviewer pro\ufb01les is negligible, with a standard deviation over all iterations is less than 0.005. The pointwise performance estimates obtained by the procedure above depend on the selection of par- ticipants who contributed to our dataset. To quantify the associated level of uncertainty, we compute 95% con\ufb01dence intervals as follows. For 1,000 iterations, we create a new reviewer pool by sampling participants with replacement and recomputing the loss of each algorithm on the bootstrapped set of reviewers. To save computation time, we do not reconstruct reviewer pro\ufb01les for each of these iterations as the uncertainty associated with the construction of reviewer pro\ufb01les is small. Instead, we reuse pro\ufb01les constructed to obtain pointwise estimates. Finally, we also build con\ufb01dence intervals for the di\ufb00erence in the performance of the algorithms. We do so in addition to the aforementioned procedure since even when the losses of the algorithms \ufb02uctuate with the choice of the bootstrapped dataset, the relative di\ufb00erence in performance of a pair of algorithms may be stable. Speci\ufb01cally, we use the procedure above to build con\ufb01dence intervals for the di\ufb00erence in performance between the TPMS algorithm and each of the more advanced algorithms. TPMS is chosen as a baseline for this comparison due to its simplicity. Results of the comparison Table 3 displays results of the comparison. The \ufb01rst pair of columns presents the loss of each algorithm on our dataset and the associated con\ufb01dence intervals."}, {"question": " How does the Specter+MFR algorithm enhance the Specter algorithm?", "answer": " Specter+MFR further enhances Specter (Chang and McCallum, 2021) by constructing multiple embeddings of each paper that correspond to different facets of the paper.", "ref_chunk": "This algorithm relies on general-purpose Embeddings from Language Models (Peters et al., 2018) to compute textual similarities between submissions and reviewers\u2019 past papers. Specter. This algorithm employs more specialized document-level embeddings of scienti\ufb01c docu- ments (Cohan et al., 2020). Speci\ufb01cally, Specter explores the citation graph to construct embeddings that are useful for a variety downstream tasks, including the similarity computation we focus on in this work. Specter+MFR. Finally, Specter+MFR further enhances Specter (Chang and McCallum, 2021). Instead of constructing a single embedding of each paper, it constructs multiple embeddings that correspond to di\ufb00erent facets of the paper. These embeddings are then used to compute the similarity scores. We use implementations of these methods that are available on the OpenReview GitHub page1 and execute them with default parameters. ACL paper matching The Association for Computational Linguistics (ACL) is a community of re- searchers working on computational problems involving natural language. The association runs multiple publication venues, including the \ufb02agship ACL conference, and has its own method to compute expertise between papers and reviewers.2 This algorithm is trained on 45,309 abstracts of papers found in the ACL anthology (https://aclanthology.org) downloaded in November 2019. The key idea of the model training is to split each abstract into two non-contiguous parts and treat two parts of the same abstract as a positive example and highly similar parts of di\ufb00erent abstracts as negative examples. The algorithm uses ideas from the work of Wieting et al. (2019, 2022) to learn a simple and e\ufb03cient similarity function that separates positive examples from negative examples. This function is eventually used to compute similarity scores between papers and reviewers. More details on the ACL algorithm are provided in Appendix B. We note that the ACL algorithm is trained on the domain of computational linguistics and hence may su\ufb00er from a distribution shift when applied to the general machine learning domain. That said, in line with ELMo, Specter, and Specter+MFR, we use the ACL algorithm without additional \ufb01ne-tuning. The algorithm used by ACL is developed by a subset of authors of this paper (JW, GN) and we use this implementation in our experiments. Importantly, we note that JW and GN executed the algorithm without having access to the ground truth expertise, and the predicted similarities were independently evaluated by another author of this paper (IS). 6 Results In this section, we report the results of evaluation of algorithms described in Section 5.2. First, we juxtapose all algorithms on our data (Section 6.1). Second, we use the TPMS algorithm to explore various aspects of the similarity-computation problem (Section 6.2). 6.1 Comparison of the algorithms Our \ufb01rst set of results compares the performance of the existing similarity-computation algorithms. To run these algorithms on our data, we need to make some modeling choices faced by conference organizers in practice: Paper representation. First, in their inner-workings, similarity-computation algorithms operate with some representation of the paper content. Possible choices of representations include: (i) title of the paper, (ii) title and abstract, and (iii) full text of the paper. We choose option (ii) as this option is often used in real conferences and is supported by all algorithms we consider in this work. Thus, to predict 1https://github.com/openreview/openreview-expertise 2https://github.com/acl-org/reviewer-paper-matching 10 expertise, algorithms are provided with the title and abstract of each paper (both papers submitted to the conference and papers in reviewers\u2019 publication pro\ufb01les). Reviewer pro\ufb01les. The second important parameter is the choice of papers to include in reviewers\u2019 pro\ufb01les. In real conferences, this choice is often left to reviewers who can manually select the papers they \ufb01nd representative of their expertise. In our experiments, we construct reviewer pro\ufb01les automatically by using the 20 most recent papers from their Semantic Scholar pro\ufb01les. If a reviewer has less than 20 papers published, we include all of them in their pro\ufb01le. Our choice of the reviewer pro\ufb01le size is governed by the observation that the mean length of the reviewer pro\ufb01le in the NeurIPS 2022 conference is 16.5 papers. By setting the maximum number of papers to 20, we achieve the mean pro\ufb01le length of 14.8, thereby operating with the amount of information close to that available to algorithms in real conferences. Statistical aspects To build reviewer pro\ufb01les, we use publication years to order papers by recency, where we break ties uniformly at random. Thus, the content of reviewer pro\ufb01les depends on randomness. To average this randomness out, we repeat the procedure of pro\ufb01le construction and similarity prediction 10 times, and report the mean loss over these iterations. That said, we note that the observed variability due to the randomness in the construction of reviewer pro\ufb01les is negligible, with a standard deviation over all iterations is less than 0.005. The pointwise performance estimates obtained by the procedure above depend on the selection of par- ticipants who contributed to our dataset. To quantify the associated level of uncertainty, we compute 95% con\ufb01dence intervals as follows. For 1,000 iterations, we create a new reviewer pool by sampling participants with replacement and recomputing the loss of each algorithm on the bootstrapped set of reviewers. To save computation time, we do not reconstruct reviewer pro\ufb01les for each of these iterations as the uncertainty associated with the construction of reviewer pro\ufb01les is small. Instead, we reuse pro\ufb01les constructed to obtain pointwise estimates. Finally, we also build con\ufb01dence intervals for the di\ufb00erence in the performance of the algorithms. We do so in addition to the aforementioned procedure since even when the losses of the algorithms \ufb02uctuate with the choice of the bootstrapped dataset, the relative di\ufb00erence in performance of a pair of algorithms may be stable. Speci\ufb01cally, we use the procedure above to build con\ufb01dence intervals for the di\ufb00erence in performance between the TPMS algorithm and each of the more advanced algorithms. TPMS is chosen as a baseline for this comparison due to its simplicity. Results of the comparison Table 3 displays results of the comparison. The \ufb01rst pair of columns presents the loss of each algorithm on our dataset and the associated con\ufb01dence intervals."}, {"question": " What is the key idea of model training in the ACL algorithm?", "answer": " The key idea of the model training in the ACL algorithm is to split each abstract into two non-contiguous parts and treat two parts of the same abstract as a positive example.", "ref_chunk": "This algorithm relies on general-purpose Embeddings from Language Models (Peters et al., 2018) to compute textual similarities between submissions and reviewers\u2019 past papers. Specter. This algorithm employs more specialized document-level embeddings of scienti\ufb01c docu- ments (Cohan et al., 2020). Speci\ufb01cally, Specter explores the citation graph to construct embeddings that are useful for a variety downstream tasks, including the similarity computation we focus on in this work. Specter+MFR. Finally, Specter+MFR further enhances Specter (Chang and McCallum, 2021). Instead of constructing a single embedding of each paper, it constructs multiple embeddings that correspond to di\ufb00erent facets of the paper. These embeddings are then used to compute the similarity scores. We use implementations of these methods that are available on the OpenReview GitHub page1 and execute them with default parameters. ACL paper matching The Association for Computational Linguistics (ACL) is a community of re- searchers working on computational problems involving natural language. The association runs multiple publication venues, including the \ufb02agship ACL conference, and has its own method to compute expertise between papers and reviewers.2 This algorithm is trained on 45,309 abstracts of papers found in the ACL anthology (https://aclanthology.org) downloaded in November 2019. The key idea of the model training is to split each abstract into two non-contiguous parts and treat two parts of the same abstract as a positive example and highly similar parts of di\ufb00erent abstracts as negative examples. The algorithm uses ideas from the work of Wieting et al. (2019, 2022) to learn a simple and e\ufb03cient similarity function that separates positive examples from negative examples. This function is eventually used to compute similarity scores between papers and reviewers. More details on the ACL algorithm are provided in Appendix B. We note that the ACL algorithm is trained on the domain of computational linguistics and hence may su\ufb00er from a distribution shift when applied to the general machine learning domain. That said, in line with ELMo, Specter, and Specter+MFR, we use the ACL algorithm without additional \ufb01ne-tuning. The algorithm used by ACL is developed by a subset of authors of this paper (JW, GN) and we use this implementation in our experiments. Importantly, we note that JW and GN executed the algorithm without having access to the ground truth expertise, and the predicted similarities were independently evaluated by another author of this paper (IS). 6 Results In this section, we report the results of evaluation of algorithms described in Section 5.2. First, we juxtapose all algorithms on our data (Section 6.1). Second, we use the TPMS algorithm to explore various aspects of the similarity-computation problem (Section 6.2). 6.1 Comparison of the algorithms Our \ufb01rst set of results compares the performance of the existing similarity-computation algorithms. To run these algorithms on our data, we need to make some modeling choices faced by conference organizers in practice: Paper representation. First, in their inner-workings, similarity-computation algorithms operate with some representation of the paper content. Possible choices of representations include: (i) title of the paper, (ii) title and abstract, and (iii) full text of the paper. We choose option (ii) as this option is often used in real conferences and is supported by all algorithms we consider in this work. Thus, to predict 1https://github.com/openreview/openreview-expertise 2https://github.com/acl-org/reviewer-paper-matching 10 expertise, algorithms are provided with the title and abstract of each paper (both papers submitted to the conference and papers in reviewers\u2019 publication pro\ufb01les). Reviewer pro\ufb01les. The second important parameter is the choice of papers to include in reviewers\u2019 pro\ufb01les. In real conferences, this choice is often left to reviewers who can manually select the papers they \ufb01nd representative of their expertise. In our experiments, we construct reviewer pro\ufb01les automatically by using the 20 most recent papers from their Semantic Scholar pro\ufb01les. If a reviewer has less than 20 papers published, we include all of them in their pro\ufb01le. Our choice of the reviewer pro\ufb01le size is governed by the observation that the mean length of the reviewer pro\ufb01le in the NeurIPS 2022 conference is 16.5 papers. By setting the maximum number of papers to 20, we achieve the mean pro\ufb01le length of 14.8, thereby operating with the amount of information close to that available to algorithms in real conferences. Statistical aspects To build reviewer pro\ufb01les, we use publication years to order papers by recency, where we break ties uniformly at random. Thus, the content of reviewer pro\ufb01les depends on randomness. To average this randomness out, we repeat the procedure of pro\ufb01le construction and similarity prediction 10 times, and report the mean loss over these iterations. That said, we note that the observed variability due to the randomness in the construction of reviewer pro\ufb01les is negligible, with a standard deviation over all iterations is less than 0.005. The pointwise performance estimates obtained by the procedure above depend on the selection of par- ticipants who contributed to our dataset. To quantify the associated level of uncertainty, we compute 95% con\ufb01dence intervals as follows. For 1,000 iterations, we create a new reviewer pool by sampling participants with replacement and recomputing the loss of each algorithm on the bootstrapped set of reviewers. To save computation time, we do not reconstruct reviewer pro\ufb01les for each of these iterations as the uncertainty associated with the construction of reviewer pro\ufb01les is small. Instead, we reuse pro\ufb01les constructed to obtain pointwise estimates. Finally, we also build con\ufb01dence intervals for the di\ufb00erence in the performance of the algorithms. We do so in addition to the aforementioned procedure since even when the losses of the algorithms \ufb02uctuate with the choice of the bootstrapped dataset, the relative di\ufb00erence in performance of a pair of algorithms may be stable. Speci\ufb01cally, we use the procedure above to build con\ufb01dence intervals for the di\ufb00erence in performance between the TPMS algorithm and each of the more advanced algorithms. TPMS is chosen as a baseline for this comparison due to its simplicity. Results of the comparison Table 3 displays results of the comparison. The \ufb01rst pair of columns presents the loss of each algorithm on our dataset and the associated con\ufb01dence intervals."}, {"question": " What method does the ACL algorithm use to compute expertise between papers and reviewers?", "answer": " The ACL algorithm uses its own method to compute expertise between papers and reviewers.", "ref_chunk": "This algorithm relies on general-purpose Embeddings from Language Models (Peters et al., 2018) to compute textual similarities between submissions and reviewers\u2019 past papers. Specter. This algorithm employs more specialized document-level embeddings of scienti\ufb01c docu- ments (Cohan et al., 2020). Speci\ufb01cally, Specter explores the citation graph to construct embeddings that are useful for a variety downstream tasks, including the similarity computation we focus on in this work. Specter+MFR. Finally, Specter+MFR further enhances Specter (Chang and McCallum, 2021). Instead of constructing a single embedding of each paper, it constructs multiple embeddings that correspond to di\ufb00erent facets of the paper. These embeddings are then used to compute the similarity scores. We use implementations of these methods that are available on the OpenReview GitHub page1 and execute them with default parameters. ACL paper matching The Association for Computational Linguistics (ACL) is a community of re- searchers working on computational problems involving natural language. The association runs multiple publication venues, including the \ufb02agship ACL conference, and has its own method to compute expertise between papers and reviewers.2 This algorithm is trained on 45,309 abstracts of papers found in the ACL anthology (https://aclanthology.org) downloaded in November 2019. The key idea of the model training is to split each abstract into two non-contiguous parts and treat two parts of the same abstract as a positive example and highly similar parts of di\ufb00erent abstracts as negative examples. The algorithm uses ideas from the work of Wieting et al. (2019, 2022) to learn a simple and e\ufb03cient similarity function that separates positive examples from negative examples. This function is eventually used to compute similarity scores between papers and reviewers. More details on the ACL algorithm are provided in Appendix B. We note that the ACL algorithm is trained on the domain of computational linguistics and hence may su\ufb00er from a distribution shift when applied to the general machine learning domain. That said, in line with ELMo, Specter, and Specter+MFR, we use the ACL algorithm without additional \ufb01ne-tuning. The algorithm used by ACL is developed by a subset of authors of this paper (JW, GN) and we use this implementation in our experiments. Importantly, we note that JW and GN executed the algorithm without having access to the ground truth expertise, and the predicted similarities were independently evaluated by another author of this paper (IS). 6 Results In this section, we report the results of evaluation of algorithms described in Section 5.2. First, we juxtapose all algorithms on our data (Section 6.1). Second, we use the TPMS algorithm to explore various aspects of the similarity-computation problem (Section 6.2). 6.1 Comparison of the algorithms Our \ufb01rst set of results compares the performance of the existing similarity-computation algorithms. To run these algorithms on our data, we need to make some modeling choices faced by conference organizers in practice: Paper representation. First, in their inner-workings, similarity-computation algorithms operate with some representation of the paper content. Possible choices of representations include: (i) title of the paper, (ii) title and abstract, and (iii) full text of the paper. We choose option (ii) as this option is often used in real conferences and is supported by all algorithms we consider in this work. Thus, to predict 1https://github.com/openreview/openreview-expertise 2https://github.com/acl-org/reviewer-paper-matching 10 expertise, algorithms are provided with the title and abstract of each paper (both papers submitted to the conference and papers in reviewers\u2019 publication pro\ufb01les). Reviewer pro\ufb01les. The second important parameter is the choice of papers to include in reviewers\u2019 pro\ufb01les. In real conferences, this choice is often left to reviewers who can manually select the papers they \ufb01nd representative of their expertise. In our experiments, we construct reviewer pro\ufb01les automatically by using the 20 most recent papers from their Semantic Scholar pro\ufb01les. If a reviewer has less than 20 papers published, we include all of them in their pro\ufb01le. Our choice of the reviewer pro\ufb01le size is governed by the observation that the mean length of the reviewer pro\ufb01le in the NeurIPS 2022 conference is 16.5 papers. By setting the maximum number of papers to 20, we achieve the mean pro\ufb01le length of 14.8, thereby operating with the amount of information close to that available to algorithms in real conferences. Statistical aspects To build reviewer pro\ufb01les, we use publication years to order papers by recency, where we break ties uniformly at random. Thus, the content of reviewer pro\ufb01les depends on randomness. To average this randomness out, we repeat the procedure of pro\ufb01le construction and similarity prediction 10 times, and report the mean loss over these iterations. That said, we note that the observed variability due to the randomness in the construction of reviewer pro\ufb01les is negligible, with a standard deviation over all iterations is less than 0.005. The pointwise performance estimates obtained by the procedure above depend on the selection of par- ticipants who contributed to our dataset. To quantify the associated level of uncertainty, we compute 95% con\ufb01dence intervals as follows. For 1,000 iterations, we create a new reviewer pool by sampling participants with replacement and recomputing the loss of each algorithm on the bootstrapped set of reviewers. To save computation time, we do not reconstruct reviewer pro\ufb01les for each of these iterations as the uncertainty associated with the construction of reviewer pro\ufb01les is small. Instead, we reuse pro\ufb01les constructed to obtain pointwise estimates. Finally, we also build con\ufb01dence intervals for the di\ufb00erence in the performance of the algorithms. We do so in addition to the aforementioned procedure since even when the losses of the algorithms \ufb02uctuate with the choice of the bootstrapped dataset, the relative di\ufb00erence in performance of a pair of algorithms may be stable. Speci\ufb01cally, we use the procedure above to build con\ufb01dence intervals for the di\ufb00erence in performance between the TPMS algorithm and each of the more advanced algorithms. TPMS is chosen as a baseline for this comparison due to its simplicity. Results of the comparison Table 3 displays results of the comparison. The \ufb01rst pair of columns presents the loss of each algorithm on our dataset and the associated con\ufb01dence intervals."}, {"question": " How many times is the procedure of profile construction and similarity prediction repeated to average out randomness in the ACL algorithm?", "answer": " The procedure of profile construction and similarity prediction is repeated 10 times to average out randomness in the ACL algorithm.", "ref_chunk": "This algorithm relies on general-purpose Embeddings from Language Models (Peters et al., 2018) to compute textual similarities between submissions and reviewers\u2019 past papers. Specter. This algorithm employs more specialized document-level embeddings of scienti\ufb01c docu- ments (Cohan et al., 2020). Speci\ufb01cally, Specter explores the citation graph to construct embeddings that are useful for a variety downstream tasks, including the similarity computation we focus on in this work. Specter+MFR. Finally, Specter+MFR further enhances Specter (Chang and McCallum, 2021). Instead of constructing a single embedding of each paper, it constructs multiple embeddings that correspond to di\ufb00erent facets of the paper. These embeddings are then used to compute the similarity scores. We use implementations of these methods that are available on the OpenReview GitHub page1 and execute them with default parameters. ACL paper matching The Association for Computational Linguistics (ACL) is a community of re- searchers working on computational problems involving natural language. The association runs multiple publication venues, including the \ufb02agship ACL conference, and has its own method to compute expertise between papers and reviewers.2 This algorithm is trained on 45,309 abstracts of papers found in the ACL anthology (https://aclanthology.org) downloaded in November 2019. The key idea of the model training is to split each abstract into two non-contiguous parts and treat two parts of the same abstract as a positive example and highly similar parts of di\ufb00erent abstracts as negative examples. The algorithm uses ideas from the work of Wieting et al. (2019, 2022) to learn a simple and e\ufb03cient similarity function that separates positive examples from negative examples. This function is eventually used to compute similarity scores between papers and reviewers. More details on the ACL algorithm are provided in Appendix B. We note that the ACL algorithm is trained on the domain of computational linguistics and hence may su\ufb00er from a distribution shift when applied to the general machine learning domain. That said, in line with ELMo, Specter, and Specter+MFR, we use the ACL algorithm without additional \ufb01ne-tuning. The algorithm used by ACL is developed by a subset of authors of this paper (JW, GN) and we use this implementation in our experiments. Importantly, we note that JW and GN executed the algorithm without having access to the ground truth expertise, and the predicted similarities were independently evaluated by another author of this paper (IS). 6 Results In this section, we report the results of evaluation of algorithms described in Section 5.2. First, we juxtapose all algorithms on our data (Section 6.1). Second, we use the TPMS algorithm to explore various aspects of the similarity-computation problem (Section 6.2). 6.1 Comparison of the algorithms Our \ufb01rst set of results compares the performance of the existing similarity-computation algorithms. To run these algorithms on our data, we need to make some modeling choices faced by conference organizers in practice: Paper representation. First, in their inner-workings, similarity-computation algorithms operate with some representation of the paper content. Possible choices of representations include: (i) title of the paper, (ii) title and abstract, and (iii) full text of the paper. We choose option (ii) as this option is often used in real conferences and is supported by all algorithms we consider in this work. Thus, to predict 1https://github.com/openreview/openreview-expertise 2https://github.com/acl-org/reviewer-paper-matching 10 expertise, algorithms are provided with the title and abstract of each paper (both papers submitted to the conference and papers in reviewers\u2019 publication pro\ufb01les). Reviewer pro\ufb01les. The second important parameter is the choice of papers to include in reviewers\u2019 pro\ufb01les. In real conferences, this choice is often left to reviewers who can manually select the papers they \ufb01nd representative of their expertise. In our experiments, we construct reviewer pro\ufb01les automatically by using the 20 most recent papers from their Semantic Scholar pro\ufb01les. If a reviewer has less than 20 papers published, we include all of them in their pro\ufb01le. Our choice of the reviewer pro\ufb01le size is governed by the observation that the mean length of the reviewer pro\ufb01le in the NeurIPS 2022 conference is 16.5 papers. By setting the maximum number of papers to 20, we achieve the mean pro\ufb01le length of 14.8, thereby operating with the amount of information close to that available to algorithms in real conferences. Statistical aspects To build reviewer pro\ufb01les, we use publication years to order papers by recency, where we break ties uniformly at random. Thus, the content of reviewer pro\ufb01les depends on randomness. To average this randomness out, we repeat the procedure of pro\ufb01le construction and similarity prediction 10 times, and report the mean loss over these iterations. That said, we note that the observed variability due to the randomness in the construction of reviewer pro\ufb01les is negligible, with a standard deviation over all iterations is less than 0.005. The pointwise performance estimates obtained by the procedure above depend on the selection of par- ticipants who contributed to our dataset. To quantify the associated level of uncertainty, we compute 95% con\ufb01dence intervals as follows. For 1,000 iterations, we create a new reviewer pool by sampling participants with replacement and recomputing the loss of each algorithm on the bootstrapped set of reviewers. To save computation time, we do not reconstruct reviewer pro\ufb01les for each of these iterations as the uncertainty associated with the construction of reviewer pro\ufb01les is small. Instead, we reuse pro\ufb01les constructed to obtain pointwise estimates. Finally, we also build con\ufb01dence intervals for the di\ufb00erence in the performance of the algorithms. We do so in addition to the aforementioned procedure since even when the losses of the algorithms \ufb02uctuate with the choice of the bootstrapped dataset, the relative di\ufb00erence in performance of a pair of algorithms may be stable. Speci\ufb01cally, we use the procedure above to build con\ufb01dence intervals for the di\ufb00erence in performance between the TPMS algorithm and each of the more advanced algorithms. TPMS is chosen as a baseline for this comparison due to its simplicity. Results of the comparison Table 3 displays results of the comparison. The \ufb01rst pair of columns presents the loss of each algorithm on our dataset and the associated con\ufb01dence intervals."}, {"question": " What do conference organizers need to decide regarding the representation of paper content for similarity-computation algorithms?", "answer": " Conference organizers need to decide on the representation of paper content, such as using the title and abstract, for similarity-computation algorithms.", "ref_chunk": "This algorithm relies on general-purpose Embeddings from Language Models (Peters et al., 2018) to compute textual similarities between submissions and reviewers\u2019 past papers. Specter. This algorithm employs more specialized document-level embeddings of scienti\ufb01c docu- ments (Cohan et al., 2020). Speci\ufb01cally, Specter explores the citation graph to construct embeddings that are useful for a variety downstream tasks, including the similarity computation we focus on in this work. Specter+MFR. Finally, Specter+MFR further enhances Specter (Chang and McCallum, 2021). Instead of constructing a single embedding of each paper, it constructs multiple embeddings that correspond to di\ufb00erent facets of the paper. These embeddings are then used to compute the similarity scores. We use implementations of these methods that are available on the OpenReview GitHub page1 and execute them with default parameters. ACL paper matching The Association for Computational Linguistics (ACL) is a community of re- searchers working on computational problems involving natural language. The association runs multiple publication venues, including the \ufb02agship ACL conference, and has its own method to compute expertise between papers and reviewers.2 This algorithm is trained on 45,309 abstracts of papers found in the ACL anthology (https://aclanthology.org) downloaded in November 2019. The key idea of the model training is to split each abstract into two non-contiguous parts and treat two parts of the same abstract as a positive example and highly similar parts of di\ufb00erent abstracts as negative examples. The algorithm uses ideas from the work of Wieting et al. (2019, 2022) to learn a simple and e\ufb03cient similarity function that separates positive examples from negative examples. This function is eventually used to compute similarity scores between papers and reviewers. More details on the ACL algorithm are provided in Appendix B. We note that the ACL algorithm is trained on the domain of computational linguistics and hence may su\ufb00er from a distribution shift when applied to the general machine learning domain. That said, in line with ELMo, Specter, and Specter+MFR, we use the ACL algorithm without additional \ufb01ne-tuning. The algorithm used by ACL is developed by a subset of authors of this paper (JW, GN) and we use this implementation in our experiments. Importantly, we note that JW and GN executed the algorithm without having access to the ground truth expertise, and the predicted similarities were independently evaluated by another author of this paper (IS). 6 Results In this section, we report the results of evaluation of algorithms described in Section 5.2. First, we juxtapose all algorithms on our data (Section 6.1). Second, we use the TPMS algorithm to explore various aspects of the similarity-computation problem (Section 6.2). 6.1 Comparison of the algorithms Our \ufb01rst set of results compares the performance of the existing similarity-computation algorithms. To run these algorithms on our data, we need to make some modeling choices faced by conference organizers in practice: Paper representation. First, in their inner-workings, similarity-computation algorithms operate with some representation of the paper content. Possible choices of representations include: (i) title of the paper, (ii) title and abstract, and (iii) full text of the paper. We choose option (ii) as this option is often used in real conferences and is supported by all algorithms we consider in this work. Thus, to predict 1https://github.com/openreview/openreview-expertise 2https://github.com/acl-org/reviewer-paper-matching 10 expertise, algorithms are provided with the title and abstract of each paper (both papers submitted to the conference and papers in reviewers\u2019 publication pro\ufb01les). Reviewer pro\ufb01les. The second important parameter is the choice of papers to include in reviewers\u2019 pro\ufb01les. In real conferences, this choice is often left to reviewers who can manually select the papers they \ufb01nd representative of their expertise. In our experiments, we construct reviewer pro\ufb01les automatically by using the 20 most recent papers from their Semantic Scholar pro\ufb01les. If a reviewer has less than 20 papers published, we include all of them in their pro\ufb01le. Our choice of the reviewer pro\ufb01le size is governed by the observation that the mean length of the reviewer pro\ufb01le in the NeurIPS 2022 conference is 16.5 papers. By setting the maximum number of papers to 20, we achieve the mean pro\ufb01le length of 14.8, thereby operating with the amount of information close to that available to algorithms in real conferences. Statistical aspects To build reviewer pro\ufb01les, we use publication years to order papers by recency, where we break ties uniformly at random. Thus, the content of reviewer pro\ufb01les depends on randomness. To average this randomness out, we repeat the procedure of pro\ufb01le construction and similarity prediction 10 times, and report the mean loss over these iterations. That said, we note that the observed variability due to the randomness in the construction of reviewer pro\ufb01les is negligible, with a standard deviation over all iterations is less than 0.005. The pointwise performance estimates obtained by the procedure above depend on the selection of par- ticipants who contributed to our dataset. To quantify the associated level of uncertainty, we compute 95% con\ufb01dence intervals as follows. For 1,000 iterations, we create a new reviewer pool by sampling participants with replacement and recomputing the loss of each algorithm on the bootstrapped set of reviewers. To save computation time, we do not reconstruct reviewer pro\ufb01les for each of these iterations as the uncertainty associated with the construction of reviewer pro\ufb01les is small. Instead, we reuse pro\ufb01les constructed to obtain pointwise estimates. Finally, we also build con\ufb01dence intervals for the di\ufb00erence in the performance of the algorithms. We do so in addition to the aforementioned procedure since even when the losses of the algorithms \ufb02uctuate with the choice of the bootstrapped dataset, the relative di\ufb00erence in performance of a pair of algorithms may be stable. Speci\ufb01cally, we use the procedure above to build con\ufb01dence intervals for the di\ufb00erence in performance between the TPMS algorithm and each of the more advanced algorithms. TPMS is chosen as a baseline for this comparison due to its simplicity. Results of the comparison Table 3 displays results of the comparison. The \ufb01rst pair of columns presents the loss of each algorithm on our dataset and the associated con\ufb01dence intervals."}, {"question": " How are reviewer profiles constructed in the experiments conducted?", "answer": " Reviewer profiles are constructed automatically by using the 20 most recent papers from Semantic Scholar profiles of reviewers.", "ref_chunk": "This algorithm relies on general-purpose Embeddings from Language Models (Peters et al., 2018) to compute textual similarities between submissions and reviewers\u2019 past papers. Specter. This algorithm employs more specialized document-level embeddings of scienti\ufb01c docu- ments (Cohan et al., 2020). Speci\ufb01cally, Specter explores the citation graph to construct embeddings that are useful for a variety downstream tasks, including the similarity computation we focus on in this work. Specter+MFR. Finally, Specter+MFR further enhances Specter (Chang and McCallum, 2021). Instead of constructing a single embedding of each paper, it constructs multiple embeddings that correspond to di\ufb00erent facets of the paper. These embeddings are then used to compute the similarity scores. We use implementations of these methods that are available on the OpenReview GitHub page1 and execute them with default parameters. ACL paper matching The Association for Computational Linguistics (ACL) is a community of re- searchers working on computational problems involving natural language. The association runs multiple publication venues, including the \ufb02agship ACL conference, and has its own method to compute expertise between papers and reviewers.2 This algorithm is trained on 45,309 abstracts of papers found in the ACL anthology (https://aclanthology.org) downloaded in November 2019. The key idea of the model training is to split each abstract into two non-contiguous parts and treat two parts of the same abstract as a positive example and highly similar parts of di\ufb00erent abstracts as negative examples. The algorithm uses ideas from the work of Wieting et al. (2019, 2022) to learn a simple and e\ufb03cient similarity function that separates positive examples from negative examples. This function is eventually used to compute similarity scores between papers and reviewers. More details on the ACL algorithm are provided in Appendix B. We note that the ACL algorithm is trained on the domain of computational linguistics and hence may su\ufb00er from a distribution shift when applied to the general machine learning domain. That said, in line with ELMo, Specter, and Specter+MFR, we use the ACL algorithm without additional \ufb01ne-tuning. The algorithm used by ACL is developed by a subset of authors of this paper (JW, GN) and we use this implementation in our experiments. Importantly, we note that JW and GN executed the algorithm without having access to the ground truth expertise, and the predicted similarities were independently evaluated by another author of this paper (IS). 6 Results In this section, we report the results of evaluation of algorithms described in Section 5.2. First, we juxtapose all algorithms on our data (Section 6.1). Second, we use the TPMS algorithm to explore various aspects of the similarity-computation problem (Section 6.2). 6.1 Comparison of the algorithms Our \ufb01rst set of results compares the performance of the existing similarity-computation algorithms. To run these algorithms on our data, we need to make some modeling choices faced by conference organizers in practice: Paper representation. First, in their inner-workings, similarity-computation algorithms operate with some representation of the paper content. Possible choices of representations include: (i) title of the paper, (ii) title and abstract, and (iii) full text of the paper. We choose option (ii) as this option is often used in real conferences and is supported by all algorithms we consider in this work. Thus, to predict 1https://github.com/openreview/openreview-expertise 2https://github.com/acl-org/reviewer-paper-matching 10 expertise, algorithms are provided with the title and abstract of each paper (both papers submitted to the conference and papers in reviewers\u2019 publication pro\ufb01les). Reviewer pro\ufb01les. The second important parameter is the choice of papers to include in reviewers\u2019 pro\ufb01les. In real conferences, this choice is often left to reviewers who can manually select the papers they \ufb01nd representative of their expertise. In our experiments, we construct reviewer pro\ufb01les automatically by using the 20 most recent papers from their Semantic Scholar pro\ufb01les. If a reviewer has less than 20 papers published, we include all of them in their pro\ufb01le. Our choice of the reviewer pro\ufb01le size is governed by the observation that the mean length of the reviewer pro\ufb01le in the NeurIPS 2022 conference is 16.5 papers. By setting the maximum number of papers to 20, we achieve the mean pro\ufb01le length of 14.8, thereby operating with the amount of information close to that available to algorithms in real conferences. Statistical aspects To build reviewer pro\ufb01les, we use publication years to order papers by recency, where we break ties uniformly at random. Thus, the content of reviewer pro\ufb01les depends on randomness. To average this randomness out, we repeat the procedure of pro\ufb01le construction and similarity prediction 10 times, and report the mean loss over these iterations. That said, we note that the observed variability due to the randomness in the construction of reviewer pro\ufb01les is negligible, with a standard deviation over all iterations is less than 0.005. The pointwise performance estimates obtained by the procedure above depend on the selection of par- ticipants who contributed to our dataset. To quantify the associated level of uncertainty, we compute 95% con\ufb01dence intervals as follows. For 1,000 iterations, we create a new reviewer pool by sampling participants with replacement and recomputing the loss of each algorithm on the bootstrapped set of reviewers. To save computation time, we do not reconstruct reviewer pro\ufb01les for each of these iterations as the uncertainty associated with the construction of reviewer pro\ufb01les is small. Instead, we reuse pro\ufb01les constructed to obtain pointwise estimates. Finally, we also build con\ufb01dence intervals for the di\ufb00erence in the performance of the algorithms. We do so in addition to the aforementioned procedure since even when the losses of the algorithms \ufb02uctuate with the choice of the bootstrapped dataset, the relative di\ufb00erence in performance of a pair of algorithms may be stable. Speci\ufb01cally, we use the procedure above to build con\ufb01dence intervals for the di\ufb00erence in performance between the TPMS algorithm and each of the more advanced algorithms. TPMS is chosen as a baseline for this comparison due to its simplicity. Results of the comparison Table 3 displays results of the comparison. The \ufb01rst pair of columns presents the loss of each algorithm on our dataset and the associated con\ufb01dence intervals."}, {"question": " Why is the TPMS algorithm chosen as a baseline for comparison with more advanced algorithms?", "answer": " The TPMS algorithm is chosen as a baseline for comparison due to its simplicity.", "ref_chunk": "This algorithm relies on general-purpose Embeddings from Language Models (Peters et al., 2018) to compute textual similarities between submissions and reviewers\u2019 past papers. Specter. This algorithm employs more specialized document-level embeddings of scienti\ufb01c docu- ments (Cohan et al., 2020). Speci\ufb01cally, Specter explores the citation graph to construct embeddings that are useful for a variety downstream tasks, including the similarity computation we focus on in this work. Specter+MFR. Finally, Specter+MFR further enhances Specter (Chang and McCallum, 2021). Instead of constructing a single embedding of each paper, it constructs multiple embeddings that correspond to di\ufb00erent facets of the paper. These embeddings are then used to compute the similarity scores. We use implementations of these methods that are available on the OpenReview GitHub page1 and execute them with default parameters. ACL paper matching The Association for Computational Linguistics (ACL) is a community of re- searchers working on computational problems involving natural language. The association runs multiple publication venues, including the \ufb02agship ACL conference, and has its own method to compute expertise between papers and reviewers.2 This algorithm is trained on 45,309 abstracts of papers found in the ACL anthology (https://aclanthology.org) downloaded in November 2019. The key idea of the model training is to split each abstract into two non-contiguous parts and treat two parts of the same abstract as a positive example and highly similar parts of di\ufb00erent abstracts as negative examples. The algorithm uses ideas from the work of Wieting et al. (2019, 2022) to learn a simple and e\ufb03cient similarity function that separates positive examples from negative examples. This function is eventually used to compute similarity scores between papers and reviewers. More details on the ACL algorithm are provided in Appendix B. We note that the ACL algorithm is trained on the domain of computational linguistics and hence may su\ufb00er from a distribution shift when applied to the general machine learning domain. That said, in line with ELMo, Specter, and Specter+MFR, we use the ACL algorithm without additional \ufb01ne-tuning. The algorithm used by ACL is developed by a subset of authors of this paper (JW, GN) and we use this implementation in our experiments. Importantly, we note that JW and GN executed the algorithm without having access to the ground truth expertise, and the predicted similarities were independently evaluated by another author of this paper (IS). 6 Results In this section, we report the results of evaluation of algorithms described in Section 5.2. First, we juxtapose all algorithms on our data (Section 6.1). Second, we use the TPMS algorithm to explore various aspects of the similarity-computation problem (Section 6.2). 6.1 Comparison of the algorithms Our \ufb01rst set of results compares the performance of the existing similarity-computation algorithms. To run these algorithms on our data, we need to make some modeling choices faced by conference organizers in practice: Paper representation. First, in their inner-workings, similarity-computation algorithms operate with some representation of the paper content. Possible choices of representations include: (i) title of the paper, (ii) title and abstract, and (iii) full text of the paper. We choose option (ii) as this option is often used in real conferences and is supported by all algorithms we consider in this work. Thus, to predict 1https://github.com/openreview/openreview-expertise 2https://github.com/acl-org/reviewer-paper-matching 10 expertise, algorithms are provided with the title and abstract of each paper (both papers submitted to the conference and papers in reviewers\u2019 publication pro\ufb01les). Reviewer pro\ufb01les. The second important parameter is the choice of papers to include in reviewers\u2019 pro\ufb01les. In real conferences, this choice is often left to reviewers who can manually select the papers they \ufb01nd representative of their expertise. In our experiments, we construct reviewer pro\ufb01les automatically by using the 20 most recent papers from their Semantic Scholar pro\ufb01les. If a reviewer has less than 20 papers published, we include all of them in their pro\ufb01le. Our choice of the reviewer pro\ufb01le size is governed by the observation that the mean length of the reviewer pro\ufb01le in the NeurIPS 2022 conference is 16.5 papers. By setting the maximum number of papers to 20, we achieve the mean pro\ufb01le length of 14.8, thereby operating with the amount of information close to that available to algorithms in real conferences. Statistical aspects To build reviewer pro\ufb01les, we use publication years to order papers by recency, where we break ties uniformly at random. Thus, the content of reviewer pro\ufb01les depends on randomness. To average this randomness out, we repeat the procedure of pro\ufb01le construction and similarity prediction 10 times, and report the mean loss over these iterations. That said, we note that the observed variability due to the randomness in the construction of reviewer pro\ufb01les is negligible, with a standard deviation over all iterations is less than 0.005. The pointwise performance estimates obtained by the procedure above depend on the selection of par- ticipants who contributed to our dataset. To quantify the associated level of uncertainty, we compute 95% con\ufb01dence intervals as follows. For 1,000 iterations, we create a new reviewer pool by sampling participants with replacement and recomputing the loss of each algorithm on the bootstrapped set of reviewers. To save computation time, we do not reconstruct reviewer pro\ufb01les for each of these iterations as the uncertainty associated with the construction of reviewer pro\ufb01les is small. Instead, we reuse pro\ufb01les constructed to obtain pointwise estimates. Finally, we also build con\ufb01dence intervals for the di\ufb00erence in the performance of the algorithms. We do so in addition to the aforementioned procedure since even when the losses of the algorithms \ufb02uctuate with the choice of the bootstrapped dataset, the relative di\ufb00erence in performance of a pair of algorithms may be stable. Speci\ufb01cally, we use the procedure above to build con\ufb01dence intervals for the di\ufb00erence in performance between the TPMS algorithm and each of the more advanced algorithms. TPMS is chosen as a baseline for this comparison due to its simplicity. Results of the comparison Table 3 displays results of the comparison. The \ufb01rst pair of columns presents the loss of each algorithm on our dataset and the associated con\ufb01dence intervals."}, {"question": " What statistical aspect is used to build confidence intervals for the difference in performance between the TPMS algorithm and more advanced algorithms?", "answer": " To build confidence intervals for the difference in performance, 1,000 iterations are run and new reviewer pools are sampled with replacement.", "ref_chunk": "This algorithm relies on general-purpose Embeddings from Language Models (Peters et al., 2018) to compute textual similarities between submissions and reviewers\u2019 past papers. Specter. This algorithm employs more specialized document-level embeddings of scienti\ufb01c docu- ments (Cohan et al., 2020). Speci\ufb01cally, Specter explores the citation graph to construct embeddings that are useful for a variety downstream tasks, including the similarity computation we focus on in this work. Specter+MFR. Finally, Specter+MFR further enhances Specter (Chang and McCallum, 2021). Instead of constructing a single embedding of each paper, it constructs multiple embeddings that correspond to di\ufb00erent facets of the paper. These embeddings are then used to compute the similarity scores. We use implementations of these methods that are available on the OpenReview GitHub page1 and execute them with default parameters. ACL paper matching The Association for Computational Linguistics (ACL) is a community of re- searchers working on computational problems involving natural language. The association runs multiple publication venues, including the \ufb02agship ACL conference, and has its own method to compute expertise between papers and reviewers.2 This algorithm is trained on 45,309 abstracts of papers found in the ACL anthology (https://aclanthology.org) downloaded in November 2019. The key idea of the model training is to split each abstract into two non-contiguous parts and treat two parts of the same abstract as a positive example and highly similar parts of di\ufb00erent abstracts as negative examples. The algorithm uses ideas from the work of Wieting et al. (2019, 2022) to learn a simple and e\ufb03cient similarity function that separates positive examples from negative examples. This function is eventually used to compute similarity scores between papers and reviewers. More details on the ACL algorithm are provided in Appendix B. We note that the ACL algorithm is trained on the domain of computational linguistics and hence may su\ufb00er from a distribution shift when applied to the general machine learning domain. That said, in line with ELMo, Specter, and Specter+MFR, we use the ACL algorithm without additional \ufb01ne-tuning. The algorithm used by ACL is developed by a subset of authors of this paper (JW, GN) and we use this implementation in our experiments. Importantly, we note that JW and GN executed the algorithm without having access to the ground truth expertise, and the predicted similarities were independently evaluated by another author of this paper (IS). 6 Results In this section, we report the results of evaluation of algorithms described in Section 5.2. First, we juxtapose all algorithms on our data (Section 6.1). Second, we use the TPMS algorithm to explore various aspects of the similarity-computation problem (Section 6.2). 6.1 Comparison of the algorithms Our \ufb01rst set of results compares the performance of the existing similarity-computation algorithms. To run these algorithms on our data, we need to make some modeling choices faced by conference organizers in practice: Paper representation. First, in their inner-workings, similarity-computation algorithms operate with some representation of the paper content. Possible choices of representations include: (i) title of the paper, (ii) title and abstract, and (iii) full text of the paper. We choose option (ii) as this option is often used in real conferences and is supported by all algorithms we consider in this work. Thus, to predict 1https://github.com/openreview/openreview-expertise 2https://github.com/acl-org/reviewer-paper-matching 10 expertise, algorithms are provided with the title and abstract of each paper (both papers submitted to the conference and papers in reviewers\u2019 publication pro\ufb01les). Reviewer pro\ufb01les. The second important parameter is the choice of papers to include in reviewers\u2019 pro\ufb01les. In real conferences, this choice is often left to reviewers who can manually select the papers they \ufb01nd representative of their expertise. In our experiments, we construct reviewer pro\ufb01les automatically by using the 20 most recent papers from their Semantic Scholar pro\ufb01les. If a reviewer has less than 20 papers published, we include all of them in their pro\ufb01le. Our choice of the reviewer pro\ufb01le size is governed by the observation that the mean length of the reviewer pro\ufb01le in the NeurIPS 2022 conference is 16.5 papers. By setting the maximum number of papers to 20, we achieve the mean pro\ufb01le length of 14.8, thereby operating with the amount of information close to that available to algorithms in real conferences. Statistical aspects To build reviewer pro\ufb01les, we use publication years to order papers by recency, where we break ties uniformly at random. Thus, the content of reviewer pro\ufb01les depends on randomness. To average this randomness out, we repeat the procedure of pro\ufb01le construction and similarity prediction 10 times, and report the mean loss over these iterations. That said, we note that the observed variability due to the randomness in the construction of reviewer pro\ufb01les is negligible, with a standard deviation over all iterations is less than 0.005. The pointwise performance estimates obtained by the procedure above depend on the selection of par- ticipants who contributed to our dataset. To quantify the associated level of uncertainty, we compute 95% con\ufb01dence intervals as follows. For 1,000 iterations, we create a new reviewer pool by sampling participants with replacement and recomputing the loss of each algorithm on the bootstrapped set of reviewers. To save computation time, we do not reconstruct reviewer pro\ufb01les for each of these iterations as the uncertainty associated with the construction of reviewer pro\ufb01les is small. Instead, we reuse pro\ufb01les constructed to obtain pointwise estimates. Finally, we also build con\ufb01dence intervals for the di\ufb00erence in the performance of the algorithms. We do so in addition to the aforementioned procedure since even when the losses of the algorithms \ufb02uctuate with the choice of the bootstrapped dataset, the relative di\ufb00erence in performance of a pair of algorithms may be stable. Speci\ufb01cally, we use the procedure above to build con\ufb01dence intervals for the di\ufb00erence in performance between the TPMS algorithm and each of the more advanced algorithms. TPMS is chosen as a baseline for this comparison due to its simplicity. Results of the comparison Table 3 displays results of the comparison. The \ufb01rst pair of columns presents the loss of each algorithm on our dataset and the associated con\ufb01dence intervals."}], "doc_text": "This algorithm relies on general-purpose Embeddings from Language Models (Peters et al., 2018) to compute textual similarities between submissions and reviewers\u2019 past papers. Specter. This algorithm employs more specialized document-level embeddings of scienti\ufb01c docu- ments (Cohan et al., 2020). Speci\ufb01cally, Specter explores the citation graph to construct embeddings that are useful for a variety downstream tasks, including the similarity computation we focus on in this work. Specter+MFR. Finally, Specter+MFR further enhances Specter (Chang and McCallum, 2021). Instead of constructing a single embedding of each paper, it constructs multiple embeddings that correspond to di\ufb00erent facets of the paper. These embeddings are then used to compute the similarity scores. We use implementations of these methods that are available on the OpenReview GitHub page1 and execute them with default parameters. ACL paper matching The Association for Computational Linguistics (ACL) is a community of re- searchers working on computational problems involving natural language. The association runs multiple publication venues, including the \ufb02agship ACL conference, and has its own method to compute expertise between papers and reviewers.2 This algorithm is trained on 45,309 abstracts of papers found in the ACL anthology (https://aclanthology.org) downloaded in November 2019. The key idea of the model training is to split each abstract into two non-contiguous parts and treat two parts of the same abstract as a positive example and highly similar parts of di\ufb00erent abstracts as negative examples. The algorithm uses ideas from the work of Wieting et al. (2019, 2022) to learn a simple and e\ufb03cient similarity function that separates positive examples from negative examples. This function is eventually used to compute similarity scores between papers and reviewers. More details on the ACL algorithm are provided in Appendix B. We note that the ACL algorithm is trained on the domain of computational linguistics and hence may su\ufb00er from a distribution shift when applied to the general machine learning domain. That said, in line with ELMo, Specter, and Specter+MFR, we use the ACL algorithm without additional \ufb01ne-tuning. The algorithm used by ACL is developed by a subset of authors of this paper (JW, GN) and we use this implementation in our experiments. Importantly, we note that JW and GN executed the algorithm without having access to the ground truth expertise, and the predicted similarities were independently evaluated by another author of this paper (IS). 6 Results In this section, we report the results of evaluation of algorithms described in Section 5.2. First, we juxtapose all algorithms on our data (Section 6.1). Second, we use the TPMS algorithm to explore various aspects of the similarity-computation problem (Section 6.2). 6.1 Comparison of the algorithms Our \ufb01rst set of results compares the performance of the existing similarity-computation algorithms. To run these algorithms on our data, we need to make some modeling choices faced by conference organizers in practice: Paper representation. First, in their inner-workings, similarity-computation algorithms operate with some representation of the paper content. Possible choices of representations include: (i) title of the paper, (ii) title and abstract, and (iii) full text of the paper. We choose option (ii) as this option is often used in real conferences and is supported by all algorithms we consider in this work. Thus, to predict 1https://github.com/openreview/openreview-expertise 2https://github.com/acl-org/reviewer-paper-matching 10 expertise, algorithms are provided with the title and abstract of each paper (both papers submitted to the conference and papers in reviewers\u2019 publication pro\ufb01les). Reviewer pro\ufb01les. The second important parameter is the choice of papers to include in reviewers\u2019 pro\ufb01les. In real conferences, this choice is often left to reviewers who can manually select the papers they \ufb01nd representative of their expertise. In our experiments, we construct reviewer pro\ufb01les automatically by using the 20 most recent papers from their Semantic Scholar pro\ufb01les. If a reviewer has less than 20 papers published, we include all of them in their pro\ufb01le. Our choice of the reviewer pro\ufb01le size is governed by the observation that the mean length of the reviewer pro\ufb01le in the NeurIPS 2022 conference is 16.5 papers. By setting the maximum number of papers to 20, we achieve the mean pro\ufb01le length of 14.8, thereby operating with the amount of information close to that available to algorithms in real conferences. Statistical aspects To build reviewer pro\ufb01les, we use publication years to order papers by recency, where we break ties uniformly at random. Thus, the content of reviewer pro\ufb01les depends on randomness. To average this randomness out, we repeat the procedure of pro\ufb01le construction and similarity prediction 10 times, and report the mean loss over these iterations. That said, we note that the observed variability due to the randomness in the construction of reviewer pro\ufb01les is negligible, with a standard deviation over all iterations is less than 0.005. The pointwise performance estimates obtained by the procedure above depend on the selection of par- ticipants who contributed to our dataset. To quantify the associated level of uncertainty, we compute 95% con\ufb01dence intervals as follows. For 1,000 iterations, we create a new reviewer pool by sampling participants with replacement and recomputing the loss of each algorithm on the bootstrapped set of reviewers. To save computation time, we do not reconstruct reviewer pro\ufb01les for each of these iterations as the uncertainty associated with the construction of reviewer pro\ufb01les is small. Instead, we reuse pro\ufb01les constructed to obtain pointwise estimates. Finally, we also build con\ufb01dence intervals for the di\ufb00erence in the performance of the algorithms. We do so in addition to the aforementioned procedure since even when the losses of the algorithms \ufb02uctuate with the choice of the bootstrapped dataset, the relative di\ufb00erence in performance of a pair of algorithms may be stable. Speci\ufb01cally, we use the procedure above to build con\ufb01dence intervals for the di\ufb00erence in performance between the TPMS algorithm and each of the more advanced algorithms. TPMS is chosen as a baseline for this comparison due to its simplicity. Results of the comparison Table 3 displays results of the comparison. The \ufb01rst pair of columns presents the loss of each algorithm on our dataset and the associated con\ufb01dence intervals."}