{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_DPHuBERT:_Joint_Distillation_and_Pruning_of_Self-Supervised_Speech_Models_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the structured pruning method HJ-Pruning?", "answer": " The main focus is to jointly prune heterogeneous components (CNN and Transformer) of speech SSL models.", "ref_chunk": "HJ-Pruning [25] is a structured pruning method that jointly prunes heterogeneous components (i.e., CNN and Transformer) of speech SSL models. It significantly reduces the total computation while retaining good performance in ASR and SLU. These methods deal with specific downstream tasks, but do not investigate the universal speech representa- tions. Our work focuses on task-agnostic structured pruning of speech SSL. As there is no labeled data for normal supervised training, we employ a distillation objective along with pruning. Once-for-all training. Compression methods typically gen- erate a single model with a pre-determined size. LightHu- BERT [28] deploys once-for-all training [29] to obtain numer- ous weight-sharing subnets, which shows very strong perfor- mance on SUPERB [6]. However, it requires an expensive two- stage training process and an advanced distillation loss inspired by data2vec [30]. According to the authors, compressing Hu- BERT Base already takes 2k GPUs hours (i.e., 62 hours with 32 V100 GPUs and 19 hours with 8 GPUs for two stages, respec- tively), which is prohibitive for academic researchers and small businesses. Unlike LightHuBERT, our work aims to compress an existing speech SSL model to a specific sparsity ratio within a manageable amount of training time, which is consistent with the standard setup of prior distillation methods [12, 13]. {0, 4, 8, 12} for base models and {0, 8, 16, 24} for large mod- els. The 0th layer is the output of CNN, which is also the input to the first Transformer layer. The loss function L measures the difference between two feature sequences, which can be L1, L2 or cosine distances [12\u201314]. We follow [12, 14] to use a combi- nation of L1 and cosine distances with equal weights. 3.3. Joint distillation and structured pruning Structured pruning of the student model is formulated as learn- ing a sparse model through L0 regularization [31], which has been explored in NLP [19,20] and speech [25]. The method will be briefly introduced below. For more comprehensive deriva- tions, please refer to prior research [19, 20, 25, 31]. Consider a frozen teacher model f tea(\u00b7) and a student model f stu(\u00b7; \u03b8) with learnable parameters \u03b8 = {\u03b8j}n j=1. Each \u03b8j is a group of prunable parameters (including convolution channels, atten- tion heads, and FFN intermediate units) and there are n groups in total. We define a binary variable zj (called mask) for each \u03b8j. The masks z follow a probability distribution q(z; \u03b1) with parameters \u03b1. The regularized distillation objective is: min \u03b8,\u03b1 Ez\u223cq (cid:34) 1 D D (cid:88) Ldis (cid:16) f tea(xk), f stu(xk; \u02dc\u03b8) (cid:17) + \u03bb\u2225 \u02dc\u03b8\u22250 (cid:35) , k=1 3. DPHuBERT (2) 3.1. Training procedure Figure 1 illustrates our training procedure consisting of two steps. In Step 1, the student model is initialized from the teacher and is jointly distilled and pruned to generate a smaller model with a pre-specified size. In Step 2, the already pruned student model is further distilled to improve performance. In both steps, only unlabeled speech data are utilized and the teacher is frozen. j=1 and each \u02dc\u03b8j = \u03b8jzj. The unlabeled dataset where \u02dc\u03b8 = {\u02dc\u03b8j}n with D samples is {xk}D k=1. \u03bb > 0 is the regularization weight. It is intractable to solve Eq. (2) using gradient descent due to the discrete nature of masks z. To make the loss differentiable, Louizos et al. propose a reparameterization trick which samples z from the Hard Concrete distribution [31]: u \u223c U (0, 1), v(\u03b1) = sigmoid (cid:18)(cid:18) log u 1 \u2212 u + log \u03b1 (cid:19) /\u03b2 (cid:19) 3.2. Distillation loss \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), Unlike DistilHuBERT [12], we use layer-to-layer distillation since the student initially has the same depth as the teacher (see Section 2.2 for discussions). Suppose the teacher has N tea Transformer layers with hidden size dtea and the student has N stu layers with hidden size dstu. Let Xtea (shape T \u00d7 dtea) i and Xstu (shape T \u00d7 dstu) be the output sequences of the i-th i Transformer layers from the teacher and student, respectively, where T is the sequence length. The distillation loss is: where u follows a uniform distribution in [0, 1]. \u03b2 is a constant. l < 0 and r > 0 are two constants to stretch v to [l, r], and it is further clamped to [0, 1]. Only \u03b1 = {\u03b1j}n j=1 are learnable parameters in this distribution. With this trick, the objective in Eq. (2) is differentiable and the regularization term has a closed- form expression [31]: (3) Ldis = (cid:88) i\u2208S L (cid:0)Xtea i , Xstu i Wi (cid:1) , (1) Ez\u223cq (cid:104) \u2225 \u02dc\u03b8\u22250 (cid:105) = n (cid:88) j=1 sigmoid (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r (cid:19) , (4) where S is a set of layers to match between the teacher and student models after a linear projection Wi. We use S = which represents the (expected) model size as a differentiable function of current parameters \u03b1. , Table 1: Comparison of our method versus previous distillation methods on SUPERB [6]. Our DPHuBERT and DPWavLM are compressed from publicly available HuBERT Base [2] and WavLM Base+ [4] checkpoints, respectively. Method #Params KS IC PR ASR w/o LM ER QbE SF SID ASV SD Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 FBANK wav2vec 2.0 Base [1] HuBERT Base [2] WavLM Base+ [4] 0 95.04 94.68 94.70 41.38 9.65 96.23 92.35 96.30 98.34 97.37 99.00 82.01 5.74 5.41 3.92 23.18 6.43 6.42 5.59 48.24 63.43 64.92 68.65 0.0058 0.0233 0.0736 0.0988 69.64 / 52.94 20.06 88.30 / 24.77 75.18 88.53 / 25.20 81.42 90.58 / 21.20 89.42 9.56 6.02 5.11 4.07 10.05 6.08 5.88 3.50 Compressed models using LibriSpeech 960h DistilHuBERT [12] FitHuBERT [13] FitW2V2 [13] 12-Layer Half [14] 3-Layer One [14] DPHuBERT (ours) DPWavLM (ours) 23.49 22.49 31.63 26.87 30.58 23.59 23.59 95.98 94.99 16.27 96.27 91.25 13.32 96.04 93.38 12.22 97.24 96.97 10.67 96.69 94.15 13.34 9.67 96.36 97.92 8.22 96.27"}, {"question": " How does HJ-Pruning impact the total computation in ASR and SLU?", "answer": " HJ-Pruning significantly reduces the total computation while retaining good performance in ASR and SLU.", "ref_chunk": "HJ-Pruning [25] is a structured pruning method that jointly prunes heterogeneous components (i.e., CNN and Transformer) of speech SSL models. It significantly reduces the total computation while retaining good performance in ASR and SLU. These methods deal with specific downstream tasks, but do not investigate the universal speech representa- tions. Our work focuses on task-agnostic structured pruning of speech SSL. As there is no labeled data for normal supervised training, we employ a distillation objective along with pruning. Once-for-all training. Compression methods typically gen- erate a single model with a pre-determined size. LightHu- BERT [28] deploys once-for-all training [29] to obtain numer- ous weight-sharing subnets, which shows very strong perfor- mance on SUPERB [6]. However, it requires an expensive two- stage training process and an advanced distillation loss inspired by data2vec [30]. According to the authors, compressing Hu- BERT Base already takes 2k GPUs hours (i.e., 62 hours with 32 V100 GPUs and 19 hours with 8 GPUs for two stages, respec- tively), which is prohibitive for academic researchers and small businesses. Unlike LightHuBERT, our work aims to compress an existing speech SSL model to a specific sparsity ratio within a manageable amount of training time, which is consistent with the standard setup of prior distillation methods [12, 13]. {0, 4, 8, 12} for base models and {0, 8, 16, 24} for large mod- els. The 0th layer is the output of CNN, which is also the input to the first Transformer layer. The loss function L measures the difference between two feature sequences, which can be L1, L2 or cosine distances [12\u201314]. We follow [12, 14] to use a combi- nation of L1 and cosine distances with equal weights. 3.3. Joint distillation and structured pruning Structured pruning of the student model is formulated as learn- ing a sparse model through L0 regularization [31], which has been explored in NLP [19,20] and speech [25]. The method will be briefly introduced below. For more comprehensive deriva- tions, please refer to prior research [19, 20, 25, 31]. Consider a frozen teacher model f tea(\u00b7) and a student model f stu(\u00b7; \u03b8) with learnable parameters \u03b8 = {\u03b8j}n j=1. Each \u03b8j is a group of prunable parameters (including convolution channels, atten- tion heads, and FFN intermediate units) and there are n groups in total. We define a binary variable zj (called mask) for each \u03b8j. The masks z follow a probability distribution q(z; \u03b1) with parameters \u03b1. The regularized distillation objective is: min \u03b8,\u03b1 Ez\u223cq (cid:34) 1 D D (cid:88) Ldis (cid:16) f tea(xk), f stu(xk; \u02dc\u03b8) (cid:17) + \u03bb\u2225 \u02dc\u03b8\u22250 (cid:35) , k=1 3. DPHuBERT (2) 3.1. Training procedure Figure 1 illustrates our training procedure consisting of two steps. In Step 1, the student model is initialized from the teacher and is jointly distilled and pruned to generate a smaller model with a pre-specified size. In Step 2, the already pruned student model is further distilled to improve performance. In both steps, only unlabeled speech data are utilized and the teacher is frozen. j=1 and each \u02dc\u03b8j = \u03b8jzj. The unlabeled dataset where \u02dc\u03b8 = {\u02dc\u03b8j}n with D samples is {xk}D k=1. \u03bb > 0 is the regularization weight. It is intractable to solve Eq. (2) using gradient descent due to the discrete nature of masks z. To make the loss differentiable, Louizos et al. propose a reparameterization trick which samples z from the Hard Concrete distribution [31]: u \u223c U (0, 1), v(\u03b1) = sigmoid (cid:18)(cid:18) log u 1 \u2212 u + log \u03b1 (cid:19) /\u03b2 (cid:19) 3.2. Distillation loss \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), Unlike DistilHuBERT [12], we use layer-to-layer distillation since the student initially has the same depth as the teacher (see Section 2.2 for discussions). Suppose the teacher has N tea Transformer layers with hidden size dtea and the student has N stu layers with hidden size dstu. Let Xtea (shape T \u00d7 dtea) i and Xstu (shape T \u00d7 dstu) be the output sequences of the i-th i Transformer layers from the teacher and student, respectively, where T is the sequence length. The distillation loss is: where u follows a uniform distribution in [0, 1]. \u03b2 is a constant. l < 0 and r > 0 are two constants to stretch v to [l, r], and it is further clamped to [0, 1]. Only \u03b1 = {\u03b1j}n j=1 are learnable parameters in this distribution. With this trick, the objective in Eq. (2) is differentiable and the regularization term has a closed- form expression [31]: (3) Ldis = (cid:88) i\u2208S L (cid:0)Xtea i , Xstu i Wi (cid:1) , (1) Ez\u223cq (cid:104) \u2225 \u02dc\u03b8\u22250 (cid:105) = n (cid:88) j=1 sigmoid (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r (cid:19) , (4) where S is a set of layers to match between the teacher and student models after a linear projection Wi. We use S = which represents the (expected) model size as a differentiable function of current parameters \u03b1. , Table 1: Comparison of our method versus previous distillation methods on SUPERB [6]. Our DPHuBERT and DPWavLM are compressed from publicly available HuBERT Base [2] and WavLM Base+ [4] checkpoints, respectively. Method #Params KS IC PR ASR w/o LM ER QbE SF SID ASV SD Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 FBANK wav2vec 2.0 Base [1] HuBERT Base [2] WavLM Base+ [4] 0 95.04 94.68 94.70 41.38 9.65 96.23 92.35 96.30 98.34 97.37 99.00 82.01 5.74 5.41 3.92 23.18 6.43 6.42 5.59 48.24 63.43 64.92 68.65 0.0058 0.0233 0.0736 0.0988 69.64 / 52.94 20.06 88.30 / 24.77 75.18 88.53 / 25.20 81.42 90.58 / 21.20 89.42 9.56 6.02 5.11 4.07 10.05 6.08 5.88 3.50 Compressed models using LibriSpeech 960h DistilHuBERT [12] FitHuBERT [13] FitW2V2 [13] 12-Layer Half [14] 3-Layer One [14] DPHuBERT (ours) DPWavLM (ours) 23.49 22.49 31.63 26.87 30.58 23.59 23.59 95.98 94.99 16.27 96.27 91.25 13.32 96.04 93.38 12.22 97.24 96.97 10.67 96.69 94.15 13.34 9.67 96.36 97.92 8.22 96.27"}, {"question": " What is the compression method used by LightHuBERT?", "answer": " LightHuBERT deploys once-for-all training to obtain weight-sharing subnets.", "ref_chunk": "HJ-Pruning [25] is a structured pruning method that jointly prunes heterogeneous components (i.e., CNN and Transformer) of speech SSL models. It significantly reduces the total computation while retaining good performance in ASR and SLU. These methods deal with specific downstream tasks, but do not investigate the universal speech representa- tions. Our work focuses on task-agnostic structured pruning of speech SSL. As there is no labeled data for normal supervised training, we employ a distillation objective along with pruning. Once-for-all training. Compression methods typically gen- erate a single model with a pre-determined size. LightHu- BERT [28] deploys once-for-all training [29] to obtain numer- ous weight-sharing subnets, which shows very strong perfor- mance on SUPERB [6]. However, it requires an expensive two- stage training process and an advanced distillation loss inspired by data2vec [30]. According to the authors, compressing Hu- BERT Base already takes 2k GPUs hours (i.e., 62 hours with 32 V100 GPUs and 19 hours with 8 GPUs for two stages, respec- tively), which is prohibitive for academic researchers and small businesses. Unlike LightHuBERT, our work aims to compress an existing speech SSL model to a specific sparsity ratio within a manageable amount of training time, which is consistent with the standard setup of prior distillation methods [12, 13]. {0, 4, 8, 12} for base models and {0, 8, 16, 24} for large mod- els. The 0th layer is the output of CNN, which is also the input to the first Transformer layer. The loss function L measures the difference between two feature sequences, which can be L1, L2 or cosine distances [12\u201314]. We follow [12, 14] to use a combi- nation of L1 and cosine distances with equal weights. 3.3. Joint distillation and structured pruning Structured pruning of the student model is formulated as learn- ing a sparse model through L0 regularization [31], which has been explored in NLP [19,20] and speech [25]. The method will be briefly introduced below. For more comprehensive deriva- tions, please refer to prior research [19, 20, 25, 31]. Consider a frozen teacher model f tea(\u00b7) and a student model f stu(\u00b7; \u03b8) with learnable parameters \u03b8 = {\u03b8j}n j=1. Each \u03b8j is a group of prunable parameters (including convolution channels, atten- tion heads, and FFN intermediate units) and there are n groups in total. We define a binary variable zj (called mask) for each \u03b8j. The masks z follow a probability distribution q(z; \u03b1) with parameters \u03b1. The regularized distillation objective is: min \u03b8,\u03b1 Ez\u223cq (cid:34) 1 D D (cid:88) Ldis (cid:16) f tea(xk), f stu(xk; \u02dc\u03b8) (cid:17) + \u03bb\u2225 \u02dc\u03b8\u22250 (cid:35) , k=1 3. DPHuBERT (2) 3.1. Training procedure Figure 1 illustrates our training procedure consisting of two steps. In Step 1, the student model is initialized from the teacher and is jointly distilled and pruned to generate a smaller model with a pre-specified size. In Step 2, the already pruned student model is further distilled to improve performance. In both steps, only unlabeled speech data are utilized and the teacher is frozen. j=1 and each \u02dc\u03b8j = \u03b8jzj. The unlabeled dataset where \u02dc\u03b8 = {\u02dc\u03b8j}n with D samples is {xk}D k=1. \u03bb > 0 is the regularization weight. It is intractable to solve Eq. (2) using gradient descent due to the discrete nature of masks z. To make the loss differentiable, Louizos et al. propose a reparameterization trick which samples z from the Hard Concrete distribution [31]: u \u223c U (0, 1), v(\u03b1) = sigmoid (cid:18)(cid:18) log u 1 \u2212 u + log \u03b1 (cid:19) /\u03b2 (cid:19) 3.2. Distillation loss \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), Unlike DistilHuBERT [12], we use layer-to-layer distillation since the student initially has the same depth as the teacher (see Section 2.2 for discussions). Suppose the teacher has N tea Transformer layers with hidden size dtea and the student has N stu layers with hidden size dstu. Let Xtea (shape T \u00d7 dtea) i and Xstu (shape T \u00d7 dstu) be the output sequences of the i-th i Transformer layers from the teacher and student, respectively, where T is the sequence length. The distillation loss is: where u follows a uniform distribution in [0, 1]. \u03b2 is a constant. l < 0 and r > 0 are two constants to stretch v to [l, r], and it is further clamped to [0, 1]. Only \u03b1 = {\u03b1j}n j=1 are learnable parameters in this distribution. With this trick, the objective in Eq. (2) is differentiable and the regularization term has a closed- form expression [31]: (3) Ldis = (cid:88) i\u2208S L (cid:0)Xtea i , Xstu i Wi (cid:1) , (1) Ez\u223cq (cid:104) \u2225 \u02dc\u03b8\u22250 (cid:105) = n (cid:88) j=1 sigmoid (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r (cid:19) , (4) where S is a set of layers to match between the teacher and student models after a linear projection Wi. We use S = which represents the (expected) model size as a differentiable function of current parameters \u03b1. , Table 1: Comparison of our method versus previous distillation methods on SUPERB [6]. Our DPHuBERT and DPWavLM are compressed from publicly available HuBERT Base [2] and WavLM Base+ [4] checkpoints, respectively. Method #Params KS IC PR ASR w/o LM ER QbE SF SID ASV SD Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 FBANK wav2vec 2.0 Base [1] HuBERT Base [2] WavLM Base+ [4] 0 95.04 94.68 94.70 41.38 9.65 96.23 92.35 96.30 98.34 97.37 99.00 82.01 5.74 5.41 3.92 23.18 6.43 6.42 5.59 48.24 63.43 64.92 68.65 0.0058 0.0233 0.0736 0.0988 69.64 / 52.94 20.06 88.30 / 24.77 75.18 88.53 / 25.20 81.42 90.58 / 21.20 89.42 9.56 6.02 5.11 4.07 10.05 6.08 5.88 3.50 Compressed models using LibriSpeech 960h DistilHuBERT [12] FitHuBERT [13] FitW2V2 [13] 12-Layer Half [14] 3-Layer One [14] DPHuBERT (ours) DPWavLM (ours) 23.49 22.49 31.63 26.87 30.58 23.59 23.59 95.98 94.99 16.27 96.27 91.25 13.32 96.04 93.38 12.22 97.24 96.97 10.67 96.69 94.15 13.34 9.67 96.36 97.92 8.22 96.27"}, {"question": " What is the drawback of the compression method used by LightHuBERT?", "answer": " It requires an expensive two-stage training process and an advanced distillation loss.", "ref_chunk": "HJ-Pruning [25] is a structured pruning method that jointly prunes heterogeneous components (i.e., CNN and Transformer) of speech SSL models. It significantly reduces the total computation while retaining good performance in ASR and SLU. These methods deal with specific downstream tasks, but do not investigate the universal speech representa- tions. Our work focuses on task-agnostic structured pruning of speech SSL. As there is no labeled data for normal supervised training, we employ a distillation objective along with pruning. Once-for-all training. Compression methods typically gen- erate a single model with a pre-determined size. LightHu- BERT [28] deploys once-for-all training [29] to obtain numer- ous weight-sharing subnets, which shows very strong perfor- mance on SUPERB [6]. However, it requires an expensive two- stage training process and an advanced distillation loss inspired by data2vec [30]. According to the authors, compressing Hu- BERT Base already takes 2k GPUs hours (i.e., 62 hours with 32 V100 GPUs and 19 hours with 8 GPUs for two stages, respec- tively), which is prohibitive for academic researchers and small businesses. Unlike LightHuBERT, our work aims to compress an existing speech SSL model to a specific sparsity ratio within a manageable amount of training time, which is consistent with the standard setup of prior distillation methods [12, 13]. {0, 4, 8, 12} for base models and {0, 8, 16, 24} for large mod- els. The 0th layer is the output of CNN, which is also the input to the first Transformer layer. The loss function L measures the difference between two feature sequences, which can be L1, L2 or cosine distances [12\u201314]. We follow [12, 14] to use a combi- nation of L1 and cosine distances with equal weights. 3.3. Joint distillation and structured pruning Structured pruning of the student model is formulated as learn- ing a sparse model through L0 regularization [31], which has been explored in NLP [19,20] and speech [25]. The method will be briefly introduced below. For more comprehensive deriva- tions, please refer to prior research [19, 20, 25, 31]. Consider a frozen teacher model f tea(\u00b7) and a student model f stu(\u00b7; \u03b8) with learnable parameters \u03b8 = {\u03b8j}n j=1. Each \u03b8j is a group of prunable parameters (including convolution channels, atten- tion heads, and FFN intermediate units) and there are n groups in total. We define a binary variable zj (called mask) for each \u03b8j. The masks z follow a probability distribution q(z; \u03b1) with parameters \u03b1. The regularized distillation objective is: min \u03b8,\u03b1 Ez\u223cq (cid:34) 1 D D (cid:88) Ldis (cid:16) f tea(xk), f stu(xk; \u02dc\u03b8) (cid:17) + \u03bb\u2225 \u02dc\u03b8\u22250 (cid:35) , k=1 3. DPHuBERT (2) 3.1. Training procedure Figure 1 illustrates our training procedure consisting of two steps. In Step 1, the student model is initialized from the teacher and is jointly distilled and pruned to generate a smaller model with a pre-specified size. In Step 2, the already pruned student model is further distilled to improve performance. In both steps, only unlabeled speech data are utilized and the teacher is frozen. j=1 and each \u02dc\u03b8j = \u03b8jzj. The unlabeled dataset where \u02dc\u03b8 = {\u02dc\u03b8j}n with D samples is {xk}D k=1. \u03bb > 0 is the regularization weight. It is intractable to solve Eq. (2) using gradient descent due to the discrete nature of masks z. To make the loss differentiable, Louizos et al. propose a reparameterization trick which samples z from the Hard Concrete distribution [31]: u \u223c U (0, 1), v(\u03b1) = sigmoid (cid:18)(cid:18) log u 1 \u2212 u + log \u03b1 (cid:19) /\u03b2 (cid:19) 3.2. Distillation loss \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), Unlike DistilHuBERT [12], we use layer-to-layer distillation since the student initially has the same depth as the teacher (see Section 2.2 for discussions). Suppose the teacher has N tea Transformer layers with hidden size dtea and the student has N stu layers with hidden size dstu. Let Xtea (shape T \u00d7 dtea) i and Xstu (shape T \u00d7 dstu) be the output sequences of the i-th i Transformer layers from the teacher and student, respectively, where T is the sequence length. The distillation loss is: where u follows a uniform distribution in [0, 1]. \u03b2 is a constant. l < 0 and r > 0 are two constants to stretch v to [l, r], and it is further clamped to [0, 1]. Only \u03b1 = {\u03b1j}n j=1 are learnable parameters in this distribution. With this trick, the objective in Eq. (2) is differentiable and the regularization term has a closed- form expression [31]: (3) Ldis = (cid:88) i\u2208S L (cid:0)Xtea i , Xstu i Wi (cid:1) , (1) Ez\u223cq (cid:104) \u2225 \u02dc\u03b8\u22250 (cid:105) = n (cid:88) j=1 sigmoid (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r (cid:19) , (4) where S is a set of layers to match between the teacher and student models after a linear projection Wi. We use S = which represents the (expected) model size as a differentiable function of current parameters \u03b1. , Table 1: Comparison of our method versus previous distillation methods on SUPERB [6]. Our DPHuBERT and DPWavLM are compressed from publicly available HuBERT Base [2] and WavLM Base+ [4] checkpoints, respectively. Method #Params KS IC PR ASR w/o LM ER QbE SF SID ASV SD Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 FBANK wav2vec 2.0 Base [1] HuBERT Base [2] WavLM Base+ [4] 0 95.04 94.68 94.70 41.38 9.65 96.23 92.35 96.30 98.34 97.37 99.00 82.01 5.74 5.41 3.92 23.18 6.43 6.42 5.59 48.24 63.43 64.92 68.65 0.0058 0.0233 0.0736 0.0988 69.64 / 52.94 20.06 88.30 / 24.77 75.18 88.53 / 25.20 81.42 90.58 / 21.20 89.42 9.56 6.02 5.11 4.07 10.05 6.08 5.88 3.50 Compressed models using LibriSpeech 960h DistilHuBERT [12] FitHuBERT [13] FitW2V2 [13] 12-Layer Half [14] 3-Layer One [14] DPHuBERT (ours) DPWavLM (ours) 23.49 22.49 31.63 26.87 30.58 23.59 23.59 95.98 94.99 16.27 96.27 91.25 13.32 96.04 93.38 12.22 97.24 96.97 10.67 96.69 94.15 13.34 9.67 96.36 97.92 8.22 96.27"}, {"question": " How does the compression time of Hu-BERT Base compare between 32 V100 GPUs and 8 GPUs?", "answer": " Compressing Hu-BERT Base takes 62 hours with 32 V100 GPUs and 19 hours with 8 GPUs for two stages, respectively.", "ref_chunk": "HJ-Pruning [25] is a structured pruning method that jointly prunes heterogeneous components (i.e., CNN and Transformer) of speech SSL models. It significantly reduces the total computation while retaining good performance in ASR and SLU. These methods deal with specific downstream tasks, but do not investigate the universal speech representa- tions. Our work focuses on task-agnostic structured pruning of speech SSL. As there is no labeled data for normal supervised training, we employ a distillation objective along with pruning. Once-for-all training. Compression methods typically gen- erate a single model with a pre-determined size. LightHu- BERT [28] deploys once-for-all training [29] to obtain numer- ous weight-sharing subnets, which shows very strong perfor- mance on SUPERB [6]. However, it requires an expensive two- stage training process and an advanced distillation loss inspired by data2vec [30]. According to the authors, compressing Hu- BERT Base already takes 2k GPUs hours (i.e., 62 hours with 32 V100 GPUs and 19 hours with 8 GPUs for two stages, respec- tively), which is prohibitive for academic researchers and small businesses. Unlike LightHuBERT, our work aims to compress an existing speech SSL model to a specific sparsity ratio within a manageable amount of training time, which is consistent with the standard setup of prior distillation methods [12, 13]. {0, 4, 8, 12} for base models and {0, 8, 16, 24} for large mod- els. The 0th layer is the output of CNN, which is also the input to the first Transformer layer. The loss function L measures the difference between two feature sequences, which can be L1, L2 or cosine distances [12\u201314]. We follow [12, 14] to use a combi- nation of L1 and cosine distances with equal weights. 3.3. Joint distillation and structured pruning Structured pruning of the student model is formulated as learn- ing a sparse model through L0 regularization [31], which has been explored in NLP [19,20] and speech [25]. The method will be briefly introduced below. For more comprehensive deriva- tions, please refer to prior research [19, 20, 25, 31]. Consider a frozen teacher model f tea(\u00b7) and a student model f stu(\u00b7; \u03b8) with learnable parameters \u03b8 = {\u03b8j}n j=1. Each \u03b8j is a group of prunable parameters (including convolution channels, atten- tion heads, and FFN intermediate units) and there are n groups in total. We define a binary variable zj (called mask) for each \u03b8j. The masks z follow a probability distribution q(z; \u03b1) with parameters \u03b1. The regularized distillation objective is: min \u03b8,\u03b1 Ez\u223cq (cid:34) 1 D D (cid:88) Ldis (cid:16) f tea(xk), f stu(xk; \u02dc\u03b8) (cid:17) + \u03bb\u2225 \u02dc\u03b8\u22250 (cid:35) , k=1 3. DPHuBERT (2) 3.1. Training procedure Figure 1 illustrates our training procedure consisting of two steps. In Step 1, the student model is initialized from the teacher and is jointly distilled and pruned to generate a smaller model with a pre-specified size. In Step 2, the already pruned student model is further distilled to improve performance. In both steps, only unlabeled speech data are utilized and the teacher is frozen. j=1 and each \u02dc\u03b8j = \u03b8jzj. The unlabeled dataset where \u02dc\u03b8 = {\u02dc\u03b8j}n with D samples is {xk}D k=1. \u03bb > 0 is the regularization weight. It is intractable to solve Eq. (2) using gradient descent due to the discrete nature of masks z. To make the loss differentiable, Louizos et al. propose a reparameterization trick which samples z from the Hard Concrete distribution [31]: u \u223c U (0, 1), v(\u03b1) = sigmoid (cid:18)(cid:18) log u 1 \u2212 u + log \u03b1 (cid:19) /\u03b2 (cid:19) 3.2. Distillation loss \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), Unlike DistilHuBERT [12], we use layer-to-layer distillation since the student initially has the same depth as the teacher (see Section 2.2 for discussions). Suppose the teacher has N tea Transformer layers with hidden size dtea and the student has N stu layers with hidden size dstu. Let Xtea (shape T \u00d7 dtea) i and Xstu (shape T \u00d7 dstu) be the output sequences of the i-th i Transformer layers from the teacher and student, respectively, where T is the sequence length. The distillation loss is: where u follows a uniform distribution in [0, 1]. \u03b2 is a constant. l < 0 and r > 0 are two constants to stretch v to [l, r], and it is further clamped to [0, 1]. Only \u03b1 = {\u03b1j}n j=1 are learnable parameters in this distribution. With this trick, the objective in Eq. (2) is differentiable and the regularization term has a closed- form expression [31]: (3) Ldis = (cid:88) i\u2208S L (cid:0)Xtea i , Xstu i Wi (cid:1) , (1) Ez\u223cq (cid:104) \u2225 \u02dc\u03b8\u22250 (cid:105) = n (cid:88) j=1 sigmoid (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r (cid:19) , (4) where S is a set of layers to match between the teacher and student models after a linear projection Wi. We use S = which represents the (expected) model size as a differentiable function of current parameters \u03b1. , Table 1: Comparison of our method versus previous distillation methods on SUPERB [6]. Our DPHuBERT and DPWavLM are compressed from publicly available HuBERT Base [2] and WavLM Base+ [4] checkpoints, respectively. Method #Params KS IC PR ASR w/o LM ER QbE SF SID ASV SD Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 FBANK wav2vec 2.0 Base [1] HuBERT Base [2] WavLM Base+ [4] 0 95.04 94.68 94.70 41.38 9.65 96.23 92.35 96.30 98.34 97.37 99.00 82.01 5.74 5.41 3.92 23.18 6.43 6.42 5.59 48.24 63.43 64.92 68.65 0.0058 0.0233 0.0736 0.0988 69.64 / 52.94 20.06 88.30 / 24.77 75.18 88.53 / 25.20 81.42 90.58 / 21.20 89.42 9.56 6.02 5.11 4.07 10.05 6.08 5.88 3.50 Compressed models using LibriSpeech 960h DistilHuBERT [12] FitHuBERT [13] FitW2V2 [13] 12-Layer Half [14] 3-Layer One [14] DPHuBERT (ours) DPWavLM (ours) 23.49 22.49 31.63 26.87 30.58 23.59 23.59 95.98 94.99 16.27 96.27 91.25 13.32 96.04 93.38 12.22 97.24 96.97 10.67 96.69 94.15 13.34 9.67 96.36 97.92 8.22 96.27"}, {"question": " What is the objective of structured pruning of the student model?", "answer": " The objective is to learn a sparse model through L0 regularization.", "ref_chunk": "HJ-Pruning [25] is a structured pruning method that jointly prunes heterogeneous components (i.e., CNN and Transformer) of speech SSL models. It significantly reduces the total computation while retaining good performance in ASR and SLU. These methods deal with specific downstream tasks, but do not investigate the universal speech representa- tions. Our work focuses on task-agnostic structured pruning of speech SSL. As there is no labeled data for normal supervised training, we employ a distillation objective along with pruning. Once-for-all training. Compression methods typically gen- erate a single model with a pre-determined size. LightHu- BERT [28] deploys once-for-all training [29] to obtain numer- ous weight-sharing subnets, which shows very strong perfor- mance on SUPERB [6]. However, it requires an expensive two- stage training process and an advanced distillation loss inspired by data2vec [30]. According to the authors, compressing Hu- BERT Base already takes 2k GPUs hours (i.e., 62 hours with 32 V100 GPUs and 19 hours with 8 GPUs for two stages, respec- tively), which is prohibitive for academic researchers and small businesses. Unlike LightHuBERT, our work aims to compress an existing speech SSL model to a specific sparsity ratio within a manageable amount of training time, which is consistent with the standard setup of prior distillation methods [12, 13]. {0, 4, 8, 12} for base models and {0, 8, 16, 24} for large mod- els. The 0th layer is the output of CNN, which is also the input to the first Transformer layer. The loss function L measures the difference between two feature sequences, which can be L1, L2 or cosine distances [12\u201314]. We follow [12, 14] to use a combi- nation of L1 and cosine distances with equal weights. 3.3. Joint distillation and structured pruning Structured pruning of the student model is formulated as learn- ing a sparse model through L0 regularization [31], which has been explored in NLP [19,20] and speech [25]. The method will be briefly introduced below. For more comprehensive deriva- tions, please refer to prior research [19, 20, 25, 31]. Consider a frozen teacher model f tea(\u00b7) and a student model f stu(\u00b7; \u03b8) with learnable parameters \u03b8 = {\u03b8j}n j=1. Each \u03b8j is a group of prunable parameters (including convolution channels, atten- tion heads, and FFN intermediate units) and there are n groups in total. We define a binary variable zj (called mask) for each \u03b8j. The masks z follow a probability distribution q(z; \u03b1) with parameters \u03b1. The regularized distillation objective is: min \u03b8,\u03b1 Ez\u223cq (cid:34) 1 D D (cid:88) Ldis (cid:16) f tea(xk), f stu(xk; \u02dc\u03b8) (cid:17) + \u03bb\u2225 \u02dc\u03b8\u22250 (cid:35) , k=1 3. DPHuBERT (2) 3.1. Training procedure Figure 1 illustrates our training procedure consisting of two steps. In Step 1, the student model is initialized from the teacher and is jointly distilled and pruned to generate a smaller model with a pre-specified size. In Step 2, the already pruned student model is further distilled to improve performance. In both steps, only unlabeled speech data are utilized and the teacher is frozen. j=1 and each \u02dc\u03b8j = \u03b8jzj. The unlabeled dataset where \u02dc\u03b8 = {\u02dc\u03b8j}n with D samples is {xk}D k=1. \u03bb > 0 is the regularization weight. It is intractable to solve Eq. (2) using gradient descent due to the discrete nature of masks z. To make the loss differentiable, Louizos et al. propose a reparameterization trick which samples z from the Hard Concrete distribution [31]: u \u223c U (0, 1), v(\u03b1) = sigmoid (cid:18)(cid:18) log u 1 \u2212 u + log \u03b1 (cid:19) /\u03b2 (cid:19) 3.2. Distillation loss \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), Unlike DistilHuBERT [12], we use layer-to-layer distillation since the student initially has the same depth as the teacher (see Section 2.2 for discussions). Suppose the teacher has N tea Transformer layers with hidden size dtea and the student has N stu layers with hidden size dstu. Let Xtea (shape T \u00d7 dtea) i and Xstu (shape T \u00d7 dstu) be the output sequences of the i-th i Transformer layers from the teacher and student, respectively, where T is the sequence length. The distillation loss is: where u follows a uniform distribution in [0, 1]. \u03b2 is a constant. l < 0 and r > 0 are two constants to stretch v to [l, r], and it is further clamped to [0, 1]. Only \u03b1 = {\u03b1j}n j=1 are learnable parameters in this distribution. With this trick, the objective in Eq. (2) is differentiable and the regularization term has a closed- form expression [31]: (3) Ldis = (cid:88) i\u2208S L (cid:0)Xtea i , Xstu i Wi (cid:1) , (1) Ez\u223cq (cid:104) \u2225 \u02dc\u03b8\u22250 (cid:105) = n (cid:88) j=1 sigmoid (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r (cid:19) , (4) where S is a set of layers to match between the teacher and student models after a linear projection Wi. We use S = which represents the (expected) model size as a differentiable function of current parameters \u03b1. , Table 1: Comparison of our method versus previous distillation methods on SUPERB [6]. Our DPHuBERT and DPWavLM are compressed from publicly available HuBERT Base [2] and WavLM Base+ [4] checkpoints, respectively. Method #Params KS IC PR ASR w/o LM ER QbE SF SID ASV SD Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 FBANK wav2vec 2.0 Base [1] HuBERT Base [2] WavLM Base+ [4] 0 95.04 94.68 94.70 41.38 9.65 96.23 92.35 96.30 98.34 97.37 99.00 82.01 5.74 5.41 3.92 23.18 6.43 6.42 5.59 48.24 63.43 64.92 68.65 0.0058 0.0233 0.0736 0.0988 69.64 / 52.94 20.06 88.30 / 24.77 75.18 88.53 / 25.20 81.42 90.58 / 21.20 89.42 9.56 6.02 5.11 4.07 10.05 6.08 5.88 3.50 Compressed models using LibriSpeech 960h DistilHuBERT [12] FitHuBERT [13] FitW2V2 [13] 12-Layer Half [14] 3-Layer One [14] DPHuBERT (ours) DPWavLM (ours) 23.49 22.49 31.63 26.87 30.58 23.59 23.59 95.98 94.99 16.27 96.27 91.25 13.32 96.04 93.38 12.22 97.24 96.97 10.67 96.69 94.15 13.34 9.67 96.36 97.92 8.22 96.27"}, {"question": " What distribution is followed by the binary variable zj used for structured pruning?", "answer": " The masks z follow a probability distribution q(z; \u03b1) with parameters \u03b1.", "ref_chunk": "HJ-Pruning [25] is a structured pruning method that jointly prunes heterogeneous components (i.e., CNN and Transformer) of speech SSL models. It significantly reduces the total computation while retaining good performance in ASR and SLU. These methods deal with specific downstream tasks, but do not investigate the universal speech representa- tions. Our work focuses on task-agnostic structured pruning of speech SSL. As there is no labeled data for normal supervised training, we employ a distillation objective along with pruning. Once-for-all training. Compression methods typically gen- erate a single model with a pre-determined size. LightHu- BERT [28] deploys once-for-all training [29] to obtain numer- ous weight-sharing subnets, which shows very strong perfor- mance on SUPERB [6]. However, it requires an expensive two- stage training process and an advanced distillation loss inspired by data2vec [30]. According to the authors, compressing Hu- BERT Base already takes 2k GPUs hours (i.e., 62 hours with 32 V100 GPUs and 19 hours with 8 GPUs for two stages, respec- tively), which is prohibitive for academic researchers and small businesses. Unlike LightHuBERT, our work aims to compress an existing speech SSL model to a specific sparsity ratio within a manageable amount of training time, which is consistent with the standard setup of prior distillation methods [12, 13]. {0, 4, 8, 12} for base models and {0, 8, 16, 24} for large mod- els. The 0th layer is the output of CNN, which is also the input to the first Transformer layer. The loss function L measures the difference between two feature sequences, which can be L1, L2 or cosine distances [12\u201314]. We follow [12, 14] to use a combi- nation of L1 and cosine distances with equal weights. 3.3. Joint distillation and structured pruning Structured pruning of the student model is formulated as learn- ing a sparse model through L0 regularization [31], which has been explored in NLP [19,20] and speech [25]. The method will be briefly introduced below. For more comprehensive deriva- tions, please refer to prior research [19, 20, 25, 31]. Consider a frozen teacher model f tea(\u00b7) and a student model f stu(\u00b7; \u03b8) with learnable parameters \u03b8 = {\u03b8j}n j=1. Each \u03b8j is a group of prunable parameters (including convolution channels, atten- tion heads, and FFN intermediate units) and there are n groups in total. We define a binary variable zj (called mask) for each \u03b8j. The masks z follow a probability distribution q(z; \u03b1) with parameters \u03b1. The regularized distillation objective is: min \u03b8,\u03b1 Ez\u223cq (cid:34) 1 D D (cid:88) Ldis (cid:16) f tea(xk), f stu(xk; \u02dc\u03b8) (cid:17) + \u03bb\u2225 \u02dc\u03b8\u22250 (cid:35) , k=1 3. DPHuBERT (2) 3.1. Training procedure Figure 1 illustrates our training procedure consisting of two steps. In Step 1, the student model is initialized from the teacher and is jointly distilled and pruned to generate a smaller model with a pre-specified size. In Step 2, the already pruned student model is further distilled to improve performance. In both steps, only unlabeled speech data are utilized and the teacher is frozen. j=1 and each \u02dc\u03b8j = \u03b8jzj. The unlabeled dataset where \u02dc\u03b8 = {\u02dc\u03b8j}n with D samples is {xk}D k=1. \u03bb > 0 is the regularization weight. It is intractable to solve Eq. (2) using gradient descent due to the discrete nature of masks z. To make the loss differentiable, Louizos et al. propose a reparameterization trick which samples z from the Hard Concrete distribution [31]: u \u223c U (0, 1), v(\u03b1) = sigmoid (cid:18)(cid:18) log u 1 \u2212 u + log \u03b1 (cid:19) /\u03b2 (cid:19) 3.2. Distillation loss \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), Unlike DistilHuBERT [12], we use layer-to-layer distillation since the student initially has the same depth as the teacher (see Section 2.2 for discussions). Suppose the teacher has N tea Transformer layers with hidden size dtea and the student has N stu layers with hidden size dstu. Let Xtea (shape T \u00d7 dtea) i and Xstu (shape T \u00d7 dstu) be the output sequences of the i-th i Transformer layers from the teacher and student, respectively, where T is the sequence length. The distillation loss is: where u follows a uniform distribution in [0, 1]. \u03b2 is a constant. l < 0 and r > 0 are two constants to stretch v to [l, r], and it is further clamped to [0, 1]. Only \u03b1 = {\u03b1j}n j=1 are learnable parameters in this distribution. With this trick, the objective in Eq. (2) is differentiable and the regularization term has a closed- form expression [31]: (3) Ldis = (cid:88) i\u2208S L (cid:0)Xtea i , Xstu i Wi (cid:1) , (1) Ez\u223cq (cid:104) \u2225 \u02dc\u03b8\u22250 (cid:105) = n (cid:88) j=1 sigmoid (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r (cid:19) , (4) where S is a set of layers to match between the teacher and student models after a linear projection Wi. We use S = which represents the (expected) model size as a differentiable function of current parameters \u03b1. , Table 1: Comparison of our method versus previous distillation methods on SUPERB [6]. Our DPHuBERT and DPWavLM are compressed from publicly available HuBERT Base [2] and WavLM Base+ [4] checkpoints, respectively. Method #Params KS IC PR ASR w/o LM ER QbE SF SID ASV SD Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 FBANK wav2vec 2.0 Base [1] HuBERT Base [2] WavLM Base+ [4] 0 95.04 94.68 94.70 41.38 9.65 96.23 92.35 96.30 98.34 97.37 99.00 82.01 5.74 5.41 3.92 23.18 6.43 6.42 5.59 48.24 63.43 64.92 68.65 0.0058 0.0233 0.0736 0.0988 69.64 / 52.94 20.06 88.30 / 24.77 75.18 88.53 / 25.20 81.42 90.58 / 21.20 89.42 9.56 6.02 5.11 4.07 10.05 6.08 5.88 3.50 Compressed models using LibriSpeech 960h DistilHuBERT [12] FitHuBERT [13] FitW2V2 [13] 12-Layer Half [14] 3-Layer One [14] DPHuBERT (ours) DPWavLM (ours) 23.49 22.49 31.63 26.87 30.58 23.59 23.59 95.98 94.99 16.27 96.27 91.25 13.32 96.04 93.38 12.22 97.24 96.97 10.67 96.69 94.15 13.34 9.67 96.36 97.92 8.22 96.27"}, {"question": " How is the loss function L defined in joint distillation and structured pruning?", "answer": " The loss function L measures the difference between two feature sequences using a combination of L1 and cosine distances.", "ref_chunk": "HJ-Pruning [25] is a structured pruning method that jointly prunes heterogeneous components (i.e., CNN and Transformer) of speech SSL models. It significantly reduces the total computation while retaining good performance in ASR and SLU. These methods deal with specific downstream tasks, but do not investigate the universal speech representa- tions. Our work focuses on task-agnostic structured pruning of speech SSL. As there is no labeled data for normal supervised training, we employ a distillation objective along with pruning. Once-for-all training. Compression methods typically gen- erate a single model with a pre-determined size. LightHu- BERT [28] deploys once-for-all training [29] to obtain numer- ous weight-sharing subnets, which shows very strong perfor- mance on SUPERB [6]. However, it requires an expensive two- stage training process and an advanced distillation loss inspired by data2vec [30]. According to the authors, compressing Hu- BERT Base already takes 2k GPUs hours (i.e., 62 hours with 32 V100 GPUs and 19 hours with 8 GPUs for two stages, respec- tively), which is prohibitive for academic researchers and small businesses. Unlike LightHuBERT, our work aims to compress an existing speech SSL model to a specific sparsity ratio within a manageable amount of training time, which is consistent with the standard setup of prior distillation methods [12, 13]. {0, 4, 8, 12} for base models and {0, 8, 16, 24} for large mod- els. The 0th layer is the output of CNN, which is also the input to the first Transformer layer. The loss function L measures the difference between two feature sequences, which can be L1, L2 or cosine distances [12\u201314]. We follow [12, 14] to use a combi- nation of L1 and cosine distances with equal weights. 3.3. Joint distillation and structured pruning Structured pruning of the student model is formulated as learn- ing a sparse model through L0 regularization [31], which has been explored in NLP [19,20] and speech [25]. The method will be briefly introduced below. For more comprehensive deriva- tions, please refer to prior research [19, 20, 25, 31]. Consider a frozen teacher model f tea(\u00b7) and a student model f stu(\u00b7; \u03b8) with learnable parameters \u03b8 = {\u03b8j}n j=1. Each \u03b8j is a group of prunable parameters (including convolution channels, atten- tion heads, and FFN intermediate units) and there are n groups in total. We define a binary variable zj (called mask) for each \u03b8j. The masks z follow a probability distribution q(z; \u03b1) with parameters \u03b1. The regularized distillation objective is: min \u03b8,\u03b1 Ez\u223cq (cid:34) 1 D D (cid:88) Ldis (cid:16) f tea(xk), f stu(xk; \u02dc\u03b8) (cid:17) + \u03bb\u2225 \u02dc\u03b8\u22250 (cid:35) , k=1 3. DPHuBERT (2) 3.1. Training procedure Figure 1 illustrates our training procedure consisting of two steps. In Step 1, the student model is initialized from the teacher and is jointly distilled and pruned to generate a smaller model with a pre-specified size. In Step 2, the already pruned student model is further distilled to improve performance. In both steps, only unlabeled speech data are utilized and the teacher is frozen. j=1 and each \u02dc\u03b8j = \u03b8jzj. The unlabeled dataset where \u02dc\u03b8 = {\u02dc\u03b8j}n with D samples is {xk}D k=1. \u03bb > 0 is the regularization weight. It is intractable to solve Eq. (2) using gradient descent due to the discrete nature of masks z. To make the loss differentiable, Louizos et al. propose a reparameterization trick which samples z from the Hard Concrete distribution [31]: u \u223c U (0, 1), v(\u03b1) = sigmoid (cid:18)(cid:18) log u 1 \u2212 u + log \u03b1 (cid:19) /\u03b2 (cid:19) 3.2. Distillation loss \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), Unlike DistilHuBERT [12], we use layer-to-layer distillation since the student initially has the same depth as the teacher (see Section 2.2 for discussions). Suppose the teacher has N tea Transformer layers with hidden size dtea and the student has N stu layers with hidden size dstu. Let Xtea (shape T \u00d7 dtea) i and Xstu (shape T \u00d7 dstu) be the output sequences of the i-th i Transformer layers from the teacher and student, respectively, where T is the sequence length. The distillation loss is: where u follows a uniform distribution in [0, 1]. \u03b2 is a constant. l < 0 and r > 0 are two constants to stretch v to [l, r], and it is further clamped to [0, 1]. Only \u03b1 = {\u03b1j}n j=1 are learnable parameters in this distribution. With this trick, the objective in Eq. (2) is differentiable and the regularization term has a closed- form expression [31]: (3) Ldis = (cid:88) i\u2208S L (cid:0)Xtea i , Xstu i Wi (cid:1) , (1) Ez\u223cq (cid:104) \u2225 \u02dc\u03b8\u22250 (cid:105) = n (cid:88) j=1 sigmoid (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r (cid:19) , (4) where S is a set of layers to match between the teacher and student models after a linear projection Wi. We use S = which represents the (expected) model size as a differentiable function of current parameters \u03b1. , Table 1: Comparison of our method versus previous distillation methods on SUPERB [6]. Our DPHuBERT and DPWavLM are compressed from publicly available HuBERT Base [2] and WavLM Base+ [4] checkpoints, respectively. Method #Params KS IC PR ASR w/o LM ER QbE SF SID ASV SD Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 FBANK wav2vec 2.0 Base [1] HuBERT Base [2] WavLM Base+ [4] 0 95.04 94.68 94.70 41.38 9.65 96.23 92.35 96.30 98.34 97.37 99.00 82.01 5.74 5.41 3.92 23.18 6.43 6.42 5.59 48.24 63.43 64.92 68.65 0.0058 0.0233 0.0736 0.0988 69.64 / 52.94 20.06 88.30 / 24.77 75.18 88.53 / 25.20 81.42 90.58 / 21.20 89.42 9.56 6.02 5.11 4.07 10.05 6.08 5.88 3.50 Compressed models using LibriSpeech 960h DistilHuBERT [12] FitHuBERT [13] FitW2V2 [13] 12-Layer Half [14] 3-Layer One [14] DPHuBERT (ours) DPWavLM (ours) 23.49 22.49 31.63 26.87 30.58 23.59 23.59 95.98 94.99 16.27 96.27 91.25 13.32 96.04 93.38 12.22 97.24 96.97 10.67 96.69 94.15 13.34 9.67 96.36 97.92 8.22 96.27"}, {"question": " What is the purpose of Step 1 in the training procedure of DPHuBERT?", "answer": " In Step 1, the student model is jointly distilled and pruned to generate a smaller model with a pre-specified size.", "ref_chunk": "HJ-Pruning [25] is a structured pruning method that jointly prunes heterogeneous components (i.e., CNN and Transformer) of speech SSL models. It significantly reduces the total computation while retaining good performance in ASR and SLU. These methods deal with specific downstream tasks, but do not investigate the universal speech representa- tions. Our work focuses on task-agnostic structured pruning of speech SSL. As there is no labeled data for normal supervised training, we employ a distillation objective along with pruning. Once-for-all training. Compression methods typically gen- erate a single model with a pre-determined size. LightHu- BERT [28] deploys once-for-all training [29] to obtain numer- ous weight-sharing subnets, which shows very strong perfor- mance on SUPERB [6]. However, it requires an expensive two- stage training process and an advanced distillation loss inspired by data2vec [30]. According to the authors, compressing Hu- BERT Base already takes 2k GPUs hours (i.e., 62 hours with 32 V100 GPUs and 19 hours with 8 GPUs for two stages, respec- tively), which is prohibitive for academic researchers and small businesses. Unlike LightHuBERT, our work aims to compress an existing speech SSL model to a specific sparsity ratio within a manageable amount of training time, which is consistent with the standard setup of prior distillation methods [12, 13]. {0, 4, 8, 12} for base models and {0, 8, 16, 24} for large mod- els. The 0th layer is the output of CNN, which is also the input to the first Transformer layer. The loss function L measures the difference between two feature sequences, which can be L1, L2 or cosine distances [12\u201314]. We follow [12, 14] to use a combi- nation of L1 and cosine distances with equal weights. 3.3. Joint distillation and structured pruning Structured pruning of the student model is formulated as learn- ing a sparse model through L0 regularization [31], which has been explored in NLP [19,20] and speech [25]. The method will be briefly introduced below. For more comprehensive deriva- tions, please refer to prior research [19, 20, 25, 31]. Consider a frozen teacher model f tea(\u00b7) and a student model f stu(\u00b7; \u03b8) with learnable parameters \u03b8 = {\u03b8j}n j=1. Each \u03b8j is a group of prunable parameters (including convolution channels, atten- tion heads, and FFN intermediate units) and there are n groups in total. We define a binary variable zj (called mask) for each \u03b8j. The masks z follow a probability distribution q(z; \u03b1) with parameters \u03b1. The regularized distillation objective is: min \u03b8,\u03b1 Ez\u223cq (cid:34) 1 D D (cid:88) Ldis (cid:16) f tea(xk), f stu(xk; \u02dc\u03b8) (cid:17) + \u03bb\u2225 \u02dc\u03b8\u22250 (cid:35) , k=1 3. DPHuBERT (2) 3.1. Training procedure Figure 1 illustrates our training procedure consisting of two steps. In Step 1, the student model is initialized from the teacher and is jointly distilled and pruned to generate a smaller model with a pre-specified size. In Step 2, the already pruned student model is further distilled to improve performance. In both steps, only unlabeled speech data are utilized and the teacher is frozen. j=1 and each \u02dc\u03b8j = \u03b8jzj. The unlabeled dataset where \u02dc\u03b8 = {\u02dc\u03b8j}n with D samples is {xk}D k=1. \u03bb > 0 is the regularization weight. It is intractable to solve Eq. (2) using gradient descent due to the discrete nature of masks z. To make the loss differentiable, Louizos et al. propose a reparameterization trick which samples z from the Hard Concrete distribution [31]: u \u223c U (0, 1), v(\u03b1) = sigmoid (cid:18)(cid:18) log u 1 \u2212 u + log \u03b1 (cid:19) /\u03b2 (cid:19) 3.2. Distillation loss \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), Unlike DistilHuBERT [12], we use layer-to-layer distillation since the student initially has the same depth as the teacher (see Section 2.2 for discussions). Suppose the teacher has N tea Transformer layers with hidden size dtea and the student has N stu layers with hidden size dstu. Let Xtea (shape T \u00d7 dtea) i and Xstu (shape T \u00d7 dstu) be the output sequences of the i-th i Transformer layers from the teacher and student, respectively, where T is the sequence length. The distillation loss is: where u follows a uniform distribution in [0, 1]. \u03b2 is a constant. l < 0 and r > 0 are two constants to stretch v to [l, r], and it is further clamped to [0, 1]. Only \u03b1 = {\u03b1j}n j=1 are learnable parameters in this distribution. With this trick, the objective in Eq. (2) is differentiable and the regularization term has a closed- form expression [31]: (3) Ldis = (cid:88) i\u2208S L (cid:0)Xtea i , Xstu i Wi (cid:1) , (1) Ez\u223cq (cid:104) \u2225 \u02dc\u03b8\u22250 (cid:105) = n (cid:88) j=1 sigmoid (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r (cid:19) , (4) where S is a set of layers to match between the teacher and student models after a linear projection Wi. We use S = which represents the (expected) model size as a differentiable function of current parameters \u03b1. , Table 1: Comparison of our method versus previous distillation methods on SUPERB [6]. Our DPHuBERT and DPWavLM are compressed from publicly available HuBERT Base [2] and WavLM Base+ [4] checkpoints, respectively. Method #Params KS IC PR ASR w/o LM ER QbE SF SID ASV SD Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 FBANK wav2vec 2.0 Base [1] HuBERT Base [2] WavLM Base+ [4] 0 95.04 94.68 94.70 41.38 9.65 96.23 92.35 96.30 98.34 97.37 99.00 82.01 5.74 5.41 3.92 23.18 6.43 6.42 5.59 48.24 63.43 64.92 68.65 0.0058 0.0233 0.0736 0.0988 69.64 / 52.94 20.06 88.30 / 24.77 75.18 88.53 / 25.20 81.42 90.58 / 21.20 89.42 9.56 6.02 5.11 4.07 10.05 6.08 5.88 3.50 Compressed models using LibriSpeech 960h DistilHuBERT [12] FitHuBERT [13] FitW2V2 [13] 12-Layer Half [14] 3-Layer One [14] DPHuBERT (ours) DPWavLM (ours) 23.49 22.49 31.63 26.87 30.58 23.59 23.59 95.98 94.99 16.27 96.27 91.25 13.32 96.04 93.38 12.22 97.24 96.97 10.67 96.69 94.15 13.34 9.67 96.36 97.92 8.22 96.27"}, {"question": " Why is it intractable to solve the objective using gradient descent in Eq. (2)?", "answer": " It is due to the discrete nature of masks z, which makes the loss non-differentiable.", "ref_chunk": "HJ-Pruning [25] is a structured pruning method that jointly prunes heterogeneous components (i.e., CNN and Transformer) of speech SSL models. It significantly reduces the total computation while retaining good performance in ASR and SLU. These methods deal with specific downstream tasks, but do not investigate the universal speech representa- tions. Our work focuses on task-agnostic structured pruning of speech SSL. As there is no labeled data for normal supervised training, we employ a distillation objective along with pruning. Once-for-all training. Compression methods typically gen- erate a single model with a pre-determined size. LightHu- BERT [28] deploys once-for-all training [29] to obtain numer- ous weight-sharing subnets, which shows very strong perfor- mance on SUPERB [6]. However, it requires an expensive two- stage training process and an advanced distillation loss inspired by data2vec [30]. According to the authors, compressing Hu- BERT Base already takes 2k GPUs hours (i.e., 62 hours with 32 V100 GPUs and 19 hours with 8 GPUs for two stages, respec- tively), which is prohibitive for academic researchers and small businesses. Unlike LightHuBERT, our work aims to compress an existing speech SSL model to a specific sparsity ratio within a manageable amount of training time, which is consistent with the standard setup of prior distillation methods [12, 13]. {0, 4, 8, 12} for base models and {0, 8, 16, 24} for large mod- els. The 0th layer is the output of CNN, which is also the input to the first Transformer layer. The loss function L measures the difference between two feature sequences, which can be L1, L2 or cosine distances [12\u201314]. We follow [12, 14] to use a combi- nation of L1 and cosine distances with equal weights. 3.3. Joint distillation and structured pruning Structured pruning of the student model is formulated as learn- ing a sparse model through L0 regularization [31], which has been explored in NLP [19,20] and speech [25]. The method will be briefly introduced below. For more comprehensive deriva- tions, please refer to prior research [19, 20, 25, 31]. Consider a frozen teacher model f tea(\u00b7) and a student model f stu(\u00b7; \u03b8) with learnable parameters \u03b8 = {\u03b8j}n j=1. Each \u03b8j is a group of prunable parameters (including convolution channels, atten- tion heads, and FFN intermediate units) and there are n groups in total. We define a binary variable zj (called mask) for each \u03b8j. The masks z follow a probability distribution q(z; \u03b1) with parameters \u03b1. The regularized distillation objective is: min \u03b8,\u03b1 Ez\u223cq (cid:34) 1 D D (cid:88) Ldis (cid:16) f tea(xk), f stu(xk; \u02dc\u03b8) (cid:17) + \u03bb\u2225 \u02dc\u03b8\u22250 (cid:35) , k=1 3. DPHuBERT (2) 3.1. Training procedure Figure 1 illustrates our training procedure consisting of two steps. In Step 1, the student model is initialized from the teacher and is jointly distilled and pruned to generate a smaller model with a pre-specified size. In Step 2, the already pruned student model is further distilled to improve performance. In both steps, only unlabeled speech data are utilized and the teacher is frozen. j=1 and each \u02dc\u03b8j = \u03b8jzj. The unlabeled dataset where \u02dc\u03b8 = {\u02dc\u03b8j}n with D samples is {xk}D k=1. \u03bb > 0 is the regularization weight. It is intractable to solve Eq. (2) using gradient descent due to the discrete nature of masks z. To make the loss differentiable, Louizos et al. propose a reparameterization trick which samples z from the Hard Concrete distribution [31]: u \u223c U (0, 1), v(\u03b1) = sigmoid (cid:18)(cid:18) log u 1 \u2212 u + log \u03b1 (cid:19) /\u03b2 (cid:19) 3.2. Distillation loss \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), Unlike DistilHuBERT [12], we use layer-to-layer distillation since the student initially has the same depth as the teacher (see Section 2.2 for discussions). Suppose the teacher has N tea Transformer layers with hidden size dtea and the student has N stu layers with hidden size dstu. Let Xtea (shape T \u00d7 dtea) i and Xstu (shape T \u00d7 dstu) be the output sequences of the i-th i Transformer layers from the teacher and student, respectively, where T is the sequence length. The distillation loss is: where u follows a uniform distribution in [0, 1]. \u03b2 is a constant. l < 0 and r > 0 are two constants to stretch v to [l, r], and it is further clamped to [0, 1]. Only \u03b1 = {\u03b1j}n j=1 are learnable parameters in this distribution. With this trick, the objective in Eq. (2) is differentiable and the regularization term has a closed- form expression [31]: (3) Ldis = (cid:88) i\u2208S L (cid:0)Xtea i , Xstu i Wi (cid:1) , (1) Ez\u223cq (cid:104) \u2225 \u02dc\u03b8\u22250 (cid:105) = n (cid:88) j=1 sigmoid (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r (cid:19) , (4) where S is a set of layers to match between the teacher and student models after a linear projection Wi. We use S = which represents the (expected) model size as a differentiable function of current parameters \u03b1. , Table 1: Comparison of our method versus previous distillation methods on SUPERB [6]. Our DPHuBERT and DPWavLM are compressed from publicly available HuBERT Base [2] and WavLM Base+ [4] checkpoints, respectively. Method #Params KS IC PR ASR w/o LM ER QbE SF SID ASV SD Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 FBANK wav2vec 2.0 Base [1] HuBERT Base [2] WavLM Base+ [4] 0 95.04 94.68 94.70 41.38 9.65 96.23 92.35 96.30 98.34 97.37 99.00 82.01 5.74 5.41 3.92 23.18 6.43 6.42 5.59 48.24 63.43 64.92 68.65 0.0058 0.0233 0.0736 0.0988 69.64 / 52.94 20.06 88.30 / 24.77 75.18 88.53 / 25.20 81.42 90.58 / 21.20 89.42 9.56 6.02 5.11 4.07 10.05 6.08 5.88 3.50 Compressed models using LibriSpeech 960h DistilHuBERT [12] FitHuBERT [13] FitW2V2 [13] 12-Layer Half [14] 3-Layer One [14] DPHuBERT (ours) DPWavLM (ours) 23.49 22.49 31.63 26.87 30.58 23.59 23.59 95.98 94.99 16.27 96.27 91.25 13.32 96.04 93.38 12.22 97.24 96.97 10.67 96.69 94.15 13.34 9.67 96.36 97.92 8.22 96.27"}], "doc_text": "HJ-Pruning [25] is a structured pruning method that jointly prunes heterogeneous components (i.e., CNN and Transformer) of speech SSL models. It significantly reduces the total computation while retaining good performance in ASR and SLU. These methods deal with specific downstream tasks, but do not investigate the universal speech representa- tions. Our work focuses on task-agnostic structured pruning of speech SSL. As there is no labeled data for normal supervised training, we employ a distillation objective along with pruning. Once-for-all training. Compression methods typically gen- erate a single model with a pre-determined size. LightHu- BERT [28] deploys once-for-all training [29] to obtain numer- ous weight-sharing subnets, which shows very strong perfor- mance on SUPERB [6]. However, it requires an expensive two- stage training process and an advanced distillation loss inspired by data2vec [30]. According to the authors, compressing Hu- BERT Base already takes 2k GPUs hours (i.e., 62 hours with 32 V100 GPUs and 19 hours with 8 GPUs for two stages, respec- tively), which is prohibitive for academic researchers and small businesses. Unlike LightHuBERT, our work aims to compress an existing speech SSL model to a specific sparsity ratio within a manageable amount of training time, which is consistent with the standard setup of prior distillation methods [12, 13]. {0, 4, 8, 12} for base models and {0, 8, 16, 24} for large mod- els. The 0th layer is the output of CNN, which is also the input to the first Transformer layer. The loss function L measures the difference between two feature sequences, which can be L1, L2 or cosine distances [12\u201314]. We follow [12, 14] to use a combi- nation of L1 and cosine distances with equal weights. 3.3. Joint distillation and structured pruning Structured pruning of the student model is formulated as learn- ing a sparse model through L0 regularization [31], which has been explored in NLP [19,20] and speech [25]. The method will be briefly introduced below. For more comprehensive deriva- tions, please refer to prior research [19, 20, 25, 31]. Consider a frozen teacher model f tea(\u00b7) and a student model f stu(\u00b7; \u03b8) with learnable parameters \u03b8 = {\u03b8j}n j=1. Each \u03b8j is a group of prunable parameters (including convolution channels, atten- tion heads, and FFN intermediate units) and there are n groups in total. We define a binary variable zj (called mask) for each \u03b8j. The masks z follow a probability distribution q(z; \u03b1) with parameters \u03b1. The regularized distillation objective is: min \u03b8,\u03b1 Ez\u223cq (cid:34) 1 D D (cid:88) Ldis (cid:16) f tea(xk), f stu(xk; \u02dc\u03b8) (cid:17) + \u03bb\u2225 \u02dc\u03b8\u22250 (cid:35) , k=1 3. DPHuBERT (2) 3.1. Training procedure Figure 1 illustrates our training procedure consisting of two steps. In Step 1, the student model is initialized from the teacher and is jointly distilled and pruned to generate a smaller model with a pre-specified size. In Step 2, the already pruned student model is further distilled to improve performance. In both steps, only unlabeled speech data are utilized and the teacher is frozen. j=1 and each \u02dc\u03b8j = \u03b8jzj. The unlabeled dataset where \u02dc\u03b8 = {\u02dc\u03b8j}n with D samples is {xk}D k=1. \u03bb > 0 is the regularization weight. It is intractable to solve Eq. (2) using gradient descent due to the discrete nature of masks z. To make the loss differentiable, Louizos et al. propose a reparameterization trick which samples z from the Hard Concrete distribution [31]: u \u223c U (0, 1), v(\u03b1) = sigmoid (cid:18)(cid:18) log u 1 \u2212 u + log \u03b1 (cid:19) /\u03b2 (cid:19) 3.2. Distillation loss \u00afv(\u03b1) = (r \u2212 l) \u00b7 v(\u03b1) + l, z = min(1, max(0, \u00afv(\u03b1))), Unlike DistilHuBERT [12], we use layer-to-layer distillation since the student initially has the same depth as the teacher (see Section 2.2 for discussions). Suppose the teacher has N tea Transformer layers with hidden size dtea and the student has N stu layers with hidden size dstu. Let Xtea (shape T \u00d7 dtea) i and Xstu (shape T \u00d7 dstu) be the output sequences of the i-th i Transformer layers from the teacher and student, respectively, where T is the sequence length. The distillation loss is: where u follows a uniform distribution in [0, 1]. \u03b2 is a constant. l < 0 and r > 0 are two constants to stretch v to [l, r], and it is further clamped to [0, 1]. Only \u03b1 = {\u03b1j}n j=1 are learnable parameters in this distribution. With this trick, the objective in Eq. (2) is differentiable and the regularization term has a closed- form expression [31]: (3) Ldis = (cid:88) i\u2208S L (cid:0)Xtea i , Xstu i Wi (cid:1) , (1) Ez\u223cq (cid:104) \u2225 \u02dc\u03b8\u22250 (cid:105) = n (cid:88) j=1 sigmoid (cid:18) log \u03b1j \u2212 \u03b2 log \u2212l r (cid:19) , (4) where S is a set of layers to match between the teacher and student models after a linear projection Wi. We use S = which represents the (expected) model size as a differentiable function of current parameters \u03b1. , Table 1: Comparison of our method versus previous distillation methods on SUPERB [6]. Our DPHuBERT and DPWavLM are compressed from publicly available HuBERT Base [2] and WavLM Base+ [4] checkpoints, respectively. Method #Params KS IC PR ASR w/o LM ER QbE SF SID ASV SD Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 FBANK wav2vec 2.0 Base [1] HuBERT Base [2] WavLM Base+ [4] 0 95.04 94.68 94.70 41.38 9.65 96.23 92.35 96.30 98.34 97.37 99.00 82.01 5.74 5.41 3.92 23.18 6.43 6.42 5.59 48.24 63.43 64.92 68.65 0.0058 0.0233 0.0736 0.0988 69.64 / 52.94 20.06 88.30 / 24.77 75.18 88.53 / 25.20 81.42 90.58 / 21.20 89.42 9.56 6.02 5.11 4.07 10.05 6.08 5.88 3.50 Compressed models using LibriSpeech 960h DistilHuBERT [12] FitHuBERT [13] FitW2V2 [13] 12-Layer Half [14] 3-Layer One [14] DPHuBERT (ours) DPWavLM (ours) 23.49 22.49 31.63 26.87 30.58 23.59 23.59 95.98 94.99 16.27 96.27 91.25 13.32 96.04 93.38 12.22 97.24 96.97 10.67 96.69 94.15 13.34 9.67 96.36 97.92 8.22 96.27"}