{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Jais_and_Jais-chat:_Arabic-Centric_Foundation_and_Instruction-Tuned_Open_Generative_Large_Language_Models_chunk_9.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What additional overall improvement does model 13B show compared to Jais-chat (6.7B)?", "answer": " +1.9 points", "ref_chunk": "(13B) widens the gap even further, with an additional overall improvement of +1.9 points over Jais-chat (6.7B). Instruction-tuning [OWJ+22] further improves the results over the corresponding base models, with the exception of Falcon (7B). The absolute improvements due to instruction-tuning for Jais-chat (1.3B, 6.7B, 13B) are +0.7, +3.2, and +1.9, respectively, and are similar to those for BLOOMz. The full results for each dataset and model can be found in the Appendix (Table 12). 16 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 34.7 47.3 33.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 29.0 24.5 24.9 30.7 30.9 25.1 28.4 32.4 33.8 27.2 31.4 32.4 36.3 29.4 29.0 28.3 27.5 24.6 28.1 33.7 29.9 30.0 30.0 37.7 36.4 36.2 39.1 39.8 37.1 38.6 40.7 43.7 38.6 41.1 42.3 44.3 36.1 39.3 39.0 38.0 37.5 39.2 44.4 39.5 40.3 40.3 49.3 34.1 49.3 49.3 51.2 50.6 45.9 49.6 51.3 46.4 45.7 49.0 52.1 46.2 47.5 47.7 46.4 47.4 47.7 44.9 49.2 47.7 47.7 36.7 32.0 34.6 38.0 38.7 35.3 36.4 39.4 41.7 36.1 38.6 40.1 42.9 35.5 37.2 36.8 35.9 34.9 36.9 40.9 37.9 38.1 38.1 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 34.2 33.9 36.6 39.6 40.0 41.4 41.6 42.8 45.5 50.3 49.8 52.3 48.6 49.5 49.3 48.4 49.8 50.6 40.3 41.0 43.2 46.4 46.5 48.4 Table 9: Zero-shot evaluation results for Arabic (%). Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Results for English We also performed an evaluation for English. The results are given in Table 10, where we can see that Jais-chat is highly competitive against existing English models, despite having seen less En- glish data in pretraining. First, we observe that the existing Arabic models perform almost randomly on this benchmark, while our models perform substantially better. This result is unsurprising given that AraT5, AraT5- V2, and AraBART were pretrained on Arabic data only. In comparison to the multilingual BLOOMz (1.1B), Jais-chat (1.3B) performs +3.4 points better. We can further see that Jais-chat (13B) performs on par with the recently released LLaMA2-chat (13B) model (57.3 vs. 57.7), even though the latter is trained on 2T of English word tokens, while our model has only seen 232B English word token. Jais-chat (13B) also outperforms other baselines including mT0-xxl (13B) and Falcon (7B), by margins ranging from +2.6 to +7.2 points absolute. Our instruction-tuning is also effective, with improvements of +3.9, +4.3, and +3.4, for the 1.3B, 6.7B, and 13B models, respectively. The full results for each dataset and model can be found in the Appendix (Table 13). 5.2 Generation Evaluation Dataset We next perform evaluation of the models over the core capability of Arabic text generation. Follow- ing prior work [PLH+23, CLL+23], we perform automatic evaluation over the generated Arabic content using GPT-4 [Ope23] based on Vicuna-Instructions-80, which were manually translated to Arabic by translators. 17 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 36.9 47.3 36.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 25.8 24.0 24.7 30.5 32.3 25.6 30.7 31.8 39.0 30.0 34.7 32.6 39.8 34.9 35 37.5 33.4 32.5 30.0 38.1 34.7 36.2 39.3 37.8 36.1 35.8 46.0 47.9 37.3 44.4 50.0 60.7 42.4 48.6 53.7 63.3 59.6 58.9 60.8 61.2 59.4 40.7 53.2 60.6 60.8 63.7 50.3 36.9 49.4 52.1 52.4 49.9 50.2 52.8 51.2 46.3 48.4 53.9 55.4 44.7 55.4 57.4 53.4 57.7 44.8 51.2 44.6 53.7 54.9 37.9 34.0 36.2 44.3 45.9 37.5 43.0 47.2 55.0 40.9 46.1 49.9 57.6 52.4 53.9 55.9 54.7 54.2 39.5 50.1 53.0 55.0 57.7 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 30.1 32.5 32.8 37.6 34.6 38.5 47.9 53.4 53.8 59.2 59.5 63.7 52.2 52.0. 54.0 53.3 53.5 53.9 45.4 49.3 50.0 54.3 53.9 57.3 Table 10: Zero-shot evaluation results for English. We can see that our model is competitive on English despite being Arabic-centric. Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Vicuna-Instructions-8041 consists of 80 challenging and open-ended questions across eight categories: knowl- edge, Fermi, counterfactual, roleplay, generic, math and coding, writing, and common-sense. Evaluation Setup We generate outputs for Arabic prompts in Vicuna-Instructions-80 using a temperature of 0.3 and a repetition penalty of 1.2. As baselines, we use two closed-source models, ChatGPT (175B) [OWJ+22] and Claude (52B).42 We further use several open-source models, which are either Arabic centric or multilin- gual: BLOOM (7B) [SFA+23], BLOOMz (7B) [MWS+23], AraT5 (220M) [NEAM22], AraT5-v2 (220M) [NEAM22], AraBART (550M) [KETH+22], and LLaMA2 (13B) [TMS+23]. We also include as baselines Bactrian-XLLaMA (13B) and Bactrian-XBLOOM (7B) [LKW+23], which are LLaMA and BLOOM base models, respectively, fine-tuned on multi-lingual (including Arabic) instruction-tuning datasets. For convenience, we name them BXLLaMA and BXBLOOM, respectively. We evaluate these baselines against our instruction-tuned models \u2013 Jais-chat (6.7B) and Jais-chat (13B). During the GPT-4 evaluation, we perform pairwise compar- isons between all pairs of models. We first prompt GPT-4 to score each pair of models based on their outputs generated for the prompts in the Arabic Vicuna-Instructions-80. We randomly permute the answers from both candidates, aiming to have any one as the first candidate"}, {"question": " Which model shows improvements due to instruction-tuning for Jais-chat of +0.7, +3.2, and +1.9 points?", "answer": " Jais-chat (1.3B, 6.7B, 13B)", "ref_chunk": "(13B) widens the gap even further, with an additional overall improvement of +1.9 points over Jais-chat (6.7B). Instruction-tuning [OWJ+22] further improves the results over the corresponding base models, with the exception of Falcon (7B). The absolute improvements due to instruction-tuning for Jais-chat (1.3B, 6.7B, 13B) are +0.7, +3.2, and +1.9, respectively, and are similar to those for BLOOMz. The full results for each dataset and model can be found in the Appendix (Table 12). 16 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 34.7 47.3 33.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 29.0 24.5 24.9 30.7 30.9 25.1 28.4 32.4 33.8 27.2 31.4 32.4 36.3 29.4 29.0 28.3 27.5 24.6 28.1 33.7 29.9 30.0 30.0 37.7 36.4 36.2 39.1 39.8 37.1 38.6 40.7 43.7 38.6 41.1 42.3 44.3 36.1 39.3 39.0 38.0 37.5 39.2 44.4 39.5 40.3 40.3 49.3 34.1 49.3 49.3 51.2 50.6 45.9 49.6 51.3 46.4 45.7 49.0 52.1 46.2 47.5 47.7 46.4 47.4 47.7 44.9 49.2 47.7 47.7 36.7 32.0 34.6 38.0 38.7 35.3 36.4 39.4 41.7 36.1 38.6 40.1 42.9 35.5 37.2 36.8 35.9 34.9 36.9 40.9 37.9 38.1 38.1 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 34.2 33.9 36.6 39.6 40.0 41.4 41.6 42.8 45.5 50.3 49.8 52.3 48.6 49.5 49.3 48.4 49.8 50.6 40.3 41.0 43.2 46.4 46.5 48.4 Table 9: Zero-shot evaluation results for Arabic (%). Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Results for English We also performed an evaluation for English. The results are given in Table 10, where we can see that Jais-chat is highly competitive against existing English models, despite having seen less En- glish data in pretraining. First, we observe that the existing Arabic models perform almost randomly on this benchmark, while our models perform substantially better. This result is unsurprising given that AraT5, AraT5- V2, and AraBART were pretrained on Arabic data only. In comparison to the multilingual BLOOMz (1.1B), Jais-chat (1.3B) performs +3.4 points better. We can further see that Jais-chat (13B) performs on par with the recently released LLaMA2-chat (13B) model (57.3 vs. 57.7), even though the latter is trained on 2T of English word tokens, while our model has only seen 232B English word token. Jais-chat (13B) also outperforms other baselines including mT0-xxl (13B) and Falcon (7B), by margins ranging from +2.6 to +7.2 points absolute. Our instruction-tuning is also effective, with improvements of +3.9, +4.3, and +3.4, for the 1.3B, 6.7B, and 13B models, respectively. The full results for each dataset and model can be found in the Appendix (Table 13). 5.2 Generation Evaluation Dataset We next perform evaluation of the models over the core capability of Arabic text generation. Follow- ing prior work [PLH+23, CLL+23], we perform automatic evaluation over the generated Arabic content using GPT-4 [Ope23] based on Vicuna-Instructions-80, which were manually translated to Arabic by translators. 17 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 36.9 47.3 36.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 25.8 24.0 24.7 30.5 32.3 25.6 30.7 31.8 39.0 30.0 34.7 32.6 39.8 34.9 35 37.5 33.4 32.5 30.0 38.1 34.7 36.2 39.3 37.8 36.1 35.8 46.0 47.9 37.3 44.4 50.0 60.7 42.4 48.6 53.7 63.3 59.6 58.9 60.8 61.2 59.4 40.7 53.2 60.6 60.8 63.7 50.3 36.9 49.4 52.1 52.4 49.9 50.2 52.8 51.2 46.3 48.4 53.9 55.4 44.7 55.4 57.4 53.4 57.7 44.8 51.2 44.6 53.7 54.9 37.9 34.0 36.2 44.3 45.9 37.5 43.0 47.2 55.0 40.9 46.1 49.9 57.6 52.4 53.9 55.9 54.7 54.2 39.5 50.1 53.0 55.0 57.7 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 30.1 32.5 32.8 37.6 34.6 38.5 47.9 53.4 53.8 59.2 59.5 63.7 52.2 52.0. 54.0 53.3 53.5 53.9 45.4 49.3 50.0 54.3 53.9 57.3 Table 10: Zero-shot evaluation results for English. We can see that our model is competitive on English despite being Arabic-centric. Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Vicuna-Instructions-8041 consists of 80 challenging and open-ended questions across eight categories: knowl- edge, Fermi, counterfactual, roleplay, generic, math and coding, writing, and common-sense. Evaluation Setup We generate outputs for Arabic prompts in Vicuna-Instructions-80 using a temperature of 0.3 and a repetition penalty of 1.2. As baselines, we use two closed-source models, ChatGPT (175B) [OWJ+22] and Claude (52B).42 We further use several open-source models, which are either Arabic centric or multilin- gual: BLOOM (7B) [SFA+23], BLOOMz (7B) [MWS+23], AraT5 (220M) [NEAM22], AraT5-v2 (220M) [NEAM22], AraBART (550M) [KETH+22], and LLaMA2 (13B) [TMS+23]. We also include as baselines Bactrian-XLLaMA (13B) and Bactrian-XBLOOM (7B) [LKW+23], which are LLaMA and BLOOM base models, respectively, fine-tuned on multi-lingual (including Arabic) instruction-tuning datasets. For convenience, we name them BXLLaMA and BXBLOOM, respectively. We evaluate these baselines against our instruction-tuned models \u2013 Jais-chat (6.7B) and Jais-chat (13B). During the GPT-4 evaluation, we perform pairwise compar- isons between all pairs of models. We first prompt GPT-4 to score each pair of models based on their outputs generated for the prompts in the Arabic Vicuna-Instructions-80. We randomly permute the answers from both candidates, aiming to have any one as the first candidate"}, {"question": " What is the performance of Jais-chat compared to existing English models?", "answer": " Jais-chat is highly competitive despite seeing less English data in pretraining", "ref_chunk": "(13B) widens the gap even further, with an additional overall improvement of +1.9 points over Jais-chat (6.7B). Instruction-tuning [OWJ+22] further improves the results over the corresponding base models, with the exception of Falcon (7B). The absolute improvements due to instruction-tuning for Jais-chat (1.3B, 6.7B, 13B) are +0.7, +3.2, and +1.9, respectively, and are similar to those for BLOOMz. The full results for each dataset and model can be found in the Appendix (Table 12). 16 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 34.7 47.3 33.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 29.0 24.5 24.9 30.7 30.9 25.1 28.4 32.4 33.8 27.2 31.4 32.4 36.3 29.4 29.0 28.3 27.5 24.6 28.1 33.7 29.9 30.0 30.0 37.7 36.4 36.2 39.1 39.8 37.1 38.6 40.7 43.7 38.6 41.1 42.3 44.3 36.1 39.3 39.0 38.0 37.5 39.2 44.4 39.5 40.3 40.3 49.3 34.1 49.3 49.3 51.2 50.6 45.9 49.6 51.3 46.4 45.7 49.0 52.1 46.2 47.5 47.7 46.4 47.4 47.7 44.9 49.2 47.7 47.7 36.7 32.0 34.6 38.0 38.7 35.3 36.4 39.4 41.7 36.1 38.6 40.1 42.9 35.5 37.2 36.8 35.9 34.9 36.9 40.9 37.9 38.1 38.1 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 34.2 33.9 36.6 39.6 40.0 41.4 41.6 42.8 45.5 50.3 49.8 52.3 48.6 49.5 49.3 48.4 49.8 50.6 40.3 41.0 43.2 46.4 46.5 48.4 Table 9: Zero-shot evaluation results for Arabic (%). Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Results for English We also performed an evaluation for English. The results are given in Table 10, where we can see that Jais-chat is highly competitive against existing English models, despite having seen less En- glish data in pretraining. First, we observe that the existing Arabic models perform almost randomly on this benchmark, while our models perform substantially better. This result is unsurprising given that AraT5, AraT5- V2, and AraBART were pretrained on Arabic data only. In comparison to the multilingual BLOOMz (1.1B), Jais-chat (1.3B) performs +3.4 points better. We can further see that Jais-chat (13B) performs on par with the recently released LLaMA2-chat (13B) model (57.3 vs. 57.7), even though the latter is trained on 2T of English word tokens, while our model has only seen 232B English word token. Jais-chat (13B) also outperforms other baselines including mT0-xxl (13B) and Falcon (7B), by margins ranging from +2.6 to +7.2 points absolute. Our instruction-tuning is also effective, with improvements of +3.9, +4.3, and +3.4, for the 1.3B, 6.7B, and 13B models, respectively. The full results for each dataset and model can be found in the Appendix (Table 13). 5.2 Generation Evaluation Dataset We next perform evaluation of the models over the core capability of Arabic text generation. Follow- ing prior work [PLH+23, CLL+23], we perform automatic evaluation over the generated Arabic content using GPT-4 [Ope23] based on Vicuna-Instructions-80, which were manually translated to Arabic by translators. 17 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 36.9 47.3 36.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 25.8 24.0 24.7 30.5 32.3 25.6 30.7 31.8 39.0 30.0 34.7 32.6 39.8 34.9 35 37.5 33.4 32.5 30.0 38.1 34.7 36.2 39.3 37.8 36.1 35.8 46.0 47.9 37.3 44.4 50.0 60.7 42.4 48.6 53.7 63.3 59.6 58.9 60.8 61.2 59.4 40.7 53.2 60.6 60.8 63.7 50.3 36.9 49.4 52.1 52.4 49.9 50.2 52.8 51.2 46.3 48.4 53.9 55.4 44.7 55.4 57.4 53.4 57.7 44.8 51.2 44.6 53.7 54.9 37.9 34.0 36.2 44.3 45.9 37.5 43.0 47.2 55.0 40.9 46.1 49.9 57.6 52.4 53.9 55.9 54.7 54.2 39.5 50.1 53.0 55.0 57.7 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 30.1 32.5 32.8 37.6 34.6 38.5 47.9 53.4 53.8 59.2 59.5 63.7 52.2 52.0. 54.0 53.3 53.5 53.9 45.4 49.3 50.0 54.3 53.9 57.3 Table 10: Zero-shot evaluation results for English. We can see that our model is competitive on English despite being Arabic-centric. Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Vicuna-Instructions-8041 consists of 80 challenging and open-ended questions across eight categories: knowl- edge, Fermi, counterfactual, roleplay, generic, math and coding, writing, and common-sense. Evaluation Setup We generate outputs for Arabic prompts in Vicuna-Instructions-80 using a temperature of 0.3 and a repetition penalty of 1.2. As baselines, we use two closed-source models, ChatGPT (175B) [OWJ+22] and Claude (52B).42 We further use several open-source models, which are either Arabic centric or multilin- gual: BLOOM (7B) [SFA+23], BLOOMz (7B) [MWS+23], AraT5 (220M) [NEAM22], AraT5-v2 (220M) [NEAM22], AraBART (550M) [KETH+22], and LLaMA2 (13B) [TMS+23]. We also include as baselines Bactrian-XLLaMA (13B) and Bactrian-XBLOOM (7B) [LKW+23], which are LLaMA and BLOOM base models, respectively, fine-tuned on multi-lingual (including Arabic) instruction-tuning datasets. For convenience, we name them BXLLaMA and BXBLOOM, respectively. We evaluate these baselines against our instruction-tuned models \u2013 Jais-chat (6.7B) and Jais-chat (13B). During the GPT-4 evaluation, we perform pairwise compar- isons between all pairs of models. We first prompt GPT-4 to score each pair of models based on their outputs generated for the prompts in the Arabic Vicuna-Instructions-80. We randomly permute the answers from both candidates, aiming to have any one as the first candidate"}, {"question": " Which model performs better in English compared to BLOOMz (1.1B)?", "answer": " Jais-chat (1.3B)", "ref_chunk": "(13B) widens the gap even further, with an additional overall improvement of +1.9 points over Jais-chat (6.7B). Instruction-tuning [OWJ+22] further improves the results over the corresponding base models, with the exception of Falcon (7B). The absolute improvements due to instruction-tuning for Jais-chat (1.3B, 6.7B, 13B) are +0.7, +3.2, and +1.9, respectively, and are similar to those for BLOOMz. The full results for each dataset and model can be found in the Appendix (Table 12). 16 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 34.7 47.3 33.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 29.0 24.5 24.9 30.7 30.9 25.1 28.4 32.4 33.8 27.2 31.4 32.4 36.3 29.4 29.0 28.3 27.5 24.6 28.1 33.7 29.9 30.0 30.0 37.7 36.4 36.2 39.1 39.8 37.1 38.6 40.7 43.7 38.6 41.1 42.3 44.3 36.1 39.3 39.0 38.0 37.5 39.2 44.4 39.5 40.3 40.3 49.3 34.1 49.3 49.3 51.2 50.6 45.9 49.6 51.3 46.4 45.7 49.0 52.1 46.2 47.5 47.7 46.4 47.4 47.7 44.9 49.2 47.7 47.7 36.7 32.0 34.6 38.0 38.7 35.3 36.4 39.4 41.7 36.1 38.6 40.1 42.9 35.5 37.2 36.8 35.9 34.9 36.9 40.9 37.9 38.1 38.1 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 34.2 33.9 36.6 39.6 40.0 41.4 41.6 42.8 45.5 50.3 49.8 52.3 48.6 49.5 49.3 48.4 49.8 50.6 40.3 41.0 43.2 46.4 46.5 48.4 Table 9: Zero-shot evaluation results for Arabic (%). Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Results for English We also performed an evaluation for English. The results are given in Table 10, where we can see that Jais-chat is highly competitive against existing English models, despite having seen less En- glish data in pretraining. First, we observe that the existing Arabic models perform almost randomly on this benchmark, while our models perform substantially better. This result is unsurprising given that AraT5, AraT5- V2, and AraBART were pretrained on Arabic data only. In comparison to the multilingual BLOOMz (1.1B), Jais-chat (1.3B) performs +3.4 points better. We can further see that Jais-chat (13B) performs on par with the recently released LLaMA2-chat (13B) model (57.3 vs. 57.7), even though the latter is trained on 2T of English word tokens, while our model has only seen 232B English word token. Jais-chat (13B) also outperforms other baselines including mT0-xxl (13B) and Falcon (7B), by margins ranging from +2.6 to +7.2 points absolute. Our instruction-tuning is also effective, with improvements of +3.9, +4.3, and +3.4, for the 1.3B, 6.7B, and 13B models, respectively. The full results for each dataset and model can be found in the Appendix (Table 13). 5.2 Generation Evaluation Dataset We next perform evaluation of the models over the core capability of Arabic text generation. Follow- ing prior work [PLH+23, CLL+23], we perform automatic evaluation over the generated Arabic content using GPT-4 [Ope23] based on Vicuna-Instructions-80, which were manually translated to Arabic by translators. 17 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 36.9 47.3 36.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 25.8 24.0 24.7 30.5 32.3 25.6 30.7 31.8 39.0 30.0 34.7 32.6 39.8 34.9 35 37.5 33.4 32.5 30.0 38.1 34.7 36.2 39.3 37.8 36.1 35.8 46.0 47.9 37.3 44.4 50.0 60.7 42.4 48.6 53.7 63.3 59.6 58.9 60.8 61.2 59.4 40.7 53.2 60.6 60.8 63.7 50.3 36.9 49.4 52.1 52.4 49.9 50.2 52.8 51.2 46.3 48.4 53.9 55.4 44.7 55.4 57.4 53.4 57.7 44.8 51.2 44.6 53.7 54.9 37.9 34.0 36.2 44.3 45.9 37.5 43.0 47.2 55.0 40.9 46.1 49.9 57.6 52.4 53.9 55.9 54.7 54.2 39.5 50.1 53.0 55.0 57.7 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 30.1 32.5 32.8 37.6 34.6 38.5 47.9 53.4 53.8 59.2 59.5 63.7 52.2 52.0. 54.0 53.3 53.5 53.9 45.4 49.3 50.0 54.3 53.9 57.3 Table 10: Zero-shot evaluation results for English. We can see that our model is competitive on English despite being Arabic-centric. Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Vicuna-Instructions-8041 consists of 80 challenging and open-ended questions across eight categories: knowl- edge, Fermi, counterfactual, roleplay, generic, math and coding, writing, and common-sense. Evaluation Setup We generate outputs for Arabic prompts in Vicuna-Instructions-80 using a temperature of 0.3 and a repetition penalty of 1.2. As baselines, we use two closed-source models, ChatGPT (175B) [OWJ+22] and Claude (52B).42 We further use several open-source models, which are either Arabic centric or multilin- gual: BLOOM (7B) [SFA+23], BLOOMz (7B) [MWS+23], AraT5 (220M) [NEAM22], AraT5-v2 (220M) [NEAM22], AraBART (550M) [KETH+22], and LLaMA2 (13B) [TMS+23]. We also include as baselines Bactrian-XLLaMA (13B) and Bactrian-XBLOOM (7B) [LKW+23], which are LLaMA and BLOOM base models, respectively, fine-tuned on multi-lingual (including Arabic) instruction-tuning datasets. For convenience, we name them BXLLaMA and BXBLOOM, respectively. We evaluate these baselines against our instruction-tuned models \u2013 Jais-chat (6.7B) and Jais-chat (13B). During the GPT-4 evaluation, we perform pairwise compar- isons between all pairs of models. We first prompt GPT-4 to score each pair of models based on their outputs generated for the prompts in the Arabic Vicuna-Instructions-80. We randomly permute the answers from both candidates, aiming to have any one as the first candidate"}, {"question": " How does Jais-chat (13B) perform against LLaMA2-chat (13B) in English?", "answer": " They perform on par, with scores of 57.3 vs. 57.7", "ref_chunk": "(13B) widens the gap even further, with an additional overall improvement of +1.9 points over Jais-chat (6.7B). Instruction-tuning [OWJ+22] further improves the results over the corresponding base models, with the exception of Falcon (7B). The absolute improvements due to instruction-tuning for Jais-chat (1.3B, 6.7B, 13B) are +0.7, +3.2, and +1.9, respectively, and are similar to those for BLOOMz. The full results for each dataset and model can be found in the Appendix (Table 12). 16 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 34.7 47.3 33.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 29.0 24.5 24.9 30.7 30.9 25.1 28.4 32.4 33.8 27.2 31.4 32.4 36.3 29.4 29.0 28.3 27.5 24.6 28.1 33.7 29.9 30.0 30.0 37.7 36.4 36.2 39.1 39.8 37.1 38.6 40.7 43.7 38.6 41.1 42.3 44.3 36.1 39.3 39.0 38.0 37.5 39.2 44.4 39.5 40.3 40.3 49.3 34.1 49.3 49.3 51.2 50.6 45.9 49.6 51.3 46.4 45.7 49.0 52.1 46.2 47.5 47.7 46.4 47.4 47.7 44.9 49.2 47.7 47.7 36.7 32.0 34.6 38.0 38.7 35.3 36.4 39.4 41.7 36.1 38.6 40.1 42.9 35.5 37.2 36.8 35.9 34.9 36.9 40.9 37.9 38.1 38.1 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 34.2 33.9 36.6 39.6 40.0 41.4 41.6 42.8 45.5 50.3 49.8 52.3 48.6 49.5 49.3 48.4 49.8 50.6 40.3 41.0 43.2 46.4 46.5 48.4 Table 9: Zero-shot evaluation results for Arabic (%). Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Results for English We also performed an evaluation for English. The results are given in Table 10, where we can see that Jais-chat is highly competitive against existing English models, despite having seen less En- glish data in pretraining. First, we observe that the existing Arabic models perform almost randomly on this benchmark, while our models perform substantially better. This result is unsurprising given that AraT5, AraT5- V2, and AraBART were pretrained on Arabic data only. In comparison to the multilingual BLOOMz (1.1B), Jais-chat (1.3B) performs +3.4 points better. We can further see that Jais-chat (13B) performs on par with the recently released LLaMA2-chat (13B) model (57.3 vs. 57.7), even though the latter is trained on 2T of English word tokens, while our model has only seen 232B English word token. Jais-chat (13B) also outperforms other baselines including mT0-xxl (13B) and Falcon (7B), by margins ranging from +2.6 to +7.2 points absolute. Our instruction-tuning is also effective, with improvements of +3.9, +4.3, and +3.4, for the 1.3B, 6.7B, and 13B models, respectively. The full results for each dataset and model can be found in the Appendix (Table 13). 5.2 Generation Evaluation Dataset We next perform evaluation of the models over the core capability of Arabic text generation. Follow- ing prior work [PLH+23, CLL+23], we perform automatic evaluation over the generated Arabic content using GPT-4 [Ope23] based on Vicuna-Instructions-80, which were manually translated to Arabic by translators. 17 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 36.9 47.3 36.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 25.8 24.0 24.7 30.5 32.3 25.6 30.7 31.8 39.0 30.0 34.7 32.6 39.8 34.9 35 37.5 33.4 32.5 30.0 38.1 34.7 36.2 39.3 37.8 36.1 35.8 46.0 47.9 37.3 44.4 50.0 60.7 42.4 48.6 53.7 63.3 59.6 58.9 60.8 61.2 59.4 40.7 53.2 60.6 60.8 63.7 50.3 36.9 49.4 52.1 52.4 49.9 50.2 52.8 51.2 46.3 48.4 53.9 55.4 44.7 55.4 57.4 53.4 57.7 44.8 51.2 44.6 53.7 54.9 37.9 34.0 36.2 44.3 45.9 37.5 43.0 47.2 55.0 40.9 46.1 49.9 57.6 52.4 53.9 55.9 54.7 54.2 39.5 50.1 53.0 55.0 57.7 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 30.1 32.5 32.8 37.6 34.6 38.5 47.9 53.4 53.8 59.2 59.5 63.7 52.2 52.0. 54.0 53.3 53.5 53.9 45.4 49.3 50.0 54.3 53.9 57.3 Table 10: Zero-shot evaluation results for English. We can see that our model is competitive on English despite being Arabic-centric. Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Vicuna-Instructions-8041 consists of 80 challenging and open-ended questions across eight categories: knowl- edge, Fermi, counterfactual, roleplay, generic, math and coding, writing, and common-sense. Evaluation Setup We generate outputs for Arabic prompts in Vicuna-Instructions-80 using a temperature of 0.3 and a repetition penalty of 1.2. As baselines, we use two closed-source models, ChatGPT (175B) [OWJ+22] and Claude (52B).42 We further use several open-source models, which are either Arabic centric or multilin- gual: BLOOM (7B) [SFA+23], BLOOMz (7B) [MWS+23], AraT5 (220M) [NEAM22], AraT5-v2 (220M) [NEAM22], AraBART (550M) [KETH+22], and LLaMA2 (13B) [TMS+23]. We also include as baselines Bactrian-XLLaMA (13B) and Bactrian-XBLOOM (7B) [LKW+23], which are LLaMA and BLOOM base models, respectively, fine-tuned on multi-lingual (including Arabic) instruction-tuning datasets. For convenience, we name them BXLLaMA and BXBLOOM, respectively. We evaluate these baselines against our instruction-tuned models \u2013 Jais-chat (6.7B) and Jais-chat (13B). During the GPT-4 evaluation, we perform pairwise compar- isons between all pairs of models. We first prompt GPT-4 to score each pair of models based on their outputs generated for the prompts in the Arabic Vicuna-Instructions-80. We randomly permute the answers from both candidates, aiming to have any one as the first candidate"}, {"question": " What is the range of improvement margins shown by Jais-chat (13B) over mT0-xxl (13B) and Falcon (7B)?", "answer": " +2.6 to +7.2 points absolute", "ref_chunk": "(13B) widens the gap even further, with an additional overall improvement of +1.9 points over Jais-chat (6.7B). Instruction-tuning [OWJ+22] further improves the results over the corresponding base models, with the exception of Falcon (7B). The absolute improvements due to instruction-tuning for Jais-chat (1.3B, 6.7B, 13B) are +0.7, +3.2, and +1.9, respectively, and are similar to those for BLOOMz. The full results for each dataset and model can be found in the Appendix (Table 12). 16 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 34.7 47.3 33.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 29.0 24.5 24.9 30.7 30.9 25.1 28.4 32.4 33.8 27.2 31.4 32.4 36.3 29.4 29.0 28.3 27.5 24.6 28.1 33.7 29.9 30.0 30.0 37.7 36.4 36.2 39.1 39.8 37.1 38.6 40.7 43.7 38.6 41.1 42.3 44.3 36.1 39.3 39.0 38.0 37.5 39.2 44.4 39.5 40.3 40.3 49.3 34.1 49.3 49.3 51.2 50.6 45.9 49.6 51.3 46.4 45.7 49.0 52.1 46.2 47.5 47.7 46.4 47.4 47.7 44.9 49.2 47.7 47.7 36.7 32.0 34.6 38.0 38.7 35.3 36.4 39.4 41.7 36.1 38.6 40.1 42.9 35.5 37.2 36.8 35.9 34.9 36.9 40.9 37.9 38.1 38.1 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 34.2 33.9 36.6 39.6 40.0 41.4 41.6 42.8 45.5 50.3 49.8 52.3 48.6 49.5 49.3 48.4 49.8 50.6 40.3 41.0 43.2 46.4 46.5 48.4 Table 9: Zero-shot evaluation results for Arabic (%). Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Results for English We also performed an evaluation for English. The results are given in Table 10, where we can see that Jais-chat is highly competitive against existing English models, despite having seen less En- glish data in pretraining. First, we observe that the existing Arabic models perform almost randomly on this benchmark, while our models perform substantially better. This result is unsurprising given that AraT5, AraT5- V2, and AraBART were pretrained on Arabic data only. In comparison to the multilingual BLOOMz (1.1B), Jais-chat (1.3B) performs +3.4 points better. We can further see that Jais-chat (13B) performs on par with the recently released LLaMA2-chat (13B) model (57.3 vs. 57.7), even though the latter is trained on 2T of English word tokens, while our model has only seen 232B English word token. Jais-chat (13B) also outperforms other baselines including mT0-xxl (13B) and Falcon (7B), by margins ranging from +2.6 to +7.2 points absolute. Our instruction-tuning is also effective, with improvements of +3.9, +4.3, and +3.4, for the 1.3B, 6.7B, and 13B models, respectively. The full results for each dataset and model can be found in the Appendix (Table 13). 5.2 Generation Evaluation Dataset We next perform evaluation of the models over the core capability of Arabic text generation. Follow- ing prior work [PLH+23, CLL+23], we perform automatic evaluation over the generated Arabic content using GPT-4 [Ope23] based on Vicuna-Instructions-80, which were manually translated to Arabic by translators. 17 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 36.9 47.3 36.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 25.8 24.0 24.7 30.5 32.3 25.6 30.7 31.8 39.0 30.0 34.7 32.6 39.8 34.9 35 37.5 33.4 32.5 30.0 38.1 34.7 36.2 39.3 37.8 36.1 35.8 46.0 47.9 37.3 44.4 50.0 60.7 42.4 48.6 53.7 63.3 59.6 58.9 60.8 61.2 59.4 40.7 53.2 60.6 60.8 63.7 50.3 36.9 49.4 52.1 52.4 49.9 50.2 52.8 51.2 46.3 48.4 53.9 55.4 44.7 55.4 57.4 53.4 57.7 44.8 51.2 44.6 53.7 54.9 37.9 34.0 36.2 44.3 45.9 37.5 43.0 47.2 55.0 40.9 46.1 49.9 57.6 52.4 53.9 55.9 54.7 54.2 39.5 50.1 53.0 55.0 57.7 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 30.1 32.5 32.8 37.6 34.6 38.5 47.9 53.4 53.8 59.2 59.5 63.7 52.2 52.0. 54.0 53.3 53.5 53.9 45.4 49.3 50.0 54.3 53.9 57.3 Table 10: Zero-shot evaluation results for English. We can see that our model is competitive on English despite being Arabic-centric. Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Vicuna-Instructions-8041 consists of 80 challenging and open-ended questions across eight categories: knowl- edge, Fermi, counterfactual, roleplay, generic, math and coding, writing, and common-sense. Evaluation Setup We generate outputs for Arabic prompts in Vicuna-Instructions-80 using a temperature of 0.3 and a repetition penalty of 1.2. As baselines, we use two closed-source models, ChatGPT (175B) [OWJ+22] and Claude (52B).42 We further use several open-source models, which are either Arabic centric or multilin- gual: BLOOM (7B) [SFA+23], BLOOMz (7B) [MWS+23], AraT5 (220M) [NEAM22], AraT5-v2 (220M) [NEAM22], AraBART (550M) [KETH+22], and LLaMA2 (13B) [TMS+23]. We also include as baselines Bactrian-XLLaMA (13B) and Bactrian-XBLOOM (7B) [LKW+23], which are LLaMA and BLOOM base models, respectively, fine-tuned on multi-lingual (including Arabic) instruction-tuning datasets. For convenience, we name them BXLLaMA and BXBLOOM, respectively. We evaluate these baselines against our instruction-tuned models \u2013 Jais-chat (6.7B) and Jais-chat (13B). During the GPT-4 evaluation, we perform pairwise compar- isons between all pairs of models. We first prompt GPT-4 to score each pair of models based on their outputs generated for the prompts in the Arabic Vicuna-Instructions-80. We randomly permute the answers from both candidates, aiming to have any one as the first candidate"}, {"question": " What are the improvements due to instruction-tuning for the models 1.3B, 6.7B, and 13B?", "answer": " +3.9, +4.3, and +3.4 points, respectively", "ref_chunk": "(13B) widens the gap even further, with an additional overall improvement of +1.9 points over Jais-chat (6.7B). Instruction-tuning [OWJ+22] further improves the results over the corresponding base models, with the exception of Falcon (7B). The absolute improvements due to instruction-tuning for Jais-chat (1.3B, 6.7B, 13B) are +0.7, +3.2, and +1.9, respectively, and are similar to those for BLOOMz. The full results for each dataset and model can be found in the Appendix (Table 12). 16 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 34.7 47.3 33.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 29.0 24.5 24.9 30.7 30.9 25.1 28.4 32.4 33.8 27.2 31.4 32.4 36.3 29.4 29.0 28.3 27.5 24.6 28.1 33.7 29.9 30.0 30.0 37.7 36.4 36.2 39.1 39.8 37.1 38.6 40.7 43.7 38.6 41.1 42.3 44.3 36.1 39.3 39.0 38.0 37.5 39.2 44.4 39.5 40.3 40.3 49.3 34.1 49.3 49.3 51.2 50.6 45.9 49.6 51.3 46.4 45.7 49.0 52.1 46.2 47.5 47.7 46.4 47.4 47.7 44.9 49.2 47.7 47.7 36.7 32.0 34.6 38.0 38.7 35.3 36.4 39.4 41.7 36.1 38.6 40.1 42.9 35.5 37.2 36.8 35.9 34.9 36.9 40.9 37.9 38.1 38.1 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 34.2 33.9 36.6 39.6 40.0 41.4 41.6 42.8 45.5 50.3 49.8 52.3 48.6 49.5 49.3 48.4 49.8 50.6 40.3 41.0 43.2 46.4 46.5 48.4 Table 9: Zero-shot evaluation results for Arabic (%). Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Results for English We also performed an evaluation for English. The results are given in Table 10, where we can see that Jais-chat is highly competitive against existing English models, despite having seen less En- glish data in pretraining. First, we observe that the existing Arabic models perform almost randomly on this benchmark, while our models perform substantially better. This result is unsurprising given that AraT5, AraT5- V2, and AraBART were pretrained on Arabic data only. In comparison to the multilingual BLOOMz (1.1B), Jais-chat (1.3B) performs +3.4 points better. We can further see that Jais-chat (13B) performs on par with the recently released LLaMA2-chat (13B) model (57.3 vs. 57.7), even though the latter is trained on 2T of English word tokens, while our model has only seen 232B English word token. Jais-chat (13B) also outperforms other baselines including mT0-xxl (13B) and Falcon (7B), by margins ranging from +2.6 to +7.2 points absolute. Our instruction-tuning is also effective, with improvements of +3.9, +4.3, and +3.4, for the 1.3B, 6.7B, and 13B models, respectively. The full results for each dataset and model can be found in the Appendix (Table 13). 5.2 Generation Evaluation Dataset We next perform evaluation of the models over the core capability of Arabic text generation. Follow- ing prior work [PLH+23, CLL+23], we perform automatic evaluation over the generated Arabic content using GPT-4 [Ope23] based on Vicuna-Instructions-80, which were manually translated to Arabic by translators. 17 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 36.9 47.3 36.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 25.8 24.0 24.7 30.5 32.3 25.6 30.7 31.8 39.0 30.0 34.7 32.6 39.8 34.9 35 37.5 33.4 32.5 30.0 38.1 34.7 36.2 39.3 37.8 36.1 35.8 46.0 47.9 37.3 44.4 50.0 60.7 42.4 48.6 53.7 63.3 59.6 58.9 60.8 61.2 59.4 40.7 53.2 60.6 60.8 63.7 50.3 36.9 49.4 52.1 52.4 49.9 50.2 52.8 51.2 46.3 48.4 53.9 55.4 44.7 55.4 57.4 53.4 57.7 44.8 51.2 44.6 53.7 54.9 37.9 34.0 36.2 44.3 45.9 37.5 43.0 47.2 55.0 40.9 46.1 49.9 57.6 52.4 53.9 55.9 54.7 54.2 39.5 50.1 53.0 55.0 57.7 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 30.1 32.5 32.8 37.6 34.6 38.5 47.9 53.4 53.8 59.2 59.5 63.7 52.2 52.0. 54.0 53.3 53.5 53.9 45.4 49.3 50.0 54.3 53.9 57.3 Table 10: Zero-shot evaluation results for English. We can see that our model is competitive on English despite being Arabic-centric. Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Vicuna-Instructions-8041 consists of 80 challenging and open-ended questions across eight categories: knowl- edge, Fermi, counterfactual, roleplay, generic, math and coding, writing, and common-sense. Evaluation Setup We generate outputs for Arabic prompts in Vicuna-Instructions-80 using a temperature of 0.3 and a repetition penalty of 1.2. As baselines, we use two closed-source models, ChatGPT (175B) [OWJ+22] and Claude (52B).42 We further use several open-source models, which are either Arabic centric or multilin- gual: BLOOM (7B) [SFA+23], BLOOMz (7B) [MWS+23], AraT5 (220M) [NEAM22], AraT5-v2 (220M) [NEAM22], AraBART (550M) [KETH+22], and LLaMA2 (13B) [TMS+23]. We also include as baselines Bactrian-XLLaMA (13B) and Bactrian-XBLOOM (7B) [LKW+23], which are LLaMA and BLOOM base models, respectively, fine-tuned on multi-lingual (including Arabic) instruction-tuning datasets. For convenience, we name them BXLLaMA and BXBLOOM, respectively. We evaluate these baselines against our instruction-tuned models \u2013 Jais-chat (6.7B) and Jais-chat (13B). During the GPT-4 evaluation, we perform pairwise compar- isons between all pairs of models. We first prompt GPT-4 to score each pair of models based on their outputs generated for the prompts in the Arabic Vicuna-Instructions-80. We randomly permute the answers from both candidates, aiming to have any one as the first candidate"}, {"question": " How many categories are included in the Vicuna-Instructions-8041 evaluation dataset?", "answer": " 8 categories: knowledge, Fermi, counterfactual, roleplay, generic, math and coding, writing, and common-sense", "ref_chunk": "(13B) widens the gap even further, with an additional overall improvement of +1.9 points over Jais-chat (6.7B). Instruction-tuning [OWJ+22] further improves the results over the corresponding base models, with the exception of Falcon (7B). The absolute improvements due to instruction-tuning for Jais-chat (1.3B, 6.7B, 13B) are +0.7, +3.2, and +1.9, respectively, and are similar to those for BLOOMz. The full results for each dataset and model can be found in the Appendix (Table 12). 16 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 34.7 47.3 33.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 29.0 24.5 24.9 30.7 30.9 25.1 28.4 32.4 33.8 27.2 31.4 32.4 36.3 29.4 29.0 28.3 27.5 24.6 28.1 33.7 29.9 30.0 30.0 37.7 36.4 36.2 39.1 39.8 37.1 38.6 40.7 43.7 38.6 41.1 42.3 44.3 36.1 39.3 39.0 38.0 37.5 39.2 44.4 39.5 40.3 40.3 49.3 34.1 49.3 49.3 51.2 50.6 45.9 49.6 51.3 46.4 45.7 49.0 52.1 46.2 47.5 47.7 46.4 47.4 47.7 44.9 49.2 47.7 47.7 36.7 32.0 34.6 38.0 38.7 35.3 36.4 39.4 41.7 36.1 38.6 40.1 42.9 35.5 37.2 36.8 35.9 34.9 36.9 40.9 37.9 38.1 38.1 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 34.2 33.9 36.6 39.6 40.0 41.4 41.6 42.8 45.5 50.3 49.8 52.3 48.6 49.5 49.3 48.4 49.8 50.6 40.3 41.0 43.2 46.4 46.5 48.4 Table 9: Zero-shot evaluation results for Arabic (%). Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Results for English We also performed an evaluation for English. The results are given in Table 10, where we can see that Jais-chat is highly competitive against existing English models, despite having seen less En- glish data in pretraining. First, we observe that the existing Arabic models perform almost randomly on this benchmark, while our models perform substantially better. This result is unsurprising given that AraT5, AraT5- V2, and AraBART were pretrained on Arabic data only. In comparison to the multilingual BLOOMz (1.1B), Jais-chat (1.3B) performs +3.4 points better. We can further see that Jais-chat (13B) performs on par with the recently released LLaMA2-chat (13B) model (57.3 vs. 57.7), even though the latter is trained on 2T of English word tokens, while our model has only seen 232B English word token. Jais-chat (13B) also outperforms other baselines including mT0-xxl (13B) and Falcon (7B), by margins ranging from +2.6 to +7.2 points absolute. Our instruction-tuning is also effective, with improvements of +3.9, +4.3, and +3.4, for the 1.3B, 6.7B, and 13B models, respectively. The full results for each dataset and model can be found in the Appendix (Table 13). 5.2 Generation Evaluation Dataset We next perform evaluation of the models over the core capability of Arabic text generation. Follow- ing prior work [PLH+23, CLL+23], we perform automatic evaluation over the generated Arabic content using GPT-4 [Ope23] based on Vicuna-Instructions-80, which were manually translated to Arabic by translators. 17 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 36.9 47.3 36.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 25.8 24.0 24.7 30.5 32.3 25.6 30.7 31.8 39.0 30.0 34.7 32.6 39.8 34.9 35 37.5 33.4 32.5 30.0 38.1 34.7 36.2 39.3 37.8 36.1 35.8 46.0 47.9 37.3 44.4 50.0 60.7 42.4 48.6 53.7 63.3 59.6 58.9 60.8 61.2 59.4 40.7 53.2 60.6 60.8 63.7 50.3 36.9 49.4 52.1 52.4 49.9 50.2 52.8 51.2 46.3 48.4 53.9 55.4 44.7 55.4 57.4 53.4 57.7 44.8 51.2 44.6 53.7 54.9 37.9 34.0 36.2 44.3 45.9 37.5 43.0 47.2 55.0 40.9 46.1 49.9 57.6 52.4 53.9 55.9 54.7 54.2 39.5 50.1 53.0 55.0 57.7 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 30.1 32.5 32.8 37.6 34.6 38.5 47.9 53.4 53.8 59.2 59.5 63.7 52.2 52.0. 54.0 53.3 53.5 53.9 45.4 49.3 50.0 54.3 53.9 57.3 Table 10: Zero-shot evaluation results for English. We can see that our model is competitive on English despite being Arabic-centric. Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Vicuna-Instructions-8041 consists of 80 challenging and open-ended questions across eight categories: knowl- edge, Fermi, counterfactual, roleplay, generic, math and coding, writing, and common-sense. Evaluation Setup We generate outputs for Arabic prompts in Vicuna-Instructions-80 using a temperature of 0.3 and a repetition penalty of 1.2. As baselines, we use two closed-source models, ChatGPT (175B) [OWJ+22] and Claude (52B).42 We further use several open-source models, which are either Arabic centric or multilin- gual: BLOOM (7B) [SFA+23], BLOOMz (7B) [MWS+23], AraT5 (220M) [NEAM22], AraT5-v2 (220M) [NEAM22], AraBART (550M) [KETH+22], and LLaMA2 (13B) [TMS+23]. We also include as baselines Bactrian-XLLaMA (13B) and Bactrian-XBLOOM (7B) [LKW+23], which are LLaMA and BLOOM base models, respectively, fine-tuned on multi-lingual (including Arabic) instruction-tuning datasets. For convenience, we name them BXLLaMA and BXBLOOM, respectively. We evaluate these baselines against our instruction-tuned models \u2013 Jais-chat (6.7B) and Jais-chat (13B). During the GPT-4 evaluation, we perform pairwise compar- isons between all pairs of models. We first prompt GPT-4 to score each pair of models based on their outputs generated for the prompts in the Arabic Vicuna-Instructions-80. We randomly permute the answers from both candidates, aiming to have any one as the first candidate"}, {"question": " What temperature and repetition penalty values are used for generating Arabic outputs in the evaluation setup?", "answer": " Temperature of 0.3 and a repetition penalty of 1.2", "ref_chunk": "(13B) widens the gap even further, with an additional overall improvement of +1.9 points over Jais-chat (6.7B). Instruction-tuning [OWJ+22] further improves the results over the corresponding base models, with the exception of Falcon (7B). The absolute improvements due to instruction-tuning for Jais-chat (1.3B, 6.7B, 13B) are +0.7, +3.2, and +1.9, respectively, and are similar to those for BLOOMz. The full results for each dataset and model can be found in the Appendix (Table 12). 16 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 34.7 47.3 33.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 29.0 24.5 24.9 30.7 30.9 25.1 28.4 32.4 33.8 27.2 31.4 32.4 36.3 29.4 29.0 28.3 27.5 24.6 28.1 33.7 29.9 30.0 30.0 37.7 36.4 36.2 39.1 39.8 37.1 38.6 40.7 43.7 38.6 41.1 42.3 44.3 36.1 39.3 39.0 38.0 37.5 39.2 44.4 39.5 40.3 40.3 49.3 34.1 49.3 49.3 51.2 50.6 45.9 49.6 51.3 46.4 45.7 49.0 52.1 46.2 47.5 47.7 46.4 47.4 47.7 44.9 49.2 47.7 47.7 36.7 32.0 34.6 38.0 38.7 35.3 36.4 39.4 41.7 36.1 38.6 40.1 42.9 35.5 37.2 36.8 35.9 34.9 36.9 40.9 37.9 38.1 38.1 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 34.2 33.9 36.6 39.6 40.0 41.4 41.6 42.8 45.5 50.3 49.8 52.3 48.6 49.5 49.3 48.4 49.8 50.6 40.3 41.0 43.2 46.4 46.5 48.4 Table 9: Zero-shot evaluation results for Arabic (%). Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Results for English We also performed an evaluation for English. The results are given in Table 10, where we can see that Jais-chat is highly competitive against existing English models, despite having seen less En- glish data in pretraining. First, we observe that the existing Arabic models perform almost randomly on this benchmark, while our models perform substantially better. This result is unsurprising given that AraT5, AraT5- V2, and AraBART were pretrained on Arabic data only. In comparison to the multilingual BLOOMz (1.1B), Jais-chat (1.3B) performs +3.4 points better. We can further see that Jais-chat (13B) performs on par with the recently released LLaMA2-chat (13B) model (57.3 vs. 57.7), even though the latter is trained on 2T of English word tokens, while our model has only seen 232B English word token. Jais-chat (13B) also outperforms other baselines including mT0-xxl (13B) and Falcon (7B), by margins ranging from +2.6 to +7.2 points absolute. Our instruction-tuning is also effective, with improvements of +3.9, +4.3, and +3.4, for the 1.3B, 6.7B, and 13B models, respectively. The full results for each dataset and model can be found in the Appendix (Table 13). 5.2 Generation Evaluation Dataset We next perform evaluation of the models over the core capability of Arabic text generation. Follow- ing prior work [PLH+23, CLL+23], we perform automatic evaluation over the generated Arabic content using GPT-4 [Ope23] based on Vicuna-Instructions-80, which were manually translated to Arabic by translators. 17 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 36.9 47.3 36.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 25.8 24.0 24.7 30.5 32.3 25.6 30.7 31.8 39.0 30.0 34.7 32.6 39.8 34.9 35 37.5 33.4 32.5 30.0 38.1 34.7 36.2 39.3 37.8 36.1 35.8 46.0 47.9 37.3 44.4 50.0 60.7 42.4 48.6 53.7 63.3 59.6 58.9 60.8 61.2 59.4 40.7 53.2 60.6 60.8 63.7 50.3 36.9 49.4 52.1 52.4 49.9 50.2 52.8 51.2 46.3 48.4 53.9 55.4 44.7 55.4 57.4 53.4 57.7 44.8 51.2 44.6 53.7 54.9 37.9 34.0 36.2 44.3 45.9 37.5 43.0 47.2 55.0 40.9 46.1 49.9 57.6 52.4 53.9 55.9 54.7 54.2 39.5 50.1 53.0 55.0 57.7 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 30.1 32.5 32.8 37.6 34.6 38.5 47.9 53.4 53.8 59.2 59.5 63.7 52.2 52.0. 54.0 53.3 53.5 53.9 45.4 49.3 50.0 54.3 53.9 57.3 Table 10: Zero-shot evaluation results for English. We can see that our model is competitive on English despite being Arabic-centric. Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Vicuna-Instructions-8041 consists of 80 challenging and open-ended questions across eight categories: knowl- edge, Fermi, counterfactual, roleplay, generic, math and coding, writing, and common-sense. Evaluation Setup We generate outputs for Arabic prompts in Vicuna-Instructions-80 using a temperature of 0.3 and a repetition penalty of 1.2. As baselines, we use two closed-source models, ChatGPT (175B) [OWJ+22] and Claude (52B).42 We further use several open-source models, which are either Arabic centric or multilin- gual: BLOOM (7B) [SFA+23], BLOOMz (7B) [MWS+23], AraT5 (220M) [NEAM22], AraT5-v2 (220M) [NEAM22], AraBART (550M) [KETH+22], and LLaMA2 (13B) [TMS+23]. We also include as baselines Bactrian-XLLaMA (13B) and Bactrian-XBLOOM (7B) [LKW+23], which are LLaMA and BLOOM base models, respectively, fine-tuned on multi-lingual (including Arabic) instruction-tuning datasets. For convenience, we name them BXLLaMA and BXBLOOM, respectively. We evaluate these baselines against our instruction-tuned models \u2013 Jais-chat (6.7B) and Jais-chat (13B). During the GPT-4 evaluation, we perform pairwise compar- isons between all pairs of models. We first prompt GPT-4 to score each pair of models based on their outputs generated for the prompts in the Arabic Vicuna-Instructions-80. We randomly permute the answers from both candidates, aiming to have any one as the first candidate"}, {"question": " Which models are used as baselines in the GPT-4 evaluation for Arabic content?", "answer": " ChatGPT (175B) and Claude (52B) as closed-source models, and BLOOM (7B), BLOOMz (7B), AraT5 (220M), AraT5-v2 (220M), AraBART (550M), and LLaMA2 (13B) as open-source models", "ref_chunk": "(13B) widens the gap even further, with an additional overall improvement of +1.9 points over Jais-chat (6.7B). Instruction-tuning [OWJ+22] further improves the results over the corresponding base models, with the exception of Falcon (7B). The absolute improvements due to instruction-tuning for Jais-chat (1.3B, 6.7B, 13B) are +0.7, +3.2, and +1.9, respectively, and are similar to those for BLOOMz. The full results for each dataset and model can be found in the Appendix (Table 12). 16 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 34.7 47.3 33.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 29.0 24.5 24.9 30.7 30.9 25.1 28.4 32.4 33.8 27.2 31.4 32.4 36.3 29.4 29.0 28.3 27.5 24.6 28.1 33.7 29.9 30.0 30.0 37.7 36.4 36.2 39.1 39.8 37.1 38.6 40.7 43.7 38.6 41.1 42.3 44.3 36.1 39.3 39.0 38.0 37.5 39.2 44.4 39.5 40.3 40.3 49.3 34.1 49.3 49.3 51.2 50.6 45.9 49.6 51.3 46.4 45.7 49.0 52.1 46.2 47.5 47.7 46.4 47.4 47.7 44.9 49.2 47.7 47.7 36.7 32.0 34.6 38.0 38.7 35.3 36.4 39.4 41.7 36.1 38.6 40.1 42.9 35.5 37.2 36.8 35.9 34.9 36.9 40.9 37.9 38.1 38.1 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 34.2 33.9 36.6 39.6 40.0 41.4 41.6 42.8 45.5 50.3 49.8 52.3 48.6 49.5 49.3 48.4 49.8 50.6 40.3 41.0 43.2 46.4 46.5 48.4 Table 9: Zero-shot evaluation results for Arabic (%). Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Results for English We also performed an evaluation for English. The results are given in Table 10, where we can see that Jais-chat is highly competitive against existing English models, despite having seen less En- glish data in pretraining. First, we observe that the existing Arabic models perform almost randomly on this benchmark, while our models perform substantially better. This result is unsurprising given that AraT5, AraT5- V2, and AraBART were pretrained on Arabic data only. In comparison to the multilingual BLOOMz (1.1B), Jais-chat (1.3B) performs +3.4 points better. We can further see that Jais-chat (13B) performs on par with the recently released LLaMA2-chat (13B) model (57.3 vs. 57.7), even though the latter is trained on 2T of English word tokens, while our model has only seen 232B English word token. Jais-chat (13B) also outperforms other baselines including mT0-xxl (13B) and Falcon (7B), by margins ranging from +2.6 to +7.2 points absolute. Our instruction-tuning is also effective, with improvements of +3.9, +4.3, and +3.4, for the 1.3B, 6.7B, and 13B models, respectively. The full results for each dataset and model can be found in the Appendix (Table 13). 5.2 Generation Evaluation Dataset We next perform evaluation of the models over the core capability of Arabic text generation. Follow- ing prior work [PLH+23, CLL+23], we perform automatic evaluation over the generated Arabic content using GPT-4 [Ope23] based on Vicuna-Instructions-80, which were manually translated to Arabic by translators. 17 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 36.9 47.3 36.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 25.8 24.0 24.7 30.5 32.3 25.6 30.7 31.8 39.0 30.0 34.7 32.6 39.8 34.9 35 37.5 33.4 32.5 30.0 38.1 34.7 36.2 39.3 37.8 36.1 35.8 46.0 47.9 37.3 44.4 50.0 60.7 42.4 48.6 53.7 63.3 59.6 58.9 60.8 61.2 59.4 40.7 53.2 60.6 60.8 63.7 50.3 36.9 49.4 52.1 52.4 49.9 50.2 52.8 51.2 46.3 48.4 53.9 55.4 44.7 55.4 57.4 53.4 57.7 44.8 51.2 44.6 53.7 54.9 37.9 34.0 36.2 44.3 45.9 37.5 43.0 47.2 55.0 40.9 46.1 49.9 57.6 52.4 53.9 55.9 54.7 54.2 39.5 50.1 53.0 55.0 57.7 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 30.1 32.5 32.8 37.6 34.6 38.5 47.9 53.4 53.8 59.2 59.5 63.7 52.2 52.0. 54.0 53.3 53.5 53.9 45.4 49.3 50.0 54.3 53.9 57.3 Table 10: Zero-shot evaluation results for English. We can see that our model is competitive on English despite being Arabic-centric. Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Vicuna-Instructions-8041 consists of 80 challenging and open-ended questions across eight categories: knowl- edge, Fermi, counterfactual, roleplay, generic, math and coding, writing, and common-sense. Evaluation Setup We generate outputs for Arabic prompts in Vicuna-Instructions-80 using a temperature of 0.3 and a repetition penalty of 1.2. As baselines, we use two closed-source models, ChatGPT (175B) [OWJ+22] and Claude (52B).42 We further use several open-source models, which are either Arabic centric or multilin- gual: BLOOM (7B) [SFA+23], BLOOMz (7B) [MWS+23], AraT5 (220M) [NEAM22], AraT5-v2 (220M) [NEAM22], AraBART (550M) [KETH+22], and LLaMA2 (13B) [TMS+23]. We also include as baselines Bactrian-XLLaMA (13B) and Bactrian-XBLOOM (7B) [LKW+23], which are LLaMA and BLOOM base models, respectively, fine-tuned on multi-lingual (including Arabic) instruction-tuning datasets. For convenience, we name them BXLLaMA and BXBLOOM, respectively. We evaluate these baselines against our instruction-tuned models \u2013 Jais-chat (6.7B) and Jais-chat (13B). During the GPT-4 evaluation, we perform pairwise compar- isons between all pairs of models. We first prompt GPT-4 to score each pair of models based on their outputs generated for the prompts in the Arabic Vicuna-Instructions-80. We randomly permute the answers from both candidates, aiming to have any one as the first candidate"}], "doc_text": "(13B) widens the gap even further, with an additional overall improvement of +1.9 points over Jais-chat (6.7B). Instruction-tuning [OWJ+22] further improves the results over the corresponding base models, with the exception of Falcon (7B). The absolute improvements due to instruction-tuning for Jais-chat (1.3B, 6.7B, 13B) are +0.7, +3.2, and +1.9, respectively, and are similar to those for BLOOMz. The full results for each dataset and model can be found in the Appendix (Table 12). 16 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 34.7 47.3 33.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 29.0 24.5 24.9 30.7 30.9 25.1 28.4 32.4 33.8 27.2 31.4 32.4 36.3 29.4 29.0 28.3 27.5 24.6 28.1 33.7 29.9 30.0 30.0 37.7 36.4 36.2 39.1 39.8 37.1 38.6 40.7 43.7 38.6 41.1 42.3 44.3 36.1 39.3 39.0 38.0 37.5 39.2 44.4 39.5 40.3 40.3 49.3 34.1 49.3 49.3 51.2 50.6 45.9 49.6 51.3 46.4 45.7 49.0 52.1 46.2 47.5 47.7 46.4 47.4 47.7 44.9 49.2 47.7 47.7 36.7 32.0 34.6 38.0 38.7 35.3 36.4 39.4 41.7 36.1 38.6 40.1 42.9 35.5 37.2 36.8 35.9 34.9 36.9 40.9 37.9 38.1 38.1 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 34.2 33.9 36.6 39.6 40.0 41.4 41.6 42.8 45.5 50.3 49.8 52.3 48.6 49.5 49.3 48.4 49.8 50.6 40.3 41.0 43.2 46.4 46.5 48.4 Table 9: Zero-shot evaluation results for Arabic (%). Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Results for English We also performed an evaluation for English. The results are given in Table 10, where we can see that Jais-chat is highly competitive against existing English models, despite having seen less En- glish data in pretraining. First, we observe that the existing Arabic models perform almost randomly on this benchmark, while our models perform substantially better. This result is unsurprising given that AraT5, AraT5- V2, and AraBART were pretrained on Arabic data only. In comparison to the multilingual BLOOMz (1.1B), Jais-chat (1.3B) performs +3.4 points better. We can further see that Jais-chat (13B) performs on par with the recently released LLaMA2-chat (13B) model (57.3 vs. 57.7), even though the latter is trained on 2T of English word tokens, while our model has only seen 232B English word token. Jais-chat (13B) also outperforms other baselines including mT0-xxl (13B) and Falcon (7B), by margins ranging from +2.6 to +7.2 points absolute. Our instruction-tuning is also effective, with improvements of +3.9, +4.3, and +3.4, for the 1.3B, 6.7B, and 13B models, respectively. The full results for each dataset and model can be found in the Appendix (Table 13). 5.2 Generation Evaluation Dataset We next perform evaluation of the models over the core capability of Arabic text generation. Follow- ing prior work [PLH+23, CLL+23], we perform automatic evaluation over the generated Arabic content using GPT-4 [Ope23] based on Vicuna-Instructions-80, which were manually translated to Arabic by translators. 17 Model (size) Tuned? Knowledge Commonsense Misinformation/Bias Average Random \u2013 25.0 36.9 47.3 36.6 AraBART (139M) AraT5 (220M) AraT5-v2 (220M) BLOOM (1.1B) BLOOMz (1.1B) mT5-large (1.2B) mT0-large (1.2B) BLOOM (3B) BLOOMz (3B) mT5-xl (3.7B) mT0-xl (3.7B) BLOOM (7.1B) BLOOMz (7.1B) LLaMA (7B) LLaMA2 (7B) LLaMA2-chat (7B) Falcon (7B) Falcon-Instruct (7B) mT5-xxl (13B) mT0-xxl (13B) LLaMA (13B) LLaMA2 (13B) LLaMA2-chat (13B) \u2013 \u2013 \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned \u2013 tuned \u2013 tuned \u2013 \u2013 tuned 25.8 24.0 24.7 30.5 32.3 25.6 30.7 31.8 39.0 30.0 34.7 32.6 39.8 34.9 35 37.5 33.4 32.5 30.0 38.1 34.7 36.2 39.3 37.8 36.1 35.8 46.0 47.9 37.3 44.4 50.0 60.7 42.4 48.6 53.7 63.3 59.6 58.9 60.8 61.2 59.4 40.7 53.2 60.6 60.8 63.7 50.3 36.9 49.4 52.1 52.4 49.9 50.2 52.8 51.2 46.3 48.4 53.9 55.4 44.7 55.4 57.4 53.4 57.7 44.8 51.2 44.6 53.7 54.9 37.9 34.0 36.2 44.3 45.9 37.5 43.0 47.2 55.0 40.9 46.1 49.9 57.6 52.4 53.9 55.9 54.7 54.2 39.5 50.1 53.0 55.0 57.7 Jais (1.3B) Jais-chat (1.3B) Jais (6.7B) Jais-chat (6.7B) Jais (13B) Jais-chat (13B) \u2013 tuned \u2013 tuned \u2013 tuned 30.1 32.5 32.8 37.6 34.6 38.5 47.9 53.4 53.8 59.2 59.5 63.7 52.2 52.0. 54.0 53.3 53.5 53.9 45.4 49.3 50.0 54.3 53.9 57.3 Table 10: Zero-shot evaluation results for English. We can see that our model is competitive on English despite being Arabic-centric. Average is the mean score computed across the entire dataset, and tuned indicates that the model is instruction-tuned. Vicuna-Instructions-8041 consists of 80 challenging and open-ended questions across eight categories: knowl- edge, Fermi, counterfactual, roleplay, generic, math and coding, writing, and common-sense. Evaluation Setup We generate outputs for Arabic prompts in Vicuna-Instructions-80 using a temperature of 0.3 and a repetition penalty of 1.2. As baselines, we use two closed-source models, ChatGPT (175B) [OWJ+22] and Claude (52B).42 We further use several open-source models, which are either Arabic centric or multilin- gual: BLOOM (7B) [SFA+23], BLOOMz (7B) [MWS+23], AraT5 (220M) [NEAM22], AraT5-v2 (220M) [NEAM22], AraBART (550M) [KETH+22], and LLaMA2 (13B) [TMS+23]. We also include as baselines Bactrian-XLLaMA (13B) and Bactrian-XBLOOM (7B) [LKW+23], which are LLaMA and BLOOM base models, respectively, fine-tuned on multi-lingual (including Arabic) instruction-tuning datasets. For convenience, we name them BXLLaMA and BXBLOOM, respectively. We evaluate these baselines against our instruction-tuned models \u2013 Jais-chat (6.7B) and Jais-chat (13B). During the GPT-4 evaluation, we perform pairwise compar- isons between all pairs of models. We first prompt GPT-4 to score each pair of models based on their outputs generated for the prompts in the Arabic Vicuna-Instructions-80. We randomly permute the answers from both candidates, aiming to have any one as the first candidate"}