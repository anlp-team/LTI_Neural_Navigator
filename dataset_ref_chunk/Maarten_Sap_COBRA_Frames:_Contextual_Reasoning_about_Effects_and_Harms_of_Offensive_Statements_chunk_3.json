{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_COBRA_Frames:_Contextual_Reasoning_about_Effects_and_Harms_of_Offensive_Statements_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does Impact (Imp.) seek to explain?", "answer": " Impact explains the biased, prejudiced, or stereotypical meaning implied by a statement.", "ref_chunk": "interpretations of statements (e.g., gay men tend to hold more privilege along the gender privilege spectrum, but less along the sexuality one). Impact (Imp.) explain the biased, prejudiced, or stereotypical meaning implied by the statement, similar to Sap et al. (2020). This implication is very closely related to the received meaning from the listener\u2019s or targeted group\u2019s perspective and may differ from the speaker\u2019s intended meaning (e.g., for microaggressions; Sue, 2010). Emotional and Cognitive Reactions (Emo. & Cog.) capture the possible negative effects and harms that the statement and its implied meaning could have on the targeted group. There is an in- creasing push to develop content moderation from the perspective of the harms that content engen- ders (Keller and Leerssen, 2020; Vaccaro et al., 2020). As such, we draw from Nadal et al. (2014) and consider the perceived emotional and cognitive reactions of the target group or listener. The emo- tional reactions capture the short-term emotional effects or reactions to a statement (e.g., \u201canger and annoyance\u201d, \u201cworthlessness\u201d) On the other hand, the cognitive reactions focus on the lessons some- one could draw, the subsequent actions someone could take, or on the long-term harms that repeated exposure to such statements could have. Examples include \u201cnot wanting to come into work anymore,\u201d \u201cavoiding a particular teacher,\u201d etc. Unique # Avg. # words Statements 11,648 14.34 t x e t n o C Situation Speakers Listeners 23,577 10,683 13,554 6.90 3.11 4.05 s n o i t a n a l p x E Intents Target group Power dynamics Implication Emo. Reaction Cog. Reaction Offensiveness 29,895 11,126 12,766 30,802 28,429 29,826 2,527 14.97 3.48 10.46 19.66 16.82 22.06 2.09 Total # in COBRACORPUS 32,582 Table 2: General data statistics of COBRACORPUS Offensiveness (Off.) captures, in 1-3 words, the type or degree of offensiveness of the statement (e.g., \u201csexism\u201d, \u201coffensive generalization\u201d). We avoid imposing a categorization or cutoff between offensive and harmless statements and instead leave this dimension as free-text, to preserve nuanced in- terpretations of statements and capture the full spec- trum of offensiveness types (Jurgens et al., 2019). 3 Collecting COBRACORPUS To study the contextual dynamics of the offensive- ness of statements at scale, we create COBRACOR- PUS using a three-stage data generation pipeline with human verification, shown in Figure 2. Given that no available corpus contains statements with their contexts and explanations,4 we prompt a large language model (GPT-3.5; Ouyang et al., 2022) to generate contexts and explanations, following (Hartvigsen et al., 2022; West et al., 2022; Kim et al., 2022b,a). Specifically, we first generate mul- tiple plausible contexts for statements, then gen- erate the explanations for each context separately, using GPT-3.5 with in-context examples. Please refer to Appendix C for examples of our prompts. To ensure data quality, we design a set of crowd- sourcing tasks to verify the generated contexts and explanations and collect suggestions. For all tasks, we pre-select crowd workers based on a qualifica- tion task that judged their understanding of each dimension. Please refer to Appendix A for the details of all crowd-sourcing experiments. 3.1 Collecting Statements We draw our statements from Toxigen (Hartvigsen et al., 2022), a dataset of GPT3-generated state- ments that are subtly or implicitly toxic, offensive, 4Note, we do not infer the demographic categories of state- ment authors or readers for ethical reasons (Tatman, 2020). prejudiced, or biased against various demographic groups. Specifically, since we focus on the dynam- ics of offensiveness, we analyze a sample of 13,000 Toxigen statements tagged as \u201coffensive\u201d. 3.2 Generating Likely Contexts Following work demonstrating that LLMs can gen- erate realistic social situations related to majority and minority groups (Park et al., 2022), we use GPT-3.5 to construct plausible or likely contexts (i.e., situation, speaker identity, listener identity) in which a statement could be made. Specifically, we manually curate fifty statement-context pairs, out of which we sample five for each statement as in-context examples. Conditioned on the in-context examples, we then sample three contexts from GPT- 3.5 for each statement. The examples of prompts for plausible context generation are presented in Appendix C. Verifying Contexts We randomly sample 500 statement-context pairs and ask three annotators to rate the plausibility of the contexts (see Appendix A.2 for the exact questions).5 Of the 500 pairs, only 1% were marked as completely implausible or gibberish. 92% of the scenarios were marked as plausible by at least two workers, and some were marked as unlikely but technically plausible (e.g., A mayor in the public saying \u201cBlack people are not humans.\u201d) We retain these contexts since such rare situations could still happen, making them helpful for our analyses and modeling experiments. 3.3 Generating COBRA Explanations Similar to context generation, we make use of GPT-3.5\u2019s ability to produce rich explanations of social commonsense (West et al., 2022) to gener- ate explanations along our seven dimensions. For each context-statement pair, we generate one full COBRA frame, using three randomly sampled in- context examples from our pool of six manually curated prompts. As shown in Table 2, this pro- cess yields a COBRACORPUS containing 32k full (context-statement-explanation) COBRA frames. Verifying Explanations To ensure data quality, we randomly sampled 567 (statement, context, ex- planations) triples and asked three annotators to rate how likely the explanations fit the statements in context. Inspired by prior work (Aguinis et al., 5On this context verification task, the agreement was moderately high, with 75.37% pairwise agreement and free- marginal multi-rater \u03ba=0.507 (Randolph, 2005). Friends Strangers Workplace Family Other more off. 5.28% 43.09% 27.54% 2.85% 21.24% 5.79% 11.38% 6.17% less off. 60.06% 16.6% Table 3: Percentage of contexts occurring under each category/scenario in COBRACORPUS-CF. Row 1 in- dicates statements that are more offensive due to their contexts vs Row 2 indicates those which are lesser of- fensive in comparison 2021; Clark et al., 2021; Liu et al., 2022), we also asked annotators to provide corrections or sugges- tions for those they consider unlikely. 97% of ex- planations were marked as likely by at least two annotators (majority vote) and 84% were"}, {"question": " What is the purpose of Emotional and Cognitive Reactions (Emo. & Cog.)?", "answer": " Emotional and Cognitive Reactions capture the possible negative effects and harms that a statement and its implied meaning could have on the targeted group.", "ref_chunk": "interpretations of statements (e.g., gay men tend to hold more privilege along the gender privilege spectrum, but less along the sexuality one). Impact (Imp.) explain the biased, prejudiced, or stereotypical meaning implied by the statement, similar to Sap et al. (2020). This implication is very closely related to the received meaning from the listener\u2019s or targeted group\u2019s perspective and may differ from the speaker\u2019s intended meaning (e.g., for microaggressions; Sue, 2010). Emotional and Cognitive Reactions (Emo. & Cog.) capture the possible negative effects and harms that the statement and its implied meaning could have on the targeted group. There is an in- creasing push to develop content moderation from the perspective of the harms that content engen- ders (Keller and Leerssen, 2020; Vaccaro et al., 2020). As such, we draw from Nadal et al. (2014) and consider the perceived emotional and cognitive reactions of the target group or listener. The emo- tional reactions capture the short-term emotional effects or reactions to a statement (e.g., \u201canger and annoyance\u201d, \u201cworthlessness\u201d) On the other hand, the cognitive reactions focus on the lessons some- one could draw, the subsequent actions someone could take, or on the long-term harms that repeated exposure to such statements could have. Examples include \u201cnot wanting to come into work anymore,\u201d \u201cavoiding a particular teacher,\u201d etc. Unique # Avg. # words Statements 11,648 14.34 t x e t n o C Situation Speakers Listeners 23,577 10,683 13,554 6.90 3.11 4.05 s n o i t a n a l p x E Intents Target group Power dynamics Implication Emo. Reaction Cog. Reaction Offensiveness 29,895 11,126 12,766 30,802 28,429 29,826 2,527 14.97 3.48 10.46 19.66 16.82 22.06 2.09 Total # in COBRACORPUS 32,582 Table 2: General data statistics of COBRACORPUS Offensiveness (Off.) captures, in 1-3 words, the type or degree of offensiveness of the statement (e.g., \u201csexism\u201d, \u201coffensive generalization\u201d). We avoid imposing a categorization or cutoff between offensive and harmless statements and instead leave this dimension as free-text, to preserve nuanced in- terpretations of statements and capture the full spec- trum of offensiveness types (Jurgens et al., 2019). 3 Collecting COBRACORPUS To study the contextual dynamics of the offensive- ness of statements at scale, we create COBRACOR- PUS using a three-stage data generation pipeline with human verification, shown in Figure 2. Given that no available corpus contains statements with their contexts and explanations,4 we prompt a large language model (GPT-3.5; Ouyang et al., 2022) to generate contexts and explanations, following (Hartvigsen et al., 2022; West et al., 2022; Kim et al., 2022b,a). Specifically, we first generate mul- tiple plausible contexts for statements, then gen- erate the explanations for each context separately, using GPT-3.5 with in-context examples. Please refer to Appendix C for examples of our prompts. To ensure data quality, we design a set of crowd- sourcing tasks to verify the generated contexts and explanations and collect suggestions. For all tasks, we pre-select crowd workers based on a qualifica- tion task that judged their understanding of each dimension. Please refer to Appendix A for the details of all crowd-sourcing experiments. 3.1 Collecting Statements We draw our statements from Toxigen (Hartvigsen et al., 2022), a dataset of GPT3-generated state- ments that are subtly or implicitly toxic, offensive, 4Note, we do not infer the demographic categories of state- ment authors or readers for ethical reasons (Tatman, 2020). prejudiced, or biased against various demographic groups. Specifically, since we focus on the dynam- ics of offensiveness, we analyze a sample of 13,000 Toxigen statements tagged as \u201coffensive\u201d. 3.2 Generating Likely Contexts Following work demonstrating that LLMs can gen- erate realistic social situations related to majority and minority groups (Park et al., 2022), we use GPT-3.5 to construct plausible or likely contexts (i.e., situation, speaker identity, listener identity) in which a statement could be made. Specifically, we manually curate fifty statement-context pairs, out of which we sample five for each statement as in-context examples. Conditioned on the in-context examples, we then sample three contexts from GPT- 3.5 for each statement. The examples of prompts for plausible context generation are presented in Appendix C. Verifying Contexts We randomly sample 500 statement-context pairs and ask three annotators to rate the plausibility of the contexts (see Appendix A.2 for the exact questions).5 Of the 500 pairs, only 1% were marked as completely implausible or gibberish. 92% of the scenarios were marked as plausible by at least two workers, and some were marked as unlikely but technically plausible (e.g., A mayor in the public saying \u201cBlack people are not humans.\u201d) We retain these contexts since such rare situations could still happen, making them helpful for our analyses and modeling experiments. 3.3 Generating COBRA Explanations Similar to context generation, we make use of GPT-3.5\u2019s ability to produce rich explanations of social commonsense (West et al., 2022) to gener- ate explanations along our seven dimensions. For each context-statement pair, we generate one full COBRA frame, using three randomly sampled in- context examples from our pool of six manually curated prompts. As shown in Table 2, this pro- cess yields a COBRACORPUS containing 32k full (context-statement-explanation) COBRA frames. Verifying Explanations To ensure data quality, we randomly sampled 567 (statement, context, ex- planations) triples and asked three annotators to rate how likely the explanations fit the statements in context. Inspired by prior work (Aguinis et al., 5On this context verification task, the agreement was moderately high, with 75.37% pairwise agreement and free- marginal multi-rater \u03ba=0.507 (Randolph, 2005). Friends Strangers Workplace Family Other more off. 5.28% 43.09% 27.54% 2.85% 21.24% 5.79% 11.38% 6.17% less off. 60.06% 16.6% Table 3: Percentage of contexts occurring under each category/scenario in COBRACORPUS-CF. Row 1 in- dicates statements that are more offensive due to their contexts vs Row 2 indicates those which are lesser of- fensive in comparison 2021; Clark et al., 2021; Liu et al., 2022), we also asked annotators to provide corrections or sugges- tions for those they consider unlikely. 97% of ex- planations were marked as likely by at least two annotators (majority vote) and 84% were"}, {"question": " According to the text, what is the increasing push moving towards in content moderation?", "answer": " There is an increasing push to develop content moderation from the perspective of the harms that content engenders.", "ref_chunk": "interpretations of statements (e.g., gay men tend to hold more privilege along the gender privilege spectrum, but less along the sexuality one). Impact (Imp.) explain the biased, prejudiced, or stereotypical meaning implied by the statement, similar to Sap et al. (2020). This implication is very closely related to the received meaning from the listener\u2019s or targeted group\u2019s perspective and may differ from the speaker\u2019s intended meaning (e.g., for microaggressions; Sue, 2010). Emotional and Cognitive Reactions (Emo. & Cog.) capture the possible negative effects and harms that the statement and its implied meaning could have on the targeted group. There is an in- creasing push to develop content moderation from the perspective of the harms that content engen- ders (Keller and Leerssen, 2020; Vaccaro et al., 2020). As such, we draw from Nadal et al. (2014) and consider the perceived emotional and cognitive reactions of the target group or listener. The emo- tional reactions capture the short-term emotional effects or reactions to a statement (e.g., \u201canger and annoyance\u201d, \u201cworthlessness\u201d) On the other hand, the cognitive reactions focus on the lessons some- one could draw, the subsequent actions someone could take, or on the long-term harms that repeated exposure to such statements could have. Examples include \u201cnot wanting to come into work anymore,\u201d \u201cavoiding a particular teacher,\u201d etc. Unique # Avg. # words Statements 11,648 14.34 t x e t n o C Situation Speakers Listeners 23,577 10,683 13,554 6.90 3.11 4.05 s n o i t a n a l p x E Intents Target group Power dynamics Implication Emo. Reaction Cog. Reaction Offensiveness 29,895 11,126 12,766 30,802 28,429 29,826 2,527 14.97 3.48 10.46 19.66 16.82 22.06 2.09 Total # in COBRACORPUS 32,582 Table 2: General data statistics of COBRACORPUS Offensiveness (Off.) captures, in 1-3 words, the type or degree of offensiveness of the statement (e.g., \u201csexism\u201d, \u201coffensive generalization\u201d). We avoid imposing a categorization or cutoff between offensive and harmless statements and instead leave this dimension as free-text, to preserve nuanced in- terpretations of statements and capture the full spec- trum of offensiveness types (Jurgens et al., 2019). 3 Collecting COBRACORPUS To study the contextual dynamics of the offensive- ness of statements at scale, we create COBRACOR- PUS using a three-stage data generation pipeline with human verification, shown in Figure 2. Given that no available corpus contains statements with their contexts and explanations,4 we prompt a large language model (GPT-3.5; Ouyang et al., 2022) to generate contexts and explanations, following (Hartvigsen et al., 2022; West et al., 2022; Kim et al., 2022b,a). Specifically, we first generate mul- tiple plausible contexts for statements, then gen- erate the explanations for each context separately, using GPT-3.5 with in-context examples. Please refer to Appendix C for examples of our prompts. To ensure data quality, we design a set of crowd- sourcing tasks to verify the generated contexts and explanations and collect suggestions. For all tasks, we pre-select crowd workers based on a qualifica- tion task that judged their understanding of each dimension. Please refer to Appendix A for the details of all crowd-sourcing experiments. 3.1 Collecting Statements We draw our statements from Toxigen (Hartvigsen et al., 2022), a dataset of GPT3-generated state- ments that are subtly or implicitly toxic, offensive, 4Note, we do not infer the demographic categories of state- ment authors or readers for ethical reasons (Tatman, 2020). prejudiced, or biased against various demographic groups. Specifically, since we focus on the dynam- ics of offensiveness, we analyze a sample of 13,000 Toxigen statements tagged as \u201coffensive\u201d. 3.2 Generating Likely Contexts Following work demonstrating that LLMs can gen- erate realistic social situations related to majority and minority groups (Park et al., 2022), we use GPT-3.5 to construct plausible or likely contexts (i.e., situation, speaker identity, listener identity) in which a statement could be made. Specifically, we manually curate fifty statement-context pairs, out of which we sample five for each statement as in-context examples. Conditioned on the in-context examples, we then sample three contexts from GPT- 3.5 for each statement. The examples of prompts for plausible context generation are presented in Appendix C. Verifying Contexts We randomly sample 500 statement-context pairs and ask three annotators to rate the plausibility of the contexts (see Appendix A.2 for the exact questions).5 Of the 500 pairs, only 1% were marked as completely implausible or gibberish. 92% of the scenarios were marked as plausible by at least two workers, and some were marked as unlikely but technically plausible (e.g., A mayor in the public saying \u201cBlack people are not humans.\u201d) We retain these contexts since such rare situations could still happen, making them helpful for our analyses and modeling experiments. 3.3 Generating COBRA Explanations Similar to context generation, we make use of GPT-3.5\u2019s ability to produce rich explanations of social commonsense (West et al., 2022) to gener- ate explanations along our seven dimensions. For each context-statement pair, we generate one full COBRA frame, using three randomly sampled in- context examples from our pool of six manually curated prompts. As shown in Table 2, this pro- cess yields a COBRACORPUS containing 32k full (context-statement-explanation) COBRA frames. Verifying Explanations To ensure data quality, we randomly sampled 567 (statement, context, ex- planations) triples and asked three annotators to rate how likely the explanations fit the statements in context. Inspired by prior work (Aguinis et al., 5On this context verification task, the agreement was moderately high, with 75.37% pairwise agreement and free- marginal multi-rater \u03ba=0.507 (Randolph, 2005). Friends Strangers Workplace Family Other more off. 5.28% 43.09% 27.54% 2.85% 21.24% 5.79% 11.38% 6.17% less off. 60.06% 16.6% Table 3: Percentage of contexts occurring under each category/scenario in COBRACORPUS-CF. Row 1 in- dicates statements that are more offensive due to their contexts vs Row 2 indicates those which are lesser of- fensive in comparison 2021; Clark et al., 2021; Liu et al., 2022), we also asked annotators to provide corrections or sugges- tions for those they consider unlikely. 97% of ex- planations were marked as likely by at least two annotators (majority vote) and 84% were"}, {"question": " From which source is the consideration of perceived emotional and cognitive reactions of the target group or listener drawn?", "answer": " The consideration of perceived emotional and cognitive reactions is drawn from Nadal et al. (2014).", "ref_chunk": "interpretations of statements (e.g., gay men tend to hold more privilege along the gender privilege spectrum, but less along the sexuality one). Impact (Imp.) explain the biased, prejudiced, or stereotypical meaning implied by the statement, similar to Sap et al. (2020). This implication is very closely related to the received meaning from the listener\u2019s or targeted group\u2019s perspective and may differ from the speaker\u2019s intended meaning (e.g., for microaggressions; Sue, 2010). Emotional and Cognitive Reactions (Emo. & Cog.) capture the possible negative effects and harms that the statement and its implied meaning could have on the targeted group. There is an in- creasing push to develop content moderation from the perspective of the harms that content engen- ders (Keller and Leerssen, 2020; Vaccaro et al., 2020). As such, we draw from Nadal et al. (2014) and consider the perceived emotional and cognitive reactions of the target group or listener. The emo- tional reactions capture the short-term emotional effects or reactions to a statement (e.g., \u201canger and annoyance\u201d, \u201cworthlessness\u201d) On the other hand, the cognitive reactions focus on the lessons some- one could draw, the subsequent actions someone could take, or on the long-term harms that repeated exposure to such statements could have. Examples include \u201cnot wanting to come into work anymore,\u201d \u201cavoiding a particular teacher,\u201d etc. Unique # Avg. # words Statements 11,648 14.34 t x e t n o C Situation Speakers Listeners 23,577 10,683 13,554 6.90 3.11 4.05 s n o i t a n a l p x E Intents Target group Power dynamics Implication Emo. Reaction Cog. Reaction Offensiveness 29,895 11,126 12,766 30,802 28,429 29,826 2,527 14.97 3.48 10.46 19.66 16.82 22.06 2.09 Total # in COBRACORPUS 32,582 Table 2: General data statistics of COBRACORPUS Offensiveness (Off.) captures, in 1-3 words, the type or degree of offensiveness of the statement (e.g., \u201csexism\u201d, \u201coffensive generalization\u201d). We avoid imposing a categorization or cutoff between offensive and harmless statements and instead leave this dimension as free-text, to preserve nuanced in- terpretations of statements and capture the full spec- trum of offensiveness types (Jurgens et al., 2019). 3 Collecting COBRACORPUS To study the contextual dynamics of the offensive- ness of statements at scale, we create COBRACOR- PUS using a three-stage data generation pipeline with human verification, shown in Figure 2. Given that no available corpus contains statements with their contexts and explanations,4 we prompt a large language model (GPT-3.5; Ouyang et al., 2022) to generate contexts and explanations, following (Hartvigsen et al., 2022; West et al., 2022; Kim et al., 2022b,a). Specifically, we first generate mul- tiple plausible contexts for statements, then gen- erate the explanations for each context separately, using GPT-3.5 with in-context examples. Please refer to Appendix C for examples of our prompts. To ensure data quality, we design a set of crowd- sourcing tasks to verify the generated contexts and explanations and collect suggestions. For all tasks, we pre-select crowd workers based on a qualifica- tion task that judged their understanding of each dimension. Please refer to Appendix A for the details of all crowd-sourcing experiments. 3.1 Collecting Statements We draw our statements from Toxigen (Hartvigsen et al., 2022), a dataset of GPT3-generated state- ments that are subtly or implicitly toxic, offensive, 4Note, we do not infer the demographic categories of state- ment authors or readers for ethical reasons (Tatman, 2020). prejudiced, or biased against various demographic groups. Specifically, since we focus on the dynam- ics of offensiveness, we analyze a sample of 13,000 Toxigen statements tagged as \u201coffensive\u201d. 3.2 Generating Likely Contexts Following work demonstrating that LLMs can gen- erate realistic social situations related to majority and minority groups (Park et al., 2022), we use GPT-3.5 to construct plausible or likely contexts (i.e., situation, speaker identity, listener identity) in which a statement could be made. Specifically, we manually curate fifty statement-context pairs, out of which we sample five for each statement as in-context examples. Conditioned on the in-context examples, we then sample three contexts from GPT- 3.5 for each statement. The examples of prompts for plausible context generation are presented in Appendix C. Verifying Contexts We randomly sample 500 statement-context pairs and ask three annotators to rate the plausibility of the contexts (see Appendix A.2 for the exact questions).5 Of the 500 pairs, only 1% were marked as completely implausible or gibberish. 92% of the scenarios were marked as plausible by at least two workers, and some were marked as unlikely but technically plausible (e.g., A mayor in the public saying \u201cBlack people are not humans.\u201d) We retain these contexts since such rare situations could still happen, making them helpful for our analyses and modeling experiments. 3.3 Generating COBRA Explanations Similar to context generation, we make use of GPT-3.5\u2019s ability to produce rich explanations of social commonsense (West et al., 2022) to gener- ate explanations along our seven dimensions. For each context-statement pair, we generate one full COBRA frame, using three randomly sampled in- context examples from our pool of six manually curated prompts. As shown in Table 2, this pro- cess yields a COBRACORPUS containing 32k full (context-statement-explanation) COBRA frames. Verifying Explanations To ensure data quality, we randomly sampled 567 (statement, context, ex- planations) triples and asked three annotators to rate how likely the explanations fit the statements in context. Inspired by prior work (Aguinis et al., 5On this context verification task, the agreement was moderately high, with 75.37% pairwise agreement and free- marginal multi-rater \u03ba=0.507 (Randolph, 2005). Friends Strangers Workplace Family Other more off. 5.28% 43.09% 27.54% 2.85% 21.24% 5.79% 11.38% 6.17% less off. 60.06% 16.6% Table 3: Percentage of contexts occurring under each category/scenario in COBRACORPUS-CF. Row 1 in- dicates statements that are more offensive due to their contexts vs Row 2 indicates those which are lesser of- fensive in comparison 2021; Clark et al., 2021; Liu et al., 2022), we also asked annotators to provide corrections or sugges- tions for those they consider unlikely. 97% of ex- planations were marked as likely by at least two annotators (majority vote) and 84% were"}, {"question": " What do emotional reactions capture?", "answer": " Emotional reactions capture the short-term emotional effects or reactions to a statement.", "ref_chunk": "interpretations of statements (e.g., gay men tend to hold more privilege along the gender privilege spectrum, but less along the sexuality one). Impact (Imp.) explain the biased, prejudiced, or stereotypical meaning implied by the statement, similar to Sap et al. (2020). This implication is very closely related to the received meaning from the listener\u2019s or targeted group\u2019s perspective and may differ from the speaker\u2019s intended meaning (e.g., for microaggressions; Sue, 2010). Emotional and Cognitive Reactions (Emo. & Cog.) capture the possible negative effects and harms that the statement and its implied meaning could have on the targeted group. There is an in- creasing push to develop content moderation from the perspective of the harms that content engen- ders (Keller and Leerssen, 2020; Vaccaro et al., 2020). As such, we draw from Nadal et al. (2014) and consider the perceived emotional and cognitive reactions of the target group or listener. The emo- tional reactions capture the short-term emotional effects or reactions to a statement (e.g., \u201canger and annoyance\u201d, \u201cworthlessness\u201d) On the other hand, the cognitive reactions focus on the lessons some- one could draw, the subsequent actions someone could take, or on the long-term harms that repeated exposure to such statements could have. Examples include \u201cnot wanting to come into work anymore,\u201d \u201cavoiding a particular teacher,\u201d etc. Unique # Avg. # words Statements 11,648 14.34 t x e t n o C Situation Speakers Listeners 23,577 10,683 13,554 6.90 3.11 4.05 s n o i t a n a l p x E Intents Target group Power dynamics Implication Emo. Reaction Cog. Reaction Offensiveness 29,895 11,126 12,766 30,802 28,429 29,826 2,527 14.97 3.48 10.46 19.66 16.82 22.06 2.09 Total # in COBRACORPUS 32,582 Table 2: General data statistics of COBRACORPUS Offensiveness (Off.) captures, in 1-3 words, the type or degree of offensiveness of the statement (e.g., \u201csexism\u201d, \u201coffensive generalization\u201d). We avoid imposing a categorization or cutoff between offensive and harmless statements and instead leave this dimension as free-text, to preserve nuanced in- terpretations of statements and capture the full spec- trum of offensiveness types (Jurgens et al., 2019). 3 Collecting COBRACORPUS To study the contextual dynamics of the offensive- ness of statements at scale, we create COBRACOR- PUS using a three-stage data generation pipeline with human verification, shown in Figure 2. Given that no available corpus contains statements with their contexts and explanations,4 we prompt a large language model (GPT-3.5; Ouyang et al., 2022) to generate contexts and explanations, following (Hartvigsen et al., 2022; West et al., 2022; Kim et al., 2022b,a). Specifically, we first generate mul- tiple plausible contexts for statements, then gen- erate the explanations for each context separately, using GPT-3.5 with in-context examples. Please refer to Appendix C for examples of our prompts. To ensure data quality, we design a set of crowd- sourcing tasks to verify the generated contexts and explanations and collect suggestions. For all tasks, we pre-select crowd workers based on a qualifica- tion task that judged their understanding of each dimension. Please refer to Appendix A for the details of all crowd-sourcing experiments. 3.1 Collecting Statements We draw our statements from Toxigen (Hartvigsen et al., 2022), a dataset of GPT3-generated state- ments that are subtly or implicitly toxic, offensive, 4Note, we do not infer the demographic categories of state- ment authors or readers for ethical reasons (Tatman, 2020). prejudiced, or biased against various demographic groups. Specifically, since we focus on the dynam- ics of offensiveness, we analyze a sample of 13,000 Toxigen statements tagged as \u201coffensive\u201d. 3.2 Generating Likely Contexts Following work demonstrating that LLMs can gen- erate realistic social situations related to majority and minority groups (Park et al., 2022), we use GPT-3.5 to construct plausible or likely contexts (i.e., situation, speaker identity, listener identity) in which a statement could be made. Specifically, we manually curate fifty statement-context pairs, out of which we sample five for each statement as in-context examples. Conditioned on the in-context examples, we then sample three contexts from GPT- 3.5 for each statement. The examples of prompts for plausible context generation are presented in Appendix C. Verifying Contexts We randomly sample 500 statement-context pairs and ask three annotators to rate the plausibility of the contexts (see Appendix A.2 for the exact questions).5 Of the 500 pairs, only 1% were marked as completely implausible or gibberish. 92% of the scenarios were marked as plausible by at least two workers, and some were marked as unlikely but technically plausible (e.g., A mayor in the public saying \u201cBlack people are not humans.\u201d) We retain these contexts since such rare situations could still happen, making them helpful for our analyses and modeling experiments. 3.3 Generating COBRA Explanations Similar to context generation, we make use of GPT-3.5\u2019s ability to produce rich explanations of social commonsense (West et al., 2022) to gener- ate explanations along our seven dimensions. For each context-statement pair, we generate one full COBRA frame, using three randomly sampled in- context examples from our pool of six manually curated prompts. As shown in Table 2, this pro- cess yields a COBRACORPUS containing 32k full (context-statement-explanation) COBRA frames. Verifying Explanations To ensure data quality, we randomly sampled 567 (statement, context, ex- planations) triples and asked three annotators to rate how likely the explanations fit the statements in context. Inspired by prior work (Aguinis et al., 5On this context verification task, the agreement was moderately high, with 75.37% pairwise agreement and free- marginal multi-rater \u03ba=0.507 (Randolph, 2005). Friends Strangers Workplace Family Other more off. 5.28% 43.09% 27.54% 2.85% 21.24% 5.79% 11.38% 6.17% less off. 60.06% 16.6% Table 3: Percentage of contexts occurring under each category/scenario in COBRACORPUS-CF. Row 1 in- dicates statements that are more offensive due to their contexts vs Row 2 indicates those which are lesser of- fensive in comparison 2021; Clark et al., 2021; Liu et al., 2022), we also asked annotators to provide corrections or sugges- tions for those they consider unlikely. 97% of ex- planations were marked as likely by at least two annotators (majority vote) and 84% were"}, {"question": " What do cognitive reactions focus on?", "answer": " Cognitive reactions focus on the lessons someone could draw, the subsequent actions someone could take, or on the long-term harms that repeated exposure to such statements could have.", "ref_chunk": "interpretations of statements (e.g., gay men tend to hold more privilege along the gender privilege spectrum, but less along the sexuality one). Impact (Imp.) explain the biased, prejudiced, or stereotypical meaning implied by the statement, similar to Sap et al. (2020). This implication is very closely related to the received meaning from the listener\u2019s or targeted group\u2019s perspective and may differ from the speaker\u2019s intended meaning (e.g., for microaggressions; Sue, 2010). Emotional and Cognitive Reactions (Emo. & Cog.) capture the possible negative effects and harms that the statement and its implied meaning could have on the targeted group. There is an in- creasing push to develop content moderation from the perspective of the harms that content engen- ders (Keller and Leerssen, 2020; Vaccaro et al., 2020). As such, we draw from Nadal et al. (2014) and consider the perceived emotional and cognitive reactions of the target group or listener. The emo- tional reactions capture the short-term emotional effects or reactions to a statement (e.g., \u201canger and annoyance\u201d, \u201cworthlessness\u201d) On the other hand, the cognitive reactions focus on the lessons some- one could draw, the subsequent actions someone could take, or on the long-term harms that repeated exposure to such statements could have. Examples include \u201cnot wanting to come into work anymore,\u201d \u201cavoiding a particular teacher,\u201d etc. Unique # Avg. # words Statements 11,648 14.34 t x e t n o C Situation Speakers Listeners 23,577 10,683 13,554 6.90 3.11 4.05 s n o i t a n a l p x E Intents Target group Power dynamics Implication Emo. Reaction Cog. Reaction Offensiveness 29,895 11,126 12,766 30,802 28,429 29,826 2,527 14.97 3.48 10.46 19.66 16.82 22.06 2.09 Total # in COBRACORPUS 32,582 Table 2: General data statistics of COBRACORPUS Offensiveness (Off.) captures, in 1-3 words, the type or degree of offensiveness of the statement (e.g., \u201csexism\u201d, \u201coffensive generalization\u201d). We avoid imposing a categorization or cutoff between offensive and harmless statements and instead leave this dimension as free-text, to preserve nuanced in- terpretations of statements and capture the full spec- trum of offensiveness types (Jurgens et al., 2019). 3 Collecting COBRACORPUS To study the contextual dynamics of the offensive- ness of statements at scale, we create COBRACOR- PUS using a three-stage data generation pipeline with human verification, shown in Figure 2. Given that no available corpus contains statements with their contexts and explanations,4 we prompt a large language model (GPT-3.5; Ouyang et al., 2022) to generate contexts and explanations, following (Hartvigsen et al., 2022; West et al., 2022; Kim et al., 2022b,a). Specifically, we first generate mul- tiple plausible contexts for statements, then gen- erate the explanations for each context separately, using GPT-3.5 with in-context examples. Please refer to Appendix C for examples of our prompts. To ensure data quality, we design a set of crowd- sourcing tasks to verify the generated contexts and explanations and collect suggestions. For all tasks, we pre-select crowd workers based on a qualifica- tion task that judged their understanding of each dimension. Please refer to Appendix A for the details of all crowd-sourcing experiments. 3.1 Collecting Statements We draw our statements from Toxigen (Hartvigsen et al., 2022), a dataset of GPT3-generated state- ments that are subtly or implicitly toxic, offensive, 4Note, we do not infer the demographic categories of state- ment authors or readers for ethical reasons (Tatman, 2020). prejudiced, or biased against various demographic groups. Specifically, since we focus on the dynam- ics of offensiveness, we analyze a sample of 13,000 Toxigen statements tagged as \u201coffensive\u201d. 3.2 Generating Likely Contexts Following work demonstrating that LLMs can gen- erate realistic social situations related to majority and minority groups (Park et al., 2022), we use GPT-3.5 to construct plausible or likely contexts (i.e., situation, speaker identity, listener identity) in which a statement could be made. Specifically, we manually curate fifty statement-context pairs, out of which we sample five for each statement as in-context examples. Conditioned on the in-context examples, we then sample three contexts from GPT- 3.5 for each statement. The examples of prompts for plausible context generation are presented in Appendix C. Verifying Contexts We randomly sample 500 statement-context pairs and ask three annotators to rate the plausibility of the contexts (see Appendix A.2 for the exact questions).5 Of the 500 pairs, only 1% were marked as completely implausible or gibberish. 92% of the scenarios were marked as plausible by at least two workers, and some were marked as unlikely but technically plausible (e.g., A mayor in the public saying \u201cBlack people are not humans.\u201d) We retain these contexts since such rare situations could still happen, making them helpful for our analyses and modeling experiments. 3.3 Generating COBRA Explanations Similar to context generation, we make use of GPT-3.5\u2019s ability to produce rich explanations of social commonsense (West et al., 2022) to gener- ate explanations along our seven dimensions. For each context-statement pair, we generate one full COBRA frame, using three randomly sampled in- context examples from our pool of six manually curated prompts. As shown in Table 2, this pro- cess yields a COBRACORPUS containing 32k full (context-statement-explanation) COBRA frames. Verifying Explanations To ensure data quality, we randomly sampled 567 (statement, context, ex- planations) triples and asked three annotators to rate how likely the explanations fit the statements in context. Inspired by prior work (Aguinis et al., 5On this context verification task, the agreement was moderately high, with 75.37% pairwise agreement and free- marginal multi-rater \u03ba=0.507 (Randolph, 2005). Friends Strangers Workplace Family Other more off. 5.28% 43.09% 27.54% 2.85% 21.24% 5.79% 11.38% 6.17% less off. 60.06% 16.6% Table 3: Percentage of contexts occurring under each category/scenario in COBRACORPUS-CF. Row 1 in- dicates statements that are more offensive due to their contexts vs Row 2 indicates those which are lesser of- fensive in comparison 2021; Clark et al., 2021; Liu et al., 2022), we also asked annotators to provide corrections or sugges- tions for those they consider unlikely. 97% of ex- planations were marked as likely by at least two annotators (majority vote) and 84% were"}, {"question": " What does Offensiveness (Off.) capture?", "answer": " Offensiveness captures, in 1-3 words, the type or degree of offensiveness of the statement.", "ref_chunk": "interpretations of statements (e.g., gay men tend to hold more privilege along the gender privilege spectrum, but less along the sexuality one). Impact (Imp.) explain the biased, prejudiced, or stereotypical meaning implied by the statement, similar to Sap et al. (2020). This implication is very closely related to the received meaning from the listener\u2019s or targeted group\u2019s perspective and may differ from the speaker\u2019s intended meaning (e.g., for microaggressions; Sue, 2010). Emotional and Cognitive Reactions (Emo. & Cog.) capture the possible negative effects and harms that the statement and its implied meaning could have on the targeted group. There is an in- creasing push to develop content moderation from the perspective of the harms that content engen- ders (Keller and Leerssen, 2020; Vaccaro et al., 2020). As such, we draw from Nadal et al. (2014) and consider the perceived emotional and cognitive reactions of the target group or listener. The emo- tional reactions capture the short-term emotional effects or reactions to a statement (e.g., \u201canger and annoyance\u201d, \u201cworthlessness\u201d) On the other hand, the cognitive reactions focus on the lessons some- one could draw, the subsequent actions someone could take, or on the long-term harms that repeated exposure to such statements could have. Examples include \u201cnot wanting to come into work anymore,\u201d \u201cavoiding a particular teacher,\u201d etc. Unique # Avg. # words Statements 11,648 14.34 t x e t n o C Situation Speakers Listeners 23,577 10,683 13,554 6.90 3.11 4.05 s n o i t a n a l p x E Intents Target group Power dynamics Implication Emo. Reaction Cog. Reaction Offensiveness 29,895 11,126 12,766 30,802 28,429 29,826 2,527 14.97 3.48 10.46 19.66 16.82 22.06 2.09 Total # in COBRACORPUS 32,582 Table 2: General data statistics of COBRACORPUS Offensiveness (Off.) captures, in 1-3 words, the type or degree of offensiveness of the statement (e.g., \u201csexism\u201d, \u201coffensive generalization\u201d). We avoid imposing a categorization or cutoff between offensive and harmless statements and instead leave this dimension as free-text, to preserve nuanced in- terpretations of statements and capture the full spec- trum of offensiveness types (Jurgens et al., 2019). 3 Collecting COBRACORPUS To study the contextual dynamics of the offensive- ness of statements at scale, we create COBRACOR- PUS using a three-stage data generation pipeline with human verification, shown in Figure 2. Given that no available corpus contains statements with their contexts and explanations,4 we prompt a large language model (GPT-3.5; Ouyang et al., 2022) to generate contexts and explanations, following (Hartvigsen et al., 2022; West et al., 2022; Kim et al., 2022b,a). Specifically, we first generate mul- tiple plausible contexts for statements, then gen- erate the explanations for each context separately, using GPT-3.5 with in-context examples. Please refer to Appendix C for examples of our prompts. To ensure data quality, we design a set of crowd- sourcing tasks to verify the generated contexts and explanations and collect suggestions. For all tasks, we pre-select crowd workers based on a qualifica- tion task that judged their understanding of each dimension. Please refer to Appendix A for the details of all crowd-sourcing experiments. 3.1 Collecting Statements We draw our statements from Toxigen (Hartvigsen et al., 2022), a dataset of GPT3-generated state- ments that are subtly or implicitly toxic, offensive, 4Note, we do not infer the demographic categories of state- ment authors or readers for ethical reasons (Tatman, 2020). prejudiced, or biased against various demographic groups. Specifically, since we focus on the dynam- ics of offensiveness, we analyze a sample of 13,000 Toxigen statements tagged as \u201coffensive\u201d. 3.2 Generating Likely Contexts Following work demonstrating that LLMs can gen- erate realistic social situations related to majority and minority groups (Park et al., 2022), we use GPT-3.5 to construct plausible or likely contexts (i.e., situation, speaker identity, listener identity) in which a statement could be made. Specifically, we manually curate fifty statement-context pairs, out of which we sample five for each statement as in-context examples. Conditioned on the in-context examples, we then sample three contexts from GPT- 3.5 for each statement. The examples of prompts for plausible context generation are presented in Appendix C. Verifying Contexts We randomly sample 500 statement-context pairs and ask three annotators to rate the plausibility of the contexts (see Appendix A.2 for the exact questions).5 Of the 500 pairs, only 1% were marked as completely implausible or gibberish. 92% of the scenarios were marked as plausible by at least two workers, and some were marked as unlikely but technically plausible (e.g., A mayor in the public saying \u201cBlack people are not humans.\u201d) We retain these contexts since such rare situations could still happen, making them helpful for our analyses and modeling experiments. 3.3 Generating COBRA Explanations Similar to context generation, we make use of GPT-3.5\u2019s ability to produce rich explanations of social commonsense (West et al., 2022) to gener- ate explanations along our seven dimensions. For each context-statement pair, we generate one full COBRA frame, using three randomly sampled in- context examples from our pool of six manually curated prompts. As shown in Table 2, this pro- cess yields a COBRACORPUS containing 32k full (context-statement-explanation) COBRA frames. Verifying Explanations To ensure data quality, we randomly sampled 567 (statement, context, ex- planations) triples and asked three annotators to rate how likely the explanations fit the statements in context. Inspired by prior work (Aguinis et al., 5On this context verification task, the agreement was moderately high, with 75.37% pairwise agreement and free- marginal multi-rater \u03ba=0.507 (Randolph, 2005). Friends Strangers Workplace Family Other more off. 5.28% 43.09% 27.54% 2.85% 21.24% 5.79% 11.38% 6.17% less off. 60.06% 16.6% Table 3: Percentage of contexts occurring under each category/scenario in COBRACORPUS-CF. Row 1 in- dicates statements that are more offensive due to their contexts vs Row 2 indicates those which are lesser of- fensive in comparison 2021; Clark et al., 2021; Liu et al., 2022), we also asked annotators to provide corrections or sugges- tions for those they consider unlikely. 97% of ex- planations were marked as likely by at least two annotators (majority vote) and 84% were"}, {"question": " How is the offensiveness of a statement categorized?", "answer": " The text avoids imposing a categorization or cutoff between offensive and harmless statements.", "ref_chunk": "interpretations of statements (e.g., gay men tend to hold more privilege along the gender privilege spectrum, but less along the sexuality one). Impact (Imp.) explain the biased, prejudiced, or stereotypical meaning implied by the statement, similar to Sap et al. (2020). This implication is very closely related to the received meaning from the listener\u2019s or targeted group\u2019s perspective and may differ from the speaker\u2019s intended meaning (e.g., for microaggressions; Sue, 2010). Emotional and Cognitive Reactions (Emo. & Cog.) capture the possible negative effects and harms that the statement and its implied meaning could have on the targeted group. There is an in- creasing push to develop content moderation from the perspective of the harms that content engen- ders (Keller and Leerssen, 2020; Vaccaro et al., 2020). As such, we draw from Nadal et al. (2014) and consider the perceived emotional and cognitive reactions of the target group or listener. The emo- tional reactions capture the short-term emotional effects or reactions to a statement (e.g., \u201canger and annoyance\u201d, \u201cworthlessness\u201d) On the other hand, the cognitive reactions focus on the lessons some- one could draw, the subsequent actions someone could take, or on the long-term harms that repeated exposure to such statements could have. Examples include \u201cnot wanting to come into work anymore,\u201d \u201cavoiding a particular teacher,\u201d etc. Unique # Avg. # words Statements 11,648 14.34 t x e t n o C Situation Speakers Listeners 23,577 10,683 13,554 6.90 3.11 4.05 s n o i t a n a l p x E Intents Target group Power dynamics Implication Emo. Reaction Cog. Reaction Offensiveness 29,895 11,126 12,766 30,802 28,429 29,826 2,527 14.97 3.48 10.46 19.66 16.82 22.06 2.09 Total # in COBRACORPUS 32,582 Table 2: General data statistics of COBRACORPUS Offensiveness (Off.) captures, in 1-3 words, the type or degree of offensiveness of the statement (e.g., \u201csexism\u201d, \u201coffensive generalization\u201d). We avoid imposing a categorization or cutoff between offensive and harmless statements and instead leave this dimension as free-text, to preserve nuanced in- terpretations of statements and capture the full spec- trum of offensiveness types (Jurgens et al., 2019). 3 Collecting COBRACORPUS To study the contextual dynamics of the offensive- ness of statements at scale, we create COBRACOR- PUS using a three-stage data generation pipeline with human verification, shown in Figure 2. Given that no available corpus contains statements with their contexts and explanations,4 we prompt a large language model (GPT-3.5; Ouyang et al., 2022) to generate contexts and explanations, following (Hartvigsen et al., 2022; West et al., 2022; Kim et al., 2022b,a). Specifically, we first generate mul- tiple plausible contexts for statements, then gen- erate the explanations for each context separately, using GPT-3.5 with in-context examples. Please refer to Appendix C for examples of our prompts. To ensure data quality, we design a set of crowd- sourcing tasks to verify the generated contexts and explanations and collect suggestions. For all tasks, we pre-select crowd workers based on a qualifica- tion task that judged their understanding of each dimension. Please refer to Appendix A for the details of all crowd-sourcing experiments. 3.1 Collecting Statements We draw our statements from Toxigen (Hartvigsen et al., 2022), a dataset of GPT3-generated state- ments that are subtly or implicitly toxic, offensive, 4Note, we do not infer the demographic categories of state- ment authors or readers for ethical reasons (Tatman, 2020). prejudiced, or biased against various demographic groups. Specifically, since we focus on the dynam- ics of offensiveness, we analyze a sample of 13,000 Toxigen statements tagged as \u201coffensive\u201d. 3.2 Generating Likely Contexts Following work demonstrating that LLMs can gen- erate realistic social situations related to majority and minority groups (Park et al., 2022), we use GPT-3.5 to construct plausible or likely contexts (i.e., situation, speaker identity, listener identity) in which a statement could be made. Specifically, we manually curate fifty statement-context pairs, out of which we sample five for each statement as in-context examples. Conditioned on the in-context examples, we then sample three contexts from GPT- 3.5 for each statement. The examples of prompts for plausible context generation are presented in Appendix C. Verifying Contexts We randomly sample 500 statement-context pairs and ask three annotators to rate the plausibility of the contexts (see Appendix A.2 for the exact questions).5 Of the 500 pairs, only 1% were marked as completely implausible or gibberish. 92% of the scenarios were marked as plausible by at least two workers, and some were marked as unlikely but technically plausible (e.g., A mayor in the public saying \u201cBlack people are not humans.\u201d) We retain these contexts since such rare situations could still happen, making them helpful for our analyses and modeling experiments. 3.3 Generating COBRA Explanations Similar to context generation, we make use of GPT-3.5\u2019s ability to produce rich explanations of social commonsense (West et al., 2022) to gener- ate explanations along our seven dimensions. For each context-statement pair, we generate one full COBRA frame, using three randomly sampled in- context examples from our pool of six manually curated prompts. As shown in Table 2, this pro- cess yields a COBRACORPUS containing 32k full (context-statement-explanation) COBRA frames. Verifying Explanations To ensure data quality, we randomly sampled 567 (statement, context, ex- planations) triples and asked three annotators to rate how likely the explanations fit the statements in context. Inspired by prior work (Aguinis et al., 5On this context verification task, the agreement was moderately high, with 75.37% pairwise agreement and free- marginal multi-rater \u03ba=0.507 (Randolph, 2005). Friends Strangers Workplace Family Other more off. 5.28% 43.09% 27.54% 2.85% 21.24% 5.79% 11.38% 6.17% less off. 60.06% 16.6% Table 3: Percentage of contexts occurring under each category/scenario in COBRACORPUS-CF. Row 1 in- dicates statements that are more offensive due to their contexts vs Row 2 indicates those which are lesser of- fensive in comparison 2021; Clark et al., 2021; Liu et al., 2022), we also asked annotators to provide corrections or sugges- tions for those they consider unlikely. 97% of ex- planations were marked as likely by at least two annotators (majority vote) and 84% were"}, {"question": " What is the main focus of creating COBRACORPUS?", "answer": " The main focus of creating COBRACORPUS is to study the contextual dynamics of the offensiveness of statements at scale.", "ref_chunk": "interpretations of statements (e.g., gay men tend to hold more privilege along the gender privilege spectrum, but less along the sexuality one). Impact (Imp.) explain the biased, prejudiced, or stereotypical meaning implied by the statement, similar to Sap et al. (2020). This implication is very closely related to the received meaning from the listener\u2019s or targeted group\u2019s perspective and may differ from the speaker\u2019s intended meaning (e.g., for microaggressions; Sue, 2010). Emotional and Cognitive Reactions (Emo. & Cog.) capture the possible negative effects and harms that the statement and its implied meaning could have on the targeted group. There is an in- creasing push to develop content moderation from the perspective of the harms that content engen- ders (Keller and Leerssen, 2020; Vaccaro et al., 2020). As such, we draw from Nadal et al. (2014) and consider the perceived emotional and cognitive reactions of the target group or listener. The emo- tional reactions capture the short-term emotional effects or reactions to a statement (e.g., \u201canger and annoyance\u201d, \u201cworthlessness\u201d) On the other hand, the cognitive reactions focus on the lessons some- one could draw, the subsequent actions someone could take, or on the long-term harms that repeated exposure to such statements could have. Examples include \u201cnot wanting to come into work anymore,\u201d \u201cavoiding a particular teacher,\u201d etc. Unique # Avg. # words Statements 11,648 14.34 t x e t n o C Situation Speakers Listeners 23,577 10,683 13,554 6.90 3.11 4.05 s n o i t a n a l p x E Intents Target group Power dynamics Implication Emo. Reaction Cog. Reaction Offensiveness 29,895 11,126 12,766 30,802 28,429 29,826 2,527 14.97 3.48 10.46 19.66 16.82 22.06 2.09 Total # in COBRACORPUS 32,582 Table 2: General data statistics of COBRACORPUS Offensiveness (Off.) captures, in 1-3 words, the type or degree of offensiveness of the statement (e.g., \u201csexism\u201d, \u201coffensive generalization\u201d). We avoid imposing a categorization or cutoff between offensive and harmless statements and instead leave this dimension as free-text, to preserve nuanced in- terpretations of statements and capture the full spec- trum of offensiveness types (Jurgens et al., 2019). 3 Collecting COBRACORPUS To study the contextual dynamics of the offensive- ness of statements at scale, we create COBRACOR- PUS using a three-stage data generation pipeline with human verification, shown in Figure 2. Given that no available corpus contains statements with their contexts and explanations,4 we prompt a large language model (GPT-3.5; Ouyang et al., 2022) to generate contexts and explanations, following (Hartvigsen et al., 2022; West et al., 2022; Kim et al., 2022b,a). Specifically, we first generate mul- tiple plausible contexts for statements, then gen- erate the explanations for each context separately, using GPT-3.5 with in-context examples. Please refer to Appendix C for examples of our prompts. To ensure data quality, we design a set of crowd- sourcing tasks to verify the generated contexts and explanations and collect suggestions. For all tasks, we pre-select crowd workers based on a qualifica- tion task that judged their understanding of each dimension. Please refer to Appendix A for the details of all crowd-sourcing experiments. 3.1 Collecting Statements We draw our statements from Toxigen (Hartvigsen et al., 2022), a dataset of GPT3-generated state- ments that are subtly or implicitly toxic, offensive, 4Note, we do not infer the demographic categories of state- ment authors or readers for ethical reasons (Tatman, 2020). prejudiced, or biased against various demographic groups. Specifically, since we focus on the dynam- ics of offensiveness, we analyze a sample of 13,000 Toxigen statements tagged as \u201coffensive\u201d. 3.2 Generating Likely Contexts Following work demonstrating that LLMs can gen- erate realistic social situations related to majority and minority groups (Park et al., 2022), we use GPT-3.5 to construct plausible or likely contexts (i.e., situation, speaker identity, listener identity) in which a statement could be made. Specifically, we manually curate fifty statement-context pairs, out of which we sample five for each statement as in-context examples. Conditioned on the in-context examples, we then sample three contexts from GPT- 3.5 for each statement. The examples of prompts for plausible context generation are presented in Appendix C. Verifying Contexts We randomly sample 500 statement-context pairs and ask three annotators to rate the plausibility of the contexts (see Appendix A.2 for the exact questions).5 Of the 500 pairs, only 1% were marked as completely implausible or gibberish. 92% of the scenarios were marked as plausible by at least two workers, and some were marked as unlikely but technically plausible (e.g., A mayor in the public saying \u201cBlack people are not humans.\u201d) We retain these contexts since such rare situations could still happen, making them helpful for our analyses and modeling experiments. 3.3 Generating COBRA Explanations Similar to context generation, we make use of GPT-3.5\u2019s ability to produce rich explanations of social commonsense (West et al., 2022) to gener- ate explanations along our seven dimensions. For each context-statement pair, we generate one full COBRA frame, using three randomly sampled in- context examples from our pool of six manually curated prompts. As shown in Table 2, this pro- cess yields a COBRACORPUS containing 32k full (context-statement-explanation) COBRA frames. Verifying Explanations To ensure data quality, we randomly sampled 567 (statement, context, ex- planations) triples and asked three annotators to rate how likely the explanations fit the statements in context. Inspired by prior work (Aguinis et al., 5On this context verification task, the agreement was moderately high, with 75.37% pairwise agreement and free- marginal multi-rater \u03ba=0.507 (Randolph, 2005). Friends Strangers Workplace Family Other more off. 5.28% 43.09% 27.54% 2.85% 21.24% 5.79% 11.38% 6.17% less off. 60.06% 16.6% Table 3: Percentage of contexts occurring under each category/scenario in COBRACORPUS-CF. Row 1 in- dicates statements that are more offensive due to their contexts vs Row 2 indicates those which are lesser of- fensive in comparison 2021; Clark et al., 2021; Liu et al., 2022), we also asked annotators to provide corrections or sugges- tions for those they consider unlikely. 97% of ex- planations were marked as likely by at least two annotators (majority vote) and 84% were"}, {"question": " Where are the statements drawn from in the text?", "answer": " The statements are drawn from Toxigen, a dataset of GPT3-generated statements that are subtly or implicitly toxic, offensive, prejudiced, or biased against various demographic groups.", "ref_chunk": "interpretations of statements (e.g., gay men tend to hold more privilege along the gender privilege spectrum, but less along the sexuality one). Impact (Imp.) explain the biased, prejudiced, or stereotypical meaning implied by the statement, similar to Sap et al. (2020). This implication is very closely related to the received meaning from the listener\u2019s or targeted group\u2019s perspective and may differ from the speaker\u2019s intended meaning (e.g., for microaggressions; Sue, 2010). Emotional and Cognitive Reactions (Emo. & Cog.) capture the possible negative effects and harms that the statement and its implied meaning could have on the targeted group. There is an in- creasing push to develop content moderation from the perspective of the harms that content engen- ders (Keller and Leerssen, 2020; Vaccaro et al., 2020). As such, we draw from Nadal et al. (2014) and consider the perceived emotional and cognitive reactions of the target group or listener. The emo- tional reactions capture the short-term emotional effects or reactions to a statement (e.g., \u201canger and annoyance\u201d, \u201cworthlessness\u201d) On the other hand, the cognitive reactions focus on the lessons some- one could draw, the subsequent actions someone could take, or on the long-term harms that repeated exposure to such statements could have. Examples include \u201cnot wanting to come into work anymore,\u201d \u201cavoiding a particular teacher,\u201d etc. Unique # Avg. # words Statements 11,648 14.34 t x e t n o C Situation Speakers Listeners 23,577 10,683 13,554 6.90 3.11 4.05 s n o i t a n a l p x E Intents Target group Power dynamics Implication Emo. Reaction Cog. Reaction Offensiveness 29,895 11,126 12,766 30,802 28,429 29,826 2,527 14.97 3.48 10.46 19.66 16.82 22.06 2.09 Total # in COBRACORPUS 32,582 Table 2: General data statistics of COBRACORPUS Offensiveness (Off.) captures, in 1-3 words, the type or degree of offensiveness of the statement (e.g., \u201csexism\u201d, \u201coffensive generalization\u201d). We avoid imposing a categorization or cutoff between offensive and harmless statements and instead leave this dimension as free-text, to preserve nuanced in- terpretations of statements and capture the full spec- trum of offensiveness types (Jurgens et al., 2019). 3 Collecting COBRACORPUS To study the contextual dynamics of the offensive- ness of statements at scale, we create COBRACOR- PUS using a three-stage data generation pipeline with human verification, shown in Figure 2. Given that no available corpus contains statements with their contexts and explanations,4 we prompt a large language model (GPT-3.5; Ouyang et al., 2022) to generate contexts and explanations, following (Hartvigsen et al., 2022; West et al., 2022; Kim et al., 2022b,a). Specifically, we first generate mul- tiple plausible contexts for statements, then gen- erate the explanations for each context separately, using GPT-3.5 with in-context examples. Please refer to Appendix C for examples of our prompts. To ensure data quality, we design a set of crowd- sourcing tasks to verify the generated contexts and explanations and collect suggestions. For all tasks, we pre-select crowd workers based on a qualifica- tion task that judged their understanding of each dimension. Please refer to Appendix A for the details of all crowd-sourcing experiments. 3.1 Collecting Statements We draw our statements from Toxigen (Hartvigsen et al., 2022), a dataset of GPT3-generated state- ments that are subtly or implicitly toxic, offensive, 4Note, we do not infer the demographic categories of state- ment authors or readers for ethical reasons (Tatman, 2020). prejudiced, or biased against various demographic groups. Specifically, since we focus on the dynam- ics of offensiveness, we analyze a sample of 13,000 Toxigen statements tagged as \u201coffensive\u201d. 3.2 Generating Likely Contexts Following work demonstrating that LLMs can gen- erate realistic social situations related to majority and minority groups (Park et al., 2022), we use GPT-3.5 to construct plausible or likely contexts (i.e., situation, speaker identity, listener identity) in which a statement could be made. Specifically, we manually curate fifty statement-context pairs, out of which we sample five for each statement as in-context examples. Conditioned on the in-context examples, we then sample three contexts from GPT- 3.5 for each statement. The examples of prompts for plausible context generation are presented in Appendix C. Verifying Contexts We randomly sample 500 statement-context pairs and ask three annotators to rate the plausibility of the contexts (see Appendix A.2 for the exact questions).5 Of the 500 pairs, only 1% were marked as completely implausible or gibberish. 92% of the scenarios were marked as plausible by at least two workers, and some were marked as unlikely but technically plausible (e.g., A mayor in the public saying \u201cBlack people are not humans.\u201d) We retain these contexts since such rare situations could still happen, making them helpful for our analyses and modeling experiments. 3.3 Generating COBRA Explanations Similar to context generation, we make use of GPT-3.5\u2019s ability to produce rich explanations of social commonsense (West et al., 2022) to gener- ate explanations along our seven dimensions. For each context-statement pair, we generate one full COBRA frame, using three randomly sampled in- context examples from our pool of six manually curated prompts. As shown in Table 2, this pro- cess yields a COBRACORPUS containing 32k full (context-statement-explanation) COBRA frames. Verifying Explanations To ensure data quality, we randomly sampled 567 (statement, context, ex- planations) triples and asked three annotators to rate how likely the explanations fit the statements in context. Inspired by prior work (Aguinis et al., 5On this context verification task, the agreement was moderately high, with 75.37% pairwise agreement and free- marginal multi-rater \u03ba=0.507 (Randolph, 2005). Friends Strangers Workplace Family Other more off. 5.28% 43.09% 27.54% 2.85% 21.24% 5.79% 11.38% 6.17% less off. 60.06% 16.6% Table 3: Percentage of contexts occurring under each category/scenario in COBRACORPUS-CF. Row 1 in- dicates statements that are more offensive due to their contexts vs Row 2 indicates those which are lesser of- fensive in comparison 2021; Clark et al., 2021; Liu et al., 2022), we also asked annotators to provide corrections or sugges- tions for those they consider unlikely. 97% of ex- planations were marked as likely by at least two annotators (majority vote) and 84% were"}], "doc_text": "interpretations of statements (e.g., gay men tend to hold more privilege along the gender privilege spectrum, but less along the sexuality one). Impact (Imp.) explain the biased, prejudiced, or stereotypical meaning implied by the statement, similar to Sap et al. (2020). This implication is very closely related to the received meaning from the listener\u2019s or targeted group\u2019s perspective and may differ from the speaker\u2019s intended meaning (e.g., for microaggressions; Sue, 2010). Emotional and Cognitive Reactions (Emo. & Cog.) capture the possible negative effects and harms that the statement and its implied meaning could have on the targeted group. There is an in- creasing push to develop content moderation from the perspective of the harms that content engen- ders (Keller and Leerssen, 2020; Vaccaro et al., 2020). As such, we draw from Nadal et al. (2014) and consider the perceived emotional and cognitive reactions of the target group or listener. The emo- tional reactions capture the short-term emotional effects or reactions to a statement (e.g., \u201canger and annoyance\u201d, \u201cworthlessness\u201d) On the other hand, the cognitive reactions focus on the lessons some- one could draw, the subsequent actions someone could take, or on the long-term harms that repeated exposure to such statements could have. Examples include \u201cnot wanting to come into work anymore,\u201d \u201cavoiding a particular teacher,\u201d etc. Unique # Avg. # words Statements 11,648 14.34 t x e t n o C Situation Speakers Listeners 23,577 10,683 13,554 6.90 3.11 4.05 s n o i t a n a l p x E Intents Target group Power dynamics Implication Emo. Reaction Cog. Reaction Offensiveness 29,895 11,126 12,766 30,802 28,429 29,826 2,527 14.97 3.48 10.46 19.66 16.82 22.06 2.09 Total # in COBRACORPUS 32,582 Table 2: General data statistics of COBRACORPUS Offensiveness (Off.) captures, in 1-3 words, the type or degree of offensiveness of the statement (e.g., \u201csexism\u201d, \u201coffensive generalization\u201d). We avoid imposing a categorization or cutoff between offensive and harmless statements and instead leave this dimension as free-text, to preserve nuanced in- terpretations of statements and capture the full spec- trum of offensiveness types (Jurgens et al., 2019). 3 Collecting COBRACORPUS To study the contextual dynamics of the offensive- ness of statements at scale, we create COBRACOR- PUS using a three-stage data generation pipeline with human verification, shown in Figure 2. Given that no available corpus contains statements with their contexts and explanations,4 we prompt a large language model (GPT-3.5; Ouyang et al., 2022) to generate contexts and explanations, following (Hartvigsen et al., 2022; West et al., 2022; Kim et al., 2022b,a). Specifically, we first generate mul- tiple plausible contexts for statements, then gen- erate the explanations for each context separately, using GPT-3.5 with in-context examples. Please refer to Appendix C for examples of our prompts. To ensure data quality, we design a set of crowd- sourcing tasks to verify the generated contexts and explanations and collect suggestions. For all tasks, we pre-select crowd workers based on a qualifica- tion task that judged their understanding of each dimension. Please refer to Appendix A for the details of all crowd-sourcing experiments. 3.1 Collecting Statements We draw our statements from Toxigen (Hartvigsen et al., 2022), a dataset of GPT3-generated state- ments that are subtly or implicitly toxic, offensive, 4Note, we do not infer the demographic categories of state- ment authors or readers for ethical reasons (Tatman, 2020). prejudiced, or biased against various demographic groups. Specifically, since we focus on the dynam- ics of offensiveness, we analyze a sample of 13,000 Toxigen statements tagged as \u201coffensive\u201d. 3.2 Generating Likely Contexts Following work demonstrating that LLMs can gen- erate realistic social situations related to majority and minority groups (Park et al., 2022), we use GPT-3.5 to construct plausible or likely contexts (i.e., situation, speaker identity, listener identity) in which a statement could be made. Specifically, we manually curate fifty statement-context pairs, out of which we sample five for each statement as in-context examples. Conditioned on the in-context examples, we then sample three contexts from GPT- 3.5 for each statement. The examples of prompts for plausible context generation are presented in Appendix C. Verifying Contexts We randomly sample 500 statement-context pairs and ask three annotators to rate the plausibility of the contexts (see Appendix A.2 for the exact questions).5 Of the 500 pairs, only 1% were marked as completely implausible or gibberish. 92% of the scenarios were marked as plausible by at least two workers, and some were marked as unlikely but technically plausible (e.g., A mayor in the public saying \u201cBlack people are not humans.\u201d) We retain these contexts since such rare situations could still happen, making them helpful for our analyses and modeling experiments. 3.3 Generating COBRA Explanations Similar to context generation, we make use of GPT-3.5\u2019s ability to produce rich explanations of social commonsense (West et al., 2022) to gener- ate explanations along our seven dimensions. For each context-statement pair, we generate one full COBRA frame, using three randomly sampled in- context examples from our pool of six manually curated prompts. As shown in Table 2, this pro- cess yields a COBRACORPUS containing 32k full (context-statement-explanation) COBRA frames. Verifying Explanations To ensure data quality, we randomly sampled 567 (statement, context, ex- planations) triples and asked three annotators to rate how likely the explanations fit the statements in context. Inspired by prior work (Aguinis et al., 5On this context verification task, the agreement was moderately high, with 75.37% pairwise agreement and free- marginal multi-rater \u03ba=0.507 (Randolph, 2005). Friends Strangers Workplace Family Other more off. 5.28% 43.09% 27.54% 2.85% 21.24% 5.79% 11.38% 6.17% less off. 60.06% 16.6% Table 3: Percentage of contexts occurring under each category/scenario in COBRACORPUS-CF. Row 1 in- dicates statements that are more offensive due to their contexts vs Row 2 indicates those which are lesser of- fensive in comparison 2021; Clark et al., 2021; Liu et al., 2022), we also asked annotators to provide corrections or sugges- tions for those they consider unlikely. 97% of ex- planations were marked as likely by at least two annotators (majority vote) and 84% were"}