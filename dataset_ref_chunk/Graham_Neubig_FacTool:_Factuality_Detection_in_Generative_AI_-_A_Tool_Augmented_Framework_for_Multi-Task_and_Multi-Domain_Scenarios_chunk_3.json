{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_FacTool:_Factuality_Detection_in_Generative_AI_-_A_Tool_Augmented_Framework_for_Multi-Task_and_Multi-Domain_Scenarios_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the definition of factuality in math problem solving at the claim level?", "answer": " The extent to which the generated statements adhere to the calculation rules.", "ref_chunk": "in code generation as how well the generated code, as a whole, can be executed correctly within a specific programming language (e.g., Python) and fulfills the provided requirements. This definition is grounded in an execution-based approach to code evaluation, which measures the correctness of gen- erated code by executing it against some test case inputs and comparing its output to the expected output. the claim. On the other hand, the ability of LLMs to utilize multiple tools paves the way for multiple tool-augmented factuality detection. For example, by directly using ChatGPT plugins,3 we can inte- grate multiple tools into a chatbot. The framework is illustrated in Fig. 1, which consists of five main components: claim extraction, query generation, tool querying, evidence collec- tion, and agreement verification. We elaborate each component below. 4.1 Claim Extraction Math Problem Solving The math problem solv- ing task involves the use of automated methods to address mathematical problems (Cobbe et al., 2021). At the claim level, factuality in math prob- lem solving is defined as the extent to which the generated statements adhere to the calculation rules. At the response level, factuality in math problem solving is defined as how effectively the overall mathematical solution addresses the given prob- lem. Extracting claims from responses under various task settings is challenging due to the inconsistent definitions of claims across tasks and scenarios. This inconsistency hinders the development of ap- plications such as text summarization evaluation and factuality detection. To tackle this, we propose an approach in this paper that treats claim extrac- tion as a process guided by LLM prompts based on the specific definition of claims. This approach offers the following advantages: Scientific Literature Review Writing The scien- tific literature review writing task (Jha et al., 2015) aims to analyze and synthesize existing research on a specific topic in a field of study. In this task, we define factuality as whether the generated scientific literature review correctly cites existing scientific literature, including the correct mention of authors and publication years.2 (i) Leveraging the strong instruction-following capabilities of LLMs can significantly reduce the costs associated with data annotation and model training for claim extraction. (ii) When developing a system or constructing a dataset for an application that relies on the def- inition of claims, one simply needs to provide a textual definition of the claim using a large model. This enables future researchers to effectively utilize these definitions as a foundation in their work. 4 Approach We propose a tool-augmented framework for detect- ing factual errors that can apply a unified approach across various tasks. The motivation for using tools is twofold. On one hand, each tool embodies the domain expertise, assisting us in the effective gath- ering of evidence that verifies the correctness of 2In this paper, our focus lies in examining the consistency of the relationship between the paper title, authors, and publi- cation year. However, the task of determining the suitability of the cited paper as the most appropriate choice is left for future investigation. (iii) Our experiments demonstrate that the claim extraction module, implemented by ChatGPT, ex- hibits strong performance in extracting claims (atomic component units). The detailed results of these experiments are discussed in Section 6.1. Here, we employ ChatGPT as a base LLM and apply different textual definitions of claims across four tasks. Our goal is to extract all verifiable claims within the generated text x, denoted as 3https://openai.com/blog/ chatgpt-plugins Knowledge-based QA ClaimExtraction ClaimExtraction (1.1) Is Jack Dorsey the CEO of Twitter?(1.2) Who is the current CEO of Twitter?(2.1) Did Jack Dorsey co-found Twitter in 2006?\u2026\u2026 Queries(1) Quantum Computing in the NISQ era and beyondEvidence (1) 0 1 2 3 4 expected: 0 1 2 3 4(2) 0 expected: 0(3) 0 1 2 3 expected: 0 1 2 3 Claims FacTool Claims(1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} LLM Response Marie ordered 5 packs of milk that costs $3 each, and some boxes of pizza. Marie paid a total of $45. How many boxes of pizza did Marie order if each box costs $3PromptClaims Math Problem Solving (1) string_sequence(4)(2) string_sequence(0)(3) string_sequence(3) Who is the CEO of Twitter?Prompt Code Generation Test Cases Return a string containing space-delimited numbers starting from 0 up to n inclusive.PromptLLM Response Claim-level Factuality: [1] Response-level Factuality: 1Scores Response-level Factuality:1ScoresPrompt Query Generation Query Generation Query Generation Evidence(1.2) Former NBC Universal advertising chief Linda Yaccarino will become\u2026(1.1) Noah Glass, Evan Williams, and Biz Stone co-founded Odeo\u2026\u2026\u2026\u2026 ! (1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} Marie ordered 5 packs of milk that costs $3 each. The total cost of these items is: 5*3 = $15-To find the cost of the boxes of pizza, we subtract the total cost of the other items from the total amount paid by Marie: 45 - 15 = $30-To find the number of boxes of pizza, we divide the remaining amount by the cost of each box: 30 / 3 = 10-Therefore, Marie ordered 10 boxes of pizza. Queries(1) print(round(5*3, 7) == 15)(2) print(round(45-15, 7)==30)(3) print(round(30/3, 7) == 10) Query GenerationExec Results Exec Results(1) True(2) True(3) True Claim-level Factuality: [0, 1, \u2026] Response-level Factuality: 0Scores def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() Discuss the applications and limitations of quantum computing, citing at least one relevant paper. When citing papers, please include the title, the author(s), and the publication year. Response-level Factuality: 1ScoresMath Claims Scientific Literature Review Writing (1) Jack Dorsey is the CEO of Twitter(2) Jack Dorsey co-founded Twitter in 2006\u2026\u2026Queries The CEO of Twitter at the time of writing this answer is Jack Dorsey. He co-founded Twitter in 2006 \u2026\u2026LLM Response Claim Extraction Claim Extraction (1) 5*3 = $15(2) 45 - 15 = $30(3) 30 / 3 = 10 Quantum computing has the potential to revolutionize various fields such as"}, {"question": " How is factuality in math problem solving defined at the response level?", "answer": " How effectively the overall mathematical solution addresses the given problem.", "ref_chunk": "in code generation as how well the generated code, as a whole, can be executed correctly within a specific programming language (e.g., Python) and fulfills the provided requirements. This definition is grounded in an execution-based approach to code evaluation, which measures the correctness of gen- erated code by executing it against some test case inputs and comparing its output to the expected output. the claim. On the other hand, the ability of LLMs to utilize multiple tools paves the way for multiple tool-augmented factuality detection. For example, by directly using ChatGPT plugins,3 we can inte- grate multiple tools into a chatbot. The framework is illustrated in Fig. 1, which consists of five main components: claim extraction, query generation, tool querying, evidence collec- tion, and agreement verification. We elaborate each component below. 4.1 Claim Extraction Math Problem Solving The math problem solv- ing task involves the use of automated methods to address mathematical problems (Cobbe et al., 2021). At the claim level, factuality in math prob- lem solving is defined as the extent to which the generated statements adhere to the calculation rules. At the response level, factuality in math problem solving is defined as how effectively the overall mathematical solution addresses the given prob- lem. Extracting claims from responses under various task settings is challenging due to the inconsistent definitions of claims across tasks and scenarios. This inconsistency hinders the development of ap- plications such as text summarization evaluation and factuality detection. To tackle this, we propose an approach in this paper that treats claim extrac- tion as a process guided by LLM prompts based on the specific definition of claims. This approach offers the following advantages: Scientific Literature Review Writing The scien- tific literature review writing task (Jha et al., 2015) aims to analyze and synthesize existing research on a specific topic in a field of study. In this task, we define factuality as whether the generated scientific literature review correctly cites existing scientific literature, including the correct mention of authors and publication years.2 (i) Leveraging the strong instruction-following capabilities of LLMs can significantly reduce the costs associated with data annotation and model training for claim extraction. (ii) When developing a system or constructing a dataset for an application that relies on the def- inition of claims, one simply needs to provide a textual definition of the claim using a large model. This enables future researchers to effectively utilize these definitions as a foundation in their work. 4 Approach We propose a tool-augmented framework for detect- ing factual errors that can apply a unified approach across various tasks. The motivation for using tools is twofold. On one hand, each tool embodies the domain expertise, assisting us in the effective gath- ering of evidence that verifies the correctness of 2In this paper, our focus lies in examining the consistency of the relationship between the paper title, authors, and publi- cation year. However, the task of determining the suitability of the cited paper as the most appropriate choice is left for future investigation. (iii) Our experiments demonstrate that the claim extraction module, implemented by ChatGPT, ex- hibits strong performance in extracting claims (atomic component units). The detailed results of these experiments are discussed in Section 6.1. Here, we employ ChatGPT as a base LLM and apply different textual definitions of claims across four tasks. Our goal is to extract all verifiable claims within the generated text x, denoted as 3https://openai.com/blog/ chatgpt-plugins Knowledge-based QA ClaimExtraction ClaimExtraction (1.1) Is Jack Dorsey the CEO of Twitter?(1.2) Who is the current CEO of Twitter?(2.1) Did Jack Dorsey co-found Twitter in 2006?\u2026\u2026 Queries(1) Quantum Computing in the NISQ era and beyondEvidence (1) 0 1 2 3 4 expected: 0 1 2 3 4(2) 0 expected: 0(3) 0 1 2 3 expected: 0 1 2 3 Claims FacTool Claims(1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} LLM Response Marie ordered 5 packs of milk that costs $3 each, and some boxes of pizza. Marie paid a total of $45. How many boxes of pizza did Marie order if each box costs $3PromptClaims Math Problem Solving (1) string_sequence(4)(2) string_sequence(0)(3) string_sequence(3) Who is the CEO of Twitter?Prompt Code Generation Test Cases Return a string containing space-delimited numbers starting from 0 up to n inclusive.PromptLLM Response Claim-level Factuality: [1] Response-level Factuality: 1Scores Response-level Factuality:1ScoresPrompt Query Generation Query Generation Query Generation Evidence(1.2) Former NBC Universal advertising chief Linda Yaccarino will become\u2026(1.1) Noah Glass, Evan Williams, and Biz Stone co-founded Odeo\u2026\u2026\u2026\u2026 ! (1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} Marie ordered 5 packs of milk that costs $3 each. The total cost of these items is: 5*3 = $15-To find the cost of the boxes of pizza, we subtract the total cost of the other items from the total amount paid by Marie: 45 - 15 = $30-To find the number of boxes of pizza, we divide the remaining amount by the cost of each box: 30 / 3 = 10-Therefore, Marie ordered 10 boxes of pizza. Queries(1) print(round(5*3, 7) == 15)(2) print(round(45-15, 7)==30)(3) print(round(30/3, 7) == 10) Query GenerationExec Results Exec Results(1) True(2) True(3) True Claim-level Factuality: [0, 1, \u2026] Response-level Factuality: 0Scores def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() Discuss the applications and limitations of quantum computing, citing at least one relevant paper. When citing papers, please include the title, the author(s), and the publication year. Response-level Factuality: 1ScoresMath Claims Scientific Literature Review Writing (1) Jack Dorsey is the CEO of Twitter(2) Jack Dorsey co-founded Twitter in 2006\u2026\u2026Queries The CEO of Twitter at the time of writing this answer is Jack Dorsey. He co-founded Twitter in 2006 \u2026\u2026LLM Response Claim Extraction Claim Extraction (1) 5*3 = $15(2) 45 - 15 = $30(3) 30 / 3 = 10 Quantum computing has the potential to revolutionize various fields such as"}, {"question": " Why is extracting claims from responses challenging in various task settings?", "answer": " Due to inconsistent definitions of claims across tasks and scenarios.", "ref_chunk": "in code generation as how well the generated code, as a whole, can be executed correctly within a specific programming language (e.g., Python) and fulfills the provided requirements. This definition is grounded in an execution-based approach to code evaluation, which measures the correctness of gen- erated code by executing it against some test case inputs and comparing its output to the expected output. the claim. On the other hand, the ability of LLMs to utilize multiple tools paves the way for multiple tool-augmented factuality detection. For example, by directly using ChatGPT plugins,3 we can inte- grate multiple tools into a chatbot. The framework is illustrated in Fig. 1, which consists of five main components: claim extraction, query generation, tool querying, evidence collec- tion, and agreement verification. We elaborate each component below. 4.1 Claim Extraction Math Problem Solving The math problem solv- ing task involves the use of automated methods to address mathematical problems (Cobbe et al., 2021). At the claim level, factuality in math prob- lem solving is defined as the extent to which the generated statements adhere to the calculation rules. At the response level, factuality in math problem solving is defined as how effectively the overall mathematical solution addresses the given prob- lem. Extracting claims from responses under various task settings is challenging due to the inconsistent definitions of claims across tasks and scenarios. This inconsistency hinders the development of ap- plications such as text summarization evaluation and factuality detection. To tackle this, we propose an approach in this paper that treats claim extrac- tion as a process guided by LLM prompts based on the specific definition of claims. This approach offers the following advantages: Scientific Literature Review Writing The scien- tific literature review writing task (Jha et al., 2015) aims to analyze and synthesize existing research on a specific topic in a field of study. In this task, we define factuality as whether the generated scientific literature review correctly cites existing scientific literature, including the correct mention of authors and publication years.2 (i) Leveraging the strong instruction-following capabilities of LLMs can significantly reduce the costs associated with data annotation and model training for claim extraction. (ii) When developing a system or constructing a dataset for an application that relies on the def- inition of claims, one simply needs to provide a textual definition of the claim using a large model. This enables future researchers to effectively utilize these definitions as a foundation in their work. 4 Approach We propose a tool-augmented framework for detect- ing factual errors that can apply a unified approach across various tasks. The motivation for using tools is twofold. On one hand, each tool embodies the domain expertise, assisting us in the effective gath- ering of evidence that verifies the correctness of 2In this paper, our focus lies in examining the consistency of the relationship between the paper title, authors, and publi- cation year. However, the task of determining the suitability of the cited paper as the most appropriate choice is left for future investigation. (iii) Our experiments demonstrate that the claim extraction module, implemented by ChatGPT, ex- hibits strong performance in extracting claims (atomic component units). The detailed results of these experiments are discussed in Section 6.1. Here, we employ ChatGPT as a base LLM and apply different textual definitions of claims across four tasks. Our goal is to extract all verifiable claims within the generated text x, denoted as 3https://openai.com/blog/ chatgpt-plugins Knowledge-based QA ClaimExtraction ClaimExtraction (1.1) Is Jack Dorsey the CEO of Twitter?(1.2) Who is the current CEO of Twitter?(2.1) Did Jack Dorsey co-found Twitter in 2006?\u2026\u2026 Queries(1) Quantum Computing in the NISQ era and beyondEvidence (1) 0 1 2 3 4 expected: 0 1 2 3 4(2) 0 expected: 0(3) 0 1 2 3 expected: 0 1 2 3 Claims FacTool Claims(1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} LLM Response Marie ordered 5 packs of milk that costs $3 each, and some boxes of pizza. Marie paid a total of $45. How many boxes of pizza did Marie order if each box costs $3PromptClaims Math Problem Solving (1) string_sequence(4)(2) string_sequence(0)(3) string_sequence(3) Who is the CEO of Twitter?Prompt Code Generation Test Cases Return a string containing space-delimited numbers starting from 0 up to n inclusive.PromptLLM Response Claim-level Factuality: [1] Response-level Factuality: 1Scores Response-level Factuality:1ScoresPrompt Query Generation Query Generation Query Generation Evidence(1.2) Former NBC Universal advertising chief Linda Yaccarino will become\u2026(1.1) Noah Glass, Evan Williams, and Biz Stone co-founded Odeo\u2026\u2026\u2026\u2026 ! (1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} Marie ordered 5 packs of milk that costs $3 each. The total cost of these items is: 5*3 = $15-To find the cost of the boxes of pizza, we subtract the total cost of the other items from the total amount paid by Marie: 45 - 15 = $30-To find the number of boxes of pizza, we divide the remaining amount by the cost of each box: 30 / 3 = 10-Therefore, Marie ordered 10 boxes of pizza. Queries(1) print(round(5*3, 7) == 15)(2) print(round(45-15, 7)==30)(3) print(round(30/3, 7) == 10) Query GenerationExec Results Exec Results(1) True(2) True(3) True Claim-level Factuality: [0, 1, \u2026] Response-level Factuality: 0Scores def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() Discuss the applications and limitations of quantum computing, citing at least one relevant paper. When citing papers, please include the title, the author(s), and the publication year. Response-level Factuality: 1ScoresMath Claims Scientific Literature Review Writing (1) Jack Dorsey is the CEO of Twitter(2) Jack Dorsey co-founded Twitter in 2006\u2026\u2026Queries The CEO of Twitter at the time of writing this answer is Jack Dorsey. He co-founded Twitter in 2006 \u2026\u2026LLM Response Claim Extraction Claim Extraction (1) 5*3 = $15(2) 45 - 15 = $30(3) 30 / 3 = 10 Quantum computing has the potential to revolutionize various fields such as"}, {"question": " What is the main advantage of leveraging the strong instruction-following capabilities of LLMs for claim extraction?", "answer": " To significantly reduce the costs associated with data annotation and model training.", "ref_chunk": "in code generation as how well the generated code, as a whole, can be executed correctly within a specific programming language (e.g., Python) and fulfills the provided requirements. This definition is grounded in an execution-based approach to code evaluation, which measures the correctness of gen- erated code by executing it against some test case inputs and comparing its output to the expected output. the claim. On the other hand, the ability of LLMs to utilize multiple tools paves the way for multiple tool-augmented factuality detection. For example, by directly using ChatGPT plugins,3 we can inte- grate multiple tools into a chatbot. The framework is illustrated in Fig. 1, which consists of five main components: claim extraction, query generation, tool querying, evidence collec- tion, and agreement verification. We elaborate each component below. 4.1 Claim Extraction Math Problem Solving The math problem solv- ing task involves the use of automated methods to address mathematical problems (Cobbe et al., 2021). At the claim level, factuality in math prob- lem solving is defined as the extent to which the generated statements adhere to the calculation rules. At the response level, factuality in math problem solving is defined as how effectively the overall mathematical solution addresses the given prob- lem. Extracting claims from responses under various task settings is challenging due to the inconsistent definitions of claims across tasks and scenarios. This inconsistency hinders the development of ap- plications such as text summarization evaluation and factuality detection. To tackle this, we propose an approach in this paper that treats claim extrac- tion as a process guided by LLM prompts based on the specific definition of claims. This approach offers the following advantages: Scientific Literature Review Writing The scien- tific literature review writing task (Jha et al., 2015) aims to analyze and synthesize existing research on a specific topic in a field of study. In this task, we define factuality as whether the generated scientific literature review correctly cites existing scientific literature, including the correct mention of authors and publication years.2 (i) Leveraging the strong instruction-following capabilities of LLMs can significantly reduce the costs associated with data annotation and model training for claim extraction. (ii) When developing a system or constructing a dataset for an application that relies on the def- inition of claims, one simply needs to provide a textual definition of the claim using a large model. This enables future researchers to effectively utilize these definitions as a foundation in their work. 4 Approach We propose a tool-augmented framework for detect- ing factual errors that can apply a unified approach across various tasks. The motivation for using tools is twofold. On one hand, each tool embodies the domain expertise, assisting us in the effective gath- ering of evidence that verifies the correctness of 2In this paper, our focus lies in examining the consistency of the relationship between the paper title, authors, and publi- cation year. However, the task of determining the suitability of the cited paper as the most appropriate choice is left for future investigation. (iii) Our experiments demonstrate that the claim extraction module, implemented by ChatGPT, ex- hibits strong performance in extracting claims (atomic component units). The detailed results of these experiments are discussed in Section 6.1. Here, we employ ChatGPT as a base LLM and apply different textual definitions of claims across four tasks. Our goal is to extract all verifiable claims within the generated text x, denoted as 3https://openai.com/blog/ chatgpt-plugins Knowledge-based QA ClaimExtraction ClaimExtraction (1.1) Is Jack Dorsey the CEO of Twitter?(1.2) Who is the current CEO of Twitter?(2.1) Did Jack Dorsey co-found Twitter in 2006?\u2026\u2026 Queries(1) Quantum Computing in the NISQ era and beyondEvidence (1) 0 1 2 3 4 expected: 0 1 2 3 4(2) 0 expected: 0(3) 0 1 2 3 expected: 0 1 2 3 Claims FacTool Claims(1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} LLM Response Marie ordered 5 packs of milk that costs $3 each, and some boxes of pizza. Marie paid a total of $45. How many boxes of pizza did Marie order if each box costs $3PromptClaims Math Problem Solving (1) string_sequence(4)(2) string_sequence(0)(3) string_sequence(3) Who is the CEO of Twitter?Prompt Code Generation Test Cases Return a string containing space-delimited numbers starting from 0 up to n inclusive.PromptLLM Response Claim-level Factuality: [1] Response-level Factuality: 1Scores Response-level Factuality:1ScoresPrompt Query Generation Query Generation Query Generation Evidence(1.2) Former NBC Universal advertising chief Linda Yaccarino will become\u2026(1.1) Noah Glass, Evan Williams, and Biz Stone co-founded Odeo\u2026\u2026\u2026\u2026 ! (1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} Marie ordered 5 packs of milk that costs $3 each. The total cost of these items is: 5*3 = $15-To find the cost of the boxes of pizza, we subtract the total cost of the other items from the total amount paid by Marie: 45 - 15 = $30-To find the number of boxes of pizza, we divide the remaining amount by the cost of each box: 30 / 3 = 10-Therefore, Marie ordered 10 boxes of pizza. Queries(1) print(round(5*3, 7) == 15)(2) print(round(45-15, 7)==30)(3) print(round(30/3, 7) == 10) Query GenerationExec Results Exec Results(1) True(2) True(3) True Claim-level Factuality: [0, 1, \u2026] Response-level Factuality: 0Scores def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() Discuss the applications and limitations of quantum computing, citing at least one relevant paper. When citing papers, please include the title, the author(s), and the publication year. Response-level Factuality: 1ScoresMath Claims Scientific Literature Review Writing (1) Jack Dorsey is the CEO of Twitter(2) Jack Dorsey co-founded Twitter in 2006\u2026\u2026Queries The CEO of Twitter at the time of writing this answer is Jack Dorsey. He co-founded Twitter in 2006 \u2026\u2026LLM Response Claim Extraction Claim Extraction (1) 5*3 = $15(2) 45 - 15 = $30(3) 30 / 3 = 10 Quantum computing has the potential to revolutionize various fields such as"}, {"question": " What is the goal of the scientific literature review writing task?", "answer": " To analyze and synthesize existing research on a specific topic in a field of study.", "ref_chunk": "in code generation as how well the generated code, as a whole, can be executed correctly within a specific programming language (e.g., Python) and fulfills the provided requirements. This definition is grounded in an execution-based approach to code evaluation, which measures the correctness of gen- erated code by executing it against some test case inputs and comparing its output to the expected output. the claim. On the other hand, the ability of LLMs to utilize multiple tools paves the way for multiple tool-augmented factuality detection. For example, by directly using ChatGPT plugins,3 we can inte- grate multiple tools into a chatbot. The framework is illustrated in Fig. 1, which consists of five main components: claim extraction, query generation, tool querying, evidence collec- tion, and agreement verification. We elaborate each component below. 4.1 Claim Extraction Math Problem Solving The math problem solv- ing task involves the use of automated methods to address mathematical problems (Cobbe et al., 2021). At the claim level, factuality in math prob- lem solving is defined as the extent to which the generated statements adhere to the calculation rules. At the response level, factuality in math problem solving is defined as how effectively the overall mathematical solution addresses the given prob- lem. Extracting claims from responses under various task settings is challenging due to the inconsistent definitions of claims across tasks and scenarios. This inconsistency hinders the development of ap- plications such as text summarization evaluation and factuality detection. To tackle this, we propose an approach in this paper that treats claim extrac- tion as a process guided by LLM prompts based on the specific definition of claims. This approach offers the following advantages: Scientific Literature Review Writing The scien- tific literature review writing task (Jha et al., 2015) aims to analyze and synthesize existing research on a specific topic in a field of study. In this task, we define factuality as whether the generated scientific literature review correctly cites existing scientific literature, including the correct mention of authors and publication years.2 (i) Leveraging the strong instruction-following capabilities of LLMs can significantly reduce the costs associated with data annotation and model training for claim extraction. (ii) When developing a system or constructing a dataset for an application that relies on the def- inition of claims, one simply needs to provide a textual definition of the claim using a large model. This enables future researchers to effectively utilize these definitions as a foundation in their work. 4 Approach We propose a tool-augmented framework for detect- ing factual errors that can apply a unified approach across various tasks. The motivation for using tools is twofold. On one hand, each tool embodies the domain expertise, assisting us in the effective gath- ering of evidence that verifies the correctness of 2In this paper, our focus lies in examining the consistency of the relationship between the paper title, authors, and publi- cation year. However, the task of determining the suitability of the cited paper as the most appropriate choice is left for future investigation. (iii) Our experiments demonstrate that the claim extraction module, implemented by ChatGPT, ex- hibits strong performance in extracting claims (atomic component units). The detailed results of these experiments are discussed in Section 6.1. Here, we employ ChatGPT as a base LLM and apply different textual definitions of claims across four tasks. Our goal is to extract all verifiable claims within the generated text x, denoted as 3https://openai.com/blog/ chatgpt-plugins Knowledge-based QA ClaimExtraction ClaimExtraction (1.1) Is Jack Dorsey the CEO of Twitter?(1.2) Who is the current CEO of Twitter?(2.1) Did Jack Dorsey co-found Twitter in 2006?\u2026\u2026 Queries(1) Quantum Computing in the NISQ era and beyondEvidence (1) 0 1 2 3 4 expected: 0 1 2 3 4(2) 0 expected: 0(3) 0 1 2 3 expected: 0 1 2 3 Claims FacTool Claims(1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} LLM Response Marie ordered 5 packs of milk that costs $3 each, and some boxes of pizza. Marie paid a total of $45. How many boxes of pizza did Marie order if each box costs $3PromptClaims Math Problem Solving (1) string_sequence(4)(2) string_sequence(0)(3) string_sequence(3) Who is the CEO of Twitter?Prompt Code Generation Test Cases Return a string containing space-delimited numbers starting from 0 up to n inclusive.PromptLLM Response Claim-level Factuality: [1] Response-level Factuality: 1Scores Response-level Factuality:1ScoresPrompt Query Generation Query Generation Query Generation Evidence(1.2) Former NBC Universal advertising chief Linda Yaccarino will become\u2026(1.1) Noah Glass, Evan Williams, and Biz Stone co-founded Odeo\u2026\u2026\u2026\u2026 ! (1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} Marie ordered 5 packs of milk that costs $3 each. The total cost of these items is: 5*3 = $15-To find the cost of the boxes of pizza, we subtract the total cost of the other items from the total amount paid by Marie: 45 - 15 = $30-To find the number of boxes of pizza, we divide the remaining amount by the cost of each box: 30 / 3 = 10-Therefore, Marie ordered 10 boxes of pizza. Queries(1) print(round(5*3, 7) == 15)(2) print(round(45-15, 7)==30)(3) print(round(30/3, 7) == 10) Query GenerationExec Results Exec Results(1) True(2) True(3) True Claim-level Factuality: [0, 1, \u2026] Response-level Factuality: 0Scores def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() Discuss the applications and limitations of quantum computing, citing at least one relevant paper. When citing papers, please include the title, the author(s), and the publication year. Response-level Factuality: 1ScoresMath Claims Scientific Literature Review Writing (1) Jack Dorsey is the CEO of Twitter(2) Jack Dorsey co-founded Twitter in 2006\u2026\u2026Queries The CEO of Twitter at the time of writing this answer is Jack Dorsey. He co-founded Twitter in 2006 \u2026\u2026LLM Response Claim Extraction Claim Extraction (1) 5*3 = $15(2) 45 - 15 = $30(3) 30 / 3 = 10 Quantum computing has the potential to revolutionize various fields such as"}, {"question": " How is factuality defined in scientific literature review writing?", "answer": " Whether the generated scientific literature review correctly cites existing literature.", "ref_chunk": "in code generation as how well the generated code, as a whole, can be executed correctly within a specific programming language (e.g., Python) and fulfills the provided requirements. This definition is grounded in an execution-based approach to code evaluation, which measures the correctness of gen- erated code by executing it against some test case inputs and comparing its output to the expected output. the claim. On the other hand, the ability of LLMs to utilize multiple tools paves the way for multiple tool-augmented factuality detection. For example, by directly using ChatGPT plugins,3 we can inte- grate multiple tools into a chatbot. The framework is illustrated in Fig. 1, which consists of five main components: claim extraction, query generation, tool querying, evidence collec- tion, and agreement verification. We elaborate each component below. 4.1 Claim Extraction Math Problem Solving The math problem solv- ing task involves the use of automated methods to address mathematical problems (Cobbe et al., 2021). At the claim level, factuality in math prob- lem solving is defined as the extent to which the generated statements adhere to the calculation rules. At the response level, factuality in math problem solving is defined as how effectively the overall mathematical solution addresses the given prob- lem. Extracting claims from responses under various task settings is challenging due to the inconsistent definitions of claims across tasks and scenarios. This inconsistency hinders the development of ap- plications such as text summarization evaluation and factuality detection. To tackle this, we propose an approach in this paper that treats claim extrac- tion as a process guided by LLM prompts based on the specific definition of claims. This approach offers the following advantages: Scientific Literature Review Writing The scien- tific literature review writing task (Jha et al., 2015) aims to analyze and synthesize existing research on a specific topic in a field of study. In this task, we define factuality as whether the generated scientific literature review correctly cites existing scientific literature, including the correct mention of authors and publication years.2 (i) Leveraging the strong instruction-following capabilities of LLMs can significantly reduce the costs associated with data annotation and model training for claim extraction. (ii) When developing a system or constructing a dataset for an application that relies on the def- inition of claims, one simply needs to provide a textual definition of the claim using a large model. This enables future researchers to effectively utilize these definitions as a foundation in their work. 4 Approach We propose a tool-augmented framework for detect- ing factual errors that can apply a unified approach across various tasks. The motivation for using tools is twofold. On one hand, each tool embodies the domain expertise, assisting us in the effective gath- ering of evidence that verifies the correctness of 2In this paper, our focus lies in examining the consistency of the relationship between the paper title, authors, and publi- cation year. However, the task of determining the suitability of the cited paper as the most appropriate choice is left for future investigation. (iii) Our experiments demonstrate that the claim extraction module, implemented by ChatGPT, ex- hibits strong performance in extracting claims (atomic component units). The detailed results of these experiments are discussed in Section 6.1. Here, we employ ChatGPT as a base LLM and apply different textual definitions of claims across four tasks. Our goal is to extract all verifiable claims within the generated text x, denoted as 3https://openai.com/blog/ chatgpt-plugins Knowledge-based QA ClaimExtraction ClaimExtraction (1.1) Is Jack Dorsey the CEO of Twitter?(1.2) Who is the current CEO of Twitter?(2.1) Did Jack Dorsey co-found Twitter in 2006?\u2026\u2026 Queries(1) Quantum Computing in the NISQ era and beyondEvidence (1) 0 1 2 3 4 expected: 0 1 2 3 4(2) 0 expected: 0(3) 0 1 2 3 expected: 0 1 2 3 Claims FacTool Claims(1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} LLM Response Marie ordered 5 packs of milk that costs $3 each, and some boxes of pizza. Marie paid a total of $45. How many boxes of pizza did Marie order if each box costs $3PromptClaims Math Problem Solving (1) string_sequence(4)(2) string_sequence(0)(3) string_sequence(3) Who is the CEO of Twitter?Prompt Code Generation Test Cases Return a string containing space-delimited numbers starting from 0 up to n inclusive.PromptLLM Response Claim-level Factuality: [1] Response-level Factuality: 1Scores Response-level Factuality:1ScoresPrompt Query Generation Query Generation Query Generation Evidence(1.2) Former NBC Universal advertising chief Linda Yaccarino will become\u2026(1.1) Noah Glass, Evan Williams, and Biz Stone co-founded Odeo\u2026\u2026\u2026\u2026 ! (1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} Marie ordered 5 packs of milk that costs $3 each. The total cost of these items is: 5*3 = $15-To find the cost of the boxes of pizza, we subtract the total cost of the other items from the total amount paid by Marie: 45 - 15 = $30-To find the number of boxes of pizza, we divide the remaining amount by the cost of each box: 30 / 3 = 10-Therefore, Marie ordered 10 boxes of pizza. Queries(1) print(round(5*3, 7) == 15)(2) print(round(45-15, 7)==30)(3) print(round(30/3, 7) == 10) Query GenerationExec Results Exec Results(1) True(2) True(3) True Claim-level Factuality: [0, 1, \u2026] Response-level Factuality: 0Scores def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() Discuss the applications and limitations of quantum computing, citing at least one relevant paper. When citing papers, please include the title, the author(s), and the publication year. Response-level Factuality: 1ScoresMath Claims Scientific Literature Review Writing (1) Jack Dorsey is the CEO of Twitter(2) Jack Dorsey co-founded Twitter in 2006\u2026\u2026Queries The CEO of Twitter at the time of writing this answer is Jack Dorsey. He co-founded Twitter in 2006 \u2026\u2026LLM Response Claim Extraction Claim Extraction (1) 5*3 = $15(2) 45 - 15 = $30(3) 30 / 3 = 10 Quantum computing has the potential to revolutionize various fields such as"}, {"question": " What are the five main components of the tool-augmented framework for detecting factual errors?", "answer": " Claim extraction, query generation, tool querying, evidence collection, and agreement verification.", "ref_chunk": "in code generation as how well the generated code, as a whole, can be executed correctly within a specific programming language (e.g., Python) and fulfills the provided requirements. This definition is grounded in an execution-based approach to code evaluation, which measures the correctness of gen- erated code by executing it against some test case inputs and comparing its output to the expected output. the claim. On the other hand, the ability of LLMs to utilize multiple tools paves the way for multiple tool-augmented factuality detection. For example, by directly using ChatGPT plugins,3 we can inte- grate multiple tools into a chatbot. The framework is illustrated in Fig. 1, which consists of five main components: claim extraction, query generation, tool querying, evidence collec- tion, and agreement verification. We elaborate each component below. 4.1 Claim Extraction Math Problem Solving The math problem solv- ing task involves the use of automated methods to address mathematical problems (Cobbe et al., 2021). At the claim level, factuality in math prob- lem solving is defined as the extent to which the generated statements adhere to the calculation rules. At the response level, factuality in math problem solving is defined as how effectively the overall mathematical solution addresses the given prob- lem. Extracting claims from responses under various task settings is challenging due to the inconsistent definitions of claims across tasks and scenarios. This inconsistency hinders the development of ap- plications such as text summarization evaluation and factuality detection. To tackle this, we propose an approach in this paper that treats claim extrac- tion as a process guided by LLM prompts based on the specific definition of claims. This approach offers the following advantages: Scientific Literature Review Writing The scien- tific literature review writing task (Jha et al., 2015) aims to analyze and synthesize existing research on a specific topic in a field of study. In this task, we define factuality as whether the generated scientific literature review correctly cites existing scientific literature, including the correct mention of authors and publication years.2 (i) Leveraging the strong instruction-following capabilities of LLMs can significantly reduce the costs associated with data annotation and model training for claim extraction. (ii) When developing a system or constructing a dataset for an application that relies on the def- inition of claims, one simply needs to provide a textual definition of the claim using a large model. This enables future researchers to effectively utilize these definitions as a foundation in their work. 4 Approach We propose a tool-augmented framework for detect- ing factual errors that can apply a unified approach across various tasks. The motivation for using tools is twofold. On one hand, each tool embodies the domain expertise, assisting us in the effective gath- ering of evidence that verifies the correctness of 2In this paper, our focus lies in examining the consistency of the relationship between the paper title, authors, and publi- cation year. However, the task of determining the suitability of the cited paper as the most appropriate choice is left for future investigation. (iii) Our experiments demonstrate that the claim extraction module, implemented by ChatGPT, ex- hibits strong performance in extracting claims (atomic component units). The detailed results of these experiments are discussed in Section 6.1. Here, we employ ChatGPT as a base LLM and apply different textual definitions of claims across four tasks. Our goal is to extract all verifiable claims within the generated text x, denoted as 3https://openai.com/blog/ chatgpt-plugins Knowledge-based QA ClaimExtraction ClaimExtraction (1.1) Is Jack Dorsey the CEO of Twitter?(1.2) Who is the current CEO of Twitter?(2.1) Did Jack Dorsey co-found Twitter in 2006?\u2026\u2026 Queries(1) Quantum Computing in the NISQ era and beyondEvidence (1) 0 1 2 3 4 expected: 0 1 2 3 4(2) 0 expected: 0(3) 0 1 2 3 expected: 0 1 2 3 Claims FacTool Claims(1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} LLM Response Marie ordered 5 packs of milk that costs $3 each, and some boxes of pizza. Marie paid a total of $45. How many boxes of pizza did Marie order if each box costs $3PromptClaims Math Problem Solving (1) string_sequence(4)(2) string_sequence(0)(3) string_sequence(3) Who is the CEO of Twitter?Prompt Code Generation Test Cases Return a string containing space-delimited numbers starting from 0 up to n inclusive.PromptLLM Response Claim-level Factuality: [1] Response-level Factuality: 1Scores Response-level Factuality:1ScoresPrompt Query Generation Query Generation Query Generation Evidence(1.2) Former NBC Universal advertising chief Linda Yaccarino will become\u2026(1.1) Noah Glass, Evan Williams, and Biz Stone co-founded Odeo\u2026\u2026\u2026\u2026 ! (1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} Marie ordered 5 packs of milk that costs $3 each. The total cost of these items is: 5*3 = $15-To find the cost of the boxes of pizza, we subtract the total cost of the other items from the total amount paid by Marie: 45 - 15 = $30-To find the number of boxes of pizza, we divide the remaining amount by the cost of each box: 30 / 3 = 10-Therefore, Marie ordered 10 boxes of pizza. Queries(1) print(round(5*3, 7) == 15)(2) print(round(45-15, 7)==30)(3) print(round(30/3, 7) == 10) Query GenerationExec Results Exec Results(1) True(2) True(3) True Claim-level Factuality: [0, 1, \u2026] Response-level Factuality: 0Scores def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() Discuss the applications and limitations of quantum computing, citing at least one relevant paper. When citing papers, please include the title, the author(s), and the publication year. Response-level Factuality: 1ScoresMath Claims Scientific Literature Review Writing (1) Jack Dorsey is the CEO of Twitter(2) Jack Dorsey co-founded Twitter in 2006\u2026\u2026Queries The CEO of Twitter at the time of writing this answer is Jack Dorsey. He co-founded Twitter in 2006 \u2026\u2026LLM Response Claim Extraction Claim Extraction (1) 5*3 = $15(2) 45 - 15 = $30(3) 30 / 3 = 10 Quantum computing has the potential to revolutionize various fields such as"}, {"question": " What do the experiments demonstrate about the claim extraction module implemented by ChatGPT?", "answer": " Exhibits strong performance in extracting claims (atomic component units).", "ref_chunk": "in code generation as how well the generated code, as a whole, can be executed correctly within a specific programming language (e.g., Python) and fulfills the provided requirements. This definition is grounded in an execution-based approach to code evaluation, which measures the correctness of gen- erated code by executing it against some test case inputs and comparing its output to the expected output. the claim. On the other hand, the ability of LLMs to utilize multiple tools paves the way for multiple tool-augmented factuality detection. For example, by directly using ChatGPT plugins,3 we can inte- grate multiple tools into a chatbot. The framework is illustrated in Fig. 1, which consists of five main components: claim extraction, query generation, tool querying, evidence collec- tion, and agreement verification. We elaborate each component below. 4.1 Claim Extraction Math Problem Solving The math problem solv- ing task involves the use of automated methods to address mathematical problems (Cobbe et al., 2021). At the claim level, factuality in math prob- lem solving is defined as the extent to which the generated statements adhere to the calculation rules. At the response level, factuality in math problem solving is defined as how effectively the overall mathematical solution addresses the given prob- lem. Extracting claims from responses under various task settings is challenging due to the inconsistent definitions of claims across tasks and scenarios. This inconsistency hinders the development of ap- plications such as text summarization evaluation and factuality detection. To tackle this, we propose an approach in this paper that treats claim extrac- tion as a process guided by LLM prompts based on the specific definition of claims. This approach offers the following advantages: Scientific Literature Review Writing The scien- tific literature review writing task (Jha et al., 2015) aims to analyze and synthesize existing research on a specific topic in a field of study. In this task, we define factuality as whether the generated scientific literature review correctly cites existing scientific literature, including the correct mention of authors and publication years.2 (i) Leveraging the strong instruction-following capabilities of LLMs can significantly reduce the costs associated with data annotation and model training for claim extraction. (ii) When developing a system or constructing a dataset for an application that relies on the def- inition of claims, one simply needs to provide a textual definition of the claim using a large model. This enables future researchers to effectively utilize these definitions as a foundation in their work. 4 Approach We propose a tool-augmented framework for detect- ing factual errors that can apply a unified approach across various tasks. The motivation for using tools is twofold. On one hand, each tool embodies the domain expertise, assisting us in the effective gath- ering of evidence that verifies the correctness of 2In this paper, our focus lies in examining the consistency of the relationship between the paper title, authors, and publi- cation year. However, the task of determining the suitability of the cited paper as the most appropriate choice is left for future investigation. (iii) Our experiments demonstrate that the claim extraction module, implemented by ChatGPT, ex- hibits strong performance in extracting claims (atomic component units). The detailed results of these experiments are discussed in Section 6.1. Here, we employ ChatGPT as a base LLM and apply different textual definitions of claims across four tasks. Our goal is to extract all verifiable claims within the generated text x, denoted as 3https://openai.com/blog/ chatgpt-plugins Knowledge-based QA ClaimExtraction ClaimExtraction (1.1) Is Jack Dorsey the CEO of Twitter?(1.2) Who is the current CEO of Twitter?(2.1) Did Jack Dorsey co-found Twitter in 2006?\u2026\u2026 Queries(1) Quantum Computing in the NISQ era and beyondEvidence (1) 0 1 2 3 4 expected: 0 1 2 3 4(2) 0 expected: 0(3) 0 1 2 3 expected: 0 1 2 3 Claims FacTool Claims(1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} LLM Response Marie ordered 5 packs of milk that costs $3 each, and some boxes of pizza. Marie paid a total of $45. How many boxes of pizza did Marie order if each box costs $3PromptClaims Math Problem Solving (1) string_sequence(4)(2) string_sequence(0)(3) string_sequence(3) Who is the CEO of Twitter?Prompt Code Generation Test Cases Return a string containing space-delimited numbers starting from 0 up to n inclusive.PromptLLM Response Claim-level Factuality: [1] Response-level Factuality: 1Scores Response-level Factuality:1ScoresPrompt Query Generation Query Generation Query Generation Evidence(1.2) Former NBC Universal advertising chief Linda Yaccarino will become\u2026(1.1) Noah Glass, Evan Williams, and Biz Stone co-founded Odeo\u2026\u2026\u2026\u2026 ! (1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} Marie ordered 5 packs of milk that costs $3 each. The total cost of these items is: 5*3 = $15-To find the cost of the boxes of pizza, we subtract the total cost of the other items from the total amount paid by Marie: 45 - 15 = $30-To find the number of boxes of pizza, we divide the remaining amount by the cost of each box: 30 / 3 = 10-Therefore, Marie ordered 10 boxes of pizza. Queries(1) print(round(5*3, 7) == 15)(2) print(round(45-15, 7)==30)(3) print(round(30/3, 7) == 10) Query GenerationExec Results Exec Results(1) True(2) True(3) True Claim-level Factuality: [0, 1, \u2026] Response-level Factuality: 0Scores def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() Discuss the applications and limitations of quantum computing, citing at least one relevant paper. When citing papers, please include the title, the author(s), and the publication year. Response-level Factuality: 1ScoresMath Claims Scientific Literature Review Writing (1) Jack Dorsey is the CEO of Twitter(2) Jack Dorsey co-founded Twitter in 2006\u2026\u2026Queries The CEO of Twitter at the time of writing this answer is Jack Dorsey. He co-founded Twitter in 2006 \u2026\u2026LLM Response Claim Extraction Claim Extraction (1) 5*3 = $15(2) 45 - 15 = $30(3) 30 / 3 = 10 Quantum computing has the potential to revolutionize various fields such as"}, {"question": " What does the approach proposed in the paper treat claim extraction as a process guided by?", "answer": " LLM prompts based on the specific definition of claims.", "ref_chunk": "in code generation as how well the generated code, as a whole, can be executed correctly within a specific programming language (e.g., Python) and fulfills the provided requirements. This definition is grounded in an execution-based approach to code evaluation, which measures the correctness of gen- erated code by executing it against some test case inputs and comparing its output to the expected output. the claim. On the other hand, the ability of LLMs to utilize multiple tools paves the way for multiple tool-augmented factuality detection. For example, by directly using ChatGPT plugins,3 we can inte- grate multiple tools into a chatbot. The framework is illustrated in Fig. 1, which consists of five main components: claim extraction, query generation, tool querying, evidence collec- tion, and agreement verification. We elaborate each component below. 4.1 Claim Extraction Math Problem Solving The math problem solv- ing task involves the use of automated methods to address mathematical problems (Cobbe et al., 2021). At the claim level, factuality in math prob- lem solving is defined as the extent to which the generated statements adhere to the calculation rules. At the response level, factuality in math problem solving is defined as how effectively the overall mathematical solution addresses the given prob- lem. Extracting claims from responses under various task settings is challenging due to the inconsistent definitions of claims across tasks and scenarios. This inconsistency hinders the development of ap- plications such as text summarization evaluation and factuality detection. To tackle this, we propose an approach in this paper that treats claim extrac- tion as a process guided by LLM prompts based on the specific definition of claims. This approach offers the following advantages: Scientific Literature Review Writing The scien- tific literature review writing task (Jha et al., 2015) aims to analyze and synthesize existing research on a specific topic in a field of study. In this task, we define factuality as whether the generated scientific literature review correctly cites existing scientific literature, including the correct mention of authors and publication years.2 (i) Leveraging the strong instruction-following capabilities of LLMs can significantly reduce the costs associated with data annotation and model training for claim extraction. (ii) When developing a system or constructing a dataset for an application that relies on the def- inition of claims, one simply needs to provide a textual definition of the claim using a large model. This enables future researchers to effectively utilize these definitions as a foundation in their work. 4 Approach We propose a tool-augmented framework for detect- ing factual errors that can apply a unified approach across various tasks. The motivation for using tools is twofold. On one hand, each tool embodies the domain expertise, assisting us in the effective gath- ering of evidence that verifies the correctness of 2In this paper, our focus lies in examining the consistency of the relationship between the paper title, authors, and publi- cation year. However, the task of determining the suitability of the cited paper as the most appropriate choice is left for future investigation. (iii) Our experiments demonstrate that the claim extraction module, implemented by ChatGPT, ex- hibits strong performance in extracting claims (atomic component units). The detailed results of these experiments are discussed in Section 6.1. Here, we employ ChatGPT as a base LLM and apply different textual definitions of claims across four tasks. Our goal is to extract all verifiable claims within the generated text x, denoted as 3https://openai.com/blog/ chatgpt-plugins Knowledge-based QA ClaimExtraction ClaimExtraction (1.1) Is Jack Dorsey the CEO of Twitter?(1.2) Who is the current CEO of Twitter?(2.1) Did Jack Dorsey co-found Twitter in 2006?\u2026\u2026 Queries(1) Quantum Computing in the NISQ era and beyondEvidence (1) 0 1 2 3 4 expected: 0 1 2 3 4(2) 0 expected: 0(3) 0 1 2 3 expected: 0 1 2 3 Claims FacTool Claims(1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} LLM Response Marie ordered 5 packs of milk that costs $3 each, and some boxes of pizza. Marie paid a total of $45. How many boxes of pizza did Marie order if each box costs $3PromptClaims Math Problem Solving (1) string_sequence(4)(2) string_sequence(0)(3) string_sequence(3) Who is the CEO of Twitter?Prompt Code Generation Test Cases Return a string containing space-delimited numbers starting from 0 up to n inclusive.PromptLLM Response Claim-level Factuality: [1] Response-level Factuality: 1Scores Response-level Factuality:1ScoresPrompt Query Generation Query Generation Query Generation Evidence(1.2) Former NBC Universal advertising chief Linda Yaccarino will become\u2026(1.1) Noah Glass, Evan Williams, and Biz Stone co-founded Odeo\u2026\u2026\u2026\u2026 ! (1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} Marie ordered 5 packs of milk that costs $3 each. The total cost of these items is: 5*3 = $15-To find the cost of the boxes of pizza, we subtract the total cost of the other items from the total amount paid by Marie: 45 - 15 = $30-To find the number of boxes of pizza, we divide the remaining amount by the cost of each box: 30 / 3 = 10-Therefore, Marie ordered 10 boxes of pizza. Queries(1) print(round(5*3, 7) == 15)(2) print(round(45-15, 7)==30)(3) print(round(30/3, 7) == 10) Query GenerationExec Results Exec Results(1) True(2) True(3) True Claim-level Factuality: [0, 1, \u2026] Response-level Factuality: 0Scores def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() Discuss the applications and limitations of quantum computing, citing at least one relevant paper. When citing papers, please include the title, the author(s), and the publication year. Response-level Factuality: 1ScoresMath Claims Scientific Literature Review Writing (1) Jack Dorsey is the CEO of Twitter(2) Jack Dorsey co-founded Twitter in 2006\u2026\u2026Queries The CEO of Twitter at the time of writing this answer is Jack Dorsey. He co-founded Twitter in 2006 \u2026\u2026LLM Response Claim Extraction Claim Extraction (1) 5*3 = $15(2) 45 - 15 = $30(3) 30 / 3 = 10 Quantum computing has the potential to revolutionize various fields such as"}, {"question": " What is the main motivation for using tools in the tool-augmented framework for detecting factual errors?", "answer": " To apply a unified approach across various tasks and gather evidence effectively.", "ref_chunk": "in code generation as how well the generated code, as a whole, can be executed correctly within a specific programming language (e.g., Python) and fulfills the provided requirements. This definition is grounded in an execution-based approach to code evaluation, which measures the correctness of gen- erated code by executing it against some test case inputs and comparing its output to the expected output. the claim. On the other hand, the ability of LLMs to utilize multiple tools paves the way for multiple tool-augmented factuality detection. For example, by directly using ChatGPT plugins,3 we can inte- grate multiple tools into a chatbot. The framework is illustrated in Fig. 1, which consists of five main components: claim extraction, query generation, tool querying, evidence collec- tion, and agreement verification. We elaborate each component below. 4.1 Claim Extraction Math Problem Solving The math problem solv- ing task involves the use of automated methods to address mathematical problems (Cobbe et al., 2021). At the claim level, factuality in math prob- lem solving is defined as the extent to which the generated statements adhere to the calculation rules. At the response level, factuality in math problem solving is defined as how effectively the overall mathematical solution addresses the given prob- lem. Extracting claims from responses under various task settings is challenging due to the inconsistent definitions of claims across tasks and scenarios. This inconsistency hinders the development of ap- plications such as text summarization evaluation and factuality detection. To tackle this, we propose an approach in this paper that treats claim extrac- tion as a process guided by LLM prompts based on the specific definition of claims. This approach offers the following advantages: Scientific Literature Review Writing The scien- tific literature review writing task (Jha et al., 2015) aims to analyze and synthesize existing research on a specific topic in a field of study. In this task, we define factuality as whether the generated scientific literature review correctly cites existing scientific literature, including the correct mention of authors and publication years.2 (i) Leveraging the strong instruction-following capabilities of LLMs can significantly reduce the costs associated with data annotation and model training for claim extraction. (ii) When developing a system or constructing a dataset for an application that relies on the def- inition of claims, one simply needs to provide a textual definition of the claim using a large model. This enables future researchers to effectively utilize these definitions as a foundation in their work. 4 Approach We propose a tool-augmented framework for detect- ing factual errors that can apply a unified approach across various tasks. The motivation for using tools is twofold. On one hand, each tool embodies the domain expertise, assisting us in the effective gath- ering of evidence that verifies the correctness of 2In this paper, our focus lies in examining the consistency of the relationship between the paper title, authors, and publi- cation year. However, the task of determining the suitability of the cited paper as the most appropriate choice is left for future investigation. (iii) Our experiments demonstrate that the claim extraction module, implemented by ChatGPT, ex- hibits strong performance in extracting claims (atomic component units). The detailed results of these experiments are discussed in Section 6.1. Here, we employ ChatGPT as a base LLM and apply different textual definitions of claims across four tasks. Our goal is to extract all verifiable claims within the generated text x, denoted as 3https://openai.com/blog/ chatgpt-plugins Knowledge-based QA ClaimExtraction ClaimExtraction (1.1) Is Jack Dorsey the CEO of Twitter?(1.2) Who is the current CEO of Twitter?(2.1) Did Jack Dorsey co-found Twitter in 2006?\u2026\u2026 Queries(1) Quantum Computing in the NISQ era and beyondEvidence (1) 0 1 2 3 4 expected: 0 1 2 3 4(2) 0 expected: 0(3) 0 1 2 3 expected: 0 1 2 3 Claims FacTool Claims(1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} LLM Response Marie ordered 5 packs of milk that costs $3 each, and some boxes of pizza. Marie paid a total of $45. How many boxes of pizza did Marie order if each box costs $3PromptClaims Math Problem Solving (1) string_sequence(4)(2) string_sequence(0)(3) string_sequence(3) Who is the CEO of Twitter?Prompt Code Generation Test Cases Return a string containing space-delimited numbers starting from 0 up to n inclusive.PromptLLM Response Claim-level Factuality: [1] Response-level Factuality: 1Scores Response-level Factuality:1ScoresPrompt Query Generation Query Generation Query Generation Evidence(1.2) Former NBC Universal advertising chief Linda Yaccarino will become\u2026(1.1) Noah Glass, Evan Williams, and Biz Stone co-founded Odeo\u2026\u2026\u2026\u2026 ! (1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} Marie ordered 5 packs of milk that costs $3 each. The total cost of these items is: 5*3 = $15-To find the cost of the boxes of pizza, we subtract the total cost of the other items from the total amount paid by Marie: 45 - 15 = $30-To find the number of boxes of pizza, we divide the remaining amount by the cost of each box: 30 / 3 = 10-Therefore, Marie ordered 10 boxes of pizza. Queries(1) print(round(5*3, 7) == 15)(2) print(round(45-15, 7)==30)(3) print(round(30/3, 7) == 10) Query GenerationExec Results Exec Results(1) True(2) True(3) True Claim-level Factuality: [0, 1, \u2026] Response-level Factuality: 0Scores def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() Discuss the applications and limitations of quantum computing, citing at least one relevant paper. When citing papers, please include the title, the author(s), and the publication year. Response-level Factuality: 1ScoresMath Claims Scientific Literature Review Writing (1) Jack Dorsey is the CEO of Twitter(2) Jack Dorsey co-founded Twitter in 2006\u2026\u2026Queries The CEO of Twitter at the time of writing this answer is Jack Dorsey. He co-founded Twitter in 2006 \u2026\u2026LLM Response Claim Extraction Claim Extraction (1) 5*3 = $15(2) 45 - 15 = $30(3) 30 / 3 = 10 Quantum computing has the potential to revolutionize various fields such as"}], "doc_text": "in code generation as how well the generated code, as a whole, can be executed correctly within a specific programming language (e.g., Python) and fulfills the provided requirements. This definition is grounded in an execution-based approach to code evaluation, which measures the correctness of gen- erated code by executing it against some test case inputs and comparing its output to the expected output. the claim. On the other hand, the ability of LLMs to utilize multiple tools paves the way for multiple tool-augmented factuality detection. For example, by directly using ChatGPT plugins,3 we can inte- grate multiple tools into a chatbot. The framework is illustrated in Fig. 1, which consists of five main components: claim extraction, query generation, tool querying, evidence collec- tion, and agreement verification. We elaborate each component below. 4.1 Claim Extraction Math Problem Solving The math problem solv- ing task involves the use of automated methods to address mathematical problems (Cobbe et al., 2021). At the claim level, factuality in math prob- lem solving is defined as the extent to which the generated statements adhere to the calculation rules. At the response level, factuality in math problem solving is defined as how effectively the overall mathematical solution addresses the given prob- lem. Extracting claims from responses under various task settings is challenging due to the inconsistent definitions of claims across tasks and scenarios. This inconsistency hinders the development of ap- plications such as text summarization evaluation and factuality detection. To tackle this, we propose an approach in this paper that treats claim extrac- tion as a process guided by LLM prompts based on the specific definition of claims. This approach offers the following advantages: Scientific Literature Review Writing The scien- tific literature review writing task (Jha et al., 2015) aims to analyze and synthesize existing research on a specific topic in a field of study. In this task, we define factuality as whether the generated scientific literature review correctly cites existing scientific literature, including the correct mention of authors and publication years.2 (i) Leveraging the strong instruction-following capabilities of LLMs can significantly reduce the costs associated with data annotation and model training for claim extraction. (ii) When developing a system or constructing a dataset for an application that relies on the def- inition of claims, one simply needs to provide a textual definition of the claim using a large model. This enables future researchers to effectively utilize these definitions as a foundation in their work. 4 Approach We propose a tool-augmented framework for detect- ing factual errors that can apply a unified approach across various tasks. The motivation for using tools is twofold. On one hand, each tool embodies the domain expertise, assisting us in the effective gath- ering of evidence that verifies the correctness of 2In this paper, our focus lies in examining the consistency of the relationship between the paper title, authors, and publi- cation year. However, the task of determining the suitability of the cited paper as the most appropriate choice is left for future investigation. (iii) Our experiments demonstrate that the claim extraction module, implemented by ChatGPT, ex- hibits strong performance in extracting claims (atomic component units). The detailed results of these experiments are discussed in Section 6.1. Here, we employ ChatGPT as a base LLM and apply different textual definitions of claims across four tasks. Our goal is to extract all verifiable claims within the generated text x, denoted as 3https://openai.com/blog/ chatgpt-plugins Knowledge-based QA ClaimExtraction ClaimExtraction (1.1) Is Jack Dorsey the CEO of Twitter?(1.2) Who is the current CEO of Twitter?(2.1) Did Jack Dorsey co-found Twitter in 2006?\u2026\u2026 Queries(1) Quantum Computing in the NISQ era and beyondEvidence (1) 0 1 2 3 4 expected: 0 1 2 3 4(2) 0 expected: 0(3) 0 1 2 3 expected: 0 1 2 3 Claims FacTool Claims(1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} LLM Response Marie ordered 5 packs of milk that costs $3 each, and some boxes of pizza. Marie paid a total of $45. How many boxes of pizza did Marie order if each box costs $3PromptClaims Math Problem Solving (1) string_sequence(4)(2) string_sequence(0)(3) string_sequence(3) Who is the CEO of Twitter?Prompt Code Generation Test Cases Return a string containing space-delimited numbers starting from 0 up to n inclusive.PromptLLM Response Claim-level Factuality: [1] Response-level Factuality: 1Scores Response-level Factuality:1ScoresPrompt Query Generation Query Generation Query Generation Evidence(1.2) Former NBC Universal advertising chief Linda Yaccarino will become\u2026(1.1) Noah Glass, Evan Williams, and Biz Stone co-founded Odeo\u2026\u2026\u2026\u2026 ! (1) {title: Quantum Computing in the NISQ era and beyond, authors: John Preskill, publication_year: 2018} Marie ordered 5 packs of milk that costs $3 each. The total cost of these items is: 5*3 = $15-To find the cost of the boxes of pizza, we subtract the total cost of the other items from the total amount paid by Marie: 45 - 15 = $30-To find the number of boxes of pizza, we divide the remaining amount by the cost of each box: 30 / 3 = 10-Therefore, Marie ordered 10 boxes of pizza. Queries(1) print(round(5*3, 7) == 15)(2) print(round(45-15, 7)==30)(3) print(round(30/3, 7) == 10) Query GenerationExec Results Exec Results(1) True(2) True(3) True Claim-level Factuality: [0, 1, \u2026] Response-level Factuality: 0Scores def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() def string_sequence(n): result = \"\" for i in range(n+1): result += str(i) + \" \" return result.strip() Discuss the applications and limitations of quantum computing, citing at least one relevant paper. When citing papers, please include the title, the author(s), and the publication year. Response-level Factuality: 1ScoresMath Claims Scientific Literature Review Writing (1) Jack Dorsey is the CEO of Twitter(2) Jack Dorsey co-founded Twitter in 2006\u2026\u2026Queries The CEO of Twitter at the time of writing this answer is Jack Dorsey. He co-founded Twitter in 2006 \u2026\u2026LLM Response Claim Extraction Claim Extraction (1) 5*3 = $15(2) 45 - 15 = $30(3) 30 / 3 = 10 Quantum computing has the potential to revolutionize various fields such as"}