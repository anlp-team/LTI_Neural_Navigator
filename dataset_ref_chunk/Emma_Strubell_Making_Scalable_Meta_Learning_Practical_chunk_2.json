{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Making_Scalable_Meta_Learning_Practical_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the acronym for the Scalable Meta Learning Algorithm developed in the text?,answer: SAMA", "ref_chunk": "[35]. Combining all these solutions, we develop SAMA, a holistic and practically ScalAble Meta learning Algorithm. 3. We evaluate the scalability and the overall performance of SAMA on a multitude of large-scale meta learning benchmarks involving large language models (e.g., BERT [31] and RoBERTa [39]) or large datasets (e.g., ImageNet-1k [10]). Notably, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. In addition, we observe that SAMA- based data optimization consistently leads to improvements in text classification accuracy with large language models, and achieves state-of-the-art results in both small-/large-scale data pruning, demonstrating the initial potential of scalable meta learning. 2 2 Background: Gradient-Based Meta Learning We begin by reviewing the basics of (gradient-based) meta learning in order to establish the key aspects that have limited its scalability. Mathematically, meta learning is commonly formulated as bilevel optimization as follows: \u03bb\u2217 = argmin Lmeta(Dmeta; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin Lbase(Dbase; \u03b8, \u03bb) \u03b8 where \u03bb (respectively, \u03b8) are the parameters of meta (base) learners, Dmeta (Dbase) are meta (base) datasets, and Lmeta (Lbase) are meta (base) loss functions. An important implication of the above formulation is that meta learning changes the task of finding the optimal inductive biases from designing heuristics to designing meta optimization problems. As an example, consider the problem of finding the optimal inductive bias for fair classification given class-imbalanced training data. A traditional approach to this problem is to use a heuristic that reweights training samples inversely proportional to class frequencies. On the contrary, L2RW [54] designs a meta optimization problem by curating a small number of class-balanced data for the meta dataset Dmeta and setting the meta learner \u03bb to be importance weights for all training data. In short, unlike heuristic-based methods that explicitly specify \u201chow to learn,\u201d meta learning methods only specify \u201cwhat to learn\u201d and let the meta learner automatically determine \u201chow.\u201d From a programming paradigm perspective, such a difference can be understood as a transition from imperative to declarative programming. While there are multiple approaches to solving meta learning, in this work we focus on gradient- based approaches due to their ability to efficiently solve high-dimensional meta optimization (i.e. dim(\u03bb) \u226b 1) problems. Such an ability is essential given that the search space for inductive biases (e.g. importance weights for training data) can increase exponentially with recent large models and datasets. Concretely, GBML computes a meta gradient composed of two terms\u2014the best-response Jacobian and direct gradient\u2014with the chain rule, as follows: \u2202\u03b8\u2217 \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) best-response Jacobian Since the direct gradient (teal) computation is straightforward with the underlying automatic dif- ferentiation library, the major challenge in GBML lies in computing the best-response Jacobian (purple), of which two common solutions are iterative differentiation [14, 15, 16, 42] and implicit differentiation [25, 40, 49, 51]. Between these two, in this paper we adopt implicit differentiation as our baseline solution to GBML, as it achieves better computation and memory efficiency than iterative differentiation [19], both of which are vital in accomplishing our goal of scalable GBML. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 (cid:124) (cid:123)(cid:122) (cid:125) direct gradient = The gist of implicit differentiation is that it calculates the best-response Jacobian by leveraging Cauchy\u2019s Implicit Function Theorem (IFT) and re-interpreting the base optimization problem from the perspective of fixed-point iteration given an iterative solver u, as follows: \u2202\u03b8\u2217 \u2202\u03bb \u2202u \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) meta Jacobian = \u2212 (cid:18) \u2202u \u2202\u03b8\u2217 (cid:124)(cid:123)(cid:122)(cid:125) base Jacobian (cid:19)\u22121 where (cid:40) \u03b8\u2217 = lim t\u2192\u221e \u03b8t \u03b8t = \u03b8t\u22121 \u2212 u(\u03b8t\u22121; \u03bb) While exact implicit differentiation requires solving the base optimization problem to convergence \u03b8\u2217 by repeatedly applying an iterative solver u (e.g., SGD or Adam) to calculate base (blue) and meta (red) Jacobians, this is computationally impractical, especially in most large-scale learning settings. Therefore, researchers oftentimes approximate \u03b8\u2217 with a small number of unrolled update steps of u. This results in a solution that alternates gradient descent between base and meta optimization problems, where the base gradient is calculated with standard backpropagation and the meta gradient with Eqs. (1) & (2). Noting that many techniques have been developed to efficiently perform and scale up standard backpropagation, we deduce that the major challenges in scaling GBML lie in meta gradient computation, which will be discussed in depth in the next section. 3 Scaling Meta Learning It has long been recognized that meta gradient computation in GBML suffers from a substantial compute/memory cost [40, 51], algorithmic instability [2, 12], and a lack of efficient distributed 3 (1) (2) training support [3, 6, 9], all of which can significantly limit its scalability. In this section, we first attempt to understand the above limitations at a technical level. Toward this end, we investigate three aspects in Eqs. (1) & (2), namely (i) base Jacobian inversion, (ii) algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of meta gradient computation, and discuss how they lead to the aforementioned limitations. Next, we propose initial solutions for each of these issues, based on which we build a holistic and ScalAble Meta Learning Algorithm, SAMA. 3.1 Base Jacobian Inverse Problem Denoting the size of the base learner (i.e., dim(\u03b8)) as nb, the computational complexity of the naive base Jacobian (blue) inversion in Eq. (2) is O(n3 b). Since such cubic complexity is impracti- cal even for small-sized base learners, practitioners typically utilize various linear systems techniques such as Neumann series [40] or conjugate gradient [51], and directly approximate (cid:0) \u2202u (cid:1)\u22121 \u2202Lmeta . \u2202\u03b8\u2217 Given that the base Jacobian is a function of the Hessian matrix of the base optimization problem in GBML, these algorithms solve linear systems by iteratively performing Hessian-vector products. However, this Hessian-vector product computation contributes negatively to all three above limita- tions. First, while many automatic differentiation engines provide an efficient Hessian-vector product implementation like Pearlmutter\u2019s algorithm [48], its memory/compute cost is still prohibitive with larger models, as demonstrated in Fig. 1. Second, in most cases, we only have"}, {"question": " What kind of benchmarks were used to evaluate the performance of SAMA?,answer: Large-scale meta learning benchmarks involving large language models or large datasets", "ref_chunk": "[35]. Combining all these solutions, we develop SAMA, a holistic and practically ScalAble Meta learning Algorithm. 3. We evaluate the scalability and the overall performance of SAMA on a multitude of large-scale meta learning benchmarks involving large language models (e.g., BERT [31] and RoBERTa [39]) or large datasets (e.g., ImageNet-1k [10]). Notably, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. In addition, we observe that SAMA- based data optimization consistently leads to improvements in text classification accuracy with large language models, and achieves state-of-the-art results in both small-/large-scale data pruning, demonstrating the initial potential of scalable meta learning. 2 2 Background: Gradient-Based Meta Learning We begin by reviewing the basics of (gradient-based) meta learning in order to establish the key aspects that have limited its scalability. Mathematically, meta learning is commonly formulated as bilevel optimization as follows: \u03bb\u2217 = argmin Lmeta(Dmeta; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin Lbase(Dbase; \u03b8, \u03bb) \u03b8 where \u03bb (respectively, \u03b8) are the parameters of meta (base) learners, Dmeta (Dbase) are meta (base) datasets, and Lmeta (Lbase) are meta (base) loss functions. An important implication of the above formulation is that meta learning changes the task of finding the optimal inductive biases from designing heuristics to designing meta optimization problems. As an example, consider the problem of finding the optimal inductive bias for fair classification given class-imbalanced training data. A traditional approach to this problem is to use a heuristic that reweights training samples inversely proportional to class frequencies. On the contrary, L2RW [54] designs a meta optimization problem by curating a small number of class-balanced data for the meta dataset Dmeta and setting the meta learner \u03bb to be importance weights for all training data. In short, unlike heuristic-based methods that explicitly specify \u201chow to learn,\u201d meta learning methods only specify \u201cwhat to learn\u201d and let the meta learner automatically determine \u201chow.\u201d From a programming paradigm perspective, such a difference can be understood as a transition from imperative to declarative programming. While there are multiple approaches to solving meta learning, in this work we focus on gradient- based approaches due to their ability to efficiently solve high-dimensional meta optimization (i.e. dim(\u03bb) \u226b 1) problems. Such an ability is essential given that the search space for inductive biases (e.g. importance weights for training data) can increase exponentially with recent large models and datasets. Concretely, GBML computes a meta gradient composed of two terms\u2014the best-response Jacobian and direct gradient\u2014with the chain rule, as follows: \u2202\u03b8\u2217 \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) best-response Jacobian Since the direct gradient (teal) computation is straightforward with the underlying automatic dif- ferentiation library, the major challenge in GBML lies in computing the best-response Jacobian (purple), of which two common solutions are iterative differentiation [14, 15, 16, 42] and implicit differentiation [25, 40, 49, 51]. Between these two, in this paper we adopt implicit differentiation as our baseline solution to GBML, as it achieves better computation and memory efficiency than iterative differentiation [19], both of which are vital in accomplishing our goal of scalable GBML. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 (cid:124) (cid:123)(cid:122) (cid:125) direct gradient = The gist of implicit differentiation is that it calculates the best-response Jacobian by leveraging Cauchy\u2019s Implicit Function Theorem (IFT) and re-interpreting the base optimization problem from the perspective of fixed-point iteration given an iterative solver u, as follows: \u2202\u03b8\u2217 \u2202\u03bb \u2202u \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) meta Jacobian = \u2212 (cid:18) \u2202u \u2202\u03b8\u2217 (cid:124)(cid:123)(cid:122)(cid:125) base Jacobian (cid:19)\u22121 where (cid:40) \u03b8\u2217 = lim t\u2192\u221e \u03b8t \u03b8t = \u03b8t\u22121 \u2212 u(\u03b8t\u22121; \u03bb) While exact implicit differentiation requires solving the base optimization problem to convergence \u03b8\u2217 by repeatedly applying an iterative solver u (e.g., SGD or Adam) to calculate base (blue) and meta (red) Jacobians, this is computationally impractical, especially in most large-scale learning settings. Therefore, researchers oftentimes approximate \u03b8\u2217 with a small number of unrolled update steps of u. This results in a solution that alternates gradient descent between base and meta optimization problems, where the base gradient is calculated with standard backpropagation and the meta gradient with Eqs. (1) & (2). Noting that many techniques have been developed to efficiently perform and scale up standard backpropagation, we deduce that the major challenges in scaling GBML lie in meta gradient computation, which will be discussed in depth in the next section. 3 Scaling Meta Learning It has long been recognized that meta gradient computation in GBML suffers from a substantial compute/memory cost [40, 51], algorithmic instability [2, 12], and a lack of efficient distributed 3 (1) (2) training support [3, 6, 9], all of which can significantly limit its scalability. In this section, we first attempt to understand the above limitations at a technical level. Toward this end, we investigate three aspects in Eqs. (1) & (2), namely (i) base Jacobian inversion, (ii) algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of meta gradient computation, and discuss how they lead to the aforementioned limitations. Next, we propose initial solutions for each of these issues, based on which we build a holistic and ScalAble Meta Learning Algorithm, SAMA. 3.1 Base Jacobian Inverse Problem Denoting the size of the base learner (i.e., dim(\u03b8)) as nb, the computational complexity of the naive base Jacobian (blue) inversion in Eq. (2) is O(n3 b). Since such cubic complexity is impracti- cal even for small-sized base learners, practitioners typically utilize various linear systems techniques such as Neumann series [40] or conjugate gradient [51], and directly approximate (cid:0) \u2202u (cid:1)\u22121 \u2202Lmeta . \u2202\u03b8\u2217 Given that the base Jacobian is a function of the Hessian matrix of the base optimization problem in GBML, these algorithms solve linear systems by iteratively performing Hessian-vector products. However, this Hessian-vector product computation contributes negatively to all three above limita- tions. First, while many automatic differentiation engines provide an efficient Hessian-vector product implementation like Pearlmutter\u2019s algorithm [48], its memory/compute cost is still prohibitive with larger models, as demonstrated in Fig. 1. Second, in most cases, we only have"}, {"question": " What increase in throughput does SAMA showcase on single-GPU setups compared to other baseline meta learning algorithms?,answer: Up to 1.7\u00d7 increase in throughput", "ref_chunk": "[35]. Combining all these solutions, we develop SAMA, a holistic and practically ScalAble Meta learning Algorithm. 3. We evaluate the scalability and the overall performance of SAMA on a multitude of large-scale meta learning benchmarks involving large language models (e.g., BERT [31] and RoBERTa [39]) or large datasets (e.g., ImageNet-1k [10]). Notably, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. In addition, we observe that SAMA- based data optimization consistently leads to improvements in text classification accuracy with large language models, and achieves state-of-the-art results in both small-/large-scale data pruning, demonstrating the initial potential of scalable meta learning. 2 2 Background: Gradient-Based Meta Learning We begin by reviewing the basics of (gradient-based) meta learning in order to establish the key aspects that have limited its scalability. Mathematically, meta learning is commonly formulated as bilevel optimization as follows: \u03bb\u2217 = argmin Lmeta(Dmeta; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin Lbase(Dbase; \u03b8, \u03bb) \u03b8 where \u03bb (respectively, \u03b8) are the parameters of meta (base) learners, Dmeta (Dbase) are meta (base) datasets, and Lmeta (Lbase) are meta (base) loss functions. An important implication of the above formulation is that meta learning changes the task of finding the optimal inductive biases from designing heuristics to designing meta optimization problems. As an example, consider the problem of finding the optimal inductive bias for fair classification given class-imbalanced training data. A traditional approach to this problem is to use a heuristic that reweights training samples inversely proportional to class frequencies. On the contrary, L2RW [54] designs a meta optimization problem by curating a small number of class-balanced data for the meta dataset Dmeta and setting the meta learner \u03bb to be importance weights for all training data. In short, unlike heuristic-based methods that explicitly specify \u201chow to learn,\u201d meta learning methods only specify \u201cwhat to learn\u201d and let the meta learner automatically determine \u201chow.\u201d From a programming paradigm perspective, such a difference can be understood as a transition from imperative to declarative programming. While there are multiple approaches to solving meta learning, in this work we focus on gradient- based approaches due to their ability to efficiently solve high-dimensional meta optimization (i.e. dim(\u03bb) \u226b 1) problems. Such an ability is essential given that the search space for inductive biases (e.g. importance weights for training data) can increase exponentially with recent large models and datasets. Concretely, GBML computes a meta gradient composed of two terms\u2014the best-response Jacobian and direct gradient\u2014with the chain rule, as follows: \u2202\u03b8\u2217 \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) best-response Jacobian Since the direct gradient (teal) computation is straightforward with the underlying automatic dif- ferentiation library, the major challenge in GBML lies in computing the best-response Jacobian (purple), of which two common solutions are iterative differentiation [14, 15, 16, 42] and implicit differentiation [25, 40, 49, 51]. Between these two, in this paper we adopt implicit differentiation as our baseline solution to GBML, as it achieves better computation and memory efficiency than iterative differentiation [19], both of which are vital in accomplishing our goal of scalable GBML. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 (cid:124) (cid:123)(cid:122) (cid:125) direct gradient = The gist of implicit differentiation is that it calculates the best-response Jacobian by leveraging Cauchy\u2019s Implicit Function Theorem (IFT) and re-interpreting the base optimization problem from the perspective of fixed-point iteration given an iterative solver u, as follows: \u2202\u03b8\u2217 \u2202\u03bb \u2202u \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) meta Jacobian = \u2212 (cid:18) \u2202u \u2202\u03b8\u2217 (cid:124)(cid:123)(cid:122)(cid:125) base Jacobian (cid:19)\u22121 where (cid:40) \u03b8\u2217 = lim t\u2192\u221e \u03b8t \u03b8t = \u03b8t\u22121 \u2212 u(\u03b8t\u22121; \u03bb) While exact implicit differentiation requires solving the base optimization problem to convergence \u03b8\u2217 by repeatedly applying an iterative solver u (e.g., SGD or Adam) to calculate base (blue) and meta (red) Jacobians, this is computationally impractical, especially in most large-scale learning settings. Therefore, researchers oftentimes approximate \u03b8\u2217 with a small number of unrolled update steps of u. This results in a solution that alternates gradient descent between base and meta optimization problems, where the base gradient is calculated with standard backpropagation and the meta gradient with Eqs. (1) & (2). Noting that many techniques have been developed to efficiently perform and scale up standard backpropagation, we deduce that the major challenges in scaling GBML lie in meta gradient computation, which will be discussed in depth in the next section. 3 Scaling Meta Learning It has long been recognized that meta gradient computation in GBML suffers from a substantial compute/memory cost [40, 51], algorithmic instability [2, 12], and a lack of efficient distributed 3 (1) (2) training support [3, 6, 9], all of which can significantly limit its scalability. In this section, we first attempt to understand the above limitations at a technical level. Toward this end, we investigate three aspects in Eqs. (1) & (2), namely (i) base Jacobian inversion, (ii) algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of meta gradient computation, and discuss how they lead to the aforementioned limitations. Next, we propose initial solutions for each of these issues, based on which we build a holistic and ScalAble Meta Learning Algorithm, SAMA. 3.1 Base Jacobian Inverse Problem Denoting the size of the base learner (i.e., dim(\u03b8)) as nb, the computational complexity of the naive base Jacobian (blue) inversion in Eq. (2) is O(n3 b). Since such cubic complexity is impracti- cal even for small-sized base learners, practitioners typically utilize various linear systems techniques such as Neumann series [40] or conjugate gradient [51], and directly approximate (cid:0) \u2202u (cid:1)\u22121 \u2202Lmeta . \u2202\u03b8\u2217 Given that the base Jacobian is a function of the Hessian matrix of the base optimization problem in GBML, these algorithms solve linear systems by iteratively performing Hessian-vector products. However, this Hessian-vector product computation contributes negatively to all three above limita- tions. First, while many automatic differentiation engines provide an efficient Hessian-vector product implementation like Pearlmutter\u2019s algorithm [48], its memory/compute cost is still prohibitive with larger models, as demonstrated in Fig. 1. Second, in most cases, we only have"}, {"question": " How does SAMA perform in terms of memory consumption on multi-GPU setups compared to other baseline meta learning algorithms?,answer: 2.0\u00d7 decrease in memory consumption", "ref_chunk": "[35]. Combining all these solutions, we develop SAMA, a holistic and practically ScalAble Meta learning Algorithm. 3. We evaluate the scalability and the overall performance of SAMA on a multitude of large-scale meta learning benchmarks involving large language models (e.g., BERT [31] and RoBERTa [39]) or large datasets (e.g., ImageNet-1k [10]). Notably, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. In addition, we observe that SAMA- based data optimization consistently leads to improvements in text classification accuracy with large language models, and achieves state-of-the-art results in both small-/large-scale data pruning, demonstrating the initial potential of scalable meta learning. 2 2 Background: Gradient-Based Meta Learning We begin by reviewing the basics of (gradient-based) meta learning in order to establish the key aspects that have limited its scalability. Mathematically, meta learning is commonly formulated as bilevel optimization as follows: \u03bb\u2217 = argmin Lmeta(Dmeta; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin Lbase(Dbase; \u03b8, \u03bb) \u03b8 where \u03bb (respectively, \u03b8) are the parameters of meta (base) learners, Dmeta (Dbase) are meta (base) datasets, and Lmeta (Lbase) are meta (base) loss functions. An important implication of the above formulation is that meta learning changes the task of finding the optimal inductive biases from designing heuristics to designing meta optimization problems. As an example, consider the problem of finding the optimal inductive bias for fair classification given class-imbalanced training data. A traditional approach to this problem is to use a heuristic that reweights training samples inversely proportional to class frequencies. On the contrary, L2RW [54] designs a meta optimization problem by curating a small number of class-balanced data for the meta dataset Dmeta and setting the meta learner \u03bb to be importance weights for all training data. In short, unlike heuristic-based methods that explicitly specify \u201chow to learn,\u201d meta learning methods only specify \u201cwhat to learn\u201d and let the meta learner automatically determine \u201chow.\u201d From a programming paradigm perspective, such a difference can be understood as a transition from imperative to declarative programming. While there are multiple approaches to solving meta learning, in this work we focus on gradient- based approaches due to their ability to efficiently solve high-dimensional meta optimization (i.e. dim(\u03bb) \u226b 1) problems. Such an ability is essential given that the search space for inductive biases (e.g. importance weights for training data) can increase exponentially with recent large models and datasets. Concretely, GBML computes a meta gradient composed of two terms\u2014the best-response Jacobian and direct gradient\u2014with the chain rule, as follows: \u2202\u03b8\u2217 \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) best-response Jacobian Since the direct gradient (teal) computation is straightforward with the underlying automatic dif- ferentiation library, the major challenge in GBML lies in computing the best-response Jacobian (purple), of which two common solutions are iterative differentiation [14, 15, 16, 42] and implicit differentiation [25, 40, 49, 51]. Between these two, in this paper we adopt implicit differentiation as our baseline solution to GBML, as it achieves better computation and memory efficiency than iterative differentiation [19], both of which are vital in accomplishing our goal of scalable GBML. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 (cid:124) (cid:123)(cid:122) (cid:125) direct gradient = The gist of implicit differentiation is that it calculates the best-response Jacobian by leveraging Cauchy\u2019s Implicit Function Theorem (IFT) and re-interpreting the base optimization problem from the perspective of fixed-point iteration given an iterative solver u, as follows: \u2202\u03b8\u2217 \u2202\u03bb \u2202u \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) meta Jacobian = \u2212 (cid:18) \u2202u \u2202\u03b8\u2217 (cid:124)(cid:123)(cid:122)(cid:125) base Jacobian (cid:19)\u22121 where (cid:40) \u03b8\u2217 = lim t\u2192\u221e \u03b8t \u03b8t = \u03b8t\u22121 \u2212 u(\u03b8t\u22121; \u03bb) While exact implicit differentiation requires solving the base optimization problem to convergence \u03b8\u2217 by repeatedly applying an iterative solver u (e.g., SGD or Adam) to calculate base (blue) and meta (red) Jacobians, this is computationally impractical, especially in most large-scale learning settings. Therefore, researchers oftentimes approximate \u03b8\u2217 with a small number of unrolled update steps of u. This results in a solution that alternates gradient descent between base and meta optimization problems, where the base gradient is calculated with standard backpropagation and the meta gradient with Eqs. (1) & (2). Noting that many techniques have been developed to efficiently perform and scale up standard backpropagation, we deduce that the major challenges in scaling GBML lie in meta gradient computation, which will be discussed in depth in the next section. 3 Scaling Meta Learning It has long been recognized that meta gradient computation in GBML suffers from a substantial compute/memory cost [40, 51], algorithmic instability [2, 12], and a lack of efficient distributed 3 (1) (2) training support [3, 6, 9], all of which can significantly limit its scalability. In this section, we first attempt to understand the above limitations at a technical level. Toward this end, we investigate three aspects in Eqs. (1) & (2), namely (i) base Jacobian inversion, (ii) algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of meta gradient computation, and discuss how they lead to the aforementioned limitations. Next, we propose initial solutions for each of these issues, based on which we build a holistic and ScalAble Meta Learning Algorithm, SAMA. 3.1 Base Jacobian Inverse Problem Denoting the size of the base learner (i.e., dim(\u03b8)) as nb, the computational complexity of the naive base Jacobian (blue) inversion in Eq. (2) is O(n3 b). Since such cubic complexity is impracti- cal even for small-sized base learners, practitioners typically utilize various linear systems techniques such as Neumann series [40] or conjugate gradient [51], and directly approximate (cid:0) \u2202u (cid:1)\u22121 \u2202Lmeta . \u2202\u03b8\u2217 Given that the base Jacobian is a function of the Hessian matrix of the base optimization problem in GBML, these algorithms solve linear systems by iteratively performing Hessian-vector products. However, this Hessian-vector product computation contributes negatively to all three above limita- tions. First, while many automatic differentiation engines provide an efficient Hessian-vector product implementation like Pearlmutter\u2019s algorithm [48], its memory/compute cost is still prohibitive with larger models, as demonstrated in Fig. 1. Second, in most cases, we only have"}, {"question": " What kind of data optimization leads to improvements in text classification accuracy with large language models according to the text?,answer: SAMA-based data optimization", "ref_chunk": "[35]. Combining all these solutions, we develop SAMA, a holistic and practically ScalAble Meta learning Algorithm. 3. We evaluate the scalability and the overall performance of SAMA on a multitude of large-scale meta learning benchmarks involving large language models (e.g., BERT [31] and RoBERTa [39]) or large datasets (e.g., ImageNet-1k [10]). Notably, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. In addition, we observe that SAMA- based data optimization consistently leads to improvements in text classification accuracy with large language models, and achieves state-of-the-art results in both small-/large-scale data pruning, demonstrating the initial potential of scalable meta learning. 2 2 Background: Gradient-Based Meta Learning We begin by reviewing the basics of (gradient-based) meta learning in order to establish the key aspects that have limited its scalability. Mathematically, meta learning is commonly formulated as bilevel optimization as follows: \u03bb\u2217 = argmin Lmeta(Dmeta; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin Lbase(Dbase; \u03b8, \u03bb) \u03b8 where \u03bb (respectively, \u03b8) are the parameters of meta (base) learners, Dmeta (Dbase) are meta (base) datasets, and Lmeta (Lbase) are meta (base) loss functions. An important implication of the above formulation is that meta learning changes the task of finding the optimal inductive biases from designing heuristics to designing meta optimization problems. As an example, consider the problem of finding the optimal inductive bias for fair classification given class-imbalanced training data. A traditional approach to this problem is to use a heuristic that reweights training samples inversely proportional to class frequencies. On the contrary, L2RW [54] designs a meta optimization problem by curating a small number of class-balanced data for the meta dataset Dmeta and setting the meta learner \u03bb to be importance weights for all training data. In short, unlike heuristic-based methods that explicitly specify \u201chow to learn,\u201d meta learning methods only specify \u201cwhat to learn\u201d and let the meta learner automatically determine \u201chow.\u201d From a programming paradigm perspective, such a difference can be understood as a transition from imperative to declarative programming. While there are multiple approaches to solving meta learning, in this work we focus on gradient- based approaches due to their ability to efficiently solve high-dimensional meta optimization (i.e. dim(\u03bb) \u226b 1) problems. Such an ability is essential given that the search space for inductive biases (e.g. importance weights for training data) can increase exponentially with recent large models and datasets. Concretely, GBML computes a meta gradient composed of two terms\u2014the best-response Jacobian and direct gradient\u2014with the chain rule, as follows: \u2202\u03b8\u2217 \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) best-response Jacobian Since the direct gradient (teal) computation is straightforward with the underlying automatic dif- ferentiation library, the major challenge in GBML lies in computing the best-response Jacobian (purple), of which two common solutions are iterative differentiation [14, 15, 16, 42] and implicit differentiation [25, 40, 49, 51]. Between these two, in this paper we adopt implicit differentiation as our baseline solution to GBML, as it achieves better computation and memory efficiency than iterative differentiation [19], both of which are vital in accomplishing our goal of scalable GBML. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 (cid:124) (cid:123)(cid:122) (cid:125) direct gradient = The gist of implicit differentiation is that it calculates the best-response Jacobian by leveraging Cauchy\u2019s Implicit Function Theorem (IFT) and re-interpreting the base optimization problem from the perspective of fixed-point iteration given an iterative solver u, as follows: \u2202\u03b8\u2217 \u2202\u03bb \u2202u \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) meta Jacobian = \u2212 (cid:18) \u2202u \u2202\u03b8\u2217 (cid:124)(cid:123)(cid:122)(cid:125) base Jacobian (cid:19)\u22121 where (cid:40) \u03b8\u2217 = lim t\u2192\u221e \u03b8t \u03b8t = \u03b8t\u22121 \u2212 u(\u03b8t\u22121; \u03bb) While exact implicit differentiation requires solving the base optimization problem to convergence \u03b8\u2217 by repeatedly applying an iterative solver u (e.g., SGD or Adam) to calculate base (blue) and meta (red) Jacobians, this is computationally impractical, especially in most large-scale learning settings. Therefore, researchers oftentimes approximate \u03b8\u2217 with a small number of unrolled update steps of u. This results in a solution that alternates gradient descent between base and meta optimization problems, where the base gradient is calculated with standard backpropagation and the meta gradient with Eqs. (1) & (2). Noting that many techniques have been developed to efficiently perform and scale up standard backpropagation, we deduce that the major challenges in scaling GBML lie in meta gradient computation, which will be discussed in depth in the next section. 3 Scaling Meta Learning It has long been recognized that meta gradient computation in GBML suffers from a substantial compute/memory cost [40, 51], algorithmic instability [2, 12], and a lack of efficient distributed 3 (1) (2) training support [3, 6, 9], all of which can significantly limit its scalability. In this section, we first attempt to understand the above limitations at a technical level. Toward this end, we investigate three aspects in Eqs. (1) & (2), namely (i) base Jacobian inversion, (ii) algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of meta gradient computation, and discuss how they lead to the aforementioned limitations. Next, we propose initial solutions for each of these issues, based on which we build a holistic and ScalAble Meta Learning Algorithm, SAMA. 3.1 Base Jacobian Inverse Problem Denoting the size of the base learner (i.e., dim(\u03b8)) as nb, the computational complexity of the naive base Jacobian (blue) inversion in Eq. (2) is O(n3 b). Since such cubic complexity is impracti- cal even for small-sized base learners, practitioners typically utilize various linear systems techniques such as Neumann series [40] or conjugate gradient [51], and directly approximate (cid:0) \u2202u (cid:1)\u22121 \u2202Lmeta . \u2202\u03b8\u2217 Given that the base Jacobian is a function of the Hessian matrix of the base optimization problem in GBML, these algorithms solve linear systems by iteratively performing Hessian-vector products. However, this Hessian-vector product computation contributes negatively to all three above limita- tions. First, while many automatic differentiation engines provide an efficient Hessian-vector product implementation like Pearlmutter\u2019s algorithm [48], its memory/compute cost is still prohibitive with larger models, as demonstrated in Fig. 1. Second, in most cases, we only have"}, {"question": " What implication does meta learning have on finding the optimal inductive biases?,answer: Meta learning changes the task from designing heuristics to designing meta optimization problems", "ref_chunk": "[35]. Combining all these solutions, we develop SAMA, a holistic and practically ScalAble Meta learning Algorithm. 3. We evaluate the scalability and the overall performance of SAMA on a multitude of large-scale meta learning benchmarks involving large language models (e.g., BERT [31] and RoBERTa [39]) or large datasets (e.g., ImageNet-1k [10]). Notably, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. In addition, we observe that SAMA- based data optimization consistently leads to improvements in text classification accuracy with large language models, and achieves state-of-the-art results in both small-/large-scale data pruning, demonstrating the initial potential of scalable meta learning. 2 2 Background: Gradient-Based Meta Learning We begin by reviewing the basics of (gradient-based) meta learning in order to establish the key aspects that have limited its scalability. Mathematically, meta learning is commonly formulated as bilevel optimization as follows: \u03bb\u2217 = argmin Lmeta(Dmeta; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin Lbase(Dbase; \u03b8, \u03bb) \u03b8 where \u03bb (respectively, \u03b8) are the parameters of meta (base) learners, Dmeta (Dbase) are meta (base) datasets, and Lmeta (Lbase) are meta (base) loss functions. An important implication of the above formulation is that meta learning changes the task of finding the optimal inductive biases from designing heuristics to designing meta optimization problems. As an example, consider the problem of finding the optimal inductive bias for fair classification given class-imbalanced training data. A traditional approach to this problem is to use a heuristic that reweights training samples inversely proportional to class frequencies. On the contrary, L2RW [54] designs a meta optimization problem by curating a small number of class-balanced data for the meta dataset Dmeta and setting the meta learner \u03bb to be importance weights for all training data. In short, unlike heuristic-based methods that explicitly specify \u201chow to learn,\u201d meta learning methods only specify \u201cwhat to learn\u201d and let the meta learner automatically determine \u201chow.\u201d From a programming paradigm perspective, such a difference can be understood as a transition from imperative to declarative programming. While there are multiple approaches to solving meta learning, in this work we focus on gradient- based approaches due to their ability to efficiently solve high-dimensional meta optimization (i.e. dim(\u03bb) \u226b 1) problems. Such an ability is essential given that the search space for inductive biases (e.g. importance weights for training data) can increase exponentially with recent large models and datasets. Concretely, GBML computes a meta gradient composed of two terms\u2014the best-response Jacobian and direct gradient\u2014with the chain rule, as follows: \u2202\u03b8\u2217 \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) best-response Jacobian Since the direct gradient (teal) computation is straightforward with the underlying automatic dif- ferentiation library, the major challenge in GBML lies in computing the best-response Jacobian (purple), of which two common solutions are iterative differentiation [14, 15, 16, 42] and implicit differentiation [25, 40, 49, 51]. Between these two, in this paper we adopt implicit differentiation as our baseline solution to GBML, as it achieves better computation and memory efficiency than iterative differentiation [19], both of which are vital in accomplishing our goal of scalable GBML. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 (cid:124) (cid:123)(cid:122) (cid:125) direct gradient = The gist of implicit differentiation is that it calculates the best-response Jacobian by leveraging Cauchy\u2019s Implicit Function Theorem (IFT) and re-interpreting the base optimization problem from the perspective of fixed-point iteration given an iterative solver u, as follows: \u2202\u03b8\u2217 \u2202\u03bb \u2202u \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) meta Jacobian = \u2212 (cid:18) \u2202u \u2202\u03b8\u2217 (cid:124)(cid:123)(cid:122)(cid:125) base Jacobian (cid:19)\u22121 where (cid:40) \u03b8\u2217 = lim t\u2192\u221e \u03b8t \u03b8t = \u03b8t\u22121 \u2212 u(\u03b8t\u22121; \u03bb) While exact implicit differentiation requires solving the base optimization problem to convergence \u03b8\u2217 by repeatedly applying an iterative solver u (e.g., SGD or Adam) to calculate base (blue) and meta (red) Jacobians, this is computationally impractical, especially in most large-scale learning settings. Therefore, researchers oftentimes approximate \u03b8\u2217 with a small number of unrolled update steps of u. This results in a solution that alternates gradient descent between base and meta optimization problems, where the base gradient is calculated with standard backpropagation and the meta gradient with Eqs. (1) & (2). Noting that many techniques have been developed to efficiently perform and scale up standard backpropagation, we deduce that the major challenges in scaling GBML lie in meta gradient computation, which will be discussed in depth in the next section. 3 Scaling Meta Learning It has long been recognized that meta gradient computation in GBML suffers from a substantial compute/memory cost [40, 51], algorithmic instability [2, 12], and a lack of efficient distributed 3 (1) (2) training support [3, 6, 9], all of which can significantly limit its scalability. In this section, we first attempt to understand the above limitations at a technical level. Toward this end, we investigate three aspects in Eqs. (1) & (2), namely (i) base Jacobian inversion, (ii) algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of meta gradient computation, and discuss how they lead to the aforementioned limitations. Next, we propose initial solutions for each of these issues, based on which we build a holistic and ScalAble Meta Learning Algorithm, SAMA. 3.1 Base Jacobian Inverse Problem Denoting the size of the base learner (i.e., dim(\u03b8)) as nb, the computational complexity of the naive base Jacobian (blue) inversion in Eq. (2) is O(n3 b). Since such cubic complexity is impracti- cal even for small-sized base learners, practitioners typically utilize various linear systems techniques such as Neumann series [40] or conjugate gradient [51], and directly approximate (cid:0) \u2202u (cid:1)\u22121 \u2202Lmeta . \u2202\u03b8\u2217 Given that the base Jacobian is a function of the Hessian matrix of the base optimization problem in GBML, these algorithms solve linear systems by iteratively performing Hessian-vector products. However, this Hessian-vector product computation contributes negatively to all three above limita- tions. First, while many automatic differentiation engines provide an efficient Hessian-vector product implementation like Pearlmutter\u2019s algorithm [48], its memory/compute cost is still prohibitive with larger models, as demonstrated in Fig. 1. Second, in most cases, we only have"}, {"question": " What approach does the text focus on when it comes to solving meta learning?,answer: Gradient-based approaches", "ref_chunk": "[35]. Combining all these solutions, we develop SAMA, a holistic and practically ScalAble Meta learning Algorithm. 3. We evaluate the scalability and the overall performance of SAMA on a multitude of large-scale meta learning benchmarks involving large language models (e.g., BERT [31] and RoBERTa [39]) or large datasets (e.g., ImageNet-1k [10]). Notably, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. In addition, we observe that SAMA- based data optimization consistently leads to improvements in text classification accuracy with large language models, and achieves state-of-the-art results in both small-/large-scale data pruning, demonstrating the initial potential of scalable meta learning. 2 2 Background: Gradient-Based Meta Learning We begin by reviewing the basics of (gradient-based) meta learning in order to establish the key aspects that have limited its scalability. Mathematically, meta learning is commonly formulated as bilevel optimization as follows: \u03bb\u2217 = argmin Lmeta(Dmeta; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin Lbase(Dbase; \u03b8, \u03bb) \u03b8 where \u03bb (respectively, \u03b8) are the parameters of meta (base) learners, Dmeta (Dbase) are meta (base) datasets, and Lmeta (Lbase) are meta (base) loss functions. An important implication of the above formulation is that meta learning changes the task of finding the optimal inductive biases from designing heuristics to designing meta optimization problems. As an example, consider the problem of finding the optimal inductive bias for fair classification given class-imbalanced training data. A traditional approach to this problem is to use a heuristic that reweights training samples inversely proportional to class frequencies. On the contrary, L2RW [54] designs a meta optimization problem by curating a small number of class-balanced data for the meta dataset Dmeta and setting the meta learner \u03bb to be importance weights for all training data. In short, unlike heuristic-based methods that explicitly specify \u201chow to learn,\u201d meta learning methods only specify \u201cwhat to learn\u201d and let the meta learner automatically determine \u201chow.\u201d From a programming paradigm perspective, such a difference can be understood as a transition from imperative to declarative programming. While there are multiple approaches to solving meta learning, in this work we focus on gradient- based approaches due to their ability to efficiently solve high-dimensional meta optimization (i.e. dim(\u03bb) \u226b 1) problems. Such an ability is essential given that the search space for inductive biases (e.g. importance weights for training data) can increase exponentially with recent large models and datasets. Concretely, GBML computes a meta gradient composed of two terms\u2014the best-response Jacobian and direct gradient\u2014with the chain rule, as follows: \u2202\u03b8\u2217 \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) best-response Jacobian Since the direct gradient (teal) computation is straightforward with the underlying automatic dif- ferentiation library, the major challenge in GBML lies in computing the best-response Jacobian (purple), of which two common solutions are iterative differentiation [14, 15, 16, 42] and implicit differentiation [25, 40, 49, 51]. Between these two, in this paper we adopt implicit differentiation as our baseline solution to GBML, as it achieves better computation and memory efficiency than iterative differentiation [19], both of which are vital in accomplishing our goal of scalable GBML. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 (cid:124) (cid:123)(cid:122) (cid:125) direct gradient = The gist of implicit differentiation is that it calculates the best-response Jacobian by leveraging Cauchy\u2019s Implicit Function Theorem (IFT) and re-interpreting the base optimization problem from the perspective of fixed-point iteration given an iterative solver u, as follows: \u2202\u03b8\u2217 \u2202\u03bb \u2202u \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) meta Jacobian = \u2212 (cid:18) \u2202u \u2202\u03b8\u2217 (cid:124)(cid:123)(cid:122)(cid:125) base Jacobian (cid:19)\u22121 where (cid:40) \u03b8\u2217 = lim t\u2192\u221e \u03b8t \u03b8t = \u03b8t\u22121 \u2212 u(\u03b8t\u22121; \u03bb) While exact implicit differentiation requires solving the base optimization problem to convergence \u03b8\u2217 by repeatedly applying an iterative solver u (e.g., SGD or Adam) to calculate base (blue) and meta (red) Jacobians, this is computationally impractical, especially in most large-scale learning settings. Therefore, researchers oftentimes approximate \u03b8\u2217 with a small number of unrolled update steps of u. This results in a solution that alternates gradient descent between base and meta optimization problems, where the base gradient is calculated with standard backpropagation and the meta gradient with Eqs. (1) & (2). Noting that many techniques have been developed to efficiently perform and scale up standard backpropagation, we deduce that the major challenges in scaling GBML lie in meta gradient computation, which will be discussed in depth in the next section. 3 Scaling Meta Learning It has long been recognized that meta gradient computation in GBML suffers from a substantial compute/memory cost [40, 51], algorithmic instability [2, 12], and a lack of efficient distributed 3 (1) (2) training support [3, 6, 9], all of which can significantly limit its scalability. In this section, we first attempt to understand the above limitations at a technical level. Toward this end, we investigate three aspects in Eqs. (1) & (2), namely (i) base Jacobian inversion, (ii) algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of meta gradient computation, and discuss how they lead to the aforementioned limitations. Next, we propose initial solutions for each of these issues, based on which we build a holistic and ScalAble Meta Learning Algorithm, SAMA. 3.1 Base Jacobian Inverse Problem Denoting the size of the base learner (i.e., dim(\u03b8)) as nb, the computational complexity of the naive base Jacobian (blue) inversion in Eq. (2) is O(n3 b). Since such cubic complexity is impracti- cal even for small-sized base learners, practitioners typically utilize various linear systems techniques such as Neumann series [40] or conjugate gradient [51], and directly approximate (cid:0) \u2202u (cid:1)\u22121 \u2202Lmeta . \u2202\u03b8\u2217 Given that the base Jacobian is a function of the Hessian matrix of the base optimization problem in GBML, these algorithms solve linear systems by iteratively performing Hessian-vector products. However, this Hessian-vector product computation contributes negatively to all three above limita- tions. First, while many automatic differentiation engines provide an efficient Hessian-vector product implementation like Pearlmutter\u2019s algorithm [48], its memory/compute cost is still prohibitive with larger models, as demonstrated in Fig. 1. Second, in most cases, we only have"}, {"question": " What is the major challenge in Gradient-Based Meta Learning according to the text?,answer: Computing the best-response Jacobian", "ref_chunk": "[35]. Combining all these solutions, we develop SAMA, a holistic and practically ScalAble Meta learning Algorithm. 3. We evaluate the scalability and the overall performance of SAMA on a multitude of large-scale meta learning benchmarks involving large language models (e.g., BERT [31] and RoBERTa [39]) or large datasets (e.g., ImageNet-1k [10]). Notably, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. In addition, we observe that SAMA- based data optimization consistently leads to improvements in text classification accuracy with large language models, and achieves state-of-the-art results in both small-/large-scale data pruning, demonstrating the initial potential of scalable meta learning. 2 2 Background: Gradient-Based Meta Learning We begin by reviewing the basics of (gradient-based) meta learning in order to establish the key aspects that have limited its scalability. Mathematically, meta learning is commonly formulated as bilevel optimization as follows: \u03bb\u2217 = argmin Lmeta(Dmeta; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin Lbase(Dbase; \u03b8, \u03bb) \u03b8 where \u03bb (respectively, \u03b8) are the parameters of meta (base) learners, Dmeta (Dbase) are meta (base) datasets, and Lmeta (Lbase) are meta (base) loss functions. An important implication of the above formulation is that meta learning changes the task of finding the optimal inductive biases from designing heuristics to designing meta optimization problems. As an example, consider the problem of finding the optimal inductive bias for fair classification given class-imbalanced training data. A traditional approach to this problem is to use a heuristic that reweights training samples inversely proportional to class frequencies. On the contrary, L2RW [54] designs a meta optimization problem by curating a small number of class-balanced data for the meta dataset Dmeta and setting the meta learner \u03bb to be importance weights for all training data. In short, unlike heuristic-based methods that explicitly specify \u201chow to learn,\u201d meta learning methods only specify \u201cwhat to learn\u201d and let the meta learner automatically determine \u201chow.\u201d From a programming paradigm perspective, such a difference can be understood as a transition from imperative to declarative programming. While there are multiple approaches to solving meta learning, in this work we focus on gradient- based approaches due to their ability to efficiently solve high-dimensional meta optimization (i.e. dim(\u03bb) \u226b 1) problems. Such an ability is essential given that the search space for inductive biases (e.g. importance weights for training data) can increase exponentially with recent large models and datasets. Concretely, GBML computes a meta gradient composed of two terms\u2014the best-response Jacobian and direct gradient\u2014with the chain rule, as follows: \u2202\u03b8\u2217 \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) best-response Jacobian Since the direct gradient (teal) computation is straightforward with the underlying automatic dif- ferentiation library, the major challenge in GBML lies in computing the best-response Jacobian (purple), of which two common solutions are iterative differentiation [14, 15, 16, 42] and implicit differentiation [25, 40, 49, 51]. Between these two, in this paper we adopt implicit differentiation as our baseline solution to GBML, as it achieves better computation and memory efficiency than iterative differentiation [19], both of which are vital in accomplishing our goal of scalable GBML. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 (cid:124) (cid:123)(cid:122) (cid:125) direct gradient = The gist of implicit differentiation is that it calculates the best-response Jacobian by leveraging Cauchy\u2019s Implicit Function Theorem (IFT) and re-interpreting the base optimization problem from the perspective of fixed-point iteration given an iterative solver u, as follows: \u2202\u03b8\u2217 \u2202\u03bb \u2202u \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) meta Jacobian = \u2212 (cid:18) \u2202u \u2202\u03b8\u2217 (cid:124)(cid:123)(cid:122)(cid:125) base Jacobian (cid:19)\u22121 where (cid:40) \u03b8\u2217 = lim t\u2192\u221e \u03b8t \u03b8t = \u03b8t\u22121 \u2212 u(\u03b8t\u22121; \u03bb) While exact implicit differentiation requires solving the base optimization problem to convergence \u03b8\u2217 by repeatedly applying an iterative solver u (e.g., SGD or Adam) to calculate base (blue) and meta (red) Jacobians, this is computationally impractical, especially in most large-scale learning settings. Therefore, researchers oftentimes approximate \u03b8\u2217 with a small number of unrolled update steps of u. This results in a solution that alternates gradient descent between base and meta optimization problems, where the base gradient is calculated with standard backpropagation and the meta gradient with Eqs. (1) & (2). Noting that many techniques have been developed to efficiently perform and scale up standard backpropagation, we deduce that the major challenges in scaling GBML lie in meta gradient computation, which will be discussed in depth in the next section. 3 Scaling Meta Learning It has long been recognized that meta gradient computation in GBML suffers from a substantial compute/memory cost [40, 51], algorithmic instability [2, 12], and a lack of efficient distributed 3 (1) (2) training support [3, 6, 9], all of which can significantly limit its scalability. In this section, we first attempt to understand the above limitations at a technical level. Toward this end, we investigate three aspects in Eqs. (1) & (2), namely (i) base Jacobian inversion, (ii) algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of meta gradient computation, and discuss how they lead to the aforementioned limitations. Next, we propose initial solutions for each of these issues, based on which we build a holistic and ScalAble Meta Learning Algorithm, SAMA. 3.1 Base Jacobian Inverse Problem Denoting the size of the base learner (i.e., dim(\u03b8)) as nb, the computational complexity of the naive base Jacobian (blue) inversion in Eq. (2) is O(n3 b). Since such cubic complexity is impracti- cal even for small-sized base learners, practitioners typically utilize various linear systems techniques such as Neumann series [40] or conjugate gradient [51], and directly approximate (cid:0) \u2202u (cid:1)\u22121 \u2202Lmeta . \u2202\u03b8\u2217 Given that the base Jacobian is a function of the Hessian matrix of the base optimization problem in GBML, these algorithms solve linear systems by iteratively performing Hessian-vector products. However, this Hessian-vector product computation contributes negatively to all three above limita- tions. First, while many automatic differentiation engines provide an efficient Hessian-vector product implementation like Pearlmutter\u2019s algorithm [48], its memory/compute cost is still prohibitive with larger models, as demonstrated in Fig. 1. Second, in most cases, we only have"}, {"question": " What is the key aspect that limits the scalability of meta gradient computation in GBML?,answer: Substantial compute/memory cost", "ref_chunk": "[35]. Combining all these solutions, we develop SAMA, a holistic and practically ScalAble Meta learning Algorithm. 3. We evaluate the scalability and the overall performance of SAMA on a multitude of large-scale meta learning benchmarks involving large language models (e.g., BERT [31] and RoBERTa [39]) or large datasets (e.g., ImageNet-1k [10]). Notably, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. In addition, we observe that SAMA- based data optimization consistently leads to improvements in text classification accuracy with large language models, and achieves state-of-the-art results in both small-/large-scale data pruning, demonstrating the initial potential of scalable meta learning. 2 2 Background: Gradient-Based Meta Learning We begin by reviewing the basics of (gradient-based) meta learning in order to establish the key aspects that have limited its scalability. Mathematically, meta learning is commonly formulated as bilevel optimization as follows: \u03bb\u2217 = argmin Lmeta(Dmeta; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin Lbase(Dbase; \u03b8, \u03bb) \u03b8 where \u03bb (respectively, \u03b8) are the parameters of meta (base) learners, Dmeta (Dbase) are meta (base) datasets, and Lmeta (Lbase) are meta (base) loss functions. An important implication of the above formulation is that meta learning changes the task of finding the optimal inductive biases from designing heuristics to designing meta optimization problems. As an example, consider the problem of finding the optimal inductive bias for fair classification given class-imbalanced training data. A traditional approach to this problem is to use a heuristic that reweights training samples inversely proportional to class frequencies. On the contrary, L2RW [54] designs a meta optimization problem by curating a small number of class-balanced data for the meta dataset Dmeta and setting the meta learner \u03bb to be importance weights for all training data. In short, unlike heuristic-based methods that explicitly specify \u201chow to learn,\u201d meta learning methods only specify \u201cwhat to learn\u201d and let the meta learner automatically determine \u201chow.\u201d From a programming paradigm perspective, such a difference can be understood as a transition from imperative to declarative programming. While there are multiple approaches to solving meta learning, in this work we focus on gradient- based approaches due to their ability to efficiently solve high-dimensional meta optimization (i.e. dim(\u03bb) \u226b 1) problems. Such an ability is essential given that the search space for inductive biases (e.g. importance weights for training data) can increase exponentially with recent large models and datasets. Concretely, GBML computes a meta gradient composed of two terms\u2014the best-response Jacobian and direct gradient\u2014with the chain rule, as follows: \u2202\u03b8\u2217 \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) best-response Jacobian Since the direct gradient (teal) computation is straightforward with the underlying automatic dif- ferentiation library, the major challenge in GBML lies in computing the best-response Jacobian (purple), of which two common solutions are iterative differentiation [14, 15, 16, 42] and implicit differentiation [25, 40, 49, 51]. Between these two, in this paper we adopt implicit differentiation as our baseline solution to GBML, as it achieves better computation and memory efficiency than iterative differentiation [19], both of which are vital in accomplishing our goal of scalable GBML. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 (cid:124) (cid:123)(cid:122) (cid:125) direct gradient = The gist of implicit differentiation is that it calculates the best-response Jacobian by leveraging Cauchy\u2019s Implicit Function Theorem (IFT) and re-interpreting the base optimization problem from the perspective of fixed-point iteration given an iterative solver u, as follows: \u2202\u03b8\u2217 \u2202\u03bb \u2202u \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) meta Jacobian = \u2212 (cid:18) \u2202u \u2202\u03b8\u2217 (cid:124)(cid:123)(cid:122)(cid:125) base Jacobian (cid:19)\u22121 where (cid:40) \u03b8\u2217 = lim t\u2192\u221e \u03b8t \u03b8t = \u03b8t\u22121 \u2212 u(\u03b8t\u22121; \u03bb) While exact implicit differentiation requires solving the base optimization problem to convergence \u03b8\u2217 by repeatedly applying an iterative solver u (e.g., SGD or Adam) to calculate base (blue) and meta (red) Jacobians, this is computationally impractical, especially in most large-scale learning settings. Therefore, researchers oftentimes approximate \u03b8\u2217 with a small number of unrolled update steps of u. This results in a solution that alternates gradient descent between base and meta optimization problems, where the base gradient is calculated with standard backpropagation and the meta gradient with Eqs. (1) & (2). Noting that many techniques have been developed to efficiently perform and scale up standard backpropagation, we deduce that the major challenges in scaling GBML lie in meta gradient computation, which will be discussed in depth in the next section. 3 Scaling Meta Learning It has long been recognized that meta gradient computation in GBML suffers from a substantial compute/memory cost [40, 51], algorithmic instability [2, 12], and a lack of efficient distributed 3 (1) (2) training support [3, 6, 9], all of which can significantly limit its scalability. In this section, we first attempt to understand the above limitations at a technical level. Toward this end, we investigate three aspects in Eqs. (1) & (2), namely (i) base Jacobian inversion, (ii) algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of meta gradient computation, and discuss how they lead to the aforementioned limitations. Next, we propose initial solutions for each of these issues, based on which we build a holistic and ScalAble Meta Learning Algorithm, SAMA. 3.1 Base Jacobian Inverse Problem Denoting the size of the base learner (i.e., dim(\u03b8)) as nb, the computational complexity of the naive base Jacobian (blue) inversion in Eq. (2) is O(n3 b). Since such cubic complexity is impracti- cal even for small-sized base learners, practitioners typically utilize various linear systems techniques such as Neumann series [40] or conjugate gradient [51], and directly approximate (cid:0) \u2202u (cid:1)\u22121 \u2202Lmeta . \u2202\u03b8\u2217 Given that the base Jacobian is a function of the Hessian matrix of the base optimization problem in GBML, these algorithms solve linear systems by iteratively performing Hessian-vector products. However, this Hessian-vector product computation contributes negatively to all three above limita- tions. First, while many automatic differentiation engines provide an efficient Hessian-vector product implementation like Pearlmutter\u2019s algorithm [48], its memory/compute cost is still prohibitive with larger models, as demonstrated in Fig. 1. Second, in most cases, we only have"}, {"question": " What are the three aspects investigated to understand the limitations in Eqs. (1) & (2) according to the text?,answer: Base Jacobian inversion, algorithmic adaptation for adaptive optimizers, and custom implementation of meta gradient computation", "ref_chunk": "[35]. Combining all these solutions, we develop SAMA, a holistic and practically ScalAble Meta learning Algorithm. 3. We evaluate the scalability and the overall performance of SAMA on a multitude of large-scale meta learning benchmarks involving large language models (e.g., BERT [31] and RoBERTa [39]) or large datasets (e.g., ImageNet-1k [10]). Notably, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. In addition, we observe that SAMA- based data optimization consistently leads to improvements in text classification accuracy with large language models, and achieves state-of-the-art results in both small-/large-scale data pruning, demonstrating the initial potential of scalable meta learning. 2 2 Background: Gradient-Based Meta Learning We begin by reviewing the basics of (gradient-based) meta learning in order to establish the key aspects that have limited its scalability. Mathematically, meta learning is commonly formulated as bilevel optimization as follows: \u03bb\u2217 = argmin Lmeta(Dmeta; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin Lbase(Dbase; \u03b8, \u03bb) \u03b8 where \u03bb (respectively, \u03b8) are the parameters of meta (base) learners, Dmeta (Dbase) are meta (base) datasets, and Lmeta (Lbase) are meta (base) loss functions. An important implication of the above formulation is that meta learning changes the task of finding the optimal inductive biases from designing heuristics to designing meta optimization problems. As an example, consider the problem of finding the optimal inductive bias for fair classification given class-imbalanced training data. A traditional approach to this problem is to use a heuristic that reweights training samples inversely proportional to class frequencies. On the contrary, L2RW [54] designs a meta optimization problem by curating a small number of class-balanced data for the meta dataset Dmeta and setting the meta learner \u03bb to be importance weights for all training data. In short, unlike heuristic-based methods that explicitly specify \u201chow to learn,\u201d meta learning methods only specify \u201cwhat to learn\u201d and let the meta learner automatically determine \u201chow.\u201d From a programming paradigm perspective, such a difference can be understood as a transition from imperative to declarative programming. While there are multiple approaches to solving meta learning, in this work we focus on gradient- based approaches due to their ability to efficiently solve high-dimensional meta optimization (i.e. dim(\u03bb) \u226b 1) problems. Such an ability is essential given that the search space for inductive biases (e.g. importance weights for training data) can increase exponentially with recent large models and datasets. Concretely, GBML computes a meta gradient composed of two terms\u2014the best-response Jacobian and direct gradient\u2014with the chain rule, as follows: \u2202\u03b8\u2217 \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) best-response Jacobian Since the direct gradient (teal) computation is straightforward with the underlying automatic dif- ferentiation library, the major challenge in GBML lies in computing the best-response Jacobian (purple), of which two common solutions are iterative differentiation [14, 15, 16, 42] and implicit differentiation [25, 40, 49, 51]. Between these two, in this paper we adopt implicit differentiation as our baseline solution to GBML, as it achieves better computation and memory efficiency than iterative differentiation [19], both of which are vital in accomplishing our goal of scalable GBML. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 (cid:124) (cid:123)(cid:122) (cid:125) direct gradient = The gist of implicit differentiation is that it calculates the best-response Jacobian by leveraging Cauchy\u2019s Implicit Function Theorem (IFT) and re-interpreting the base optimization problem from the perspective of fixed-point iteration given an iterative solver u, as follows: \u2202\u03b8\u2217 \u2202\u03bb \u2202u \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) meta Jacobian = \u2212 (cid:18) \u2202u \u2202\u03b8\u2217 (cid:124)(cid:123)(cid:122)(cid:125) base Jacobian (cid:19)\u22121 where (cid:40) \u03b8\u2217 = lim t\u2192\u221e \u03b8t \u03b8t = \u03b8t\u22121 \u2212 u(\u03b8t\u22121; \u03bb) While exact implicit differentiation requires solving the base optimization problem to convergence \u03b8\u2217 by repeatedly applying an iterative solver u (e.g., SGD or Adam) to calculate base (blue) and meta (red) Jacobians, this is computationally impractical, especially in most large-scale learning settings. Therefore, researchers oftentimes approximate \u03b8\u2217 with a small number of unrolled update steps of u. This results in a solution that alternates gradient descent between base and meta optimization problems, where the base gradient is calculated with standard backpropagation and the meta gradient with Eqs. (1) & (2). Noting that many techniques have been developed to efficiently perform and scale up standard backpropagation, we deduce that the major challenges in scaling GBML lie in meta gradient computation, which will be discussed in depth in the next section. 3 Scaling Meta Learning It has long been recognized that meta gradient computation in GBML suffers from a substantial compute/memory cost [40, 51], algorithmic instability [2, 12], and a lack of efficient distributed 3 (1) (2) training support [3, 6, 9], all of which can significantly limit its scalability. In this section, we first attempt to understand the above limitations at a technical level. Toward this end, we investigate three aspects in Eqs. (1) & (2), namely (i) base Jacobian inversion, (ii) algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of meta gradient computation, and discuss how they lead to the aforementioned limitations. Next, we propose initial solutions for each of these issues, based on which we build a holistic and ScalAble Meta Learning Algorithm, SAMA. 3.1 Base Jacobian Inverse Problem Denoting the size of the base learner (i.e., dim(\u03b8)) as nb, the computational complexity of the naive base Jacobian (blue) inversion in Eq. (2) is O(n3 b). Since such cubic complexity is impracti- cal even for small-sized base learners, practitioners typically utilize various linear systems techniques such as Neumann series [40] or conjugate gradient [51], and directly approximate (cid:0) \u2202u (cid:1)\u22121 \u2202Lmeta . \u2202\u03b8\u2217 Given that the base Jacobian is a function of the Hessian matrix of the base optimization problem in GBML, these algorithms solve linear systems by iteratively performing Hessian-vector products. However, this Hessian-vector product computation contributes negatively to all three above limita- tions. First, while many automatic differentiation engines provide an efficient Hessian-vector product implementation like Pearlmutter\u2019s algorithm [48], its memory/compute cost is still prohibitive with larger models, as demonstrated in Fig. 1. Second, in most cases, we only have"}], "doc_text": "[35]. Combining all these solutions, we develop SAMA, a holistic and practically ScalAble Meta learning Algorithm. 3. We evaluate the scalability and the overall performance of SAMA on a multitude of large-scale meta learning benchmarks involving large language models (e.g., BERT [31] and RoBERTa [39]) or large datasets (e.g., ImageNet-1k [10]). Notably, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. In addition, we observe that SAMA- based data optimization consistently leads to improvements in text classification accuracy with large language models, and achieves state-of-the-art results in both small-/large-scale data pruning, demonstrating the initial potential of scalable meta learning. 2 2 Background: Gradient-Based Meta Learning We begin by reviewing the basics of (gradient-based) meta learning in order to establish the key aspects that have limited its scalability. Mathematically, meta learning is commonly formulated as bilevel optimization as follows: \u03bb\u2217 = argmin Lmeta(Dmeta; \u03b8\u2217(\u03bb)) \u03bb s.t. \u03b8\u2217(\u03bb) = argmin Lbase(Dbase; \u03b8, \u03bb) \u03b8 where \u03bb (respectively, \u03b8) are the parameters of meta (base) learners, Dmeta (Dbase) are meta (base) datasets, and Lmeta (Lbase) are meta (base) loss functions. An important implication of the above formulation is that meta learning changes the task of finding the optimal inductive biases from designing heuristics to designing meta optimization problems. As an example, consider the problem of finding the optimal inductive bias for fair classification given class-imbalanced training data. A traditional approach to this problem is to use a heuristic that reweights training samples inversely proportional to class frequencies. On the contrary, L2RW [54] designs a meta optimization problem by curating a small number of class-balanced data for the meta dataset Dmeta and setting the meta learner \u03bb to be importance weights for all training data. In short, unlike heuristic-based methods that explicitly specify \u201chow to learn,\u201d meta learning methods only specify \u201cwhat to learn\u201d and let the meta learner automatically determine \u201chow.\u201d From a programming paradigm perspective, such a difference can be understood as a transition from imperative to declarative programming. While there are multiple approaches to solving meta learning, in this work we focus on gradient- based approaches due to their ability to efficiently solve high-dimensional meta optimization (i.e. dim(\u03bb) \u226b 1) problems. Such an ability is essential given that the search space for inductive biases (e.g. importance weights for training data) can increase exponentially with recent large models and datasets. Concretely, GBML computes a meta gradient composed of two terms\u2014the best-response Jacobian and direct gradient\u2014with the chain rule, as follows: \u2202\u03b8\u2217 \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) best-response Jacobian Since the direct gradient (teal) computation is straightforward with the underlying automatic dif- ferentiation library, the major challenge in GBML lies in computing the best-response Jacobian (purple), of which two common solutions are iterative differentiation [14, 15, 16, 42] and implicit differentiation [25, 40, 49, 51]. Between these two, in this paper we adopt implicit differentiation as our baseline solution to GBML, as it achieves better computation and memory efficiency than iterative differentiation [19], both of which are vital in accomplishing our goal of scalable GBML. \u2202Lmeta \u2202\u03bb \u2202Lmeta \u2202\u03b8\u2217 (cid:124) (cid:123)(cid:122) (cid:125) direct gradient = The gist of implicit differentiation is that it calculates the best-response Jacobian by leveraging Cauchy\u2019s Implicit Function Theorem (IFT) and re-interpreting the base optimization problem from the perspective of fixed-point iteration given an iterative solver u, as follows: \u2202\u03b8\u2217 \u2202\u03bb \u2202u \u2202\u03bb (cid:124)(cid:123)(cid:122)(cid:125) meta Jacobian = \u2212 (cid:18) \u2202u \u2202\u03b8\u2217 (cid:124)(cid:123)(cid:122)(cid:125) base Jacobian (cid:19)\u22121 where (cid:40) \u03b8\u2217 = lim t\u2192\u221e \u03b8t \u03b8t = \u03b8t\u22121 \u2212 u(\u03b8t\u22121; \u03bb) While exact implicit differentiation requires solving the base optimization problem to convergence \u03b8\u2217 by repeatedly applying an iterative solver u (e.g., SGD or Adam) to calculate base (blue) and meta (red) Jacobians, this is computationally impractical, especially in most large-scale learning settings. Therefore, researchers oftentimes approximate \u03b8\u2217 with a small number of unrolled update steps of u. This results in a solution that alternates gradient descent between base and meta optimization problems, where the base gradient is calculated with standard backpropagation and the meta gradient with Eqs. (1) & (2). Noting that many techniques have been developed to efficiently perform and scale up standard backpropagation, we deduce that the major challenges in scaling GBML lie in meta gradient computation, which will be discussed in depth in the next section. 3 Scaling Meta Learning It has long been recognized that meta gradient computation in GBML suffers from a substantial compute/memory cost [40, 51], algorithmic instability [2, 12], and a lack of efficient distributed 3 (1) (2) training support [3, 6, 9], all of which can significantly limit its scalability. In this section, we first attempt to understand the above limitations at a technical level. Toward this end, we investigate three aspects in Eqs. (1) & (2), namely (i) base Jacobian inversion, (ii) algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of meta gradient computation, and discuss how they lead to the aforementioned limitations. Next, we propose initial solutions for each of these issues, based on which we build a holistic and ScalAble Meta Learning Algorithm, SAMA. 3.1 Base Jacobian Inverse Problem Denoting the size of the base learner (i.e., dim(\u03b8)) as nb, the computational complexity of the naive base Jacobian (blue) inversion in Eq. (2) is O(n3 b). Since such cubic complexity is impracti- cal even for small-sized base learners, practitioners typically utilize various linear systems techniques such as Neumann series [40] or conjugate gradient [51], and directly approximate (cid:0) \u2202u (cid:1)\u22121 \u2202Lmeta . \u2202\u03b8\u2217 Given that the base Jacobian is a function of the Hessian matrix of the base optimization problem in GBML, these algorithms solve linear systems by iteratively performing Hessian-vector products. However, this Hessian-vector product computation contributes negatively to all three above limita- tions. First, while many automatic differentiation engines provide an efficient Hessian-vector product implementation like Pearlmutter\u2019s algorithm [48], its memory/compute cost is still prohibitive with larger models, as demonstrated in Fig. 1. Second, in most cases, we only have"}