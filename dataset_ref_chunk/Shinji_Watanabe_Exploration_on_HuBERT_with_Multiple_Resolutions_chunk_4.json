{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Exploration_on_HuBERT_with_Multiple_Resolutions_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What were the three different resolutions of the HuBERT models that were analyzed?", "answer": " The three resolutions were 100ms, 40ms, and 20ms.", "ref_chunk": "features from the same speech source in dis- tinct ways that are useful for downstream tasks. To investigate these differences, we analyzed the features extracted by three pre-trained HuBERT models with 100ms, 40ms, and 20ms res- olutions. Specifically, we extracted the 6th layer representations and used them as input features to train a HiFi-GAN vocoder [34] with the ParallelWaveGAN toolkit [35, 36].3 We trained the vocoder on the LJSpeech dataset and adapted the upsam- pling modules to match the resolution of each HuBERT model. The vocoder was trained for 50k steps using the same config- uration as the ParallelWaveGAN toolkit. Finally, we generated and compared the spectrograms of synthesized test-set speech produced from different representations, as shown in Figure 2.4 The followings are some interesting findings: HuBERT features at different resolutions are capable of pro- ducing high-quality re-synthesized speech. Despite not per- forming well on ASR fine-tuning tasks (as shown in Table 4), HuBERT with 100ms resolution exhibits excellent speech re- synthesis quality. This suggests that the feature still contains In this study, we revisit the use of HuBERT with multiple res- olutions, recognizing that the original 20ms resolution may not be optimal for various downstream tasks. To address this, we propose HuBERT-MR, which integrates information from three HuBERT base models pre-trained with different resolutions. We examine two approaches for integration: a parallel approach (HuBERT-MR-P) and a hierarchical approach (HuBERT-MR- H). We evaluate HuBERT-MR-P on the SUPERB benchmark and both HuBERT-MR models on ASR fine-tuning. Our exper- iments demonstrate that the HuBERT-MR models significantly improve model performance on various downstream tasks, in- dicating that pre-trained features from multiple resolutions are complementary. Furthermore, we find that HuBERT-MR can outperform larger models in some scenarios, even with less pre- training data and fewer parameters. To further highlight the dif- ferences among HuBERT features at different resolutions, we conduct speech re-synthesis with the HiFi-GAN vocoder. Our results demonstrate that the features do differ across resolutions, while all retain the essential information for intelligibility. We believe this work offers valuable insights into the potential ben- efits of considering multi-resolution SSL in the field. 5. Acknowledgement 3https://github.com/kan-bayashi/ParallelWaveGAN. 4Synthesized in examples audio can be found https://www.dropbox.com/s/61ap65iegii93il/ audio-samples-resynthesis.zip. the This work was supported by a Meta AI SRA grant. J. Shi and S. Watanabe are funded in part of the Delta project, supported by the NSF (award OCI 2005572), and the State of Illinois. 6. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [2] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech representa- tion learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021. [3] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [4] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [5] T.-h. Feng et al., \u201cSUPERB@ SLT 2022: Challenge on gener- alization and efficiency of self-supervised speech representation learning,\u201d in Proc. SLT, 2023, pp. 1096\u20131103. [6] X. Chang et al., \u201cAn exploration of self-supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021, pp. 228\u2013235. [7] D. Berrebbi et al., \u201cCombining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Transla- tion,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [8] S. H. Mallidi and H. Hermansky, \u201cNovel neural network based fusion for multistream asr,\u201d in Proc. ICASSP, 2016, pp. 5680\u2013 5684. [9] S. H. R. Mallidi et al., \u201cA practical and efficient multistream framework for noise robust speech recognition,\u201d Ph.D. disserta- tion, Johns Hopkins University, 2018. [10] H. Hermansky, \u201cMultistream recognition of speech: Dealing with unknown unknowns,\u201d Proceedings of the IEEE, vol. 101, no. 5, pp. 1076\u20131088, 2013. [11] K. J. Han et al., \u201cMultistream cnn for robust acoustic modeling,\u201d in Proc. ICASSP, 2021, pp. 6873\u20136877. [12] J. Luo et al., \u201cMulti-quartznet: Multi-resolution convolution for speech recognition with multi-layer feature fusion,\u201d in Proc. SLT, 2021, pp. 82\u201388. [13] R. Li et al., \u201cMulti-stream end-to-end speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Pro- cessing, vol. 28, pp. 646\u2013655, 2019. [14] Y. Kong et al., \u201cMulti-channel automatic speech recognition us- ing deep complex unet,\u201d in Proc. SLT, 2021, pp. 104\u2013110. [15] A. Andrusenko, R. Nasretdinov, and A. Romanenko, \u201cUconv- conformer: High reduction of input sequence length for end- to-end speech recognition,\u201d arXiv preprint arXiv:2208.07657, 2022. [16] S. Kim et al., \u201cSqueezeformer: An efficient transformer for au- tomatic speech recognition,\u201d in Proc. NeurIPS. [17] W. Yao et al., \u201cMulti-stream convolutional neural network with frequency selection for robust speaker verification,\u201d arXiv preprint arXiv:2012.11159, 2020. [18] Z. Gao, M.-W. Mak, and W. Lin, \u201cUnet-densenet for robust far- field speaker verification,\u201d Proc. Interspeech 2022, pp. 3714\u2013 3718, 2022. [19] M. Burchi and V. Vielzeuf, \u201cEfficient conformer: Progres- sive downsampling and grouped attention for automatic speech recognition,\u201d in Proc. ASRU, 2021, pp. 8\u201315. [20] Y. Zhang et al., \u201cResearch on speech enhancement algorithm based on sa-unet,\u201d in 2019 4th International Conference on Me- chanical, Control and Computer Engineering (ICMCCE), 2019, pp. 818\u20138183. [21] T. Zhao et al., \u201cUnet++-based multi-channel speech dereverber- ation and distant speech recognition,\u201d in 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2021, pp. 1\u20135. [22] R. Li et al., \u201cUnet-tts: Improving unseen speaker and style trans- fer in one-shot voice cloning,\u201d in Proc. ICASSP, 2022, pp. 8327\u2013 8331. [23] X. Xiang, X. Zhang, and H. Chen, \u201cA nested u-net with self- attention and dense connectivity for monaural speech enhance- ment,\u201d IEEE Signal Processing Letters, vol. 29, pp. 105\u2013109, 2021. [24] Y. Xian et al., \u201cA multi-scale feature recalibration network for end-to-end single channel speech enhancement,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 1, pp. 143\u2013 155, 2020. [25] G. Liu et al., \u201cCp-GAN: Context pyramid generative adversar- ial network for speech enhancement,\u201d in Proc. ICASSP, 2020, pp. 6624\u20136628. [26] X. Xiang, X. Zhang, and H. Chen, \u201cA convolutional net- work with multi-scale and"}, {"question": " What dataset was used to train the HiFi-GAN vocoder?", "answer": " The vocoder was trained on the LJSpeech dataset.", "ref_chunk": "features from the same speech source in dis- tinct ways that are useful for downstream tasks. To investigate these differences, we analyzed the features extracted by three pre-trained HuBERT models with 100ms, 40ms, and 20ms res- olutions. Specifically, we extracted the 6th layer representations and used them as input features to train a HiFi-GAN vocoder [34] with the ParallelWaveGAN toolkit [35, 36].3 We trained the vocoder on the LJSpeech dataset and adapted the upsam- pling modules to match the resolution of each HuBERT model. The vocoder was trained for 50k steps using the same config- uration as the ParallelWaveGAN toolkit. Finally, we generated and compared the spectrograms of synthesized test-set speech produced from different representations, as shown in Figure 2.4 The followings are some interesting findings: HuBERT features at different resolutions are capable of pro- ducing high-quality re-synthesized speech. Despite not per- forming well on ASR fine-tuning tasks (as shown in Table 4), HuBERT with 100ms resolution exhibits excellent speech re- synthesis quality. This suggests that the feature still contains In this study, we revisit the use of HuBERT with multiple res- olutions, recognizing that the original 20ms resolution may not be optimal for various downstream tasks. To address this, we propose HuBERT-MR, which integrates information from three HuBERT base models pre-trained with different resolutions. We examine two approaches for integration: a parallel approach (HuBERT-MR-P) and a hierarchical approach (HuBERT-MR- H). We evaluate HuBERT-MR-P on the SUPERB benchmark and both HuBERT-MR models on ASR fine-tuning. Our exper- iments demonstrate that the HuBERT-MR models significantly improve model performance on various downstream tasks, in- dicating that pre-trained features from multiple resolutions are complementary. Furthermore, we find that HuBERT-MR can outperform larger models in some scenarios, even with less pre- training data and fewer parameters. To further highlight the dif- ferences among HuBERT features at different resolutions, we conduct speech re-synthesis with the HiFi-GAN vocoder. Our results demonstrate that the features do differ across resolutions, while all retain the essential information for intelligibility. We believe this work offers valuable insights into the potential ben- efits of considering multi-resolution SSL in the field. 5. Acknowledgement 3https://github.com/kan-bayashi/ParallelWaveGAN. 4Synthesized in examples audio can be found https://www.dropbox.com/s/61ap65iegii93il/ audio-samples-resynthesis.zip. the This work was supported by a Meta AI SRA grant. J. Shi and S. Watanabe are funded in part of the Delta project, supported by the NSF (award OCI 2005572), and the State of Illinois. 6. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [2] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech representa- tion learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021. [3] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [4] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [5] T.-h. Feng et al., \u201cSUPERB@ SLT 2022: Challenge on gener- alization and efficiency of self-supervised speech representation learning,\u201d in Proc. SLT, 2023, pp. 1096\u20131103. [6] X. Chang et al., \u201cAn exploration of self-supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021, pp. 228\u2013235. [7] D. Berrebbi et al., \u201cCombining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Transla- tion,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [8] S. H. Mallidi and H. Hermansky, \u201cNovel neural network based fusion for multistream asr,\u201d in Proc. ICASSP, 2016, pp. 5680\u2013 5684. [9] S. H. R. Mallidi et al., \u201cA practical and efficient multistream framework for noise robust speech recognition,\u201d Ph.D. disserta- tion, Johns Hopkins University, 2018. [10] H. Hermansky, \u201cMultistream recognition of speech: Dealing with unknown unknowns,\u201d Proceedings of the IEEE, vol. 101, no. 5, pp. 1076\u20131088, 2013. [11] K. J. Han et al., \u201cMultistream cnn for robust acoustic modeling,\u201d in Proc. ICASSP, 2021, pp. 6873\u20136877. [12] J. Luo et al., \u201cMulti-quartznet: Multi-resolution convolution for speech recognition with multi-layer feature fusion,\u201d in Proc. SLT, 2021, pp. 82\u201388. [13] R. Li et al., \u201cMulti-stream end-to-end speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Pro- cessing, vol. 28, pp. 646\u2013655, 2019. [14] Y. Kong et al., \u201cMulti-channel automatic speech recognition us- ing deep complex unet,\u201d in Proc. SLT, 2021, pp. 104\u2013110. [15] A. Andrusenko, R. Nasretdinov, and A. Romanenko, \u201cUconv- conformer: High reduction of input sequence length for end- to-end speech recognition,\u201d arXiv preprint arXiv:2208.07657, 2022. [16] S. Kim et al., \u201cSqueezeformer: An efficient transformer for au- tomatic speech recognition,\u201d in Proc. NeurIPS. [17] W. Yao et al., \u201cMulti-stream convolutional neural network with frequency selection for robust speaker verification,\u201d arXiv preprint arXiv:2012.11159, 2020. [18] Z. Gao, M.-W. Mak, and W. Lin, \u201cUnet-densenet for robust far- field speaker verification,\u201d Proc. Interspeech 2022, pp. 3714\u2013 3718, 2022. [19] M. Burchi and V. Vielzeuf, \u201cEfficient conformer: Progres- sive downsampling and grouped attention for automatic speech recognition,\u201d in Proc. ASRU, 2021, pp. 8\u201315. [20] Y. Zhang et al., \u201cResearch on speech enhancement algorithm based on sa-unet,\u201d in 2019 4th International Conference on Me- chanical, Control and Computer Engineering (ICMCCE), 2019, pp. 818\u20138183. [21] T. Zhao et al., \u201cUnet++-based multi-channel speech dereverber- ation and distant speech recognition,\u201d in 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2021, pp. 1\u20135. [22] R. Li et al., \u201cUnet-tts: Improving unseen speaker and style trans- fer in one-shot voice cloning,\u201d in Proc. ICASSP, 2022, pp. 8327\u2013 8331. [23] X. Xiang, X. Zhang, and H. Chen, \u201cA nested u-net with self- attention and dense connectivity for monaural speech enhance- ment,\u201d IEEE Signal Processing Letters, vol. 29, pp. 105\u2013109, 2021. [24] Y. Xian et al., \u201cA multi-scale feature recalibration network for end-to-end single channel speech enhancement,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 1, pp. 143\u2013 155, 2020. [25] G. Liu et al., \u201cCp-GAN: Context pyramid generative adversar- ial network for speech enhancement,\u201d in Proc. ICASSP, 2020, pp. 6624\u20136628. [26] X. Xiang, X. Zhang, and H. Chen, \u201cA convolutional net- work with multi-scale and"}, {"question": " What were the two approaches for integrating information in the HuBERT-MR model?", "answer": " The two approaches were a parallel approach (HuBERT-MR-P) and a hierarchical approach (HuBERT-MR-H).", "ref_chunk": "features from the same speech source in dis- tinct ways that are useful for downstream tasks. To investigate these differences, we analyzed the features extracted by three pre-trained HuBERT models with 100ms, 40ms, and 20ms res- olutions. Specifically, we extracted the 6th layer representations and used them as input features to train a HiFi-GAN vocoder [34] with the ParallelWaveGAN toolkit [35, 36].3 We trained the vocoder on the LJSpeech dataset and adapted the upsam- pling modules to match the resolution of each HuBERT model. The vocoder was trained for 50k steps using the same config- uration as the ParallelWaveGAN toolkit. Finally, we generated and compared the spectrograms of synthesized test-set speech produced from different representations, as shown in Figure 2.4 The followings are some interesting findings: HuBERT features at different resolutions are capable of pro- ducing high-quality re-synthesized speech. Despite not per- forming well on ASR fine-tuning tasks (as shown in Table 4), HuBERT with 100ms resolution exhibits excellent speech re- synthesis quality. This suggests that the feature still contains In this study, we revisit the use of HuBERT with multiple res- olutions, recognizing that the original 20ms resolution may not be optimal for various downstream tasks. To address this, we propose HuBERT-MR, which integrates information from three HuBERT base models pre-trained with different resolutions. We examine two approaches for integration: a parallel approach (HuBERT-MR-P) and a hierarchical approach (HuBERT-MR- H). We evaluate HuBERT-MR-P on the SUPERB benchmark and both HuBERT-MR models on ASR fine-tuning. Our exper- iments demonstrate that the HuBERT-MR models significantly improve model performance on various downstream tasks, in- dicating that pre-trained features from multiple resolutions are complementary. Furthermore, we find that HuBERT-MR can outperform larger models in some scenarios, even with less pre- training data and fewer parameters. To further highlight the dif- ferences among HuBERT features at different resolutions, we conduct speech re-synthesis with the HiFi-GAN vocoder. Our results demonstrate that the features do differ across resolutions, while all retain the essential information for intelligibility. We believe this work offers valuable insights into the potential ben- efits of considering multi-resolution SSL in the field. 5. Acknowledgement 3https://github.com/kan-bayashi/ParallelWaveGAN. 4Synthesized in examples audio can be found https://www.dropbox.com/s/61ap65iegii93il/ audio-samples-resynthesis.zip. the This work was supported by a Meta AI SRA grant. J. Shi and S. Watanabe are funded in part of the Delta project, supported by the NSF (award OCI 2005572), and the State of Illinois. 6. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [2] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech representa- tion learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021. [3] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [4] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [5] T.-h. Feng et al., \u201cSUPERB@ SLT 2022: Challenge on gener- alization and efficiency of self-supervised speech representation learning,\u201d in Proc. SLT, 2023, pp. 1096\u20131103. [6] X. Chang et al., \u201cAn exploration of self-supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021, pp. 228\u2013235. [7] D. Berrebbi et al., \u201cCombining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Transla- tion,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [8] S. H. Mallidi and H. Hermansky, \u201cNovel neural network based fusion for multistream asr,\u201d in Proc. ICASSP, 2016, pp. 5680\u2013 5684. [9] S. H. R. Mallidi et al., \u201cA practical and efficient multistream framework for noise robust speech recognition,\u201d Ph.D. disserta- tion, Johns Hopkins University, 2018. [10] H. Hermansky, \u201cMultistream recognition of speech: Dealing with unknown unknowns,\u201d Proceedings of the IEEE, vol. 101, no. 5, pp. 1076\u20131088, 2013. [11] K. J. Han et al., \u201cMultistream cnn for robust acoustic modeling,\u201d in Proc. ICASSP, 2021, pp. 6873\u20136877. [12] J. Luo et al., \u201cMulti-quartznet: Multi-resolution convolution for speech recognition with multi-layer feature fusion,\u201d in Proc. SLT, 2021, pp. 82\u201388. [13] R. Li et al., \u201cMulti-stream end-to-end speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Pro- cessing, vol. 28, pp. 646\u2013655, 2019. [14] Y. Kong et al., \u201cMulti-channel automatic speech recognition us- ing deep complex unet,\u201d in Proc. SLT, 2021, pp. 104\u2013110. [15] A. Andrusenko, R. Nasretdinov, and A. Romanenko, \u201cUconv- conformer: High reduction of input sequence length for end- to-end speech recognition,\u201d arXiv preprint arXiv:2208.07657, 2022. [16] S. Kim et al., \u201cSqueezeformer: An efficient transformer for au- tomatic speech recognition,\u201d in Proc. NeurIPS. [17] W. Yao et al., \u201cMulti-stream convolutional neural network with frequency selection for robust speaker verification,\u201d arXiv preprint arXiv:2012.11159, 2020. [18] Z. Gao, M.-W. Mak, and W. Lin, \u201cUnet-densenet for robust far- field speaker verification,\u201d Proc. Interspeech 2022, pp. 3714\u2013 3718, 2022. [19] M. Burchi and V. Vielzeuf, \u201cEfficient conformer: Progres- sive downsampling and grouped attention for automatic speech recognition,\u201d in Proc. ASRU, 2021, pp. 8\u201315. [20] Y. Zhang et al., \u201cResearch on speech enhancement algorithm based on sa-unet,\u201d in 2019 4th International Conference on Me- chanical, Control and Computer Engineering (ICMCCE), 2019, pp. 818\u20138183. [21] T. Zhao et al., \u201cUnet++-based multi-channel speech dereverber- ation and distant speech recognition,\u201d in 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2021, pp. 1\u20135. [22] R. Li et al., \u201cUnet-tts: Improving unseen speaker and style trans- fer in one-shot voice cloning,\u201d in Proc. ICASSP, 2022, pp. 8327\u2013 8331. [23] X. Xiang, X. Zhang, and H. Chen, \u201cA nested u-net with self- attention and dense connectivity for monaural speech enhance- ment,\u201d IEEE Signal Processing Letters, vol. 29, pp. 105\u2013109, 2021. [24] Y. Xian et al., \u201cA multi-scale feature recalibration network for end-to-end single channel speech enhancement,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 1, pp. 143\u2013 155, 2020. [25] G. Liu et al., \u201cCp-GAN: Context pyramid generative adversar- ial network for speech enhancement,\u201d in Proc. ICASSP, 2020, pp. 6624\u20136628. [26] X. Xiang, X. Zhang, and H. Chen, \u201cA convolutional net- work with multi-scale and"}, {"question": " What benchmark was HuBERT-MR-P evaluated on?", "answer": " HuBERT-MR-P was evaluated on the SUPERB benchmark.", "ref_chunk": "features from the same speech source in dis- tinct ways that are useful for downstream tasks. To investigate these differences, we analyzed the features extracted by three pre-trained HuBERT models with 100ms, 40ms, and 20ms res- olutions. Specifically, we extracted the 6th layer representations and used them as input features to train a HiFi-GAN vocoder [34] with the ParallelWaveGAN toolkit [35, 36].3 We trained the vocoder on the LJSpeech dataset and adapted the upsam- pling modules to match the resolution of each HuBERT model. The vocoder was trained for 50k steps using the same config- uration as the ParallelWaveGAN toolkit. Finally, we generated and compared the spectrograms of synthesized test-set speech produced from different representations, as shown in Figure 2.4 The followings are some interesting findings: HuBERT features at different resolutions are capable of pro- ducing high-quality re-synthesized speech. Despite not per- forming well on ASR fine-tuning tasks (as shown in Table 4), HuBERT with 100ms resolution exhibits excellent speech re- synthesis quality. This suggests that the feature still contains In this study, we revisit the use of HuBERT with multiple res- olutions, recognizing that the original 20ms resolution may not be optimal for various downstream tasks. To address this, we propose HuBERT-MR, which integrates information from three HuBERT base models pre-trained with different resolutions. We examine two approaches for integration: a parallel approach (HuBERT-MR-P) and a hierarchical approach (HuBERT-MR- H). We evaluate HuBERT-MR-P on the SUPERB benchmark and both HuBERT-MR models on ASR fine-tuning. Our exper- iments demonstrate that the HuBERT-MR models significantly improve model performance on various downstream tasks, in- dicating that pre-trained features from multiple resolutions are complementary. Furthermore, we find that HuBERT-MR can outperform larger models in some scenarios, even with less pre- training data and fewer parameters. To further highlight the dif- ferences among HuBERT features at different resolutions, we conduct speech re-synthesis with the HiFi-GAN vocoder. Our results demonstrate that the features do differ across resolutions, while all retain the essential information for intelligibility. We believe this work offers valuable insights into the potential ben- efits of considering multi-resolution SSL in the field. 5. Acknowledgement 3https://github.com/kan-bayashi/ParallelWaveGAN. 4Synthesized in examples audio can be found https://www.dropbox.com/s/61ap65iegii93il/ audio-samples-resynthesis.zip. the This work was supported by a Meta AI SRA grant. J. Shi and S. Watanabe are funded in part of the Delta project, supported by the NSF (award OCI 2005572), and the State of Illinois. 6. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [2] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech representa- tion learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021. [3] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [4] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [5] T.-h. Feng et al., \u201cSUPERB@ SLT 2022: Challenge on gener- alization and efficiency of self-supervised speech representation learning,\u201d in Proc. SLT, 2023, pp. 1096\u20131103. [6] X. Chang et al., \u201cAn exploration of self-supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021, pp. 228\u2013235. [7] D. Berrebbi et al., \u201cCombining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Transla- tion,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [8] S. H. Mallidi and H. Hermansky, \u201cNovel neural network based fusion for multistream asr,\u201d in Proc. ICASSP, 2016, pp. 5680\u2013 5684. [9] S. H. R. Mallidi et al., \u201cA practical and efficient multistream framework for noise robust speech recognition,\u201d Ph.D. disserta- tion, Johns Hopkins University, 2018. [10] H. Hermansky, \u201cMultistream recognition of speech: Dealing with unknown unknowns,\u201d Proceedings of the IEEE, vol. 101, no. 5, pp. 1076\u20131088, 2013. [11] K. J. Han et al., \u201cMultistream cnn for robust acoustic modeling,\u201d in Proc. ICASSP, 2021, pp. 6873\u20136877. [12] J. Luo et al., \u201cMulti-quartznet: Multi-resolution convolution for speech recognition with multi-layer feature fusion,\u201d in Proc. SLT, 2021, pp. 82\u201388. [13] R. Li et al., \u201cMulti-stream end-to-end speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Pro- cessing, vol. 28, pp. 646\u2013655, 2019. [14] Y. Kong et al., \u201cMulti-channel automatic speech recognition us- ing deep complex unet,\u201d in Proc. SLT, 2021, pp. 104\u2013110. [15] A. Andrusenko, R. Nasretdinov, and A. Romanenko, \u201cUconv- conformer: High reduction of input sequence length for end- to-end speech recognition,\u201d arXiv preprint arXiv:2208.07657, 2022. [16] S. Kim et al., \u201cSqueezeformer: An efficient transformer for au- tomatic speech recognition,\u201d in Proc. NeurIPS. [17] W. Yao et al., \u201cMulti-stream convolutional neural network with frequency selection for robust speaker verification,\u201d arXiv preprint arXiv:2012.11159, 2020. [18] Z. Gao, M.-W. Mak, and W. Lin, \u201cUnet-densenet for robust far- field speaker verification,\u201d Proc. Interspeech 2022, pp. 3714\u2013 3718, 2022. [19] M. Burchi and V. Vielzeuf, \u201cEfficient conformer: Progres- sive downsampling and grouped attention for automatic speech recognition,\u201d in Proc. ASRU, 2021, pp. 8\u201315. [20] Y. Zhang et al., \u201cResearch on speech enhancement algorithm based on sa-unet,\u201d in 2019 4th International Conference on Me- chanical, Control and Computer Engineering (ICMCCE), 2019, pp. 818\u20138183. [21] T. Zhao et al., \u201cUnet++-based multi-channel speech dereverber- ation and distant speech recognition,\u201d in 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2021, pp. 1\u20135. [22] R. Li et al., \u201cUnet-tts: Improving unseen speaker and style trans- fer in one-shot voice cloning,\u201d in Proc. ICASSP, 2022, pp. 8327\u2013 8331. [23] X. Xiang, X. Zhang, and H. Chen, \u201cA nested u-net with self- attention and dense connectivity for monaural speech enhance- ment,\u201d IEEE Signal Processing Letters, vol. 29, pp. 105\u2013109, 2021. [24] Y. Xian et al., \u201cA multi-scale feature recalibration network for end-to-end single channel speech enhancement,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 1, pp. 143\u2013 155, 2020. [25] G. Liu et al., \u201cCp-GAN: Context pyramid generative adversar- ial network for speech enhancement,\u201d in Proc. ICASSP, 2020, pp. 6624\u20136628. [26] X. Xiang, X. Zhang, and H. Chen, \u201cA convolutional net- work with multi-scale and"}, {"question": " What do the experiments with the HuBERT-MR models demonstrate about model performance?", "answer": " The experiments demonstrate that the HuBERT-MR models significantly improve model performance on various downstream tasks.", "ref_chunk": "features from the same speech source in dis- tinct ways that are useful for downstream tasks. To investigate these differences, we analyzed the features extracted by three pre-trained HuBERT models with 100ms, 40ms, and 20ms res- olutions. Specifically, we extracted the 6th layer representations and used them as input features to train a HiFi-GAN vocoder [34] with the ParallelWaveGAN toolkit [35, 36].3 We trained the vocoder on the LJSpeech dataset and adapted the upsam- pling modules to match the resolution of each HuBERT model. The vocoder was trained for 50k steps using the same config- uration as the ParallelWaveGAN toolkit. Finally, we generated and compared the spectrograms of synthesized test-set speech produced from different representations, as shown in Figure 2.4 The followings are some interesting findings: HuBERT features at different resolutions are capable of pro- ducing high-quality re-synthesized speech. Despite not per- forming well on ASR fine-tuning tasks (as shown in Table 4), HuBERT with 100ms resolution exhibits excellent speech re- synthesis quality. This suggests that the feature still contains In this study, we revisit the use of HuBERT with multiple res- olutions, recognizing that the original 20ms resolution may not be optimal for various downstream tasks. To address this, we propose HuBERT-MR, which integrates information from three HuBERT base models pre-trained with different resolutions. We examine two approaches for integration: a parallel approach (HuBERT-MR-P) and a hierarchical approach (HuBERT-MR- H). We evaluate HuBERT-MR-P on the SUPERB benchmark and both HuBERT-MR models on ASR fine-tuning. Our exper- iments demonstrate that the HuBERT-MR models significantly improve model performance on various downstream tasks, in- dicating that pre-trained features from multiple resolutions are complementary. Furthermore, we find that HuBERT-MR can outperform larger models in some scenarios, even with less pre- training data and fewer parameters. To further highlight the dif- ferences among HuBERT features at different resolutions, we conduct speech re-synthesis with the HiFi-GAN vocoder. Our results demonstrate that the features do differ across resolutions, while all retain the essential information for intelligibility. We believe this work offers valuable insights into the potential ben- efits of considering multi-resolution SSL in the field. 5. Acknowledgement 3https://github.com/kan-bayashi/ParallelWaveGAN. 4Synthesized in examples audio can be found https://www.dropbox.com/s/61ap65iegii93il/ audio-samples-resynthesis.zip. the This work was supported by a Meta AI SRA grant. J. Shi and S. Watanabe are funded in part of the Delta project, supported by the NSF (award OCI 2005572), and the State of Illinois. 6. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [2] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech representa- tion learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021. [3] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [4] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [5] T.-h. Feng et al., \u201cSUPERB@ SLT 2022: Challenge on gener- alization and efficiency of self-supervised speech representation learning,\u201d in Proc. SLT, 2023, pp. 1096\u20131103. [6] X. Chang et al., \u201cAn exploration of self-supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021, pp. 228\u2013235. [7] D. Berrebbi et al., \u201cCombining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Transla- tion,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [8] S. H. Mallidi and H. Hermansky, \u201cNovel neural network based fusion for multistream asr,\u201d in Proc. ICASSP, 2016, pp. 5680\u2013 5684. [9] S. H. R. Mallidi et al., \u201cA practical and efficient multistream framework for noise robust speech recognition,\u201d Ph.D. disserta- tion, Johns Hopkins University, 2018. [10] H. Hermansky, \u201cMultistream recognition of speech: Dealing with unknown unknowns,\u201d Proceedings of the IEEE, vol. 101, no. 5, pp. 1076\u20131088, 2013. [11] K. J. Han et al., \u201cMultistream cnn for robust acoustic modeling,\u201d in Proc. ICASSP, 2021, pp. 6873\u20136877. [12] J. Luo et al., \u201cMulti-quartznet: Multi-resolution convolution for speech recognition with multi-layer feature fusion,\u201d in Proc. SLT, 2021, pp. 82\u201388. [13] R. Li et al., \u201cMulti-stream end-to-end speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Pro- cessing, vol. 28, pp. 646\u2013655, 2019. [14] Y. Kong et al., \u201cMulti-channel automatic speech recognition us- ing deep complex unet,\u201d in Proc. SLT, 2021, pp. 104\u2013110. [15] A. Andrusenko, R. Nasretdinov, and A. Romanenko, \u201cUconv- conformer: High reduction of input sequence length for end- to-end speech recognition,\u201d arXiv preprint arXiv:2208.07657, 2022. [16] S. Kim et al., \u201cSqueezeformer: An efficient transformer for au- tomatic speech recognition,\u201d in Proc. NeurIPS. [17] W. Yao et al., \u201cMulti-stream convolutional neural network with frequency selection for robust speaker verification,\u201d arXiv preprint arXiv:2012.11159, 2020. [18] Z. Gao, M.-W. Mak, and W. Lin, \u201cUnet-densenet for robust far- field speaker verification,\u201d Proc. Interspeech 2022, pp. 3714\u2013 3718, 2022. [19] M. Burchi and V. Vielzeuf, \u201cEfficient conformer: Progres- sive downsampling and grouped attention for automatic speech recognition,\u201d in Proc. ASRU, 2021, pp. 8\u201315. [20] Y. Zhang et al., \u201cResearch on speech enhancement algorithm based on sa-unet,\u201d in 2019 4th International Conference on Me- chanical, Control and Computer Engineering (ICMCCE), 2019, pp. 818\u20138183. [21] T. Zhao et al., \u201cUnet++-based multi-channel speech dereverber- ation and distant speech recognition,\u201d in 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2021, pp. 1\u20135. [22] R. Li et al., \u201cUnet-tts: Improving unseen speaker and style trans- fer in one-shot voice cloning,\u201d in Proc. ICASSP, 2022, pp. 8327\u2013 8331. [23] X. Xiang, X. Zhang, and H. Chen, \u201cA nested u-net with self- attention and dense connectivity for monaural speech enhance- ment,\u201d IEEE Signal Processing Letters, vol. 29, pp. 105\u2013109, 2021. [24] Y. Xian et al., \u201cA multi-scale feature recalibration network for end-to-end single channel speech enhancement,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 1, pp. 143\u2013 155, 2020. [25] G. Liu et al., \u201cCp-GAN: Context pyramid generative adversar- ial network for speech enhancement,\u201d in Proc. ICASSP, 2020, pp. 6624\u20136628. [26] X. Xiang, X. Zhang, and H. Chen, \u201cA convolutional net- work with multi-scale and"}, {"question": " What do the results of the speech re-synthesis with the HiFi-GAN vocoder show?", "answer": " The results demonstrate that the features differ across resolutions, while all retain the essential information for intelligibility.", "ref_chunk": "features from the same speech source in dis- tinct ways that are useful for downstream tasks. To investigate these differences, we analyzed the features extracted by three pre-trained HuBERT models with 100ms, 40ms, and 20ms res- olutions. Specifically, we extracted the 6th layer representations and used them as input features to train a HiFi-GAN vocoder [34] with the ParallelWaveGAN toolkit [35, 36].3 We trained the vocoder on the LJSpeech dataset and adapted the upsam- pling modules to match the resolution of each HuBERT model. The vocoder was trained for 50k steps using the same config- uration as the ParallelWaveGAN toolkit. Finally, we generated and compared the spectrograms of synthesized test-set speech produced from different representations, as shown in Figure 2.4 The followings are some interesting findings: HuBERT features at different resolutions are capable of pro- ducing high-quality re-synthesized speech. Despite not per- forming well on ASR fine-tuning tasks (as shown in Table 4), HuBERT with 100ms resolution exhibits excellent speech re- synthesis quality. This suggests that the feature still contains In this study, we revisit the use of HuBERT with multiple res- olutions, recognizing that the original 20ms resolution may not be optimal for various downstream tasks. To address this, we propose HuBERT-MR, which integrates information from three HuBERT base models pre-trained with different resolutions. We examine two approaches for integration: a parallel approach (HuBERT-MR-P) and a hierarchical approach (HuBERT-MR- H). We evaluate HuBERT-MR-P on the SUPERB benchmark and both HuBERT-MR models on ASR fine-tuning. Our exper- iments demonstrate that the HuBERT-MR models significantly improve model performance on various downstream tasks, in- dicating that pre-trained features from multiple resolutions are complementary. Furthermore, we find that HuBERT-MR can outperform larger models in some scenarios, even with less pre- training data and fewer parameters. To further highlight the dif- ferences among HuBERT features at different resolutions, we conduct speech re-synthesis with the HiFi-GAN vocoder. Our results demonstrate that the features do differ across resolutions, while all retain the essential information for intelligibility. We believe this work offers valuable insights into the potential ben- efits of considering multi-resolution SSL in the field. 5. Acknowledgement 3https://github.com/kan-bayashi/ParallelWaveGAN. 4Synthesized in examples audio can be found https://www.dropbox.com/s/61ap65iegii93il/ audio-samples-resynthesis.zip. the This work was supported by a Meta AI SRA grant. J. Shi and S. Watanabe are funded in part of the Delta project, supported by the NSF (award OCI 2005572), and the State of Illinois. 6. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [2] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech representa- tion learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021. [3] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [4] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [5] T.-h. Feng et al., \u201cSUPERB@ SLT 2022: Challenge on gener- alization and efficiency of self-supervised speech representation learning,\u201d in Proc. SLT, 2023, pp. 1096\u20131103. [6] X. Chang et al., \u201cAn exploration of self-supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021, pp. 228\u2013235. [7] D. Berrebbi et al., \u201cCombining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Transla- tion,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [8] S. H. Mallidi and H. Hermansky, \u201cNovel neural network based fusion for multistream asr,\u201d in Proc. ICASSP, 2016, pp. 5680\u2013 5684. [9] S. H. R. Mallidi et al., \u201cA practical and efficient multistream framework for noise robust speech recognition,\u201d Ph.D. disserta- tion, Johns Hopkins University, 2018. [10] H. Hermansky, \u201cMultistream recognition of speech: Dealing with unknown unknowns,\u201d Proceedings of the IEEE, vol. 101, no. 5, pp. 1076\u20131088, 2013. [11] K. J. Han et al., \u201cMultistream cnn for robust acoustic modeling,\u201d in Proc. ICASSP, 2021, pp. 6873\u20136877. [12] J. Luo et al., \u201cMulti-quartznet: Multi-resolution convolution for speech recognition with multi-layer feature fusion,\u201d in Proc. SLT, 2021, pp. 82\u201388. [13] R. Li et al., \u201cMulti-stream end-to-end speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Pro- cessing, vol. 28, pp. 646\u2013655, 2019. [14] Y. Kong et al., \u201cMulti-channel automatic speech recognition us- ing deep complex unet,\u201d in Proc. SLT, 2021, pp. 104\u2013110. [15] A. Andrusenko, R. Nasretdinov, and A. Romanenko, \u201cUconv- conformer: High reduction of input sequence length for end- to-end speech recognition,\u201d arXiv preprint arXiv:2208.07657, 2022. [16] S. Kim et al., \u201cSqueezeformer: An efficient transformer for au- tomatic speech recognition,\u201d in Proc. NeurIPS. [17] W. Yao et al., \u201cMulti-stream convolutional neural network with frequency selection for robust speaker verification,\u201d arXiv preprint arXiv:2012.11159, 2020. [18] Z. Gao, M.-W. Mak, and W. Lin, \u201cUnet-densenet for robust far- field speaker verification,\u201d Proc. Interspeech 2022, pp. 3714\u2013 3718, 2022. [19] M. Burchi and V. Vielzeuf, \u201cEfficient conformer: Progres- sive downsampling and grouped attention for automatic speech recognition,\u201d in Proc. ASRU, 2021, pp. 8\u201315. [20] Y. Zhang et al., \u201cResearch on speech enhancement algorithm based on sa-unet,\u201d in 2019 4th International Conference on Me- chanical, Control and Computer Engineering (ICMCCE), 2019, pp. 818\u20138183. [21] T. Zhao et al., \u201cUnet++-based multi-channel speech dereverber- ation and distant speech recognition,\u201d in 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2021, pp. 1\u20135. [22] R. Li et al., \u201cUnet-tts: Improving unseen speaker and style trans- fer in one-shot voice cloning,\u201d in Proc. ICASSP, 2022, pp. 8327\u2013 8331. [23] X. Xiang, X. Zhang, and H. Chen, \u201cA nested u-net with self- attention and dense connectivity for monaural speech enhance- ment,\u201d IEEE Signal Processing Letters, vol. 29, pp. 105\u2013109, 2021. [24] Y. Xian et al., \u201cA multi-scale feature recalibration network for end-to-end single channel speech enhancement,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 1, pp. 143\u2013 155, 2020. [25] G. Liu et al., \u201cCp-GAN: Context pyramid generative adversar- ial network for speech enhancement,\u201d in Proc. ICASSP, 2020, pp. 6624\u20136628. [26] X. Xiang, X. Zhang, and H. Chen, \u201cA convolutional net- work with multi-scale and"}, {"question": " What type of grant supported the work mentioned in the acknowledgment?", "answer": " The work was supported by a Meta AI SRA grant.", "ref_chunk": "features from the same speech source in dis- tinct ways that are useful for downstream tasks. To investigate these differences, we analyzed the features extracted by three pre-trained HuBERT models with 100ms, 40ms, and 20ms res- olutions. Specifically, we extracted the 6th layer representations and used them as input features to train a HiFi-GAN vocoder [34] with the ParallelWaveGAN toolkit [35, 36].3 We trained the vocoder on the LJSpeech dataset and adapted the upsam- pling modules to match the resolution of each HuBERT model. The vocoder was trained for 50k steps using the same config- uration as the ParallelWaveGAN toolkit. Finally, we generated and compared the spectrograms of synthesized test-set speech produced from different representations, as shown in Figure 2.4 The followings are some interesting findings: HuBERT features at different resolutions are capable of pro- ducing high-quality re-synthesized speech. Despite not per- forming well on ASR fine-tuning tasks (as shown in Table 4), HuBERT with 100ms resolution exhibits excellent speech re- synthesis quality. This suggests that the feature still contains In this study, we revisit the use of HuBERT with multiple res- olutions, recognizing that the original 20ms resolution may not be optimal for various downstream tasks. To address this, we propose HuBERT-MR, which integrates information from three HuBERT base models pre-trained with different resolutions. We examine two approaches for integration: a parallel approach (HuBERT-MR-P) and a hierarchical approach (HuBERT-MR- H). We evaluate HuBERT-MR-P on the SUPERB benchmark and both HuBERT-MR models on ASR fine-tuning. Our exper- iments demonstrate that the HuBERT-MR models significantly improve model performance on various downstream tasks, in- dicating that pre-trained features from multiple resolutions are complementary. Furthermore, we find that HuBERT-MR can outperform larger models in some scenarios, even with less pre- training data and fewer parameters. To further highlight the dif- ferences among HuBERT features at different resolutions, we conduct speech re-synthesis with the HiFi-GAN vocoder. Our results demonstrate that the features do differ across resolutions, while all retain the essential information for intelligibility. We believe this work offers valuable insights into the potential ben- efits of considering multi-resolution SSL in the field. 5. Acknowledgement 3https://github.com/kan-bayashi/ParallelWaveGAN. 4Synthesized in examples audio can be found https://www.dropbox.com/s/61ap65iegii93il/ audio-samples-resynthesis.zip. the This work was supported by a Meta AI SRA grant. J. Shi and S. Watanabe are funded in part of the Delta project, supported by the NSF (award OCI 2005572), and the State of Illinois. 6. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [2] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech representa- tion learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021. [3] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [4] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [5] T.-h. Feng et al., \u201cSUPERB@ SLT 2022: Challenge on gener- alization and efficiency of self-supervised speech representation learning,\u201d in Proc. SLT, 2023, pp. 1096\u20131103. [6] X. Chang et al., \u201cAn exploration of self-supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021, pp. 228\u2013235. [7] D. Berrebbi et al., \u201cCombining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Transla- tion,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [8] S. H. Mallidi and H. Hermansky, \u201cNovel neural network based fusion for multistream asr,\u201d in Proc. ICASSP, 2016, pp. 5680\u2013 5684. [9] S. H. R. Mallidi et al., \u201cA practical and efficient multistream framework for noise robust speech recognition,\u201d Ph.D. disserta- tion, Johns Hopkins University, 2018. [10] H. Hermansky, \u201cMultistream recognition of speech: Dealing with unknown unknowns,\u201d Proceedings of the IEEE, vol. 101, no. 5, pp. 1076\u20131088, 2013. [11] K. J. Han et al., \u201cMultistream cnn for robust acoustic modeling,\u201d in Proc. ICASSP, 2021, pp. 6873\u20136877. [12] J. Luo et al., \u201cMulti-quartznet: Multi-resolution convolution for speech recognition with multi-layer feature fusion,\u201d in Proc. SLT, 2021, pp. 82\u201388. [13] R. Li et al., \u201cMulti-stream end-to-end speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Pro- cessing, vol. 28, pp. 646\u2013655, 2019. [14] Y. Kong et al., \u201cMulti-channel automatic speech recognition us- ing deep complex unet,\u201d in Proc. SLT, 2021, pp. 104\u2013110. [15] A. Andrusenko, R. Nasretdinov, and A. Romanenko, \u201cUconv- conformer: High reduction of input sequence length for end- to-end speech recognition,\u201d arXiv preprint arXiv:2208.07657, 2022. [16] S. Kim et al., \u201cSqueezeformer: An efficient transformer for au- tomatic speech recognition,\u201d in Proc. NeurIPS. [17] W. Yao et al., \u201cMulti-stream convolutional neural network with frequency selection for robust speaker verification,\u201d arXiv preprint arXiv:2012.11159, 2020. [18] Z. Gao, M.-W. Mak, and W. Lin, \u201cUnet-densenet for robust far- field speaker verification,\u201d Proc. Interspeech 2022, pp. 3714\u2013 3718, 2022. [19] M. Burchi and V. Vielzeuf, \u201cEfficient conformer: Progres- sive downsampling and grouped attention for automatic speech recognition,\u201d in Proc. ASRU, 2021, pp. 8\u201315. [20] Y. Zhang et al., \u201cResearch on speech enhancement algorithm based on sa-unet,\u201d in 2019 4th International Conference on Me- chanical, Control and Computer Engineering (ICMCCE), 2019, pp. 818\u20138183. [21] T. Zhao et al., \u201cUnet++-based multi-channel speech dereverber- ation and distant speech recognition,\u201d in 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2021, pp. 1\u20135. [22] R. Li et al., \u201cUnet-tts: Improving unseen speaker and style trans- fer in one-shot voice cloning,\u201d in Proc. ICASSP, 2022, pp. 8327\u2013 8331. [23] X. Xiang, X. Zhang, and H. Chen, \u201cA nested u-net with self- attention and dense connectivity for monaural speech enhance- ment,\u201d IEEE Signal Processing Letters, vol. 29, pp. 105\u2013109, 2021. [24] Y. Xian et al., \u201cA multi-scale feature recalibration network for end-to-end single channel speech enhancement,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 1, pp. 143\u2013 155, 2020. [25] G. Liu et al., \u201cCp-GAN: Context pyramid generative adversar- ial network for speech enhancement,\u201d in Proc. ICASSP, 2020, pp. 6624\u20136628. [26] X. Xiang, X. Zhang, and H. Chen, \u201cA convolutional net- work with multi-scale and"}, {"question": " What study is highlighted in reference [1]?", "answer": " Reference [1] highlights a study on self-supervised speech representation learning.", "ref_chunk": "features from the same speech source in dis- tinct ways that are useful for downstream tasks. To investigate these differences, we analyzed the features extracted by three pre-trained HuBERT models with 100ms, 40ms, and 20ms res- olutions. Specifically, we extracted the 6th layer representations and used them as input features to train a HiFi-GAN vocoder [34] with the ParallelWaveGAN toolkit [35, 36].3 We trained the vocoder on the LJSpeech dataset and adapted the upsam- pling modules to match the resolution of each HuBERT model. The vocoder was trained for 50k steps using the same config- uration as the ParallelWaveGAN toolkit. Finally, we generated and compared the spectrograms of synthesized test-set speech produced from different representations, as shown in Figure 2.4 The followings are some interesting findings: HuBERT features at different resolutions are capable of pro- ducing high-quality re-synthesized speech. Despite not per- forming well on ASR fine-tuning tasks (as shown in Table 4), HuBERT with 100ms resolution exhibits excellent speech re- synthesis quality. This suggests that the feature still contains In this study, we revisit the use of HuBERT with multiple res- olutions, recognizing that the original 20ms resolution may not be optimal for various downstream tasks. To address this, we propose HuBERT-MR, which integrates information from three HuBERT base models pre-trained with different resolutions. We examine two approaches for integration: a parallel approach (HuBERT-MR-P) and a hierarchical approach (HuBERT-MR- H). We evaluate HuBERT-MR-P on the SUPERB benchmark and both HuBERT-MR models on ASR fine-tuning. Our exper- iments demonstrate that the HuBERT-MR models significantly improve model performance on various downstream tasks, in- dicating that pre-trained features from multiple resolutions are complementary. Furthermore, we find that HuBERT-MR can outperform larger models in some scenarios, even with less pre- training data and fewer parameters. To further highlight the dif- ferences among HuBERT features at different resolutions, we conduct speech re-synthesis with the HiFi-GAN vocoder. Our results demonstrate that the features do differ across resolutions, while all retain the essential information for intelligibility. We believe this work offers valuable insights into the potential ben- efits of considering multi-resolution SSL in the field. 5. Acknowledgement 3https://github.com/kan-bayashi/ParallelWaveGAN. 4Synthesized in examples audio can be found https://www.dropbox.com/s/61ap65iegii93il/ audio-samples-resynthesis.zip. the This work was supported by a Meta AI SRA grant. J. Shi and S. Watanabe are funded in part of the Delta project, supported by the NSF (award OCI 2005572), and the State of Illinois. 6. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [2] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech representa- tion learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021. [3] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [4] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [5] T.-h. Feng et al., \u201cSUPERB@ SLT 2022: Challenge on gener- alization and efficiency of self-supervised speech representation learning,\u201d in Proc. SLT, 2023, pp. 1096\u20131103. [6] X. Chang et al., \u201cAn exploration of self-supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021, pp. 228\u2013235. [7] D. Berrebbi et al., \u201cCombining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Transla- tion,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [8] S. H. Mallidi and H. Hermansky, \u201cNovel neural network based fusion for multistream asr,\u201d in Proc. ICASSP, 2016, pp. 5680\u2013 5684. [9] S. H. R. Mallidi et al., \u201cA practical and efficient multistream framework for noise robust speech recognition,\u201d Ph.D. disserta- tion, Johns Hopkins University, 2018. [10] H. Hermansky, \u201cMultistream recognition of speech: Dealing with unknown unknowns,\u201d Proceedings of the IEEE, vol. 101, no. 5, pp. 1076\u20131088, 2013. [11] K. J. Han et al., \u201cMultistream cnn for robust acoustic modeling,\u201d in Proc. ICASSP, 2021, pp. 6873\u20136877. [12] J. Luo et al., \u201cMulti-quartznet: Multi-resolution convolution for speech recognition with multi-layer feature fusion,\u201d in Proc. SLT, 2021, pp. 82\u201388. [13] R. Li et al., \u201cMulti-stream end-to-end speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Pro- cessing, vol. 28, pp. 646\u2013655, 2019. [14] Y. Kong et al., \u201cMulti-channel automatic speech recognition us- ing deep complex unet,\u201d in Proc. SLT, 2021, pp. 104\u2013110. [15] A. Andrusenko, R. Nasretdinov, and A. Romanenko, \u201cUconv- conformer: High reduction of input sequence length for end- to-end speech recognition,\u201d arXiv preprint arXiv:2208.07657, 2022. [16] S. Kim et al., \u201cSqueezeformer: An efficient transformer for au- tomatic speech recognition,\u201d in Proc. NeurIPS. [17] W. Yao et al., \u201cMulti-stream convolutional neural network with frequency selection for robust speaker verification,\u201d arXiv preprint arXiv:2012.11159, 2020. [18] Z. Gao, M.-W. Mak, and W. Lin, \u201cUnet-densenet for robust far- field speaker verification,\u201d Proc. Interspeech 2022, pp. 3714\u2013 3718, 2022. [19] M. Burchi and V. Vielzeuf, \u201cEfficient conformer: Progres- sive downsampling and grouped attention for automatic speech recognition,\u201d in Proc. ASRU, 2021, pp. 8\u201315. [20] Y. Zhang et al., \u201cResearch on speech enhancement algorithm based on sa-unet,\u201d in 2019 4th International Conference on Me- chanical, Control and Computer Engineering (ICMCCE), 2019, pp. 818\u20138183. [21] T. Zhao et al., \u201cUnet++-based multi-channel speech dereverber- ation and distant speech recognition,\u201d in 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2021, pp. 1\u20135. [22] R. Li et al., \u201cUnet-tts: Improving unseen speaker and style trans- fer in one-shot voice cloning,\u201d in Proc. ICASSP, 2022, pp. 8327\u2013 8331. [23] X. Xiang, X. Zhang, and H. Chen, \u201cA nested u-net with self- attention and dense connectivity for monaural speech enhance- ment,\u201d IEEE Signal Processing Letters, vol. 29, pp. 105\u2013109, 2021. [24] Y. Xian et al., \u201cA multi-scale feature recalibration network for end-to-end single channel speech enhancement,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 1, pp. 143\u2013 155, 2020. [25] G. Liu et al., \u201cCp-GAN: Context pyramid generative adversar- ial network for speech enhancement,\u201d in Proc. ICASSP, 2020, pp. 6624\u20136628. [26] X. Xiang, X. Zhang, and H. Chen, \u201cA convolutional net- work with multi-scale and"}, {"question": " What does the study mentioned in reference [6] explore?", "answer": " The study explores self-supervised pretrained representations for end-to-end speech recognition.", "ref_chunk": "features from the same speech source in dis- tinct ways that are useful for downstream tasks. To investigate these differences, we analyzed the features extracted by three pre-trained HuBERT models with 100ms, 40ms, and 20ms res- olutions. Specifically, we extracted the 6th layer representations and used them as input features to train a HiFi-GAN vocoder [34] with the ParallelWaveGAN toolkit [35, 36].3 We trained the vocoder on the LJSpeech dataset and adapted the upsam- pling modules to match the resolution of each HuBERT model. The vocoder was trained for 50k steps using the same config- uration as the ParallelWaveGAN toolkit. Finally, we generated and compared the spectrograms of synthesized test-set speech produced from different representations, as shown in Figure 2.4 The followings are some interesting findings: HuBERT features at different resolutions are capable of pro- ducing high-quality re-synthesized speech. Despite not per- forming well on ASR fine-tuning tasks (as shown in Table 4), HuBERT with 100ms resolution exhibits excellent speech re- synthesis quality. This suggests that the feature still contains In this study, we revisit the use of HuBERT with multiple res- olutions, recognizing that the original 20ms resolution may not be optimal for various downstream tasks. To address this, we propose HuBERT-MR, which integrates information from three HuBERT base models pre-trained with different resolutions. We examine two approaches for integration: a parallel approach (HuBERT-MR-P) and a hierarchical approach (HuBERT-MR- H). We evaluate HuBERT-MR-P on the SUPERB benchmark and both HuBERT-MR models on ASR fine-tuning. Our exper- iments demonstrate that the HuBERT-MR models significantly improve model performance on various downstream tasks, in- dicating that pre-trained features from multiple resolutions are complementary. Furthermore, we find that HuBERT-MR can outperform larger models in some scenarios, even with less pre- training data and fewer parameters. To further highlight the dif- ferences among HuBERT features at different resolutions, we conduct speech re-synthesis with the HiFi-GAN vocoder. Our results demonstrate that the features do differ across resolutions, while all retain the essential information for intelligibility. We believe this work offers valuable insights into the potential ben- efits of considering multi-resolution SSL in the field. 5. Acknowledgement 3https://github.com/kan-bayashi/ParallelWaveGAN. 4Synthesized in examples audio can be found https://www.dropbox.com/s/61ap65iegii93il/ audio-samples-resynthesis.zip. the This work was supported by a Meta AI SRA grant. J. Shi and S. Watanabe are funded in part of the Delta project, supported by the NSF (award OCI 2005572), and the State of Illinois. 6. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [2] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech representa- tion learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021. [3] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [4] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [5] T.-h. Feng et al., \u201cSUPERB@ SLT 2022: Challenge on gener- alization and efficiency of self-supervised speech representation learning,\u201d in Proc. SLT, 2023, pp. 1096\u20131103. [6] X. Chang et al., \u201cAn exploration of self-supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021, pp. 228\u2013235. [7] D. Berrebbi et al., \u201cCombining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Transla- tion,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [8] S. H. Mallidi and H. Hermansky, \u201cNovel neural network based fusion for multistream asr,\u201d in Proc. ICASSP, 2016, pp. 5680\u2013 5684. [9] S. H. R. Mallidi et al., \u201cA practical and efficient multistream framework for noise robust speech recognition,\u201d Ph.D. disserta- tion, Johns Hopkins University, 2018. [10] H. Hermansky, \u201cMultistream recognition of speech: Dealing with unknown unknowns,\u201d Proceedings of the IEEE, vol. 101, no. 5, pp. 1076\u20131088, 2013. [11] K. J. Han et al., \u201cMultistream cnn for robust acoustic modeling,\u201d in Proc. ICASSP, 2021, pp. 6873\u20136877. [12] J. Luo et al., \u201cMulti-quartznet: Multi-resolution convolution for speech recognition with multi-layer feature fusion,\u201d in Proc. SLT, 2021, pp. 82\u201388. [13] R. Li et al., \u201cMulti-stream end-to-end speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Pro- cessing, vol. 28, pp. 646\u2013655, 2019. [14] Y. Kong et al., \u201cMulti-channel automatic speech recognition us- ing deep complex unet,\u201d in Proc. SLT, 2021, pp. 104\u2013110. [15] A. Andrusenko, R. Nasretdinov, and A. Romanenko, \u201cUconv- conformer: High reduction of input sequence length for end- to-end speech recognition,\u201d arXiv preprint arXiv:2208.07657, 2022. [16] S. Kim et al., \u201cSqueezeformer: An efficient transformer for au- tomatic speech recognition,\u201d in Proc. NeurIPS. [17] W. Yao et al., \u201cMulti-stream convolutional neural network with frequency selection for robust speaker verification,\u201d arXiv preprint arXiv:2012.11159, 2020. [18] Z. Gao, M.-W. Mak, and W. Lin, \u201cUnet-densenet for robust far- field speaker verification,\u201d Proc. Interspeech 2022, pp. 3714\u2013 3718, 2022. [19] M. Burchi and V. Vielzeuf, \u201cEfficient conformer: Progres- sive downsampling and grouped attention for automatic speech recognition,\u201d in Proc. ASRU, 2021, pp. 8\u201315. [20] Y. Zhang et al., \u201cResearch on speech enhancement algorithm based on sa-unet,\u201d in 2019 4th International Conference on Me- chanical, Control and Computer Engineering (ICMCCE), 2019, pp. 818\u20138183. [21] T. Zhao et al., \u201cUnet++-based multi-channel speech dereverber- ation and distant speech recognition,\u201d in 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2021, pp. 1\u20135. [22] R. Li et al., \u201cUnet-tts: Improving unseen speaker and style trans- fer in one-shot voice cloning,\u201d in Proc. ICASSP, 2022, pp. 8327\u2013 8331. [23] X. Xiang, X. Zhang, and H. Chen, \u201cA nested u-net with self- attention and dense connectivity for monaural speech enhance- ment,\u201d IEEE Signal Processing Letters, vol. 29, pp. 105\u2013109, 2021. [24] Y. Xian et al., \u201cA multi-scale feature recalibration network for end-to-end single channel speech enhancement,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 1, pp. 143\u2013 155, 2020. [25] G. Liu et al., \u201cCp-GAN: Context pyramid generative adversar- ial network for speech enhancement,\u201d in Proc. ICASSP, 2020, pp. 6624\u20136628. [26] X. Xiang, X. Zhang, and H. Chen, \u201cA convolutional net- work with multi-scale and"}, {"question": " What does reference [11] discuss in relation to speech recognition?", "answer": " Reference [11] discusses a Multistream CNN for robust acoustic modeling.", "ref_chunk": "features from the same speech source in dis- tinct ways that are useful for downstream tasks. To investigate these differences, we analyzed the features extracted by three pre-trained HuBERT models with 100ms, 40ms, and 20ms res- olutions. Specifically, we extracted the 6th layer representations and used them as input features to train a HiFi-GAN vocoder [34] with the ParallelWaveGAN toolkit [35, 36].3 We trained the vocoder on the LJSpeech dataset and adapted the upsam- pling modules to match the resolution of each HuBERT model. The vocoder was trained for 50k steps using the same config- uration as the ParallelWaveGAN toolkit. Finally, we generated and compared the spectrograms of synthesized test-set speech produced from different representations, as shown in Figure 2.4 The followings are some interesting findings: HuBERT features at different resolutions are capable of pro- ducing high-quality re-synthesized speech. Despite not per- forming well on ASR fine-tuning tasks (as shown in Table 4), HuBERT with 100ms resolution exhibits excellent speech re- synthesis quality. This suggests that the feature still contains In this study, we revisit the use of HuBERT with multiple res- olutions, recognizing that the original 20ms resolution may not be optimal for various downstream tasks. To address this, we propose HuBERT-MR, which integrates information from three HuBERT base models pre-trained with different resolutions. We examine two approaches for integration: a parallel approach (HuBERT-MR-P) and a hierarchical approach (HuBERT-MR- H). We evaluate HuBERT-MR-P on the SUPERB benchmark and both HuBERT-MR models on ASR fine-tuning. Our exper- iments demonstrate that the HuBERT-MR models significantly improve model performance on various downstream tasks, in- dicating that pre-trained features from multiple resolutions are complementary. Furthermore, we find that HuBERT-MR can outperform larger models in some scenarios, even with less pre- training data and fewer parameters. To further highlight the dif- ferences among HuBERT features at different resolutions, we conduct speech re-synthesis with the HiFi-GAN vocoder. Our results demonstrate that the features do differ across resolutions, while all retain the essential information for intelligibility. We believe this work offers valuable insights into the potential ben- efits of considering multi-resolution SSL in the field. 5. Acknowledgement 3https://github.com/kan-bayashi/ParallelWaveGAN. 4Synthesized in examples audio can be found https://www.dropbox.com/s/61ap65iegii93il/ audio-samples-resynthesis.zip. the This work was supported by a Meta AI SRA grant. J. Shi and S. Watanabe are funded in part of the Delta project, supported by the NSF (award OCI 2005572), and the State of Illinois. 6. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [2] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech representa- tion learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021. [3] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [4] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [5] T.-h. Feng et al., \u201cSUPERB@ SLT 2022: Challenge on gener- alization and efficiency of self-supervised speech representation learning,\u201d in Proc. SLT, 2023, pp. 1096\u20131103. [6] X. Chang et al., \u201cAn exploration of self-supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021, pp. 228\u2013235. [7] D. Berrebbi et al., \u201cCombining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Transla- tion,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [8] S. H. Mallidi and H. Hermansky, \u201cNovel neural network based fusion for multistream asr,\u201d in Proc. ICASSP, 2016, pp. 5680\u2013 5684. [9] S. H. R. Mallidi et al., \u201cA practical and efficient multistream framework for noise robust speech recognition,\u201d Ph.D. disserta- tion, Johns Hopkins University, 2018. [10] H. Hermansky, \u201cMultistream recognition of speech: Dealing with unknown unknowns,\u201d Proceedings of the IEEE, vol. 101, no. 5, pp. 1076\u20131088, 2013. [11] K. J. Han et al., \u201cMultistream cnn for robust acoustic modeling,\u201d in Proc. ICASSP, 2021, pp. 6873\u20136877. [12] J. Luo et al., \u201cMulti-quartznet: Multi-resolution convolution for speech recognition with multi-layer feature fusion,\u201d in Proc. SLT, 2021, pp. 82\u201388. [13] R. Li et al., \u201cMulti-stream end-to-end speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Pro- cessing, vol. 28, pp. 646\u2013655, 2019. [14] Y. Kong et al., \u201cMulti-channel automatic speech recognition us- ing deep complex unet,\u201d in Proc. SLT, 2021, pp. 104\u2013110. [15] A. Andrusenko, R. Nasretdinov, and A. Romanenko, \u201cUconv- conformer: High reduction of input sequence length for end- to-end speech recognition,\u201d arXiv preprint arXiv:2208.07657, 2022. [16] S. Kim et al., \u201cSqueezeformer: An efficient transformer for au- tomatic speech recognition,\u201d in Proc. NeurIPS. [17] W. Yao et al., \u201cMulti-stream convolutional neural network with frequency selection for robust speaker verification,\u201d arXiv preprint arXiv:2012.11159, 2020. [18] Z. Gao, M.-W. Mak, and W. Lin, \u201cUnet-densenet for robust far- field speaker verification,\u201d Proc. Interspeech 2022, pp. 3714\u2013 3718, 2022. [19] M. Burchi and V. Vielzeuf, \u201cEfficient conformer: Progres- sive downsampling and grouped attention for automatic speech recognition,\u201d in Proc. ASRU, 2021, pp. 8\u201315. [20] Y. Zhang et al., \u201cResearch on speech enhancement algorithm based on sa-unet,\u201d in 2019 4th International Conference on Me- chanical, Control and Computer Engineering (ICMCCE), 2019, pp. 818\u20138183. [21] T. Zhao et al., \u201cUnet++-based multi-channel speech dereverber- ation and distant speech recognition,\u201d in 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2021, pp. 1\u20135. [22] R. Li et al., \u201cUnet-tts: Improving unseen speaker and style trans- fer in one-shot voice cloning,\u201d in Proc. ICASSP, 2022, pp. 8327\u2013 8331. [23] X. Xiang, X. Zhang, and H. Chen, \u201cA nested u-net with self- attention and dense connectivity for monaural speech enhance- ment,\u201d IEEE Signal Processing Letters, vol. 29, pp. 105\u2013109, 2021. [24] Y. Xian et al., \u201cA multi-scale feature recalibration network for end-to-end single channel speech enhancement,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 1, pp. 143\u2013 155, 2020. [25] G. Liu et al., \u201cCp-GAN: Context pyramid generative adversar- ial network for speech enhancement,\u201d in Proc. ICASSP, 2020, pp. 6624\u20136628. [26] X. Xiang, X. Zhang, and H. Chen, \u201cA convolutional net- work with multi-scale and"}], "doc_text": "features from the same speech source in dis- tinct ways that are useful for downstream tasks. To investigate these differences, we analyzed the features extracted by three pre-trained HuBERT models with 100ms, 40ms, and 20ms res- olutions. Specifically, we extracted the 6th layer representations and used them as input features to train a HiFi-GAN vocoder [34] with the ParallelWaveGAN toolkit [35, 36].3 We trained the vocoder on the LJSpeech dataset and adapted the upsam- pling modules to match the resolution of each HuBERT model. The vocoder was trained for 50k steps using the same config- uration as the ParallelWaveGAN toolkit. Finally, we generated and compared the spectrograms of synthesized test-set speech produced from different representations, as shown in Figure 2.4 The followings are some interesting findings: HuBERT features at different resolutions are capable of pro- ducing high-quality re-synthesized speech. Despite not per- forming well on ASR fine-tuning tasks (as shown in Table 4), HuBERT with 100ms resolution exhibits excellent speech re- synthesis quality. This suggests that the feature still contains In this study, we revisit the use of HuBERT with multiple res- olutions, recognizing that the original 20ms resolution may not be optimal for various downstream tasks. To address this, we propose HuBERT-MR, which integrates information from three HuBERT base models pre-trained with different resolutions. We examine two approaches for integration: a parallel approach (HuBERT-MR-P) and a hierarchical approach (HuBERT-MR- H). We evaluate HuBERT-MR-P on the SUPERB benchmark and both HuBERT-MR models on ASR fine-tuning. Our exper- iments demonstrate that the HuBERT-MR models significantly improve model performance on various downstream tasks, in- dicating that pre-trained features from multiple resolutions are complementary. Furthermore, we find that HuBERT-MR can outperform larger models in some scenarios, even with less pre- training data and fewer parameters. To further highlight the dif- ferences among HuBERT features at different resolutions, we conduct speech re-synthesis with the HiFi-GAN vocoder. Our results demonstrate that the features do differ across resolutions, while all retain the essential information for intelligibility. We believe this work offers valuable insights into the potential ben- efits of considering multi-resolution SSL in the field. 5. Acknowledgement 3https://github.com/kan-bayashi/ParallelWaveGAN. 4Synthesized in examples audio can be found https://www.dropbox.com/s/61ap65iegii93il/ audio-samples-resynthesis.zip. the This work was supported by a Meta AI SRA grant. J. Shi and S. Watanabe are funded in part of the Delta project, supported by the NSF (award OCI 2005572), and the State of Illinois. 6. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [2] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech representa- tion learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021. [3] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [4] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [5] T.-h. Feng et al., \u201cSUPERB@ SLT 2022: Challenge on gener- alization and efficiency of self-supervised speech representation learning,\u201d in Proc. SLT, 2023, pp. 1096\u20131103. [6] X. Chang et al., \u201cAn exploration of self-supervised pretrained representations for end-to-end speech recognition,\u201d in Proc. ASRU, 2021, pp. 228\u2013235. [7] D. Berrebbi et al., \u201cCombining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Transla- tion,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [8] S. H. Mallidi and H. Hermansky, \u201cNovel neural network based fusion for multistream asr,\u201d in Proc. ICASSP, 2016, pp. 5680\u2013 5684. [9] S. H. R. Mallidi et al., \u201cA practical and efficient multistream framework for noise robust speech recognition,\u201d Ph.D. disserta- tion, Johns Hopkins University, 2018. [10] H. Hermansky, \u201cMultistream recognition of speech: Dealing with unknown unknowns,\u201d Proceedings of the IEEE, vol. 101, no. 5, pp. 1076\u20131088, 2013. [11] K. J. Han et al., \u201cMultistream cnn for robust acoustic modeling,\u201d in Proc. ICASSP, 2021, pp. 6873\u20136877. [12] J. Luo et al., \u201cMulti-quartznet: Multi-resolution convolution for speech recognition with multi-layer feature fusion,\u201d in Proc. SLT, 2021, pp. 82\u201388. [13] R. Li et al., \u201cMulti-stream end-to-end speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Pro- cessing, vol. 28, pp. 646\u2013655, 2019. [14] Y. Kong et al., \u201cMulti-channel automatic speech recognition us- ing deep complex unet,\u201d in Proc. SLT, 2021, pp. 104\u2013110. [15] A. Andrusenko, R. Nasretdinov, and A. Romanenko, \u201cUconv- conformer: High reduction of input sequence length for end- to-end speech recognition,\u201d arXiv preprint arXiv:2208.07657, 2022. [16] S. Kim et al., \u201cSqueezeformer: An efficient transformer for au- tomatic speech recognition,\u201d in Proc. NeurIPS. [17] W. Yao et al., \u201cMulti-stream convolutional neural network with frequency selection for robust speaker verification,\u201d arXiv preprint arXiv:2012.11159, 2020. [18] Z. Gao, M.-W. Mak, and W. Lin, \u201cUnet-densenet for robust far- field speaker verification,\u201d Proc. Interspeech 2022, pp. 3714\u2013 3718, 2022. [19] M. Burchi and V. Vielzeuf, \u201cEfficient conformer: Progres- sive downsampling and grouped attention for automatic speech recognition,\u201d in Proc. ASRU, 2021, pp. 8\u201315. [20] Y. Zhang et al., \u201cResearch on speech enhancement algorithm based on sa-unet,\u201d in 2019 4th International Conference on Me- chanical, Control and Computer Engineering (ICMCCE), 2019, pp. 818\u20138183. [21] T. Zhao et al., \u201cUnet++-based multi-channel speech dereverber- ation and distant speech recognition,\u201d in 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2021, pp. 1\u20135. [22] R. Li et al., \u201cUnet-tts: Improving unseen speaker and style trans- fer in one-shot voice cloning,\u201d in Proc. ICASSP, 2022, pp. 8327\u2013 8331. [23] X. Xiang, X. Zhang, and H. Chen, \u201cA nested u-net with self- attention and dense connectivity for monaural speech enhance- ment,\u201d IEEE Signal Processing Letters, vol. 29, pp. 105\u2013109, 2021. [24] Y. Xian et al., \u201cA multi-scale feature recalibration network for end-to-end single channel speech enhancement,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 1, pp. 143\u2013 155, 2020. [25] G. Liu et al., \u201cCp-GAN: Context pyramid generative adversar- ial network for speech enhancement,\u201d in Proc. ICASSP, 2020, pp. 6624\u20136628. [26] X. Xiang, X. Zhang, and H. Chen, \u201cA convolutional net- work with multi-scale and"}