{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_3D_Semantic_Segmentation_in_the_Wild:_Learning_Generalized_Models_for_Adverse-Condition_Point_Clouds_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are some of the types of deep neural networks mentioned in the text?", "answer": " Standard convolutional network, multi-layer perceptron (MLP)-based networks, 3D voxel convolution-based networks, and hybrid networks", "ref_chunk": "been developed rapidly over the past few years, largely through the devel- opment of various deep neural networks (DNNs) such as standard convolutional network for projection-based meth- ods [9, 30, 48, 52, 61], multi-layer perceptron (MLP)-based networks [19, 35, 35], 3D voxel convolution-based net- works [7, 64], or hybrid networks [6, 27, 43, 53, 59]. While existing 3DSS networks are mainly evaluated over normal weather point clouds, their performance for adverse weather point clouds is far under-investigated. The proposed Se- manticSTF closes the gap and provides a solid ground for the study and evaluation of all-weather 3DSS. By enabling investigations into various new research directions, Seman- ticSTF represents a valuable tool for advancing the field. Vision recognition under adverse conditions. Scene un- derstanding under adverse conditions has recently attracted increasing attention due to the strict safety demand in var- ious outdoor navigation and perception tasks. In 2D vi- sion, several large-scale datasets have been proposed to investigate perceptions tasks in adverse visual conditions including localization [29], detection [58], and segmenta- tion [37]. On the other hand, learning 3D point clouds of adverse conditions is far under-explored due to the absence of comprehensive dataset benchmarks. The recently pro- posed datasets such as STF [3] and CADC [34] contain LiDAR point clouds captured under adverse weather con- ditions. However, these studies focus on the object detec- tion task [15, 16] with bounding-box annotations, without providing any point-wise annotations. Our introduced Se- manticSTF is the first large-scale dataset that consists of Li- DAR point clouds in adverse weather conditions with high- quality dense annotations, to the best of our knowledge. Domain generalization [4,31] aims to learn a generalizable model from single or multiple related but distinct source do- mains where target data is inaccessible during model learn- It has been widely studied in 2D computer vision ing. tasks [1, 21, 26, 63] while few studies explore it in point cloud learning. Recently, [25] studies domain generaliza- tion for 3D object detection by deforming point clouds via vector fields. Differently, this work is the first attempt that explores domain generalization for 3DSS. Unsupervised domain adaptation is a method of transfer- ring knowledge learned from a labeled source domain to a target domain by leveraging the unlabeled target data. It has been widely studied in 2D image learning [12,14,20,22\u201324] and 3D point clouds [15, 16, 28, 40, 54, 55, 60]. Recently, domain adaptive 3D LiDAR segmentation has drawn in- creasing attention due to the challenge in point-wise an- notation. Different UDA approaches have been designed to mitigate discrepancies across LiDAR point clouds of different domains. For example, [48, 62] project point clouds into depth images and leverage 2D UDA techniques while [38, 50, 51, 57] directly work in the 3D space. How- ever, these methods either work for synthetic-to-real UDA scenarios [48, 51] or normal-to-normal point cloud adap- tation [57], ignoring normal-to-adverse adaptation which is highly practical in real applications. Our SemanticSTF dataset fills up this blank and will inspire more development of new algorithms for normal-to-adverse adaptation. 3. The SemanticSTF Dataset 3.1. Background LiDAR sensors send out laser pulses and measure their flight time based on the echoes it receives from targets. The travel distance as derived from the time-of-flight and the registered angular information (between the LiDAR sensors and the targets) can be combined to compute the 3D coordi- nates of target surface which form point clouds that capture the 3D shape of the targets. However, the active LiDAR pulse system can be easily affected by the scattering media such as particles of rain droplets and snow [10, 18, 33, 36], leading to shifts of measured distances, variation of echo intensity, point missing, etc. Hence, point clouds captured under adverse weather usually have clear distribution dis- crepancy as compared with those collected under normal weather as illustrated in Fig. 1. However, existing 3DSS benchmarks are dominated by normal-weather point clouds which are insufficient for the study of universal 3DSS un- der all-weather conditions. To this end, we propose Seman- ticSTF, a point-wise annotated large-scale adverse-weather dataset that can be explored for the study of 3DSS and point cloud parsing under various adverse weather conditions. 3.2. Data Selection and Split We collect SemanticSTF by leveraging the STF bench- mark [3], a multi-modal adverse-weather dataset that was jointly collected in Germany, Sweden, Denmark, and Fin- land. The data in STF have multiple modalities including LiDAR point clouds and they are collected under various adverse weather conditions such as snow and fog. How- ever, STF provides bounding-box annotations only for the In SemanticSTF, we manu- study of 3D detection tasks. ally selected 2,076 scans captured by a Velodyne HDL64 S3D LiDAR sensor from STF that cover various adverse weather conditions including 694 snowy, 637 dense-foggy, 631 light-foggy, and 114 rainy (all rainy LiDAR scans in STF). During the selection, we pay special attention to the geographical diversity of the point clouds aiming for min- imizing data redundancy. We ignore the factor of day- time/nighttime since LiDAR sensors are robust to lighting conditions. We split SemanticSTF into three parts including 1,326 full 3D scans for training, 250 for validating, and 500 for testing. All three splits have approximately the same 3 proportion of LiDAR scans of different adverse weathers. 3.3. Data Annotation Point-wise annotation of LiDAR point clouds is an ex- tremely laborious task due to several factors, such as 3D view changes, inconsistency between point cloud display and human visual perception, sweeping occlusion, point sparsity, etc. However, point-wise annotating of adverse- weather point clouds is even more challenging due to two new factors. First, the perceived distance shifts under ad- verse weather often lead to various geometry distortions in the collected points which make them different from those collected under normal weather. This presents significant challenges for annotators who must recognize various ob- jects and assign a semantic label to each point. Second, LiDAR point clouds collected under adverse weather often contain a significant portion of invalid regions that consist of indiscernible semantic contents (e.g., thick"}, {"question": " What is the purpose of SemanticSTF dataset mentioned in the text?", "answer": " To provide a solid ground for the study and evaluation of all-weather 3DSS and to enable investigations into various new research directions", "ref_chunk": "been developed rapidly over the past few years, largely through the devel- opment of various deep neural networks (DNNs) such as standard convolutional network for projection-based meth- ods [9, 30, 48, 52, 61], multi-layer perceptron (MLP)-based networks [19, 35, 35], 3D voxel convolution-based net- works [7, 64], or hybrid networks [6, 27, 43, 53, 59]. While existing 3DSS networks are mainly evaluated over normal weather point clouds, their performance for adverse weather point clouds is far under-investigated. The proposed Se- manticSTF closes the gap and provides a solid ground for the study and evaluation of all-weather 3DSS. By enabling investigations into various new research directions, Seman- ticSTF represents a valuable tool for advancing the field. Vision recognition under adverse conditions. Scene un- derstanding under adverse conditions has recently attracted increasing attention due to the strict safety demand in var- ious outdoor navigation and perception tasks. In 2D vi- sion, several large-scale datasets have been proposed to investigate perceptions tasks in adverse visual conditions including localization [29], detection [58], and segmenta- tion [37]. On the other hand, learning 3D point clouds of adverse conditions is far under-explored due to the absence of comprehensive dataset benchmarks. The recently pro- posed datasets such as STF [3] and CADC [34] contain LiDAR point clouds captured under adverse weather con- ditions. However, these studies focus on the object detec- tion task [15, 16] with bounding-box annotations, without providing any point-wise annotations. Our introduced Se- manticSTF is the first large-scale dataset that consists of Li- DAR point clouds in adverse weather conditions with high- quality dense annotations, to the best of our knowledge. Domain generalization [4,31] aims to learn a generalizable model from single or multiple related but distinct source do- mains where target data is inaccessible during model learn- It has been widely studied in 2D computer vision ing. tasks [1, 21, 26, 63] while few studies explore it in point cloud learning. Recently, [25] studies domain generaliza- tion for 3D object detection by deforming point clouds via vector fields. Differently, this work is the first attempt that explores domain generalization for 3DSS. Unsupervised domain adaptation is a method of transfer- ring knowledge learned from a labeled source domain to a target domain by leveraging the unlabeled target data. It has been widely studied in 2D image learning [12,14,20,22\u201324] and 3D point clouds [15, 16, 28, 40, 54, 55, 60]. Recently, domain adaptive 3D LiDAR segmentation has drawn in- creasing attention due to the challenge in point-wise an- notation. Different UDA approaches have been designed to mitigate discrepancies across LiDAR point clouds of different domains. For example, [48, 62] project point clouds into depth images and leverage 2D UDA techniques while [38, 50, 51, 57] directly work in the 3D space. How- ever, these methods either work for synthetic-to-real UDA scenarios [48, 51] or normal-to-normal point cloud adap- tation [57], ignoring normal-to-adverse adaptation which is highly practical in real applications. Our SemanticSTF dataset fills up this blank and will inspire more development of new algorithms for normal-to-adverse adaptation. 3. The SemanticSTF Dataset 3.1. Background LiDAR sensors send out laser pulses and measure their flight time based on the echoes it receives from targets. The travel distance as derived from the time-of-flight and the registered angular information (between the LiDAR sensors and the targets) can be combined to compute the 3D coordi- nates of target surface which form point clouds that capture the 3D shape of the targets. However, the active LiDAR pulse system can be easily affected by the scattering media such as particles of rain droplets and snow [10, 18, 33, 36], leading to shifts of measured distances, variation of echo intensity, point missing, etc. Hence, point clouds captured under adverse weather usually have clear distribution dis- crepancy as compared with those collected under normal weather as illustrated in Fig. 1. However, existing 3DSS benchmarks are dominated by normal-weather point clouds which are insufficient for the study of universal 3DSS un- der all-weather conditions. To this end, we propose Seman- ticSTF, a point-wise annotated large-scale adverse-weather dataset that can be explored for the study of 3DSS and point cloud parsing under various adverse weather conditions. 3.2. Data Selection and Split We collect SemanticSTF by leveraging the STF bench- mark [3], a multi-modal adverse-weather dataset that was jointly collected in Germany, Sweden, Denmark, and Fin- land. The data in STF have multiple modalities including LiDAR point clouds and they are collected under various adverse weather conditions such as snow and fog. How- ever, STF provides bounding-box annotations only for the In SemanticSTF, we manu- study of 3D detection tasks. ally selected 2,076 scans captured by a Velodyne HDL64 S3D LiDAR sensor from STF that cover various adverse weather conditions including 694 snowy, 637 dense-foggy, 631 light-foggy, and 114 rainy (all rainy LiDAR scans in STF). During the selection, we pay special attention to the geographical diversity of the point clouds aiming for min- imizing data redundancy. We ignore the factor of day- time/nighttime since LiDAR sensors are robust to lighting conditions. We split SemanticSTF into three parts including 1,326 full 3D scans for training, 250 for validating, and 500 for testing. All three splits have approximately the same 3 proportion of LiDAR scans of different adverse weathers. 3.3. Data Annotation Point-wise annotation of LiDAR point clouds is an ex- tremely laborious task due to several factors, such as 3D view changes, inconsistency between point cloud display and human visual perception, sweeping occlusion, point sparsity, etc. However, point-wise annotating of adverse- weather point clouds is even more challenging due to two new factors. First, the perceived distance shifts under ad- verse weather often lead to various geometry distortions in the collected points which make them different from those collected under normal weather. This presents significant challenges for annotators who must recognize various ob- jects and assign a semantic label to each point. Second, LiDAR point clouds collected under adverse weather often contain a significant portion of invalid regions that consist of indiscernible semantic contents (e.g., thick"}, {"question": " Why has scene understanding under adverse conditions attracted increasing attention?", "answer": " Due to the strict safety demand in various outdoor navigation and perception tasks", "ref_chunk": "been developed rapidly over the past few years, largely through the devel- opment of various deep neural networks (DNNs) such as standard convolutional network for projection-based meth- ods [9, 30, 48, 52, 61], multi-layer perceptron (MLP)-based networks [19, 35, 35], 3D voxel convolution-based net- works [7, 64], or hybrid networks [6, 27, 43, 53, 59]. While existing 3DSS networks are mainly evaluated over normal weather point clouds, their performance for adverse weather point clouds is far under-investigated. The proposed Se- manticSTF closes the gap and provides a solid ground for the study and evaluation of all-weather 3DSS. By enabling investigations into various new research directions, Seman- ticSTF represents a valuable tool for advancing the field. Vision recognition under adverse conditions. Scene un- derstanding under adverse conditions has recently attracted increasing attention due to the strict safety demand in var- ious outdoor navigation and perception tasks. In 2D vi- sion, several large-scale datasets have been proposed to investigate perceptions tasks in adverse visual conditions including localization [29], detection [58], and segmenta- tion [37]. On the other hand, learning 3D point clouds of adverse conditions is far under-explored due to the absence of comprehensive dataset benchmarks. The recently pro- posed datasets such as STF [3] and CADC [34] contain LiDAR point clouds captured under adverse weather con- ditions. However, these studies focus on the object detec- tion task [15, 16] with bounding-box annotations, without providing any point-wise annotations. Our introduced Se- manticSTF is the first large-scale dataset that consists of Li- DAR point clouds in adverse weather conditions with high- quality dense annotations, to the best of our knowledge. Domain generalization [4,31] aims to learn a generalizable model from single or multiple related but distinct source do- mains where target data is inaccessible during model learn- It has been widely studied in 2D computer vision ing. tasks [1, 21, 26, 63] while few studies explore it in point cloud learning. Recently, [25] studies domain generaliza- tion for 3D object detection by deforming point clouds via vector fields. Differently, this work is the first attempt that explores domain generalization for 3DSS. Unsupervised domain adaptation is a method of transfer- ring knowledge learned from a labeled source domain to a target domain by leveraging the unlabeled target data. It has been widely studied in 2D image learning [12,14,20,22\u201324] and 3D point clouds [15, 16, 28, 40, 54, 55, 60]. Recently, domain adaptive 3D LiDAR segmentation has drawn in- creasing attention due to the challenge in point-wise an- notation. Different UDA approaches have been designed to mitigate discrepancies across LiDAR point clouds of different domains. For example, [48, 62] project point clouds into depth images and leverage 2D UDA techniques while [38, 50, 51, 57] directly work in the 3D space. How- ever, these methods either work for synthetic-to-real UDA scenarios [48, 51] or normal-to-normal point cloud adap- tation [57], ignoring normal-to-adverse adaptation which is highly practical in real applications. Our SemanticSTF dataset fills up this blank and will inspire more development of new algorithms for normal-to-adverse adaptation. 3. The SemanticSTF Dataset 3.1. Background LiDAR sensors send out laser pulses and measure their flight time based on the echoes it receives from targets. The travel distance as derived from the time-of-flight and the registered angular information (between the LiDAR sensors and the targets) can be combined to compute the 3D coordi- nates of target surface which form point clouds that capture the 3D shape of the targets. However, the active LiDAR pulse system can be easily affected by the scattering media such as particles of rain droplets and snow [10, 18, 33, 36], leading to shifts of measured distances, variation of echo intensity, point missing, etc. Hence, point clouds captured under adverse weather usually have clear distribution dis- crepancy as compared with those collected under normal weather as illustrated in Fig. 1. However, existing 3DSS benchmarks are dominated by normal-weather point clouds which are insufficient for the study of universal 3DSS un- der all-weather conditions. To this end, we propose Seman- ticSTF, a point-wise annotated large-scale adverse-weather dataset that can be explored for the study of 3DSS and point cloud parsing under various adverse weather conditions. 3.2. Data Selection and Split We collect SemanticSTF by leveraging the STF bench- mark [3], a multi-modal adverse-weather dataset that was jointly collected in Germany, Sweden, Denmark, and Fin- land. The data in STF have multiple modalities including LiDAR point clouds and they are collected under various adverse weather conditions such as snow and fog. How- ever, STF provides bounding-box annotations only for the In SemanticSTF, we manu- study of 3D detection tasks. ally selected 2,076 scans captured by a Velodyne HDL64 S3D LiDAR sensor from STF that cover various adverse weather conditions including 694 snowy, 637 dense-foggy, 631 light-foggy, and 114 rainy (all rainy LiDAR scans in STF). During the selection, we pay special attention to the geographical diversity of the point clouds aiming for min- imizing data redundancy. We ignore the factor of day- time/nighttime since LiDAR sensors are robust to lighting conditions. We split SemanticSTF into three parts including 1,326 full 3D scans for training, 250 for validating, and 500 for testing. All three splits have approximately the same 3 proportion of LiDAR scans of different adverse weathers. 3.3. Data Annotation Point-wise annotation of LiDAR point clouds is an ex- tremely laborious task due to several factors, such as 3D view changes, inconsistency between point cloud display and human visual perception, sweeping occlusion, point sparsity, etc. However, point-wise annotating of adverse- weather point clouds is even more challenging due to two new factors. First, the perceived distance shifts under ad- verse weather often lead to various geometry distortions in the collected points which make them different from those collected under normal weather. This presents significant challenges for annotators who must recognize various ob- jects and assign a semantic label to each point. Second, LiDAR point clouds collected under adverse weather often contain a significant portion of invalid regions that consist of indiscernible semantic contents (e.g., thick"}, {"question": " What are some tasks investigated in large-scale datasets for adverse visual conditions?", "answer": " Localization, detection, and segmentation", "ref_chunk": "been developed rapidly over the past few years, largely through the devel- opment of various deep neural networks (DNNs) such as standard convolutional network for projection-based meth- ods [9, 30, 48, 52, 61], multi-layer perceptron (MLP)-based networks [19, 35, 35], 3D voxel convolution-based net- works [7, 64], or hybrid networks [6, 27, 43, 53, 59]. While existing 3DSS networks are mainly evaluated over normal weather point clouds, their performance for adverse weather point clouds is far under-investigated. The proposed Se- manticSTF closes the gap and provides a solid ground for the study and evaluation of all-weather 3DSS. By enabling investigations into various new research directions, Seman- ticSTF represents a valuable tool for advancing the field. Vision recognition under adverse conditions. Scene un- derstanding under adverse conditions has recently attracted increasing attention due to the strict safety demand in var- ious outdoor navigation and perception tasks. In 2D vi- sion, several large-scale datasets have been proposed to investigate perceptions tasks in adverse visual conditions including localization [29], detection [58], and segmenta- tion [37]. On the other hand, learning 3D point clouds of adverse conditions is far under-explored due to the absence of comprehensive dataset benchmarks. The recently pro- posed datasets such as STF [3] and CADC [34] contain LiDAR point clouds captured under adverse weather con- ditions. However, these studies focus on the object detec- tion task [15, 16] with bounding-box annotations, without providing any point-wise annotations. Our introduced Se- manticSTF is the first large-scale dataset that consists of Li- DAR point clouds in adverse weather conditions with high- quality dense annotations, to the best of our knowledge. Domain generalization [4,31] aims to learn a generalizable model from single or multiple related but distinct source do- mains where target data is inaccessible during model learn- It has been widely studied in 2D computer vision ing. tasks [1, 21, 26, 63] while few studies explore it in point cloud learning. Recently, [25] studies domain generaliza- tion for 3D object detection by deforming point clouds via vector fields. Differently, this work is the first attempt that explores domain generalization for 3DSS. Unsupervised domain adaptation is a method of transfer- ring knowledge learned from a labeled source domain to a target domain by leveraging the unlabeled target data. It has been widely studied in 2D image learning [12,14,20,22\u201324] and 3D point clouds [15, 16, 28, 40, 54, 55, 60]. Recently, domain adaptive 3D LiDAR segmentation has drawn in- creasing attention due to the challenge in point-wise an- notation. Different UDA approaches have been designed to mitigate discrepancies across LiDAR point clouds of different domains. For example, [48, 62] project point clouds into depth images and leverage 2D UDA techniques while [38, 50, 51, 57] directly work in the 3D space. How- ever, these methods either work for synthetic-to-real UDA scenarios [48, 51] or normal-to-normal point cloud adap- tation [57], ignoring normal-to-adverse adaptation which is highly practical in real applications. Our SemanticSTF dataset fills up this blank and will inspire more development of new algorithms for normal-to-adverse adaptation. 3. The SemanticSTF Dataset 3.1. Background LiDAR sensors send out laser pulses and measure their flight time based on the echoes it receives from targets. The travel distance as derived from the time-of-flight and the registered angular information (between the LiDAR sensors and the targets) can be combined to compute the 3D coordi- nates of target surface which form point clouds that capture the 3D shape of the targets. However, the active LiDAR pulse system can be easily affected by the scattering media such as particles of rain droplets and snow [10, 18, 33, 36], leading to shifts of measured distances, variation of echo intensity, point missing, etc. Hence, point clouds captured under adverse weather usually have clear distribution dis- crepancy as compared with those collected under normal weather as illustrated in Fig. 1. However, existing 3DSS benchmarks are dominated by normal-weather point clouds which are insufficient for the study of universal 3DSS un- der all-weather conditions. To this end, we propose Seman- ticSTF, a point-wise annotated large-scale adverse-weather dataset that can be explored for the study of 3DSS and point cloud parsing under various adverse weather conditions. 3.2. Data Selection and Split We collect SemanticSTF by leveraging the STF bench- mark [3], a multi-modal adverse-weather dataset that was jointly collected in Germany, Sweden, Denmark, and Fin- land. The data in STF have multiple modalities including LiDAR point clouds and they are collected under various adverse weather conditions such as snow and fog. How- ever, STF provides bounding-box annotations only for the In SemanticSTF, we manu- study of 3D detection tasks. ally selected 2,076 scans captured by a Velodyne HDL64 S3D LiDAR sensor from STF that cover various adverse weather conditions including 694 snowy, 637 dense-foggy, 631 light-foggy, and 114 rainy (all rainy LiDAR scans in STF). During the selection, we pay special attention to the geographical diversity of the point clouds aiming for min- imizing data redundancy. We ignore the factor of day- time/nighttime since LiDAR sensors are robust to lighting conditions. We split SemanticSTF into three parts including 1,326 full 3D scans for training, 250 for validating, and 500 for testing. All three splits have approximately the same 3 proportion of LiDAR scans of different adverse weathers. 3.3. Data Annotation Point-wise annotation of LiDAR point clouds is an ex- tremely laborious task due to several factors, such as 3D view changes, inconsistency between point cloud display and human visual perception, sweeping occlusion, point sparsity, etc. However, point-wise annotating of adverse- weather point clouds is even more challenging due to two new factors. First, the perceived distance shifts under ad- verse weather often lead to various geometry distortions in the collected points which make them different from those collected under normal weather. This presents significant challenges for annotators who must recognize various ob- jects and assign a semantic label to each point. Second, LiDAR point clouds collected under adverse weather often contain a significant portion of invalid regions that consist of indiscernible semantic contents (e.g., thick"}, {"question": " What is the challenge associated with learning 3D point clouds of adverse conditions?", "answer": " Due to the absence of comprehensive dataset benchmarks", "ref_chunk": "been developed rapidly over the past few years, largely through the devel- opment of various deep neural networks (DNNs) such as standard convolutional network for projection-based meth- ods [9, 30, 48, 52, 61], multi-layer perceptron (MLP)-based networks [19, 35, 35], 3D voxel convolution-based net- works [7, 64], or hybrid networks [6, 27, 43, 53, 59]. While existing 3DSS networks are mainly evaluated over normal weather point clouds, their performance for adverse weather point clouds is far under-investigated. The proposed Se- manticSTF closes the gap and provides a solid ground for the study and evaluation of all-weather 3DSS. By enabling investigations into various new research directions, Seman- ticSTF represents a valuable tool for advancing the field. Vision recognition under adverse conditions. Scene un- derstanding under adverse conditions has recently attracted increasing attention due to the strict safety demand in var- ious outdoor navigation and perception tasks. In 2D vi- sion, several large-scale datasets have been proposed to investigate perceptions tasks in adverse visual conditions including localization [29], detection [58], and segmenta- tion [37]. On the other hand, learning 3D point clouds of adverse conditions is far under-explored due to the absence of comprehensive dataset benchmarks. The recently pro- posed datasets such as STF [3] and CADC [34] contain LiDAR point clouds captured under adverse weather con- ditions. However, these studies focus on the object detec- tion task [15, 16] with bounding-box annotations, without providing any point-wise annotations. Our introduced Se- manticSTF is the first large-scale dataset that consists of Li- DAR point clouds in adverse weather conditions with high- quality dense annotations, to the best of our knowledge. Domain generalization [4,31] aims to learn a generalizable model from single or multiple related but distinct source do- mains where target data is inaccessible during model learn- It has been widely studied in 2D computer vision ing. tasks [1, 21, 26, 63] while few studies explore it in point cloud learning. Recently, [25] studies domain generaliza- tion for 3D object detection by deforming point clouds via vector fields. Differently, this work is the first attempt that explores domain generalization for 3DSS. Unsupervised domain adaptation is a method of transfer- ring knowledge learned from a labeled source domain to a target domain by leveraging the unlabeled target data. It has been widely studied in 2D image learning [12,14,20,22\u201324] and 3D point clouds [15, 16, 28, 40, 54, 55, 60]. Recently, domain adaptive 3D LiDAR segmentation has drawn in- creasing attention due to the challenge in point-wise an- notation. Different UDA approaches have been designed to mitigate discrepancies across LiDAR point clouds of different domains. For example, [48, 62] project point clouds into depth images and leverage 2D UDA techniques while [38, 50, 51, 57] directly work in the 3D space. How- ever, these methods either work for synthetic-to-real UDA scenarios [48, 51] or normal-to-normal point cloud adap- tation [57], ignoring normal-to-adverse adaptation which is highly practical in real applications. Our SemanticSTF dataset fills up this blank and will inspire more development of new algorithms for normal-to-adverse adaptation. 3. The SemanticSTF Dataset 3.1. Background LiDAR sensors send out laser pulses and measure their flight time based on the echoes it receives from targets. The travel distance as derived from the time-of-flight and the registered angular information (between the LiDAR sensors and the targets) can be combined to compute the 3D coordi- nates of target surface which form point clouds that capture the 3D shape of the targets. However, the active LiDAR pulse system can be easily affected by the scattering media such as particles of rain droplets and snow [10, 18, 33, 36], leading to shifts of measured distances, variation of echo intensity, point missing, etc. Hence, point clouds captured under adverse weather usually have clear distribution dis- crepancy as compared with those collected under normal weather as illustrated in Fig. 1. However, existing 3DSS benchmarks are dominated by normal-weather point clouds which are insufficient for the study of universal 3DSS un- der all-weather conditions. To this end, we propose Seman- ticSTF, a point-wise annotated large-scale adverse-weather dataset that can be explored for the study of 3DSS and point cloud parsing under various adverse weather conditions. 3.2. Data Selection and Split We collect SemanticSTF by leveraging the STF bench- mark [3], a multi-modal adverse-weather dataset that was jointly collected in Germany, Sweden, Denmark, and Fin- land. The data in STF have multiple modalities including LiDAR point clouds and they are collected under various adverse weather conditions such as snow and fog. How- ever, STF provides bounding-box annotations only for the In SemanticSTF, we manu- study of 3D detection tasks. ally selected 2,076 scans captured by a Velodyne HDL64 S3D LiDAR sensor from STF that cover various adverse weather conditions including 694 snowy, 637 dense-foggy, 631 light-foggy, and 114 rainy (all rainy LiDAR scans in STF). During the selection, we pay special attention to the geographical diversity of the point clouds aiming for min- imizing data redundancy. We ignore the factor of day- time/nighttime since LiDAR sensors are robust to lighting conditions. We split SemanticSTF into three parts including 1,326 full 3D scans for training, 250 for validating, and 500 for testing. All three splits have approximately the same 3 proportion of LiDAR scans of different adverse weathers. 3.3. Data Annotation Point-wise annotation of LiDAR point clouds is an ex- tremely laborious task due to several factors, such as 3D view changes, inconsistency between point cloud display and human visual perception, sweeping occlusion, point sparsity, etc. However, point-wise annotating of adverse- weather point clouds is even more challenging due to two new factors. First, the perceived distance shifts under ad- verse weather often lead to various geometry distortions in the collected points which make them different from those collected under normal weather. This presents significant challenges for annotators who must recognize various ob- jects and assign a semantic label to each point. Second, LiDAR point clouds collected under adverse weather often contain a significant portion of invalid regions that consist of indiscernible semantic contents (e.g., thick"}, {"question": " What is the goal of domain generalization?", "answer": " To learn a generalizable model from related but distinct source domains where target data is inaccessible during model learning", "ref_chunk": "been developed rapidly over the past few years, largely through the devel- opment of various deep neural networks (DNNs) such as standard convolutional network for projection-based meth- ods [9, 30, 48, 52, 61], multi-layer perceptron (MLP)-based networks [19, 35, 35], 3D voxel convolution-based net- works [7, 64], or hybrid networks [6, 27, 43, 53, 59]. While existing 3DSS networks are mainly evaluated over normal weather point clouds, their performance for adverse weather point clouds is far under-investigated. The proposed Se- manticSTF closes the gap and provides a solid ground for the study and evaluation of all-weather 3DSS. By enabling investigations into various new research directions, Seman- ticSTF represents a valuable tool for advancing the field. Vision recognition under adverse conditions. Scene un- derstanding under adverse conditions has recently attracted increasing attention due to the strict safety demand in var- ious outdoor navigation and perception tasks. In 2D vi- sion, several large-scale datasets have been proposed to investigate perceptions tasks in adverse visual conditions including localization [29], detection [58], and segmenta- tion [37]. On the other hand, learning 3D point clouds of adverse conditions is far under-explored due to the absence of comprehensive dataset benchmarks. The recently pro- posed datasets such as STF [3] and CADC [34] contain LiDAR point clouds captured under adverse weather con- ditions. However, these studies focus on the object detec- tion task [15, 16] with bounding-box annotations, without providing any point-wise annotations. Our introduced Se- manticSTF is the first large-scale dataset that consists of Li- DAR point clouds in adverse weather conditions with high- quality dense annotations, to the best of our knowledge. Domain generalization [4,31] aims to learn a generalizable model from single or multiple related but distinct source do- mains where target data is inaccessible during model learn- It has been widely studied in 2D computer vision ing. tasks [1, 21, 26, 63] while few studies explore it in point cloud learning. Recently, [25] studies domain generaliza- tion for 3D object detection by deforming point clouds via vector fields. Differently, this work is the first attempt that explores domain generalization for 3DSS. Unsupervised domain adaptation is a method of transfer- ring knowledge learned from a labeled source domain to a target domain by leveraging the unlabeled target data. It has been widely studied in 2D image learning [12,14,20,22\u201324] and 3D point clouds [15, 16, 28, 40, 54, 55, 60]. Recently, domain adaptive 3D LiDAR segmentation has drawn in- creasing attention due to the challenge in point-wise an- notation. Different UDA approaches have been designed to mitigate discrepancies across LiDAR point clouds of different domains. For example, [48, 62] project point clouds into depth images and leverage 2D UDA techniques while [38, 50, 51, 57] directly work in the 3D space. How- ever, these methods either work for synthetic-to-real UDA scenarios [48, 51] or normal-to-normal point cloud adap- tation [57], ignoring normal-to-adverse adaptation which is highly practical in real applications. Our SemanticSTF dataset fills up this blank and will inspire more development of new algorithms for normal-to-adverse adaptation. 3. The SemanticSTF Dataset 3.1. Background LiDAR sensors send out laser pulses and measure their flight time based on the echoes it receives from targets. The travel distance as derived from the time-of-flight and the registered angular information (between the LiDAR sensors and the targets) can be combined to compute the 3D coordi- nates of target surface which form point clouds that capture the 3D shape of the targets. However, the active LiDAR pulse system can be easily affected by the scattering media such as particles of rain droplets and snow [10, 18, 33, 36], leading to shifts of measured distances, variation of echo intensity, point missing, etc. Hence, point clouds captured under adverse weather usually have clear distribution dis- crepancy as compared with those collected under normal weather as illustrated in Fig. 1. However, existing 3DSS benchmarks are dominated by normal-weather point clouds which are insufficient for the study of universal 3DSS un- der all-weather conditions. To this end, we propose Seman- ticSTF, a point-wise annotated large-scale adverse-weather dataset that can be explored for the study of 3DSS and point cloud parsing under various adverse weather conditions. 3.2. Data Selection and Split We collect SemanticSTF by leveraging the STF bench- mark [3], a multi-modal adverse-weather dataset that was jointly collected in Germany, Sweden, Denmark, and Fin- land. The data in STF have multiple modalities including LiDAR point clouds and they are collected under various adverse weather conditions such as snow and fog. How- ever, STF provides bounding-box annotations only for the In SemanticSTF, we manu- study of 3D detection tasks. ally selected 2,076 scans captured by a Velodyne HDL64 S3D LiDAR sensor from STF that cover various adverse weather conditions including 694 snowy, 637 dense-foggy, 631 light-foggy, and 114 rainy (all rainy LiDAR scans in STF). During the selection, we pay special attention to the geographical diversity of the point clouds aiming for min- imizing data redundancy. We ignore the factor of day- time/nighttime since LiDAR sensors are robust to lighting conditions. We split SemanticSTF into three parts including 1,326 full 3D scans for training, 250 for validating, and 500 for testing. All three splits have approximately the same 3 proportion of LiDAR scans of different adverse weathers. 3.3. Data Annotation Point-wise annotation of LiDAR point clouds is an ex- tremely laborious task due to several factors, such as 3D view changes, inconsistency between point cloud display and human visual perception, sweeping occlusion, point sparsity, etc. However, point-wise annotating of adverse- weather point clouds is even more challenging due to two new factors. First, the perceived distance shifts under ad- verse weather often lead to various geometry distortions in the collected points which make them different from those collected under normal weather. This presents significant challenges for annotators who must recognize various ob- jects and assign a semantic label to each point. Second, LiDAR point clouds collected under adverse weather often contain a significant portion of invalid regions that consist of indiscernible semantic contents (e.g., thick"}, {"question": " What is unsupervised domain adaptation and why is it important?", "answer": " It is a method of transferring knowledge from a labeled source domain to an unlabeled target domain; important due to the challenge in point-wise annotation", "ref_chunk": "been developed rapidly over the past few years, largely through the devel- opment of various deep neural networks (DNNs) such as standard convolutional network for projection-based meth- ods [9, 30, 48, 52, 61], multi-layer perceptron (MLP)-based networks [19, 35, 35], 3D voxel convolution-based net- works [7, 64], or hybrid networks [6, 27, 43, 53, 59]. While existing 3DSS networks are mainly evaluated over normal weather point clouds, their performance for adverse weather point clouds is far under-investigated. The proposed Se- manticSTF closes the gap and provides a solid ground for the study and evaluation of all-weather 3DSS. By enabling investigations into various new research directions, Seman- ticSTF represents a valuable tool for advancing the field. Vision recognition under adverse conditions. Scene un- derstanding under adverse conditions has recently attracted increasing attention due to the strict safety demand in var- ious outdoor navigation and perception tasks. In 2D vi- sion, several large-scale datasets have been proposed to investigate perceptions tasks in adverse visual conditions including localization [29], detection [58], and segmenta- tion [37]. On the other hand, learning 3D point clouds of adverse conditions is far under-explored due to the absence of comprehensive dataset benchmarks. The recently pro- posed datasets such as STF [3] and CADC [34] contain LiDAR point clouds captured under adverse weather con- ditions. However, these studies focus on the object detec- tion task [15, 16] with bounding-box annotations, without providing any point-wise annotations. Our introduced Se- manticSTF is the first large-scale dataset that consists of Li- DAR point clouds in adverse weather conditions with high- quality dense annotations, to the best of our knowledge. Domain generalization [4,31] aims to learn a generalizable model from single or multiple related but distinct source do- mains where target data is inaccessible during model learn- It has been widely studied in 2D computer vision ing. tasks [1, 21, 26, 63] while few studies explore it in point cloud learning. Recently, [25] studies domain generaliza- tion for 3D object detection by deforming point clouds via vector fields. Differently, this work is the first attempt that explores domain generalization for 3DSS. Unsupervised domain adaptation is a method of transfer- ring knowledge learned from a labeled source domain to a target domain by leveraging the unlabeled target data. It has been widely studied in 2D image learning [12,14,20,22\u201324] and 3D point clouds [15, 16, 28, 40, 54, 55, 60]. Recently, domain adaptive 3D LiDAR segmentation has drawn in- creasing attention due to the challenge in point-wise an- notation. Different UDA approaches have been designed to mitigate discrepancies across LiDAR point clouds of different domains. For example, [48, 62] project point clouds into depth images and leverage 2D UDA techniques while [38, 50, 51, 57] directly work in the 3D space. How- ever, these methods either work for synthetic-to-real UDA scenarios [48, 51] or normal-to-normal point cloud adap- tation [57], ignoring normal-to-adverse adaptation which is highly practical in real applications. Our SemanticSTF dataset fills up this blank and will inspire more development of new algorithms for normal-to-adverse adaptation. 3. The SemanticSTF Dataset 3.1. Background LiDAR sensors send out laser pulses and measure their flight time based on the echoes it receives from targets. The travel distance as derived from the time-of-flight and the registered angular information (between the LiDAR sensors and the targets) can be combined to compute the 3D coordi- nates of target surface which form point clouds that capture the 3D shape of the targets. However, the active LiDAR pulse system can be easily affected by the scattering media such as particles of rain droplets and snow [10, 18, 33, 36], leading to shifts of measured distances, variation of echo intensity, point missing, etc. Hence, point clouds captured under adverse weather usually have clear distribution dis- crepancy as compared with those collected under normal weather as illustrated in Fig. 1. However, existing 3DSS benchmarks are dominated by normal-weather point clouds which are insufficient for the study of universal 3DSS un- der all-weather conditions. To this end, we propose Seman- ticSTF, a point-wise annotated large-scale adverse-weather dataset that can be explored for the study of 3DSS and point cloud parsing under various adverse weather conditions. 3.2. Data Selection and Split We collect SemanticSTF by leveraging the STF bench- mark [3], a multi-modal adverse-weather dataset that was jointly collected in Germany, Sweden, Denmark, and Fin- land. The data in STF have multiple modalities including LiDAR point clouds and they are collected under various adverse weather conditions such as snow and fog. How- ever, STF provides bounding-box annotations only for the In SemanticSTF, we manu- study of 3D detection tasks. ally selected 2,076 scans captured by a Velodyne HDL64 S3D LiDAR sensor from STF that cover various adverse weather conditions including 694 snowy, 637 dense-foggy, 631 light-foggy, and 114 rainy (all rainy LiDAR scans in STF). During the selection, we pay special attention to the geographical diversity of the point clouds aiming for min- imizing data redundancy. We ignore the factor of day- time/nighttime since LiDAR sensors are robust to lighting conditions. We split SemanticSTF into three parts including 1,326 full 3D scans for training, 250 for validating, and 500 for testing. All three splits have approximately the same 3 proportion of LiDAR scans of different adverse weathers. 3.3. Data Annotation Point-wise annotation of LiDAR point clouds is an ex- tremely laborious task due to several factors, such as 3D view changes, inconsistency between point cloud display and human visual perception, sweeping occlusion, point sparsity, etc. However, point-wise annotating of adverse- weather point clouds is even more challenging due to two new factors. First, the perceived distance shifts under ad- verse weather often lead to various geometry distortions in the collected points which make them different from those collected under normal weather. This presents significant challenges for annotators who must recognize various ob- jects and assign a semantic label to each point. Second, LiDAR point clouds collected under adverse weather often contain a significant portion of invalid regions that consist of indiscernible semantic contents (e.g., thick"}, {"question": " How does point cloud learning differ from 2D computer vision tasks in terms of domain generalization?", "answer": " While domain generalization has been widely studied in 2D tasks, few studies explore it in point cloud learning", "ref_chunk": "been developed rapidly over the past few years, largely through the devel- opment of various deep neural networks (DNNs) such as standard convolutional network for projection-based meth- ods [9, 30, 48, 52, 61], multi-layer perceptron (MLP)-based networks [19, 35, 35], 3D voxel convolution-based net- works [7, 64], or hybrid networks [6, 27, 43, 53, 59]. While existing 3DSS networks are mainly evaluated over normal weather point clouds, their performance for adverse weather point clouds is far under-investigated. The proposed Se- manticSTF closes the gap and provides a solid ground for the study and evaluation of all-weather 3DSS. By enabling investigations into various new research directions, Seman- ticSTF represents a valuable tool for advancing the field. Vision recognition under adverse conditions. Scene un- derstanding under adverse conditions has recently attracted increasing attention due to the strict safety demand in var- ious outdoor navigation and perception tasks. In 2D vi- sion, several large-scale datasets have been proposed to investigate perceptions tasks in adverse visual conditions including localization [29], detection [58], and segmenta- tion [37]. On the other hand, learning 3D point clouds of adverse conditions is far under-explored due to the absence of comprehensive dataset benchmarks. The recently pro- posed datasets such as STF [3] and CADC [34] contain LiDAR point clouds captured under adverse weather con- ditions. However, these studies focus on the object detec- tion task [15, 16] with bounding-box annotations, without providing any point-wise annotations. Our introduced Se- manticSTF is the first large-scale dataset that consists of Li- DAR point clouds in adverse weather conditions with high- quality dense annotations, to the best of our knowledge. Domain generalization [4,31] aims to learn a generalizable model from single or multiple related but distinct source do- mains where target data is inaccessible during model learn- It has been widely studied in 2D computer vision ing. tasks [1, 21, 26, 63] while few studies explore it in point cloud learning. Recently, [25] studies domain generaliza- tion for 3D object detection by deforming point clouds via vector fields. Differently, this work is the first attempt that explores domain generalization for 3DSS. Unsupervised domain adaptation is a method of transfer- ring knowledge learned from a labeled source domain to a target domain by leveraging the unlabeled target data. It has been widely studied in 2D image learning [12,14,20,22\u201324] and 3D point clouds [15, 16, 28, 40, 54, 55, 60]. Recently, domain adaptive 3D LiDAR segmentation has drawn in- creasing attention due to the challenge in point-wise an- notation. Different UDA approaches have been designed to mitigate discrepancies across LiDAR point clouds of different domains. For example, [48, 62] project point clouds into depth images and leverage 2D UDA techniques while [38, 50, 51, 57] directly work in the 3D space. How- ever, these methods either work for synthetic-to-real UDA scenarios [48, 51] or normal-to-normal point cloud adap- tation [57], ignoring normal-to-adverse adaptation which is highly practical in real applications. Our SemanticSTF dataset fills up this blank and will inspire more development of new algorithms for normal-to-adverse adaptation. 3. The SemanticSTF Dataset 3.1. Background LiDAR sensors send out laser pulses and measure their flight time based on the echoes it receives from targets. The travel distance as derived from the time-of-flight and the registered angular information (between the LiDAR sensors and the targets) can be combined to compute the 3D coordi- nates of target surface which form point clouds that capture the 3D shape of the targets. However, the active LiDAR pulse system can be easily affected by the scattering media such as particles of rain droplets and snow [10, 18, 33, 36], leading to shifts of measured distances, variation of echo intensity, point missing, etc. Hence, point clouds captured under adverse weather usually have clear distribution dis- crepancy as compared with those collected under normal weather as illustrated in Fig. 1. However, existing 3DSS benchmarks are dominated by normal-weather point clouds which are insufficient for the study of universal 3DSS un- der all-weather conditions. To this end, we propose Seman- ticSTF, a point-wise annotated large-scale adverse-weather dataset that can be explored for the study of 3DSS and point cloud parsing under various adverse weather conditions. 3.2. Data Selection and Split We collect SemanticSTF by leveraging the STF bench- mark [3], a multi-modal adverse-weather dataset that was jointly collected in Germany, Sweden, Denmark, and Fin- land. The data in STF have multiple modalities including LiDAR point clouds and they are collected under various adverse weather conditions such as snow and fog. How- ever, STF provides bounding-box annotations only for the In SemanticSTF, we manu- study of 3D detection tasks. ally selected 2,076 scans captured by a Velodyne HDL64 S3D LiDAR sensor from STF that cover various adverse weather conditions including 694 snowy, 637 dense-foggy, 631 light-foggy, and 114 rainy (all rainy LiDAR scans in STF). During the selection, we pay special attention to the geographical diversity of the point clouds aiming for min- imizing data redundancy. We ignore the factor of day- time/nighttime since LiDAR sensors are robust to lighting conditions. We split SemanticSTF into three parts including 1,326 full 3D scans for training, 250 for validating, and 500 for testing. All three splits have approximately the same 3 proportion of LiDAR scans of different adverse weathers. 3.3. Data Annotation Point-wise annotation of LiDAR point clouds is an ex- tremely laborious task due to several factors, such as 3D view changes, inconsistency between point cloud display and human visual perception, sweeping occlusion, point sparsity, etc. However, point-wise annotating of adverse- weather point clouds is even more challenging due to two new factors. First, the perceived distance shifts under ad- verse weather often lead to various geometry distortions in the collected points which make them different from those collected under normal weather. This presents significant challenges for annotators who must recognize various ob- jects and assign a semantic label to each point. Second, LiDAR point clouds collected under adverse weather often contain a significant portion of invalid regions that consist of indiscernible semantic contents (e.g., thick"}, {"question": " What are some factors that make point-wise annotation of adverse-weather point clouds challenging?", "answer": " Factors include geometry distortions, perceived distance shifts, inconsistency in collected points, and invalid semantic contents", "ref_chunk": "been developed rapidly over the past few years, largely through the devel- opment of various deep neural networks (DNNs) such as standard convolutional network for projection-based meth- ods [9, 30, 48, 52, 61], multi-layer perceptron (MLP)-based networks [19, 35, 35], 3D voxel convolution-based net- works [7, 64], or hybrid networks [6, 27, 43, 53, 59]. While existing 3DSS networks are mainly evaluated over normal weather point clouds, their performance for adverse weather point clouds is far under-investigated. The proposed Se- manticSTF closes the gap and provides a solid ground for the study and evaluation of all-weather 3DSS. By enabling investigations into various new research directions, Seman- ticSTF represents a valuable tool for advancing the field. Vision recognition under adverse conditions. Scene un- derstanding under adverse conditions has recently attracted increasing attention due to the strict safety demand in var- ious outdoor navigation and perception tasks. In 2D vi- sion, several large-scale datasets have been proposed to investigate perceptions tasks in adverse visual conditions including localization [29], detection [58], and segmenta- tion [37]. On the other hand, learning 3D point clouds of adverse conditions is far under-explored due to the absence of comprehensive dataset benchmarks. The recently pro- posed datasets such as STF [3] and CADC [34] contain LiDAR point clouds captured under adverse weather con- ditions. However, these studies focus on the object detec- tion task [15, 16] with bounding-box annotations, without providing any point-wise annotations. Our introduced Se- manticSTF is the first large-scale dataset that consists of Li- DAR point clouds in adverse weather conditions with high- quality dense annotations, to the best of our knowledge. Domain generalization [4,31] aims to learn a generalizable model from single or multiple related but distinct source do- mains where target data is inaccessible during model learn- It has been widely studied in 2D computer vision ing. tasks [1, 21, 26, 63] while few studies explore it in point cloud learning. Recently, [25] studies domain generaliza- tion for 3D object detection by deforming point clouds via vector fields. Differently, this work is the first attempt that explores domain generalization for 3DSS. Unsupervised domain adaptation is a method of transfer- ring knowledge learned from a labeled source domain to a target domain by leveraging the unlabeled target data. It has been widely studied in 2D image learning [12,14,20,22\u201324] and 3D point clouds [15, 16, 28, 40, 54, 55, 60]. Recently, domain adaptive 3D LiDAR segmentation has drawn in- creasing attention due to the challenge in point-wise an- notation. Different UDA approaches have been designed to mitigate discrepancies across LiDAR point clouds of different domains. For example, [48, 62] project point clouds into depth images and leverage 2D UDA techniques while [38, 50, 51, 57] directly work in the 3D space. How- ever, these methods either work for synthetic-to-real UDA scenarios [48, 51] or normal-to-normal point cloud adap- tation [57], ignoring normal-to-adverse adaptation which is highly practical in real applications. Our SemanticSTF dataset fills up this blank and will inspire more development of new algorithms for normal-to-adverse adaptation. 3. The SemanticSTF Dataset 3.1. Background LiDAR sensors send out laser pulses and measure their flight time based on the echoes it receives from targets. The travel distance as derived from the time-of-flight and the registered angular information (between the LiDAR sensors and the targets) can be combined to compute the 3D coordi- nates of target surface which form point clouds that capture the 3D shape of the targets. However, the active LiDAR pulse system can be easily affected by the scattering media such as particles of rain droplets and snow [10, 18, 33, 36], leading to shifts of measured distances, variation of echo intensity, point missing, etc. Hence, point clouds captured under adverse weather usually have clear distribution dis- crepancy as compared with those collected under normal weather as illustrated in Fig. 1. However, existing 3DSS benchmarks are dominated by normal-weather point clouds which are insufficient for the study of universal 3DSS un- der all-weather conditions. To this end, we propose Seman- ticSTF, a point-wise annotated large-scale adverse-weather dataset that can be explored for the study of 3DSS and point cloud parsing under various adverse weather conditions. 3.2. Data Selection and Split We collect SemanticSTF by leveraging the STF bench- mark [3], a multi-modal adverse-weather dataset that was jointly collected in Germany, Sweden, Denmark, and Fin- land. The data in STF have multiple modalities including LiDAR point clouds and they are collected under various adverse weather conditions such as snow and fog. How- ever, STF provides bounding-box annotations only for the In SemanticSTF, we manu- study of 3D detection tasks. ally selected 2,076 scans captured by a Velodyne HDL64 S3D LiDAR sensor from STF that cover various adverse weather conditions including 694 snowy, 637 dense-foggy, 631 light-foggy, and 114 rainy (all rainy LiDAR scans in STF). During the selection, we pay special attention to the geographical diversity of the point clouds aiming for min- imizing data redundancy. We ignore the factor of day- time/nighttime since LiDAR sensors are robust to lighting conditions. We split SemanticSTF into three parts including 1,326 full 3D scans for training, 250 for validating, and 500 for testing. All three splits have approximately the same 3 proportion of LiDAR scans of different adverse weathers. 3.3. Data Annotation Point-wise annotation of LiDAR point clouds is an ex- tremely laborious task due to several factors, such as 3D view changes, inconsistency between point cloud display and human visual perception, sweeping occlusion, point sparsity, etc. However, point-wise annotating of adverse- weather point clouds is even more challenging due to two new factors. First, the perceived distance shifts under ad- verse weather often lead to various geometry distortions in the collected points which make them different from those collected under normal weather. This presents significant challenges for annotators who must recognize various ob- jects and assign a semantic label to each point. Second, LiDAR point clouds collected under adverse weather often contain a significant portion of invalid regions that consist of indiscernible semantic contents (e.g., thick"}, {"question": " Why is SemanticSTF dataset considered valuable according to the text?", "answer": " It consists of LiDAR point clouds in adverse weather conditions with high-quality dense annotations, addressing a gap in existing datasets", "ref_chunk": "been developed rapidly over the past few years, largely through the devel- opment of various deep neural networks (DNNs) such as standard convolutional network for projection-based meth- ods [9, 30, 48, 52, 61], multi-layer perceptron (MLP)-based networks [19, 35, 35], 3D voxel convolution-based net- works [7, 64], or hybrid networks [6, 27, 43, 53, 59]. While existing 3DSS networks are mainly evaluated over normal weather point clouds, their performance for adverse weather point clouds is far under-investigated. The proposed Se- manticSTF closes the gap and provides a solid ground for the study and evaluation of all-weather 3DSS. By enabling investigations into various new research directions, Seman- ticSTF represents a valuable tool for advancing the field. Vision recognition under adverse conditions. Scene un- derstanding under adverse conditions has recently attracted increasing attention due to the strict safety demand in var- ious outdoor navigation and perception tasks. In 2D vi- sion, several large-scale datasets have been proposed to investigate perceptions tasks in adverse visual conditions including localization [29], detection [58], and segmenta- tion [37]. On the other hand, learning 3D point clouds of adverse conditions is far under-explored due to the absence of comprehensive dataset benchmarks. The recently pro- posed datasets such as STF [3] and CADC [34] contain LiDAR point clouds captured under adverse weather con- ditions. However, these studies focus on the object detec- tion task [15, 16] with bounding-box annotations, without providing any point-wise annotations. Our introduced Se- manticSTF is the first large-scale dataset that consists of Li- DAR point clouds in adverse weather conditions with high- quality dense annotations, to the best of our knowledge. Domain generalization [4,31] aims to learn a generalizable model from single or multiple related but distinct source do- mains where target data is inaccessible during model learn- It has been widely studied in 2D computer vision ing. tasks [1, 21, 26, 63] while few studies explore it in point cloud learning. Recently, [25] studies domain generaliza- tion for 3D object detection by deforming point clouds via vector fields. Differently, this work is the first attempt that explores domain generalization for 3DSS. Unsupervised domain adaptation is a method of transfer- ring knowledge learned from a labeled source domain to a target domain by leveraging the unlabeled target data. It has been widely studied in 2D image learning [12,14,20,22\u201324] and 3D point clouds [15, 16, 28, 40, 54, 55, 60]. Recently, domain adaptive 3D LiDAR segmentation has drawn in- creasing attention due to the challenge in point-wise an- notation. Different UDA approaches have been designed to mitigate discrepancies across LiDAR point clouds of different domains. For example, [48, 62] project point clouds into depth images and leverage 2D UDA techniques while [38, 50, 51, 57] directly work in the 3D space. How- ever, these methods either work for synthetic-to-real UDA scenarios [48, 51] or normal-to-normal point cloud adap- tation [57], ignoring normal-to-adverse adaptation which is highly practical in real applications. Our SemanticSTF dataset fills up this blank and will inspire more development of new algorithms for normal-to-adverse adaptation. 3. The SemanticSTF Dataset 3.1. Background LiDAR sensors send out laser pulses and measure their flight time based on the echoes it receives from targets. The travel distance as derived from the time-of-flight and the registered angular information (between the LiDAR sensors and the targets) can be combined to compute the 3D coordi- nates of target surface which form point clouds that capture the 3D shape of the targets. However, the active LiDAR pulse system can be easily affected by the scattering media such as particles of rain droplets and snow [10, 18, 33, 36], leading to shifts of measured distances, variation of echo intensity, point missing, etc. Hence, point clouds captured under adverse weather usually have clear distribution dis- crepancy as compared with those collected under normal weather as illustrated in Fig. 1. However, existing 3DSS benchmarks are dominated by normal-weather point clouds which are insufficient for the study of universal 3DSS un- der all-weather conditions. To this end, we propose Seman- ticSTF, a point-wise annotated large-scale adverse-weather dataset that can be explored for the study of 3DSS and point cloud parsing under various adverse weather conditions. 3.2. Data Selection and Split We collect SemanticSTF by leveraging the STF bench- mark [3], a multi-modal adverse-weather dataset that was jointly collected in Germany, Sweden, Denmark, and Fin- land. The data in STF have multiple modalities including LiDAR point clouds and they are collected under various adverse weather conditions such as snow and fog. How- ever, STF provides bounding-box annotations only for the In SemanticSTF, we manu- study of 3D detection tasks. ally selected 2,076 scans captured by a Velodyne HDL64 S3D LiDAR sensor from STF that cover various adverse weather conditions including 694 snowy, 637 dense-foggy, 631 light-foggy, and 114 rainy (all rainy LiDAR scans in STF). During the selection, we pay special attention to the geographical diversity of the point clouds aiming for min- imizing data redundancy. We ignore the factor of day- time/nighttime since LiDAR sensors are robust to lighting conditions. We split SemanticSTF into three parts including 1,326 full 3D scans for training, 250 for validating, and 500 for testing. All three splits have approximately the same 3 proportion of LiDAR scans of different adverse weathers. 3.3. Data Annotation Point-wise annotation of LiDAR point clouds is an ex- tremely laborious task due to several factors, such as 3D view changes, inconsistency between point cloud display and human visual perception, sweeping occlusion, point sparsity, etc. However, point-wise annotating of adverse- weather point clouds is even more challenging due to two new factors. First, the perceived distance shifts under ad- verse weather often lead to various geometry distortions in the collected points which make them different from those collected under normal weather. This presents significant challenges for annotators who must recognize various ob- jects and assign a semantic label to each point. Second, LiDAR point clouds collected under adverse weather often contain a significant portion of invalid regions that consist of indiscernible semantic contents (e.g., thick"}], "doc_text": "been developed rapidly over the past few years, largely through the devel- opment of various deep neural networks (DNNs) such as standard convolutional network for projection-based meth- ods [9, 30, 48, 52, 61], multi-layer perceptron (MLP)-based networks [19, 35, 35], 3D voxel convolution-based net- works [7, 64], or hybrid networks [6, 27, 43, 53, 59]. While existing 3DSS networks are mainly evaluated over normal weather point clouds, their performance for adverse weather point clouds is far under-investigated. The proposed Se- manticSTF closes the gap and provides a solid ground for the study and evaluation of all-weather 3DSS. By enabling investigations into various new research directions, Seman- ticSTF represents a valuable tool for advancing the field. Vision recognition under adverse conditions. Scene un- derstanding under adverse conditions has recently attracted increasing attention due to the strict safety demand in var- ious outdoor navigation and perception tasks. In 2D vi- sion, several large-scale datasets have been proposed to investigate perceptions tasks in adverse visual conditions including localization [29], detection [58], and segmenta- tion [37]. On the other hand, learning 3D point clouds of adverse conditions is far under-explored due to the absence of comprehensive dataset benchmarks. The recently pro- posed datasets such as STF [3] and CADC [34] contain LiDAR point clouds captured under adverse weather con- ditions. However, these studies focus on the object detec- tion task [15, 16] with bounding-box annotations, without providing any point-wise annotations. Our introduced Se- manticSTF is the first large-scale dataset that consists of Li- DAR point clouds in adverse weather conditions with high- quality dense annotations, to the best of our knowledge. Domain generalization [4,31] aims to learn a generalizable model from single or multiple related but distinct source do- mains where target data is inaccessible during model learn- It has been widely studied in 2D computer vision ing. tasks [1, 21, 26, 63] while few studies explore it in point cloud learning. Recently, [25] studies domain generaliza- tion for 3D object detection by deforming point clouds via vector fields. Differently, this work is the first attempt that explores domain generalization for 3DSS. Unsupervised domain adaptation is a method of transfer- ring knowledge learned from a labeled source domain to a target domain by leveraging the unlabeled target data. It has been widely studied in 2D image learning [12,14,20,22\u201324] and 3D point clouds [15, 16, 28, 40, 54, 55, 60]. Recently, domain adaptive 3D LiDAR segmentation has drawn in- creasing attention due to the challenge in point-wise an- notation. Different UDA approaches have been designed to mitigate discrepancies across LiDAR point clouds of different domains. For example, [48, 62] project point clouds into depth images and leverage 2D UDA techniques while [38, 50, 51, 57] directly work in the 3D space. How- ever, these methods either work for synthetic-to-real UDA scenarios [48, 51] or normal-to-normal point cloud adap- tation [57], ignoring normal-to-adverse adaptation which is highly practical in real applications. Our SemanticSTF dataset fills up this blank and will inspire more development of new algorithms for normal-to-adverse adaptation. 3. The SemanticSTF Dataset 3.1. Background LiDAR sensors send out laser pulses and measure their flight time based on the echoes it receives from targets. The travel distance as derived from the time-of-flight and the registered angular information (between the LiDAR sensors and the targets) can be combined to compute the 3D coordi- nates of target surface which form point clouds that capture the 3D shape of the targets. However, the active LiDAR pulse system can be easily affected by the scattering media such as particles of rain droplets and snow [10, 18, 33, 36], leading to shifts of measured distances, variation of echo intensity, point missing, etc. Hence, point clouds captured under adverse weather usually have clear distribution dis- crepancy as compared with those collected under normal weather as illustrated in Fig. 1. However, existing 3DSS benchmarks are dominated by normal-weather point clouds which are insufficient for the study of universal 3DSS un- der all-weather conditions. To this end, we propose Seman- ticSTF, a point-wise annotated large-scale adverse-weather dataset that can be explored for the study of 3DSS and point cloud parsing under various adverse weather conditions. 3.2. Data Selection and Split We collect SemanticSTF by leveraging the STF bench- mark [3], a multi-modal adverse-weather dataset that was jointly collected in Germany, Sweden, Denmark, and Fin- land. The data in STF have multiple modalities including LiDAR point clouds and they are collected under various adverse weather conditions such as snow and fog. How- ever, STF provides bounding-box annotations only for the In SemanticSTF, we manu- study of 3D detection tasks. ally selected 2,076 scans captured by a Velodyne HDL64 S3D LiDAR sensor from STF that cover various adverse weather conditions including 694 snowy, 637 dense-foggy, 631 light-foggy, and 114 rainy (all rainy LiDAR scans in STF). During the selection, we pay special attention to the geographical diversity of the point clouds aiming for min- imizing data redundancy. We ignore the factor of day- time/nighttime since LiDAR sensors are robust to lighting conditions. We split SemanticSTF into three parts including 1,326 full 3D scans for training, 250 for validating, and 500 for testing. All three splits have approximately the same 3 proportion of LiDAR scans of different adverse weathers. 3.3. Data Annotation Point-wise annotation of LiDAR point clouds is an ex- tremely laborious task due to several factors, such as 3D view changes, inconsistency between point cloud display and human visual perception, sweeping occlusion, point sparsity, etc. However, point-wise annotating of adverse- weather point clouds is even more challenging due to two new factors. First, the perceived distance shifts under ad- verse weather often lead to various geometry distortions in the collected points which make them different from those collected under normal weather. This presents significant challenges for annotators who must recognize various ob- jects and assign a semantic label to each point. Second, LiDAR point clouds collected under adverse weather often contain a significant portion of invalid regions that consist of indiscernible semantic contents (e.g., thick"}