{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Alexander_I._Rudnicky_A_Vector_Quantized_Approach_for_Text_to_Speech_Synthesis_on_Real-World_Spontaneous_Speech_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does [R] refer to in the text?", "answer": " The repetition token", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " What are [S] and [E] in the text?", "answer": " Start and end tokens, respectively", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " What are the hyper-parameters \u03b3 and \u03bb used for in equation (3)?", "answer": " Commitment loss weight and quantization loss weight", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " What is the difference between LV Q and previous work in image generation?", "answer": " LV Q does not include the reconstruction loss as the mel-spectrogram loss in LGAN serves that purpose", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " How is the transformer conditioned in the conditional synthesis stage?", "answer": " It is conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " What is the role of ALiBi in the architectural design of the model?", "answer": " ALiBi is used to replace positional encoding in order to enable the model to extrapolate to long input sequences and syntheses for both encoder and decoder", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " What is the purpose of the Sub-decoder module in the multi-output transformer?", "answer": " The Sub-decoder module explicitly models the conditional distribution of codes and operates sequentially over the codebook groups", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " Why is a repetition token introduced in the transformer model?", "answer": " To explicitly model transitions where consecutive repeated codes occur", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " How is nucleus sampling utilized in transformer inference?", "answer": " Nucleus sampling is used to generate the fully decoded codes which are then passed to QD to reconstruct the raw waveform", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " How does the model attempt to prompt the synthesis of clean speech in the presence of background noise?", "answer": " By prepending 3 frames of codes from a silence clip encoded by QE before the actual synthesis", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}], "doc_text": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}