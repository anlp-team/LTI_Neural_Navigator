{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/David_R._Mortensen_Do_All_Languages_Cost_the_Same?_Tokenization_in_the_Era_of_Commercial_Language_Models_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is one challenge that users face in determining the total token count in a model?", "answer": " The total token count is not immediately obvious to users except through a separate tokenizer interface.", "ref_chunk": "design choice that the model developers make. The total token count is also not immediately obvious to users ex- cept through a tokenizer interface4 separate from the chat interface. 2While most services also have free tiers, they limit daily usage to a small number of tokens per use. 3e.g. see OpenAI models\u2019 cost: https://openai.com/p ricing. 4https://platform.openai.com/tokenizer LMs Tokenization\u2014 Tokenization segmenting text an active research area. Proposed approaches range from defining tokens as whitespace-delimited words (for languages that use whitespace) which makes the vocabulary extremely large, to defining tokens as characters or even bytes, which makes the tokenized sequences extremely long in terms of number of tokens; see Mielke et al. (2021) for a detailed survey. A commonly-used solution now is to tokenize text into subword chunks (Sennrich et al., 2016; Kudo, 2018; Song et al., 2021). With Sennrich et al. (2016), one starts with a base vocab- ulary of only characters and adds new vocabulary items by recursively merging existing ones based on their frequency statistics in a training corpus. Other approaches judge subword candidates to be included into the vocabulary using a language model (Kudo, 2018; Song et al., 2021). For multilingual models containing data in a variety of scripts, even the base vocabulary of only characters (based on Unicode symbols) can be very large with over 130K types. Radford et al. (2019) instead proposed using a byte-level base vocabulary with only 256 tokens. Termed byte-level byte pair encoding (BBPE), this approach has become the de facto standard used in most modern language modeling efforts (Brown et al., 2020; Muennighoff et al., 2022; Scao et al., 2022; Black et al., 2022; Rae et al., 2022; Zhang et al., 2022b; Dey et al., 2023; Nagoudi et al., 2022; Zhang et al., 2022b). in into atomic units\u2014is In this work, we investigate the impact this tok- enization strategy has on LM API cost disparity as well as downstream task performance (i.e., utility) across different languages. 2.2 Investigating the Impact of Byte-level Subword Segmentation There are hundreds of distinct writing systems in the world. BBPE, by design, makes vocabulary construction script-agnostic, even allowing (in prin- ciple) new scripts to be supported later on without modifying the vocabulary. However, not only are different scripts encoded differently, their distri- bution in the training corpora varies widely. To investigate the effects of this variation, we propose the following research questions as the main focus of this work. RQ1 (number of tokens): do all languages con- vey the same information with the same num- ber of tokens? We analyze the fragmentation of sequences in different languages with different tokenizers. We find that among the supported lan- guages in popular LLMs, there is a large variance in the average number of tokens required to convey the same information with some languages requir- ing 5 times as many tokens than others. Previous work has shown that tokenization in multilingual models is usually biased towards high-resourced languages in the pretraining data (\u00c1cs, 2019; Rust et al., 2021); we observe that this is not always the case, but it could also be dependent on linguistic features or properties of language scripts. RQ2 (cost): do non-uniform tokenization rates lead to LM API cost disparity for speakers of dif- ferent languages? LM APIs like ChatGPT are available worldwide and have been widely claimed to have multilingual capabilities (Kasai et al., 2023; Lai et al., 2023).5 We show that disparate frag- mentation rates in different languages can lead to significantly high usage costs for less represented languages and argue for a more equitable API pric- ing system. RQ3 (model utility): do non-uniform tokeniza- tion rates affect the models\u2019 utility? LMs have exhibited in-context learning capabilities, perform- ing new tasks with a few demonstrations as input (without parameter finetuning). This is a highly desirable property in any LM API as it avoids com- putational, annotation (and thus financial) costs. We show that the high fragmentation rate of a lan- guage can negatively affect the in-context learning performance in that language, resulting in reduced model utility. RQ4 (socio-economic aspects): what are the socio-economic implications of the API\u2019s cross- lingual cost and performance disparity? Our analysis shows evidence that not only are LMs more expensive for certain languages, they are also less effective for them. To highlight the implica- tions of these findings, we correlate those measure- ments with the socio-economic indicators of lan- guage speakers as a proxy for affordability of the APIs. This analysis indicates that users who likely cannot afford high API costs are charged more for poorer service, hindering uniform accessibility of this technology. 5https://help.openai.com/en/articles/674236 9-how-do-i-use-the-openai-api-in-different-lan guages 3 Experimental Setup 3.1 Models Throughout this work, we focus on two language models: ChatGPT (Ouyang et al., 2022; Brown et al., 2020) (gpt-3.5-turbo) and BLOOMZ (Muen- nighoff et al., 2022). Both of these models are trained and advertised as general-purpose models capable of following instructions and performing a wide range of tasks (Bang et al., 2023; Qin et al., 2023; Zhu et al., 2023b; Ahuja et al., 2023; Huang et al., 2023; Zhu et al., 2023a). ChatGPT (Ouyang et al., 2022) is a closed model only accessible through an API (with a premium tier) provided by OpenAI. Studies report that it supports as many as 90 languages (Ahuja et al., 2023). ChatGPT can handle a maximum sequence length of 4096 tokens (including both the prompt and generated tokens). BLOOMZ (Muennighoff et al., 2022) is an open- source multilingual model trained on 46 natural lan- guages and 13 programming languages. The best- performing version of this model has 175B parame- ters and is not feasible to be loaded on our academic servers; hence we rely on a free API of BLOOMZ hosted by Huggingface.6 Although BLOOMZ was trained with ALiBi positional embeddings (Press et al., 2022) which allows the model to extrapolate to any length sequences during inference, the Hug- gingface API has a context limit of 1000 tokens. 3.2 Tasks and Datasets To answer RQ1\u2014whether the same information is conveyed with similar numbers of"}, {"question": " How do most services limit daily usage of tokens in their free tiers?", "answer": " Most services limit daily token usage to a small number of tokens per use.", "ref_chunk": "design choice that the model developers make. The total token count is also not immediately obvious to users ex- cept through a tokenizer interface4 separate from the chat interface. 2While most services also have free tiers, they limit daily usage to a small number of tokens per use. 3e.g. see OpenAI models\u2019 cost: https://openai.com/p ricing. 4https://platform.openai.com/tokenizer LMs Tokenization\u2014 Tokenization segmenting text an active research area. Proposed approaches range from defining tokens as whitespace-delimited words (for languages that use whitespace) which makes the vocabulary extremely large, to defining tokens as characters or even bytes, which makes the tokenized sequences extremely long in terms of number of tokens; see Mielke et al. (2021) for a detailed survey. A commonly-used solution now is to tokenize text into subword chunks (Sennrich et al., 2016; Kudo, 2018; Song et al., 2021). With Sennrich et al. (2016), one starts with a base vocab- ulary of only characters and adds new vocabulary items by recursively merging existing ones based on their frequency statistics in a training corpus. Other approaches judge subword candidates to be included into the vocabulary using a language model (Kudo, 2018; Song et al., 2021). For multilingual models containing data in a variety of scripts, even the base vocabulary of only characters (based on Unicode symbols) can be very large with over 130K types. Radford et al. (2019) instead proposed using a byte-level base vocabulary with only 256 tokens. Termed byte-level byte pair encoding (BBPE), this approach has become the de facto standard used in most modern language modeling efforts (Brown et al., 2020; Muennighoff et al., 2022; Scao et al., 2022; Black et al., 2022; Rae et al., 2022; Zhang et al., 2022b; Dey et al., 2023; Nagoudi et al., 2022; Zhang et al., 2022b). in into atomic units\u2014is In this work, we investigate the impact this tok- enization strategy has on LM API cost disparity as well as downstream task performance (i.e., utility) across different languages. 2.2 Investigating the Impact of Byte-level Subword Segmentation There are hundreds of distinct writing systems in the world. BBPE, by design, makes vocabulary construction script-agnostic, even allowing (in prin- ciple) new scripts to be supported later on without modifying the vocabulary. However, not only are different scripts encoded differently, their distri- bution in the training corpora varies widely. To investigate the effects of this variation, we propose the following research questions as the main focus of this work. RQ1 (number of tokens): do all languages con- vey the same information with the same num- ber of tokens? We analyze the fragmentation of sequences in different languages with different tokenizers. We find that among the supported lan- guages in popular LLMs, there is a large variance in the average number of tokens required to convey the same information with some languages requir- ing 5 times as many tokens than others. Previous work has shown that tokenization in multilingual models is usually biased towards high-resourced languages in the pretraining data (\u00c1cs, 2019; Rust et al., 2021); we observe that this is not always the case, but it could also be dependent on linguistic features or properties of language scripts. RQ2 (cost): do non-uniform tokenization rates lead to LM API cost disparity for speakers of dif- ferent languages? LM APIs like ChatGPT are available worldwide and have been widely claimed to have multilingual capabilities (Kasai et al., 2023; Lai et al., 2023).5 We show that disparate frag- mentation rates in different languages can lead to significantly high usage costs for less represented languages and argue for a more equitable API pric- ing system. RQ3 (model utility): do non-uniform tokeniza- tion rates affect the models\u2019 utility? LMs have exhibited in-context learning capabilities, perform- ing new tasks with a few demonstrations as input (without parameter finetuning). This is a highly desirable property in any LM API as it avoids com- putational, annotation (and thus financial) costs. We show that the high fragmentation rate of a lan- guage can negatively affect the in-context learning performance in that language, resulting in reduced model utility. RQ4 (socio-economic aspects): what are the socio-economic implications of the API\u2019s cross- lingual cost and performance disparity? Our analysis shows evidence that not only are LMs more expensive for certain languages, they are also less effective for them. To highlight the implica- tions of these findings, we correlate those measure- ments with the socio-economic indicators of lan- guage speakers as a proxy for affordability of the APIs. This analysis indicates that users who likely cannot afford high API costs are charged more for poorer service, hindering uniform accessibility of this technology. 5https://help.openai.com/en/articles/674236 9-how-do-i-use-the-openai-api-in-different-lan guages 3 Experimental Setup 3.1 Models Throughout this work, we focus on two language models: ChatGPT (Ouyang et al., 2022; Brown et al., 2020) (gpt-3.5-turbo) and BLOOMZ (Muen- nighoff et al., 2022). Both of these models are trained and advertised as general-purpose models capable of following instructions and performing a wide range of tasks (Bang et al., 2023; Qin et al., 2023; Zhu et al., 2023b; Ahuja et al., 2023; Huang et al., 2023; Zhu et al., 2023a). ChatGPT (Ouyang et al., 2022) is a closed model only accessible through an API (with a premium tier) provided by OpenAI. Studies report that it supports as many as 90 languages (Ahuja et al., 2023). ChatGPT can handle a maximum sequence length of 4096 tokens (including both the prompt and generated tokens). BLOOMZ (Muennighoff et al., 2022) is an open- source multilingual model trained on 46 natural lan- guages and 13 programming languages. The best- performing version of this model has 175B parame- ters and is not feasible to be loaded on our academic servers; hence we rely on a free API of BLOOMZ hosted by Huggingface.6 Although BLOOMZ was trained with ALiBi positional embeddings (Press et al., 2022) which allows the model to extrapolate to any length sequences during inference, the Hug- gingface API has a context limit of 1000 tokens. 3.2 Tasks and Datasets To answer RQ1\u2014whether the same information is conveyed with similar numbers of"}, {"question": " What are some proposed approaches for tokenizing text?", "answer": " Proposed approaches range from defining tokens as whitespace-delimited words to defining tokens as characters or bytes.", "ref_chunk": "design choice that the model developers make. The total token count is also not immediately obvious to users ex- cept through a tokenizer interface4 separate from the chat interface. 2While most services also have free tiers, they limit daily usage to a small number of tokens per use. 3e.g. see OpenAI models\u2019 cost: https://openai.com/p ricing. 4https://platform.openai.com/tokenizer LMs Tokenization\u2014 Tokenization segmenting text an active research area. Proposed approaches range from defining tokens as whitespace-delimited words (for languages that use whitespace) which makes the vocabulary extremely large, to defining tokens as characters or even bytes, which makes the tokenized sequences extremely long in terms of number of tokens; see Mielke et al. (2021) for a detailed survey. A commonly-used solution now is to tokenize text into subword chunks (Sennrich et al., 2016; Kudo, 2018; Song et al., 2021). With Sennrich et al. (2016), one starts with a base vocab- ulary of only characters and adds new vocabulary items by recursively merging existing ones based on their frequency statistics in a training corpus. Other approaches judge subword candidates to be included into the vocabulary using a language model (Kudo, 2018; Song et al., 2021). For multilingual models containing data in a variety of scripts, even the base vocabulary of only characters (based on Unicode symbols) can be very large with over 130K types. Radford et al. (2019) instead proposed using a byte-level base vocabulary with only 256 tokens. Termed byte-level byte pair encoding (BBPE), this approach has become the de facto standard used in most modern language modeling efforts (Brown et al., 2020; Muennighoff et al., 2022; Scao et al., 2022; Black et al., 2022; Rae et al., 2022; Zhang et al., 2022b; Dey et al., 2023; Nagoudi et al., 2022; Zhang et al., 2022b). in into atomic units\u2014is In this work, we investigate the impact this tok- enization strategy has on LM API cost disparity as well as downstream task performance (i.e., utility) across different languages. 2.2 Investigating the Impact of Byte-level Subword Segmentation There are hundreds of distinct writing systems in the world. BBPE, by design, makes vocabulary construction script-agnostic, even allowing (in prin- ciple) new scripts to be supported later on without modifying the vocabulary. However, not only are different scripts encoded differently, their distri- bution in the training corpora varies widely. To investigate the effects of this variation, we propose the following research questions as the main focus of this work. RQ1 (number of tokens): do all languages con- vey the same information with the same num- ber of tokens? We analyze the fragmentation of sequences in different languages with different tokenizers. We find that among the supported lan- guages in popular LLMs, there is a large variance in the average number of tokens required to convey the same information with some languages requir- ing 5 times as many tokens than others. Previous work has shown that tokenization in multilingual models is usually biased towards high-resourced languages in the pretraining data (\u00c1cs, 2019; Rust et al., 2021); we observe that this is not always the case, but it could also be dependent on linguistic features or properties of language scripts. RQ2 (cost): do non-uniform tokenization rates lead to LM API cost disparity for speakers of dif- ferent languages? LM APIs like ChatGPT are available worldwide and have been widely claimed to have multilingual capabilities (Kasai et al., 2023; Lai et al., 2023).5 We show that disparate frag- mentation rates in different languages can lead to significantly high usage costs for less represented languages and argue for a more equitable API pric- ing system. RQ3 (model utility): do non-uniform tokeniza- tion rates affect the models\u2019 utility? LMs have exhibited in-context learning capabilities, perform- ing new tasks with a few demonstrations as input (without parameter finetuning). This is a highly desirable property in any LM API as it avoids com- putational, annotation (and thus financial) costs. We show that the high fragmentation rate of a lan- guage can negatively affect the in-context learning performance in that language, resulting in reduced model utility. RQ4 (socio-economic aspects): what are the socio-economic implications of the API\u2019s cross- lingual cost and performance disparity? Our analysis shows evidence that not only are LMs more expensive for certain languages, they are also less effective for them. To highlight the implica- tions of these findings, we correlate those measure- ments with the socio-economic indicators of lan- guage speakers as a proxy for affordability of the APIs. This analysis indicates that users who likely cannot afford high API costs are charged more for poorer service, hindering uniform accessibility of this technology. 5https://help.openai.com/en/articles/674236 9-how-do-i-use-the-openai-api-in-different-lan guages 3 Experimental Setup 3.1 Models Throughout this work, we focus on two language models: ChatGPT (Ouyang et al., 2022; Brown et al., 2020) (gpt-3.5-turbo) and BLOOMZ (Muen- nighoff et al., 2022). Both of these models are trained and advertised as general-purpose models capable of following instructions and performing a wide range of tasks (Bang et al., 2023; Qin et al., 2023; Zhu et al., 2023b; Ahuja et al., 2023; Huang et al., 2023; Zhu et al., 2023a). ChatGPT (Ouyang et al., 2022) is a closed model only accessible through an API (with a premium tier) provided by OpenAI. Studies report that it supports as many as 90 languages (Ahuja et al., 2023). ChatGPT can handle a maximum sequence length of 4096 tokens (including both the prompt and generated tokens). BLOOMZ (Muennighoff et al., 2022) is an open- source multilingual model trained on 46 natural lan- guages and 13 programming languages. The best- performing version of this model has 175B parame- ters and is not feasible to be loaded on our academic servers; hence we rely on a free API of BLOOMZ hosted by Huggingface.6 Although BLOOMZ was trained with ALiBi positional embeddings (Press et al., 2022) which allows the model to extrapolate to any length sequences during inference, the Hug- gingface API has a context limit of 1000 tokens. 3.2 Tasks and Datasets To answer RQ1\u2014whether the same information is conveyed with similar numbers of"}, {"question": " What is BBPE (Byte-level Byte Pair Encoding) and why is it used in modern language modeling efforts?", "answer": " BBPE is a byte-level base vocabulary with only 256 tokens used in modern language modeling efforts for its effectiveness.", "ref_chunk": "design choice that the model developers make. The total token count is also not immediately obvious to users ex- cept through a tokenizer interface4 separate from the chat interface. 2While most services also have free tiers, they limit daily usage to a small number of tokens per use. 3e.g. see OpenAI models\u2019 cost: https://openai.com/p ricing. 4https://platform.openai.com/tokenizer LMs Tokenization\u2014 Tokenization segmenting text an active research area. Proposed approaches range from defining tokens as whitespace-delimited words (for languages that use whitespace) which makes the vocabulary extremely large, to defining tokens as characters or even bytes, which makes the tokenized sequences extremely long in terms of number of tokens; see Mielke et al. (2021) for a detailed survey. A commonly-used solution now is to tokenize text into subword chunks (Sennrich et al., 2016; Kudo, 2018; Song et al., 2021). With Sennrich et al. (2016), one starts with a base vocab- ulary of only characters and adds new vocabulary items by recursively merging existing ones based on their frequency statistics in a training corpus. Other approaches judge subword candidates to be included into the vocabulary using a language model (Kudo, 2018; Song et al., 2021). For multilingual models containing data in a variety of scripts, even the base vocabulary of only characters (based on Unicode symbols) can be very large with over 130K types. Radford et al. (2019) instead proposed using a byte-level base vocabulary with only 256 tokens. Termed byte-level byte pair encoding (BBPE), this approach has become the de facto standard used in most modern language modeling efforts (Brown et al., 2020; Muennighoff et al., 2022; Scao et al., 2022; Black et al., 2022; Rae et al., 2022; Zhang et al., 2022b; Dey et al., 2023; Nagoudi et al., 2022; Zhang et al., 2022b). in into atomic units\u2014is In this work, we investigate the impact this tok- enization strategy has on LM API cost disparity as well as downstream task performance (i.e., utility) across different languages. 2.2 Investigating the Impact of Byte-level Subword Segmentation There are hundreds of distinct writing systems in the world. BBPE, by design, makes vocabulary construction script-agnostic, even allowing (in prin- ciple) new scripts to be supported later on without modifying the vocabulary. However, not only are different scripts encoded differently, their distri- bution in the training corpora varies widely. To investigate the effects of this variation, we propose the following research questions as the main focus of this work. RQ1 (number of tokens): do all languages con- vey the same information with the same num- ber of tokens? We analyze the fragmentation of sequences in different languages with different tokenizers. We find that among the supported lan- guages in popular LLMs, there is a large variance in the average number of tokens required to convey the same information with some languages requir- ing 5 times as many tokens than others. Previous work has shown that tokenization in multilingual models is usually biased towards high-resourced languages in the pretraining data (\u00c1cs, 2019; Rust et al., 2021); we observe that this is not always the case, but it could also be dependent on linguistic features or properties of language scripts. RQ2 (cost): do non-uniform tokenization rates lead to LM API cost disparity for speakers of dif- ferent languages? LM APIs like ChatGPT are available worldwide and have been widely claimed to have multilingual capabilities (Kasai et al., 2023; Lai et al., 2023).5 We show that disparate frag- mentation rates in different languages can lead to significantly high usage costs for less represented languages and argue for a more equitable API pric- ing system. RQ3 (model utility): do non-uniform tokeniza- tion rates affect the models\u2019 utility? LMs have exhibited in-context learning capabilities, perform- ing new tasks with a few demonstrations as input (without parameter finetuning). This is a highly desirable property in any LM API as it avoids com- putational, annotation (and thus financial) costs. We show that the high fragmentation rate of a lan- guage can negatively affect the in-context learning performance in that language, resulting in reduced model utility. RQ4 (socio-economic aspects): what are the socio-economic implications of the API\u2019s cross- lingual cost and performance disparity? Our analysis shows evidence that not only are LMs more expensive for certain languages, they are also less effective for them. To highlight the implica- tions of these findings, we correlate those measure- ments with the socio-economic indicators of lan- guage speakers as a proxy for affordability of the APIs. This analysis indicates that users who likely cannot afford high API costs are charged more for poorer service, hindering uniform accessibility of this technology. 5https://help.openai.com/en/articles/674236 9-how-do-i-use-the-openai-api-in-different-lan guages 3 Experimental Setup 3.1 Models Throughout this work, we focus on two language models: ChatGPT (Ouyang et al., 2022; Brown et al., 2020) (gpt-3.5-turbo) and BLOOMZ (Muen- nighoff et al., 2022). Both of these models are trained and advertised as general-purpose models capable of following instructions and performing a wide range of tasks (Bang et al., 2023; Qin et al., 2023; Zhu et al., 2023b; Ahuja et al., 2023; Huang et al., 2023; Zhu et al., 2023a). ChatGPT (Ouyang et al., 2022) is a closed model only accessible through an API (with a premium tier) provided by OpenAI. Studies report that it supports as many as 90 languages (Ahuja et al., 2023). ChatGPT can handle a maximum sequence length of 4096 tokens (including both the prompt and generated tokens). BLOOMZ (Muennighoff et al., 2022) is an open- source multilingual model trained on 46 natural lan- guages and 13 programming languages. The best- performing version of this model has 175B parame- ters and is not feasible to be loaded on our academic servers; hence we rely on a free API of BLOOMZ hosted by Huggingface.6 Although BLOOMZ was trained with ALiBi positional embeddings (Press et al., 2022) which allows the model to extrapolate to any length sequences during inference, the Hug- gingface API has a context limit of 1000 tokens. 3.2 Tasks and Datasets To answer RQ1\u2014whether the same information is conveyed with similar numbers of"}, {"question": " What is the main focus of the research questions proposed in the text?", "answer": " The main focus is on investigating the impact of tokenization strategy on LM API cost disparity and downstream task performance across different languages.", "ref_chunk": "design choice that the model developers make. The total token count is also not immediately obvious to users ex- cept through a tokenizer interface4 separate from the chat interface. 2While most services also have free tiers, they limit daily usage to a small number of tokens per use. 3e.g. see OpenAI models\u2019 cost: https://openai.com/p ricing. 4https://platform.openai.com/tokenizer LMs Tokenization\u2014 Tokenization segmenting text an active research area. Proposed approaches range from defining tokens as whitespace-delimited words (for languages that use whitespace) which makes the vocabulary extremely large, to defining tokens as characters or even bytes, which makes the tokenized sequences extremely long in terms of number of tokens; see Mielke et al. (2021) for a detailed survey. A commonly-used solution now is to tokenize text into subword chunks (Sennrich et al., 2016; Kudo, 2018; Song et al., 2021). With Sennrich et al. (2016), one starts with a base vocab- ulary of only characters and adds new vocabulary items by recursively merging existing ones based on their frequency statistics in a training corpus. Other approaches judge subword candidates to be included into the vocabulary using a language model (Kudo, 2018; Song et al., 2021). For multilingual models containing data in a variety of scripts, even the base vocabulary of only characters (based on Unicode symbols) can be very large with over 130K types. Radford et al. (2019) instead proposed using a byte-level base vocabulary with only 256 tokens. Termed byte-level byte pair encoding (BBPE), this approach has become the de facto standard used in most modern language modeling efforts (Brown et al., 2020; Muennighoff et al., 2022; Scao et al., 2022; Black et al., 2022; Rae et al., 2022; Zhang et al., 2022b; Dey et al., 2023; Nagoudi et al., 2022; Zhang et al., 2022b). in into atomic units\u2014is In this work, we investigate the impact this tok- enization strategy has on LM API cost disparity as well as downstream task performance (i.e., utility) across different languages. 2.2 Investigating the Impact of Byte-level Subword Segmentation There are hundreds of distinct writing systems in the world. BBPE, by design, makes vocabulary construction script-agnostic, even allowing (in prin- ciple) new scripts to be supported later on without modifying the vocabulary. However, not only are different scripts encoded differently, their distri- bution in the training corpora varies widely. To investigate the effects of this variation, we propose the following research questions as the main focus of this work. RQ1 (number of tokens): do all languages con- vey the same information with the same num- ber of tokens? We analyze the fragmentation of sequences in different languages with different tokenizers. We find that among the supported lan- guages in popular LLMs, there is a large variance in the average number of tokens required to convey the same information with some languages requir- ing 5 times as many tokens than others. Previous work has shown that tokenization in multilingual models is usually biased towards high-resourced languages in the pretraining data (\u00c1cs, 2019; Rust et al., 2021); we observe that this is not always the case, but it could also be dependent on linguistic features or properties of language scripts. RQ2 (cost): do non-uniform tokenization rates lead to LM API cost disparity for speakers of dif- ferent languages? LM APIs like ChatGPT are available worldwide and have been widely claimed to have multilingual capabilities (Kasai et al., 2023; Lai et al., 2023).5 We show that disparate frag- mentation rates in different languages can lead to significantly high usage costs for less represented languages and argue for a more equitable API pric- ing system. RQ3 (model utility): do non-uniform tokeniza- tion rates affect the models\u2019 utility? LMs have exhibited in-context learning capabilities, perform- ing new tasks with a few demonstrations as input (without parameter finetuning). This is a highly desirable property in any LM API as it avoids com- putational, annotation (and thus financial) costs. We show that the high fragmentation rate of a lan- guage can negatively affect the in-context learning performance in that language, resulting in reduced model utility. RQ4 (socio-economic aspects): what are the socio-economic implications of the API\u2019s cross- lingual cost and performance disparity? Our analysis shows evidence that not only are LMs more expensive for certain languages, they are also less effective for them. To highlight the implica- tions of these findings, we correlate those measure- ments with the socio-economic indicators of lan- guage speakers as a proxy for affordability of the APIs. This analysis indicates that users who likely cannot afford high API costs are charged more for poorer service, hindering uniform accessibility of this technology. 5https://help.openai.com/en/articles/674236 9-how-do-i-use-the-openai-api-in-different-lan guages 3 Experimental Setup 3.1 Models Throughout this work, we focus on two language models: ChatGPT (Ouyang et al., 2022; Brown et al., 2020) (gpt-3.5-turbo) and BLOOMZ (Muen- nighoff et al., 2022). Both of these models are trained and advertised as general-purpose models capable of following instructions and performing a wide range of tasks (Bang et al., 2023; Qin et al., 2023; Zhu et al., 2023b; Ahuja et al., 2023; Huang et al., 2023; Zhu et al., 2023a). ChatGPT (Ouyang et al., 2022) is a closed model only accessible through an API (with a premium tier) provided by OpenAI. Studies report that it supports as many as 90 languages (Ahuja et al., 2023). ChatGPT can handle a maximum sequence length of 4096 tokens (including both the prompt and generated tokens). BLOOMZ (Muennighoff et al., 2022) is an open- source multilingual model trained on 46 natural lan- guages and 13 programming languages. The best- performing version of this model has 175B parame- ters and is not feasible to be loaded on our academic servers; hence we rely on a free API of BLOOMZ hosted by Huggingface.6 Although BLOOMZ was trained with ALiBi positional embeddings (Press et al., 2022) which allows the model to extrapolate to any length sequences during inference, the Hug- gingface API has a context limit of 1000 tokens. 3.2 Tasks and Datasets To answer RQ1\u2014whether the same information is conveyed with similar numbers of"}, {"question": " How do non-uniform tokenization rates affect LM API cost disparity for speakers of different languages?", "answer": " Non-uniform tokenization rates can lead to significantly high usage costs for less represented languages.", "ref_chunk": "design choice that the model developers make. The total token count is also not immediately obvious to users ex- cept through a tokenizer interface4 separate from the chat interface. 2While most services also have free tiers, they limit daily usage to a small number of tokens per use. 3e.g. see OpenAI models\u2019 cost: https://openai.com/p ricing. 4https://platform.openai.com/tokenizer LMs Tokenization\u2014 Tokenization segmenting text an active research area. Proposed approaches range from defining tokens as whitespace-delimited words (for languages that use whitespace) which makes the vocabulary extremely large, to defining tokens as characters or even bytes, which makes the tokenized sequences extremely long in terms of number of tokens; see Mielke et al. (2021) for a detailed survey. A commonly-used solution now is to tokenize text into subword chunks (Sennrich et al., 2016; Kudo, 2018; Song et al., 2021). With Sennrich et al. (2016), one starts with a base vocab- ulary of only characters and adds new vocabulary items by recursively merging existing ones based on their frequency statistics in a training corpus. Other approaches judge subword candidates to be included into the vocabulary using a language model (Kudo, 2018; Song et al., 2021). For multilingual models containing data in a variety of scripts, even the base vocabulary of only characters (based on Unicode symbols) can be very large with over 130K types. Radford et al. (2019) instead proposed using a byte-level base vocabulary with only 256 tokens. Termed byte-level byte pair encoding (BBPE), this approach has become the de facto standard used in most modern language modeling efforts (Brown et al., 2020; Muennighoff et al., 2022; Scao et al., 2022; Black et al., 2022; Rae et al., 2022; Zhang et al., 2022b; Dey et al., 2023; Nagoudi et al., 2022; Zhang et al., 2022b). in into atomic units\u2014is In this work, we investigate the impact this tok- enization strategy has on LM API cost disparity as well as downstream task performance (i.e., utility) across different languages. 2.2 Investigating the Impact of Byte-level Subword Segmentation There are hundreds of distinct writing systems in the world. BBPE, by design, makes vocabulary construction script-agnostic, even allowing (in prin- ciple) new scripts to be supported later on without modifying the vocabulary. However, not only are different scripts encoded differently, their distri- bution in the training corpora varies widely. To investigate the effects of this variation, we propose the following research questions as the main focus of this work. RQ1 (number of tokens): do all languages con- vey the same information with the same num- ber of tokens? We analyze the fragmentation of sequences in different languages with different tokenizers. We find that among the supported lan- guages in popular LLMs, there is a large variance in the average number of tokens required to convey the same information with some languages requir- ing 5 times as many tokens than others. Previous work has shown that tokenization in multilingual models is usually biased towards high-resourced languages in the pretraining data (\u00c1cs, 2019; Rust et al., 2021); we observe that this is not always the case, but it could also be dependent on linguistic features or properties of language scripts. RQ2 (cost): do non-uniform tokenization rates lead to LM API cost disparity for speakers of dif- ferent languages? LM APIs like ChatGPT are available worldwide and have been widely claimed to have multilingual capabilities (Kasai et al., 2023; Lai et al., 2023).5 We show that disparate frag- mentation rates in different languages can lead to significantly high usage costs for less represented languages and argue for a more equitable API pric- ing system. RQ3 (model utility): do non-uniform tokeniza- tion rates affect the models\u2019 utility? LMs have exhibited in-context learning capabilities, perform- ing new tasks with a few demonstrations as input (without parameter finetuning). This is a highly desirable property in any LM API as it avoids com- putational, annotation (and thus financial) costs. We show that the high fragmentation rate of a lan- guage can negatively affect the in-context learning performance in that language, resulting in reduced model utility. RQ4 (socio-economic aspects): what are the socio-economic implications of the API\u2019s cross- lingual cost and performance disparity? Our analysis shows evidence that not only are LMs more expensive for certain languages, they are also less effective for them. To highlight the implica- tions of these findings, we correlate those measure- ments with the socio-economic indicators of lan- guage speakers as a proxy for affordability of the APIs. This analysis indicates that users who likely cannot afford high API costs are charged more for poorer service, hindering uniform accessibility of this technology. 5https://help.openai.com/en/articles/674236 9-how-do-i-use-the-openai-api-in-different-lan guages 3 Experimental Setup 3.1 Models Throughout this work, we focus on two language models: ChatGPT (Ouyang et al., 2022; Brown et al., 2020) (gpt-3.5-turbo) and BLOOMZ (Muen- nighoff et al., 2022). Both of these models are trained and advertised as general-purpose models capable of following instructions and performing a wide range of tasks (Bang et al., 2023; Qin et al., 2023; Zhu et al., 2023b; Ahuja et al., 2023; Huang et al., 2023; Zhu et al., 2023a). ChatGPT (Ouyang et al., 2022) is a closed model only accessible through an API (with a premium tier) provided by OpenAI. Studies report that it supports as many as 90 languages (Ahuja et al., 2023). ChatGPT can handle a maximum sequence length of 4096 tokens (including both the prompt and generated tokens). BLOOMZ (Muennighoff et al., 2022) is an open- source multilingual model trained on 46 natural lan- guages and 13 programming languages. The best- performing version of this model has 175B parame- ters and is not feasible to be loaded on our academic servers; hence we rely on a free API of BLOOMZ hosted by Huggingface.6 Although BLOOMZ was trained with ALiBi positional embeddings (Press et al., 2022) which allows the model to extrapolate to any length sequences during inference, the Hug- gingface API has a context limit of 1000 tokens. 3.2 Tasks and Datasets To answer RQ1\u2014whether the same information is conveyed with similar numbers of"}, {"question": " How can high fragmentation rates of a language affect the in-context learning performance of models?", "answer": " High fragmentation rates can negatively affect the in-context learning performance, resulting in reduced model utility.", "ref_chunk": "design choice that the model developers make. The total token count is also not immediately obvious to users ex- cept through a tokenizer interface4 separate from the chat interface. 2While most services also have free tiers, they limit daily usage to a small number of tokens per use. 3e.g. see OpenAI models\u2019 cost: https://openai.com/p ricing. 4https://platform.openai.com/tokenizer LMs Tokenization\u2014 Tokenization segmenting text an active research area. Proposed approaches range from defining tokens as whitespace-delimited words (for languages that use whitespace) which makes the vocabulary extremely large, to defining tokens as characters or even bytes, which makes the tokenized sequences extremely long in terms of number of tokens; see Mielke et al. (2021) for a detailed survey. A commonly-used solution now is to tokenize text into subword chunks (Sennrich et al., 2016; Kudo, 2018; Song et al., 2021). With Sennrich et al. (2016), one starts with a base vocab- ulary of only characters and adds new vocabulary items by recursively merging existing ones based on their frequency statistics in a training corpus. Other approaches judge subword candidates to be included into the vocabulary using a language model (Kudo, 2018; Song et al., 2021). For multilingual models containing data in a variety of scripts, even the base vocabulary of only characters (based on Unicode symbols) can be very large with over 130K types. Radford et al. (2019) instead proposed using a byte-level base vocabulary with only 256 tokens. Termed byte-level byte pair encoding (BBPE), this approach has become the de facto standard used in most modern language modeling efforts (Brown et al., 2020; Muennighoff et al., 2022; Scao et al., 2022; Black et al., 2022; Rae et al., 2022; Zhang et al., 2022b; Dey et al., 2023; Nagoudi et al., 2022; Zhang et al., 2022b). in into atomic units\u2014is In this work, we investigate the impact this tok- enization strategy has on LM API cost disparity as well as downstream task performance (i.e., utility) across different languages. 2.2 Investigating the Impact of Byte-level Subword Segmentation There are hundreds of distinct writing systems in the world. BBPE, by design, makes vocabulary construction script-agnostic, even allowing (in prin- ciple) new scripts to be supported later on without modifying the vocabulary. However, not only are different scripts encoded differently, their distri- bution in the training corpora varies widely. To investigate the effects of this variation, we propose the following research questions as the main focus of this work. RQ1 (number of tokens): do all languages con- vey the same information with the same num- ber of tokens? We analyze the fragmentation of sequences in different languages with different tokenizers. We find that among the supported lan- guages in popular LLMs, there is a large variance in the average number of tokens required to convey the same information with some languages requir- ing 5 times as many tokens than others. Previous work has shown that tokenization in multilingual models is usually biased towards high-resourced languages in the pretraining data (\u00c1cs, 2019; Rust et al., 2021); we observe that this is not always the case, but it could also be dependent on linguistic features or properties of language scripts. RQ2 (cost): do non-uniform tokenization rates lead to LM API cost disparity for speakers of dif- ferent languages? LM APIs like ChatGPT are available worldwide and have been widely claimed to have multilingual capabilities (Kasai et al., 2023; Lai et al., 2023).5 We show that disparate frag- mentation rates in different languages can lead to significantly high usage costs for less represented languages and argue for a more equitable API pric- ing system. RQ3 (model utility): do non-uniform tokeniza- tion rates affect the models\u2019 utility? LMs have exhibited in-context learning capabilities, perform- ing new tasks with a few demonstrations as input (without parameter finetuning). This is a highly desirable property in any LM API as it avoids com- putational, annotation (and thus financial) costs. We show that the high fragmentation rate of a lan- guage can negatively affect the in-context learning performance in that language, resulting in reduced model utility. RQ4 (socio-economic aspects): what are the socio-economic implications of the API\u2019s cross- lingual cost and performance disparity? Our analysis shows evidence that not only are LMs more expensive for certain languages, they are also less effective for them. To highlight the implica- tions of these findings, we correlate those measure- ments with the socio-economic indicators of lan- guage speakers as a proxy for affordability of the APIs. This analysis indicates that users who likely cannot afford high API costs are charged more for poorer service, hindering uniform accessibility of this technology. 5https://help.openai.com/en/articles/674236 9-how-do-i-use-the-openai-api-in-different-lan guages 3 Experimental Setup 3.1 Models Throughout this work, we focus on two language models: ChatGPT (Ouyang et al., 2022; Brown et al., 2020) (gpt-3.5-turbo) and BLOOMZ (Muen- nighoff et al., 2022). Both of these models are trained and advertised as general-purpose models capable of following instructions and performing a wide range of tasks (Bang et al., 2023; Qin et al., 2023; Zhu et al., 2023b; Ahuja et al., 2023; Huang et al., 2023; Zhu et al., 2023a). ChatGPT (Ouyang et al., 2022) is a closed model only accessible through an API (with a premium tier) provided by OpenAI. Studies report that it supports as many as 90 languages (Ahuja et al., 2023). ChatGPT can handle a maximum sequence length of 4096 tokens (including both the prompt and generated tokens). BLOOMZ (Muennighoff et al., 2022) is an open- source multilingual model trained on 46 natural lan- guages and 13 programming languages. The best- performing version of this model has 175B parame- ters and is not feasible to be loaded on our academic servers; hence we rely on a free API of BLOOMZ hosted by Huggingface.6 Although BLOOMZ was trained with ALiBi positional embeddings (Press et al., 2022) which allows the model to extrapolate to any length sequences during inference, the Hug- gingface API has a context limit of 1000 tokens. 3.2 Tasks and Datasets To answer RQ1\u2014whether the same information is conveyed with similar numbers of"}, {"question": " What socio-economic implications are associated with the cross-lingual cost and performance disparity of APIs?", "answer": " The implications revolve around certain languages being more expensive and less effective, hindering uniform accessibility of the technology.", "ref_chunk": "design choice that the model developers make. The total token count is also not immediately obvious to users ex- cept through a tokenizer interface4 separate from the chat interface. 2While most services also have free tiers, they limit daily usage to a small number of tokens per use. 3e.g. see OpenAI models\u2019 cost: https://openai.com/p ricing. 4https://platform.openai.com/tokenizer LMs Tokenization\u2014 Tokenization segmenting text an active research area. Proposed approaches range from defining tokens as whitespace-delimited words (for languages that use whitespace) which makes the vocabulary extremely large, to defining tokens as characters or even bytes, which makes the tokenized sequences extremely long in terms of number of tokens; see Mielke et al. (2021) for a detailed survey. A commonly-used solution now is to tokenize text into subword chunks (Sennrich et al., 2016; Kudo, 2018; Song et al., 2021). With Sennrich et al. (2016), one starts with a base vocab- ulary of only characters and adds new vocabulary items by recursively merging existing ones based on their frequency statistics in a training corpus. Other approaches judge subword candidates to be included into the vocabulary using a language model (Kudo, 2018; Song et al., 2021). For multilingual models containing data in a variety of scripts, even the base vocabulary of only characters (based on Unicode symbols) can be very large with over 130K types. Radford et al. (2019) instead proposed using a byte-level base vocabulary with only 256 tokens. Termed byte-level byte pair encoding (BBPE), this approach has become the de facto standard used in most modern language modeling efforts (Brown et al., 2020; Muennighoff et al., 2022; Scao et al., 2022; Black et al., 2022; Rae et al., 2022; Zhang et al., 2022b; Dey et al., 2023; Nagoudi et al., 2022; Zhang et al., 2022b). in into atomic units\u2014is In this work, we investigate the impact this tok- enization strategy has on LM API cost disparity as well as downstream task performance (i.e., utility) across different languages. 2.2 Investigating the Impact of Byte-level Subword Segmentation There are hundreds of distinct writing systems in the world. BBPE, by design, makes vocabulary construction script-agnostic, even allowing (in prin- ciple) new scripts to be supported later on without modifying the vocabulary. However, not only are different scripts encoded differently, their distri- bution in the training corpora varies widely. To investigate the effects of this variation, we propose the following research questions as the main focus of this work. RQ1 (number of tokens): do all languages con- vey the same information with the same num- ber of tokens? We analyze the fragmentation of sequences in different languages with different tokenizers. We find that among the supported lan- guages in popular LLMs, there is a large variance in the average number of tokens required to convey the same information with some languages requir- ing 5 times as many tokens than others. Previous work has shown that tokenization in multilingual models is usually biased towards high-resourced languages in the pretraining data (\u00c1cs, 2019; Rust et al., 2021); we observe that this is not always the case, but it could also be dependent on linguistic features or properties of language scripts. RQ2 (cost): do non-uniform tokenization rates lead to LM API cost disparity for speakers of dif- ferent languages? LM APIs like ChatGPT are available worldwide and have been widely claimed to have multilingual capabilities (Kasai et al., 2023; Lai et al., 2023).5 We show that disparate frag- mentation rates in different languages can lead to significantly high usage costs for less represented languages and argue for a more equitable API pric- ing system. RQ3 (model utility): do non-uniform tokeniza- tion rates affect the models\u2019 utility? LMs have exhibited in-context learning capabilities, perform- ing new tasks with a few demonstrations as input (without parameter finetuning). This is a highly desirable property in any LM API as it avoids com- putational, annotation (and thus financial) costs. We show that the high fragmentation rate of a lan- guage can negatively affect the in-context learning performance in that language, resulting in reduced model utility. RQ4 (socio-economic aspects): what are the socio-economic implications of the API\u2019s cross- lingual cost and performance disparity? Our analysis shows evidence that not only are LMs more expensive for certain languages, they are also less effective for them. To highlight the implica- tions of these findings, we correlate those measure- ments with the socio-economic indicators of lan- guage speakers as a proxy for affordability of the APIs. This analysis indicates that users who likely cannot afford high API costs are charged more for poorer service, hindering uniform accessibility of this technology. 5https://help.openai.com/en/articles/674236 9-how-do-i-use-the-openai-api-in-different-lan guages 3 Experimental Setup 3.1 Models Throughout this work, we focus on two language models: ChatGPT (Ouyang et al., 2022; Brown et al., 2020) (gpt-3.5-turbo) and BLOOMZ (Muen- nighoff et al., 2022). Both of these models are trained and advertised as general-purpose models capable of following instructions and performing a wide range of tasks (Bang et al., 2023; Qin et al., 2023; Zhu et al., 2023b; Ahuja et al., 2023; Huang et al., 2023; Zhu et al., 2023a). ChatGPT (Ouyang et al., 2022) is a closed model only accessible through an API (with a premium tier) provided by OpenAI. Studies report that it supports as many as 90 languages (Ahuja et al., 2023). ChatGPT can handle a maximum sequence length of 4096 tokens (including both the prompt and generated tokens). BLOOMZ (Muennighoff et al., 2022) is an open- source multilingual model trained on 46 natural lan- guages and 13 programming languages. The best- performing version of this model has 175B parame- ters and is not feasible to be loaded on our academic servers; hence we rely on a free API of BLOOMZ hosted by Huggingface.6 Although BLOOMZ was trained with ALiBi positional embeddings (Press et al., 2022) which allows the model to extrapolate to any length sequences during inference, the Hug- gingface API has a context limit of 1000 tokens. 3.2 Tasks and Datasets To answer RQ1\u2014whether the same information is conveyed with similar numbers of"}, {"question": " Which two language models are focused on in the text, and what are their key characteristics?", "answer": " The two models are ChatGPT and BLOOMZ, with ChatGPT being closed and BLOOMZ being open-source trained on multiple languages.", "ref_chunk": "design choice that the model developers make. The total token count is also not immediately obvious to users ex- cept through a tokenizer interface4 separate from the chat interface. 2While most services also have free tiers, they limit daily usage to a small number of tokens per use. 3e.g. see OpenAI models\u2019 cost: https://openai.com/p ricing. 4https://platform.openai.com/tokenizer LMs Tokenization\u2014 Tokenization segmenting text an active research area. Proposed approaches range from defining tokens as whitespace-delimited words (for languages that use whitespace) which makes the vocabulary extremely large, to defining tokens as characters or even bytes, which makes the tokenized sequences extremely long in terms of number of tokens; see Mielke et al. (2021) for a detailed survey. A commonly-used solution now is to tokenize text into subword chunks (Sennrich et al., 2016; Kudo, 2018; Song et al., 2021). With Sennrich et al. (2016), one starts with a base vocab- ulary of only characters and adds new vocabulary items by recursively merging existing ones based on their frequency statistics in a training corpus. Other approaches judge subword candidates to be included into the vocabulary using a language model (Kudo, 2018; Song et al., 2021). For multilingual models containing data in a variety of scripts, even the base vocabulary of only characters (based on Unicode symbols) can be very large with over 130K types. Radford et al. (2019) instead proposed using a byte-level base vocabulary with only 256 tokens. Termed byte-level byte pair encoding (BBPE), this approach has become the de facto standard used in most modern language modeling efforts (Brown et al., 2020; Muennighoff et al., 2022; Scao et al., 2022; Black et al., 2022; Rae et al., 2022; Zhang et al., 2022b; Dey et al., 2023; Nagoudi et al., 2022; Zhang et al., 2022b). in into atomic units\u2014is In this work, we investigate the impact this tok- enization strategy has on LM API cost disparity as well as downstream task performance (i.e., utility) across different languages. 2.2 Investigating the Impact of Byte-level Subword Segmentation There are hundreds of distinct writing systems in the world. BBPE, by design, makes vocabulary construction script-agnostic, even allowing (in prin- ciple) new scripts to be supported later on without modifying the vocabulary. However, not only are different scripts encoded differently, their distri- bution in the training corpora varies widely. To investigate the effects of this variation, we propose the following research questions as the main focus of this work. RQ1 (number of tokens): do all languages con- vey the same information with the same num- ber of tokens? We analyze the fragmentation of sequences in different languages with different tokenizers. We find that among the supported lan- guages in popular LLMs, there is a large variance in the average number of tokens required to convey the same information with some languages requir- ing 5 times as many tokens than others. Previous work has shown that tokenization in multilingual models is usually biased towards high-resourced languages in the pretraining data (\u00c1cs, 2019; Rust et al., 2021); we observe that this is not always the case, but it could also be dependent on linguistic features or properties of language scripts. RQ2 (cost): do non-uniform tokenization rates lead to LM API cost disparity for speakers of dif- ferent languages? LM APIs like ChatGPT are available worldwide and have been widely claimed to have multilingual capabilities (Kasai et al., 2023; Lai et al., 2023).5 We show that disparate frag- mentation rates in different languages can lead to significantly high usage costs for less represented languages and argue for a more equitable API pric- ing system. RQ3 (model utility): do non-uniform tokeniza- tion rates affect the models\u2019 utility? LMs have exhibited in-context learning capabilities, perform- ing new tasks with a few demonstrations as input (without parameter finetuning). This is a highly desirable property in any LM API as it avoids com- putational, annotation (and thus financial) costs. We show that the high fragmentation rate of a lan- guage can negatively affect the in-context learning performance in that language, resulting in reduced model utility. RQ4 (socio-economic aspects): what are the socio-economic implications of the API\u2019s cross- lingual cost and performance disparity? Our analysis shows evidence that not only are LMs more expensive for certain languages, they are also less effective for them. To highlight the implica- tions of these findings, we correlate those measure- ments with the socio-economic indicators of lan- guage speakers as a proxy for affordability of the APIs. This analysis indicates that users who likely cannot afford high API costs are charged more for poorer service, hindering uniform accessibility of this technology. 5https://help.openai.com/en/articles/674236 9-how-do-i-use-the-openai-api-in-different-lan guages 3 Experimental Setup 3.1 Models Throughout this work, we focus on two language models: ChatGPT (Ouyang et al., 2022; Brown et al., 2020) (gpt-3.5-turbo) and BLOOMZ (Muen- nighoff et al., 2022). Both of these models are trained and advertised as general-purpose models capable of following instructions and performing a wide range of tasks (Bang et al., 2023; Qin et al., 2023; Zhu et al., 2023b; Ahuja et al., 2023; Huang et al., 2023; Zhu et al., 2023a). ChatGPT (Ouyang et al., 2022) is a closed model only accessible through an API (with a premium tier) provided by OpenAI. Studies report that it supports as many as 90 languages (Ahuja et al., 2023). ChatGPT can handle a maximum sequence length of 4096 tokens (including both the prompt and generated tokens). BLOOMZ (Muennighoff et al., 2022) is an open- source multilingual model trained on 46 natural lan- guages and 13 programming languages. The best- performing version of this model has 175B parame- ters and is not feasible to be loaded on our academic servers; hence we rely on a free API of BLOOMZ hosted by Huggingface.6 Although BLOOMZ was trained with ALiBi positional embeddings (Press et al., 2022) which allows the model to extrapolate to any length sequences during inference, the Hug- gingface API has a context limit of 1000 tokens. 3.2 Tasks and Datasets To answer RQ1\u2014whether the same information is conveyed with similar numbers of"}, {"question": " What is the context limit provided by the Huggingface API for BLOOMZ?", "answer": " The Huggingface API has a context limit of 1000 tokens for BLOOMZ.", "ref_chunk": "design choice that the model developers make. The total token count is also not immediately obvious to users ex- cept through a tokenizer interface4 separate from the chat interface. 2While most services also have free tiers, they limit daily usage to a small number of tokens per use. 3e.g. see OpenAI models\u2019 cost: https://openai.com/p ricing. 4https://platform.openai.com/tokenizer LMs Tokenization\u2014 Tokenization segmenting text an active research area. Proposed approaches range from defining tokens as whitespace-delimited words (for languages that use whitespace) which makes the vocabulary extremely large, to defining tokens as characters or even bytes, which makes the tokenized sequences extremely long in terms of number of tokens; see Mielke et al. (2021) for a detailed survey. A commonly-used solution now is to tokenize text into subword chunks (Sennrich et al., 2016; Kudo, 2018; Song et al., 2021). With Sennrich et al. (2016), one starts with a base vocab- ulary of only characters and adds new vocabulary items by recursively merging existing ones based on their frequency statistics in a training corpus. Other approaches judge subword candidates to be included into the vocabulary using a language model (Kudo, 2018; Song et al., 2021). For multilingual models containing data in a variety of scripts, even the base vocabulary of only characters (based on Unicode symbols) can be very large with over 130K types. Radford et al. (2019) instead proposed using a byte-level base vocabulary with only 256 tokens. Termed byte-level byte pair encoding (BBPE), this approach has become the de facto standard used in most modern language modeling efforts (Brown et al., 2020; Muennighoff et al., 2022; Scao et al., 2022; Black et al., 2022; Rae et al., 2022; Zhang et al., 2022b; Dey et al., 2023; Nagoudi et al., 2022; Zhang et al., 2022b). in into atomic units\u2014is In this work, we investigate the impact this tok- enization strategy has on LM API cost disparity as well as downstream task performance (i.e., utility) across different languages. 2.2 Investigating the Impact of Byte-level Subword Segmentation There are hundreds of distinct writing systems in the world. BBPE, by design, makes vocabulary construction script-agnostic, even allowing (in prin- ciple) new scripts to be supported later on without modifying the vocabulary. However, not only are different scripts encoded differently, their distri- bution in the training corpora varies widely. To investigate the effects of this variation, we propose the following research questions as the main focus of this work. RQ1 (number of tokens): do all languages con- vey the same information with the same num- ber of tokens? We analyze the fragmentation of sequences in different languages with different tokenizers. We find that among the supported lan- guages in popular LLMs, there is a large variance in the average number of tokens required to convey the same information with some languages requir- ing 5 times as many tokens than others. Previous work has shown that tokenization in multilingual models is usually biased towards high-resourced languages in the pretraining data (\u00c1cs, 2019; Rust et al., 2021); we observe that this is not always the case, but it could also be dependent on linguistic features or properties of language scripts. RQ2 (cost): do non-uniform tokenization rates lead to LM API cost disparity for speakers of dif- ferent languages? LM APIs like ChatGPT are available worldwide and have been widely claimed to have multilingual capabilities (Kasai et al., 2023; Lai et al., 2023).5 We show that disparate frag- mentation rates in different languages can lead to significantly high usage costs for less represented languages and argue for a more equitable API pric- ing system. RQ3 (model utility): do non-uniform tokeniza- tion rates affect the models\u2019 utility? LMs have exhibited in-context learning capabilities, perform- ing new tasks with a few demonstrations as input (without parameter finetuning). This is a highly desirable property in any LM API as it avoids com- putational, annotation (and thus financial) costs. We show that the high fragmentation rate of a lan- guage can negatively affect the in-context learning performance in that language, resulting in reduced model utility. RQ4 (socio-economic aspects): what are the socio-economic implications of the API\u2019s cross- lingual cost and performance disparity? Our analysis shows evidence that not only are LMs more expensive for certain languages, they are also less effective for them. To highlight the implica- tions of these findings, we correlate those measure- ments with the socio-economic indicators of lan- guage speakers as a proxy for affordability of the APIs. This analysis indicates that users who likely cannot afford high API costs are charged more for poorer service, hindering uniform accessibility of this technology. 5https://help.openai.com/en/articles/674236 9-how-do-i-use-the-openai-api-in-different-lan guages 3 Experimental Setup 3.1 Models Throughout this work, we focus on two language models: ChatGPT (Ouyang et al., 2022; Brown et al., 2020) (gpt-3.5-turbo) and BLOOMZ (Muen- nighoff et al., 2022). Both of these models are trained and advertised as general-purpose models capable of following instructions and performing a wide range of tasks (Bang et al., 2023; Qin et al., 2023; Zhu et al., 2023b; Ahuja et al., 2023; Huang et al., 2023; Zhu et al., 2023a). ChatGPT (Ouyang et al., 2022) is a closed model only accessible through an API (with a premium tier) provided by OpenAI. Studies report that it supports as many as 90 languages (Ahuja et al., 2023). ChatGPT can handle a maximum sequence length of 4096 tokens (including both the prompt and generated tokens). BLOOMZ (Muennighoff et al., 2022) is an open- source multilingual model trained on 46 natural lan- guages and 13 programming languages. The best- performing version of this model has 175B parame- ters and is not feasible to be loaded on our academic servers; hence we rely on a free API of BLOOMZ hosted by Huggingface.6 Although BLOOMZ was trained with ALiBi positional embeddings (Press et al., 2022) which allows the model to extrapolate to any length sequences during inference, the Hug- gingface API has a context limit of 1000 tokens. 3.2 Tasks and Datasets To answer RQ1\u2014whether the same information is conveyed with similar numbers of"}], "doc_text": "design choice that the model developers make. The total token count is also not immediately obvious to users ex- cept through a tokenizer interface4 separate from the chat interface. 2While most services also have free tiers, they limit daily usage to a small number of tokens per use. 3e.g. see OpenAI models\u2019 cost: https://openai.com/p ricing. 4https://platform.openai.com/tokenizer LMs Tokenization\u2014 Tokenization segmenting text an active research area. Proposed approaches range from defining tokens as whitespace-delimited words (for languages that use whitespace) which makes the vocabulary extremely large, to defining tokens as characters or even bytes, which makes the tokenized sequences extremely long in terms of number of tokens; see Mielke et al. (2021) for a detailed survey. A commonly-used solution now is to tokenize text into subword chunks (Sennrich et al., 2016; Kudo, 2018; Song et al., 2021). With Sennrich et al. (2016), one starts with a base vocab- ulary of only characters and adds new vocabulary items by recursively merging existing ones based on their frequency statistics in a training corpus. Other approaches judge subword candidates to be included into the vocabulary using a language model (Kudo, 2018; Song et al., 2021). For multilingual models containing data in a variety of scripts, even the base vocabulary of only characters (based on Unicode symbols) can be very large with over 130K types. Radford et al. (2019) instead proposed using a byte-level base vocabulary with only 256 tokens. Termed byte-level byte pair encoding (BBPE), this approach has become the de facto standard used in most modern language modeling efforts (Brown et al., 2020; Muennighoff et al., 2022; Scao et al., 2022; Black et al., 2022; Rae et al., 2022; Zhang et al., 2022b; Dey et al., 2023; Nagoudi et al., 2022; Zhang et al., 2022b). in into atomic units\u2014is In this work, we investigate the impact this tok- enization strategy has on LM API cost disparity as well as downstream task performance (i.e., utility) across different languages. 2.2 Investigating the Impact of Byte-level Subword Segmentation There are hundreds of distinct writing systems in the world. BBPE, by design, makes vocabulary construction script-agnostic, even allowing (in prin- ciple) new scripts to be supported later on without modifying the vocabulary. However, not only are different scripts encoded differently, their distri- bution in the training corpora varies widely. To investigate the effects of this variation, we propose the following research questions as the main focus of this work. RQ1 (number of tokens): do all languages con- vey the same information with the same num- ber of tokens? We analyze the fragmentation of sequences in different languages with different tokenizers. We find that among the supported lan- guages in popular LLMs, there is a large variance in the average number of tokens required to convey the same information with some languages requir- ing 5 times as many tokens than others. Previous work has shown that tokenization in multilingual models is usually biased towards high-resourced languages in the pretraining data (\u00c1cs, 2019; Rust et al., 2021); we observe that this is not always the case, but it could also be dependent on linguistic features or properties of language scripts. RQ2 (cost): do non-uniform tokenization rates lead to LM API cost disparity for speakers of dif- ferent languages? LM APIs like ChatGPT are available worldwide and have been widely claimed to have multilingual capabilities (Kasai et al., 2023; Lai et al., 2023).5 We show that disparate frag- mentation rates in different languages can lead to significantly high usage costs for less represented languages and argue for a more equitable API pric- ing system. RQ3 (model utility): do non-uniform tokeniza- tion rates affect the models\u2019 utility? LMs have exhibited in-context learning capabilities, perform- ing new tasks with a few demonstrations as input (without parameter finetuning). This is a highly desirable property in any LM API as it avoids com- putational, annotation (and thus financial) costs. We show that the high fragmentation rate of a lan- guage can negatively affect the in-context learning performance in that language, resulting in reduced model utility. RQ4 (socio-economic aspects): what are the socio-economic implications of the API\u2019s cross- lingual cost and performance disparity? Our analysis shows evidence that not only are LMs more expensive for certain languages, they are also less effective for them. To highlight the implica- tions of these findings, we correlate those measure- ments with the socio-economic indicators of lan- guage speakers as a proxy for affordability of the APIs. This analysis indicates that users who likely cannot afford high API costs are charged more for poorer service, hindering uniform accessibility of this technology. 5https://help.openai.com/en/articles/674236 9-how-do-i-use-the-openai-api-in-different-lan guages 3 Experimental Setup 3.1 Models Throughout this work, we focus on two language models: ChatGPT (Ouyang et al., 2022; Brown et al., 2020) (gpt-3.5-turbo) and BLOOMZ (Muen- nighoff et al., 2022). Both of these models are trained and advertised as general-purpose models capable of following instructions and performing a wide range of tasks (Bang et al., 2023; Qin et al., 2023; Zhu et al., 2023b; Ahuja et al., 2023; Huang et al., 2023; Zhu et al., 2023a). ChatGPT (Ouyang et al., 2022) is a closed model only accessible through an API (with a premium tier) provided by OpenAI. Studies report that it supports as many as 90 languages (Ahuja et al., 2023). ChatGPT can handle a maximum sequence length of 4096 tokens (including both the prompt and generated tokens). BLOOMZ (Muennighoff et al., 2022) is an open- source multilingual model trained on 46 natural lan- guages and 13 programming languages. The best- performing version of this model has 175B parame- ters and is not feasible to be loaded on our academic servers; hence we rely on a free API of BLOOMZ hosted by Huggingface.6 Although BLOOMZ was trained with ALiBi positional embeddings (Press et al., 2022) which allows the model to extrapolate to any length sequences during inference, the Hug- gingface API has a context limit of 1000 tokens. 3.2 Tasks and Datasets To answer RQ1\u2014whether the same information is conveyed with similar numbers of"}