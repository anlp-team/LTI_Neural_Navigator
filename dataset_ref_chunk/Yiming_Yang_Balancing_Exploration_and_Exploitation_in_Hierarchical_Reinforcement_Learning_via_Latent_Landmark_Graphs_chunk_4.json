{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yiming_Yang_Balancing_Exploration_and_Exploitation_in_Hierarchical_Reinforcement_Learning_via_Latent_Landmark_Graphs_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the strategy in the pseudo-code of HILL aim to achieve?", "answer": " The strategy aims to select a landmark with a smaller historical visit count and a larger transitional benefit as the subgoal for the next time interval.", "ref_chunk": "the number of landmarks in Lt. Then we define a balanced strategy based on N and P and select a landmark with a smaller historical visit count and a larger transitional benefit as the subgoal gt of the next time interval: gt = arg min [(1 \u2212 P(U (wi\u2032,j))) \u00d7 N (lj)], lj \u2208Lt where li\u2032 = \u03c6(st) and st is the state of the current time step. Algorithm 1 shows the pseudo-code of HILL. Note that we use the subgoal selection strategy in Equation 9 as a high- level teacher policy and use \u03c0\u03b8h modeled by a neural network as a high-level student policy. Both policies map states to subgoals and are employed with a probability of p and 1 \u2212 p, respectively. The role of the student policy is mainly to help increase randomness and prevent falling into local optimum. p is initialized to 0.5 and gradually increases to 1 in the training process. \u03b8h is trained with SAC using episodes in Bl. V. EXPERIMENTS A. Environmental Settings We evaluate HILL on a suite of challenging continuous control tasks based on the MuJoCo simulator [13]. These tasks require the agent to master a combination of locomotion and object manipulation skills. The reward is sparse: the agent gets (8) (9) (a) Ant Maze (b) Ant FourRooms (c) Ant Push (d) HalfCheetah Hurdle (e) HalfCheetah Climbing (f) HalfCheetah Ascending Fig. 2. The continuous control environments with sparse rewards for evaluating HILL and baselines. 3 1.0Success 5Steps \u00d71e6 4 0.2 0.6 0.8 0.0 1 0 2 0.4 0.8 2 0.2 0.0 12 0 8 0.4 0.6 14Steps \u00d71e6 4 10 6 1.0Success 0.6 0.8 3 0.4 0 1 4 0.0 1.0Success 5Steps \u00d71e6 0.2 2 (a) Ant Maze (b) Ant FourRooms (c) Ant Push 0.8 1.0 0.5 1.0Success 2.0 0.6 2.5Steps \u00d71e6 0.0 0.2 0.0 1.5 0.4 1.5 0.8 0.2 0.0 1.0Success 2.5 0.4 0.0 0.6 0.5 2.0 3.0Steps \u00d71e6 1.0 2.00Steps \u00d71e6 0.00 1.50 0.2 1.0Success 0.6 0.8 0.75 0.0 0.25 0.50 1.00 1.75 1.25 0.4 (e) HalfCheetah Climbing Fig. 3. Learning curves of HILL and baselines on MuJoCo tasks. The x-axis shows the time step of training, and the y-axis shows the average test success rate of 10 episodes. Each line is the mean of 5 runs with shaded regions corresponding to the 95% confidence interval. (d) HalfCheetah Hurdle (f) HalfCheetah Ascending 0 when reaching the goal and \u22121 otherwise. During training, the agent is initialized with random positions and is tested under the most challenging setting to reach the other side of the environment. We conduct experiments on six environments in MuJoCo, and their visualizations are presented in Figure 2. The maximum episode length is limited to 500 steps for the Ant Maze, Ant Push, HalfCheetah Hurdle, and HalfCheetah Ascending tasks and 1000 steps for the Ant FourRooms and HalfCheetah Climbing tasks. B. Comparative Experiments We compare HILL with the SOTA baselines, which include: (1) HESS [10]: a GCHRL method realizing active exploration based on stable subgoal representation learning. (2) LESSON [9]: a GCHRL method learning subgoal representations with slow dynamics. (3) HIRO [5]: a GCHRL method using off- policy correction for efficient exploitation. (4) SAC [29]: the base RL method we used for training the bi-level policies. Note that we use the original implementations of LESSON and HESS on all tasks. The experimental results, as presented in Figure 3, demon- strate that HILL outperforms all baselines in terms of both sample efficiency and asymptotic performance. This success can be attributed to the use of efficient subgoal representations with temporal coherence and a subgoal selection strategy that effectively balances exploration and exploitation. In compari- son, HESS underperforms our method due to its focus on ex- ploring novel and reachable subgoals, lacking consideration of the potential benefits that subgoals can provide in completing the source task. LESSON yields an uptrend similar to HILL in Ant Push in the early stage. However, its unstable subgoal representations lead to a subsequent drop in performance. HIRO improves sample efficiency by relabeling subgoals to maximize the likelihood of past low-level action sequences. However, it lacks active exploration and fails to achieve a balance between exploration and exploitation. SAC performs poorly across all tasks, showing hierarchical advantages in solving long-term tasks with sparse rewards. Fig. 4. Episodes in the state space and representations in the latent space at different learning stages in the Ant Maze task. 3.5 0.2 1.0Success 4.0Steps \u00d71e6 0.8 3.0 1.5 0.0 0.0 0.5 1.0 2.5 0.6 2.0 0.4 0.2 0.8 5Steps \u00d71e6 4 1.0Success 2 0.6 0.0 0.4 1 3 0 (a) Ant Maze (b) Ant Push (a) State DIST. (0.5M steps) (b) State DIST. (1.5M steps) Fig. 6. Ablation studies of critical modules. \u201cB\u201d denotes \u201cBasic GCHRL\u201d, \u201cCSRL\u201d denotes \u201cContrastive Subgoal Representation Learning\u201d, \u201cG-N\u201d de- notes \u201cbuilding Graphs with the Novelty measure\u201d, and \u201cG-N-U\u201d denotes \u201cbuilding Graphs with the Novelty measure and the Utility measure\u201d. stabilize and exhibit little difference. The active exploration in the early stage and adequate exploitation in the later stage demonstrate the effectiveness of our proposed balanced strategy. (c) REP. DIST. (0.5M steps) (d) REP. DIST. (1.5M steps) Fig. 5. Visualization of exploration and exploitation in the Ant Maze task at 0.5 and 1.5 million steps, respectively. The x, y-axes are positions in the corresponding space, and the z-axis is the visit counts statistically obtained from the replay buffer with a capacity of 105. \u201cREP.\u201d denotes the word \u201crepresentation\u201d and \u201cDIST.\u201d denotes the word \u201cdistribution\u201d. C. Visualization Analysis 1) Exploration and Exploitation: We visualize the explo- ration and exploitation statuses by counting the visits of states in the state space and representations in the latent space, respectively, as shown in Figure 5. With the guidance of the novelty measure, HILL actively selects subgoals that are less explored, resulting in an extensive range of representations in the latent space and a dispersed distribution in the state space. As training progresses, the cumulative visits to various latent representations become uniform, and the utility measure enables more precise"}, {"question": " How is the high-level teacher policy and high-level student policy in HILL utilized?", "answer": " The high-level teacher policy and high-level student policy in HILL map states to subgoals and are employed with probabilities of p and 1-p, respectively.", "ref_chunk": "the number of landmarks in Lt. Then we define a balanced strategy based on N and P and select a landmark with a smaller historical visit count and a larger transitional benefit as the subgoal gt of the next time interval: gt = arg min [(1 \u2212 P(U (wi\u2032,j))) \u00d7 N (lj)], lj \u2208Lt where li\u2032 = \u03c6(st) and st is the state of the current time step. Algorithm 1 shows the pseudo-code of HILL. Note that we use the subgoal selection strategy in Equation 9 as a high- level teacher policy and use \u03c0\u03b8h modeled by a neural network as a high-level student policy. Both policies map states to subgoals and are employed with a probability of p and 1 \u2212 p, respectively. The role of the student policy is mainly to help increase randomness and prevent falling into local optimum. p is initialized to 0.5 and gradually increases to 1 in the training process. \u03b8h is trained with SAC using episodes in Bl. V. EXPERIMENTS A. Environmental Settings We evaluate HILL on a suite of challenging continuous control tasks based on the MuJoCo simulator [13]. These tasks require the agent to master a combination of locomotion and object manipulation skills. The reward is sparse: the agent gets (8) (9) (a) Ant Maze (b) Ant FourRooms (c) Ant Push (d) HalfCheetah Hurdle (e) HalfCheetah Climbing (f) HalfCheetah Ascending Fig. 2. The continuous control environments with sparse rewards for evaluating HILL and baselines. 3 1.0Success 5Steps \u00d71e6 4 0.2 0.6 0.8 0.0 1 0 2 0.4 0.8 2 0.2 0.0 12 0 8 0.4 0.6 14Steps \u00d71e6 4 10 6 1.0Success 0.6 0.8 3 0.4 0 1 4 0.0 1.0Success 5Steps \u00d71e6 0.2 2 (a) Ant Maze (b) Ant FourRooms (c) Ant Push 0.8 1.0 0.5 1.0Success 2.0 0.6 2.5Steps \u00d71e6 0.0 0.2 0.0 1.5 0.4 1.5 0.8 0.2 0.0 1.0Success 2.5 0.4 0.0 0.6 0.5 2.0 3.0Steps \u00d71e6 1.0 2.00Steps \u00d71e6 0.00 1.50 0.2 1.0Success 0.6 0.8 0.75 0.0 0.25 0.50 1.00 1.75 1.25 0.4 (e) HalfCheetah Climbing Fig. 3. Learning curves of HILL and baselines on MuJoCo tasks. The x-axis shows the time step of training, and the y-axis shows the average test success rate of 10 episodes. Each line is the mean of 5 runs with shaded regions corresponding to the 95% confidence interval. (d) HalfCheetah Hurdle (f) HalfCheetah Ascending 0 when reaching the goal and \u22121 otherwise. During training, the agent is initialized with random positions and is tested under the most challenging setting to reach the other side of the environment. We conduct experiments on six environments in MuJoCo, and their visualizations are presented in Figure 2. The maximum episode length is limited to 500 steps for the Ant Maze, Ant Push, HalfCheetah Hurdle, and HalfCheetah Ascending tasks and 1000 steps for the Ant FourRooms and HalfCheetah Climbing tasks. B. Comparative Experiments We compare HILL with the SOTA baselines, which include: (1) HESS [10]: a GCHRL method realizing active exploration based on stable subgoal representation learning. (2) LESSON [9]: a GCHRL method learning subgoal representations with slow dynamics. (3) HIRO [5]: a GCHRL method using off- policy correction for efficient exploitation. (4) SAC [29]: the base RL method we used for training the bi-level policies. Note that we use the original implementations of LESSON and HESS on all tasks. The experimental results, as presented in Figure 3, demon- strate that HILL outperforms all baselines in terms of both sample efficiency and asymptotic performance. This success can be attributed to the use of efficient subgoal representations with temporal coherence and a subgoal selection strategy that effectively balances exploration and exploitation. In compari- son, HESS underperforms our method due to its focus on ex- ploring novel and reachable subgoals, lacking consideration of the potential benefits that subgoals can provide in completing the source task. LESSON yields an uptrend similar to HILL in Ant Push in the early stage. However, its unstable subgoal representations lead to a subsequent drop in performance. HIRO improves sample efficiency by relabeling subgoals to maximize the likelihood of past low-level action sequences. However, it lacks active exploration and fails to achieve a balance between exploration and exploitation. SAC performs poorly across all tasks, showing hierarchical advantages in solving long-term tasks with sparse rewards. Fig. 4. Episodes in the state space and representations in the latent space at different learning stages in the Ant Maze task. 3.5 0.2 1.0Success 4.0Steps \u00d71e6 0.8 3.0 1.5 0.0 0.0 0.5 1.0 2.5 0.6 2.0 0.4 0.2 0.8 5Steps \u00d71e6 4 1.0Success 2 0.6 0.0 0.4 1 3 0 (a) Ant Maze (b) Ant Push (a) State DIST. (0.5M steps) (b) State DIST. (1.5M steps) Fig. 6. Ablation studies of critical modules. \u201cB\u201d denotes \u201cBasic GCHRL\u201d, \u201cCSRL\u201d denotes \u201cContrastive Subgoal Representation Learning\u201d, \u201cG-N\u201d de- notes \u201cbuilding Graphs with the Novelty measure\u201d, and \u201cG-N-U\u201d denotes \u201cbuilding Graphs with the Novelty measure and the Utility measure\u201d. stabilize and exhibit little difference. The active exploration in the early stage and adequate exploitation in the later stage demonstrate the effectiveness of our proposed balanced strategy. (c) REP. DIST. (0.5M steps) (d) REP. DIST. (1.5M steps) Fig. 5. Visualization of exploration and exploitation in the Ant Maze task at 0.5 and 1.5 million steps, respectively. The x, y-axes are positions in the corresponding space, and the z-axis is the visit counts statistically obtained from the replay buffer with a capacity of 105. \u201cREP.\u201d denotes the word \u201crepresentation\u201d and \u201cDIST.\u201d denotes the word \u201cdistribution\u201d. C. Visualization Analysis 1) Exploration and Exploitation: We visualize the explo- ration and exploitation statuses by counting the visits of states in the state space and representations in the latent space, respectively, as shown in Figure 5. With the guidance of the novelty measure, HILL actively selects subgoals that are less explored, resulting in an extensive range of representations in the latent space and a dispersed distribution in the state space. As training progresses, the cumulative visits to various latent representations become uniform, and the utility measure enables more precise"}, {"question": " How is the role of the student policy described in HILL?", "answer": " The role of the student policy is mainly to help increase randomness and prevent falling into local optima.", "ref_chunk": "the number of landmarks in Lt. Then we define a balanced strategy based on N and P and select a landmark with a smaller historical visit count and a larger transitional benefit as the subgoal gt of the next time interval: gt = arg min [(1 \u2212 P(U (wi\u2032,j))) \u00d7 N (lj)], lj \u2208Lt where li\u2032 = \u03c6(st) and st is the state of the current time step. Algorithm 1 shows the pseudo-code of HILL. Note that we use the subgoal selection strategy in Equation 9 as a high- level teacher policy and use \u03c0\u03b8h modeled by a neural network as a high-level student policy. Both policies map states to subgoals and are employed with a probability of p and 1 \u2212 p, respectively. The role of the student policy is mainly to help increase randomness and prevent falling into local optimum. p is initialized to 0.5 and gradually increases to 1 in the training process. \u03b8h is trained with SAC using episodes in Bl. V. EXPERIMENTS A. Environmental Settings We evaluate HILL on a suite of challenging continuous control tasks based on the MuJoCo simulator [13]. These tasks require the agent to master a combination of locomotion and object manipulation skills. The reward is sparse: the agent gets (8) (9) (a) Ant Maze (b) Ant FourRooms (c) Ant Push (d) HalfCheetah Hurdle (e) HalfCheetah Climbing (f) HalfCheetah Ascending Fig. 2. The continuous control environments with sparse rewards for evaluating HILL and baselines. 3 1.0Success 5Steps \u00d71e6 4 0.2 0.6 0.8 0.0 1 0 2 0.4 0.8 2 0.2 0.0 12 0 8 0.4 0.6 14Steps \u00d71e6 4 10 6 1.0Success 0.6 0.8 3 0.4 0 1 4 0.0 1.0Success 5Steps \u00d71e6 0.2 2 (a) Ant Maze (b) Ant FourRooms (c) Ant Push 0.8 1.0 0.5 1.0Success 2.0 0.6 2.5Steps \u00d71e6 0.0 0.2 0.0 1.5 0.4 1.5 0.8 0.2 0.0 1.0Success 2.5 0.4 0.0 0.6 0.5 2.0 3.0Steps \u00d71e6 1.0 2.00Steps \u00d71e6 0.00 1.50 0.2 1.0Success 0.6 0.8 0.75 0.0 0.25 0.50 1.00 1.75 1.25 0.4 (e) HalfCheetah Climbing Fig. 3. Learning curves of HILL and baselines on MuJoCo tasks. The x-axis shows the time step of training, and the y-axis shows the average test success rate of 10 episodes. Each line is the mean of 5 runs with shaded regions corresponding to the 95% confidence interval. (d) HalfCheetah Hurdle (f) HalfCheetah Ascending 0 when reaching the goal and \u22121 otherwise. During training, the agent is initialized with random positions and is tested under the most challenging setting to reach the other side of the environment. We conduct experiments on six environments in MuJoCo, and their visualizations are presented in Figure 2. The maximum episode length is limited to 500 steps for the Ant Maze, Ant Push, HalfCheetah Hurdle, and HalfCheetah Ascending tasks and 1000 steps for the Ant FourRooms and HalfCheetah Climbing tasks. B. Comparative Experiments We compare HILL with the SOTA baselines, which include: (1) HESS [10]: a GCHRL method realizing active exploration based on stable subgoal representation learning. (2) LESSON [9]: a GCHRL method learning subgoal representations with slow dynamics. (3) HIRO [5]: a GCHRL method using off- policy correction for efficient exploitation. (4) SAC [29]: the base RL method we used for training the bi-level policies. Note that we use the original implementations of LESSON and HESS on all tasks. The experimental results, as presented in Figure 3, demon- strate that HILL outperforms all baselines in terms of both sample efficiency and asymptotic performance. This success can be attributed to the use of efficient subgoal representations with temporal coherence and a subgoal selection strategy that effectively balances exploration and exploitation. In compari- son, HESS underperforms our method due to its focus on ex- ploring novel and reachable subgoals, lacking consideration of the potential benefits that subgoals can provide in completing the source task. LESSON yields an uptrend similar to HILL in Ant Push in the early stage. However, its unstable subgoal representations lead to a subsequent drop in performance. HIRO improves sample efficiency by relabeling subgoals to maximize the likelihood of past low-level action sequences. However, it lacks active exploration and fails to achieve a balance between exploration and exploitation. SAC performs poorly across all tasks, showing hierarchical advantages in solving long-term tasks with sparse rewards. Fig. 4. Episodes in the state space and representations in the latent space at different learning stages in the Ant Maze task. 3.5 0.2 1.0Success 4.0Steps \u00d71e6 0.8 3.0 1.5 0.0 0.0 0.5 1.0 2.5 0.6 2.0 0.4 0.2 0.8 5Steps \u00d71e6 4 1.0Success 2 0.6 0.0 0.4 1 3 0 (a) Ant Maze (b) Ant Push (a) State DIST. (0.5M steps) (b) State DIST. (1.5M steps) Fig. 6. Ablation studies of critical modules. \u201cB\u201d denotes \u201cBasic GCHRL\u201d, \u201cCSRL\u201d denotes \u201cContrastive Subgoal Representation Learning\u201d, \u201cG-N\u201d de- notes \u201cbuilding Graphs with the Novelty measure\u201d, and \u201cG-N-U\u201d denotes \u201cbuilding Graphs with the Novelty measure and the Utility measure\u201d. stabilize and exhibit little difference. The active exploration in the early stage and adequate exploitation in the later stage demonstrate the effectiveness of our proposed balanced strategy. (c) REP. DIST. (0.5M steps) (d) REP. DIST. (1.5M steps) Fig. 5. Visualization of exploration and exploitation in the Ant Maze task at 0.5 and 1.5 million steps, respectively. The x, y-axes are positions in the corresponding space, and the z-axis is the visit counts statistically obtained from the replay buffer with a capacity of 105. \u201cREP.\u201d denotes the word \u201crepresentation\u201d and \u201cDIST.\u201d denotes the word \u201cdistribution\u201d. C. Visualization Analysis 1) Exploration and Exploitation: We visualize the explo- ration and exploitation statuses by counting the visits of states in the state space and representations in the latent space, respectively, as shown in Figure 5. With the guidance of the novelty measure, HILL actively selects subgoals that are less explored, resulting in an extensive range of representations in the latent space and a dispersed distribution in the state space. As training progresses, the cumulative visits to various latent representations become uniform, and the utility measure enables more precise"}, {"question": " What is the initial value of p in HILL?", "answer": " p is initialized to 0.5 in HILL.", "ref_chunk": "the number of landmarks in Lt. Then we define a balanced strategy based on N and P and select a landmark with a smaller historical visit count and a larger transitional benefit as the subgoal gt of the next time interval: gt = arg min [(1 \u2212 P(U (wi\u2032,j))) \u00d7 N (lj)], lj \u2208Lt where li\u2032 = \u03c6(st) and st is the state of the current time step. Algorithm 1 shows the pseudo-code of HILL. Note that we use the subgoal selection strategy in Equation 9 as a high- level teacher policy and use \u03c0\u03b8h modeled by a neural network as a high-level student policy. Both policies map states to subgoals and are employed with a probability of p and 1 \u2212 p, respectively. The role of the student policy is mainly to help increase randomness and prevent falling into local optimum. p is initialized to 0.5 and gradually increases to 1 in the training process. \u03b8h is trained with SAC using episodes in Bl. V. EXPERIMENTS A. Environmental Settings We evaluate HILL on a suite of challenging continuous control tasks based on the MuJoCo simulator [13]. These tasks require the agent to master a combination of locomotion and object manipulation skills. The reward is sparse: the agent gets (8) (9) (a) Ant Maze (b) Ant FourRooms (c) Ant Push (d) HalfCheetah Hurdle (e) HalfCheetah Climbing (f) HalfCheetah Ascending Fig. 2. The continuous control environments with sparse rewards for evaluating HILL and baselines. 3 1.0Success 5Steps \u00d71e6 4 0.2 0.6 0.8 0.0 1 0 2 0.4 0.8 2 0.2 0.0 12 0 8 0.4 0.6 14Steps \u00d71e6 4 10 6 1.0Success 0.6 0.8 3 0.4 0 1 4 0.0 1.0Success 5Steps \u00d71e6 0.2 2 (a) Ant Maze (b) Ant FourRooms (c) Ant Push 0.8 1.0 0.5 1.0Success 2.0 0.6 2.5Steps \u00d71e6 0.0 0.2 0.0 1.5 0.4 1.5 0.8 0.2 0.0 1.0Success 2.5 0.4 0.0 0.6 0.5 2.0 3.0Steps \u00d71e6 1.0 2.00Steps \u00d71e6 0.00 1.50 0.2 1.0Success 0.6 0.8 0.75 0.0 0.25 0.50 1.00 1.75 1.25 0.4 (e) HalfCheetah Climbing Fig. 3. Learning curves of HILL and baselines on MuJoCo tasks. The x-axis shows the time step of training, and the y-axis shows the average test success rate of 10 episodes. Each line is the mean of 5 runs with shaded regions corresponding to the 95% confidence interval. (d) HalfCheetah Hurdle (f) HalfCheetah Ascending 0 when reaching the goal and \u22121 otherwise. During training, the agent is initialized with random positions and is tested under the most challenging setting to reach the other side of the environment. We conduct experiments on six environments in MuJoCo, and their visualizations are presented in Figure 2. The maximum episode length is limited to 500 steps for the Ant Maze, Ant Push, HalfCheetah Hurdle, and HalfCheetah Ascending tasks and 1000 steps for the Ant FourRooms and HalfCheetah Climbing tasks. B. Comparative Experiments We compare HILL with the SOTA baselines, which include: (1) HESS [10]: a GCHRL method realizing active exploration based on stable subgoal representation learning. (2) LESSON [9]: a GCHRL method learning subgoal representations with slow dynamics. (3) HIRO [5]: a GCHRL method using off- policy correction for efficient exploitation. (4) SAC [29]: the base RL method we used for training the bi-level policies. Note that we use the original implementations of LESSON and HESS on all tasks. The experimental results, as presented in Figure 3, demon- strate that HILL outperforms all baselines in terms of both sample efficiency and asymptotic performance. This success can be attributed to the use of efficient subgoal representations with temporal coherence and a subgoal selection strategy that effectively balances exploration and exploitation. In compari- son, HESS underperforms our method due to its focus on ex- ploring novel and reachable subgoals, lacking consideration of the potential benefits that subgoals can provide in completing the source task. LESSON yields an uptrend similar to HILL in Ant Push in the early stage. However, its unstable subgoal representations lead to a subsequent drop in performance. HIRO improves sample efficiency by relabeling subgoals to maximize the likelihood of past low-level action sequences. However, it lacks active exploration and fails to achieve a balance between exploration and exploitation. SAC performs poorly across all tasks, showing hierarchical advantages in solving long-term tasks with sparse rewards. Fig. 4. Episodes in the state space and representations in the latent space at different learning stages in the Ant Maze task. 3.5 0.2 1.0Success 4.0Steps \u00d71e6 0.8 3.0 1.5 0.0 0.0 0.5 1.0 2.5 0.6 2.0 0.4 0.2 0.8 5Steps \u00d71e6 4 1.0Success 2 0.6 0.0 0.4 1 3 0 (a) Ant Maze (b) Ant Push (a) State DIST. (0.5M steps) (b) State DIST. (1.5M steps) Fig. 6. Ablation studies of critical modules. \u201cB\u201d denotes \u201cBasic GCHRL\u201d, \u201cCSRL\u201d denotes \u201cContrastive Subgoal Representation Learning\u201d, \u201cG-N\u201d de- notes \u201cbuilding Graphs with the Novelty measure\u201d, and \u201cG-N-U\u201d denotes \u201cbuilding Graphs with the Novelty measure and the Utility measure\u201d. stabilize and exhibit little difference. The active exploration in the early stage and adequate exploitation in the later stage demonstrate the effectiveness of our proposed balanced strategy. (c) REP. DIST. (0.5M steps) (d) REP. DIST. (1.5M steps) Fig. 5. Visualization of exploration and exploitation in the Ant Maze task at 0.5 and 1.5 million steps, respectively. The x, y-axes are positions in the corresponding space, and the z-axis is the visit counts statistically obtained from the replay buffer with a capacity of 105. \u201cREP.\u201d denotes the word \u201crepresentation\u201d and \u201cDIST.\u201d denotes the word \u201cdistribution\u201d. C. Visualization Analysis 1) Exploration and Exploitation: We visualize the explo- ration and exploitation statuses by counting the visits of states in the state space and representations in the latent space, respectively, as shown in Figure 5. With the guidance of the novelty measure, HILL actively selects subgoals that are less explored, resulting in an extensive range of representations in the latent space and a dispersed distribution in the state space. As training progresses, the cumulative visits to various latent representations become uniform, and the utility measure enables more precise"}, {"question": " What type of tasks are used to evaluate HILL?", "answer": " HILL is evaluated on challenging continuous control tasks based on the MuJoCo simulator.", "ref_chunk": "the number of landmarks in Lt. Then we define a balanced strategy based on N and P and select a landmark with a smaller historical visit count and a larger transitional benefit as the subgoal gt of the next time interval: gt = arg min [(1 \u2212 P(U (wi\u2032,j))) \u00d7 N (lj)], lj \u2208Lt where li\u2032 = \u03c6(st) and st is the state of the current time step. Algorithm 1 shows the pseudo-code of HILL. Note that we use the subgoal selection strategy in Equation 9 as a high- level teacher policy and use \u03c0\u03b8h modeled by a neural network as a high-level student policy. Both policies map states to subgoals and are employed with a probability of p and 1 \u2212 p, respectively. The role of the student policy is mainly to help increase randomness and prevent falling into local optimum. p is initialized to 0.5 and gradually increases to 1 in the training process. \u03b8h is trained with SAC using episodes in Bl. V. EXPERIMENTS A. Environmental Settings We evaluate HILL on a suite of challenging continuous control tasks based on the MuJoCo simulator [13]. These tasks require the agent to master a combination of locomotion and object manipulation skills. The reward is sparse: the agent gets (8) (9) (a) Ant Maze (b) Ant FourRooms (c) Ant Push (d) HalfCheetah Hurdle (e) HalfCheetah Climbing (f) HalfCheetah Ascending Fig. 2. The continuous control environments with sparse rewards for evaluating HILL and baselines. 3 1.0Success 5Steps \u00d71e6 4 0.2 0.6 0.8 0.0 1 0 2 0.4 0.8 2 0.2 0.0 12 0 8 0.4 0.6 14Steps \u00d71e6 4 10 6 1.0Success 0.6 0.8 3 0.4 0 1 4 0.0 1.0Success 5Steps \u00d71e6 0.2 2 (a) Ant Maze (b) Ant FourRooms (c) Ant Push 0.8 1.0 0.5 1.0Success 2.0 0.6 2.5Steps \u00d71e6 0.0 0.2 0.0 1.5 0.4 1.5 0.8 0.2 0.0 1.0Success 2.5 0.4 0.0 0.6 0.5 2.0 3.0Steps \u00d71e6 1.0 2.00Steps \u00d71e6 0.00 1.50 0.2 1.0Success 0.6 0.8 0.75 0.0 0.25 0.50 1.00 1.75 1.25 0.4 (e) HalfCheetah Climbing Fig. 3. Learning curves of HILL and baselines on MuJoCo tasks. The x-axis shows the time step of training, and the y-axis shows the average test success rate of 10 episodes. Each line is the mean of 5 runs with shaded regions corresponding to the 95% confidence interval. (d) HalfCheetah Hurdle (f) HalfCheetah Ascending 0 when reaching the goal and \u22121 otherwise. During training, the agent is initialized with random positions and is tested under the most challenging setting to reach the other side of the environment. We conduct experiments on six environments in MuJoCo, and their visualizations are presented in Figure 2. The maximum episode length is limited to 500 steps for the Ant Maze, Ant Push, HalfCheetah Hurdle, and HalfCheetah Ascending tasks and 1000 steps for the Ant FourRooms and HalfCheetah Climbing tasks. B. Comparative Experiments We compare HILL with the SOTA baselines, which include: (1) HESS [10]: a GCHRL method realizing active exploration based on stable subgoal representation learning. (2) LESSON [9]: a GCHRL method learning subgoal representations with slow dynamics. (3) HIRO [5]: a GCHRL method using off- policy correction for efficient exploitation. (4) SAC [29]: the base RL method we used for training the bi-level policies. Note that we use the original implementations of LESSON and HESS on all tasks. The experimental results, as presented in Figure 3, demon- strate that HILL outperforms all baselines in terms of both sample efficiency and asymptotic performance. This success can be attributed to the use of efficient subgoal representations with temporal coherence and a subgoal selection strategy that effectively balances exploration and exploitation. In compari- son, HESS underperforms our method due to its focus on ex- ploring novel and reachable subgoals, lacking consideration of the potential benefits that subgoals can provide in completing the source task. LESSON yields an uptrend similar to HILL in Ant Push in the early stage. However, its unstable subgoal representations lead to a subsequent drop in performance. HIRO improves sample efficiency by relabeling subgoals to maximize the likelihood of past low-level action sequences. However, it lacks active exploration and fails to achieve a balance between exploration and exploitation. SAC performs poorly across all tasks, showing hierarchical advantages in solving long-term tasks with sparse rewards. Fig. 4. Episodes in the state space and representations in the latent space at different learning stages in the Ant Maze task. 3.5 0.2 1.0Success 4.0Steps \u00d71e6 0.8 3.0 1.5 0.0 0.0 0.5 1.0 2.5 0.6 2.0 0.4 0.2 0.8 5Steps \u00d71e6 4 1.0Success 2 0.6 0.0 0.4 1 3 0 (a) Ant Maze (b) Ant Push (a) State DIST. (0.5M steps) (b) State DIST. (1.5M steps) Fig. 6. Ablation studies of critical modules. \u201cB\u201d denotes \u201cBasic GCHRL\u201d, \u201cCSRL\u201d denotes \u201cContrastive Subgoal Representation Learning\u201d, \u201cG-N\u201d de- notes \u201cbuilding Graphs with the Novelty measure\u201d, and \u201cG-N-U\u201d denotes \u201cbuilding Graphs with the Novelty measure and the Utility measure\u201d. stabilize and exhibit little difference. The active exploration in the early stage and adequate exploitation in the later stage demonstrate the effectiveness of our proposed balanced strategy. (c) REP. DIST. (0.5M steps) (d) REP. DIST. (1.5M steps) Fig. 5. Visualization of exploration and exploitation in the Ant Maze task at 0.5 and 1.5 million steps, respectively. The x, y-axes are positions in the corresponding space, and the z-axis is the visit counts statistically obtained from the replay buffer with a capacity of 105. \u201cREP.\u201d denotes the word \u201crepresentation\u201d and \u201cDIST.\u201d denotes the word \u201cdistribution\u201d. C. Visualization Analysis 1) Exploration and Exploitation: We visualize the explo- ration and exploitation statuses by counting the visits of states in the state space and representations in the latent space, respectively, as shown in Figure 5. With the guidance of the novelty measure, HILL actively selects subgoals that are less explored, resulting in an extensive range of representations in the latent space and a dispersed distribution in the state space. As training progresses, the cumulative visits to various latent representations become uniform, and the utility measure enables more precise"}, {"question": " What kind of reward does the agent receive in the tasks evaluated in HILL?", "answer": " The agent gets a reward of 0 when reaching the goal and -1 otherwise in the tasks evaluated in HILL.", "ref_chunk": "the number of landmarks in Lt. Then we define a balanced strategy based on N and P and select a landmark with a smaller historical visit count and a larger transitional benefit as the subgoal gt of the next time interval: gt = arg min [(1 \u2212 P(U (wi\u2032,j))) \u00d7 N (lj)], lj \u2208Lt where li\u2032 = \u03c6(st) and st is the state of the current time step. Algorithm 1 shows the pseudo-code of HILL. Note that we use the subgoal selection strategy in Equation 9 as a high- level teacher policy and use \u03c0\u03b8h modeled by a neural network as a high-level student policy. Both policies map states to subgoals and are employed with a probability of p and 1 \u2212 p, respectively. The role of the student policy is mainly to help increase randomness and prevent falling into local optimum. p is initialized to 0.5 and gradually increases to 1 in the training process. \u03b8h is trained with SAC using episodes in Bl. V. EXPERIMENTS A. Environmental Settings We evaluate HILL on a suite of challenging continuous control tasks based on the MuJoCo simulator [13]. These tasks require the agent to master a combination of locomotion and object manipulation skills. The reward is sparse: the agent gets (8) (9) (a) Ant Maze (b) Ant FourRooms (c) Ant Push (d) HalfCheetah Hurdle (e) HalfCheetah Climbing (f) HalfCheetah Ascending Fig. 2. The continuous control environments with sparse rewards for evaluating HILL and baselines. 3 1.0Success 5Steps \u00d71e6 4 0.2 0.6 0.8 0.0 1 0 2 0.4 0.8 2 0.2 0.0 12 0 8 0.4 0.6 14Steps \u00d71e6 4 10 6 1.0Success 0.6 0.8 3 0.4 0 1 4 0.0 1.0Success 5Steps \u00d71e6 0.2 2 (a) Ant Maze (b) Ant FourRooms (c) Ant Push 0.8 1.0 0.5 1.0Success 2.0 0.6 2.5Steps \u00d71e6 0.0 0.2 0.0 1.5 0.4 1.5 0.8 0.2 0.0 1.0Success 2.5 0.4 0.0 0.6 0.5 2.0 3.0Steps \u00d71e6 1.0 2.00Steps \u00d71e6 0.00 1.50 0.2 1.0Success 0.6 0.8 0.75 0.0 0.25 0.50 1.00 1.75 1.25 0.4 (e) HalfCheetah Climbing Fig. 3. Learning curves of HILL and baselines on MuJoCo tasks. The x-axis shows the time step of training, and the y-axis shows the average test success rate of 10 episodes. Each line is the mean of 5 runs with shaded regions corresponding to the 95% confidence interval. (d) HalfCheetah Hurdle (f) HalfCheetah Ascending 0 when reaching the goal and \u22121 otherwise. During training, the agent is initialized with random positions and is tested under the most challenging setting to reach the other side of the environment. We conduct experiments on six environments in MuJoCo, and their visualizations are presented in Figure 2. The maximum episode length is limited to 500 steps for the Ant Maze, Ant Push, HalfCheetah Hurdle, and HalfCheetah Ascending tasks and 1000 steps for the Ant FourRooms and HalfCheetah Climbing tasks. B. Comparative Experiments We compare HILL with the SOTA baselines, which include: (1) HESS [10]: a GCHRL method realizing active exploration based on stable subgoal representation learning. (2) LESSON [9]: a GCHRL method learning subgoal representations with slow dynamics. (3) HIRO [5]: a GCHRL method using off- policy correction for efficient exploitation. (4) SAC [29]: the base RL method we used for training the bi-level policies. Note that we use the original implementations of LESSON and HESS on all tasks. The experimental results, as presented in Figure 3, demon- strate that HILL outperforms all baselines in terms of both sample efficiency and asymptotic performance. This success can be attributed to the use of efficient subgoal representations with temporal coherence and a subgoal selection strategy that effectively balances exploration and exploitation. In compari- son, HESS underperforms our method due to its focus on ex- ploring novel and reachable subgoals, lacking consideration of the potential benefits that subgoals can provide in completing the source task. LESSON yields an uptrend similar to HILL in Ant Push in the early stage. However, its unstable subgoal representations lead to a subsequent drop in performance. HIRO improves sample efficiency by relabeling subgoals to maximize the likelihood of past low-level action sequences. However, it lacks active exploration and fails to achieve a balance between exploration and exploitation. SAC performs poorly across all tasks, showing hierarchical advantages in solving long-term tasks with sparse rewards. Fig. 4. Episodes in the state space and representations in the latent space at different learning stages in the Ant Maze task. 3.5 0.2 1.0Success 4.0Steps \u00d71e6 0.8 3.0 1.5 0.0 0.0 0.5 1.0 2.5 0.6 2.0 0.4 0.2 0.8 5Steps \u00d71e6 4 1.0Success 2 0.6 0.0 0.4 1 3 0 (a) Ant Maze (b) Ant Push (a) State DIST. (0.5M steps) (b) State DIST. (1.5M steps) Fig. 6. Ablation studies of critical modules. \u201cB\u201d denotes \u201cBasic GCHRL\u201d, \u201cCSRL\u201d denotes \u201cContrastive Subgoal Representation Learning\u201d, \u201cG-N\u201d de- notes \u201cbuilding Graphs with the Novelty measure\u201d, and \u201cG-N-U\u201d denotes \u201cbuilding Graphs with the Novelty measure and the Utility measure\u201d. stabilize and exhibit little difference. The active exploration in the early stage and adequate exploitation in the later stage demonstrate the effectiveness of our proposed balanced strategy. (c) REP. DIST. (0.5M steps) (d) REP. DIST. (1.5M steps) Fig. 5. Visualization of exploration and exploitation in the Ant Maze task at 0.5 and 1.5 million steps, respectively. The x, y-axes are positions in the corresponding space, and the z-axis is the visit counts statistically obtained from the replay buffer with a capacity of 105. \u201cREP.\u201d denotes the word \u201crepresentation\u201d and \u201cDIST.\u201d denotes the word \u201cdistribution\u201d. C. Visualization Analysis 1) Exploration and Exploitation: We visualize the explo- ration and exploitation statuses by counting the visits of states in the state space and representations in the latent space, respectively, as shown in Figure 5. With the guidance of the novelty measure, HILL actively selects subgoals that are less explored, resulting in an extensive range of representations in the latent space and a dispersed distribution in the state space. As training progresses, the cumulative visits to various latent representations become uniform, and the utility measure enables more precise"}, {"question": " What is the maximum episode length for the Ant Maze and Ant Push tasks in HILL?", "answer": " The maximum episode length is limited to 500 steps for the Ant Maze and Ant Push tasks.", "ref_chunk": "the number of landmarks in Lt. Then we define a balanced strategy based on N and P and select a landmark with a smaller historical visit count and a larger transitional benefit as the subgoal gt of the next time interval: gt = arg min [(1 \u2212 P(U (wi\u2032,j))) \u00d7 N (lj)], lj \u2208Lt where li\u2032 = \u03c6(st) and st is the state of the current time step. Algorithm 1 shows the pseudo-code of HILL. Note that we use the subgoal selection strategy in Equation 9 as a high- level teacher policy and use \u03c0\u03b8h modeled by a neural network as a high-level student policy. Both policies map states to subgoals and are employed with a probability of p and 1 \u2212 p, respectively. The role of the student policy is mainly to help increase randomness and prevent falling into local optimum. p is initialized to 0.5 and gradually increases to 1 in the training process. \u03b8h is trained with SAC using episodes in Bl. V. EXPERIMENTS A. Environmental Settings We evaluate HILL on a suite of challenging continuous control tasks based on the MuJoCo simulator [13]. These tasks require the agent to master a combination of locomotion and object manipulation skills. The reward is sparse: the agent gets (8) (9) (a) Ant Maze (b) Ant FourRooms (c) Ant Push (d) HalfCheetah Hurdle (e) HalfCheetah Climbing (f) HalfCheetah Ascending Fig. 2. The continuous control environments with sparse rewards for evaluating HILL and baselines. 3 1.0Success 5Steps \u00d71e6 4 0.2 0.6 0.8 0.0 1 0 2 0.4 0.8 2 0.2 0.0 12 0 8 0.4 0.6 14Steps \u00d71e6 4 10 6 1.0Success 0.6 0.8 3 0.4 0 1 4 0.0 1.0Success 5Steps \u00d71e6 0.2 2 (a) Ant Maze (b) Ant FourRooms (c) Ant Push 0.8 1.0 0.5 1.0Success 2.0 0.6 2.5Steps \u00d71e6 0.0 0.2 0.0 1.5 0.4 1.5 0.8 0.2 0.0 1.0Success 2.5 0.4 0.0 0.6 0.5 2.0 3.0Steps \u00d71e6 1.0 2.00Steps \u00d71e6 0.00 1.50 0.2 1.0Success 0.6 0.8 0.75 0.0 0.25 0.50 1.00 1.75 1.25 0.4 (e) HalfCheetah Climbing Fig. 3. Learning curves of HILL and baselines on MuJoCo tasks. The x-axis shows the time step of training, and the y-axis shows the average test success rate of 10 episodes. Each line is the mean of 5 runs with shaded regions corresponding to the 95% confidence interval. (d) HalfCheetah Hurdle (f) HalfCheetah Ascending 0 when reaching the goal and \u22121 otherwise. During training, the agent is initialized with random positions and is tested under the most challenging setting to reach the other side of the environment. We conduct experiments on six environments in MuJoCo, and their visualizations are presented in Figure 2. The maximum episode length is limited to 500 steps for the Ant Maze, Ant Push, HalfCheetah Hurdle, and HalfCheetah Ascending tasks and 1000 steps for the Ant FourRooms and HalfCheetah Climbing tasks. B. Comparative Experiments We compare HILL with the SOTA baselines, which include: (1) HESS [10]: a GCHRL method realizing active exploration based on stable subgoal representation learning. (2) LESSON [9]: a GCHRL method learning subgoal representations with slow dynamics. (3) HIRO [5]: a GCHRL method using off- policy correction for efficient exploitation. (4) SAC [29]: the base RL method we used for training the bi-level policies. Note that we use the original implementations of LESSON and HESS on all tasks. The experimental results, as presented in Figure 3, demon- strate that HILL outperforms all baselines in terms of both sample efficiency and asymptotic performance. This success can be attributed to the use of efficient subgoal representations with temporal coherence and a subgoal selection strategy that effectively balances exploration and exploitation. In compari- son, HESS underperforms our method due to its focus on ex- ploring novel and reachable subgoals, lacking consideration of the potential benefits that subgoals can provide in completing the source task. LESSON yields an uptrend similar to HILL in Ant Push in the early stage. However, its unstable subgoal representations lead to a subsequent drop in performance. HIRO improves sample efficiency by relabeling subgoals to maximize the likelihood of past low-level action sequences. However, it lacks active exploration and fails to achieve a balance between exploration and exploitation. SAC performs poorly across all tasks, showing hierarchical advantages in solving long-term tasks with sparse rewards. Fig. 4. Episodes in the state space and representations in the latent space at different learning stages in the Ant Maze task. 3.5 0.2 1.0Success 4.0Steps \u00d71e6 0.8 3.0 1.5 0.0 0.0 0.5 1.0 2.5 0.6 2.0 0.4 0.2 0.8 5Steps \u00d71e6 4 1.0Success 2 0.6 0.0 0.4 1 3 0 (a) Ant Maze (b) Ant Push (a) State DIST. (0.5M steps) (b) State DIST. (1.5M steps) Fig. 6. Ablation studies of critical modules. \u201cB\u201d denotes \u201cBasic GCHRL\u201d, \u201cCSRL\u201d denotes \u201cContrastive Subgoal Representation Learning\u201d, \u201cG-N\u201d de- notes \u201cbuilding Graphs with the Novelty measure\u201d, and \u201cG-N-U\u201d denotes \u201cbuilding Graphs with the Novelty measure and the Utility measure\u201d. stabilize and exhibit little difference. The active exploration in the early stage and adequate exploitation in the later stage demonstrate the effectiveness of our proposed balanced strategy. (c) REP. DIST. (0.5M steps) (d) REP. DIST. (1.5M steps) Fig. 5. Visualization of exploration and exploitation in the Ant Maze task at 0.5 and 1.5 million steps, respectively. The x, y-axes are positions in the corresponding space, and the z-axis is the visit counts statistically obtained from the replay buffer with a capacity of 105. \u201cREP.\u201d denotes the word \u201crepresentation\u201d and \u201cDIST.\u201d denotes the word \u201cdistribution\u201d. C. Visualization Analysis 1) Exploration and Exploitation: We visualize the explo- ration and exploitation statuses by counting the visits of states in the state space and representations in the latent space, respectively, as shown in Figure 5. With the guidance of the novelty measure, HILL actively selects subgoals that are less explored, resulting in an extensive range of representations in the latent space and a dispersed distribution in the state space. As training progresses, the cumulative visits to various latent representations become uniform, and the utility measure enables more precise"}, {"question": " Which method outperforms all baselines in terms of both sample efficiency and asymptotic performance?", "answer": " HILL outperforms all baselines in terms of both sample efficiency and asymptotic performance.", "ref_chunk": "the number of landmarks in Lt. Then we define a balanced strategy based on N and P and select a landmark with a smaller historical visit count and a larger transitional benefit as the subgoal gt of the next time interval: gt = arg min [(1 \u2212 P(U (wi\u2032,j))) \u00d7 N (lj)], lj \u2208Lt where li\u2032 = \u03c6(st) and st is the state of the current time step. Algorithm 1 shows the pseudo-code of HILL. Note that we use the subgoal selection strategy in Equation 9 as a high- level teacher policy and use \u03c0\u03b8h modeled by a neural network as a high-level student policy. Both policies map states to subgoals and are employed with a probability of p and 1 \u2212 p, respectively. The role of the student policy is mainly to help increase randomness and prevent falling into local optimum. p is initialized to 0.5 and gradually increases to 1 in the training process. \u03b8h is trained with SAC using episodes in Bl. V. EXPERIMENTS A. Environmental Settings We evaluate HILL on a suite of challenging continuous control tasks based on the MuJoCo simulator [13]. These tasks require the agent to master a combination of locomotion and object manipulation skills. The reward is sparse: the agent gets (8) (9) (a) Ant Maze (b) Ant FourRooms (c) Ant Push (d) HalfCheetah Hurdle (e) HalfCheetah Climbing (f) HalfCheetah Ascending Fig. 2. The continuous control environments with sparse rewards for evaluating HILL and baselines. 3 1.0Success 5Steps \u00d71e6 4 0.2 0.6 0.8 0.0 1 0 2 0.4 0.8 2 0.2 0.0 12 0 8 0.4 0.6 14Steps \u00d71e6 4 10 6 1.0Success 0.6 0.8 3 0.4 0 1 4 0.0 1.0Success 5Steps \u00d71e6 0.2 2 (a) Ant Maze (b) Ant FourRooms (c) Ant Push 0.8 1.0 0.5 1.0Success 2.0 0.6 2.5Steps \u00d71e6 0.0 0.2 0.0 1.5 0.4 1.5 0.8 0.2 0.0 1.0Success 2.5 0.4 0.0 0.6 0.5 2.0 3.0Steps \u00d71e6 1.0 2.00Steps \u00d71e6 0.00 1.50 0.2 1.0Success 0.6 0.8 0.75 0.0 0.25 0.50 1.00 1.75 1.25 0.4 (e) HalfCheetah Climbing Fig. 3. Learning curves of HILL and baselines on MuJoCo tasks. The x-axis shows the time step of training, and the y-axis shows the average test success rate of 10 episodes. Each line is the mean of 5 runs with shaded regions corresponding to the 95% confidence interval. (d) HalfCheetah Hurdle (f) HalfCheetah Ascending 0 when reaching the goal and \u22121 otherwise. During training, the agent is initialized with random positions and is tested under the most challenging setting to reach the other side of the environment. We conduct experiments on six environments in MuJoCo, and their visualizations are presented in Figure 2. The maximum episode length is limited to 500 steps for the Ant Maze, Ant Push, HalfCheetah Hurdle, and HalfCheetah Ascending tasks and 1000 steps for the Ant FourRooms and HalfCheetah Climbing tasks. B. Comparative Experiments We compare HILL with the SOTA baselines, which include: (1) HESS [10]: a GCHRL method realizing active exploration based on stable subgoal representation learning. (2) LESSON [9]: a GCHRL method learning subgoal representations with slow dynamics. (3) HIRO [5]: a GCHRL method using off- policy correction for efficient exploitation. (4) SAC [29]: the base RL method we used for training the bi-level policies. Note that we use the original implementations of LESSON and HESS on all tasks. The experimental results, as presented in Figure 3, demon- strate that HILL outperforms all baselines in terms of both sample efficiency and asymptotic performance. This success can be attributed to the use of efficient subgoal representations with temporal coherence and a subgoal selection strategy that effectively balances exploration and exploitation. In compari- son, HESS underperforms our method due to its focus on ex- ploring novel and reachable subgoals, lacking consideration of the potential benefits that subgoals can provide in completing the source task. LESSON yields an uptrend similar to HILL in Ant Push in the early stage. However, its unstable subgoal representations lead to a subsequent drop in performance. HIRO improves sample efficiency by relabeling subgoals to maximize the likelihood of past low-level action sequences. However, it lacks active exploration and fails to achieve a balance between exploration and exploitation. SAC performs poorly across all tasks, showing hierarchical advantages in solving long-term tasks with sparse rewards. Fig. 4. Episodes in the state space and representations in the latent space at different learning stages in the Ant Maze task. 3.5 0.2 1.0Success 4.0Steps \u00d71e6 0.8 3.0 1.5 0.0 0.0 0.5 1.0 2.5 0.6 2.0 0.4 0.2 0.8 5Steps \u00d71e6 4 1.0Success 2 0.6 0.0 0.4 1 3 0 (a) Ant Maze (b) Ant Push (a) State DIST. (0.5M steps) (b) State DIST. (1.5M steps) Fig. 6. Ablation studies of critical modules. \u201cB\u201d denotes \u201cBasic GCHRL\u201d, \u201cCSRL\u201d denotes \u201cContrastive Subgoal Representation Learning\u201d, \u201cG-N\u201d de- notes \u201cbuilding Graphs with the Novelty measure\u201d, and \u201cG-N-U\u201d denotes \u201cbuilding Graphs with the Novelty measure and the Utility measure\u201d. stabilize and exhibit little difference. The active exploration in the early stage and adequate exploitation in the later stage demonstrate the effectiveness of our proposed balanced strategy. (c) REP. DIST. (0.5M steps) (d) REP. DIST. (1.5M steps) Fig. 5. Visualization of exploration and exploitation in the Ant Maze task at 0.5 and 1.5 million steps, respectively. The x, y-axes are positions in the corresponding space, and the z-axis is the visit counts statistically obtained from the replay buffer with a capacity of 105. \u201cREP.\u201d denotes the word \u201crepresentation\u201d and \u201cDIST.\u201d denotes the word \u201cdistribution\u201d. C. Visualization Analysis 1) Exploration and Exploitation: We visualize the explo- ration and exploitation statuses by counting the visits of states in the state space and representations in the latent space, respectively, as shown in Figure 5. With the guidance of the novelty measure, HILL actively selects subgoals that are less explored, resulting in an extensive range of representations in the latent space and a dispersed distribution in the state space. As training progresses, the cumulative visits to various latent representations become uniform, and the utility measure enables more precise"}, {"question": " Why does HESS underperform in comparison to HILL?", "answer": " HESS focuses on exploring novel and reachable subgoals, lacking consideration of the potential benefits that subgoals can provide in completing the source task.", "ref_chunk": "the number of landmarks in Lt. Then we define a balanced strategy based on N and P and select a landmark with a smaller historical visit count and a larger transitional benefit as the subgoal gt of the next time interval: gt = arg min [(1 \u2212 P(U (wi\u2032,j))) \u00d7 N (lj)], lj \u2208Lt where li\u2032 = \u03c6(st) and st is the state of the current time step. Algorithm 1 shows the pseudo-code of HILL. Note that we use the subgoal selection strategy in Equation 9 as a high- level teacher policy and use \u03c0\u03b8h modeled by a neural network as a high-level student policy. Both policies map states to subgoals and are employed with a probability of p and 1 \u2212 p, respectively. The role of the student policy is mainly to help increase randomness and prevent falling into local optimum. p is initialized to 0.5 and gradually increases to 1 in the training process. \u03b8h is trained with SAC using episodes in Bl. V. EXPERIMENTS A. Environmental Settings We evaluate HILL on a suite of challenging continuous control tasks based on the MuJoCo simulator [13]. These tasks require the agent to master a combination of locomotion and object manipulation skills. The reward is sparse: the agent gets (8) (9) (a) Ant Maze (b) Ant FourRooms (c) Ant Push (d) HalfCheetah Hurdle (e) HalfCheetah Climbing (f) HalfCheetah Ascending Fig. 2. The continuous control environments with sparse rewards for evaluating HILL and baselines. 3 1.0Success 5Steps \u00d71e6 4 0.2 0.6 0.8 0.0 1 0 2 0.4 0.8 2 0.2 0.0 12 0 8 0.4 0.6 14Steps \u00d71e6 4 10 6 1.0Success 0.6 0.8 3 0.4 0 1 4 0.0 1.0Success 5Steps \u00d71e6 0.2 2 (a) Ant Maze (b) Ant FourRooms (c) Ant Push 0.8 1.0 0.5 1.0Success 2.0 0.6 2.5Steps \u00d71e6 0.0 0.2 0.0 1.5 0.4 1.5 0.8 0.2 0.0 1.0Success 2.5 0.4 0.0 0.6 0.5 2.0 3.0Steps \u00d71e6 1.0 2.00Steps \u00d71e6 0.00 1.50 0.2 1.0Success 0.6 0.8 0.75 0.0 0.25 0.50 1.00 1.75 1.25 0.4 (e) HalfCheetah Climbing Fig. 3. Learning curves of HILL and baselines on MuJoCo tasks. The x-axis shows the time step of training, and the y-axis shows the average test success rate of 10 episodes. Each line is the mean of 5 runs with shaded regions corresponding to the 95% confidence interval. (d) HalfCheetah Hurdle (f) HalfCheetah Ascending 0 when reaching the goal and \u22121 otherwise. During training, the agent is initialized with random positions and is tested under the most challenging setting to reach the other side of the environment. We conduct experiments on six environments in MuJoCo, and their visualizations are presented in Figure 2. The maximum episode length is limited to 500 steps for the Ant Maze, Ant Push, HalfCheetah Hurdle, and HalfCheetah Ascending tasks and 1000 steps for the Ant FourRooms and HalfCheetah Climbing tasks. B. Comparative Experiments We compare HILL with the SOTA baselines, which include: (1) HESS [10]: a GCHRL method realizing active exploration based on stable subgoal representation learning. (2) LESSON [9]: a GCHRL method learning subgoal representations with slow dynamics. (3) HIRO [5]: a GCHRL method using off- policy correction for efficient exploitation. (4) SAC [29]: the base RL method we used for training the bi-level policies. Note that we use the original implementations of LESSON and HESS on all tasks. The experimental results, as presented in Figure 3, demon- strate that HILL outperforms all baselines in terms of both sample efficiency and asymptotic performance. This success can be attributed to the use of efficient subgoal representations with temporal coherence and a subgoal selection strategy that effectively balances exploration and exploitation. In compari- son, HESS underperforms our method due to its focus on ex- ploring novel and reachable subgoals, lacking consideration of the potential benefits that subgoals can provide in completing the source task. LESSON yields an uptrend similar to HILL in Ant Push in the early stage. However, its unstable subgoal representations lead to a subsequent drop in performance. HIRO improves sample efficiency by relabeling subgoals to maximize the likelihood of past low-level action sequences. However, it lacks active exploration and fails to achieve a balance between exploration and exploitation. SAC performs poorly across all tasks, showing hierarchical advantages in solving long-term tasks with sparse rewards. Fig. 4. Episodes in the state space and representations in the latent space at different learning stages in the Ant Maze task. 3.5 0.2 1.0Success 4.0Steps \u00d71e6 0.8 3.0 1.5 0.0 0.0 0.5 1.0 2.5 0.6 2.0 0.4 0.2 0.8 5Steps \u00d71e6 4 1.0Success 2 0.6 0.0 0.4 1 3 0 (a) Ant Maze (b) Ant Push (a) State DIST. (0.5M steps) (b) State DIST. (1.5M steps) Fig. 6. Ablation studies of critical modules. \u201cB\u201d denotes \u201cBasic GCHRL\u201d, \u201cCSRL\u201d denotes \u201cContrastive Subgoal Representation Learning\u201d, \u201cG-N\u201d de- notes \u201cbuilding Graphs with the Novelty measure\u201d, and \u201cG-N-U\u201d denotes \u201cbuilding Graphs with the Novelty measure and the Utility measure\u201d. stabilize and exhibit little difference. The active exploration in the early stage and adequate exploitation in the later stage demonstrate the effectiveness of our proposed balanced strategy. (c) REP. DIST. (0.5M steps) (d) REP. DIST. (1.5M steps) Fig. 5. Visualization of exploration and exploitation in the Ant Maze task at 0.5 and 1.5 million steps, respectively. The x, y-axes are positions in the corresponding space, and the z-axis is the visit counts statistically obtained from the replay buffer with a capacity of 105. \u201cREP.\u201d denotes the word \u201crepresentation\u201d and \u201cDIST.\u201d denotes the word \u201cdistribution\u201d. C. Visualization Analysis 1) Exploration and Exploitation: We visualize the explo- ration and exploitation statuses by counting the visits of states in the state space and representations in the latent space, respectively, as shown in Figure 5. With the guidance of the novelty measure, HILL actively selects subgoals that are less explored, resulting in an extensive range of representations in the latent space and a dispersed distribution in the state space. As training progresses, the cumulative visits to various latent representations become uniform, and the utility measure enables more precise"}, {"question": " How does HIRO improve sample efficiency?", "answer": " HIRO improves sample efficiency by relabeling subgoals to maximize the likelihood of past low-level action sequences.", "ref_chunk": "the number of landmarks in Lt. Then we define a balanced strategy based on N and P and select a landmark with a smaller historical visit count and a larger transitional benefit as the subgoal gt of the next time interval: gt = arg min [(1 \u2212 P(U (wi\u2032,j))) \u00d7 N (lj)], lj \u2208Lt where li\u2032 = \u03c6(st) and st is the state of the current time step. Algorithm 1 shows the pseudo-code of HILL. Note that we use the subgoal selection strategy in Equation 9 as a high- level teacher policy and use \u03c0\u03b8h modeled by a neural network as a high-level student policy. Both policies map states to subgoals and are employed with a probability of p and 1 \u2212 p, respectively. The role of the student policy is mainly to help increase randomness and prevent falling into local optimum. p is initialized to 0.5 and gradually increases to 1 in the training process. \u03b8h is trained with SAC using episodes in Bl. V. EXPERIMENTS A. Environmental Settings We evaluate HILL on a suite of challenging continuous control tasks based on the MuJoCo simulator [13]. These tasks require the agent to master a combination of locomotion and object manipulation skills. The reward is sparse: the agent gets (8) (9) (a) Ant Maze (b) Ant FourRooms (c) Ant Push (d) HalfCheetah Hurdle (e) HalfCheetah Climbing (f) HalfCheetah Ascending Fig. 2. The continuous control environments with sparse rewards for evaluating HILL and baselines. 3 1.0Success 5Steps \u00d71e6 4 0.2 0.6 0.8 0.0 1 0 2 0.4 0.8 2 0.2 0.0 12 0 8 0.4 0.6 14Steps \u00d71e6 4 10 6 1.0Success 0.6 0.8 3 0.4 0 1 4 0.0 1.0Success 5Steps \u00d71e6 0.2 2 (a) Ant Maze (b) Ant FourRooms (c) Ant Push 0.8 1.0 0.5 1.0Success 2.0 0.6 2.5Steps \u00d71e6 0.0 0.2 0.0 1.5 0.4 1.5 0.8 0.2 0.0 1.0Success 2.5 0.4 0.0 0.6 0.5 2.0 3.0Steps \u00d71e6 1.0 2.00Steps \u00d71e6 0.00 1.50 0.2 1.0Success 0.6 0.8 0.75 0.0 0.25 0.50 1.00 1.75 1.25 0.4 (e) HalfCheetah Climbing Fig. 3. Learning curves of HILL and baselines on MuJoCo tasks. The x-axis shows the time step of training, and the y-axis shows the average test success rate of 10 episodes. Each line is the mean of 5 runs with shaded regions corresponding to the 95% confidence interval. (d) HalfCheetah Hurdle (f) HalfCheetah Ascending 0 when reaching the goal and \u22121 otherwise. During training, the agent is initialized with random positions and is tested under the most challenging setting to reach the other side of the environment. We conduct experiments on six environments in MuJoCo, and their visualizations are presented in Figure 2. The maximum episode length is limited to 500 steps for the Ant Maze, Ant Push, HalfCheetah Hurdle, and HalfCheetah Ascending tasks and 1000 steps for the Ant FourRooms and HalfCheetah Climbing tasks. B. Comparative Experiments We compare HILL with the SOTA baselines, which include: (1) HESS [10]: a GCHRL method realizing active exploration based on stable subgoal representation learning. (2) LESSON [9]: a GCHRL method learning subgoal representations with slow dynamics. (3) HIRO [5]: a GCHRL method using off- policy correction for efficient exploitation. (4) SAC [29]: the base RL method we used for training the bi-level policies. Note that we use the original implementations of LESSON and HESS on all tasks. The experimental results, as presented in Figure 3, demon- strate that HILL outperforms all baselines in terms of both sample efficiency and asymptotic performance. This success can be attributed to the use of efficient subgoal representations with temporal coherence and a subgoal selection strategy that effectively balances exploration and exploitation. In compari- son, HESS underperforms our method due to its focus on ex- ploring novel and reachable subgoals, lacking consideration of the potential benefits that subgoals can provide in completing the source task. LESSON yields an uptrend similar to HILL in Ant Push in the early stage. However, its unstable subgoal representations lead to a subsequent drop in performance. HIRO improves sample efficiency by relabeling subgoals to maximize the likelihood of past low-level action sequences. However, it lacks active exploration and fails to achieve a balance between exploration and exploitation. SAC performs poorly across all tasks, showing hierarchical advantages in solving long-term tasks with sparse rewards. Fig. 4. Episodes in the state space and representations in the latent space at different learning stages in the Ant Maze task. 3.5 0.2 1.0Success 4.0Steps \u00d71e6 0.8 3.0 1.5 0.0 0.0 0.5 1.0 2.5 0.6 2.0 0.4 0.2 0.8 5Steps \u00d71e6 4 1.0Success 2 0.6 0.0 0.4 1 3 0 (a) Ant Maze (b) Ant Push (a) State DIST. (0.5M steps) (b) State DIST. (1.5M steps) Fig. 6. Ablation studies of critical modules. \u201cB\u201d denotes \u201cBasic GCHRL\u201d, \u201cCSRL\u201d denotes \u201cContrastive Subgoal Representation Learning\u201d, \u201cG-N\u201d de- notes \u201cbuilding Graphs with the Novelty measure\u201d, and \u201cG-N-U\u201d denotes \u201cbuilding Graphs with the Novelty measure and the Utility measure\u201d. stabilize and exhibit little difference. The active exploration in the early stage and adequate exploitation in the later stage demonstrate the effectiveness of our proposed balanced strategy. (c) REP. DIST. (0.5M steps) (d) REP. DIST. (1.5M steps) Fig. 5. Visualization of exploration and exploitation in the Ant Maze task at 0.5 and 1.5 million steps, respectively. The x, y-axes are positions in the corresponding space, and the z-axis is the visit counts statistically obtained from the replay buffer with a capacity of 105. \u201cREP.\u201d denotes the word \u201crepresentation\u201d and \u201cDIST.\u201d denotes the word \u201cdistribution\u201d. C. Visualization Analysis 1) Exploration and Exploitation: We visualize the explo- ration and exploitation statuses by counting the visits of states in the state space and representations in the latent space, respectively, as shown in Figure 5. With the guidance of the novelty measure, HILL actively selects subgoals that are less explored, resulting in an extensive range of representations in the latent space and a dispersed distribution in the state space. As training progresses, the cumulative visits to various latent representations become uniform, and the utility measure enables more precise"}], "doc_text": "the number of landmarks in Lt. Then we define a balanced strategy based on N and P and select a landmark with a smaller historical visit count and a larger transitional benefit as the subgoal gt of the next time interval: gt = arg min [(1 \u2212 P(U (wi\u2032,j))) \u00d7 N (lj)], lj \u2208Lt where li\u2032 = \u03c6(st) and st is the state of the current time step. Algorithm 1 shows the pseudo-code of HILL. Note that we use the subgoal selection strategy in Equation 9 as a high- level teacher policy and use \u03c0\u03b8h modeled by a neural network as a high-level student policy. Both policies map states to subgoals and are employed with a probability of p and 1 \u2212 p, respectively. The role of the student policy is mainly to help increase randomness and prevent falling into local optimum. p is initialized to 0.5 and gradually increases to 1 in the training process. \u03b8h is trained with SAC using episodes in Bl. V. EXPERIMENTS A. Environmental Settings We evaluate HILL on a suite of challenging continuous control tasks based on the MuJoCo simulator [13]. These tasks require the agent to master a combination of locomotion and object manipulation skills. The reward is sparse: the agent gets (8) (9) (a) Ant Maze (b) Ant FourRooms (c) Ant Push (d) HalfCheetah Hurdle (e) HalfCheetah Climbing (f) HalfCheetah Ascending Fig. 2. The continuous control environments with sparse rewards for evaluating HILL and baselines. 3 1.0Success 5Steps \u00d71e6 4 0.2 0.6 0.8 0.0 1 0 2 0.4 0.8 2 0.2 0.0 12 0 8 0.4 0.6 14Steps \u00d71e6 4 10 6 1.0Success 0.6 0.8 3 0.4 0 1 4 0.0 1.0Success 5Steps \u00d71e6 0.2 2 (a) Ant Maze (b) Ant FourRooms (c) Ant Push 0.8 1.0 0.5 1.0Success 2.0 0.6 2.5Steps \u00d71e6 0.0 0.2 0.0 1.5 0.4 1.5 0.8 0.2 0.0 1.0Success 2.5 0.4 0.0 0.6 0.5 2.0 3.0Steps \u00d71e6 1.0 2.00Steps \u00d71e6 0.00 1.50 0.2 1.0Success 0.6 0.8 0.75 0.0 0.25 0.50 1.00 1.75 1.25 0.4 (e) HalfCheetah Climbing Fig. 3. Learning curves of HILL and baselines on MuJoCo tasks. The x-axis shows the time step of training, and the y-axis shows the average test success rate of 10 episodes. Each line is the mean of 5 runs with shaded regions corresponding to the 95% confidence interval. (d) HalfCheetah Hurdle (f) HalfCheetah Ascending 0 when reaching the goal and \u22121 otherwise. During training, the agent is initialized with random positions and is tested under the most challenging setting to reach the other side of the environment. We conduct experiments on six environments in MuJoCo, and their visualizations are presented in Figure 2. The maximum episode length is limited to 500 steps for the Ant Maze, Ant Push, HalfCheetah Hurdle, and HalfCheetah Ascending tasks and 1000 steps for the Ant FourRooms and HalfCheetah Climbing tasks. B. Comparative Experiments We compare HILL with the SOTA baselines, which include: (1) HESS [10]: a GCHRL method realizing active exploration based on stable subgoal representation learning. (2) LESSON [9]: a GCHRL method learning subgoal representations with slow dynamics. (3) HIRO [5]: a GCHRL method using off- policy correction for efficient exploitation. (4) SAC [29]: the base RL method we used for training the bi-level policies. Note that we use the original implementations of LESSON and HESS on all tasks. The experimental results, as presented in Figure 3, demon- strate that HILL outperforms all baselines in terms of both sample efficiency and asymptotic performance. This success can be attributed to the use of efficient subgoal representations with temporal coherence and a subgoal selection strategy that effectively balances exploration and exploitation. In compari- son, HESS underperforms our method due to its focus on ex- ploring novel and reachable subgoals, lacking consideration of the potential benefits that subgoals can provide in completing the source task. LESSON yields an uptrend similar to HILL in Ant Push in the early stage. However, its unstable subgoal representations lead to a subsequent drop in performance. HIRO improves sample efficiency by relabeling subgoals to maximize the likelihood of past low-level action sequences. However, it lacks active exploration and fails to achieve a balance between exploration and exploitation. SAC performs poorly across all tasks, showing hierarchical advantages in solving long-term tasks with sparse rewards. Fig. 4. Episodes in the state space and representations in the latent space at different learning stages in the Ant Maze task. 3.5 0.2 1.0Success 4.0Steps \u00d71e6 0.8 3.0 1.5 0.0 0.0 0.5 1.0 2.5 0.6 2.0 0.4 0.2 0.8 5Steps \u00d71e6 4 1.0Success 2 0.6 0.0 0.4 1 3 0 (a) Ant Maze (b) Ant Push (a) State DIST. (0.5M steps) (b) State DIST. (1.5M steps) Fig. 6. Ablation studies of critical modules. \u201cB\u201d denotes \u201cBasic GCHRL\u201d, \u201cCSRL\u201d denotes \u201cContrastive Subgoal Representation Learning\u201d, \u201cG-N\u201d de- notes \u201cbuilding Graphs with the Novelty measure\u201d, and \u201cG-N-U\u201d denotes \u201cbuilding Graphs with the Novelty measure and the Utility measure\u201d. stabilize and exhibit little difference. The active exploration in the early stage and adequate exploitation in the later stage demonstrate the effectiveness of our proposed balanced strategy. (c) REP. DIST. (0.5M steps) (d) REP. DIST. (1.5M steps) Fig. 5. Visualization of exploration and exploitation in the Ant Maze task at 0.5 and 1.5 million steps, respectively. The x, y-axes are positions in the corresponding space, and the z-axis is the visit counts statistically obtained from the replay buffer with a capacity of 105. \u201cREP.\u201d denotes the word \u201crepresentation\u201d and \u201cDIST.\u201d denotes the word \u201cdistribution\u201d. C. Visualization Analysis 1) Exploration and Exploitation: We visualize the explo- ration and exploitation statuses by counting the visits of states in the state space and representations in the latent space, respectively, as shown in Figure 5. With the guidance of the novelty measure, HILL actively selects subgoals that are less explored, resulting in an extensive range of representations in the latent space and a dispersed distribution in the state space. As training progresses, the cumulative visits to various latent representations become uniform, and the utility measure enables more precise"}