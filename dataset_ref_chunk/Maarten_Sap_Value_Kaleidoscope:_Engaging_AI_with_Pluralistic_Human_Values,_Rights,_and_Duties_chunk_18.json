{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_Value_Kaleidoscope:_Engaging_AI_with_Pluralistic_Human_Values,_Rights,_and_Duties_chunk_18.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What was used to create a synthetic dataset of values in the study?", "answer": " LLM", "ref_chunk": "desire would be prohibitive. As such, we follow prior work [CITE, add in from related work] and decide to use a LLM to create a syn- thetic dataset of values. We verify the quality (Section 4.1) and representativeness (Section 4.2) of the outputs using hu- man annotators. Values, Rights, and Duties Generation Given the set of 31k situations, we prompt GPT-4 (OpenAI 2023) to gener- ate relevant values, rights, and duties23, along with an open- text explanation. Given the output, the model also predicts whether the corresponding value, right, or duty supports (or justifies) the action, opposes (or condemns) the action, or could either support or oppose depending on the context or interpretation. The cost to generate the entire dataset was $1,043.80. While the data was generated in a batch manner to pro- duce all values and related data at once, we exploit the struc- ture of the generated data to cast the Generation, Valence, and Explanation tasks as sequence-to-sequence (seq2seq) tasks. The relevance task data is sampled contrastively, where positive examples are values generated by GPT-4 for the situation negative samples are drawn from other gener- ated values. We split the data (by actions) into train/valida- tion/test splits of 80%/10%/10% respectively (See Table 7). G Model Training Details For training, we set our model size at 3 billion parameters us- ing the T5 encoder-decoder architecture (Raffel et al. 2020), and test the following hyperparameters: weight initializa- tion in {t5-3B, flan-t5-xl}, learning rate in {1e-4, 3e-4, 1e- 5, 3e-5}, and a dataset mixture of either {Generation, Rele- vance, Valence} or {Generation, Relevance, Valence, Expla- nation}. Because the explanation is post-hoc and of lesser interest to us than the other tasks, we choose the optimal set up on the validation set of the task mixture without the explanation task. We conduct a grid search and settle with learning rate at 3e-5 and a batch size of 32 with a mixture of all four tasks. For further analysis of the relationship of data mixture and model size with performance, see App. D.1. We train with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping and a batch size of 32, although we find that the majority of runs start to overfit after about 2 epochs. Training takes 19 hours per run on two A100 GPUs. We fix hyperparameters for the remainder of our experi- ments at the optimal hyperparameters: flan-t5-xl, 3e-5, and the mixture including explanations (which we find to as- sist generalization on the non-explanation tasks). For further analysis of the relationship of data mixture and model size with performance, see Section D.1. We refer to our default 3B trained model as KALEIDO. H System Details H.1 Algorithm See Algorithm 1. H.2 System Parameters For KALEIDOSYS 3B, we use these parameters for all exper- iments, which were found by maximizing RougeLSum f1- score VALUEPRISM-val. We also fix the number of genera- tions at 100 and take the top generations with beam search. Parameters determine the threshold for embedding cosine 23For the prompt used, please refer to Appendix M. Situation Claim Description Hacking a company to give them information about their flaws Preventing theft Firing Mike Information transparency: Providing information about flaws within a system might promote open- ness and transparency, which can be seen as a positive value. However, such information in the wrong hands could be dangerous. Personal Responsibility: Encouraging individuals to take steps to prevent theft can promote a sense of personal responsibility for one\u2019s belongings and actions. Duty to uphold company policies: If Mike\u2019s ac- tions violated company policies, management has a duty to take appropriate actions, which in this case may include firing. Claims for which all peo- ple surveyed agreed. 100% agreement 100% agreement 100% agreement Frowning at a friend Shooting a courier in the head who was responsible for the destruction of an en- tire city. Aiding a fugitive by provid- ing food or shelter in 1851 Pennsylvania. Respect: Not frowning at a friend if the situation doesn\u2019t warrant it could be a way of respecting their feelings. Duty to protect others: If the courier poses an on- going threat, there might be a duty to protect oth- ers from harm, which could include killing the courier. Compassion: Providing food and shelter for a fugitive showcases compassion towards those in need, regardless of their legal status. Claims with closest to aver- age rates of agreement. 81% agreement 82% agreement 82% agreement Tipping to your landlord Killing a potato to save a frog. Somali pirates need to feed their family so they ransom a ship Duty of gratitude: Tenants might have a moral duty to show gratitude to those who help or pro- vide a service, even if tipping is not customary for landlords. Respect for nature: Some might argue that inter- fering with nature by deciding which being lives or dies could disrupt the natural balance, thus showing a lack of respect for nature. Justice: The pirates are attempting to resolve eco- nomic inequality by gaining money, which they perceive as a just cause. Claims for which the least people surveyed agreed. 8% agreement 24% agreement 24% agreement Table 14: GPT-4 outputs from VALUEPRISM with most, average, and lowest levels of agreement based from CloudResearch study. similarity and the ngram overlap threshold for deduplicating, and the relevance score at which to drop poor outputs. There is a separete threshold for each category of value, right, and duty. 1 \"embed_threshold\": \"{\u2019Value\u2019: 0.53, \u2019Right\u2019: 0.63, \u2019Duty\u2019: 0.55}\", 2 \"ngram_threshold\": \"0.05\", 3 \"relevance_threshold\": \"{\u2019Value\u2019: 0.77, \u2019Right\u2019: 0.82, \u2019Duty\u2019: 0.9} I.1 Quality Annotation For this study, 3 crowdowkers for GPT-4 output for 3k sit- uations (10% of VALUEPRISM). See Figure 10 for the tem- plate used. For this study, note that we do not ask annotators to provide their own judgments of the situation, but merely to assess the relevance of the generations, which we expect to have much lower variation (e.g., someone may see how a value could be relevant for someone else while disagreeing"}, {"question": " How was the quality and representativeness of the dataset verified?", "answer": " Using human annotators", "ref_chunk": "desire would be prohibitive. As such, we follow prior work [CITE, add in from related work] and decide to use a LLM to create a syn- thetic dataset of values. We verify the quality (Section 4.1) and representativeness (Section 4.2) of the outputs using hu- man annotators. Values, Rights, and Duties Generation Given the set of 31k situations, we prompt GPT-4 (OpenAI 2023) to gener- ate relevant values, rights, and duties23, along with an open- text explanation. Given the output, the model also predicts whether the corresponding value, right, or duty supports (or justifies) the action, opposes (or condemns) the action, or could either support or oppose depending on the context or interpretation. The cost to generate the entire dataset was $1,043.80. While the data was generated in a batch manner to pro- duce all values and related data at once, we exploit the struc- ture of the generated data to cast the Generation, Valence, and Explanation tasks as sequence-to-sequence (seq2seq) tasks. The relevance task data is sampled contrastively, where positive examples are values generated by GPT-4 for the situation negative samples are drawn from other gener- ated values. We split the data (by actions) into train/valida- tion/test splits of 80%/10%/10% respectively (See Table 7). G Model Training Details For training, we set our model size at 3 billion parameters us- ing the T5 encoder-decoder architecture (Raffel et al. 2020), and test the following hyperparameters: weight initializa- tion in {t5-3B, flan-t5-xl}, learning rate in {1e-4, 3e-4, 1e- 5, 3e-5}, and a dataset mixture of either {Generation, Rele- vance, Valence} or {Generation, Relevance, Valence, Expla- nation}. Because the explanation is post-hoc and of lesser interest to us than the other tasks, we choose the optimal set up on the validation set of the task mixture without the explanation task. We conduct a grid search and settle with learning rate at 3e-5 and a batch size of 32 with a mixture of all four tasks. For further analysis of the relationship of data mixture and model size with performance, see App. D.1. We train with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping and a batch size of 32, although we find that the majority of runs start to overfit after about 2 epochs. Training takes 19 hours per run on two A100 GPUs. We fix hyperparameters for the remainder of our experi- ments at the optimal hyperparameters: flan-t5-xl, 3e-5, and the mixture including explanations (which we find to as- sist generalization on the non-explanation tasks). For further analysis of the relationship of data mixture and model size with performance, see Section D.1. We refer to our default 3B trained model as KALEIDO. H System Details H.1 Algorithm See Algorithm 1. H.2 System Parameters For KALEIDOSYS 3B, we use these parameters for all exper- iments, which were found by maximizing RougeLSum f1- score VALUEPRISM-val. We also fix the number of genera- tions at 100 and take the top generations with beam search. Parameters determine the threshold for embedding cosine 23For the prompt used, please refer to Appendix M. Situation Claim Description Hacking a company to give them information about their flaws Preventing theft Firing Mike Information transparency: Providing information about flaws within a system might promote open- ness and transparency, which can be seen as a positive value. However, such information in the wrong hands could be dangerous. Personal Responsibility: Encouraging individuals to take steps to prevent theft can promote a sense of personal responsibility for one\u2019s belongings and actions. Duty to uphold company policies: If Mike\u2019s ac- tions violated company policies, management has a duty to take appropriate actions, which in this case may include firing. Claims for which all peo- ple surveyed agreed. 100% agreement 100% agreement 100% agreement Frowning at a friend Shooting a courier in the head who was responsible for the destruction of an en- tire city. Aiding a fugitive by provid- ing food or shelter in 1851 Pennsylvania. Respect: Not frowning at a friend if the situation doesn\u2019t warrant it could be a way of respecting their feelings. Duty to protect others: If the courier poses an on- going threat, there might be a duty to protect oth- ers from harm, which could include killing the courier. Compassion: Providing food and shelter for a fugitive showcases compassion towards those in need, regardless of their legal status. Claims with closest to aver- age rates of agreement. 81% agreement 82% agreement 82% agreement Tipping to your landlord Killing a potato to save a frog. Somali pirates need to feed their family so they ransom a ship Duty of gratitude: Tenants might have a moral duty to show gratitude to those who help or pro- vide a service, even if tipping is not customary for landlords. Respect for nature: Some might argue that inter- fering with nature by deciding which being lives or dies could disrupt the natural balance, thus showing a lack of respect for nature. Justice: The pirates are attempting to resolve eco- nomic inequality by gaining money, which they perceive as a just cause. Claims for which the least people surveyed agreed. 8% agreement 24% agreement 24% agreement Table 14: GPT-4 outputs from VALUEPRISM with most, average, and lowest levels of agreement based from CloudResearch study. similarity and the ngram overlap threshold for deduplicating, and the relevance score at which to drop poor outputs. There is a separete threshold for each category of value, right, and duty. 1 \"embed_threshold\": \"{\u2019Value\u2019: 0.53, \u2019Right\u2019: 0.63, \u2019Duty\u2019: 0.55}\", 2 \"ngram_threshold\": \"0.05\", 3 \"relevance_threshold\": \"{\u2019Value\u2019: 0.77, \u2019Right\u2019: 0.82, \u2019Duty\u2019: 0.9} I.1 Quality Annotation For this study, 3 crowdowkers for GPT-4 output for 3k sit- uations (10% of VALUEPRISM). See Figure 10 for the tem- plate used. For this study, note that we do not ask annotators to provide their own judgments of the situation, but merely to assess the relevance of the generations, which we expect to have much lower variation (e.g., someone may see how a value could be relevant for someone else while disagreeing"}, {"question": " What was the prompt given to GPT-4 in the study?", "answer": " Generate relevant values, rights, and duties along with an open-text explanation", "ref_chunk": "desire would be prohibitive. As such, we follow prior work [CITE, add in from related work] and decide to use a LLM to create a syn- thetic dataset of values. We verify the quality (Section 4.1) and representativeness (Section 4.2) of the outputs using hu- man annotators. Values, Rights, and Duties Generation Given the set of 31k situations, we prompt GPT-4 (OpenAI 2023) to gener- ate relevant values, rights, and duties23, along with an open- text explanation. Given the output, the model also predicts whether the corresponding value, right, or duty supports (or justifies) the action, opposes (or condemns) the action, or could either support or oppose depending on the context or interpretation. The cost to generate the entire dataset was $1,043.80. While the data was generated in a batch manner to pro- duce all values and related data at once, we exploit the struc- ture of the generated data to cast the Generation, Valence, and Explanation tasks as sequence-to-sequence (seq2seq) tasks. The relevance task data is sampled contrastively, where positive examples are values generated by GPT-4 for the situation negative samples are drawn from other gener- ated values. We split the data (by actions) into train/valida- tion/test splits of 80%/10%/10% respectively (See Table 7). G Model Training Details For training, we set our model size at 3 billion parameters us- ing the T5 encoder-decoder architecture (Raffel et al. 2020), and test the following hyperparameters: weight initializa- tion in {t5-3B, flan-t5-xl}, learning rate in {1e-4, 3e-4, 1e- 5, 3e-5}, and a dataset mixture of either {Generation, Rele- vance, Valence} or {Generation, Relevance, Valence, Expla- nation}. Because the explanation is post-hoc and of lesser interest to us than the other tasks, we choose the optimal set up on the validation set of the task mixture without the explanation task. We conduct a grid search and settle with learning rate at 3e-5 and a batch size of 32 with a mixture of all four tasks. For further analysis of the relationship of data mixture and model size with performance, see App. D.1. We train with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping and a batch size of 32, although we find that the majority of runs start to overfit after about 2 epochs. Training takes 19 hours per run on two A100 GPUs. We fix hyperparameters for the remainder of our experi- ments at the optimal hyperparameters: flan-t5-xl, 3e-5, and the mixture including explanations (which we find to as- sist generalization on the non-explanation tasks). For further analysis of the relationship of data mixture and model size with performance, see Section D.1. We refer to our default 3B trained model as KALEIDO. H System Details H.1 Algorithm See Algorithm 1. H.2 System Parameters For KALEIDOSYS 3B, we use these parameters for all exper- iments, which were found by maximizing RougeLSum f1- score VALUEPRISM-val. We also fix the number of genera- tions at 100 and take the top generations with beam search. Parameters determine the threshold for embedding cosine 23For the prompt used, please refer to Appendix M. Situation Claim Description Hacking a company to give them information about their flaws Preventing theft Firing Mike Information transparency: Providing information about flaws within a system might promote open- ness and transparency, which can be seen as a positive value. However, such information in the wrong hands could be dangerous. Personal Responsibility: Encouraging individuals to take steps to prevent theft can promote a sense of personal responsibility for one\u2019s belongings and actions. Duty to uphold company policies: If Mike\u2019s ac- tions violated company policies, management has a duty to take appropriate actions, which in this case may include firing. Claims for which all peo- ple surveyed agreed. 100% agreement 100% agreement 100% agreement Frowning at a friend Shooting a courier in the head who was responsible for the destruction of an en- tire city. Aiding a fugitive by provid- ing food or shelter in 1851 Pennsylvania. Respect: Not frowning at a friend if the situation doesn\u2019t warrant it could be a way of respecting their feelings. Duty to protect others: If the courier poses an on- going threat, there might be a duty to protect oth- ers from harm, which could include killing the courier. Compassion: Providing food and shelter for a fugitive showcases compassion towards those in need, regardless of their legal status. Claims with closest to aver- age rates of agreement. 81% agreement 82% agreement 82% agreement Tipping to your landlord Killing a potato to save a frog. Somali pirates need to feed their family so they ransom a ship Duty of gratitude: Tenants might have a moral duty to show gratitude to those who help or pro- vide a service, even if tipping is not customary for landlords. Respect for nature: Some might argue that inter- fering with nature by deciding which being lives or dies could disrupt the natural balance, thus showing a lack of respect for nature. Justice: The pirates are attempting to resolve eco- nomic inequality by gaining money, which they perceive as a just cause. Claims for which the least people surveyed agreed. 8% agreement 24% agreement 24% agreement Table 14: GPT-4 outputs from VALUEPRISM with most, average, and lowest levels of agreement based from CloudResearch study. similarity and the ngram overlap threshold for deduplicating, and the relevance score at which to drop poor outputs. There is a separete threshold for each category of value, right, and duty. 1 \"embed_threshold\": \"{\u2019Value\u2019: 0.53, \u2019Right\u2019: 0.63, \u2019Duty\u2019: 0.55}\", 2 \"ngram_threshold\": \"0.05\", 3 \"relevance_threshold\": \"{\u2019Value\u2019: 0.77, \u2019Right\u2019: 0.82, \u2019Duty\u2019: 0.9} I.1 Quality Annotation For this study, 3 crowdowkers for GPT-4 output for 3k sit- uations (10% of VALUEPRISM). See Figure 10 for the tem- plate used. For this study, note that we do not ask annotators to provide their own judgments of the situation, but merely to assess the relevance of the generations, which we expect to have much lower variation (e.g., someone may see how a value could be relevant for someone else while disagreeing"}, {"question": " What tasks were the Generation, Valence, and Explanation tasks cast as?", "answer": " Sequence-to-sequence (seq2seq) tasks", "ref_chunk": "desire would be prohibitive. As such, we follow prior work [CITE, add in from related work] and decide to use a LLM to create a syn- thetic dataset of values. We verify the quality (Section 4.1) and representativeness (Section 4.2) of the outputs using hu- man annotators. Values, Rights, and Duties Generation Given the set of 31k situations, we prompt GPT-4 (OpenAI 2023) to gener- ate relevant values, rights, and duties23, along with an open- text explanation. Given the output, the model also predicts whether the corresponding value, right, or duty supports (or justifies) the action, opposes (or condemns) the action, or could either support or oppose depending on the context or interpretation. The cost to generate the entire dataset was $1,043.80. While the data was generated in a batch manner to pro- duce all values and related data at once, we exploit the struc- ture of the generated data to cast the Generation, Valence, and Explanation tasks as sequence-to-sequence (seq2seq) tasks. The relevance task data is sampled contrastively, where positive examples are values generated by GPT-4 for the situation negative samples are drawn from other gener- ated values. We split the data (by actions) into train/valida- tion/test splits of 80%/10%/10% respectively (See Table 7). G Model Training Details For training, we set our model size at 3 billion parameters us- ing the T5 encoder-decoder architecture (Raffel et al. 2020), and test the following hyperparameters: weight initializa- tion in {t5-3B, flan-t5-xl}, learning rate in {1e-4, 3e-4, 1e- 5, 3e-5}, and a dataset mixture of either {Generation, Rele- vance, Valence} or {Generation, Relevance, Valence, Expla- nation}. Because the explanation is post-hoc and of lesser interest to us than the other tasks, we choose the optimal set up on the validation set of the task mixture without the explanation task. We conduct a grid search and settle with learning rate at 3e-5 and a batch size of 32 with a mixture of all four tasks. For further analysis of the relationship of data mixture and model size with performance, see App. D.1. We train with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping and a batch size of 32, although we find that the majority of runs start to overfit after about 2 epochs. Training takes 19 hours per run on two A100 GPUs. We fix hyperparameters for the remainder of our experi- ments at the optimal hyperparameters: flan-t5-xl, 3e-5, and the mixture including explanations (which we find to as- sist generalization on the non-explanation tasks). For further analysis of the relationship of data mixture and model size with performance, see Section D.1. We refer to our default 3B trained model as KALEIDO. H System Details H.1 Algorithm See Algorithm 1. H.2 System Parameters For KALEIDOSYS 3B, we use these parameters for all exper- iments, which were found by maximizing RougeLSum f1- score VALUEPRISM-val. We also fix the number of genera- tions at 100 and take the top generations with beam search. Parameters determine the threshold for embedding cosine 23For the prompt used, please refer to Appendix M. Situation Claim Description Hacking a company to give them information about their flaws Preventing theft Firing Mike Information transparency: Providing information about flaws within a system might promote open- ness and transparency, which can be seen as a positive value. However, such information in the wrong hands could be dangerous. Personal Responsibility: Encouraging individuals to take steps to prevent theft can promote a sense of personal responsibility for one\u2019s belongings and actions. Duty to uphold company policies: If Mike\u2019s ac- tions violated company policies, management has a duty to take appropriate actions, which in this case may include firing. Claims for which all peo- ple surveyed agreed. 100% agreement 100% agreement 100% agreement Frowning at a friend Shooting a courier in the head who was responsible for the destruction of an en- tire city. Aiding a fugitive by provid- ing food or shelter in 1851 Pennsylvania. Respect: Not frowning at a friend if the situation doesn\u2019t warrant it could be a way of respecting their feelings. Duty to protect others: If the courier poses an on- going threat, there might be a duty to protect oth- ers from harm, which could include killing the courier. Compassion: Providing food and shelter for a fugitive showcases compassion towards those in need, regardless of their legal status. Claims with closest to aver- age rates of agreement. 81% agreement 82% agreement 82% agreement Tipping to your landlord Killing a potato to save a frog. Somali pirates need to feed their family so they ransom a ship Duty of gratitude: Tenants might have a moral duty to show gratitude to those who help or pro- vide a service, even if tipping is not customary for landlords. Respect for nature: Some might argue that inter- fering with nature by deciding which being lives or dies could disrupt the natural balance, thus showing a lack of respect for nature. Justice: The pirates are attempting to resolve eco- nomic inequality by gaining money, which they perceive as a just cause. Claims for which the least people surveyed agreed. 8% agreement 24% agreement 24% agreement Table 14: GPT-4 outputs from VALUEPRISM with most, average, and lowest levels of agreement based from CloudResearch study. similarity and the ngram overlap threshold for deduplicating, and the relevance score at which to drop poor outputs. There is a separete threshold for each category of value, right, and duty. 1 \"embed_threshold\": \"{\u2019Value\u2019: 0.53, \u2019Right\u2019: 0.63, \u2019Duty\u2019: 0.55}\", 2 \"ngram_threshold\": \"0.05\", 3 \"relevance_threshold\": \"{\u2019Value\u2019: 0.77, \u2019Right\u2019: 0.82, \u2019Duty\u2019: 0.9} I.1 Quality Annotation For this study, 3 crowdowkers for GPT-4 output for 3k sit- uations (10% of VALUEPRISM). See Figure 10 for the tem- plate used. For this study, note that we do not ask annotators to provide their own judgments of the situation, but merely to assess the relevance of the generations, which we expect to have much lower variation (e.g., someone may see how a value could be relevant for someone else while disagreeing"}, {"question": " What was the cost to generate the entire dataset in the study?", "answer": " $1,043.80", "ref_chunk": "desire would be prohibitive. As such, we follow prior work [CITE, add in from related work] and decide to use a LLM to create a syn- thetic dataset of values. We verify the quality (Section 4.1) and representativeness (Section 4.2) of the outputs using hu- man annotators. Values, Rights, and Duties Generation Given the set of 31k situations, we prompt GPT-4 (OpenAI 2023) to gener- ate relevant values, rights, and duties23, along with an open- text explanation. Given the output, the model also predicts whether the corresponding value, right, or duty supports (or justifies) the action, opposes (or condemns) the action, or could either support or oppose depending on the context or interpretation. The cost to generate the entire dataset was $1,043.80. While the data was generated in a batch manner to pro- duce all values and related data at once, we exploit the struc- ture of the generated data to cast the Generation, Valence, and Explanation tasks as sequence-to-sequence (seq2seq) tasks. The relevance task data is sampled contrastively, where positive examples are values generated by GPT-4 for the situation negative samples are drawn from other gener- ated values. We split the data (by actions) into train/valida- tion/test splits of 80%/10%/10% respectively (See Table 7). G Model Training Details For training, we set our model size at 3 billion parameters us- ing the T5 encoder-decoder architecture (Raffel et al. 2020), and test the following hyperparameters: weight initializa- tion in {t5-3B, flan-t5-xl}, learning rate in {1e-4, 3e-4, 1e- 5, 3e-5}, and a dataset mixture of either {Generation, Rele- vance, Valence} or {Generation, Relevance, Valence, Expla- nation}. Because the explanation is post-hoc and of lesser interest to us than the other tasks, we choose the optimal set up on the validation set of the task mixture without the explanation task. We conduct a grid search and settle with learning rate at 3e-5 and a batch size of 32 with a mixture of all four tasks. For further analysis of the relationship of data mixture and model size with performance, see App. D.1. We train with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping and a batch size of 32, although we find that the majority of runs start to overfit after about 2 epochs. Training takes 19 hours per run on two A100 GPUs. We fix hyperparameters for the remainder of our experi- ments at the optimal hyperparameters: flan-t5-xl, 3e-5, and the mixture including explanations (which we find to as- sist generalization on the non-explanation tasks). For further analysis of the relationship of data mixture and model size with performance, see Section D.1. We refer to our default 3B trained model as KALEIDO. H System Details H.1 Algorithm See Algorithm 1. H.2 System Parameters For KALEIDOSYS 3B, we use these parameters for all exper- iments, which were found by maximizing RougeLSum f1- score VALUEPRISM-val. We also fix the number of genera- tions at 100 and take the top generations with beam search. Parameters determine the threshold for embedding cosine 23For the prompt used, please refer to Appendix M. Situation Claim Description Hacking a company to give them information about their flaws Preventing theft Firing Mike Information transparency: Providing information about flaws within a system might promote open- ness and transparency, which can be seen as a positive value. However, such information in the wrong hands could be dangerous. Personal Responsibility: Encouraging individuals to take steps to prevent theft can promote a sense of personal responsibility for one\u2019s belongings and actions. Duty to uphold company policies: If Mike\u2019s ac- tions violated company policies, management has a duty to take appropriate actions, which in this case may include firing. Claims for which all peo- ple surveyed agreed. 100% agreement 100% agreement 100% agreement Frowning at a friend Shooting a courier in the head who was responsible for the destruction of an en- tire city. Aiding a fugitive by provid- ing food or shelter in 1851 Pennsylvania. Respect: Not frowning at a friend if the situation doesn\u2019t warrant it could be a way of respecting their feelings. Duty to protect others: If the courier poses an on- going threat, there might be a duty to protect oth- ers from harm, which could include killing the courier. Compassion: Providing food and shelter for a fugitive showcases compassion towards those in need, regardless of their legal status. Claims with closest to aver- age rates of agreement. 81% agreement 82% agreement 82% agreement Tipping to your landlord Killing a potato to save a frog. Somali pirates need to feed their family so they ransom a ship Duty of gratitude: Tenants might have a moral duty to show gratitude to those who help or pro- vide a service, even if tipping is not customary for landlords. Respect for nature: Some might argue that inter- fering with nature by deciding which being lives or dies could disrupt the natural balance, thus showing a lack of respect for nature. Justice: The pirates are attempting to resolve eco- nomic inequality by gaining money, which they perceive as a just cause. Claims for which the least people surveyed agreed. 8% agreement 24% agreement 24% agreement Table 14: GPT-4 outputs from VALUEPRISM with most, average, and lowest levels of agreement based from CloudResearch study. similarity and the ngram overlap threshold for deduplicating, and the relevance score at which to drop poor outputs. There is a separete threshold for each category of value, right, and duty. 1 \"embed_threshold\": \"{\u2019Value\u2019: 0.53, \u2019Right\u2019: 0.63, \u2019Duty\u2019: 0.55}\", 2 \"ngram_threshold\": \"0.05\", 3 \"relevance_threshold\": \"{\u2019Value\u2019: 0.77, \u2019Right\u2019: 0.82, \u2019Duty\u2019: 0.9} I.1 Quality Annotation For this study, 3 crowdowkers for GPT-4 output for 3k sit- uations (10% of VALUEPRISM). See Figure 10 for the tem- plate used. For this study, note that we do not ask annotators to provide their own judgments of the situation, but merely to assess the relevance of the generations, which we expect to have much lower variation (e.g., someone may see how a value could be relevant for someone else while disagreeing"}, {"question": " How was the data split for training purposes?", "answer": " Train/validation/test splits of 80%/10%/10%", "ref_chunk": "desire would be prohibitive. As such, we follow prior work [CITE, add in from related work] and decide to use a LLM to create a syn- thetic dataset of values. We verify the quality (Section 4.1) and representativeness (Section 4.2) of the outputs using hu- man annotators. Values, Rights, and Duties Generation Given the set of 31k situations, we prompt GPT-4 (OpenAI 2023) to gener- ate relevant values, rights, and duties23, along with an open- text explanation. Given the output, the model also predicts whether the corresponding value, right, or duty supports (or justifies) the action, opposes (or condemns) the action, or could either support or oppose depending on the context or interpretation. The cost to generate the entire dataset was $1,043.80. While the data was generated in a batch manner to pro- duce all values and related data at once, we exploit the struc- ture of the generated data to cast the Generation, Valence, and Explanation tasks as sequence-to-sequence (seq2seq) tasks. The relevance task data is sampled contrastively, where positive examples are values generated by GPT-4 for the situation negative samples are drawn from other gener- ated values. We split the data (by actions) into train/valida- tion/test splits of 80%/10%/10% respectively (See Table 7). G Model Training Details For training, we set our model size at 3 billion parameters us- ing the T5 encoder-decoder architecture (Raffel et al. 2020), and test the following hyperparameters: weight initializa- tion in {t5-3B, flan-t5-xl}, learning rate in {1e-4, 3e-4, 1e- 5, 3e-5}, and a dataset mixture of either {Generation, Rele- vance, Valence} or {Generation, Relevance, Valence, Expla- nation}. Because the explanation is post-hoc and of lesser interest to us than the other tasks, we choose the optimal set up on the validation set of the task mixture without the explanation task. We conduct a grid search and settle with learning rate at 3e-5 and a batch size of 32 with a mixture of all four tasks. For further analysis of the relationship of data mixture and model size with performance, see App. D.1. We train with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping and a batch size of 32, although we find that the majority of runs start to overfit after about 2 epochs. Training takes 19 hours per run on two A100 GPUs. We fix hyperparameters for the remainder of our experi- ments at the optimal hyperparameters: flan-t5-xl, 3e-5, and the mixture including explanations (which we find to as- sist generalization on the non-explanation tasks). For further analysis of the relationship of data mixture and model size with performance, see Section D.1. We refer to our default 3B trained model as KALEIDO. H System Details H.1 Algorithm See Algorithm 1. H.2 System Parameters For KALEIDOSYS 3B, we use these parameters for all exper- iments, which were found by maximizing RougeLSum f1- score VALUEPRISM-val. We also fix the number of genera- tions at 100 and take the top generations with beam search. Parameters determine the threshold for embedding cosine 23For the prompt used, please refer to Appendix M. Situation Claim Description Hacking a company to give them information about their flaws Preventing theft Firing Mike Information transparency: Providing information about flaws within a system might promote open- ness and transparency, which can be seen as a positive value. However, such information in the wrong hands could be dangerous. Personal Responsibility: Encouraging individuals to take steps to prevent theft can promote a sense of personal responsibility for one\u2019s belongings and actions. Duty to uphold company policies: If Mike\u2019s ac- tions violated company policies, management has a duty to take appropriate actions, which in this case may include firing. Claims for which all peo- ple surveyed agreed. 100% agreement 100% agreement 100% agreement Frowning at a friend Shooting a courier in the head who was responsible for the destruction of an en- tire city. Aiding a fugitive by provid- ing food or shelter in 1851 Pennsylvania. Respect: Not frowning at a friend if the situation doesn\u2019t warrant it could be a way of respecting their feelings. Duty to protect others: If the courier poses an on- going threat, there might be a duty to protect oth- ers from harm, which could include killing the courier. Compassion: Providing food and shelter for a fugitive showcases compassion towards those in need, regardless of their legal status. Claims with closest to aver- age rates of agreement. 81% agreement 82% agreement 82% agreement Tipping to your landlord Killing a potato to save a frog. Somali pirates need to feed their family so they ransom a ship Duty of gratitude: Tenants might have a moral duty to show gratitude to those who help or pro- vide a service, even if tipping is not customary for landlords. Respect for nature: Some might argue that inter- fering with nature by deciding which being lives or dies could disrupt the natural balance, thus showing a lack of respect for nature. Justice: The pirates are attempting to resolve eco- nomic inequality by gaining money, which they perceive as a just cause. Claims for which the least people surveyed agreed. 8% agreement 24% agreement 24% agreement Table 14: GPT-4 outputs from VALUEPRISM with most, average, and lowest levels of agreement based from CloudResearch study. similarity and the ngram overlap threshold for deduplicating, and the relevance score at which to drop poor outputs. There is a separete threshold for each category of value, right, and duty. 1 \"embed_threshold\": \"{\u2019Value\u2019: 0.53, \u2019Right\u2019: 0.63, \u2019Duty\u2019: 0.55}\", 2 \"ngram_threshold\": \"0.05\", 3 \"relevance_threshold\": \"{\u2019Value\u2019: 0.77, \u2019Right\u2019: 0.82, \u2019Duty\u2019: 0.9} I.1 Quality Annotation For this study, 3 crowdowkers for GPT-4 output for 3k sit- uations (10% of VALUEPRISM). See Figure 10 for the tem- plate used. For this study, note that we do not ask annotators to provide their own judgments of the situation, but merely to assess the relevance of the generations, which we expect to have much lower variation (e.g., someone may see how a value could be relevant for someone else while disagreeing"}, {"question": " What hyperparameters were tested during training?", "answer": " Weight initialization, learning rate, and dataset mixture", "ref_chunk": "desire would be prohibitive. As such, we follow prior work [CITE, add in from related work] and decide to use a LLM to create a syn- thetic dataset of values. We verify the quality (Section 4.1) and representativeness (Section 4.2) of the outputs using hu- man annotators. Values, Rights, and Duties Generation Given the set of 31k situations, we prompt GPT-4 (OpenAI 2023) to gener- ate relevant values, rights, and duties23, along with an open- text explanation. Given the output, the model also predicts whether the corresponding value, right, or duty supports (or justifies) the action, opposes (or condemns) the action, or could either support or oppose depending on the context or interpretation. The cost to generate the entire dataset was $1,043.80. While the data was generated in a batch manner to pro- duce all values and related data at once, we exploit the struc- ture of the generated data to cast the Generation, Valence, and Explanation tasks as sequence-to-sequence (seq2seq) tasks. The relevance task data is sampled contrastively, where positive examples are values generated by GPT-4 for the situation negative samples are drawn from other gener- ated values. We split the data (by actions) into train/valida- tion/test splits of 80%/10%/10% respectively (See Table 7). G Model Training Details For training, we set our model size at 3 billion parameters us- ing the T5 encoder-decoder architecture (Raffel et al. 2020), and test the following hyperparameters: weight initializa- tion in {t5-3B, flan-t5-xl}, learning rate in {1e-4, 3e-4, 1e- 5, 3e-5}, and a dataset mixture of either {Generation, Rele- vance, Valence} or {Generation, Relevance, Valence, Expla- nation}. Because the explanation is post-hoc and of lesser interest to us than the other tasks, we choose the optimal set up on the validation set of the task mixture without the explanation task. We conduct a grid search and settle with learning rate at 3e-5 and a batch size of 32 with a mixture of all four tasks. For further analysis of the relationship of data mixture and model size with performance, see App. D.1. We train with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping and a batch size of 32, although we find that the majority of runs start to overfit after about 2 epochs. Training takes 19 hours per run on two A100 GPUs. We fix hyperparameters for the remainder of our experi- ments at the optimal hyperparameters: flan-t5-xl, 3e-5, and the mixture including explanations (which we find to as- sist generalization on the non-explanation tasks). For further analysis of the relationship of data mixture and model size with performance, see Section D.1. We refer to our default 3B trained model as KALEIDO. H System Details H.1 Algorithm See Algorithm 1. H.2 System Parameters For KALEIDOSYS 3B, we use these parameters for all exper- iments, which were found by maximizing RougeLSum f1- score VALUEPRISM-val. We also fix the number of genera- tions at 100 and take the top generations with beam search. Parameters determine the threshold for embedding cosine 23For the prompt used, please refer to Appendix M. Situation Claim Description Hacking a company to give them information about their flaws Preventing theft Firing Mike Information transparency: Providing information about flaws within a system might promote open- ness and transparency, which can be seen as a positive value. However, such information in the wrong hands could be dangerous. Personal Responsibility: Encouraging individuals to take steps to prevent theft can promote a sense of personal responsibility for one\u2019s belongings and actions. Duty to uphold company policies: If Mike\u2019s ac- tions violated company policies, management has a duty to take appropriate actions, which in this case may include firing. Claims for which all peo- ple surveyed agreed. 100% agreement 100% agreement 100% agreement Frowning at a friend Shooting a courier in the head who was responsible for the destruction of an en- tire city. Aiding a fugitive by provid- ing food or shelter in 1851 Pennsylvania. Respect: Not frowning at a friend if the situation doesn\u2019t warrant it could be a way of respecting their feelings. Duty to protect others: If the courier poses an on- going threat, there might be a duty to protect oth- ers from harm, which could include killing the courier. Compassion: Providing food and shelter for a fugitive showcases compassion towards those in need, regardless of their legal status. Claims with closest to aver- age rates of agreement. 81% agreement 82% agreement 82% agreement Tipping to your landlord Killing a potato to save a frog. Somali pirates need to feed their family so they ransom a ship Duty of gratitude: Tenants might have a moral duty to show gratitude to those who help or pro- vide a service, even if tipping is not customary for landlords. Respect for nature: Some might argue that inter- fering with nature by deciding which being lives or dies could disrupt the natural balance, thus showing a lack of respect for nature. Justice: The pirates are attempting to resolve eco- nomic inequality by gaining money, which they perceive as a just cause. Claims for which the least people surveyed agreed. 8% agreement 24% agreement 24% agreement Table 14: GPT-4 outputs from VALUEPRISM with most, average, and lowest levels of agreement based from CloudResearch study. similarity and the ngram overlap threshold for deduplicating, and the relevance score at which to drop poor outputs. There is a separete threshold for each category of value, right, and duty. 1 \"embed_threshold\": \"{\u2019Value\u2019: 0.53, \u2019Right\u2019: 0.63, \u2019Duty\u2019: 0.55}\", 2 \"ngram_threshold\": \"0.05\", 3 \"relevance_threshold\": \"{\u2019Value\u2019: 0.77, \u2019Right\u2019: 0.82, \u2019Duty\u2019: 0.9} I.1 Quality Annotation For this study, 3 crowdowkers for GPT-4 output for 3k sit- uations (10% of VALUEPRISM). See Figure 10 for the tem- plate used. For this study, note that we do not ask annotators to provide their own judgments of the situation, but merely to assess the relevance of the generations, which we expect to have much lower variation (e.g., someone may see how a value could be relevant for someone else while disagreeing"}, {"question": " How long did training take per run?", "answer": " 19 hours on two A100 GPUs", "ref_chunk": "desire would be prohibitive. As such, we follow prior work [CITE, add in from related work] and decide to use a LLM to create a syn- thetic dataset of values. We verify the quality (Section 4.1) and representativeness (Section 4.2) of the outputs using hu- man annotators. Values, Rights, and Duties Generation Given the set of 31k situations, we prompt GPT-4 (OpenAI 2023) to gener- ate relevant values, rights, and duties23, along with an open- text explanation. Given the output, the model also predicts whether the corresponding value, right, or duty supports (or justifies) the action, opposes (or condemns) the action, or could either support or oppose depending on the context or interpretation. The cost to generate the entire dataset was $1,043.80. While the data was generated in a batch manner to pro- duce all values and related data at once, we exploit the struc- ture of the generated data to cast the Generation, Valence, and Explanation tasks as sequence-to-sequence (seq2seq) tasks. The relevance task data is sampled contrastively, where positive examples are values generated by GPT-4 for the situation negative samples are drawn from other gener- ated values. We split the data (by actions) into train/valida- tion/test splits of 80%/10%/10% respectively (See Table 7). G Model Training Details For training, we set our model size at 3 billion parameters us- ing the T5 encoder-decoder architecture (Raffel et al. 2020), and test the following hyperparameters: weight initializa- tion in {t5-3B, flan-t5-xl}, learning rate in {1e-4, 3e-4, 1e- 5, 3e-5}, and a dataset mixture of either {Generation, Rele- vance, Valence} or {Generation, Relevance, Valence, Expla- nation}. Because the explanation is post-hoc and of lesser interest to us than the other tasks, we choose the optimal set up on the validation set of the task mixture without the explanation task. We conduct a grid search and settle with learning rate at 3e-5 and a batch size of 32 with a mixture of all four tasks. For further analysis of the relationship of data mixture and model size with performance, see App. D.1. We train with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping and a batch size of 32, although we find that the majority of runs start to overfit after about 2 epochs. Training takes 19 hours per run on two A100 GPUs. We fix hyperparameters for the remainder of our experi- ments at the optimal hyperparameters: flan-t5-xl, 3e-5, and the mixture including explanations (which we find to as- sist generalization on the non-explanation tasks). For further analysis of the relationship of data mixture and model size with performance, see Section D.1. We refer to our default 3B trained model as KALEIDO. H System Details H.1 Algorithm See Algorithm 1. H.2 System Parameters For KALEIDOSYS 3B, we use these parameters for all exper- iments, which were found by maximizing RougeLSum f1- score VALUEPRISM-val. We also fix the number of genera- tions at 100 and take the top generations with beam search. Parameters determine the threshold for embedding cosine 23For the prompt used, please refer to Appendix M. Situation Claim Description Hacking a company to give them information about their flaws Preventing theft Firing Mike Information transparency: Providing information about flaws within a system might promote open- ness and transparency, which can be seen as a positive value. However, such information in the wrong hands could be dangerous. Personal Responsibility: Encouraging individuals to take steps to prevent theft can promote a sense of personal responsibility for one\u2019s belongings and actions. Duty to uphold company policies: If Mike\u2019s ac- tions violated company policies, management has a duty to take appropriate actions, which in this case may include firing. Claims for which all peo- ple surveyed agreed. 100% agreement 100% agreement 100% agreement Frowning at a friend Shooting a courier in the head who was responsible for the destruction of an en- tire city. Aiding a fugitive by provid- ing food or shelter in 1851 Pennsylvania. Respect: Not frowning at a friend if the situation doesn\u2019t warrant it could be a way of respecting their feelings. Duty to protect others: If the courier poses an on- going threat, there might be a duty to protect oth- ers from harm, which could include killing the courier. Compassion: Providing food and shelter for a fugitive showcases compassion towards those in need, regardless of their legal status. Claims with closest to aver- age rates of agreement. 81% agreement 82% agreement 82% agreement Tipping to your landlord Killing a potato to save a frog. Somali pirates need to feed their family so they ransom a ship Duty of gratitude: Tenants might have a moral duty to show gratitude to those who help or pro- vide a service, even if tipping is not customary for landlords. Respect for nature: Some might argue that inter- fering with nature by deciding which being lives or dies could disrupt the natural balance, thus showing a lack of respect for nature. Justice: The pirates are attempting to resolve eco- nomic inequality by gaining money, which they perceive as a just cause. Claims for which the least people surveyed agreed. 8% agreement 24% agreement 24% agreement Table 14: GPT-4 outputs from VALUEPRISM with most, average, and lowest levels of agreement based from CloudResearch study. similarity and the ngram overlap threshold for deduplicating, and the relevance score at which to drop poor outputs. There is a separete threshold for each category of value, right, and duty. 1 \"embed_threshold\": \"{\u2019Value\u2019: 0.53, \u2019Right\u2019: 0.63, \u2019Duty\u2019: 0.55}\", 2 \"ngram_threshold\": \"0.05\", 3 \"relevance_threshold\": \"{\u2019Value\u2019: 0.77, \u2019Right\u2019: 0.82, \u2019Duty\u2019: 0.9} I.1 Quality Annotation For this study, 3 crowdowkers for GPT-4 output for 3k sit- uations (10% of VALUEPRISM). See Figure 10 for the tem- plate used. For this study, note that we do not ask annotators to provide their own judgments of the situation, but merely to assess the relevance of the generations, which we expect to have much lower variation (e.g., someone may see how a value could be relevant for someone else while disagreeing"}, {"question": " What was the default trained model referred to as?", "answer": " KALEIDO", "ref_chunk": "desire would be prohibitive. As such, we follow prior work [CITE, add in from related work] and decide to use a LLM to create a syn- thetic dataset of values. We verify the quality (Section 4.1) and representativeness (Section 4.2) of the outputs using hu- man annotators. Values, Rights, and Duties Generation Given the set of 31k situations, we prompt GPT-4 (OpenAI 2023) to gener- ate relevant values, rights, and duties23, along with an open- text explanation. Given the output, the model also predicts whether the corresponding value, right, or duty supports (or justifies) the action, opposes (or condemns) the action, or could either support or oppose depending on the context or interpretation. The cost to generate the entire dataset was $1,043.80. While the data was generated in a batch manner to pro- duce all values and related data at once, we exploit the struc- ture of the generated data to cast the Generation, Valence, and Explanation tasks as sequence-to-sequence (seq2seq) tasks. The relevance task data is sampled contrastively, where positive examples are values generated by GPT-4 for the situation negative samples are drawn from other gener- ated values. We split the data (by actions) into train/valida- tion/test splits of 80%/10%/10% respectively (See Table 7). G Model Training Details For training, we set our model size at 3 billion parameters us- ing the T5 encoder-decoder architecture (Raffel et al. 2020), and test the following hyperparameters: weight initializa- tion in {t5-3B, flan-t5-xl}, learning rate in {1e-4, 3e-4, 1e- 5, 3e-5}, and a dataset mixture of either {Generation, Rele- vance, Valence} or {Generation, Relevance, Valence, Expla- nation}. Because the explanation is post-hoc and of lesser interest to us than the other tasks, we choose the optimal set up on the validation set of the task mixture without the explanation task. We conduct a grid search and settle with learning rate at 3e-5 and a batch size of 32 with a mixture of all four tasks. For further analysis of the relationship of data mixture and model size with performance, see App. D.1. We train with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping and a batch size of 32, although we find that the majority of runs start to overfit after about 2 epochs. Training takes 19 hours per run on two A100 GPUs. We fix hyperparameters for the remainder of our experi- ments at the optimal hyperparameters: flan-t5-xl, 3e-5, and the mixture including explanations (which we find to as- sist generalization on the non-explanation tasks). For further analysis of the relationship of data mixture and model size with performance, see Section D.1. We refer to our default 3B trained model as KALEIDO. H System Details H.1 Algorithm See Algorithm 1. H.2 System Parameters For KALEIDOSYS 3B, we use these parameters for all exper- iments, which were found by maximizing RougeLSum f1- score VALUEPRISM-val. We also fix the number of genera- tions at 100 and take the top generations with beam search. Parameters determine the threshold for embedding cosine 23For the prompt used, please refer to Appendix M. Situation Claim Description Hacking a company to give them information about their flaws Preventing theft Firing Mike Information transparency: Providing information about flaws within a system might promote open- ness and transparency, which can be seen as a positive value. However, such information in the wrong hands could be dangerous. Personal Responsibility: Encouraging individuals to take steps to prevent theft can promote a sense of personal responsibility for one\u2019s belongings and actions. Duty to uphold company policies: If Mike\u2019s ac- tions violated company policies, management has a duty to take appropriate actions, which in this case may include firing. Claims for which all peo- ple surveyed agreed. 100% agreement 100% agreement 100% agreement Frowning at a friend Shooting a courier in the head who was responsible for the destruction of an en- tire city. Aiding a fugitive by provid- ing food or shelter in 1851 Pennsylvania. Respect: Not frowning at a friend if the situation doesn\u2019t warrant it could be a way of respecting their feelings. Duty to protect others: If the courier poses an on- going threat, there might be a duty to protect oth- ers from harm, which could include killing the courier. Compassion: Providing food and shelter for a fugitive showcases compassion towards those in need, regardless of their legal status. Claims with closest to aver- age rates of agreement. 81% agreement 82% agreement 82% agreement Tipping to your landlord Killing a potato to save a frog. Somali pirates need to feed their family so they ransom a ship Duty of gratitude: Tenants might have a moral duty to show gratitude to those who help or pro- vide a service, even if tipping is not customary for landlords. Respect for nature: Some might argue that inter- fering with nature by deciding which being lives or dies could disrupt the natural balance, thus showing a lack of respect for nature. Justice: The pirates are attempting to resolve eco- nomic inequality by gaining money, which they perceive as a just cause. Claims for which the least people surveyed agreed. 8% agreement 24% agreement 24% agreement Table 14: GPT-4 outputs from VALUEPRISM with most, average, and lowest levels of agreement based from CloudResearch study. similarity and the ngram overlap threshold for deduplicating, and the relevance score at which to drop poor outputs. There is a separete threshold for each category of value, right, and duty. 1 \"embed_threshold\": \"{\u2019Value\u2019: 0.53, \u2019Right\u2019: 0.63, \u2019Duty\u2019: 0.55}\", 2 \"ngram_threshold\": \"0.05\", 3 \"relevance_threshold\": \"{\u2019Value\u2019: 0.77, \u2019Right\u2019: 0.82, \u2019Duty\u2019: 0.9} I.1 Quality Annotation For this study, 3 crowdowkers for GPT-4 output for 3k sit- uations (10% of VALUEPRISM). See Figure 10 for the tem- plate used. For this study, note that we do not ask annotators to provide their own judgments of the situation, but merely to assess the relevance of the generations, which we expect to have much lower variation (e.g., someone may see how a value could be relevant for someone else while disagreeing"}, {"question": " How many crowdworkers were used for GPT-4 output annotation in the study?", "answer": " 3", "ref_chunk": "desire would be prohibitive. As such, we follow prior work [CITE, add in from related work] and decide to use a LLM to create a syn- thetic dataset of values. We verify the quality (Section 4.1) and representativeness (Section 4.2) of the outputs using hu- man annotators. Values, Rights, and Duties Generation Given the set of 31k situations, we prompt GPT-4 (OpenAI 2023) to gener- ate relevant values, rights, and duties23, along with an open- text explanation. Given the output, the model also predicts whether the corresponding value, right, or duty supports (or justifies) the action, opposes (or condemns) the action, or could either support or oppose depending on the context or interpretation. The cost to generate the entire dataset was $1,043.80. While the data was generated in a batch manner to pro- duce all values and related data at once, we exploit the struc- ture of the generated data to cast the Generation, Valence, and Explanation tasks as sequence-to-sequence (seq2seq) tasks. The relevance task data is sampled contrastively, where positive examples are values generated by GPT-4 for the situation negative samples are drawn from other gener- ated values. We split the data (by actions) into train/valida- tion/test splits of 80%/10%/10% respectively (See Table 7). G Model Training Details For training, we set our model size at 3 billion parameters us- ing the T5 encoder-decoder architecture (Raffel et al. 2020), and test the following hyperparameters: weight initializa- tion in {t5-3B, flan-t5-xl}, learning rate in {1e-4, 3e-4, 1e- 5, 3e-5}, and a dataset mixture of either {Generation, Rele- vance, Valence} or {Generation, Relevance, Valence, Expla- nation}. Because the explanation is post-hoc and of lesser interest to us than the other tasks, we choose the optimal set up on the validation set of the task mixture without the explanation task. We conduct a grid search and settle with learning rate at 3e-5 and a batch size of 32 with a mixture of all four tasks. For further analysis of the relationship of data mixture and model size with performance, see App. D.1. We train with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping and a batch size of 32, although we find that the majority of runs start to overfit after about 2 epochs. Training takes 19 hours per run on two A100 GPUs. We fix hyperparameters for the remainder of our experi- ments at the optimal hyperparameters: flan-t5-xl, 3e-5, and the mixture including explanations (which we find to as- sist generalization on the non-explanation tasks). For further analysis of the relationship of data mixture and model size with performance, see Section D.1. We refer to our default 3B trained model as KALEIDO. H System Details H.1 Algorithm See Algorithm 1. H.2 System Parameters For KALEIDOSYS 3B, we use these parameters for all exper- iments, which were found by maximizing RougeLSum f1- score VALUEPRISM-val. We also fix the number of genera- tions at 100 and take the top generations with beam search. Parameters determine the threshold for embedding cosine 23For the prompt used, please refer to Appendix M. Situation Claim Description Hacking a company to give them information about their flaws Preventing theft Firing Mike Information transparency: Providing information about flaws within a system might promote open- ness and transparency, which can be seen as a positive value. However, such information in the wrong hands could be dangerous. Personal Responsibility: Encouraging individuals to take steps to prevent theft can promote a sense of personal responsibility for one\u2019s belongings and actions. Duty to uphold company policies: If Mike\u2019s ac- tions violated company policies, management has a duty to take appropriate actions, which in this case may include firing. Claims for which all peo- ple surveyed agreed. 100% agreement 100% agreement 100% agreement Frowning at a friend Shooting a courier in the head who was responsible for the destruction of an en- tire city. Aiding a fugitive by provid- ing food or shelter in 1851 Pennsylvania. Respect: Not frowning at a friend if the situation doesn\u2019t warrant it could be a way of respecting their feelings. Duty to protect others: If the courier poses an on- going threat, there might be a duty to protect oth- ers from harm, which could include killing the courier. Compassion: Providing food and shelter for a fugitive showcases compassion towards those in need, regardless of their legal status. Claims with closest to aver- age rates of agreement. 81% agreement 82% agreement 82% agreement Tipping to your landlord Killing a potato to save a frog. Somali pirates need to feed their family so they ransom a ship Duty of gratitude: Tenants might have a moral duty to show gratitude to those who help or pro- vide a service, even if tipping is not customary for landlords. Respect for nature: Some might argue that inter- fering with nature by deciding which being lives or dies could disrupt the natural balance, thus showing a lack of respect for nature. Justice: The pirates are attempting to resolve eco- nomic inequality by gaining money, which they perceive as a just cause. Claims for which the least people surveyed agreed. 8% agreement 24% agreement 24% agreement Table 14: GPT-4 outputs from VALUEPRISM with most, average, and lowest levels of agreement based from CloudResearch study. similarity and the ngram overlap threshold for deduplicating, and the relevance score at which to drop poor outputs. There is a separete threshold for each category of value, right, and duty. 1 \"embed_threshold\": \"{\u2019Value\u2019: 0.53, \u2019Right\u2019: 0.63, \u2019Duty\u2019: 0.55}\", 2 \"ngram_threshold\": \"0.05\", 3 \"relevance_threshold\": \"{\u2019Value\u2019: 0.77, \u2019Right\u2019: 0.82, \u2019Duty\u2019: 0.9} I.1 Quality Annotation For this study, 3 crowdowkers for GPT-4 output for 3k sit- uations (10% of VALUEPRISM). See Figure 10 for the tem- plate used. For this study, note that we do not ask annotators to provide their own judgments of the situation, but merely to assess the relevance of the generations, which we expect to have much lower variation (e.g., someone may see how a value could be relevant for someone else while disagreeing"}], "doc_text": "desire would be prohibitive. As such, we follow prior work [CITE, add in from related work] and decide to use a LLM to create a syn- thetic dataset of values. We verify the quality (Section 4.1) and representativeness (Section 4.2) of the outputs using hu- man annotators. Values, Rights, and Duties Generation Given the set of 31k situations, we prompt GPT-4 (OpenAI 2023) to gener- ate relevant values, rights, and duties23, along with an open- text explanation. Given the output, the model also predicts whether the corresponding value, right, or duty supports (or justifies) the action, opposes (or condemns) the action, or could either support or oppose depending on the context or interpretation. The cost to generate the entire dataset was $1,043.80. While the data was generated in a batch manner to pro- duce all values and related data at once, we exploit the struc- ture of the generated data to cast the Generation, Valence, and Explanation tasks as sequence-to-sequence (seq2seq) tasks. The relevance task data is sampled contrastively, where positive examples are values generated by GPT-4 for the situation negative samples are drawn from other gener- ated values. We split the data (by actions) into train/valida- tion/test splits of 80%/10%/10% respectively (See Table 7). G Model Training Details For training, we set our model size at 3 billion parameters us- ing the T5 encoder-decoder architecture (Raffel et al. 2020), and test the following hyperparameters: weight initializa- tion in {t5-3B, flan-t5-xl}, learning rate in {1e-4, 3e-4, 1e- 5, 3e-5}, and a dataset mixture of either {Generation, Rele- vance, Valence} or {Generation, Relevance, Valence, Expla- nation}. Because the explanation is post-hoc and of lesser interest to us than the other tasks, we choose the optimal set up on the validation set of the task mixture without the explanation task. We conduct a grid search and settle with learning rate at 3e-5 and a batch size of 32 with a mixture of all four tasks. For further analysis of the relationship of data mixture and model size with performance, see App. D.1. We train with Huggingface\u2019s Trainer (Wolf et al. 2020) for 4 epochs with early stopping and a batch size of 32, although we find that the majority of runs start to overfit after about 2 epochs. Training takes 19 hours per run on two A100 GPUs. We fix hyperparameters for the remainder of our experi- ments at the optimal hyperparameters: flan-t5-xl, 3e-5, and the mixture including explanations (which we find to as- sist generalization on the non-explanation tasks). For further analysis of the relationship of data mixture and model size with performance, see Section D.1. We refer to our default 3B trained model as KALEIDO. H System Details H.1 Algorithm See Algorithm 1. H.2 System Parameters For KALEIDOSYS 3B, we use these parameters for all exper- iments, which were found by maximizing RougeLSum f1- score VALUEPRISM-val. We also fix the number of genera- tions at 100 and take the top generations with beam search. Parameters determine the threshold for embedding cosine 23For the prompt used, please refer to Appendix M. Situation Claim Description Hacking a company to give them information about their flaws Preventing theft Firing Mike Information transparency: Providing information about flaws within a system might promote open- ness and transparency, which can be seen as a positive value. However, such information in the wrong hands could be dangerous. Personal Responsibility: Encouraging individuals to take steps to prevent theft can promote a sense of personal responsibility for one\u2019s belongings and actions. Duty to uphold company policies: If Mike\u2019s ac- tions violated company policies, management has a duty to take appropriate actions, which in this case may include firing. Claims for which all peo- ple surveyed agreed. 100% agreement 100% agreement 100% agreement Frowning at a friend Shooting a courier in the head who was responsible for the destruction of an en- tire city. Aiding a fugitive by provid- ing food or shelter in 1851 Pennsylvania. Respect: Not frowning at a friend if the situation doesn\u2019t warrant it could be a way of respecting their feelings. Duty to protect others: If the courier poses an on- going threat, there might be a duty to protect oth- ers from harm, which could include killing the courier. Compassion: Providing food and shelter for a fugitive showcases compassion towards those in need, regardless of their legal status. Claims with closest to aver- age rates of agreement. 81% agreement 82% agreement 82% agreement Tipping to your landlord Killing a potato to save a frog. Somali pirates need to feed their family so they ransom a ship Duty of gratitude: Tenants might have a moral duty to show gratitude to those who help or pro- vide a service, even if tipping is not customary for landlords. Respect for nature: Some might argue that inter- fering with nature by deciding which being lives or dies could disrupt the natural balance, thus showing a lack of respect for nature. Justice: The pirates are attempting to resolve eco- nomic inequality by gaining money, which they perceive as a just cause. Claims for which the least people surveyed agreed. 8% agreement 24% agreement 24% agreement Table 14: GPT-4 outputs from VALUEPRISM with most, average, and lowest levels of agreement based from CloudResearch study. similarity and the ngram overlap threshold for deduplicating, and the relevance score at which to drop poor outputs. There is a separete threshold for each category of value, right, and duty. 1 \"embed_threshold\": \"{\u2019Value\u2019: 0.53, \u2019Right\u2019: 0.63, \u2019Duty\u2019: 0.55}\", 2 \"ngram_threshold\": \"0.05\", 3 \"relevance_threshold\": \"{\u2019Value\u2019: 0.77, \u2019Right\u2019: 0.82, \u2019Duty\u2019: 0.9} I.1 Quality Annotation For this study, 3 crowdowkers for GPT-4 output for 3k sit- uations (10% of VALUEPRISM). See Figure 10 for the tem- plate used. For this study, note that we do not ask annotators to provide their own judgments of the situation, but merely to assess the relevance of the generations, which we expect to have much lower variation (e.g., someone may see how a value could be relevant for someone else while disagreeing"}