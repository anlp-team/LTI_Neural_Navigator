{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/S._Welleck_Self-Refine:_Iterative_Refinement_with_Self-Feedback_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the maximum number of iterations allowed for the FEEDBACK-REFINE process?", "answer": " Up to a maximum of 4 iterations", "ref_chunk": "in Section 2. The FEEDBACK- REFINE iterations continue until the desired output quality or task-specific criterion is reached, up to a maximum of 4 iterations. To make our evaluation consistent across different models, we implemented both FEEDBACK and REFINE as few-shot prompts even with models that respond well to instructions, such as ChatGPT and GPT-4. Base LLMs Our main goal is to evaluate whether we can improve the performance of any strong base LLMs using SELF-REFINE. Therefore, we compare SELF-REFINE to the same base LLMs but without feedback-refine iterations. We used three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (OpenAI, 2023). For code-based tasks, we also experimented with CODEX (code-davinci-002). In all tasks, either GPT-3.5 or GPT-4 is the previous state-of-the-art.3 We used the same prompts from previous work when 3A comparison with other few-shot and fine-tuned approaches is provided in Appendix F 4 (3) (4) GPT-3.5 ChatGPT GPT-4 Task Base +SELF-REFINE Base +SELF-REFINE Base +SELF-REFINE 8.8 30.4 (\u219121.6) Sentiment Reversal 36.4 63.6 (\u219127.2) Dialogue Response 14.8 23.0 (\u21918.2) Code Optimization 37.4 51.3 (\u219113.9) Code Readability 64.1 64.1 (0) Math Reasoning 41.6 56.4 (\u219114.8) Acronym Generation Constrained Generation 28.0 37.0 (\u21919.0) 11.4 43.2 (\u219131.8) 40.1 59.9 (\u219119.8) 23.9 27.5 (\u21913.6) 27.7 63.1 (\u219135.4) 74.8 75.0 (\u21910.2) 27.2 37.2 (\u219110.0) 44.0 67.0 (\u219123.0) 3.8 36.2 (\u219132.4) 25.4 74.6 (\u219149.2) 27.3 36.0 (\u21918.7) 27.4 56.2 (\u219128.8) 92.9 93.1 (\u21910.2) 30.4 56.0 (\u219125.6) 15.0 45.0 (\u219130.0) Table 1: SELF-REFINE results on various tasks using GPT-3.5, ChatGPT, and GPT-4 as base LLM. SELF-REFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2. available (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as detailed in Appendix S. We use greedy decoding with a temperature of 0.7 for all setups. 3.2 Metrics We report three types of metrics: Task specific metric: When available, we use automated metrics from prior work (Math Reasoning: % solve rate; Code Optimization: % programs optimized; Constrained Gen: coverage %) Human-pref: In Dialogue Response Generation, Code Readability Improvement, Sentiment Reversal, and Acronym Generation, since no automated metrics are available, we perform a blind human A/B evaluation on a subset of the outputs to select the preferred output. Additional details are provided in Appendix C. GPT-4-pref: In addition to human-pref, we use GPT-4 as a proxy for human preference following prior work (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), and found high correlation (82% for Sentiment Reversal, 68% for Acronym Generation, and 71% for Dialogue Response Generation) with human-pref. For Code Readability Improvement, we prompt GPT- 4 to calculate fraction of the variables that are appropriately named given the context (e.g., x = [] \u2192 input_buffer = []). Additional details are provided in Appendix D. 3.3 Results Table 1 shows our main results: SELF-REFINE consistently improves over base models across all model sizes, and additionally outperforms the previous state-of-the-art across all tasks. For example, GPT-4+SELF-REFINE improves over the base GPT-4 by 8.7% (absolute) in Code Optimization, increasing optimization percentage from 27.3% to 36.0%. Confidence intervals are provided in Appendix J. For code-based tasks, we found similar trends when using CODEX; those results are included in Appendix F. One of the tasks in which we observe the highest gains compared to the base models is Constrained Generation, where the model is asked to generate a sentence containing up to 30 given concepts. We believe that this task benefits significantly from SELF-REFINE because there are more opportunities to miss some of the concepts on the first attempt, and thus SELF-REFINE allows the model to fix these mistakes subsequently. Further, this task has an extremely large number of reasonable outputs, and thus SELF-REFINE allows to better explore the space of possible outputs. In preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym Generation, SELF-REFINE leads to especially high gains. For example in Dialogue Response Generation, GPT-4 preference score improve by 49.2% \u2013 from 25.4% to 74.6%. Similarly, we see remarkable improvements in the other preference-based tasks across all models. The modest performance gains in Math Reasoning can be traced back to the inability to accurately identify whether there is any error. In math, errors can be nuanced and sometimes limited to a single line or incorrect operation. Besides, a consistent-looking reasoning chain can deceive LLMs to 5 think that \u201ceverything looks good\u201d (e.g., ChatGPT feedback for 94% instances is \u2019everything looks good\u2019). In Appendix H.1, we show that the gains with SELF-REFINE on Math Reasoning are much bigger (5%+) if an external source can identify if the current math answer is incorrect. Improvement is consistent across base LLMs sizes Generally, GPT-4+SELF-REFINE performs better than GPT-3.5+SELF-REFINE and ChatGPT+SELF-REFINE across all tasks, even in tasks where the initial base results of GPT-4 were lower than GPT-3.5 or ChatGPT. We thus believe that SELF-REFINE allows stronger models (such as GPT-4) to unlock their full potential, even in cases where this potential is not expressed in the standard, single-pass, output generation. Comparison to additional strong baselines is provided in Appendix F. 4 Analysis The three main steps of SELF-REFINE are FEEDBACK, REFINE, and repeating them iteratively. In this section, we perform additional experiments to analyze the importance of each of these steps. Task SELF-REFINE feedback Generic feedback No feedback Code Optimization Sentiment Reversal Acronym Generation 27.5 43.2 56.4 26.0 31.2 54.0 24.8 0 48.0 Table 2: Prompting to generate generic feedback (or having the model generate no feedback at all) leads to reduced scores, indicating the importance of the FEEDBACK step of SELF-REFINE. These experiments were performed with ChatGPT (Code Optimization and Sentiment Reversal) and GPT-3.5 (Acronym Generation), and metrics used are defined in Section 3.2. The impact of the feedback quality Feedback quality plays a crucial role in SELF-REFINE. To quantify its impact, we compare SELF-REFINE, which utilizes specific, actionable feedback, with two ablations: one using generic feedback and another without feedback (the model may still iteratively refine its generations, but is not explicitly"}, {"question": " Which models were used as base LLMs in the evaluation of SELF-REFINE?", "answer": " GPT-3.5, ChatGPT, and GPT-4", "ref_chunk": "in Section 2. The FEEDBACK- REFINE iterations continue until the desired output quality or task-specific criterion is reached, up to a maximum of 4 iterations. To make our evaluation consistent across different models, we implemented both FEEDBACK and REFINE as few-shot prompts even with models that respond well to instructions, such as ChatGPT and GPT-4. Base LLMs Our main goal is to evaluate whether we can improve the performance of any strong base LLMs using SELF-REFINE. Therefore, we compare SELF-REFINE to the same base LLMs but without feedback-refine iterations. We used three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (OpenAI, 2023). For code-based tasks, we also experimented with CODEX (code-davinci-002). In all tasks, either GPT-3.5 or GPT-4 is the previous state-of-the-art.3 We used the same prompts from previous work when 3A comparison with other few-shot and fine-tuned approaches is provided in Appendix F 4 (3) (4) GPT-3.5 ChatGPT GPT-4 Task Base +SELF-REFINE Base +SELF-REFINE Base +SELF-REFINE 8.8 30.4 (\u219121.6) Sentiment Reversal 36.4 63.6 (\u219127.2) Dialogue Response 14.8 23.0 (\u21918.2) Code Optimization 37.4 51.3 (\u219113.9) Code Readability 64.1 64.1 (0) Math Reasoning 41.6 56.4 (\u219114.8) Acronym Generation Constrained Generation 28.0 37.0 (\u21919.0) 11.4 43.2 (\u219131.8) 40.1 59.9 (\u219119.8) 23.9 27.5 (\u21913.6) 27.7 63.1 (\u219135.4) 74.8 75.0 (\u21910.2) 27.2 37.2 (\u219110.0) 44.0 67.0 (\u219123.0) 3.8 36.2 (\u219132.4) 25.4 74.6 (\u219149.2) 27.3 36.0 (\u21918.7) 27.4 56.2 (\u219128.8) 92.9 93.1 (\u21910.2) 30.4 56.0 (\u219125.6) 15.0 45.0 (\u219130.0) Table 1: SELF-REFINE results on various tasks using GPT-3.5, ChatGPT, and GPT-4 as base LLM. SELF-REFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2. available (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as detailed in Appendix S. We use greedy decoding with a temperature of 0.7 for all setups. 3.2 Metrics We report three types of metrics: Task specific metric: When available, we use automated metrics from prior work (Math Reasoning: % solve rate; Code Optimization: % programs optimized; Constrained Gen: coverage %) Human-pref: In Dialogue Response Generation, Code Readability Improvement, Sentiment Reversal, and Acronym Generation, since no automated metrics are available, we perform a blind human A/B evaluation on a subset of the outputs to select the preferred output. Additional details are provided in Appendix C. GPT-4-pref: In addition to human-pref, we use GPT-4 as a proxy for human preference following prior work (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), and found high correlation (82% for Sentiment Reversal, 68% for Acronym Generation, and 71% for Dialogue Response Generation) with human-pref. For Code Readability Improvement, we prompt GPT- 4 to calculate fraction of the variables that are appropriately named given the context (e.g., x = [] \u2192 input_buffer = []). Additional details are provided in Appendix D. 3.3 Results Table 1 shows our main results: SELF-REFINE consistently improves over base models across all model sizes, and additionally outperforms the previous state-of-the-art across all tasks. For example, GPT-4+SELF-REFINE improves over the base GPT-4 by 8.7% (absolute) in Code Optimization, increasing optimization percentage from 27.3% to 36.0%. Confidence intervals are provided in Appendix J. For code-based tasks, we found similar trends when using CODEX; those results are included in Appendix F. One of the tasks in which we observe the highest gains compared to the base models is Constrained Generation, where the model is asked to generate a sentence containing up to 30 given concepts. We believe that this task benefits significantly from SELF-REFINE because there are more opportunities to miss some of the concepts on the first attempt, and thus SELF-REFINE allows the model to fix these mistakes subsequently. Further, this task has an extremely large number of reasonable outputs, and thus SELF-REFINE allows to better explore the space of possible outputs. In preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym Generation, SELF-REFINE leads to especially high gains. For example in Dialogue Response Generation, GPT-4 preference score improve by 49.2% \u2013 from 25.4% to 74.6%. Similarly, we see remarkable improvements in the other preference-based tasks across all models. The modest performance gains in Math Reasoning can be traced back to the inability to accurately identify whether there is any error. In math, errors can be nuanced and sometimes limited to a single line or incorrect operation. Besides, a consistent-looking reasoning chain can deceive LLMs to 5 think that \u201ceverything looks good\u201d (e.g., ChatGPT feedback for 94% instances is \u2019everything looks good\u2019). In Appendix H.1, we show that the gains with SELF-REFINE on Math Reasoning are much bigger (5%+) if an external source can identify if the current math answer is incorrect. Improvement is consistent across base LLMs sizes Generally, GPT-4+SELF-REFINE performs better than GPT-3.5+SELF-REFINE and ChatGPT+SELF-REFINE across all tasks, even in tasks where the initial base results of GPT-4 were lower than GPT-3.5 or ChatGPT. We thus believe that SELF-REFINE allows stronger models (such as GPT-4) to unlock their full potential, even in cases where this potential is not expressed in the standard, single-pass, output generation. Comparison to additional strong baselines is provided in Appendix F. 4 Analysis The three main steps of SELF-REFINE are FEEDBACK, REFINE, and repeating them iteratively. In this section, we perform additional experiments to analyze the importance of each of these steps. Task SELF-REFINE feedback Generic feedback No feedback Code Optimization Sentiment Reversal Acronym Generation 27.5 43.2 56.4 26.0 31.2 54.0 24.8 0 48.0 Table 2: Prompting to generate generic feedback (or having the model generate no feedback at all) leads to reduced scores, indicating the importance of the FEEDBACK step of SELF-REFINE. These experiments were performed with ChatGPT (Code Optimization and Sentiment Reversal) and GPT-3.5 (Acronym Generation), and metrics used are defined in Section 3.2. The impact of the feedback quality Feedback quality plays a crucial role in SELF-REFINE. To quantify its impact, we compare SELF-REFINE, which utilizes specific, actionable feedback, with two ablations: one using generic feedback and another without feedback (the model may still iteratively refine its generations, but is not explicitly"}, {"question": " What is the main goal of evaluating SELF-REFINE in this context?", "answer": " To improve the performance of strong base LLMs", "ref_chunk": "in Section 2. The FEEDBACK- REFINE iterations continue until the desired output quality or task-specific criterion is reached, up to a maximum of 4 iterations. To make our evaluation consistent across different models, we implemented both FEEDBACK and REFINE as few-shot prompts even with models that respond well to instructions, such as ChatGPT and GPT-4. Base LLMs Our main goal is to evaluate whether we can improve the performance of any strong base LLMs using SELF-REFINE. Therefore, we compare SELF-REFINE to the same base LLMs but without feedback-refine iterations. We used three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (OpenAI, 2023). For code-based tasks, we also experimented with CODEX (code-davinci-002). In all tasks, either GPT-3.5 or GPT-4 is the previous state-of-the-art.3 We used the same prompts from previous work when 3A comparison with other few-shot and fine-tuned approaches is provided in Appendix F 4 (3) (4) GPT-3.5 ChatGPT GPT-4 Task Base +SELF-REFINE Base +SELF-REFINE Base +SELF-REFINE 8.8 30.4 (\u219121.6) Sentiment Reversal 36.4 63.6 (\u219127.2) Dialogue Response 14.8 23.0 (\u21918.2) Code Optimization 37.4 51.3 (\u219113.9) Code Readability 64.1 64.1 (0) Math Reasoning 41.6 56.4 (\u219114.8) Acronym Generation Constrained Generation 28.0 37.0 (\u21919.0) 11.4 43.2 (\u219131.8) 40.1 59.9 (\u219119.8) 23.9 27.5 (\u21913.6) 27.7 63.1 (\u219135.4) 74.8 75.0 (\u21910.2) 27.2 37.2 (\u219110.0) 44.0 67.0 (\u219123.0) 3.8 36.2 (\u219132.4) 25.4 74.6 (\u219149.2) 27.3 36.0 (\u21918.7) 27.4 56.2 (\u219128.8) 92.9 93.1 (\u21910.2) 30.4 56.0 (\u219125.6) 15.0 45.0 (\u219130.0) Table 1: SELF-REFINE results on various tasks using GPT-3.5, ChatGPT, and GPT-4 as base LLM. SELF-REFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2. available (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as detailed in Appendix S. We use greedy decoding with a temperature of 0.7 for all setups. 3.2 Metrics We report three types of metrics: Task specific metric: When available, we use automated metrics from prior work (Math Reasoning: % solve rate; Code Optimization: % programs optimized; Constrained Gen: coverage %) Human-pref: In Dialogue Response Generation, Code Readability Improvement, Sentiment Reversal, and Acronym Generation, since no automated metrics are available, we perform a blind human A/B evaluation on a subset of the outputs to select the preferred output. Additional details are provided in Appendix C. GPT-4-pref: In addition to human-pref, we use GPT-4 as a proxy for human preference following prior work (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), and found high correlation (82% for Sentiment Reversal, 68% for Acronym Generation, and 71% for Dialogue Response Generation) with human-pref. For Code Readability Improvement, we prompt GPT- 4 to calculate fraction of the variables that are appropriately named given the context (e.g., x = [] \u2192 input_buffer = []). Additional details are provided in Appendix D. 3.3 Results Table 1 shows our main results: SELF-REFINE consistently improves over base models across all model sizes, and additionally outperforms the previous state-of-the-art across all tasks. For example, GPT-4+SELF-REFINE improves over the base GPT-4 by 8.7% (absolute) in Code Optimization, increasing optimization percentage from 27.3% to 36.0%. Confidence intervals are provided in Appendix J. For code-based tasks, we found similar trends when using CODEX; those results are included in Appendix F. One of the tasks in which we observe the highest gains compared to the base models is Constrained Generation, where the model is asked to generate a sentence containing up to 30 given concepts. We believe that this task benefits significantly from SELF-REFINE because there are more opportunities to miss some of the concepts on the first attempt, and thus SELF-REFINE allows the model to fix these mistakes subsequently. Further, this task has an extremely large number of reasonable outputs, and thus SELF-REFINE allows to better explore the space of possible outputs. In preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym Generation, SELF-REFINE leads to especially high gains. For example in Dialogue Response Generation, GPT-4 preference score improve by 49.2% \u2013 from 25.4% to 74.6%. Similarly, we see remarkable improvements in the other preference-based tasks across all models. The modest performance gains in Math Reasoning can be traced back to the inability to accurately identify whether there is any error. In math, errors can be nuanced and sometimes limited to a single line or incorrect operation. Besides, a consistent-looking reasoning chain can deceive LLMs to 5 think that \u201ceverything looks good\u201d (e.g., ChatGPT feedback for 94% instances is \u2019everything looks good\u2019). In Appendix H.1, we show that the gains with SELF-REFINE on Math Reasoning are much bigger (5%+) if an external source can identify if the current math answer is incorrect. Improvement is consistent across base LLMs sizes Generally, GPT-4+SELF-REFINE performs better than GPT-3.5+SELF-REFINE and ChatGPT+SELF-REFINE across all tasks, even in tasks where the initial base results of GPT-4 were lower than GPT-3.5 or ChatGPT. We thus believe that SELF-REFINE allows stronger models (such as GPT-4) to unlock their full potential, even in cases where this potential is not expressed in the standard, single-pass, output generation. Comparison to additional strong baselines is provided in Appendix F. 4 Analysis The three main steps of SELF-REFINE are FEEDBACK, REFINE, and repeating them iteratively. In this section, we perform additional experiments to analyze the importance of each of these steps. Task SELF-REFINE feedback Generic feedback No feedback Code Optimization Sentiment Reversal Acronym Generation 27.5 43.2 56.4 26.0 31.2 54.0 24.8 0 48.0 Table 2: Prompting to generate generic feedback (or having the model generate no feedback at all) leads to reduced scores, indicating the importance of the FEEDBACK step of SELF-REFINE. These experiments were performed with ChatGPT (Code Optimization and Sentiment Reversal) and GPT-3.5 (Acronym Generation), and metrics used are defined in Section 3.2. The impact of the feedback quality Feedback quality plays a crucial role in SELF-REFINE. To quantify its impact, we compare SELF-REFINE, which utilizes specific, actionable feedback, with two ablations: one using generic feedback and another without feedback (the model may still iteratively refine its generations, but is not explicitly"}, {"question": " What type of decoding was used for all setups in the evaluation?", "answer": " Greedy decoding with a temperature of 0.7", "ref_chunk": "in Section 2. The FEEDBACK- REFINE iterations continue until the desired output quality or task-specific criterion is reached, up to a maximum of 4 iterations. To make our evaluation consistent across different models, we implemented both FEEDBACK and REFINE as few-shot prompts even with models that respond well to instructions, such as ChatGPT and GPT-4. Base LLMs Our main goal is to evaluate whether we can improve the performance of any strong base LLMs using SELF-REFINE. Therefore, we compare SELF-REFINE to the same base LLMs but without feedback-refine iterations. We used three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (OpenAI, 2023). For code-based tasks, we also experimented with CODEX (code-davinci-002). In all tasks, either GPT-3.5 or GPT-4 is the previous state-of-the-art.3 We used the same prompts from previous work when 3A comparison with other few-shot and fine-tuned approaches is provided in Appendix F 4 (3) (4) GPT-3.5 ChatGPT GPT-4 Task Base +SELF-REFINE Base +SELF-REFINE Base +SELF-REFINE 8.8 30.4 (\u219121.6) Sentiment Reversal 36.4 63.6 (\u219127.2) Dialogue Response 14.8 23.0 (\u21918.2) Code Optimization 37.4 51.3 (\u219113.9) Code Readability 64.1 64.1 (0) Math Reasoning 41.6 56.4 (\u219114.8) Acronym Generation Constrained Generation 28.0 37.0 (\u21919.0) 11.4 43.2 (\u219131.8) 40.1 59.9 (\u219119.8) 23.9 27.5 (\u21913.6) 27.7 63.1 (\u219135.4) 74.8 75.0 (\u21910.2) 27.2 37.2 (\u219110.0) 44.0 67.0 (\u219123.0) 3.8 36.2 (\u219132.4) 25.4 74.6 (\u219149.2) 27.3 36.0 (\u21918.7) 27.4 56.2 (\u219128.8) 92.9 93.1 (\u21910.2) 30.4 56.0 (\u219125.6) 15.0 45.0 (\u219130.0) Table 1: SELF-REFINE results on various tasks using GPT-3.5, ChatGPT, and GPT-4 as base LLM. SELF-REFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2. available (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as detailed in Appendix S. We use greedy decoding with a temperature of 0.7 for all setups. 3.2 Metrics We report three types of metrics: Task specific metric: When available, we use automated metrics from prior work (Math Reasoning: % solve rate; Code Optimization: % programs optimized; Constrained Gen: coverage %) Human-pref: In Dialogue Response Generation, Code Readability Improvement, Sentiment Reversal, and Acronym Generation, since no automated metrics are available, we perform a blind human A/B evaluation on a subset of the outputs to select the preferred output. Additional details are provided in Appendix C. GPT-4-pref: In addition to human-pref, we use GPT-4 as a proxy for human preference following prior work (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), and found high correlation (82% for Sentiment Reversal, 68% for Acronym Generation, and 71% for Dialogue Response Generation) with human-pref. For Code Readability Improvement, we prompt GPT- 4 to calculate fraction of the variables that are appropriately named given the context (e.g., x = [] \u2192 input_buffer = []). Additional details are provided in Appendix D. 3.3 Results Table 1 shows our main results: SELF-REFINE consistently improves over base models across all model sizes, and additionally outperforms the previous state-of-the-art across all tasks. For example, GPT-4+SELF-REFINE improves over the base GPT-4 by 8.7% (absolute) in Code Optimization, increasing optimization percentage from 27.3% to 36.0%. Confidence intervals are provided in Appendix J. For code-based tasks, we found similar trends when using CODEX; those results are included in Appendix F. One of the tasks in which we observe the highest gains compared to the base models is Constrained Generation, where the model is asked to generate a sentence containing up to 30 given concepts. We believe that this task benefits significantly from SELF-REFINE because there are more opportunities to miss some of the concepts on the first attempt, and thus SELF-REFINE allows the model to fix these mistakes subsequently. Further, this task has an extremely large number of reasonable outputs, and thus SELF-REFINE allows to better explore the space of possible outputs. In preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym Generation, SELF-REFINE leads to especially high gains. For example in Dialogue Response Generation, GPT-4 preference score improve by 49.2% \u2013 from 25.4% to 74.6%. Similarly, we see remarkable improvements in the other preference-based tasks across all models. The modest performance gains in Math Reasoning can be traced back to the inability to accurately identify whether there is any error. In math, errors can be nuanced and sometimes limited to a single line or incorrect operation. Besides, a consistent-looking reasoning chain can deceive LLMs to 5 think that \u201ceverything looks good\u201d (e.g., ChatGPT feedback for 94% instances is \u2019everything looks good\u2019). In Appendix H.1, we show that the gains with SELF-REFINE on Math Reasoning are much bigger (5%+) if an external source can identify if the current math answer is incorrect. Improvement is consistent across base LLMs sizes Generally, GPT-4+SELF-REFINE performs better than GPT-3.5+SELF-REFINE and ChatGPT+SELF-REFINE across all tasks, even in tasks where the initial base results of GPT-4 were lower than GPT-3.5 or ChatGPT. We thus believe that SELF-REFINE allows stronger models (such as GPT-4) to unlock their full potential, even in cases where this potential is not expressed in the standard, single-pass, output generation. Comparison to additional strong baselines is provided in Appendix F. 4 Analysis The three main steps of SELF-REFINE are FEEDBACK, REFINE, and repeating them iteratively. In this section, we perform additional experiments to analyze the importance of each of these steps. Task SELF-REFINE feedback Generic feedback No feedback Code Optimization Sentiment Reversal Acronym Generation 27.5 43.2 56.4 26.0 31.2 54.0 24.8 0 48.0 Table 2: Prompting to generate generic feedback (or having the model generate no feedback at all) leads to reduced scores, indicating the importance of the FEEDBACK step of SELF-REFINE. These experiments were performed with ChatGPT (Code Optimization and Sentiment Reversal) and GPT-3.5 (Acronym Generation), and metrics used are defined in Section 3.2. The impact of the feedback quality Feedback quality plays a crucial role in SELF-REFINE. To quantify its impact, we compare SELF-REFINE, which utilizes specific, actionable feedback, with two ablations: one using generic feedback and another without feedback (the model may still iteratively refine its generations, but is not explicitly"}, {"question": " What types of metrics were reported in the study?", "answer": " Task-specific metric, Human-pref, and GPT-4-pref", "ref_chunk": "in Section 2. The FEEDBACK- REFINE iterations continue until the desired output quality or task-specific criterion is reached, up to a maximum of 4 iterations. To make our evaluation consistent across different models, we implemented both FEEDBACK and REFINE as few-shot prompts even with models that respond well to instructions, such as ChatGPT and GPT-4. Base LLMs Our main goal is to evaluate whether we can improve the performance of any strong base LLMs using SELF-REFINE. Therefore, we compare SELF-REFINE to the same base LLMs but without feedback-refine iterations. We used three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (OpenAI, 2023). For code-based tasks, we also experimented with CODEX (code-davinci-002). In all tasks, either GPT-3.5 or GPT-4 is the previous state-of-the-art.3 We used the same prompts from previous work when 3A comparison with other few-shot and fine-tuned approaches is provided in Appendix F 4 (3) (4) GPT-3.5 ChatGPT GPT-4 Task Base +SELF-REFINE Base +SELF-REFINE Base +SELF-REFINE 8.8 30.4 (\u219121.6) Sentiment Reversal 36.4 63.6 (\u219127.2) Dialogue Response 14.8 23.0 (\u21918.2) Code Optimization 37.4 51.3 (\u219113.9) Code Readability 64.1 64.1 (0) Math Reasoning 41.6 56.4 (\u219114.8) Acronym Generation Constrained Generation 28.0 37.0 (\u21919.0) 11.4 43.2 (\u219131.8) 40.1 59.9 (\u219119.8) 23.9 27.5 (\u21913.6) 27.7 63.1 (\u219135.4) 74.8 75.0 (\u21910.2) 27.2 37.2 (\u219110.0) 44.0 67.0 (\u219123.0) 3.8 36.2 (\u219132.4) 25.4 74.6 (\u219149.2) 27.3 36.0 (\u21918.7) 27.4 56.2 (\u219128.8) 92.9 93.1 (\u21910.2) 30.4 56.0 (\u219125.6) 15.0 45.0 (\u219130.0) Table 1: SELF-REFINE results on various tasks using GPT-3.5, ChatGPT, and GPT-4 as base LLM. SELF-REFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2. available (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as detailed in Appendix S. We use greedy decoding with a temperature of 0.7 for all setups. 3.2 Metrics We report three types of metrics: Task specific metric: When available, we use automated metrics from prior work (Math Reasoning: % solve rate; Code Optimization: % programs optimized; Constrained Gen: coverage %) Human-pref: In Dialogue Response Generation, Code Readability Improvement, Sentiment Reversal, and Acronym Generation, since no automated metrics are available, we perform a blind human A/B evaluation on a subset of the outputs to select the preferred output. Additional details are provided in Appendix C. GPT-4-pref: In addition to human-pref, we use GPT-4 as a proxy for human preference following prior work (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), and found high correlation (82% for Sentiment Reversal, 68% for Acronym Generation, and 71% for Dialogue Response Generation) with human-pref. For Code Readability Improvement, we prompt GPT- 4 to calculate fraction of the variables that are appropriately named given the context (e.g., x = [] \u2192 input_buffer = []). Additional details are provided in Appendix D. 3.3 Results Table 1 shows our main results: SELF-REFINE consistently improves over base models across all model sizes, and additionally outperforms the previous state-of-the-art across all tasks. For example, GPT-4+SELF-REFINE improves over the base GPT-4 by 8.7% (absolute) in Code Optimization, increasing optimization percentage from 27.3% to 36.0%. Confidence intervals are provided in Appendix J. For code-based tasks, we found similar trends when using CODEX; those results are included in Appendix F. One of the tasks in which we observe the highest gains compared to the base models is Constrained Generation, where the model is asked to generate a sentence containing up to 30 given concepts. We believe that this task benefits significantly from SELF-REFINE because there are more opportunities to miss some of the concepts on the first attempt, and thus SELF-REFINE allows the model to fix these mistakes subsequently. Further, this task has an extremely large number of reasonable outputs, and thus SELF-REFINE allows to better explore the space of possible outputs. In preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym Generation, SELF-REFINE leads to especially high gains. For example in Dialogue Response Generation, GPT-4 preference score improve by 49.2% \u2013 from 25.4% to 74.6%. Similarly, we see remarkable improvements in the other preference-based tasks across all models. The modest performance gains in Math Reasoning can be traced back to the inability to accurately identify whether there is any error. In math, errors can be nuanced and sometimes limited to a single line or incorrect operation. Besides, a consistent-looking reasoning chain can deceive LLMs to 5 think that \u201ceverything looks good\u201d (e.g., ChatGPT feedback for 94% instances is \u2019everything looks good\u2019). In Appendix H.1, we show that the gains with SELF-REFINE on Math Reasoning are much bigger (5%+) if an external source can identify if the current math answer is incorrect. Improvement is consistent across base LLMs sizes Generally, GPT-4+SELF-REFINE performs better than GPT-3.5+SELF-REFINE and ChatGPT+SELF-REFINE across all tasks, even in tasks where the initial base results of GPT-4 were lower than GPT-3.5 or ChatGPT. We thus believe that SELF-REFINE allows stronger models (such as GPT-4) to unlock their full potential, even in cases where this potential is not expressed in the standard, single-pass, output generation. Comparison to additional strong baselines is provided in Appendix F. 4 Analysis The three main steps of SELF-REFINE are FEEDBACK, REFINE, and repeating them iteratively. In this section, we perform additional experiments to analyze the importance of each of these steps. Task SELF-REFINE feedback Generic feedback No feedback Code Optimization Sentiment Reversal Acronym Generation 27.5 43.2 56.4 26.0 31.2 54.0 24.8 0 48.0 Table 2: Prompting to generate generic feedback (or having the model generate no feedback at all) leads to reduced scores, indicating the importance of the FEEDBACK step of SELF-REFINE. These experiments were performed with ChatGPT (Code Optimization and Sentiment Reversal) and GPT-3.5 (Acronym Generation), and metrics used are defined in Section 3.2. The impact of the feedback quality Feedback quality plays a crucial role in SELF-REFINE. To quantify its impact, we compare SELF-REFINE, which utilizes specific, actionable feedback, with two ablations: one using generic feedback and another without feedback (the model may still iteratively refine its generations, but is not explicitly"}, {"question": " According to the results, does SELF-REFINE consistently improve over base models?", "answer": " Yes, SELF-REFINE consistently improves over base models", "ref_chunk": "in Section 2. The FEEDBACK- REFINE iterations continue until the desired output quality or task-specific criterion is reached, up to a maximum of 4 iterations. To make our evaluation consistent across different models, we implemented both FEEDBACK and REFINE as few-shot prompts even with models that respond well to instructions, such as ChatGPT and GPT-4. Base LLMs Our main goal is to evaluate whether we can improve the performance of any strong base LLMs using SELF-REFINE. Therefore, we compare SELF-REFINE to the same base LLMs but without feedback-refine iterations. We used three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (OpenAI, 2023). For code-based tasks, we also experimented with CODEX (code-davinci-002). In all tasks, either GPT-3.5 or GPT-4 is the previous state-of-the-art.3 We used the same prompts from previous work when 3A comparison with other few-shot and fine-tuned approaches is provided in Appendix F 4 (3) (4) GPT-3.5 ChatGPT GPT-4 Task Base +SELF-REFINE Base +SELF-REFINE Base +SELF-REFINE 8.8 30.4 (\u219121.6) Sentiment Reversal 36.4 63.6 (\u219127.2) Dialogue Response 14.8 23.0 (\u21918.2) Code Optimization 37.4 51.3 (\u219113.9) Code Readability 64.1 64.1 (0) Math Reasoning 41.6 56.4 (\u219114.8) Acronym Generation Constrained Generation 28.0 37.0 (\u21919.0) 11.4 43.2 (\u219131.8) 40.1 59.9 (\u219119.8) 23.9 27.5 (\u21913.6) 27.7 63.1 (\u219135.4) 74.8 75.0 (\u21910.2) 27.2 37.2 (\u219110.0) 44.0 67.0 (\u219123.0) 3.8 36.2 (\u219132.4) 25.4 74.6 (\u219149.2) 27.3 36.0 (\u21918.7) 27.4 56.2 (\u219128.8) 92.9 93.1 (\u21910.2) 30.4 56.0 (\u219125.6) 15.0 45.0 (\u219130.0) Table 1: SELF-REFINE results on various tasks using GPT-3.5, ChatGPT, and GPT-4 as base LLM. SELF-REFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2. available (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as detailed in Appendix S. We use greedy decoding with a temperature of 0.7 for all setups. 3.2 Metrics We report three types of metrics: Task specific metric: When available, we use automated metrics from prior work (Math Reasoning: % solve rate; Code Optimization: % programs optimized; Constrained Gen: coverage %) Human-pref: In Dialogue Response Generation, Code Readability Improvement, Sentiment Reversal, and Acronym Generation, since no automated metrics are available, we perform a blind human A/B evaluation on a subset of the outputs to select the preferred output. Additional details are provided in Appendix C. GPT-4-pref: In addition to human-pref, we use GPT-4 as a proxy for human preference following prior work (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), and found high correlation (82% for Sentiment Reversal, 68% for Acronym Generation, and 71% for Dialogue Response Generation) with human-pref. For Code Readability Improvement, we prompt GPT- 4 to calculate fraction of the variables that are appropriately named given the context (e.g., x = [] \u2192 input_buffer = []). Additional details are provided in Appendix D. 3.3 Results Table 1 shows our main results: SELF-REFINE consistently improves over base models across all model sizes, and additionally outperforms the previous state-of-the-art across all tasks. For example, GPT-4+SELF-REFINE improves over the base GPT-4 by 8.7% (absolute) in Code Optimization, increasing optimization percentage from 27.3% to 36.0%. Confidence intervals are provided in Appendix J. For code-based tasks, we found similar trends when using CODEX; those results are included in Appendix F. One of the tasks in which we observe the highest gains compared to the base models is Constrained Generation, where the model is asked to generate a sentence containing up to 30 given concepts. We believe that this task benefits significantly from SELF-REFINE because there are more opportunities to miss some of the concepts on the first attempt, and thus SELF-REFINE allows the model to fix these mistakes subsequently. Further, this task has an extremely large number of reasonable outputs, and thus SELF-REFINE allows to better explore the space of possible outputs. In preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym Generation, SELF-REFINE leads to especially high gains. For example in Dialogue Response Generation, GPT-4 preference score improve by 49.2% \u2013 from 25.4% to 74.6%. Similarly, we see remarkable improvements in the other preference-based tasks across all models. The modest performance gains in Math Reasoning can be traced back to the inability to accurately identify whether there is any error. In math, errors can be nuanced and sometimes limited to a single line or incorrect operation. Besides, a consistent-looking reasoning chain can deceive LLMs to 5 think that \u201ceverything looks good\u201d (e.g., ChatGPT feedback for 94% instances is \u2019everything looks good\u2019). In Appendix H.1, we show that the gains with SELF-REFINE on Math Reasoning are much bigger (5%+) if an external source can identify if the current math answer is incorrect. Improvement is consistent across base LLMs sizes Generally, GPT-4+SELF-REFINE performs better than GPT-3.5+SELF-REFINE and ChatGPT+SELF-REFINE across all tasks, even in tasks where the initial base results of GPT-4 were lower than GPT-3.5 or ChatGPT. We thus believe that SELF-REFINE allows stronger models (such as GPT-4) to unlock their full potential, even in cases where this potential is not expressed in the standard, single-pass, output generation. Comparison to additional strong baselines is provided in Appendix F. 4 Analysis The three main steps of SELF-REFINE are FEEDBACK, REFINE, and repeating them iteratively. In this section, we perform additional experiments to analyze the importance of each of these steps. Task SELF-REFINE feedback Generic feedback No feedback Code Optimization Sentiment Reversal Acronym Generation 27.5 43.2 56.4 26.0 31.2 54.0 24.8 0 48.0 Table 2: Prompting to generate generic feedback (or having the model generate no feedback at all) leads to reduced scores, indicating the importance of the FEEDBACK step of SELF-REFINE. These experiments were performed with ChatGPT (Code Optimization and Sentiment Reversal) and GPT-3.5 (Acronym Generation), and metrics used are defined in Section 3.2. The impact of the feedback quality Feedback quality plays a crucial role in SELF-REFINE. To quantify its impact, we compare SELF-REFINE, which utilizes specific, actionable feedback, with two ablations: one using generic feedback and another without feedback (the model may still iteratively refine its generations, but is not explicitly"}, {"question": " In which task did SELF-REFINE show the highest gains compared to the base models?", "answer": " Constrained Generation", "ref_chunk": "in Section 2. The FEEDBACK- REFINE iterations continue until the desired output quality or task-specific criterion is reached, up to a maximum of 4 iterations. To make our evaluation consistent across different models, we implemented both FEEDBACK and REFINE as few-shot prompts even with models that respond well to instructions, such as ChatGPT and GPT-4. Base LLMs Our main goal is to evaluate whether we can improve the performance of any strong base LLMs using SELF-REFINE. Therefore, we compare SELF-REFINE to the same base LLMs but without feedback-refine iterations. We used three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (OpenAI, 2023). For code-based tasks, we also experimented with CODEX (code-davinci-002). In all tasks, either GPT-3.5 or GPT-4 is the previous state-of-the-art.3 We used the same prompts from previous work when 3A comparison with other few-shot and fine-tuned approaches is provided in Appendix F 4 (3) (4) GPT-3.5 ChatGPT GPT-4 Task Base +SELF-REFINE Base +SELF-REFINE Base +SELF-REFINE 8.8 30.4 (\u219121.6) Sentiment Reversal 36.4 63.6 (\u219127.2) Dialogue Response 14.8 23.0 (\u21918.2) Code Optimization 37.4 51.3 (\u219113.9) Code Readability 64.1 64.1 (0) Math Reasoning 41.6 56.4 (\u219114.8) Acronym Generation Constrained Generation 28.0 37.0 (\u21919.0) 11.4 43.2 (\u219131.8) 40.1 59.9 (\u219119.8) 23.9 27.5 (\u21913.6) 27.7 63.1 (\u219135.4) 74.8 75.0 (\u21910.2) 27.2 37.2 (\u219110.0) 44.0 67.0 (\u219123.0) 3.8 36.2 (\u219132.4) 25.4 74.6 (\u219149.2) 27.3 36.0 (\u21918.7) 27.4 56.2 (\u219128.8) 92.9 93.1 (\u21910.2) 30.4 56.0 (\u219125.6) 15.0 45.0 (\u219130.0) Table 1: SELF-REFINE results on various tasks using GPT-3.5, ChatGPT, and GPT-4 as base LLM. SELF-REFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2. available (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as detailed in Appendix S. We use greedy decoding with a temperature of 0.7 for all setups. 3.2 Metrics We report three types of metrics: Task specific metric: When available, we use automated metrics from prior work (Math Reasoning: % solve rate; Code Optimization: % programs optimized; Constrained Gen: coverage %) Human-pref: In Dialogue Response Generation, Code Readability Improvement, Sentiment Reversal, and Acronym Generation, since no automated metrics are available, we perform a blind human A/B evaluation on a subset of the outputs to select the preferred output. Additional details are provided in Appendix C. GPT-4-pref: In addition to human-pref, we use GPT-4 as a proxy for human preference following prior work (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), and found high correlation (82% for Sentiment Reversal, 68% for Acronym Generation, and 71% for Dialogue Response Generation) with human-pref. For Code Readability Improvement, we prompt GPT- 4 to calculate fraction of the variables that are appropriately named given the context (e.g., x = [] \u2192 input_buffer = []). Additional details are provided in Appendix D. 3.3 Results Table 1 shows our main results: SELF-REFINE consistently improves over base models across all model sizes, and additionally outperforms the previous state-of-the-art across all tasks. For example, GPT-4+SELF-REFINE improves over the base GPT-4 by 8.7% (absolute) in Code Optimization, increasing optimization percentage from 27.3% to 36.0%. Confidence intervals are provided in Appendix J. For code-based tasks, we found similar trends when using CODEX; those results are included in Appendix F. One of the tasks in which we observe the highest gains compared to the base models is Constrained Generation, where the model is asked to generate a sentence containing up to 30 given concepts. We believe that this task benefits significantly from SELF-REFINE because there are more opportunities to miss some of the concepts on the first attempt, and thus SELF-REFINE allows the model to fix these mistakes subsequently. Further, this task has an extremely large number of reasonable outputs, and thus SELF-REFINE allows to better explore the space of possible outputs. In preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym Generation, SELF-REFINE leads to especially high gains. For example in Dialogue Response Generation, GPT-4 preference score improve by 49.2% \u2013 from 25.4% to 74.6%. Similarly, we see remarkable improvements in the other preference-based tasks across all models. The modest performance gains in Math Reasoning can be traced back to the inability to accurately identify whether there is any error. In math, errors can be nuanced and sometimes limited to a single line or incorrect operation. Besides, a consistent-looking reasoning chain can deceive LLMs to 5 think that \u201ceverything looks good\u201d (e.g., ChatGPT feedback for 94% instances is \u2019everything looks good\u2019). In Appendix H.1, we show that the gains with SELF-REFINE on Math Reasoning are much bigger (5%+) if an external source can identify if the current math answer is incorrect. Improvement is consistent across base LLMs sizes Generally, GPT-4+SELF-REFINE performs better than GPT-3.5+SELF-REFINE and ChatGPT+SELF-REFINE across all tasks, even in tasks where the initial base results of GPT-4 were lower than GPT-3.5 or ChatGPT. We thus believe that SELF-REFINE allows stronger models (such as GPT-4) to unlock their full potential, even in cases where this potential is not expressed in the standard, single-pass, output generation. Comparison to additional strong baselines is provided in Appendix F. 4 Analysis The three main steps of SELF-REFINE are FEEDBACK, REFINE, and repeating them iteratively. In this section, we perform additional experiments to analyze the importance of each of these steps. Task SELF-REFINE feedback Generic feedback No feedback Code Optimization Sentiment Reversal Acronym Generation 27.5 43.2 56.4 26.0 31.2 54.0 24.8 0 48.0 Table 2: Prompting to generate generic feedback (or having the model generate no feedback at all) leads to reduced scores, indicating the importance of the FEEDBACK step of SELF-REFINE. These experiments were performed with ChatGPT (Code Optimization and Sentiment Reversal) and GPT-3.5 (Acronym Generation), and metrics used are defined in Section 3.2. The impact of the feedback quality Feedback quality plays a crucial role in SELF-REFINE. To quantify its impact, we compare SELF-REFINE, which utilizes specific, actionable feedback, with two ablations: one using generic feedback and another without feedback (the model may still iteratively refine its generations, but is not explicitly"}, {"question": " Why does the text suggest that SELF-REFINE is beneficial for the Constrained Generation task?", "answer": " SELF-REFINE allows the model to fix mistakes and explore a larger space of possible outputs", "ref_chunk": "in Section 2. The FEEDBACK- REFINE iterations continue until the desired output quality or task-specific criterion is reached, up to a maximum of 4 iterations. To make our evaluation consistent across different models, we implemented both FEEDBACK and REFINE as few-shot prompts even with models that respond well to instructions, such as ChatGPT and GPT-4. Base LLMs Our main goal is to evaluate whether we can improve the performance of any strong base LLMs using SELF-REFINE. Therefore, we compare SELF-REFINE to the same base LLMs but without feedback-refine iterations. We used three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (OpenAI, 2023). For code-based tasks, we also experimented with CODEX (code-davinci-002). In all tasks, either GPT-3.5 or GPT-4 is the previous state-of-the-art.3 We used the same prompts from previous work when 3A comparison with other few-shot and fine-tuned approaches is provided in Appendix F 4 (3) (4) GPT-3.5 ChatGPT GPT-4 Task Base +SELF-REFINE Base +SELF-REFINE Base +SELF-REFINE 8.8 30.4 (\u219121.6) Sentiment Reversal 36.4 63.6 (\u219127.2) Dialogue Response 14.8 23.0 (\u21918.2) Code Optimization 37.4 51.3 (\u219113.9) Code Readability 64.1 64.1 (0) Math Reasoning 41.6 56.4 (\u219114.8) Acronym Generation Constrained Generation 28.0 37.0 (\u21919.0) 11.4 43.2 (\u219131.8) 40.1 59.9 (\u219119.8) 23.9 27.5 (\u21913.6) 27.7 63.1 (\u219135.4) 74.8 75.0 (\u21910.2) 27.2 37.2 (\u219110.0) 44.0 67.0 (\u219123.0) 3.8 36.2 (\u219132.4) 25.4 74.6 (\u219149.2) 27.3 36.0 (\u21918.7) 27.4 56.2 (\u219128.8) 92.9 93.1 (\u21910.2) 30.4 56.0 (\u219125.6) 15.0 45.0 (\u219130.0) Table 1: SELF-REFINE results on various tasks using GPT-3.5, ChatGPT, and GPT-4 as base LLM. SELF-REFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2. available (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as detailed in Appendix S. We use greedy decoding with a temperature of 0.7 for all setups. 3.2 Metrics We report three types of metrics: Task specific metric: When available, we use automated metrics from prior work (Math Reasoning: % solve rate; Code Optimization: % programs optimized; Constrained Gen: coverage %) Human-pref: In Dialogue Response Generation, Code Readability Improvement, Sentiment Reversal, and Acronym Generation, since no automated metrics are available, we perform a blind human A/B evaluation on a subset of the outputs to select the preferred output. Additional details are provided in Appendix C. GPT-4-pref: In addition to human-pref, we use GPT-4 as a proxy for human preference following prior work (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), and found high correlation (82% for Sentiment Reversal, 68% for Acronym Generation, and 71% for Dialogue Response Generation) with human-pref. For Code Readability Improvement, we prompt GPT- 4 to calculate fraction of the variables that are appropriately named given the context (e.g., x = [] \u2192 input_buffer = []). Additional details are provided in Appendix D. 3.3 Results Table 1 shows our main results: SELF-REFINE consistently improves over base models across all model sizes, and additionally outperforms the previous state-of-the-art across all tasks. For example, GPT-4+SELF-REFINE improves over the base GPT-4 by 8.7% (absolute) in Code Optimization, increasing optimization percentage from 27.3% to 36.0%. Confidence intervals are provided in Appendix J. For code-based tasks, we found similar trends when using CODEX; those results are included in Appendix F. One of the tasks in which we observe the highest gains compared to the base models is Constrained Generation, where the model is asked to generate a sentence containing up to 30 given concepts. We believe that this task benefits significantly from SELF-REFINE because there are more opportunities to miss some of the concepts on the first attempt, and thus SELF-REFINE allows the model to fix these mistakes subsequently. Further, this task has an extremely large number of reasonable outputs, and thus SELF-REFINE allows to better explore the space of possible outputs. In preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym Generation, SELF-REFINE leads to especially high gains. For example in Dialogue Response Generation, GPT-4 preference score improve by 49.2% \u2013 from 25.4% to 74.6%. Similarly, we see remarkable improvements in the other preference-based tasks across all models. The modest performance gains in Math Reasoning can be traced back to the inability to accurately identify whether there is any error. In math, errors can be nuanced and sometimes limited to a single line or incorrect operation. Besides, a consistent-looking reasoning chain can deceive LLMs to 5 think that \u201ceverything looks good\u201d (e.g., ChatGPT feedback for 94% instances is \u2019everything looks good\u2019). In Appendix H.1, we show that the gains with SELF-REFINE on Math Reasoning are much bigger (5%+) if an external source can identify if the current math answer is incorrect. Improvement is consistent across base LLMs sizes Generally, GPT-4+SELF-REFINE performs better than GPT-3.5+SELF-REFINE and ChatGPT+SELF-REFINE across all tasks, even in tasks where the initial base results of GPT-4 were lower than GPT-3.5 or ChatGPT. We thus believe that SELF-REFINE allows stronger models (such as GPT-4) to unlock their full potential, even in cases where this potential is not expressed in the standard, single-pass, output generation. Comparison to additional strong baselines is provided in Appendix F. 4 Analysis The three main steps of SELF-REFINE are FEEDBACK, REFINE, and repeating them iteratively. In this section, we perform additional experiments to analyze the importance of each of these steps. Task SELF-REFINE feedback Generic feedback No feedback Code Optimization Sentiment Reversal Acronym Generation 27.5 43.2 56.4 26.0 31.2 54.0 24.8 0 48.0 Table 2: Prompting to generate generic feedback (or having the model generate no feedback at all) leads to reduced scores, indicating the importance of the FEEDBACK step of SELF-REFINE. These experiments were performed with ChatGPT (Code Optimization and Sentiment Reversal) and GPT-3.5 (Acronym Generation), and metrics used are defined in Section 3.2. The impact of the feedback quality Feedback quality plays a crucial role in SELF-REFINE. To quantify its impact, we compare SELF-REFINE, which utilizes specific, actionable feedback, with two ablations: one using generic feedback and another without feedback (the model may still iteratively refine its generations, but is not explicitly"}, {"question": " How did the performance gains vary in Math Reasoning tasks?", "answer": " Modest due to the difficulty in accurately identifying errors", "ref_chunk": "in Section 2. The FEEDBACK- REFINE iterations continue until the desired output quality or task-specific criterion is reached, up to a maximum of 4 iterations. To make our evaluation consistent across different models, we implemented both FEEDBACK and REFINE as few-shot prompts even with models that respond well to instructions, such as ChatGPT and GPT-4. Base LLMs Our main goal is to evaluate whether we can improve the performance of any strong base LLMs using SELF-REFINE. Therefore, we compare SELF-REFINE to the same base LLMs but without feedback-refine iterations. We used three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (OpenAI, 2023). For code-based tasks, we also experimented with CODEX (code-davinci-002). In all tasks, either GPT-3.5 or GPT-4 is the previous state-of-the-art.3 We used the same prompts from previous work when 3A comparison with other few-shot and fine-tuned approaches is provided in Appendix F 4 (3) (4) GPT-3.5 ChatGPT GPT-4 Task Base +SELF-REFINE Base +SELF-REFINE Base +SELF-REFINE 8.8 30.4 (\u219121.6) Sentiment Reversal 36.4 63.6 (\u219127.2) Dialogue Response 14.8 23.0 (\u21918.2) Code Optimization 37.4 51.3 (\u219113.9) Code Readability 64.1 64.1 (0) Math Reasoning 41.6 56.4 (\u219114.8) Acronym Generation Constrained Generation 28.0 37.0 (\u21919.0) 11.4 43.2 (\u219131.8) 40.1 59.9 (\u219119.8) 23.9 27.5 (\u21913.6) 27.7 63.1 (\u219135.4) 74.8 75.0 (\u21910.2) 27.2 37.2 (\u219110.0) 44.0 67.0 (\u219123.0) 3.8 36.2 (\u219132.4) 25.4 74.6 (\u219149.2) 27.3 36.0 (\u21918.7) 27.4 56.2 (\u219128.8) 92.9 93.1 (\u21910.2) 30.4 56.0 (\u219125.6) 15.0 45.0 (\u219130.0) Table 1: SELF-REFINE results on various tasks using GPT-3.5, ChatGPT, and GPT-4 as base LLM. SELF-REFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2. available (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as detailed in Appendix S. We use greedy decoding with a temperature of 0.7 for all setups. 3.2 Metrics We report three types of metrics: Task specific metric: When available, we use automated metrics from prior work (Math Reasoning: % solve rate; Code Optimization: % programs optimized; Constrained Gen: coverage %) Human-pref: In Dialogue Response Generation, Code Readability Improvement, Sentiment Reversal, and Acronym Generation, since no automated metrics are available, we perform a blind human A/B evaluation on a subset of the outputs to select the preferred output. Additional details are provided in Appendix C. GPT-4-pref: In addition to human-pref, we use GPT-4 as a proxy for human preference following prior work (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), and found high correlation (82% for Sentiment Reversal, 68% for Acronym Generation, and 71% for Dialogue Response Generation) with human-pref. For Code Readability Improvement, we prompt GPT- 4 to calculate fraction of the variables that are appropriately named given the context (e.g., x = [] \u2192 input_buffer = []). Additional details are provided in Appendix D. 3.3 Results Table 1 shows our main results: SELF-REFINE consistently improves over base models across all model sizes, and additionally outperforms the previous state-of-the-art across all tasks. For example, GPT-4+SELF-REFINE improves over the base GPT-4 by 8.7% (absolute) in Code Optimization, increasing optimization percentage from 27.3% to 36.0%. Confidence intervals are provided in Appendix J. For code-based tasks, we found similar trends when using CODEX; those results are included in Appendix F. One of the tasks in which we observe the highest gains compared to the base models is Constrained Generation, where the model is asked to generate a sentence containing up to 30 given concepts. We believe that this task benefits significantly from SELF-REFINE because there are more opportunities to miss some of the concepts on the first attempt, and thus SELF-REFINE allows the model to fix these mistakes subsequently. Further, this task has an extremely large number of reasonable outputs, and thus SELF-REFINE allows to better explore the space of possible outputs. In preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym Generation, SELF-REFINE leads to especially high gains. For example in Dialogue Response Generation, GPT-4 preference score improve by 49.2% \u2013 from 25.4% to 74.6%. Similarly, we see remarkable improvements in the other preference-based tasks across all models. The modest performance gains in Math Reasoning can be traced back to the inability to accurately identify whether there is any error. In math, errors can be nuanced and sometimes limited to a single line or incorrect operation. Besides, a consistent-looking reasoning chain can deceive LLMs to 5 think that \u201ceverything looks good\u201d (e.g., ChatGPT feedback for 94% instances is \u2019everything looks good\u2019). In Appendix H.1, we show that the gains with SELF-REFINE on Math Reasoning are much bigger (5%+) if an external source can identify if the current math answer is incorrect. Improvement is consistent across base LLMs sizes Generally, GPT-4+SELF-REFINE performs better than GPT-3.5+SELF-REFINE and ChatGPT+SELF-REFINE across all tasks, even in tasks where the initial base results of GPT-4 were lower than GPT-3.5 or ChatGPT. We thus believe that SELF-REFINE allows stronger models (such as GPT-4) to unlock their full potential, even in cases where this potential is not expressed in the standard, single-pass, output generation. Comparison to additional strong baselines is provided in Appendix F. 4 Analysis The three main steps of SELF-REFINE are FEEDBACK, REFINE, and repeating them iteratively. In this section, we perform additional experiments to analyze the importance of each of these steps. Task SELF-REFINE feedback Generic feedback No feedback Code Optimization Sentiment Reversal Acronym Generation 27.5 43.2 56.4 26.0 31.2 54.0 24.8 0 48.0 Table 2: Prompting to generate generic feedback (or having the model generate no feedback at all) leads to reduced scores, indicating the importance of the FEEDBACK step of SELF-REFINE. These experiments were performed with ChatGPT (Code Optimization and Sentiment Reversal) and GPT-3.5 (Acronym Generation), and metrics used are defined in Section 3.2. The impact of the feedback quality Feedback quality plays a crucial role in SELF-REFINE. To quantify its impact, we compare SELF-REFINE, which utilizes specific, actionable feedback, with two ablations: one using generic feedback and another without feedback (the model may still iteratively refine its generations, but is not explicitly"}, {"question": " Which model performed better with SELF-REFINE across all tasks, GPT-4 or GPT-3.5?", "answer": " GPT-4+SELF-REFINE performed better than GPT-3.5+SELF-REFINE", "ref_chunk": "in Section 2. The FEEDBACK- REFINE iterations continue until the desired output quality or task-specific criterion is reached, up to a maximum of 4 iterations. To make our evaluation consistent across different models, we implemented both FEEDBACK and REFINE as few-shot prompts even with models that respond well to instructions, such as ChatGPT and GPT-4. Base LLMs Our main goal is to evaluate whether we can improve the performance of any strong base LLMs using SELF-REFINE. Therefore, we compare SELF-REFINE to the same base LLMs but without feedback-refine iterations. We used three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (OpenAI, 2023). For code-based tasks, we also experimented with CODEX (code-davinci-002). In all tasks, either GPT-3.5 or GPT-4 is the previous state-of-the-art.3 We used the same prompts from previous work when 3A comparison with other few-shot and fine-tuned approaches is provided in Appendix F 4 (3) (4) GPT-3.5 ChatGPT GPT-4 Task Base +SELF-REFINE Base +SELF-REFINE Base +SELF-REFINE 8.8 30.4 (\u219121.6) Sentiment Reversal 36.4 63.6 (\u219127.2) Dialogue Response 14.8 23.0 (\u21918.2) Code Optimization 37.4 51.3 (\u219113.9) Code Readability 64.1 64.1 (0) Math Reasoning 41.6 56.4 (\u219114.8) Acronym Generation Constrained Generation 28.0 37.0 (\u21919.0) 11.4 43.2 (\u219131.8) 40.1 59.9 (\u219119.8) 23.9 27.5 (\u21913.6) 27.7 63.1 (\u219135.4) 74.8 75.0 (\u21910.2) 27.2 37.2 (\u219110.0) 44.0 67.0 (\u219123.0) 3.8 36.2 (\u219132.4) 25.4 74.6 (\u219149.2) 27.3 36.0 (\u21918.7) 27.4 56.2 (\u219128.8) 92.9 93.1 (\u21910.2) 30.4 56.0 (\u219125.6) 15.0 45.0 (\u219130.0) Table 1: SELF-REFINE results on various tasks using GPT-3.5, ChatGPT, and GPT-4 as base LLM. SELF-REFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2. available (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as detailed in Appendix S. We use greedy decoding with a temperature of 0.7 for all setups. 3.2 Metrics We report three types of metrics: Task specific metric: When available, we use automated metrics from prior work (Math Reasoning: % solve rate; Code Optimization: % programs optimized; Constrained Gen: coverage %) Human-pref: In Dialogue Response Generation, Code Readability Improvement, Sentiment Reversal, and Acronym Generation, since no automated metrics are available, we perform a blind human A/B evaluation on a subset of the outputs to select the preferred output. Additional details are provided in Appendix C. GPT-4-pref: In addition to human-pref, we use GPT-4 as a proxy for human preference following prior work (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), and found high correlation (82% for Sentiment Reversal, 68% for Acronym Generation, and 71% for Dialogue Response Generation) with human-pref. For Code Readability Improvement, we prompt GPT- 4 to calculate fraction of the variables that are appropriately named given the context (e.g., x = [] \u2192 input_buffer = []). Additional details are provided in Appendix D. 3.3 Results Table 1 shows our main results: SELF-REFINE consistently improves over base models across all model sizes, and additionally outperforms the previous state-of-the-art across all tasks. For example, GPT-4+SELF-REFINE improves over the base GPT-4 by 8.7% (absolute) in Code Optimization, increasing optimization percentage from 27.3% to 36.0%. Confidence intervals are provided in Appendix J. For code-based tasks, we found similar trends when using CODEX; those results are included in Appendix F. One of the tasks in which we observe the highest gains compared to the base models is Constrained Generation, where the model is asked to generate a sentence containing up to 30 given concepts. We believe that this task benefits significantly from SELF-REFINE because there are more opportunities to miss some of the concepts on the first attempt, and thus SELF-REFINE allows the model to fix these mistakes subsequently. Further, this task has an extremely large number of reasonable outputs, and thus SELF-REFINE allows to better explore the space of possible outputs. In preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym Generation, SELF-REFINE leads to especially high gains. For example in Dialogue Response Generation, GPT-4 preference score improve by 49.2% \u2013 from 25.4% to 74.6%. Similarly, we see remarkable improvements in the other preference-based tasks across all models. The modest performance gains in Math Reasoning can be traced back to the inability to accurately identify whether there is any error. In math, errors can be nuanced and sometimes limited to a single line or incorrect operation. Besides, a consistent-looking reasoning chain can deceive LLMs to 5 think that \u201ceverything looks good\u201d (e.g., ChatGPT feedback for 94% instances is \u2019everything looks good\u2019). In Appendix H.1, we show that the gains with SELF-REFINE on Math Reasoning are much bigger (5%+) if an external source can identify if the current math answer is incorrect. Improvement is consistent across base LLMs sizes Generally, GPT-4+SELF-REFINE performs better than GPT-3.5+SELF-REFINE and ChatGPT+SELF-REFINE across all tasks, even in tasks where the initial base results of GPT-4 were lower than GPT-3.5 or ChatGPT. We thus believe that SELF-REFINE allows stronger models (such as GPT-4) to unlock their full potential, even in cases where this potential is not expressed in the standard, single-pass, output generation. Comparison to additional strong baselines is provided in Appendix F. 4 Analysis The three main steps of SELF-REFINE are FEEDBACK, REFINE, and repeating them iteratively. In this section, we perform additional experiments to analyze the importance of each of these steps. Task SELF-REFINE feedback Generic feedback No feedback Code Optimization Sentiment Reversal Acronym Generation 27.5 43.2 56.4 26.0 31.2 54.0 24.8 0 48.0 Table 2: Prompting to generate generic feedback (or having the model generate no feedback at all) leads to reduced scores, indicating the importance of the FEEDBACK step of SELF-REFINE. These experiments were performed with ChatGPT (Code Optimization and Sentiment Reversal) and GPT-3.5 (Acronym Generation), and metrics used are defined in Section 3.2. The impact of the feedback quality Feedback quality plays a crucial role in SELF-REFINE. To quantify its impact, we compare SELF-REFINE, which utilizes specific, actionable feedback, with two ablations: one using generic feedback and another without feedback (the model may still iteratively refine its generations, but is not explicitly"}], "doc_text": "in Section 2. The FEEDBACK- REFINE iterations continue until the desired output quality or task-specific criterion is reached, up to a maximum of 4 iterations. To make our evaluation consistent across different models, we implemented both FEEDBACK and REFINE as few-shot prompts even with models that respond well to instructions, such as ChatGPT and GPT-4. Base LLMs Our main goal is to evaluate whether we can improve the performance of any strong base LLMs using SELF-REFINE. Therefore, we compare SELF-REFINE to the same base LLMs but without feedback-refine iterations. We used three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (OpenAI, 2023). For code-based tasks, we also experimented with CODEX (code-davinci-002). In all tasks, either GPT-3.5 or GPT-4 is the previous state-of-the-art.3 We used the same prompts from previous work when 3A comparison with other few-shot and fine-tuned approaches is provided in Appendix F 4 (3) (4) GPT-3.5 ChatGPT GPT-4 Task Base +SELF-REFINE Base +SELF-REFINE Base +SELF-REFINE 8.8 30.4 (\u219121.6) Sentiment Reversal 36.4 63.6 (\u219127.2) Dialogue Response 14.8 23.0 (\u21918.2) Code Optimization 37.4 51.3 (\u219113.9) Code Readability 64.1 64.1 (0) Math Reasoning 41.6 56.4 (\u219114.8) Acronym Generation Constrained Generation 28.0 37.0 (\u21919.0) 11.4 43.2 (\u219131.8) 40.1 59.9 (\u219119.8) 23.9 27.5 (\u21913.6) 27.7 63.1 (\u219135.4) 74.8 75.0 (\u21910.2) 27.2 37.2 (\u219110.0) 44.0 67.0 (\u219123.0) 3.8 36.2 (\u219132.4) 25.4 74.6 (\u219149.2) 27.3 36.0 (\u21918.7) 27.4 56.2 (\u219128.8) 92.9 93.1 (\u21910.2) 30.4 56.0 (\u219125.6) 15.0 45.0 (\u219130.0) Table 1: SELF-REFINE results on various tasks using GPT-3.5, ChatGPT, and GPT-4 as base LLM. SELF-REFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2. available (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as detailed in Appendix S. We use greedy decoding with a temperature of 0.7 for all setups. 3.2 Metrics We report three types of metrics: Task specific metric: When available, we use automated metrics from prior work (Math Reasoning: % solve rate; Code Optimization: % programs optimized; Constrained Gen: coverage %) Human-pref: In Dialogue Response Generation, Code Readability Improvement, Sentiment Reversal, and Acronym Generation, since no automated metrics are available, we perform a blind human A/B evaluation on a subset of the outputs to select the preferred output. Additional details are provided in Appendix C. GPT-4-pref: In addition to human-pref, we use GPT-4 as a proxy for human preference following prior work (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), and found high correlation (82% for Sentiment Reversal, 68% for Acronym Generation, and 71% for Dialogue Response Generation) with human-pref. For Code Readability Improvement, we prompt GPT- 4 to calculate fraction of the variables that are appropriately named given the context (e.g., x = [] \u2192 input_buffer = []). Additional details are provided in Appendix D. 3.3 Results Table 1 shows our main results: SELF-REFINE consistently improves over base models across all model sizes, and additionally outperforms the previous state-of-the-art across all tasks. For example, GPT-4+SELF-REFINE improves over the base GPT-4 by 8.7% (absolute) in Code Optimization, increasing optimization percentage from 27.3% to 36.0%. Confidence intervals are provided in Appendix J. For code-based tasks, we found similar trends when using CODEX; those results are included in Appendix F. One of the tasks in which we observe the highest gains compared to the base models is Constrained Generation, where the model is asked to generate a sentence containing up to 30 given concepts. We believe that this task benefits significantly from SELF-REFINE because there are more opportunities to miss some of the concepts on the first attempt, and thus SELF-REFINE allows the model to fix these mistakes subsequently. Further, this task has an extremely large number of reasonable outputs, and thus SELF-REFINE allows to better explore the space of possible outputs. In preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym Generation, SELF-REFINE leads to especially high gains. For example in Dialogue Response Generation, GPT-4 preference score improve by 49.2% \u2013 from 25.4% to 74.6%. Similarly, we see remarkable improvements in the other preference-based tasks across all models. The modest performance gains in Math Reasoning can be traced back to the inability to accurately identify whether there is any error. In math, errors can be nuanced and sometimes limited to a single line or incorrect operation. Besides, a consistent-looking reasoning chain can deceive LLMs to 5 think that \u201ceverything looks good\u201d (e.g., ChatGPT feedback for 94% instances is \u2019everything looks good\u2019). In Appendix H.1, we show that the gains with SELF-REFINE on Math Reasoning are much bigger (5%+) if an external source can identify if the current math answer is incorrect. Improvement is consistent across base LLMs sizes Generally, GPT-4+SELF-REFINE performs better than GPT-3.5+SELF-REFINE and ChatGPT+SELF-REFINE across all tasks, even in tasks where the initial base results of GPT-4 were lower than GPT-3.5 or ChatGPT. We thus believe that SELF-REFINE allows stronger models (such as GPT-4) to unlock their full potential, even in cases where this potential is not expressed in the standard, single-pass, output generation. Comparison to additional strong baselines is provided in Appendix F. 4 Analysis The three main steps of SELF-REFINE are FEEDBACK, REFINE, and repeating them iteratively. In this section, we perform additional experiments to analyze the importance of each of these steps. Task SELF-REFINE feedback Generic feedback No feedback Code Optimization Sentiment Reversal Acronym Generation 27.5 43.2 56.4 26.0 31.2 54.0 24.8 0 48.0 Table 2: Prompting to generate generic feedback (or having the model generate no feedback at all) leads to reduced scores, indicating the importance of the FEEDBACK step of SELF-REFINE. These experiments were performed with ChatGPT (Code Optimization and Sentiment Reversal) and GPT-3.5 (Acronym Generation), and metrics used are defined in Section 3.2. The impact of the feedback quality Feedback quality plays a crucial role in SELF-REFINE. To quantify its impact, we compare SELF-REFINE, which utilizes specific, actionable feedback, with two ablations: one using generic feedback and another without feedback (the model may still iteratively refine its generations, but is not explicitly"}