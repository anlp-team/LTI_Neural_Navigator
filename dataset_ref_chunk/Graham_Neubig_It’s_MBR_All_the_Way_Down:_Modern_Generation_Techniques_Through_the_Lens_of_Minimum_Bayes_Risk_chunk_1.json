{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_It\u2019s_MBR_All_the_Way_Down:_Modern_Generation_Techniques_Through_the_Lens_of_Minimum_Bayes_Risk_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is Minimum Bayes Risk (MBR) decoding?", "answer": " MBR decoding is a method for choosing the outputs of a machine learning system based on the output with the lowest risk among multiple candidates, rather than the output with the highest probability.", "ref_chunk": "3 2 0 2 t c O 2 ] L C . s c [ 1 v 7 8 3 1 0 . 0 1 3 2 : v i X r a It\u2019s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk Amanda Bertsch\u2217 and Alex Xie\u2217 and Graham Neubig and Matthew R. Gormley Carnegie Mellon University [abertsch, alexx]@cs.cmu.edu Abstract Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks with- out any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoret- ical justification for the performance of these methods, explaining some results that were pre- viously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area. than the consensus). MBR thus provides an alter- native to the more standard maximum-likelihood decoding; when a sample of sufficient size is taken, MBR almost uniformly outperforms beam search and single-output sampling across tasks, metrics, and datasets (see \u00a76). It is also notable in its flexibil- ity; in \u00a73 we organize and discuss several different design decisions that go into the use of MBR and how they affect the efficacy of the method. While MBR is rarely applied by name in modern NLP, a number of methods with similar intuitions have gained popularity. In \u00a74, we demonstrate that a number of generation techniques widely used with modern language models can be viewed as special instances of MBR: self-consistency (Wang et al., 2023) and its extensions, range vot- ing (Borgeaud and Emerson, 2020), output en- sembling (DeNero et al., 2010; Mart\u00ednez Lorenzo et al., 2023), and some types of density estimation (Kobayashi, 2018). This view exposes connections between seemingly disparate methods and presents theoretical justifications for existing empirical re- sults using these methods. We also discuss how insights from the MBR literature can inform the use of these other MBR-like methods. 1 Introduction \u201cSometimes innovation is only old ideas reappearing in new guises . . . [b]ut the new costumes are better made, of better materials, as well as more becoming: so research is not so much going round in circles as ascending a spiral.\u201d (Jones, 1994) Minimum Bayes Risk (MBR) decoding (Bickel and Doksum (1977); \u00a72) is a decoding method fol- lowing a simple intuition: when choosing a best output from a set of candidates, the desirable output should be both 1) high probability and 2) relatively consistent with the rest of the outputs (i.e., outputs that are not consistent with the other outputs are high risk\u2013 they may be dramatically better or worse With the framing of MBR, the theoretical jus- tification for the empirical performance of sev- eral methods becomes clear; the extension of self- consistency to open-ended generations becomes trivial; and several promising modifications to self- consistency and output ensembling are exposed. In particular, modern MBR-like methods often do not apply the insights from research on MBR, suggest- ing that these methods could be further improved. In \u00a75, we show that some design choices, though seemingly intuitive to a practitioner accustomed to search-based decoding methods, should be avoided when applying MBR. 2 Formalization \u2217Denotes equal contribution. We begin with the basics of decoding and MBR. 2.1 Standard decoding Decoding from an autoregressive model (such as a transformer decoder) is performed tokenwise. The distribution at each decoding step is conditioned on the prior tokens and the input text: p(yi|y<i, x) The model is locally normalized; the probabilities of next tokens sum to 1. The probability of a se- quence under this global model distribution is p(y|x) = T (cid:89) p(yi|y<i, x) i=1 Given this distribution, there are several ways of extracting an output: by sampling at each de- coding step from the distribution over next tokens (often with some modification to the distribution, e.g. temperature, nucleus, or epsilon sampling; Holtzman et al. (2019)); by always choosing the most probable next token (i.e. greedy decoding); or by performing a search over some subset of the output space, guided by the distribution (e.g. beam search, best-first search). These methods generally return a single output; if multiple output candidates are present, the one with the maximum likelihood under the model distribution is returned. 2.2 Minimum Bayes Risk decoding The traditional formulation of MBR is as a mini- mization objective. Given a output space Y and a probability distribution over this space p(y|x), we compute the risk R(y\u2032) of a candidate decoding y\u2032 as the expected error (also called loss) under this distribution (Bickel and Doksum, 1977; Kumar and Byrne, 2004; Tromble et al., 2008). The MBR de- coding is then the y\u2032 within Y that minimizes risk: \u02c6y = argmin R(y\u2032) y\u2032\u2208Y = argmin y\u2032\u2208Y Ey|x[L(y, y\u2032)] = argmin y\u2032\u2208Y (cid:88) y\u2208Y L(y, y\u2032)p(y|x) We can trivially rewrite the risk as a maximiza- tion of gain (also called utility) rather than a mini- mization of error, where G(y, y\u2032) = \u2212L(y, y\u2032). Approximating risk Computing this sum over the space of all possible outputs Y is intractable (1) (2) (3) (4) (5) for most models.1 In these cases, we approximate the risk R(y\u2032) by using a subset of the full space Y \u2282 Y ; that is, instead of exact computation of the expectation, we approximate it with a sum over independent samples from p(y|x). Generally, this is performed by"}, {"question": " What is the benefit of using Minimum Bayes Risk (MBR) decoding?", "answer": " MBR provides reliable improvements across metrics for various tasks without requiring additional data or training.", "ref_chunk": "3 2 0 2 t c O 2 ] L C . s c [ 1 v 7 8 3 1 0 . 0 1 3 2 : v i X r a It\u2019s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk Amanda Bertsch\u2217 and Alex Xie\u2217 and Graham Neubig and Matthew R. Gormley Carnegie Mellon University [abertsch, alexx]@cs.cmu.edu Abstract Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks with- out any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoret- ical justification for the performance of these methods, explaining some results that were pre- viously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area. than the consensus). MBR thus provides an alter- native to the more standard maximum-likelihood decoding; when a sample of sufficient size is taken, MBR almost uniformly outperforms beam search and single-output sampling across tasks, metrics, and datasets (see \u00a76). It is also notable in its flexibil- ity; in \u00a73 we organize and discuss several different design decisions that go into the use of MBR and how they affect the efficacy of the method. While MBR is rarely applied by name in modern NLP, a number of methods with similar intuitions have gained popularity. In \u00a74, we demonstrate that a number of generation techniques widely used with modern language models can be viewed as special instances of MBR: self-consistency (Wang et al., 2023) and its extensions, range vot- ing (Borgeaud and Emerson, 2020), output en- sembling (DeNero et al., 2010; Mart\u00ednez Lorenzo et al., 2023), and some types of density estimation (Kobayashi, 2018). This view exposes connections between seemingly disparate methods and presents theoretical justifications for existing empirical re- sults using these methods. We also discuss how insights from the MBR literature can inform the use of these other MBR-like methods. 1 Introduction \u201cSometimes innovation is only old ideas reappearing in new guises . . . [b]ut the new costumes are better made, of better materials, as well as more becoming: so research is not so much going round in circles as ascending a spiral.\u201d (Jones, 1994) Minimum Bayes Risk (MBR) decoding (Bickel and Doksum (1977); \u00a72) is a decoding method fol- lowing a simple intuition: when choosing a best output from a set of candidates, the desirable output should be both 1) high probability and 2) relatively consistent with the rest of the outputs (i.e., outputs that are not consistent with the other outputs are high risk\u2013 they may be dramatically better or worse With the framing of MBR, the theoretical jus- tification for the empirical performance of sev- eral methods becomes clear; the extension of self- consistency to open-ended generations becomes trivial; and several promising modifications to self- consistency and output ensembling are exposed. In particular, modern MBR-like methods often do not apply the insights from research on MBR, suggest- ing that these methods could be further improved. In \u00a75, we show that some design choices, though seemingly intuitive to a practitioner accustomed to search-based decoding methods, should be avoided when applying MBR. 2 Formalization \u2217Denotes equal contribution. We begin with the basics of decoding and MBR. 2.1 Standard decoding Decoding from an autoregressive model (such as a transformer decoder) is performed tokenwise. The distribution at each decoding step is conditioned on the prior tokens and the input text: p(yi|y<i, x) The model is locally normalized; the probabilities of next tokens sum to 1. The probability of a se- quence under this global model distribution is p(y|x) = T (cid:89) p(yi|y<i, x) i=1 Given this distribution, there are several ways of extracting an output: by sampling at each de- coding step from the distribution over next tokens (often with some modification to the distribution, e.g. temperature, nucleus, or epsilon sampling; Holtzman et al. (2019)); by always choosing the most probable next token (i.e. greedy decoding); or by performing a search over some subset of the output space, guided by the distribution (e.g. beam search, best-first search). These methods generally return a single output; if multiple output candidates are present, the one with the maximum likelihood under the model distribution is returned. 2.2 Minimum Bayes Risk decoding The traditional formulation of MBR is as a mini- mization objective. Given a output space Y and a probability distribution over this space p(y|x), we compute the risk R(y\u2032) of a candidate decoding y\u2032 as the expected error (also called loss) under this distribution (Bickel and Doksum, 1977; Kumar and Byrne, 2004; Tromble et al., 2008). The MBR de- coding is then the y\u2032 within Y that minimizes risk: \u02c6y = argmin R(y\u2032) y\u2032\u2208Y = argmin y\u2032\u2208Y Ey|x[L(y, y\u2032)] = argmin y\u2032\u2208Y (cid:88) y\u2208Y L(y, y\u2032)p(y|x) We can trivially rewrite the risk as a maximiza- tion of gain (also called utility) rather than a mini- mization of error, where G(y, y\u2032) = \u2212L(y, y\u2032). Approximating risk Computing this sum over the space of all possible outputs Y is intractable (1) (2) (3) (4) (5) for most models.1 In these cases, we approximate the risk R(y\u2032) by using a subset of the full space Y \u2282 Y ; that is, instead of exact computation of the expectation, we approximate it with a sum over independent samples from p(y|x). Generally, this is performed by"}, {"question": " Why is Minimum Bayes Risk (MBR) decoding not frequently applied in NLP works?", "answer": " Despite its benefits, MBR is not frequently applied in natural language processing (NLP) works and knowledge of the method itself is limited.", "ref_chunk": "3 2 0 2 t c O 2 ] L C . s c [ 1 v 7 8 3 1 0 . 0 1 3 2 : v i X r a It\u2019s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk Amanda Bertsch\u2217 and Alex Xie\u2217 and Graham Neubig and Matthew R. Gormley Carnegie Mellon University [abertsch, alexx]@cs.cmu.edu Abstract Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks with- out any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoret- ical justification for the performance of these methods, explaining some results that were pre- viously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area. than the consensus). MBR thus provides an alter- native to the more standard maximum-likelihood decoding; when a sample of sufficient size is taken, MBR almost uniformly outperforms beam search and single-output sampling across tasks, metrics, and datasets (see \u00a76). It is also notable in its flexibil- ity; in \u00a73 we organize and discuss several different design decisions that go into the use of MBR and how they affect the efficacy of the method. While MBR is rarely applied by name in modern NLP, a number of methods with similar intuitions have gained popularity. In \u00a74, we demonstrate that a number of generation techniques widely used with modern language models can be viewed as special instances of MBR: self-consistency (Wang et al., 2023) and its extensions, range vot- ing (Borgeaud and Emerson, 2020), output en- sembling (DeNero et al., 2010; Mart\u00ednez Lorenzo et al., 2023), and some types of density estimation (Kobayashi, 2018). This view exposes connections between seemingly disparate methods and presents theoretical justifications for existing empirical re- sults using these methods. We also discuss how insights from the MBR literature can inform the use of these other MBR-like methods. 1 Introduction \u201cSometimes innovation is only old ideas reappearing in new guises . . . [b]ut the new costumes are better made, of better materials, as well as more becoming: so research is not so much going round in circles as ascending a spiral.\u201d (Jones, 1994) Minimum Bayes Risk (MBR) decoding (Bickel and Doksum (1977); \u00a72) is a decoding method fol- lowing a simple intuition: when choosing a best output from a set of candidates, the desirable output should be both 1) high probability and 2) relatively consistent with the rest of the outputs (i.e., outputs that are not consistent with the other outputs are high risk\u2013 they may be dramatically better or worse With the framing of MBR, the theoretical jus- tification for the empirical performance of sev- eral methods becomes clear; the extension of self- consistency to open-ended generations becomes trivial; and several promising modifications to self- consistency and output ensembling are exposed. In particular, modern MBR-like methods often do not apply the insights from research on MBR, suggest- ing that these methods could be further improved. In \u00a75, we show that some design choices, though seemingly intuitive to a practitioner accustomed to search-based decoding methods, should be avoided when applying MBR. 2 Formalization \u2217Denotes equal contribution. We begin with the basics of decoding and MBR. 2.1 Standard decoding Decoding from an autoregressive model (such as a transformer decoder) is performed tokenwise. The distribution at each decoding step is conditioned on the prior tokens and the input text: p(yi|y<i, x) The model is locally normalized; the probabilities of next tokens sum to 1. The probability of a se- quence under this global model distribution is p(y|x) = T (cid:89) p(yi|y<i, x) i=1 Given this distribution, there are several ways of extracting an output: by sampling at each de- coding step from the distribution over next tokens (often with some modification to the distribution, e.g. temperature, nucleus, or epsilon sampling; Holtzman et al. (2019)); by always choosing the most probable next token (i.e. greedy decoding); or by performing a search over some subset of the output space, guided by the distribution (e.g. beam search, best-first search). These methods generally return a single output; if multiple output candidates are present, the one with the maximum likelihood under the model distribution is returned. 2.2 Minimum Bayes Risk decoding The traditional formulation of MBR is as a mini- mization objective. Given a output space Y and a probability distribution over this space p(y|x), we compute the risk R(y\u2032) of a candidate decoding y\u2032 as the expected error (also called loss) under this distribution (Bickel and Doksum, 1977; Kumar and Byrne, 2004; Tromble et al., 2008). The MBR de- coding is then the y\u2032 within Y that minimizes risk: \u02c6y = argmin R(y\u2032) y\u2032\u2208Y = argmin y\u2032\u2208Y Ey|x[L(y, y\u2032)] = argmin y\u2032\u2208Y (cid:88) y\u2208Y L(y, y\u2032)p(y|x) We can trivially rewrite the risk as a maximiza- tion of gain (also called utility) rather than a mini- mization of error, where G(y, y\u2032) = \u2212L(y, y\u2032). Approximating risk Computing this sum over the space of all possible outputs Y is intractable (1) (2) (3) (4) (5) for most models.1 In these cases, we approximate the risk R(y\u2032) by using a subset of the full space Y \u2282 Y ; that is, instead of exact computation of the expectation, we approximate it with a sum over independent samples from p(y|x). Generally, this is performed by"}, {"question": " How does Minimum Bayes Risk (MBR) decoding compare to maximum-likelihood decoding?", "answer": " When a sample of sufficient size is taken, MBR almost uniformly outperforms beam search and single-output sampling across tasks, metrics, and datasets.", "ref_chunk": "3 2 0 2 t c O 2 ] L C . s c [ 1 v 7 8 3 1 0 . 0 1 3 2 : v i X r a It\u2019s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk Amanda Bertsch\u2217 and Alex Xie\u2217 and Graham Neubig and Matthew R. Gormley Carnegie Mellon University [abertsch, alexx]@cs.cmu.edu Abstract Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks with- out any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoret- ical justification for the performance of these methods, explaining some results that were pre- viously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area. than the consensus). MBR thus provides an alter- native to the more standard maximum-likelihood decoding; when a sample of sufficient size is taken, MBR almost uniformly outperforms beam search and single-output sampling across tasks, metrics, and datasets (see \u00a76). It is also notable in its flexibil- ity; in \u00a73 we organize and discuss several different design decisions that go into the use of MBR and how they affect the efficacy of the method. While MBR is rarely applied by name in modern NLP, a number of methods with similar intuitions have gained popularity. In \u00a74, we demonstrate that a number of generation techniques widely used with modern language models can be viewed as special instances of MBR: self-consistency (Wang et al., 2023) and its extensions, range vot- ing (Borgeaud and Emerson, 2020), output en- sembling (DeNero et al., 2010; Mart\u00ednez Lorenzo et al., 2023), and some types of density estimation (Kobayashi, 2018). This view exposes connections between seemingly disparate methods and presents theoretical justifications for existing empirical re- sults using these methods. We also discuss how insights from the MBR literature can inform the use of these other MBR-like methods. 1 Introduction \u201cSometimes innovation is only old ideas reappearing in new guises . . . [b]ut the new costumes are better made, of better materials, as well as more becoming: so research is not so much going round in circles as ascending a spiral.\u201d (Jones, 1994) Minimum Bayes Risk (MBR) decoding (Bickel and Doksum (1977); \u00a72) is a decoding method fol- lowing a simple intuition: when choosing a best output from a set of candidates, the desirable output should be both 1) high probability and 2) relatively consistent with the rest of the outputs (i.e., outputs that are not consistent with the other outputs are high risk\u2013 they may be dramatically better or worse With the framing of MBR, the theoretical jus- tification for the empirical performance of sev- eral methods becomes clear; the extension of self- consistency to open-ended generations becomes trivial; and several promising modifications to self- consistency and output ensembling are exposed. In particular, modern MBR-like methods often do not apply the insights from research on MBR, suggest- ing that these methods could be further improved. In \u00a75, we show that some design choices, though seemingly intuitive to a practitioner accustomed to search-based decoding methods, should be avoided when applying MBR. 2 Formalization \u2217Denotes equal contribution. We begin with the basics of decoding and MBR. 2.1 Standard decoding Decoding from an autoregressive model (such as a transformer decoder) is performed tokenwise. The distribution at each decoding step is conditioned on the prior tokens and the input text: p(yi|y<i, x) The model is locally normalized; the probabilities of next tokens sum to 1. The probability of a se- quence under this global model distribution is p(y|x) = T (cid:89) p(yi|y<i, x) i=1 Given this distribution, there are several ways of extracting an output: by sampling at each de- coding step from the distribution over next tokens (often with some modification to the distribution, e.g. temperature, nucleus, or epsilon sampling; Holtzman et al. (2019)); by always choosing the most probable next token (i.e. greedy decoding); or by performing a search over some subset of the output space, guided by the distribution (e.g. beam search, best-first search). These methods generally return a single output; if multiple output candidates are present, the one with the maximum likelihood under the model distribution is returned. 2.2 Minimum Bayes Risk decoding The traditional formulation of MBR is as a mini- mization objective. Given a output space Y and a probability distribution over this space p(y|x), we compute the risk R(y\u2032) of a candidate decoding y\u2032 as the expected error (also called loss) under this distribution (Bickel and Doksum, 1977; Kumar and Byrne, 2004; Tromble et al., 2008). The MBR de- coding is then the y\u2032 within Y that minimizes risk: \u02c6y = argmin R(y\u2032) y\u2032\u2208Y = argmin y\u2032\u2208Y Ey|x[L(y, y\u2032)] = argmin y\u2032\u2208Y (cid:88) y\u2208Y L(y, y\u2032)p(y|x) We can trivially rewrite the risk as a maximiza- tion of gain (also called utility) rather than a mini- mization of error, where G(y, y\u2032) = \u2212L(y, y\u2032). Approximating risk Computing this sum over the space of all possible outputs Y is intractable (1) (2) (3) (4) (5) for most models.1 In these cases, we approximate the risk R(y\u2032) by using a subset of the full space Y \u2282 Y ; that is, instead of exact computation of the expectation, we approximate it with a sum over independent samples from p(y|x). Generally, this is performed by"}, {"question": " What are some modern generation techniques that can be viewed as special instances of Minimum Bayes Risk (MBR)?", "answer": " Self-consistency, range voting, output ensembling, and some types of density estimation are modern generation techniques that can be viewed as special instances of MBR.", "ref_chunk": "3 2 0 2 t c O 2 ] L C . s c [ 1 v 7 8 3 1 0 . 0 1 3 2 : v i X r a It\u2019s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk Amanda Bertsch\u2217 and Alex Xie\u2217 and Graham Neubig and Matthew R. Gormley Carnegie Mellon University [abertsch, alexx]@cs.cmu.edu Abstract Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks with- out any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoret- ical justification for the performance of these methods, explaining some results that were pre- viously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area. than the consensus). MBR thus provides an alter- native to the more standard maximum-likelihood decoding; when a sample of sufficient size is taken, MBR almost uniformly outperforms beam search and single-output sampling across tasks, metrics, and datasets (see \u00a76). It is also notable in its flexibil- ity; in \u00a73 we organize and discuss several different design decisions that go into the use of MBR and how they affect the efficacy of the method. While MBR is rarely applied by name in modern NLP, a number of methods with similar intuitions have gained popularity. In \u00a74, we demonstrate that a number of generation techniques widely used with modern language models can be viewed as special instances of MBR: self-consistency (Wang et al., 2023) and its extensions, range vot- ing (Borgeaud and Emerson, 2020), output en- sembling (DeNero et al., 2010; Mart\u00ednez Lorenzo et al., 2023), and some types of density estimation (Kobayashi, 2018). This view exposes connections between seemingly disparate methods and presents theoretical justifications for existing empirical re- sults using these methods. We also discuss how insights from the MBR literature can inform the use of these other MBR-like methods. 1 Introduction \u201cSometimes innovation is only old ideas reappearing in new guises . . . [b]ut the new costumes are better made, of better materials, as well as more becoming: so research is not so much going round in circles as ascending a spiral.\u201d (Jones, 1994) Minimum Bayes Risk (MBR) decoding (Bickel and Doksum (1977); \u00a72) is a decoding method fol- lowing a simple intuition: when choosing a best output from a set of candidates, the desirable output should be both 1) high probability and 2) relatively consistent with the rest of the outputs (i.e., outputs that are not consistent with the other outputs are high risk\u2013 they may be dramatically better or worse With the framing of MBR, the theoretical jus- tification for the empirical performance of sev- eral methods becomes clear; the extension of self- consistency to open-ended generations becomes trivial; and several promising modifications to self- consistency and output ensembling are exposed. In particular, modern MBR-like methods often do not apply the insights from research on MBR, suggest- ing that these methods could be further improved. In \u00a75, we show that some design choices, though seemingly intuitive to a practitioner accustomed to search-based decoding methods, should be avoided when applying MBR. 2 Formalization \u2217Denotes equal contribution. We begin with the basics of decoding and MBR. 2.1 Standard decoding Decoding from an autoregressive model (such as a transformer decoder) is performed tokenwise. The distribution at each decoding step is conditioned on the prior tokens and the input text: p(yi|y<i, x) The model is locally normalized; the probabilities of next tokens sum to 1. The probability of a se- quence under this global model distribution is p(y|x) = T (cid:89) p(yi|y<i, x) i=1 Given this distribution, there are several ways of extracting an output: by sampling at each de- coding step from the distribution over next tokens (often with some modification to the distribution, e.g. temperature, nucleus, or epsilon sampling; Holtzman et al. (2019)); by always choosing the most probable next token (i.e. greedy decoding); or by performing a search over some subset of the output space, guided by the distribution (e.g. beam search, best-first search). These methods generally return a single output; if multiple output candidates are present, the one with the maximum likelihood under the model distribution is returned. 2.2 Minimum Bayes Risk decoding The traditional formulation of MBR is as a mini- mization objective. Given a output space Y and a probability distribution over this space p(y|x), we compute the risk R(y\u2032) of a candidate decoding y\u2032 as the expected error (also called loss) under this distribution (Bickel and Doksum, 1977; Kumar and Byrne, 2004; Tromble et al., 2008). The MBR de- coding is then the y\u2032 within Y that minimizes risk: \u02c6y = argmin R(y\u2032) y\u2032\u2208Y = argmin y\u2032\u2208Y Ey|x[L(y, y\u2032)] = argmin y\u2032\u2208Y (cid:88) y\u2208Y L(y, y\u2032)p(y|x) We can trivially rewrite the risk as a maximiza- tion of gain (also called utility) rather than a mini- mization of error, where G(y, y\u2032) = \u2212L(y, y\u2032). Approximating risk Computing this sum over the space of all possible outputs Y is intractable (1) (2) (3) (4) (5) for most models.1 In these cases, we approximate the risk R(y\u2032) by using a subset of the full space Y \u2282 Y ; that is, instead of exact computation of the expectation, we approximate it with a sum over independent samples from p(y|x). Generally, this is performed by"}, {"question": " According to the text, why is it important to choose a best output that is relatively consistent with the other outputs?", "answer": " The desirable output should be both high probability and relatively consistent with the rest of the outputs to minimize risk.", "ref_chunk": "3 2 0 2 t c O 2 ] L C . s c [ 1 v 7 8 3 1 0 . 0 1 3 2 : v i X r a It\u2019s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk Amanda Bertsch\u2217 and Alex Xie\u2217 and Graham Neubig and Matthew R. Gormley Carnegie Mellon University [abertsch, alexx]@cs.cmu.edu Abstract Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks with- out any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoret- ical justification for the performance of these methods, explaining some results that were pre- viously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area. than the consensus). MBR thus provides an alter- native to the more standard maximum-likelihood decoding; when a sample of sufficient size is taken, MBR almost uniformly outperforms beam search and single-output sampling across tasks, metrics, and datasets (see \u00a76). It is also notable in its flexibil- ity; in \u00a73 we organize and discuss several different design decisions that go into the use of MBR and how they affect the efficacy of the method. While MBR is rarely applied by name in modern NLP, a number of methods with similar intuitions have gained popularity. In \u00a74, we demonstrate that a number of generation techniques widely used with modern language models can be viewed as special instances of MBR: self-consistency (Wang et al., 2023) and its extensions, range vot- ing (Borgeaud and Emerson, 2020), output en- sembling (DeNero et al., 2010; Mart\u00ednez Lorenzo et al., 2023), and some types of density estimation (Kobayashi, 2018). This view exposes connections between seemingly disparate methods and presents theoretical justifications for existing empirical re- sults using these methods. We also discuss how insights from the MBR literature can inform the use of these other MBR-like methods. 1 Introduction \u201cSometimes innovation is only old ideas reappearing in new guises . . . [b]ut the new costumes are better made, of better materials, as well as more becoming: so research is not so much going round in circles as ascending a spiral.\u201d (Jones, 1994) Minimum Bayes Risk (MBR) decoding (Bickel and Doksum (1977); \u00a72) is a decoding method fol- lowing a simple intuition: when choosing a best output from a set of candidates, the desirable output should be both 1) high probability and 2) relatively consistent with the rest of the outputs (i.e., outputs that are not consistent with the other outputs are high risk\u2013 they may be dramatically better or worse With the framing of MBR, the theoretical jus- tification for the empirical performance of sev- eral methods becomes clear; the extension of self- consistency to open-ended generations becomes trivial; and several promising modifications to self- consistency and output ensembling are exposed. In particular, modern MBR-like methods often do not apply the insights from research on MBR, suggest- ing that these methods could be further improved. In \u00a75, we show that some design choices, though seemingly intuitive to a practitioner accustomed to search-based decoding methods, should be avoided when applying MBR. 2 Formalization \u2217Denotes equal contribution. We begin with the basics of decoding and MBR. 2.1 Standard decoding Decoding from an autoregressive model (such as a transformer decoder) is performed tokenwise. The distribution at each decoding step is conditioned on the prior tokens and the input text: p(yi|y<i, x) The model is locally normalized; the probabilities of next tokens sum to 1. The probability of a se- quence under this global model distribution is p(y|x) = T (cid:89) p(yi|y<i, x) i=1 Given this distribution, there are several ways of extracting an output: by sampling at each de- coding step from the distribution over next tokens (often with some modification to the distribution, e.g. temperature, nucleus, or epsilon sampling; Holtzman et al. (2019)); by always choosing the most probable next token (i.e. greedy decoding); or by performing a search over some subset of the output space, guided by the distribution (e.g. beam search, best-first search). These methods generally return a single output; if multiple output candidates are present, the one with the maximum likelihood under the model distribution is returned. 2.2 Minimum Bayes Risk decoding The traditional formulation of MBR is as a mini- mization objective. Given a output space Y and a probability distribution over this space p(y|x), we compute the risk R(y\u2032) of a candidate decoding y\u2032 as the expected error (also called loss) under this distribution (Bickel and Doksum, 1977; Kumar and Byrne, 2004; Tromble et al., 2008). The MBR de- coding is then the y\u2032 within Y that minimizes risk: \u02c6y = argmin R(y\u2032) y\u2032\u2208Y = argmin y\u2032\u2208Y Ey|x[L(y, y\u2032)] = argmin y\u2032\u2208Y (cid:88) y\u2208Y L(y, y\u2032)p(y|x) We can trivially rewrite the risk as a maximiza- tion of gain (also called utility) rather than a mini- mization of error, where G(y, y\u2032) = \u2212L(y, y\u2032). Approximating risk Computing this sum over the space of all possible outputs Y is intractable (1) (2) (3) (4) (5) for most models.1 In these cases, we approximate the risk R(y\u2032) by using a subset of the full space Y \u2282 Y ; that is, instead of exact computation of the expectation, we approximate it with a sum over independent samples from p(y|x). Generally, this is performed by"}, {"question": " What is the difference between standard decoding and Minimum Bayes Risk (MBR) decoding?", "answer": " Standard decoding chooses the most probable next token or performs a search over the output space, while MBR decoding chooses the output with the lowest risk among multiple candidates.", "ref_chunk": "3 2 0 2 t c O 2 ] L C . s c [ 1 v 7 8 3 1 0 . 0 1 3 2 : v i X r a It\u2019s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk Amanda Bertsch\u2217 and Alex Xie\u2217 and Graham Neubig and Matthew R. Gormley Carnegie Mellon University [abertsch, alexx]@cs.cmu.edu Abstract Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks with- out any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoret- ical justification for the performance of these methods, explaining some results that were pre- viously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area. than the consensus). MBR thus provides an alter- native to the more standard maximum-likelihood decoding; when a sample of sufficient size is taken, MBR almost uniformly outperforms beam search and single-output sampling across tasks, metrics, and datasets (see \u00a76). It is also notable in its flexibil- ity; in \u00a73 we organize and discuss several different design decisions that go into the use of MBR and how they affect the efficacy of the method. While MBR is rarely applied by name in modern NLP, a number of methods with similar intuitions have gained popularity. In \u00a74, we demonstrate that a number of generation techniques widely used with modern language models can be viewed as special instances of MBR: self-consistency (Wang et al., 2023) and its extensions, range vot- ing (Borgeaud and Emerson, 2020), output en- sembling (DeNero et al., 2010; Mart\u00ednez Lorenzo et al., 2023), and some types of density estimation (Kobayashi, 2018). This view exposes connections between seemingly disparate methods and presents theoretical justifications for existing empirical re- sults using these methods. We also discuss how insights from the MBR literature can inform the use of these other MBR-like methods. 1 Introduction \u201cSometimes innovation is only old ideas reappearing in new guises . . . [b]ut the new costumes are better made, of better materials, as well as more becoming: so research is not so much going round in circles as ascending a spiral.\u201d (Jones, 1994) Minimum Bayes Risk (MBR) decoding (Bickel and Doksum (1977); \u00a72) is a decoding method fol- lowing a simple intuition: when choosing a best output from a set of candidates, the desirable output should be both 1) high probability and 2) relatively consistent with the rest of the outputs (i.e., outputs that are not consistent with the other outputs are high risk\u2013 they may be dramatically better or worse With the framing of MBR, the theoretical jus- tification for the empirical performance of sev- eral methods becomes clear; the extension of self- consistency to open-ended generations becomes trivial; and several promising modifications to self- consistency and output ensembling are exposed. In particular, modern MBR-like methods often do not apply the insights from research on MBR, suggest- ing that these methods could be further improved. In \u00a75, we show that some design choices, though seemingly intuitive to a practitioner accustomed to search-based decoding methods, should be avoided when applying MBR. 2 Formalization \u2217Denotes equal contribution. We begin with the basics of decoding and MBR. 2.1 Standard decoding Decoding from an autoregressive model (such as a transformer decoder) is performed tokenwise. The distribution at each decoding step is conditioned on the prior tokens and the input text: p(yi|y<i, x) The model is locally normalized; the probabilities of next tokens sum to 1. The probability of a se- quence under this global model distribution is p(y|x) = T (cid:89) p(yi|y<i, x) i=1 Given this distribution, there are several ways of extracting an output: by sampling at each de- coding step from the distribution over next tokens (often with some modification to the distribution, e.g. temperature, nucleus, or epsilon sampling; Holtzman et al. (2019)); by always choosing the most probable next token (i.e. greedy decoding); or by performing a search over some subset of the output space, guided by the distribution (e.g. beam search, best-first search). These methods generally return a single output; if multiple output candidates are present, the one with the maximum likelihood under the model distribution is returned. 2.2 Minimum Bayes Risk decoding The traditional formulation of MBR is as a mini- mization objective. Given a output space Y and a probability distribution over this space p(y|x), we compute the risk R(y\u2032) of a candidate decoding y\u2032 as the expected error (also called loss) under this distribution (Bickel and Doksum, 1977; Kumar and Byrne, 2004; Tromble et al., 2008). The MBR de- coding is then the y\u2032 within Y that minimizes risk: \u02c6y = argmin R(y\u2032) y\u2032\u2208Y = argmin y\u2032\u2208Y Ey|x[L(y, y\u2032)] = argmin y\u2032\u2208Y (cid:88) y\u2208Y L(y, y\u2032)p(y|x) We can trivially rewrite the risk as a maximiza- tion of gain (also called utility) rather than a mini- mization of error, where G(y, y\u2032) = \u2212L(y, y\u2032). Approximating risk Computing this sum over the space of all possible outputs Y is intractable (1) (2) (3) (4) (5) for most models.1 In these cases, we approximate the risk R(y\u2032) by using a subset of the full space Y \u2282 Y ; that is, instead of exact computation of the expectation, we approximate it with a sum over independent samples from p(y|x). Generally, this is performed by"}, {"question": " How is the risk of a candidate decoding calculated in Minimum Bayes Risk (MBR) decoding?", "answer": " The risk of a candidate decoding is calculated as the expected error under the probability distribution over the output space.", "ref_chunk": "3 2 0 2 t c O 2 ] L C . s c [ 1 v 7 8 3 1 0 . 0 1 3 2 : v i X r a It\u2019s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk Amanda Bertsch\u2217 and Alex Xie\u2217 and Graham Neubig and Matthew R. Gormley Carnegie Mellon University [abertsch, alexx]@cs.cmu.edu Abstract Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks with- out any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoret- ical justification for the performance of these methods, explaining some results that were pre- viously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area. than the consensus). MBR thus provides an alter- native to the more standard maximum-likelihood decoding; when a sample of sufficient size is taken, MBR almost uniformly outperforms beam search and single-output sampling across tasks, metrics, and datasets (see \u00a76). It is also notable in its flexibil- ity; in \u00a73 we organize and discuss several different design decisions that go into the use of MBR and how they affect the efficacy of the method. While MBR is rarely applied by name in modern NLP, a number of methods with similar intuitions have gained popularity. In \u00a74, we demonstrate that a number of generation techniques widely used with modern language models can be viewed as special instances of MBR: self-consistency (Wang et al., 2023) and its extensions, range vot- ing (Borgeaud and Emerson, 2020), output en- sembling (DeNero et al., 2010; Mart\u00ednez Lorenzo et al., 2023), and some types of density estimation (Kobayashi, 2018). This view exposes connections between seemingly disparate methods and presents theoretical justifications for existing empirical re- sults using these methods. We also discuss how insights from the MBR literature can inform the use of these other MBR-like methods. 1 Introduction \u201cSometimes innovation is only old ideas reappearing in new guises . . . [b]ut the new costumes are better made, of better materials, as well as more becoming: so research is not so much going round in circles as ascending a spiral.\u201d (Jones, 1994) Minimum Bayes Risk (MBR) decoding (Bickel and Doksum (1977); \u00a72) is a decoding method fol- lowing a simple intuition: when choosing a best output from a set of candidates, the desirable output should be both 1) high probability and 2) relatively consistent with the rest of the outputs (i.e., outputs that are not consistent with the other outputs are high risk\u2013 they may be dramatically better or worse With the framing of MBR, the theoretical jus- tification for the empirical performance of sev- eral methods becomes clear; the extension of self- consistency to open-ended generations becomes trivial; and several promising modifications to self- consistency and output ensembling are exposed. In particular, modern MBR-like methods often do not apply the insights from research on MBR, suggest- ing that these methods could be further improved. In \u00a75, we show that some design choices, though seemingly intuitive to a practitioner accustomed to search-based decoding methods, should be avoided when applying MBR. 2 Formalization \u2217Denotes equal contribution. We begin with the basics of decoding and MBR. 2.1 Standard decoding Decoding from an autoregressive model (such as a transformer decoder) is performed tokenwise. The distribution at each decoding step is conditioned on the prior tokens and the input text: p(yi|y<i, x) The model is locally normalized; the probabilities of next tokens sum to 1. The probability of a se- quence under this global model distribution is p(y|x) = T (cid:89) p(yi|y<i, x) i=1 Given this distribution, there are several ways of extracting an output: by sampling at each de- coding step from the distribution over next tokens (often with some modification to the distribution, e.g. temperature, nucleus, or epsilon sampling; Holtzman et al. (2019)); by always choosing the most probable next token (i.e. greedy decoding); or by performing a search over some subset of the output space, guided by the distribution (e.g. beam search, best-first search). These methods generally return a single output; if multiple output candidates are present, the one with the maximum likelihood under the model distribution is returned. 2.2 Minimum Bayes Risk decoding The traditional formulation of MBR is as a mini- mization objective. Given a output space Y and a probability distribution over this space p(y|x), we compute the risk R(y\u2032) of a candidate decoding y\u2032 as the expected error (also called loss) under this distribution (Bickel and Doksum, 1977; Kumar and Byrne, 2004; Tromble et al., 2008). The MBR de- coding is then the y\u2032 within Y that minimizes risk: \u02c6y = argmin R(y\u2032) y\u2032\u2208Y = argmin y\u2032\u2208Y Ey|x[L(y, y\u2032)] = argmin y\u2032\u2208Y (cid:88) y\u2208Y L(y, y\u2032)p(y|x) We can trivially rewrite the risk as a maximiza- tion of gain (also called utility) rather than a mini- mization of error, where G(y, y\u2032) = \u2212L(y, y\u2032). Approximating risk Computing this sum over the space of all possible outputs Y is intractable (1) (2) (3) (4) (5) for most models.1 In these cases, we approximate the risk R(y\u2032) by using a subset of the full space Y \u2282 Y ; that is, instead of exact computation of the expectation, we approximate it with a sum over independent samples from p(y|x). Generally, this is performed by"}, {"question": " What is the objective of MBR decoding?", "answer": " The objective of MBR decoding is to find the candidate decoding that minimizes the expected error under the probability distribution.", "ref_chunk": "3 2 0 2 t c O 2 ] L C . s c [ 1 v 7 8 3 1 0 . 0 1 3 2 : v i X r a It\u2019s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk Amanda Bertsch\u2217 and Alex Xie\u2217 and Graham Neubig and Matthew R. Gormley Carnegie Mellon University [abertsch, alexx]@cs.cmu.edu Abstract Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks with- out any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoret- ical justification for the performance of these methods, explaining some results that were pre- viously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area. than the consensus). MBR thus provides an alter- native to the more standard maximum-likelihood decoding; when a sample of sufficient size is taken, MBR almost uniformly outperforms beam search and single-output sampling across tasks, metrics, and datasets (see \u00a76). It is also notable in its flexibil- ity; in \u00a73 we organize and discuss several different design decisions that go into the use of MBR and how they affect the efficacy of the method. While MBR is rarely applied by name in modern NLP, a number of methods with similar intuitions have gained popularity. In \u00a74, we demonstrate that a number of generation techniques widely used with modern language models can be viewed as special instances of MBR: self-consistency (Wang et al., 2023) and its extensions, range vot- ing (Borgeaud and Emerson, 2020), output en- sembling (DeNero et al., 2010; Mart\u00ednez Lorenzo et al., 2023), and some types of density estimation (Kobayashi, 2018). This view exposes connections between seemingly disparate methods and presents theoretical justifications for existing empirical re- sults using these methods. We also discuss how insights from the MBR literature can inform the use of these other MBR-like methods. 1 Introduction \u201cSometimes innovation is only old ideas reappearing in new guises . . . [b]ut the new costumes are better made, of better materials, as well as more becoming: so research is not so much going round in circles as ascending a spiral.\u201d (Jones, 1994) Minimum Bayes Risk (MBR) decoding (Bickel and Doksum (1977); \u00a72) is a decoding method fol- lowing a simple intuition: when choosing a best output from a set of candidates, the desirable output should be both 1) high probability and 2) relatively consistent with the rest of the outputs (i.e., outputs that are not consistent with the other outputs are high risk\u2013 they may be dramatically better or worse With the framing of MBR, the theoretical jus- tification for the empirical performance of sev- eral methods becomes clear; the extension of self- consistency to open-ended generations becomes trivial; and several promising modifications to self- consistency and output ensembling are exposed. In particular, modern MBR-like methods often do not apply the insights from research on MBR, suggest- ing that these methods could be further improved. In \u00a75, we show that some design choices, though seemingly intuitive to a practitioner accustomed to search-based decoding methods, should be avoided when applying MBR. 2 Formalization \u2217Denotes equal contribution. We begin with the basics of decoding and MBR. 2.1 Standard decoding Decoding from an autoregressive model (such as a transformer decoder) is performed tokenwise. The distribution at each decoding step is conditioned on the prior tokens and the input text: p(yi|y<i, x) The model is locally normalized; the probabilities of next tokens sum to 1. The probability of a se- quence under this global model distribution is p(y|x) = T (cid:89) p(yi|y<i, x) i=1 Given this distribution, there are several ways of extracting an output: by sampling at each de- coding step from the distribution over next tokens (often with some modification to the distribution, e.g. temperature, nucleus, or epsilon sampling; Holtzman et al. (2019)); by always choosing the most probable next token (i.e. greedy decoding); or by performing a search over some subset of the output space, guided by the distribution (e.g. beam search, best-first search). These methods generally return a single output; if multiple output candidates are present, the one with the maximum likelihood under the model distribution is returned. 2.2 Minimum Bayes Risk decoding The traditional formulation of MBR is as a mini- mization objective. Given a output space Y and a probability distribution over this space p(y|x), we compute the risk R(y\u2032) of a candidate decoding y\u2032 as the expected error (also called loss) under this distribution (Bickel and Doksum, 1977; Kumar and Byrne, 2004; Tromble et al., 2008). The MBR de- coding is then the y\u2032 within Y that minimizes risk: \u02c6y = argmin R(y\u2032) y\u2032\u2208Y = argmin y\u2032\u2208Y Ey|x[L(y, y\u2032)] = argmin y\u2032\u2208Y (cid:88) y\u2208Y L(y, y\u2032)p(y|x) We can trivially rewrite the risk as a maximiza- tion of gain (also called utility) rather than a mini- mization of error, where G(y, y\u2032) = \u2212L(y, y\u2032). Approximating risk Computing this sum over the space of all possible outputs Y is intractable (1) (2) (3) (4) (5) for most models.1 In these cases, we approximate the risk R(y\u2032) by using a subset of the full space Y \u2282 Y ; that is, instead of exact computation of the expectation, we approximate it with a sum over independent samples from p(y|x). Generally, this is performed by"}, {"question": " How is the intractability issue of computing the risk in MBR decoding addressed?", "answer": " The risk is approximated by using a subset of the full output space instead of exact computation, typically by summing over independent samples from the probability distribution.", "ref_chunk": "3 2 0 2 t c O 2 ] L C . s c [ 1 v 7 8 3 1 0 . 0 1 3 2 : v i X r a It\u2019s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk Amanda Bertsch\u2217 and Alex Xie\u2217 and Graham Neubig and Matthew R. Gormley Carnegie Mellon University [abertsch, alexx]@cs.cmu.edu Abstract Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks with- out any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoret- ical justification for the performance of these methods, explaining some results that were pre- viously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area. than the consensus). MBR thus provides an alter- native to the more standard maximum-likelihood decoding; when a sample of sufficient size is taken, MBR almost uniformly outperforms beam search and single-output sampling across tasks, metrics, and datasets (see \u00a76). It is also notable in its flexibil- ity; in \u00a73 we organize and discuss several different design decisions that go into the use of MBR and how they affect the efficacy of the method. While MBR is rarely applied by name in modern NLP, a number of methods with similar intuitions have gained popularity. In \u00a74, we demonstrate that a number of generation techniques widely used with modern language models can be viewed as special instances of MBR: self-consistency (Wang et al., 2023) and its extensions, range vot- ing (Borgeaud and Emerson, 2020), output en- sembling (DeNero et al., 2010; Mart\u00ednez Lorenzo et al., 2023), and some types of density estimation (Kobayashi, 2018). This view exposes connections between seemingly disparate methods and presents theoretical justifications for existing empirical re- sults using these methods. We also discuss how insights from the MBR literature can inform the use of these other MBR-like methods. 1 Introduction \u201cSometimes innovation is only old ideas reappearing in new guises . . . [b]ut the new costumes are better made, of better materials, as well as more becoming: so research is not so much going round in circles as ascending a spiral.\u201d (Jones, 1994) Minimum Bayes Risk (MBR) decoding (Bickel and Doksum (1977); \u00a72) is a decoding method fol- lowing a simple intuition: when choosing a best output from a set of candidates, the desirable output should be both 1) high probability and 2) relatively consistent with the rest of the outputs (i.e., outputs that are not consistent with the other outputs are high risk\u2013 they may be dramatically better or worse With the framing of MBR, the theoretical jus- tification for the empirical performance of sev- eral methods becomes clear; the extension of self- consistency to open-ended generations becomes trivial; and several promising modifications to self- consistency and output ensembling are exposed. In particular, modern MBR-like methods often do not apply the insights from research on MBR, suggest- ing that these methods could be further improved. In \u00a75, we show that some design choices, though seemingly intuitive to a practitioner accustomed to search-based decoding methods, should be avoided when applying MBR. 2 Formalization \u2217Denotes equal contribution. We begin with the basics of decoding and MBR. 2.1 Standard decoding Decoding from an autoregressive model (such as a transformer decoder) is performed tokenwise. The distribution at each decoding step is conditioned on the prior tokens and the input text: p(yi|y<i, x) The model is locally normalized; the probabilities of next tokens sum to 1. The probability of a se- quence under this global model distribution is p(y|x) = T (cid:89) p(yi|y<i, x) i=1 Given this distribution, there are several ways of extracting an output: by sampling at each de- coding step from the distribution over next tokens (often with some modification to the distribution, e.g. temperature, nucleus, or epsilon sampling; Holtzman et al. (2019)); by always choosing the most probable next token (i.e. greedy decoding); or by performing a search over some subset of the output space, guided by the distribution (e.g. beam search, best-first search). These methods generally return a single output; if multiple output candidates are present, the one with the maximum likelihood under the model distribution is returned. 2.2 Minimum Bayes Risk decoding The traditional formulation of MBR is as a mini- mization objective. Given a output space Y and a probability distribution over this space p(y|x), we compute the risk R(y\u2032) of a candidate decoding y\u2032 as the expected error (also called loss) under this distribution (Bickel and Doksum, 1977; Kumar and Byrne, 2004; Tromble et al., 2008). The MBR de- coding is then the y\u2032 within Y that minimizes risk: \u02c6y = argmin R(y\u2032) y\u2032\u2208Y = argmin y\u2032\u2208Y Ey|x[L(y, y\u2032)] = argmin y\u2032\u2208Y (cid:88) y\u2208Y L(y, y\u2032)p(y|x) We can trivially rewrite the risk as a maximiza- tion of gain (also called utility) rather than a mini- mization of error, where G(y, y\u2032) = \u2212L(y, y\u2032). Approximating risk Computing this sum over the space of all possible outputs Y is intractable (1) (2) (3) (4) (5) for most models.1 In these cases, we approximate the risk R(y\u2032) by using a subset of the full space Y \u2282 Y ; that is, instead of exact computation of the expectation, we approximate it with a sum over independent samples from p(y|x). Generally, this is performed by"}], "doc_text": "3 2 0 2 t c O 2 ] L C . s c [ 1 v 7 8 3 1 0 . 0 1 3 2 : v i X r a It\u2019s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk Amanda Bertsch\u2217 and Alex Xie\u2217 and Graham Neubig and Matthew R. Gormley Carnegie Mellon University [abertsch, alexx]@cs.cmu.edu Abstract Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks with- out any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoret- ical justification for the performance of these methods, explaining some results that were pre- viously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area. than the consensus). MBR thus provides an alter- native to the more standard maximum-likelihood decoding; when a sample of sufficient size is taken, MBR almost uniformly outperforms beam search and single-output sampling across tasks, metrics, and datasets (see \u00a76). It is also notable in its flexibil- ity; in \u00a73 we organize and discuss several different design decisions that go into the use of MBR and how they affect the efficacy of the method. While MBR is rarely applied by name in modern NLP, a number of methods with similar intuitions have gained popularity. In \u00a74, we demonstrate that a number of generation techniques widely used with modern language models can be viewed as special instances of MBR: self-consistency (Wang et al., 2023) and its extensions, range vot- ing (Borgeaud and Emerson, 2020), output en- sembling (DeNero et al., 2010; Mart\u00ednez Lorenzo et al., 2023), and some types of density estimation (Kobayashi, 2018). This view exposes connections between seemingly disparate methods and presents theoretical justifications for existing empirical re- sults using these methods. We also discuss how insights from the MBR literature can inform the use of these other MBR-like methods. 1 Introduction \u201cSometimes innovation is only old ideas reappearing in new guises . . . [b]ut the new costumes are better made, of better materials, as well as more becoming: so research is not so much going round in circles as ascending a spiral.\u201d (Jones, 1994) Minimum Bayes Risk (MBR) decoding (Bickel and Doksum (1977); \u00a72) is a decoding method fol- lowing a simple intuition: when choosing a best output from a set of candidates, the desirable output should be both 1) high probability and 2) relatively consistent with the rest of the outputs (i.e., outputs that are not consistent with the other outputs are high risk\u2013 they may be dramatically better or worse With the framing of MBR, the theoretical jus- tification for the empirical performance of sev- eral methods becomes clear; the extension of self- consistency to open-ended generations becomes trivial; and several promising modifications to self- consistency and output ensembling are exposed. In particular, modern MBR-like methods often do not apply the insights from research on MBR, suggest- ing that these methods could be further improved. In \u00a75, we show that some design choices, though seemingly intuitive to a practitioner accustomed to search-based decoding methods, should be avoided when applying MBR. 2 Formalization \u2217Denotes equal contribution. We begin with the basics of decoding and MBR. 2.1 Standard decoding Decoding from an autoregressive model (such as a transformer decoder) is performed tokenwise. The distribution at each decoding step is conditioned on the prior tokens and the input text: p(yi|y<i, x) The model is locally normalized; the probabilities of next tokens sum to 1. The probability of a se- quence under this global model distribution is p(y|x) = T (cid:89) p(yi|y<i, x) i=1 Given this distribution, there are several ways of extracting an output: by sampling at each de- coding step from the distribution over next tokens (often with some modification to the distribution, e.g. temperature, nucleus, or epsilon sampling; Holtzman et al. (2019)); by always choosing the most probable next token (i.e. greedy decoding); or by performing a search over some subset of the output space, guided by the distribution (e.g. beam search, best-first search). These methods generally return a single output; if multiple output candidates are present, the one with the maximum likelihood under the model distribution is returned. 2.2 Minimum Bayes Risk decoding The traditional formulation of MBR is as a mini- mization objective. Given a output space Y and a probability distribution over this space p(y|x), we compute the risk R(y\u2032) of a candidate decoding y\u2032 as the expected error (also called loss) under this distribution (Bickel and Doksum, 1977; Kumar and Byrne, 2004; Tromble et al., 2008). The MBR de- coding is then the y\u2032 within Y that minimizes risk: \u02c6y = argmin R(y\u2032) y\u2032\u2208Y = argmin y\u2032\u2208Y Ey|x[L(y, y\u2032)] = argmin y\u2032\u2208Y (cid:88) y\u2208Y L(y, y\u2032)p(y|x) We can trivially rewrite the risk as a maximiza- tion of gain (also called utility) rather than a mini- mization of error, where G(y, y\u2032) = \u2212L(y, y\u2032). Approximating risk Computing this sum over the space of all possible outputs Y is intractable (1) (2) (3) (4) (5) for most models.1 In these cases, we approximate the risk R(y\u2032) by using a subset of the full space Y \u2282 Y ; that is, instead of exact computation of the expectation, we approximate it with a sum over independent samples from p(y|x). Generally, this is performed by"}