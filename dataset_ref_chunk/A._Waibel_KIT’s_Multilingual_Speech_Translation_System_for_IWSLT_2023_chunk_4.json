{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/A._Waibel_KIT\u2019s_Multilingual_Speech_Translation_System_for_IWSLT_2023_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What might be a side effect of the postnorm layer normalization?", "answer": " instabilities in training", "ref_chunk": "batch sizes showed more instabilities in training. This might be a side effect of the postnorm layer normaliza- tion (Nguyen and Salazar, 2019). The results of the baseline are shown in Row (1) of Table 7, with an average score of 37.4 BLEU4 on ACL dev. Data Diversification As motivated in \u00a72.3, we use data diversification as an alternative data aug- mentation method in absence of monolingual target data for backtranslation. As data diversification needs forward and backward translations on the training data, we additionally train a 10-to-English 4By default using tok.13a from sacreBLEU (Post, 2018), except for zh and ja where we use tok.zh and tok.ja-mecab-0.996-IPA. model to create the backward translations. Row (2) of Table 7 shows the results after data diversifica- tion on all languages pairs. On average, this data augmentation approach improves MT quality by 2.1 BLEU and (37.4 \u2192 39.5), and ST quality by 1.6 BLEU (31.1 \u2192 32.7). Adapters for Incremental Data Retraining on the new training data after diversification (Row (2) of Table 7) is time-consuming and costly. To adapt the initial model (Row (1) of Table 7) rapidly towards to the augmented data, we use adapters (Bapna and Firat, 2019; Philip et al., 2020). the adapters are target-language- In this case, specific. The adapters are inserted after each en- coder and decoder layer. We initialize from the trained baseline (Row (1) in Table 7), freeze trained parameters and update the adapters only. We use the efficient implementation from Baziotis et al. (2022). As shown in Row (3) of Table 7, only train- ing the adapters on the new diversified training data performs on par with the re-training setup in Row (2) (39.6 on MT and 32.7 on ST on average for ACL dev). These results demonstrate that adapters are suitable for fast and effective incremental learn- ing when additional training data emerges later. To our surprise, adding adapters to the model trained with full data diversification (Row (2) from Table 7) does not bring further gain. A similar observation was reported by Pires et al. (2023), who opted for training the full network from scratch along with adapters instead. In our case, it therefore would be interesting to see the impact of training on data diversification with adapters from scratch. Multilingual vs Bilingual To investigate the im- pact of interference from multiple target languages, in preliminary experiments, we also compare the multilingual and bilingual translation performance for selected language pairs. As shown in Table 8, compared to bilingual models, the multilingual model lags behind especially on higher-resource languages. Adding the adapters partly closes this gap. Note the score difference to main result table (Table 7) is because the preliminary experiments did not fully use diversified data for all languages. Model ACL dev tst-COMMON en-de en-ru en-fa en-de en-ru en-fa bilingual multilingual + adapters 41.0 39.8 40.9 20.0 19.5 20.2 24.2 23.6 23.7 34.3 34.1 34.7 22.7 21.9 22.2 16.0 15.9 16.3 Table 8: Comparison of bilingual vs multilingual trans- lation performance in BLEU (\u2191) on German (de), Rus- sian (ru), Farsi (fa), which are high-, mid-, low-resource in the training data (Table 3). Multilingual system falls behind bilingual system, while adapters partly closes the gap. Note the score difference to main result table (Table 7) is because the experiments here did not fully use diversification. Ensemble Although the models in Row (2) and (3) in Table 7 are trained on the same data and share the same base architecture, we expect their representations to be sufficiently different, as (3) additionally uses adapters. We therefore ensemble these two models. The results are in Row (4) of Ta- ble 7. On MT and ST, for ACL, ensembling shows an improvement of 0.6 and 0.5 BLEU respectively over the single models in Row (2) and (3). On TED, however, ensembling does not seem to im- pact the scores compared to the single models. One explanation is that the adapter model from Row (3) performs worse than its non-adapter counter- part (Row (2)) on TED, which limits the overall effectiveness of ensembling. kNN-MT We also adapt the MT model to the tar- get domain of scientific talks. A challenge is that we do not have sufficient training data to fully fine- tune the MT model towards the desired domain or style. In this case, we use kNN-MT (Khandelwal et al., 2021) to adapt the model at inference time. In kNN-MT, bitexts are passed through a trained Source (ASR output): ... in a zero shot evaluation setup, meaning that pre trained word embedding models are ap- plied out of the box without any additional fine tuning w/o kNN-MT (Table 7 row (4)): in einer Null-Shot-Bewertungs-Setup (zero-shot evaluation setup), was bedeutet, dass vorgebildete (pre-educated) Wort- Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung di- rekt angewendet werden. w/ kNN-MT (Table 7 row (5)): ... in einer Null-Shot- Bewertung (zero-shot evaluation), was bedeutet, dass vortrainierte (pretrained) Wort-Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung direkt angewendet werden. ... Source (ASR output): Hello. My name is Ramachandra, and I will present our paper. w/o kNN-MT (Table 7 row (4)): \u4f60\u597d (Hello; addressing a single person),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9 \u6211\u8981\u53d1\u8868 (publish)\u6211 \u4eec\u7684\u8bba\u6587 w/ kNN-MT (Table 7 row (5)): \u5927\u5bb6\u597d (Hi all; addressing a group of audience),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9, \u6211\u8981\u4ecb\u7ecd (intro- duce)\u6211\u4eec\u7684\u8bba\u6587\u3002 Table 9: Examples of kNN-MT improving transla- tion quality for en\u2192de (upper) and en\u2192zh (lower). kNN-MT creates more accurate terminology transla- tions (\u201cpre trained\u201d for en\u2192de) and create more context- appropriate translation (\u201cHello\u201d for en\u2192zh). MT model. For each target token, its decoder hid- den state is stored in a datastore. At inference time, based on the current decoder hidden state, k candi- date target tokens are retrieved from the datastore using a nearest neighbor lookup. The retrieved to- ken distribution is then interpolated with the MT target distribution, which in turn generates the out- put tokens. Hyperparameters for kNN-MT include the number of retrieved neighbors k, the tempera- ture for smoothing the kNN distribution T , and the interpolation weight w. In our experiments, we use systems (2) and (3) from Table"}, {"question": " What was the average BLEU4 score on ACL dev for the baseline results in Table 7?", "answer": " 37.4", "ref_chunk": "batch sizes showed more instabilities in training. This might be a side effect of the postnorm layer normaliza- tion (Nguyen and Salazar, 2019). The results of the baseline are shown in Row (1) of Table 7, with an average score of 37.4 BLEU4 on ACL dev. Data Diversification As motivated in \u00a72.3, we use data diversification as an alternative data aug- mentation method in absence of monolingual target data for backtranslation. As data diversification needs forward and backward translations on the training data, we additionally train a 10-to-English 4By default using tok.13a from sacreBLEU (Post, 2018), except for zh and ja where we use tok.zh and tok.ja-mecab-0.996-IPA. model to create the backward translations. Row (2) of Table 7 shows the results after data diversifica- tion on all languages pairs. On average, this data augmentation approach improves MT quality by 2.1 BLEU and (37.4 \u2192 39.5), and ST quality by 1.6 BLEU (31.1 \u2192 32.7). Adapters for Incremental Data Retraining on the new training data after diversification (Row (2) of Table 7) is time-consuming and costly. To adapt the initial model (Row (1) of Table 7) rapidly towards to the augmented data, we use adapters (Bapna and Firat, 2019; Philip et al., 2020). the adapters are target-language- In this case, specific. The adapters are inserted after each en- coder and decoder layer. We initialize from the trained baseline (Row (1) in Table 7), freeze trained parameters and update the adapters only. We use the efficient implementation from Baziotis et al. (2022). As shown in Row (3) of Table 7, only train- ing the adapters on the new diversified training data performs on par with the re-training setup in Row (2) (39.6 on MT and 32.7 on ST on average for ACL dev). These results demonstrate that adapters are suitable for fast and effective incremental learn- ing when additional training data emerges later. To our surprise, adding adapters to the model trained with full data diversification (Row (2) from Table 7) does not bring further gain. A similar observation was reported by Pires et al. (2023), who opted for training the full network from scratch along with adapters instead. In our case, it therefore would be interesting to see the impact of training on data diversification with adapters from scratch. Multilingual vs Bilingual To investigate the im- pact of interference from multiple target languages, in preliminary experiments, we also compare the multilingual and bilingual translation performance for selected language pairs. As shown in Table 8, compared to bilingual models, the multilingual model lags behind especially on higher-resource languages. Adding the adapters partly closes this gap. Note the score difference to main result table (Table 7) is because the preliminary experiments did not fully use diversified data for all languages. Model ACL dev tst-COMMON en-de en-ru en-fa en-de en-ru en-fa bilingual multilingual + adapters 41.0 39.8 40.9 20.0 19.5 20.2 24.2 23.6 23.7 34.3 34.1 34.7 22.7 21.9 22.2 16.0 15.9 16.3 Table 8: Comparison of bilingual vs multilingual trans- lation performance in BLEU (\u2191) on German (de), Rus- sian (ru), Farsi (fa), which are high-, mid-, low-resource in the training data (Table 3). Multilingual system falls behind bilingual system, while adapters partly closes the gap. Note the score difference to main result table (Table 7) is because the experiments here did not fully use diversification. Ensemble Although the models in Row (2) and (3) in Table 7 are trained on the same data and share the same base architecture, we expect their representations to be sufficiently different, as (3) additionally uses adapters. We therefore ensemble these two models. The results are in Row (4) of Ta- ble 7. On MT and ST, for ACL, ensembling shows an improvement of 0.6 and 0.5 BLEU respectively over the single models in Row (2) and (3). On TED, however, ensembling does not seem to im- pact the scores compared to the single models. One explanation is that the adapter model from Row (3) performs worse than its non-adapter counter- part (Row (2)) on TED, which limits the overall effectiveness of ensembling. kNN-MT We also adapt the MT model to the tar- get domain of scientific talks. A challenge is that we do not have sufficient training data to fully fine- tune the MT model towards the desired domain or style. In this case, we use kNN-MT (Khandelwal et al., 2021) to adapt the model at inference time. In kNN-MT, bitexts are passed through a trained Source (ASR output): ... in a zero shot evaluation setup, meaning that pre trained word embedding models are ap- plied out of the box without any additional fine tuning w/o kNN-MT (Table 7 row (4)): in einer Null-Shot-Bewertungs-Setup (zero-shot evaluation setup), was bedeutet, dass vorgebildete (pre-educated) Wort- Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung di- rekt angewendet werden. w/ kNN-MT (Table 7 row (5)): ... in einer Null-Shot- Bewertung (zero-shot evaluation), was bedeutet, dass vortrainierte (pretrained) Wort-Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung direkt angewendet werden. ... Source (ASR output): Hello. My name is Ramachandra, and I will present our paper. w/o kNN-MT (Table 7 row (4)): \u4f60\u597d (Hello; addressing a single person),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9 \u6211\u8981\u53d1\u8868 (publish)\u6211 \u4eec\u7684\u8bba\u6587 w/ kNN-MT (Table 7 row (5)): \u5927\u5bb6\u597d (Hi all; addressing a group of audience),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9, \u6211\u8981\u4ecb\u7ecd (intro- duce)\u6211\u4eec\u7684\u8bba\u6587\u3002 Table 9: Examples of kNN-MT improving transla- tion quality for en\u2192de (upper) and en\u2192zh (lower). kNN-MT creates more accurate terminology transla- tions (\u201cpre trained\u201d for en\u2192de) and create more context- appropriate translation (\u201cHello\u201d for en\u2192zh). MT model. For each target token, its decoder hid- den state is stored in a datastore. At inference time, based on the current decoder hidden state, k candi- date target tokens are retrieved from the datastore using a nearest neighbor lookup. The retrieved to- ken distribution is then interpolated with the MT target distribution, which in turn generates the out- put tokens. Hyperparameters for kNN-MT include the number of retrieved neighbors k, the tempera- ture for smoothing the kNN distribution T , and the interpolation weight w. In our experiments, we use systems (2) and (3) from Table"}, {"question": " What method was used as an alternative data augmentation method in the absence of monolingual target data for backtranslation?", "answer": " data diversification", "ref_chunk": "batch sizes showed more instabilities in training. This might be a side effect of the postnorm layer normaliza- tion (Nguyen and Salazar, 2019). The results of the baseline are shown in Row (1) of Table 7, with an average score of 37.4 BLEU4 on ACL dev. Data Diversification As motivated in \u00a72.3, we use data diversification as an alternative data aug- mentation method in absence of monolingual target data for backtranslation. As data diversification needs forward and backward translations on the training data, we additionally train a 10-to-English 4By default using tok.13a from sacreBLEU (Post, 2018), except for zh and ja where we use tok.zh and tok.ja-mecab-0.996-IPA. model to create the backward translations. Row (2) of Table 7 shows the results after data diversifica- tion on all languages pairs. On average, this data augmentation approach improves MT quality by 2.1 BLEU and (37.4 \u2192 39.5), and ST quality by 1.6 BLEU (31.1 \u2192 32.7). Adapters for Incremental Data Retraining on the new training data after diversification (Row (2) of Table 7) is time-consuming and costly. To adapt the initial model (Row (1) of Table 7) rapidly towards to the augmented data, we use adapters (Bapna and Firat, 2019; Philip et al., 2020). the adapters are target-language- In this case, specific. The adapters are inserted after each en- coder and decoder layer. We initialize from the trained baseline (Row (1) in Table 7), freeze trained parameters and update the adapters only. We use the efficient implementation from Baziotis et al. (2022). As shown in Row (3) of Table 7, only train- ing the adapters on the new diversified training data performs on par with the re-training setup in Row (2) (39.6 on MT and 32.7 on ST on average for ACL dev). These results demonstrate that adapters are suitable for fast and effective incremental learn- ing when additional training data emerges later. To our surprise, adding adapters to the model trained with full data diversification (Row (2) from Table 7) does not bring further gain. A similar observation was reported by Pires et al. (2023), who opted for training the full network from scratch along with adapters instead. In our case, it therefore would be interesting to see the impact of training on data diversification with adapters from scratch. Multilingual vs Bilingual To investigate the im- pact of interference from multiple target languages, in preliminary experiments, we also compare the multilingual and bilingual translation performance for selected language pairs. As shown in Table 8, compared to bilingual models, the multilingual model lags behind especially on higher-resource languages. Adding the adapters partly closes this gap. Note the score difference to main result table (Table 7) is because the preliminary experiments did not fully use diversified data for all languages. Model ACL dev tst-COMMON en-de en-ru en-fa en-de en-ru en-fa bilingual multilingual + adapters 41.0 39.8 40.9 20.0 19.5 20.2 24.2 23.6 23.7 34.3 34.1 34.7 22.7 21.9 22.2 16.0 15.9 16.3 Table 8: Comparison of bilingual vs multilingual trans- lation performance in BLEU (\u2191) on German (de), Rus- sian (ru), Farsi (fa), which are high-, mid-, low-resource in the training data (Table 3). Multilingual system falls behind bilingual system, while adapters partly closes the gap. Note the score difference to main result table (Table 7) is because the experiments here did not fully use diversification. Ensemble Although the models in Row (2) and (3) in Table 7 are trained on the same data and share the same base architecture, we expect their representations to be sufficiently different, as (3) additionally uses adapters. We therefore ensemble these two models. The results are in Row (4) of Ta- ble 7. On MT and ST, for ACL, ensembling shows an improvement of 0.6 and 0.5 BLEU respectively over the single models in Row (2) and (3). On TED, however, ensembling does not seem to im- pact the scores compared to the single models. One explanation is that the adapter model from Row (3) performs worse than its non-adapter counter- part (Row (2)) on TED, which limits the overall effectiveness of ensembling. kNN-MT We also adapt the MT model to the tar- get domain of scientific talks. A challenge is that we do not have sufficient training data to fully fine- tune the MT model towards the desired domain or style. In this case, we use kNN-MT (Khandelwal et al., 2021) to adapt the model at inference time. In kNN-MT, bitexts are passed through a trained Source (ASR output): ... in a zero shot evaluation setup, meaning that pre trained word embedding models are ap- plied out of the box without any additional fine tuning w/o kNN-MT (Table 7 row (4)): in einer Null-Shot-Bewertungs-Setup (zero-shot evaluation setup), was bedeutet, dass vorgebildete (pre-educated) Wort- Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung di- rekt angewendet werden. w/ kNN-MT (Table 7 row (5)): ... in einer Null-Shot- Bewertung (zero-shot evaluation), was bedeutet, dass vortrainierte (pretrained) Wort-Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung direkt angewendet werden. ... Source (ASR output): Hello. My name is Ramachandra, and I will present our paper. w/o kNN-MT (Table 7 row (4)): \u4f60\u597d (Hello; addressing a single person),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9 \u6211\u8981\u53d1\u8868 (publish)\u6211 \u4eec\u7684\u8bba\u6587 w/ kNN-MT (Table 7 row (5)): \u5927\u5bb6\u597d (Hi all; addressing a group of audience),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9, \u6211\u8981\u4ecb\u7ecd (intro- duce)\u6211\u4eec\u7684\u8bba\u6587\u3002 Table 9: Examples of kNN-MT improving transla- tion quality for en\u2192de (upper) and en\u2192zh (lower). kNN-MT creates more accurate terminology transla- tions (\u201cpre trained\u201d for en\u2192de) and create more context- appropriate translation (\u201cHello\u201d for en\u2192zh). MT model. For each target token, its decoder hid- den state is stored in a datastore. At inference time, based on the current decoder hidden state, k candi- date target tokens are retrieved from the datastore using a nearest neighbor lookup. The retrieved to- ken distribution is then interpolated with the MT target distribution, which in turn generates the out- put tokens. Hyperparameters for kNN-MT include the number of retrieved neighbors k, the tempera- ture for smoothing the kNN distribution T , and the interpolation weight w. In our experiments, we use systems (2) and (3) from Table"}, {"question": " How much does the data augmentation approach with data diversification improve MT quality on average according to Table 7?", "answer": " 2.1 BLEU", "ref_chunk": "batch sizes showed more instabilities in training. This might be a side effect of the postnorm layer normaliza- tion (Nguyen and Salazar, 2019). The results of the baseline are shown in Row (1) of Table 7, with an average score of 37.4 BLEU4 on ACL dev. Data Diversification As motivated in \u00a72.3, we use data diversification as an alternative data aug- mentation method in absence of monolingual target data for backtranslation. As data diversification needs forward and backward translations on the training data, we additionally train a 10-to-English 4By default using tok.13a from sacreBLEU (Post, 2018), except for zh and ja where we use tok.zh and tok.ja-mecab-0.996-IPA. model to create the backward translations. Row (2) of Table 7 shows the results after data diversifica- tion on all languages pairs. On average, this data augmentation approach improves MT quality by 2.1 BLEU and (37.4 \u2192 39.5), and ST quality by 1.6 BLEU (31.1 \u2192 32.7). Adapters for Incremental Data Retraining on the new training data after diversification (Row (2) of Table 7) is time-consuming and costly. To adapt the initial model (Row (1) of Table 7) rapidly towards to the augmented data, we use adapters (Bapna and Firat, 2019; Philip et al., 2020). the adapters are target-language- In this case, specific. The adapters are inserted after each en- coder and decoder layer. We initialize from the trained baseline (Row (1) in Table 7), freeze trained parameters and update the adapters only. We use the efficient implementation from Baziotis et al. (2022). As shown in Row (3) of Table 7, only train- ing the adapters on the new diversified training data performs on par with the re-training setup in Row (2) (39.6 on MT and 32.7 on ST on average for ACL dev). These results demonstrate that adapters are suitable for fast and effective incremental learn- ing when additional training data emerges later. To our surprise, adding adapters to the model trained with full data diversification (Row (2) from Table 7) does not bring further gain. A similar observation was reported by Pires et al. (2023), who opted for training the full network from scratch along with adapters instead. In our case, it therefore would be interesting to see the impact of training on data diversification with adapters from scratch. Multilingual vs Bilingual To investigate the im- pact of interference from multiple target languages, in preliminary experiments, we also compare the multilingual and bilingual translation performance for selected language pairs. As shown in Table 8, compared to bilingual models, the multilingual model lags behind especially on higher-resource languages. Adding the adapters partly closes this gap. Note the score difference to main result table (Table 7) is because the preliminary experiments did not fully use diversified data for all languages. Model ACL dev tst-COMMON en-de en-ru en-fa en-de en-ru en-fa bilingual multilingual + adapters 41.0 39.8 40.9 20.0 19.5 20.2 24.2 23.6 23.7 34.3 34.1 34.7 22.7 21.9 22.2 16.0 15.9 16.3 Table 8: Comparison of bilingual vs multilingual trans- lation performance in BLEU (\u2191) on German (de), Rus- sian (ru), Farsi (fa), which are high-, mid-, low-resource in the training data (Table 3). Multilingual system falls behind bilingual system, while adapters partly closes the gap. Note the score difference to main result table (Table 7) is because the experiments here did not fully use diversification. Ensemble Although the models in Row (2) and (3) in Table 7 are trained on the same data and share the same base architecture, we expect their representations to be sufficiently different, as (3) additionally uses adapters. We therefore ensemble these two models. The results are in Row (4) of Ta- ble 7. On MT and ST, for ACL, ensembling shows an improvement of 0.6 and 0.5 BLEU respectively over the single models in Row (2) and (3). On TED, however, ensembling does not seem to im- pact the scores compared to the single models. One explanation is that the adapter model from Row (3) performs worse than its non-adapter counter- part (Row (2)) on TED, which limits the overall effectiveness of ensembling. kNN-MT We also adapt the MT model to the tar- get domain of scientific talks. A challenge is that we do not have sufficient training data to fully fine- tune the MT model towards the desired domain or style. In this case, we use kNN-MT (Khandelwal et al., 2021) to adapt the model at inference time. In kNN-MT, bitexts are passed through a trained Source (ASR output): ... in a zero shot evaluation setup, meaning that pre trained word embedding models are ap- plied out of the box without any additional fine tuning w/o kNN-MT (Table 7 row (4)): in einer Null-Shot-Bewertungs-Setup (zero-shot evaluation setup), was bedeutet, dass vorgebildete (pre-educated) Wort- Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung di- rekt angewendet werden. w/ kNN-MT (Table 7 row (5)): ... in einer Null-Shot- Bewertung (zero-shot evaluation), was bedeutet, dass vortrainierte (pretrained) Wort-Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung direkt angewendet werden. ... Source (ASR output): Hello. My name is Ramachandra, and I will present our paper. w/o kNN-MT (Table 7 row (4)): \u4f60\u597d (Hello; addressing a single person),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9 \u6211\u8981\u53d1\u8868 (publish)\u6211 \u4eec\u7684\u8bba\u6587 w/ kNN-MT (Table 7 row (5)): \u5927\u5bb6\u597d (Hi all; addressing a group of audience),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9, \u6211\u8981\u4ecb\u7ecd (intro- duce)\u6211\u4eec\u7684\u8bba\u6587\u3002 Table 9: Examples of kNN-MT improving transla- tion quality for en\u2192de (upper) and en\u2192zh (lower). kNN-MT creates more accurate terminology transla- tions (\u201cpre trained\u201d for en\u2192de) and create more context- appropriate translation (\u201cHello\u201d for en\u2192zh). MT model. For each target token, its decoder hid- den state is stored in a datastore. At inference time, based on the current decoder hidden state, k candi- date target tokens are retrieved from the datastore using a nearest neighbor lookup. The retrieved to- ken distribution is then interpolated with the MT target distribution, which in turn generates the out- put tokens. Hyperparameters for kNN-MT include the number of retrieved neighbors k, the tempera- ture for smoothing the kNN distribution T , and the interpolation weight w. In our experiments, we use systems (2) and (3) from Table"}, {"question": " What do adapters do in the context of incremental data retraining?", "answer": " adapts the initial model towards the augmented data", "ref_chunk": "batch sizes showed more instabilities in training. This might be a side effect of the postnorm layer normaliza- tion (Nguyen and Salazar, 2019). The results of the baseline are shown in Row (1) of Table 7, with an average score of 37.4 BLEU4 on ACL dev. Data Diversification As motivated in \u00a72.3, we use data diversification as an alternative data aug- mentation method in absence of monolingual target data for backtranslation. As data diversification needs forward and backward translations on the training data, we additionally train a 10-to-English 4By default using tok.13a from sacreBLEU (Post, 2018), except for zh and ja where we use tok.zh and tok.ja-mecab-0.996-IPA. model to create the backward translations. Row (2) of Table 7 shows the results after data diversifica- tion on all languages pairs. On average, this data augmentation approach improves MT quality by 2.1 BLEU and (37.4 \u2192 39.5), and ST quality by 1.6 BLEU (31.1 \u2192 32.7). Adapters for Incremental Data Retraining on the new training data after diversification (Row (2) of Table 7) is time-consuming and costly. To adapt the initial model (Row (1) of Table 7) rapidly towards to the augmented data, we use adapters (Bapna and Firat, 2019; Philip et al., 2020). the adapters are target-language- In this case, specific. The adapters are inserted after each en- coder and decoder layer. We initialize from the trained baseline (Row (1) in Table 7), freeze trained parameters and update the adapters only. We use the efficient implementation from Baziotis et al. (2022). As shown in Row (3) of Table 7, only train- ing the adapters on the new diversified training data performs on par with the re-training setup in Row (2) (39.6 on MT and 32.7 on ST on average for ACL dev). These results demonstrate that adapters are suitable for fast and effective incremental learn- ing when additional training data emerges later. To our surprise, adding adapters to the model trained with full data diversification (Row (2) from Table 7) does not bring further gain. A similar observation was reported by Pires et al. (2023), who opted for training the full network from scratch along with adapters instead. In our case, it therefore would be interesting to see the impact of training on data diversification with adapters from scratch. Multilingual vs Bilingual To investigate the im- pact of interference from multiple target languages, in preliminary experiments, we also compare the multilingual and bilingual translation performance for selected language pairs. As shown in Table 8, compared to bilingual models, the multilingual model lags behind especially on higher-resource languages. Adding the adapters partly closes this gap. Note the score difference to main result table (Table 7) is because the preliminary experiments did not fully use diversified data for all languages. Model ACL dev tst-COMMON en-de en-ru en-fa en-de en-ru en-fa bilingual multilingual + adapters 41.0 39.8 40.9 20.0 19.5 20.2 24.2 23.6 23.7 34.3 34.1 34.7 22.7 21.9 22.2 16.0 15.9 16.3 Table 8: Comparison of bilingual vs multilingual trans- lation performance in BLEU (\u2191) on German (de), Rus- sian (ru), Farsi (fa), which are high-, mid-, low-resource in the training data (Table 3). Multilingual system falls behind bilingual system, while adapters partly closes the gap. Note the score difference to main result table (Table 7) is because the experiments here did not fully use diversification. Ensemble Although the models in Row (2) and (3) in Table 7 are trained on the same data and share the same base architecture, we expect their representations to be sufficiently different, as (3) additionally uses adapters. We therefore ensemble these two models. The results are in Row (4) of Ta- ble 7. On MT and ST, for ACL, ensembling shows an improvement of 0.6 and 0.5 BLEU respectively over the single models in Row (2) and (3). On TED, however, ensembling does not seem to im- pact the scores compared to the single models. One explanation is that the adapter model from Row (3) performs worse than its non-adapter counter- part (Row (2)) on TED, which limits the overall effectiveness of ensembling. kNN-MT We also adapt the MT model to the tar- get domain of scientific talks. A challenge is that we do not have sufficient training data to fully fine- tune the MT model towards the desired domain or style. In this case, we use kNN-MT (Khandelwal et al., 2021) to adapt the model at inference time. In kNN-MT, bitexts are passed through a trained Source (ASR output): ... in a zero shot evaluation setup, meaning that pre trained word embedding models are ap- plied out of the box without any additional fine tuning w/o kNN-MT (Table 7 row (4)): in einer Null-Shot-Bewertungs-Setup (zero-shot evaluation setup), was bedeutet, dass vorgebildete (pre-educated) Wort- Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung di- rekt angewendet werden. w/ kNN-MT (Table 7 row (5)): ... in einer Null-Shot- Bewertung (zero-shot evaluation), was bedeutet, dass vortrainierte (pretrained) Wort-Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung direkt angewendet werden. ... Source (ASR output): Hello. My name is Ramachandra, and I will present our paper. w/o kNN-MT (Table 7 row (4)): \u4f60\u597d (Hello; addressing a single person),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9 \u6211\u8981\u53d1\u8868 (publish)\u6211 \u4eec\u7684\u8bba\u6587 w/ kNN-MT (Table 7 row (5)): \u5927\u5bb6\u597d (Hi all; addressing a group of audience),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9, \u6211\u8981\u4ecb\u7ecd (intro- duce)\u6211\u4eec\u7684\u8bba\u6587\u3002 Table 9: Examples of kNN-MT improving transla- tion quality for en\u2192de (upper) and en\u2192zh (lower). kNN-MT creates more accurate terminology transla- tions (\u201cpre trained\u201d for en\u2192de) and create more context- appropriate translation (\u201cHello\u201d for en\u2192zh). MT model. For each target token, its decoder hid- den state is stored in a datastore. At inference time, based on the current decoder hidden state, k candi- date target tokens are retrieved from the datastore using a nearest neighbor lookup. The retrieved to- ken distribution is then interpolated with the MT target distribution, which in turn generates the out- put tokens. Hyperparameters for kNN-MT include the number of retrieved neighbors k, the tempera- ture for smoothing the kNN distribution T , and the interpolation weight w. In our experiments, we use systems (2) and (3) from Table"}, {"question": " What is the purpose of freezing trained parameters during adapter usage?", "answer": " to update the adapters only", "ref_chunk": "batch sizes showed more instabilities in training. This might be a side effect of the postnorm layer normaliza- tion (Nguyen and Salazar, 2019). The results of the baseline are shown in Row (1) of Table 7, with an average score of 37.4 BLEU4 on ACL dev. Data Diversification As motivated in \u00a72.3, we use data diversification as an alternative data aug- mentation method in absence of monolingual target data for backtranslation. As data diversification needs forward and backward translations on the training data, we additionally train a 10-to-English 4By default using tok.13a from sacreBLEU (Post, 2018), except for zh and ja where we use tok.zh and tok.ja-mecab-0.996-IPA. model to create the backward translations. Row (2) of Table 7 shows the results after data diversifica- tion on all languages pairs. On average, this data augmentation approach improves MT quality by 2.1 BLEU and (37.4 \u2192 39.5), and ST quality by 1.6 BLEU (31.1 \u2192 32.7). Adapters for Incremental Data Retraining on the new training data after diversification (Row (2) of Table 7) is time-consuming and costly. To adapt the initial model (Row (1) of Table 7) rapidly towards to the augmented data, we use adapters (Bapna and Firat, 2019; Philip et al., 2020). the adapters are target-language- In this case, specific. The adapters are inserted after each en- coder and decoder layer. We initialize from the trained baseline (Row (1) in Table 7), freeze trained parameters and update the adapters only. We use the efficient implementation from Baziotis et al. (2022). As shown in Row (3) of Table 7, only train- ing the adapters on the new diversified training data performs on par with the re-training setup in Row (2) (39.6 on MT and 32.7 on ST on average for ACL dev). These results demonstrate that adapters are suitable for fast and effective incremental learn- ing when additional training data emerges later. To our surprise, adding adapters to the model trained with full data diversification (Row (2) from Table 7) does not bring further gain. A similar observation was reported by Pires et al. (2023), who opted for training the full network from scratch along with adapters instead. In our case, it therefore would be interesting to see the impact of training on data diversification with adapters from scratch. Multilingual vs Bilingual To investigate the im- pact of interference from multiple target languages, in preliminary experiments, we also compare the multilingual and bilingual translation performance for selected language pairs. As shown in Table 8, compared to bilingual models, the multilingual model lags behind especially on higher-resource languages. Adding the adapters partly closes this gap. Note the score difference to main result table (Table 7) is because the preliminary experiments did not fully use diversified data for all languages. Model ACL dev tst-COMMON en-de en-ru en-fa en-de en-ru en-fa bilingual multilingual + adapters 41.0 39.8 40.9 20.0 19.5 20.2 24.2 23.6 23.7 34.3 34.1 34.7 22.7 21.9 22.2 16.0 15.9 16.3 Table 8: Comparison of bilingual vs multilingual trans- lation performance in BLEU (\u2191) on German (de), Rus- sian (ru), Farsi (fa), which are high-, mid-, low-resource in the training data (Table 3). Multilingual system falls behind bilingual system, while adapters partly closes the gap. Note the score difference to main result table (Table 7) is because the experiments here did not fully use diversification. Ensemble Although the models in Row (2) and (3) in Table 7 are trained on the same data and share the same base architecture, we expect their representations to be sufficiently different, as (3) additionally uses adapters. We therefore ensemble these two models. The results are in Row (4) of Ta- ble 7. On MT and ST, for ACL, ensembling shows an improvement of 0.6 and 0.5 BLEU respectively over the single models in Row (2) and (3). On TED, however, ensembling does not seem to im- pact the scores compared to the single models. One explanation is that the adapter model from Row (3) performs worse than its non-adapter counter- part (Row (2)) on TED, which limits the overall effectiveness of ensembling. kNN-MT We also adapt the MT model to the tar- get domain of scientific talks. A challenge is that we do not have sufficient training data to fully fine- tune the MT model towards the desired domain or style. In this case, we use kNN-MT (Khandelwal et al., 2021) to adapt the model at inference time. In kNN-MT, bitexts are passed through a trained Source (ASR output): ... in a zero shot evaluation setup, meaning that pre trained word embedding models are ap- plied out of the box without any additional fine tuning w/o kNN-MT (Table 7 row (4)): in einer Null-Shot-Bewertungs-Setup (zero-shot evaluation setup), was bedeutet, dass vorgebildete (pre-educated) Wort- Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung di- rekt angewendet werden. w/ kNN-MT (Table 7 row (5)): ... in einer Null-Shot- Bewertung (zero-shot evaluation), was bedeutet, dass vortrainierte (pretrained) Wort-Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung direkt angewendet werden. ... Source (ASR output): Hello. My name is Ramachandra, and I will present our paper. w/o kNN-MT (Table 7 row (4)): \u4f60\u597d (Hello; addressing a single person),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9 \u6211\u8981\u53d1\u8868 (publish)\u6211 \u4eec\u7684\u8bba\u6587 w/ kNN-MT (Table 7 row (5)): \u5927\u5bb6\u597d (Hi all; addressing a group of audience),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9, \u6211\u8981\u4ecb\u7ecd (intro- duce)\u6211\u4eec\u7684\u8bba\u6587\u3002 Table 9: Examples of kNN-MT improving transla- tion quality for en\u2192de (upper) and en\u2192zh (lower). kNN-MT creates more accurate terminology transla- tions (\u201cpre trained\u201d for en\u2192de) and create more context- appropriate translation (\u201cHello\u201d for en\u2192zh). MT model. For each target token, its decoder hid- den state is stored in a datastore. At inference time, based on the current decoder hidden state, k candi- date target tokens are retrieved from the datastore using a nearest neighbor lookup. The retrieved to- ken distribution is then interpolated with the MT target distribution, which in turn generates the out- put tokens. Hyperparameters for kNN-MT include the number of retrieved neighbors k, the tempera- ture for smoothing the kNN distribution T , and the interpolation weight w. In our experiments, we use systems (2) and (3) from Table"}, {"question": " How does training the adapters on the new diversified training data compare to the re-training setup on average for ACL dev?", "answer": " performs on par (39.6 on MT and 32.7 on ST)", "ref_chunk": "batch sizes showed more instabilities in training. This might be a side effect of the postnorm layer normaliza- tion (Nguyen and Salazar, 2019). The results of the baseline are shown in Row (1) of Table 7, with an average score of 37.4 BLEU4 on ACL dev. Data Diversification As motivated in \u00a72.3, we use data diversification as an alternative data aug- mentation method in absence of monolingual target data for backtranslation. As data diversification needs forward and backward translations on the training data, we additionally train a 10-to-English 4By default using tok.13a from sacreBLEU (Post, 2018), except for zh and ja where we use tok.zh and tok.ja-mecab-0.996-IPA. model to create the backward translations. Row (2) of Table 7 shows the results after data diversifica- tion on all languages pairs. On average, this data augmentation approach improves MT quality by 2.1 BLEU and (37.4 \u2192 39.5), and ST quality by 1.6 BLEU (31.1 \u2192 32.7). Adapters for Incremental Data Retraining on the new training data after diversification (Row (2) of Table 7) is time-consuming and costly. To adapt the initial model (Row (1) of Table 7) rapidly towards to the augmented data, we use adapters (Bapna and Firat, 2019; Philip et al., 2020). the adapters are target-language- In this case, specific. The adapters are inserted after each en- coder and decoder layer. We initialize from the trained baseline (Row (1) in Table 7), freeze trained parameters and update the adapters only. We use the efficient implementation from Baziotis et al. (2022). As shown in Row (3) of Table 7, only train- ing the adapters on the new diversified training data performs on par with the re-training setup in Row (2) (39.6 on MT and 32.7 on ST on average for ACL dev). These results demonstrate that adapters are suitable for fast and effective incremental learn- ing when additional training data emerges later. To our surprise, adding adapters to the model trained with full data diversification (Row (2) from Table 7) does not bring further gain. A similar observation was reported by Pires et al. (2023), who opted for training the full network from scratch along with adapters instead. In our case, it therefore would be interesting to see the impact of training on data diversification with adapters from scratch. Multilingual vs Bilingual To investigate the im- pact of interference from multiple target languages, in preliminary experiments, we also compare the multilingual and bilingual translation performance for selected language pairs. As shown in Table 8, compared to bilingual models, the multilingual model lags behind especially on higher-resource languages. Adding the adapters partly closes this gap. Note the score difference to main result table (Table 7) is because the preliminary experiments did not fully use diversified data for all languages. Model ACL dev tst-COMMON en-de en-ru en-fa en-de en-ru en-fa bilingual multilingual + adapters 41.0 39.8 40.9 20.0 19.5 20.2 24.2 23.6 23.7 34.3 34.1 34.7 22.7 21.9 22.2 16.0 15.9 16.3 Table 8: Comparison of bilingual vs multilingual trans- lation performance in BLEU (\u2191) on German (de), Rus- sian (ru), Farsi (fa), which are high-, mid-, low-resource in the training data (Table 3). Multilingual system falls behind bilingual system, while adapters partly closes the gap. Note the score difference to main result table (Table 7) is because the experiments here did not fully use diversification. Ensemble Although the models in Row (2) and (3) in Table 7 are trained on the same data and share the same base architecture, we expect their representations to be sufficiently different, as (3) additionally uses adapters. We therefore ensemble these two models. The results are in Row (4) of Ta- ble 7. On MT and ST, for ACL, ensembling shows an improvement of 0.6 and 0.5 BLEU respectively over the single models in Row (2) and (3). On TED, however, ensembling does not seem to im- pact the scores compared to the single models. One explanation is that the adapter model from Row (3) performs worse than its non-adapter counter- part (Row (2)) on TED, which limits the overall effectiveness of ensembling. kNN-MT We also adapt the MT model to the tar- get domain of scientific talks. A challenge is that we do not have sufficient training data to fully fine- tune the MT model towards the desired domain or style. In this case, we use kNN-MT (Khandelwal et al., 2021) to adapt the model at inference time. In kNN-MT, bitexts are passed through a trained Source (ASR output): ... in a zero shot evaluation setup, meaning that pre trained word embedding models are ap- plied out of the box without any additional fine tuning w/o kNN-MT (Table 7 row (4)): in einer Null-Shot-Bewertungs-Setup (zero-shot evaluation setup), was bedeutet, dass vorgebildete (pre-educated) Wort- Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung di- rekt angewendet werden. w/ kNN-MT (Table 7 row (5)): ... in einer Null-Shot- Bewertung (zero-shot evaluation), was bedeutet, dass vortrainierte (pretrained) Wort-Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung direkt angewendet werden. ... Source (ASR output): Hello. My name is Ramachandra, and I will present our paper. w/o kNN-MT (Table 7 row (4)): \u4f60\u597d (Hello; addressing a single person),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9 \u6211\u8981\u53d1\u8868 (publish)\u6211 \u4eec\u7684\u8bba\u6587 w/ kNN-MT (Table 7 row (5)): \u5927\u5bb6\u597d (Hi all; addressing a group of audience),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9, \u6211\u8981\u4ecb\u7ecd (intro- duce)\u6211\u4eec\u7684\u8bba\u6587\u3002 Table 9: Examples of kNN-MT improving transla- tion quality for en\u2192de (upper) and en\u2192zh (lower). kNN-MT creates more accurate terminology transla- tions (\u201cpre trained\u201d for en\u2192de) and create more context- appropriate translation (\u201cHello\u201d for en\u2192zh). MT model. For each target token, its decoder hid- den state is stored in a datastore. At inference time, based on the current decoder hidden state, k candi- date target tokens are retrieved from the datastore using a nearest neighbor lookup. The retrieved to- ken distribution is then interpolated with the MT target distribution, which in turn generates the out- put tokens. Hyperparameters for kNN-MT include the number of retrieved neighbors k, the tempera- ture for smoothing the kNN distribution T , and the interpolation weight w. In our experiments, we use systems (2) and (3) from Table"}, {"question": " Why does adding adapters to the model trained with full data diversification not bring further gain?", "answer": " similar observation to Pires et al. (2023) opting for training the full network from scratch with adapters", "ref_chunk": "batch sizes showed more instabilities in training. This might be a side effect of the postnorm layer normaliza- tion (Nguyen and Salazar, 2019). The results of the baseline are shown in Row (1) of Table 7, with an average score of 37.4 BLEU4 on ACL dev. Data Diversification As motivated in \u00a72.3, we use data diversification as an alternative data aug- mentation method in absence of monolingual target data for backtranslation. As data diversification needs forward and backward translations on the training data, we additionally train a 10-to-English 4By default using tok.13a from sacreBLEU (Post, 2018), except for zh and ja where we use tok.zh and tok.ja-mecab-0.996-IPA. model to create the backward translations. Row (2) of Table 7 shows the results after data diversifica- tion on all languages pairs. On average, this data augmentation approach improves MT quality by 2.1 BLEU and (37.4 \u2192 39.5), and ST quality by 1.6 BLEU (31.1 \u2192 32.7). Adapters for Incremental Data Retraining on the new training data after diversification (Row (2) of Table 7) is time-consuming and costly. To adapt the initial model (Row (1) of Table 7) rapidly towards to the augmented data, we use adapters (Bapna and Firat, 2019; Philip et al., 2020). the adapters are target-language- In this case, specific. The adapters are inserted after each en- coder and decoder layer. We initialize from the trained baseline (Row (1) in Table 7), freeze trained parameters and update the adapters only. We use the efficient implementation from Baziotis et al. (2022). As shown in Row (3) of Table 7, only train- ing the adapters on the new diversified training data performs on par with the re-training setup in Row (2) (39.6 on MT and 32.7 on ST on average for ACL dev). These results demonstrate that adapters are suitable for fast and effective incremental learn- ing when additional training data emerges later. To our surprise, adding adapters to the model trained with full data diversification (Row (2) from Table 7) does not bring further gain. A similar observation was reported by Pires et al. (2023), who opted for training the full network from scratch along with adapters instead. In our case, it therefore would be interesting to see the impact of training on data diversification with adapters from scratch. Multilingual vs Bilingual To investigate the im- pact of interference from multiple target languages, in preliminary experiments, we also compare the multilingual and bilingual translation performance for selected language pairs. As shown in Table 8, compared to bilingual models, the multilingual model lags behind especially on higher-resource languages. Adding the adapters partly closes this gap. Note the score difference to main result table (Table 7) is because the preliminary experiments did not fully use diversified data for all languages. Model ACL dev tst-COMMON en-de en-ru en-fa en-de en-ru en-fa bilingual multilingual + adapters 41.0 39.8 40.9 20.0 19.5 20.2 24.2 23.6 23.7 34.3 34.1 34.7 22.7 21.9 22.2 16.0 15.9 16.3 Table 8: Comparison of bilingual vs multilingual trans- lation performance in BLEU (\u2191) on German (de), Rus- sian (ru), Farsi (fa), which are high-, mid-, low-resource in the training data (Table 3). Multilingual system falls behind bilingual system, while adapters partly closes the gap. Note the score difference to main result table (Table 7) is because the experiments here did not fully use diversification. Ensemble Although the models in Row (2) and (3) in Table 7 are trained on the same data and share the same base architecture, we expect their representations to be sufficiently different, as (3) additionally uses adapters. We therefore ensemble these two models. The results are in Row (4) of Ta- ble 7. On MT and ST, for ACL, ensembling shows an improvement of 0.6 and 0.5 BLEU respectively over the single models in Row (2) and (3). On TED, however, ensembling does not seem to im- pact the scores compared to the single models. One explanation is that the adapter model from Row (3) performs worse than its non-adapter counter- part (Row (2)) on TED, which limits the overall effectiveness of ensembling. kNN-MT We also adapt the MT model to the tar- get domain of scientific talks. A challenge is that we do not have sufficient training data to fully fine- tune the MT model towards the desired domain or style. In this case, we use kNN-MT (Khandelwal et al., 2021) to adapt the model at inference time. In kNN-MT, bitexts are passed through a trained Source (ASR output): ... in a zero shot evaluation setup, meaning that pre trained word embedding models are ap- plied out of the box without any additional fine tuning w/o kNN-MT (Table 7 row (4)): in einer Null-Shot-Bewertungs-Setup (zero-shot evaluation setup), was bedeutet, dass vorgebildete (pre-educated) Wort- Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung di- rekt angewendet werden. w/ kNN-MT (Table 7 row (5)): ... in einer Null-Shot- Bewertung (zero-shot evaluation), was bedeutet, dass vortrainierte (pretrained) Wort-Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung direkt angewendet werden. ... Source (ASR output): Hello. My name is Ramachandra, and I will present our paper. w/o kNN-MT (Table 7 row (4)): \u4f60\u597d (Hello; addressing a single person),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9 \u6211\u8981\u53d1\u8868 (publish)\u6211 \u4eec\u7684\u8bba\u6587 w/ kNN-MT (Table 7 row (5)): \u5927\u5bb6\u597d (Hi all; addressing a group of audience),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9, \u6211\u8981\u4ecb\u7ecd (intro- duce)\u6211\u4eec\u7684\u8bba\u6587\u3002 Table 9: Examples of kNN-MT improving transla- tion quality for en\u2192de (upper) and en\u2192zh (lower). kNN-MT creates more accurate terminology transla- tions (\u201cpre trained\u201d for en\u2192de) and create more context- appropriate translation (\u201cHello\u201d for en\u2192zh). MT model. For each target token, its decoder hid- den state is stored in a datastore. At inference time, based on the current decoder hidden state, k candi- date target tokens are retrieved from the datastore using a nearest neighbor lookup. The retrieved to- ken distribution is then interpolated with the MT target distribution, which in turn generates the out- put tokens. Hyperparameters for kNN-MT include the number of retrieved neighbors k, the tempera- ture for smoothing the kNN distribution T , and the interpolation weight w. In our experiments, we use systems (2) and (3) from Table"}, {"question": " In Table 8, which model lags behind especially on higher-resource languages?", "answer": " multilingual model", "ref_chunk": "batch sizes showed more instabilities in training. This might be a side effect of the postnorm layer normaliza- tion (Nguyen and Salazar, 2019). The results of the baseline are shown in Row (1) of Table 7, with an average score of 37.4 BLEU4 on ACL dev. Data Diversification As motivated in \u00a72.3, we use data diversification as an alternative data aug- mentation method in absence of monolingual target data for backtranslation. As data diversification needs forward and backward translations on the training data, we additionally train a 10-to-English 4By default using tok.13a from sacreBLEU (Post, 2018), except for zh and ja where we use tok.zh and tok.ja-mecab-0.996-IPA. model to create the backward translations. Row (2) of Table 7 shows the results after data diversifica- tion on all languages pairs. On average, this data augmentation approach improves MT quality by 2.1 BLEU and (37.4 \u2192 39.5), and ST quality by 1.6 BLEU (31.1 \u2192 32.7). Adapters for Incremental Data Retraining on the new training data after diversification (Row (2) of Table 7) is time-consuming and costly. To adapt the initial model (Row (1) of Table 7) rapidly towards to the augmented data, we use adapters (Bapna and Firat, 2019; Philip et al., 2020). the adapters are target-language- In this case, specific. The adapters are inserted after each en- coder and decoder layer. We initialize from the trained baseline (Row (1) in Table 7), freeze trained parameters and update the adapters only. We use the efficient implementation from Baziotis et al. (2022). As shown in Row (3) of Table 7, only train- ing the adapters on the new diversified training data performs on par with the re-training setup in Row (2) (39.6 on MT and 32.7 on ST on average for ACL dev). These results demonstrate that adapters are suitable for fast and effective incremental learn- ing when additional training data emerges later. To our surprise, adding adapters to the model trained with full data diversification (Row (2) from Table 7) does not bring further gain. A similar observation was reported by Pires et al. (2023), who opted for training the full network from scratch along with adapters instead. In our case, it therefore would be interesting to see the impact of training on data diversification with adapters from scratch. Multilingual vs Bilingual To investigate the im- pact of interference from multiple target languages, in preliminary experiments, we also compare the multilingual and bilingual translation performance for selected language pairs. As shown in Table 8, compared to bilingual models, the multilingual model lags behind especially on higher-resource languages. Adding the adapters partly closes this gap. Note the score difference to main result table (Table 7) is because the preliminary experiments did not fully use diversified data for all languages. Model ACL dev tst-COMMON en-de en-ru en-fa en-de en-ru en-fa bilingual multilingual + adapters 41.0 39.8 40.9 20.0 19.5 20.2 24.2 23.6 23.7 34.3 34.1 34.7 22.7 21.9 22.2 16.0 15.9 16.3 Table 8: Comparison of bilingual vs multilingual trans- lation performance in BLEU (\u2191) on German (de), Rus- sian (ru), Farsi (fa), which are high-, mid-, low-resource in the training data (Table 3). Multilingual system falls behind bilingual system, while adapters partly closes the gap. Note the score difference to main result table (Table 7) is because the experiments here did not fully use diversification. Ensemble Although the models in Row (2) and (3) in Table 7 are trained on the same data and share the same base architecture, we expect their representations to be sufficiently different, as (3) additionally uses adapters. We therefore ensemble these two models. The results are in Row (4) of Ta- ble 7. On MT and ST, for ACL, ensembling shows an improvement of 0.6 and 0.5 BLEU respectively over the single models in Row (2) and (3). On TED, however, ensembling does not seem to im- pact the scores compared to the single models. One explanation is that the adapter model from Row (3) performs worse than its non-adapter counter- part (Row (2)) on TED, which limits the overall effectiveness of ensembling. kNN-MT We also adapt the MT model to the tar- get domain of scientific talks. A challenge is that we do not have sufficient training data to fully fine- tune the MT model towards the desired domain or style. In this case, we use kNN-MT (Khandelwal et al., 2021) to adapt the model at inference time. In kNN-MT, bitexts are passed through a trained Source (ASR output): ... in a zero shot evaluation setup, meaning that pre trained word embedding models are ap- plied out of the box without any additional fine tuning w/o kNN-MT (Table 7 row (4)): in einer Null-Shot-Bewertungs-Setup (zero-shot evaluation setup), was bedeutet, dass vorgebildete (pre-educated) Wort- Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung di- rekt angewendet werden. w/ kNN-MT (Table 7 row (5)): ... in einer Null-Shot- Bewertung (zero-shot evaluation), was bedeutet, dass vortrainierte (pretrained) Wort-Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung direkt angewendet werden. ... Source (ASR output): Hello. My name is Ramachandra, and I will present our paper. w/o kNN-MT (Table 7 row (4)): \u4f60\u597d (Hello; addressing a single person),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9 \u6211\u8981\u53d1\u8868 (publish)\u6211 \u4eec\u7684\u8bba\u6587 w/ kNN-MT (Table 7 row (5)): \u5927\u5bb6\u597d (Hi all; addressing a group of audience),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9, \u6211\u8981\u4ecb\u7ecd (intro- duce)\u6211\u4eec\u7684\u8bba\u6587\u3002 Table 9: Examples of kNN-MT improving transla- tion quality for en\u2192de (upper) and en\u2192zh (lower). kNN-MT creates more accurate terminology transla- tions (\u201cpre trained\u201d for en\u2192de) and create more context- appropriate translation (\u201cHello\u201d for en\u2192zh). MT model. For each target token, its decoder hid- den state is stored in a datastore. At inference time, based on the current decoder hidden state, k candi- date target tokens are retrieved from the datastore using a nearest neighbor lookup. The retrieved to- ken distribution is then interpolated with the MT target distribution, which in turn generates the out- put tokens. Hyperparameters for kNN-MT include the number of retrieved neighbors k, the tempera- ture for smoothing the kNN distribution T , and the interpolation weight w. In our experiments, we use systems (2) and (3) from Table"}, {"question": " What type of models are ensembled in Row (4) of Table 7?", "answer": " models trained on the same data with the same base architecture, one with adapters (Row 3) and one without (Row 2)", "ref_chunk": "batch sizes showed more instabilities in training. This might be a side effect of the postnorm layer normaliza- tion (Nguyen and Salazar, 2019). The results of the baseline are shown in Row (1) of Table 7, with an average score of 37.4 BLEU4 on ACL dev. Data Diversification As motivated in \u00a72.3, we use data diversification as an alternative data aug- mentation method in absence of monolingual target data for backtranslation. As data diversification needs forward and backward translations on the training data, we additionally train a 10-to-English 4By default using tok.13a from sacreBLEU (Post, 2018), except for zh and ja where we use tok.zh and tok.ja-mecab-0.996-IPA. model to create the backward translations. Row (2) of Table 7 shows the results after data diversifica- tion on all languages pairs. On average, this data augmentation approach improves MT quality by 2.1 BLEU and (37.4 \u2192 39.5), and ST quality by 1.6 BLEU (31.1 \u2192 32.7). Adapters for Incremental Data Retraining on the new training data after diversification (Row (2) of Table 7) is time-consuming and costly. To adapt the initial model (Row (1) of Table 7) rapidly towards to the augmented data, we use adapters (Bapna and Firat, 2019; Philip et al., 2020). the adapters are target-language- In this case, specific. The adapters are inserted after each en- coder and decoder layer. We initialize from the trained baseline (Row (1) in Table 7), freeze trained parameters and update the adapters only. We use the efficient implementation from Baziotis et al. (2022). As shown in Row (3) of Table 7, only train- ing the adapters on the new diversified training data performs on par with the re-training setup in Row (2) (39.6 on MT and 32.7 on ST on average for ACL dev). These results demonstrate that adapters are suitable for fast and effective incremental learn- ing when additional training data emerges later. To our surprise, adding adapters to the model trained with full data diversification (Row (2) from Table 7) does not bring further gain. A similar observation was reported by Pires et al. (2023), who opted for training the full network from scratch along with adapters instead. In our case, it therefore would be interesting to see the impact of training on data diversification with adapters from scratch. Multilingual vs Bilingual To investigate the im- pact of interference from multiple target languages, in preliminary experiments, we also compare the multilingual and bilingual translation performance for selected language pairs. As shown in Table 8, compared to bilingual models, the multilingual model lags behind especially on higher-resource languages. Adding the adapters partly closes this gap. Note the score difference to main result table (Table 7) is because the preliminary experiments did not fully use diversified data for all languages. Model ACL dev tst-COMMON en-de en-ru en-fa en-de en-ru en-fa bilingual multilingual + adapters 41.0 39.8 40.9 20.0 19.5 20.2 24.2 23.6 23.7 34.3 34.1 34.7 22.7 21.9 22.2 16.0 15.9 16.3 Table 8: Comparison of bilingual vs multilingual trans- lation performance in BLEU (\u2191) on German (de), Rus- sian (ru), Farsi (fa), which are high-, mid-, low-resource in the training data (Table 3). Multilingual system falls behind bilingual system, while adapters partly closes the gap. Note the score difference to main result table (Table 7) is because the experiments here did not fully use diversification. Ensemble Although the models in Row (2) and (3) in Table 7 are trained on the same data and share the same base architecture, we expect their representations to be sufficiently different, as (3) additionally uses adapters. We therefore ensemble these two models. The results are in Row (4) of Ta- ble 7. On MT and ST, for ACL, ensembling shows an improvement of 0.6 and 0.5 BLEU respectively over the single models in Row (2) and (3). On TED, however, ensembling does not seem to im- pact the scores compared to the single models. One explanation is that the adapter model from Row (3) performs worse than its non-adapter counter- part (Row (2)) on TED, which limits the overall effectiveness of ensembling. kNN-MT We also adapt the MT model to the tar- get domain of scientific talks. A challenge is that we do not have sufficient training data to fully fine- tune the MT model towards the desired domain or style. In this case, we use kNN-MT (Khandelwal et al., 2021) to adapt the model at inference time. In kNN-MT, bitexts are passed through a trained Source (ASR output): ... in a zero shot evaluation setup, meaning that pre trained word embedding models are ap- plied out of the box without any additional fine tuning w/o kNN-MT (Table 7 row (4)): in einer Null-Shot-Bewertungs-Setup (zero-shot evaluation setup), was bedeutet, dass vorgebildete (pre-educated) Wort- Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung di- rekt angewendet werden. w/ kNN-MT (Table 7 row (5)): ... in einer Null-Shot- Bewertung (zero-shot evaluation), was bedeutet, dass vortrainierte (pretrained) Wort-Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung direkt angewendet werden. ... Source (ASR output): Hello. My name is Ramachandra, and I will present our paper. w/o kNN-MT (Table 7 row (4)): \u4f60\u597d (Hello; addressing a single person),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9 \u6211\u8981\u53d1\u8868 (publish)\u6211 \u4eec\u7684\u8bba\u6587 w/ kNN-MT (Table 7 row (5)): \u5927\u5bb6\u597d (Hi all; addressing a group of audience),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9, \u6211\u8981\u4ecb\u7ecd (intro- duce)\u6211\u4eec\u7684\u8bba\u6587\u3002 Table 9: Examples of kNN-MT improving transla- tion quality for en\u2192de (upper) and en\u2192zh (lower). kNN-MT creates more accurate terminology transla- tions (\u201cpre trained\u201d for en\u2192de) and create more context- appropriate translation (\u201cHello\u201d for en\u2192zh). MT model. For each target token, its decoder hid- den state is stored in a datastore. At inference time, based on the current decoder hidden state, k candi- date target tokens are retrieved from the datastore using a nearest neighbor lookup. The retrieved to- ken distribution is then interpolated with the MT target distribution, which in turn generates the out- put tokens. Hyperparameters for kNN-MT include the number of retrieved neighbors k, the tempera- ture for smoothing the kNN distribution T , and the interpolation weight w. In our experiments, we use systems (2) and (3) from Table"}], "doc_text": "batch sizes showed more instabilities in training. This might be a side effect of the postnorm layer normaliza- tion (Nguyen and Salazar, 2019). The results of the baseline are shown in Row (1) of Table 7, with an average score of 37.4 BLEU4 on ACL dev. Data Diversification As motivated in \u00a72.3, we use data diversification as an alternative data aug- mentation method in absence of monolingual target data for backtranslation. As data diversification needs forward and backward translations on the training data, we additionally train a 10-to-English 4By default using tok.13a from sacreBLEU (Post, 2018), except for zh and ja where we use tok.zh and tok.ja-mecab-0.996-IPA. model to create the backward translations. Row (2) of Table 7 shows the results after data diversifica- tion on all languages pairs. On average, this data augmentation approach improves MT quality by 2.1 BLEU and (37.4 \u2192 39.5), and ST quality by 1.6 BLEU (31.1 \u2192 32.7). Adapters for Incremental Data Retraining on the new training data after diversification (Row (2) of Table 7) is time-consuming and costly. To adapt the initial model (Row (1) of Table 7) rapidly towards to the augmented data, we use adapters (Bapna and Firat, 2019; Philip et al., 2020). the adapters are target-language- In this case, specific. The adapters are inserted after each en- coder and decoder layer. We initialize from the trained baseline (Row (1) in Table 7), freeze trained parameters and update the adapters only. We use the efficient implementation from Baziotis et al. (2022). As shown in Row (3) of Table 7, only train- ing the adapters on the new diversified training data performs on par with the re-training setup in Row (2) (39.6 on MT and 32.7 on ST on average for ACL dev). These results demonstrate that adapters are suitable for fast and effective incremental learn- ing when additional training data emerges later. To our surprise, adding adapters to the model trained with full data diversification (Row (2) from Table 7) does not bring further gain. A similar observation was reported by Pires et al. (2023), who opted for training the full network from scratch along with adapters instead. In our case, it therefore would be interesting to see the impact of training on data diversification with adapters from scratch. Multilingual vs Bilingual To investigate the im- pact of interference from multiple target languages, in preliminary experiments, we also compare the multilingual and bilingual translation performance for selected language pairs. As shown in Table 8, compared to bilingual models, the multilingual model lags behind especially on higher-resource languages. Adding the adapters partly closes this gap. Note the score difference to main result table (Table 7) is because the preliminary experiments did not fully use diversified data for all languages. Model ACL dev tst-COMMON en-de en-ru en-fa en-de en-ru en-fa bilingual multilingual + adapters 41.0 39.8 40.9 20.0 19.5 20.2 24.2 23.6 23.7 34.3 34.1 34.7 22.7 21.9 22.2 16.0 15.9 16.3 Table 8: Comparison of bilingual vs multilingual trans- lation performance in BLEU (\u2191) on German (de), Rus- sian (ru), Farsi (fa), which are high-, mid-, low-resource in the training data (Table 3). Multilingual system falls behind bilingual system, while adapters partly closes the gap. Note the score difference to main result table (Table 7) is because the experiments here did not fully use diversification. Ensemble Although the models in Row (2) and (3) in Table 7 are trained on the same data and share the same base architecture, we expect their representations to be sufficiently different, as (3) additionally uses adapters. We therefore ensemble these two models. The results are in Row (4) of Ta- ble 7. On MT and ST, for ACL, ensembling shows an improvement of 0.6 and 0.5 BLEU respectively over the single models in Row (2) and (3). On TED, however, ensembling does not seem to im- pact the scores compared to the single models. One explanation is that the adapter model from Row (3) performs worse than its non-adapter counter- part (Row (2)) on TED, which limits the overall effectiveness of ensembling. kNN-MT We also adapt the MT model to the tar- get domain of scientific talks. A challenge is that we do not have sufficient training data to fully fine- tune the MT model towards the desired domain or style. In this case, we use kNN-MT (Khandelwal et al., 2021) to adapt the model at inference time. In kNN-MT, bitexts are passed through a trained Source (ASR output): ... in a zero shot evaluation setup, meaning that pre trained word embedding models are ap- plied out of the box without any additional fine tuning w/o kNN-MT (Table 7 row (4)): in einer Null-Shot-Bewertungs-Setup (zero-shot evaluation setup), was bedeutet, dass vorgebildete (pre-educated) Wort- Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung di- rekt angewendet werden. w/ kNN-MT (Table 7 row (5)): ... in einer Null-Shot- Bewertung (zero-shot evaluation), was bedeutet, dass vortrainierte (pretrained) Wort-Einbettungsmodelle ohne zus\u00e4tzliche Feinabstimmung direkt angewendet werden. ... Source (ASR output): Hello. My name is Ramachandra, and I will present our paper. w/o kNN-MT (Table 7 row (4)): \u4f60\u597d (Hello; addressing a single person),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9 \u6211\u8981\u53d1\u8868 (publish)\u6211 \u4eec\u7684\u8bba\u6587 w/ kNN-MT (Table 7 row (5)): \u5927\u5bb6\u597d (Hi all; addressing a group of audience),\u6211\u53eb\u62c9\u739b\u94b1\u5fb7\u62c9, \u6211\u8981\u4ecb\u7ecd (intro- duce)\u6211\u4eec\u7684\u8bba\u6587\u3002 Table 9: Examples of kNN-MT improving transla- tion quality for en\u2192de (upper) and en\u2192zh (lower). kNN-MT creates more accurate terminology transla- tions (\u201cpre trained\u201d for en\u2192de) and create more context- appropriate translation (\u201cHello\u201d for en\u2192zh). MT model. For each target token, its decoder hid- den state is stored in a datastore. At inference time, based on the current decoder hidden state, k candi- date target tokens are retrieved from the datastore using a nearest neighbor lookup. The retrieved to- ken distribution is then interpolated with the MT target distribution, which in turn generates the out- put tokens. Hyperparameters for kNN-MT include the number of retrieved neighbors k, the tempera- ture for smoothing the kNN distribution T , and the interpolation weight w. In our experiments, we use systems (2) and (3) from Table"}