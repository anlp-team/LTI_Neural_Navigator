{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Surveying_(Dis)Parities_and_Concerns_of_Compute_Hungry_NLP_Research_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What percentage of participants experienced being asked to conduct additional experiments that were too expensive for them?", "answer": " 30.1%", "ref_chunk": "Reviewing Finally, we quantified how the concerns about the environmental impact and differences in terms of available compute resources affect peer reviewing (\ud835\udc4410\u2013\ud835\udc4413). We further asked our participants four questions (\ud835\udc4414\u2013\ud835\udc4417) which relate to concrete ideas that would change the reviewing process and en- courage model release (\ud835\udc4418). 5.1 Analysis Figure 7a shows that 30.1% of the participants ex- perienced being asked (during peer-review) to con- duct additional experiments that were too expensive for them (\ud835\udc4410) with 77 respondents having experi- enced this more than once (\ud835\udc4411) and 19.2% having a substantially higher number (five or more times) according to our outlier analysis. Most participants (65.9%) further thought that this criticism was un- justified (\ud835\udc4412). Figure 7b (\ud835\udc4413) shows that 34.3% of the respondents often or always lacked resources to reproduce previous experiments and 41.4% some- times. Only 7.7% never or 16.7% rarely faced a lack of resources to reproduce experiments. With respect to the concrete reviewing actions, Fig. 9a shows that a large majority (89.8%) of our participants would consider submitting their work to a dedicated track on efficient methods (\ud835\udc4414). Following up on the results from the survey, such an efficiency track was implemented at EMNLP 2022. 35.9% of our participants were unsure about requesting authors to justify the allocation of bud- get for experiments (\ud835\udc4415), with 41% voting for yes. Also, even though 52.6% of the participants had not been asked for experiments that were too expensive for them, a clear majority of the partic- ipants (83.7%) would like to require reviewers to justify their petitions for more experiments (\ud835\udc4416). Lastly, we also see a large majority (75.6%) that be- lieved that their work could benefit from the release of small versions of pretrained models alongside large ones (\ud835\udc4417). To promote this, a majority of our respondents thought that venues should have a visible branding of papers to release a model (59.3%) and that reviewers should be instructed to reward model release (50.6%). 42.6% of respon- dents thought that the venues should grant a best artifact award. 11.5% of respondents supported none of the options. A first step towards increasing the reproducibility and ensuring the submission of experimental code was implemented at NAACL (a) Reviewer critique (b) Reproducing results Q10: Did reviewers ask for too expensive experiments? Q13: Lack of resources to reproduce results? Q12: Was the critique justified? 50 0 100 40 Q10 Q12 15 30 19 17 53 66 s t n a p i c i t r a P % 30 20 10 % Participants Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Never Rarely Sometimes Often Always Figure 7: Analysis on how of a lack of resources can affect research. In (a), we show what percentage of participants had been asked by reviewers for too expensive experiments (\ud835\udc4410) and if so, if they felt the critique was justified (\ud835\udc4412). In (b), we show how often our participants could not reproduce previous results due to a lack of computational resources (\ud835\udc4413). 2022 by introducing a badge system at the repro- ducibility track.4 Upon acceptance, the authors could follow specific procedures to earn three types of badges: 1) open-source code, 2) trained model, and 3) reproducible results. Seniority. We find no significant differences w.r.t. the seniority of our participants regarding \ud835\udc4410\u2013\ud835\udc4418. However, junior researchers (1\u20135 years) showed a substantially higher tendency towards requesting authors to justify their compute bud- get (\ud835\udc4415) against all other age groups (p-values < 0.035). We also observe in Figure 9c diverging preferences between junior and senior groups in terms of ideas to improve the reviewing process (\ud835\udc4418). Junior researchers (1\u20135 years) seemed to be more inclined towards a visual branding as well as instructing reviewers than senior researchers (11\u2013 15 years with a p-value of 0.089 and 16+ years with a p-value of 0.085). reproduce experiments (\ud835\udc4413, Fig. 8); where we find significant differences and conduct pairwise analyses.5 In general, students suffered most, with a significant difference compared to the large in- dustry sector with a p-value of 0.002 < 0.005 = \ud835\udefc (Bonferroni-corrected). We further find substantial differences between students and academic PIs (p- value = 0.026) and between academic post-docs and large industry labs (p-value = 0.088). We find no substantial differences when it comes to actionable items for the *CL community (\ud835\udc4414\u2013 \ud835\udc4417), indicating that implementing popular ideas would be welcomed by all groups. However, we find some differences when it comes to encourag- ing the release of models (\ud835\udc4418). For instance, Fig- ure 9d shows that academic post-docs had a higher preference towards reviewers rewarding papers that promise to release models than academic PIs. Also, participants from small industry would prefer visual branding over awards in contrast to large industry. In terms of the job sector, we again Job Sector. find no significant differences with respect to re- viewers asking for too expensive experiments (\ud835\udc4410) or critique being justified (\ud835\udc4412). Interestingly, re- spondents from small industry received fewer such requests (\ud835\udc4411) compared to post-docs (p-value = 0.024), PIs (p-value = 0.061), and larger industry (p-value = 0.087). The most concerning trend can be observed when comparing the different groups with respect to their lack of compute resources to 5.2 Discussion and Recommendations Our analysis shows that the two most pressing is- sues among our respondents are the lack of re- sources to reproduce results and reviewers request- ing for too expensive experiments without proper justification. This is reflected in the large support for both respective counter measures; namely, ask- ing reviewers to provide justification and the release of smaller models that would allow researchers to at least reproduce some experiments. Considering the 4https://2022.naacl.org/blog/ reproducibility-track/ 5Kruskal-Wallis test: \ud835\udc3b 5 = 12.486 > \ud835\udc3b 5 0 = 9.488. Always 5 4 3 2 Never 1 Student PD Aca. PI Aca. (s) Ind. (l) Ind. Figure 8: \ud835\udc4413: Lack of resources by job sector. success of badges at NAACL 2022 with 175 code, 98 model and 20 reproducibility badges, introduc- ing an explicit badge for small"}, {"question": " How many respondents experienced being asked more than once to conduct additional experiments that were too expensive for them?", "answer": " 77", "ref_chunk": "Reviewing Finally, we quantified how the concerns about the environmental impact and differences in terms of available compute resources affect peer reviewing (\ud835\udc4410\u2013\ud835\udc4413). We further asked our participants four questions (\ud835\udc4414\u2013\ud835\udc4417) which relate to concrete ideas that would change the reviewing process and en- courage model release (\ud835\udc4418). 5.1 Analysis Figure 7a shows that 30.1% of the participants ex- perienced being asked (during peer-review) to con- duct additional experiments that were too expensive for them (\ud835\udc4410) with 77 respondents having experi- enced this more than once (\ud835\udc4411) and 19.2% having a substantially higher number (five or more times) according to our outlier analysis. Most participants (65.9%) further thought that this criticism was un- justified (\ud835\udc4412). Figure 7b (\ud835\udc4413) shows that 34.3% of the respondents often or always lacked resources to reproduce previous experiments and 41.4% some- times. Only 7.7% never or 16.7% rarely faced a lack of resources to reproduce experiments. With respect to the concrete reviewing actions, Fig. 9a shows that a large majority (89.8%) of our participants would consider submitting their work to a dedicated track on efficient methods (\ud835\udc4414). Following up on the results from the survey, such an efficiency track was implemented at EMNLP 2022. 35.9% of our participants were unsure about requesting authors to justify the allocation of bud- get for experiments (\ud835\udc4415), with 41% voting for yes. Also, even though 52.6% of the participants had not been asked for experiments that were too expensive for them, a clear majority of the partic- ipants (83.7%) would like to require reviewers to justify their petitions for more experiments (\ud835\udc4416). Lastly, we also see a large majority (75.6%) that be- lieved that their work could benefit from the release of small versions of pretrained models alongside large ones (\ud835\udc4417). To promote this, a majority of our respondents thought that venues should have a visible branding of papers to release a model (59.3%) and that reviewers should be instructed to reward model release (50.6%). 42.6% of respon- dents thought that the venues should grant a best artifact award. 11.5% of respondents supported none of the options. A first step towards increasing the reproducibility and ensuring the submission of experimental code was implemented at NAACL (a) Reviewer critique (b) Reproducing results Q10: Did reviewers ask for too expensive experiments? Q13: Lack of resources to reproduce results? Q12: Was the critique justified? 50 0 100 40 Q10 Q12 15 30 19 17 53 66 s t n a p i c i t r a P % 30 20 10 % Participants Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Never Rarely Sometimes Often Always Figure 7: Analysis on how of a lack of resources can affect research. In (a), we show what percentage of participants had been asked by reviewers for too expensive experiments (\ud835\udc4410) and if so, if they felt the critique was justified (\ud835\udc4412). In (b), we show how often our participants could not reproduce previous results due to a lack of computational resources (\ud835\udc4413). 2022 by introducing a badge system at the repro- ducibility track.4 Upon acceptance, the authors could follow specific procedures to earn three types of badges: 1) open-source code, 2) trained model, and 3) reproducible results. Seniority. We find no significant differences w.r.t. the seniority of our participants regarding \ud835\udc4410\u2013\ud835\udc4418. However, junior researchers (1\u20135 years) showed a substantially higher tendency towards requesting authors to justify their compute bud- get (\ud835\udc4415) against all other age groups (p-values < 0.035). We also observe in Figure 9c diverging preferences between junior and senior groups in terms of ideas to improve the reviewing process (\ud835\udc4418). Junior researchers (1\u20135 years) seemed to be more inclined towards a visual branding as well as instructing reviewers than senior researchers (11\u2013 15 years with a p-value of 0.089 and 16+ years with a p-value of 0.085). reproduce experiments (\ud835\udc4413, Fig. 8); where we find significant differences and conduct pairwise analyses.5 In general, students suffered most, with a significant difference compared to the large in- dustry sector with a p-value of 0.002 < 0.005 = \ud835\udefc (Bonferroni-corrected). We further find substantial differences between students and academic PIs (p- value = 0.026) and between academic post-docs and large industry labs (p-value = 0.088). We find no substantial differences when it comes to actionable items for the *CL community (\ud835\udc4414\u2013 \ud835\udc4417), indicating that implementing popular ideas would be welcomed by all groups. However, we find some differences when it comes to encourag- ing the release of models (\ud835\udc4418). For instance, Fig- ure 9d shows that academic post-docs had a higher preference towards reviewers rewarding papers that promise to release models than academic PIs. Also, participants from small industry would prefer visual branding over awards in contrast to large industry. In terms of the job sector, we again Job Sector. find no significant differences with respect to re- viewers asking for too expensive experiments (\ud835\udc4410) or critique being justified (\ud835\udc4412). Interestingly, re- spondents from small industry received fewer such requests (\ud835\udc4411) compared to post-docs (p-value = 0.024), PIs (p-value = 0.061), and larger industry (p-value = 0.087). The most concerning trend can be observed when comparing the different groups with respect to their lack of compute resources to 5.2 Discussion and Recommendations Our analysis shows that the two most pressing is- sues among our respondents are the lack of re- sources to reproduce results and reviewers request- ing for too expensive experiments without proper justification. This is reflected in the large support for both respective counter measures; namely, ask- ing reviewers to provide justification and the release of smaller models that would allow researchers to at least reproduce some experiments. Considering the 4https://2022.naacl.org/blog/ reproducibility-track/ 5Kruskal-Wallis test: \ud835\udc3b 5 = 12.486 > \ud835\udc3b 5 0 = 9.488. Always 5 4 3 2 Never 1 Student PD Aca. PI Aca. (s) Ind. (l) Ind. Figure 8: \ud835\udc4413: Lack of resources by job sector. success of badges at NAACL 2022 with 175 code, 98 model and 20 reproducibility badges, introduc- ing an explicit badge for small"}, {"question": " What percentage of participants often or always lacked resources to reproduce previous experiments?", "answer": " 34.3%", "ref_chunk": "Reviewing Finally, we quantified how the concerns about the environmental impact and differences in terms of available compute resources affect peer reviewing (\ud835\udc4410\u2013\ud835\udc4413). We further asked our participants four questions (\ud835\udc4414\u2013\ud835\udc4417) which relate to concrete ideas that would change the reviewing process and en- courage model release (\ud835\udc4418). 5.1 Analysis Figure 7a shows that 30.1% of the participants ex- perienced being asked (during peer-review) to con- duct additional experiments that were too expensive for them (\ud835\udc4410) with 77 respondents having experi- enced this more than once (\ud835\udc4411) and 19.2% having a substantially higher number (five or more times) according to our outlier analysis. Most participants (65.9%) further thought that this criticism was un- justified (\ud835\udc4412). Figure 7b (\ud835\udc4413) shows that 34.3% of the respondents often or always lacked resources to reproduce previous experiments and 41.4% some- times. Only 7.7% never or 16.7% rarely faced a lack of resources to reproduce experiments. With respect to the concrete reviewing actions, Fig. 9a shows that a large majority (89.8%) of our participants would consider submitting their work to a dedicated track on efficient methods (\ud835\udc4414). Following up on the results from the survey, such an efficiency track was implemented at EMNLP 2022. 35.9% of our participants were unsure about requesting authors to justify the allocation of bud- get for experiments (\ud835\udc4415), with 41% voting for yes. Also, even though 52.6% of the participants had not been asked for experiments that were too expensive for them, a clear majority of the partic- ipants (83.7%) would like to require reviewers to justify their petitions for more experiments (\ud835\udc4416). Lastly, we also see a large majority (75.6%) that be- lieved that their work could benefit from the release of small versions of pretrained models alongside large ones (\ud835\udc4417). To promote this, a majority of our respondents thought that venues should have a visible branding of papers to release a model (59.3%) and that reviewers should be instructed to reward model release (50.6%). 42.6% of respon- dents thought that the venues should grant a best artifact award. 11.5% of respondents supported none of the options. A first step towards increasing the reproducibility and ensuring the submission of experimental code was implemented at NAACL (a) Reviewer critique (b) Reproducing results Q10: Did reviewers ask for too expensive experiments? Q13: Lack of resources to reproduce results? Q12: Was the critique justified? 50 0 100 40 Q10 Q12 15 30 19 17 53 66 s t n a p i c i t r a P % 30 20 10 % Participants Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Never Rarely Sometimes Often Always Figure 7: Analysis on how of a lack of resources can affect research. In (a), we show what percentage of participants had been asked by reviewers for too expensive experiments (\ud835\udc4410) and if so, if they felt the critique was justified (\ud835\udc4412). In (b), we show how often our participants could not reproduce previous results due to a lack of computational resources (\ud835\udc4413). 2022 by introducing a badge system at the repro- ducibility track.4 Upon acceptance, the authors could follow specific procedures to earn three types of badges: 1) open-source code, 2) trained model, and 3) reproducible results. Seniority. We find no significant differences w.r.t. the seniority of our participants regarding \ud835\udc4410\u2013\ud835\udc4418. However, junior researchers (1\u20135 years) showed a substantially higher tendency towards requesting authors to justify their compute bud- get (\ud835\udc4415) against all other age groups (p-values < 0.035). We also observe in Figure 9c diverging preferences between junior and senior groups in terms of ideas to improve the reviewing process (\ud835\udc4418). Junior researchers (1\u20135 years) seemed to be more inclined towards a visual branding as well as instructing reviewers than senior researchers (11\u2013 15 years with a p-value of 0.089 and 16+ years with a p-value of 0.085). reproduce experiments (\ud835\udc4413, Fig. 8); where we find significant differences and conduct pairwise analyses.5 In general, students suffered most, with a significant difference compared to the large in- dustry sector with a p-value of 0.002 < 0.005 = \ud835\udefc (Bonferroni-corrected). We further find substantial differences between students and academic PIs (p- value = 0.026) and between academic post-docs and large industry labs (p-value = 0.088). We find no substantial differences when it comes to actionable items for the *CL community (\ud835\udc4414\u2013 \ud835\udc4417), indicating that implementing popular ideas would be welcomed by all groups. However, we find some differences when it comes to encourag- ing the release of models (\ud835\udc4418). For instance, Fig- ure 9d shows that academic post-docs had a higher preference towards reviewers rewarding papers that promise to release models than academic PIs. Also, participants from small industry would prefer visual branding over awards in contrast to large industry. In terms of the job sector, we again Job Sector. find no significant differences with respect to re- viewers asking for too expensive experiments (\ud835\udc4410) or critique being justified (\ud835\udc4412). Interestingly, re- spondents from small industry received fewer such requests (\ud835\udc4411) compared to post-docs (p-value = 0.024), PIs (p-value = 0.061), and larger industry (p-value = 0.087). The most concerning trend can be observed when comparing the different groups with respect to their lack of compute resources to 5.2 Discussion and Recommendations Our analysis shows that the two most pressing is- sues among our respondents are the lack of re- sources to reproduce results and reviewers request- ing for too expensive experiments without proper justification. This is reflected in the large support for both respective counter measures; namely, ask- ing reviewers to provide justification and the release of smaller models that would allow researchers to at least reproduce some experiments. Considering the 4https://2022.naacl.org/blog/ reproducibility-track/ 5Kruskal-Wallis test: \ud835\udc3b 5 = 12.486 > \ud835\udc3b 5 0 = 9.488. Always 5 4 3 2 Never 1 Student PD Aca. PI Aca. (s) Ind. (l) Ind. Figure 8: \ud835\udc4413: Lack of resources by job sector. success of badges at NAACL 2022 with 175 code, 98 model and 20 reproducibility badges, introduc- ing an explicit badge for small"}, {"question": " Did most participants feel that the criticism of conducting additional expensive experiments was justified?", "answer": " No, 65.9% thought it was unjustified", "ref_chunk": "Reviewing Finally, we quantified how the concerns about the environmental impact and differences in terms of available compute resources affect peer reviewing (\ud835\udc4410\u2013\ud835\udc4413). We further asked our participants four questions (\ud835\udc4414\u2013\ud835\udc4417) which relate to concrete ideas that would change the reviewing process and en- courage model release (\ud835\udc4418). 5.1 Analysis Figure 7a shows that 30.1% of the participants ex- perienced being asked (during peer-review) to con- duct additional experiments that were too expensive for them (\ud835\udc4410) with 77 respondents having experi- enced this more than once (\ud835\udc4411) and 19.2% having a substantially higher number (five or more times) according to our outlier analysis. Most participants (65.9%) further thought that this criticism was un- justified (\ud835\udc4412). Figure 7b (\ud835\udc4413) shows that 34.3% of the respondents often or always lacked resources to reproduce previous experiments and 41.4% some- times. Only 7.7% never or 16.7% rarely faced a lack of resources to reproduce experiments. With respect to the concrete reviewing actions, Fig. 9a shows that a large majority (89.8%) of our participants would consider submitting their work to a dedicated track on efficient methods (\ud835\udc4414). Following up on the results from the survey, such an efficiency track was implemented at EMNLP 2022. 35.9% of our participants were unsure about requesting authors to justify the allocation of bud- get for experiments (\ud835\udc4415), with 41% voting for yes. Also, even though 52.6% of the participants had not been asked for experiments that were too expensive for them, a clear majority of the partic- ipants (83.7%) would like to require reviewers to justify their petitions for more experiments (\ud835\udc4416). Lastly, we also see a large majority (75.6%) that be- lieved that their work could benefit from the release of small versions of pretrained models alongside large ones (\ud835\udc4417). To promote this, a majority of our respondents thought that venues should have a visible branding of papers to release a model (59.3%) and that reviewers should be instructed to reward model release (50.6%). 42.6% of respon- dents thought that the venues should grant a best artifact award. 11.5% of respondents supported none of the options. A first step towards increasing the reproducibility and ensuring the submission of experimental code was implemented at NAACL (a) Reviewer critique (b) Reproducing results Q10: Did reviewers ask for too expensive experiments? Q13: Lack of resources to reproduce results? Q12: Was the critique justified? 50 0 100 40 Q10 Q12 15 30 19 17 53 66 s t n a p i c i t r a P % 30 20 10 % Participants Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Never Rarely Sometimes Often Always Figure 7: Analysis on how of a lack of resources can affect research. In (a), we show what percentage of participants had been asked by reviewers for too expensive experiments (\ud835\udc4410) and if so, if they felt the critique was justified (\ud835\udc4412). In (b), we show how often our participants could not reproduce previous results due to a lack of computational resources (\ud835\udc4413). 2022 by introducing a badge system at the repro- ducibility track.4 Upon acceptance, the authors could follow specific procedures to earn three types of badges: 1) open-source code, 2) trained model, and 3) reproducible results. Seniority. We find no significant differences w.r.t. the seniority of our participants regarding \ud835\udc4410\u2013\ud835\udc4418. However, junior researchers (1\u20135 years) showed a substantially higher tendency towards requesting authors to justify their compute bud- get (\ud835\udc4415) against all other age groups (p-values < 0.035). We also observe in Figure 9c diverging preferences between junior and senior groups in terms of ideas to improve the reviewing process (\ud835\udc4418). Junior researchers (1\u20135 years) seemed to be more inclined towards a visual branding as well as instructing reviewers than senior researchers (11\u2013 15 years with a p-value of 0.089 and 16+ years with a p-value of 0.085). reproduce experiments (\ud835\udc4413, Fig. 8); where we find significant differences and conduct pairwise analyses.5 In general, students suffered most, with a significant difference compared to the large in- dustry sector with a p-value of 0.002 < 0.005 = \ud835\udefc (Bonferroni-corrected). We further find substantial differences between students and academic PIs (p- value = 0.026) and between academic post-docs and large industry labs (p-value = 0.088). We find no substantial differences when it comes to actionable items for the *CL community (\ud835\udc4414\u2013 \ud835\udc4417), indicating that implementing popular ideas would be welcomed by all groups. However, we find some differences when it comes to encourag- ing the release of models (\ud835\udc4418). For instance, Fig- ure 9d shows that academic post-docs had a higher preference towards reviewers rewarding papers that promise to release models than academic PIs. Also, participants from small industry would prefer visual branding over awards in contrast to large industry. In terms of the job sector, we again Job Sector. find no significant differences with respect to re- viewers asking for too expensive experiments (\ud835\udc4410) or critique being justified (\ud835\udc4412). Interestingly, re- spondents from small industry received fewer such requests (\ud835\udc4411) compared to post-docs (p-value = 0.024), PIs (p-value = 0.061), and larger industry (p-value = 0.087). The most concerning trend can be observed when comparing the different groups with respect to their lack of compute resources to 5.2 Discussion and Recommendations Our analysis shows that the two most pressing is- sues among our respondents are the lack of re- sources to reproduce results and reviewers request- ing for too expensive experiments without proper justification. This is reflected in the large support for both respective counter measures; namely, ask- ing reviewers to provide justification and the release of smaller models that would allow researchers to at least reproduce some experiments. Considering the 4https://2022.naacl.org/blog/ reproducibility-track/ 5Kruskal-Wallis test: \ud835\udc3b 5 = 12.486 > \ud835\udc3b 5 0 = 9.488. Always 5 4 3 2 Never 1 Student PD Aca. PI Aca. (s) Ind. (l) Ind. Figure 8: \ud835\udc4413: Lack of resources by job sector. success of badges at NAACL 2022 with 175 code, 98 model and 20 reproducibility badges, introduc- ing an explicit badge for small"}, {"question": " What percentage of participants would consider submitting their work to a dedicated track on efficient methods?", "answer": " 89.8%", "ref_chunk": "Reviewing Finally, we quantified how the concerns about the environmental impact and differences in terms of available compute resources affect peer reviewing (\ud835\udc4410\u2013\ud835\udc4413). We further asked our participants four questions (\ud835\udc4414\u2013\ud835\udc4417) which relate to concrete ideas that would change the reviewing process and en- courage model release (\ud835\udc4418). 5.1 Analysis Figure 7a shows that 30.1% of the participants ex- perienced being asked (during peer-review) to con- duct additional experiments that were too expensive for them (\ud835\udc4410) with 77 respondents having experi- enced this more than once (\ud835\udc4411) and 19.2% having a substantially higher number (five or more times) according to our outlier analysis. Most participants (65.9%) further thought that this criticism was un- justified (\ud835\udc4412). Figure 7b (\ud835\udc4413) shows that 34.3% of the respondents often or always lacked resources to reproduce previous experiments and 41.4% some- times. Only 7.7% never or 16.7% rarely faced a lack of resources to reproduce experiments. With respect to the concrete reviewing actions, Fig. 9a shows that a large majority (89.8%) of our participants would consider submitting their work to a dedicated track on efficient methods (\ud835\udc4414). Following up on the results from the survey, such an efficiency track was implemented at EMNLP 2022. 35.9% of our participants were unsure about requesting authors to justify the allocation of bud- get for experiments (\ud835\udc4415), with 41% voting for yes. Also, even though 52.6% of the participants had not been asked for experiments that were too expensive for them, a clear majority of the partic- ipants (83.7%) would like to require reviewers to justify their petitions for more experiments (\ud835\udc4416). Lastly, we also see a large majority (75.6%) that be- lieved that their work could benefit from the release of small versions of pretrained models alongside large ones (\ud835\udc4417). To promote this, a majority of our respondents thought that venues should have a visible branding of papers to release a model (59.3%) and that reviewers should be instructed to reward model release (50.6%). 42.6% of respon- dents thought that the venues should grant a best artifact award. 11.5% of respondents supported none of the options. A first step towards increasing the reproducibility and ensuring the submission of experimental code was implemented at NAACL (a) Reviewer critique (b) Reproducing results Q10: Did reviewers ask for too expensive experiments? Q13: Lack of resources to reproduce results? Q12: Was the critique justified? 50 0 100 40 Q10 Q12 15 30 19 17 53 66 s t n a p i c i t r a P % 30 20 10 % Participants Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Never Rarely Sometimes Often Always Figure 7: Analysis on how of a lack of resources can affect research. In (a), we show what percentage of participants had been asked by reviewers for too expensive experiments (\ud835\udc4410) and if so, if they felt the critique was justified (\ud835\udc4412). In (b), we show how often our participants could not reproduce previous results due to a lack of computational resources (\ud835\udc4413). 2022 by introducing a badge system at the repro- ducibility track.4 Upon acceptance, the authors could follow specific procedures to earn three types of badges: 1) open-source code, 2) trained model, and 3) reproducible results. Seniority. We find no significant differences w.r.t. the seniority of our participants regarding \ud835\udc4410\u2013\ud835\udc4418. However, junior researchers (1\u20135 years) showed a substantially higher tendency towards requesting authors to justify their compute bud- get (\ud835\udc4415) against all other age groups (p-values < 0.035). We also observe in Figure 9c diverging preferences between junior and senior groups in terms of ideas to improve the reviewing process (\ud835\udc4418). Junior researchers (1\u20135 years) seemed to be more inclined towards a visual branding as well as instructing reviewers than senior researchers (11\u2013 15 years with a p-value of 0.089 and 16+ years with a p-value of 0.085). reproduce experiments (\ud835\udc4413, Fig. 8); where we find significant differences and conduct pairwise analyses.5 In general, students suffered most, with a significant difference compared to the large in- dustry sector with a p-value of 0.002 < 0.005 = \ud835\udefc (Bonferroni-corrected). We further find substantial differences between students and academic PIs (p- value = 0.026) and between academic post-docs and large industry labs (p-value = 0.088). We find no substantial differences when it comes to actionable items for the *CL community (\ud835\udc4414\u2013 \ud835\udc4417), indicating that implementing popular ideas would be welcomed by all groups. However, we find some differences when it comes to encourag- ing the release of models (\ud835\udc4418). For instance, Fig- ure 9d shows that academic post-docs had a higher preference towards reviewers rewarding papers that promise to release models than academic PIs. Also, participants from small industry would prefer visual branding over awards in contrast to large industry. In terms of the job sector, we again Job Sector. find no significant differences with respect to re- viewers asking for too expensive experiments (\ud835\udc4410) or critique being justified (\ud835\udc4412). Interestingly, re- spondents from small industry received fewer such requests (\ud835\udc4411) compared to post-docs (p-value = 0.024), PIs (p-value = 0.061), and larger industry (p-value = 0.087). The most concerning trend can be observed when comparing the different groups with respect to their lack of compute resources to 5.2 Discussion and Recommendations Our analysis shows that the two most pressing is- sues among our respondents are the lack of re- sources to reproduce results and reviewers request- ing for too expensive experiments without proper justification. This is reflected in the large support for both respective counter measures; namely, ask- ing reviewers to provide justification and the release of smaller models that would allow researchers to at least reproduce some experiments. Considering the 4https://2022.naacl.org/blog/ reproducibility-track/ 5Kruskal-Wallis test: \ud835\udc3b 5 = 12.486 > \ud835\udc3b 5 0 = 9.488. Always 5 4 3 2 Never 1 Student PD Aca. PI Aca. (s) Ind. (l) Ind. Figure 8: \ud835\udc4413: Lack of resources by job sector. success of badges at NAACL 2022 with 175 code, 98 model and 20 reproducibility badges, introduc- ing an explicit badge for small"}, {"question": " What percentage of participants would want reviewers to justify their petitions for more experiments, even if they had not been asked for expensive experiments themselves?", "answer": " 83.7%", "ref_chunk": "Reviewing Finally, we quantified how the concerns about the environmental impact and differences in terms of available compute resources affect peer reviewing (\ud835\udc4410\u2013\ud835\udc4413). We further asked our participants four questions (\ud835\udc4414\u2013\ud835\udc4417) which relate to concrete ideas that would change the reviewing process and en- courage model release (\ud835\udc4418). 5.1 Analysis Figure 7a shows that 30.1% of the participants ex- perienced being asked (during peer-review) to con- duct additional experiments that were too expensive for them (\ud835\udc4410) with 77 respondents having experi- enced this more than once (\ud835\udc4411) and 19.2% having a substantially higher number (five or more times) according to our outlier analysis. Most participants (65.9%) further thought that this criticism was un- justified (\ud835\udc4412). Figure 7b (\ud835\udc4413) shows that 34.3% of the respondents often or always lacked resources to reproduce previous experiments and 41.4% some- times. Only 7.7% never or 16.7% rarely faced a lack of resources to reproduce experiments. With respect to the concrete reviewing actions, Fig. 9a shows that a large majority (89.8%) of our participants would consider submitting their work to a dedicated track on efficient methods (\ud835\udc4414). Following up on the results from the survey, such an efficiency track was implemented at EMNLP 2022. 35.9% of our participants were unsure about requesting authors to justify the allocation of bud- get for experiments (\ud835\udc4415), with 41% voting for yes. Also, even though 52.6% of the participants had not been asked for experiments that were too expensive for them, a clear majority of the partic- ipants (83.7%) would like to require reviewers to justify their petitions for more experiments (\ud835\udc4416). Lastly, we also see a large majority (75.6%) that be- lieved that their work could benefit from the release of small versions of pretrained models alongside large ones (\ud835\udc4417). To promote this, a majority of our respondents thought that venues should have a visible branding of papers to release a model (59.3%) and that reviewers should be instructed to reward model release (50.6%). 42.6% of respon- dents thought that the venues should grant a best artifact award. 11.5% of respondents supported none of the options. A first step towards increasing the reproducibility and ensuring the submission of experimental code was implemented at NAACL (a) Reviewer critique (b) Reproducing results Q10: Did reviewers ask for too expensive experiments? Q13: Lack of resources to reproduce results? Q12: Was the critique justified? 50 0 100 40 Q10 Q12 15 30 19 17 53 66 s t n a p i c i t r a P % 30 20 10 % Participants Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Never Rarely Sometimes Often Always Figure 7: Analysis on how of a lack of resources can affect research. In (a), we show what percentage of participants had been asked by reviewers for too expensive experiments (\ud835\udc4410) and if so, if they felt the critique was justified (\ud835\udc4412). In (b), we show how often our participants could not reproduce previous results due to a lack of computational resources (\ud835\udc4413). 2022 by introducing a badge system at the repro- ducibility track.4 Upon acceptance, the authors could follow specific procedures to earn three types of badges: 1) open-source code, 2) trained model, and 3) reproducible results. Seniority. We find no significant differences w.r.t. the seniority of our participants regarding \ud835\udc4410\u2013\ud835\udc4418. However, junior researchers (1\u20135 years) showed a substantially higher tendency towards requesting authors to justify their compute bud- get (\ud835\udc4415) against all other age groups (p-values < 0.035). We also observe in Figure 9c diverging preferences between junior and senior groups in terms of ideas to improve the reviewing process (\ud835\udc4418). Junior researchers (1\u20135 years) seemed to be more inclined towards a visual branding as well as instructing reviewers than senior researchers (11\u2013 15 years with a p-value of 0.089 and 16+ years with a p-value of 0.085). reproduce experiments (\ud835\udc4413, Fig. 8); where we find significant differences and conduct pairwise analyses.5 In general, students suffered most, with a significant difference compared to the large in- dustry sector with a p-value of 0.002 < 0.005 = \ud835\udefc (Bonferroni-corrected). We further find substantial differences between students and academic PIs (p- value = 0.026) and between academic post-docs and large industry labs (p-value = 0.088). We find no substantial differences when it comes to actionable items for the *CL community (\ud835\udc4414\u2013 \ud835\udc4417), indicating that implementing popular ideas would be welcomed by all groups. However, we find some differences when it comes to encourag- ing the release of models (\ud835\udc4418). For instance, Fig- ure 9d shows that academic post-docs had a higher preference towards reviewers rewarding papers that promise to release models than academic PIs. Also, participants from small industry would prefer visual branding over awards in contrast to large industry. In terms of the job sector, we again Job Sector. find no significant differences with respect to re- viewers asking for too expensive experiments (\ud835\udc4410) or critique being justified (\ud835\udc4412). Interestingly, re- spondents from small industry received fewer such requests (\ud835\udc4411) compared to post-docs (p-value = 0.024), PIs (p-value = 0.061), and larger industry (p-value = 0.087). The most concerning trend can be observed when comparing the different groups with respect to their lack of compute resources to 5.2 Discussion and Recommendations Our analysis shows that the two most pressing is- sues among our respondents are the lack of re- sources to reproduce results and reviewers request- ing for too expensive experiments without proper justification. This is reflected in the large support for both respective counter measures; namely, ask- ing reviewers to provide justification and the release of smaller models that would allow researchers to at least reproduce some experiments. Considering the 4https://2022.naacl.org/blog/ reproducibility-track/ 5Kruskal-Wallis test: \ud835\udc3b 5 = 12.486 > \ud835\udc3b 5 0 = 9.488. Always 5 4 3 2 Never 1 Student PD Aca. PI Aca. (s) Ind. (l) Ind. Figure 8: \ud835\udc4413: Lack of resources by job sector. success of badges at NAACL 2022 with 175 code, 98 model and 20 reproducibility badges, introduc- ing an explicit badge for small"}, {"question": " What percentage of participants believed that their work could benefit from the release of small versions of pretrained models alongside large ones?", "answer": " 75.6%", "ref_chunk": "Reviewing Finally, we quantified how the concerns about the environmental impact and differences in terms of available compute resources affect peer reviewing (\ud835\udc4410\u2013\ud835\udc4413). We further asked our participants four questions (\ud835\udc4414\u2013\ud835\udc4417) which relate to concrete ideas that would change the reviewing process and en- courage model release (\ud835\udc4418). 5.1 Analysis Figure 7a shows that 30.1% of the participants ex- perienced being asked (during peer-review) to con- duct additional experiments that were too expensive for them (\ud835\udc4410) with 77 respondents having experi- enced this more than once (\ud835\udc4411) and 19.2% having a substantially higher number (five or more times) according to our outlier analysis. Most participants (65.9%) further thought that this criticism was un- justified (\ud835\udc4412). Figure 7b (\ud835\udc4413) shows that 34.3% of the respondents often or always lacked resources to reproduce previous experiments and 41.4% some- times. Only 7.7% never or 16.7% rarely faced a lack of resources to reproduce experiments. With respect to the concrete reviewing actions, Fig. 9a shows that a large majority (89.8%) of our participants would consider submitting their work to a dedicated track on efficient methods (\ud835\udc4414). Following up on the results from the survey, such an efficiency track was implemented at EMNLP 2022. 35.9% of our participants were unsure about requesting authors to justify the allocation of bud- get for experiments (\ud835\udc4415), with 41% voting for yes. Also, even though 52.6% of the participants had not been asked for experiments that were too expensive for them, a clear majority of the partic- ipants (83.7%) would like to require reviewers to justify their petitions for more experiments (\ud835\udc4416). Lastly, we also see a large majority (75.6%) that be- lieved that their work could benefit from the release of small versions of pretrained models alongside large ones (\ud835\udc4417). To promote this, a majority of our respondents thought that venues should have a visible branding of papers to release a model (59.3%) and that reviewers should be instructed to reward model release (50.6%). 42.6% of respon- dents thought that the venues should grant a best artifact award. 11.5% of respondents supported none of the options. A first step towards increasing the reproducibility and ensuring the submission of experimental code was implemented at NAACL (a) Reviewer critique (b) Reproducing results Q10: Did reviewers ask for too expensive experiments? Q13: Lack of resources to reproduce results? Q12: Was the critique justified? 50 0 100 40 Q10 Q12 15 30 19 17 53 66 s t n a p i c i t r a P % 30 20 10 % Participants Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Never Rarely Sometimes Often Always Figure 7: Analysis on how of a lack of resources can affect research. In (a), we show what percentage of participants had been asked by reviewers for too expensive experiments (\ud835\udc4410) and if so, if they felt the critique was justified (\ud835\udc4412). In (b), we show how often our participants could not reproduce previous results due to a lack of computational resources (\ud835\udc4413). 2022 by introducing a badge system at the repro- ducibility track.4 Upon acceptance, the authors could follow specific procedures to earn three types of badges: 1) open-source code, 2) trained model, and 3) reproducible results. Seniority. We find no significant differences w.r.t. the seniority of our participants regarding \ud835\udc4410\u2013\ud835\udc4418. However, junior researchers (1\u20135 years) showed a substantially higher tendency towards requesting authors to justify their compute bud- get (\ud835\udc4415) against all other age groups (p-values < 0.035). We also observe in Figure 9c diverging preferences between junior and senior groups in terms of ideas to improve the reviewing process (\ud835\udc4418). Junior researchers (1\u20135 years) seemed to be more inclined towards a visual branding as well as instructing reviewers than senior researchers (11\u2013 15 years with a p-value of 0.089 and 16+ years with a p-value of 0.085). reproduce experiments (\ud835\udc4413, Fig. 8); where we find significant differences and conduct pairwise analyses.5 In general, students suffered most, with a significant difference compared to the large in- dustry sector with a p-value of 0.002 < 0.005 = \ud835\udefc (Bonferroni-corrected). We further find substantial differences between students and academic PIs (p- value = 0.026) and between academic post-docs and large industry labs (p-value = 0.088). We find no substantial differences when it comes to actionable items for the *CL community (\ud835\udc4414\u2013 \ud835\udc4417), indicating that implementing popular ideas would be welcomed by all groups. However, we find some differences when it comes to encourag- ing the release of models (\ud835\udc4418). For instance, Fig- ure 9d shows that academic post-docs had a higher preference towards reviewers rewarding papers that promise to release models than academic PIs. Also, participants from small industry would prefer visual branding over awards in contrast to large industry. In terms of the job sector, we again Job Sector. find no significant differences with respect to re- viewers asking for too expensive experiments (\ud835\udc4410) or critique being justified (\ud835\udc4412). Interestingly, re- spondents from small industry received fewer such requests (\ud835\udc4411) compared to post-docs (p-value = 0.024), PIs (p-value = 0.061), and larger industry (p-value = 0.087). The most concerning trend can be observed when comparing the different groups with respect to their lack of compute resources to 5.2 Discussion and Recommendations Our analysis shows that the two most pressing is- sues among our respondents are the lack of re- sources to reproduce results and reviewers request- ing for too expensive experiments without proper justification. This is reflected in the large support for both respective counter measures; namely, ask- ing reviewers to provide justification and the release of smaller models that would allow researchers to at least reproduce some experiments. Considering the 4https://2022.naacl.org/blog/ reproducibility-track/ 5Kruskal-Wallis test: \ud835\udc3b 5 = 12.486 > \ud835\udc3b 5 0 = 9.488. Always 5 4 3 2 Never 1 Student PD Aca. PI Aca. (s) Ind. (l) Ind. Figure 8: \ud835\udc4413: Lack of resources by job sector. success of badges at NAACL 2022 with 175 code, 98 model and 20 reproducibility badges, introduc- ing an explicit badge for small"}, {"question": " What type of badges could authors earn at NAACL 2022 for their accepted submissions?", "answer": " 1) open-source code, 2) trained model, and 3) reproducible results", "ref_chunk": "Reviewing Finally, we quantified how the concerns about the environmental impact and differences in terms of available compute resources affect peer reviewing (\ud835\udc4410\u2013\ud835\udc4413). We further asked our participants four questions (\ud835\udc4414\u2013\ud835\udc4417) which relate to concrete ideas that would change the reviewing process and en- courage model release (\ud835\udc4418). 5.1 Analysis Figure 7a shows that 30.1% of the participants ex- perienced being asked (during peer-review) to con- duct additional experiments that were too expensive for them (\ud835\udc4410) with 77 respondents having experi- enced this more than once (\ud835\udc4411) and 19.2% having a substantially higher number (five or more times) according to our outlier analysis. Most participants (65.9%) further thought that this criticism was un- justified (\ud835\udc4412). Figure 7b (\ud835\udc4413) shows that 34.3% of the respondents often or always lacked resources to reproduce previous experiments and 41.4% some- times. Only 7.7% never or 16.7% rarely faced a lack of resources to reproduce experiments. With respect to the concrete reviewing actions, Fig. 9a shows that a large majority (89.8%) of our participants would consider submitting their work to a dedicated track on efficient methods (\ud835\udc4414). Following up on the results from the survey, such an efficiency track was implemented at EMNLP 2022. 35.9% of our participants were unsure about requesting authors to justify the allocation of bud- get for experiments (\ud835\udc4415), with 41% voting for yes. Also, even though 52.6% of the participants had not been asked for experiments that were too expensive for them, a clear majority of the partic- ipants (83.7%) would like to require reviewers to justify their petitions for more experiments (\ud835\udc4416). Lastly, we also see a large majority (75.6%) that be- lieved that their work could benefit from the release of small versions of pretrained models alongside large ones (\ud835\udc4417). To promote this, a majority of our respondents thought that venues should have a visible branding of papers to release a model (59.3%) and that reviewers should be instructed to reward model release (50.6%). 42.6% of respon- dents thought that the venues should grant a best artifact award. 11.5% of respondents supported none of the options. A first step towards increasing the reproducibility and ensuring the submission of experimental code was implemented at NAACL (a) Reviewer critique (b) Reproducing results Q10: Did reviewers ask for too expensive experiments? Q13: Lack of resources to reproduce results? Q12: Was the critique justified? 50 0 100 40 Q10 Q12 15 30 19 17 53 66 s t n a p i c i t r a P % 30 20 10 % Participants Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Never Rarely Sometimes Often Always Figure 7: Analysis on how of a lack of resources can affect research. In (a), we show what percentage of participants had been asked by reviewers for too expensive experiments (\ud835\udc4410) and if so, if they felt the critique was justified (\ud835\udc4412). In (b), we show how often our participants could not reproduce previous results due to a lack of computational resources (\ud835\udc4413). 2022 by introducing a badge system at the repro- ducibility track.4 Upon acceptance, the authors could follow specific procedures to earn three types of badges: 1) open-source code, 2) trained model, and 3) reproducible results. Seniority. We find no significant differences w.r.t. the seniority of our participants regarding \ud835\udc4410\u2013\ud835\udc4418. However, junior researchers (1\u20135 years) showed a substantially higher tendency towards requesting authors to justify their compute bud- get (\ud835\udc4415) against all other age groups (p-values < 0.035). We also observe in Figure 9c diverging preferences between junior and senior groups in terms of ideas to improve the reviewing process (\ud835\udc4418). Junior researchers (1\u20135 years) seemed to be more inclined towards a visual branding as well as instructing reviewers than senior researchers (11\u2013 15 years with a p-value of 0.089 and 16+ years with a p-value of 0.085). reproduce experiments (\ud835\udc4413, Fig. 8); where we find significant differences and conduct pairwise analyses.5 In general, students suffered most, with a significant difference compared to the large in- dustry sector with a p-value of 0.002 < 0.005 = \ud835\udefc (Bonferroni-corrected). We further find substantial differences between students and academic PIs (p- value = 0.026) and between academic post-docs and large industry labs (p-value = 0.088). We find no substantial differences when it comes to actionable items for the *CL community (\ud835\udc4414\u2013 \ud835\udc4417), indicating that implementing popular ideas would be welcomed by all groups. However, we find some differences when it comes to encourag- ing the release of models (\ud835\udc4418). For instance, Fig- ure 9d shows that academic post-docs had a higher preference towards reviewers rewarding papers that promise to release models than academic PIs. Also, participants from small industry would prefer visual branding over awards in contrast to large industry. In terms of the job sector, we again Job Sector. find no significant differences with respect to re- viewers asking for too expensive experiments (\ud835\udc4410) or critique being justified (\ud835\udc4412). Interestingly, re- spondents from small industry received fewer such requests (\ud835\udc4411) compared to post-docs (p-value = 0.024), PIs (p-value = 0.061), and larger industry (p-value = 0.087). The most concerning trend can be observed when comparing the different groups with respect to their lack of compute resources to 5.2 Discussion and Recommendations Our analysis shows that the two most pressing is- sues among our respondents are the lack of re- sources to reproduce results and reviewers request- ing for too expensive experiments without proper justification. This is reflected in the large support for both respective counter measures; namely, ask- ing reviewers to provide justification and the release of smaller models that would allow researchers to at least reproduce some experiments. Considering the 4https://2022.naacl.org/blog/ reproducibility-track/ 5Kruskal-Wallis test: \ud835\udc3b 5 = 12.486 > \ud835\udc3b 5 0 = 9.488. Always 5 4 3 2 Never 1 Student PD Aca. PI Aca. (s) Ind. (l) Ind. Figure 8: \ud835\udc4413: Lack of resources by job sector. success of badges at NAACL 2022 with 175 code, 98 model and 20 reproducibility badges, introduc- ing an explicit badge for small"}, {"question": " Which group showed a higher tendency towards requesting authors to justify their compute budget?", "answer": " Junior researchers (1\u20135 years)", "ref_chunk": "Reviewing Finally, we quantified how the concerns about the environmental impact and differences in terms of available compute resources affect peer reviewing (\ud835\udc4410\u2013\ud835\udc4413). We further asked our participants four questions (\ud835\udc4414\u2013\ud835\udc4417) which relate to concrete ideas that would change the reviewing process and en- courage model release (\ud835\udc4418). 5.1 Analysis Figure 7a shows that 30.1% of the participants ex- perienced being asked (during peer-review) to con- duct additional experiments that were too expensive for them (\ud835\udc4410) with 77 respondents having experi- enced this more than once (\ud835\udc4411) and 19.2% having a substantially higher number (five or more times) according to our outlier analysis. Most participants (65.9%) further thought that this criticism was un- justified (\ud835\udc4412). Figure 7b (\ud835\udc4413) shows that 34.3% of the respondents often or always lacked resources to reproduce previous experiments and 41.4% some- times. Only 7.7% never or 16.7% rarely faced a lack of resources to reproduce experiments. With respect to the concrete reviewing actions, Fig. 9a shows that a large majority (89.8%) of our participants would consider submitting their work to a dedicated track on efficient methods (\ud835\udc4414). Following up on the results from the survey, such an efficiency track was implemented at EMNLP 2022. 35.9% of our participants were unsure about requesting authors to justify the allocation of bud- get for experiments (\ud835\udc4415), with 41% voting for yes. Also, even though 52.6% of the participants had not been asked for experiments that were too expensive for them, a clear majority of the partic- ipants (83.7%) would like to require reviewers to justify their petitions for more experiments (\ud835\udc4416). Lastly, we also see a large majority (75.6%) that be- lieved that their work could benefit from the release of small versions of pretrained models alongside large ones (\ud835\udc4417). To promote this, a majority of our respondents thought that venues should have a visible branding of papers to release a model (59.3%) and that reviewers should be instructed to reward model release (50.6%). 42.6% of respon- dents thought that the venues should grant a best artifact award. 11.5% of respondents supported none of the options. A first step towards increasing the reproducibility and ensuring the submission of experimental code was implemented at NAACL (a) Reviewer critique (b) Reproducing results Q10: Did reviewers ask for too expensive experiments? Q13: Lack of resources to reproduce results? Q12: Was the critique justified? 50 0 100 40 Q10 Q12 15 30 19 17 53 66 s t n a p i c i t r a P % 30 20 10 % Participants Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Never Rarely Sometimes Often Always Figure 7: Analysis on how of a lack of resources can affect research. In (a), we show what percentage of participants had been asked by reviewers for too expensive experiments (\ud835\udc4410) and if so, if they felt the critique was justified (\ud835\udc4412). In (b), we show how often our participants could not reproduce previous results due to a lack of computational resources (\ud835\udc4413). 2022 by introducing a badge system at the repro- ducibility track.4 Upon acceptance, the authors could follow specific procedures to earn three types of badges: 1) open-source code, 2) trained model, and 3) reproducible results. Seniority. We find no significant differences w.r.t. the seniority of our participants regarding \ud835\udc4410\u2013\ud835\udc4418. However, junior researchers (1\u20135 years) showed a substantially higher tendency towards requesting authors to justify their compute bud- get (\ud835\udc4415) against all other age groups (p-values < 0.035). We also observe in Figure 9c diverging preferences between junior and senior groups in terms of ideas to improve the reviewing process (\ud835\udc4418). Junior researchers (1\u20135 years) seemed to be more inclined towards a visual branding as well as instructing reviewers than senior researchers (11\u2013 15 years with a p-value of 0.089 and 16+ years with a p-value of 0.085). reproduce experiments (\ud835\udc4413, Fig. 8); where we find significant differences and conduct pairwise analyses.5 In general, students suffered most, with a significant difference compared to the large in- dustry sector with a p-value of 0.002 < 0.005 = \ud835\udefc (Bonferroni-corrected). We further find substantial differences between students and academic PIs (p- value = 0.026) and between academic post-docs and large industry labs (p-value = 0.088). We find no substantial differences when it comes to actionable items for the *CL community (\ud835\udc4414\u2013 \ud835\udc4417), indicating that implementing popular ideas would be welcomed by all groups. However, we find some differences when it comes to encourag- ing the release of models (\ud835\udc4418). For instance, Fig- ure 9d shows that academic post-docs had a higher preference towards reviewers rewarding papers that promise to release models than academic PIs. Also, participants from small industry would prefer visual branding over awards in contrast to large industry. In terms of the job sector, we again Job Sector. find no significant differences with respect to re- viewers asking for too expensive experiments (\ud835\udc4410) or critique being justified (\ud835\udc4412). Interestingly, re- spondents from small industry received fewer such requests (\ud835\udc4411) compared to post-docs (p-value = 0.024), PIs (p-value = 0.061), and larger industry (p-value = 0.087). The most concerning trend can be observed when comparing the different groups with respect to their lack of compute resources to 5.2 Discussion and Recommendations Our analysis shows that the two most pressing is- sues among our respondents are the lack of re- sources to reproduce results and reviewers request- ing for too expensive experiments without proper justification. This is reflected in the large support for both respective counter measures; namely, ask- ing reviewers to provide justification and the release of smaller models that would allow researchers to at least reproduce some experiments. Considering the 4https://2022.naacl.org/blog/ reproducibility-track/ 5Kruskal-Wallis test: \ud835\udc3b 5 = 12.486 > \ud835\udc3b 5 0 = 9.488. Always 5 4 3 2 Never 1 Student PD Aca. PI Aca. (s) Ind. (l) Ind. Figure 8: \ud835\udc4413: Lack of resources by job sector. success of badges at NAACL 2022 with 175 code, 98 model and 20 reproducibility badges, introduc- ing an explicit badge for small"}, {"question": " Which group had a higher preference towards reviewers rewarding papers that promise to release models over visual branding?", "answer": " Academic post-docs", "ref_chunk": "Reviewing Finally, we quantified how the concerns about the environmental impact and differences in terms of available compute resources affect peer reviewing (\ud835\udc4410\u2013\ud835\udc4413). We further asked our participants four questions (\ud835\udc4414\u2013\ud835\udc4417) which relate to concrete ideas that would change the reviewing process and en- courage model release (\ud835\udc4418). 5.1 Analysis Figure 7a shows that 30.1% of the participants ex- perienced being asked (during peer-review) to con- duct additional experiments that were too expensive for them (\ud835\udc4410) with 77 respondents having experi- enced this more than once (\ud835\udc4411) and 19.2% having a substantially higher number (five or more times) according to our outlier analysis. Most participants (65.9%) further thought that this criticism was un- justified (\ud835\udc4412). Figure 7b (\ud835\udc4413) shows that 34.3% of the respondents often or always lacked resources to reproduce previous experiments and 41.4% some- times. Only 7.7% never or 16.7% rarely faced a lack of resources to reproduce experiments. With respect to the concrete reviewing actions, Fig. 9a shows that a large majority (89.8%) of our participants would consider submitting their work to a dedicated track on efficient methods (\ud835\udc4414). Following up on the results from the survey, such an efficiency track was implemented at EMNLP 2022. 35.9% of our participants were unsure about requesting authors to justify the allocation of bud- get for experiments (\ud835\udc4415), with 41% voting for yes. Also, even though 52.6% of the participants had not been asked for experiments that were too expensive for them, a clear majority of the partic- ipants (83.7%) would like to require reviewers to justify their petitions for more experiments (\ud835\udc4416). Lastly, we also see a large majority (75.6%) that be- lieved that their work could benefit from the release of small versions of pretrained models alongside large ones (\ud835\udc4417). To promote this, a majority of our respondents thought that venues should have a visible branding of papers to release a model (59.3%) and that reviewers should be instructed to reward model release (50.6%). 42.6% of respon- dents thought that the venues should grant a best artifact award. 11.5% of respondents supported none of the options. A first step towards increasing the reproducibility and ensuring the submission of experimental code was implemented at NAACL (a) Reviewer critique (b) Reproducing results Q10: Did reviewers ask for too expensive experiments? Q13: Lack of resources to reproduce results? Q12: Was the critique justified? 50 0 100 40 Q10 Q12 15 30 19 17 53 66 s t n a p i c i t r a P % 30 20 10 % Participants Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Never Rarely Sometimes Often Always Figure 7: Analysis on how of a lack of resources can affect research. In (a), we show what percentage of participants had been asked by reviewers for too expensive experiments (\ud835\udc4410) and if so, if they felt the critique was justified (\ud835\udc4412). In (b), we show how often our participants could not reproduce previous results due to a lack of computational resources (\ud835\udc4413). 2022 by introducing a badge system at the repro- ducibility track.4 Upon acceptance, the authors could follow specific procedures to earn three types of badges: 1) open-source code, 2) trained model, and 3) reproducible results. Seniority. We find no significant differences w.r.t. the seniority of our participants regarding \ud835\udc4410\u2013\ud835\udc4418. However, junior researchers (1\u20135 years) showed a substantially higher tendency towards requesting authors to justify their compute bud- get (\ud835\udc4415) against all other age groups (p-values < 0.035). We also observe in Figure 9c diverging preferences between junior and senior groups in terms of ideas to improve the reviewing process (\ud835\udc4418). Junior researchers (1\u20135 years) seemed to be more inclined towards a visual branding as well as instructing reviewers than senior researchers (11\u2013 15 years with a p-value of 0.089 and 16+ years with a p-value of 0.085). reproduce experiments (\ud835\udc4413, Fig. 8); where we find significant differences and conduct pairwise analyses.5 In general, students suffered most, with a significant difference compared to the large in- dustry sector with a p-value of 0.002 < 0.005 = \ud835\udefc (Bonferroni-corrected). We further find substantial differences between students and academic PIs (p- value = 0.026) and between academic post-docs and large industry labs (p-value = 0.088). We find no substantial differences when it comes to actionable items for the *CL community (\ud835\udc4414\u2013 \ud835\udc4417), indicating that implementing popular ideas would be welcomed by all groups. However, we find some differences when it comes to encourag- ing the release of models (\ud835\udc4418). For instance, Fig- ure 9d shows that academic post-docs had a higher preference towards reviewers rewarding papers that promise to release models than academic PIs. Also, participants from small industry would prefer visual branding over awards in contrast to large industry. In terms of the job sector, we again Job Sector. find no significant differences with respect to re- viewers asking for too expensive experiments (\ud835\udc4410) or critique being justified (\ud835\udc4412). Interestingly, re- spondents from small industry received fewer such requests (\ud835\udc4411) compared to post-docs (p-value = 0.024), PIs (p-value = 0.061), and larger industry (p-value = 0.087). The most concerning trend can be observed when comparing the different groups with respect to their lack of compute resources to 5.2 Discussion and Recommendations Our analysis shows that the two most pressing is- sues among our respondents are the lack of re- sources to reproduce results and reviewers request- ing for too expensive experiments without proper justification. This is reflected in the large support for both respective counter measures; namely, ask- ing reviewers to provide justification and the release of smaller models that would allow researchers to at least reproduce some experiments. Considering the 4https://2022.naacl.org/blog/ reproducibility-track/ 5Kruskal-Wallis test: \ud835\udc3b 5 = 12.486 > \ud835\udc3b 5 0 = 9.488. Always 5 4 3 2 Never 1 Student PD Aca. PI Aca. (s) Ind. (l) Ind. Figure 8: \ud835\udc4413: Lack of resources by job sector. success of badges at NAACL 2022 with 175 code, 98 model and 20 reproducibility badges, introduc- ing an explicit badge for small"}], "doc_text": "Reviewing Finally, we quantified how the concerns about the environmental impact and differences in terms of available compute resources affect peer reviewing (\ud835\udc4410\u2013\ud835\udc4413). We further asked our participants four questions (\ud835\udc4414\u2013\ud835\udc4417) which relate to concrete ideas that would change the reviewing process and en- courage model release (\ud835\udc4418). 5.1 Analysis Figure 7a shows that 30.1% of the participants ex- perienced being asked (during peer-review) to con- duct additional experiments that were too expensive for them (\ud835\udc4410) with 77 respondents having experi- enced this more than once (\ud835\udc4411) and 19.2% having a substantially higher number (five or more times) according to our outlier analysis. Most participants (65.9%) further thought that this criticism was un- justified (\ud835\udc4412). Figure 7b (\ud835\udc4413) shows that 34.3% of the respondents often or always lacked resources to reproduce previous experiments and 41.4% some- times. Only 7.7% never or 16.7% rarely faced a lack of resources to reproduce experiments. With respect to the concrete reviewing actions, Fig. 9a shows that a large majority (89.8%) of our participants would consider submitting their work to a dedicated track on efficient methods (\ud835\udc4414). Following up on the results from the survey, such an efficiency track was implemented at EMNLP 2022. 35.9% of our participants were unsure about requesting authors to justify the allocation of bud- get for experiments (\ud835\udc4415), with 41% voting for yes. Also, even though 52.6% of the participants had not been asked for experiments that were too expensive for them, a clear majority of the partic- ipants (83.7%) would like to require reviewers to justify their petitions for more experiments (\ud835\udc4416). Lastly, we also see a large majority (75.6%) that be- lieved that their work could benefit from the release of small versions of pretrained models alongside large ones (\ud835\udc4417). To promote this, a majority of our respondents thought that venues should have a visible branding of papers to release a model (59.3%) and that reviewers should be instructed to reward model release (50.6%). 42.6% of respon- dents thought that the venues should grant a best artifact award. 11.5% of respondents supported none of the options. A first step towards increasing the reproducibility and ensuring the submission of experimental code was implemented at NAACL (a) Reviewer critique (b) Reproducing results Q10: Did reviewers ask for too expensive experiments? Q13: Lack of resources to reproduce results? Q12: Was the critique justified? 50 0 100 40 Q10 Q12 15 30 19 17 53 66 s t n a p i c i t r a P % 30 20 10 % Participants Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Never Rarely Sometimes Often Always Figure 7: Analysis on how of a lack of resources can affect research. In (a), we show what percentage of participants had been asked by reviewers for too expensive experiments (\ud835\udc4410) and if so, if they felt the critique was justified (\ud835\udc4412). In (b), we show how often our participants could not reproduce previous results due to a lack of computational resources (\ud835\udc4413). 2022 by introducing a badge system at the repro- ducibility track.4 Upon acceptance, the authors could follow specific procedures to earn three types of badges: 1) open-source code, 2) trained model, and 3) reproducible results. Seniority. We find no significant differences w.r.t. the seniority of our participants regarding \ud835\udc4410\u2013\ud835\udc4418. However, junior researchers (1\u20135 years) showed a substantially higher tendency towards requesting authors to justify their compute bud- get (\ud835\udc4415) against all other age groups (p-values < 0.035). We also observe in Figure 9c diverging preferences between junior and senior groups in terms of ideas to improve the reviewing process (\ud835\udc4418). Junior researchers (1\u20135 years) seemed to be more inclined towards a visual branding as well as instructing reviewers than senior researchers (11\u2013 15 years with a p-value of 0.089 and 16+ years with a p-value of 0.085). reproduce experiments (\ud835\udc4413, Fig. 8); where we find significant differences and conduct pairwise analyses.5 In general, students suffered most, with a significant difference compared to the large in- dustry sector with a p-value of 0.002 < 0.005 = \ud835\udefc (Bonferroni-corrected). We further find substantial differences between students and academic PIs (p- value = 0.026) and between academic post-docs and large industry labs (p-value = 0.088). We find no substantial differences when it comes to actionable items for the *CL community (\ud835\udc4414\u2013 \ud835\udc4417), indicating that implementing popular ideas would be welcomed by all groups. However, we find some differences when it comes to encourag- ing the release of models (\ud835\udc4418). For instance, Fig- ure 9d shows that academic post-docs had a higher preference towards reviewers rewarding papers that promise to release models than academic PIs. Also, participants from small industry would prefer visual branding over awards in contrast to large industry. In terms of the job sector, we again Job Sector. find no significant differences with respect to re- viewers asking for too expensive experiments (\ud835\udc4410) or critique being justified (\ud835\udc4412). Interestingly, re- spondents from small industry received fewer such requests (\ud835\udc4411) compared to post-docs (p-value = 0.024), PIs (p-value = 0.061), and larger industry (p-value = 0.087). The most concerning trend can be observed when comparing the different groups with respect to their lack of compute resources to 5.2 Discussion and Recommendations Our analysis shows that the two most pressing is- sues among our respondents are the lack of re- sources to reproduce results and reviewers request- ing for too expensive experiments without proper justification. This is reflected in the large support for both respective counter measures; namely, ask- ing reviewers to provide justification and the release of smaller models that would allow researchers to at least reproduce some experiments. Considering the 4https://2022.naacl.org/blog/ reproducibility-track/ 5Kruskal-Wallis test: \ud835\udc3b 5 = 12.486 > \ud835\udc3b 5 0 = 9.488. Always 5 4 3 2 Never 1 Student PD Aca. PI Aca. (s) Ind. (l) Ind. Figure 8: \ud835\udc4413: Lack of resources by job sector. success of badges at NAACL 2022 with 175 code, 98 model and 20 reproducibility badges, introduc- ing an explicit badge for small"}