{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Improved_Logical_Reasoning_of_Language_Models_via_Differentiable_Symbolic_Programming_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the CLUTRR dataset divided into based on difficulties?", "answer": " The CLUTRR dataset is divided into different difficulties measured by k, the number of facts used in the reasoning chain.", "ref_chunk": "dataset is synthetic, the sentences are crowd- sourced and hence there is a considerable amount of naturalness inside the dataset. The family kin- ship graph is synthetic and the names of the family members are randomized. For ablation study, we manually crafted 92 kinship composition rules as an external symbolic knowledge base. This yields the following symbolic information for each data- point: 1) the full kinship graph corresponding to the story, 2) the symbolic knowledge base (KB), and 3) a query representing the question. The CLUTRR dataset is divided into different dif\ufb01cul- ties measured by k, the number of facts used in the reasoning chain. For training, we only have 10K data points with 5K k = 2 and another 5K k = 3, meaning that we can only receive supervi- sion on data with short reasoning chains. The test set, on the other hand, contains 1.1K examples with k \u2208 {2, . . . , 10}. DBpedia-INF is a curated subset of the evalua- tion dataset used in RuleBert (Saeed et al., 2021). Similar to CLUTRR, it is generated synthetically to test the reasoning capability of LMs. Given a synthetic passage describing the relation between entities, and soft deductive logic rules, we aim to deduce the relationship between any two entities. The symbolic program of DBpedia-INF consists of 26 predicates, 161 soft rules mined from DB- pedia, and 16 rules de\ufb01ning the negation and sym- metricity between the predicates. The dif\ufb01culty of the questions is represented in terms of reason- ing length from k \u2208 {0, . . . , 5}.2 Larger k implies harder question. Compared to the exact dataset used in Rulebert, we clean it in order to ensure the question-answer pairs are logically consistent and probabilistically correct. 4.2 Experimental Setup Implementation. We employ Scallop (Huang et al., 2021) as the differentiable symbolic infer- ence module. We show the program used for CLUTRR reasoning task in Figure 2. It comprises relation type declarations, deductive rules for kin- ship reasoning, and integrity constraints for com- puting semantic loss (attached in the Appendix). The program used for DBpedia-INF is written in a similar manner with additional high-order predi- cates listed in Table 2. Pre-trained LMs for \ufb01ne-tuning. We used the HuggingFace (Wolf et al., 2019) pre-trained w2v- google-news-300, RoBERTa-base, and DeBERTa- base as the pretrained language models. We \ufb01ne- tune RoBERTa-base and DeBERTa-base during training with binary cross entropy loss. Our rela- tion extraction module is implemented by adding an MLP classi\ufb01er after the LM, accepting a con- catenation of the embedding of the two entities and the embedding of the whole windowed context. Our model. Our main model, DSR-LM, uses RoBERTa as the underlying LM. The relation clas- si\ufb01er is a 2-layer fully connected MLP. For training, we initialize \u03c6 by prompting the LM. To accelerate the learning process, we use multinomial sampling to retrieve 150 rules for symbolic reasoning. Dur- ing testing, we will instead pick the top 150 rules. We use two Adam optimizer to update \u03b8 and \u03c6, with learning rate 10\u22125 and 10\u22122 respectively. For ablation studies, we present a few other mod- els. First, we ablate on back-bone LMs. Speci\ufb01- cally, we have DSR-LM-DeBERTa which uses De- 2A length of 0 means that the hypothesis can be veri\ufb01ed using the facts alone without using any rules. // Relation declaration type kinship(rela: String, subject: String, object: String) type query(subject: String, object: String) type composite(r1: String, r2: String, r3: String) // Rules to derive the final answer rel kinship(r3,a,c) = kinship(r1,a,b), kinship(r2,b,c), composite(r1,r2,r3), a != c rel answer(r) = query(s, o), derive(r, s, o) // Integrity constraints (6 for kinship graph and 2 for rule learning) rel violation(!r) = r := forall(a, b: kinship(FATHER, a, b) => kinship(SON, b, a) or kinship(DAUGHTER, b, a)) // Other constraints are omitted... Figure 2: The Scallop program used in the CLUTRR reasoning task. BERTa as the back-bone LM. DSR-w2v-BiLSTM, on the other hand, uses as back-bone the word2vec (Mikolov et al., 2013) model for word embedding and BiLSTM (Huang et al., 2015) for sequential en- coding. For DSR-LM-with-Manual-Rule we treat the logic rules as given, meaning that we provide 92 composition rules for CLUTRR and around 180 rules for DBpedia-INF. In this case, we set ground truth rules to have 1.0 weight and therefore \u03c6 is not learnt. Then, we have DSR-LM-without-IC which does not have integrity constraints and se- mantic loss. Lastly, we have DSR-without-LM that takes ground truth structured entity relation graph as input. This way, we do not need the underlying relation extractor and only \u03c6 needs to be learned. tion network built upon the LM. GPT-3 contains 175B parameters, and RoBERTa uses 123M param- eters. The classi\ufb01cation model of our method has 2.97M parameters (assuming using embeddings from RoBERTa). With extra 10K parameters for rule weights, our DSR-LM framework has around 127M parameters. For GPT-3 variants, we conduct experiments on CLUTRR with GPT-3 under the Zero-Shot (GPT-3 ZS), GPT-3 Fine-Tuned (GPT-3 FT), and Few(5)- Shot (GPT-3 5S) (Brown et al., 2020), as well as Zero-Shot-CoT (GPT-3 ZS-CoT) (Kojima et al., 2022a) settings. For fair comparison, we also in- clude the ground truth kinship composition knowl- edge in GPT-3 zero shot (GPT-3 ZS w/ Rule), and 5 shot (GPT-3 5S w/ Rule). We include the prompts we used and additional details in Appendix A. Baselines. We compare DSR-LM with a spec- trum of baselines from purely neural to logically structured. The baselines include pretrained large language models (BERT (Kenton and Toutanova, 2019) and RoBERTa (Liu et al., 2019)), non-LM counterparts (BiLSTM (Hochreiter and Schmid- huber, 1997; Cho et al., 2014) and BERT-LSTM), structured models (GAT (Veli\u02c7ckovi\u00b4c et al., 2018), RN (Santoro et al., 2017), and MAC (Hudson and Manning, 2018)), and other neuro-symbolic mod- els (CTP (Minervini et al., 2020), RuleBert (Saeed et al., 2021)). The structured models include those models with relational inductive biases, while the neuro-symbolic model uses logic constraints. 60.98 60 ) % ( y c a r u c c A"}, {"question": " How many data points are there in the training set for the CLUTRR dataset?", "answer": " There are 10K data points in the training set for the CLUTRR dataset.", "ref_chunk": "dataset is synthetic, the sentences are crowd- sourced and hence there is a considerable amount of naturalness inside the dataset. The family kin- ship graph is synthetic and the names of the family members are randomized. For ablation study, we manually crafted 92 kinship composition rules as an external symbolic knowledge base. This yields the following symbolic information for each data- point: 1) the full kinship graph corresponding to the story, 2) the symbolic knowledge base (KB), and 3) a query representing the question. The CLUTRR dataset is divided into different dif\ufb01cul- ties measured by k, the number of facts used in the reasoning chain. For training, we only have 10K data points with 5K k = 2 and another 5K k = 3, meaning that we can only receive supervi- sion on data with short reasoning chains. The test set, on the other hand, contains 1.1K examples with k \u2208 {2, . . . , 10}. DBpedia-INF is a curated subset of the evalua- tion dataset used in RuleBert (Saeed et al., 2021). Similar to CLUTRR, it is generated synthetically to test the reasoning capability of LMs. Given a synthetic passage describing the relation between entities, and soft deductive logic rules, we aim to deduce the relationship between any two entities. The symbolic program of DBpedia-INF consists of 26 predicates, 161 soft rules mined from DB- pedia, and 16 rules de\ufb01ning the negation and sym- metricity between the predicates. The dif\ufb01culty of the questions is represented in terms of reason- ing length from k \u2208 {0, . . . , 5}.2 Larger k implies harder question. Compared to the exact dataset used in Rulebert, we clean it in order to ensure the question-answer pairs are logically consistent and probabilistically correct. 4.2 Experimental Setup Implementation. We employ Scallop (Huang et al., 2021) as the differentiable symbolic infer- ence module. We show the program used for CLUTRR reasoning task in Figure 2. It comprises relation type declarations, deductive rules for kin- ship reasoning, and integrity constraints for com- puting semantic loss (attached in the Appendix). The program used for DBpedia-INF is written in a similar manner with additional high-order predi- cates listed in Table 2. Pre-trained LMs for \ufb01ne-tuning. We used the HuggingFace (Wolf et al., 2019) pre-trained w2v- google-news-300, RoBERTa-base, and DeBERTa- base as the pretrained language models. We \ufb01ne- tune RoBERTa-base and DeBERTa-base during training with binary cross entropy loss. Our rela- tion extraction module is implemented by adding an MLP classi\ufb01er after the LM, accepting a con- catenation of the embedding of the two entities and the embedding of the whole windowed context. Our model. Our main model, DSR-LM, uses RoBERTa as the underlying LM. The relation clas- si\ufb01er is a 2-layer fully connected MLP. For training, we initialize \u03c6 by prompting the LM. To accelerate the learning process, we use multinomial sampling to retrieve 150 rules for symbolic reasoning. Dur- ing testing, we will instead pick the top 150 rules. We use two Adam optimizer to update \u03b8 and \u03c6, with learning rate 10\u22125 and 10\u22122 respectively. For ablation studies, we present a few other mod- els. First, we ablate on back-bone LMs. Speci\ufb01- cally, we have DSR-LM-DeBERTa which uses De- 2A length of 0 means that the hypothesis can be veri\ufb01ed using the facts alone without using any rules. // Relation declaration type kinship(rela: String, subject: String, object: String) type query(subject: String, object: String) type composite(r1: String, r2: String, r3: String) // Rules to derive the final answer rel kinship(r3,a,c) = kinship(r1,a,b), kinship(r2,b,c), composite(r1,r2,r3), a != c rel answer(r) = query(s, o), derive(r, s, o) // Integrity constraints (6 for kinship graph and 2 for rule learning) rel violation(!r) = r := forall(a, b: kinship(FATHER, a, b) => kinship(SON, b, a) or kinship(DAUGHTER, b, a)) // Other constraints are omitted... Figure 2: The Scallop program used in the CLUTRR reasoning task. BERTa as the back-bone LM. DSR-w2v-BiLSTM, on the other hand, uses as back-bone the word2vec (Mikolov et al., 2013) model for word embedding and BiLSTM (Huang et al., 2015) for sequential en- coding. For DSR-LM-with-Manual-Rule we treat the logic rules as given, meaning that we provide 92 composition rules for CLUTRR and around 180 rules for DBpedia-INF. In this case, we set ground truth rules to have 1.0 weight and therefore \u03c6 is not learnt. Then, we have DSR-LM-without-IC which does not have integrity constraints and se- mantic loss. Lastly, we have DSR-without-LM that takes ground truth structured entity relation graph as input. This way, we do not need the underlying relation extractor and only \u03c6 needs to be learned. tion network built upon the LM. GPT-3 contains 175B parameters, and RoBERTa uses 123M param- eters. The classi\ufb01cation model of our method has 2.97M parameters (assuming using embeddings from RoBERTa). With extra 10K parameters for rule weights, our DSR-LM framework has around 127M parameters. For GPT-3 variants, we conduct experiments on CLUTRR with GPT-3 under the Zero-Shot (GPT-3 ZS), GPT-3 Fine-Tuned (GPT-3 FT), and Few(5)- Shot (GPT-3 5S) (Brown et al., 2020), as well as Zero-Shot-CoT (GPT-3 ZS-CoT) (Kojima et al., 2022a) settings. For fair comparison, we also in- clude the ground truth kinship composition knowl- edge in GPT-3 zero shot (GPT-3 ZS w/ Rule), and 5 shot (GPT-3 5S w/ Rule). We include the prompts we used and additional details in Appendix A. Baselines. We compare DSR-LM with a spec- trum of baselines from purely neural to logically structured. The baselines include pretrained large language models (BERT (Kenton and Toutanova, 2019) and RoBERTa (Liu et al., 2019)), non-LM counterparts (BiLSTM (Hochreiter and Schmid- huber, 1997; Cho et al., 2014) and BERT-LSTM), structured models (GAT (Veli\u02c7ckovi\u00b4c et al., 2018), RN (Santoro et al., 2017), and MAC (Hudson and Manning, 2018)), and other neuro-symbolic mod- els (CTP (Minervini et al., 2020), RuleBert (Saeed et al., 2021)). The structured models include those models with relational inductive biases, while the neuro-symbolic model uses logic constraints. 60.98 60 ) % ( y c a r u c c A"}, {"question": " What is the symbolic program of DBpedia-INF composed of?", "answer": " The symbolic program of DBpedia-INF consists of 26 predicates, 161 soft rules mined from DBpedia, and 16 rules defining the negation and symmetry between the predicates.", "ref_chunk": "dataset is synthetic, the sentences are crowd- sourced and hence there is a considerable amount of naturalness inside the dataset. The family kin- ship graph is synthetic and the names of the family members are randomized. For ablation study, we manually crafted 92 kinship composition rules as an external symbolic knowledge base. This yields the following symbolic information for each data- point: 1) the full kinship graph corresponding to the story, 2) the symbolic knowledge base (KB), and 3) a query representing the question. The CLUTRR dataset is divided into different dif\ufb01cul- ties measured by k, the number of facts used in the reasoning chain. For training, we only have 10K data points with 5K k = 2 and another 5K k = 3, meaning that we can only receive supervi- sion on data with short reasoning chains. The test set, on the other hand, contains 1.1K examples with k \u2208 {2, . . . , 10}. DBpedia-INF is a curated subset of the evalua- tion dataset used in RuleBert (Saeed et al., 2021). Similar to CLUTRR, it is generated synthetically to test the reasoning capability of LMs. Given a synthetic passage describing the relation between entities, and soft deductive logic rules, we aim to deduce the relationship between any two entities. The symbolic program of DBpedia-INF consists of 26 predicates, 161 soft rules mined from DB- pedia, and 16 rules de\ufb01ning the negation and sym- metricity between the predicates. The dif\ufb01culty of the questions is represented in terms of reason- ing length from k \u2208 {0, . . . , 5}.2 Larger k implies harder question. Compared to the exact dataset used in Rulebert, we clean it in order to ensure the question-answer pairs are logically consistent and probabilistically correct. 4.2 Experimental Setup Implementation. We employ Scallop (Huang et al., 2021) as the differentiable symbolic infer- ence module. We show the program used for CLUTRR reasoning task in Figure 2. It comprises relation type declarations, deductive rules for kin- ship reasoning, and integrity constraints for com- puting semantic loss (attached in the Appendix). The program used for DBpedia-INF is written in a similar manner with additional high-order predi- cates listed in Table 2. Pre-trained LMs for \ufb01ne-tuning. We used the HuggingFace (Wolf et al., 2019) pre-trained w2v- google-news-300, RoBERTa-base, and DeBERTa- base as the pretrained language models. We \ufb01ne- tune RoBERTa-base and DeBERTa-base during training with binary cross entropy loss. Our rela- tion extraction module is implemented by adding an MLP classi\ufb01er after the LM, accepting a con- catenation of the embedding of the two entities and the embedding of the whole windowed context. Our model. Our main model, DSR-LM, uses RoBERTa as the underlying LM. The relation clas- si\ufb01er is a 2-layer fully connected MLP. For training, we initialize \u03c6 by prompting the LM. To accelerate the learning process, we use multinomial sampling to retrieve 150 rules for symbolic reasoning. Dur- ing testing, we will instead pick the top 150 rules. We use two Adam optimizer to update \u03b8 and \u03c6, with learning rate 10\u22125 and 10\u22122 respectively. For ablation studies, we present a few other mod- els. First, we ablate on back-bone LMs. Speci\ufb01- cally, we have DSR-LM-DeBERTa which uses De- 2A length of 0 means that the hypothesis can be veri\ufb01ed using the facts alone without using any rules. // Relation declaration type kinship(rela: String, subject: String, object: String) type query(subject: String, object: String) type composite(r1: String, r2: String, r3: String) // Rules to derive the final answer rel kinship(r3,a,c) = kinship(r1,a,b), kinship(r2,b,c), composite(r1,r2,r3), a != c rel answer(r) = query(s, o), derive(r, s, o) // Integrity constraints (6 for kinship graph and 2 for rule learning) rel violation(!r) = r := forall(a, b: kinship(FATHER, a, b) => kinship(SON, b, a) or kinship(DAUGHTER, b, a)) // Other constraints are omitted... Figure 2: The Scallop program used in the CLUTRR reasoning task. BERTa as the back-bone LM. DSR-w2v-BiLSTM, on the other hand, uses as back-bone the word2vec (Mikolov et al., 2013) model for word embedding and BiLSTM (Huang et al., 2015) for sequential en- coding. For DSR-LM-with-Manual-Rule we treat the logic rules as given, meaning that we provide 92 composition rules for CLUTRR and around 180 rules for DBpedia-INF. In this case, we set ground truth rules to have 1.0 weight and therefore \u03c6 is not learnt. Then, we have DSR-LM-without-IC which does not have integrity constraints and se- mantic loss. Lastly, we have DSR-without-LM that takes ground truth structured entity relation graph as input. This way, we do not need the underlying relation extractor and only \u03c6 needs to be learned. tion network built upon the LM. GPT-3 contains 175B parameters, and RoBERTa uses 123M param- eters. The classi\ufb01cation model of our method has 2.97M parameters (assuming using embeddings from RoBERTa). With extra 10K parameters for rule weights, our DSR-LM framework has around 127M parameters. For GPT-3 variants, we conduct experiments on CLUTRR with GPT-3 under the Zero-Shot (GPT-3 ZS), GPT-3 Fine-Tuned (GPT-3 FT), and Few(5)- Shot (GPT-3 5S) (Brown et al., 2020), as well as Zero-Shot-CoT (GPT-3 ZS-CoT) (Kojima et al., 2022a) settings. For fair comparison, we also in- clude the ground truth kinship composition knowl- edge in GPT-3 zero shot (GPT-3 ZS w/ Rule), and 5 shot (GPT-3 5S w/ Rule). We include the prompts we used and additional details in Appendix A. Baselines. We compare DSR-LM with a spec- trum of baselines from purely neural to logically structured. The baselines include pretrained large language models (BERT (Kenton and Toutanova, 2019) and RoBERTa (Liu et al., 2019)), non-LM counterparts (BiLSTM (Hochreiter and Schmid- huber, 1997; Cho et al., 2014) and BERT-LSTM), structured models (GAT (Veli\u02c7ckovi\u00b4c et al., 2018), RN (Santoro et al., 2017), and MAC (Hudson and Manning, 2018)), and other neuro-symbolic mod- els (CTP (Minervini et al., 2020), RuleBert (Saeed et al., 2021)). The structured models include those models with relational inductive biases, while the neuro-symbolic model uses logic constraints. 60.98 60 ) % ( y c a r u c c A"}, {"question": " What is the main model used in the study and what LM does it use?", "answer": " The main model is DSR-LM, which uses RoBERTa as the underlying LM.", "ref_chunk": "dataset is synthetic, the sentences are crowd- sourced and hence there is a considerable amount of naturalness inside the dataset. The family kin- ship graph is synthetic and the names of the family members are randomized. For ablation study, we manually crafted 92 kinship composition rules as an external symbolic knowledge base. This yields the following symbolic information for each data- point: 1) the full kinship graph corresponding to the story, 2) the symbolic knowledge base (KB), and 3) a query representing the question. The CLUTRR dataset is divided into different dif\ufb01cul- ties measured by k, the number of facts used in the reasoning chain. For training, we only have 10K data points with 5K k = 2 and another 5K k = 3, meaning that we can only receive supervi- sion on data with short reasoning chains. The test set, on the other hand, contains 1.1K examples with k \u2208 {2, . . . , 10}. DBpedia-INF is a curated subset of the evalua- tion dataset used in RuleBert (Saeed et al., 2021). Similar to CLUTRR, it is generated synthetically to test the reasoning capability of LMs. Given a synthetic passage describing the relation between entities, and soft deductive logic rules, we aim to deduce the relationship between any two entities. The symbolic program of DBpedia-INF consists of 26 predicates, 161 soft rules mined from DB- pedia, and 16 rules de\ufb01ning the negation and sym- metricity between the predicates. The dif\ufb01culty of the questions is represented in terms of reason- ing length from k \u2208 {0, . . . , 5}.2 Larger k implies harder question. Compared to the exact dataset used in Rulebert, we clean it in order to ensure the question-answer pairs are logically consistent and probabilistically correct. 4.2 Experimental Setup Implementation. We employ Scallop (Huang et al., 2021) as the differentiable symbolic infer- ence module. We show the program used for CLUTRR reasoning task in Figure 2. It comprises relation type declarations, deductive rules for kin- ship reasoning, and integrity constraints for com- puting semantic loss (attached in the Appendix). The program used for DBpedia-INF is written in a similar manner with additional high-order predi- cates listed in Table 2. Pre-trained LMs for \ufb01ne-tuning. We used the HuggingFace (Wolf et al., 2019) pre-trained w2v- google-news-300, RoBERTa-base, and DeBERTa- base as the pretrained language models. We \ufb01ne- tune RoBERTa-base and DeBERTa-base during training with binary cross entropy loss. Our rela- tion extraction module is implemented by adding an MLP classi\ufb01er after the LM, accepting a con- catenation of the embedding of the two entities and the embedding of the whole windowed context. Our model. Our main model, DSR-LM, uses RoBERTa as the underlying LM. The relation clas- si\ufb01er is a 2-layer fully connected MLP. For training, we initialize \u03c6 by prompting the LM. To accelerate the learning process, we use multinomial sampling to retrieve 150 rules for symbolic reasoning. Dur- ing testing, we will instead pick the top 150 rules. We use two Adam optimizer to update \u03b8 and \u03c6, with learning rate 10\u22125 and 10\u22122 respectively. For ablation studies, we present a few other mod- els. First, we ablate on back-bone LMs. Speci\ufb01- cally, we have DSR-LM-DeBERTa which uses De- 2A length of 0 means that the hypothesis can be veri\ufb01ed using the facts alone without using any rules. // Relation declaration type kinship(rela: String, subject: String, object: String) type query(subject: String, object: String) type composite(r1: String, r2: String, r3: String) // Rules to derive the final answer rel kinship(r3,a,c) = kinship(r1,a,b), kinship(r2,b,c), composite(r1,r2,r3), a != c rel answer(r) = query(s, o), derive(r, s, o) // Integrity constraints (6 for kinship graph and 2 for rule learning) rel violation(!r) = r := forall(a, b: kinship(FATHER, a, b) => kinship(SON, b, a) or kinship(DAUGHTER, b, a)) // Other constraints are omitted... Figure 2: The Scallop program used in the CLUTRR reasoning task. BERTa as the back-bone LM. DSR-w2v-BiLSTM, on the other hand, uses as back-bone the word2vec (Mikolov et al., 2013) model for word embedding and BiLSTM (Huang et al., 2015) for sequential en- coding. For DSR-LM-with-Manual-Rule we treat the logic rules as given, meaning that we provide 92 composition rules for CLUTRR and around 180 rules for DBpedia-INF. In this case, we set ground truth rules to have 1.0 weight and therefore \u03c6 is not learnt. Then, we have DSR-LM-without-IC which does not have integrity constraints and se- mantic loss. Lastly, we have DSR-without-LM that takes ground truth structured entity relation graph as input. This way, we do not need the underlying relation extractor and only \u03c6 needs to be learned. tion network built upon the LM. GPT-3 contains 175B parameters, and RoBERTa uses 123M param- eters. The classi\ufb01cation model of our method has 2.97M parameters (assuming using embeddings from RoBERTa). With extra 10K parameters for rule weights, our DSR-LM framework has around 127M parameters. For GPT-3 variants, we conduct experiments on CLUTRR with GPT-3 under the Zero-Shot (GPT-3 ZS), GPT-3 Fine-Tuned (GPT-3 FT), and Few(5)- Shot (GPT-3 5S) (Brown et al., 2020), as well as Zero-Shot-CoT (GPT-3 ZS-CoT) (Kojima et al., 2022a) settings. For fair comparison, we also in- clude the ground truth kinship composition knowl- edge in GPT-3 zero shot (GPT-3 ZS w/ Rule), and 5 shot (GPT-3 5S w/ Rule). We include the prompts we used and additional details in Appendix A. Baselines. We compare DSR-LM with a spec- trum of baselines from purely neural to logically structured. The baselines include pretrained large language models (BERT (Kenton and Toutanova, 2019) and RoBERTa (Liu et al., 2019)), non-LM counterparts (BiLSTM (Hochreiter and Schmid- huber, 1997; Cho et al., 2014) and BERT-LSTM), structured models (GAT (Veli\u02c7ckovi\u00b4c et al., 2018), RN (Santoro et al., 2017), and MAC (Hudson and Manning, 2018)), and other neuro-symbolic mod- els (CTP (Minervini et al., 2020), RuleBert (Saeed et al., 2021)). The structured models include those models with relational inductive biases, while the neuro-symbolic model uses logic constraints. 60.98 60 ) % ( y c a r u c c A"}, {"question": " What is the main purpose of DBpedia-INF dataset?", "answer": " The main purpose of the DBpedia-INF dataset is to deduce the relationship between any two entities based on a synthetic passage and deductive logic rules.", "ref_chunk": "dataset is synthetic, the sentences are crowd- sourced and hence there is a considerable amount of naturalness inside the dataset. The family kin- ship graph is synthetic and the names of the family members are randomized. For ablation study, we manually crafted 92 kinship composition rules as an external symbolic knowledge base. This yields the following symbolic information for each data- point: 1) the full kinship graph corresponding to the story, 2) the symbolic knowledge base (KB), and 3) a query representing the question. The CLUTRR dataset is divided into different dif\ufb01cul- ties measured by k, the number of facts used in the reasoning chain. For training, we only have 10K data points with 5K k = 2 and another 5K k = 3, meaning that we can only receive supervi- sion on data with short reasoning chains. The test set, on the other hand, contains 1.1K examples with k \u2208 {2, . . . , 10}. DBpedia-INF is a curated subset of the evalua- tion dataset used in RuleBert (Saeed et al., 2021). Similar to CLUTRR, it is generated synthetically to test the reasoning capability of LMs. Given a synthetic passage describing the relation between entities, and soft deductive logic rules, we aim to deduce the relationship between any two entities. The symbolic program of DBpedia-INF consists of 26 predicates, 161 soft rules mined from DB- pedia, and 16 rules de\ufb01ning the negation and sym- metricity between the predicates. The dif\ufb01culty of the questions is represented in terms of reason- ing length from k \u2208 {0, . . . , 5}.2 Larger k implies harder question. Compared to the exact dataset used in Rulebert, we clean it in order to ensure the question-answer pairs are logically consistent and probabilistically correct. 4.2 Experimental Setup Implementation. We employ Scallop (Huang et al., 2021) as the differentiable symbolic infer- ence module. We show the program used for CLUTRR reasoning task in Figure 2. It comprises relation type declarations, deductive rules for kin- ship reasoning, and integrity constraints for com- puting semantic loss (attached in the Appendix). The program used for DBpedia-INF is written in a similar manner with additional high-order predi- cates listed in Table 2. Pre-trained LMs for \ufb01ne-tuning. We used the HuggingFace (Wolf et al., 2019) pre-trained w2v- google-news-300, RoBERTa-base, and DeBERTa- base as the pretrained language models. We \ufb01ne- tune RoBERTa-base and DeBERTa-base during training with binary cross entropy loss. Our rela- tion extraction module is implemented by adding an MLP classi\ufb01er after the LM, accepting a con- catenation of the embedding of the two entities and the embedding of the whole windowed context. Our model. Our main model, DSR-LM, uses RoBERTa as the underlying LM. The relation clas- si\ufb01er is a 2-layer fully connected MLP. For training, we initialize \u03c6 by prompting the LM. To accelerate the learning process, we use multinomial sampling to retrieve 150 rules for symbolic reasoning. Dur- ing testing, we will instead pick the top 150 rules. We use two Adam optimizer to update \u03b8 and \u03c6, with learning rate 10\u22125 and 10\u22122 respectively. For ablation studies, we present a few other mod- els. First, we ablate on back-bone LMs. Speci\ufb01- cally, we have DSR-LM-DeBERTa which uses De- 2A length of 0 means that the hypothesis can be veri\ufb01ed using the facts alone without using any rules. // Relation declaration type kinship(rela: String, subject: String, object: String) type query(subject: String, object: String) type composite(r1: String, r2: String, r3: String) // Rules to derive the final answer rel kinship(r3,a,c) = kinship(r1,a,b), kinship(r2,b,c), composite(r1,r2,r3), a != c rel answer(r) = query(s, o), derive(r, s, o) // Integrity constraints (6 for kinship graph and 2 for rule learning) rel violation(!r) = r := forall(a, b: kinship(FATHER, a, b) => kinship(SON, b, a) or kinship(DAUGHTER, b, a)) // Other constraints are omitted... Figure 2: The Scallop program used in the CLUTRR reasoning task. BERTa as the back-bone LM. DSR-w2v-BiLSTM, on the other hand, uses as back-bone the word2vec (Mikolov et al., 2013) model for word embedding and BiLSTM (Huang et al., 2015) for sequential en- coding. For DSR-LM-with-Manual-Rule we treat the logic rules as given, meaning that we provide 92 composition rules for CLUTRR and around 180 rules for DBpedia-INF. In this case, we set ground truth rules to have 1.0 weight and therefore \u03c6 is not learnt. Then, we have DSR-LM-without-IC which does not have integrity constraints and se- mantic loss. Lastly, we have DSR-without-LM that takes ground truth structured entity relation graph as input. This way, we do not need the underlying relation extractor and only \u03c6 needs to be learned. tion network built upon the LM. GPT-3 contains 175B parameters, and RoBERTa uses 123M param- eters. The classi\ufb01cation model of our method has 2.97M parameters (assuming using embeddings from RoBERTa). With extra 10K parameters for rule weights, our DSR-LM framework has around 127M parameters. For GPT-3 variants, we conduct experiments on CLUTRR with GPT-3 under the Zero-Shot (GPT-3 ZS), GPT-3 Fine-Tuned (GPT-3 FT), and Few(5)- Shot (GPT-3 5S) (Brown et al., 2020), as well as Zero-Shot-CoT (GPT-3 ZS-CoT) (Kojima et al., 2022a) settings. For fair comparison, we also in- clude the ground truth kinship composition knowl- edge in GPT-3 zero shot (GPT-3 ZS w/ Rule), and 5 shot (GPT-3 5S w/ Rule). We include the prompts we used and additional details in Appendix A. Baselines. We compare DSR-LM with a spec- trum of baselines from purely neural to logically structured. The baselines include pretrained large language models (BERT (Kenton and Toutanova, 2019) and RoBERTa (Liu et al., 2019)), non-LM counterparts (BiLSTM (Hochreiter and Schmid- huber, 1997; Cho et al., 2014) and BERT-LSTM), structured models (GAT (Veli\u02c7ckovi\u00b4c et al., 2018), RN (Santoro et al., 2017), and MAC (Hudson and Manning, 2018)), and other neuro-symbolic mod- els (CTP (Minervini et al., 2020), RuleBert (Saeed et al., 2021)). The structured models include those models with relational inductive biases, while the neuro-symbolic model uses logic constraints. 60.98 60 ) % ( y c a r u c c A"}, {"question": " How many rules are used for symbolic reasoning during testing?", "answer": " During testing, 150 rules are used for symbolic reasoning.", "ref_chunk": "dataset is synthetic, the sentences are crowd- sourced and hence there is a considerable amount of naturalness inside the dataset. The family kin- ship graph is synthetic and the names of the family members are randomized. For ablation study, we manually crafted 92 kinship composition rules as an external symbolic knowledge base. This yields the following symbolic information for each data- point: 1) the full kinship graph corresponding to the story, 2) the symbolic knowledge base (KB), and 3) a query representing the question. The CLUTRR dataset is divided into different dif\ufb01cul- ties measured by k, the number of facts used in the reasoning chain. For training, we only have 10K data points with 5K k = 2 and another 5K k = 3, meaning that we can only receive supervi- sion on data with short reasoning chains. The test set, on the other hand, contains 1.1K examples with k \u2208 {2, . . . , 10}. DBpedia-INF is a curated subset of the evalua- tion dataset used in RuleBert (Saeed et al., 2021). Similar to CLUTRR, it is generated synthetically to test the reasoning capability of LMs. Given a synthetic passage describing the relation between entities, and soft deductive logic rules, we aim to deduce the relationship between any two entities. The symbolic program of DBpedia-INF consists of 26 predicates, 161 soft rules mined from DB- pedia, and 16 rules de\ufb01ning the negation and sym- metricity between the predicates. The dif\ufb01culty of the questions is represented in terms of reason- ing length from k \u2208 {0, . . . , 5}.2 Larger k implies harder question. Compared to the exact dataset used in Rulebert, we clean it in order to ensure the question-answer pairs are logically consistent and probabilistically correct. 4.2 Experimental Setup Implementation. We employ Scallop (Huang et al., 2021) as the differentiable symbolic infer- ence module. We show the program used for CLUTRR reasoning task in Figure 2. It comprises relation type declarations, deductive rules for kin- ship reasoning, and integrity constraints for com- puting semantic loss (attached in the Appendix). The program used for DBpedia-INF is written in a similar manner with additional high-order predi- cates listed in Table 2. Pre-trained LMs for \ufb01ne-tuning. We used the HuggingFace (Wolf et al., 2019) pre-trained w2v- google-news-300, RoBERTa-base, and DeBERTa- base as the pretrained language models. We \ufb01ne- tune RoBERTa-base and DeBERTa-base during training with binary cross entropy loss. Our rela- tion extraction module is implemented by adding an MLP classi\ufb01er after the LM, accepting a con- catenation of the embedding of the two entities and the embedding of the whole windowed context. Our model. Our main model, DSR-LM, uses RoBERTa as the underlying LM. The relation clas- si\ufb01er is a 2-layer fully connected MLP. For training, we initialize \u03c6 by prompting the LM. To accelerate the learning process, we use multinomial sampling to retrieve 150 rules for symbolic reasoning. Dur- ing testing, we will instead pick the top 150 rules. We use two Adam optimizer to update \u03b8 and \u03c6, with learning rate 10\u22125 and 10\u22122 respectively. For ablation studies, we present a few other mod- els. First, we ablate on back-bone LMs. Speci\ufb01- cally, we have DSR-LM-DeBERTa which uses De- 2A length of 0 means that the hypothesis can be veri\ufb01ed using the facts alone without using any rules. // Relation declaration type kinship(rela: String, subject: String, object: String) type query(subject: String, object: String) type composite(r1: String, r2: String, r3: String) // Rules to derive the final answer rel kinship(r3,a,c) = kinship(r1,a,b), kinship(r2,b,c), composite(r1,r2,r3), a != c rel answer(r) = query(s, o), derive(r, s, o) // Integrity constraints (6 for kinship graph and 2 for rule learning) rel violation(!r) = r := forall(a, b: kinship(FATHER, a, b) => kinship(SON, b, a) or kinship(DAUGHTER, b, a)) // Other constraints are omitted... Figure 2: The Scallop program used in the CLUTRR reasoning task. BERTa as the back-bone LM. DSR-w2v-BiLSTM, on the other hand, uses as back-bone the word2vec (Mikolov et al., 2013) model for word embedding and BiLSTM (Huang et al., 2015) for sequential en- coding. For DSR-LM-with-Manual-Rule we treat the logic rules as given, meaning that we provide 92 composition rules for CLUTRR and around 180 rules for DBpedia-INF. In this case, we set ground truth rules to have 1.0 weight and therefore \u03c6 is not learnt. Then, we have DSR-LM-without-IC which does not have integrity constraints and se- mantic loss. Lastly, we have DSR-without-LM that takes ground truth structured entity relation graph as input. This way, we do not need the underlying relation extractor and only \u03c6 needs to be learned. tion network built upon the LM. GPT-3 contains 175B parameters, and RoBERTa uses 123M param- eters. The classi\ufb01cation model of our method has 2.97M parameters (assuming using embeddings from RoBERTa). With extra 10K parameters for rule weights, our DSR-LM framework has around 127M parameters. For GPT-3 variants, we conduct experiments on CLUTRR with GPT-3 under the Zero-Shot (GPT-3 ZS), GPT-3 Fine-Tuned (GPT-3 FT), and Few(5)- Shot (GPT-3 5S) (Brown et al., 2020), as well as Zero-Shot-CoT (GPT-3 ZS-CoT) (Kojima et al., 2022a) settings. For fair comparison, we also in- clude the ground truth kinship composition knowl- edge in GPT-3 zero shot (GPT-3 ZS w/ Rule), and 5 shot (GPT-3 5S w/ Rule). We include the prompts we used and additional details in Appendix A. Baselines. We compare DSR-LM with a spec- trum of baselines from purely neural to logically structured. The baselines include pretrained large language models (BERT (Kenton and Toutanova, 2019) and RoBERTa (Liu et al., 2019)), non-LM counterparts (BiLSTM (Hochreiter and Schmid- huber, 1997; Cho et al., 2014) and BERT-LSTM), structured models (GAT (Veli\u02c7ckovi\u00b4c et al., 2018), RN (Santoro et al., 2017), and MAC (Hudson and Manning, 2018)), and other neuro-symbolic mod- els (CTP (Minervini et al., 2020), RuleBert (Saeed et al., 2021)). The structured models include those models with relational inductive biases, while the neuro-symbolic model uses logic constraints. 60.98 60 ) % ( y c a r u c c A"}, {"question": " What is the main difference between DSR-LM-DeBERTa and DSR-w2v-BiLSTM models?", "answer": " DSR-LM-DeBERTa uses DeBERTa as the backbone LM, while DSR-w2v-BiLSTM uses word2vec for word embedding and BiLSTM for sequential encoding.", "ref_chunk": "dataset is synthetic, the sentences are crowd- sourced and hence there is a considerable amount of naturalness inside the dataset. The family kin- ship graph is synthetic and the names of the family members are randomized. For ablation study, we manually crafted 92 kinship composition rules as an external symbolic knowledge base. This yields the following symbolic information for each data- point: 1) the full kinship graph corresponding to the story, 2) the symbolic knowledge base (KB), and 3) a query representing the question. The CLUTRR dataset is divided into different dif\ufb01cul- ties measured by k, the number of facts used in the reasoning chain. For training, we only have 10K data points with 5K k = 2 and another 5K k = 3, meaning that we can only receive supervi- sion on data with short reasoning chains. The test set, on the other hand, contains 1.1K examples with k \u2208 {2, . . . , 10}. DBpedia-INF is a curated subset of the evalua- tion dataset used in RuleBert (Saeed et al., 2021). Similar to CLUTRR, it is generated synthetically to test the reasoning capability of LMs. Given a synthetic passage describing the relation between entities, and soft deductive logic rules, we aim to deduce the relationship between any two entities. The symbolic program of DBpedia-INF consists of 26 predicates, 161 soft rules mined from DB- pedia, and 16 rules de\ufb01ning the negation and sym- metricity between the predicates. The dif\ufb01culty of the questions is represented in terms of reason- ing length from k \u2208 {0, . . . , 5}.2 Larger k implies harder question. Compared to the exact dataset used in Rulebert, we clean it in order to ensure the question-answer pairs are logically consistent and probabilistically correct. 4.2 Experimental Setup Implementation. We employ Scallop (Huang et al., 2021) as the differentiable symbolic infer- ence module. We show the program used for CLUTRR reasoning task in Figure 2. It comprises relation type declarations, deductive rules for kin- ship reasoning, and integrity constraints for com- puting semantic loss (attached in the Appendix). The program used for DBpedia-INF is written in a similar manner with additional high-order predi- cates listed in Table 2. Pre-trained LMs for \ufb01ne-tuning. We used the HuggingFace (Wolf et al., 2019) pre-trained w2v- google-news-300, RoBERTa-base, and DeBERTa- base as the pretrained language models. We \ufb01ne- tune RoBERTa-base and DeBERTa-base during training with binary cross entropy loss. Our rela- tion extraction module is implemented by adding an MLP classi\ufb01er after the LM, accepting a con- catenation of the embedding of the two entities and the embedding of the whole windowed context. Our model. Our main model, DSR-LM, uses RoBERTa as the underlying LM. The relation clas- si\ufb01er is a 2-layer fully connected MLP. For training, we initialize \u03c6 by prompting the LM. To accelerate the learning process, we use multinomial sampling to retrieve 150 rules for symbolic reasoning. Dur- ing testing, we will instead pick the top 150 rules. We use two Adam optimizer to update \u03b8 and \u03c6, with learning rate 10\u22125 and 10\u22122 respectively. For ablation studies, we present a few other mod- els. First, we ablate on back-bone LMs. Speci\ufb01- cally, we have DSR-LM-DeBERTa which uses De- 2A length of 0 means that the hypothesis can be veri\ufb01ed using the facts alone without using any rules. // Relation declaration type kinship(rela: String, subject: String, object: String) type query(subject: String, object: String) type composite(r1: String, r2: String, r3: String) // Rules to derive the final answer rel kinship(r3,a,c) = kinship(r1,a,b), kinship(r2,b,c), composite(r1,r2,r3), a != c rel answer(r) = query(s, o), derive(r, s, o) // Integrity constraints (6 for kinship graph and 2 for rule learning) rel violation(!r) = r := forall(a, b: kinship(FATHER, a, b) => kinship(SON, b, a) or kinship(DAUGHTER, b, a)) // Other constraints are omitted... Figure 2: The Scallop program used in the CLUTRR reasoning task. BERTa as the back-bone LM. DSR-w2v-BiLSTM, on the other hand, uses as back-bone the word2vec (Mikolov et al., 2013) model for word embedding and BiLSTM (Huang et al., 2015) for sequential en- coding. For DSR-LM-with-Manual-Rule we treat the logic rules as given, meaning that we provide 92 composition rules for CLUTRR and around 180 rules for DBpedia-INF. In this case, we set ground truth rules to have 1.0 weight and therefore \u03c6 is not learnt. Then, we have DSR-LM-without-IC which does not have integrity constraints and se- mantic loss. Lastly, we have DSR-without-LM that takes ground truth structured entity relation graph as input. This way, we do not need the underlying relation extractor and only \u03c6 needs to be learned. tion network built upon the LM. GPT-3 contains 175B parameters, and RoBERTa uses 123M param- eters. The classi\ufb01cation model of our method has 2.97M parameters (assuming using embeddings from RoBERTa). With extra 10K parameters for rule weights, our DSR-LM framework has around 127M parameters. For GPT-3 variants, we conduct experiments on CLUTRR with GPT-3 under the Zero-Shot (GPT-3 ZS), GPT-3 Fine-Tuned (GPT-3 FT), and Few(5)- Shot (GPT-3 5S) (Brown et al., 2020), as well as Zero-Shot-CoT (GPT-3 ZS-CoT) (Kojima et al., 2022a) settings. For fair comparison, we also in- clude the ground truth kinship composition knowl- edge in GPT-3 zero shot (GPT-3 ZS w/ Rule), and 5 shot (GPT-3 5S w/ Rule). We include the prompts we used and additional details in Appendix A. Baselines. We compare DSR-LM with a spec- trum of baselines from purely neural to logically structured. The baselines include pretrained large language models (BERT (Kenton and Toutanova, 2019) and RoBERTa (Liu et al., 2019)), non-LM counterparts (BiLSTM (Hochreiter and Schmid- huber, 1997; Cho et al., 2014) and BERT-LSTM), structured models (GAT (Veli\u02c7ckovi\u00b4c et al., 2018), RN (Santoro et al., 2017), and MAC (Hudson and Manning, 2018)), and other neuro-symbolic mod- els (CTP (Minervini et al., 2020), RuleBert (Saeed et al., 2021)). The structured models include those models with relational inductive biases, while the neuro-symbolic model uses logic constraints. 60.98 60 ) % ( y c a r u c c A"}, {"question": " How many parameters does the GPT-3 variant contain?", "answer": " GPT-3 variant contains 175B parameters.", "ref_chunk": "dataset is synthetic, the sentences are crowd- sourced and hence there is a considerable amount of naturalness inside the dataset. The family kin- ship graph is synthetic and the names of the family members are randomized. For ablation study, we manually crafted 92 kinship composition rules as an external symbolic knowledge base. This yields the following symbolic information for each data- point: 1) the full kinship graph corresponding to the story, 2) the symbolic knowledge base (KB), and 3) a query representing the question. The CLUTRR dataset is divided into different dif\ufb01cul- ties measured by k, the number of facts used in the reasoning chain. For training, we only have 10K data points with 5K k = 2 and another 5K k = 3, meaning that we can only receive supervi- sion on data with short reasoning chains. The test set, on the other hand, contains 1.1K examples with k \u2208 {2, . . . , 10}. DBpedia-INF is a curated subset of the evalua- tion dataset used in RuleBert (Saeed et al., 2021). Similar to CLUTRR, it is generated synthetically to test the reasoning capability of LMs. Given a synthetic passage describing the relation between entities, and soft deductive logic rules, we aim to deduce the relationship between any two entities. The symbolic program of DBpedia-INF consists of 26 predicates, 161 soft rules mined from DB- pedia, and 16 rules de\ufb01ning the negation and sym- metricity between the predicates. The dif\ufb01culty of the questions is represented in terms of reason- ing length from k \u2208 {0, . . . , 5}.2 Larger k implies harder question. Compared to the exact dataset used in Rulebert, we clean it in order to ensure the question-answer pairs are logically consistent and probabilistically correct. 4.2 Experimental Setup Implementation. We employ Scallop (Huang et al., 2021) as the differentiable symbolic infer- ence module. We show the program used for CLUTRR reasoning task in Figure 2. It comprises relation type declarations, deductive rules for kin- ship reasoning, and integrity constraints for com- puting semantic loss (attached in the Appendix). The program used for DBpedia-INF is written in a similar manner with additional high-order predi- cates listed in Table 2. Pre-trained LMs for \ufb01ne-tuning. We used the HuggingFace (Wolf et al., 2019) pre-trained w2v- google-news-300, RoBERTa-base, and DeBERTa- base as the pretrained language models. We \ufb01ne- tune RoBERTa-base and DeBERTa-base during training with binary cross entropy loss. Our rela- tion extraction module is implemented by adding an MLP classi\ufb01er after the LM, accepting a con- catenation of the embedding of the two entities and the embedding of the whole windowed context. Our model. Our main model, DSR-LM, uses RoBERTa as the underlying LM. The relation clas- si\ufb01er is a 2-layer fully connected MLP. For training, we initialize \u03c6 by prompting the LM. To accelerate the learning process, we use multinomial sampling to retrieve 150 rules for symbolic reasoning. Dur- ing testing, we will instead pick the top 150 rules. We use two Adam optimizer to update \u03b8 and \u03c6, with learning rate 10\u22125 and 10\u22122 respectively. For ablation studies, we present a few other mod- els. First, we ablate on back-bone LMs. Speci\ufb01- cally, we have DSR-LM-DeBERTa which uses De- 2A length of 0 means that the hypothesis can be veri\ufb01ed using the facts alone without using any rules. // Relation declaration type kinship(rela: String, subject: String, object: String) type query(subject: String, object: String) type composite(r1: String, r2: String, r3: String) // Rules to derive the final answer rel kinship(r3,a,c) = kinship(r1,a,b), kinship(r2,b,c), composite(r1,r2,r3), a != c rel answer(r) = query(s, o), derive(r, s, o) // Integrity constraints (6 for kinship graph and 2 for rule learning) rel violation(!r) = r := forall(a, b: kinship(FATHER, a, b) => kinship(SON, b, a) or kinship(DAUGHTER, b, a)) // Other constraints are omitted... Figure 2: The Scallop program used in the CLUTRR reasoning task. BERTa as the back-bone LM. DSR-w2v-BiLSTM, on the other hand, uses as back-bone the word2vec (Mikolov et al., 2013) model for word embedding and BiLSTM (Huang et al., 2015) for sequential en- coding. For DSR-LM-with-Manual-Rule we treat the logic rules as given, meaning that we provide 92 composition rules for CLUTRR and around 180 rules for DBpedia-INF. In this case, we set ground truth rules to have 1.0 weight and therefore \u03c6 is not learnt. Then, we have DSR-LM-without-IC which does not have integrity constraints and se- mantic loss. Lastly, we have DSR-without-LM that takes ground truth structured entity relation graph as input. This way, we do not need the underlying relation extractor and only \u03c6 needs to be learned. tion network built upon the LM. GPT-3 contains 175B parameters, and RoBERTa uses 123M param- eters. The classi\ufb01cation model of our method has 2.97M parameters (assuming using embeddings from RoBERTa). With extra 10K parameters for rule weights, our DSR-LM framework has around 127M parameters. For GPT-3 variants, we conduct experiments on CLUTRR with GPT-3 under the Zero-Shot (GPT-3 ZS), GPT-3 Fine-Tuned (GPT-3 FT), and Few(5)- Shot (GPT-3 5S) (Brown et al., 2020), as well as Zero-Shot-CoT (GPT-3 ZS-CoT) (Kojima et al., 2022a) settings. For fair comparison, we also in- clude the ground truth kinship composition knowl- edge in GPT-3 zero shot (GPT-3 ZS w/ Rule), and 5 shot (GPT-3 5S w/ Rule). We include the prompts we used and additional details in Appendix A. Baselines. We compare DSR-LM with a spec- trum of baselines from purely neural to logically structured. The baselines include pretrained large language models (BERT (Kenton and Toutanova, 2019) and RoBERTa (Liu et al., 2019)), non-LM counterparts (BiLSTM (Hochreiter and Schmid- huber, 1997; Cho et al., 2014) and BERT-LSTM), structured models (GAT (Veli\u02c7ckovi\u00b4c et al., 2018), RN (Santoro et al., 2017), and MAC (Hudson and Manning, 2018)), and other neuro-symbolic mod- els (CTP (Minervini et al., 2020), RuleBert (Saeed et al., 2021)). The structured models include those models with relational inductive biases, while the neuro-symbolic model uses logic constraints. 60.98 60 ) % ( y c a r u c c A"}, {"question": " What models are compared to DSR-LM in the study?", "answer": " The models compared to DSR-LM include pretrained large language models like BERT and RoBERTa, non-LM counterparts like BiLSTM and BERT-LSTM, structured models like GAT, RN, and MAC, and neuro-symbolic models like CTP and RuleBert.", "ref_chunk": "dataset is synthetic, the sentences are crowd- sourced and hence there is a considerable amount of naturalness inside the dataset. The family kin- ship graph is synthetic and the names of the family members are randomized. For ablation study, we manually crafted 92 kinship composition rules as an external symbolic knowledge base. This yields the following symbolic information for each data- point: 1) the full kinship graph corresponding to the story, 2) the symbolic knowledge base (KB), and 3) a query representing the question. The CLUTRR dataset is divided into different dif\ufb01cul- ties measured by k, the number of facts used in the reasoning chain. For training, we only have 10K data points with 5K k = 2 and another 5K k = 3, meaning that we can only receive supervi- sion on data with short reasoning chains. The test set, on the other hand, contains 1.1K examples with k \u2208 {2, . . . , 10}. DBpedia-INF is a curated subset of the evalua- tion dataset used in RuleBert (Saeed et al., 2021). Similar to CLUTRR, it is generated synthetically to test the reasoning capability of LMs. Given a synthetic passage describing the relation between entities, and soft deductive logic rules, we aim to deduce the relationship between any two entities. The symbolic program of DBpedia-INF consists of 26 predicates, 161 soft rules mined from DB- pedia, and 16 rules de\ufb01ning the negation and sym- metricity between the predicates. The dif\ufb01culty of the questions is represented in terms of reason- ing length from k \u2208 {0, . . . , 5}.2 Larger k implies harder question. Compared to the exact dataset used in Rulebert, we clean it in order to ensure the question-answer pairs are logically consistent and probabilistically correct. 4.2 Experimental Setup Implementation. We employ Scallop (Huang et al., 2021) as the differentiable symbolic infer- ence module. We show the program used for CLUTRR reasoning task in Figure 2. It comprises relation type declarations, deductive rules for kin- ship reasoning, and integrity constraints for com- puting semantic loss (attached in the Appendix). The program used for DBpedia-INF is written in a similar manner with additional high-order predi- cates listed in Table 2. Pre-trained LMs for \ufb01ne-tuning. We used the HuggingFace (Wolf et al., 2019) pre-trained w2v- google-news-300, RoBERTa-base, and DeBERTa- base as the pretrained language models. We \ufb01ne- tune RoBERTa-base and DeBERTa-base during training with binary cross entropy loss. Our rela- tion extraction module is implemented by adding an MLP classi\ufb01er after the LM, accepting a con- catenation of the embedding of the two entities and the embedding of the whole windowed context. Our model. Our main model, DSR-LM, uses RoBERTa as the underlying LM. The relation clas- si\ufb01er is a 2-layer fully connected MLP. For training, we initialize \u03c6 by prompting the LM. To accelerate the learning process, we use multinomial sampling to retrieve 150 rules for symbolic reasoning. Dur- ing testing, we will instead pick the top 150 rules. We use two Adam optimizer to update \u03b8 and \u03c6, with learning rate 10\u22125 and 10\u22122 respectively. For ablation studies, we present a few other mod- els. First, we ablate on back-bone LMs. Speci\ufb01- cally, we have DSR-LM-DeBERTa which uses De- 2A length of 0 means that the hypothesis can be veri\ufb01ed using the facts alone without using any rules. // Relation declaration type kinship(rela: String, subject: String, object: String) type query(subject: String, object: String) type composite(r1: String, r2: String, r3: String) // Rules to derive the final answer rel kinship(r3,a,c) = kinship(r1,a,b), kinship(r2,b,c), composite(r1,r2,r3), a != c rel answer(r) = query(s, o), derive(r, s, o) // Integrity constraints (6 for kinship graph and 2 for rule learning) rel violation(!r) = r := forall(a, b: kinship(FATHER, a, b) => kinship(SON, b, a) or kinship(DAUGHTER, b, a)) // Other constraints are omitted... Figure 2: The Scallop program used in the CLUTRR reasoning task. BERTa as the back-bone LM. DSR-w2v-BiLSTM, on the other hand, uses as back-bone the word2vec (Mikolov et al., 2013) model for word embedding and BiLSTM (Huang et al., 2015) for sequential en- coding. For DSR-LM-with-Manual-Rule we treat the logic rules as given, meaning that we provide 92 composition rules for CLUTRR and around 180 rules for DBpedia-INF. In this case, we set ground truth rules to have 1.0 weight and therefore \u03c6 is not learnt. Then, we have DSR-LM-without-IC which does not have integrity constraints and se- mantic loss. Lastly, we have DSR-without-LM that takes ground truth structured entity relation graph as input. This way, we do not need the underlying relation extractor and only \u03c6 needs to be learned. tion network built upon the LM. GPT-3 contains 175B parameters, and RoBERTa uses 123M param- eters. The classi\ufb01cation model of our method has 2.97M parameters (assuming using embeddings from RoBERTa). With extra 10K parameters for rule weights, our DSR-LM framework has around 127M parameters. For GPT-3 variants, we conduct experiments on CLUTRR with GPT-3 under the Zero-Shot (GPT-3 ZS), GPT-3 Fine-Tuned (GPT-3 FT), and Few(5)- Shot (GPT-3 5S) (Brown et al., 2020), as well as Zero-Shot-CoT (GPT-3 ZS-CoT) (Kojima et al., 2022a) settings. For fair comparison, we also in- clude the ground truth kinship composition knowl- edge in GPT-3 zero shot (GPT-3 ZS w/ Rule), and 5 shot (GPT-3 5S w/ Rule). We include the prompts we used and additional details in Appendix A. Baselines. We compare DSR-LM with a spec- trum of baselines from purely neural to logically structured. The baselines include pretrained large language models (BERT (Kenton and Toutanova, 2019) and RoBERTa (Liu et al., 2019)), non-LM counterparts (BiLSTM (Hochreiter and Schmid- huber, 1997; Cho et al., 2014) and BERT-LSTM), structured models (GAT (Veli\u02c7ckovi\u00b4c et al., 2018), RN (Santoro et al., 2017), and MAC (Hudson and Manning, 2018)), and other neuro-symbolic mod- els (CTP (Minervini et al., 2020), RuleBert (Saeed et al., 2021)). The structured models include those models with relational inductive biases, while the neuro-symbolic model uses logic constraints. 60.98 60 ) % ( y c a r u c c A"}, {"question": " What does the DSR-without-LM model take as input?", "answer": " DSR-without-LM takes the ground truth structured entity relation graph as input.", "ref_chunk": "dataset is synthetic, the sentences are crowd- sourced and hence there is a considerable amount of naturalness inside the dataset. The family kin- ship graph is synthetic and the names of the family members are randomized. For ablation study, we manually crafted 92 kinship composition rules as an external symbolic knowledge base. This yields the following symbolic information for each data- point: 1) the full kinship graph corresponding to the story, 2) the symbolic knowledge base (KB), and 3) a query representing the question. The CLUTRR dataset is divided into different dif\ufb01cul- ties measured by k, the number of facts used in the reasoning chain. For training, we only have 10K data points with 5K k = 2 and another 5K k = 3, meaning that we can only receive supervi- sion on data with short reasoning chains. The test set, on the other hand, contains 1.1K examples with k \u2208 {2, . . . , 10}. DBpedia-INF is a curated subset of the evalua- tion dataset used in RuleBert (Saeed et al., 2021). Similar to CLUTRR, it is generated synthetically to test the reasoning capability of LMs. Given a synthetic passage describing the relation between entities, and soft deductive logic rules, we aim to deduce the relationship between any two entities. The symbolic program of DBpedia-INF consists of 26 predicates, 161 soft rules mined from DB- pedia, and 16 rules de\ufb01ning the negation and sym- metricity between the predicates. The dif\ufb01culty of the questions is represented in terms of reason- ing length from k \u2208 {0, . . . , 5}.2 Larger k implies harder question. Compared to the exact dataset used in Rulebert, we clean it in order to ensure the question-answer pairs are logically consistent and probabilistically correct. 4.2 Experimental Setup Implementation. We employ Scallop (Huang et al., 2021) as the differentiable symbolic infer- ence module. We show the program used for CLUTRR reasoning task in Figure 2. It comprises relation type declarations, deductive rules for kin- ship reasoning, and integrity constraints for com- puting semantic loss (attached in the Appendix). The program used for DBpedia-INF is written in a similar manner with additional high-order predi- cates listed in Table 2. Pre-trained LMs for \ufb01ne-tuning. We used the HuggingFace (Wolf et al., 2019) pre-trained w2v- google-news-300, RoBERTa-base, and DeBERTa- base as the pretrained language models. We \ufb01ne- tune RoBERTa-base and DeBERTa-base during training with binary cross entropy loss. Our rela- tion extraction module is implemented by adding an MLP classi\ufb01er after the LM, accepting a con- catenation of the embedding of the two entities and the embedding of the whole windowed context. Our model. Our main model, DSR-LM, uses RoBERTa as the underlying LM. The relation clas- si\ufb01er is a 2-layer fully connected MLP. For training, we initialize \u03c6 by prompting the LM. To accelerate the learning process, we use multinomial sampling to retrieve 150 rules for symbolic reasoning. Dur- ing testing, we will instead pick the top 150 rules. We use two Adam optimizer to update \u03b8 and \u03c6, with learning rate 10\u22125 and 10\u22122 respectively. For ablation studies, we present a few other mod- els. First, we ablate on back-bone LMs. Speci\ufb01- cally, we have DSR-LM-DeBERTa which uses De- 2A length of 0 means that the hypothesis can be veri\ufb01ed using the facts alone without using any rules. // Relation declaration type kinship(rela: String, subject: String, object: String) type query(subject: String, object: String) type composite(r1: String, r2: String, r3: String) // Rules to derive the final answer rel kinship(r3,a,c) = kinship(r1,a,b), kinship(r2,b,c), composite(r1,r2,r3), a != c rel answer(r) = query(s, o), derive(r, s, o) // Integrity constraints (6 for kinship graph and 2 for rule learning) rel violation(!r) = r := forall(a, b: kinship(FATHER, a, b) => kinship(SON, b, a) or kinship(DAUGHTER, b, a)) // Other constraints are omitted... Figure 2: The Scallop program used in the CLUTRR reasoning task. BERTa as the back-bone LM. DSR-w2v-BiLSTM, on the other hand, uses as back-bone the word2vec (Mikolov et al., 2013) model for word embedding and BiLSTM (Huang et al., 2015) for sequential en- coding. For DSR-LM-with-Manual-Rule we treat the logic rules as given, meaning that we provide 92 composition rules for CLUTRR and around 180 rules for DBpedia-INF. In this case, we set ground truth rules to have 1.0 weight and therefore \u03c6 is not learnt. Then, we have DSR-LM-without-IC which does not have integrity constraints and se- mantic loss. Lastly, we have DSR-without-LM that takes ground truth structured entity relation graph as input. This way, we do not need the underlying relation extractor and only \u03c6 needs to be learned. tion network built upon the LM. GPT-3 contains 175B parameters, and RoBERTa uses 123M param- eters. The classi\ufb01cation model of our method has 2.97M parameters (assuming using embeddings from RoBERTa). With extra 10K parameters for rule weights, our DSR-LM framework has around 127M parameters. For GPT-3 variants, we conduct experiments on CLUTRR with GPT-3 under the Zero-Shot (GPT-3 ZS), GPT-3 Fine-Tuned (GPT-3 FT), and Few(5)- Shot (GPT-3 5S) (Brown et al., 2020), as well as Zero-Shot-CoT (GPT-3 ZS-CoT) (Kojima et al., 2022a) settings. For fair comparison, we also in- clude the ground truth kinship composition knowl- edge in GPT-3 zero shot (GPT-3 ZS w/ Rule), and 5 shot (GPT-3 5S w/ Rule). We include the prompts we used and additional details in Appendix A. Baselines. We compare DSR-LM with a spec- trum of baselines from purely neural to logically structured. The baselines include pretrained large language models (BERT (Kenton and Toutanova, 2019) and RoBERTa (Liu et al., 2019)), non-LM counterparts (BiLSTM (Hochreiter and Schmid- huber, 1997; Cho et al., 2014) and BERT-LSTM), structured models (GAT (Veli\u02c7ckovi\u00b4c et al., 2018), RN (Santoro et al., 2017), and MAC (Hudson and Manning, 2018)), and other neuro-symbolic mod- els (CTP (Minervini et al., 2020), RuleBert (Saeed et al., 2021)). The structured models include those models with relational inductive biases, while the neuro-symbolic model uses logic constraints. 60.98 60 ) % ( y c a r u c c A"}], "doc_text": "dataset is synthetic, the sentences are crowd- sourced and hence there is a considerable amount of naturalness inside the dataset. The family kin- ship graph is synthetic and the names of the family members are randomized. For ablation study, we manually crafted 92 kinship composition rules as an external symbolic knowledge base. This yields the following symbolic information for each data- point: 1) the full kinship graph corresponding to the story, 2) the symbolic knowledge base (KB), and 3) a query representing the question. The CLUTRR dataset is divided into different dif\ufb01cul- ties measured by k, the number of facts used in the reasoning chain. For training, we only have 10K data points with 5K k = 2 and another 5K k = 3, meaning that we can only receive supervi- sion on data with short reasoning chains. The test set, on the other hand, contains 1.1K examples with k \u2208 {2, . . . , 10}. DBpedia-INF is a curated subset of the evalua- tion dataset used in RuleBert (Saeed et al., 2021). Similar to CLUTRR, it is generated synthetically to test the reasoning capability of LMs. Given a synthetic passage describing the relation between entities, and soft deductive logic rules, we aim to deduce the relationship between any two entities. The symbolic program of DBpedia-INF consists of 26 predicates, 161 soft rules mined from DB- pedia, and 16 rules de\ufb01ning the negation and sym- metricity between the predicates. The dif\ufb01culty of the questions is represented in terms of reason- ing length from k \u2208 {0, . . . , 5}.2 Larger k implies harder question. Compared to the exact dataset used in Rulebert, we clean it in order to ensure the question-answer pairs are logically consistent and probabilistically correct. 4.2 Experimental Setup Implementation. We employ Scallop (Huang et al., 2021) as the differentiable symbolic infer- ence module. We show the program used for CLUTRR reasoning task in Figure 2. It comprises relation type declarations, deductive rules for kin- ship reasoning, and integrity constraints for com- puting semantic loss (attached in the Appendix). The program used for DBpedia-INF is written in a similar manner with additional high-order predi- cates listed in Table 2. Pre-trained LMs for \ufb01ne-tuning. We used the HuggingFace (Wolf et al., 2019) pre-trained w2v- google-news-300, RoBERTa-base, and DeBERTa- base as the pretrained language models. We \ufb01ne- tune RoBERTa-base and DeBERTa-base during training with binary cross entropy loss. Our rela- tion extraction module is implemented by adding an MLP classi\ufb01er after the LM, accepting a con- catenation of the embedding of the two entities and the embedding of the whole windowed context. Our model. Our main model, DSR-LM, uses RoBERTa as the underlying LM. The relation clas- si\ufb01er is a 2-layer fully connected MLP. For training, we initialize \u03c6 by prompting the LM. To accelerate the learning process, we use multinomial sampling to retrieve 150 rules for symbolic reasoning. Dur- ing testing, we will instead pick the top 150 rules. We use two Adam optimizer to update \u03b8 and \u03c6, with learning rate 10\u22125 and 10\u22122 respectively. For ablation studies, we present a few other mod- els. First, we ablate on back-bone LMs. Speci\ufb01- cally, we have DSR-LM-DeBERTa which uses De- 2A length of 0 means that the hypothesis can be veri\ufb01ed using the facts alone without using any rules. // Relation declaration type kinship(rela: String, subject: String, object: String) type query(subject: String, object: String) type composite(r1: String, r2: String, r3: String) // Rules to derive the final answer rel kinship(r3,a,c) = kinship(r1,a,b), kinship(r2,b,c), composite(r1,r2,r3), a != c rel answer(r) = query(s, o), derive(r, s, o) // Integrity constraints (6 for kinship graph and 2 for rule learning) rel violation(!r) = r := forall(a, b: kinship(FATHER, a, b) => kinship(SON, b, a) or kinship(DAUGHTER, b, a)) // Other constraints are omitted... Figure 2: The Scallop program used in the CLUTRR reasoning task. BERTa as the back-bone LM. DSR-w2v-BiLSTM, on the other hand, uses as back-bone the word2vec (Mikolov et al., 2013) model for word embedding and BiLSTM (Huang et al., 2015) for sequential en- coding. For DSR-LM-with-Manual-Rule we treat the logic rules as given, meaning that we provide 92 composition rules for CLUTRR and around 180 rules for DBpedia-INF. In this case, we set ground truth rules to have 1.0 weight and therefore \u03c6 is not learnt. Then, we have DSR-LM-without-IC which does not have integrity constraints and se- mantic loss. Lastly, we have DSR-without-LM that takes ground truth structured entity relation graph as input. This way, we do not need the underlying relation extractor and only \u03c6 needs to be learned. tion network built upon the LM. GPT-3 contains 175B parameters, and RoBERTa uses 123M param- eters. The classi\ufb01cation model of our method has 2.97M parameters (assuming using embeddings from RoBERTa). With extra 10K parameters for rule weights, our DSR-LM framework has around 127M parameters. For GPT-3 variants, we conduct experiments on CLUTRR with GPT-3 under the Zero-Shot (GPT-3 ZS), GPT-3 Fine-Tuned (GPT-3 FT), and Few(5)- Shot (GPT-3 5S) (Brown et al., 2020), as well as Zero-Shot-CoT (GPT-3 ZS-CoT) (Kojima et al., 2022a) settings. For fair comparison, we also in- clude the ground truth kinship composition knowl- edge in GPT-3 zero shot (GPT-3 ZS w/ Rule), and 5 shot (GPT-3 5S w/ Rule). We include the prompts we used and additional details in Appendix A. Baselines. We compare DSR-LM with a spec- trum of baselines from purely neural to logically structured. The baselines include pretrained large language models (BERT (Kenton and Toutanova, 2019) and RoBERTa (Liu et al., 2019)), non-LM counterparts (BiLSTM (Hochreiter and Schmid- huber, 1997; Cho et al., 2014) and BERT-LSTM), structured models (GAT (Veli\u02c7ckovi\u00b4c et al., 2018), RN (Santoro et al., 2017), and MAC (Hudson and Manning, 2018)), and other neuro-symbolic mod- els (CTP (Minervini et al., 2020), RuleBert (Saeed et al., 2021)). The structured models include those models with relational inductive biases, while the neuro-symbolic model uses logic constraints. 60.98 60 ) % ( y c a r u c c A"}