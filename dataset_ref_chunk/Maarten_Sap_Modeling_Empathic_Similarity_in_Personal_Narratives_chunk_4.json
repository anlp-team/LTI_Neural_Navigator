{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_Modeling_Empathic_Similarity_in_Personal_Narratives_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What type of annotation paradigm are the researchers aiming for in this study?", "answer": " A more descriptive annotation paradigm.", "ref_chunk": "distance away). We are aiming for a more descriptive annotation paradigm and thus do not expect annotators to perfectly agree (Rottger et al., 2022). Furthermore, our agreement rates are in line with other inherently personal and affect-driven annotation tasks (Sap et al., 2017; Rashkin et al., 2018). Given the difficulty of our task (reading longer stories and projecting the men- tal state of 2 characters), our agreement is in line with prior work, which achieve around 0.51 - 0.91 PPA and 0.29 - 0.34 KA. 5 Modeling Empathic Similarity To enable the retrieval and analysis of empathically similar stories, we design a task detailed below. In Appendix B, we also propose an auxiliary reason- ing task to automatically extract event, emotion, and moral features from stories, which could be used in future work to quickly generate story anno- tations. 5.1 Task Formulation task is given a query Our ultimate retrieval story Q and selects a story Si from a set of N stories {S1, S2, ..., SN } such that i = argmaxi sim(f\u03b8(Si), f\u03b8(Q)). Here, sim(\u00b7, \u00b7) is a similarity metric (e.g. cosine similarity) between two story representations f\u03b8(Si) and f\u03b8(Q) that are learned from human ratings of empathic similarity. Empathic Similarity Prediction. The overall task is, given a story pair (S1, S2), return a similar- ity score sim(f\u03b8(Si), f\u03b8(Q)) such that sim(\u00b7, \u00b7) is large for empathically similar stories and small for empathically dissimilar stories. 5.2 Models We propose finetuning LLMs to learn embeddings that capture empathic similarity using cosine dis- tance, for efficient retrieval at test time. In contrast, a popular approach is to use few-shot prompting of very large language models (e.g., GPT-3 and Chat- GPT), which have shown impressive performance across a variety of tasks (Brown et al., 2020). How- ever, in a real deployment setting, retrieval through prompting every possible pair of stories is expen- sive and inefficient. Model SBERT + finetuning BART + finetuning GPT-3 + 5 examples ChatGPT + 5 examples r 30.93 35.93 10.24 34.20 3.24 4.94 19.56 27.75 \u03c1 29.86 35.21 11.54 34.43 2.79 6.71 20.16 28.07 Acc 62.75 64.75 57.00 64.75 51.25 51.25 56.25 63.25 P 57.81 58.68 52.19 58.2 51.25 51.27 55.24 60.43 R 90.24 90.73 99.02 88.29 100 98.54 77.07 81.95 F 1 70.48 71.26 68.35 70.16 67.77 67.45 64.36 69.57 Pk=1 57.92 57.43 49.51 65.84 90.59 72.77 80.69 85.15 \u03c4rank 17.46 17.59 7.56 24.68 0.33 -4.8 13.48 21.27 \u03c1rank 18.74 18.98 9.28 26.55 0.79 -5.33 14.10 22.10 Table 4: Model performance for empathic similarity prediction task across correlation, accuracy, and retrieval metrics. r = Pearson\u2019s correlation, \u03c1 = Spearman\u2019s correlation, Acc = accuracy, P = precision, R = recall, Pk=1 = precision at k where k is 1, \u03c4rank = Kendall Tau of ranking and \u03c1rank = Spearman of ranking. Note that all scores are multiplied by 100 for easier comparison, and the maximum for each metric is 100. In bold is the best performing and underlined is the second-best performing condition for the metric. Baseline Models. We compare performance to finetuning with SBERT (multi-qa-mpnet-base-dot- v1) (Reimers and Gurevych, 2019; Brown et al., 2020) and BART model (bart-base) (Lewis et al., 2019). As a few-shot baseline, we evaluate GPT-3 (text-davinci-003) and ChatGPT\u2019s (gpt-3.5-turbo) ability to distinguish empathically similar stories by using a k-shot prompting setup as done in Sap et al. (2022); Brown et al. (2020). For the query story pair, we ask for an empathic similarity score from 1-4. We compare across k = 0 examples and k = 5 examples from the training set. We also evaluate these models\u2019 ability to generate human- like main event, emotion description, and moral summaries for each story. Again, we use a k-shot prompting setup, comparing across k = 0 and k = 10 examples. See Appendix G and Appendix C for prompts used and finetuning details. Empathy Similarity Prediction. We propose a bi- encoder architecture finetuned with mean-squared error (MSE) loss of the cosine-similarity between story pairs, as compared to the empathic similar- ity gold labels. For each of the encoders, we use a shared pretrained transformer-based model and further finetune on the 1,500 annotated story pairs in our training set. We obtain the final embedding using mean pooling of the encoder last hidden state. k = 1 (what proportion of the top-ranked stories by our model are the top-ranked story as rated by human annotators), Kendall\u2019s Tau (Abdi, 2007), and Spearman\u2019s correlation (Schober et al., 2018) for the ranking of the stories (how close the overall rankings are). Shown in Table 4, our results indicate that fine- tuning SBERT and BART with EMPATHICSTO- RIES results in performance gains across all metrics. SBERT has relatively high off-the-shelf perfor- mance, as it is trained with 215M examples specif- ically for semantic similarity tasks. However, we see that finetuning with our dataset, which contains far fewer training examples relative to SBERT\u2019s pretraining corpus, improves performance. (+ 5.35 \u03c1, +2 accuracy). BART, which is not specifically pre-trained for semantic similarity tasks, shows even greater gains across retrieval metrics when finetuned on our dataset. (22.89 \u03c1, +7.75 accuracy). We find that for BART models, fine tuning im- provements (p = 0.02, p = 0.0006 respectively), as measured with McNemar\u2019s test on the accuracy scores and Fisher\u2019s transformation on correlations, are significantly higher than baselines. While GPT-3 and ChatGPT have high perfor- mance on the precision at k retrieval metric, in practice, it is not feasible to prompt the models with every pair of stories in the retrieval corpus. 6 Automatic Evaluation To evaluate the quality of empathic similarity pre- dictions, we first compare the Spearman\u2019s and Pear- son\u2019s correlations between the cosine similarity of the sentence embeddings and the gold empathic similarity labels. Next, we bin scores into binary similar/dissimilar categories (> 2.5 and \u2264 2.5 re- spectively) compute the accuracy, precision, re- call, and F1 scores. Finally, we compute a series of retrieval-based metrics including precision at 7 User Study Prior work\u2019s versions of human evaluations (Zhou and Jurgens, 2020;"}, {"question": " Do the researchers expect annotators to perfectly agree on their annotations?", "answer": " No, the researchers do not expect annotators to perfectly agree.", "ref_chunk": "distance away). We are aiming for a more descriptive annotation paradigm and thus do not expect annotators to perfectly agree (Rottger et al., 2022). Furthermore, our agreement rates are in line with other inherently personal and affect-driven annotation tasks (Sap et al., 2017; Rashkin et al., 2018). Given the difficulty of our task (reading longer stories and projecting the men- tal state of 2 characters), our agreement is in line with prior work, which achieve around 0.51 - 0.91 PPA and 0.29 - 0.34 KA. 5 Modeling Empathic Similarity To enable the retrieval and analysis of empathically similar stories, we design a task detailed below. In Appendix B, we also propose an auxiliary reason- ing task to automatically extract event, emotion, and moral features from stories, which could be used in future work to quickly generate story anno- tations. 5.1 Task Formulation task is given a query Our ultimate retrieval story Q and selects a story Si from a set of N stories {S1, S2, ..., SN } such that i = argmaxi sim(f\u03b8(Si), f\u03b8(Q)). Here, sim(\u00b7, \u00b7) is a similarity metric (e.g. cosine similarity) between two story representations f\u03b8(Si) and f\u03b8(Q) that are learned from human ratings of empathic similarity. Empathic Similarity Prediction. The overall task is, given a story pair (S1, S2), return a similar- ity score sim(f\u03b8(Si), f\u03b8(Q)) such that sim(\u00b7, \u00b7) is large for empathically similar stories and small for empathically dissimilar stories. 5.2 Models We propose finetuning LLMs to learn embeddings that capture empathic similarity using cosine dis- tance, for efficient retrieval at test time. In contrast, a popular approach is to use few-shot prompting of very large language models (e.g., GPT-3 and Chat- GPT), which have shown impressive performance across a variety of tasks (Brown et al., 2020). How- ever, in a real deployment setting, retrieval through prompting every possible pair of stories is expen- sive and inefficient. Model SBERT + finetuning BART + finetuning GPT-3 + 5 examples ChatGPT + 5 examples r 30.93 35.93 10.24 34.20 3.24 4.94 19.56 27.75 \u03c1 29.86 35.21 11.54 34.43 2.79 6.71 20.16 28.07 Acc 62.75 64.75 57.00 64.75 51.25 51.25 56.25 63.25 P 57.81 58.68 52.19 58.2 51.25 51.27 55.24 60.43 R 90.24 90.73 99.02 88.29 100 98.54 77.07 81.95 F 1 70.48 71.26 68.35 70.16 67.77 67.45 64.36 69.57 Pk=1 57.92 57.43 49.51 65.84 90.59 72.77 80.69 85.15 \u03c4rank 17.46 17.59 7.56 24.68 0.33 -4.8 13.48 21.27 \u03c1rank 18.74 18.98 9.28 26.55 0.79 -5.33 14.10 22.10 Table 4: Model performance for empathic similarity prediction task across correlation, accuracy, and retrieval metrics. r = Pearson\u2019s correlation, \u03c1 = Spearman\u2019s correlation, Acc = accuracy, P = precision, R = recall, Pk=1 = precision at k where k is 1, \u03c4rank = Kendall Tau of ranking and \u03c1rank = Spearman of ranking. Note that all scores are multiplied by 100 for easier comparison, and the maximum for each metric is 100. In bold is the best performing and underlined is the second-best performing condition for the metric. Baseline Models. We compare performance to finetuning with SBERT (multi-qa-mpnet-base-dot- v1) (Reimers and Gurevych, 2019; Brown et al., 2020) and BART model (bart-base) (Lewis et al., 2019). As a few-shot baseline, we evaluate GPT-3 (text-davinci-003) and ChatGPT\u2019s (gpt-3.5-turbo) ability to distinguish empathically similar stories by using a k-shot prompting setup as done in Sap et al. (2022); Brown et al. (2020). For the query story pair, we ask for an empathic similarity score from 1-4. We compare across k = 0 examples and k = 5 examples from the training set. We also evaluate these models\u2019 ability to generate human- like main event, emotion description, and moral summaries for each story. Again, we use a k-shot prompting setup, comparing across k = 0 and k = 10 examples. See Appendix G and Appendix C for prompts used and finetuning details. Empathy Similarity Prediction. We propose a bi- encoder architecture finetuned with mean-squared error (MSE) loss of the cosine-similarity between story pairs, as compared to the empathic similar- ity gold labels. For each of the encoders, we use a shared pretrained transformer-based model and further finetune on the 1,500 annotated story pairs in our training set. We obtain the final embedding using mean pooling of the encoder last hidden state. k = 1 (what proportion of the top-ranked stories by our model are the top-ranked story as rated by human annotators), Kendall\u2019s Tau (Abdi, 2007), and Spearman\u2019s correlation (Schober et al., 2018) for the ranking of the stories (how close the overall rankings are). Shown in Table 4, our results indicate that fine- tuning SBERT and BART with EMPATHICSTO- RIES results in performance gains across all metrics. SBERT has relatively high off-the-shelf perfor- mance, as it is trained with 215M examples specif- ically for semantic similarity tasks. However, we see that finetuning with our dataset, which contains far fewer training examples relative to SBERT\u2019s pretraining corpus, improves performance. (+ 5.35 \u03c1, +2 accuracy). BART, which is not specifically pre-trained for semantic similarity tasks, shows even greater gains across retrieval metrics when finetuned on our dataset. (22.89 \u03c1, +7.75 accuracy). We find that for BART models, fine tuning im- provements (p = 0.02, p = 0.0006 respectively), as measured with McNemar\u2019s test on the accuracy scores and Fisher\u2019s transformation on correlations, are significantly higher than baselines. While GPT-3 and ChatGPT have high perfor- mance on the precision at k retrieval metric, in practice, it is not feasible to prompt the models with every pair of stories in the retrieval corpus. 6 Automatic Evaluation To evaluate the quality of empathic similarity pre- dictions, we first compare the Spearman\u2019s and Pear- son\u2019s correlations between the cosine similarity of the sentence embeddings and the gold empathic similarity labels. Next, we bin scores into binary similar/dissimilar categories (> 2.5 and \u2264 2.5 re- spectively) compute the accuracy, precision, re- call, and F1 scores. Finally, we compute a series of retrieval-based metrics including precision at 7 User Study Prior work\u2019s versions of human evaluations (Zhou and Jurgens, 2020;"}, {"question": " What is the agreement rate of the annotators in this study?", "answer": " The agreement rates are in line with other personal and affect-driven annotation tasks.", "ref_chunk": "distance away). We are aiming for a more descriptive annotation paradigm and thus do not expect annotators to perfectly agree (Rottger et al., 2022). Furthermore, our agreement rates are in line with other inherently personal and affect-driven annotation tasks (Sap et al., 2017; Rashkin et al., 2018). Given the difficulty of our task (reading longer stories and projecting the men- tal state of 2 characters), our agreement is in line with prior work, which achieve around 0.51 - 0.91 PPA and 0.29 - 0.34 KA. 5 Modeling Empathic Similarity To enable the retrieval and analysis of empathically similar stories, we design a task detailed below. In Appendix B, we also propose an auxiliary reason- ing task to automatically extract event, emotion, and moral features from stories, which could be used in future work to quickly generate story anno- tations. 5.1 Task Formulation task is given a query Our ultimate retrieval story Q and selects a story Si from a set of N stories {S1, S2, ..., SN } such that i = argmaxi sim(f\u03b8(Si), f\u03b8(Q)). Here, sim(\u00b7, \u00b7) is a similarity metric (e.g. cosine similarity) between two story representations f\u03b8(Si) and f\u03b8(Q) that are learned from human ratings of empathic similarity. Empathic Similarity Prediction. The overall task is, given a story pair (S1, S2), return a similar- ity score sim(f\u03b8(Si), f\u03b8(Q)) such that sim(\u00b7, \u00b7) is large for empathically similar stories and small for empathically dissimilar stories. 5.2 Models We propose finetuning LLMs to learn embeddings that capture empathic similarity using cosine dis- tance, for efficient retrieval at test time. In contrast, a popular approach is to use few-shot prompting of very large language models (e.g., GPT-3 and Chat- GPT), which have shown impressive performance across a variety of tasks (Brown et al., 2020). How- ever, in a real deployment setting, retrieval through prompting every possible pair of stories is expen- sive and inefficient. Model SBERT + finetuning BART + finetuning GPT-3 + 5 examples ChatGPT + 5 examples r 30.93 35.93 10.24 34.20 3.24 4.94 19.56 27.75 \u03c1 29.86 35.21 11.54 34.43 2.79 6.71 20.16 28.07 Acc 62.75 64.75 57.00 64.75 51.25 51.25 56.25 63.25 P 57.81 58.68 52.19 58.2 51.25 51.27 55.24 60.43 R 90.24 90.73 99.02 88.29 100 98.54 77.07 81.95 F 1 70.48 71.26 68.35 70.16 67.77 67.45 64.36 69.57 Pk=1 57.92 57.43 49.51 65.84 90.59 72.77 80.69 85.15 \u03c4rank 17.46 17.59 7.56 24.68 0.33 -4.8 13.48 21.27 \u03c1rank 18.74 18.98 9.28 26.55 0.79 -5.33 14.10 22.10 Table 4: Model performance for empathic similarity prediction task across correlation, accuracy, and retrieval metrics. r = Pearson\u2019s correlation, \u03c1 = Spearman\u2019s correlation, Acc = accuracy, P = precision, R = recall, Pk=1 = precision at k where k is 1, \u03c4rank = Kendall Tau of ranking and \u03c1rank = Spearman of ranking. Note that all scores are multiplied by 100 for easier comparison, and the maximum for each metric is 100. In bold is the best performing and underlined is the second-best performing condition for the metric. Baseline Models. We compare performance to finetuning with SBERT (multi-qa-mpnet-base-dot- v1) (Reimers and Gurevych, 2019; Brown et al., 2020) and BART model (bart-base) (Lewis et al., 2019). As a few-shot baseline, we evaluate GPT-3 (text-davinci-003) and ChatGPT\u2019s (gpt-3.5-turbo) ability to distinguish empathically similar stories by using a k-shot prompting setup as done in Sap et al. (2022); Brown et al. (2020). For the query story pair, we ask for an empathic similarity score from 1-4. We compare across k = 0 examples and k = 5 examples from the training set. We also evaluate these models\u2019 ability to generate human- like main event, emotion description, and moral summaries for each story. Again, we use a k-shot prompting setup, comparing across k = 0 and k = 10 examples. See Appendix G and Appendix C for prompts used and finetuning details. Empathy Similarity Prediction. We propose a bi- encoder architecture finetuned with mean-squared error (MSE) loss of the cosine-similarity between story pairs, as compared to the empathic similar- ity gold labels. For each of the encoders, we use a shared pretrained transformer-based model and further finetune on the 1,500 annotated story pairs in our training set. We obtain the final embedding using mean pooling of the encoder last hidden state. k = 1 (what proportion of the top-ranked stories by our model are the top-ranked story as rated by human annotators), Kendall\u2019s Tau (Abdi, 2007), and Spearman\u2019s correlation (Schober et al., 2018) for the ranking of the stories (how close the overall rankings are). Shown in Table 4, our results indicate that fine- tuning SBERT and BART with EMPATHICSTO- RIES results in performance gains across all metrics. SBERT has relatively high off-the-shelf perfor- mance, as it is trained with 215M examples specif- ically for semantic similarity tasks. However, we see that finetuning with our dataset, which contains far fewer training examples relative to SBERT\u2019s pretraining corpus, improves performance. (+ 5.35 \u03c1, +2 accuracy). BART, which is not specifically pre-trained for semantic similarity tasks, shows even greater gains across retrieval metrics when finetuned on our dataset. (22.89 \u03c1, +7.75 accuracy). We find that for BART models, fine tuning im- provements (p = 0.02, p = 0.0006 respectively), as measured with McNemar\u2019s test on the accuracy scores and Fisher\u2019s transformation on correlations, are significantly higher than baselines. While GPT-3 and ChatGPT have high perfor- mance on the precision at k retrieval metric, in practice, it is not feasible to prompt the models with every pair of stories in the retrieval corpus. 6 Automatic Evaluation To evaluate the quality of empathic similarity pre- dictions, we first compare the Spearman\u2019s and Pear- son\u2019s correlations between the cosine similarity of the sentence embeddings and the gold empathic similarity labels. Next, we bin scores into binary similar/dissimilar categories (> 2.5 and \u2264 2.5 re- spectively) compute the accuracy, precision, re- call, and F1 scores. Finally, we compute a series of retrieval-based metrics including precision at 7 User Study Prior work\u2019s versions of human evaluations (Zhou and Jurgens, 2020;"}, {"question": " What task are the annotators required to perform in this study?", "answer": " Reading longer stories and projecting the mental state of 2 characters.", "ref_chunk": "distance away). We are aiming for a more descriptive annotation paradigm and thus do not expect annotators to perfectly agree (Rottger et al., 2022). Furthermore, our agreement rates are in line with other inherently personal and affect-driven annotation tasks (Sap et al., 2017; Rashkin et al., 2018). Given the difficulty of our task (reading longer stories and projecting the men- tal state of 2 characters), our agreement is in line with prior work, which achieve around 0.51 - 0.91 PPA and 0.29 - 0.34 KA. 5 Modeling Empathic Similarity To enable the retrieval and analysis of empathically similar stories, we design a task detailed below. In Appendix B, we also propose an auxiliary reason- ing task to automatically extract event, emotion, and moral features from stories, which could be used in future work to quickly generate story anno- tations. 5.1 Task Formulation task is given a query Our ultimate retrieval story Q and selects a story Si from a set of N stories {S1, S2, ..., SN } such that i = argmaxi sim(f\u03b8(Si), f\u03b8(Q)). Here, sim(\u00b7, \u00b7) is a similarity metric (e.g. cosine similarity) between two story representations f\u03b8(Si) and f\u03b8(Q) that are learned from human ratings of empathic similarity. Empathic Similarity Prediction. The overall task is, given a story pair (S1, S2), return a similar- ity score sim(f\u03b8(Si), f\u03b8(Q)) such that sim(\u00b7, \u00b7) is large for empathically similar stories and small for empathically dissimilar stories. 5.2 Models We propose finetuning LLMs to learn embeddings that capture empathic similarity using cosine dis- tance, for efficient retrieval at test time. In contrast, a popular approach is to use few-shot prompting of very large language models (e.g., GPT-3 and Chat- GPT), which have shown impressive performance across a variety of tasks (Brown et al., 2020). How- ever, in a real deployment setting, retrieval through prompting every possible pair of stories is expen- sive and inefficient. Model SBERT + finetuning BART + finetuning GPT-3 + 5 examples ChatGPT + 5 examples r 30.93 35.93 10.24 34.20 3.24 4.94 19.56 27.75 \u03c1 29.86 35.21 11.54 34.43 2.79 6.71 20.16 28.07 Acc 62.75 64.75 57.00 64.75 51.25 51.25 56.25 63.25 P 57.81 58.68 52.19 58.2 51.25 51.27 55.24 60.43 R 90.24 90.73 99.02 88.29 100 98.54 77.07 81.95 F 1 70.48 71.26 68.35 70.16 67.77 67.45 64.36 69.57 Pk=1 57.92 57.43 49.51 65.84 90.59 72.77 80.69 85.15 \u03c4rank 17.46 17.59 7.56 24.68 0.33 -4.8 13.48 21.27 \u03c1rank 18.74 18.98 9.28 26.55 0.79 -5.33 14.10 22.10 Table 4: Model performance for empathic similarity prediction task across correlation, accuracy, and retrieval metrics. r = Pearson\u2019s correlation, \u03c1 = Spearman\u2019s correlation, Acc = accuracy, P = precision, R = recall, Pk=1 = precision at k where k is 1, \u03c4rank = Kendall Tau of ranking and \u03c1rank = Spearman of ranking. Note that all scores are multiplied by 100 for easier comparison, and the maximum for each metric is 100. In bold is the best performing and underlined is the second-best performing condition for the metric. Baseline Models. We compare performance to finetuning with SBERT (multi-qa-mpnet-base-dot- v1) (Reimers and Gurevych, 2019; Brown et al., 2020) and BART model (bart-base) (Lewis et al., 2019). As a few-shot baseline, we evaluate GPT-3 (text-davinci-003) and ChatGPT\u2019s (gpt-3.5-turbo) ability to distinguish empathically similar stories by using a k-shot prompting setup as done in Sap et al. (2022); Brown et al. (2020). For the query story pair, we ask for an empathic similarity score from 1-4. We compare across k = 0 examples and k = 5 examples from the training set. We also evaluate these models\u2019 ability to generate human- like main event, emotion description, and moral summaries for each story. Again, we use a k-shot prompting setup, comparing across k = 0 and k = 10 examples. See Appendix G and Appendix C for prompts used and finetuning details. Empathy Similarity Prediction. We propose a bi- encoder architecture finetuned with mean-squared error (MSE) loss of the cosine-similarity between story pairs, as compared to the empathic similar- ity gold labels. For each of the encoders, we use a shared pretrained transformer-based model and further finetune on the 1,500 annotated story pairs in our training set. We obtain the final embedding using mean pooling of the encoder last hidden state. k = 1 (what proportion of the top-ranked stories by our model are the top-ranked story as rated by human annotators), Kendall\u2019s Tau (Abdi, 2007), and Spearman\u2019s correlation (Schober et al., 2018) for the ranking of the stories (how close the overall rankings are). Shown in Table 4, our results indicate that fine- tuning SBERT and BART with EMPATHICSTO- RIES results in performance gains across all metrics. SBERT has relatively high off-the-shelf perfor- mance, as it is trained with 215M examples specif- ically for semantic similarity tasks. However, we see that finetuning with our dataset, which contains far fewer training examples relative to SBERT\u2019s pretraining corpus, improves performance. (+ 5.35 \u03c1, +2 accuracy). BART, which is not specifically pre-trained for semantic similarity tasks, shows even greater gains across retrieval metrics when finetuned on our dataset. (22.89 \u03c1, +7.75 accuracy). We find that for BART models, fine tuning im- provements (p = 0.02, p = 0.0006 respectively), as measured with McNemar\u2019s test on the accuracy scores and Fisher\u2019s transformation on correlations, are significantly higher than baselines. While GPT-3 and ChatGPT have high perfor- mance on the precision at k retrieval metric, in practice, it is not feasible to prompt the models with every pair of stories in the retrieval corpus. 6 Automatic Evaluation To evaluate the quality of empathic similarity pre- dictions, we first compare the Spearman\u2019s and Pear- son\u2019s correlations between the cosine similarity of the sentence embeddings and the gold empathic similarity labels. Next, we bin scores into binary similar/dissimilar categories (> 2.5 and \u2264 2.5 re- spectively) compute the accuracy, precision, re- call, and F1 scores. Finally, we compute a series of retrieval-based metrics including precision at 7 User Study Prior work\u2019s versions of human evaluations (Zhou and Jurgens, 2020;"}, {"question": " What is the similarity metric used for story representations in this study?", "answer": " Cosine similarity.", "ref_chunk": "distance away). We are aiming for a more descriptive annotation paradigm and thus do not expect annotators to perfectly agree (Rottger et al., 2022). Furthermore, our agreement rates are in line with other inherently personal and affect-driven annotation tasks (Sap et al., 2017; Rashkin et al., 2018). Given the difficulty of our task (reading longer stories and projecting the men- tal state of 2 characters), our agreement is in line with prior work, which achieve around 0.51 - 0.91 PPA and 0.29 - 0.34 KA. 5 Modeling Empathic Similarity To enable the retrieval and analysis of empathically similar stories, we design a task detailed below. In Appendix B, we also propose an auxiliary reason- ing task to automatically extract event, emotion, and moral features from stories, which could be used in future work to quickly generate story anno- tations. 5.1 Task Formulation task is given a query Our ultimate retrieval story Q and selects a story Si from a set of N stories {S1, S2, ..., SN } such that i = argmaxi sim(f\u03b8(Si), f\u03b8(Q)). Here, sim(\u00b7, \u00b7) is a similarity metric (e.g. cosine similarity) between two story representations f\u03b8(Si) and f\u03b8(Q) that are learned from human ratings of empathic similarity. Empathic Similarity Prediction. The overall task is, given a story pair (S1, S2), return a similar- ity score sim(f\u03b8(Si), f\u03b8(Q)) such that sim(\u00b7, \u00b7) is large for empathically similar stories and small for empathically dissimilar stories. 5.2 Models We propose finetuning LLMs to learn embeddings that capture empathic similarity using cosine dis- tance, for efficient retrieval at test time. In contrast, a popular approach is to use few-shot prompting of very large language models (e.g., GPT-3 and Chat- GPT), which have shown impressive performance across a variety of tasks (Brown et al., 2020). How- ever, in a real deployment setting, retrieval through prompting every possible pair of stories is expen- sive and inefficient. Model SBERT + finetuning BART + finetuning GPT-3 + 5 examples ChatGPT + 5 examples r 30.93 35.93 10.24 34.20 3.24 4.94 19.56 27.75 \u03c1 29.86 35.21 11.54 34.43 2.79 6.71 20.16 28.07 Acc 62.75 64.75 57.00 64.75 51.25 51.25 56.25 63.25 P 57.81 58.68 52.19 58.2 51.25 51.27 55.24 60.43 R 90.24 90.73 99.02 88.29 100 98.54 77.07 81.95 F 1 70.48 71.26 68.35 70.16 67.77 67.45 64.36 69.57 Pk=1 57.92 57.43 49.51 65.84 90.59 72.77 80.69 85.15 \u03c4rank 17.46 17.59 7.56 24.68 0.33 -4.8 13.48 21.27 \u03c1rank 18.74 18.98 9.28 26.55 0.79 -5.33 14.10 22.10 Table 4: Model performance for empathic similarity prediction task across correlation, accuracy, and retrieval metrics. r = Pearson\u2019s correlation, \u03c1 = Spearman\u2019s correlation, Acc = accuracy, P = precision, R = recall, Pk=1 = precision at k where k is 1, \u03c4rank = Kendall Tau of ranking and \u03c1rank = Spearman of ranking. Note that all scores are multiplied by 100 for easier comparison, and the maximum for each metric is 100. In bold is the best performing and underlined is the second-best performing condition for the metric. Baseline Models. We compare performance to finetuning with SBERT (multi-qa-mpnet-base-dot- v1) (Reimers and Gurevych, 2019; Brown et al., 2020) and BART model (bart-base) (Lewis et al., 2019). As a few-shot baseline, we evaluate GPT-3 (text-davinci-003) and ChatGPT\u2019s (gpt-3.5-turbo) ability to distinguish empathically similar stories by using a k-shot prompting setup as done in Sap et al. (2022); Brown et al. (2020). For the query story pair, we ask for an empathic similarity score from 1-4. We compare across k = 0 examples and k = 5 examples from the training set. We also evaluate these models\u2019 ability to generate human- like main event, emotion description, and moral summaries for each story. Again, we use a k-shot prompting setup, comparing across k = 0 and k = 10 examples. See Appendix G and Appendix C for prompts used and finetuning details. Empathy Similarity Prediction. We propose a bi- encoder architecture finetuned with mean-squared error (MSE) loss of the cosine-similarity between story pairs, as compared to the empathic similar- ity gold labels. For each of the encoders, we use a shared pretrained transformer-based model and further finetune on the 1,500 annotated story pairs in our training set. We obtain the final embedding using mean pooling of the encoder last hidden state. k = 1 (what proportion of the top-ranked stories by our model are the top-ranked story as rated by human annotators), Kendall\u2019s Tau (Abdi, 2007), and Spearman\u2019s correlation (Schober et al., 2018) for the ranking of the stories (how close the overall rankings are). Shown in Table 4, our results indicate that fine- tuning SBERT and BART with EMPATHICSTO- RIES results in performance gains across all metrics. SBERT has relatively high off-the-shelf perfor- mance, as it is trained with 215M examples specif- ically for semantic similarity tasks. However, we see that finetuning with our dataset, which contains far fewer training examples relative to SBERT\u2019s pretraining corpus, improves performance. (+ 5.35 \u03c1, +2 accuracy). BART, which is not specifically pre-trained for semantic similarity tasks, shows even greater gains across retrieval metrics when finetuned on our dataset. (22.89 \u03c1, +7.75 accuracy). We find that for BART models, fine tuning im- provements (p = 0.02, p = 0.0006 respectively), as measured with McNemar\u2019s test on the accuracy scores and Fisher\u2019s transformation on correlations, are significantly higher than baselines. While GPT-3 and ChatGPT have high perfor- mance on the precision at k retrieval metric, in practice, it is not feasible to prompt the models with every pair of stories in the retrieval corpus. 6 Automatic Evaluation To evaluate the quality of empathic similarity pre- dictions, we first compare the Spearman\u2019s and Pear- son\u2019s correlations between the cosine similarity of the sentence embeddings and the gold empathic similarity labels. Next, we bin scores into binary similar/dissimilar categories (> 2.5 and \u2264 2.5 re- spectively) compute the accuracy, precision, re- call, and F1 scores. Finally, we compute a series of retrieval-based metrics including precision at 7 User Study Prior work\u2019s versions of human evaluations (Zhou and Jurgens, 2020;"}, {"question": " What models are proposed for learning embeddings that capture empathic similarity in the study?", "answer": " SBERT + finetuning, BART + finetuning, GPT-3 + 5 examples, and ChatGPT + 5 examples.", "ref_chunk": "distance away). We are aiming for a more descriptive annotation paradigm and thus do not expect annotators to perfectly agree (Rottger et al., 2022). Furthermore, our agreement rates are in line with other inherently personal and affect-driven annotation tasks (Sap et al., 2017; Rashkin et al., 2018). Given the difficulty of our task (reading longer stories and projecting the men- tal state of 2 characters), our agreement is in line with prior work, which achieve around 0.51 - 0.91 PPA and 0.29 - 0.34 KA. 5 Modeling Empathic Similarity To enable the retrieval and analysis of empathically similar stories, we design a task detailed below. In Appendix B, we also propose an auxiliary reason- ing task to automatically extract event, emotion, and moral features from stories, which could be used in future work to quickly generate story anno- tations. 5.1 Task Formulation task is given a query Our ultimate retrieval story Q and selects a story Si from a set of N stories {S1, S2, ..., SN } such that i = argmaxi sim(f\u03b8(Si), f\u03b8(Q)). Here, sim(\u00b7, \u00b7) is a similarity metric (e.g. cosine similarity) between two story representations f\u03b8(Si) and f\u03b8(Q) that are learned from human ratings of empathic similarity. Empathic Similarity Prediction. The overall task is, given a story pair (S1, S2), return a similar- ity score sim(f\u03b8(Si), f\u03b8(Q)) such that sim(\u00b7, \u00b7) is large for empathically similar stories and small for empathically dissimilar stories. 5.2 Models We propose finetuning LLMs to learn embeddings that capture empathic similarity using cosine dis- tance, for efficient retrieval at test time. In contrast, a popular approach is to use few-shot prompting of very large language models (e.g., GPT-3 and Chat- GPT), which have shown impressive performance across a variety of tasks (Brown et al., 2020). How- ever, in a real deployment setting, retrieval through prompting every possible pair of stories is expen- sive and inefficient. Model SBERT + finetuning BART + finetuning GPT-3 + 5 examples ChatGPT + 5 examples r 30.93 35.93 10.24 34.20 3.24 4.94 19.56 27.75 \u03c1 29.86 35.21 11.54 34.43 2.79 6.71 20.16 28.07 Acc 62.75 64.75 57.00 64.75 51.25 51.25 56.25 63.25 P 57.81 58.68 52.19 58.2 51.25 51.27 55.24 60.43 R 90.24 90.73 99.02 88.29 100 98.54 77.07 81.95 F 1 70.48 71.26 68.35 70.16 67.77 67.45 64.36 69.57 Pk=1 57.92 57.43 49.51 65.84 90.59 72.77 80.69 85.15 \u03c4rank 17.46 17.59 7.56 24.68 0.33 -4.8 13.48 21.27 \u03c1rank 18.74 18.98 9.28 26.55 0.79 -5.33 14.10 22.10 Table 4: Model performance for empathic similarity prediction task across correlation, accuracy, and retrieval metrics. r = Pearson\u2019s correlation, \u03c1 = Spearman\u2019s correlation, Acc = accuracy, P = precision, R = recall, Pk=1 = precision at k where k is 1, \u03c4rank = Kendall Tau of ranking and \u03c1rank = Spearman of ranking. Note that all scores are multiplied by 100 for easier comparison, and the maximum for each metric is 100. In bold is the best performing and underlined is the second-best performing condition for the metric. Baseline Models. We compare performance to finetuning with SBERT (multi-qa-mpnet-base-dot- v1) (Reimers and Gurevych, 2019; Brown et al., 2020) and BART model (bart-base) (Lewis et al., 2019). As a few-shot baseline, we evaluate GPT-3 (text-davinci-003) and ChatGPT\u2019s (gpt-3.5-turbo) ability to distinguish empathically similar stories by using a k-shot prompting setup as done in Sap et al. (2022); Brown et al. (2020). For the query story pair, we ask for an empathic similarity score from 1-4. We compare across k = 0 examples and k = 5 examples from the training set. We also evaluate these models\u2019 ability to generate human- like main event, emotion description, and moral summaries for each story. Again, we use a k-shot prompting setup, comparing across k = 0 and k = 10 examples. See Appendix G and Appendix C for prompts used and finetuning details. Empathy Similarity Prediction. We propose a bi- encoder architecture finetuned with mean-squared error (MSE) loss of the cosine-similarity between story pairs, as compared to the empathic similar- ity gold labels. For each of the encoders, we use a shared pretrained transformer-based model and further finetune on the 1,500 annotated story pairs in our training set. We obtain the final embedding using mean pooling of the encoder last hidden state. k = 1 (what proportion of the top-ranked stories by our model are the top-ranked story as rated by human annotators), Kendall\u2019s Tau (Abdi, 2007), and Spearman\u2019s correlation (Schober et al., 2018) for the ranking of the stories (how close the overall rankings are). Shown in Table 4, our results indicate that fine- tuning SBERT and BART with EMPATHICSTO- RIES results in performance gains across all metrics. SBERT has relatively high off-the-shelf perfor- mance, as it is trained with 215M examples specif- ically for semantic similarity tasks. However, we see that finetuning with our dataset, which contains far fewer training examples relative to SBERT\u2019s pretraining corpus, improves performance. (+ 5.35 \u03c1, +2 accuracy). BART, which is not specifically pre-trained for semantic similarity tasks, shows even greater gains across retrieval metrics when finetuned on our dataset. (22.89 \u03c1, +7.75 accuracy). We find that for BART models, fine tuning im- provements (p = 0.02, p = 0.0006 respectively), as measured with McNemar\u2019s test on the accuracy scores and Fisher\u2019s transformation on correlations, are significantly higher than baselines. While GPT-3 and ChatGPT have high perfor- mance on the precision at k retrieval metric, in practice, it is not feasible to prompt the models with every pair of stories in the retrieval corpus. 6 Automatic Evaluation To evaluate the quality of empathic similarity pre- dictions, we first compare the Spearman\u2019s and Pear- son\u2019s correlations between the cosine similarity of the sentence embeddings and the gold empathic similarity labels. Next, we bin scores into binary similar/dissimilar categories (> 2.5 and \u2264 2.5 re- spectively) compute the accuracy, precision, re- call, and F1 scores. Finally, we compute a series of retrieval-based metrics including precision at 7 User Study Prior work\u2019s versions of human evaluations (Zhou and Jurgens, 2020;"}, {"question": " Which baseline models are compared in the study?", "answer": " SBERT (multi-qa-mpnet-base-dot-v1), BART (bart-base), GPT-3 (text-davinci-003), and ChatGPT (gpt-3.5-turbo).", "ref_chunk": "distance away). We are aiming for a more descriptive annotation paradigm and thus do not expect annotators to perfectly agree (Rottger et al., 2022). Furthermore, our agreement rates are in line with other inherently personal and affect-driven annotation tasks (Sap et al., 2017; Rashkin et al., 2018). Given the difficulty of our task (reading longer stories and projecting the men- tal state of 2 characters), our agreement is in line with prior work, which achieve around 0.51 - 0.91 PPA and 0.29 - 0.34 KA. 5 Modeling Empathic Similarity To enable the retrieval and analysis of empathically similar stories, we design a task detailed below. In Appendix B, we also propose an auxiliary reason- ing task to automatically extract event, emotion, and moral features from stories, which could be used in future work to quickly generate story anno- tations. 5.1 Task Formulation task is given a query Our ultimate retrieval story Q and selects a story Si from a set of N stories {S1, S2, ..., SN } such that i = argmaxi sim(f\u03b8(Si), f\u03b8(Q)). Here, sim(\u00b7, \u00b7) is a similarity metric (e.g. cosine similarity) between two story representations f\u03b8(Si) and f\u03b8(Q) that are learned from human ratings of empathic similarity. Empathic Similarity Prediction. The overall task is, given a story pair (S1, S2), return a similar- ity score sim(f\u03b8(Si), f\u03b8(Q)) such that sim(\u00b7, \u00b7) is large for empathically similar stories and small for empathically dissimilar stories. 5.2 Models We propose finetuning LLMs to learn embeddings that capture empathic similarity using cosine dis- tance, for efficient retrieval at test time. In contrast, a popular approach is to use few-shot prompting of very large language models (e.g., GPT-3 and Chat- GPT), which have shown impressive performance across a variety of tasks (Brown et al., 2020). How- ever, in a real deployment setting, retrieval through prompting every possible pair of stories is expen- sive and inefficient. Model SBERT + finetuning BART + finetuning GPT-3 + 5 examples ChatGPT + 5 examples r 30.93 35.93 10.24 34.20 3.24 4.94 19.56 27.75 \u03c1 29.86 35.21 11.54 34.43 2.79 6.71 20.16 28.07 Acc 62.75 64.75 57.00 64.75 51.25 51.25 56.25 63.25 P 57.81 58.68 52.19 58.2 51.25 51.27 55.24 60.43 R 90.24 90.73 99.02 88.29 100 98.54 77.07 81.95 F 1 70.48 71.26 68.35 70.16 67.77 67.45 64.36 69.57 Pk=1 57.92 57.43 49.51 65.84 90.59 72.77 80.69 85.15 \u03c4rank 17.46 17.59 7.56 24.68 0.33 -4.8 13.48 21.27 \u03c1rank 18.74 18.98 9.28 26.55 0.79 -5.33 14.10 22.10 Table 4: Model performance for empathic similarity prediction task across correlation, accuracy, and retrieval metrics. r = Pearson\u2019s correlation, \u03c1 = Spearman\u2019s correlation, Acc = accuracy, P = precision, R = recall, Pk=1 = precision at k where k is 1, \u03c4rank = Kendall Tau of ranking and \u03c1rank = Spearman of ranking. Note that all scores are multiplied by 100 for easier comparison, and the maximum for each metric is 100. In bold is the best performing and underlined is the second-best performing condition for the metric. Baseline Models. We compare performance to finetuning with SBERT (multi-qa-mpnet-base-dot- v1) (Reimers and Gurevych, 2019; Brown et al., 2020) and BART model (bart-base) (Lewis et al., 2019). As a few-shot baseline, we evaluate GPT-3 (text-davinci-003) and ChatGPT\u2019s (gpt-3.5-turbo) ability to distinguish empathically similar stories by using a k-shot prompting setup as done in Sap et al. (2022); Brown et al. (2020). For the query story pair, we ask for an empathic similarity score from 1-4. We compare across k = 0 examples and k = 5 examples from the training set. We also evaluate these models\u2019 ability to generate human- like main event, emotion description, and moral summaries for each story. Again, we use a k-shot prompting setup, comparing across k = 0 and k = 10 examples. See Appendix G and Appendix C for prompts used and finetuning details. Empathy Similarity Prediction. We propose a bi- encoder architecture finetuned with mean-squared error (MSE) loss of the cosine-similarity between story pairs, as compared to the empathic similar- ity gold labels. For each of the encoders, we use a shared pretrained transformer-based model and further finetune on the 1,500 annotated story pairs in our training set. We obtain the final embedding using mean pooling of the encoder last hidden state. k = 1 (what proportion of the top-ranked stories by our model are the top-ranked story as rated by human annotators), Kendall\u2019s Tau (Abdi, 2007), and Spearman\u2019s correlation (Schober et al., 2018) for the ranking of the stories (how close the overall rankings are). Shown in Table 4, our results indicate that fine- tuning SBERT and BART with EMPATHICSTO- RIES results in performance gains across all metrics. SBERT has relatively high off-the-shelf perfor- mance, as it is trained with 215M examples specif- ically for semantic similarity tasks. However, we see that finetuning with our dataset, which contains far fewer training examples relative to SBERT\u2019s pretraining corpus, improves performance. (+ 5.35 \u03c1, +2 accuracy). BART, which is not specifically pre-trained for semantic similarity tasks, shows even greater gains across retrieval metrics when finetuned on our dataset. (22.89 \u03c1, +7.75 accuracy). We find that for BART models, fine tuning im- provements (p = 0.02, p = 0.0006 respectively), as measured with McNemar\u2019s test on the accuracy scores and Fisher\u2019s transformation on correlations, are significantly higher than baselines. While GPT-3 and ChatGPT have high perfor- mance on the precision at k retrieval metric, in practice, it is not feasible to prompt the models with every pair of stories in the retrieval corpus. 6 Automatic Evaluation To evaluate the quality of empathic similarity pre- dictions, we first compare the Spearman\u2019s and Pear- son\u2019s correlations between the cosine similarity of the sentence embeddings and the gold empathic similarity labels. Next, we bin scores into binary similar/dissimilar categories (> 2.5 and \u2264 2.5 re- spectively) compute the accuracy, precision, re- call, and F1 scores. Finally, we compute a series of retrieval-based metrics including precision at 7 User Study Prior work\u2019s versions of human evaluations (Zhou and Jurgens, 2020;"}, {"question": " What is the metric for measuring performance in the empathic similarity prediction task?", "answer": " It includes correlation, accuracy, precision, recall, precision at k (where k is 1), Kendall Tau of ranking, and Spearman of ranking.", "ref_chunk": "distance away). We are aiming for a more descriptive annotation paradigm and thus do not expect annotators to perfectly agree (Rottger et al., 2022). Furthermore, our agreement rates are in line with other inherently personal and affect-driven annotation tasks (Sap et al., 2017; Rashkin et al., 2018). Given the difficulty of our task (reading longer stories and projecting the men- tal state of 2 characters), our agreement is in line with prior work, which achieve around 0.51 - 0.91 PPA and 0.29 - 0.34 KA. 5 Modeling Empathic Similarity To enable the retrieval and analysis of empathically similar stories, we design a task detailed below. In Appendix B, we also propose an auxiliary reason- ing task to automatically extract event, emotion, and moral features from stories, which could be used in future work to quickly generate story anno- tations. 5.1 Task Formulation task is given a query Our ultimate retrieval story Q and selects a story Si from a set of N stories {S1, S2, ..., SN } such that i = argmaxi sim(f\u03b8(Si), f\u03b8(Q)). Here, sim(\u00b7, \u00b7) is a similarity metric (e.g. cosine similarity) between two story representations f\u03b8(Si) and f\u03b8(Q) that are learned from human ratings of empathic similarity. Empathic Similarity Prediction. The overall task is, given a story pair (S1, S2), return a similar- ity score sim(f\u03b8(Si), f\u03b8(Q)) such that sim(\u00b7, \u00b7) is large for empathically similar stories and small for empathically dissimilar stories. 5.2 Models We propose finetuning LLMs to learn embeddings that capture empathic similarity using cosine dis- tance, for efficient retrieval at test time. In contrast, a popular approach is to use few-shot prompting of very large language models (e.g., GPT-3 and Chat- GPT), which have shown impressive performance across a variety of tasks (Brown et al., 2020). How- ever, in a real deployment setting, retrieval through prompting every possible pair of stories is expen- sive and inefficient. Model SBERT + finetuning BART + finetuning GPT-3 + 5 examples ChatGPT + 5 examples r 30.93 35.93 10.24 34.20 3.24 4.94 19.56 27.75 \u03c1 29.86 35.21 11.54 34.43 2.79 6.71 20.16 28.07 Acc 62.75 64.75 57.00 64.75 51.25 51.25 56.25 63.25 P 57.81 58.68 52.19 58.2 51.25 51.27 55.24 60.43 R 90.24 90.73 99.02 88.29 100 98.54 77.07 81.95 F 1 70.48 71.26 68.35 70.16 67.77 67.45 64.36 69.57 Pk=1 57.92 57.43 49.51 65.84 90.59 72.77 80.69 85.15 \u03c4rank 17.46 17.59 7.56 24.68 0.33 -4.8 13.48 21.27 \u03c1rank 18.74 18.98 9.28 26.55 0.79 -5.33 14.10 22.10 Table 4: Model performance for empathic similarity prediction task across correlation, accuracy, and retrieval metrics. r = Pearson\u2019s correlation, \u03c1 = Spearman\u2019s correlation, Acc = accuracy, P = precision, R = recall, Pk=1 = precision at k where k is 1, \u03c4rank = Kendall Tau of ranking and \u03c1rank = Spearman of ranking. Note that all scores are multiplied by 100 for easier comparison, and the maximum for each metric is 100. In bold is the best performing and underlined is the second-best performing condition for the metric. Baseline Models. We compare performance to finetuning with SBERT (multi-qa-mpnet-base-dot- v1) (Reimers and Gurevych, 2019; Brown et al., 2020) and BART model (bart-base) (Lewis et al., 2019). As a few-shot baseline, we evaluate GPT-3 (text-davinci-003) and ChatGPT\u2019s (gpt-3.5-turbo) ability to distinguish empathically similar stories by using a k-shot prompting setup as done in Sap et al. (2022); Brown et al. (2020). For the query story pair, we ask for an empathic similarity score from 1-4. We compare across k = 0 examples and k = 5 examples from the training set. We also evaluate these models\u2019 ability to generate human- like main event, emotion description, and moral summaries for each story. Again, we use a k-shot prompting setup, comparing across k = 0 and k = 10 examples. See Appendix G and Appendix C for prompts used and finetuning details. Empathy Similarity Prediction. We propose a bi- encoder architecture finetuned with mean-squared error (MSE) loss of the cosine-similarity between story pairs, as compared to the empathic similar- ity gold labels. For each of the encoders, we use a shared pretrained transformer-based model and further finetune on the 1,500 annotated story pairs in our training set. We obtain the final embedding using mean pooling of the encoder last hidden state. k = 1 (what proportion of the top-ranked stories by our model are the top-ranked story as rated by human annotators), Kendall\u2019s Tau (Abdi, 2007), and Spearman\u2019s correlation (Schober et al., 2018) for the ranking of the stories (how close the overall rankings are). Shown in Table 4, our results indicate that fine- tuning SBERT and BART with EMPATHICSTO- RIES results in performance gains across all metrics. SBERT has relatively high off-the-shelf perfor- mance, as it is trained with 215M examples specif- ically for semantic similarity tasks. However, we see that finetuning with our dataset, which contains far fewer training examples relative to SBERT\u2019s pretraining corpus, improves performance. (+ 5.35 \u03c1, +2 accuracy). BART, which is not specifically pre-trained for semantic similarity tasks, shows even greater gains across retrieval metrics when finetuned on our dataset. (22.89 \u03c1, +7.75 accuracy). We find that for BART models, fine tuning im- provements (p = 0.02, p = 0.0006 respectively), as measured with McNemar\u2019s test on the accuracy scores and Fisher\u2019s transformation on correlations, are significantly higher than baselines. While GPT-3 and ChatGPT have high perfor- mance on the precision at k retrieval metric, in practice, it is not feasible to prompt the models with every pair of stories in the retrieval corpus. 6 Automatic Evaluation To evaluate the quality of empathic similarity pre- dictions, we first compare the Spearman\u2019s and Pear- son\u2019s correlations between the cosine similarity of the sentence embeddings and the gold empathic similarity labels. Next, we bin scores into binary similar/dissimilar categories (> 2.5 and \u2264 2.5 re- spectively) compute the accuracy, precision, re- call, and F1 scores. Finally, we compute a series of retrieval-based metrics including precision at 7 User Study Prior work\u2019s versions of human evaluations (Zhou and Jurgens, 2020;"}, {"question": " How do the researchers evaluate the quality of empathic similarity predictions?", "answer": " By comparing correlations between sentence embeddings and gold empathic similarity labels, and by using accuracy, precision, recall, F1 scores, and retrieval-based metrics.", "ref_chunk": "distance away). We are aiming for a more descriptive annotation paradigm and thus do not expect annotators to perfectly agree (Rottger et al., 2022). Furthermore, our agreement rates are in line with other inherently personal and affect-driven annotation tasks (Sap et al., 2017; Rashkin et al., 2018). Given the difficulty of our task (reading longer stories and projecting the men- tal state of 2 characters), our agreement is in line with prior work, which achieve around 0.51 - 0.91 PPA and 0.29 - 0.34 KA. 5 Modeling Empathic Similarity To enable the retrieval and analysis of empathically similar stories, we design a task detailed below. In Appendix B, we also propose an auxiliary reason- ing task to automatically extract event, emotion, and moral features from stories, which could be used in future work to quickly generate story anno- tations. 5.1 Task Formulation task is given a query Our ultimate retrieval story Q and selects a story Si from a set of N stories {S1, S2, ..., SN } such that i = argmaxi sim(f\u03b8(Si), f\u03b8(Q)). Here, sim(\u00b7, \u00b7) is a similarity metric (e.g. cosine similarity) between two story representations f\u03b8(Si) and f\u03b8(Q) that are learned from human ratings of empathic similarity. Empathic Similarity Prediction. The overall task is, given a story pair (S1, S2), return a similar- ity score sim(f\u03b8(Si), f\u03b8(Q)) such that sim(\u00b7, \u00b7) is large for empathically similar stories and small for empathically dissimilar stories. 5.2 Models We propose finetuning LLMs to learn embeddings that capture empathic similarity using cosine dis- tance, for efficient retrieval at test time. In contrast, a popular approach is to use few-shot prompting of very large language models (e.g., GPT-3 and Chat- GPT), which have shown impressive performance across a variety of tasks (Brown et al., 2020). How- ever, in a real deployment setting, retrieval through prompting every possible pair of stories is expen- sive and inefficient. Model SBERT + finetuning BART + finetuning GPT-3 + 5 examples ChatGPT + 5 examples r 30.93 35.93 10.24 34.20 3.24 4.94 19.56 27.75 \u03c1 29.86 35.21 11.54 34.43 2.79 6.71 20.16 28.07 Acc 62.75 64.75 57.00 64.75 51.25 51.25 56.25 63.25 P 57.81 58.68 52.19 58.2 51.25 51.27 55.24 60.43 R 90.24 90.73 99.02 88.29 100 98.54 77.07 81.95 F 1 70.48 71.26 68.35 70.16 67.77 67.45 64.36 69.57 Pk=1 57.92 57.43 49.51 65.84 90.59 72.77 80.69 85.15 \u03c4rank 17.46 17.59 7.56 24.68 0.33 -4.8 13.48 21.27 \u03c1rank 18.74 18.98 9.28 26.55 0.79 -5.33 14.10 22.10 Table 4: Model performance for empathic similarity prediction task across correlation, accuracy, and retrieval metrics. r = Pearson\u2019s correlation, \u03c1 = Spearman\u2019s correlation, Acc = accuracy, P = precision, R = recall, Pk=1 = precision at k where k is 1, \u03c4rank = Kendall Tau of ranking and \u03c1rank = Spearman of ranking. Note that all scores are multiplied by 100 for easier comparison, and the maximum for each metric is 100. In bold is the best performing and underlined is the second-best performing condition for the metric. Baseline Models. We compare performance to finetuning with SBERT (multi-qa-mpnet-base-dot- v1) (Reimers and Gurevych, 2019; Brown et al., 2020) and BART model (bart-base) (Lewis et al., 2019). As a few-shot baseline, we evaluate GPT-3 (text-davinci-003) and ChatGPT\u2019s (gpt-3.5-turbo) ability to distinguish empathically similar stories by using a k-shot prompting setup as done in Sap et al. (2022); Brown et al. (2020). For the query story pair, we ask for an empathic similarity score from 1-4. We compare across k = 0 examples and k = 5 examples from the training set. We also evaluate these models\u2019 ability to generate human- like main event, emotion description, and moral summaries for each story. Again, we use a k-shot prompting setup, comparing across k = 0 and k = 10 examples. See Appendix G and Appendix C for prompts used and finetuning details. Empathy Similarity Prediction. We propose a bi- encoder architecture finetuned with mean-squared error (MSE) loss of the cosine-similarity between story pairs, as compared to the empathic similar- ity gold labels. For each of the encoders, we use a shared pretrained transformer-based model and further finetune on the 1,500 annotated story pairs in our training set. We obtain the final embedding using mean pooling of the encoder last hidden state. k = 1 (what proportion of the top-ranked stories by our model are the top-ranked story as rated by human annotators), Kendall\u2019s Tau (Abdi, 2007), and Spearman\u2019s correlation (Schober et al., 2018) for the ranking of the stories (how close the overall rankings are). Shown in Table 4, our results indicate that fine- tuning SBERT and BART with EMPATHICSTO- RIES results in performance gains across all metrics. SBERT has relatively high off-the-shelf perfor- mance, as it is trained with 215M examples specif- ically for semantic similarity tasks. However, we see that finetuning with our dataset, which contains far fewer training examples relative to SBERT\u2019s pretraining corpus, improves performance. (+ 5.35 \u03c1, +2 accuracy). BART, which is not specifically pre-trained for semantic similarity tasks, shows even greater gains across retrieval metrics when finetuned on our dataset. (22.89 \u03c1, +7.75 accuracy). We find that for BART models, fine tuning im- provements (p = 0.02, p = 0.0006 respectively), as measured with McNemar\u2019s test on the accuracy scores and Fisher\u2019s transformation on correlations, are significantly higher than baselines. While GPT-3 and ChatGPT have high perfor- mance on the precision at k retrieval metric, in practice, it is not feasible to prompt the models with every pair of stories in the retrieval corpus. 6 Automatic Evaluation To evaluate the quality of empathic similarity pre- dictions, we first compare the Spearman\u2019s and Pear- son\u2019s correlations between the cosine similarity of the sentence embeddings and the gold empathic similarity labels. Next, we bin scores into binary similar/dissimilar categories (> 2.5 and \u2264 2.5 re- spectively) compute the accuracy, precision, re- call, and F1 scores. Finally, we compute a series of retrieval-based metrics including precision at 7 User Study Prior work\u2019s versions of human evaluations (Zhou and Jurgens, 2020;"}, {"question": " What improvements were observed when finetuning SBERT and BART with EMPATHICSTORIES dataset?", "answer": " SBERT showed improved performance with +5.35 \u03c1 and +2 accuracy, while BART showed greater gains with +22.89 \u03c1 and +7.75 accuracy.", "ref_chunk": "distance away). We are aiming for a more descriptive annotation paradigm and thus do not expect annotators to perfectly agree (Rottger et al., 2022). Furthermore, our agreement rates are in line with other inherently personal and affect-driven annotation tasks (Sap et al., 2017; Rashkin et al., 2018). Given the difficulty of our task (reading longer stories and projecting the men- tal state of 2 characters), our agreement is in line with prior work, which achieve around 0.51 - 0.91 PPA and 0.29 - 0.34 KA. 5 Modeling Empathic Similarity To enable the retrieval and analysis of empathically similar stories, we design a task detailed below. In Appendix B, we also propose an auxiliary reason- ing task to automatically extract event, emotion, and moral features from stories, which could be used in future work to quickly generate story anno- tations. 5.1 Task Formulation task is given a query Our ultimate retrieval story Q and selects a story Si from a set of N stories {S1, S2, ..., SN } such that i = argmaxi sim(f\u03b8(Si), f\u03b8(Q)). Here, sim(\u00b7, \u00b7) is a similarity metric (e.g. cosine similarity) between two story representations f\u03b8(Si) and f\u03b8(Q) that are learned from human ratings of empathic similarity. Empathic Similarity Prediction. The overall task is, given a story pair (S1, S2), return a similar- ity score sim(f\u03b8(Si), f\u03b8(Q)) such that sim(\u00b7, \u00b7) is large for empathically similar stories and small for empathically dissimilar stories. 5.2 Models We propose finetuning LLMs to learn embeddings that capture empathic similarity using cosine dis- tance, for efficient retrieval at test time. In contrast, a popular approach is to use few-shot prompting of very large language models (e.g., GPT-3 and Chat- GPT), which have shown impressive performance across a variety of tasks (Brown et al., 2020). How- ever, in a real deployment setting, retrieval through prompting every possible pair of stories is expen- sive and inefficient. Model SBERT + finetuning BART + finetuning GPT-3 + 5 examples ChatGPT + 5 examples r 30.93 35.93 10.24 34.20 3.24 4.94 19.56 27.75 \u03c1 29.86 35.21 11.54 34.43 2.79 6.71 20.16 28.07 Acc 62.75 64.75 57.00 64.75 51.25 51.25 56.25 63.25 P 57.81 58.68 52.19 58.2 51.25 51.27 55.24 60.43 R 90.24 90.73 99.02 88.29 100 98.54 77.07 81.95 F 1 70.48 71.26 68.35 70.16 67.77 67.45 64.36 69.57 Pk=1 57.92 57.43 49.51 65.84 90.59 72.77 80.69 85.15 \u03c4rank 17.46 17.59 7.56 24.68 0.33 -4.8 13.48 21.27 \u03c1rank 18.74 18.98 9.28 26.55 0.79 -5.33 14.10 22.10 Table 4: Model performance for empathic similarity prediction task across correlation, accuracy, and retrieval metrics. r = Pearson\u2019s correlation, \u03c1 = Spearman\u2019s correlation, Acc = accuracy, P = precision, R = recall, Pk=1 = precision at k where k is 1, \u03c4rank = Kendall Tau of ranking and \u03c1rank = Spearman of ranking. Note that all scores are multiplied by 100 for easier comparison, and the maximum for each metric is 100. In bold is the best performing and underlined is the second-best performing condition for the metric. Baseline Models. We compare performance to finetuning with SBERT (multi-qa-mpnet-base-dot- v1) (Reimers and Gurevych, 2019; Brown et al., 2020) and BART model (bart-base) (Lewis et al., 2019). As a few-shot baseline, we evaluate GPT-3 (text-davinci-003) and ChatGPT\u2019s (gpt-3.5-turbo) ability to distinguish empathically similar stories by using a k-shot prompting setup as done in Sap et al. (2022); Brown et al. (2020). For the query story pair, we ask for an empathic similarity score from 1-4. We compare across k = 0 examples and k = 5 examples from the training set. We also evaluate these models\u2019 ability to generate human- like main event, emotion description, and moral summaries for each story. Again, we use a k-shot prompting setup, comparing across k = 0 and k = 10 examples. See Appendix G and Appendix C for prompts used and finetuning details. Empathy Similarity Prediction. We propose a bi- encoder architecture finetuned with mean-squared error (MSE) loss of the cosine-similarity between story pairs, as compared to the empathic similar- ity gold labels. For each of the encoders, we use a shared pretrained transformer-based model and further finetune on the 1,500 annotated story pairs in our training set. We obtain the final embedding using mean pooling of the encoder last hidden state. k = 1 (what proportion of the top-ranked stories by our model are the top-ranked story as rated by human annotators), Kendall\u2019s Tau (Abdi, 2007), and Spearman\u2019s correlation (Schober et al., 2018) for the ranking of the stories (how close the overall rankings are). Shown in Table 4, our results indicate that fine- tuning SBERT and BART with EMPATHICSTO- RIES results in performance gains across all metrics. SBERT has relatively high off-the-shelf perfor- mance, as it is trained with 215M examples specif- ically for semantic similarity tasks. However, we see that finetuning with our dataset, which contains far fewer training examples relative to SBERT\u2019s pretraining corpus, improves performance. (+ 5.35 \u03c1, +2 accuracy). BART, which is not specifically pre-trained for semantic similarity tasks, shows even greater gains across retrieval metrics when finetuned on our dataset. (22.89 \u03c1, +7.75 accuracy). We find that for BART models, fine tuning im- provements (p = 0.02, p = 0.0006 respectively), as measured with McNemar\u2019s test on the accuracy scores and Fisher\u2019s transformation on correlations, are significantly higher than baselines. While GPT-3 and ChatGPT have high perfor- mance on the precision at k retrieval metric, in practice, it is not feasible to prompt the models with every pair of stories in the retrieval corpus. 6 Automatic Evaluation To evaluate the quality of empathic similarity pre- dictions, we first compare the Spearman\u2019s and Pear- son\u2019s correlations between the cosine similarity of the sentence embeddings and the gold empathic similarity labels. Next, we bin scores into binary similar/dissimilar categories (> 2.5 and \u2264 2.5 re- spectively) compute the accuracy, precision, re- call, and F1 scores. Finally, we compute a series of retrieval-based metrics including precision at 7 User Study Prior work\u2019s versions of human evaluations (Zhou and Jurgens, 2020;"}], "doc_text": "distance away). We are aiming for a more descriptive annotation paradigm and thus do not expect annotators to perfectly agree (Rottger et al., 2022). Furthermore, our agreement rates are in line with other inherently personal and affect-driven annotation tasks (Sap et al., 2017; Rashkin et al., 2018). Given the difficulty of our task (reading longer stories and projecting the men- tal state of 2 characters), our agreement is in line with prior work, which achieve around 0.51 - 0.91 PPA and 0.29 - 0.34 KA. 5 Modeling Empathic Similarity To enable the retrieval and analysis of empathically similar stories, we design a task detailed below. In Appendix B, we also propose an auxiliary reason- ing task to automatically extract event, emotion, and moral features from stories, which could be used in future work to quickly generate story anno- tations. 5.1 Task Formulation task is given a query Our ultimate retrieval story Q and selects a story Si from a set of N stories {S1, S2, ..., SN } such that i = argmaxi sim(f\u03b8(Si), f\u03b8(Q)). Here, sim(\u00b7, \u00b7) is a similarity metric (e.g. cosine similarity) between two story representations f\u03b8(Si) and f\u03b8(Q) that are learned from human ratings of empathic similarity. Empathic Similarity Prediction. The overall task is, given a story pair (S1, S2), return a similar- ity score sim(f\u03b8(Si), f\u03b8(Q)) such that sim(\u00b7, \u00b7) is large for empathically similar stories and small for empathically dissimilar stories. 5.2 Models We propose finetuning LLMs to learn embeddings that capture empathic similarity using cosine dis- tance, for efficient retrieval at test time. In contrast, a popular approach is to use few-shot prompting of very large language models (e.g., GPT-3 and Chat- GPT), which have shown impressive performance across a variety of tasks (Brown et al., 2020). How- ever, in a real deployment setting, retrieval through prompting every possible pair of stories is expen- sive and inefficient. Model SBERT + finetuning BART + finetuning GPT-3 + 5 examples ChatGPT + 5 examples r 30.93 35.93 10.24 34.20 3.24 4.94 19.56 27.75 \u03c1 29.86 35.21 11.54 34.43 2.79 6.71 20.16 28.07 Acc 62.75 64.75 57.00 64.75 51.25 51.25 56.25 63.25 P 57.81 58.68 52.19 58.2 51.25 51.27 55.24 60.43 R 90.24 90.73 99.02 88.29 100 98.54 77.07 81.95 F 1 70.48 71.26 68.35 70.16 67.77 67.45 64.36 69.57 Pk=1 57.92 57.43 49.51 65.84 90.59 72.77 80.69 85.15 \u03c4rank 17.46 17.59 7.56 24.68 0.33 -4.8 13.48 21.27 \u03c1rank 18.74 18.98 9.28 26.55 0.79 -5.33 14.10 22.10 Table 4: Model performance for empathic similarity prediction task across correlation, accuracy, and retrieval metrics. r = Pearson\u2019s correlation, \u03c1 = Spearman\u2019s correlation, Acc = accuracy, P = precision, R = recall, Pk=1 = precision at k where k is 1, \u03c4rank = Kendall Tau of ranking and \u03c1rank = Spearman of ranking. Note that all scores are multiplied by 100 for easier comparison, and the maximum for each metric is 100. In bold is the best performing and underlined is the second-best performing condition for the metric. Baseline Models. We compare performance to finetuning with SBERT (multi-qa-mpnet-base-dot- v1) (Reimers and Gurevych, 2019; Brown et al., 2020) and BART model (bart-base) (Lewis et al., 2019). As a few-shot baseline, we evaluate GPT-3 (text-davinci-003) and ChatGPT\u2019s (gpt-3.5-turbo) ability to distinguish empathically similar stories by using a k-shot prompting setup as done in Sap et al. (2022); Brown et al. (2020). For the query story pair, we ask for an empathic similarity score from 1-4. We compare across k = 0 examples and k = 5 examples from the training set. We also evaluate these models\u2019 ability to generate human- like main event, emotion description, and moral summaries for each story. Again, we use a k-shot prompting setup, comparing across k = 0 and k = 10 examples. See Appendix G and Appendix C for prompts used and finetuning details. Empathy Similarity Prediction. We propose a bi- encoder architecture finetuned with mean-squared error (MSE) loss of the cosine-similarity between story pairs, as compared to the empathic similar- ity gold labels. For each of the encoders, we use a shared pretrained transformer-based model and further finetune on the 1,500 annotated story pairs in our training set. We obtain the final embedding using mean pooling of the encoder last hidden state. k = 1 (what proportion of the top-ranked stories by our model are the top-ranked story as rated by human annotators), Kendall\u2019s Tau (Abdi, 2007), and Spearman\u2019s correlation (Schober et al., 2018) for the ranking of the stories (how close the overall rankings are). Shown in Table 4, our results indicate that fine- tuning SBERT and BART with EMPATHICSTO- RIES results in performance gains across all metrics. SBERT has relatively high off-the-shelf perfor- mance, as it is trained with 215M examples specif- ically for semantic similarity tasks. However, we see that finetuning with our dataset, which contains far fewer training examples relative to SBERT\u2019s pretraining corpus, improves performance. (+ 5.35 \u03c1, +2 accuracy). BART, which is not specifically pre-trained for semantic similarity tasks, shows even greater gains across retrieval metrics when finetuned on our dataset. (22.89 \u03c1, +7.75 accuracy). We find that for BART models, fine tuning im- provements (p = 0.02, p = 0.0006 respectively), as measured with McNemar\u2019s test on the accuracy scores and Fisher\u2019s transformation on correlations, are significantly higher than baselines. While GPT-3 and ChatGPT have high perfor- mance on the precision at k retrieval metric, in practice, it is not feasible to prompt the models with every pair of stories in the retrieval corpus. 6 Automatic Evaluation To evaluate the quality of empathic similarity pre- dictions, we first compare the Spearman\u2019s and Pear- son\u2019s correlations between the cosine similarity of the sentence embeddings and the gold empathic similarity labels. Next, we bin scores into binary similar/dissimilar categories (> 2.5 and \u2264 2.5 re- spectively) compute the accuracy, precision, re- call, and F1 scores. Finally, we compute a series of retrieval-based metrics including precision at 7 User Study Prior work\u2019s versions of human evaluations (Zhou and Jurgens, 2020;"}