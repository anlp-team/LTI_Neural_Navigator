{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_Generating_Images_with_Multimodal_Language_Models_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the proposed method in the text?", "answer": " To fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces.", "ref_chunk": "3 2 0 2 t c O 3 1 ] L C . s c [ 3 v 6 1 2 7 1 . 5 0 3 2 : v i X r a Generating Images with Multimodal Language Models Jing Yu Koh Carnegie Mellon University jingyuk@cs.cmu.edu Daniel Fried Carnegie Mellon University dfried@cs.cmu.edu Ruslan Salakhutdinov Carnegie Mellon University rsalakhu@cs.cmu.edu Abstract We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text \u2014 outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence. 1 Introduction Autoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56, 4, 29, 33, 31, 35], to embodied settings for robotics [3, 18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. In this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4, 2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Less sensitive to longer text inputs (such as dialogue). Show me an illustration of a beaver model. Image outputs limited to the candidate retrieval set. Show me an illustration of a beaver Retrieved User prompts Show me an illustration of a beaver What about a comic illustration of one? Yes, what about a pencil drawing of one? What about a comic illustration of one? Generated Yes, what about a pencil drawing of one? FROMAGeRetrieval only multimodal language Yes, what about a pencil drawing of one? What about a comic illustration of one? Stable DiffusionGeneration only text-to-image model. No, the animal I am thinking of lives in the river. Decides when to retrieve or generate. A beaver. A beaver. A beaver. A beaver. A beaver. A beaver. A squirrel. No, the animal I am thinking of lives in the river. No, the animal I am thinking of lives in the river. A squirrel. I am thinking of an animal. It is brown and furry, and has a tail. What is it? A squirrel. OursRetrieval and generation multimodal LM. I am thinking of an animal. It is brown and furry, and has a tail. What is it? I am thinking of an animal. It is brown and furry, and has a tail. What is it? Figure 1: Our model is capable of generating text, retrieving images, generating novel images, and interleaving results into coherent multimodal dialogue. learnt text tokens. We train it by minimizing the l2 distance between its outputs and the outputs of the text encoder of a text-to-image generation model. This distillation training allows us to use the image decoder of the text-to-image model at inference time. Despite its simplicity, we show that this allows us to outperform the baseline text-to-image generation model on several tasks that measure language context dependence. Finally, to decide whether to produce a retrieved image or a generated one at inference time, we train a decision model that outputs a decision conditioned on the LM hidden representations. This allows us to both generate and retrieve in output sequences, as shown in Fig. 1. Our experimental results demonstrate that GILL is more effective than Stable Diffusion at processing longer-form text, including dialogue and discourse. We show on dialogue-conditioned image gener- ation that GILL can outperform non-LLM based generation models, and benefit from multimodal context: generating images that match text better than the backbone generation models that we distill from. In addition, GILL can process arbitrarily interleaved image-text inputs, unlike typical text-to-image models which only process text. GILL is the first model capable of outputting retrieved images, novel images, and text \u2014 interleaving these for coherent multimodal dialogue generation.1 2 Related Work Multimodal Language Models Several prior works"}, {"question": " What are the capabilities demonstrated by the model in the text?", "answer": " Image retrieval, novel image generation, and multimodal dialogue.", "ref_chunk": "3 2 0 2 t c O 3 1 ] L C . s c [ 3 v 6 1 2 7 1 . 5 0 3 2 : v i X r a Generating Images with Multimodal Language Models Jing Yu Koh Carnegie Mellon University jingyuk@cs.cmu.edu Daniel Fried Carnegie Mellon University dfried@cs.cmu.edu Ruslan Salakhutdinov Carnegie Mellon University rsalakhu@cs.cmu.edu Abstract We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text \u2014 outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence. 1 Introduction Autoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56, 4, 29, 33, 31, 35], to embodied settings for robotics [3, 18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. In this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4, 2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Less sensitive to longer text inputs (such as dialogue). Show me an illustration of a beaver model. Image outputs limited to the candidate retrieval set. Show me an illustration of a beaver Retrieved User prompts Show me an illustration of a beaver What about a comic illustration of one? Yes, what about a pencil drawing of one? What about a comic illustration of one? Generated Yes, what about a pencil drawing of one? FROMAGeRetrieval only multimodal language Yes, what about a pencil drawing of one? What about a comic illustration of one? Stable DiffusionGeneration only text-to-image model. No, the animal I am thinking of lives in the river. Decides when to retrieve or generate. A beaver. A beaver. A beaver. A beaver. A beaver. A beaver. A squirrel. No, the animal I am thinking of lives in the river. No, the animal I am thinking of lives in the river. A squirrel. I am thinking of an animal. It is brown and furry, and has a tail. What is it? A squirrel. OursRetrieval and generation multimodal LM. I am thinking of an animal. It is brown and furry, and has a tail. What is it? I am thinking of an animal. It is brown and furry, and has a tail. What is it? Figure 1: Our model is capable of generating text, retrieving images, generating novel images, and interleaving results into coherent multimodal dialogue. learnt text tokens. We train it by minimizing the l2 distance between its outputs and the outputs of the text encoder of a text-to-image generation model. This distillation training allows us to use the image decoder of the text-to-image model at inference time. Despite its simplicity, we show that this allows us to outperform the baseline text-to-image generation model on several tasks that measure language context dependence. Finally, to decide whether to produce a retrieved image or a generated one at inference time, we train a decision model that outputs a decision conditioned on the LM hidden representations. This allows us to both generate and retrieve in output sequences, as shown in Fig. 1. Our experimental results demonstrate that GILL is more effective than Stable Diffusion at processing longer-form text, including dialogue and discourse. We show on dialogue-conditioned image gener- ation that GILL can outperform non-LLM based generation models, and benefit from multimodal context: generating images that match text better than the backbone generation models that we distill from. In addition, GILL can process arbitrarily interleaved image-text inputs, unlike typical text-to-image models which only process text. GILL is the first model capable of outputting retrieved images, novel images, and text \u2014 interleaving these for coherent multimodal dialogue generation.1 2 Related Work Multimodal Language Models Several prior works"}, {"question": " What is the main contribution of the proposed model according to the text?", "answer": " It is capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs.", "ref_chunk": "3 2 0 2 t c O 3 1 ] L C . s c [ 3 v 6 1 2 7 1 . 5 0 3 2 : v i X r a Generating Images with Multimodal Language Models Jing Yu Koh Carnegie Mellon University jingyuk@cs.cmu.edu Daniel Fried Carnegie Mellon University dfried@cs.cmu.edu Ruslan Salakhutdinov Carnegie Mellon University rsalakhu@cs.cmu.edu Abstract We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text \u2014 outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence. 1 Introduction Autoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56, 4, 29, 33, 31, 35], to embodied settings for robotics [3, 18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. In this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4, 2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Less sensitive to longer text inputs (such as dialogue). Show me an illustration of a beaver model. Image outputs limited to the candidate retrieval set. Show me an illustration of a beaver Retrieved User prompts Show me an illustration of a beaver What about a comic illustration of one? Yes, what about a pencil drawing of one? What about a comic illustration of one? Generated Yes, what about a pencil drawing of one? FROMAGeRetrieval only multimodal language Yes, what about a pencil drawing of one? What about a comic illustration of one? Stable DiffusionGeneration only text-to-image model. No, the animal I am thinking of lives in the river. Decides when to retrieve or generate. A beaver. A beaver. A beaver. A beaver. A beaver. A beaver. A squirrel. No, the animal I am thinking of lives in the river. No, the animal I am thinking of lives in the river. A squirrel. I am thinking of an animal. It is brown and furry, and has a tail. What is it? A squirrel. OursRetrieval and generation multimodal LM. I am thinking of an animal. It is brown and furry, and has a tail. What is it? I am thinking of an animal. It is brown and furry, and has a tail. What is it? Figure 1: Our model is capable of generating text, retrieving images, generating novel images, and interleaving results into coherent multimodal dialogue. learnt text tokens. We train it by minimizing the l2 distance between its outputs and the outputs of the text encoder of a text-to-image generation model. This distillation training allows us to use the image decoder of the text-to-image model at inference time. Despite its simplicity, we show that this allows us to outperform the baseline text-to-image generation model on several tasks that measure language context dependence. Finally, to decide whether to produce a retrieved image or a generated one at inference time, we train a decision model that outputs a decision conditioned on the LM hidden representations. This allows us to both generate and retrieve in output sequences, as shown in Fig. 1. Our experimental results demonstrate that GILL is more effective than Stable Diffusion at processing longer-form text, including dialogue and discourse. We show on dialogue-conditioned image gener- ation that GILL can outperform non-LLM based generation models, and benefit from multimodal context: generating images that match text better than the backbone generation models that we distill from. In addition, GILL can process arbitrarily interleaved image-text inputs, unlike typical text-to-image models which only process text. GILL is the first model capable of outputting retrieved images, novel images, and text \u2014 interleaving these for coherent multimodal dialogue generation.1 2 Related Work Multimodal Language Models Several prior works"}, {"question": " How does the proposed model achieve strong performance on image generation?", "answer": " By proposing an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model.", "ref_chunk": "3 2 0 2 t c O 3 1 ] L C . s c [ 3 v 6 1 2 7 1 . 5 0 3 2 : v i X r a Generating Images with Multimodal Language Models Jing Yu Koh Carnegie Mellon University jingyuk@cs.cmu.edu Daniel Fried Carnegie Mellon University dfried@cs.cmu.edu Ruslan Salakhutdinov Carnegie Mellon University rsalakhu@cs.cmu.edu Abstract We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text \u2014 outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence. 1 Introduction Autoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56, 4, 29, 33, 31, 35], to embodied settings for robotics [3, 18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. In this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4, 2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Less sensitive to longer text inputs (such as dialogue). Show me an illustration of a beaver model. Image outputs limited to the candidate retrieval set. Show me an illustration of a beaver Retrieved User prompts Show me an illustration of a beaver What about a comic illustration of one? Yes, what about a pencil drawing of one? What about a comic illustration of one? Generated Yes, what about a pencil drawing of one? FROMAGeRetrieval only multimodal language Yes, what about a pencil drawing of one? What about a comic illustration of one? Stable DiffusionGeneration only text-to-image model. No, the animal I am thinking of lives in the river. Decides when to retrieve or generate. A beaver. A beaver. A beaver. A beaver. A beaver. A beaver. A squirrel. No, the animal I am thinking of lives in the river. No, the animal I am thinking of lives in the river. A squirrel. I am thinking of an animal. It is brown and furry, and has a tail. What is it? A squirrel. OursRetrieval and generation multimodal LM. I am thinking of an animal. It is brown and furry, and has a tail. What is it? I am thinking of an animal. It is brown and furry, and has a tail. What is it? Figure 1: Our model is capable of generating text, retrieving images, generating novel images, and interleaving results into coherent multimodal dialogue. learnt text tokens. We train it by minimizing the l2 distance between its outputs and the outputs of the text encoder of a text-to-image generation model. This distillation training allows us to use the image decoder of the text-to-image model at inference time. Despite its simplicity, we show that this allows us to outperform the baseline text-to-image generation model on several tasks that measure language context dependence. Finally, to decide whether to produce a retrieved image or a generated one at inference time, we train a decision model that outputs a decision conditioned on the LM hidden representations. This allows us to both generate and retrieve in output sequences, as shown in Fig. 1. Our experimental results demonstrate that GILL is more effective than Stable Diffusion at processing longer-form text, including dialogue and discourse. We show on dialogue-conditioned image gener- ation that GILL can outperform non-LLM based generation models, and benefit from multimodal context: generating images that match text better than the backbone generation models that we distill from. In addition, GILL can process arbitrarily interleaved image-text inputs, unlike typical text-to-image models which only process text. GILL is the first model capable of outputting retrieved images, novel images, and text \u2014 interleaving these for coherent multimodal dialogue generation.1 2 Related Work Multimodal Language Models Several prior works"}, {"question": " What does the approach in the text outperform baseline generation models on?", "answer": " Tasks with longer and more complex language.", "ref_chunk": "3 2 0 2 t c O 3 1 ] L C . s c [ 3 v 6 1 2 7 1 . 5 0 3 2 : v i X r a Generating Images with Multimodal Language Models Jing Yu Koh Carnegie Mellon University jingyuk@cs.cmu.edu Daniel Fried Carnegie Mellon University dfried@cs.cmu.edu Ruslan Salakhutdinov Carnegie Mellon University rsalakhu@cs.cmu.edu Abstract We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text \u2014 outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence. 1 Introduction Autoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56, 4, 29, 33, 31, 35], to embodied settings for robotics [3, 18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. In this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4, 2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Less sensitive to longer text inputs (such as dialogue). Show me an illustration of a beaver model. Image outputs limited to the candidate retrieval set. Show me an illustration of a beaver Retrieved User prompts Show me an illustration of a beaver What about a comic illustration of one? Yes, what about a pencil drawing of one? What about a comic illustration of one? Generated Yes, what about a pencil drawing of one? FROMAGeRetrieval only multimodal language Yes, what about a pencil drawing of one? What about a comic illustration of one? Stable DiffusionGeneration only text-to-image model. No, the animal I am thinking of lives in the river. Decides when to retrieve or generate. A beaver. A beaver. A beaver. A beaver. A beaver. A beaver. A squirrel. No, the animal I am thinking of lives in the river. No, the animal I am thinking of lives in the river. A squirrel. I am thinking of an animal. It is brown and furry, and has a tail. What is it? A squirrel. OursRetrieval and generation multimodal LM. I am thinking of an animal. It is brown and furry, and has a tail. What is it? I am thinking of an animal. It is brown and furry, and has a tail. What is it? Figure 1: Our model is capable of generating text, retrieving images, generating novel images, and interleaving results into coherent multimodal dialogue. learnt text tokens. We train it by minimizing the l2 distance between its outputs and the outputs of the text encoder of a text-to-image generation model. This distillation training allows us to use the image decoder of the text-to-image model at inference time. Despite its simplicity, we show that this allows us to outperform the baseline text-to-image generation model on several tasks that measure language context dependence. Finally, to decide whether to produce a retrieved image or a generated one at inference time, we train a decision model that outputs a decision conditioned on the LM hidden representations. This allows us to both generate and retrieve in output sequences, as shown in Fig. 1. Our experimental results demonstrate that GILL is more effective than Stable Diffusion at processing longer-form text, including dialogue and discourse. We show on dialogue-conditioned image gener- ation that GILL can outperform non-LLM based generation models, and benefit from multimodal context: generating images that match text better than the backbone generation models that we distill from. In addition, GILL can process arbitrarily interleaved image-text inputs, unlike typical text-to-image models which only process text. GILL is the first model capable of outputting retrieved images, novel images, and text \u2014 interleaving these for coherent multimodal dialogue generation.1 2 Related Work Multimodal Language Models Several prior works"}, {"question": " What is the model also capable of, besides novel image generation, according to the text?", "answer": " Image retrieval from a prespecified dataset, and deciding whether to retrieve or generate at inference time.", "ref_chunk": "3 2 0 2 t c O 3 1 ] L C . s c [ 3 v 6 1 2 7 1 . 5 0 3 2 : v i X r a Generating Images with Multimodal Language Models Jing Yu Koh Carnegie Mellon University jingyuk@cs.cmu.edu Daniel Fried Carnegie Mellon University dfried@cs.cmu.edu Ruslan Salakhutdinov Carnegie Mellon University rsalakhu@cs.cmu.edu Abstract We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text \u2014 outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence. 1 Introduction Autoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56, 4, 29, 33, 31, 35], to embodied settings for robotics [3, 18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. In this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4, 2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Less sensitive to longer text inputs (such as dialogue). Show me an illustration of a beaver model. Image outputs limited to the candidate retrieval set. Show me an illustration of a beaver Retrieved User prompts Show me an illustration of a beaver What about a comic illustration of one? Yes, what about a pencil drawing of one? What about a comic illustration of one? Generated Yes, what about a pencil drawing of one? FROMAGeRetrieval only multimodal language Yes, what about a pencil drawing of one? What about a comic illustration of one? Stable DiffusionGeneration only text-to-image model. No, the animal I am thinking of lives in the river. Decides when to retrieve or generate. A beaver. A beaver. A beaver. A beaver. A beaver. A beaver. A squirrel. No, the animal I am thinking of lives in the river. No, the animal I am thinking of lives in the river. A squirrel. I am thinking of an animal. It is brown and furry, and has a tail. What is it? A squirrel. OursRetrieval and generation multimodal LM. I am thinking of an animal. It is brown and furry, and has a tail. What is it? I am thinking of an animal. It is brown and furry, and has a tail. What is it? Figure 1: Our model is capable of generating text, retrieving images, generating novel images, and interleaving results into coherent multimodal dialogue. learnt text tokens. We train it by minimizing the l2 distance between its outputs and the outputs of the text encoder of a text-to-image generation model. This distillation training allows us to use the image decoder of the text-to-image model at inference time. Despite its simplicity, we show that this allows us to outperform the baseline text-to-image generation model on several tasks that measure language context dependence. Finally, to decide whether to produce a retrieved image or a generated one at inference time, we train a decision model that outputs a decision conditioned on the LM hidden representations. This allows us to both generate and retrieve in output sequences, as shown in Fig. 1. Our experimental results demonstrate that GILL is more effective than Stable Diffusion at processing longer-form text, including dialogue and discourse. We show on dialogue-conditioned image gener- ation that GILL can outperform non-LLM based generation models, and benefit from multimodal context: generating images that match text better than the backbone generation models that we distill from. In addition, GILL can process arbitrarily interleaved image-text inputs, unlike typical text-to-image models which only process text. GILL is the first model capable of outputting retrieved images, novel images, and text \u2014 interleaving these for coherent multimodal dialogue generation.1 2 Related Work Multimodal Language Models Several prior works"}, {"question": " What enables the model to leverage the strong text representations of the LLM for visual outputs in the text?", "answer": " The mapping network that translates hidden representations of text into the embedding space of the visual models.", "ref_chunk": "3 2 0 2 t c O 3 1 ] L C . s c [ 3 v 6 1 2 7 1 . 5 0 3 2 : v i X r a Generating Images with Multimodal Language Models Jing Yu Koh Carnegie Mellon University jingyuk@cs.cmu.edu Daniel Fried Carnegie Mellon University dfried@cs.cmu.edu Ruslan Salakhutdinov Carnegie Mellon University rsalakhu@cs.cmu.edu Abstract We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text \u2014 outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence. 1 Introduction Autoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56, 4, 29, 33, 31, 35], to embodied settings for robotics [3, 18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. In this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4, 2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Less sensitive to longer text inputs (such as dialogue). Show me an illustration of a beaver model. Image outputs limited to the candidate retrieval set. Show me an illustration of a beaver Retrieved User prompts Show me an illustration of a beaver What about a comic illustration of one? Yes, what about a pencil drawing of one? What about a comic illustration of one? Generated Yes, what about a pencil drawing of one? FROMAGeRetrieval only multimodal language Yes, what about a pencil drawing of one? What about a comic illustration of one? Stable DiffusionGeneration only text-to-image model. No, the animal I am thinking of lives in the river. Decides when to retrieve or generate. A beaver. A beaver. A beaver. A beaver. A beaver. A beaver. A squirrel. No, the animal I am thinking of lives in the river. No, the animal I am thinking of lives in the river. A squirrel. I am thinking of an animal. It is brown and furry, and has a tail. What is it? A squirrel. OursRetrieval and generation multimodal LM. I am thinking of an animal. It is brown and furry, and has a tail. What is it? I am thinking of an animal. It is brown and furry, and has a tail. What is it? Figure 1: Our model is capable of generating text, retrieving images, generating novel images, and interleaving results into coherent multimodal dialogue. learnt text tokens. We train it by minimizing the l2 distance between its outputs and the outputs of the text encoder of a text-to-image generation model. This distillation training allows us to use the image decoder of the text-to-image model at inference time. Despite its simplicity, we show that this allows us to outperform the baseline text-to-image generation model on several tasks that measure language context dependence. Finally, to decide whether to produce a retrieved image or a generated one at inference time, we train a decision model that outputs a decision conditioned on the LM hidden representations. This allows us to both generate and retrieve in output sequences, as shown in Fig. 1. Our experimental results demonstrate that GILL is more effective than Stable Diffusion at processing longer-form text, including dialogue and discourse. We show on dialogue-conditioned image gener- ation that GILL can outperform non-LLM based generation models, and benefit from multimodal context: generating images that match text better than the backbone generation models that we distill from. In addition, GILL can process arbitrarily interleaved image-text inputs, unlike typical text-to-image models which only process text. GILL is the first model capable of outputting retrieved images, novel images, and text \u2014 interleaving these for coherent multimodal dialogue generation.1 2 Related Work Multimodal Language Models Several prior works"}, {"question": " What type of tasks does the proposed approach achieve strong performance on according to the text?", "answer": " Text-to-image tasks that measure context dependence.", "ref_chunk": "3 2 0 2 t c O 3 1 ] L C . s c [ 3 v 6 1 2 7 1 . 5 0 3 2 : v i X r a Generating Images with Multimodal Language Models Jing Yu Koh Carnegie Mellon University jingyuk@cs.cmu.edu Daniel Fried Carnegie Mellon University dfried@cs.cmu.edu Ruslan Salakhutdinov Carnegie Mellon University rsalakhu@cs.cmu.edu Abstract We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text \u2014 outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence. 1 Introduction Autoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56, 4, 29, 33, 31, 35], to embodied settings for robotics [3, 18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. In this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4, 2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Less sensitive to longer text inputs (such as dialogue). Show me an illustration of a beaver model. Image outputs limited to the candidate retrieval set. Show me an illustration of a beaver Retrieved User prompts Show me an illustration of a beaver What about a comic illustration of one? Yes, what about a pencil drawing of one? What about a comic illustration of one? Generated Yes, what about a pencil drawing of one? FROMAGeRetrieval only multimodal language Yes, what about a pencil drawing of one? What about a comic illustration of one? Stable DiffusionGeneration only text-to-image model. No, the animal I am thinking of lives in the river. Decides when to retrieve or generate. A beaver. A beaver. A beaver. A beaver. A beaver. A beaver. A squirrel. No, the animal I am thinking of lives in the river. No, the animal I am thinking of lives in the river. A squirrel. I am thinking of an animal. It is brown and furry, and has a tail. What is it? A squirrel. OursRetrieval and generation multimodal LM. I am thinking of an animal. It is brown and furry, and has a tail. What is it? I am thinking of an animal. It is brown and furry, and has a tail. What is it? Figure 1: Our model is capable of generating text, retrieving images, generating novel images, and interleaving results into coherent multimodal dialogue. learnt text tokens. We train it by minimizing the l2 distance between its outputs and the outputs of the text encoder of a text-to-image generation model. This distillation training allows us to use the image decoder of the text-to-image model at inference time. Despite its simplicity, we show that this allows us to outperform the baseline text-to-image generation model on several tasks that measure language context dependence. Finally, to decide whether to produce a retrieved image or a generated one at inference time, we train a decision model that outputs a decision conditioned on the LM hidden representations. This allows us to both generate and retrieve in output sequences, as shown in Fig. 1. Our experimental results demonstrate that GILL is more effective than Stable Diffusion at processing longer-form text, including dialogue and discourse. We show on dialogue-conditioned image gener- ation that GILL can outperform non-LLM based generation models, and benefit from multimodal context: generating images that match text better than the backbone generation models that we distill from. In addition, GILL can process arbitrarily interleaved image-text inputs, unlike typical text-to-image models which only process text. GILL is the first model capable of outputting retrieved images, novel images, and text \u2014 interleaving these for coherent multimodal dialogue generation.1 2 Related Work Multimodal Language Models Several prior works"}, {"question": " What is the task tackled by the proposed model in the text?", "answer": " Extending multimodal language models to generate novel images.", "ref_chunk": "3 2 0 2 t c O 3 1 ] L C . s c [ 3 v 6 1 2 7 1 . 5 0 3 2 : v i X r a Generating Images with Multimodal Language Models Jing Yu Koh Carnegie Mellon University jingyuk@cs.cmu.edu Daniel Fried Carnegie Mellon University dfried@cs.cmu.edu Ruslan Salakhutdinov Carnegie Mellon University rsalakhu@cs.cmu.edu Abstract We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text \u2014 outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence. 1 Introduction Autoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56, 4, 29, 33, 31, 35], to embodied settings for robotics [3, 18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. In this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4, 2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Less sensitive to longer text inputs (such as dialogue). Show me an illustration of a beaver model. Image outputs limited to the candidate retrieval set. Show me an illustration of a beaver Retrieved User prompts Show me an illustration of a beaver What about a comic illustration of one? Yes, what about a pencil drawing of one? What about a comic illustration of one? Generated Yes, what about a pencil drawing of one? FROMAGeRetrieval only multimodal language Yes, what about a pencil drawing of one? What about a comic illustration of one? Stable DiffusionGeneration only text-to-image model. No, the animal I am thinking of lives in the river. Decides when to retrieve or generate. A beaver. A beaver. A beaver. A beaver. A beaver. A beaver. A squirrel. No, the animal I am thinking of lives in the river. No, the animal I am thinking of lives in the river. A squirrel. I am thinking of an animal. It is brown and furry, and has a tail. What is it? A squirrel. OursRetrieval and generation multimodal LM. I am thinking of an animal. It is brown and furry, and has a tail. What is it? I am thinking of an animal. It is brown and furry, and has a tail. What is it? Figure 1: Our model is capable of generating text, retrieving images, generating novel images, and interleaving results into coherent multimodal dialogue. learnt text tokens. We train it by minimizing the l2 distance between its outputs and the outputs of the text encoder of a text-to-image generation model. This distillation training allows us to use the image decoder of the text-to-image model at inference time. Despite its simplicity, we show that this allows us to outperform the baseline text-to-image generation model on several tasks that measure language context dependence. Finally, to decide whether to produce a retrieved image or a generated one at inference time, we train a decision model that outputs a decision conditioned on the LM hidden representations. This allows us to both generate and retrieve in output sequences, as shown in Fig. 1. Our experimental results demonstrate that GILL is more effective than Stable Diffusion at processing longer-form text, including dialogue and discourse. We show on dialogue-conditioned image gener- ation that GILL can outperform non-LLM based generation models, and benefit from multimodal context: generating images that match text better than the backbone generation models that we distill from. In addition, GILL can process arbitrarily interleaved image-text inputs, unlike typical text-to-image models which only process text. GILL is the first model capable of outputting retrieved images, novel images, and text \u2014 interleaving these for coherent multimodal dialogue generation.1 2 Related Work Multimodal Language Models Several prior works"}, {"question": " How does the proposed model efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model?", "answer": " By finetuning a small number of parameters on image-caption pairs.", "ref_chunk": "3 2 0 2 t c O 3 1 ] L C . s c [ 3 v 6 1 2 7 1 . 5 0 3 2 : v i X r a Generating Images with Multimodal Language Models Jing Yu Koh Carnegie Mellon University jingyuk@cs.cmu.edu Daniel Fried Carnegie Mellon University dfried@cs.cmu.edu Ruslan Salakhutdinov Carnegie Mellon University rsalakhu@cs.cmu.edu Abstract We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text \u2014 outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence. 1 Introduction Autoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56, 4, 29, 33, 31, 35], to embodied settings for robotics [3, 18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. In this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4, 2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Less sensitive to longer text inputs (such as dialogue). Show me an illustration of a beaver model. Image outputs limited to the candidate retrieval set. Show me an illustration of a beaver Retrieved User prompts Show me an illustration of a beaver What about a comic illustration of one? Yes, what about a pencil drawing of one? What about a comic illustration of one? Generated Yes, what about a pencil drawing of one? FROMAGeRetrieval only multimodal language Yes, what about a pencil drawing of one? What about a comic illustration of one? Stable DiffusionGeneration only text-to-image model. No, the animal I am thinking of lives in the river. Decides when to retrieve or generate. A beaver. A beaver. A beaver. A beaver. A beaver. A beaver. A squirrel. No, the animal I am thinking of lives in the river. No, the animal I am thinking of lives in the river. A squirrel. I am thinking of an animal. It is brown and furry, and has a tail. What is it? A squirrel. OursRetrieval and generation multimodal LM. I am thinking of an animal. It is brown and furry, and has a tail. What is it? I am thinking of an animal. It is brown and furry, and has a tail. What is it? Figure 1: Our model is capable of generating text, retrieving images, generating novel images, and interleaving results into coherent multimodal dialogue. learnt text tokens. We train it by minimizing the l2 distance between its outputs and the outputs of the text encoder of a text-to-image generation model. This distillation training allows us to use the image decoder of the text-to-image model at inference time. Despite its simplicity, we show that this allows us to outperform the baseline text-to-image generation model on several tasks that measure language context dependence. Finally, to decide whether to produce a retrieved image or a generated one at inference time, we train a decision model that outputs a decision conditioned on the LM hidden representations. This allows us to both generate and retrieve in output sequences, as shown in Fig. 1. Our experimental results demonstrate that GILL is more effective than Stable Diffusion at processing longer-form text, including dialogue and discourse. We show on dialogue-conditioned image gener- ation that GILL can outperform non-LLM based generation models, and benefit from multimodal context: generating images that match text better than the backbone generation models that we distill from. In addition, GILL can process arbitrarily interleaved image-text inputs, unlike typical text-to-image models which only process text. GILL is the first model capable of outputting retrieved images, novel images, and text \u2014 interleaving these for coherent multimodal dialogue generation.1 2 Related Work Multimodal Language Models Several prior works"}], "doc_text": "3 2 0 2 t c O 3 1 ] L C . s c [ 3 v 6 1 2 7 1 . 5 0 3 2 : v i X r a Generating Images with Multimodal Language Models Jing Yu Koh Carnegie Mellon University jingyuk@cs.cmu.edu Daniel Fried Carnegie Mellon University dfried@cs.cmu.edu Ruslan Salakhutdinov Carnegie Mellon University rsalakhu@cs.cmu.edu Abstract We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text \u2014 outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence. 1 Introduction Autoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56, 4, 29, 33, 31, 35], to embodied settings for robotics [3, 18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. In this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4, 2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Less sensitive to longer text inputs (such as dialogue). Show me an illustration of a beaver model. Image outputs limited to the candidate retrieval set. Show me an illustration of a beaver Retrieved User prompts Show me an illustration of a beaver What about a comic illustration of one? Yes, what about a pencil drawing of one? What about a comic illustration of one? Generated Yes, what about a pencil drawing of one? FROMAGeRetrieval only multimodal language Yes, what about a pencil drawing of one? What about a comic illustration of one? Stable DiffusionGeneration only text-to-image model. No, the animal I am thinking of lives in the river. Decides when to retrieve or generate. A beaver. A beaver. A beaver. A beaver. A beaver. A beaver. A squirrel. No, the animal I am thinking of lives in the river. No, the animal I am thinking of lives in the river. A squirrel. I am thinking of an animal. It is brown and furry, and has a tail. What is it? A squirrel. OursRetrieval and generation multimodal LM. I am thinking of an animal. It is brown and furry, and has a tail. What is it? I am thinking of an animal. It is brown and furry, and has a tail. What is it? Figure 1: Our model is capable of generating text, retrieving images, generating novel images, and interleaving results into coherent multimodal dialogue. learnt text tokens. We train it by minimizing the l2 distance between its outputs and the outputs of the text encoder of a text-to-image generation model. This distillation training allows us to use the image decoder of the text-to-image model at inference time. Despite its simplicity, we show that this allows us to outperform the baseline text-to-image generation model on several tasks that measure language context dependence. Finally, to decide whether to produce a retrieved image or a generated one at inference time, we train a decision model that outputs a decision conditioned on the LM hidden representations. This allows us to both generate and retrieve in output sequences, as shown in Fig. 1. Our experimental results demonstrate that GILL is more effective than Stable Diffusion at processing longer-form text, including dialogue and discourse. We show on dialogue-conditioned image gener- ation that GILL can outperform non-LLM based generation models, and benefit from multimodal context: generating images that match text better than the backbone generation models that we distill from. In addition, GILL can process arbitrarily interleaved image-text inputs, unlike typical text-to-image models which only process text. GILL is the first model capable of outputting retrieved images, novel images, and text \u2014 interleaving these for coherent multimodal dialogue generation.1 2 Related Work Multimodal Language Models Several prior works"}