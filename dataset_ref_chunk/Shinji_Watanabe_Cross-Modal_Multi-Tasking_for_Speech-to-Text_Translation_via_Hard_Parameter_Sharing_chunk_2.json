{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Cross-Modal_Multi-Tasking_for_Speech-to-Text_Translation_via_Hard_Parameter_Sharing_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What discretization technique is used in the approach described in the text?", "answer": " k-means clustering and SSL representations", "ref_chunk": "discrete units representing chunks of speech. There are many discretization techniques which can accom- plish this [18, 27\u201329]; we opt for the approach described by [14] which uses k-means clustering and SSL representations in a manner similar to HuBERT [30]. We first select an appropriate SSL model and intermediate layer to work with. In this work, we use WavLM which is pre-trained on 94k hours of English speech with masked prediction and de- noising objectives [22]. We select the 21st layer from WavLM as it has the highest canonical correlation analysis (CCA) similarity to word labels [31], suggesting that these representations contain use- ful semantic information. Next, we extract these representations for a portion of our training data and use them to train a k-means model with 2000 centroids. We then use this k-means model to convert the entire training set into sequences of k-means cluster assignments; note that at this point we have a sequence of discrete units but have only down-sampled the sequence length to 50 kHz via WavLM. As noted by prior works [14, 20], these sequences of k-means cluster assignments can be collapsed by removing repeats of the same consecutive unit. Finally, subword modeling can be applied to further reduce the sequence length [14]; we use the unigram al- gorithm from SentencePiece [32] to construct a vocabulary of 4000 tokens. To reduce over-fitting on particular segmentation patterns, we also apply BPE-dropout [33] during training. Ultimately, we ob- tain discrete speech sequences with an average length of 122 tokens from original audio with an average duration of 6.4 seconds. 2.2. ST/MT Multi-Tasking with Hard Parameter Sharing 2.2.1. ST from Discrete Speech Inputs With discrete speech inputs, the ST task seeks to map a sequence of tokens X DISC into another sequence of tokens Y TGT = {yTGT m \u2208 V TGT|l = 1, ..., M } where V TGT is the subword vocabulary for the target language \u2013 this is analogous to the MT task. We therefore re- place the convolutional feature extractor typically used by systems which process continuous speech inputs [34] with a learned embed- ding layer [35]. Following [14], we apply time-masking to the se- quences produced by the embedding layer; this is an additional form of data augmentation similar to SpecAugment [36] which is com- monly applied to continuous speech inputs. Our ST data is defined as a set of speech, source text, and target text triplets {(X DISC, Y SRC, Y TGT)}. Similar to target text, source text is a sequence of tokens Y SRC = {ySRC n \u2208 V SRC|l = 1, ..., N } where V SRC is the subword vocabulary for the source language. 2.2.2. Incorporating the MT Task Since the discrete speech sequences X DISC are still longer than their corresponding source text Y SRC, we repeat the source text tokens by a factor of 4 to further reduce the gap between speech and text in- puts (see \u00a74.4 for up-sampling factor ablations). For instance, the sequence \u201c a b\u201d becomes \u201c a a a a b b b b\u201d. This length adjust- ment approach has been shown to be effective for injecting text into speech models [37, 38]. We denote this up-sampled source text as X TEXT and define our MT data as triplets of up-sampled source text, source text, and target text {(X TEXT, Y SRC, Y TGT)}. To incorporate textual inputs into our discrete ST models, we simply extend the input vocabulary V CROSS = V SPE \u222a V SRC to include textual subword tokens V SRC from the source language in addition to the speech subword tokens V SPE. This modification correspond- ingly expands the embedding layer, but does not impact any other component in the architecture. Note that this joint speech-text vo- cabulary allows our models to indiscriminately ingest speech or text into any seq2seq model, sharing all non-embedding parameters be- tween ST and MT. The only modality-specific parameters are within the embedding layer, as speech and text tokens are still disjoint. Now MT multi-tasking can be achieved by simply combin- ing ST and MT training data: the training set consists of triplets {(X CROSS, Y SRC, Y TGT)} where X CROSS = {xl \u2208 V CROSS|l = 1, ..., L}. Note that the source text Y SRC and the target text Y TGT are identical for ST and MT examples. The same losses (described in the following section) are applied with equal weighting between the two tasks. All parameters are updated in each iteration. Models do not have any explicit sense of whether a particular example is an MT or ST task \u2013 all are processed in the same manner. 2.3. Seq2Seq Models In this work, we examine AED [39, 40], CTC [41], RNN-T [42], and CTC/Attn [43, 44] models. We use a hierarchical encoding scheme, as in [44], for all of our models. This method applies an ASR CTC objective at an intermediate encoder layer, denoted as LSRC CTC, and a second ST CTC objective at the final encoder layer, denoted as LTGT CTC. The ASR CTC objective allows our models to utilize source language transcriptions to improve encoder represen- tations [45, 46]. The ST CTC objective acts as a form of regular- ization which encourages encoder representations to be monotonic with respect to the target sequence; this has been shown to improve the translation quality of auto-regressive systems [44, 47]. Our AED and CTC/Attn models use an additional cross-entropy loss, denoted as LCE, while our RNN-T models use an additional RNN-T loss, denoted as LRNNT. AED and RNN-T models are jointly trained with CTC losses but CTC likelihoods are not applied during decod- ing. All told, our models are optimized using an interpolated loss defined as L = \u03bb1LSRC CTC + \u03bb2LTGT CTC + \u03bb3LCE/RNNT. We use \u03bb1 = \u03bb2 = 0.3 and \u03bb3 = 1 for our experiments. For CTC models, the last term is omitted and we use \u03bb1 = \u03bb2 = 1. 3. EXPERIMENTAL SETUP We compare"}, {"question": " How many hours of English speech is the WavLM model pre-trained on?", "answer": " 94k hours", "ref_chunk": "discrete units representing chunks of speech. There are many discretization techniques which can accom- plish this [18, 27\u201329]; we opt for the approach described by [14] which uses k-means clustering and SSL representations in a manner similar to HuBERT [30]. We first select an appropriate SSL model and intermediate layer to work with. In this work, we use WavLM which is pre-trained on 94k hours of English speech with masked prediction and de- noising objectives [22]. We select the 21st layer from WavLM as it has the highest canonical correlation analysis (CCA) similarity to word labels [31], suggesting that these representations contain use- ful semantic information. Next, we extract these representations for a portion of our training data and use them to train a k-means model with 2000 centroids. We then use this k-means model to convert the entire training set into sequences of k-means cluster assignments; note that at this point we have a sequence of discrete units but have only down-sampled the sequence length to 50 kHz via WavLM. As noted by prior works [14, 20], these sequences of k-means cluster assignments can be collapsed by removing repeats of the same consecutive unit. Finally, subword modeling can be applied to further reduce the sequence length [14]; we use the unigram al- gorithm from SentencePiece [32] to construct a vocabulary of 4000 tokens. To reduce over-fitting on particular segmentation patterns, we also apply BPE-dropout [33] during training. Ultimately, we ob- tain discrete speech sequences with an average length of 122 tokens from original audio with an average duration of 6.4 seconds. 2.2. ST/MT Multi-Tasking with Hard Parameter Sharing 2.2.1. ST from Discrete Speech Inputs With discrete speech inputs, the ST task seeks to map a sequence of tokens X DISC into another sequence of tokens Y TGT = {yTGT m \u2208 V TGT|l = 1, ..., M } where V TGT is the subword vocabulary for the target language \u2013 this is analogous to the MT task. We therefore re- place the convolutional feature extractor typically used by systems which process continuous speech inputs [34] with a learned embed- ding layer [35]. Following [14], we apply time-masking to the se- quences produced by the embedding layer; this is an additional form of data augmentation similar to SpecAugment [36] which is com- monly applied to continuous speech inputs. Our ST data is defined as a set of speech, source text, and target text triplets {(X DISC, Y SRC, Y TGT)}. Similar to target text, source text is a sequence of tokens Y SRC = {ySRC n \u2208 V SRC|l = 1, ..., N } where V SRC is the subword vocabulary for the source language. 2.2.2. Incorporating the MT Task Since the discrete speech sequences X DISC are still longer than their corresponding source text Y SRC, we repeat the source text tokens by a factor of 4 to further reduce the gap between speech and text in- puts (see \u00a74.4 for up-sampling factor ablations). For instance, the sequence \u201c a b\u201d becomes \u201c a a a a b b b b\u201d. This length adjust- ment approach has been shown to be effective for injecting text into speech models [37, 38]. We denote this up-sampled source text as X TEXT and define our MT data as triplets of up-sampled source text, source text, and target text {(X TEXT, Y SRC, Y TGT)}. To incorporate textual inputs into our discrete ST models, we simply extend the input vocabulary V CROSS = V SPE \u222a V SRC to include textual subword tokens V SRC from the source language in addition to the speech subword tokens V SPE. This modification correspond- ingly expands the embedding layer, but does not impact any other component in the architecture. Note that this joint speech-text vo- cabulary allows our models to indiscriminately ingest speech or text into any seq2seq model, sharing all non-embedding parameters be- tween ST and MT. The only modality-specific parameters are within the embedding layer, as speech and text tokens are still disjoint. Now MT multi-tasking can be achieved by simply combin- ing ST and MT training data: the training set consists of triplets {(X CROSS, Y SRC, Y TGT)} where X CROSS = {xl \u2208 V CROSS|l = 1, ..., L}. Note that the source text Y SRC and the target text Y TGT are identical for ST and MT examples. The same losses (described in the following section) are applied with equal weighting between the two tasks. All parameters are updated in each iteration. Models do not have any explicit sense of whether a particular example is an MT or ST task \u2013 all are processed in the same manner. 2.3. Seq2Seq Models In this work, we examine AED [39, 40], CTC [41], RNN-T [42], and CTC/Attn [43, 44] models. We use a hierarchical encoding scheme, as in [44], for all of our models. This method applies an ASR CTC objective at an intermediate encoder layer, denoted as LSRC CTC, and a second ST CTC objective at the final encoder layer, denoted as LTGT CTC. The ASR CTC objective allows our models to utilize source language transcriptions to improve encoder represen- tations [45, 46]. The ST CTC objective acts as a form of regular- ization which encourages encoder representations to be monotonic with respect to the target sequence; this has been shown to improve the translation quality of auto-regressive systems [44, 47]. Our AED and CTC/Attn models use an additional cross-entropy loss, denoted as LCE, while our RNN-T models use an additional RNN-T loss, denoted as LRNNT. AED and RNN-T models are jointly trained with CTC losses but CTC likelihoods are not applied during decod- ing. All told, our models are optimized using an interpolated loss defined as L = \u03bb1LSRC CTC + \u03bb2LTGT CTC + \u03bb3LCE/RNNT. We use \u03bb1 = \u03bb2 = 0.3 and \u03bb3 = 1 for our experiments. For CTC models, the last term is omitted and we use \u03bb1 = \u03bb2 = 1. 3. EXPERIMENTAL SETUP We compare"}, {"question": " How many centroids are used in the k-means model in the text?", "answer": " 2000", "ref_chunk": "discrete units representing chunks of speech. There are many discretization techniques which can accom- plish this [18, 27\u201329]; we opt for the approach described by [14] which uses k-means clustering and SSL representations in a manner similar to HuBERT [30]. We first select an appropriate SSL model and intermediate layer to work with. In this work, we use WavLM which is pre-trained on 94k hours of English speech with masked prediction and de- noising objectives [22]. We select the 21st layer from WavLM as it has the highest canonical correlation analysis (CCA) similarity to word labels [31], suggesting that these representations contain use- ful semantic information. Next, we extract these representations for a portion of our training data and use them to train a k-means model with 2000 centroids. We then use this k-means model to convert the entire training set into sequences of k-means cluster assignments; note that at this point we have a sequence of discrete units but have only down-sampled the sequence length to 50 kHz via WavLM. As noted by prior works [14, 20], these sequences of k-means cluster assignments can be collapsed by removing repeats of the same consecutive unit. Finally, subword modeling can be applied to further reduce the sequence length [14]; we use the unigram al- gorithm from SentencePiece [32] to construct a vocabulary of 4000 tokens. To reduce over-fitting on particular segmentation patterns, we also apply BPE-dropout [33] during training. Ultimately, we ob- tain discrete speech sequences with an average length of 122 tokens from original audio with an average duration of 6.4 seconds. 2.2. ST/MT Multi-Tasking with Hard Parameter Sharing 2.2.1. ST from Discrete Speech Inputs With discrete speech inputs, the ST task seeks to map a sequence of tokens X DISC into another sequence of tokens Y TGT = {yTGT m \u2208 V TGT|l = 1, ..., M } where V TGT is the subword vocabulary for the target language \u2013 this is analogous to the MT task. We therefore re- place the convolutional feature extractor typically used by systems which process continuous speech inputs [34] with a learned embed- ding layer [35]. Following [14], we apply time-masking to the se- quences produced by the embedding layer; this is an additional form of data augmentation similar to SpecAugment [36] which is com- monly applied to continuous speech inputs. Our ST data is defined as a set of speech, source text, and target text triplets {(X DISC, Y SRC, Y TGT)}. Similar to target text, source text is a sequence of tokens Y SRC = {ySRC n \u2208 V SRC|l = 1, ..., N } where V SRC is the subword vocabulary for the source language. 2.2.2. Incorporating the MT Task Since the discrete speech sequences X DISC are still longer than their corresponding source text Y SRC, we repeat the source text tokens by a factor of 4 to further reduce the gap between speech and text in- puts (see \u00a74.4 for up-sampling factor ablations). For instance, the sequence \u201c a b\u201d becomes \u201c a a a a b b b b\u201d. This length adjust- ment approach has been shown to be effective for injecting text into speech models [37, 38]. We denote this up-sampled source text as X TEXT and define our MT data as triplets of up-sampled source text, source text, and target text {(X TEXT, Y SRC, Y TGT)}. To incorporate textual inputs into our discrete ST models, we simply extend the input vocabulary V CROSS = V SPE \u222a V SRC to include textual subword tokens V SRC from the source language in addition to the speech subword tokens V SPE. This modification correspond- ingly expands the embedding layer, but does not impact any other component in the architecture. Note that this joint speech-text vo- cabulary allows our models to indiscriminately ingest speech or text into any seq2seq model, sharing all non-embedding parameters be- tween ST and MT. The only modality-specific parameters are within the embedding layer, as speech and text tokens are still disjoint. Now MT multi-tasking can be achieved by simply combin- ing ST and MT training data: the training set consists of triplets {(X CROSS, Y SRC, Y TGT)} where X CROSS = {xl \u2208 V CROSS|l = 1, ..., L}. Note that the source text Y SRC and the target text Y TGT are identical for ST and MT examples. The same losses (described in the following section) are applied with equal weighting between the two tasks. All parameters are updated in each iteration. Models do not have any explicit sense of whether a particular example is an MT or ST task \u2013 all are processed in the same manner. 2.3. Seq2Seq Models In this work, we examine AED [39, 40], CTC [41], RNN-T [42], and CTC/Attn [43, 44] models. We use a hierarchical encoding scheme, as in [44], for all of our models. This method applies an ASR CTC objective at an intermediate encoder layer, denoted as LSRC CTC, and a second ST CTC objective at the final encoder layer, denoted as LTGT CTC. The ASR CTC objective allows our models to utilize source language transcriptions to improve encoder represen- tations [45, 46]. The ST CTC objective acts as a form of regular- ization which encourages encoder representations to be monotonic with respect to the target sequence; this has been shown to improve the translation quality of auto-regressive systems [44, 47]. Our AED and CTC/Attn models use an additional cross-entropy loss, denoted as LCE, while our RNN-T models use an additional RNN-T loss, denoted as LRNNT. AED and RNN-T models are jointly trained with CTC losses but CTC likelihoods are not applied during decod- ing. All told, our models are optimized using an interpolated loss defined as L = \u03bb1LSRC CTC + \u03bb2LTGT CTC + \u03bb3LCE/RNNT. We use \u03bb1 = \u03bb2 = 0.3 and \u03bb3 = 1 for our experiments. For CTC models, the last term is omitted and we use \u03bb1 = \u03bb2 = 1. 3. EXPERIMENTAL SETUP We compare"}, {"question": " What algorithm is used to construct a vocabulary of 4000 tokens in the text?", "answer": " unigram algorithm from SentencePiece", "ref_chunk": "discrete units representing chunks of speech. There are many discretization techniques which can accom- plish this [18, 27\u201329]; we opt for the approach described by [14] which uses k-means clustering and SSL representations in a manner similar to HuBERT [30]. We first select an appropriate SSL model and intermediate layer to work with. In this work, we use WavLM which is pre-trained on 94k hours of English speech with masked prediction and de- noising objectives [22]. We select the 21st layer from WavLM as it has the highest canonical correlation analysis (CCA) similarity to word labels [31], suggesting that these representations contain use- ful semantic information. Next, we extract these representations for a portion of our training data and use them to train a k-means model with 2000 centroids. We then use this k-means model to convert the entire training set into sequences of k-means cluster assignments; note that at this point we have a sequence of discrete units but have only down-sampled the sequence length to 50 kHz via WavLM. As noted by prior works [14, 20], these sequences of k-means cluster assignments can be collapsed by removing repeats of the same consecutive unit. Finally, subword modeling can be applied to further reduce the sequence length [14]; we use the unigram al- gorithm from SentencePiece [32] to construct a vocabulary of 4000 tokens. To reduce over-fitting on particular segmentation patterns, we also apply BPE-dropout [33] during training. Ultimately, we ob- tain discrete speech sequences with an average length of 122 tokens from original audio with an average duration of 6.4 seconds. 2.2. ST/MT Multi-Tasking with Hard Parameter Sharing 2.2.1. ST from Discrete Speech Inputs With discrete speech inputs, the ST task seeks to map a sequence of tokens X DISC into another sequence of tokens Y TGT = {yTGT m \u2208 V TGT|l = 1, ..., M } where V TGT is the subword vocabulary for the target language \u2013 this is analogous to the MT task. We therefore re- place the convolutional feature extractor typically used by systems which process continuous speech inputs [34] with a learned embed- ding layer [35]. Following [14], we apply time-masking to the se- quences produced by the embedding layer; this is an additional form of data augmentation similar to SpecAugment [36] which is com- monly applied to continuous speech inputs. Our ST data is defined as a set of speech, source text, and target text triplets {(X DISC, Y SRC, Y TGT)}. Similar to target text, source text is a sequence of tokens Y SRC = {ySRC n \u2208 V SRC|l = 1, ..., N } where V SRC is the subword vocabulary for the source language. 2.2.2. Incorporating the MT Task Since the discrete speech sequences X DISC are still longer than their corresponding source text Y SRC, we repeat the source text tokens by a factor of 4 to further reduce the gap between speech and text in- puts (see \u00a74.4 for up-sampling factor ablations). For instance, the sequence \u201c a b\u201d becomes \u201c a a a a b b b b\u201d. This length adjust- ment approach has been shown to be effective for injecting text into speech models [37, 38]. We denote this up-sampled source text as X TEXT and define our MT data as triplets of up-sampled source text, source text, and target text {(X TEXT, Y SRC, Y TGT)}. To incorporate textual inputs into our discrete ST models, we simply extend the input vocabulary V CROSS = V SPE \u222a V SRC to include textual subword tokens V SRC from the source language in addition to the speech subword tokens V SPE. This modification correspond- ingly expands the embedding layer, but does not impact any other component in the architecture. Note that this joint speech-text vo- cabulary allows our models to indiscriminately ingest speech or text into any seq2seq model, sharing all non-embedding parameters be- tween ST and MT. The only modality-specific parameters are within the embedding layer, as speech and text tokens are still disjoint. Now MT multi-tasking can be achieved by simply combin- ing ST and MT training data: the training set consists of triplets {(X CROSS, Y SRC, Y TGT)} where X CROSS = {xl \u2208 V CROSS|l = 1, ..., L}. Note that the source text Y SRC and the target text Y TGT are identical for ST and MT examples. The same losses (described in the following section) are applied with equal weighting between the two tasks. All parameters are updated in each iteration. Models do not have any explicit sense of whether a particular example is an MT or ST task \u2013 all are processed in the same manner. 2.3. Seq2Seq Models In this work, we examine AED [39, 40], CTC [41], RNN-T [42], and CTC/Attn [43, 44] models. We use a hierarchical encoding scheme, as in [44], for all of our models. This method applies an ASR CTC objective at an intermediate encoder layer, denoted as LSRC CTC, and a second ST CTC objective at the final encoder layer, denoted as LTGT CTC. The ASR CTC objective allows our models to utilize source language transcriptions to improve encoder represen- tations [45, 46]. The ST CTC objective acts as a form of regular- ization which encourages encoder representations to be monotonic with respect to the target sequence; this has been shown to improve the translation quality of auto-regressive systems [44, 47]. Our AED and CTC/Attn models use an additional cross-entropy loss, denoted as LCE, while our RNN-T models use an additional RNN-T loss, denoted as LRNNT. AED and RNN-T models are jointly trained with CTC losses but CTC likelihoods are not applied during decod- ing. All told, our models are optimized using an interpolated loss defined as L = \u03bb1LSRC CTC + \u03bb2LTGT CTC + \u03bb3LCE/RNNT. We use \u03bb1 = \u03bb2 = 0.3 and \u03bb3 = 1 for our experiments. For CTC models, the last term is omitted and we use \u03bb1 = \u03bb2 = 1. 3. EXPERIMENTAL SETUP We compare"}, {"question": " What is the average length of the discrete speech sequences obtained in the text?", "answer": " 122 tokens", "ref_chunk": "discrete units representing chunks of speech. There are many discretization techniques which can accom- plish this [18, 27\u201329]; we opt for the approach described by [14] which uses k-means clustering and SSL representations in a manner similar to HuBERT [30]. We first select an appropriate SSL model and intermediate layer to work with. In this work, we use WavLM which is pre-trained on 94k hours of English speech with masked prediction and de- noising objectives [22]. We select the 21st layer from WavLM as it has the highest canonical correlation analysis (CCA) similarity to word labels [31], suggesting that these representations contain use- ful semantic information. Next, we extract these representations for a portion of our training data and use them to train a k-means model with 2000 centroids. We then use this k-means model to convert the entire training set into sequences of k-means cluster assignments; note that at this point we have a sequence of discrete units but have only down-sampled the sequence length to 50 kHz via WavLM. As noted by prior works [14, 20], these sequences of k-means cluster assignments can be collapsed by removing repeats of the same consecutive unit. Finally, subword modeling can be applied to further reduce the sequence length [14]; we use the unigram al- gorithm from SentencePiece [32] to construct a vocabulary of 4000 tokens. To reduce over-fitting on particular segmentation patterns, we also apply BPE-dropout [33] during training. Ultimately, we ob- tain discrete speech sequences with an average length of 122 tokens from original audio with an average duration of 6.4 seconds. 2.2. ST/MT Multi-Tasking with Hard Parameter Sharing 2.2.1. ST from Discrete Speech Inputs With discrete speech inputs, the ST task seeks to map a sequence of tokens X DISC into another sequence of tokens Y TGT = {yTGT m \u2208 V TGT|l = 1, ..., M } where V TGT is the subword vocabulary for the target language \u2013 this is analogous to the MT task. We therefore re- place the convolutional feature extractor typically used by systems which process continuous speech inputs [34] with a learned embed- ding layer [35]. Following [14], we apply time-masking to the se- quences produced by the embedding layer; this is an additional form of data augmentation similar to SpecAugment [36] which is com- monly applied to continuous speech inputs. Our ST data is defined as a set of speech, source text, and target text triplets {(X DISC, Y SRC, Y TGT)}. Similar to target text, source text is a sequence of tokens Y SRC = {ySRC n \u2208 V SRC|l = 1, ..., N } where V SRC is the subword vocabulary for the source language. 2.2.2. Incorporating the MT Task Since the discrete speech sequences X DISC are still longer than their corresponding source text Y SRC, we repeat the source text tokens by a factor of 4 to further reduce the gap between speech and text in- puts (see \u00a74.4 for up-sampling factor ablations). For instance, the sequence \u201c a b\u201d becomes \u201c a a a a b b b b\u201d. This length adjust- ment approach has been shown to be effective for injecting text into speech models [37, 38]. We denote this up-sampled source text as X TEXT and define our MT data as triplets of up-sampled source text, source text, and target text {(X TEXT, Y SRC, Y TGT)}. To incorporate textual inputs into our discrete ST models, we simply extend the input vocabulary V CROSS = V SPE \u222a V SRC to include textual subword tokens V SRC from the source language in addition to the speech subword tokens V SPE. This modification correspond- ingly expands the embedding layer, but does not impact any other component in the architecture. Note that this joint speech-text vo- cabulary allows our models to indiscriminately ingest speech or text into any seq2seq model, sharing all non-embedding parameters be- tween ST and MT. The only modality-specific parameters are within the embedding layer, as speech and text tokens are still disjoint. Now MT multi-tasking can be achieved by simply combin- ing ST and MT training data: the training set consists of triplets {(X CROSS, Y SRC, Y TGT)} where X CROSS = {xl \u2208 V CROSS|l = 1, ..., L}. Note that the source text Y SRC and the target text Y TGT are identical for ST and MT examples. The same losses (described in the following section) are applied with equal weighting between the two tasks. All parameters are updated in each iteration. Models do not have any explicit sense of whether a particular example is an MT or ST task \u2013 all are processed in the same manner. 2.3. Seq2Seq Models In this work, we examine AED [39, 40], CTC [41], RNN-T [42], and CTC/Attn [43, 44] models. We use a hierarchical encoding scheme, as in [44], for all of our models. This method applies an ASR CTC objective at an intermediate encoder layer, denoted as LSRC CTC, and a second ST CTC objective at the final encoder layer, denoted as LTGT CTC. The ASR CTC objective allows our models to utilize source language transcriptions to improve encoder represen- tations [45, 46]. The ST CTC objective acts as a form of regular- ization which encourages encoder representations to be monotonic with respect to the target sequence; this has been shown to improve the translation quality of auto-regressive systems [44, 47]. Our AED and CTC/Attn models use an additional cross-entropy loss, denoted as LCE, while our RNN-T models use an additional RNN-T loss, denoted as LRNNT. AED and RNN-T models are jointly trained with CTC losses but CTC likelihoods are not applied during decod- ing. All told, our models are optimized using an interpolated loss defined as L = \u03bb1LSRC CTC + \u03bb2LTGT CTC + \u03bb3LCE/RNNT. We use \u03bb1 = \u03bb2 = 0.3 and \u03bb3 = 1 for our experiments. For CTC models, the last term is omitted and we use \u03bb1 = \u03bb2 = 1. 3. EXPERIMENTAL SETUP We compare"}, {"question": " What task seeks to map a sequence of tokens X DISC into another sequence of tokens Y TGT?", "answer": " ST task", "ref_chunk": "discrete units representing chunks of speech. There are many discretization techniques which can accom- plish this [18, 27\u201329]; we opt for the approach described by [14] which uses k-means clustering and SSL representations in a manner similar to HuBERT [30]. We first select an appropriate SSL model and intermediate layer to work with. In this work, we use WavLM which is pre-trained on 94k hours of English speech with masked prediction and de- noising objectives [22]. We select the 21st layer from WavLM as it has the highest canonical correlation analysis (CCA) similarity to word labels [31], suggesting that these representations contain use- ful semantic information. Next, we extract these representations for a portion of our training data and use them to train a k-means model with 2000 centroids. We then use this k-means model to convert the entire training set into sequences of k-means cluster assignments; note that at this point we have a sequence of discrete units but have only down-sampled the sequence length to 50 kHz via WavLM. As noted by prior works [14, 20], these sequences of k-means cluster assignments can be collapsed by removing repeats of the same consecutive unit. Finally, subword modeling can be applied to further reduce the sequence length [14]; we use the unigram al- gorithm from SentencePiece [32] to construct a vocabulary of 4000 tokens. To reduce over-fitting on particular segmentation patterns, we also apply BPE-dropout [33] during training. Ultimately, we ob- tain discrete speech sequences with an average length of 122 tokens from original audio with an average duration of 6.4 seconds. 2.2. ST/MT Multi-Tasking with Hard Parameter Sharing 2.2.1. ST from Discrete Speech Inputs With discrete speech inputs, the ST task seeks to map a sequence of tokens X DISC into another sequence of tokens Y TGT = {yTGT m \u2208 V TGT|l = 1, ..., M } where V TGT is the subword vocabulary for the target language \u2013 this is analogous to the MT task. We therefore re- place the convolutional feature extractor typically used by systems which process continuous speech inputs [34] with a learned embed- ding layer [35]. Following [14], we apply time-masking to the se- quences produced by the embedding layer; this is an additional form of data augmentation similar to SpecAugment [36] which is com- monly applied to continuous speech inputs. Our ST data is defined as a set of speech, source text, and target text triplets {(X DISC, Y SRC, Y TGT)}. Similar to target text, source text is a sequence of tokens Y SRC = {ySRC n \u2208 V SRC|l = 1, ..., N } where V SRC is the subword vocabulary for the source language. 2.2.2. Incorporating the MT Task Since the discrete speech sequences X DISC are still longer than their corresponding source text Y SRC, we repeat the source text tokens by a factor of 4 to further reduce the gap between speech and text in- puts (see \u00a74.4 for up-sampling factor ablations). For instance, the sequence \u201c a b\u201d becomes \u201c a a a a b b b b\u201d. This length adjust- ment approach has been shown to be effective for injecting text into speech models [37, 38]. We denote this up-sampled source text as X TEXT and define our MT data as triplets of up-sampled source text, source text, and target text {(X TEXT, Y SRC, Y TGT)}. To incorporate textual inputs into our discrete ST models, we simply extend the input vocabulary V CROSS = V SPE \u222a V SRC to include textual subword tokens V SRC from the source language in addition to the speech subword tokens V SPE. This modification correspond- ingly expands the embedding layer, but does not impact any other component in the architecture. Note that this joint speech-text vo- cabulary allows our models to indiscriminately ingest speech or text into any seq2seq model, sharing all non-embedding parameters be- tween ST and MT. The only modality-specific parameters are within the embedding layer, as speech and text tokens are still disjoint. Now MT multi-tasking can be achieved by simply combin- ing ST and MT training data: the training set consists of triplets {(X CROSS, Y SRC, Y TGT)} where X CROSS = {xl \u2208 V CROSS|l = 1, ..., L}. Note that the source text Y SRC and the target text Y TGT are identical for ST and MT examples. The same losses (described in the following section) are applied with equal weighting between the two tasks. All parameters are updated in each iteration. Models do not have any explicit sense of whether a particular example is an MT or ST task \u2013 all are processed in the same manner. 2.3. Seq2Seq Models In this work, we examine AED [39, 40], CTC [41], RNN-T [42], and CTC/Attn [43, 44] models. We use a hierarchical encoding scheme, as in [44], for all of our models. This method applies an ASR CTC objective at an intermediate encoder layer, denoted as LSRC CTC, and a second ST CTC objective at the final encoder layer, denoted as LTGT CTC. The ASR CTC objective allows our models to utilize source language transcriptions to improve encoder represen- tations [45, 46]. The ST CTC objective acts as a form of regular- ization which encourages encoder representations to be monotonic with respect to the target sequence; this has been shown to improve the translation quality of auto-regressive systems [44, 47]. Our AED and CTC/Attn models use an additional cross-entropy loss, denoted as LCE, while our RNN-T models use an additional RNN-T loss, denoted as LRNNT. AED and RNN-T models are jointly trained with CTC losses but CTC likelihoods are not applied during decod- ing. All told, our models are optimized using an interpolated loss defined as L = \u03bb1LSRC CTC + \u03bb2LTGT CTC + \u03bb3LCE/RNNT. We use \u03bb1 = \u03bb2 = 0.3 and \u03bb3 = 1 for our experiments. For CTC models, the last term is omitted and we use \u03bb1 = \u03bb2 = 1. 3. EXPERIMENTAL SETUP We compare"}, {"question": " What form of data augmentation is applied to the sequences produced by the embedding layer in the text?", "answer": " time-masking", "ref_chunk": "discrete units representing chunks of speech. There are many discretization techniques which can accom- plish this [18, 27\u201329]; we opt for the approach described by [14] which uses k-means clustering and SSL representations in a manner similar to HuBERT [30]. We first select an appropriate SSL model and intermediate layer to work with. In this work, we use WavLM which is pre-trained on 94k hours of English speech with masked prediction and de- noising objectives [22]. We select the 21st layer from WavLM as it has the highest canonical correlation analysis (CCA) similarity to word labels [31], suggesting that these representations contain use- ful semantic information. Next, we extract these representations for a portion of our training data and use them to train a k-means model with 2000 centroids. We then use this k-means model to convert the entire training set into sequences of k-means cluster assignments; note that at this point we have a sequence of discrete units but have only down-sampled the sequence length to 50 kHz via WavLM. As noted by prior works [14, 20], these sequences of k-means cluster assignments can be collapsed by removing repeats of the same consecutive unit. Finally, subword modeling can be applied to further reduce the sequence length [14]; we use the unigram al- gorithm from SentencePiece [32] to construct a vocabulary of 4000 tokens. To reduce over-fitting on particular segmentation patterns, we also apply BPE-dropout [33] during training. Ultimately, we ob- tain discrete speech sequences with an average length of 122 tokens from original audio with an average duration of 6.4 seconds. 2.2. ST/MT Multi-Tasking with Hard Parameter Sharing 2.2.1. ST from Discrete Speech Inputs With discrete speech inputs, the ST task seeks to map a sequence of tokens X DISC into another sequence of tokens Y TGT = {yTGT m \u2208 V TGT|l = 1, ..., M } where V TGT is the subword vocabulary for the target language \u2013 this is analogous to the MT task. We therefore re- place the convolutional feature extractor typically used by systems which process continuous speech inputs [34] with a learned embed- ding layer [35]. Following [14], we apply time-masking to the se- quences produced by the embedding layer; this is an additional form of data augmentation similar to SpecAugment [36] which is com- monly applied to continuous speech inputs. Our ST data is defined as a set of speech, source text, and target text triplets {(X DISC, Y SRC, Y TGT)}. Similar to target text, source text is a sequence of tokens Y SRC = {ySRC n \u2208 V SRC|l = 1, ..., N } where V SRC is the subword vocabulary for the source language. 2.2.2. Incorporating the MT Task Since the discrete speech sequences X DISC are still longer than their corresponding source text Y SRC, we repeat the source text tokens by a factor of 4 to further reduce the gap between speech and text in- puts (see \u00a74.4 for up-sampling factor ablations). For instance, the sequence \u201c a b\u201d becomes \u201c a a a a b b b b\u201d. This length adjust- ment approach has been shown to be effective for injecting text into speech models [37, 38]. We denote this up-sampled source text as X TEXT and define our MT data as triplets of up-sampled source text, source text, and target text {(X TEXT, Y SRC, Y TGT)}. To incorporate textual inputs into our discrete ST models, we simply extend the input vocabulary V CROSS = V SPE \u222a V SRC to include textual subword tokens V SRC from the source language in addition to the speech subword tokens V SPE. This modification correspond- ingly expands the embedding layer, but does not impact any other component in the architecture. Note that this joint speech-text vo- cabulary allows our models to indiscriminately ingest speech or text into any seq2seq model, sharing all non-embedding parameters be- tween ST and MT. The only modality-specific parameters are within the embedding layer, as speech and text tokens are still disjoint. Now MT multi-tasking can be achieved by simply combin- ing ST and MT training data: the training set consists of triplets {(X CROSS, Y SRC, Y TGT)} where X CROSS = {xl \u2208 V CROSS|l = 1, ..., L}. Note that the source text Y SRC and the target text Y TGT are identical for ST and MT examples. The same losses (described in the following section) are applied with equal weighting between the two tasks. All parameters are updated in each iteration. Models do not have any explicit sense of whether a particular example is an MT or ST task \u2013 all are processed in the same manner. 2.3. Seq2Seq Models In this work, we examine AED [39, 40], CTC [41], RNN-T [42], and CTC/Attn [43, 44] models. We use a hierarchical encoding scheme, as in [44], for all of our models. This method applies an ASR CTC objective at an intermediate encoder layer, denoted as LSRC CTC, and a second ST CTC objective at the final encoder layer, denoted as LTGT CTC. The ASR CTC objective allows our models to utilize source language transcriptions to improve encoder represen- tations [45, 46]. The ST CTC objective acts as a form of regular- ization which encourages encoder representations to be monotonic with respect to the target sequence; this has been shown to improve the translation quality of auto-regressive systems [44, 47]. Our AED and CTC/Attn models use an additional cross-entropy loss, denoted as LCE, while our RNN-T models use an additional RNN-T loss, denoted as LRNNT. AED and RNN-T models are jointly trained with CTC losses but CTC likelihoods are not applied during decod- ing. All told, our models are optimized using an interpolated loss defined as L = \u03bb1LSRC CTC + \u03bb2LTGT CTC + \u03bb3LCE/RNNT. We use \u03bb1 = \u03bb2 = 0.3 and \u03bb3 = 1 for our experiments. For CTC models, the last term is omitted and we use \u03bb1 = \u03bb2 = 1. 3. EXPERIMENTAL SETUP We compare"}, {"question": " How many times are the source text tokens repeated to reduce the gap between speech and text inputs?", "answer": " 4 times", "ref_chunk": "discrete units representing chunks of speech. There are many discretization techniques which can accom- plish this [18, 27\u201329]; we opt for the approach described by [14] which uses k-means clustering and SSL representations in a manner similar to HuBERT [30]. We first select an appropriate SSL model and intermediate layer to work with. In this work, we use WavLM which is pre-trained on 94k hours of English speech with masked prediction and de- noising objectives [22]. We select the 21st layer from WavLM as it has the highest canonical correlation analysis (CCA) similarity to word labels [31], suggesting that these representations contain use- ful semantic information. Next, we extract these representations for a portion of our training data and use them to train a k-means model with 2000 centroids. We then use this k-means model to convert the entire training set into sequences of k-means cluster assignments; note that at this point we have a sequence of discrete units but have only down-sampled the sequence length to 50 kHz via WavLM. As noted by prior works [14, 20], these sequences of k-means cluster assignments can be collapsed by removing repeats of the same consecutive unit. Finally, subword modeling can be applied to further reduce the sequence length [14]; we use the unigram al- gorithm from SentencePiece [32] to construct a vocabulary of 4000 tokens. To reduce over-fitting on particular segmentation patterns, we also apply BPE-dropout [33] during training. Ultimately, we ob- tain discrete speech sequences with an average length of 122 tokens from original audio with an average duration of 6.4 seconds. 2.2. ST/MT Multi-Tasking with Hard Parameter Sharing 2.2.1. ST from Discrete Speech Inputs With discrete speech inputs, the ST task seeks to map a sequence of tokens X DISC into another sequence of tokens Y TGT = {yTGT m \u2208 V TGT|l = 1, ..., M } where V TGT is the subword vocabulary for the target language \u2013 this is analogous to the MT task. We therefore re- place the convolutional feature extractor typically used by systems which process continuous speech inputs [34] with a learned embed- ding layer [35]. Following [14], we apply time-masking to the se- quences produced by the embedding layer; this is an additional form of data augmentation similar to SpecAugment [36] which is com- monly applied to continuous speech inputs. Our ST data is defined as a set of speech, source text, and target text triplets {(X DISC, Y SRC, Y TGT)}. Similar to target text, source text is a sequence of tokens Y SRC = {ySRC n \u2208 V SRC|l = 1, ..., N } where V SRC is the subword vocabulary for the source language. 2.2.2. Incorporating the MT Task Since the discrete speech sequences X DISC are still longer than their corresponding source text Y SRC, we repeat the source text tokens by a factor of 4 to further reduce the gap between speech and text in- puts (see \u00a74.4 for up-sampling factor ablations). For instance, the sequence \u201c a b\u201d becomes \u201c a a a a b b b b\u201d. This length adjust- ment approach has been shown to be effective for injecting text into speech models [37, 38]. We denote this up-sampled source text as X TEXT and define our MT data as triplets of up-sampled source text, source text, and target text {(X TEXT, Y SRC, Y TGT)}. To incorporate textual inputs into our discrete ST models, we simply extend the input vocabulary V CROSS = V SPE \u222a V SRC to include textual subword tokens V SRC from the source language in addition to the speech subword tokens V SPE. This modification correspond- ingly expands the embedding layer, but does not impact any other component in the architecture. Note that this joint speech-text vo- cabulary allows our models to indiscriminately ingest speech or text into any seq2seq model, sharing all non-embedding parameters be- tween ST and MT. The only modality-specific parameters are within the embedding layer, as speech and text tokens are still disjoint. Now MT multi-tasking can be achieved by simply combin- ing ST and MT training data: the training set consists of triplets {(X CROSS, Y SRC, Y TGT)} where X CROSS = {xl \u2208 V CROSS|l = 1, ..., L}. Note that the source text Y SRC and the target text Y TGT are identical for ST and MT examples. The same losses (described in the following section) are applied with equal weighting between the two tasks. All parameters are updated in each iteration. Models do not have any explicit sense of whether a particular example is an MT or ST task \u2013 all are processed in the same manner. 2.3. Seq2Seq Models In this work, we examine AED [39, 40], CTC [41], RNN-T [42], and CTC/Attn [43, 44] models. We use a hierarchical encoding scheme, as in [44], for all of our models. This method applies an ASR CTC objective at an intermediate encoder layer, denoted as LSRC CTC, and a second ST CTC objective at the final encoder layer, denoted as LTGT CTC. The ASR CTC objective allows our models to utilize source language transcriptions to improve encoder represen- tations [45, 46]. The ST CTC objective acts as a form of regular- ization which encourages encoder representations to be monotonic with respect to the target sequence; this has been shown to improve the translation quality of auto-regressive systems [44, 47]. Our AED and CTC/Attn models use an additional cross-entropy loss, denoted as LCE, while our RNN-T models use an additional RNN-T loss, denoted as LRNNT. AED and RNN-T models are jointly trained with CTC losses but CTC likelihoods are not applied during decod- ing. All told, our models are optimized using an interpolated loss defined as L = \u03bb1LSRC CTC + \u03bb2LTGT CTC + \u03bb3LCE/RNNT. We use \u03bb1 = \u03bb2 = 0.3 and \u03bb3 = 1 for our experiments. For CTC models, the last term is omitted and we use \u03bb1 = \u03bb2 = 1. 3. EXPERIMENTAL SETUP We compare"}, {"question": " What does the up-sampled source text X TEXT represent in the text?", "answer": " sequence of up-sampled source text tokens", "ref_chunk": "discrete units representing chunks of speech. There are many discretization techniques which can accom- plish this [18, 27\u201329]; we opt for the approach described by [14] which uses k-means clustering and SSL representations in a manner similar to HuBERT [30]. We first select an appropriate SSL model and intermediate layer to work with. In this work, we use WavLM which is pre-trained on 94k hours of English speech with masked prediction and de- noising objectives [22]. We select the 21st layer from WavLM as it has the highest canonical correlation analysis (CCA) similarity to word labels [31], suggesting that these representations contain use- ful semantic information. Next, we extract these representations for a portion of our training data and use them to train a k-means model with 2000 centroids. We then use this k-means model to convert the entire training set into sequences of k-means cluster assignments; note that at this point we have a sequence of discrete units but have only down-sampled the sequence length to 50 kHz via WavLM. As noted by prior works [14, 20], these sequences of k-means cluster assignments can be collapsed by removing repeats of the same consecutive unit. Finally, subword modeling can be applied to further reduce the sequence length [14]; we use the unigram al- gorithm from SentencePiece [32] to construct a vocabulary of 4000 tokens. To reduce over-fitting on particular segmentation patterns, we also apply BPE-dropout [33] during training. Ultimately, we ob- tain discrete speech sequences with an average length of 122 tokens from original audio with an average duration of 6.4 seconds. 2.2. ST/MT Multi-Tasking with Hard Parameter Sharing 2.2.1. ST from Discrete Speech Inputs With discrete speech inputs, the ST task seeks to map a sequence of tokens X DISC into another sequence of tokens Y TGT = {yTGT m \u2208 V TGT|l = 1, ..., M } where V TGT is the subword vocabulary for the target language \u2013 this is analogous to the MT task. We therefore re- place the convolutional feature extractor typically used by systems which process continuous speech inputs [34] with a learned embed- ding layer [35]. Following [14], we apply time-masking to the se- quences produced by the embedding layer; this is an additional form of data augmentation similar to SpecAugment [36] which is com- monly applied to continuous speech inputs. Our ST data is defined as a set of speech, source text, and target text triplets {(X DISC, Y SRC, Y TGT)}. Similar to target text, source text is a sequence of tokens Y SRC = {ySRC n \u2208 V SRC|l = 1, ..., N } where V SRC is the subword vocabulary for the source language. 2.2.2. Incorporating the MT Task Since the discrete speech sequences X DISC are still longer than their corresponding source text Y SRC, we repeat the source text tokens by a factor of 4 to further reduce the gap between speech and text in- puts (see \u00a74.4 for up-sampling factor ablations). For instance, the sequence \u201c a b\u201d becomes \u201c a a a a b b b b\u201d. This length adjust- ment approach has been shown to be effective for injecting text into speech models [37, 38]. We denote this up-sampled source text as X TEXT and define our MT data as triplets of up-sampled source text, source text, and target text {(X TEXT, Y SRC, Y TGT)}. To incorporate textual inputs into our discrete ST models, we simply extend the input vocabulary V CROSS = V SPE \u222a V SRC to include textual subword tokens V SRC from the source language in addition to the speech subword tokens V SPE. This modification correspond- ingly expands the embedding layer, but does not impact any other component in the architecture. Note that this joint speech-text vo- cabulary allows our models to indiscriminately ingest speech or text into any seq2seq model, sharing all non-embedding parameters be- tween ST and MT. The only modality-specific parameters are within the embedding layer, as speech and text tokens are still disjoint. Now MT multi-tasking can be achieved by simply combin- ing ST and MT training data: the training set consists of triplets {(X CROSS, Y SRC, Y TGT)} where X CROSS = {xl \u2208 V CROSS|l = 1, ..., L}. Note that the source text Y SRC and the target text Y TGT are identical for ST and MT examples. The same losses (described in the following section) are applied with equal weighting between the two tasks. All parameters are updated in each iteration. Models do not have any explicit sense of whether a particular example is an MT or ST task \u2013 all are processed in the same manner. 2.3. Seq2Seq Models In this work, we examine AED [39, 40], CTC [41], RNN-T [42], and CTC/Attn [43, 44] models. We use a hierarchical encoding scheme, as in [44], for all of our models. This method applies an ASR CTC objective at an intermediate encoder layer, denoted as LSRC CTC, and a second ST CTC objective at the final encoder layer, denoted as LTGT CTC. The ASR CTC objective allows our models to utilize source language transcriptions to improve encoder represen- tations [45, 46]. The ST CTC objective acts as a form of regular- ization which encourages encoder representations to be monotonic with respect to the target sequence; this has been shown to improve the translation quality of auto-regressive systems [44, 47]. Our AED and CTC/Attn models use an additional cross-entropy loss, denoted as LCE, while our RNN-T models use an additional RNN-T loss, denoted as LRNNT. AED and RNN-T models are jointly trained with CTC losses but CTC likelihoods are not applied during decod- ing. All told, our models are optimized using an interpolated loss defined as L = \u03bb1LSRC CTC + \u03bb2LTGT CTC + \u03bb3LCE/RNNT. We use \u03bb1 = \u03bb2 = 0.3 and \u03bb3 = 1 for our experiments. For CTC models, the last term is omitted and we use \u03bb1 = \u03bb2 = 1. 3. EXPERIMENTAL SETUP We compare"}, {"question": " What loss function is used for optimizing the models in the text?", "answer": " interpolated loss", "ref_chunk": "discrete units representing chunks of speech. There are many discretization techniques which can accom- plish this [18, 27\u201329]; we opt for the approach described by [14] which uses k-means clustering and SSL representations in a manner similar to HuBERT [30]. We first select an appropriate SSL model and intermediate layer to work with. In this work, we use WavLM which is pre-trained on 94k hours of English speech with masked prediction and de- noising objectives [22]. We select the 21st layer from WavLM as it has the highest canonical correlation analysis (CCA) similarity to word labels [31], suggesting that these representations contain use- ful semantic information. Next, we extract these representations for a portion of our training data and use them to train a k-means model with 2000 centroids. We then use this k-means model to convert the entire training set into sequences of k-means cluster assignments; note that at this point we have a sequence of discrete units but have only down-sampled the sequence length to 50 kHz via WavLM. As noted by prior works [14, 20], these sequences of k-means cluster assignments can be collapsed by removing repeats of the same consecutive unit. Finally, subword modeling can be applied to further reduce the sequence length [14]; we use the unigram al- gorithm from SentencePiece [32] to construct a vocabulary of 4000 tokens. To reduce over-fitting on particular segmentation patterns, we also apply BPE-dropout [33] during training. Ultimately, we ob- tain discrete speech sequences with an average length of 122 tokens from original audio with an average duration of 6.4 seconds. 2.2. ST/MT Multi-Tasking with Hard Parameter Sharing 2.2.1. ST from Discrete Speech Inputs With discrete speech inputs, the ST task seeks to map a sequence of tokens X DISC into another sequence of tokens Y TGT = {yTGT m \u2208 V TGT|l = 1, ..., M } where V TGT is the subword vocabulary for the target language \u2013 this is analogous to the MT task. We therefore re- place the convolutional feature extractor typically used by systems which process continuous speech inputs [34] with a learned embed- ding layer [35]. Following [14], we apply time-masking to the se- quences produced by the embedding layer; this is an additional form of data augmentation similar to SpecAugment [36] which is com- monly applied to continuous speech inputs. Our ST data is defined as a set of speech, source text, and target text triplets {(X DISC, Y SRC, Y TGT)}. Similar to target text, source text is a sequence of tokens Y SRC = {ySRC n \u2208 V SRC|l = 1, ..., N } where V SRC is the subword vocabulary for the source language. 2.2.2. Incorporating the MT Task Since the discrete speech sequences X DISC are still longer than their corresponding source text Y SRC, we repeat the source text tokens by a factor of 4 to further reduce the gap between speech and text in- puts (see \u00a74.4 for up-sampling factor ablations). For instance, the sequence \u201c a b\u201d becomes \u201c a a a a b b b b\u201d. This length adjust- ment approach has been shown to be effective for injecting text into speech models [37, 38]. We denote this up-sampled source text as X TEXT and define our MT data as triplets of up-sampled source text, source text, and target text {(X TEXT, Y SRC, Y TGT)}. To incorporate textual inputs into our discrete ST models, we simply extend the input vocabulary V CROSS = V SPE \u222a V SRC to include textual subword tokens V SRC from the source language in addition to the speech subword tokens V SPE. This modification correspond- ingly expands the embedding layer, but does not impact any other component in the architecture. Note that this joint speech-text vo- cabulary allows our models to indiscriminately ingest speech or text into any seq2seq model, sharing all non-embedding parameters be- tween ST and MT. The only modality-specific parameters are within the embedding layer, as speech and text tokens are still disjoint. Now MT multi-tasking can be achieved by simply combin- ing ST and MT training data: the training set consists of triplets {(X CROSS, Y SRC, Y TGT)} where X CROSS = {xl \u2208 V CROSS|l = 1, ..., L}. Note that the source text Y SRC and the target text Y TGT are identical for ST and MT examples. The same losses (described in the following section) are applied with equal weighting between the two tasks. All parameters are updated in each iteration. Models do not have any explicit sense of whether a particular example is an MT or ST task \u2013 all are processed in the same manner. 2.3. Seq2Seq Models In this work, we examine AED [39, 40], CTC [41], RNN-T [42], and CTC/Attn [43, 44] models. We use a hierarchical encoding scheme, as in [44], for all of our models. This method applies an ASR CTC objective at an intermediate encoder layer, denoted as LSRC CTC, and a second ST CTC objective at the final encoder layer, denoted as LTGT CTC. The ASR CTC objective allows our models to utilize source language transcriptions to improve encoder represen- tations [45, 46]. The ST CTC objective acts as a form of regular- ization which encourages encoder representations to be monotonic with respect to the target sequence; this has been shown to improve the translation quality of auto-regressive systems [44, 47]. Our AED and CTC/Attn models use an additional cross-entropy loss, denoted as LCE, while our RNN-T models use an additional RNN-T loss, denoted as LRNNT. AED and RNN-T models are jointly trained with CTC losses but CTC likelihoods are not applied during decod- ing. All told, our models are optimized using an interpolated loss defined as L = \u03bb1LSRC CTC + \u03bb2LTGT CTC + \u03bb3LCE/RNNT. We use \u03bb1 = \u03bb2 = 0.3 and \u03bb3 = 1 for our experiments. For CTC models, the last term is omitted and we use \u03bb1 = \u03bb2 = 1. 3. EXPERIMENTAL SETUP We compare"}], "doc_text": "discrete units representing chunks of speech. There are many discretization techniques which can accom- plish this [18, 27\u201329]; we opt for the approach described by [14] which uses k-means clustering and SSL representations in a manner similar to HuBERT [30]. We first select an appropriate SSL model and intermediate layer to work with. In this work, we use WavLM which is pre-trained on 94k hours of English speech with masked prediction and de- noising objectives [22]. We select the 21st layer from WavLM as it has the highest canonical correlation analysis (CCA) similarity to word labels [31], suggesting that these representations contain use- ful semantic information. Next, we extract these representations for a portion of our training data and use them to train a k-means model with 2000 centroids. We then use this k-means model to convert the entire training set into sequences of k-means cluster assignments; note that at this point we have a sequence of discrete units but have only down-sampled the sequence length to 50 kHz via WavLM. As noted by prior works [14, 20], these sequences of k-means cluster assignments can be collapsed by removing repeats of the same consecutive unit. Finally, subword modeling can be applied to further reduce the sequence length [14]; we use the unigram al- gorithm from SentencePiece [32] to construct a vocabulary of 4000 tokens. To reduce over-fitting on particular segmentation patterns, we also apply BPE-dropout [33] during training. Ultimately, we ob- tain discrete speech sequences with an average length of 122 tokens from original audio with an average duration of 6.4 seconds. 2.2. ST/MT Multi-Tasking with Hard Parameter Sharing 2.2.1. ST from Discrete Speech Inputs With discrete speech inputs, the ST task seeks to map a sequence of tokens X DISC into another sequence of tokens Y TGT = {yTGT m \u2208 V TGT|l = 1, ..., M } where V TGT is the subword vocabulary for the target language \u2013 this is analogous to the MT task. We therefore re- place the convolutional feature extractor typically used by systems which process continuous speech inputs [34] with a learned embed- ding layer [35]. Following [14], we apply time-masking to the se- quences produced by the embedding layer; this is an additional form of data augmentation similar to SpecAugment [36] which is com- monly applied to continuous speech inputs. Our ST data is defined as a set of speech, source text, and target text triplets {(X DISC, Y SRC, Y TGT)}. Similar to target text, source text is a sequence of tokens Y SRC = {ySRC n \u2208 V SRC|l = 1, ..., N } where V SRC is the subword vocabulary for the source language. 2.2.2. Incorporating the MT Task Since the discrete speech sequences X DISC are still longer than their corresponding source text Y SRC, we repeat the source text tokens by a factor of 4 to further reduce the gap between speech and text in- puts (see \u00a74.4 for up-sampling factor ablations). For instance, the sequence \u201c a b\u201d becomes \u201c a a a a b b b b\u201d. This length adjust- ment approach has been shown to be effective for injecting text into speech models [37, 38]. We denote this up-sampled source text as X TEXT and define our MT data as triplets of up-sampled source text, source text, and target text {(X TEXT, Y SRC, Y TGT)}. To incorporate textual inputs into our discrete ST models, we simply extend the input vocabulary V CROSS = V SPE \u222a V SRC to include textual subword tokens V SRC from the source language in addition to the speech subword tokens V SPE. This modification correspond- ingly expands the embedding layer, but does not impact any other component in the architecture. Note that this joint speech-text vo- cabulary allows our models to indiscriminately ingest speech or text into any seq2seq model, sharing all non-embedding parameters be- tween ST and MT. The only modality-specific parameters are within the embedding layer, as speech and text tokens are still disjoint. Now MT multi-tasking can be achieved by simply combin- ing ST and MT training data: the training set consists of triplets {(X CROSS, Y SRC, Y TGT)} where X CROSS = {xl \u2208 V CROSS|l = 1, ..., L}. Note that the source text Y SRC and the target text Y TGT are identical for ST and MT examples. The same losses (described in the following section) are applied with equal weighting between the two tasks. All parameters are updated in each iteration. Models do not have any explicit sense of whether a particular example is an MT or ST task \u2013 all are processed in the same manner. 2.3. Seq2Seq Models In this work, we examine AED [39, 40], CTC [41], RNN-T [42], and CTC/Attn [43, 44] models. We use a hierarchical encoding scheme, as in [44], for all of our models. This method applies an ASR CTC objective at an intermediate encoder layer, denoted as LSRC CTC, and a second ST CTC objective at the final encoder layer, denoted as LTGT CTC. The ASR CTC objective allows our models to utilize source language transcriptions to improve encoder represen- tations [45, 46]. The ST CTC objective acts as a form of regular- ization which encourages encoder representations to be monotonic with respect to the target sequence; this has been shown to improve the translation quality of auto-regressive systems [44, 47]. Our AED and CTC/Attn models use an additional cross-entropy loss, denoted as LCE, while our RNN-T models use an additional RNN-T loss, denoted as LRNNT. AED and RNN-T models are jointly trained with CTC losses but CTC likelihoods are not applied during decod- ing. All told, our models are optimized using an interpolated loss defined as L = \u03bb1LSRC CTC + \u03bb2LTGT CTC + \u03bb3LCE/RNNT. We use \u03bb1 = \u03bb2 = 0.3 and \u03bb3 = 1 for our experiments. For CTC models, the last term is omitted and we use \u03bb1 = \u03bb2 = 1. 3. EXPERIMENTAL SETUP We compare"}