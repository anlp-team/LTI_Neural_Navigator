{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Dynamic-SUPERB:_Towards_A_Dynamic,_Collaborative,_and_Comprehensive_Instruction-Tuning_Benchmark_for_Speech_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the name of the benchmark presented in the text?", "answer": " Dynamic-SUPERB", "ref_chunk": "3 2 0 2 p e S 8 1 ] S A . s s e e [ 1 v 0 1 5 9 0 . 9 0 3 2 : v i X r a DYNAMIC-SUPERB: TOWARDS A DYNAMIC, COLLABORATIVE, AND COMPREHENSIVE INSTRUCTION-TUNING BENCHMARK FOR SPEECH Chien-yu Huang1, Ke-Han Lu\u22171, Shih-Heng Wang\u22171, Chi-Yuan Hsiao\u20201, Chun-Yi Kuan\u20201, Haibin Wu\u20201 Siddhant Arora\u00a72, Kai-Wei Chang\u00a71, Jiatong Shi2, Yifan Peng2, Roshan Sharma2, Shinji Watanabe2 Bhiksha Ramakrishnan2,3, Shady Shehata3, Hung-yi Lee1 1National Taiwan University, Taiwan, 2Carnegie Mellon University, USA 3Mohamed bin Zayed University of Arti\ufb01cial Intelligence, United Arab Emirates ABSTRACT Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primar- ily focus on limited or speci\ufb01c tasks. Moreover, the lack of stan- dardized benchmarks hinders a fair comparison across different ap- proaches. Thus, we present Dynamic-SUPERB, a benchmark de- signed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and har- ness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To ini- tiate, Dynamic-SUPERB features 55 evaluation instances by com- bining 33 tasks and 22 datasets. This spans a broad spectrum of di- mensions, providing a comprehensive platform for evaluation. Ad- ditionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text lan- guage models, and the multimodal encoder. Evaluation results in- dicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the \ufb01eld to- gether1. Index Terms\u2014 self-supervised learning, instruction tuning, benchmark 1. INTRODUCTION Self-supervised learning (SSL) signi\ufb01cantly improves the scalabil- ity of machine learning models and their ability to generalize across a wide range of tasks [1\u20134]. When applying an SSL model to a downstream task, a common approach is to build a task-dependent network on top of the SSL model and \ufb01ne-tune it with task-speci\ufb01c data. However, this approach demands that users construct a unique model for each task. As downstream tasks increase, this process becomes more resource-intensive and time-consuming. To address these issues, several parameter-ef\ufb01cient tuning approaches have thus gained popularity, such as prompting [5\u201313] and adapters [14\u201317]. Particularly, in natural language processing (NLP), instruction tun- ing involves \ufb01ne-tuning language models (LM) on datasets where the tasks are de\ufb01ned by speci\ufb01c instructions [18]. This approach \u2217co-second authors. \u2020co-third authors. \u00a7co-fourth authors. 1https://github.com/dynamic-superb/dynamic-superb considerably ampli\ufb01es the zero-shot learning capacity of LMs, en- abling them to ef\ufb01ciently handle unseen tasks. However, it remains largely unexplored in the speech-processing \ufb01eld, where the aspects are even more abundant and diverse. While there are existing stud- ies like SPECTRON [19] and AudioPaLM [20] that focus on jointly processing text and speech, they are designed for speci\ufb01c tasks. On the other hand, AudioGPT [21], though driven by text instructions, is limited to a pre-de\ufb01ned set of tasks. This paper presents Dynamic-SUPERB, the \ufb01rst collaborative benchmark for instruction-tuning speech models. Although previous benchmarks provide evaluations on several aspects, they are static [22\u201329]. In contrast, Dynamic-SUPERB encourages the commu- nity to contribute a broader range of tasks so that the task variations are dynamically extended, aiming for a more comprehensive assess- ment. For the \ufb01rst version, we gathered over 20 publicly available datasets and transformed them into a diverse set of tasks. These tasks span 6 dimensions: content, speaker, semantics, degradation, par- alinguistics, and audio (non-speech). In Dynamic-SUPERB, each task is basically composed of three parts: (I) text instructions, (II) speech utterances, and (III) text labels. Following this, the system receives both the text instruction and speech utterances as input, and then functions based on the instruction. For example, given the in- struction \u201cIdentify the emotion conveyed in the utterance\u201d, the model performs emotion recognition and outputs the label \u201cHappy\u201d textu- ally. For simplicity in evaluation, we currently focus on classi\ufb01- cation tasks, such as intent classi\ufb01cation and speaker veri\ufb01cation, and leave generative ones for future collaboration with the commu- nity. To enhance user engagement, we offer detailed documentation and establish a clear process for submitting new tasks to Dynamic- SUPERB. All submitted tasks are subject to a review before inclu- sion. Reviews mainly focus on technical accuracy and completeness, ensure clarity of the proposal, and determine if the task aptly evalu- ates its intended objectives. We propose \ufb01ve approaches to establish baselines in Dynamic- SUPERB. We integrated text embeddings from BERT [30] into the generative spoken language model (GSLM) [31], enabling opera- tions on both speech and text. We also adapted Whisper [32], which was primarily designed for speech recognition involving both speech and text, to instruction tuning. Besides modifying speech models, we integrated speech representations from either Whisper or ImageBind [33], into LLaMA [34,35], a prevalent large language model (LLM). We also utilize Whisper (ASR) and ChatGPT [36] to build a con- catenative system (ASR-ChatGPT). Due to the lack of instruction- based tasks in the \ufb01eld, we developed Dynamic-SUPERB-Train us- ing academic resources as a preliminary training set for the base- lines. Its overlap with Dynamic-SUPERB differentiates tasks into seen and unseen categories, assessing model generalizability. Eval- uation results indicate that no single model dominates across all tasks. ASR-ChatGPT excels in some semantic tasks but underper- forms with speaker and paralinguistic tasks. Speech models that integrate text struggle with unseen tasks due to their limited com- prehension of text instructions. Conversely, combining speech rep- resentations with a text language model surpassed other methods. 2. DYNAMIC-SUPERB 2.1. Objectives Dynamic-SUPERB is committed to offering a comprehensive eval- uation of universal speech models and facilitating advancements in this \ufb01eld. We believe that instruction tuning is a pivotal step to- wards universal speech models. While we included a wide range of tasks to kickstart"}, {"question": " Which institutions are involved in the research project mentioned in the text?", "answer": " National Taiwan University, Carnegie Mellon University, and Mohamed bin Zayed University of Artificial Intelligence", "ref_chunk": "3 2 0 2 p e S 8 1 ] S A . s s e e [ 1 v 0 1 5 9 0 . 9 0 3 2 : v i X r a DYNAMIC-SUPERB: TOWARDS A DYNAMIC, COLLABORATIVE, AND COMPREHENSIVE INSTRUCTION-TUNING BENCHMARK FOR SPEECH Chien-yu Huang1, Ke-Han Lu\u22171, Shih-Heng Wang\u22171, Chi-Yuan Hsiao\u20201, Chun-Yi Kuan\u20201, Haibin Wu\u20201 Siddhant Arora\u00a72, Kai-Wei Chang\u00a71, Jiatong Shi2, Yifan Peng2, Roshan Sharma2, Shinji Watanabe2 Bhiksha Ramakrishnan2,3, Shady Shehata3, Hung-yi Lee1 1National Taiwan University, Taiwan, 2Carnegie Mellon University, USA 3Mohamed bin Zayed University of Arti\ufb01cial Intelligence, United Arab Emirates ABSTRACT Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primar- ily focus on limited or speci\ufb01c tasks. Moreover, the lack of stan- dardized benchmarks hinders a fair comparison across different ap- proaches. Thus, we present Dynamic-SUPERB, a benchmark de- signed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and har- ness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To ini- tiate, Dynamic-SUPERB features 55 evaluation instances by com- bining 33 tasks and 22 datasets. This spans a broad spectrum of di- mensions, providing a comprehensive platform for evaluation. Ad- ditionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text lan- guage models, and the multimodal encoder. Evaluation results in- dicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the \ufb01eld to- gether1. Index Terms\u2014 self-supervised learning, instruction tuning, benchmark 1. INTRODUCTION Self-supervised learning (SSL) signi\ufb01cantly improves the scalabil- ity of machine learning models and their ability to generalize across a wide range of tasks [1\u20134]. When applying an SSL model to a downstream task, a common approach is to build a task-dependent network on top of the SSL model and \ufb01ne-tune it with task-speci\ufb01c data. However, this approach demands that users construct a unique model for each task. As downstream tasks increase, this process becomes more resource-intensive and time-consuming. To address these issues, several parameter-ef\ufb01cient tuning approaches have thus gained popularity, such as prompting [5\u201313] and adapters [14\u201317]. Particularly, in natural language processing (NLP), instruction tun- ing involves \ufb01ne-tuning language models (LM) on datasets where the tasks are de\ufb01ned by speci\ufb01c instructions [18]. This approach \u2217co-second authors. \u2020co-third authors. \u00a7co-fourth authors. 1https://github.com/dynamic-superb/dynamic-superb considerably ampli\ufb01es the zero-shot learning capacity of LMs, en- abling them to ef\ufb01ciently handle unseen tasks. However, it remains largely unexplored in the speech-processing \ufb01eld, where the aspects are even more abundant and diverse. While there are existing stud- ies like SPECTRON [19] and AudioPaLM [20] that focus on jointly processing text and speech, they are designed for speci\ufb01c tasks. On the other hand, AudioGPT [21], though driven by text instructions, is limited to a pre-de\ufb01ned set of tasks. This paper presents Dynamic-SUPERB, the \ufb01rst collaborative benchmark for instruction-tuning speech models. Although previous benchmarks provide evaluations on several aspects, they are static [22\u201329]. In contrast, Dynamic-SUPERB encourages the commu- nity to contribute a broader range of tasks so that the task variations are dynamically extended, aiming for a more comprehensive assess- ment. For the \ufb01rst version, we gathered over 20 publicly available datasets and transformed them into a diverse set of tasks. These tasks span 6 dimensions: content, speaker, semantics, degradation, par- alinguistics, and audio (non-speech). In Dynamic-SUPERB, each task is basically composed of three parts: (I) text instructions, (II) speech utterances, and (III) text labels. Following this, the system receives both the text instruction and speech utterances as input, and then functions based on the instruction. For example, given the in- struction \u201cIdentify the emotion conveyed in the utterance\u201d, the model performs emotion recognition and outputs the label \u201cHappy\u201d textu- ally. For simplicity in evaluation, we currently focus on classi\ufb01- cation tasks, such as intent classi\ufb01cation and speaker veri\ufb01cation, and leave generative ones for future collaboration with the commu- nity. To enhance user engagement, we offer detailed documentation and establish a clear process for submitting new tasks to Dynamic- SUPERB. All submitted tasks are subject to a review before inclu- sion. Reviews mainly focus on technical accuracy and completeness, ensure clarity of the proposal, and determine if the task aptly evalu- ates its intended objectives. We propose \ufb01ve approaches to establish baselines in Dynamic- SUPERB. We integrated text embeddings from BERT [30] into the generative spoken language model (GSLM) [31], enabling opera- tions on both speech and text. We also adapted Whisper [32], which was primarily designed for speech recognition involving both speech and text, to instruction tuning. Besides modifying speech models, we integrated speech representations from either Whisper or ImageBind [33], into LLaMA [34,35], a prevalent large language model (LLM). We also utilize Whisper (ASR) and ChatGPT [36] to build a con- catenative system (ASR-ChatGPT). Due to the lack of instruction- based tasks in the \ufb01eld, we developed Dynamic-SUPERB-Train us- ing academic resources as a preliminary training set for the base- lines. Its overlap with Dynamic-SUPERB differentiates tasks into seen and unseen categories, assessing model generalizability. Eval- uation results indicate that no single model dominates across all tasks. ASR-ChatGPT excels in some semantic tasks but underper- forms with speaker and paralinguistic tasks. Speech models that integrate text struggle with unseen tasks due to their limited com- prehension of text instructions. Conversely, combining speech rep- resentations with a text language model surpassed other methods. 2. DYNAMIC-SUPERB 2.1. Objectives Dynamic-SUPERB is committed to offering a comprehensive eval- uation of universal speech models and facilitating advancements in this \ufb01eld. We believe that instruction tuning is a pivotal step to- wards universal speech models. While we included a wide range of tasks to kickstart"}, {"question": " How many evaluation instances does the Dynamic-SUPERB benchmark feature?", "answer": " 55", "ref_chunk": "3 2 0 2 p e S 8 1 ] S A . s s e e [ 1 v 0 1 5 9 0 . 9 0 3 2 : v i X r a DYNAMIC-SUPERB: TOWARDS A DYNAMIC, COLLABORATIVE, AND COMPREHENSIVE INSTRUCTION-TUNING BENCHMARK FOR SPEECH Chien-yu Huang1, Ke-Han Lu\u22171, Shih-Heng Wang\u22171, Chi-Yuan Hsiao\u20201, Chun-Yi Kuan\u20201, Haibin Wu\u20201 Siddhant Arora\u00a72, Kai-Wei Chang\u00a71, Jiatong Shi2, Yifan Peng2, Roshan Sharma2, Shinji Watanabe2 Bhiksha Ramakrishnan2,3, Shady Shehata3, Hung-yi Lee1 1National Taiwan University, Taiwan, 2Carnegie Mellon University, USA 3Mohamed bin Zayed University of Arti\ufb01cial Intelligence, United Arab Emirates ABSTRACT Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primar- ily focus on limited or speci\ufb01c tasks. Moreover, the lack of stan- dardized benchmarks hinders a fair comparison across different ap- proaches. Thus, we present Dynamic-SUPERB, a benchmark de- signed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and har- ness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To ini- tiate, Dynamic-SUPERB features 55 evaluation instances by com- bining 33 tasks and 22 datasets. This spans a broad spectrum of di- mensions, providing a comprehensive platform for evaluation. Ad- ditionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text lan- guage models, and the multimodal encoder. Evaluation results in- dicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the \ufb01eld to- gether1. Index Terms\u2014 self-supervised learning, instruction tuning, benchmark 1. INTRODUCTION Self-supervised learning (SSL) signi\ufb01cantly improves the scalabil- ity of machine learning models and their ability to generalize across a wide range of tasks [1\u20134]. When applying an SSL model to a downstream task, a common approach is to build a task-dependent network on top of the SSL model and \ufb01ne-tune it with task-speci\ufb01c data. However, this approach demands that users construct a unique model for each task. As downstream tasks increase, this process becomes more resource-intensive and time-consuming. To address these issues, several parameter-ef\ufb01cient tuning approaches have thus gained popularity, such as prompting [5\u201313] and adapters [14\u201317]. Particularly, in natural language processing (NLP), instruction tun- ing involves \ufb01ne-tuning language models (LM) on datasets where the tasks are de\ufb01ned by speci\ufb01c instructions [18]. This approach \u2217co-second authors. \u2020co-third authors. \u00a7co-fourth authors. 1https://github.com/dynamic-superb/dynamic-superb considerably ampli\ufb01es the zero-shot learning capacity of LMs, en- abling them to ef\ufb01ciently handle unseen tasks. However, it remains largely unexplored in the speech-processing \ufb01eld, where the aspects are even more abundant and diverse. While there are existing stud- ies like SPECTRON [19] and AudioPaLM [20] that focus on jointly processing text and speech, they are designed for speci\ufb01c tasks. On the other hand, AudioGPT [21], though driven by text instructions, is limited to a pre-de\ufb01ned set of tasks. This paper presents Dynamic-SUPERB, the \ufb01rst collaborative benchmark for instruction-tuning speech models. Although previous benchmarks provide evaluations on several aspects, they are static [22\u201329]. In contrast, Dynamic-SUPERB encourages the commu- nity to contribute a broader range of tasks so that the task variations are dynamically extended, aiming for a more comprehensive assess- ment. For the \ufb01rst version, we gathered over 20 publicly available datasets and transformed them into a diverse set of tasks. These tasks span 6 dimensions: content, speaker, semantics, degradation, par- alinguistics, and audio (non-speech). In Dynamic-SUPERB, each task is basically composed of three parts: (I) text instructions, (II) speech utterances, and (III) text labels. Following this, the system receives both the text instruction and speech utterances as input, and then functions based on the instruction. For example, given the in- struction \u201cIdentify the emotion conveyed in the utterance\u201d, the model performs emotion recognition and outputs the label \u201cHappy\u201d textu- ally. For simplicity in evaluation, we currently focus on classi\ufb01- cation tasks, such as intent classi\ufb01cation and speaker veri\ufb01cation, and leave generative ones for future collaboration with the commu- nity. To enhance user engagement, we offer detailed documentation and establish a clear process for submitting new tasks to Dynamic- SUPERB. All submitted tasks are subject to a review before inclu- sion. Reviews mainly focus on technical accuracy and completeness, ensure clarity of the proposal, and determine if the task aptly evalu- ates its intended objectives. We propose \ufb01ve approaches to establish baselines in Dynamic- SUPERB. We integrated text embeddings from BERT [30] into the generative spoken language model (GSLM) [31], enabling opera- tions on both speech and text. We also adapted Whisper [32], which was primarily designed for speech recognition involving both speech and text, to instruction tuning. Besides modifying speech models, we integrated speech representations from either Whisper or ImageBind [33], into LLaMA [34,35], a prevalent large language model (LLM). We also utilize Whisper (ASR) and ChatGPT [36] to build a con- catenative system (ASR-ChatGPT). Due to the lack of instruction- based tasks in the \ufb01eld, we developed Dynamic-SUPERB-Train us- ing academic resources as a preliminary training set for the base- lines. Its overlap with Dynamic-SUPERB differentiates tasks into seen and unseen categories, assessing model generalizability. Eval- uation results indicate that no single model dominates across all tasks. ASR-ChatGPT excels in some semantic tasks but underper- forms with speaker and paralinguistic tasks. Speech models that integrate text struggle with unseen tasks due to their limited com- prehension of text instructions. Conversely, combining speech rep- resentations with a text language model surpassed other methods. 2. DYNAMIC-SUPERB 2.1. Objectives Dynamic-SUPERB is committed to offering a comprehensive eval- uation of universal speech models and facilitating advancements in this \ufb01eld. We believe that instruction tuning is a pivotal step to- wards universal speech models. While we included a wide range of tasks to kickstart"}, {"question": " What are some approaches proposed to establish benchmark baselines?", "answer": " Utilization of speech models, text language models, and multimodal encoder", "ref_chunk": "3 2 0 2 p e S 8 1 ] S A . s s e e [ 1 v 0 1 5 9 0 . 9 0 3 2 : v i X r a DYNAMIC-SUPERB: TOWARDS A DYNAMIC, COLLABORATIVE, AND COMPREHENSIVE INSTRUCTION-TUNING BENCHMARK FOR SPEECH Chien-yu Huang1, Ke-Han Lu\u22171, Shih-Heng Wang\u22171, Chi-Yuan Hsiao\u20201, Chun-Yi Kuan\u20201, Haibin Wu\u20201 Siddhant Arora\u00a72, Kai-Wei Chang\u00a71, Jiatong Shi2, Yifan Peng2, Roshan Sharma2, Shinji Watanabe2 Bhiksha Ramakrishnan2,3, Shady Shehata3, Hung-yi Lee1 1National Taiwan University, Taiwan, 2Carnegie Mellon University, USA 3Mohamed bin Zayed University of Arti\ufb01cial Intelligence, United Arab Emirates ABSTRACT Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primar- ily focus on limited or speci\ufb01c tasks. Moreover, the lack of stan- dardized benchmarks hinders a fair comparison across different ap- proaches. Thus, we present Dynamic-SUPERB, a benchmark de- signed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and har- ness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To ini- tiate, Dynamic-SUPERB features 55 evaluation instances by com- bining 33 tasks and 22 datasets. This spans a broad spectrum of di- mensions, providing a comprehensive platform for evaluation. Ad- ditionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text lan- guage models, and the multimodal encoder. Evaluation results in- dicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the \ufb01eld to- gether1. Index Terms\u2014 self-supervised learning, instruction tuning, benchmark 1. INTRODUCTION Self-supervised learning (SSL) signi\ufb01cantly improves the scalabil- ity of machine learning models and their ability to generalize across a wide range of tasks [1\u20134]. When applying an SSL model to a downstream task, a common approach is to build a task-dependent network on top of the SSL model and \ufb01ne-tune it with task-speci\ufb01c data. However, this approach demands that users construct a unique model for each task. As downstream tasks increase, this process becomes more resource-intensive and time-consuming. To address these issues, several parameter-ef\ufb01cient tuning approaches have thus gained popularity, such as prompting [5\u201313] and adapters [14\u201317]. Particularly, in natural language processing (NLP), instruction tun- ing involves \ufb01ne-tuning language models (LM) on datasets where the tasks are de\ufb01ned by speci\ufb01c instructions [18]. This approach \u2217co-second authors. \u2020co-third authors. \u00a7co-fourth authors. 1https://github.com/dynamic-superb/dynamic-superb considerably ampli\ufb01es the zero-shot learning capacity of LMs, en- abling them to ef\ufb01ciently handle unseen tasks. However, it remains largely unexplored in the speech-processing \ufb01eld, where the aspects are even more abundant and diverse. While there are existing stud- ies like SPECTRON [19] and AudioPaLM [20] that focus on jointly processing text and speech, they are designed for speci\ufb01c tasks. On the other hand, AudioGPT [21], though driven by text instructions, is limited to a pre-de\ufb01ned set of tasks. This paper presents Dynamic-SUPERB, the \ufb01rst collaborative benchmark for instruction-tuning speech models. Although previous benchmarks provide evaluations on several aspects, they are static [22\u201329]. In contrast, Dynamic-SUPERB encourages the commu- nity to contribute a broader range of tasks so that the task variations are dynamically extended, aiming for a more comprehensive assess- ment. For the \ufb01rst version, we gathered over 20 publicly available datasets and transformed them into a diverse set of tasks. These tasks span 6 dimensions: content, speaker, semantics, degradation, par- alinguistics, and audio (non-speech). In Dynamic-SUPERB, each task is basically composed of three parts: (I) text instructions, (II) speech utterances, and (III) text labels. Following this, the system receives both the text instruction and speech utterances as input, and then functions based on the instruction. For example, given the in- struction \u201cIdentify the emotion conveyed in the utterance\u201d, the model performs emotion recognition and outputs the label \u201cHappy\u201d textu- ally. For simplicity in evaluation, we currently focus on classi\ufb01- cation tasks, such as intent classi\ufb01cation and speaker veri\ufb01cation, and leave generative ones for future collaboration with the commu- nity. To enhance user engagement, we offer detailed documentation and establish a clear process for submitting new tasks to Dynamic- SUPERB. All submitted tasks are subject to a review before inclu- sion. Reviews mainly focus on technical accuracy and completeness, ensure clarity of the proposal, and determine if the task aptly evalu- ates its intended objectives. We propose \ufb01ve approaches to establish baselines in Dynamic- SUPERB. We integrated text embeddings from BERT [30] into the generative spoken language model (GSLM) [31], enabling opera- tions on both speech and text. We also adapted Whisper [32], which was primarily designed for speech recognition involving both speech and text, to instruction tuning. Besides modifying speech models, we integrated speech representations from either Whisper or ImageBind [33], into LLaMA [34,35], a prevalent large language model (LLM). We also utilize Whisper (ASR) and ChatGPT [36] to build a con- catenative system (ASR-ChatGPT). Due to the lack of instruction- based tasks in the \ufb01eld, we developed Dynamic-SUPERB-Train us- ing academic resources as a preliminary training set for the base- lines. Its overlap with Dynamic-SUPERB differentiates tasks into seen and unseen categories, assessing model generalizability. Eval- uation results indicate that no single model dominates across all tasks. ASR-ChatGPT excels in some semantic tasks but underper- forms with speaker and paralinguistic tasks. Speech models that integrate text struggle with unseen tasks due to their limited com- prehension of text instructions. Conversely, combining speech rep- resentations with a text language model surpassed other methods. 2. DYNAMIC-SUPERB 2.1. Objectives Dynamic-SUPERB is committed to offering a comprehensive eval- uation of universal speech models and facilitating advancements in this \ufb01eld. We believe that instruction tuning is a pivotal step to- wards universal speech models. While we included a wide range of tasks to kickstart"}, {"question": " What is the goal of Dynamic-SUPERB in relation to speech models?", "answer": " To build universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion", "ref_chunk": "3 2 0 2 p e S 8 1 ] S A . s s e e [ 1 v 0 1 5 9 0 . 9 0 3 2 : v i X r a DYNAMIC-SUPERB: TOWARDS A DYNAMIC, COLLABORATIVE, AND COMPREHENSIVE INSTRUCTION-TUNING BENCHMARK FOR SPEECH Chien-yu Huang1, Ke-Han Lu\u22171, Shih-Heng Wang\u22171, Chi-Yuan Hsiao\u20201, Chun-Yi Kuan\u20201, Haibin Wu\u20201 Siddhant Arora\u00a72, Kai-Wei Chang\u00a71, Jiatong Shi2, Yifan Peng2, Roshan Sharma2, Shinji Watanabe2 Bhiksha Ramakrishnan2,3, Shady Shehata3, Hung-yi Lee1 1National Taiwan University, Taiwan, 2Carnegie Mellon University, USA 3Mohamed bin Zayed University of Arti\ufb01cial Intelligence, United Arab Emirates ABSTRACT Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primar- ily focus on limited or speci\ufb01c tasks. Moreover, the lack of stan- dardized benchmarks hinders a fair comparison across different ap- proaches. Thus, we present Dynamic-SUPERB, a benchmark de- signed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and har- ness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To ini- tiate, Dynamic-SUPERB features 55 evaluation instances by com- bining 33 tasks and 22 datasets. This spans a broad spectrum of di- mensions, providing a comprehensive platform for evaluation. Ad- ditionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text lan- guage models, and the multimodal encoder. Evaluation results in- dicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the \ufb01eld to- gether1. Index Terms\u2014 self-supervised learning, instruction tuning, benchmark 1. INTRODUCTION Self-supervised learning (SSL) signi\ufb01cantly improves the scalabil- ity of machine learning models and their ability to generalize across a wide range of tasks [1\u20134]. When applying an SSL model to a downstream task, a common approach is to build a task-dependent network on top of the SSL model and \ufb01ne-tune it with task-speci\ufb01c data. However, this approach demands that users construct a unique model for each task. As downstream tasks increase, this process becomes more resource-intensive and time-consuming. To address these issues, several parameter-ef\ufb01cient tuning approaches have thus gained popularity, such as prompting [5\u201313] and adapters [14\u201317]. Particularly, in natural language processing (NLP), instruction tun- ing involves \ufb01ne-tuning language models (LM) on datasets where the tasks are de\ufb01ned by speci\ufb01c instructions [18]. This approach \u2217co-second authors. \u2020co-third authors. \u00a7co-fourth authors. 1https://github.com/dynamic-superb/dynamic-superb considerably ampli\ufb01es the zero-shot learning capacity of LMs, en- abling them to ef\ufb01ciently handle unseen tasks. However, it remains largely unexplored in the speech-processing \ufb01eld, where the aspects are even more abundant and diverse. While there are existing stud- ies like SPECTRON [19] and AudioPaLM [20] that focus on jointly processing text and speech, they are designed for speci\ufb01c tasks. On the other hand, AudioGPT [21], though driven by text instructions, is limited to a pre-de\ufb01ned set of tasks. This paper presents Dynamic-SUPERB, the \ufb01rst collaborative benchmark for instruction-tuning speech models. Although previous benchmarks provide evaluations on several aspects, they are static [22\u201329]. In contrast, Dynamic-SUPERB encourages the commu- nity to contribute a broader range of tasks so that the task variations are dynamically extended, aiming for a more comprehensive assess- ment. For the \ufb01rst version, we gathered over 20 publicly available datasets and transformed them into a diverse set of tasks. These tasks span 6 dimensions: content, speaker, semantics, degradation, par- alinguistics, and audio (non-speech). In Dynamic-SUPERB, each task is basically composed of three parts: (I) text instructions, (II) speech utterances, and (III) text labels. Following this, the system receives both the text instruction and speech utterances as input, and then functions based on the instruction. For example, given the in- struction \u201cIdentify the emotion conveyed in the utterance\u201d, the model performs emotion recognition and outputs the label \u201cHappy\u201d textu- ally. For simplicity in evaluation, we currently focus on classi\ufb01- cation tasks, such as intent classi\ufb01cation and speaker veri\ufb01cation, and leave generative ones for future collaboration with the commu- nity. To enhance user engagement, we offer detailed documentation and establish a clear process for submitting new tasks to Dynamic- SUPERB. All submitted tasks are subject to a review before inclu- sion. Reviews mainly focus on technical accuracy and completeness, ensure clarity of the proposal, and determine if the task aptly evalu- ates its intended objectives. We propose \ufb01ve approaches to establish baselines in Dynamic- SUPERB. We integrated text embeddings from BERT [30] into the generative spoken language model (GSLM) [31], enabling opera- tions on both speech and text. We also adapted Whisper [32], which was primarily designed for speech recognition involving both speech and text, to instruction tuning. Besides modifying speech models, we integrated speech representations from either Whisper or ImageBind [33], into LLaMA [34,35], a prevalent large language model (LLM). We also utilize Whisper (ASR) and ChatGPT [36] to build a con- catenative system (ASR-ChatGPT). Due to the lack of instruction- based tasks in the \ufb01eld, we developed Dynamic-SUPERB-Train us- ing academic resources as a preliminary training set for the base- lines. Its overlap with Dynamic-SUPERB differentiates tasks into seen and unseen categories, assessing model generalizability. Eval- uation results indicate that no single model dominates across all tasks. ASR-ChatGPT excels in some semantic tasks but underper- forms with speaker and paralinguistic tasks. Speech models that integrate text struggle with unseen tasks due to their limited com- prehension of text instructions. Conversely, combining speech rep- resentations with a text language model surpassed other methods. 2. DYNAMIC-SUPERB 2.1. Objectives Dynamic-SUPERB is committed to offering a comprehensive eval- uation of universal speech models and facilitating advancements in this \ufb01eld. We believe that instruction tuning is a pivotal step to- wards universal speech models. While we included a wide range of tasks to kickstart"}, {"question": " What is the main focus of Dynamic-SUPERB in terms of speech tasks?", "answer": " To provide a comprehensive platform for evaluation by combining diverse speech tasks and instruction tuning", "ref_chunk": "3 2 0 2 p e S 8 1 ] S A . s s e e [ 1 v 0 1 5 9 0 . 9 0 3 2 : v i X r a DYNAMIC-SUPERB: TOWARDS A DYNAMIC, COLLABORATIVE, AND COMPREHENSIVE INSTRUCTION-TUNING BENCHMARK FOR SPEECH Chien-yu Huang1, Ke-Han Lu\u22171, Shih-Heng Wang\u22171, Chi-Yuan Hsiao\u20201, Chun-Yi Kuan\u20201, Haibin Wu\u20201 Siddhant Arora\u00a72, Kai-Wei Chang\u00a71, Jiatong Shi2, Yifan Peng2, Roshan Sharma2, Shinji Watanabe2 Bhiksha Ramakrishnan2,3, Shady Shehata3, Hung-yi Lee1 1National Taiwan University, Taiwan, 2Carnegie Mellon University, USA 3Mohamed bin Zayed University of Arti\ufb01cial Intelligence, United Arab Emirates ABSTRACT Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primar- ily focus on limited or speci\ufb01c tasks. Moreover, the lack of stan- dardized benchmarks hinders a fair comparison across different ap- proaches. Thus, we present Dynamic-SUPERB, a benchmark de- signed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and har- ness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To ini- tiate, Dynamic-SUPERB features 55 evaluation instances by com- bining 33 tasks and 22 datasets. This spans a broad spectrum of di- mensions, providing a comprehensive platform for evaluation. Ad- ditionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text lan- guage models, and the multimodal encoder. Evaluation results in- dicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the \ufb01eld to- gether1. Index Terms\u2014 self-supervised learning, instruction tuning, benchmark 1. INTRODUCTION Self-supervised learning (SSL) signi\ufb01cantly improves the scalabil- ity of machine learning models and their ability to generalize across a wide range of tasks [1\u20134]. When applying an SSL model to a downstream task, a common approach is to build a task-dependent network on top of the SSL model and \ufb01ne-tune it with task-speci\ufb01c data. However, this approach demands that users construct a unique model for each task. As downstream tasks increase, this process becomes more resource-intensive and time-consuming. To address these issues, several parameter-ef\ufb01cient tuning approaches have thus gained popularity, such as prompting [5\u201313] and adapters [14\u201317]. Particularly, in natural language processing (NLP), instruction tun- ing involves \ufb01ne-tuning language models (LM) on datasets where the tasks are de\ufb01ned by speci\ufb01c instructions [18]. This approach \u2217co-second authors. \u2020co-third authors. \u00a7co-fourth authors. 1https://github.com/dynamic-superb/dynamic-superb considerably ampli\ufb01es the zero-shot learning capacity of LMs, en- abling them to ef\ufb01ciently handle unseen tasks. However, it remains largely unexplored in the speech-processing \ufb01eld, where the aspects are even more abundant and diverse. While there are existing stud- ies like SPECTRON [19] and AudioPaLM [20] that focus on jointly processing text and speech, they are designed for speci\ufb01c tasks. On the other hand, AudioGPT [21], though driven by text instructions, is limited to a pre-de\ufb01ned set of tasks. This paper presents Dynamic-SUPERB, the \ufb01rst collaborative benchmark for instruction-tuning speech models. Although previous benchmarks provide evaluations on several aspects, they are static [22\u201329]. In contrast, Dynamic-SUPERB encourages the commu- nity to contribute a broader range of tasks so that the task variations are dynamically extended, aiming for a more comprehensive assess- ment. For the \ufb01rst version, we gathered over 20 publicly available datasets and transformed them into a diverse set of tasks. These tasks span 6 dimensions: content, speaker, semantics, degradation, par- alinguistics, and audio (non-speech). In Dynamic-SUPERB, each task is basically composed of three parts: (I) text instructions, (II) speech utterances, and (III) text labels. Following this, the system receives both the text instruction and speech utterances as input, and then functions based on the instruction. For example, given the in- struction \u201cIdentify the emotion conveyed in the utterance\u201d, the model performs emotion recognition and outputs the label \u201cHappy\u201d textu- ally. For simplicity in evaluation, we currently focus on classi\ufb01- cation tasks, such as intent classi\ufb01cation and speaker veri\ufb01cation, and leave generative ones for future collaboration with the commu- nity. To enhance user engagement, we offer detailed documentation and establish a clear process for submitting new tasks to Dynamic- SUPERB. All submitted tasks are subject to a review before inclu- sion. Reviews mainly focus on technical accuracy and completeness, ensure clarity of the proposal, and determine if the task aptly evalu- ates its intended objectives. We propose \ufb01ve approaches to establish baselines in Dynamic- SUPERB. We integrated text embeddings from BERT [30] into the generative spoken language model (GSLM) [31], enabling opera- tions on both speech and text. We also adapted Whisper [32], which was primarily designed for speech recognition involving both speech and text, to instruction tuning. Besides modifying speech models, we integrated speech representations from either Whisper or ImageBind [33], into LLaMA [34,35], a prevalent large language model (LLM). We also utilize Whisper (ASR) and ChatGPT [36] to build a con- catenative system (ASR-ChatGPT). Due to the lack of instruction- based tasks in the \ufb01eld, we developed Dynamic-SUPERB-Train us- ing academic resources as a preliminary training set for the base- lines. Its overlap with Dynamic-SUPERB differentiates tasks into seen and unseen categories, assessing model generalizability. Eval- uation results indicate that no single model dominates across all tasks. ASR-ChatGPT excels in some semantic tasks but underper- forms with speaker and paralinguistic tasks. Speech models that integrate text struggle with unseen tasks due to their limited com- prehension of text instructions. Conversely, combining speech rep- resentations with a text language model surpassed other methods. 2. DYNAMIC-SUPERB 2.1. Objectives Dynamic-SUPERB is committed to offering a comprehensive eval- uation of universal speech models and facilitating advancements in this \ufb01eld. We believe that instruction tuning is a pivotal step to- wards universal speech models. While we included a wide range of tasks to kickstart"}, {"question": " What are the six dimensions that the tasks in Dynamic-SUPERB span across?", "answer": " Content, speaker, semantics, degradation, paralinguistics, and audio (non-speech)", "ref_chunk": "3 2 0 2 p e S 8 1 ] S A . s s e e [ 1 v 0 1 5 9 0 . 9 0 3 2 : v i X r a DYNAMIC-SUPERB: TOWARDS A DYNAMIC, COLLABORATIVE, AND COMPREHENSIVE INSTRUCTION-TUNING BENCHMARK FOR SPEECH Chien-yu Huang1, Ke-Han Lu\u22171, Shih-Heng Wang\u22171, Chi-Yuan Hsiao\u20201, Chun-Yi Kuan\u20201, Haibin Wu\u20201 Siddhant Arora\u00a72, Kai-Wei Chang\u00a71, Jiatong Shi2, Yifan Peng2, Roshan Sharma2, Shinji Watanabe2 Bhiksha Ramakrishnan2,3, Shady Shehata3, Hung-yi Lee1 1National Taiwan University, Taiwan, 2Carnegie Mellon University, USA 3Mohamed bin Zayed University of Arti\ufb01cial Intelligence, United Arab Emirates ABSTRACT Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primar- ily focus on limited or speci\ufb01c tasks. Moreover, the lack of stan- dardized benchmarks hinders a fair comparison across different ap- proaches. Thus, we present Dynamic-SUPERB, a benchmark de- signed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and har- ness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To ini- tiate, Dynamic-SUPERB features 55 evaluation instances by com- bining 33 tasks and 22 datasets. This spans a broad spectrum of di- mensions, providing a comprehensive platform for evaluation. Ad- ditionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text lan- guage models, and the multimodal encoder. Evaluation results in- dicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the \ufb01eld to- gether1. Index Terms\u2014 self-supervised learning, instruction tuning, benchmark 1. INTRODUCTION Self-supervised learning (SSL) signi\ufb01cantly improves the scalabil- ity of machine learning models and their ability to generalize across a wide range of tasks [1\u20134]. When applying an SSL model to a downstream task, a common approach is to build a task-dependent network on top of the SSL model and \ufb01ne-tune it with task-speci\ufb01c data. However, this approach demands that users construct a unique model for each task. As downstream tasks increase, this process becomes more resource-intensive and time-consuming. To address these issues, several parameter-ef\ufb01cient tuning approaches have thus gained popularity, such as prompting [5\u201313] and adapters [14\u201317]. Particularly, in natural language processing (NLP), instruction tun- ing involves \ufb01ne-tuning language models (LM) on datasets where the tasks are de\ufb01ned by speci\ufb01c instructions [18]. This approach \u2217co-second authors. \u2020co-third authors. \u00a7co-fourth authors. 1https://github.com/dynamic-superb/dynamic-superb considerably ampli\ufb01es the zero-shot learning capacity of LMs, en- abling them to ef\ufb01ciently handle unseen tasks. However, it remains largely unexplored in the speech-processing \ufb01eld, where the aspects are even more abundant and diverse. While there are existing stud- ies like SPECTRON [19] and AudioPaLM [20] that focus on jointly processing text and speech, they are designed for speci\ufb01c tasks. On the other hand, AudioGPT [21], though driven by text instructions, is limited to a pre-de\ufb01ned set of tasks. This paper presents Dynamic-SUPERB, the \ufb01rst collaborative benchmark for instruction-tuning speech models. Although previous benchmarks provide evaluations on several aspects, they are static [22\u201329]. In contrast, Dynamic-SUPERB encourages the commu- nity to contribute a broader range of tasks so that the task variations are dynamically extended, aiming for a more comprehensive assess- ment. For the \ufb01rst version, we gathered over 20 publicly available datasets and transformed them into a diverse set of tasks. These tasks span 6 dimensions: content, speaker, semantics, degradation, par- alinguistics, and audio (non-speech). In Dynamic-SUPERB, each task is basically composed of three parts: (I) text instructions, (II) speech utterances, and (III) text labels. Following this, the system receives both the text instruction and speech utterances as input, and then functions based on the instruction. For example, given the in- struction \u201cIdentify the emotion conveyed in the utterance\u201d, the model performs emotion recognition and outputs the label \u201cHappy\u201d textu- ally. For simplicity in evaluation, we currently focus on classi\ufb01- cation tasks, such as intent classi\ufb01cation and speaker veri\ufb01cation, and leave generative ones for future collaboration with the commu- nity. To enhance user engagement, we offer detailed documentation and establish a clear process for submitting new tasks to Dynamic- SUPERB. All submitted tasks are subject to a review before inclu- sion. Reviews mainly focus on technical accuracy and completeness, ensure clarity of the proposal, and determine if the task aptly evalu- ates its intended objectives. We propose \ufb01ve approaches to establish baselines in Dynamic- SUPERB. We integrated text embeddings from BERT [30] into the generative spoken language model (GSLM) [31], enabling opera- tions on both speech and text. We also adapted Whisper [32], which was primarily designed for speech recognition involving both speech and text, to instruction tuning. Besides modifying speech models, we integrated speech representations from either Whisper or ImageBind [33], into LLaMA [34,35], a prevalent large language model (LLM). We also utilize Whisper (ASR) and ChatGPT [36] to build a con- catenative system (ASR-ChatGPT). Due to the lack of instruction- based tasks in the \ufb01eld, we developed Dynamic-SUPERB-Train us- ing academic resources as a preliminary training set for the base- lines. Its overlap with Dynamic-SUPERB differentiates tasks into seen and unseen categories, assessing model generalizability. Eval- uation results indicate that no single model dominates across all tasks. ASR-ChatGPT excels in some semantic tasks but underper- forms with speaker and paralinguistic tasks. Speech models that integrate text struggle with unseen tasks due to their limited com- prehension of text instructions. Conversely, combining speech rep- resentations with a text language model surpassed other methods. 2. DYNAMIC-SUPERB 2.1. Objectives Dynamic-SUPERB is committed to offering a comprehensive eval- uation of universal speech models and facilitating advancements in this \ufb01eld. We believe that instruction tuning is a pivotal step to- wards universal speech models. While we included a wide range of tasks to kickstart"}, {"question": " What are the three components that each task in Dynamic-SUPERB is composed of?", "answer": " Text instructions, speech utterances, and text labels", "ref_chunk": "3 2 0 2 p e S 8 1 ] S A . s s e e [ 1 v 0 1 5 9 0 . 9 0 3 2 : v i X r a DYNAMIC-SUPERB: TOWARDS A DYNAMIC, COLLABORATIVE, AND COMPREHENSIVE INSTRUCTION-TUNING BENCHMARK FOR SPEECH Chien-yu Huang1, Ke-Han Lu\u22171, Shih-Heng Wang\u22171, Chi-Yuan Hsiao\u20201, Chun-Yi Kuan\u20201, Haibin Wu\u20201 Siddhant Arora\u00a72, Kai-Wei Chang\u00a71, Jiatong Shi2, Yifan Peng2, Roshan Sharma2, Shinji Watanabe2 Bhiksha Ramakrishnan2,3, Shady Shehata3, Hung-yi Lee1 1National Taiwan University, Taiwan, 2Carnegie Mellon University, USA 3Mohamed bin Zayed University of Arti\ufb01cial Intelligence, United Arab Emirates ABSTRACT Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primar- ily focus on limited or speci\ufb01c tasks. Moreover, the lack of stan- dardized benchmarks hinders a fair comparison across different ap- proaches. Thus, we present Dynamic-SUPERB, a benchmark de- signed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and har- ness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To ini- tiate, Dynamic-SUPERB features 55 evaluation instances by com- bining 33 tasks and 22 datasets. This spans a broad spectrum of di- mensions, providing a comprehensive platform for evaluation. Ad- ditionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text lan- guage models, and the multimodal encoder. Evaluation results in- dicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the \ufb01eld to- gether1. Index Terms\u2014 self-supervised learning, instruction tuning, benchmark 1. INTRODUCTION Self-supervised learning (SSL) signi\ufb01cantly improves the scalabil- ity of machine learning models and their ability to generalize across a wide range of tasks [1\u20134]. When applying an SSL model to a downstream task, a common approach is to build a task-dependent network on top of the SSL model and \ufb01ne-tune it with task-speci\ufb01c data. However, this approach demands that users construct a unique model for each task. As downstream tasks increase, this process becomes more resource-intensive and time-consuming. To address these issues, several parameter-ef\ufb01cient tuning approaches have thus gained popularity, such as prompting [5\u201313] and adapters [14\u201317]. Particularly, in natural language processing (NLP), instruction tun- ing involves \ufb01ne-tuning language models (LM) on datasets where the tasks are de\ufb01ned by speci\ufb01c instructions [18]. This approach \u2217co-second authors. \u2020co-third authors. \u00a7co-fourth authors. 1https://github.com/dynamic-superb/dynamic-superb considerably ampli\ufb01es the zero-shot learning capacity of LMs, en- abling them to ef\ufb01ciently handle unseen tasks. However, it remains largely unexplored in the speech-processing \ufb01eld, where the aspects are even more abundant and diverse. While there are existing stud- ies like SPECTRON [19] and AudioPaLM [20] that focus on jointly processing text and speech, they are designed for speci\ufb01c tasks. On the other hand, AudioGPT [21], though driven by text instructions, is limited to a pre-de\ufb01ned set of tasks. This paper presents Dynamic-SUPERB, the \ufb01rst collaborative benchmark for instruction-tuning speech models. Although previous benchmarks provide evaluations on several aspects, they are static [22\u201329]. In contrast, Dynamic-SUPERB encourages the commu- nity to contribute a broader range of tasks so that the task variations are dynamically extended, aiming for a more comprehensive assess- ment. For the \ufb01rst version, we gathered over 20 publicly available datasets and transformed them into a diverse set of tasks. These tasks span 6 dimensions: content, speaker, semantics, degradation, par- alinguistics, and audio (non-speech). In Dynamic-SUPERB, each task is basically composed of three parts: (I) text instructions, (II) speech utterances, and (III) text labels. Following this, the system receives both the text instruction and speech utterances as input, and then functions based on the instruction. For example, given the in- struction \u201cIdentify the emotion conveyed in the utterance\u201d, the model performs emotion recognition and outputs the label \u201cHappy\u201d textu- ally. For simplicity in evaluation, we currently focus on classi\ufb01- cation tasks, such as intent classi\ufb01cation and speaker veri\ufb01cation, and leave generative ones for future collaboration with the commu- nity. To enhance user engagement, we offer detailed documentation and establish a clear process for submitting new tasks to Dynamic- SUPERB. All submitted tasks are subject to a review before inclu- sion. Reviews mainly focus on technical accuracy and completeness, ensure clarity of the proposal, and determine if the task aptly evalu- ates its intended objectives. We propose \ufb01ve approaches to establish baselines in Dynamic- SUPERB. We integrated text embeddings from BERT [30] into the generative spoken language model (GSLM) [31], enabling opera- tions on both speech and text. We also adapted Whisper [32], which was primarily designed for speech recognition involving both speech and text, to instruction tuning. Besides modifying speech models, we integrated speech representations from either Whisper or ImageBind [33], into LLaMA [34,35], a prevalent large language model (LLM). We also utilize Whisper (ASR) and ChatGPT [36] to build a con- catenative system (ASR-ChatGPT). Due to the lack of instruction- based tasks in the \ufb01eld, we developed Dynamic-SUPERB-Train us- ing academic resources as a preliminary training set for the base- lines. Its overlap with Dynamic-SUPERB differentiates tasks into seen and unseen categories, assessing model generalizability. Eval- uation results indicate that no single model dominates across all tasks. ASR-ChatGPT excels in some semantic tasks but underper- forms with speaker and paralinguistic tasks. Speech models that integrate text struggle with unseen tasks due to their limited com- prehension of text instructions. Conversely, combining speech rep- resentations with a text language model surpassed other methods. 2. DYNAMIC-SUPERB 2.1. Objectives Dynamic-SUPERB is committed to offering a comprehensive eval- uation of universal speech models and facilitating advancements in this \ufb01eld. We believe that instruction tuning is a pivotal step to- wards universal speech models. While we included a wide range of tasks to kickstart"}, {"question": " Why is instruction tuning considered important for universal speech models?", "answer": " It amplifies the zero-shot learning capacity of language models, enabling them to efficiently handle unseen tasks", "ref_chunk": "3 2 0 2 p e S 8 1 ] S A . s s e e [ 1 v 0 1 5 9 0 . 9 0 3 2 : v i X r a DYNAMIC-SUPERB: TOWARDS A DYNAMIC, COLLABORATIVE, AND COMPREHENSIVE INSTRUCTION-TUNING BENCHMARK FOR SPEECH Chien-yu Huang1, Ke-Han Lu\u22171, Shih-Heng Wang\u22171, Chi-Yuan Hsiao\u20201, Chun-Yi Kuan\u20201, Haibin Wu\u20201 Siddhant Arora\u00a72, Kai-Wei Chang\u00a71, Jiatong Shi2, Yifan Peng2, Roshan Sharma2, Shinji Watanabe2 Bhiksha Ramakrishnan2,3, Shady Shehata3, Hung-yi Lee1 1National Taiwan University, Taiwan, 2Carnegie Mellon University, USA 3Mohamed bin Zayed University of Arti\ufb01cial Intelligence, United Arab Emirates ABSTRACT Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primar- ily focus on limited or speci\ufb01c tasks. Moreover, the lack of stan- dardized benchmarks hinders a fair comparison across different ap- proaches. Thus, we present Dynamic-SUPERB, a benchmark de- signed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and har- ness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To ini- tiate, Dynamic-SUPERB features 55 evaluation instances by com- bining 33 tasks and 22 datasets. This spans a broad spectrum of di- mensions, providing a comprehensive platform for evaluation. Ad- ditionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text lan- guage models, and the multimodal encoder. Evaluation results in- dicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the \ufb01eld to- gether1. Index Terms\u2014 self-supervised learning, instruction tuning, benchmark 1. INTRODUCTION Self-supervised learning (SSL) signi\ufb01cantly improves the scalabil- ity of machine learning models and their ability to generalize across a wide range of tasks [1\u20134]. When applying an SSL model to a downstream task, a common approach is to build a task-dependent network on top of the SSL model and \ufb01ne-tune it with task-speci\ufb01c data. However, this approach demands that users construct a unique model for each task. As downstream tasks increase, this process becomes more resource-intensive and time-consuming. To address these issues, several parameter-ef\ufb01cient tuning approaches have thus gained popularity, such as prompting [5\u201313] and adapters [14\u201317]. Particularly, in natural language processing (NLP), instruction tun- ing involves \ufb01ne-tuning language models (LM) on datasets where the tasks are de\ufb01ned by speci\ufb01c instructions [18]. This approach \u2217co-second authors. \u2020co-third authors. \u00a7co-fourth authors. 1https://github.com/dynamic-superb/dynamic-superb considerably ampli\ufb01es the zero-shot learning capacity of LMs, en- abling them to ef\ufb01ciently handle unseen tasks. However, it remains largely unexplored in the speech-processing \ufb01eld, where the aspects are even more abundant and diverse. While there are existing stud- ies like SPECTRON [19] and AudioPaLM [20] that focus on jointly processing text and speech, they are designed for speci\ufb01c tasks. On the other hand, AudioGPT [21], though driven by text instructions, is limited to a pre-de\ufb01ned set of tasks. This paper presents Dynamic-SUPERB, the \ufb01rst collaborative benchmark for instruction-tuning speech models. Although previous benchmarks provide evaluations on several aspects, they are static [22\u201329]. In contrast, Dynamic-SUPERB encourages the commu- nity to contribute a broader range of tasks so that the task variations are dynamically extended, aiming for a more comprehensive assess- ment. For the \ufb01rst version, we gathered over 20 publicly available datasets and transformed them into a diverse set of tasks. These tasks span 6 dimensions: content, speaker, semantics, degradation, par- alinguistics, and audio (non-speech). In Dynamic-SUPERB, each task is basically composed of three parts: (I) text instructions, (II) speech utterances, and (III) text labels. Following this, the system receives both the text instruction and speech utterances as input, and then functions based on the instruction. For example, given the in- struction \u201cIdentify the emotion conveyed in the utterance\u201d, the model performs emotion recognition and outputs the label \u201cHappy\u201d textu- ally. For simplicity in evaluation, we currently focus on classi\ufb01- cation tasks, such as intent classi\ufb01cation and speaker veri\ufb01cation, and leave generative ones for future collaboration with the commu- nity. To enhance user engagement, we offer detailed documentation and establish a clear process for submitting new tasks to Dynamic- SUPERB. All submitted tasks are subject to a review before inclu- sion. Reviews mainly focus on technical accuracy and completeness, ensure clarity of the proposal, and determine if the task aptly evalu- ates its intended objectives. We propose \ufb01ve approaches to establish baselines in Dynamic- SUPERB. We integrated text embeddings from BERT [30] into the generative spoken language model (GSLM) [31], enabling opera- tions on both speech and text. We also adapted Whisper [32], which was primarily designed for speech recognition involving both speech and text, to instruction tuning. Besides modifying speech models, we integrated speech representations from either Whisper or ImageBind [33], into LLaMA [34,35], a prevalent large language model (LLM). We also utilize Whisper (ASR) and ChatGPT [36] to build a con- catenative system (ASR-ChatGPT). Due to the lack of instruction- based tasks in the \ufb01eld, we developed Dynamic-SUPERB-Train us- ing academic resources as a preliminary training set for the base- lines. Its overlap with Dynamic-SUPERB differentiates tasks into seen and unseen categories, assessing model generalizability. Eval- uation results indicate that no single model dominates across all tasks. ASR-ChatGPT excels in some semantic tasks but underper- forms with speaker and paralinguistic tasks. Speech models that integrate text struggle with unseen tasks due to their limited com- prehension of text instructions. Conversely, combining speech rep- resentations with a text language model surpassed other methods. 2. DYNAMIC-SUPERB 2.1. Objectives Dynamic-SUPERB is committed to offering a comprehensive eval- uation of universal speech models and facilitating advancements in this \ufb01eld. We believe that instruction tuning is a pivotal step to- wards universal speech models. While we included a wide range of tasks to kickstart"}, {"question": " What is the outcome of the evaluation results mentioned in the text regarding speech models?", "answer": " No single model dominates across all tasks, ASR-ChatGPT excels in some semantic tasks, while speech models integrating text struggle with unseen tasks", "ref_chunk": "3 2 0 2 p e S 8 1 ] S A . s s e e [ 1 v 0 1 5 9 0 . 9 0 3 2 : v i X r a DYNAMIC-SUPERB: TOWARDS A DYNAMIC, COLLABORATIVE, AND COMPREHENSIVE INSTRUCTION-TUNING BENCHMARK FOR SPEECH Chien-yu Huang1, Ke-Han Lu\u22171, Shih-Heng Wang\u22171, Chi-Yuan Hsiao\u20201, Chun-Yi Kuan\u20201, Haibin Wu\u20201 Siddhant Arora\u00a72, Kai-Wei Chang\u00a71, Jiatong Shi2, Yifan Peng2, Roshan Sharma2, Shinji Watanabe2 Bhiksha Ramakrishnan2,3, Shady Shehata3, Hung-yi Lee1 1National Taiwan University, Taiwan, 2Carnegie Mellon University, USA 3Mohamed bin Zayed University of Arti\ufb01cial Intelligence, United Arab Emirates ABSTRACT Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primar- ily focus on limited or speci\ufb01c tasks. Moreover, the lack of stan- dardized benchmarks hinders a fair comparison across different ap- proaches. Thus, we present Dynamic-SUPERB, a benchmark de- signed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and har- ness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To ini- tiate, Dynamic-SUPERB features 55 evaluation instances by com- bining 33 tasks and 22 datasets. This spans a broad spectrum of di- mensions, providing a comprehensive platform for evaluation. Ad- ditionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text lan- guage models, and the multimodal encoder. Evaluation results in- dicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the \ufb01eld to- gether1. Index Terms\u2014 self-supervised learning, instruction tuning, benchmark 1. INTRODUCTION Self-supervised learning (SSL) signi\ufb01cantly improves the scalabil- ity of machine learning models and their ability to generalize across a wide range of tasks [1\u20134]. When applying an SSL model to a downstream task, a common approach is to build a task-dependent network on top of the SSL model and \ufb01ne-tune it with task-speci\ufb01c data. However, this approach demands that users construct a unique model for each task. As downstream tasks increase, this process becomes more resource-intensive and time-consuming. To address these issues, several parameter-ef\ufb01cient tuning approaches have thus gained popularity, such as prompting [5\u201313] and adapters [14\u201317]. Particularly, in natural language processing (NLP), instruction tun- ing involves \ufb01ne-tuning language models (LM) on datasets where the tasks are de\ufb01ned by speci\ufb01c instructions [18]. This approach \u2217co-second authors. \u2020co-third authors. \u00a7co-fourth authors. 1https://github.com/dynamic-superb/dynamic-superb considerably ampli\ufb01es the zero-shot learning capacity of LMs, en- abling them to ef\ufb01ciently handle unseen tasks. However, it remains largely unexplored in the speech-processing \ufb01eld, where the aspects are even more abundant and diverse. While there are existing stud- ies like SPECTRON [19] and AudioPaLM [20] that focus on jointly processing text and speech, they are designed for speci\ufb01c tasks. On the other hand, AudioGPT [21], though driven by text instructions, is limited to a pre-de\ufb01ned set of tasks. This paper presents Dynamic-SUPERB, the \ufb01rst collaborative benchmark for instruction-tuning speech models. Although previous benchmarks provide evaluations on several aspects, they are static [22\u201329]. In contrast, Dynamic-SUPERB encourages the commu- nity to contribute a broader range of tasks so that the task variations are dynamically extended, aiming for a more comprehensive assess- ment. For the \ufb01rst version, we gathered over 20 publicly available datasets and transformed them into a diverse set of tasks. These tasks span 6 dimensions: content, speaker, semantics, degradation, par- alinguistics, and audio (non-speech). In Dynamic-SUPERB, each task is basically composed of three parts: (I) text instructions, (II) speech utterances, and (III) text labels. Following this, the system receives both the text instruction and speech utterances as input, and then functions based on the instruction. For example, given the in- struction \u201cIdentify the emotion conveyed in the utterance\u201d, the model performs emotion recognition and outputs the label \u201cHappy\u201d textu- ally. For simplicity in evaluation, we currently focus on classi\ufb01- cation tasks, such as intent classi\ufb01cation and speaker veri\ufb01cation, and leave generative ones for future collaboration with the commu- nity. To enhance user engagement, we offer detailed documentation and establish a clear process for submitting new tasks to Dynamic- SUPERB. All submitted tasks are subject to a review before inclu- sion. Reviews mainly focus on technical accuracy and completeness, ensure clarity of the proposal, and determine if the task aptly evalu- ates its intended objectives. We propose \ufb01ve approaches to establish baselines in Dynamic- SUPERB. We integrated text embeddings from BERT [30] into the generative spoken language model (GSLM) [31], enabling opera- tions on both speech and text. We also adapted Whisper [32], which was primarily designed for speech recognition involving both speech and text, to instruction tuning. Besides modifying speech models, we integrated speech representations from either Whisper or ImageBind [33], into LLaMA [34,35], a prevalent large language model (LLM). We also utilize Whisper (ASR) and ChatGPT [36] to build a con- catenative system (ASR-ChatGPT). Due to the lack of instruction- based tasks in the \ufb01eld, we developed Dynamic-SUPERB-Train us- ing academic resources as a preliminary training set for the base- lines. Its overlap with Dynamic-SUPERB differentiates tasks into seen and unseen categories, assessing model generalizability. Eval- uation results indicate that no single model dominates across all tasks. ASR-ChatGPT excels in some semantic tasks but underper- forms with speaker and paralinguistic tasks. Speech models that integrate text struggle with unseen tasks due to their limited com- prehension of text instructions. Conversely, combining speech rep- resentations with a text language model surpassed other methods. 2. DYNAMIC-SUPERB 2.1. Objectives Dynamic-SUPERB is committed to offering a comprehensive eval- uation of universal speech models and facilitating advancements in this \ufb01eld. We believe that instruction tuning is a pivotal step to- wards universal speech models. While we included a wide range of tasks to kickstart"}], "doc_text": "3 2 0 2 p e S 8 1 ] S A . s s e e [ 1 v 0 1 5 9 0 . 9 0 3 2 : v i X r a DYNAMIC-SUPERB: TOWARDS A DYNAMIC, COLLABORATIVE, AND COMPREHENSIVE INSTRUCTION-TUNING BENCHMARK FOR SPEECH Chien-yu Huang1, Ke-Han Lu\u22171, Shih-Heng Wang\u22171, Chi-Yuan Hsiao\u20201, Chun-Yi Kuan\u20201, Haibin Wu\u20201 Siddhant Arora\u00a72, Kai-Wei Chang\u00a71, Jiatong Shi2, Yifan Peng2, Roshan Sharma2, Shinji Watanabe2 Bhiksha Ramakrishnan2,3, Shady Shehata3, Hung-yi Lee1 1National Taiwan University, Taiwan, 2Carnegie Mellon University, USA 3Mohamed bin Zayed University of Arti\ufb01cial Intelligence, United Arab Emirates ABSTRACT Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primar- ily focus on limited or speci\ufb01c tasks. Moreover, the lack of stan- dardized benchmarks hinders a fair comparison across different ap- proaches. Thus, we present Dynamic-SUPERB, a benchmark de- signed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and har- ness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To ini- tiate, Dynamic-SUPERB features 55 evaluation instances by com- bining 33 tasks and 22 datasets. This spans a broad spectrum of di- mensions, providing a comprehensive platform for evaluation. Ad- ditionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text lan- guage models, and the multimodal encoder. Evaluation results in- dicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the \ufb01eld to- gether1. Index Terms\u2014 self-supervised learning, instruction tuning, benchmark 1. INTRODUCTION Self-supervised learning (SSL) signi\ufb01cantly improves the scalabil- ity of machine learning models and their ability to generalize across a wide range of tasks [1\u20134]. When applying an SSL model to a downstream task, a common approach is to build a task-dependent network on top of the SSL model and \ufb01ne-tune it with task-speci\ufb01c data. However, this approach demands that users construct a unique model for each task. As downstream tasks increase, this process becomes more resource-intensive and time-consuming. To address these issues, several parameter-ef\ufb01cient tuning approaches have thus gained popularity, such as prompting [5\u201313] and adapters [14\u201317]. Particularly, in natural language processing (NLP), instruction tun- ing involves \ufb01ne-tuning language models (LM) on datasets where the tasks are de\ufb01ned by speci\ufb01c instructions [18]. This approach \u2217co-second authors. \u2020co-third authors. \u00a7co-fourth authors. 1https://github.com/dynamic-superb/dynamic-superb considerably ampli\ufb01es the zero-shot learning capacity of LMs, en- abling them to ef\ufb01ciently handle unseen tasks. However, it remains largely unexplored in the speech-processing \ufb01eld, where the aspects are even more abundant and diverse. While there are existing stud- ies like SPECTRON [19] and AudioPaLM [20] that focus on jointly processing text and speech, they are designed for speci\ufb01c tasks. On the other hand, AudioGPT [21], though driven by text instructions, is limited to a pre-de\ufb01ned set of tasks. This paper presents Dynamic-SUPERB, the \ufb01rst collaborative benchmark for instruction-tuning speech models. Although previous benchmarks provide evaluations on several aspects, they are static [22\u201329]. In contrast, Dynamic-SUPERB encourages the commu- nity to contribute a broader range of tasks so that the task variations are dynamically extended, aiming for a more comprehensive assess- ment. For the \ufb01rst version, we gathered over 20 publicly available datasets and transformed them into a diverse set of tasks. These tasks span 6 dimensions: content, speaker, semantics, degradation, par- alinguistics, and audio (non-speech). In Dynamic-SUPERB, each task is basically composed of three parts: (I) text instructions, (II) speech utterances, and (III) text labels. Following this, the system receives both the text instruction and speech utterances as input, and then functions based on the instruction. For example, given the in- struction \u201cIdentify the emotion conveyed in the utterance\u201d, the model performs emotion recognition and outputs the label \u201cHappy\u201d textu- ally. For simplicity in evaluation, we currently focus on classi\ufb01- cation tasks, such as intent classi\ufb01cation and speaker veri\ufb01cation, and leave generative ones for future collaboration with the commu- nity. To enhance user engagement, we offer detailed documentation and establish a clear process for submitting new tasks to Dynamic- SUPERB. All submitted tasks are subject to a review before inclu- sion. Reviews mainly focus on technical accuracy and completeness, ensure clarity of the proposal, and determine if the task aptly evalu- ates its intended objectives. We propose \ufb01ve approaches to establish baselines in Dynamic- SUPERB. We integrated text embeddings from BERT [30] into the generative spoken language model (GSLM) [31], enabling opera- tions on both speech and text. We also adapted Whisper [32], which was primarily designed for speech recognition involving both speech and text, to instruction tuning. Besides modifying speech models, we integrated speech representations from either Whisper or ImageBind [33], into LLaMA [34,35], a prevalent large language model (LLM). We also utilize Whisper (ASR) and ChatGPT [36] to build a con- catenative system (ASR-ChatGPT). Due to the lack of instruction- based tasks in the \ufb01eld, we developed Dynamic-SUPERB-Train us- ing academic resources as a preliminary training set for the base- lines. Its overlap with Dynamic-SUPERB differentiates tasks into seen and unseen categories, assessing model generalizability. Eval- uation results indicate that no single model dominates across all tasks. ASR-ChatGPT excels in some semantic tasks but underper- forms with speaker and paralinguistic tasks. Speech models that integrate text struggle with unseen tasks due to their limited com- prehension of text instructions. Conversely, combining speech rep- resentations with a text language model surpassed other methods. 2. DYNAMIC-SUPERB 2.1. Objectives Dynamic-SUPERB is committed to offering a comprehensive eval- uation of universal speech models and facilitating advancements in this \ufb01eld. We believe that instruction tuning is a pivotal step to- wards universal speech models. While we included a wide range of tasks to kickstart"}