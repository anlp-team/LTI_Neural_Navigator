{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_DPHuBERT:_Joint_Distillation_and_Pruning_of_Self-Supervised_Speech_Models_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the name of the compression method proposed in the text?,answer: DPHuBERT", "ref_chunk": "3 2 0 2 y a M 8 2 ] L C . s c [ 1 v 1 5 6 7 1 . 5 0 3 2 : v i X r a DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Yifan Peng1, Yui Sudo2, Shakeel Muhammad2, Shinji Watanabe1 1Carnegie Mellon University, Pittsburgh, PA, USA 2Honda Research Institute Japan Co., Ltd., Saitama, Japan yifanpen@andrew.cmu.edu, {yui.sudo, shakeel.muhammad}@jp.honda-ri.com, shinjiw@ieee.org Abstract Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behav- ior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task- specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in al- most all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available. Index Terms: model compression, knowledge distillation, structured pruning, self-supervised learning Experiments on SUPERB [6] show that our method outper- forms prior distillation methods in almost all tasks. Our method also performs well for various speech SSL models such as Hu- BERT Base [2], WavLM Base+ [4] and HuBERT Large [2], even with limited training resources. We will submit our re- sults to the SUPERB leaderboard and release the code and mod- els publicly for reproducibility: https://github.com/ pyf98/DPHuBERT. 2. Background and related work 2.1. Architectures of speech SSL Speech SSL models such as wav2vec 2.0 [1], HuBERT [2] and WavLM [4] share a similar architecture. The model consists of a convolutional feature extractor (CNN) and a Transformer [26] encoder. The CNN has seven temporal convolutions with nor- malizations and activations. The Transformer encoder contains 12 layers with hidden size 768 for base models and 24 layers with hidden size 1024 for large models. Each layer is com- posed of a multi-head self-attention (MHA) and a position-wise feed-forward network (FFN). 1. Introduction Self-supervised speech representation learning (speech SSL) has achieved remarkable results in various tasks [1\u201310]. How- ever, speech SSL models are usually large and slow, making them unsuitable for real-world applications with limited re- sources. Compressing speech SSL has become an important topic. A popular method is knowledge distillation [11], which trains a small student model to match the outputs of a large teacher model. Prior studies such as DistilHuBERT [12] and FitHuBERT [13] have achieved promising results with different student models. Another work [14] shows that the student archi- tecture affects its performance substantially, even if the model size is similar. However, in distillation methods, the student ar- chitecture is pre-specified and remains unchanged, which needs special expertise and might lead to suboptimal results. In con- trast, pruning [15, 16] automatically discovers a compact sub- network from a large model, which has been explored in nat- ural language processing (NLP) [17\u201320] and speech process- ing [21\u201325]. Previous pruning methods for speech SSL focus on specific downstream tasks such as automatic speech recognition (ASR) [24, 25] and spoken language understanding (SLU) [25]. It is unclear how they will perform in task-agnostic compres- sion, which is more challenging because the model needs to capture various aspects of speech including content, speaker, semantics and paralinguistics [6]. In this work, we propose DPHuBERT, a task-agnostic com- pression method based on joint Distillation and Pruning. It al- lows the student architecture to be learned during distillation. 2.2. Compression methods for speech SSL Distillation. Distillation methods optimize a small student model to match the targets generated by a large teacher model. Since different layers of speech SSL capture different infor- mation [27], the student model needs to learn both final and intermediate representations of the teacher [12\u201314]. DistilHu- BERT [12] trains a shallow student model by mapping the last student layer to multiple intermediate teacher layers. FitHu- BERT [13] learns a deep and thin student model through layer- to-layer mapping. Another work [14] compares prediction- layer and layer-to-layer distillation using various student archi- tectures. It shows that the architecture of a student model af- fects its performance, even when the model size is kept similar. It also finds that deeper networks perform better with layer-to- layer distillation, likely because it explicitly aligns intermediate layers. These observations have inspired our work which allows the student architecture to evolve during distillation. Pruning. Pruning methods identify and remove redundant pa- rameters from a pre-trained model. Unstructured pruning re- moves individual parameters (e.g., a connection between neu- rons) by setting them to zero, which requires sparse matrix computation libraries to achieve actual speedup, whereas struc- tured pruning removes groups of parameters (e.g., an attention head or even an entire layer), which directly reduces the model size and computational cost. For speech SSL, PARP [24] is a magnitude-based unstructured pruning method which prunes It improves downstream tasks like the Transformer encoder. FFN FFN CNN Audio Waveform Teacher(frozen)Student(prunable)\u2026 Linear FFN\u2026 MHA FFN Linear MHA MHA MHA Linear CNN Linear MHA Linear Linear FFN CNN CNN FFN FFN FFN MHA Teacher(frozen)Student(already pruned)\u2026\u2026 Audio Waveform MHA MHA (a) Step 1: jointly distill and prune the student model. (b) Step 2: further distill the already pruned model. Figure 1: Two training steps of our task-agnostic compression method, DPHuBERT. (a) The student model is initialized from the teacher model and is jointly distilled and pruned to produce a smaller model which meets a pre-specified sparsity ratio. (b) The already pruned student model is further distilled for better performance. To obtain DPHuBERT (24M) from HuBERT Base (95M), the two steps take around 18 and 6 GPU hours, respectively. (Dashed modules are prunable, i.e., their architectures can evolve during training.) low-resource ASR."}, {"question": " What is the purpose of knowledge distillation in the context of speech SSL?,answer: To train a small student model to mimic the behavior of a large teacher model", "ref_chunk": "3 2 0 2 y a M 8 2 ] L C . s c [ 1 v 1 5 6 7 1 . 5 0 3 2 : v i X r a DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Yifan Peng1, Yui Sudo2, Shakeel Muhammad2, Shinji Watanabe1 1Carnegie Mellon University, Pittsburgh, PA, USA 2Honda Research Institute Japan Co., Ltd., Saitama, Japan yifanpen@andrew.cmu.edu, {yui.sudo, shakeel.muhammad}@jp.honda-ri.com, shinjiw@ieee.org Abstract Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behav- ior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task- specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in al- most all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available. Index Terms: model compression, knowledge distillation, structured pruning, self-supervised learning Experiments on SUPERB [6] show that our method outper- forms prior distillation methods in almost all tasks. Our method also performs well for various speech SSL models such as Hu- BERT Base [2], WavLM Base+ [4] and HuBERT Large [2], even with limited training resources. We will submit our re- sults to the SUPERB leaderboard and release the code and mod- els publicly for reproducibility: https://github.com/ pyf98/DPHuBERT. 2. Background and related work 2.1. Architectures of speech SSL Speech SSL models such as wav2vec 2.0 [1], HuBERT [2] and WavLM [4] share a similar architecture. The model consists of a convolutional feature extractor (CNN) and a Transformer [26] encoder. The CNN has seven temporal convolutions with nor- malizations and activations. The Transformer encoder contains 12 layers with hidden size 768 for base models and 24 layers with hidden size 1024 for large models. Each layer is com- posed of a multi-head self-attention (MHA) and a position-wise feed-forward network (FFN). 1. Introduction Self-supervised speech representation learning (speech SSL) has achieved remarkable results in various tasks [1\u201310]. How- ever, speech SSL models are usually large and slow, making them unsuitable for real-world applications with limited re- sources. Compressing speech SSL has become an important topic. A popular method is knowledge distillation [11], which trains a small student model to match the outputs of a large teacher model. Prior studies such as DistilHuBERT [12] and FitHuBERT [13] have achieved promising results with different student models. Another work [14] shows that the student archi- tecture affects its performance substantially, even if the model size is similar. However, in distillation methods, the student ar- chitecture is pre-specified and remains unchanged, which needs special expertise and might lead to suboptimal results. In con- trast, pruning [15, 16] automatically discovers a compact sub- network from a large model, which has been explored in nat- ural language processing (NLP) [17\u201320] and speech process- ing [21\u201325]. Previous pruning methods for speech SSL focus on specific downstream tasks such as automatic speech recognition (ASR) [24, 25] and spoken language understanding (SLU) [25]. It is unclear how they will perform in task-agnostic compres- sion, which is more challenging because the model needs to capture various aspects of speech including content, speaker, semantics and paralinguistics [6]. In this work, we propose DPHuBERT, a task-agnostic com- pression method based on joint Distillation and Pruning. It al- lows the student architecture to be learned during distillation. 2.2. Compression methods for speech SSL Distillation. Distillation methods optimize a small student model to match the targets generated by a large teacher model. Since different layers of speech SSL capture different infor- mation [27], the student model needs to learn both final and intermediate representations of the teacher [12\u201314]. DistilHu- BERT [12] trains a shallow student model by mapping the last student layer to multiple intermediate teacher layers. FitHu- BERT [13] learns a deep and thin student model through layer- to-layer mapping. Another work [14] compares prediction- layer and layer-to-layer distillation using various student archi- tectures. It shows that the architecture of a student model af- fects its performance, even when the model size is kept similar. It also finds that deeper networks perform better with layer-to- layer distillation, likely because it explicitly aligns intermediate layers. These observations have inspired our work which allows the student architecture to evolve during distillation. Pruning. Pruning methods identify and remove redundant pa- rameters from a pre-trained model. Unstructured pruning re- moves individual parameters (e.g., a connection between neu- rons) by setting them to zero, which requires sparse matrix computation libraries to achieve actual speedup, whereas struc- tured pruning removes groups of parameters (e.g., an attention head or even an entire layer), which directly reduces the model size and computational cost. For speech SSL, PARP [24] is a magnitude-based unstructured pruning method which prunes It improves downstream tasks like the Transformer encoder. FFN FFN CNN Audio Waveform Teacher(frozen)Student(prunable)\u2026 Linear FFN\u2026 MHA FFN Linear MHA MHA MHA Linear CNN Linear MHA Linear Linear FFN CNN CNN FFN FFN FFN MHA Teacher(frozen)Student(already pruned)\u2026\u2026 Audio Waveform MHA MHA (a) Step 1: jointly distill and prune the student model. (b) Step 2: further distill the already pruned model. Figure 1: Two training steps of our task-agnostic compression method, DPHuBERT. (a) The student model is initialized from the teacher model and is jointly distilled and pruned to produce a smaller model which meets a pre-specified sparsity ratio. (b) The already pruned student model is further distilled for better performance. To obtain DPHuBERT (24M) from HuBERT Base (95M), the two steps take around 18 and 6 GPU hours, respectively. (Dashed modules are prunable, i.e., their architectures can evolve during training.) low-resource ASR."}, {"question": " Why is distillation used to compress speech SSL models?,answer: To make them more suitable for real-world applications with limited resources", "ref_chunk": "3 2 0 2 y a M 8 2 ] L C . s c [ 1 v 1 5 6 7 1 . 5 0 3 2 : v i X r a DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Yifan Peng1, Yui Sudo2, Shakeel Muhammad2, Shinji Watanabe1 1Carnegie Mellon University, Pittsburgh, PA, USA 2Honda Research Institute Japan Co., Ltd., Saitama, Japan yifanpen@andrew.cmu.edu, {yui.sudo, shakeel.muhammad}@jp.honda-ri.com, shinjiw@ieee.org Abstract Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behav- ior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task- specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in al- most all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available. Index Terms: model compression, knowledge distillation, structured pruning, self-supervised learning Experiments on SUPERB [6] show that our method outper- forms prior distillation methods in almost all tasks. Our method also performs well for various speech SSL models such as Hu- BERT Base [2], WavLM Base+ [4] and HuBERT Large [2], even with limited training resources. We will submit our re- sults to the SUPERB leaderboard and release the code and mod- els publicly for reproducibility: https://github.com/ pyf98/DPHuBERT. 2. Background and related work 2.1. Architectures of speech SSL Speech SSL models such as wav2vec 2.0 [1], HuBERT [2] and WavLM [4] share a similar architecture. The model consists of a convolutional feature extractor (CNN) and a Transformer [26] encoder. The CNN has seven temporal convolutions with nor- malizations and activations. The Transformer encoder contains 12 layers with hidden size 768 for base models and 24 layers with hidden size 1024 for large models. Each layer is com- posed of a multi-head self-attention (MHA) and a position-wise feed-forward network (FFN). 1. Introduction Self-supervised speech representation learning (speech SSL) has achieved remarkable results in various tasks [1\u201310]. How- ever, speech SSL models are usually large and slow, making them unsuitable for real-world applications with limited re- sources. Compressing speech SSL has become an important topic. A popular method is knowledge distillation [11], which trains a small student model to match the outputs of a large teacher model. Prior studies such as DistilHuBERT [12] and FitHuBERT [13] have achieved promising results with different student models. Another work [14] shows that the student archi- tecture affects its performance substantially, even if the model size is similar. However, in distillation methods, the student ar- chitecture is pre-specified and remains unchanged, which needs special expertise and might lead to suboptimal results. In con- trast, pruning [15, 16] automatically discovers a compact sub- network from a large model, which has been explored in nat- ural language processing (NLP) [17\u201320] and speech process- ing [21\u201325]. Previous pruning methods for speech SSL focus on specific downstream tasks such as automatic speech recognition (ASR) [24, 25] and spoken language understanding (SLU) [25]. It is unclear how they will perform in task-agnostic compres- sion, which is more challenging because the model needs to capture various aspects of speech including content, speaker, semantics and paralinguistics [6]. In this work, we propose DPHuBERT, a task-agnostic com- pression method based on joint Distillation and Pruning. It al- lows the student architecture to be learned during distillation. 2.2. Compression methods for speech SSL Distillation. Distillation methods optimize a small student model to match the targets generated by a large teacher model. Since different layers of speech SSL capture different infor- mation [27], the student model needs to learn both final and intermediate representations of the teacher [12\u201314]. DistilHu- BERT [12] trains a shallow student model by mapping the last student layer to multiple intermediate teacher layers. FitHu- BERT [13] learns a deep and thin student model through layer- to-layer mapping. Another work [14] compares prediction- layer and layer-to-layer distillation using various student archi- tectures. It shows that the architecture of a student model af- fects its performance, even when the model size is kept similar. It also finds that deeper networks perform better with layer-to- layer distillation, likely because it explicitly aligns intermediate layers. These observations have inspired our work which allows the student architecture to evolve during distillation. Pruning. Pruning methods identify and remove redundant pa- rameters from a pre-trained model. Unstructured pruning re- moves individual parameters (e.g., a connection between neu- rons) by setting them to zero, which requires sparse matrix computation libraries to achieve actual speedup, whereas struc- tured pruning removes groups of parameters (e.g., an attention head or even an entire layer), which directly reduces the model size and computational cost. For speech SSL, PARP [24] is a magnitude-based unstructured pruning method which prunes It improves downstream tasks like the Transformer encoder. FFN FFN CNN Audio Waveform Teacher(frozen)Student(prunable)\u2026 Linear FFN\u2026 MHA FFN Linear MHA MHA MHA Linear CNN Linear MHA Linear Linear FFN CNN CNN FFN FFN FFN MHA Teacher(frozen)Student(already pruned)\u2026\u2026 Audio Waveform MHA MHA (a) Step 1: jointly distill and prune the student model. (b) Step 2: further distill the already pruned model. Figure 1: Two training steps of our task-agnostic compression method, DPHuBERT. (a) The student model is initialized from the teacher model and is jointly distilled and pruned to produce a smaller model which meets a pre-specified sparsity ratio. (b) The already pruned student model is further distilled for better performance. To obtain DPHuBERT (24M) from HuBERT Base (95M), the two steps take around 18 and 6 GPU hours, respectively. (Dashed modules are prunable, i.e., their architectures can evolve during training.) low-resource ASR."}, {"question": " How many layers does the Transformer encoder contain in the base models mentioned in the text?,answer: 12 layers", "ref_chunk": "3 2 0 2 y a M 8 2 ] L C . s c [ 1 v 1 5 6 7 1 . 5 0 3 2 : v i X r a DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Yifan Peng1, Yui Sudo2, Shakeel Muhammad2, Shinji Watanabe1 1Carnegie Mellon University, Pittsburgh, PA, USA 2Honda Research Institute Japan Co., Ltd., Saitama, Japan yifanpen@andrew.cmu.edu, {yui.sudo, shakeel.muhammad}@jp.honda-ri.com, shinjiw@ieee.org Abstract Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behav- ior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task- specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in al- most all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available. Index Terms: model compression, knowledge distillation, structured pruning, self-supervised learning Experiments on SUPERB [6] show that our method outper- forms prior distillation methods in almost all tasks. Our method also performs well for various speech SSL models such as Hu- BERT Base [2], WavLM Base+ [4] and HuBERT Large [2], even with limited training resources. We will submit our re- sults to the SUPERB leaderboard and release the code and mod- els publicly for reproducibility: https://github.com/ pyf98/DPHuBERT. 2. Background and related work 2.1. Architectures of speech SSL Speech SSL models such as wav2vec 2.0 [1], HuBERT [2] and WavLM [4] share a similar architecture. The model consists of a convolutional feature extractor (CNN) and a Transformer [26] encoder. The CNN has seven temporal convolutions with nor- malizations and activations. The Transformer encoder contains 12 layers with hidden size 768 for base models and 24 layers with hidden size 1024 for large models. Each layer is com- posed of a multi-head self-attention (MHA) and a position-wise feed-forward network (FFN). 1. Introduction Self-supervised speech representation learning (speech SSL) has achieved remarkable results in various tasks [1\u201310]. How- ever, speech SSL models are usually large and slow, making them unsuitable for real-world applications with limited re- sources. Compressing speech SSL has become an important topic. A popular method is knowledge distillation [11], which trains a small student model to match the outputs of a large teacher model. Prior studies such as DistilHuBERT [12] and FitHuBERT [13] have achieved promising results with different student models. Another work [14] shows that the student archi- tecture affects its performance substantially, even if the model size is similar. However, in distillation methods, the student ar- chitecture is pre-specified and remains unchanged, which needs special expertise and might lead to suboptimal results. In con- trast, pruning [15, 16] automatically discovers a compact sub- network from a large model, which has been explored in nat- ural language processing (NLP) [17\u201320] and speech process- ing [21\u201325]. Previous pruning methods for speech SSL focus on specific downstream tasks such as automatic speech recognition (ASR) [24, 25] and spoken language understanding (SLU) [25]. It is unclear how they will perform in task-agnostic compres- sion, which is more challenging because the model needs to capture various aspects of speech including content, speaker, semantics and paralinguistics [6]. In this work, we propose DPHuBERT, a task-agnostic com- pression method based on joint Distillation and Pruning. It al- lows the student architecture to be learned during distillation. 2.2. Compression methods for speech SSL Distillation. Distillation methods optimize a small student model to match the targets generated by a large teacher model. Since different layers of speech SSL capture different infor- mation [27], the student model needs to learn both final and intermediate representations of the teacher [12\u201314]. DistilHu- BERT [12] trains a shallow student model by mapping the last student layer to multiple intermediate teacher layers. FitHu- BERT [13] learns a deep and thin student model through layer- to-layer mapping. Another work [14] compares prediction- layer and layer-to-layer distillation using various student archi- tectures. It shows that the architecture of a student model af- fects its performance, even when the model size is kept similar. It also finds that deeper networks perform better with layer-to- layer distillation, likely because it explicitly aligns intermediate layers. These observations have inspired our work which allows the student architecture to evolve during distillation. Pruning. Pruning methods identify and remove redundant pa- rameters from a pre-trained model. Unstructured pruning re- moves individual parameters (e.g., a connection between neu- rons) by setting them to zero, which requires sparse matrix computation libraries to achieve actual speedup, whereas struc- tured pruning removes groups of parameters (e.g., an attention head or even an entire layer), which directly reduces the model size and computational cost. For speech SSL, PARP [24] is a magnitude-based unstructured pruning method which prunes It improves downstream tasks like the Transformer encoder. FFN FFN CNN Audio Waveform Teacher(frozen)Student(prunable)\u2026 Linear FFN\u2026 MHA FFN Linear MHA MHA MHA Linear CNN Linear MHA Linear Linear FFN CNN CNN FFN FFN FFN MHA Teacher(frozen)Student(already pruned)\u2026\u2026 Audio Waveform MHA MHA (a) Step 1: jointly distill and prune the student model. (b) Step 2: further distill the already pruned model. Figure 1: Two training steps of our task-agnostic compression method, DPHuBERT. (a) The student model is initialized from the teacher model and is jointly distilled and pruned to produce a smaller model which meets a pre-specified sparsity ratio. (b) The already pruned student model is further distilled for better performance. To obtain DPHuBERT (24M) from HuBERT Base (95M), the two steps take around 18 and 6 GPU hours, respectively. (Dashed modules are prunable, i.e., their architectures can evolve during training.) low-resource ASR."}, {"question": " What type of network does the Transformer encoder consist of in the speech SSL models described?,answer: A multi-head self-attention (MHA) and a position-wise feed-forward network (FFN)", "ref_chunk": "3 2 0 2 y a M 8 2 ] L C . s c [ 1 v 1 5 6 7 1 . 5 0 3 2 : v i X r a DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Yifan Peng1, Yui Sudo2, Shakeel Muhammad2, Shinji Watanabe1 1Carnegie Mellon University, Pittsburgh, PA, USA 2Honda Research Institute Japan Co., Ltd., Saitama, Japan yifanpen@andrew.cmu.edu, {yui.sudo, shakeel.muhammad}@jp.honda-ri.com, shinjiw@ieee.org Abstract Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behav- ior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task- specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in al- most all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available. Index Terms: model compression, knowledge distillation, structured pruning, self-supervised learning Experiments on SUPERB [6] show that our method outper- forms prior distillation methods in almost all tasks. Our method also performs well for various speech SSL models such as Hu- BERT Base [2], WavLM Base+ [4] and HuBERT Large [2], even with limited training resources. We will submit our re- sults to the SUPERB leaderboard and release the code and mod- els publicly for reproducibility: https://github.com/ pyf98/DPHuBERT. 2. Background and related work 2.1. Architectures of speech SSL Speech SSL models such as wav2vec 2.0 [1], HuBERT [2] and WavLM [4] share a similar architecture. The model consists of a convolutional feature extractor (CNN) and a Transformer [26] encoder. The CNN has seven temporal convolutions with nor- malizations and activations. The Transformer encoder contains 12 layers with hidden size 768 for base models and 24 layers with hidden size 1024 for large models. Each layer is com- posed of a multi-head self-attention (MHA) and a position-wise feed-forward network (FFN). 1. Introduction Self-supervised speech representation learning (speech SSL) has achieved remarkable results in various tasks [1\u201310]. How- ever, speech SSL models are usually large and slow, making them unsuitable for real-world applications with limited re- sources. Compressing speech SSL has become an important topic. A popular method is knowledge distillation [11], which trains a small student model to match the outputs of a large teacher model. Prior studies such as DistilHuBERT [12] and FitHuBERT [13] have achieved promising results with different student models. Another work [14] shows that the student archi- tecture affects its performance substantially, even if the model size is similar. However, in distillation methods, the student ar- chitecture is pre-specified and remains unchanged, which needs special expertise and might lead to suboptimal results. In con- trast, pruning [15, 16] automatically discovers a compact sub- network from a large model, which has been explored in nat- ural language processing (NLP) [17\u201320] and speech process- ing [21\u201325]. Previous pruning methods for speech SSL focus on specific downstream tasks such as automatic speech recognition (ASR) [24, 25] and spoken language understanding (SLU) [25]. It is unclear how they will perform in task-agnostic compres- sion, which is more challenging because the model needs to capture various aspects of speech including content, speaker, semantics and paralinguistics [6]. In this work, we propose DPHuBERT, a task-agnostic com- pression method based on joint Distillation and Pruning. It al- lows the student architecture to be learned during distillation. 2.2. Compression methods for speech SSL Distillation. Distillation methods optimize a small student model to match the targets generated by a large teacher model. Since different layers of speech SSL capture different infor- mation [27], the student model needs to learn both final and intermediate representations of the teacher [12\u201314]. DistilHu- BERT [12] trains a shallow student model by mapping the last student layer to multiple intermediate teacher layers. FitHu- BERT [13] learns a deep and thin student model through layer- to-layer mapping. Another work [14] compares prediction- layer and layer-to-layer distillation using various student archi- tectures. It shows that the architecture of a student model af- fects its performance, even when the model size is kept similar. It also finds that deeper networks perform better with layer-to- layer distillation, likely because it explicitly aligns intermediate layers. These observations have inspired our work which allows the student architecture to evolve during distillation. Pruning. Pruning methods identify and remove redundant pa- rameters from a pre-trained model. Unstructured pruning re- moves individual parameters (e.g., a connection between neu- rons) by setting them to zero, which requires sparse matrix computation libraries to achieve actual speedup, whereas struc- tured pruning removes groups of parameters (e.g., an attention head or even an entire layer), which directly reduces the model size and computational cost. For speech SSL, PARP [24] is a magnitude-based unstructured pruning method which prunes It improves downstream tasks like the Transformer encoder. FFN FFN CNN Audio Waveform Teacher(frozen)Student(prunable)\u2026 Linear FFN\u2026 MHA FFN Linear MHA MHA MHA Linear CNN Linear MHA Linear Linear FFN CNN CNN FFN FFN FFN MHA Teacher(frozen)Student(already pruned)\u2026\u2026 Audio Waveform MHA MHA (a) Step 1: jointly distill and prune the student model. (b) Step 2: further distill the already pruned model. Figure 1: Two training steps of our task-agnostic compression method, DPHuBERT. (a) The student model is initialized from the teacher model and is jointly distilled and pruned to produce a smaller model which meets a pre-specified sparsity ratio. (b) The already pruned student model is further distilled for better performance. To obtain DPHuBERT (24M) from HuBERT Base (95M), the two steps take around 18 and 6 GPU hours, respectively. (Dashed modules are prunable, i.e., their architectures can evolve during training.) low-resource ASR."}, {"question": " What is the purpose of pruning in the context of compressing speech SSL models?,answer: To automatically discover a compact sub-network from a large model", "ref_chunk": "3 2 0 2 y a M 8 2 ] L C . s c [ 1 v 1 5 6 7 1 . 5 0 3 2 : v i X r a DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Yifan Peng1, Yui Sudo2, Shakeel Muhammad2, Shinji Watanabe1 1Carnegie Mellon University, Pittsburgh, PA, USA 2Honda Research Institute Japan Co., Ltd., Saitama, Japan yifanpen@andrew.cmu.edu, {yui.sudo, shakeel.muhammad}@jp.honda-ri.com, shinjiw@ieee.org Abstract Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behav- ior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task- specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in al- most all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available. Index Terms: model compression, knowledge distillation, structured pruning, self-supervised learning Experiments on SUPERB [6] show that our method outper- forms prior distillation methods in almost all tasks. Our method also performs well for various speech SSL models such as Hu- BERT Base [2], WavLM Base+ [4] and HuBERT Large [2], even with limited training resources. We will submit our re- sults to the SUPERB leaderboard and release the code and mod- els publicly for reproducibility: https://github.com/ pyf98/DPHuBERT. 2. Background and related work 2.1. Architectures of speech SSL Speech SSL models such as wav2vec 2.0 [1], HuBERT [2] and WavLM [4] share a similar architecture. The model consists of a convolutional feature extractor (CNN) and a Transformer [26] encoder. The CNN has seven temporal convolutions with nor- malizations and activations. The Transformer encoder contains 12 layers with hidden size 768 for base models and 24 layers with hidden size 1024 for large models. Each layer is com- posed of a multi-head self-attention (MHA) and a position-wise feed-forward network (FFN). 1. Introduction Self-supervised speech representation learning (speech SSL) has achieved remarkable results in various tasks [1\u201310]. How- ever, speech SSL models are usually large and slow, making them unsuitable for real-world applications with limited re- sources. Compressing speech SSL has become an important topic. A popular method is knowledge distillation [11], which trains a small student model to match the outputs of a large teacher model. Prior studies such as DistilHuBERT [12] and FitHuBERT [13] have achieved promising results with different student models. Another work [14] shows that the student archi- tecture affects its performance substantially, even if the model size is similar. However, in distillation methods, the student ar- chitecture is pre-specified and remains unchanged, which needs special expertise and might lead to suboptimal results. In con- trast, pruning [15, 16] automatically discovers a compact sub- network from a large model, which has been explored in nat- ural language processing (NLP) [17\u201320] and speech process- ing [21\u201325]. Previous pruning methods for speech SSL focus on specific downstream tasks such as automatic speech recognition (ASR) [24, 25] and spoken language understanding (SLU) [25]. It is unclear how they will perform in task-agnostic compres- sion, which is more challenging because the model needs to capture various aspects of speech including content, speaker, semantics and paralinguistics [6]. In this work, we propose DPHuBERT, a task-agnostic com- pression method based on joint Distillation and Pruning. It al- lows the student architecture to be learned during distillation. 2.2. Compression methods for speech SSL Distillation. Distillation methods optimize a small student model to match the targets generated by a large teacher model. Since different layers of speech SSL capture different infor- mation [27], the student model needs to learn both final and intermediate representations of the teacher [12\u201314]. DistilHu- BERT [12] trains a shallow student model by mapping the last student layer to multiple intermediate teacher layers. FitHu- BERT [13] learns a deep and thin student model through layer- to-layer mapping. Another work [14] compares prediction- layer and layer-to-layer distillation using various student archi- tectures. It shows that the architecture of a student model af- fects its performance, even when the model size is kept similar. It also finds that deeper networks perform better with layer-to- layer distillation, likely because it explicitly aligns intermediate layers. These observations have inspired our work which allows the student architecture to evolve during distillation. Pruning. Pruning methods identify and remove redundant pa- rameters from a pre-trained model. Unstructured pruning re- moves individual parameters (e.g., a connection between neu- rons) by setting them to zero, which requires sparse matrix computation libraries to achieve actual speedup, whereas struc- tured pruning removes groups of parameters (e.g., an attention head or even an entire layer), which directly reduces the model size and computational cost. For speech SSL, PARP [24] is a magnitude-based unstructured pruning method which prunes It improves downstream tasks like the Transformer encoder. FFN FFN CNN Audio Waveform Teacher(frozen)Student(prunable)\u2026 Linear FFN\u2026 MHA FFN Linear MHA MHA MHA Linear CNN Linear MHA Linear Linear FFN CNN CNN FFN FFN FFN MHA Teacher(frozen)Student(already pruned)\u2026\u2026 Audio Waveform MHA MHA (a) Step 1: jointly distill and prune the student model. (b) Step 2: further distill the already pruned model. Figure 1: Two training steps of our task-agnostic compression method, DPHuBERT. (a) The student model is initialized from the teacher model and is jointly distilled and pruned to produce a smaller model which meets a pre-specified sparsity ratio. (b) The already pruned student model is further distilled for better performance. To obtain DPHuBERT (24M) from HuBERT Base (95M), the two steps take around 18 and 6 GPU hours, respectively. (Dashed modules are prunable, i.e., their architectures can evolve during training.) low-resource ASR."}, {"question": " What are the methods mentioned in the text for compression of speech SSL models?,answer: Knowledge distillation and structured pruning", "ref_chunk": "3 2 0 2 y a M 8 2 ] L C . s c [ 1 v 1 5 6 7 1 . 5 0 3 2 : v i X r a DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Yifan Peng1, Yui Sudo2, Shakeel Muhammad2, Shinji Watanabe1 1Carnegie Mellon University, Pittsburgh, PA, USA 2Honda Research Institute Japan Co., Ltd., Saitama, Japan yifanpen@andrew.cmu.edu, {yui.sudo, shakeel.muhammad}@jp.honda-ri.com, shinjiw@ieee.org Abstract Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behav- ior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task- specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in al- most all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available. Index Terms: model compression, knowledge distillation, structured pruning, self-supervised learning Experiments on SUPERB [6] show that our method outper- forms prior distillation methods in almost all tasks. Our method also performs well for various speech SSL models such as Hu- BERT Base [2], WavLM Base+ [4] and HuBERT Large [2], even with limited training resources. We will submit our re- sults to the SUPERB leaderboard and release the code and mod- els publicly for reproducibility: https://github.com/ pyf98/DPHuBERT. 2. Background and related work 2.1. Architectures of speech SSL Speech SSL models such as wav2vec 2.0 [1], HuBERT [2] and WavLM [4] share a similar architecture. The model consists of a convolutional feature extractor (CNN) and a Transformer [26] encoder. The CNN has seven temporal convolutions with nor- malizations and activations. The Transformer encoder contains 12 layers with hidden size 768 for base models and 24 layers with hidden size 1024 for large models. Each layer is com- posed of a multi-head self-attention (MHA) and a position-wise feed-forward network (FFN). 1. Introduction Self-supervised speech representation learning (speech SSL) has achieved remarkable results in various tasks [1\u201310]. How- ever, speech SSL models are usually large and slow, making them unsuitable for real-world applications with limited re- sources. Compressing speech SSL has become an important topic. A popular method is knowledge distillation [11], which trains a small student model to match the outputs of a large teacher model. Prior studies such as DistilHuBERT [12] and FitHuBERT [13] have achieved promising results with different student models. Another work [14] shows that the student archi- tecture affects its performance substantially, even if the model size is similar. However, in distillation methods, the student ar- chitecture is pre-specified and remains unchanged, which needs special expertise and might lead to suboptimal results. In con- trast, pruning [15, 16] automatically discovers a compact sub- network from a large model, which has been explored in nat- ural language processing (NLP) [17\u201320] and speech process- ing [21\u201325]. Previous pruning methods for speech SSL focus on specific downstream tasks such as automatic speech recognition (ASR) [24, 25] and spoken language understanding (SLU) [25]. It is unclear how they will perform in task-agnostic compres- sion, which is more challenging because the model needs to capture various aspects of speech including content, speaker, semantics and paralinguistics [6]. In this work, we propose DPHuBERT, a task-agnostic com- pression method based on joint Distillation and Pruning. It al- lows the student architecture to be learned during distillation. 2.2. Compression methods for speech SSL Distillation. Distillation methods optimize a small student model to match the targets generated by a large teacher model. Since different layers of speech SSL capture different infor- mation [27], the student model needs to learn both final and intermediate representations of the teacher [12\u201314]. DistilHu- BERT [12] trains a shallow student model by mapping the last student layer to multiple intermediate teacher layers. FitHu- BERT [13] learns a deep and thin student model through layer- to-layer mapping. Another work [14] compares prediction- layer and layer-to-layer distillation using various student archi- tectures. It shows that the architecture of a student model af- fects its performance, even when the model size is kept similar. It also finds that deeper networks perform better with layer-to- layer distillation, likely because it explicitly aligns intermediate layers. These observations have inspired our work which allows the student architecture to evolve during distillation. Pruning. Pruning methods identify and remove redundant pa- rameters from a pre-trained model. Unstructured pruning re- moves individual parameters (e.g., a connection between neu- rons) by setting them to zero, which requires sparse matrix computation libraries to achieve actual speedup, whereas struc- tured pruning removes groups of parameters (e.g., an attention head or even an entire layer), which directly reduces the model size and computational cost. For speech SSL, PARP [24] is a magnitude-based unstructured pruning method which prunes It improves downstream tasks like the Transformer encoder. FFN FFN CNN Audio Waveform Teacher(frozen)Student(prunable)\u2026 Linear FFN\u2026 MHA FFN Linear MHA MHA MHA Linear CNN Linear MHA Linear Linear FFN CNN CNN FFN FFN FFN MHA Teacher(frozen)Student(already pruned)\u2026\u2026 Audio Waveform MHA MHA (a) Step 1: jointly distill and prune the student model. (b) Step 2: further distill the already pruned model. Figure 1: Two training steps of our task-agnostic compression method, DPHuBERT. (a) The student model is initialized from the teacher model and is jointly distilled and pruned to produce a smaller model which meets a pre-specified sparsity ratio. (b) The already pruned student model is further distilled for better performance. To obtain DPHuBERT (24M) from HuBERT Base (95M), the two steps take around 18 and 6 GPU hours, respectively. (Dashed modules are prunable, i.e., their architectures can evolve during training.) low-resource ASR."}, {"question": " Why is task-agnostic compression considered more challenging?,answer: Because the model needs to capture various aspects of speech including content, speaker, semantics, and paralinguistics", "ref_chunk": "3 2 0 2 y a M 8 2 ] L C . s c [ 1 v 1 5 6 7 1 . 5 0 3 2 : v i X r a DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Yifan Peng1, Yui Sudo2, Shakeel Muhammad2, Shinji Watanabe1 1Carnegie Mellon University, Pittsburgh, PA, USA 2Honda Research Institute Japan Co., Ltd., Saitama, Japan yifanpen@andrew.cmu.edu, {yui.sudo, shakeel.muhammad}@jp.honda-ri.com, shinjiw@ieee.org Abstract Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behav- ior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task- specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in al- most all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available. Index Terms: model compression, knowledge distillation, structured pruning, self-supervised learning Experiments on SUPERB [6] show that our method outper- forms prior distillation methods in almost all tasks. Our method also performs well for various speech SSL models such as Hu- BERT Base [2], WavLM Base+ [4] and HuBERT Large [2], even with limited training resources. We will submit our re- sults to the SUPERB leaderboard and release the code and mod- els publicly for reproducibility: https://github.com/ pyf98/DPHuBERT. 2. Background and related work 2.1. Architectures of speech SSL Speech SSL models such as wav2vec 2.0 [1], HuBERT [2] and WavLM [4] share a similar architecture. The model consists of a convolutional feature extractor (CNN) and a Transformer [26] encoder. The CNN has seven temporal convolutions with nor- malizations and activations. The Transformer encoder contains 12 layers with hidden size 768 for base models and 24 layers with hidden size 1024 for large models. Each layer is com- posed of a multi-head self-attention (MHA) and a position-wise feed-forward network (FFN). 1. Introduction Self-supervised speech representation learning (speech SSL) has achieved remarkable results in various tasks [1\u201310]. How- ever, speech SSL models are usually large and slow, making them unsuitable for real-world applications with limited re- sources. Compressing speech SSL has become an important topic. A popular method is knowledge distillation [11], which trains a small student model to match the outputs of a large teacher model. Prior studies such as DistilHuBERT [12] and FitHuBERT [13] have achieved promising results with different student models. Another work [14] shows that the student archi- tecture affects its performance substantially, even if the model size is similar. However, in distillation methods, the student ar- chitecture is pre-specified and remains unchanged, which needs special expertise and might lead to suboptimal results. In con- trast, pruning [15, 16] automatically discovers a compact sub- network from a large model, which has been explored in nat- ural language processing (NLP) [17\u201320] and speech process- ing [21\u201325]. Previous pruning methods for speech SSL focus on specific downstream tasks such as automatic speech recognition (ASR) [24, 25] and spoken language understanding (SLU) [25]. It is unclear how they will perform in task-agnostic compres- sion, which is more challenging because the model needs to capture various aspects of speech including content, speaker, semantics and paralinguistics [6]. In this work, we propose DPHuBERT, a task-agnostic com- pression method based on joint Distillation and Pruning. It al- lows the student architecture to be learned during distillation. 2.2. Compression methods for speech SSL Distillation. Distillation methods optimize a small student model to match the targets generated by a large teacher model. Since different layers of speech SSL capture different infor- mation [27], the student model needs to learn both final and intermediate representations of the teacher [12\u201314]. DistilHu- BERT [12] trains a shallow student model by mapping the last student layer to multiple intermediate teacher layers. FitHu- BERT [13] learns a deep and thin student model through layer- to-layer mapping. Another work [14] compares prediction- layer and layer-to-layer distillation using various student archi- tectures. It shows that the architecture of a student model af- fects its performance, even when the model size is kept similar. It also finds that deeper networks perform better with layer-to- layer distillation, likely because it explicitly aligns intermediate layers. These observations have inspired our work which allows the student architecture to evolve during distillation. Pruning. Pruning methods identify and remove redundant pa- rameters from a pre-trained model. Unstructured pruning re- moves individual parameters (e.g., a connection between neu- rons) by setting them to zero, which requires sparse matrix computation libraries to achieve actual speedup, whereas struc- tured pruning removes groups of parameters (e.g., an attention head or even an entire layer), which directly reduces the model size and computational cost. For speech SSL, PARP [24] is a magnitude-based unstructured pruning method which prunes It improves downstream tasks like the Transformer encoder. FFN FFN CNN Audio Waveform Teacher(frozen)Student(prunable)\u2026 Linear FFN\u2026 MHA FFN Linear MHA MHA MHA Linear CNN Linear MHA Linear Linear FFN CNN CNN FFN FFN FFN MHA Teacher(frozen)Student(already pruned)\u2026\u2026 Audio Waveform MHA MHA (a) Step 1: jointly distill and prune the student model. (b) Step 2: further distill the already pruned model. Figure 1: Two training steps of our task-agnostic compression method, DPHuBERT. (a) The student model is initialized from the teacher model and is jointly distilled and pruned to produce a smaller model which meets a pre-specified sparsity ratio. (b) The already pruned student model is further distilled for better performance. To obtain DPHuBERT (24M) from HuBERT Base (95M), the two steps take around 18 and 6 GPU hours, respectively. (Dashed modules are prunable, i.e., their architectures can evolve during training.) low-resource ASR."}, {"question": " How does DPHuBERT differ from traditional distillation methods?,answer: It allows the student architecture to evolve during distillation", "ref_chunk": "3 2 0 2 y a M 8 2 ] L C . s c [ 1 v 1 5 6 7 1 . 5 0 3 2 : v i X r a DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Yifan Peng1, Yui Sudo2, Shakeel Muhammad2, Shinji Watanabe1 1Carnegie Mellon University, Pittsburgh, PA, USA 2Honda Research Institute Japan Co., Ltd., Saitama, Japan yifanpen@andrew.cmu.edu, {yui.sudo, shakeel.muhammad}@jp.honda-ri.com, shinjiw@ieee.org Abstract Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behav- ior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task- specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in al- most all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available. Index Terms: model compression, knowledge distillation, structured pruning, self-supervised learning Experiments on SUPERB [6] show that our method outper- forms prior distillation methods in almost all tasks. Our method also performs well for various speech SSL models such as Hu- BERT Base [2], WavLM Base+ [4] and HuBERT Large [2], even with limited training resources. We will submit our re- sults to the SUPERB leaderboard and release the code and mod- els publicly for reproducibility: https://github.com/ pyf98/DPHuBERT. 2. Background and related work 2.1. Architectures of speech SSL Speech SSL models such as wav2vec 2.0 [1], HuBERT [2] and WavLM [4] share a similar architecture. The model consists of a convolutional feature extractor (CNN) and a Transformer [26] encoder. The CNN has seven temporal convolutions with nor- malizations and activations. The Transformer encoder contains 12 layers with hidden size 768 for base models and 24 layers with hidden size 1024 for large models. Each layer is com- posed of a multi-head self-attention (MHA) and a position-wise feed-forward network (FFN). 1. Introduction Self-supervised speech representation learning (speech SSL) has achieved remarkable results in various tasks [1\u201310]. How- ever, speech SSL models are usually large and slow, making them unsuitable for real-world applications with limited re- sources. Compressing speech SSL has become an important topic. A popular method is knowledge distillation [11], which trains a small student model to match the outputs of a large teacher model. Prior studies such as DistilHuBERT [12] and FitHuBERT [13] have achieved promising results with different student models. Another work [14] shows that the student archi- tecture affects its performance substantially, even if the model size is similar. However, in distillation methods, the student ar- chitecture is pre-specified and remains unchanged, which needs special expertise and might lead to suboptimal results. In con- trast, pruning [15, 16] automatically discovers a compact sub- network from a large model, which has been explored in nat- ural language processing (NLP) [17\u201320] and speech process- ing [21\u201325]. Previous pruning methods for speech SSL focus on specific downstream tasks such as automatic speech recognition (ASR) [24, 25] and spoken language understanding (SLU) [25]. It is unclear how they will perform in task-agnostic compres- sion, which is more challenging because the model needs to capture various aspects of speech including content, speaker, semantics and paralinguistics [6]. In this work, we propose DPHuBERT, a task-agnostic com- pression method based on joint Distillation and Pruning. It al- lows the student architecture to be learned during distillation. 2.2. Compression methods for speech SSL Distillation. Distillation methods optimize a small student model to match the targets generated by a large teacher model. Since different layers of speech SSL capture different infor- mation [27], the student model needs to learn both final and intermediate representations of the teacher [12\u201314]. DistilHu- BERT [12] trains a shallow student model by mapping the last student layer to multiple intermediate teacher layers. FitHu- BERT [13] learns a deep and thin student model through layer- to-layer mapping. Another work [14] compares prediction- layer and layer-to-layer distillation using various student archi- tectures. It shows that the architecture of a student model af- fects its performance, even when the model size is kept similar. It also finds that deeper networks perform better with layer-to- layer distillation, likely because it explicitly aligns intermediate layers. These observations have inspired our work which allows the student architecture to evolve during distillation. Pruning. Pruning methods identify and remove redundant pa- rameters from a pre-trained model. Unstructured pruning re- moves individual parameters (e.g., a connection between neu- rons) by setting them to zero, which requires sparse matrix computation libraries to achieve actual speedup, whereas struc- tured pruning removes groups of parameters (e.g., an attention head or even an entire layer), which directly reduces the model size and computational cost. For speech SSL, PARP [24] is a magnitude-based unstructured pruning method which prunes It improves downstream tasks like the Transformer encoder. FFN FFN CNN Audio Waveform Teacher(frozen)Student(prunable)\u2026 Linear FFN\u2026 MHA FFN Linear MHA MHA MHA Linear CNN Linear MHA Linear Linear FFN CNN CNN FFN FFN FFN MHA Teacher(frozen)Student(already pruned)\u2026\u2026 Audio Waveform MHA MHA (a) Step 1: jointly distill and prune the student model. (b) Step 2: further distill the already pruned model. Figure 1: Two training steps of our task-agnostic compression method, DPHuBERT. (a) The student model is initialized from the teacher model and is jointly distilled and pruned to produce a smaller model which meets a pre-specified sparsity ratio. (b) The already pruned student model is further distilled for better performance. To obtain DPHuBERT (24M) from HuBERT Base (95M), the two steps take around 18 and 6 GPU hours, respectively. (Dashed modules are prunable, i.e., their architectures can evolve during training.) low-resource ASR."}, {"question": " How many GPU hours are required to obtain DPHuBERT (24M) from HuBERT Base (95M) through the two training steps described?,answer: Around 18 and 6 GPU hours, respectively", "ref_chunk": "3 2 0 2 y a M 8 2 ] L C . s c [ 1 v 1 5 6 7 1 . 5 0 3 2 : v i X r a DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Yifan Peng1, Yui Sudo2, Shakeel Muhammad2, Shinji Watanabe1 1Carnegie Mellon University, Pittsburgh, PA, USA 2Honda Research Institute Japan Co., Ltd., Saitama, Japan yifanpen@andrew.cmu.edu, {yui.sudo, shakeel.muhammad}@jp.honda-ri.com, shinjiw@ieee.org Abstract Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behav- ior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task- specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in al- most all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available. Index Terms: model compression, knowledge distillation, structured pruning, self-supervised learning Experiments on SUPERB [6] show that our method outper- forms prior distillation methods in almost all tasks. Our method also performs well for various speech SSL models such as Hu- BERT Base [2], WavLM Base+ [4] and HuBERT Large [2], even with limited training resources. We will submit our re- sults to the SUPERB leaderboard and release the code and mod- els publicly for reproducibility: https://github.com/ pyf98/DPHuBERT. 2. Background and related work 2.1. Architectures of speech SSL Speech SSL models such as wav2vec 2.0 [1], HuBERT [2] and WavLM [4] share a similar architecture. The model consists of a convolutional feature extractor (CNN) and a Transformer [26] encoder. The CNN has seven temporal convolutions with nor- malizations and activations. The Transformer encoder contains 12 layers with hidden size 768 for base models and 24 layers with hidden size 1024 for large models. Each layer is com- posed of a multi-head self-attention (MHA) and a position-wise feed-forward network (FFN). 1. Introduction Self-supervised speech representation learning (speech SSL) has achieved remarkable results in various tasks [1\u201310]. How- ever, speech SSL models are usually large and slow, making them unsuitable for real-world applications with limited re- sources. Compressing speech SSL has become an important topic. A popular method is knowledge distillation [11], which trains a small student model to match the outputs of a large teacher model. Prior studies such as DistilHuBERT [12] and FitHuBERT [13] have achieved promising results with different student models. Another work [14] shows that the student archi- tecture affects its performance substantially, even if the model size is similar. However, in distillation methods, the student ar- chitecture is pre-specified and remains unchanged, which needs special expertise and might lead to suboptimal results. In con- trast, pruning [15, 16] automatically discovers a compact sub- network from a large model, which has been explored in nat- ural language processing (NLP) [17\u201320] and speech process- ing [21\u201325]. Previous pruning methods for speech SSL focus on specific downstream tasks such as automatic speech recognition (ASR) [24, 25] and spoken language understanding (SLU) [25]. It is unclear how they will perform in task-agnostic compres- sion, which is more challenging because the model needs to capture various aspects of speech including content, speaker, semantics and paralinguistics [6]. In this work, we propose DPHuBERT, a task-agnostic com- pression method based on joint Distillation and Pruning. It al- lows the student architecture to be learned during distillation. 2.2. Compression methods for speech SSL Distillation. Distillation methods optimize a small student model to match the targets generated by a large teacher model. Since different layers of speech SSL capture different infor- mation [27], the student model needs to learn both final and intermediate representations of the teacher [12\u201314]. DistilHu- BERT [12] trains a shallow student model by mapping the last student layer to multiple intermediate teacher layers. FitHu- BERT [13] learns a deep and thin student model through layer- to-layer mapping. Another work [14] compares prediction- layer and layer-to-layer distillation using various student archi- tectures. It shows that the architecture of a student model af- fects its performance, even when the model size is kept similar. It also finds that deeper networks perform better with layer-to- layer distillation, likely because it explicitly aligns intermediate layers. These observations have inspired our work which allows the student architecture to evolve during distillation. Pruning. Pruning methods identify and remove redundant pa- rameters from a pre-trained model. Unstructured pruning re- moves individual parameters (e.g., a connection between neu- rons) by setting them to zero, which requires sparse matrix computation libraries to achieve actual speedup, whereas struc- tured pruning removes groups of parameters (e.g., an attention head or even an entire layer), which directly reduces the model size and computational cost. For speech SSL, PARP [24] is a magnitude-based unstructured pruning method which prunes It improves downstream tasks like the Transformer encoder. FFN FFN CNN Audio Waveform Teacher(frozen)Student(prunable)\u2026 Linear FFN\u2026 MHA FFN Linear MHA MHA MHA Linear CNN Linear MHA Linear Linear FFN CNN CNN FFN FFN FFN MHA Teacher(frozen)Student(already pruned)\u2026\u2026 Audio Waveform MHA MHA (a) Step 1: jointly distill and prune the student model. (b) Step 2: further distill the already pruned model. Figure 1: Two training steps of our task-agnostic compression method, DPHuBERT. (a) The student model is initialized from the teacher model and is jointly distilled and pruned to produce a smaller model which meets a pre-specified sparsity ratio. (b) The already pruned student model is further distilled for better performance. To obtain DPHuBERT (24M) from HuBERT Base (95M), the two steps take around 18 and 6 GPU hours, respectively. (Dashed modules are prunable, i.e., their architectures can evolve during training.) low-resource ASR."}], "doc_text": "3 2 0 2 y a M 8 2 ] L C . s c [ 1 v 1 5 6 7 1 . 5 0 3 2 : v i X r a DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Yifan Peng1, Yui Sudo2, Shakeel Muhammad2, Shinji Watanabe1 1Carnegie Mellon University, Pittsburgh, PA, USA 2Honda Research Institute Japan Co., Ltd., Saitama, Japan yifanpen@andrew.cmu.edu, {yui.sudo, shakeel.muhammad}@jp.honda-ri.com, shinjiw@ieee.org Abstract Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behav- ior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task- specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in al- most all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available. Index Terms: model compression, knowledge distillation, structured pruning, self-supervised learning Experiments on SUPERB [6] show that our method outper- forms prior distillation methods in almost all tasks. Our method also performs well for various speech SSL models such as Hu- BERT Base [2], WavLM Base+ [4] and HuBERT Large [2], even with limited training resources. We will submit our re- sults to the SUPERB leaderboard and release the code and mod- els publicly for reproducibility: https://github.com/ pyf98/DPHuBERT. 2. Background and related work 2.1. Architectures of speech SSL Speech SSL models such as wav2vec 2.0 [1], HuBERT [2] and WavLM [4] share a similar architecture. The model consists of a convolutional feature extractor (CNN) and a Transformer [26] encoder. The CNN has seven temporal convolutions with nor- malizations and activations. The Transformer encoder contains 12 layers with hidden size 768 for base models and 24 layers with hidden size 1024 for large models. Each layer is com- posed of a multi-head self-attention (MHA) and a position-wise feed-forward network (FFN). 1. Introduction Self-supervised speech representation learning (speech SSL) has achieved remarkable results in various tasks [1\u201310]. How- ever, speech SSL models are usually large and slow, making them unsuitable for real-world applications with limited re- sources. Compressing speech SSL has become an important topic. A popular method is knowledge distillation [11], which trains a small student model to match the outputs of a large teacher model. Prior studies such as DistilHuBERT [12] and FitHuBERT [13] have achieved promising results with different student models. Another work [14] shows that the student archi- tecture affects its performance substantially, even if the model size is similar. However, in distillation methods, the student ar- chitecture is pre-specified and remains unchanged, which needs special expertise and might lead to suboptimal results. In con- trast, pruning [15, 16] automatically discovers a compact sub- network from a large model, which has been explored in nat- ural language processing (NLP) [17\u201320] and speech process- ing [21\u201325]. Previous pruning methods for speech SSL focus on specific downstream tasks such as automatic speech recognition (ASR) [24, 25] and spoken language understanding (SLU) [25]. It is unclear how they will perform in task-agnostic compres- sion, which is more challenging because the model needs to capture various aspects of speech including content, speaker, semantics and paralinguistics [6]. In this work, we propose DPHuBERT, a task-agnostic com- pression method based on joint Distillation and Pruning. It al- lows the student architecture to be learned during distillation. 2.2. Compression methods for speech SSL Distillation. Distillation methods optimize a small student model to match the targets generated by a large teacher model. Since different layers of speech SSL capture different infor- mation [27], the student model needs to learn both final and intermediate representations of the teacher [12\u201314]. DistilHu- BERT [12] trains a shallow student model by mapping the last student layer to multiple intermediate teacher layers. FitHu- BERT [13] learns a deep and thin student model through layer- to-layer mapping. Another work [14] compares prediction- layer and layer-to-layer distillation using various student archi- tectures. It shows that the architecture of a student model af- fects its performance, even when the model size is kept similar. It also finds that deeper networks perform better with layer-to- layer distillation, likely because it explicitly aligns intermediate layers. These observations have inspired our work which allows the student architecture to evolve during distillation. Pruning. Pruning methods identify and remove redundant pa- rameters from a pre-trained model. Unstructured pruning re- moves individual parameters (e.g., a connection between neu- rons) by setting them to zero, which requires sparse matrix computation libraries to achieve actual speedup, whereas struc- tured pruning removes groups of parameters (e.g., an attention head or even an entire layer), which directly reduces the model size and computational cost. For speech SSL, PARP [24] is a magnitude-based unstructured pruning method which prunes It improves downstream tasks like the Transformer encoder. FFN FFN CNN Audio Waveform Teacher(frozen)Student(prunable)\u2026 Linear FFN\u2026 MHA FFN Linear MHA MHA MHA Linear CNN Linear MHA Linear Linear FFN CNN CNN FFN FFN FFN MHA Teacher(frozen)Student(already pruned)\u2026\u2026 Audio Waveform MHA MHA (a) Step 1: jointly distill and prune the student model. (b) Step 2: further distill the already pruned model. Figure 1: Two training steps of our task-agnostic compression method, DPHuBERT. (a) The student model is initialized from the teacher model and is jointly distilled and pruned to produce a smaller model which meets a pre-specified sparsity ratio. (b) The already pruned student model is further distilled for better performance. To obtain DPHuBERT (24M) from HuBERT Base (95M), the two steps take around 18 and 6 GPU hours, respectively. (Dashed modules are prunable, i.e., their architectures can evolve during training.) low-resource ASR."}