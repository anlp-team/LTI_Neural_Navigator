{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yonatan_Bisk_Plan,_Eliminate,_and_Track_-_Language_Models_are_Good_Teachers_for_Embodied_Agents_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the work by Ahn et al. (2022)?", "answer": " Solving the executability issue by training an action scoring model to re-weigh LLM action choices", "ref_chunk": "have recently achieved success in high-level planning. Huang et al. (2022a) shows that pre-trained LLMs can generate plausible plans for day-to-day tasks, but the generated sub-tasks cannot be directly executed in an end-to-end control environment. Ahn et al. (2022) solves the executability issue by training an action scoring model to re-weigh LLM action choices and demonstrates success on a robot. However, LLM scores work for simple environ- ments with actions limited to pick/place (Ahn et al., 2022), but fails with environments with more objects and diverse actions (Shridhar et al., 2020b). Song et al. (2022) uses GPT3 to generate step-by-step low- level commands, which are then executed by respective control policies. the work improves Ahn et al. (2022) with more action diversity and on-the-\ufb02y re-plan. In addition, all the above LLMs require few-shot demon- strations of up to 17 examples, making the length of the prompt infeasible for AlfWorld. Micheli & Fleuret (2021) \ufb01ne-tuned a GPT2-medium model on expert trajectories in AlfWorld and demonstrated impressive evaluation results. However, LM \ufb01ne-tuning requires a fully text-based environment, consistent expert tra- jectories, and a fully text-based action space. Such requirements greatly limit the generalization to other domains, and even to other forms of task speci\ufb01cation. We show that our PET framework achieves better gener- alization to human goal speci\ufb01cations which the agents were not trained on. 2. An Action Attention agent that handles the chang- ing action space for text environments. 3. A 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. 2. Related Work Language Conditioned Policies A considerable portion of prior work studies imitation learning (Tellex et al., 2011; Mei et al., 2016; Nair et al., 2022; Stepput- tis et al., 2020; Jang et al., 2022; Shridhar et al., 2022; Hierarchical Planning with Natural Language Due to the structured nature of natural language, An- dreas et al. (2017) explored associating each task de- scription to a modular sub-policy. Later works extend the above approach by using a single conditional policy (Mei et al., 2016), or by matching sub-tasks to tem- plates (Oh et al., 2017). Recent works have shown that LLMs are pro\ufb01cient high-level planners (Huang et al., 2022a; Ahn et al., 2022; Lin et al., 2022), and therefore motivates us to revisit the idea of hierarchical task plan- Plan, Eliminate, and Track ning with progress tracking. To our knowledge, PET is the \ufb01rst work combining a zero-shot subtask-level LLM planner and zero-shot LLM progress tracker with a low-level conditional sub-task policy. Text Games Text-based games are complex, interac- tive simulations where the game state and action space are in natural lanugage. They are fertile ground for language-focused machine learning research. In addi- tion to language understanding, successful play requires skills like memory and planning, exploration (trial and error), and common sense. The AlfWorld (Shridhar et al., 2020b) simulator extends a common text-based game simulator, TextWorld C\u02c6ot\u00b4e et al. (2018a), to create text-based analogs of each ALFRED scene. Agents for Large Action Space He et al. (2015) learns representation for state and actions with two di\ufb00erent models and computes the Q function as the inner product of the representations. While this could generalize to large action space, they only considered a small number of actions. Fulda et al. (2017); Ahn et al. (2022) explore action elimination in the setting of a\ufb00ordances. Zahavy et al. (2018) trains a model to eliminate invalid actions on Zork from external environment signals. However, the functionality depends on the existence of external elim- ination signal. Figure 2. Plan Module (Sub-task Generation). 5 full exam- ples are chosen from the training set based on RoBERTa embedding similarity with the task query description. Then the examples are concatenated with the task query to get the prompt. Finally, we prompt the LLM to generate the desired sub-tasks. receptacles and objects within the observation as rt i and ot i respectively. The classi\ufb01cation between receptacles and objects is de\ufb01ned by the environment (Shridhar et al., 2020b). For a task T , we assume there exists a list of sub-tasks ST = {s1, . . . sk} that solves T . 3.1. Plan 3. Plan, Eliminate, and Track In this section, we explain our 3-step framework: Plan, Eliminate, and Track (PET). In Plan module (MP), a pre-trained LLM generates a list of sub-tasks for an input task description using samples from the training set as in-context examples. The Eliminate module (ME) uses a zero-shot QA language model to score and mask objects and receptacles that are irrelevant to the current sub-task. The Track module (MT) uses a zero- shot QA language model to determine if the current sub-task is complete and moves to the next sub-task. Note that Plan is a generative task and Eliminate and Track are classi\ufb01cation tasks. We also implement an attention-based agent (Action Attention), which scores each permissible action and is trained on imitation learning on the expert. The agent observes the masked observation and takes an action conditioned on the current sub-task. Tasks in the real world are often complex and need more than one step to be completed. Motivated by the ability of humans to plan high-level sub-tasks given a complex task, we design the Plan module (MP) to generate a list of high-level sub-tasks for a task description T . Inspired by the contextual prompting techniques for planning with LLMs (Huang et al., 2022a), we use an LLM as our plan module MP. For a given task description T , we compose the query question QT as \u201cWhat are the middle steps required to T ?\u201d, and require MP to generate a list sub-tasks ST = {s1, . . . sk}. Speci\ufb01cally, we select the top 5 example tasks T E from the training set based on RoBERTa (Liu et al., 2019) embedding similarity with the query task T . We then concatenate the example tasks with example sub-tasks in a query-answer format to build the prompt PT for MP (Fig. 2): PT = concat(QT E 1 , ST"}, {"question": " What are the limitations of the work by Shridhar et al. (2020b) regarding LLM scores?", "answer": " LLM scores work for simple environments with limited actions like pick/place but fail with environments containing more objects and diverse actions", "ref_chunk": "have recently achieved success in high-level planning. Huang et al. (2022a) shows that pre-trained LLMs can generate plausible plans for day-to-day tasks, but the generated sub-tasks cannot be directly executed in an end-to-end control environment. Ahn et al. (2022) solves the executability issue by training an action scoring model to re-weigh LLM action choices and demonstrates success on a robot. However, LLM scores work for simple environ- ments with actions limited to pick/place (Ahn et al., 2022), but fails with environments with more objects and diverse actions (Shridhar et al., 2020b). Song et al. (2022) uses GPT3 to generate step-by-step low- level commands, which are then executed by respective control policies. the work improves Ahn et al. (2022) with more action diversity and on-the-\ufb02y re-plan. In addition, all the above LLMs require few-shot demon- strations of up to 17 examples, making the length of the prompt infeasible for AlfWorld. Micheli & Fleuret (2021) \ufb01ne-tuned a GPT2-medium model on expert trajectories in AlfWorld and demonstrated impressive evaluation results. However, LM \ufb01ne-tuning requires a fully text-based environment, consistent expert tra- jectories, and a fully text-based action space. Such requirements greatly limit the generalization to other domains, and even to other forms of task speci\ufb01cation. We show that our PET framework achieves better gener- alization to human goal speci\ufb01cations which the agents were not trained on. 2. An Action Attention agent that handles the chang- ing action space for text environments. 3. A 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. 2. Related Work Language Conditioned Policies A considerable portion of prior work studies imitation learning (Tellex et al., 2011; Mei et al., 2016; Nair et al., 2022; Stepput- tis et al., 2020; Jang et al., 2022; Shridhar et al., 2022; Hierarchical Planning with Natural Language Due to the structured nature of natural language, An- dreas et al. (2017) explored associating each task de- scription to a modular sub-policy. Later works extend the above approach by using a single conditional policy (Mei et al., 2016), or by matching sub-tasks to tem- plates (Oh et al., 2017). Recent works have shown that LLMs are pro\ufb01cient high-level planners (Huang et al., 2022a; Ahn et al., 2022; Lin et al., 2022), and therefore motivates us to revisit the idea of hierarchical task plan- Plan, Eliminate, and Track ning with progress tracking. To our knowledge, PET is the \ufb01rst work combining a zero-shot subtask-level LLM planner and zero-shot LLM progress tracker with a low-level conditional sub-task policy. Text Games Text-based games are complex, interac- tive simulations where the game state and action space are in natural lanugage. They are fertile ground for language-focused machine learning research. In addi- tion to language understanding, successful play requires skills like memory and planning, exploration (trial and error), and common sense. The AlfWorld (Shridhar et al., 2020b) simulator extends a common text-based game simulator, TextWorld C\u02c6ot\u00b4e et al. (2018a), to create text-based analogs of each ALFRED scene. Agents for Large Action Space He et al. (2015) learns representation for state and actions with two di\ufb00erent models and computes the Q function as the inner product of the representations. While this could generalize to large action space, they only considered a small number of actions. Fulda et al. (2017); Ahn et al. (2022) explore action elimination in the setting of a\ufb00ordances. Zahavy et al. (2018) trains a model to eliminate invalid actions on Zork from external environment signals. However, the functionality depends on the existence of external elim- ination signal. Figure 2. Plan Module (Sub-task Generation). 5 full exam- ples are chosen from the training set based on RoBERTa embedding similarity with the task query description. Then the examples are concatenated with the task query to get the prompt. Finally, we prompt the LLM to generate the desired sub-tasks. receptacles and objects within the observation as rt i and ot i respectively. The classi\ufb01cation between receptacles and objects is de\ufb01ned by the environment (Shridhar et al., 2020b). For a task T , we assume there exists a list of sub-tasks ST = {s1, . . . sk} that solves T . 3.1. Plan 3. Plan, Eliminate, and Track In this section, we explain our 3-step framework: Plan, Eliminate, and Track (PET). In Plan module (MP), a pre-trained LLM generates a list of sub-tasks for an input task description using samples from the training set as in-context examples. The Eliminate module (ME) uses a zero-shot QA language model to score and mask objects and receptacles that are irrelevant to the current sub-task. The Track module (MT) uses a zero- shot QA language model to determine if the current sub-task is complete and moves to the next sub-task. Note that Plan is a generative task and Eliminate and Track are classi\ufb01cation tasks. We also implement an attention-based agent (Action Attention), which scores each permissible action and is trained on imitation learning on the expert. The agent observes the masked observation and takes an action conditioned on the current sub-task. Tasks in the real world are often complex and need more than one step to be completed. Motivated by the ability of humans to plan high-level sub-tasks given a complex task, we design the Plan module (MP) to generate a list of high-level sub-tasks for a task description T . Inspired by the contextual prompting techniques for planning with LLMs (Huang et al., 2022a), we use an LLM as our plan module MP. For a given task description T , we compose the query question QT as \u201cWhat are the middle steps required to T ?\u201d, and require MP to generate a list sub-tasks ST = {s1, . . . sk}. Speci\ufb01cally, we select the top 5 example tasks T E from the training set based on RoBERTa (Liu et al., 2019) embedding similarity with the query task T . We then concatenate the example tasks with example sub-tasks in a query-answer format to build the prompt PT for MP (Fig. 2): PT = concat(QT E 1 , ST"}, {"question": " How does Song et al. (2022) improve on the work by Ahn et al. (2022)?", "answer": " By using GPT3 to generate step-by-step low-level commands with more action diversity and on-the-fly re-plan", "ref_chunk": "have recently achieved success in high-level planning. Huang et al. (2022a) shows that pre-trained LLMs can generate plausible plans for day-to-day tasks, but the generated sub-tasks cannot be directly executed in an end-to-end control environment. Ahn et al. (2022) solves the executability issue by training an action scoring model to re-weigh LLM action choices and demonstrates success on a robot. However, LLM scores work for simple environ- ments with actions limited to pick/place (Ahn et al., 2022), but fails with environments with more objects and diverse actions (Shridhar et al., 2020b). Song et al. (2022) uses GPT3 to generate step-by-step low- level commands, which are then executed by respective control policies. the work improves Ahn et al. (2022) with more action diversity and on-the-\ufb02y re-plan. In addition, all the above LLMs require few-shot demon- strations of up to 17 examples, making the length of the prompt infeasible for AlfWorld. Micheli & Fleuret (2021) \ufb01ne-tuned a GPT2-medium model on expert trajectories in AlfWorld and demonstrated impressive evaluation results. However, LM \ufb01ne-tuning requires a fully text-based environment, consistent expert tra- jectories, and a fully text-based action space. Such requirements greatly limit the generalization to other domains, and even to other forms of task speci\ufb01cation. We show that our PET framework achieves better gener- alization to human goal speci\ufb01cations which the agents were not trained on. 2. An Action Attention agent that handles the chang- ing action space for text environments. 3. A 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. 2. Related Work Language Conditioned Policies A considerable portion of prior work studies imitation learning (Tellex et al., 2011; Mei et al., 2016; Nair et al., 2022; Stepput- tis et al., 2020; Jang et al., 2022; Shridhar et al., 2022; Hierarchical Planning with Natural Language Due to the structured nature of natural language, An- dreas et al. (2017) explored associating each task de- scription to a modular sub-policy. Later works extend the above approach by using a single conditional policy (Mei et al., 2016), or by matching sub-tasks to tem- plates (Oh et al., 2017). Recent works have shown that LLMs are pro\ufb01cient high-level planners (Huang et al., 2022a; Ahn et al., 2022; Lin et al., 2022), and therefore motivates us to revisit the idea of hierarchical task plan- Plan, Eliminate, and Track ning with progress tracking. To our knowledge, PET is the \ufb01rst work combining a zero-shot subtask-level LLM planner and zero-shot LLM progress tracker with a low-level conditional sub-task policy. Text Games Text-based games are complex, interac- tive simulations where the game state and action space are in natural lanugage. They are fertile ground for language-focused machine learning research. In addi- tion to language understanding, successful play requires skills like memory and planning, exploration (trial and error), and common sense. The AlfWorld (Shridhar et al., 2020b) simulator extends a common text-based game simulator, TextWorld C\u02c6ot\u00b4e et al. (2018a), to create text-based analogs of each ALFRED scene. Agents for Large Action Space He et al. (2015) learns representation for state and actions with two di\ufb00erent models and computes the Q function as the inner product of the representations. While this could generalize to large action space, they only considered a small number of actions. Fulda et al. (2017); Ahn et al. (2022) explore action elimination in the setting of a\ufb00ordances. Zahavy et al. (2018) trains a model to eliminate invalid actions on Zork from external environment signals. However, the functionality depends on the existence of external elim- ination signal. Figure 2. Plan Module (Sub-task Generation). 5 full exam- ples are chosen from the training set based on RoBERTa embedding similarity with the task query description. Then the examples are concatenated with the task query to get the prompt. Finally, we prompt the LLM to generate the desired sub-tasks. receptacles and objects within the observation as rt i and ot i respectively. The classi\ufb01cation between receptacles and objects is de\ufb01ned by the environment (Shridhar et al., 2020b). For a task T , we assume there exists a list of sub-tasks ST = {s1, . . . sk} that solves T . 3.1. Plan 3. Plan, Eliminate, and Track In this section, we explain our 3-step framework: Plan, Eliminate, and Track (PET). In Plan module (MP), a pre-trained LLM generates a list of sub-tasks for an input task description using samples from the training set as in-context examples. The Eliminate module (ME) uses a zero-shot QA language model to score and mask objects and receptacles that are irrelevant to the current sub-task. The Track module (MT) uses a zero- shot QA language model to determine if the current sub-task is complete and moves to the next sub-task. Note that Plan is a generative task and Eliminate and Track are classi\ufb01cation tasks. We also implement an attention-based agent (Action Attention), which scores each permissible action and is trained on imitation learning on the expert. The agent observes the masked observation and takes an action conditioned on the current sub-task. Tasks in the real world are often complex and need more than one step to be completed. Motivated by the ability of humans to plan high-level sub-tasks given a complex task, we design the Plan module (MP) to generate a list of high-level sub-tasks for a task description T . Inspired by the contextual prompting techniques for planning with LLMs (Huang et al., 2022a), we use an LLM as our plan module MP. For a given task description T , we compose the query question QT as \u201cWhat are the middle steps required to T ?\u201d, and require MP to generate a list sub-tasks ST = {s1, . . . sk}. Speci\ufb01cally, we select the top 5 example tasks T E from the training set based on RoBERTa (Liu et al., 2019) embedding similarity with the query task T . We then concatenate the example tasks with example sub-tasks in a query-answer format to build the prompt PT for MP (Fig. 2): PT = concat(QT E 1 , ST"}, {"question": " What are the requirements for fine-tuning LM models according to Micheli & Fleuret (2021)?", "answer": " Fully text-based environment, consistent expert trajectories, and a fully text-based action space", "ref_chunk": "have recently achieved success in high-level planning. Huang et al. (2022a) shows that pre-trained LLMs can generate plausible plans for day-to-day tasks, but the generated sub-tasks cannot be directly executed in an end-to-end control environment. Ahn et al. (2022) solves the executability issue by training an action scoring model to re-weigh LLM action choices and demonstrates success on a robot. However, LLM scores work for simple environ- ments with actions limited to pick/place (Ahn et al., 2022), but fails with environments with more objects and diverse actions (Shridhar et al., 2020b). Song et al. (2022) uses GPT3 to generate step-by-step low- level commands, which are then executed by respective control policies. the work improves Ahn et al. (2022) with more action diversity and on-the-\ufb02y re-plan. In addition, all the above LLMs require few-shot demon- strations of up to 17 examples, making the length of the prompt infeasible for AlfWorld. Micheli & Fleuret (2021) \ufb01ne-tuned a GPT2-medium model on expert trajectories in AlfWorld and demonstrated impressive evaluation results. However, LM \ufb01ne-tuning requires a fully text-based environment, consistent expert tra- jectories, and a fully text-based action space. Such requirements greatly limit the generalization to other domains, and even to other forms of task speci\ufb01cation. We show that our PET framework achieves better gener- alization to human goal speci\ufb01cations which the agents were not trained on. 2. An Action Attention agent that handles the chang- ing action space for text environments. 3. A 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. 2. Related Work Language Conditioned Policies A considerable portion of prior work studies imitation learning (Tellex et al., 2011; Mei et al., 2016; Nair et al., 2022; Stepput- tis et al., 2020; Jang et al., 2022; Shridhar et al., 2022; Hierarchical Planning with Natural Language Due to the structured nature of natural language, An- dreas et al. (2017) explored associating each task de- scription to a modular sub-policy. Later works extend the above approach by using a single conditional policy (Mei et al., 2016), or by matching sub-tasks to tem- plates (Oh et al., 2017). Recent works have shown that LLMs are pro\ufb01cient high-level planners (Huang et al., 2022a; Ahn et al., 2022; Lin et al., 2022), and therefore motivates us to revisit the idea of hierarchical task plan- Plan, Eliminate, and Track ning with progress tracking. To our knowledge, PET is the \ufb01rst work combining a zero-shot subtask-level LLM planner and zero-shot LLM progress tracker with a low-level conditional sub-task policy. Text Games Text-based games are complex, interac- tive simulations where the game state and action space are in natural lanugage. They are fertile ground for language-focused machine learning research. In addi- tion to language understanding, successful play requires skills like memory and planning, exploration (trial and error), and common sense. The AlfWorld (Shridhar et al., 2020b) simulator extends a common text-based game simulator, TextWorld C\u02c6ot\u00b4e et al. (2018a), to create text-based analogs of each ALFRED scene. Agents for Large Action Space He et al. (2015) learns representation for state and actions with two di\ufb00erent models and computes the Q function as the inner product of the representations. While this could generalize to large action space, they only considered a small number of actions. Fulda et al. (2017); Ahn et al. (2022) explore action elimination in the setting of a\ufb00ordances. Zahavy et al. (2018) trains a model to eliminate invalid actions on Zork from external environment signals. However, the functionality depends on the existence of external elim- ination signal. Figure 2. Plan Module (Sub-task Generation). 5 full exam- ples are chosen from the training set based on RoBERTa embedding similarity with the task query description. Then the examples are concatenated with the task query to get the prompt. Finally, we prompt the LLM to generate the desired sub-tasks. receptacles and objects within the observation as rt i and ot i respectively. The classi\ufb01cation between receptacles and objects is de\ufb01ned by the environment (Shridhar et al., 2020b). For a task T , we assume there exists a list of sub-tasks ST = {s1, . . . sk} that solves T . 3.1. Plan 3. Plan, Eliminate, and Track In this section, we explain our 3-step framework: Plan, Eliminate, and Track (PET). In Plan module (MP), a pre-trained LLM generates a list of sub-tasks for an input task description using samples from the training set as in-context examples. The Eliminate module (ME) uses a zero-shot QA language model to score and mask objects and receptacles that are irrelevant to the current sub-task. The Track module (MT) uses a zero- shot QA language model to determine if the current sub-task is complete and moves to the next sub-task. Note that Plan is a generative task and Eliminate and Track are classi\ufb01cation tasks. We also implement an attention-based agent (Action Attention), which scores each permissible action and is trained on imitation learning on the expert. The agent observes the masked observation and takes an action conditioned on the current sub-task. Tasks in the real world are often complex and need more than one step to be completed. Motivated by the ability of humans to plan high-level sub-tasks given a complex task, we design the Plan module (MP) to generate a list of high-level sub-tasks for a task description T . Inspired by the contextual prompting techniques for planning with LLMs (Huang et al., 2022a), we use an LLM as our plan module MP. For a given task description T , we compose the query question QT as \u201cWhat are the middle steps required to T ?\u201d, and require MP to generate a list sub-tasks ST = {s1, . . . sk}. Speci\ufb01cally, we select the top 5 example tasks T E from the training set based on RoBERTa (Liu et al., 2019) embedding similarity with the query task T . We then concatenate the example tasks with example sub-tasks in a query-answer format to build the prompt PT for MP (Fig. 2): PT = concat(QT E 1 , ST"}, {"question": " What is the PET framework designed to achieve, as mentioned in the text?", "answer": " Better generalization to human goal specifications which the agents were not trained on", "ref_chunk": "have recently achieved success in high-level planning. Huang et al. (2022a) shows that pre-trained LLMs can generate plausible plans for day-to-day tasks, but the generated sub-tasks cannot be directly executed in an end-to-end control environment. Ahn et al. (2022) solves the executability issue by training an action scoring model to re-weigh LLM action choices and demonstrates success on a robot. However, LLM scores work for simple environ- ments with actions limited to pick/place (Ahn et al., 2022), but fails with environments with more objects and diverse actions (Shridhar et al., 2020b). Song et al. (2022) uses GPT3 to generate step-by-step low- level commands, which are then executed by respective control policies. the work improves Ahn et al. (2022) with more action diversity and on-the-\ufb02y re-plan. In addition, all the above LLMs require few-shot demon- strations of up to 17 examples, making the length of the prompt infeasible for AlfWorld. Micheli & Fleuret (2021) \ufb01ne-tuned a GPT2-medium model on expert trajectories in AlfWorld and demonstrated impressive evaluation results. However, LM \ufb01ne-tuning requires a fully text-based environment, consistent expert tra- jectories, and a fully text-based action space. Such requirements greatly limit the generalization to other domains, and even to other forms of task speci\ufb01cation. We show that our PET framework achieves better gener- alization to human goal speci\ufb01cations which the agents were not trained on. 2. An Action Attention agent that handles the chang- ing action space for text environments. 3. A 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. 2. Related Work Language Conditioned Policies A considerable portion of prior work studies imitation learning (Tellex et al., 2011; Mei et al., 2016; Nair et al., 2022; Stepput- tis et al., 2020; Jang et al., 2022; Shridhar et al., 2022; Hierarchical Planning with Natural Language Due to the structured nature of natural language, An- dreas et al. (2017) explored associating each task de- scription to a modular sub-policy. Later works extend the above approach by using a single conditional policy (Mei et al., 2016), or by matching sub-tasks to tem- plates (Oh et al., 2017). Recent works have shown that LLMs are pro\ufb01cient high-level planners (Huang et al., 2022a; Ahn et al., 2022; Lin et al., 2022), and therefore motivates us to revisit the idea of hierarchical task plan- Plan, Eliminate, and Track ning with progress tracking. To our knowledge, PET is the \ufb01rst work combining a zero-shot subtask-level LLM planner and zero-shot LLM progress tracker with a low-level conditional sub-task policy. Text Games Text-based games are complex, interac- tive simulations where the game state and action space are in natural lanugage. They are fertile ground for language-focused machine learning research. In addi- tion to language understanding, successful play requires skills like memory and planning, exploration (trial and error), and common sense. The AlfWorld (Shridhar et al., 2020b) simulator extends a common text-based game simulator, TextWorld C\u02c6ot\u00b4e et al. (2018a), to create text-based analogs of each ALFRED scene. Agents for Large Action Space He et al. (2015) learns representation for state and actions with two di\ufb00erent models and computes the Q function as the inner product of the representations. While this could generalize to large action space, they only considered a small number of actions. Fulda et al. (2017); Ahn et al. (2022) explore action elimination in the setting of a\ufb00ordances. Zahavy et al. (2018) trains a model to eliminate invalid actions on Zork from external environment signals. However, the functionality depends on the existence of external elim- ination signal. Figure 2. Plan Module (Sub-task Generation). 5 full exam- ples are chosen from the training set based on RoBERTa embedding similarity with the task query description. Then the examples are concatenated with the task query to get the prompt. Finally, we prompt the LLM to generate the desired sub-tasks. receptacles and objects within the observation as rt i and ot i respectively. The classi\ufb01cation between receptacles and objects is de\ufb01ned by the environment (Shridhar et al., 2020b). For a task T , we assume there exists a list of sub-tasks ST = {s1, . . . sk} that solves T . 3.1. Plan 3. Plan, Eliminate, and Track In this section, we explain our 3-step framework: Plan, Eliminate, and Track (PET). In Plan module (MP), a pre-trained LLM generates a list of sub-tasks for an input task description using samples from the training set as in-context examples. The Eliminate module (ME) uses a zero-shot QA language model to score and mask objects and receptacles that are irrelevant to the current sub-task. The Track module (MT) uses a zero- shot QA language model to determine if the current sub-task is complete and moves to the next sub-task. Note that Plan is a generative task and Eliminate and Track are classi\ufb01cation tasks. We also implement an attention-based agent (Action Attention), which scores each permissible action and is trained on imitation learning on the expert. The agent observes the masked observation and takes an action conditioned on the current sub-task. Tasks in the real world are often complex and need more than one step to be completed. Motivated by the ability of humans to plan high-level sub-tasks given a complex task, we design the Plan module (MP) to generate a list of high-level sub-tasks for a task description T . Inspired by the contextual prompting techniques for planning with LLMs (Huang et al., 2022a), we use an LLM as our plan module MP. For a given task description T , we compose the query question QT as \u201cWhat are the middle steps required to T ?\u201d, and require MP to generate a list sub-tasks ST = {s1, . . . sk}. Speci\ufb01cally, we select the top 5 example tasks T E from the training set based on RoBERTa (Liu et al., 2019) embedding similarity with the query task T . We then concatenate the example tasks with example sub-tasks in a query-answer format to build the prompt PT for MP (Fig. 2): PT = concat(QT E 1 , ST"}, {"question": " What is the purpose of the Action Attention agent mentioned in the text?", "answer": " To handle the changing action space for text environments", "ref_chunk": "have recently achieved success in high-level planning. Huang et al. (2022a) shows that pre-trained LLMs can generate plausible plans for day-to-day tasks, but the generated sub-tasks cannot be directly executed in an end-to-end control environment. Ahn et al. (2022) solves the executability issue by training an action scoring model to re-weigh LLM action choices and demonstrates success on a robot. However, LLM scores work for simple environ- ments with actions limited to pick/place (Ahn et al., 2022), but fails with environments with more objects and diverse actions (Shridhar et al., 2020b). Song et al. (2022) uses GPT3 to generate step-by-step low- level commands, which are then executed by respective control policies. the work improves Ahn et al. (2022) with more action diversity and on-the-\ufb02y re-plan. In addition, all the above LLMs require few-shot demon- strations of up to 17 examples, making the length of the prompt infeasible for AlfWorld. Micheli & Fleuret (2021) \ufb01ne-tuned a GPT2-medium model on expert trajectories in AlfWorld and demonstrated impressive evaluation results. However, LM \ufb01ne-tuning requires a fully text-based environment, consistent expert tra- jectories, and a fully text-based action space. Such requirements greatly limit the generalization to other domains, and even to other forms of task speci\ufb01cation. We show that our PET framework achieves better gener- alization to human goal speci\ufb01cations which the agents were not trained on. 2. An Action Attention agent that handles the chang- ing action space for text environments. 3. A 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. 2. Related Work Language Conditioned Policies A considerable portion of prior work studies imitation learning (Tellex et al., 2011; Mei et al., 2016; Nair et al., 2022; Stepput- tis et al., 2020; Jang et al., 2022; Shridhar et al., 2022; Hierarchical Planning with Natural Language Due to the structured nature of natural language, An- dreas et al. (2017) explored associating each task de- scription to a modular sub-policy. Later works extend the above approach by using a single conditional policy (Mei et al., 2016), or by matching sub-tasks to tem- plates (Oh et al., 2017). Recent works have shown that LLMs are pro\ufb01cient high-level planners (Huang et al., 2022a; Ahn et al., 2022; Lin et al., 2022), and therefore motivates us to revisit the idea of hierarchical task plan- Plan, Eliminate, and Track ning with progress tracking. To our knowledge, PET is the \ufb01rst work combining a zero-shot subtask-level LLM planner and zero-shot LLM progress tracker with a low-level conditional sub-task policy. Text Games Text-based games are complex, interac- tive simulations where the game state and action space are in natural lanugage. They are fertile ground for language-focused machine learning research. In addi- tion to language understanding, successful play requires skills like memory and planning, exploration (trial and error), and common sense. The AlfWorld (Shridhar et al., 2020b) simulator extends a common text-based game simulator, TextWorld C\u02c6ot\u00b4e et al. (2018a), to create text-based analogs of each ALFRED scene. Agents for Large Action Space He et al. (2015) learns representation for state and actions with two di\ufb00erent models and computes the Q function as the inner product of the representations. While this could generalize to large action space, they only considered a small number of actions. Fulda et al. (2017); Ahn et al. (2022) explore action elimination in the setting of a\ufb00ordances. Zahavy et al. (2018) trains a model to eliminate invalid actions on Zork from external environment signals. However, the functionality depends on the existence of external elim- ination signal. Figure 2. Plan Module (Sub-task Generation). 5 full exam- ples are chosen from the training set based on RoBERTa embedding similarity with the task query description. Then the examples are concatenated with the task query to get the prompt. Finally, we prompt the LLM to generate the desired sub-tasks. receptacles and objects within the observation as rt i and ot i respectively. The classi\ufb01cation between receptacles and objects is de\ufb01ned by the environment (Shridhar et al., 2020b). For a task T , we assume there exists a list of sub-tasks ST = {s1, . . . sk} that solves T . 3.1. Plan 3. Plan, Eliminate, and Track In this section, we explain our 3-step framework: Plan, Eliminate, and Track (PET). In Plan module (MP), a pre-trained LLM generates a list of sub-tasks for an input task description using samples from the training set as in-context examples. The Eliminate module (ME) uses a zero-shot QA language model to score and mask objects and receptacles that are irrelevant to the current sub-task. The Track module (MT) uses a zero- shot QA language model to determine if the current sub-task is complete and moves to the next sub-task. Note that Plan is a generative task and Eliminate and Track are classi\ufb01cation tasks. We also implement an attention-based agent (Action Attention), which scores each permissible action and is trained on imitation learning on the expert. The agent observes the masked observation and takes an action conditioned on the current sub-task. Tasks in the real world are often complex and need more than one step to be completed. Motivated by the ability of humans to plan high-level sub-tasks given a complex task, we design the Plan module (MP) to generate a list of high-level sub-tasks for a task description T . Inspired by the contextual prompting techniques for planning with LLMs (Huang et al., 2022a), we use an LLM as our plan module MP. For a given task description T , we compose the query question QT as \u201cWhat are the middle steps required to T ?\u201d, and require MP to generate a list sub-tasks ST = {s1, . . . sk}. Speci\ufb01cally, we select the top 5 example tasks T E from the training set based on RoBERTa (Liu et al., 2019) embedding similarity with the query task T . We then concatenate the example tasks with example sub-tasks in a query-answer format to build the prompt PT for MP (Fig. 2): PT = concat(QT E 1 , ST"}, {"question": " What does the PET framework achieve in terms of generalization to human goals?", "answer": " A 15% improvement over the state-of-the-art for generalization via sub-task planning and tracking", "ref_chunk": "have recently achieved success in high-level planning. Huang et al. (2022a) shows that pre-trained LLMs can generate plausible plans for day-to-day tasks, but the generated sub-tasks cannot be directly executed in an end-to-end control environment. Ahn et al. (2022) solves the executability issue by training an action scoring model to re-weigh LLM action choices and demonstrates success on a robot. However, LLM scores work for simple environ- ments with actions limited to pick/place (Ahn et al., 2022), but fails with environments with more objects and diverse actions (Shridhar et al., 2020b). Song et al. (2022) uses GPT3 to generate step-by-step low- level commands, which are then executed by respective control policies. the work improves Ahn et al. (2022) with more action diversity and on-the-\ufb02y re-plan. In addition, all the above LLMs require few-shot demon- strations of up to 17 examples, making the length of the prompt infeasible for AlfWorld. Micheli & Fleuret (2021) \ufb01ne-tuned a GPT2-medium model on expert trajectories in AlfWorld and demonstrated impressive evaluation results. However, LM \ufb01ne-tuning requires a fully text-based environment, consistent expert tra- jectories, and a fully text-based action space. Such requirements greatly limit the generalization to other domains, and even to other forms of task speci\ufb01cation. We show that our PET framework achieves better gener- alization to human goal speci\ufb01cations which the agents were not trained on. 2. An Action Attention agent that handles the chang- ing action space for text environments. 3. A 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. 2. Related Work Language Conditioned Policies A considerable portion of prior work studies imitation learning (Tellex et al., 2011; Mei et al., 2016; Nair et al., 2022; Stepput- tis et al., 2020; Jang et al., 2022; Shridhar et al., 2022; Hierarchical Planning with Natural Language Due to the structured nature of natural language, An- dreas et al. (2017) explored associating each task de- scription to a modular sub-policy. Later works extend the above approach by using a single conditional policy (Mei et al., 2016), or by matching sub-tasks to tem- plates (Oh et al., 2017). Recent works have shown that LLMs are pro\ufb01cient high-level planners (Huang et al., 2022a; Ahn et al., 2022; Lin et al., 2022), and therefore motivates us to revisit the idea of hierarchical task plan- Plan, Eliminate, and Track ning with progress tracking. To our knowledge, PET is the \ufb01rst work combining a zero-shot subtask-level LLM planner and zero-shot LLM progress tracker with a low-level conditional sub-task policy. Text Games Text-based games are complex, interac- tive simulations where the game state and action space are in natural lanugage. They are fertile ground for language-focused machine learning research. In addi- tion to language understanding, successful play requires skills like memory and planning, exploration (trial and error), and common sense. The AlfWorld (Shridhar et al., 2020b) simulator extends a common text-based game simulator, TextWorld C\u02c6ot\u00b4e et al. (2018a), to create text-based analogs of each ALFRED scene. Agents for Large Action Space He et al. (2015) learns representation for state and actions with two di\ufb00erent models and computes the Q function as the inner product of the representations. While this could generalize to large action space, they only considered a small number of actions. Fulda et al. (2017); Ahn et al. (2022) explore action elimination in the setting of a\ufb00ordances. Zahavy et al. (2018) trains a model to eliminate invalid actions on Zork from external environment signals. However, the functionality depends on the existence of external elim- ination signal. Figure 2. Plan Module (Sub-task Generation). 5 full exam- ples are chosen from the training set based on RoBERTa embedding similarity with the task query description. Then the examples are concatenated with the task query to get the prompt. Finally, we prompt the LLM to generate the desired sub-tasks. receptacles and objects within the observation as rt i and ot i respectively. The classi\ufb01cation between receptacles and objects is de\ufb01ned by the environment (Shridhar et al., 2020b). For a task T , we assume there exists a list of sub-tasks ST = {s1, . . . sk} that solves T . 3.1. Plan 3. Plan, Eliminate, and Track In this section, we explain our 3-step framework: Plan, Eliminate, and Track (PET). In Plan module (MP), a pre-trained LLM generates a list of sub-tasks for an input task description using samples from the training set as in-context examples. The Eliminate module (ME) uses a zero-shot QA language model to score and mask objects and receptacles that are irrelevant to the current sub-task. The Track module (MT) uses a zero- shot QA language model to determine if the current sub-task is complete and moves to the next sub-task. Note that Plan is a generative task and Eliminate and Track are classi\ufb01cation tasks. We also implement an attention-based agent (Action Attention), which scores each permissible action and is trained on imitation learning on the expert. The agent observes the masked observation and takes an action conditioned on the current sub-task. Tasks in the real world are often complex and need more than one step to be completed. Motivated by the ability of humans to plan high-level sub-tasks given a complex task, we design the Plan module (MP) to generate a list of high-level sub-tasks for a task description T . Inspired by the contextual prompting techniques for planning with LLMs (Huang et al., 2022a), we use an LLM as our plan module MP. For a given task description T , we compose the query question QT as \u201cWhat are the middle steps required to T ?\u201d, and require MP to generate a list sub-tasks ST = {s1, . . . sk}. Speci\ufb01cally, we select the top 5 example tasks T E from the training set based on RoBERTa (Liu et al., 2019) embedding similarity with the query task T . We then concatenate the example tasks with example sub-tasks in a query-answer format to build the prompt PT for MP (Fig. 2): PT = concat(QT E 1 , ST"}, {"question": " What is the main idea behind the hierarchical planning with progress tracking discussed in the text?", "answer": " Combining a zero-shot subtask-level LLM planner and progress tracker with a low-level conditional sub-task policy", "ref_chunk": "have recently achieved success in high-level planning. Huang et al. (2022a) shows that pre-trained LLMs can generate plausible plans for day-to-day tasks, but the generated sub-tasks cannot be directly executed in an end-to-end control environment. Ahn et al. (2022) solves the executability issue by training an action scoring model to re-weigh LLM action choices and demonstrates success on a robot. However, LLM scores work for simple environ- ments with actions limited to pick/place (Ahn et al., 2022), but fails with environments with more objects and diverse actions (Shridhar et al., 2020b). Song et al. (2022) uses GPT3 to generate step-by-step low- level commands, which are then executed by respective control policies. the work improves Ahn et al. (2022) with more action diversity and on-the-\ufb02y re-plan. In addition, all the above LLMs require few-shot demon- strations of up to 17 examples, making the length of the prompt infeasible for AlfWorld. Micheli & Fleuret (2021) \ufb01ne-tuned a GPT2-medium model on expert trajectories in AlfWorld and demonstrated impressive evaluation results. However, LM \ufb01ne-tuning requires a fully text-based environment, consistent expert tra- jectories, and a fully text-based action space. Such requirements greatly limit the generalization to other domains, and even to other forms of task speci\ufb01cation. We show that our PET framework achieves better gener- alization to human goal speci\ufb01cations which the agents were not trained on. 2. An Action Attention agent that handles the chang- ing action space for text environments. 3. A 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. 2. Related Work Language Conditioned Policies A considerable portion of prior work studies imitation learning (Tellex et al., 2011; Mei et al., 2016; Nair et al., 2022; Stepput- tis et al., 2020; Jang et al., 2022; Shridhar et al., 2022; Hierarchical Planning with Natural Language Due to the structured nature of natural language, An- dreas et al. (2017) explored associating each task de- scription to a modular sub-policy. Later works extend the above approach by using a single conditional policy (Mei et al., 2016), or by matching sub-tasks to tem- plates (Oh et al., 2017). Recent works have shown that LLMs are pro\ufb01cient high-level planners (Huang et al., 2022a; Ahn et al., 2022; Lin et al., 2022), and therefore motivates us to revisit the idea of hierarchical task plan- Plan, Eliminate, and Track ning with progress tracking. To our knowledge, PET is the \ufb01rst work combining a zero-shot subtask-level LLM planner and zero-shot LLM progress tracker with a low-level conditional sub-task policy. Text Games Text-based games are complex, interac- tive simulations where the game state and action space are in natural lanugage. They are fertile ground for language-focused machine learning research. In addi- tion to language understanding, successful play requires skills like memory and planning, exploration (trial and error), and common sense. The AlfWorld (Shridhar et al., 2020b) simulator extends a common text-based game simulator, TextWorld C\u02c6ot\u00b4e et al. (2018a), to create text-based analogs of each ALFRED scene. Agents for Large Action Space He et al. (2015) learns representation for state and actions with two di\ufb00erent models and computes the Q function as the inner product of the representations. While this could generalize to large action space, they only considered a small number of actions. Fulda et al. (2017); Ahn et al. (2022) explore action elimination in the setting of a\ufb00ordances. Zahavy et al. (2018) trains a model to eliminate invalid actions on Zork from external environment signals. However, the functionality depends on the existence of external elim- ination signal. Figure 2. Plan Module (Sub-task Generation). 5 full exam- ples are chosen from the training set based on RoBERTa embedding similarity with the task query description. Then the examples are concatenated with the task query to get the prompt. Finally, we prompt the LLM to generate the desired sub-tasks. receptacles and objects within the observation as rt i and ot i respectively. The classi\ufb01cation between receptacles and objects is de\ufb01ned by the environment (Shridhar et al., 2020b). For a task T , we assume there exists a list of sub-tasks ST = {s1, . . . sk} that solves T . 3.1. Plan 3. Plan, Eliminate, and Track In this section, we explain our 3-step framework: Plan, Eliminate, and Track (PET). In Plan module (MP), a pre-trained LLM generates a list of sub-tasks for an input task description using samples from the training set as in-context examples. The Eliminate module (ME) uses a zero-shot QA language model to score and mask objects and receptacles that are irrelevant to the current sub-task. The Track module (MT) uses a zero- shot QA language model to determine if the current sub-task is complete and moves to the next sub-task. Note that Plan is a generative task and Eliminate and Track are classi\ufb01cation tasks. We also implement an attention-based agent (Action Attention), which scores each permissible action and is trained on imitation learning on the expert. The agent observes the masked observation and takes an action conditioned on the current sub-task. Tasks in the real world are often complex and need more than one step to be completed. Motivated by the ability of humans to plan high-level sub-tasks given a complex task, we design the Plan module (MP) to generate a list of high-level sub-tasks for a task description T . Inspired by the contextual prompting techniques for planning with LLMs (Huang et al., 2022a), we use an LLM as our plan module MP. For a given task description T , we compose the query question QT as \u201cWhat are the middle steps required to T ?\u201d, and require MP to generate a list sub-tasks ST = {s1, . . . sk}. Speci\ufb01cally, we select the top 5 example tasks T E from the training set based on RoBERTa (Liu et al., 2019) embedding similarity with the query task T . We then concatenate the example tasks with example sub-tasks in a query-answer format to build the prompt PT for MP (Fig. 2): PT = concat(QT E 1 , ST"}, {"question": " What skills are required for successful play in text-based games according to the text?", "answer": " Memory, planning, exploration, and common sense", "ref_chunk": "have recently achieved success in high-level planning. Huang et al. (2022a) shows that pre-trained LLMs can generate plausible plans for day-to-day tasks, but the generated sub-tasks cannot be directly executed in an end-to-end control environment. Ahn et al. (2022) solves the executability issue by training an action scoring model to re-weigh LLM action choices and demonstrates success on a robot. However, LLM scores work for simple environ- ments with actions limited to pick/place (Ahn et al., 2022), but fails with environments with more objects and diverse actions (Shridhar et al., 2020b). Song et al. (2022) uses GPT3 to generate step-by-step low- level commands, which are then executed by respective control policies. the work improves Ahn et al. (2022) with more action diversity and on-the-\ufb02y re-plan. In addition, all the above LLMs require few-shot demon- strations of up to 17 examples, making the length of the prompt infeasible for AlfWorld. Micheli & Fleuret (2021) \ufb01ne-tuned a GPT2-medium model on expert trajectories in AlfWorld and demonstrated impressive evaluation results. However, LM \ufb01ne-tuning requires a fully text-based environment, consistent expert tra- jectories, and a fully text-based action space. Such requirements greatly limit the generalization to other domains, and even to other forms of task speci\ufb01cation. We show that our PET framework achieves better gener- alization to human goal speci\ufb01cations which the agents were not trained on. 2. An Action Attention agent that handles the chang- ing action space for text environments. 3. A 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. 2. Related Work Language Conditioned Policies A considerable portion of prior work studies imitation learning (Tellex et al., 2011; Mei et al., 2016; Nair et al., 2022; Stepput- tis et al., 2020; Jang et al., 2022; Shridhar et al., 2022; Hierarchical Planning with Natural Language Due to the structured nature of natural language, An- dreas et al. (2017) explored associating each task de- scription to a modular sub-policy. Later works extend the above approach by using a single conditional policy (Mei et al., 2016), or by matching sub-tasks to tem- plates (Oh et al., 2017). Recent works have shown that LLMs are pro\ufb01cient high-level planners (Huang et al., 2022a; Ahn et al., 2022; Lin et al., 2022), and therefore motivates us to revisit the idea of hierarchical task plan- Plan, Eliminate, and Track ning with progress tracking. To our knowledge, PET is the \ufb01rst work combining a zero-shot subtask-level LLM planner and zero-shot LLM progress tracker with a low-level conditional sub-task policy. Text Games Text-based games are complex, interac- tive simulations where the game state and action space are in natural lanugage. They are fertile ground for language-focused machine learning research. In addi- tion to language understanding, successful play requires skills like memory and planning, exploration (trial and error), and common sense. The AlfWorld (Shridhar et al., 2020b) simulator extends a common text-based game simulator, TextWorld C\u02c6ot\u00b4e et al. (2018a), to create text-based analogs of each ALFRED scene. Agents for Large Action Space He et al. (2015) learns representation for state and actions with two di\ufb00erent models and computes the Q function as the inner product of the representations. While this could generalize to large action space, they only considered a small number of actions. Fulda et al. (2017); Ahn et al. (2022) explore action elimination in the setting of a\ufb00ordances. Zahavy et al. (2018) trains a model to eliminate invalid actions on Zork from external environment signals. However, the functionality depends on the existence of external elim- ination signal. Figure 2. Plan Module (Sub-task Generation). 5 full exam- ples are chosen from the training set based on RoBERTa embedding similarity with the task query description. Then the examples are concatenated with the task query to get the prompt. Finally, we prompt the LLM to generate the desired sub-tasks. receptacles and objects within the observation as rt i and ot i respectively. The classi\ufb01cation between receptacles and objects is de\ufb01ned by the environment (Shridhar et al., 2020b). For a task T , we assume there exists a list of sub-tasks ST = {s1, . . . sk} that solves T . 3.1. Plan 3. Plan, Eliminate, and Track In this section, we explain our 3-step framework: Plan, Eliminate, and Track (PET). In Plan module (MP), a pre-trained LLM generates a list of sub-tasks for an input task description using samples from the training set as in-context examples. The Eliminate module (ME) uses a zero-shot QA language model to score and mask objects and receptacles that are irrelevant to the current sub-task. The Track module (MT) uses a zero- shot QA language model to determine if the current sub-task is complete and moves to the next sub-task. Note that Plan is a generative task and Eliminate and Track are classi\ufb01cation tasks. We also implement an attention-based agent (Action Attention), which scores each permissible action and is trained on imitation learning on the expert. The agent observes the masked observation and takes an action conditioned on the current sub-task. Tasks in the real world are often complex and need more than one step to be completed. Motivated by the ability of humans to plan high-level sub-tasks given a complex task, we design the Plan module (MP) to generate a list of high-level sub-tasks for a task description T . Inspired by the contextual prompting techniques for planning with LLMs (Huang et al., 2022a), we use an LLM as our plan module MP. For a given task description T , we compose the query question QT as \u201cWhat are the middle steps required to T ?\u201d, and require MP to generate a list sub-tasks ST = {s1, . . . sk}. Speci\ufb01cally, we select the top 5 example tasks T E from the training set based on RoBERTa (Liu et al., 2019) embedding similarity with the query task T . We then concatenate the example tasks with example sub-tasks in a query-answer format to build the prompt PT for MP (Fig. 2): PT = concat(QT E 1 , ST"}, {"question": " How does the Plan module in the PET framework work?", "answer": " Pre-trained LLM generates a list of sub-tasks for an input task description using samples from the training set as in-context examples", "ref_chunk": "have recently achieved success in high-level planning. Huang et al. (2022a) shows that pre-trained LLMs can generate plausible plans for day-to-day tasks, but the generated sub-tasks cannot be directly executed in an end-to-end control environment. Ahn et al. (2022) solves the executability issue by training an action scoring model to re-weigh LLM action choices and demonstrates success on a robot. However, LLM scores work for simple environ- ments with actions limited to pick/place (Ahn et al., 2022), but fails with environments with more objects and diverse actions (Shridhar et al., 2020b). Song et al. (2022) uses GPT3 to generate step-by-step low- level commands, which are then executed by respective control policies. the work improves Ahn et al. (2022) with more action diversity and on-the-\ufb02y re-plan. In addition, all the above LLMs require few-shot demon- strations of up to 17 examples, making the length of the prompt infeasible for AlfWorld. Micheli & Fleuret (2021) \ufb01ne-tuned a GPT2-medium model on expert trajectories in AlfWorld and demonstrated impressive evaluation results. However, LM \ufb01ne-tuning requires a fully text-based environment, consistent expert tra- jectories, and a fully text-based action space. Such requirements greatly limit the generalization to other domains, and even to other forms of task speci\ufb01cation. We show that our PET framework achieves better gener- alization to human goal speci\ufb01cations which the agents were not trained on. 2. An Action Attention agent that handles the chang- ing action space for text environments. 3. A 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. 2. Related Work Language Conditioned Policies A considerable portion of prior work studies imitation learning (Tellex et al., 2011; Mei et al., 2016; Nair et al., 2022; Stepput- tis et al., 2020; Jang et al., 2022; Shridhar et al., 2022; Hierarchical Planning with Natural Language Due to the structured nature of natural language, An- dreas et al. (2017) explored associating each task de- scription to a modular sub-policy. Later works extend the above approach by using a single conditional policy (Mei et al., 2016), or by matching sub-tasks to tem- plates (Oh et al., 2017). Recent works have shown that LLMs are pro\ufb01cient high-level planners (Huang et al., 2022a; Ahn et al., 2022; Lin et al., 2022), and therefore motivates us to revisit the idea of hierarchical task plan- Plan, Eliminate, and Track ning with progress tracking. To our knowledge, PET is the \ufb01rst work combining a zero-shot subtask-level LLM planner and zero-shot LLM progress tracker with a low-level conditional sub-task policy. Text Games Text-based games are complex, interac- tive simulations where the game state and action space are in natural lanugage. They are fertile ground for language-focused machine learning research. In addi- tion to language understanding, successful play requires skills like memory and planning, exploration (trial and error), and common sense. The AlfWorld (Shridhar et al., 2020b) simulator extends a common text-based game simulator, TextWorld C\u02c6ot\u00b4e et al. (2018a), to create text-based analogs of each ALFRED scene. Agents for Large Action Space He et al. (2015) learns representation for state and actions with two di\ufb00erent models and computes the Q function as the inner product of the representations. While this could generalize to large action space, they only considered a small number of actions. Fulda et al. (2017); Ahn et al. (2022) explore action elimination in the setting of a\ufb00ordances. Zahavy et al. (2018) trains a model to eliminate invalid actions on Zork from external environment signals. However, the functionality depends on the existence of external elim- ination signal. Figure 2. Plan Module (Sub-task Generation). 5 full exam- ples are chosen from the training set based on RoBERTa embedding similarity with the task query description. Then the examples are concatenated with the task query to get the prompt. Finally, we prompt the LLM to generate the desired sub-tasks. receptacles and objects within the observation as rt i and ot i respectively. The classi\ufb01cation between receptacles and objects is de\ufb01ned by the environment (Shridhar et al., 2020b). For a task T , we assume there exists a list of sub-tasks ST = {s1, . . . sk} that solves T . 3.1. Plan 3. Plan, Eliminate, and Track In this section, we explain our 3-step framework: Plan, Eliminate, and Track (PET). In Plan module (MP), a pre-trained LLM generates a list of sub-tasks for an input task description using samples from the training set as in-context examples. The Eliminate module (ME) uses a zero-shot QA language model to score and mask objects and receptacles that are irrelevant to the current sub-task. The Track module (MT) uses a zero- shot QA language model to determine if the current sub-task is complete and moves to the next sub-task. Note that Plan is a generative task and Eliminate and Track are classi\ufb01cation tasks. We also implement an attention-based agent (Action Attention), which scores each permissible action and is trained on imitation learning on the expert. The agent observes the masked observation and takes an action conditioned on the current sub-task. Tasks in the real world are often complex and need more than one step to be completed. Motivated by the ability of humans to plan high-level sub-tasks given a complex task, we design the Plan module (MP) to generate a list of high-level sub-tasks for a task description T . Inspired by the contextual prompting techniques for planning with LLMs (Huang et al., 2022a), we use an LLM as our plan module MP. For a given task description T , we compose the query question QT as \u201cWhat are the middle steps required to T ?\u201d, and require MP to generate a list sub-tasks ST = {s1, . . . sk}. Speci\ufb01cally, we select the top 5 example tasks T E from the training set based on RoBERTa (Liu et al., 2019) embedding similarity with the query task T . We then concatenate the example tasks with example sub-tasks in a query-answer format to build the prompt PT for MP (Fig. 2): PT = concat(QT E 1 , ST"}], "doc_text": "have recently achieved success in high-level planning. Huang et al. (2022a) shows that pre-trained LLMs can generate plausible plans for day-to-day tasks, but the generated sub-tasks cannot be directly executed in an end-to-end control environment. Ahn et al. (2022) solves the executability issue by training an action scoring model to re-weigh LLM action choices and demonstrates success on a robot. However, LLM scores work for simple environ- ments with actions limited to pick/place (Ahn et al., 2022), but fails with environments with more objects and diverse actions (Shridhar et al., 2020b). Song et al. (2022) uses GPT3 to generate step-by-step low- level commands, which are then executed by respective control policies. the work improves Ahn et al. (2022) with more action diversity and on-the-\ufb02y re-plan. In addition, all the above LLMs require few-shot demon- strations of up to 17 examples, making the length of the prompt infeasible for AlfWorld. Micheli & Fleuret (2021) \ufb01ne-tuned a GPT2-medium model on expert trajectories in AlfWorld and demonstrated impressive evaluation results. However, LM \ufb01ne-tuning requires a fully text-based environment, consistent expert tra- jectories, and a fully text-based action space. Such requirements greatly limit the generalization to other domains, and even to other forms of task speci\ufb01cation. We show that our PET framework achieves better gener- alization to human goal speci\ufb01cations which the agents were not trained on. 2. An Action Attention agent that handles the chang- ing action space for text environments. 3. A 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. 2. Related Work Language Conditioned Policies A considerable portion of prior work studies imitation learning (Tellex et al., 2011; Mei et al., 2016; Nair et al., 2022; Stepput- tis et al., 2020; Jang et al., 2022; Shridhar et al., 2022; Hierarchical Planning with Natural Language Due to the structured nature of natural language, An- dreas et al. (2017) explored associating each task de- scription to a modular sub-policy. Later works extend the above approach by using a single conditional policy (Mei et al., 2016), or by matching sub-tasks to tem- plates (Oh et al., 2017). Recent works have shown that LLMs are pro\ufb01cient high-level planners (Huang et al., 2022a; Ahn et al., 2022; Lin et al., 2022), and therefore motivates us to revisit the idea of hierarchical task plan- Plan, Eliminate, and Track ning with progress tracking. To our knowledge, PET is the \ufb01rst work combining a zero-shot subtask-level LLM planner and zero-shot LLM progress tracker with a low-level conditional sub-task policy. Text Games Text-based games are complex, interac- tive simulations where the game state and action space are in natural lanugage. They are fertile ground for language-focused machine learning research. In addi- tion to language understanding, successful play requires skills like memory and planning, exploration (trial and error), and common sense. The AlfWorld (Shridhar et al., 2020b) simulator extends a common text-based game simulator, TextWorld C\u02c6ot\u00b4e et al. (2018a), to create text-based analogs of each ALFRED scene. Agents for Large Action Space He et al. (2015) learns representation for state and actions with two di\ufb00erent models and computes the Q function as the inner product of the representations. While this could generalize to large action space, they only considered a small number of actions. Fulda et al. (2017); Ahn et al. (2022) explore action elimination in the setting of a\ufb00ordances. Zahavy et al. (2018) trains a model to eliminate invalid actions on Zork from external environment signals. However, the functionality depends on the existence of external elim- ination signal. Figure 2. Plan Module (Sub-task Generation). 5 full exam- ples are chosen from the training set based on RoBERTa embedding similarity with the task query description. Then the examples are concatenated with the task query to get the prompt. Finally, we prompt the LLM to generate the desired sub-tasks. receptacles and objects within the observation as rt i and ot i respectively. The classi\ufb01cation between receptacles and objects is de\ufb01ned by the environment (Shridhar et al., 2020b). For a task T , we assume there exists a list of sub-tasks ST = {s1, . . . sk} that solves T . 3.1. Plan 3. Plan, Eliminate, and Track In this section, we explain our 3-step framework: Plan, Eliminate, and Track (PET). In Plan module (MP), a pre-trained LLM generates a list of sub-tasks for an input task description using samples from the training set as in-context examples. The Eliminate module (ME) uses a zero-shot QA language model to score and mask objects and receptacles that are irrelevant to the current sub-task. The Track module (MT) uses a zero- shot QA language model to determine if the current sub-task is complete and moves to the next sub-task. Note that Plan is a generative task and Eliminate and Track are classi\ufb01cation tasks. We also implement an attention-based agent (Action Attention), which scores each permissible action and is trained on imitation learning on the expert. The agent observes the masked observation and takes an action conditioned on the current sub-task. Tasks in the real world are often complex and need more than one step to be completed. Motivated by the ability of humans to plan high-level sub-tasks given a complex task, we design the Plan module (MP) to generate a list of high-level sub-tasks for a task description T . Inspired by the contextual prompting techniques for planning with LLMs (Huang et al., 2022a), we use an LLM as our plan module MP. For a given task description T , we compose the query question QT as \u201cWhat are the middle steps required to T ?\u201d, and require MP to generate a list sub-tasks ST = {s1, . . . sk}. Speci\ufb01cally, we select the top 5 example tasks T E from the training set based on RoBERTa (Liu et al., 2019) embedding similarity with the query task T . We then concatenate the example tasks with example sub-tasks in a query-answer format to build the prompt PT for MP (Fig. 2): PT = concat(QT E 1 , ST"}