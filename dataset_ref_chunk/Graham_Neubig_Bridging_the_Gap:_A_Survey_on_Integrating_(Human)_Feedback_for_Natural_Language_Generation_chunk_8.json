{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Bridging_the_Gap:_A_Survey_on_Integrating_(Human)_Feedback_for_Natural_Language_Generation_chunk_8.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are some considerations discussed for experimenters creating preference datasets for their own use cases?,answer: Experimenters should leverage existing resources, consider additional data collection carefully, and be mindful of ethical considerations.", "ref_chunk": "ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and 10 Preprint their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethi- cal considerations in the use and collection of hu- man feedback. In future, richer types of feedback may be col- lected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007; Amershi et al., 2014a; Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022; Gehrmann et al., 2022b). 6.2 Pitfalls and Ethical Considerations of Human Feedback Although we have focused on the idealized form of human feedback in \u00a72.1, actual feedback may be low-quality, contradictory, or adversarial. 5 As discussed in \u00a73.2, we must carefully specify an- notation guidelines so that feedback is aligned to- wards the actual goals for the model (Ziegler et al., 2019). Even in the case where human experts are available, different groups of experts may not agree (Kahneman et al., 2021). In this section, we enu- merate possible issues with human feedback, most of which are shared with other annotation tasks. We also touch on possible mitigation strategies. 6.1 Considerations in Data Collection 6.2.1 Subjectivity and variance in judgment There are multiple facets to consider when collect- ing human feedback data for a generation task; a non-exhaustive list of axes along which data collec- tion can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008; Sheng et al., 2008; Clark et al., 2021; Gillick and Liu, 2010; Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020; Bai et al., 2022a; Freitag et al., 2021). 3. Collection method: Data can be gathered ex- plicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018; Freitag et al., 2021). 4. Collection platform: Common platforms in- clude Amazon Mechanical Turk, Upwork, and Scale AI. Considering K annotators with feedback functions K hi i=1, judgments are given on data D = d1, ..., dN . Inter-rater reliability metrics, such as Cohen\u2019s Kappa, Fleiss\u2019 Kappa, or Krippendorff\u2019s alpha, can assess annotator agreement (Hayes and Krip- pendorff, 2007; Fleiss, 1971; Cohen, 1960). Low reliability may result from unclear tasks or evalu- ation criteria (Gehrmann et al., 2022b; Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022; Nie et al., 2020; Gordon et al., 2022). Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021; Zerva et al., 2022). Clear annotation guidelines and in- cluding rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019). 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 6.2.2 Bias in judgment Even if all K annotators agree on a particular judg- ment for a certain data point, they may all be mis- taken. There are well-known biases in human rea- soning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account. Furthermore, even if anno- tators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of Preprint Task Dataset & their descriptions Collection method Platform Feedback Type Language assistant HH-RLHF (Bai et al., 2022a; Perez et al., 2022a) Explicit Upwork, MTurk Ranking Language assistant SHP (Ethayarajh et al., 2023) Implicit Scraped from Reddit Ranking/Score Summarization summarize-from-feedback (Stiennon et al., 2020) Explicit Upwork Ranking Question Answering FeedbackQA (Li et al., 2022) Explicit MTurk Score, NL Translation WMT Metrics Shared Task (Freitag et al., 2022b) Explicit Pro translation workflow MQM, DA Summarization TAC Shared Tasks (TAC-2008, TAC-2009) Explicit N/A Score Table 2: Summary of existing human feedback datasets and their collection methods, which vary along several dimensions. Refer to Table 1 for definitions related to feedback types. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT. N/A means we could not find information. systematic bias away from the originally intended task (Parmar et al., 2023). Anchoring/Confirmation bias: When annota- tors are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021)."}, {"question": " What is mentioned as a potential future direction for human feedback data collection?,answer: Richer types of feedback may be collected to make use of more nuanced signals beyond ranking or numerical scores.", "ref_chunk": "ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and 10 Preprint their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethi- cal considerations in the use and collection of hu- man feedback. In future, richer types of feedback may be col- lected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007; Amershi et al., 2014a; Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022; Gehrmann et al., 2022b). 6.2 Pitfalls and Ethical Considerations of Human Feedback Although we have focused on the idealized form of human feedback in \u00a72.1, actual feedback may be low-quality, contradictory, or adversarial. 5 As discussed in \u00a73.2, we must carefully specify an- notation guidelines so that feedback is aligned to- wards the actual goals for the model (Ziegler et al., 2019). Even in the case where human experts are available, different groups of experts may not agree (Kahneman et al., 2021). In this section, we enu- merate possible issues with human feedback, most of which are shared with other annotation tasks. We also touch on possible mitigation strategies. 6.1 Considerations in Data Collection 6.2.1 Subjectivity and variance in judgment There are multiple facets to consider when collect- ing human feedback data for a generation task; a non-exhaustive list of axes along which data collec- tion can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008; Sheng et al., 2008; Clark et al., 2021; Gillick and Liu, 2010; Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020; Bai et al., 2022a; Freitag et al., 2021). 3. Collection method: Data can be gathered ex- plicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018; Freitag et al., 2021). 4. Collection platform: Common platforms in- clude Amazon Mechanical Turk, Upwork, and Scale AI. Considering K annotators with feedback functions K hi i=1, judgments are given on data D = d1, ..., dN . Inter-rater reliability metrics, such as Cohen\u2019s Kappa, Fleiss\u2019 Kappa, or Krippendorff\u2019s alpha, can assess annotator agreement (Hayes and Krip- pendorff, 2007; Fleiss, 1971; Cohen, 1960). Low reliability may result from unclear tasks or evalu- ation criteria (Gehrmann et al., 2022b; Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022; Nie et al., 2020; Gordon et al., 2022). Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021; Zerva et al., 2022). Clear annotation guidelines and in- cluding rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019). 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 6.2.2 Bias in judgment Even if all K annotators agree on a particular judg- ment for a certain data point, they may all be mis- taken. There are well-known biases in human rea- soning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account. Furthermore, even if anno- tators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of Preprint Task Dataset & their descriptions Collection method Platform Feedback Type Language assistant HH-RLHF (Bai et al., 2022a; Perez et al., 2022a) Explicit Upwork, MTurk Ranking Language assistant SHP (Ethayarajh et al., 2023) Implicit Scraped from Reddit Ranking/Score Summarization summarize-from-feedback (Stiennon et al., 2020) Explicit Upwork Ranking Question Answering FeedbackQA (Li et al., 2022) Explicit MTurk Score, NL Translation WMT Metrics Shared Task (Freitag et al., 2022b) Explicit Pro translation workflow MQM, DA Summarization TAC Shared Tasks (TAC-2008, TAC-2009) Explicit N/A Score Table 2: Summary of existing human feedback datasets and their collection methods, which vary along several dimensions. Refer to Table 1 for definitions related to feedback types. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT. N/A means we could not find information. systematic bias away from the originally intended task (Parmar et al., 2023). Anchoring/Confirmation bias: When annota- tors are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021)."}, {"question": " What are some pitfalls and ethical considerations of human feedback mentioned in the text?,answer: Human feedback may be low-quality, contradictory, or adversarial, and there may be issues with annotator agreement and conflicting expert opinions.", "ref_chunk": "ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and 10 Preprint their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethi- cal considerations in the use and collection of hu- man feedback. In future, richer types of feedback may be col- lected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007; Amershi et al., 2014a; Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022; Gehrmann et al., 2022b). 6.2 Pitfalls and Ethical Considerations of Human Feedback Although we have focused on the idealized form of human feedback in \u00a72.1, actual feedback may be low-quality, contradictory, or adversarial. 5 As discussed in \u00a73.2, we must carefully specify an- notation guidelines so that feedback is aligned to- wards the actual goals for the model (Ziegler et al., 2019). Even in the case where human experts are available, different groups of experts may not agree (Kahneman et al., 2021). In this section, we enu- merate possible issues with human feedback, most of which are shared with other annotation tasks. We also touch on possible mitigation strategies. 6.1 Considerations in Data Collection 6.2.1 Subjectivity and variance in judgment There are multiple facets to consider when collect- ing human feedback data for a generation task; a non-exhaustive list of axes along which data collec- tion can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008; Sheng et al., 2008; Clark et al., 2021; Gillick and Liu, 2010; Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020; Bai et al., 2022a; Freitag et al., 2021). 3. Collection method: Data can be gathered ex- plicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018; Freitag et al., 2021). 4. Collection platform: Common platforms in- clude Amazon Mechanical Turk, Upwork, and Scale AI. Considering K annotators with feedback functions K hi i=1, judgments are given on data D = d1, ..., dN . Inter-rater reliability metrics, such as Cohen\u2019s Kappa, Fleiss\u2019 Kappa, or Krippendorff\u2019s alpha, can assess annotator agreement (Hayes and Krip- pendorff, 2007; Fleiss, 1971; Cohen, 1960). Low reliability may result from unclear tasks or evalu- ation criteria (Gehrmann et al., 2022b; Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022; Nie et al., 2020; Gordon et al., 2022). Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021; Zerva et al., 2022). Clear annotation guidelines and in- cluding rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019). 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 6.2.2 Bias in judgment Even if all K annotators agree on a particular judg- ment for a certain data point, they may all be mis- taken. There are well-known biases in human rea- soning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account. Furthermore, even if anno- tators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of Preprint Task Dataset & their descriptions Collection method Platform Feedback Type Language assistant HH-RLHF (Bai et al., 2022a; Perez et al., 2022a) Explicit Upwork, MTurk Ranking Language assistant SHP (Ethayarajh et al., 2023) Implicit Scraped from Reddit Ranking/Score Summarization summarize-from-feedback (Stiennon et al., 2020) Explicit Upwork Ranking Question Answering FeedbackQA (Li et al., 2022) Explicit MTurk Score, NL Translation WMT Metrics Shared Task (Freitag et al., 2022b) Explicit Pro translation workflow MQM, DA Summarization TAC Shared Tasks (TAC-2008, TAC-2009) Explicit N/A Score Table 2: Summary of existing human feedback datasets and their collection methods, which vary along several dimensions. Refer to Table 1 for definitions related to feedback types. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT. N/A means we could not find information. systematic bias away from the originally intended task (Parmar et al., 2023). Anchoring/Confirmation bias: When annota- tors are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021)."}, {"question": " What are some facets to consider when collecting human feedback data for a generation task?,answer: Annotator expertise, length of engagement, collection method, collection platform, and annotator demographics are important facets to consider.", "ref_chunk": "ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and 10 Preprint their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethi- cal considerations in the use and collection of hu- man feedback. In future, richer types of feedback may be col- lected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007; Amershi et al., 2014a; Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022; Gehrmann et al., 2022b). 6.2 Pitfalls and Ethical Considerations of Human Feedback Although we have focused on the idealized form of human feedback in \u00a72.1, actual feedback may be low-quality, contradictory, or adversarial. 5 As discussed in \u00a73.2, we must carefully specify an- notation guidelines so that feedback is aligned to- wards the actual goals for the model (Ziegler et al., 2019). Even in the case where human experts are available, different groups of experts may not agree (Kahneman et al., 2021). In this section, we enu- merate possible issues with human feedback, most of which are shared with other annotation tasks. We also touch on possible mitigation strategies. 6.1 Considerations in Data Collection 6.2.1 Subjectivity and variance in judgment There are multiple facets to consider when collect- ing human feedback data for a generation task; a non-exhaustive list of axes along which data collec- tion can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008; Sheng et al., 2008; Clark et al., 2021; Gillick and Liu, 2010; Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020; Bai et al., 2022a; Freitag et al., 2021). 3. Collection method: Data can be gathered ex- plicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018; Freitag et al., 2021). 4. Collection platform: Common platforms in- clude Amazon Mechanical Turk, Upwork, and Scale AI. Considering K annotators with feedback functions K hi i=1, judgments are given on data D = d1, ..., dN . Inter-rater reliability metrics, such as Cohen\u2019s Kappa, Fleiss\u2019 Kappa, or Krippendorff\u2019s alpha, can assess annotator agreement (Hayes and Krip- pendorff, 2007; Fleiss, 1971; Cohen, 1960). Low reliability may result from unclear tasks or evalu- ation criteria (Gehrmann et al., 2022b; Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022; Nie et al., 2020; Gordon et al., 2022). Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021; Zerva et al., 2022). Clear annotation guidelines and in- cluding rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019). 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 6.2.2 Bias in judgment Even if all K annotators agree on a particular judg- ment for a certain data point, they may all be mis- taken. There are well-known biases in human rea- soning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account. Furthermore, even if anno- tators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of Preprint Task Dataset & their descriptions Collection method Platform Feedback Type Language assistant HH-RLHF (Bai et al., 2022a; Perez et al., 2022a) Explicit Upwork, MTurk Ranking Language assistant SHP (Ethayarajh et al., 2023) Implicit Scraped from Reddit Ranking/Score Summarization summarize-from-feedback (Stiennon et al., 2020) Explicit Upwork Ranking Question Answering FeedbackQA (Li et al., 2022) Explicit MTurk Score, NL Translation WMT Metrics Shared Task (Freitag et al., 2022b) Explicit Pro translation workflow MQM, DA Summarization TAC Shared Tasks (TAC-2008, TAC-2009) Explicit N/A Score Table 2: Summary of existing human feedback datasets and their collection methods, which vary along several dimensions. Refer to Table 1 for definitions related to feedback types. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT. N/A means we could not find information. systematic bias away from the originally intended task (Parmar et al., 2023). Anchoring/Confirmation bias: When annota- tors are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021)."}, {"question": " How can low reliability in annotator agreement be mitigated?,answer: Mitigation strategies include clarifying tasks, learning from multiple humans, and augmenting evaluation metrics with confidence intervals.", "ref_chunk": "ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and 10 Preprint their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethi- cal considerations in the use and collection of hu- man feedback. In future, richer types of feedback may be col- lected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007; Amershi et al., 2014a; Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022; Gehrmann et al., 2022b). 6.2 Pitfalls and Ethical Considerations of Human Feedback Although we have focused on the idealized form of human feedback in \u00a72.1, actual feedback may be low-quality, contradictory, or adversarial. 5 As discussed in \u00a73.2, we must carefully specify an- notation guidelines so that feedback is aligned to- wards the actual goals for the model (Ziegler et al., 2019). Even in the case where human experts are available, different groups of experts may not agree (Kahneman et al., 2021). In this section, we enu- merate possible issues with human feedback, most of which are shared with other annotation tasks. We also touch on possible mitigation strategies. 6.1 Considerations in Data Collection 6.2.1 Subjectivity and variance in judgment There are multiple facets to consider when collect- ing human feedback data for a generation task; a non-exhaustive list of axes along which data collec- tion can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008; Sheng et al., 2008; Clark et al., 2021; Gillick and Liu, 2010; Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020; Bai et al., 2022a; Freitag et al., 2021). 3. Collection method: Data can be gathered ex- plicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018; Freitag et al., 2021). 4. Collection platform: Common platforms in- clude Amazon Mechanical Turk, Upwork, and Scale AI. Considering K annotators with feedback functions K hi i=1, judgments are given on data D = d1, ..., dN . Inter-rater reliability metrics, such as Cohen\u2019s Kappa, Fleiss\u2019 Kappa, or Krippendorff\u2019s alpha, can assess annotator agreement (Hayes and Krip- pendorff, 2007; Fleiss, 1971; Cohen, 1960). Low reliability may result from unclear tasks or evalu- ation criteria (Gehrmann et al., 2022b; Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022; Nie et al., 2020; Gordon et al., 2022). Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021; Zerva et al., 2022). Clear annotation guidelines and in- cluding rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019). 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 6.2.2 Bias in judgment Even if all K annotators agree on a particular judg- ment for a certain data point, they may all be mis- taken. There are well-known biases in human rea- soning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account. Furthermore, even if anno- tators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of Preprint Task Dataset & their descriptions Collection method Platform Feedback Type Language assistant HH-RLHF (Bai et al., 2022a; Perez et al., 2022a) Explicit Upwork, MTurk Ranking Language assistant SHP (Ethayarajh et al., 2023) Implicit Scraped from Reddit Ranking/Score Summarization summarize-from-feedback (Stiennon et al., 2020) Explicit Upwork Ranking Question Answering FeedbackQA (Li et al., 2022) Explicit MTurk Score, NL Translation WMT Metrics Shared Task (Freitag et al., 2022b) Explicit Pro translation workflow MQM, DA Summarization TAC Shared Tasks (TAC-2008, TAC-2009) Explicit N/A Score Table 2: Summary of existing human feedback datasets and their collection methods, which vary along several dimensions. Refer to Table 1 for definitions related to feedback types. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT. N/A means we could not find information. systematic bias away from the originally intended task (Parmar et al., 2023). Anchoring/Confirmation bias: When annota- tors are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021)."}, {"question": " What is the trade-off mentioned in data collection between effort and reliability of judgments?,answer: There is a trade-off between the effort needed to create datasets and the reliability of judgments collected, especially in higher-stakes applications.", "ref_chunk": "ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and 10 Preprint their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethi- cal considerations in the use and collection of hu- man feedback. In future, richer types of feedback may be col- lected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007; Amershi et al., 2014a; Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022; Gehrmann et al., 2022b). 6.2 Pitfalls and Ethical Considerations of Human Feedback Although we have focused on the idealized form of human feedback in \u00a72.1, actual feedback may be low-quality, contradictory, or adversarial. 5 As discussed in \u00a73.2, we must carefully specify an- notation guidelines so that feedback is aligned to- wards the actual goals for the model (Ziegler et al., 2019). Even in the case where human experts are available, different groups of experts may not agree (Kahneman et al., 2021). In this section, we enu- merate possible issues with human feedback, most of which are shared with other annotation tasks. We also touch on possible mitigation strategies. 6.1 Considerations in Data Collection 6.2.1 Subjectivity and variance in judgment There are multiple facets to consider when collect- ing human feedback data for a generation task; a non-exhaustive list of axes along which data collec- tion can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008; Sheng et al., 2008; Clark et al., 2021; Gillick and Liu, 2010; Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020; Bai et al., 2022a; Freitag et al., 2021). 3. Collection method: Data can be gathered ex- plicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018; Freitag et al., 2021). 4. Collection platform: Common platforms in- clude Amazon Mechanical Turk, Upwork, and Scale AI. Considering K annotators with feedback functions K hi i=1, judgments are given on data D = d1, ..., dN . Inter-rater reliability metrics, such as Cohen\u2019s Kappa, Fleiss\u2019 Kappa, or Krippendorff\u2019s alpha, can assess annotator agreement (Hayes and Krip- pendorff, 2007; Fleiss, 1971; Cohen, 1960). Low reliability may result from unclear tasks or evalu- ation criteria (Gehrmann et al., 2022b; Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022; Nie et al., 2020; Gordon et al., 2022). Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021; Zerva et al., 2022). Clear annotation guidelines and in- cluding rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019). 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 6.2.2 Bias in judgment Even if all K annotators agree on a particular judg- ment for a certain data point, they may all be mis- taken. There are well-known biases in human rea- soning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account. Furthermore, even if anno- tators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of Preprint Task Dataset & their descriptions Collection method Platform Feedback Type Language assistant HH-RLHF (Bai et al., 2022a; Perez et al., 2022a) Explicit Upwork, MTurk Ranking Language assistant SHP (Ethayarajh et al., 2023) Implicit Scraped from Reddit Ranking/Score Summarization summarize-from-feedback (Stiennon et al., 2020) Explicit Upwork Ranking Question Answering FeedbackQA (Li et al., 2022) Explicit MTurk Score, NL Translation WMT Metrics Shared Task (Freitag et al., 2022b) Explicit Pro translation workflow MQM, DA Summarization TAC Shared Tasks (TAC-2008, TAC-2009) Explicit N/A Score Table 2: Summary of existing human feedback datasets and their collection methods, which vary along several dimensions. Refer to Table 1 for definitions related to feedback types. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT. N/A means we could not find information. systematic bias away from the originally intended task (Parmar et al., 2023). Anchoring/Confirmation bias: When annota- tors are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021)."}, {"question": " What is the potential impact of bias in judgment by annotators?,answer: Bias in judgment can lead to all annotators being mistaken or not considering evidence, impacting the quality of feedback and evaluations.", "ref_chunk": "ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and 10 Preprint their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethi- cal considerations in the use and collection of hu- man feedback. In future, richer types of feedback may be col- lected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007; Amershi et al., 2014a; Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022; Gehrmann et al., 2022b). 6.2 Pitfalls and Ethical Considerations of Human Feedback Although we have focused on the idealized form of human feedback in \u00a72.1, actual feedback may be low-quality, contradictory, or adversarial. 5 As discussed in \u00a73.2, we must carefully specify an- notation guidelines so that feedback is aligned to- wards the actual goals for the model (Ziegler et al., 2019). Even in the case where human experts are available, different groups of experts may not agree (Kahneman et al., 2021). In this section, we enu- merate possible issues with human feedback, most of which are shared with other annotation tasks. We also touch on possible mitigation strategies. 6.1 Considerations in Data Collection 6.2.1 Subjectivity and variance in judgment There are multiple facets to consider when collect- ing human feedback data for a generation task; a non-exhaustive list of axes along which data collec- tion can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008; Sheng et al., 2008; Clark et al., 2021; Gillick and Liu, 2010; Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020; Bai et al., 2022a; Freitag et al., 2021). 3. Collection method: Data can be gathered ex- plicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018; Freitag et al., 2021). 4. Collection platform: Common platforms in- clude Amazon Mechanical Turk, Upwork, and Scale AI. Considering K annotators with feedback functions K hi i=1, judgments are given on data D = d1, ..., dN . Inter-rater reliability metrics, such as Cohen\u2019s Kappa, Fleiss\u2019 Kappa, or Krippendorff\u2019s alpha, can assess annotator agreement (Hayes and Krip- pendorff, 2007; Fleiss, 1971; Cohen, 1960). Low reliability may result from unclear tasks or evalu- ation criteria (Gehrmann et al., 2022b; Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022; Nie et al., 2020; Gordon et al., 2022). Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021; Zerva et al., 2022). Clear annotation guidelines and in- cluding rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019). 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 6.2.2 Bias in judgment Even if all K annotators agree on a particular judg- ment for a certain data point, they may all be mis- taken. There are well-known biases in human rea- soning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account. Furthermore, even if anno- tators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of Preprint Task Dataset & their descriptions Collection method Platform Feedback Type Language assistant HH-RLHF (Bai et al., 2022a; Perez et al., 2022a) Explicit Upwork, MTurk Ranking Language assistant SHP (Ethayarajh et al., 2023) Implicit Scraped from Reddit Ranking/Score Summarization summarize-from-feedback (Stiennon et al., 2020) Explicit Upwork Ranking Question Answering FeedbackQA (Li et al., 2022) Explicit MTurk Score, NL Translation WMT Metrics Shared Task (Freitag et al., 2022b) Explicit Pro translation workflow MQM, DA Summarization TAC Shared Tasks (TAC-2008, TAC-2009) Explicit N/A Score Table 2: Summary of existing human feedback datasets and their collection methods, which vary along several dimensions. Refer to Table 1 for definitions related to feedback types. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT. N/A means we could not find information. systematic bias away from the originally intended task (Parmar et al., 2023). Anchoring/Confirmation bias: When annota- tors are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021)."}, {"question": " What platforms are commonly used for data collection, as mentioned in the text?,answer: Common platforms include Amazon Mechanical Turk, Upwork, and Scale AI.", "ref_chunk": "ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and 10 Preprint their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethi- cal considerations in the use and collection of hu- man feedback. In future, richer types of feedback may be col- lected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007; Amershi et al., 2014a; Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022; Gehrmann et al., 2022b). 6.2 Pitfalls and Ethical Considerations of Human Feedback Although we have focused on the idealized form of human feedback in \u00a72.1, actual feedback may be low-quality, contradictory, or adversarial. 5 As discussed in \u00a73.2, we must carefully specify an- notation guidelines so that feedback is aligned to- wards the actual goals for the model (Ziegler et al., 2019). Even in the case where human experts are available, different groups of experts may not agree (Kahneman et al., 2021). In this section, we enu- merate possible issues with human feedback, most of which are shared with other annotation tasks. We also touch on possible mitigation strategies. 6.1 Considerations in Data Collection 6.2.1 Subjectivity and variance in judgment There are multiple facets to consider when collect- ing human feedback data for a generation task; a non-exhaustive list of axes along which data collec- tion can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008; Sheng et al., 2008; Clark et al., 2021; Gillick and Liu, 2010; Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020; Bai et al., 2022a; Freitag et al., 2021). 3. Collection method: Data can be gathered ex- plicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018; Freitag et al., 2021). 4. Collection platform: Common platforms in- clude Amazon Mechanical Turk, Upwork, and Scale AI. Considering K annotators with feedback functions K hi i=1, judgments are given on data D = d1, ..., dN . Inter-rater reliability metrics, such as Cohen\u2019s Kappa, Fleiss\u2019 Kappa, or Krippendorff\u2019s alpha, can assess annotator agreement (Hayes and Krip- pendorff, 2007; Fleiss, 1971; Cohen, 1960). Low reliability may result from unclear tasks or evalu- ation criteria (Gehrmann et al., 2022b; Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022; Nie et al., 2020; Gordon et al., 2022). Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021; Zerva et al., 2022). Clear annotation guidelines and in- cluding rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019). 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 6.2.2 Bias in judgment Even if all K annotators agree on a particular judg- ment for a certain data point, they may all be mis- taken. There are well-known biases in human rea- soning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account. Furthermore, even if anno- tators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of Preprint Task Dataset & their descriptions Collection method Platform Feedback Type Language assistant HH-RLHF (Bai et al., 2022a; Perez et al., 2022a) Explicit Upwork, MTurk Ranking Language assistant SHP (Ethayarajh et al., 2023) Implicit Scraped from Reddit Ranking/Score Summarization summarize-from-feedback (Stiennon et al., 2020) Explicit Upwork Ranking Question Answering FeedbackQA (Li et al., 2022) Explicit MTurk Score, NL Translation WMT Metrics Shared Task (Freitag et al., 2022b) Explicit Pro translation workflow MQM, DA Summarization TAC Shared Tasks (TAC-2008, TAC-2009) Explicit N/A Score Table 2: Summary of existing human feedback datasets and their collection methods, which vary along several dimensions. Refer to Table 1 for definitions related to feedback types. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT. N/A means we could not find information. systematic bias away from the originally intended task (Parmar et al., 2023). Anchoring/Confirmation bias: When annota- tors are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021)."}, {"question": " What is the significance of annotator demographics in data collection?,answer: Annotator demographics play a role in providing diverse opinions on quality generations and avoiding overfitting to specific demographics.", "ref_chunk": "ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and 10 Preprint their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethi- cal considerations in the use and collection of hu- man feedback. In future, richer types of feedback may be col- lected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007; Amershi et al., 2014a; Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022; Gehrmann et al., 2022b). 6.2 Pitfalls and Ethical Considerations of Human Feedback Although we have focused on the idealized form of human feedback in \u00a72.1, actual feedback may be low-quality, contradictory, or adversarial. 5 As discussed in \u00a73.2, we must carefully specify an- notation guidelines so that feedback is aligned to- wards the actual goals for the model (Ziegler et al., 2019). Even in the case where human experts are available, different groups of experts may not agree (Kahneman et al., 2021). In this section, we enu- merate possible issues with human feedback, most of which are shared with other annotation tasks. We also touch on possible mitigation strategies. 6.1 Considerations in Data Collection 6.2.1 Subjectivity and variance in judgment There are multiple facets to consider when collect- ing human feedback data for a generation task; a non-exhaustive list of axes along which data collec- tion can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008; Sheng et al., 2008; Clark et al., 2021; Gillick and Liu, 2010; Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020; Bai et al., 2022a; Freitag et al., 2021). 3. Collection method: Data can be gathered ex- plicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018; Freitag et al., 2021). 4. Collection platform: Common platforms in- clude Amazon Mechanical Turk, Upwork, and Scale AI. Considering K annotators with feedback functions K hi i=1, judgments are given on data D = d1, ..., dN . Inter-rater reliability metrics, such as Cohen\u2019s Kappa, Fleiss\u2019 Kappa, or Krippendorff\u2019s alpha, can assess annotator agreement (Hayes and Krip- pendorff, 2007; Fleiss, 1971; Cohen, 1960). Low reliability may result from unclear tasks or evalu- ation criteria (Gehrmann et al., 2022b; Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022; Nie et al., 2020; Gordon et al., 2022). Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021; Zerva et al., 2022). Clear annotation guidelines and in- cluding rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019). 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 6.2.2 Bias in judgment Even if all K annotators agree on a particular judg- ment for a certain data point, they may all be mis- taken. There are well-known biases in human rea- soning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account. Furthermore, even if anno- tators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of Preprint Task Dataset & their descriptions Collection method Platform Feedback Type Language assistant HH-RLHF (Bai et al., 2022a; Perez et al., 2022a) Explicit Upwork, MTurk Ranking Language assistant SHP (Ethayarajh et al., 2023) Implicit Scraped from Reddit Ranking/Score Summarization summarize-from-feedback (Stiennon et al., 2020) Explicit Upwork Ranking Question Answering FeedbackQA (Li et al., 2022) Explicit MTurk Score, NL Translation WMT Metrics Shared Task (Freitag et al., 2022b) Explicit Pro translation workflow MQM, DA Summarization TAC Shared Tasks (TAC-2008, TAC-2009) Explicit N/A Score Table 2: Summary of existing human feedback datasets and their collection methods, which vary along several dimensions. Refer to Table 1 for definitions related to feedback types. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT. N/A means we could not find information. systematic bias away from the originally intended task (Parmar et al., 2023). Anchoring/Confirmation bias: When annota- tors are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021)."}, {"question": " What are some biases in human reasoning mentioned in the text?,answer: Anchoring/Confirmation bias is highlighted, where annotators may fail to consider alternatives when evaluating text in isolation.", "ref_chunk": "ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and 10 Preprint their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethi- cal considerations in the use and collection of hu- man feedback. In future, richer types of feedback may be col- lected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007; Amershi et al., 2014a; Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022; Gehrmann et al., 2022b). 6.2 Pitfalls and Ethical Considerations of Human Feedback Although we have focused on the idealized form of human feedback in \u00a72.1, actual feedback may be low-quality, contradictory, or adversarial. 5 As discussed in \u00a73.2, we must carefully specify an- notation guidelines so that feedback is aligned to- wards the actual goals for the model (Ziegler et al., 2019). Even in the case where human experts are available, different groups of experts may not agree (Kahneman et al., 2021). In this section, we enu- merate possible issues with human feedback, most of which are shared with other annotation tasks. We also touch on possible mitigation strategies. 6.1 Considerations in Data Collection 6.2.1 Subjectivity and variance in judgment There are multiple facets to consider when collect- ing human feedback data for a generation task; a non-exhaustive list of axes along which data collec- tion can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008; Sheng et al., 2008; Clark et al., 2021; Gillick and Liu, 2010; Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020; Bai et al., 2022a; Freitag et al., 2021). 3. Collection method: Data can be gathered ex- plicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018; Freitag et al., 2021). 4. Collection platform: Common platforms in- clude Amazon Mechanical Turk, Upwork, and Scale AI. Considering K annotators with feedback functions K hi i=1, judgments are given on data D = d1, ..., dN . Inter-rater reliability metrics, such as Cohen\u2019s Kappa, Fleiss\u2019 Kappa, or Krippendorff\u2019s alpha, can assess annotator agreement (Hayes and Krip- pendorff, 2007; Fleiss, 1971; Cohen, 1960). Low reliability may result from unclear tasks or evalu- ation criteria (Gehrmann et al., 2022b; Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022; Nie et al., 2020; Gordon et al., 2022). Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021; Zerva et al., 2022). Clear annotation guidelines and in- cluding rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019). 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 6.2.2 Bias in judgment Even if all K annotators agree on a particular judg- ment for a certain data point, they may all be mis- taken. There are well-known biases in human rea- soning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account. Furthermore, even if anno- tators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of Preprint Task Dataset & their descriptions Collection method Platform Feedback Type Language assistant HH-RLHF (Bai et al., 2022a; Perez et al., 2022a) Explicit Upwork, MTurk Ranking Language assistant SHP (Ethayarajh et al., 2023) Implicit Scraped from Reddit Ranking/Score Summarization summarize-from-feedback (Stiennon et al., 2020) Explicit Upwork Ranking Question Answering FeedbackQA (Li et al., 2022) Explicit MTurk Score, NL Translation WMT Metrics Shared Task (Freitag et al., 2022b) Explicit Pro translation workflow MQM, DA Summarization TAC Shared Tasks (TAC-2008, TAC-2009) Explicit N/A Score Table 2: Summary of existing human feedback datasets and their collection methods, which vary along several dimensions. Refer to Table 1 for definitions related to feedback types. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT. N/A means we could not find information. systematic bias away from the originally intended task (Parmar et al., 2023). Anchoring/Confirmation bias: When annota- tors are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021)."}], "doc_text": "ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and 10 Preprint their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethi- cal considerations in the use and collection of hu- man feedback. In future, richer types of feedback may be col- lected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007; Amershi et al., 2014a; Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022; Gehrmann et al., 2022b). 6.2 Pitfalls and Ethical Considerations of Human Feedback Although we have focused on the idealized form of human feedback in \u00a72.1, actual feedback may be low-quality, contradictory, or adversarial. 5 As discussed in \u00a73.2, we must carefully specify an- notation guidelines so that feedback is aligned to- wards the actual goals for the model (Ziegler et al., 2019). Even in the case where human experts are available, different groups of experts may not agree (Kahneman et al., 2021). In this section, we enu- merate possible issues with human feedback, most of which are shared with other annotation tasks. We also touch on possible mitigation strategies. 6.1 Considerations in Data Collection 6.2.1 Subjectivity and variance in judgment There are multiple facets to consider when collect- ing human feedback data for a generation task; a non-exhaustive list of axes along which data collec- tion can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008; Sheng et al., 2008; Clark et al., 2021; Gillick and Liu, 2010; Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020; Bai et al., 2022a; Freitag et al., 2021). 3. Collection method: Data can be gathered ex- plicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018; Freitag et al., 2021). 4. Collection platform: Common platforms in- clude Amazon Mechanical Turk, Upwork, and Scale AI. Considering K annotators with feedback functions K hi i=1, judgments are given on data D = d1, ..., dN . Inter-rater reliability metrics, such as Cohen\u2019s Kappa, Fleiss\u2019 Kappa, or Krippendorff\u2019s alpha, can assess annotator agreement (Hayes and Krip- pendorff, 2007; Fleiss, 1971; Cohen, 1960). Low reliability may result from unclear tasks or evalu- ation criteria (Gehrmann et al., 2022b; Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022; Nie et al., 2020; Gordon et al., 2022). Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021; Zerva et al., 2022). Clear annotation guidelines and in- cluding rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019). 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11 6.2.2 Bias in judgment Even if all K annotators agree on a particular judg- ment for a certain data point, they may all be mis- taken. There are well-known biases in human rea- soning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account. Furthermore, even if anno- tators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of Preprint Task Dataset & their descriptions Collection method Platform Feedback Type Language assistant HH-RLHF (Bai et al., 2022a; Perez et al., 2022a) Explicit Upwork, MTurk Ranking Language assistant SHP (Ethayarajh et al., 2023) Implicit Scraped from Reddit Ranking/Score Summarization summarize-from-feedback (Stiennon et al., 2020) Explicit Upwork Ranking Question Answering FeedbackQA (Li et al., 2022) Explicit MTurk Score, NL Translation WMT Metrics Shared Task (Freitag et al., 2022b) Explicit Pro translation workflow MQM, DA Summarization TAC Shared Tasks (TAC-2008, TAC-2009) Explicit N/A Score Table 2: Summary of existing human feedback datasets and their collection methods, which vary along several dimensions. Refer to Table 1 for definitions related to feedback types. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT. N/A means we could not find information. systematic bias away from the originally intended task (Parmar et al., 2023). Anchoring/Confirmation bias: When annota- tors are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021)."}