{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Why_do_Nearest_Neighbor_Language_Models_Work?_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the relative perplexity gap between the default and optimal temperature in some settings?,answer: 3.7%", "ref_chunk": "is close to optimal, which hid the importance of this term). In some settings, the relative perplexity gap between the default and optimal temperature can be as high as 3.7%. Finally, one signi\ufb01cant drawback to the current kNN-LM is the inef\ufb01ciency of kNN search performed at each step (He et al., 2021; Borgeaud et al., 2022; Alon et al., 2022; Wang et al., 2022). Because of the similarity between kNN-LM and the parametric LM\u2019s last layers and the many design choices, we also demonstrate that we are able to make kNN-LM more ef\ufb01cient by substituting the kNN search with another matrix operation that can \ufb01t in accelerator memory while maintaining more than half the perplexity improvement, or more than 6.5% relative improvement compared to the base LM. 2 2 Formalizing and Generalizing kNN-LM kNN-LM (Khandelwal et al., 2020b) is a linear interpolation between a base LM and a kNN model. Given a set of contexts ci and their corresponding next token wi as a pair (ci, wi) \u2208 D, kNN-LMs create a datastore (K, V) = {(ki, vi)}, as a set of key-value pairs: (K, V) = {(f (ci) , wi) | (ci, wi) \u2208 D} During inference, the parametric component of the LM generates the output distribution pLM (wt|ct; \u03b8) over the next tokens and produces the corresponding context representation f (ct), given the test input context ct. Then the non-parametric component of the LM queries the datastore with the f (ct) representation to retrieve its k-nearest neighbors N according to a distance function d(\u00b7, \u00b7). Next, the kNN-LM computes a probability distribution over these neighbors using the softmax of their negative distances, and aggregates the probability mass for each vocabulary item across all of its occurrences in the retrieved targets: pkNN(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208N Finally, this distribution is interpolated with the parametric LM distribution pLM to produce the \ufb01nal kNN-LM distribution: p(wt|ct; \u03b8) = (1 \u2212 \u03bb)pLM(wt|ct; \u03b8) + \u03bbpkNN(wt|ct) where \u03bb is a scalar that controls the weights of the interpolation between two components, with higher \u03bb putting more weight on the non-parametric component. Looking closely at Equation 2, we can notice a similarity between the calculation of PkN N and the standard PLM . The kNN distribution is based on the distances between the current context and the nearest neighbors from the datastore, normalized by a softmax function. Recall that in (standard) parametric language models, the distribution over the vocabulary is also based on a measure of distance, the inner product between the current context embedding and the word embeddings of every token in the vocabulary. Because each context embedding in the datastore (K, V) corresponds to a target token, we can also view this datastore as a large word embedding matrix with multiple word embeddings for each of the vocabulary words. Theoretically, given unlimited computation, we could calculate the distribution based on the distances to every embedding in the datastore, and aggregate by vocabulary items, making it more closely resemble PLM . In this case, k = |D|, the size of the entire datastore, and Equation 2 becomes the following, based on the distances to every context in the datastore D instead of a subset of nearest neighbors N . pD(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208D In practice, we use kNN search as a way of approximation, by limiting the calculation to only k nearest neighbors to avoid the computational cost of calculating the distribution over the entire datastore. If we re-write and generalize Equation 2, both the kNN-LM of Khandelwal et al. (2020b) and a large number of related models can be expressed through the following equation: Pinterp = (1 \u2212 \u03bb) softmax(Wsm \u00b7 hsm) (cid:124) (cid:125) (cid:123)(cid:122) PLM parametric component +\u03bb M softmax(mask-to-k(Wds \u2297 hds)/\u03c4 ) (cid:125) (cid:124) (cid:123)(cid:122) PkN N non-parametric component . Figure 1 provides an illustration of Equation 5. The \ufb01rst term of the equation is the standard parametric language model, whereas the second represents a generalized version of utilizing an external datastore. The \ufb01rst component, the output layer of a common parametric language model, is relatively straightforward. Wsm of size V \u00d7 D is the embedding matrix of the output token, and hsm is the context vector used to calculate the distribution of the output token, usually the output of the \ufb01nal feedforward layer in the transformer. In the second component, Wds represents the datastore, of size Nds \u00d7 D. Nds is the number of entries in the datastore, and D is the size of each context vector. hds represents the context vector used to query the datastore. As shown in Figure 1, these vectors can come from different layers of the transformer architecture. \u2297 represents the operation type used to calculate the similarity between context vectors and the query vector, which also has several alternatives that we discuss below. mask-to-k(\u00b7) represents a function to sparsify similarity scores across the datastore, setting all but k similarity scores to \u2212\u221e, which results in probabilities of zero for all masked similarity scores after the softmax. 3 (1) (2) (3) (4) (5) Practically, this is necessary for kNN-LMs because the size of the datastore Nds makes it infeasible to calculate all outputs at the same time. With masked logits, we apply a more generalized version of softmax with temperature \u03c4 . Intuitively adding the temperature can adjust the peakiness or con\ufb01dence of the softmax probability distribution output. After the softmax, the matrix M of dimension V \u00d7 Nds sums the probability of the Nds datastore entries corresponding to each of the V vocabulary entries. Each column in this matrix consists of a one-hot vector with a value of 1 and the index corresponding to the vocabulary item wi corresponding to the datastore entry for ci. Within this formulation, it becomes obvious that there are many design choices for kNN-LM-like models. One important thing to note is that the right side of Equation 5 is actually very similar to the left side representing the standard parametric language"}, {"question": " What is a significant drawback of the current kNN-LM?,answer: The inefficiency of kNN search performed at each step", "ref_chunk": "is close to optimal, which hid the importance of this term). In some settings, the relative perplexity gap between the default and optimal temperature can be as high as 3.7%. Finally, one signi\ufb01cant drawback to the current kNN-LM is the inef\ufb01ciency of kNN search performed at each step (He et al., 2021; Borgeaud et al., 2022; Alon et al., 2022; Wang et al., 2022). Because of the similarity between kNN-LM and the parametric LM\u2019s last layers and the many design choices, we also demonstrate that we are able to make kNN-LM more ef\ufb01cient by substituting the kNN search with another matrix operation that can \ufb01t in accelerator memory while maintaining more than half the perplexity improvement, or more than 6.5% relative improvement compared to the base LM. 2 2 Formalizing and Generalizing kNN-LM kNN-LM (Khandelwal et al., 2020b) is a linear interpolation between a base LM and a kNN model. Given a set of contexts ci and their corresponding next token wi as a pair (ci, wi) \u2208 D, kNN-LMs create a datastore (K, V) = {(ki, vi)}, as a set of key-value pairs: (K, V) = {(f (ci) , wi) | (ci, wi) \u2208 D} During inference, the parametric component of the LM generates the output distribution pLM (wt|ct; \u03b8) over the next tokens and produces the corresponding context representation f (ct), given the test input context ct. Then the non-parametric component of the LM queries the datastore with the f (ct) representation to retrieve its k-nearest neighbors N according to a distance function d(\u00b7, \u00b7). Next, the kNN-LM computes a probability distribution over these neighbors using the softmax of their negative distances, and aggregates the probability mass for each vocabulary item across all of its occurrences in the retrieved targets: pkNN(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208N Finally, this distribution is interpolated with the parametric LM distribution pLM to produce the \ufb01nal kNN-LM distribution: p(wt|ct; \u03b8) = (1 \u2212 \u03bb)pLM(wt|ct; \u03b8) + \u03bbpkNN(wt|ct) where \u03bb is a scalar that controls the weights of the interpolation between two components, with higher \u03bb putting more weight on the non-parametric component. Looking closely at Equation 2, we can notice a similarity between the calculation of PkN N and the standard PLM . The kNN distribution is based on the distances between the current context and the nearest neighbors from the datastore, normalized by a softmax function. Recall that in (standard) parametric language models, the distribution over the vocabulary is also based on a measure of distance, the inner product between the current context embedding and the word embeddings of every token in the vocabulary. Because each context embedding in the datastore (K, V) corresponds to a target token, we can also view this datastore as a large word embedding matrix with multiple word embeddings for each of the vocabulary words. Theoretically, given unlimited computation, we could calculate the distribution based on the distances to every embedding in the datastore, and aggregate by vocabulary items, making it more closely resemble PLM . In this case, k = |D|, the size of the entire datastore, and Equation 2 becomes the following, based on the distances to every context in the datastore D instead of a subset of nearest neighbors N . pD(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208D In practice, we use kNN search as a way of approximation, by limiting the calculation to only k nearest neighbors to avoid the computational cost of calculating the distribution over the entire datastore. If we re-write and generalize Equation 2, both the kNN-LM of Khandelwal et al. (2020b) and a large number of related models can be expressed through the following equation: Pinterp = (1 \u2212 \u03bb) softmax(Wsm \u00b7 hsm) (cid:124) (cid:125) (cid:123)(cid:122) PLM parametric component +\u03bb M softmax(mask-to-k(Wds \u2297 hds)/\u03c4 ) (cid:125) (cid:124) (cid:123)(cid:122) PkN N non-parametric component . Figure 1 provides an illustration of Equation 5. The \ufb01rst term of the equation is the standard parametric language model, whereas the second represents a generalized version of utilizing an external datastore. The \ufb01rst component, the output layer of a common parametric language model, is relatively straightforward. Wsm of size V \u00d7 D is the embedding matrix of the output token, and hsm is the context vector used to calculate the distribution of the output token, usually the output of the \ufb01nal feedforward layer in the transformer. In the second component, Wds represents the datastore, of size Nds \u00d7 D. Nds is the number of entries in the datastore, and D is the size of each context vector. hds represents the context vector used to query the datastore. As shown in Figure 1, these vectors can come from different layers of the transformer architecture. \u2297 represents the operation type used to calculate the similarity between context vectors and the query vector, which also has several alternatives that we discuss below. mask-to-k(\u00b7) represents a function to sparsify similarity scores across the datastore, setting all but k similarity scores to \u2212\u221e, which results in probabilities of zero for all masked similarity scores after the softmax. 3 (1) (2) (3) (4) (5) Practically, this is necessary for kNN-LMs because the size of the datastore Nds makes it infeasible to calculate all outputs at the same time. With masked logits, we apply a more generalized version of softmax with temperature \u03c4 . Intuitively adding the temperature can adjust the peakiness or con\ufb01dence of the softmax probability distribution output. After the softmax, the matrix M of dimension V \u00d7 Nds sums the probability of the Nds datastore entries corresponding to each of the V vocabulary entries. Each column in this matrix consists of a one-hot vector with a value of 1 and the index corresponding to the vocabulary item wi corresponding to the datastore entry for ci. Within this formulation, it becomes obvious that there are many design choices for kNN-LM-like models. One important thing to note is that the right side of Equation 5 is actually very similar to the left side representing the standard parametric language"}, {"question": " How is kNN-LM defined?,answer: KNN-LM is a linear interpolation between a base LM and a kNN model.", "ref_chunk": "is close to optimal, which hid the importance of this term). In some settings, the relative perplexity gap between the default and optimal temperature can be as high as 3.7%. Finally, one signi\ufb01cant drawback to the current kNN-LM is the inef\ufb01ciency of kNN search performed at each step (He et al., 2021; Borgeaud et al., 2022; Alon et al., 2022; Wang et al., 2022). Because of the similarity between kNN-LM and the parametric LM\u2019s last layers and the many design choices, we also demonstrate that we are able to make kNN-LM more ef\ufb01cient by substituting the kNN search with another matrix operation that can \ufb01t in accelerator memory while maintaining more than half the perplexity improvement, or more than 6.5% relative improvement compared to the base LM. 2 2 Formalizing and Generalizing kNN-LM kNN-LM (Khandelwal et al., 2020b) is a linear interpolation between a base LM and a kNN model. Given a set of contexts ci and their corresponding next token wi as a pair (ci, wi) \u2208 D, kNN-LMs create a datastore (K, V) = {(ki, vi)}, as a set of key-value pairs: (K, V) = {(f (ci) , wi) | (ci, wi) \u2208 D} During inference, the parametric component of the LM generates the output distribution pLM (wt|ct; \u03b8) over the next tokens and produces the corresponding context representation f (ct), given the test input context ct. Then the non-parametric component of the LM queries the datastore with the f (ct) representation to retrieve its k-nearest neighbors N according to a distance function d(\u00b7, \u00b7). Next, the kNN-LM computes a probability distribution over these neighbors using the softmax of their negative distances, and aggregates the probability mass for each vocabulary item across all of its occurrences in the retrieved targets: pkNN(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208N Finally, this distribution is interpolated with the parametric LM distribution pLM to produce the \ufb01nal kNN-LM distribution: p(wt|ct; \u03b8) = (1 \u2212 \u03bb)pLM(wt|ct; \u03b8) + \u03bbpkNN(wt|ct) where \u03bb is a scalar that controls the weights of the interpolation between two components, with higher \u03bb putting more weight on the non-parametric component. Looking closely at Equation 2, we can notice a similarity between the calculation of PkN N and the standard PLM . The kNN distribution is based on the distances between the current context and the nearest neighbors from the datastore, normalized by a softmax function. Recall that in (standard) parametric language models, the distribution over the vocabulary is also based on a measure of distance, the inner product between the current context embedding and the word embeddings of every token in the vocabulary. Because each context embedding in the datastore (K, V) corresponds to a target token, we can also view this datastore as a large word embedding matrix with multiple word embeddings for each of the vocabulary words. Theoretically, given unlimited computation, we could calculate the distribution based on the distances to every embedding in the datastore, and aggregate by vocabulary items, making it more closely resemble PLM . In this case, k = |D|, the size of the entire datastore, and Equation 2 becomes the following, based on the distances to every context in the datastore D instead of a subset of nearest neighbors N . pD(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208D In practice, we use kNN search as a way of approximation, by limiting the calculation to only k nearest neighbors to avoid the computational cost of calculating the distribution over the entire datastore. If we re-write and generalize Equation 2, both the kNN-LM of Khandelwal et al. (2020b) and a large number of related models can be expressed through the following equation: Pinterp = (1 \u2212 \u03bb) softmax(Wsm \u00b7 hsm) (cid:124) (cid:125) (cid:123)(cid:122) PLM parametric component +\u03bb M softmax(mask-to-k(Wds \u2297 hds)/\u03c4 ) (cid:125) (cid:124) (cid:123)(cid:122) PkN N non-parametric component . Figure 1 provides an illustration of Equation 5. The \ufb01rst term of the equation is the standard parametric language model, whereas the second represents a generalized version of utilizing an external datastore. The \ufb01rst component, the output layer of a common parametric language model, is relatively straightforward. Wsm of size V \u00d7 D is the embedding matrix of the output token, and hsm is the context vector used to calculate the distribution of the output token, usually the output of the \ufb01nal feedforward layer in the transformer. In the second component, Wds represents the datastore, of size Nds \u00d7 D. Nds is the number of entries in the datastore, and D is the size of each context vector. hds represents the context vector used to query the datastore. As shown in Figure 1, these vectors can come from different layers of the transformer architecture. \u2297 represents the operation type used to calculate the similarity between context vectors and the query vector, which also has several alternatives that we discuss below. mask-to-k(\u00b7) represents a function to sparsify similarity scores across the datastore, setting all but k similarity scores to \u2212\u221e, which results in probabilities of zero for all masked similarity scores after the softmax. 3 (1) (2) (3) (4) (5) Practically, this is necessary for kNN-LMs because the size of the datastore Nds makes it infeasible to calculate all outputs at the same time. With masked logits, we apply a more generalized version of softmax with temperature \u03c4 . Intuitively adding the temperature can adjust the peakiness or con\ufb01dence of the softmax probability distribution output. After the softmax, the matrix M of dimension V \u00d7 Nds sums the probability of the Nds datastore entries corresponding to each of the V vocabulary entries. Each column in this matrix consists of a one-hot vector with a value of 1 and the index corresponding to the vocabulary item wi corresponding to the datastore entry for ci. Within this formulation, it becomes obvious that there are many design choices for kNN-LM-like models. One important thing to note is that the right side of Equation 5 is actually very similar to the left side representing the standard parametric language"}, {"question": " What does the parametric component of the LM generate during inference?,answer: The output distribution pLM (wt|ct; \u03b8) over the next tokens and produces the corresponding context representation f (ct).", "ref_chunk": "is close to optimal, which hid the importance of this term). In some settings, the relative perplexity gap between the default and optimal temperature can be as high as 3.7%. Finally, one signi\ufb01cant drawback to the current kNN-LM is the inef\ufb01ciency of kNN search performed at each step (He et al., 2021; Borgeaud et al., 2022; Alon et al., 2022; Wang et al., 2022). Because of the similarity between kNN-LM and the parametric LM\u2019s last layers and the many design choices, we also demonstrate that we are able to make kNN-LM more ef\ufb01cient by substituting the kNN search with another matrix operation that can \ufb01t in accelerator memory while maintaining more than half the perplexity improvement, or more than 6.5% relative improvement compared to the base LM. 2 2 Formalizing and Generalizing kNN-LM kNN-LM (Khandelwal et al., 2020b) is a linear interpolation between a base LM and a kNN model. Given a set of contexts ci and their corresponding next token wi as a pair (ci, wi) \u2208 D, kNN-LMs create a datastore (K, V) = {(ki, vi)}, as a set of key-value pairs: (K, V) = {(f (ci) , wi) | (ci, wi) \u2208 D} During inference, the parametric component of the LM generates the output distribution pLM (wt|ct; \u03b8) over the next tokens and produces the corresponding context representation f (ct), given the test input context ct. Then the non-parametric component of the LM queries the datastore with the f (ct) representation to retrieve its k-nearest neighbors N according to a distance function d(\u00b7, \u00b7). Next, the kNN-LM computes a probability distribution over these neighbors using the softmax of their negative distances, and aggregates the probability mass for each vocabulary item across all of its occurrences in the retrieved targets: pkNN(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208N Finally, this distribution is interpolated with the parametric LM distribution pLM to produce the \ufb01nal kNN-LM distribution: p(wt|ct; \u03b8) = (1 \u2212 \u03bb)pLM(wt|ct; \u03b8) + \u03bbpkNN(wt|ct) where \u03bb is a scalar that controls the weights of the interpolation between two components, with higher \u03bb putting more weight on the non-parametric component. Looking closely at Equation 2, we can notice a similarity between the calculation of PkN N and the standard PLM . The kNN distribution is based on the distances between the current context and the nearest neighbors from the datastore, normalized by a softmax function. Recall that in (standard) parametric language models, the distribution over the vocabulary is also based on a measure of distance, the inner product between the current context embedding and the word embeddings of every token in the vocabulary. Because each context embedding in the datastore (K, V) corresponds to a target token, we can also view this datastore as a large word embedding matrix with multiple word embeddings for each of the vocabulary words. Theoretically, given unlimited computation, we could calculate the distribution based on the distances to every embedding in the datastore, and aggregate by vocabulary items, making it more closely resemble PLM . In this case, k = |D|, the size of the entire datastore, and Equation 2 becomes the following, based on the distances to every context in the datastore D instead of a subset of nearest neighbors N . pD(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208D In practice, we use kNN search as a way of approximation, by limiting the calculation to only k nearest neighbors to avoid the computational cost of calculating the distribution over the entire datastore. If we re-write and generalize Equation 2, both the kNN-LM of Khandelwal et al. (2020b) and a large number of related models can be expressed through the following equation: Pinterp = (1 \u2212 \u03bb) softmax(Wsm \u00b7 hsm) (cid:124) (cid:125) (cid:123)(cid:122) PLM parametric component +\u03bb M softmax(mask-to-k(Wds \u2297 hds)/\u03c4 ) (cid:125) (cid:124) (cid:123)(cid:122) PkN N non-parametric component . Figure 1 provides an illustration of Equation 5. The \ufb01rst term of the equation is the standard parametric language model, whereas the second represents a generalized version of utilizing an external datastore. The \ufb01rst component, the output layer of a common parametric language model, is relatively straightforward. Wsm of size V \u00d7 D is the embedding matrix of the output token, and hsm is the context vector used to calculate the distribution of the output token, usually the output of the \ufb01nal feedforward layer in the transformer. In the second component, Wds represents the datastore, of size Nds \u00d7 D. Nds is the number of entries in the datastore, and D is the size of each context vector. hds represents the context vector used to query the datastore. As shown in Figure 1, these vectors can come from different layers of the transformer architecture. \u2297 represents the operation type used to calculate the similarity between context vectors and the query vector, which also has several alternatives that we discuss below. mask-to-k(\u00b7) represents a function to sparsify similarity scores across the datastore, setting all but k similarity scores to \u2212\u221e, which results in probabilities of zero for all masked similarity scores after the softmax. 3 (1) (2) (3) (4) (5) Practically, this is necessary for kNN-LMs because the size of the datastore Nds makes it infeasible to calculate all outputs at the same time. With masked logits, we apply a more generalized version of softmax with temperature \u03c4 . Intuitively adding the temperature can adjust the peakiness or con\ufb01dence of the softmax probability distribution output. After the softmax, the matrix M of dimension V \u00d7 Nds sums the probability of the Nds datastore entries corresponding to each of the V vocabulary entries. Each column in this matrix consists of a one-hot vector with a value of 1 and the index corresponding to the vocabulary item wi corresponding to the datastore entry for ci. Within this formulation, it becomes obvious that there are many design choices for kNN-LM-like models. One important thing to note is that the right side of Equation 5 is actually very similar to the left side representing the standard parametric language"}, {"question": " What controls the weights of the interpolation between two components in kNN-LM?,answer: Scalar \u03bb", "ref_chunk": "is close to optimal, which hid the importance of this term). In some settings, the relative perplexity gap between the default and optimal temperature can be as high as 3.7%. Finally, one signi\ufb01cant drawback to the current kNN-LM is the inef\ufb01ciency of kNN search performed at each step (He et al., 2021; Borgeaud et al., 2022; Alon et al., 2022; Wang et al., 2022). Because of the similarity between kNN-LM and the parametric LM\u2019s last layers and the many design choices, we also demonstrate that we are able to make kNN-LM more ef\ufb01cient by substituting the kNN search with another matrix operation that can \ufb01t in accelerator memory while maintaining more than half the perplexity improvement, or more than 6.5% relative improvement compared to the base LM. 2 2 Formalizing and Generalizing kNN-LM kNN-LM (Khandelwal et al., 2020b) is a linear interpolation between a base LM and a kNN model. Given a set of contexts ci and their corresponding next token wi as a pair (ci, wi) \u2208 D, kNN-LMs create a datastore (K, V) = {(ki, vi)}, as a set of key-value pairs: (K, V) = {(f (ci) , wi) | (ci, wi) \u2208 D} During inference, the parametric component of the LM generates the output distribution pLM (wt|ct; \u03b8) over the next tokens and produces the corresponding context representation f (ct), given the test input context ct. Then the non-parametric component of the LM queries the datastore with the f (ct) representation to retrieve its k-nearest neighbors N according to a distance function d(\u00b7, \u00b7). Next, the kNN-LM computes a probability distribution over these neighbors using the softmax of their negative distances, and aggregates the probability mass for each vocabulary item across all of its occurrences in the retrieved targets: pkNN(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208N Finally, this distribution is interpolated with the parametric LM distribution pLM to produce the \ufb01nal kNN-LM distribution: p(wt|ct; \u03b8) = (1 \u2212 \u03bb)pLM(wt|ct; \u03b8) + \u03bbpkNN(wt|ct) where \u03bb is a scalar that controls the weights of the interpolation between two components, with higher \u03bb putting more weight on the non-parametric component. Looking closely at Equation 2, we can notice a similarity between the calculation of PkN N and the standard PLM . The kNN distribution is based on the distances between the current context and the nearest neighbors from the datastore, normalized by a softmax function. Recall that in (standard) parametric language models, the distribution over the vocabulary is also based on a measure of distance, the inner product between the current context embedding and the word embeddings of every token in the vocabulary. Because each context embedding in the datastore (K, V) corresponds to a target token, we can also view this datastore as a large word embedding matrix with multiple word embeddings for each of the vocabulary words. Theoretically, given unlimited computation, we could calculate the distribution based on the distances to every embedding in the datastore, and aggregate by vocabulary items, making it more closely resemble PLM . In this case, k = |D|, the size of the entire datastore, and Equation 2 becomes the following, based on the distances to every context in the datastore D instead of a subset of nearest neighbors N . pD(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208D In practice, we use kNN search as a way of approximation, by limiting the calculation to only k nearest neighbors to avoid the computational cost of calculating the distribution over the entire datastore. If we re-write and generalize Equation 2, both the kNN-LM of Khandelwal et al. (2020b) and a large number of related models can be expressed through the following equation: Pinterp = (1 \u2212 \u03bb) softmax(Wsm \u00b7 hsm) (cid:124) (cid:125) (cid:123)(cid:122) PLM parametric component +\u03bb M softmax(mask-to-k(Wds \u2297 hds)/\u03c4 ) (cid:125) (cid:124) (cid:123)(cid:122) PkN N non-parametric component . Figure 1 provides an illustration of Equation 5. The \ufb01rst term of the equation is the standard parametric language model, whereas the second represents a generalized version of utilizing an external datastore. The \ufb01rst component, the output layer of a common parametric language model, is relatively straightforward. Wsm of size V \u00d7 D is the embedding matrix of the output token, and hsm is the context vector used to calculate the distribution of the output token, usually the output of the \ufb01nal feedforward layer in the transformer. In the second component, Wds represents the datastore, of size Nds \u00d7 D. Nds is the number of entries in the datastore, and D is the size of each context vector. hds represents the context vector used to query the datastore. As shown in Figure 1, these vectors can come from different layers of the transformer architecture. \u2297 represents the operation type used to calculate the similarity between context vectors and the query vector, which also has several alternatives that we discuss below. mask-to-k(\u00b7) represents a function to sparsify similarity scores across the datastore, setting all but k similarity scores to \u2212\u221e, which results in probabilities of zero for all masked similarity scores after the softmax. 3 (1) (2) (3) (4) (5) Practically, this is necessary for kNN-LMs because the size of the datastore Nds makes it infeasible to calculate all outputs at the same time. With masked logits, we apply a more generalized version of softmax with temperature \u03c4 . Intuitively adding the temperature can adjust the peakiness or con\ufb01dence of the softmax probability distribution output. After the softmax, the matrix M of dimension V \u00d7 Nds sums the probability of the Nds datastore entries corresponding to each of the V vocabulary entries. Each column in this matrix consists of a one-hot vector with a value of 1 and the index corresponding to the vocabulary item wi corresponding to the datastore entry for ci. Within this formulation, it becomes obvious that there are many design choices for kNN-LM-like models. One important thing to note is that the right side of Equation 5 is actually very similar to the left side representing the standard parametric language"}, {"question": " What does the non-parametric component of kNN-LM query to retrieve k-nearest neighbors?,answer: Datastore with the f (ct) representation.", "ref_chunk": "is close to optimal, which hid the importance of this term). In some settings, the relative perplexity gap between the default and optimal temperature can be as high as 3.7%. Finally, one signi\ufb01cant drawback to the current kNN-LM is the inef\ufb01ciency of kNN search performed at each step (He et al., 2021; Borgeaud et al., 2022; Alon et al., 2022; Wang et al., 2022). Because of the similarity between kNN-LM and the parametric LM\u2019s last layers and the many design choices, we also demonstrate that we are able to make kNN-LM more ef\ufb01cient by substituting the kNN search with another matrix operation that can \ufb01t in accelerator memory while maintaining more than half the perplexity improvement, or more than 6.5% relative improvement compared to the base LM. 2 2 Formalizing and Generalizing kNN-LM kNN-LM (Khandelwal et al., 2020b) is a linear interpolation between a base LM and a kNN model. Given a set of contexts ci and their corresponding next token wi as a pair (ci, wi) \u2208 D, kNN-LMs create a datastore (K, V) = {(ki, vi)}, as a set of key-value pairs: (K, V) = {(f (ci) , wi) | (ci, wi) \u2208 D} During inference, the parametric component of the LM generates the output distribution pLM (wt|ct; \u03b8) over the next tokens and produces the corresponding context representation f (ct), given the test input context ct. Then the non-parametric component of the LM queries the datastore with the f (ct) representation to retrieve its k-nearest neighbors N according to a distance function d(\u00b7, \u00b7). Next, the kNN-LM computes a probability distribution over these neighbors using the softmax of their negative distances, and aggregates the probability mass for each vocabulary item across all of its occurrences in the retrieved targets: pkNN(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208N Finally, this distribution is interpolated with the parametric LM distribution pLM to produce the \ufb01nal kNN-LM distribution: p(wt|ct; \u03b8) = (1 \u2212 \u03bb)pLM(wt|ct; \u03b8) + \u03bbpkNN(wt|ct) where \u03bb is a scalar that controls the weights of the interpolation between two components, with higher \u03bb putting more weight on the non-parametric component. Looking closely at Equation 2, we can notice a similarity between the calculation of PkN N and the standard PLM . The kNN distribution is based on the distances between the current context and the nearest neighbors from the datastore, normalized by a softmax function. Recall that in (standard) parametric language models, the distribution over the vocabulary is also based on a measure of distance, the inner product between the current context embedding and the word embeddings of every token in the vocabulary. Because each context embedding in the datastore (K, V) corresponds to a target token, we can also view this datastore as a large word embedding matrix with multiple word embeddings for each of the vocabulary words. Theoretically, given unlimited computation, we could calculate the distribution based on the distances to every embedding in the datastore, and aggregate by vocabulary items, making it more closely resemble PLM . In this case, k = |D|, the size of the entire datastore, and Equation 2 becomes the following, based on the distances to every context in the datastore D instead of a subset of nearest neighbors N . pD(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208D In practice, we use kNN search as a way of approximation, by limiting the calculation to only k nearest neighbors to avoid the computational cost of calculating the distribution over the entire datastore. If we re-write and generalize Equation 2, both the kNN-LM of Khandelwal et al. (2020b) and a large number of related models can be expressed through the following equation: Pinterp = (1 \u2212 \u03bb) softmax(Wsm \u00b7 hsm) (cid:124) (cid:125) (cid:123)(cid:122) PLM parametric component +\u03bb M softmax(mask-to-k(Wds \u2297 hds)/\u03c4 ) (cid:125) (cid:124) (cid:123)(cid:122) PkN N non-parametric component . Figure 1 provides an illustration of Equation 5. The \ufb01rst term of the equation is the standard parametric language model, whereas the second represents a generalized version of utilizing an external datastore. The \ufb01rst component, the output layer of a common parametric language model, is relatively straightforward. Wsm of size V \u00d7 D is the embedding matrix of the output token, and hsm is the context vector used to calculate the distribution of the output token, usually the output of the \ufb01nal feedforward layer in the transformer. In the second component, Wds represents the datastore, of size Nds \u00d7 D. Nds is the number of entries in the datastore, and D is the size of each context vector. hds represents the context vector used to query the datastore. As shown in Figure 1, these vectors can come from different layers of the transformer architecture. \u2297 represents the operation type used to calculate the similarity between context vectors and the query vector, which also has several alternatives that we discuss below. mask-to-k(\u00b7) represents a function to sparsify similarity scores across the datastore, setting all but k similarity scores to \u2212\u221e, which results in probabilities of zero for all masked similarity scores after the softmax. 3 (1) (2) (3) (4) (5) Practically, this is necessary for kNN-LMs because the size of the datastore Nds makes it infeasible to calculate all outputs at the same time. With masked logits, we apply a more generalized version of softmax with temperature \u03c4 . Intuitively adding the temperature can adjust the peakiness or con\ufb01dence of the softmax probability distribution output. After the softmax, the matrix M of dimension V \u00d7 Nds sums the probability of the Nds datastore entries corresponding to each of the V vocabulary entries. Each column in this matrix consists of a one-hot vector with a value of 1 and the index corresponding to the vocabulary item wi corresponding to the datastore entry for ci. Within this formulation, it becomes obvious that there are many design choices for kNN-LM-like models. One important thing to note is that the right side of Equation 5 is actually very similar to the left side representing the standard parametric language"}, {"question": " How is the kNN-LM distribution interpolated with the parametric LM distribution to produce the final distribution?,answer: (1 \u2212 \u03bb)pLM(wt|ct; \u03b8) + \u03bbpkNN(wt|ct)", "ref_chunk": "is close to optimal, which hid the importance of this term). In some settings, the relative perplexity gap between the default and optimal temperature can be as high as 3.7%. Finally, one signi\ufb01cant drawback to the current kNN-LM is the inef\ufb01ciency of kNN search performed at each step (He et al., 2021; Borgeaud et al., 2022; Alon et al., 2022; Wang et al., 2022). Because of the similarity between kNN-LM and the parametric LM\u2019s last layers and the many design choices, we also demonstrate that we are able to make kNN-LM more ef\ufb01cient by substituting the kNN search with another matrix operation that can \ufb01t in accelerator memory while maintaining more than half the perplexity improvement, or more than 6.5% relative improvement compared to the base LM. 2 2 Formalizing and Generalizing kNN-LM kNN-LM (Khandelwal et al., 2020b) is a linear interpolation between a base LM and a kNN model. Given a set of contexts ci and their corresponding next token wi as a pair (ci, wi) \u2208 D, kNN-LMs create a datastore (K, V) = {(ki, vi)}, as a set of key-value pairs: (K, V) = {(f (ci) , wi) | (ci, wi) \u2208 D} During inference, the parametric component of the LM generates the output distribution pLM (wt|ct; \u03b8) over the next tokens and produces the corresponding context representation f (ct), given the test input context ct. Then the non-parametric component of the LM queries the datastore with the f (ct) representation to retrieve its k-nearest neighbors N according to a distance function d(\u00b7, \u00b7). Next, the kNN-LM computes a probability distribution over these neighbors using the softmax of their negative distances, and aggregates the probability mass for each vocabulary item across all of its occurrences in the retrieved targets: pkNN(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208N Finally, this distribution is interpolated with the parametric LM distribution pLM to produce the \ufb01nal kNN-LM distribution: p(wt|ct; \u03b8) = (1 \u2212 \u03bb)pLM(wt|ct; \u03b8) + \u03bbpkNN(wt|ct) where \u03bb is a scalar that controls the weights of the interpolation between two components, with higher \u03bb putting more weight on the non-parametric component. Looking closely at Equation 2, we can notice a similarity between the calculation of PkN N and the standard PLM . The kNN distribution is based on the distances between the current context and the nearest neighbors from the datastore, normalized by a softmax function. Recall that in (standard) parametric language models, the distribution over the vocabulary is also based on a measure of distance, the inner product between the current context embedding and the word embeddings of every token in the vocabulary. Because each context embedding in the datastore (K, V) corresponds to a target token, we can also view this datastore as a large word embedding matrix with multiple word embeddings for each of the vocabulary words. Theoretically, given unlimited computation, we could calculate the distribution based on the distances to every embedding in the datastore, and aggregate by vocabulary items, making it more closely resemble PLM . In this case, k = |D|, the size of the entire datastore, and Equation 2 becomes the following, based on the distances to every context in the datastore D instead of a subset of nearest neighbors N . pD(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208D In practice, we use kNN search as a way of approximation, by limiting the calculation to only k nearest neighbors to avoid the computational cost of calculating the distribution over the entire datastore. If we re-write and generalize Equation 2, both the kNN-LM of Khandelwal et al. (2020b) and a large number of related models can be expressed through the following equation: Pinterp = (1 \u2212 \u03bb) softmax(Wsm \u00b7 hsm) (cid:124) (cid:125) (cid:123)(cid:122) PLM parametric component +\u03bb M softmax(mask-to-k(Wds \u2297 hds)/\u03c4 ) (cid:125) (cid:124) (cid:123)(cid:122) PkN N non-parametric component . Figure 1 provides an illustration of Equation 5. The \ufb01rst term of the equation is the standard parametric language model, whereas the second represents a generalized version of utilizing an external datastore. The \ufb01rst component, the output layer of a common parametric language model, is relatively straightforward. Wsm of size V \u00d7 D is the embedding matrix of the output token, and hsm is the context vector used to calculate the distribution of the output token, usually the output of the \ufb01nal feedforward layer in the transformer. In the second component, Wds represents the datastore, of size Nds \u00d7 D. Nds is the number of entries in the datastore, and D is the size of each context vector. hds represents the context vector used to query the datastore. As shown in Figure 1, these vectors can come from different layers of the transformer architecture. \u2297 represents the operation type used to calculate the similarity between context vectors and the query vector, which also has several alternatives that we discuss below. mask-to-k(\u00b7) represents a function to sparsify similarity scores across the datastore, setting all but k similarity scores to \u2212\u221e, which results in probabilities of zero for all masked similarity scores after the softmax. 3 (1) (2) (3) (4) (5) Practically, this is necessary for kNN-LMs because the size of the datastore Nds makes it infeasible to calculate all outputs at the same time. With masked logits, we apply a more generalized version of softmax with temperature \u03c4 . Intuitively adding the temperature can adjust the peakiness or con\ufb01dence of the softmax probability distribution output. After the softmax, the matrix M of dimension V \u00d7 Nds sums the probability of the Nds datastore entries corresponding to each of the V vocabulary entries. Each column in this matrix consists of a one-hot vector with a value of 1 and the index corresponding to the vocabulary item wi corresponding to the datastore entry for ci. Within this formulation, it becomes obvious that there are many design choices for kNN-LM-like models. One important thing to note is that the right side of Equation 5 is actually very similar to the left side representing the standard parametric language"}, {"question": " What does the formula pD(wt|ct) \u221d represent in the context of kNN-LM?,answer: It represents the calculation of the distribution based on the distances to every context in the entire datastore D.", "ref_chunk": "is close to optimal, which hid the importance of this term). In some settings, the relative perplexity gap between the default and optimal temperature can be as high as 3.7%. Finally, one signi\ufb01cant drawback to the current kNN-LM is the inef\ufb01ciency of kNN search performed at each step (He et al., 2021; Borgeaud et al., 2022; Alon et al., 2022; Wang et al., 2022). Because of the similarity between kNN-LM and the parametric LM\u2019s last layers and the many design choices, we also demonstrate that we are able to make kNN-LM more ef\ufb01cient by substituting the kNN search with another matrix operation that can \ufb01t in accelerator memory while maintaining more than half the perplexity improvement, or more than 6.5% relative improvement compared to the base LM. 2 2 Formalizing and Generalizing kNN-LM kNN-LM (Khandelwal et al., 2020b) is a linear interpolation between a base LM and a kNN model. Given a set of contexts ci and their corresponding next token wi as a pair (ci, wi) \u2208 D, kNN-LMs create a datastore (K, V) = {(ki, vi)}, as a set of key-value pairs: (K, V) = {(f (ci) , wi) | (ci, wi) \u2208 D} During inference, the parametric component of the LM generates the output distribution pLM (wt|ct; \u03b8) over the next tokens and produces the corresponding context representation f (ct), given the test input context ct. Then the non-parametric component of the LM queries the datastore with the f (ct) representation to retrieve its k-nearest neighbors N according to a distance function d(\u00b7, \u00b7). Next, the kNN-LM computes a probability distribution over these neighbors using the softmax of their negative distances, and aggregates the probability mass for each vocabulary item across all of its occurrences in the retrieved targets: pkNN(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208N Finally, this distribution is interpolated with the parametric LM distribution pLM to produce the \ufb01nal kNN-LM distribution: p(wt|ct; \u03b8) = (1 \u2212 \u03bb)pLM(wt|ct; \u03b8) + \u03bbpkNN(wt|ct) where \u03bb is a scalar that controls the weights of the interpolation between two components, with higher \u03bb putting more weight on the non-parametric component. Looking closely at Equation 2, we can notice a similarity between the calculation of PkN N and the standard PLM . The kNN distribution is based on the distances between the current context and the nearest neighbors from the datastore, normalized by a softmax function. Recall that in (standard) parametric language models, the distribution over the vocabulary is also based on a measure of distance, the inner product between the current context embedding and the word embeddings of every token in the vocabulary. Because each context embedding in the datastore (K, V) corresponds to a target token, we can also view this datastore as a large word embedding matrix with multiple word embeddings for each of the vocabulary words. Theoretically, given unlimited computation, we could calculate the distribution based on the distances to every embedding in the datastore, and aggregate by vocabulary items, making it more closely resemble PLM . In this case, k = |D|, the size of the entire datastore, and Equation 2 becomes the following, based on the distances to every context in the datastore D instead of a subset of nearest neighbors N . pD(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208D In practice, we use kNN search as a way of approximation, by limiting the calculation to only k nearest neighbors to avoid the computational cost of calculating the distribution over the entire datastore. If we re-write and generalize Equation 2, both the kNN-LM of Khandelwal et al. (2020b) and a large number of related models can be expressed through the following equation: Pinterp = (1 \u2212 \u03bb) softmax(Wsm \u00b7 hsm) (cid:124) (cid:125) (cid:123)(cid:122) PLM parametric component +\u03bb M softmax(mask-to-k(Wds \u2297 hds)/\u03c4 ) (cid:125) (cid:124) (cid:123)(cid:122) PkN N non-parametric component . Figure 1 provides an illustration of Equation 5. The \ufb01rst term of the equation is the standard parametric language model, whereas the second represents a generalized version of utilizing an external datastore. The \ufb01rst component, the output layer of a common parametric language model, is relatively straightforward. Wsm of size V \u00d7 D is the embedding matrix of the output token, and hsm is the context vector used to calculate the distribution of the output token, usually the output of the \ufb01nal feedforward layer in the transformer. In the second component, Wds represents the datastore, of size Nds \u00d7 D. Nds is the number of entries in the datastore, and D is the size of each context vector. hds represents the context vector used to query the datastore. As shown in Figure 1, these vectors can come from different layers of the transformer architecture. \u2297 represents the operation type used to calculate the similarity between context vectors and the query vector, which also has several alternatives that we discuss below. mask-to-k(\u00b7) represents a function to sparsify similarity scores across the datastore, setting all but k similarity scores to \u2212\u221e, which results in probabilities of zero for all masked similarity scores after the softmax. 3 (1) (2) (3) (4) (5) Practically, this is necessary for kNN-LMs because the size of the datastore Nds makes it infeasible to calculate all outputs at the same time. With masked logits, we apply a more generalized version of softmax with temperature \u03c4 . Intuitively adding the temperature can adjust the peakiness or con\ufb01dence of the softmax probability distribution output. After the softmax, the matrix M of dimension V \u00d7 Nds sums the probability of the Nds datastore entries corresponding to each of the V vocabulary entries. Each column in this matrix consists of a one-hot vector with a value of 1 and the index corresponding to the vocabulary item wi corresponding to the datastore entry for ci. Within this formulation, it becomes obvious that there are many design choices for kNN-LM-like models. One important thing to note is that the right side of Equation 5 is actually very similar to the left side representing the standard parametric language"}, {"question": " What does the matrix M of dimension V \u00d7 Nds sum within the kNN-LM model?,answer: The probability of the Nds datastore entries corresponding to each of the V vocabulary entries.", "ref_chunk": "is close to optimal, which hid the importance of this term). In some settings, the relative perplexity gap between the default and optimal temperature can be as high as 3.7%. Finally, one signi\ufb01cant drawback to the current kNN-LM is the inef\ufb01ciency of kNN search performed at each step (He et al., 2021; Borgeaud et al., 2022; Alon et al., 2022; Wang et al., 2022). Because of the similarity between kNN-LM and the parametric LM\u2019s last layers and the many design choices, we also demonstrate that we are able to make kNN-LM more ef\ufb01cient by substituting the kNN search with another matrix operation that can \ufb01t in accelerator memory while maintaining more than half the perplexity improvement, or more than 6.5% relative improvement compared to the base LM. 2 2 Formalizing and Generalizing kNN-LM kNN-LM (Khandelwal et al., 2020b) is a linear interpolation between a base LM and a kNN model. Given a set of contexts ci and their corresponding next token wi as a pair (ci, wi) \u2208 D, kNN-LMs create a datastore (K, V) = {(ki, vi)}, as a set of key-value pairs: (K, V) = {(f (ci) , wi) | (ci, wi) \u2208 D} During inference, the parametric component of the LM generates the output distribution pLM (wt|ct; \u03b8) over the next tokens and produces the corresponding context representation f (ct), given the test input context ct. Then the non-parametric component of the LM queries the datastore with the f (ct) representation to retrieve its k-nearest neighbors N according to a distance function d(\u00b7, \u00b7). Next, the kNN-LM computes a probability distribution over these neighbors using the softmax of their negative distances, and aggregates the probability mass for each vocabulary item across all of its occurrences in the retrieved targets: pkNN(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208N Finally, this distribution is interpolated with the parametric LM distribution pLM to produce the \ufb01nal kNN-LM distribution: p(wt|ct; \u03b8) = (1 \u2212 \u03bb)pLM(wt|ct; \u03b8) + \u03bbpkNN(wt|ct) where \u03bb is a scalar that controls the weights of the interpolation between two components, with higher \u03bb putting more weight on the non-parametric component. Looking closely at Equation 2, we can notice a similarity between the calculation of PkN N and the standard PLM . The kNN distribution is based on the distances between the current context and the nearest neighbors from the datastore, normalized by a softmax function. Recall that in (standard) parametric language models, the distribution over the vocabulary is also based on a measure of distance, the inner product between the current context embedding and the word embeddings of every token in the vocabulary. Because each context embedding in the datastore (K, V) corresponds to a target token, we can also view this datastore as a large word embedding matrix with multiple word embeddings for each of the vocabulary words. Theoretically, given unlimited computation, we could calculate the distribution based on the distances to every embedding in the datastore, and aggregate by vocabulary items, making it more closely resemble PLM . In this case, k = |D|, the size of the entire datastore, and Equation 2 becomes the following, based on the distances to every context in the datastore D instead of a subset of nearest neighbors N . pD(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208D In practice, we use kNN search as a way of approximation, by limiting the calculation to only k nearest neighbors to avoid the computational cost of calculating the distribution over the entire datastore. If we re-write and generalize Equation 2, both the kNN-LM of Khandelwal et al. (2020b) and a large number of related models can be expressed through the following equation: Pinterp = (1 \u2212 \u03bb) softmax(Wsm \u00b7 hsm) (cid:124) (cid:125) (cid:123)(cid:122) PLM parametric component +\u03bb M softmax(mask-to-k(Wds \u2297 hds)/\u03c4 ) (cid:125) (cid:124) (cid:123)(cid:122) PkN N non-parametric component . Figure 1 provides an illustration of Equation 5. The \ufb01rst term of the equation is the standard parametric language model, whereas the second represents a generalized version of utilizing an external datastore. The \ufb01rst component, the output layer of a common parametric language model, is relatively straightforward. Wsm of size V \u00d7 D is the embedding matrix of the output token, and hsm is the context vector used to calculate the distribution of the output token, usually the output of the \ufb01nal feedforward layer in the transformer. In the second component, Wds represents the datastore, of size Nds \u00d7 D. Nds is the number of entries in the datastore, and D is the size of each context vector. hds represents the context vector used to query the datastore. As shown in Figure 1, these vectors can come from different layers of the transformer architecture. \u2297 represents the operation type used to calculate the similarity between context vectors and the query vector, which also has several alternatives that we discuss below. mask-to-k(\u00b7) represents a function to sparsify similarity scores across the datastore, setting all but k similarity scores to \u2212\u221e, which results in probabilities of zero for all masked similarity scores after the softmax. 3 (1) (2) (3) (4) (5) Practically, this is necessary for kNN-LMs because the size of the datastore Nds makes it infeasible to calculate all outputs at the same time. With masked logits, we apply a more generalized version of softmax with temperature \u03c4 . Intuitively adding the temperature can adjust the peakiness or con\ufb01dence of the softmax probability distribution output. After the softmax, the matrix M of dimension V \u00d7 Nds sums the probability of the Nds datastore entries corresponding to each of the V vocabulary entries. Each column in this matrix consists of a one-hot vector with a value of 1 and the index corresponding to the vocabulary item wi corresponding to the datastore entry for ci. Within this formulation, it becomes obvious that there are many design choices for kNN-LM-like models. One important thing to note is that the right side of Equation 5 is actually very similar to the left side representing the standard parametric language"}, {"question": " Why is it necessary to sparsify similarity scores across the datastore in kNN-LMs?,answer: Because the size of the datastore makes it infeasible to calculate all outputs at the same time.", "ref_chunk": "is close to optimal, which hid the importance of this term). In some settings, the relative perplexity gap between the default and optimal temperature can be as high as 3.7%. Finally, one signi\ufb01cant drawback to the current kNN-LM is the inef\ufb01ciency of kNN search performed at each step (He et al., 2021; Borgeaud et al., 2022; Alon et al., 2022; Wang et al., 2022). Because of the similarity between kNN-LM and the parametric LM\u2019s last layers and the many design choices, we also demonstrate that we are able to make kNN-LM more ef\ufb01cient by substituting the kNN search with another matrix operation that can \ufb01t in accelerator memory while maintaining more than half the perplexity improvement, or more than 6.5% relative improvement compared to the base LM. 2 2 Formalizing and Generalizing kNN-LM kNN-LM (Khandelwal et al., 2020b) is a linear interpolation between a base LM and a kNN model. Given a set of contexts ci and their corresponding next token wi as a pair (ci, wi) \u2208 D, kNN-LMs create a datastore (K, V) = {(ki, vi)}, as a set of key-value pairs: (K, V) = {(f (ci) , wi) | (ci, wi) \u2208 D} During inference, the parametric component of the LM generates the output distribution pLM (wt|ct; \u03b8) over the next tokens and produces the corresponding context representation f (ct), given the test input context ct. Then the non-parametric component of the LM queries the datastore with the f (ct) representation to retrieve its k-nearest neighbors N according to a distance function d(\u00b7, \u00b7). Next, the kNN-LM computes a probability distribution over these neighbors using the softmax of their negative distances, and aggregates the probability mass for each vocabulary item across all of its occurrences in the retrieved targets: pkNN(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208N Finally, this distribution is interpolated with the parametric LM distribution pLM to produce the \ufb01nal kNN-LM distribution: p(wt|ct; \u03b8) = (1 \u2212 \u03bb)pLM(wt|ct; \u03b8) + \u03bbpkNN(wt|ct) where \u03bb is a scalar that controls the weights of the interpolation between two components, with higher \u03bb putting more weight on the non-parametric component. Looking closely at Equation 2, we can notice a similarity between the calculation of PkN N and the standard PLM . The kNN distribution is based on the distances between the current context and the nearest neighbors from the datastore, normalized by a softmax function. Recall that in (standard) parametric language models, the distribution over the vocabulary is also based on a measure of distance, the inner product between the current context embedding and the word embeddings of every token in the vocabulary. Because each context embedding in the datastore (K, V) corresponds to a target token, we can also view this datastore as a large word embedding matrix with multiple word embeddings for each of the vocabulary words. Theoretically, given unlimited computation, we could calculate the distribution based on the distances to every embedding in the datastore, and aggregate by vocabulary items, making it more closely resemble PLM . In this case, k = |D|, the size of the entire datastore, and Equation 2 becomes the following, based on the distances to every context in the datastore D instead of a subset of nearest neighbors N . pD(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208D In practice, we use kNN search as a way of approximation, by limiting the calculation to only k nearest neighbors to avoid the computational cost of calculating the distribution over the entire datastore. If we re-write and generalize Equation 2, both the kNN-LM of Khandelwal et al. (2020b) and a large number of related models can be expressed through the following equation: Pinterp = (1 \u2212 \u03bb) softmax(Wsm \u00b7 hsm) (cid:124) (cid:125) (cid:123)(cid:122) PLM parametric component +\u03bb M softmax(mask-to-k(Wds \u2297 hds)/\u03c4 ) (cid:125) (cid:124) (cid:123)(cid:122) PkN N non-parametric component . Figure 1 provides an illustration of Equation 5. The \ufb01rst term of the equation is the standard parametric language model, whereas the second represents a generalized version of utilizing an external datastore. The \ufb01rst component, the output layer of a common parametric language model, is relatively straightforward. Wsm of size V \u00d7 D is the embedding matrix of the output token, and hsm is the context vector used to calculate the distribution of the output token, usually the output of the \ufb01nal feedforward layer in the transformer. In the second component, Wds represents the datastore, of size Nds \u00d7 D. Nds is the number of entries in the datastore, and D is the size of each context vector. hds represents the context vector used to query the datastore. As shown in Figure 1, these vectors can come from different layers of the transformer architecture. \u2297 represents the operation type used to calculate the similarity between context vectors and the query vector, which also has several alternatives that we discuss below. mask-to-k(\u00b7) represents a function to sparsify similarity scores across the datastore, setting all but k similarity scores to \u2212\u221e, which results in probabilities of zero for all masked similarity scores after the softmax. 3 (1) (2) (3) (4) (5) Practically, this is necessary for kNN-LMs because the size of the datastore Nds makes it infeasible to calculate all outputs at the same time. With masked logits, we apply a more generalized version of softmax with temperature \u03c4 . Intuitively adding the temperature can adjust the peakiness or con\ufb01dence of the softmax probability distribution output. After the softmax, the matrix M of dimension V \u00d7 Nds sums the probability of the Nds datastore entries corresponding to each of the V vocabulary entries. Each column in this matrix consists of a one-hot vector with a value of 1 and the index corresponding to the vocabulary item wi corresponding to the datastore entry for ci. Within this formulation, it becomes obvious that there are many design choices for kNN-LM-like models. One important thing to note is that the right side of Equation 5 is actually very similar to the left side representing the standard parametric language"}], "doc_text": "is close to optimal, which hid the importance of this term). In some settings, the relative perplexity gap between the default and optimal temperature can be as high as 3.7%. Finally, one signi\ufb01cant drawback to the current kNN-LM is the inef\ufb01ciency of kNN search performed at each step (He et al., 2021; Borgeaud et al., 2022; Alon et al., 2022; Wang et al., 2022). Because of the similarity between kNN-LM and the parametric LM\u2019s last layers and the many design choices, we also demonstrate that we are able to make kNN-LM more ef\ufb01cient by substituting the kNN search with another matrix operation that can \ufb01t in accelerator memory while maintaining more than half the perplexity improvement, or more than 6.5% relative improvement compared to the base LM. 2 2 Formalizing and Generalizing kNN-LM kNN-LM (Khandelwal et al., 2020b) is a linear interpolation between a base LM and a kNN model. Given a set of contexts ci and their corresponding next token wi as a pair (ci, wi) \u2208 D, kNN-LMs create a datastore (K, V) = {(ki, vi)}, as a set of key-value pairs: (K, V) = {(f (ci) , wi) | (ci, wi) \u2208 D} During inference, the parametric component of the LM generates the output distribution pLM (wt|ct; \u03b8) over the next tokens and produces the corresponding context representation f (ct), given the test input context ct. Then the non-parametric component of the LM queries the datastore with the f (ct) representation to retrieve its k-nearest neighbors N according to a distance function d(\u00b7, \u00b7). Next, the kNN-LM computes a probability distribution over these neighbors using the softmax of their negative distances, and aggregates the probability mass for each vocabulary item across all of its occurrences in the retrieved targets: pkNN(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208N Finally, this distribution is interpolated with the parametric LM distribution pLM to produce the \ufb01nal kNN-LM distribution: p(wt|ct; \u03b8) = (1 \u2212 \u03bb)pLM(wt|ct; \u03b8) + \u03bbpkNN(wt|ct) where \u03bb is a scalar that controls the weights of the interpolation between two components, with higher \u03bb putting more weight on the non-parametric component. Looking closely at Equation 2, we can notice a similarity between the calculation of PkN N and the standard PLM . The kNN distribution is based on the distances between the current context and the nearest neighbors from the datastore, normalized by a softmax function. Recall that in (standard) parametric language models, the distribution over the vocabulary is also based on a measure of distance, the inner product between the current context embedding and the word embeddings of every token in the vocabulary. Because each context embedding in the datastore (K, V) corresponds to a target token, we can also view this datastore as a large word embedding matrix with multiple word embeddings for each of the vocabulary words. Theoretically, given unlimited computation, we could calculate the distribution based on the distances to every embedding in the datastore, and aggregate by vocabulary items, making it more closely resemble PLM . In this case, k = |D|, the size of the entire datastore, and Equation 2 becomes the following, based on the distances to every context in the datastore D instead of a subset of nearest neighbors N . pD(wt|ct) \u221d (cid:88) 1wt=vi exp(\u2212d(ki, f (ct))) (ki,vi)\u2208D In practice, we use kNN search as a way of approximation, by limiting the calculation to only k nearest neighbors to avoid the computational cost of calculating the distribution over the entire datastore. If we re-write and generalize Equation 2, both the kNN-LM of Khandelwal et al. (2020b) and a large number of related models can be expressed through the following equation: Pinterp = (1 \u2212 \u03bb) softmax(Wsm \u00b7 hsm) (cid:124) (cid:125) (cid:123)(cid:122) PLM parametric component +\u03bb M softmax(mask-to-k(Wds \u2297 hds)/\u03c4 ) (cid:125) (cid:124) (cid:123)(cid:122) PkN N non-parametric component . Figure 1 provides an illustration of Equation 5. The \ufb01rst term of the equation is the standard parametric language model, whereas the second represents a generalized version of utilizing an external datastore. The \ufb01rst component, the output layer of a common parametric language model, is relatively straightforward. Wsm of size V \u00d7 D is the embedding matrix of the output token, and hsm is the context vector used to calculate the distribution of the output token, usually the output of the \ufb01nal feedforward layer in the transformer. In the second component, Wds represents the datastore, of size Nds \u00d7 D. Nds is the number of entries in the datastore, and D is the size of each context vector. hds represents the context vector used to query the datastore. As shown in Figure 1, these vectors can come from different layers of the transformer architecture. \u2297 represents the operation type used to calculate the similarity between context vectors and the query vector, which also has several alternatives that we discuss below. mask-to-k(\u00b7) represents a function to sparsify similarity scores across the datastore, setting all but k similarity scores to \u2212\u221e, which results in probabilities of zero for all masked similarity scores after the softmax. 3 (1) (2) (3) (4) (5) Practically, this is necessary for kNN-LMs because the size of the datastore Nds makes it infeasible to calculate all outputs at the same time. With masked logits, we apply a more generalized version of softmax with temperature \u03c4 . Intuitively adding the temperature can adjust the peakiness or con\ufb01dence of the softmax probability distribution output. After the softmax, the matrix M of dimension V \u00d7 Nds sums the probability of the Nds datastore entries corresponding to each of the V vocabulary entries. Each column in this matrix consists of a one-hot vector with a value of 1 and the index corresponding to the vocabulary item wi corresponding to the datastore entry for ci. Within this formulation, it becomes obvious that there are many design choices for kNN-LM-like models. One important thing to note is that the right side of Equation 5 is actually very similar to the left side representing the standard parametric language"}