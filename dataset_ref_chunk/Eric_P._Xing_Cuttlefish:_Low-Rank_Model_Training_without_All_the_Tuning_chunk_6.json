{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Cuttlefish:_Low-Rank_Model_Training_without_All_the_Tuning_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main algorithm outlined in CUTTLEFISH?,        answer: The main algorithm of CUTTLEFISH begins with profiling to determine \u02c6K, followed by full-rank training until the stable ranks for the layers to be factorized converge, factorizing the partially trained full-rank network, and obtaining the factorized low-rank model.    ", "ref_chunk": "\u2190 A(Ht, D); else Ht+1 \u2190 A(Ht, D); end end Algorithm 2 Pro\ufb01ling Input: The dataset D, full-rank model weights W, the pro\ufb01ling iterations \u03c4 , and a pro\ufb01ling rank ratio candidates: \u00af\u03c1. Output: Determined \u02c6K. init timer() for layer range (lbeg, lend) \u2208 layer stacks do H = factorize layer stack(W, \u00af\u03c1, lbeg, lend); start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train H for one iteration 3.6 Putting things together The main algorithm of CUTTLEFISH is outlined in Algo- rithm 1. CUTTLEFISH begins with pro\ufb01ling to determine \u02c6K. Following this, the training method commences with full-rank training until the stable ranks for the layers to be factorized converge, i.e., at epoch \u02c6E. Subsequently, CUT- TLEFISH factorizes the partially trained full-rank network using the converged scaled stable ranks R to obtain the factorized low-rank model. Finally, the low-rank model is end end end time = timer.toc(); avg low-rank time = (end time-start time)/\u03c4 ; start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train W for one iteration; end end time = timer.toc(); avg fullrank time = (end time-start time)/\u03c4 ; if fullrank time > \u03c5 \u00b7 avg low-rank time then \u02c6K = lend CUTTLEFISH: Low-rank Model Training without All The Tuning 4 EXPERIMENTS We have developed an ef\ufb01cient implementation of CUT- TLEFISH and conducted extensive experiments to evaluate its performance across various vision and natural language processing tasks. Our study focuses on the following as- pects: (i) the sizes of factorized models CUTTLEFISH dis- covers and their \ufb01nal accuracy; (ii) the end-to-end training speedups that CUTTLEFISH achieves in comparison to full- rank training and other baseline methods; (iii) how the \u02c6ss found by CUTTLEFISH compare to manually tuned and explicitly learned ones. Our comprehensive experimental results demonstrate that CUTTLEFISH automatically selects all factorization hyperparameters during training on-the- \ufb02y, eliminating the need for multiple experimental trials for factorization hyperparameter tuning. More speci\ufb01cally, the experimental results reveal that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and at- tains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training meth- ods and other prominent baselines. 4.1 Experimental setup and implementation details Pre-training ML tasks. We conducted experiments on various computer vision pre-training tasks, in- cluding CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and ImageNet (ILSVRC2012) (Deng et al., 2009). For CIFAR-10, CIFAR- 100, and SVHN (Netzer et al., 2011), we trained VGG- 19-BN (referred to as VGG-19) (Simonyan & Zisserman, 2014) and ResNet-18 (He et al., 2016). In the case of the SVHN dataset, we utilized the original training images and excluded the additional images. For ImageNet, our ex- periments involved ResNet-50, WideResNet-50-2 (referred to as WideResNet-50), DeiT-base, and ResMLP-S36 (He et al., 2016; Zagoruyko & Komodakis, 2016; Touvron et al., 2021b;a). Further details about the machine learning tasks can be found in the appendix. Fine-tuning ML tasks. We experiment on \ufb01ne-tuning BERTBASE over the GLUE benchmark (Wang et al., 2018). Hyperparameters & training schedule. For VGG-19 and ResNet-18 training on CIFAR-10 and CIFAR-100, we train the NNs for 300 epochs, while on SVHN, we train the NNs for 200 epochs. We employ a batch size of 1,024 for CIFAR and SVHN tasks to achieve high arithmetic inten- sity. The initial learning rate is linearly scaled up from 0.1 to 0.8 within \ufb01ve epochs and then decayed at milestones of 50% and 75% of the total training epochs (Goyal et al., 2017). For WideResNet-50 and ResNet-50 training on the ImageNet dataset, we follow the hyperparameter settings in (Goyal et al., 2017), where the models are trained for 90 epochs with an initial learning rate of 0.1, which is decayed by a factor of 0.1 at epochs 30, 60, and 80 using a batch size of 256. In addition to (Goyal et al., 2017), we apply label smoothing as described in (Wang et al., 2021a). For DeiT and ResMLP, we train them from scratch, adhering to the training schedule proposed in (Touvron et al., 2021b). For the GLUE \ufb01ne-tuning benchmark, we follow the default hy- perparameter setup in (Devlin et al., 2018; Jiao et al., 2020). Since CUTTLEFISH generalizes spectral initialization (SI) and is compatible with Frobenius decay (FD), we deploy FD in conjunction with CUTTLEFISH when it contributes to better accuracy. Further details on hyperparameters and training schedules can be found in the appendix. Experimental environment. We employ the NVIDIA NGC Docker container for software dependencies. Experi- ments for CIFAR, SVHN, and GLUE tasks are conducted on an EC2 p3.2xlarge instance (featuring a single V100 GPU) using FP32 precision. For BERT \ufb01ne-tuning and ImageNet training of ResNet-50 and WideResNet-50, the experiments are carried out on an EC2 g4dn.metal instance (equipped with eight T4 GPUs) using FP32 precision. For ImageNet training of DeiT and ResMLP, the experiments are performed on a single p4d.24xlarge instance (housing eight A100 GPUs) with mixed-precision training enabled. Baseline methods. We implement CUTTLEFISH and all considered baselines in PyTorch (Paszke et al., 2019). The baseline methods under consideration are: (i) PUFFERFISH, which employs manually tuned s (Wang et al., 2021a). To compare with PUFFERFISH, we use the same factorized ResNet-18, VGG-19, ResNet-50, and WideResNet-50 as reported in (Wang et al., 2021a). For DeiT and ResMLP models, not explored in the original PUFFERFISH paper, we adopt the same heuristic of using a \ufb01xed global rank ratio \u03c1 = 1 4 , tuning K to match the factorized model sizes found by CUTTLEFISH, and setting E = 80 for the entire training epochs T = 300 (Wang et al., 2021a); (ii) the factorized low- rank training method with SI and FD proposed by (Khodak et al., 2020) (referred to as \u201cSI&FD\u201d), with \u03c1s of SI&FD tuned to match the sizes of factorized models found by CUT- TLEFISH; (iii) training time structured pruning method, or \u201cearly bird ticket\u201d (EB Train) (You et al., 2020); (iv) the IMP method where each"}, {"question": " What is the purpose of the pro\ufb01ling iterations \u03c4 in Algorithm 2?,        answer: The pro\ufb01ling iterations \u03c4 in Algorithm 2 are used in the training process - H is trained for one iteration for each value of iter from 1 to \u03c4.    ", "ref_chunk": "\u2190 A(Ht, D); else Ht+1 \u2190 A(Ht, D); end end Algorithm 2 Pro\ufb01ling Input: The dataset D, full-rank model weights W, the pro\ufb01ling iterations \u03c4 , and a pro\ufb01ling rank ratio candidates: \u00af\u03c1. Output: Determined \u02c6K. init timer() for layer range (lbeg, lend) \u2208 layer stacks do H = factorize layer stack(W, \u00af\u03c1, lbeg, lend); start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train H for one iteration 3.6 Putting things together The main algorithm of CUTTLEFISH is outlined in Algo- rithm 1. CUTTLEFISH begins with pro\ufb01ling to determine \u02c6K. Following this, the training method commences with full-rank training until the stable ranks for the layers to be factorized converge, i.e., at epoch \u02c6E. Subsequently, CUT- TLEFISH factorizes the partially trained full-rank network using the converged scaled stable ranks R to obtain the factorized low-rank model. Finally, the low-rank model is end end end time = timer.toc(); avg low-rank time = (end time-start time)/\u03c4 ; start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train W for one iteration; end end time = timer.toc(); avg fullrank time = (end time-start time)/\u03c4 ; if fullrank time > \u03c5 \u00b7 avg low-rank time then \u02c6K = lend CUTTLEFISH: Low-rank Model Training without All The Tuning 4 EXPERIMENTS We have developed an ef\ufb01cient implementation of CUT- TLEFISH and conducted extensive experiments to evaluate its performance across various vision and natural language processing tasks. Our study focuses on the following as- pects: (i) the sizes of factorized models CUTTLEFISH dis- covers and their \ufb01nal accuracy; (ii) the end-to-end training speedups that CUTTLEFISH achieves in comparison to full- rank training and other baseline methods; (iii) how the \u02c6ss found by CUTTLEFISH compare to manually tuned and explicitly learned ones. Our comprehensive experimental results demonstrate that CUTTLEFISH automatically selects all factorization hyperparameters during training on-the- \ufb02y, eliminating the need for multiple experimental trials for factorization hyperparameter tuning. More speci\ufb01cally, the experimental results reveal that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and at- tains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training meth- ods and other prominent baselines. 4.1 Experimental setup and implementation details Pre-training ML tasks. We conducted experiments on various computer vision pre-training tasks, in- cluding CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and ImageNet (ILSVRC2012) (Deng et al., 2009). For CIFAR-10, CIFAR- 100, and SVHN (Netzer et al., 2011), we trained VGG- 19-BN (referred to as VGG-19) (Simonyan & Zisserman, 2014) and ResNet-18 (He et al., 2016). In the case of the SVHN dataset, we utilized the original training images and excluded the additional images. For ImageNet, our ex- periments involved ResNet-50, WideResNet-50-2 (referred to as WideResNet-50), DeiT-base, and ResMLP-S36 (He et al., 2016; Zagoruyko & Komodakis, 2016; Touvron et al., 2021b;a). Further details about the machine learning tasks can be found in the appendix. Fine-tuning ML tasks. We experiment on \ufb01ne-tuning BERTBASE over the GLUE benchmark (Wang et al., 2018). Hyperparameters & training schedule. For VGG-19 and ResNet-18 training on CIFAR-10 and CIFAR-100, we train the NNs for 300 epochs, while on SVHN, we train the NNs for 200 epochs. We employ a batch size of 1,024 for CIFAR and SVHN tasks to achieve high arithmetic inten- sity. The initial learning rate is linearly scaled up from 0.1 to 0.8 within \ufb01ve epochs and then decayed at milestones of 50% and 75% of the total training epochs (Goyal et al., 2017). For WideResNet-50 and ResNet-50 training on the ImageNet dataset, we follow the hyperparameter settings in (Goyal et al., 2017), where the models are trained for 90 epochs with an initial learning rate of 0.1, which is decayed by a factor of 0.1 at epochs 30, 60, and 80 using a batch size of 256. In addition to (Goyal et al., 2017), we apply label smoothing as described in (Wang et al., 2021a). For DeiT and ResMLP, we train them from scratch, adhering to the training schedule proposed in (Touvron et al., 2021b). For the GLUE \ufb01ne-tuning benchmark, we follow the default hy- perparameter setup in (Devlin et al., 2018; Jiao et al., 2020). Since CUTTLEFISH generalizes spectral initialization (SI) and is compatible with Frobenius decay (FD), we deploy FD in conjunction with CUTTLEFISH when it contributes to better accuracy. Further details on hyperparameters and training schedules can be found in the appendix. Experimental environment. We employ the NVIDIA NGC Docker container for software dependencies. Experi- ments for CIFAR, SVHN, and GLUE tasks are conducted on an EC2 p3.2xlarge instance (featuring a single V100 GPU) using FP32 precision. For BERT \ufb01ne-tuning and ImageNet training of ResNet-50 and WideResNet-50, the experiments are carried out on an EC2 g4dn.metal instance (equipped with eight T4 GPUs) using FP32 precision. For ImageNet training of DeiT and ResMLP, the experiments are performed on a single p4d.24xlarge instance (housing eight A100 GPUs) with mixed-precision training enabled. Baseline methods. We implement CUTTLEFISH and all considered baselines in PyTorch (Paszke et al., 2019). The baseline methods under consideration are: (i) PUFFERFISH, which employs manually tuned s (Wang et al., 2021a). To compare with PUFFERFISH, we use the same factorized ResNet-18, VGG-19, ResNet-50, and WideResNet-50 as reported in (Wang et al., 2021a). For DeiT and ResMLP models, not explored in the original PUFFERFISH paper, we adopt the same heuristic of using a \ufb01xed global rank ratio \u03c1 = 1 4 , tuning K to match the factorized model sizes found by CUTTLEFISH, and setting E = 80 for the entire training epochs T = 300 (Wang et al., 2021a); (ii) the factorized low- rank training method with SI and FD proposed by (Khodak et al., 2020) (referred to as \u201cSI&FD\u201d), with \u03c1s of SI&FD tuned to match the sizes of factorized models found by CUT- TLEFISH; (iii) training time structured pruning method, or \u201cearly bird ticket\u201d (EB Train) (You et al., 2020); (iv) the IMP method where each"}, {"question": " What does CUTTLEFISH automatically select during training on-the-fly?,        answer: CUTTLEFISH automatically selects all factorization hyperparameters during training on-the-fly, eliminating the need for multiple experimental trials for factorization hyperparameter tuning.    ", "ref_chunk": "\u2190 A(Ht, D); else Ht+1 \u2190 A(Ht, D); end end Algorithm 2 Pro\ufb01ling Input: The dataset D, full-rank model weights W, the pro\ufb01ling iterations \u03c4 , and a pro\ufb01ling rank ratio candidates: \u00af\u03c1. Output: Determined \u02c6K. init timer() for layer range (lbeg, lend) \u2208 layer stacks do H = factorize layer stack(W, \u00af\u03c1, lbeg, lend); start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train H for one iteration 3.6 Putting things together The main algorithm of CUTTLEFISH is outlined in Algo- rithm 1. CUTTLEFISH begins with pro\ufb01ling to determine \u02c6K. Following this, the training method commences with full-rank training until the stable ranks for the layers to be factorized converge, i.e., at epoch \u02c6E. Subsequently, CUT- TLEFISH factorizes the partially trained full-rank network using the converged scaled stable ranks R to obtain the factorized low-rank model. Finally, the low-rank model is end end end time = timer.toc(); avg low-rank time = (end time-start time)/\u03c4 ; start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train W for one iteration; end end time = timer.toc(); avg fullrank time = (end time-start time)/\u03c4 ; if fullrank time > \u03c5 \u00b7 avg low-rank time then \u02c6K = lend CUTTLEFISH: Low-rank Model Training without All The Tuning 4 EXPERIMENTS We have developed an ef\ufb01cient implementation of CUT- TLEFISH and conducted extensive experiments to evaluate its performance across various vision and natural language processing tasks. Our study focuses on the following as- pects: (i) the sizes of factorized models CUTTLEFISH dis- covers and their \ufb01nal accuracy; (ii) the end-to-end training speedups that CUTTLEFISH achieves in comparison to full- rank training and other baseline methods; (iii) how the \u02c6ss found by CUTTLEFISH compare to manually tuned and explicitly learned ones. Our comprehensive experimental results demonstrate that CUTTLEFISH automatically selects all factorization hyperparameters during training on-the- \ufb02y, eliminating the need for multiple experimental trials for factorization hyperparameter tuning. More speci\ufb01cally, the experimental results reveal that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and at- tains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training meth- ods and other prominent baselines. 4.1 Experimental setup and implementation details Pre-training ML tasks. We conducted experiments on various computer vision pre-training tasks, in- cluding CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and ImageNet (ILSVRC2012) (Deng et al., 2009). For CIFAR-10, CIFAR- 100, and SVHN (Netzer et al., 2011), we trained VGG- 19-BN (referred to as VGG-19) (Simonyan & Zisserman, 2014) and ResNet-18 (He et al., 2016). In the case of the SVHN dataset, we utilized the original training images and excluded the additional images. For ImageNet, our ex- periments involved ResNet-50, WideResNet-50-2 (referred to as WideResNet-50), DeiT-base, and ResMLP-S36 (He et al., 2016; Zagoruyko & Komodakis, 2016; Touvron et al., 2021b;a). Further details about the machine learning tasks can be found in the appendix. Fine-tuning ML tasks. We experiment on \ufb01ne-tuning BERTBASE over the GLUE benchmark (Wang et al., 2018). Hyperparameters & training schedule. For VGG-19 and ResNet-18 training on CIFAR-10 and CIFAR-100, we train the NNs for 300 epochs, while on SVHN, we train the NNs for 200 epochs. We employ a batch size of 1,024 for CIFAR and SVHN tasks to achieve high arithmetic inten- sity. The initial learning rate is linearly scaled up from 0.1 to 0.8 within \ufb01ve epochs and then decayed at milestones of 50% and 75% of the total training epochs (Goyal et al., 2017). For WideResNet-50 and ResNet-50 training on the ImageNet dataset, we follow the hyperparameter settings in (Goyal et al., 2017), where the models are trained for 90 epochs with an initial learning rate of 0.1, which is decayed by a factor of 0.1 at epochs 30, 60, and 80 using a batch size of 256. In addition to (Goyal et al., 2017), we apply label smoothing as described in (Wang et al., 2021a). For DeiT and ResMLP, we train them from scratch, adhering to the training schedule proposed in (Touvron et al., 2021b). For the GLUE \ufb01ne-tuning benchmark, we follow the default hy- perparameter setup in (Devlin et al., 2018; Jiao et al., 2020). Since CUTTLEFISH generalizes spectral initialization (SI) and is compatible with Frobenius decay (FD), we deploy FD in conjunction with CUTTLEFISH when it contributes to better accuracy. Further details on hyperparameters and training schedules can be found in the appendix. Experimental environment. We employ the NVIDIA NGC Docker container for software dependencies. Experi- ments for CIFAR, SVHN, and GLUE tasks are conducted on an EC2 p3.2xlarge instance (featuring a single V100 GPU) using FP32 precision. For BERT \ufb01ne-tuning and ImageNet training of ResNet-50 and WideResNet-50, the experiments are carried out on an EC2 g4dn.metal instance (equipped with eight T4 GPUs) using FP32 precision. For ImageNet training of DeiT and ResMLP, the experiments are performed on a single p4d.24xlarge instance (housing eight A100 GPUs) with mixed-precision training enabled. Baseline methods. We implement CUTTLEFISH and all considered baselines in PyTorch (Paszke et al., 2019). The baseline methods under consideration are: (i) PUFFERFISH, which employs manually tuned s (Wang et al., 2021a). To compare with PUFFERFISH, we use the same factorized ResNet-18, VGG-19, ResNet-50, and WideResNet-50 as reported in (Wang et al., 2021a). For DeiT and ResMLP models, not explored in the original PUFFERFISH paper, we adopt the same heuristic of using a \ufb01xed global rank ratio \u03c1 = 1 4 , tuning K to match the factorized model sizes found by CUTTLEFISH, and setting E = 80 for the entire training epochs T = 300 (Wang et al., 2021a); (ii) the factorized low- rank training method with SI and FD proposed by (Khodak et al., 2020) (referred to as \u201cSI&FD\u201d), with \u03c1s of SI&FD tuned to match the sizes of factorized models found by CUT- TLEFISH; (iii) training time structured pruning method, or \u201cearly bird ticket\u201d (EB Train) (You et al., 2020); (iv) the IMP method where each"}, {"question": " How do the sizes of factorized models generated by CUTTLEFISH compare to full-rank models?,        answer: Experimental results reveal that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models.    ", "ref_chunk": "\u2190 A(Ht, D); else Ht+1 \u2190 A(Ht, D); end end Algorithm 2 Pro\ufb01ling Input: The dataset D, full-rank model weights W, the pro\ufb01ling iterations \u03c4 , and a pro\ufb01ling rank ratio candidates: \u00af\u03c1. Output: Determined \u02c6K. init timer() for layer range (lbeg, lend) \u2208 layer stacks do H = factorize layer stack(W, \u00af\u03c1, lbeg, lend); start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train H for one iteration 3.6 Putting things together The main algorithm of CUTTLEFISH is outlined in Algo- rithm 1. CUTTLEFISH begins with pro\ufb01ling to determine \u02c6K. Following this, the training method commences with full-rank training until the stable ranks for the layers to be factorized converge, i.e., at epoch \u02c6E. Subsequently, CUT- TLEFISH factorizes the partially trained full-rank network using the converged scaled stable ranks R to obtain the factorized low-rank model. Finally, the low-rank model is end end end time = timer.toc(); avg low-rank time = (end time-start time)/\u03c4 ; start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train W for one iteration; end end time = timer.toc(); avg fullrank time = (end time-start time)/\u03c4 ; if fullrank time > \u03c5 \u00b7 avg low-rank time then \u02c6K = lend CUTTLEFISH: Low-rank Model Training without All The Tuning 4 EXPERIMENTS We have developed an ef\ufb01cient implementation of CUT- TLEFISH and conducted extensive experiments to evaluate its performance across various vision and natural language processing tasks. Our study focuses on the following as- pects: (i) the sizes of factorized models CUTTLEFISH dis- covers and their \ufb01nal accuracy; (ii) the end-to-end training speedups that CUTTLEFISH achieves in comparison to full- rank training and other baseline methods; (iii) how the \u02c6ss found by CUTTLEFISH compare to manually tuned and explicitly learned ones. Our comprehensive experimental results demonstrate that CUTTLEFISH automatically selects all factorization hyperparameters during training on-the- \ufb02y, eliminating the need for multiple experimental trials for factorization hyperparameter tuning. More speci\ufb01cally, the experimental results reveal that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and at- tains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training meth- ods and other prominent baselines. 4.1 Experimental setup and implementation details Pre-training ML tasks. We conducted experiments on various computer vision pre-training tasks, in- cluding CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and ImageNet (ILSVRC2012) (Deng et al., 2009). For CIFAR-10, CIFAR- 100, and SVHN (Netzer et al., 2011), we trained VGG- 19-BN (referred to as VGG-19) (Simonyan & Zisserman, 2014) and ResNet-18 (He et al., 2016). In the case of the SVHN dataset, we utilized the original training images and excluded the additional images. For ImageNet, our ex- periments involved ResNet-50, WideResNet-50-2 (referred to as WideResNet-50), DeiT-base, and ResMLP-S36 (He et al., 2016; Zagoruyko & Komodakis, 2016; Touvron et al., 2021b;a). Further details about the machine learning tasks can be found in the appendix. Fine-tuning ML tasks. We experiment on \ufb01ne-tuning BERTBASE over the GLUE benchmark (Wang et al., 2018). Hyperparameters & training schedule. For VGG-19 and ResNet-18 training on CIFAR-10 and CIFAR-100, we train the NNs for 300 epochs, while on SVHN, we train the NNs for 200 epochs. We employ a batch size of 1,024 for CIFAR and SVHN tasks to achieve high arithmetic inten- sity. The initial learning rate is linearly scaled up from 0.1 to 0.8 within \ufb01ve epochs and then decayed at milestones of 50% and 75% of the total training epochs (Goyal et al., 2017). For WideResNet-50 and ResNet-50 training on the ImageNet dataset, we follow the hyperparameter settings in (Goyal et al., 2017), where the models are trained for 90 epochs with an initial learning rate of 0.1, which is decayed by a factor of 0.1 at epochs 30, 60, and 80 using a batch size of 256. In addition to (Goyal et al., 2017), we apply label smoothing as described in (Wang et al., 2021a). For DeiT and ResMLP, we train them from scratch, adhering to the training schedule proposed in (Touvron et al., 2021b). For the GLUE \ufb01ne-tuning benchmark, we follow the default hy- perparameter setup in (Devlin et al., 2018; Jiao et al., 2020). Since CUTTLEFISH generalizes spectral initialization (SI) and is compatible with Frobenius decay (FD), we deploy FD in conjunction with CUTTLEFISH when it contributes to better accuracy. Further details on hyperparameters and training schedules can be found in the appendix. Experimental environment. We employ the NVIDIA NGC Docker container for software dependencies. Experi- ments for CIFAR, SVHN, and GLUE tasks are conducted on an EC2 p3.2xlarge instance (featuring a single V100 GPU) using FP32 precision. For BERT \ufb01ne-tuning and ImageNet training of ResNet-50 and WideResNet-50, the experiments are carried out on an EC2 g4dn.metal instance (equipped with eight T4 GPUs) using FP32 precision. For ImageNet training of DeiT and ResMLP, the experiments are performed on a single p4d.24xlarge instance (housing eight A100 GPUs) with mixed-precision training enabled. Baseline methods. We implement CUTTLEFISH and all considered baselines in PyTorch (Paszke et al., 2019). The baseline methods under consideration are: (i) PUFFERFISH, which employs manually tuned s (Wang et al., 2021a). To compare with PUFFERFISH, we use the same factorized ResNet-18, VGG-19, ResNet-50, and WideResNet-50 as reported in (Wang et al., 2021a). For DeiT and ResMLP models, not explored in the original PUFFERFISH paper, we adopt the same heuristic of using a \ufb01xed global rank ratio \u03c1 = 1 4 , tuning K to match the factorized model sizes found by CUTTLEFISH, and setting E = 80 for the entire training epochs T = 300 (Wang et al., 2021a); (ii) the factorized low- rank training method with SI and FD proposed by (Khodak et al., 2020) (referred to as \u201cSI&FD\u201d), with \u03c1s of SI&FD tuned to match the sizes of factorized models found by CUT- TLEFISH; (iii) training time structured pruning method, or \u201cearly bird ticket\u201d (EB Train) (You et al., 2020); (iv) the IMP method where each"}, {"question": " What is the impact of CUTTLEFISH on the end-to-end training process?,        answer: CUTTLEFISH attains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy compared to full-rank training methods.    ", "ref_chunk": "\u2190 A(Ht, D); else Ht+1 \u2190 A(Ht, D); end end Algorithm 2 Pro\ufb01ling Input: The dataset D, full-rank model weights W, the pro\ufb01ling iterations \u03c4 , and a pro\ufb01ling rank ratio candidates: \u00af\u03c1. Output: Determined \u02c6K. init timer() for layer range (lbeg, lend) \u2208 layer stacks do H = factorize layer stack(W, \u00af\u03c1, lbeg, lend); start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train H for one iteration 3.6 Putting things together The main algorithm of CUTTLEFISH is outlined in Algo- rithm 1. CUTTLEFISH begins with pro\ufb01ling to determine \u02c6K. Following this, the training method commences with full-rank training until the stable ranks for the layers to be factorized converge, i.e., at epoch \u02c6E. Subsequently, CUT- TLEFISH factorizes the partially trained full-rank network using the converged scaled stable ranks R to obtain the factorized low-rank model. Finally, the low-rank model is end end end time = timer.toc(); avg low-rank time = (end time-start time)/\u03c4 ; start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train W for one iteration; end end time = timer.toc(); avg fullrank time = (end time-start time)/\u03c4 ; if fullrank time > \u03c5 \u00b7 avg low-rank time then \u02c6K = lend CUTTLEFISH: Low-rank Model Training without All The Tuning 4 EXPERIMENTS We have developed an ef\ufb01cient implementation of CUT- TLEFISH and conducted extensive experiments to evaluate its performance across various vision and natural language processing tasks. Our study focuses on the following as- pects: (i) the sizes of factorized models CUTTLEFISH dis- covers and their \ufb01nal accuracy; (ii) the end-to-end training speedups that CUTTLEFISH achieves in comparison to full- rank training and other baseline methods; (iii) how the \u02c6ss found by CUTTLEFISH compare to manually tuned and explicitly learned ones. Our comprehensive experimental results demonstrate that CUTTLEFISH automatically selects all factorization hyperparameters during training on-the- \ufb02y, eliminating the need for multiple experimental trials for factorization hyperparameter tuning. More speci\ufb01cally, the experimental results reveal that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and at- tains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training meth- ods and other prominent baselines. 4.1 Experimental setup and implementation details Pre-training ML tasks. We conducted experiments on various computer vision pre-training tasks, in- cluding CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and ImageNet (ILSVRC2012) (Deng et al., 2009). For CIFAR-10, CIFAR- 100, and SVHN (Netzer et al., 2011), we trained VGG- 19-BN (referred to as VGG-19) (Simonyan & Zisserman, 2014) and ResNet-18 (He et al., 2016). In the case of the SVHN dataset, we utilized the original training images and excluded the additional images. For ImageNet, our ex- periments involved ResNet-50, WideResNet-50-2 (referred to as WideResNet-50), DeiT-base, and ResMLP-S36 (He et al., 2016; Zagoruyko & Komodakis, 2016; Touvron et al., 2021b;a). Further details about the machine learning tasks can be found in the appendix. Fine-tuning ML tasks. We experiment on \ufb01ne-tuning BERTBASE over the GLUE benchmark (Wang et al., 2018). Hyperparameters & training schedule. For VGG-19 and ResNet-18 training on CIFAR-10 and CIFAR-100, we train the NNs for 300 epochs, while on SVHN, we train the NNs for 200 epochs. We employ a batch size of 1,024 for CIFAR and SVHN tasks to achieve high arithmetic inten- sity. The initial learning rate is linearly scaled up from 0.1 to 0.8 within \ufb01ve epochs and then decayed at milestones of 50% and 75% of the total training epochs (Goyal et al., 2017). For WideResNet-50 and ResNet-50 training on the ImageNet dataset, we follow the hyperparameter settings in (Goyal et al., 2017), where the models are trained for 90 epochs with an initial learning rate of 0.1, which is decayed by a factor of 0.1 at epochs 30, 60, and 80 using a batch size of 256. In addition to (Goyal et al., 2017), we apply label smoothing as described in (Wang et al., 2021a). For DeiT and ResMLP, we train them from scratch, adhering to the training schedule proposed in (Touvron et al., 2021b). For the GLUE \ufb01ne-tuning benchmark, we follow the default hy- perparameter setup in (Devlin et al., 2018; Jiao et al., 2020). Since CUTTLEFISH generalizes spectral initialization (SI) and is compatible with Frobenius decay (FD), we deploy FD in conjunction with CUTTLEFISH when it contributes to better accuracy. Further details on hyperparameters and training schedules can be found in the appendix. Experimental environment. We employ the NVIDIA NGC Docker container for software dependencies. Experi- ments for CIFAR, SVHN, and GLUE tasks are conducted on an EC2 p3.2xlarge instance (featuring a single V100 GPU) using FP32 precision. For BERT \ufb01ne-tuning and ImageNet training of ResNet-50 and WideResNet-50, the experiments are carried out on an EC2 g4dn.metal instance (equipped with eight T4 GPUs) using FP32 precision. For ImageNet training of DeiT and ResMLP, the experiments are performed on a single p4d.24xlarge instance (housing eight A100 GPUs) with mixed-precision training enabled. Baseline methods. We implement CUTTLEFISH and all considered baselines in PyTorch (Paszke et al., 2019). The baseline methods under consideration are: (i) PUFFERFISH, which employs manually tuned s (Wang et al., 2021a). To compare with PUFFERFISH, we use the same factorized ResNet-18, VGG-19, ResNet-50, and WideResNet-50 as reported in (Wang et al., 2021a). For DeiT and ResMLP models, not explored in the original PUFFERFISH paper, we adopt the same heuristic of using a \ufb01xed global rank ratio \u03c1 = 1 4 , tuning K to match the factorized model sizes found by CUTTLEFISH, and setting E = 80 for the entire training epochs T = 300 (Wang et al., 2021a); (ii) the factorized low- rank training method with SI and FD proposed by (Khodak et al., 2020) (referred to as \u201cSI&FD\u201d), with \u03c1s of SI&FD tuned to match the sizes of factorized models found by CUT- TLEFISH; (iii) training time structured pruning method, or \u201cearly bird ticket\u201d (EB Train) (You et al., 2020); (iv) the IMP method where each"}, {"question": " What tasks were included in the study of CUTTLEFISH performance?,        answer: The study focused on various vision and natural language processing tasks, evaluating aspects such as model sizes, training speedups, and the comparison of \u02c6ss found by CUTTLEFISH with manually tuned and explicitly learned ones.    ", "ref_chunk": "\u2190 A(Ht, D); else Ht+1 \u2190 A(Ht, D); end end Algorithm 2 Pro\ufb01ling Input: The dataset D, full-rank model weights W, the pro\ufb01ling iterations \u03c4 , and a pro\ufb01ling rank ratio candidates: \u00af\u03c1. Output: Determined \u02c6K. init timer() for layer range (lbeg, lend) \u2208 layer stacks do H = factorize layer stack(W, \u00af\u03c1, lbeg, lend); start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train H for one iteration 3.6 Putting things together The main algorithm of CUTTLEFISH is outlined in Algo- rithm 1. CUTTLEFISH begins with pro\ufb01ling to determine \u02c6K. Following this, the training method commences with full-rank training until the stable ranks for the layers to be factorized converge, i.e., at epoch \u02c6E. Subsequently, CUT- TLEFISH factorizes the partially trained full-rank network using the converged scaled stable ranks R to obtain the factorized low-rank model. Finally, the low-rank model is end end end time = timer.toc(); avg low-rank time = (end time-start time)/\u03c4 ; start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train W for one iteration; end end time = timer.toc(); avg fullrank time = (end time-start time)/\u03c4 ; if fullrank time > \u03c5 \u00b7 avg low-rank time then \u02c6K = lend CUTTLEFISH: Low-rank Model Training without All The Tuning 4 EXPERIMENTS We have developed an ef\ufb01cient implementation of CUT- TLEFISH and conducted extensive experiments to evaluate its performance across various vision and natural language processing tasks. Our study focuses on the following as- pects: (i) the sizes of factorized models CUTTLEFISH dis- covers and their \ufb01nal accuracy; (ii) the end-to-end training speedups that CUTTLEFISH achieves in comparison to full- rank training and other baseline methods; (iii) how the \u02c6ss found by CUTTLEFISH compare to manually tuned and explicitly learned ones. Our comprehensive experimental results demonstrate that CUTTLEFISH automatically selects all factorization hyperparameters during training on-the- \ufb02y, eliminating the need for multiple experimental trials for factorization hyperparameter tuning. More speci\ufb01cally, the experimental results reveal that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and at- tains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training meth- ods and other prominent baselines. 4.1 Experimental setup and implementation details Pre-training ML tasks. We conducted experiments on various computer vision pre-training tasks, in- cluding CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and ImageNet (ILSVRC2012) (Deng et al., 2009). For CIFAR-10, CIFAR- 100, and SVHN (Netzer et al., 2011), we trained VGG- 19-BN (referred to as VGG-19) (Simonyan & Zisserman, 2014) and ResNet-18 (He et al., 2016). In the case of the SVHN dataset, we utilized the original training images and excluded the additional images. For ImageNet, our ex- periments involved ResNet-50, WideResNet-50-2 (referred to as WideResNet-50), DeiT-base, and ResMLP-S36 (He et al., 2016; Zagoruyko & Komodakis, 2016; Touvron et al., 2021b;a). Further details about the machine learning tasks can be found in the appendix. Fine-tuning ML tasks. We experiment on \ufb01ne-tuning BERTBASE over the GLUE benchmark (Wang et al., 2018). Hyperparameters & training schedule. For VGG-19 and ResNet-18 training on CIFAR-10 and CIFAR-100, we train the NNs for 300 epochs, while on SVHN, we train the NNs for 200 epochs. We employ a batch size of 1,024 for CIFAR and SVHN tasks to achieve high arithmetic inten- sity. The initial learning rate is linearly scaled up from 0.1 to 0.8 within \ufb01ve epochs and then decayed at milestones of 50% and 75% of the total training epochs (Goyal et al., 2017). For WideResNet-50 and ResNet-50 training on the ImageNet dataset, we follow the hyperparameter settings in (Goyal et al., 2017), where the models are trained for 90 epochs with an initial learning rate of 0.1, which is decayed by a factor of 0.1 at epochs 30, 60, and 80 using a batch size of 256. In addition to (Goyal et al., 2017), we apply label smoothing as described in (Wang et al., 2021a). For DeiT and ResMLP, we train them from scratch, adhering to the training schedule proposed in (Touvron et al., 2021b). For the GLUE \ufb01ne-tuning benchmark, we follow the default hy- perparameter setup in (Devlin et al., 2018; Jiao et al., 2020). Since CUTTLEFISH generalizes spectral initialization (SI) and is compatible with Frobenius decay (FD), we deploy FD in conjunction with CUTTLEFISH when it contributes to better accuracy. Further details on hyperparameters and training schedules can be found in the appendix. Experimental environment. We employ the NVIDIA NGC Docker container for software dependencies. Experi- ments for CIFAR, SVHN, and GLUE tasks are conducted on an EC2 p3.2xlarge instance (featuring a single V100 GPU) using FP32 precision. For BERT \ufb01ne-tuning and ImageNet training of ResNet-50 and WideResNet-50, the experiments are carried out on an EC2 g4dn.metal instance (equipped with eight T4 GPUs) using FP32 precision. For ImageNet training of DeiT and ResMLP, the experiments are performed on a single p4d.24xlarge instance (housing eight A100 GPUs) with mixed-precision training enabled. Baseline methods. We implement CUTTLEFISH and all considered baselines in PyTorch (Paszke et al., 2019). The baseline methods under consideration are: (i) PUFFERFISH, which employs manually tuned s (Wang et al., 2021a). To compare with PUFFERFISH, we use the same factorized ResNet-18, VGG-19, ResNet-50, and WideResNet-50 as reported in (Wang et al., 2021a). For DeiT and ResMLP models, not explored in the original PUFFERFISH paper, we adopt the same heuristic of using a \ufb01xed global rank ratio \u03c1 = 1 4 , tuning K to match the factorized model sizes found by CUTTLEFISH, and setting E = 80 for the entire training epochs T = 300 (Wang et al., 2021a); (ii) the factorized low- rank training method with SI and FD proposed by (Khodak et al., 2020) (referred to as \u201cSI&FD\u201d), with \u03c1s of SI&FD tuned to match the sizes of factorized models found by CUT- TLEFISH; (iii) training time structured pruning method, or \u201cearly bird ticket\u201d (EB Train) (You et al., 2020); (iv) the IMP method where each"}, {"question": " What is the approach of CUTTLEFISH towards hyperparameter tuning?,        answer: CUTTLEFISH generalizes spectral initialization (SI) and is compatible with Frobenius decay (FD), deploying FD in conjunction with CUTTLEFISH when it contributes to better accuracy.    ", "ref_chunk": "\u2190 A(Ht, D); else Ht+1 \u2190 A(Ht, D); end end Algorithm 2 Pro\ufb01ling Input: The dataset D, full-rank model weights W, the pro\ufb01ling iterations \u03c4 , and a pro\ufb01ling rank ratio candidates: \u00af\u03c1. Output: Determined \u02c6K. init timer() for layer range (lbeg, lend) \u2208 layer stacks do H = factorize layer stack(W, \u00af\u03c1, lbeg, lend); start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train H for one iteration 3.6 Putting things together The main algorithm of CUTTLEFISH is outlined in Algo- rithm 1. CUTTLEFISH begins with pro\ufb01ling to determine \u02c6K. Following this, the training method commences with full-rank training until the stable ranks for the layers to be factorized converge, i.e., at epoch \u02c6E. Subsequently, CUT- TLEFISH factorizes the partially trained full-rank network using the converged scaled stable ranks R to obtain the factorized low-rank model. Finally, the low-rank model is end end end time = timer.toc(); avg low-rank time = (end time-start time)/\u03c4 ; start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train W for one iteration; end end time = timer.toc(); avg fullrank time = (end time-start time)/\u03c4 ; if fullrank time > \u03c5 \u00b7 avg low-rank time then \u02c6K = lend CUTTLEFISH: Low-rank Model Training without All The Tuning 4 EXPERIMENTS We have developed an ef\ufb01cient implementation of CUT- TLEFISH and conducted extensive experiments to evaluate its performance across various vision and natural language processing tasks. Our study focuses on the following as- pects: (i) the sizes of factorized models CUTTLEFISH dis- covers and their \ufb01nal accuracy; (ii) the end-to-end training speedups that CUTTLEFISH achieves in comparison to full- rank training and other baseline methods; (iii) how the \u02c6ss found by CUTTLEFISH compare to manually tuned and explicitly learned ones. Our comprehensive experimental results demonstrate that CUTTLEFISH automatically selects all factorization hyperparameters during training on-the- \ufb02y, eliminating the need for multiple experimental trials for factorization hyperparameter tuning. More speci\ufb01cally, the experimental results reveal that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and at- tains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training meth- ods and other prominent baselines. 4.1 Experimental setup and implementation details Pre-training ML tasks. We conducted experiments on various computer vision pre-training tasks, in- cluding CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and ImageNet (ILSVRC2012) (Deng et al., 2009). For CIFAR-10, CIFAR- 100, and SVHN (Netzer et al., 2011), we trained VGG- 19-BN (referred to as VGG-19) (Simonyan & Zisserman, 2014) and ResNet-18 (He et al., 2016). In the case of the SVHN dataset, we utilized the original training images and excluded the additional images. For ImageNet, our ex- periments involved ResNet-50, WideResNet-50-2 (referred to as WideResNet-50), DeiT-base, and ResMLP-S36 (He et al., 2016; Zagoruyko & Komodakis, 2016; Touvron et al., 2021b;a). Further details about the machine learning tasks can be found in the appendix. Fine-tuning ML tasks. We experiment on \ufb01ne-tuning BERTBASE over the GLUE benchmark (Wang et al., 2018). Hyperparameters & training schedule. For VGG-19 and ResNet-18 training on CIFAR-10 and CIFAR-100, we train the NNs for 300 epochs, while on SVHN, we train the NNs for 200 epochs. We employ a batch size of 1,024 for CIFAR and SVHN tasks to achieve high arithmetic inten- sity. The initial learning rate is linearly scaled up from 0.1 to 0.8 within \ufb01ve epochs and then decayed at milestones of 50% and 75% of the total training epochs (Goyal et al., 2017). For WideResNet-50 and ResNet-50 training on the ImageNet dataset, we follow the hyperparameter settings in (Goyal et al., 2017), where the models are trained for 90 epochs with an initial learning rate of 0.1, which is decayed by a factor of 0.1 at epochs 30, 60, and 80 using a batch size of 256. In addition to (Goyal et al., 2017), we apply label smoothing as described in (Wang et al., 2021a). For DeiT and ResMLP, we train them from scratch, adhering to the training schedule proposed in (Touvron et al., 2021b). For the GLUE \ufb01ne-tuning benchmark, we follow the default hy- perparameter setup in (Devlin et al., 2018; Jiao et al., 2020). Since CUTTLEFISH generalizes spectral initialization (SI) and is compatible with Frobenius decay (FD), we deploy FD in conjunction with CUTTLEFISH when it contributes to better accuracy. Further details on hyperparameters and training schedules can be found in the appendix. Experimental environment. We employ the NVIDIA NGC Docker container for software dependencies. Experi- ments for CIFAR, SVHN, and GLUE tasks are conducted on an EC2 p3.2xlarge instance (featuring a single V100 GPU) using FP32 precision. For BERT \ufb01ne-tuning and ImageNet training of ResNet-50 and WideResNet-50, the experiments are carried out on an EC2 g4dn.metal instance (equipped with eight T4 GPUs) using FP32 precision. For ImageNet training of DeiT and ResMLP, the experiments are performed on a single p4d.24xlarge instance (housing eight A100 GPUs) with mixed-precision training enabled. Baseline methods. We implement CUTTLEFISH and all considered baselines in PyTorch (Paszke et al., 2019). The baseline methods under consideration are: (i) PUFFERFISH, which employs manually tuned s (Wang et al., 2021a). To compare with PUFFERFISH, we use the same factorized ResNet-18, VGG-19, ResNet-50, and WideResNet-50 as reported in (Wang et al., 2021a). For DeiT and ResMLP models, not explored in the original PUFFERFISH paper, we adopt the same heuristic of using a \ufb01xed global rank ratio \u03c1 = 1 4 , tuning K to match the factorized model sizes found by CUTTLEFISH, and setting E = 80 for the entire training epochs T = 300 (Wang et al., 2021a); (ii) the factorized low- rank training method with SI and FD proposed by (Khodak et al., 2020) (referred to as \u201cSI&FD\u201d), with \u03c1s of SI&FD tuned to match the sizes of factorized models found by CUT- TLEFISH; (iii) training time structured pruning method, or \u201cearly bird ticket\u201d (EB Train) (You et al., 2020); (iv) the IMP method where each"}, {"question": " What is the role of the pro\ufb01ling rank ratio candidates in Algorithm 2?,        answer: The pro\ufb01ling rank ratio candidates (\u00af\u03c1) are used to determine the factorized low-rank model during the training process.    ", "ref_chunk": "\u2190 A(Ht, D); else Ht+1 \u2190 A(Ht, D); end end Algorithm 2 Pro\ufb01ling Input: The dataset D, full-rank model weights W, the pro\ufb01ling iterations \u03c4 , and a pro\ufb01ling rank ratio candidates: \u00af\u03c1. Output: Determined \u02c6K. init timer() for layer range (lbeg, lend) \u2208 layer stacks do H = factorize layer stack(W, \u00af\u03c1, lbeg, lend); start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train H for one iteration 3.6 Putting things together The main algorithm of CUTTLEFISH is outlined in Algo- rithm 1. CUTTLEFISH begins with pro\ufb01ling to determine \u02c6K. Following this, the training method commences with full-rank training until the stable ranks for the layers to be factorized converge, i.e., at epoch \u02c6E. Subsequently, CUT- TLEFISH factorizes the partially trained full-rank network using the converged scaled stable ranks R to obtain the factorized low-rank model. Finally, the low-rank model is end end end time = timer.toc(); avg low-rank time = (end time-start time)/\u03c4 ; start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train W for one iteration; end end time = timer.toc(); avg fullrank time = (end time-start time)/\u03c4 ; if fullrank time > \u03c5 \u00b7 avg low-rank time then \u02c6K = lend CUTTLEFISH: Low-rank Model Training without All The Tuning 4 EXPERIMENTS We have developed an ef\ufb01cient implementation of CUT- TLEFISH and conducted extensive experiments to evaluate its performance across various vision and natural language processing tasks. Our study focuses on the following as- pects: (i) the sizes of factorized models CUTTLEFISH dis- covers and their \ufb01nal accuracy; (ii) the end-to-end training speedups that CUTTLEFISH achieves in comparison to full- rank training and other baseline methods; (iii) how the \u02c6ss found by CUTTLEFISH compare to manually tuned and explicitly learned ones. Our comprehensive experimental results demonstrate that CUTTLEFISH automatically selects all factorization hyperparameters during training on-the- \ufb02y, eliminating the need for multiple experimental trials for factorization hyperparameter tuning. More speci\ufb01cally, the experimental results reveal that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and at- tains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training meth- ods and other prominent baselines. 4.1 Experimental setup and implementation details Pre-training ML tasks. We conducted experiments on various computer vision pre-training tasks, in- cluding CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and ImageNet (ILSVRC2012) (Deng et al., 2009). For CIFAR-10, CIFAR- 100, and SVHN (Netzer et al., 2011), we trained VGG- 19-BN (referred to as VGG-19) (Simonyan & Zisserman, 2014) and ResNet-18 (He et al., 2016). In the case of the SVHN dataset, we utilized the original training images and excluded the additional images. For ImageNet, our ex- periments involved ResNet-50, WideResNet-50-2 (referred to as WideResNet-50), DeiT-base, and ResMLP-S36 (He et al., 2016; Zagoruyko & Komodakis, 2016; Touvron et al., 2021b;a). Further details about the machine learning tasks can be found in the appendix. Fine-tuning ML tasks. We experiment on \ufb01ne-tuning BERTBASE over the GLUE benchmark (Wang et al., 2018). Hyperparameters & training schedule. For VGG-19 and ResNet-18 training on CIFAR-10 and CIFAR-100, we train the NNs for 300 epochs, while on SVHN, we train the NNs for 200 epochs. We employ a batch size of 1,024 for CIFAR and SVHN tasks to achieve high arithmetic inten- sity. The initial learning rate is linearly scaled up from 0.1 to 0.8 within \ufb01ve epochs and then decayed at milestones of 50% and 75% of the total training epochs (Goyal et al., 2017). For WideResNet-50 and ResNet-50 training on the ImageNet dataset, we follow the hyperparameter settings in (Goyal et al., 2017), where the models are trained for 90 epochs with an initial learning rate of 0.1, which is decayed by a factor of 0.1 at epochs 30, 60, and 80 using a batch size of 256. In addition to (Goyal et al., 2017), we apply label smoothing as described in (Wang et al., 2021a). For DeiT and ResMLP, we train them from scratch, adhering to the training schedule proposed in (Touvron et al., 2021b). For the GLUE \ufb01ne-tuning benchmark, we follow the default hy- perparameter setup in (Devlin et al., 2018; Jiao et al., 2020). Since CUTTLEFISH generalizes spectral initialization (SI) and is compatible with Frobenius decay (FD), we deploy FD in conjunction with CUTTLEFISH when it contributes to better accuracy. Further details on hyperparameters and training schedules can be found in the appendix. Experimental environment. We employ the NVIDIA NGC Docker container for software dependencies. Experi- ments for CIFAR, SVHN, and GLUE tasks are conducted on an EC2 p3.2xlarge instance (featuring a single V100 GPU) using FP32 precision. For BERT \ufb01ne-tuning and ImageNet training of ResNet-50 and WideResNet-50, the experiments are carried out on an EC2 g4dn.metal instance (equipped with eight T4 GPUs) using FP32 precision. For ImageNet training of DeiT and ResMLP, the experiments are performed on a single p4d.24xlarge instance (housing eight A100 GPUs) with mixed-precision training enabled. Baseline methods. We implement CUTTLEFISH and all considered baselines in PyTorch (Paszke et al., 2019). The baseline methods under consideration are: (i) PUFFERFISH, which employs manually tuned s (Wang et al., 2021a). To compare with PUFFERFISH, we use the same factorized ResNet-18, VGG-19, ResNet-50, and WideResNet-50 as reported in (Wang et al., 2021a). For DeiT and ResMLP models, not explored in the original PUFFERFISH paper, we adopt the same heuristic of using a \ufb01xed global rank ratio \u03c1 = 1 4 , tuning K to match the factorized model sizes found by CUTTLEFISH, and setting E = 80 for the entire training epochs T = 300 (Wang et al., 2021a); (ii) the factorized low- rank training method with SI and FD proposed by (Khodak et al., 2020) (referred to as \u201cSI&FD\u201d), with \u03c1s of SI&FD tuned to match the sizes of factorized models found by CUT- TLEFISH; (iii) training time structured pruning method, or \u201cearly bird ticket\u201d (EB Train) (You et al., 2020); (iv) the IMP method where each"}, {"question": " What is the training approach followed for DeiT and ResMLP models?,        answer: DeiT and ResMLP models are trained from scratch, adhering to the training schedule proposed in relevant literature.    ", "ref_chunk": "\u2190 A(Ht, D); else Ht+1 \u2190 A(Ht, D); end end Algorithm 2 Pro\ufb01ling Input: The dataset D, full-rank model weights W, the pro\ufb01ling iterations \u03c4 , and a pro\ufb01ling rank ratio candidates: \u00af\u03c1. Output: Determined \u02c6K. init timer() for layer range (lbeg, lend) \u2208 layer stacks do H = factorize layer stack(W, \u00af\u03c1, lbeg, lend); start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train H for one iteration 3.6 Putting things together The main algorithm of CUTTLEFISH is outlined in Algo- rithm 1. CUTTLEFISH begins with pro\ufb01ling to determine \u02c6K. Following this, the training method commences with full-rank training until the stable ranks for the layers to be factorized converge, i.e., at epoch \u02c6E. Subsequently, CUT- TLEFISH factorizes the partially trained full-rank network using the converged scaled stable ranks R to obtain the factorized low-rank model. Finally, the low-rank model is end end end time = timer.toc(); avg low-rank time = (end time-start time)/\u03c4 ; start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train W for one iteration; end end time = timer.toc(); avg fullrank time = (end time-start time)/\u03c4 ; if fullrank time > \u03c5 \u00b7 avg low-rank time then \u02c6K = lend CUTTLEFISH: Low-rank Model Training without All The Tuning 4 EXPERIMENTS We have developed an ef\ufb01cient implementation of CUT- TLEFISH and conducted extensive experiments to evaluate its performance across various vision and natural language processing tasks. Our study focuses on the following as- pects: (i) the sizes of factorized models CUTTLEFISH dis- covers and their \ufb01nal accuracy; (ii) the end-to-end training speedups that CUTTLEFISH achieves in comparison to full- rank training and other baseline methods; (iii) how the \u02c6ss found by CUTTLEFISH compare to manually tuned and explicitly learned ones. Our comprehensive experimental results demonstrate that CUTTLEFISH automatically selects all factorization hyperparameters during training on-the- \ufb02y, eliminating the need for multiple experimental trials for factorization hyperparameter tuning. More speci\ufb01cally, the experimental results reveal that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and at- tains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training meth- ods and other prominent baselines. 4.1 Experimental setup and implementation details Pre-training ML tasks. We conducted experiments on various computer vision pre-training tasks, in- cluding CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and ImageNet (ILSVRC2012) (Deng et al., 2009). For CIFAR-10, CIFAR- 100, and SVHN (Netzer et al., 2011), we trained VGG- 19-BN (referred to as VGG-19) (Simonyan & Zisserman, 2014) and ResNet-18 (He et al., 2016). In the case of the SVHN dataset, we utilized the original training images and excluded the additional images. For ImageNet, our ex- periments involved ResNet-50, WideResNet-50-2 (referred to as WideResNet-50), DeiT-base, and ResMLP-S36 (He et al., 2016; Zagoruyko & Komodakis, 2016; Touvron et al., 2021b;a). Further details about the machine learning tasks can be found in the appendix. Fine-tuning ML tasks. We experiment on \ufb01ne-tuning BERTBASE over the GLUE benchmark (Wang et al., 2018). Hyperparameters & training schedule. For VGG-19 and ResNet-18 training on CIFAR-10 and CIFAR-100, we train the NNs for 300 epochs, while on SVHN, we train the NNs for 200 epochs. We employ a batch size of 1,024 for CIFAR and SVHN tasks to achieve high arithmetic inten- sity. The initial learning rate is linearly scaled up from 0.1 to 0.8 within \ufb01ve epochs and then decayed at milestones of 50% and 75% of the total training epochs (Goyal et al., 2017). For WideResNet-50 and ResNet-50 training on the ImageNet dataset, we follow the hyperparameter settings in (Goyal et al., 2017), where the models are trained for 90 epochs with an initial learning rate of 0.1, which is decayed by a factor of 0.1 at epochs 30, 60, and 80 using a batch size of 256. In addition to (Goyal et al., 2017), we apply label smoothing as described in (Wang et al., 2021a). For DeiT and ResMLP, we train them from scratch, adhering to the training schedule proposed in (Touvron et al., 2021b). For the GLUE \ufb01ne-tuning benchmark, we follow the default hy- perparameter setup in (Devlin et al., 2018; Jiao et al., 2020). Since CUTTLEFISH generalizes spectral initialization (SI) and is compatible with Frobenius decay (FD), we deploy FD in conjunction with CUTTLEFISH when it contributes to better accuracy. Further details on hyperparameters and training schedules can be found in the appendix. Experimental environment. We employ the NVIDIA NGC Docker container for software dependencies. Experi- ments for CIFAR, SVHN, and GLUE tasks are conducted on an EC2 p3.2xlarge instance (featuring a single V100 GPU) using FP32 precision. For BERT \ufb01ne-tuning and ImageNet training of ResNet-50 and WideResNet-50, the experiments are carried out on an EC2 g4dn.metal instance (equipped with eight T4 GPUs) using FP32 precision. For ImageNet training of DeiT and ResMLP, the experiments are performed on a single p4d.24xlarge instance (housing eight A100 GPUs) with mixed-precision training enabled. Baseline methods. We implement CUTTLEFISH and all considered baselines in PyTorch (Paszke et al., 2019). The baseline methods under consideration are: (i) PUFFERFISH, which employs manually tuned s (Wang et al., 2021a). To compare with PUFFERFISH, we use the same factorized ResNet-18, VGG-19, ResNet-50, and WideResNet-50 as reported in (Wang et al., 2021a). For DeiT and ResMLP models, not explored in the original PUFFERFISH paper, we adopt the same heuristic of using a \ufb01xed global rank ratio \u03c1 = 1 4 , tuning K to match the factorized model sizes found by CUTTLEFISH, and setting E = 80 for the entire training epochs T = 300 (Wang et al., 2021a); (ii) the factorized low- rank training method with SI and FD proposed by (Khodak et al., 2020) (referred to as \u201cSI&FD\u201d), with \u03c1s of SI&FD tuned to match the sizes of factorized models found by CUT- TLEFISH; (iii) training time structured pruning method, or \u201cearly bird ticket\u201d (EB Train) (You et al., 2020); (iv) the IMP method where each"}, {"question": " In which software environment were the experiments conducted for CUTTLEFISH and baselines?,        answer: The experiments for CUTTLEFISH and baselines were implemented in PyTorch and conducted using the NVIDIA NGC Docker container for software dependencies.    ", "ref_chunk": "\u2190 A(Ht, D); else Ht+1 \u2190 A(Ht, D); end end Algorithm 2 Pro\ufb01ling Input: The dataset D, full-rank model weights W, the pro\ufb01ling iterations \u03c4 , and a pro\ufb01ling rank ratio candidates: \u00af\u03c1. Output: Determined \u02c6K. init timer() for layer range (lbeg, lend) \u2208 layer stacks do H = factorize layer stack(W, \u00af\u03c1, lbeg, lend); start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train H for one iteration 3.6 Putting things together The main algorithm of CUTTLEFISH is outlined in Algo- rithm 1. CUTTLEFISH begins with pro\ufb01ling to determine \u02c6K. Following this, the training method commences with full-rank training until the stable ranks for the layers to be factorized converge, i.e., at epoch \u02c6E. Subsequently, CUT- TLEFISH factorizes the partially trained full-rank network using the converged scaled stable ranks R to obtain the factorized low-rank model. Finally, the low-rank model is end end end time = timer.toc(); avg low-rank time = (end time-start time)/\u03c4 ; start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train W for one iteration; end end time = timer.toc(); avg fullrank time = (end time-start time)/\u03c4 ; if fullrank time > \u03c5 \u00b7 avg low-rank time then \u02c6K = lend CUTTLEFISH: Low-rank Model Training without All The Tuning 4 EXPERIMENTS We have developed an ef\ufb01cient implementation of CUT- TLEFISH and conducted extensive experiments to evaluate its performance across various vision and natural language processing tasks. Our study focuses on the following as- pects: (i) the sizes of factorized models CUTTLEFISH dis- covers and their \ufb01nal accuracy; (ii) the end-to-end training speedups that CUTTLEFISH achieves in comparison to full- rank training and other baseline methods; (iii) how the \u02c6ss found by CUTTLEFISH compare to manually tuned and explicitly learned ones. Our comprehensive experimental results demonstrate that CUTTLEFISH automatically selects all factorization hyperparameters during training on-the- \ufb02y, eliminating the need for multiple experimental trials for factorization hyperparameter tuning. More speci\ufb01cally, the experimental results reveal that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and at- tains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training meth- ods and other prominent baselines. 4.1 Experimental setup and implementation details Pre-training ML tasks. We conducted experiments on various computer vision pre-training tasks, in- cluding CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and ImageNet (ILSVRC2012) (Deng et al., 2009). For CIFAR-10, CIFAR- 100, and SVHN (Netzer et al., 2011), we trained VGG- 19-BN (referred to as VGG-19) (Simonyan & Zisserman, 2014) and ResNet-18 (He et al., 2016). In the case of the SVHN dataset, we utilized the original training images and excluded the additional images. For ImageNet, our ex- periments involved ResNet-50, WideResNet-50-2 (referred to as WideResNet-50), DeiT-base, and ResMLP-S36 (He et al., 2016; Zagoruyko & Komodakis, 2016; Touvron et al., 2021b;a). Further details about the machine learning tasks can be found in the appendix. Fine-tuning ML tasks. We experiment on \ufb01ne-tuning BERTBASE over the GLUE benchmark (Wang et al., 2018). Hyperparameters & training schedule. For VGG-19 and ResNet-18 training on CIFAR-10 and CIFAR-100, we train the NNs for 300 epochs, while on SVHN, we train the NNs for 200 epochs. We employ a batch size of 1,024 for CIFAR and SVHN tasks to achieve high arithmetic inten- sity. The initial learning rate is linearly scaled up from 0.1 to 0.8 within \ufb01ve epochs and then decayed at milestones of 50% and 75% of the total training epochs (Goyal et al., 2017). For WideResNet-50 and ResNet-50 training on the ImageNet dataset, we follow the hyperparameter settings in (Goyal et al., 2017), where the models are trained for 90 epochs with an initial learning rate of 0.1, which is decayed by a factor of 0.1 at epochs 30, 60, and 80 using a batch size of 256. In addition to (Goyal et al., 2017), we apply label smoothing as described in (Wang et al., 2021a). For DeiT and ResMLP, we train them from scratch, adhering to the training schedule proposed in (Touvron et al., 2021b). For the GLUE \ufb01ne-tuning benchmark, we follow the default hy- perparameter setup in (Devlin et al., 2018; Jiao et al., 2020). Since CUTTLEFISH generalizes spectral initialization (SI) and is compatible with Frobenius decay (FD), we deploy FD in conjunction with CUTTLEFISH when it contributes to better accuracy. Further details on hyperparameters and training schedules can be found in the appendix. Experimental environment. We employ the NVIDIA NGC Docker container for software dependencies. Experi- ments for CIFAR, SVHN, and GLUE tasks are conducted on an EC2 p3.2xlarge instance (featuring a single V100 GPU) using FP32 precision. For BERT \ufb01ne-tuning and ImageNet training of ResNet-50 and WideResNet-50, the experiments are carried out on an EC2 g4dn.metal instance (equipped with eight T4 GPUs) using FP32 precision. For ImageNet training of DeiT and ResMLP, the experiments are performed on a single p4d.24xlarge instance (housing eight A100 GPUs) with mixed-precision training enabled. Baseline methods. We implement CUTTLEFISH and all considered baselines in PyTorch (Paszke et al., 2019). The baseline methods under consideration are: (i) PUFFERFISH, which employs manually tuned s (Wang et al., 2021a). To compare with PUFFERFISH, we use the same factorized ResNet-18, VGG-19, ResNet-50, and WideResNet-50 as reported in (Wang et al., 2021a). For DeiT and ResMLP models, not explored in the original PUFFERFISH paper, we adopt the same heuristic of using a \ufb01xed global rank ratio \u03c1 = 1 4 , tuning K to match the factorized model sizes found by CUTTLEFISH, and setting E = 80 for the entire training epochs T = 300 (Wang et al., 2021a); (ii) the factorized low- rank training method with SI and FD proposed by (Khodak et al., 2020) (referred to as \u201cSI&FD\u201d), with \u03c1s of SI&FD tuned to match the sizes of factorized models found by CUT- TLEFISH; (iii) training time structured pruning method, or \u201cearly bird ticket\u201d (EB Train) (You et al., 2020); (iv) the IMP method where each"}], "doc_text": "\u2190 A(Ht, D); else Ht+1 \u2190 A(Ht, D); end end Algorithm 2 Pro\ufb01ling Input: The dataset D, full-rank model weights W, the pro\ufb01ling iterations \u03c4 , and a pro\ufb01ling rank ratio candidates: \u00af\u03c1. Output: Determined \u02c6K. init timer() for layer range (lbeg, lend) \u2208 layer stacks do H = factorize layer stack(W, \u00af\u03c1, lbeg, lend); start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train H for one iteration 3.6 Putting things together The main algorithm of CUTTLEFISH is outlined in Algo- rithm 1. CUTTLEFISH begins with pro\ufb01ling to determine \u02c6K. Following this, the training method commences with full-rank training until the stable ranks for the layers to be factorized converge, i.e., at epoch \u02c6E. Subsequently, CUT- TLEFISH factorizes the partially trained full-rank network using the converged scaled stable ranks R to obtain the factorized low-rank model. Finally, the low-rank model is end end end time = timer.toc(); avg low-rank time = (end time-start time)/\u03c4 ; start time = timer.tic(); for iter \u2208 {1, 2, . . . , \u03c4 } do Train W for one iteration; end end time = timer.toc(); avg fullrank time = (end time-start time)/\u03c4 ; if fullrank time > \u03c5 \u00b7 avg low-rank time then \u02c6K = lend CUTTLEFISH: Low-rank Model Training without All The Tuning 4 EXPERIMENTS We have developed an ef\ufb01cient implementation of CUT- TLEFISH and conducted extensive experiments to evaluate its performance across various vision and natural language processing tasks. Our study focuses on the following as- pects: (i) the sizes of factorized models CUTTLEFISH dis- covers and their \ufb01nal accuracy; (ii) the end-to-end training speedups that CUTTLEFISH achieves in comparison to full- rank training and other baseline methods; (iii) how the \u02c6ss found by CUTTLEFISH compare to manually tuned and explicitly learned ones. Our comprehensive experimental results demonstrate that CUTTLEFISH automatically selects all factorization hyperparameters during training on-the- \ufb02y, eliminating the need for multiple experimental trials for factorization hyperparameter tuning. More speci\ufb01cally, the experimental results reveal that CUTTLEFISH generates models up to 5.6\u00d7 smaller than full-rank models, and at- tains up to a 1.2\u00d7 faster end-to-end training process while preserving comparable accuracy. Moreover, CUTTLEFISH outperforms state-of-the-art low-rank model training meth- ods and other prominent baselines. 4.1 Experimental setup and implementation details Pre-training ML tasks. We conducted experiments on various computer vision pre-training tasks, in- cluding CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and ImageNet (ILSVRC2012) (Deng et al., 2009). For CIFAR-10, CIFAR- 100, and SVHN (Netzer et al., 2011), we trained VGG- 19-BN (referred to as VGG-19) (Simonyan & Zisserman, 2014) and ResNet-18 (He et al., 2016). In the case of the SVHN dataset, we utilized the original training images and excluded the additional images. For ImageNet, our ex- periments involved ResNet-50, WideResNet-50-2 (referred to as WideResNet-50), DeiT-base, and ResMLP-S36 (He et al., 2016; Zagoruyko & Komodakis, 2016; Touvron et al., 2021b;a). Further details about the machine learning tasks can be found in the appendix. Fine-tuning ML tasks. We experiment on \ufb01ne-tuning BERTBASE over the GLUE benchmark (Wang et al., 2018). Hyperparameters & training schedule. For VGG-19 and ResNet-18 training on CIFAR-10 and CIFAR-100, we train the NNs for 300 epochs, while on SVHN, we train the NNs for 200 epochs. We employ a batch size of 1,024 for CIFAR and SVHN tasks to achieve high arithmetic inten- sity. The initial learning rate is linearly scaled up from 0.1 to 0.8 within \ufb01ve epochs and then decayed at milestones of 50% and 75% of the total training epochs (Goyal et al., 2017). For WideResNet-50 and ResNet-50 training on the ImageNet dataset, we follow the hyperparameter settings in (Goyal et al., 2017), where the models are trained for 90 epochs with an initial learning rate of 0.1, which is decayed by a factor of 0.1 at epochs 30, 60, and 80 using a batch size of 256. In addition to (Goyal et al., 2017), we apply label smoothing as described in (Wang et al., 2021a). For DeiT and ResMLP, we train them from scratch, adhering to the training schedule proposed in (Touvron et al., 2021b). For the GLUE \ufb01ne-tuning benchmark, we follow the default hy- perparameter setup in (Devlin et al., 2018; Jiao et al., 2020). Since CUTTLEFISH generalizes spectral initialization (SI) and is compatible with Frobenius decay (FD), we deploy FD in conjunction with CUTTLEFISH when it contributes to better accuracy. Further details on hyperparameters and training schedules can be found in the appendix. Experimental environment. We employ the NVIDIA NGC Docker container for software dependencies. Experi- ments for CIFAR, SVHN, and GLUE tasks are conducted on an EC2 p3.2xlarge instance (featuring a single V100 GPU) using FP32 precision. For BERT \ufb01ne-tuning and ImageNet training of ResNet-50 and WideResNet-50, the experiments are carried out on an EC2 g4dn.metal instance (equipped with eight T4 GPUs) using FP32 precision. For ImageNet training of DeiT and ResMLP, the experiments are performed on a single p4d.24xlarge instance (housing eight A100 GPUs) with mixed-precision training enabled. Baseline methods. We implement CUTTLEFISH and all considered baselines in PyTorch (Paszke et al., 2019). The baseline methods under consideration are: (i) PUFFERFISH, which employs manually tuned s (Wang et al., 2021a). To compare with PUFFERFISH, we use the same factorized ResNet-18, VGG-19, ResNet-50, and WideResNet-50 as reported in (Wang et al., 2021a). For DeiT and ResMLP models, not explored in the original PUFFERFISH paper, we adopt the same heuristic of using a \ufb01xed global rank ratio \u03c1 = 1 4 , tuning K to match the factorized model sizes found by CUTTLEFISH, and setting E = 80 for the entire training epochs T = 300 (Wang et al., 2021a); (ii) the factorized low- rank training method with SI and FD proposed by (Khodak et al., 2020) (referred to as \u201cSI&FD\u201d), with \u03c1s of SI&FD tuned to match the sizes of factorized models found by CUT- TLEFISH; (iii) training time structured pruning method, or \u201cearly bird ticket\u201d (EB Train) (You et al., 2020); (iv) the IMP method where each"}