{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/S._Welleck_Inference-Time_Policy_Adapters_(IPA):_Tailoring_Extreme-Scale_LMs_without_Fine-tuning_chunk_12.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What hyperparameters were varied during the grid search for the lexically constrained generation task?", "answer": " The number of training steps, KL coefficient \u03b2, and frequency of exploration.", "ref_chunk": "and for the frequency of exploration over the range [5, 20]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 18000 64 Adam 1e-8 1e-5 linear with warmup 800 0.05 8 Table 16: Hyperparameters for training policy adapter to reduce toxicity C.2.2 Lexically Constrained Generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 17. We performed a hyperparameter grid search for the number of training steps over the range [5k, 20k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. C.2.3 Open-ended generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 18. We performed a hyperparameter grid search for the number of training steps over the range [30k, 50k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 14000 64 Adam 1e-8 1e-5 linear with warmup 500 0.01 15 Table 17: Hyperparameters for training policy adapter to lexically constrained generation frequency of exploration over the range [15, 25]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 50000 64 Adam 1e-8 1e-5 linear with warmup 1500 0.05 25 Table 18: Hyperparameters for training policy adapter to open-ended generation C.2.4 Dialogue Safety Control We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 19. We performed a hyperparameter grid search for the number of training steps over the range [10k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. C.2.5 Knowledge-grounded Dialogue We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 20. We performed a hyperparameter grid search for the number of training steps over the range [7.5k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 15000 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 15 Table 19: Hyperparameters for training policy adapter to control dialogue safety for the frequency of exploration over the range [15, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 12500 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 25 Table 20: Hyperparameters for training policy adapter to improve dialogue faithfulness D Additional Related Works Parameter-Efficient Fine-Tuning Prompting and prefix-tuning (Li and Liang, 2021) adapt a very large model to a specific task. However, they are affected by sensitivity based on order of words or examples (Zhao et al., 2021; Webson and Pavlick, 2022), lack associative clarity (Min et al., 2022) and tuning prompts work for only very large mod- els (Mahabadi et al., 2021; Liu et al., 2022b). These methods compose the input to the model. In con- trast, parameter-efficient finetuning offers a clean way to compose parameters directly by adding or updating a smaller subset of model parameters. A common strategy is to prune the model parameters and introduce sparsity (Han et al., 2017; Frankle and Carbin, 2019; Frankle et al., 2020). The ef- fectiveness of this approach is also substantiated with the use of RL (Yu et al., 2020). Instead of pruning individual units, structured-pruning prunes an entire group, such as attention heads in pre- trained models (Michel et al., 2019; Voita et al., 2019). Additionally, (Li et al., 2018) demonstrate the effectiveness of optimizing a model in a low- dimensional randomly oriented subspace. Later studies (Aghajanyan et al., 2021) have also shown that the intrinsic dimensionality decreases with pre- training larger models. (Hu et al., 2022) learns a low-rank factorization via projection matrix and ap- plies them to the self-attention weights. Recently, adding a small subset of parameters called adapters (Rebuffi et al., 2017) and compact adapters (Ma- habadi et al., 2021) which are model-specific (Stick- land and Murray, 2019). Pfeiffer et al. (2020) intro- duced a continuously evolving Adapter-Hub that stitches different pre-trained adapters for languages and tasks inspired from routing networks (Rosen- baum et al., 2019) optimized through reinforcement learning (Kirsch et al., 2018; Chang et al., 2019). Though these methods are efficient, they require access to the internal representation for model and gradient, which is not feasible for large models like GPT3 with limited access. Refinement. Recent work controls (L)LMs by re- fining a generated sequence into an improved one with a refinement module (Yasunaga and Liang, 2020; Saunders et al., 2022; Schick et al., 2022; Yang et al., 2022; Welleck et al., 2023; Madaan"}, {"question": " What sampling method was used during inference for the open-ended generation task?", "answer": " Nucleus sampling with p = 0.9 and temperature 1.0.", "ref_chunk": "and for the frequency of exploration over the range [5, 20]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 18000 64 Adam 1e-8 1e-5 linear with warmup 800 0.05 8 Table 16: Hyperparameters for training policy adapter to reduce toxicity C.2.2 Lexically Constrained Generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 17. We performed a hyperparameter grid search for the number of training steps over the range [5k, 20k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. C.2.3 Open-ended generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 18. We performed a hyperparameter grid search for the number of training steps over the range [30k, 50k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 14000 64 Adam 1e-8 1e-5 linear with warmup 500 0.01 15 Table 17: Hyperparameters for training policy adapter to lexically constrained generation frequency of exploration over the range [15, 25]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 50000 64 Adam 1e-8 1e-5 linear with warmup 1500 0.05 25 Table 18: Hyperparameters for training policy adapter to open-ended generation C.2.4 Dialogue Safety Control We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 19. We performed a hyperparameter grid search for the number of training steps over the range [10k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. C.2.5 Knowledge-grounded Dialogue We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 20. We performed a hyperparameter grid search for the number of training steps over the range [7.5k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 15000 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 15 Table 19: Hyperparameters for training policy adapter to control dialogue safety for the frequency of exploration over the range [15, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 12500 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 25 Table 20: Hyperparameters for training policy adapter to improve dialogue faithfulness D Additional Related Works Parameter-Efficient Fine-Tuning Prompting and prefix-tuning (Li and Liang, 2021) adapt a very large model to a specific task. However, they are affected by sensitivity based on order of words or examples (Zhao et al., 2021; Webson and Pavlick, 2022), lack associative clarity (Min et al., 2022) and tuning prompts work for only very large mod- els (Mahabadi et al., 2021; Liu et al., 2022b). These methods compose the input to the model. In con- trast, parameter-efficient finetuning offers a clean way to compose parameters directly by adding or updating a smaller subset of model parameters. A common strategy is to prune the model parameters and introduce sparsity (Han et al., 2017; Frankle and Carbin, 2019; Frankle et al., 2020). The ef- fectiveness of this approach is also substantiated with the use of RL (Yu et al., 2020). Instead of pruning individual units, structured-pruning prunes an entire group, such as attention heads in pre- trained models (Michel et al., 2019; Voita et al., 2019). Additionally, (Li et al., 2018) demonstrate the effectiveness of optimizing a model in a low- dimensional randomly oriented subspace. Later studies (Aghajanyan et al., 2021) have also shown that the intrinsic dimensionality decreases with pre- training larger models. (Hu et al., 2022) learns a low-rank factorization via projection matrix and ap- plies them to the self-attention weights. Recently, adding a small subset of parameters called adapters (Rebuffi et al., 2017) and compact adapters (Ma- habadi et al., 2021) which are model-specific (Stick- land and Murray, 2019). Pfeiffer et al. (2020) intro- duced a continuously evolving Adapter-Hub that stitches different pre-trained adapters for languages and tasks inspired from routing networks (Rosen- baum et al., 2019) optimized through reinforcement learning (Kirsch et al., 2018; Chang et al., 2019). Though these methods are efficient, they require access to the internal representation for model and gradient, which is not feasible for large models like GPT3 with limited access. Refinement. Recent work controls (L)LMs by re- fining a generated sequence into an improved one with a refinement module (Yasunaga and Liang, 2020; Saunders et al., 2022; Schick et al., 2022; Yang et al., 2022; Welleck et al., 2023; Madaan"}, {"question": " Which model was used to initialize the policy adapter for the dialogue safety control task?", "answer": " blenderbot-1B-distill model.", "ref_chunk": "and for the frequency of exploration over the range [5, 20]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 18000 64 Adam 1e-8 1e-5 linear with warmup 800 0.05 8 Table 16: Hyperparameters for training policy adapter to reduce toxicity C.2.2 Lexically Constrained Generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 17. We performed a hyperparameter grid search for the number of training steps over the range [5k, 20k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. C.2.3 Open-ended generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 18. We performed a hyperparameter grid search for the number of training steps over the range [30k, 50k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 14000 64 Adam 1e-8 1e-5 linear with warmup 500 0.01 15 Table 17: Hyperparameters for training policy adapter to lexically constrained generation frequency of exploration over the range [15, 25]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 50000 64 Adam 1e-8 1e-5 linear with warmup 1500 0.05 25 Table 18: Hyperparameters for training policy adapter to open-ended generation C.2.4 Dialogue Safety Control We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 19. We performed a hyperparameter grid search for the number of training steps over the range [10k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. C.2.5 Knowledge-grounded Dialogue We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 20. We performed a hyperparameter grid search for the number of training steps over the range [7.5k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 15000 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 15 Table 19: Hyperparameters for training policy adapter to control dialogue safety for the frequency of exploration over the range [15, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 12500 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 25 Table 20: Hyperparameters for training policy adapter to improve dialogue faithfulness D Additional Related Works Parameter-Efficient Fine-Tuning Prompting and prefix-tuning (Li and Liang, 2021) adapt a very large model to a specific task. However, they are affected by sensitivity based on order of words or examples (Zhao et al., 2021; Webson and Pavlick, 2022), lack associative clarity (Min et al., 2022) and tuning prompts work for only very large mod- els (Mahabadi et al., 2021; Liu et al., 2022b). These methods compose the input to the model. In con- trast, parameter-efficient finetuning offers a clean way to compose parameters directly by adding or updating a smaller subset of model parameters. A common strategy is to prune the model parameters and introduce sparsity (Han et al., 2017; Frankle and Carbin, 2019; Frankle et al., 2020). The ef- fectiveness of this approach is also substantiated with the use of RL (Yu et al., 2020). Instead of pruning individual units, structured-pruning prunes an entire group, such as attention heads in pre- trained models (Michel et al., 2019; Voita et al., 2019). Additionally, (Li et al., 2018) demonstrate the effectiveness of optimizing a model in a low- dimensional randomly oriented subspace. Later studies (Aghajanyan et al., 2021) have also shown that the intrinsic dimensionality decreases with pre- training larger models. (Hu et al., 2022) learns a low-rank factorization via projection matrix and ap- plies them to the self-attention weights. Recently, adding a small subset of parameters called adapters (Rebuffi et al., 2017) and compact adapters (Ma- habadi et al., 2021) which are model-specific (Stick- land and Murray, 2019). Pfeiffer et al. (2020) intro- duced a continuously evolving Adapter-Hub that stitches different pre-trained adapters for languages and tasks inspired from routing networks (Rosen- baum et al., 2019) optimized through reinforcement learning (Kirsch et al., 2018; Chang et al., 2019). Though these methods are efficient, they require access to the internal representation for model and gradient, which is not feasible for large models like GPT3 with limited access. Refinement. Recent work controls (L)LMs by re- fining a generated sequence into an improved one with a refinement module (Yasunaga and Liang, 2020; Saunders et al., 2022; Schick et al., 2022; Yang et al., 2022; Welleck et al., 2023; Madaan"}, {"question": " What sampling method was used during inference for the knowledge-grounded dialogue task?", "answer": " Nucleus sampling with p = 0.6 and temperature 1.0.", "ref_chunk": "and for the frequency of exploration over the range [5, 20]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 18000 64 Adam 1e-8 1e-5 linear with warmup 800 0.05 8 Table 16: Hyperparameters for training policy adapter to reduce toxicity C.2.2 Lexically Constrained Generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 17. We performed a hyperparameter grid search for the number of training steps over the range [5k, 20k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. C.2.3 Open-ended generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 18. We performed a hyperparameter grid search for the number of training steps over the range [30k, 50k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 14000 64 Adam 1e-8 1e-5 linear with warmup 500 0.01 15 Table 17: Hyperparameters for training policy adapter to lexically constrained generation frequency of exploration over the range [15, 25]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 50000 64 Adam 1e-8 1e-5 linear with warmup 1500 0.05 25 Table 18: Hyperparameters for training policy adapter to open-ended generation C.2.4 Dialogue Safety Control We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 19. We performed a hyperparameter grid search for the number of training steps over the range [10k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. C.2.5 Knowledge-grounded Dialogue We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 20. We performed a hyperparameter grid search for the number of training steps over the range [7.5k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 15000 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 15 Table 19: Hyperparameters for training policy adapter to control dialogue safety for the frequency of exploration over the range [15, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 12500 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 25 Table 20: Hyperparameters for training policy adapter to improve dialogue faithfulness D Additional Related Works Parameter-Efficient Fine-Tuning Prompting and prefix-tuning (Li and Liang, 2021) adapt a very large model to a specific task. However, they are affected by sensitivity based on order of words or examples (Zhao et al., 2021; Webson and Pavlick, 2022), lack associative clarity (Min et al., 2022) and tuning prompts work for only very large mod- els (Mahabadi et al., 2021; Liu et al., 2022b). These methods compose the input to the model. In con- trast, parameter-efficient finetuning offers a clean way to compose parameters directly by adding or updating a smaller subset of model parameters. A common strategy is to prune the model parameters and introduce sparsity (Han et al., 2017; Frankle and Carbin, 2019; Frankle et al., 2020). The ef- fectiveness of this approach is also substantiated with the use of RL (Yu et al., 2020). Instead of pruning individual units, structured-pruning prunes an entire group, such as attention heads in pre- trained models (Michel et al., 2019; Voita et al., 2019). Additionally, (Li et al., 2018) demonstrate the effectiveness of optimizing a model in a low- dimensional randomly oriented subspace. Later studies (Aghajanyan et al., 2021) have also shown that the intrinsic dimensionality decreases with pre- training larger models. (Hu et al., 2022) learns a low-rank factorization via projection matrix and ap- plies them to the self-attention weights. Recently, adding a small subset of parameters called adapters (Rebuffi et al., 2017) and compact adapters (Ma- habadi et al., 2021) which are model-specific (Stick- land and Murray, 2019). Pfeiffer et al. (2020) intro- duced a continuously evolving Adapter-Hub that stitches different pre-trained adapters for languages and tasks inspired from routing networks (Rosen- baum et al., 2019) optimized through reinforcement learning (Kirsch et al., 2018; Chang et al., 2019). Though these methods are efficient, they require access to the internal representation for model and gradient, which is not feasible for large models like GPT3 with limited access. Refinement. Recent work controls (L)LMs by re- fining a generated sequence into an improved one with a refinement module (Yasunaga and Liang, 2020; Saunders et al., 2022; Schick et al., 2022; Yang et al., 2022; Welleck et al., 2023; Madaan"}, {"question": " What is the goal of parameter-efficient fine-tuning?", "answer": " To compose parameters directly by adding or updating a smaller subset of model parameters.", "ref_chunk": "and for the frequency of exploration over the range [5, 20]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 18000 64 Adam 1e-8 1e-5 linear with warmup 800 0.05 8 Table 16: Hyperparameters for training policy adapter to reduce toxicity C.2.2 Lexically Constrained Generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 17. We performed a hyperparameter grid search for the number of training steps over the range [5k, 20k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. C.2.3 Open-ended generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 18. We performed a hyperparameter grid search for the number of training steps over the range [30k, 50k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 14000 64 Adam 1e-8 1e-5 linear with warmup 500 0.01 15 Table 17: Hyperparameters for training policy adapter to lexically constrained generation frequency of exploration over the range [15, 25]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 50000 64 Adam 1e-8 1e-5 linear with warmup 1500 0.05 25 Table 18: Hyperparameters for training policy adapter to open-ended generation C.2.4 Dialogue Safety Control We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 19. We performed a hyperparameter grid search for the number of training steps over the range [10k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. C.2.5 Knowledge-grounded Dialogue We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 20. We performed a hyperparameter grid search for the number of training steps over the range [7.5k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 15000 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 15 Table 19: Hyperparameters for training policy adapter to control dialogue safety for the frequency of exploration over the range [15, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 12500 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 25 Table 20: Hyperparameters for training policy adapter to improve dialogue faithfulness D Additional Related Works Parameter-Efficient Fine-Tuning Prompting and prefix-tuning (Li and Liang, 2021) adapt a very large model to a specific task. However, they are affected by sensitivity based on order of words or examples (Zhao et al., 2021; Webson and Pavlick, 2022), lack associative clarity (Min et al., 2022) and tuning prompts work for only very large mod- els (Mahabadi et al., 2021; Liu et al., 2022b). These methods compose the input to the model. In con- trast, parameter-efficient finetuning offers a clean way to compose parameters directly by adding or updating a smaller subset of model parameters. A common strategy is to prune the model parameters and introduce sparsity (Han et al., 2017; Frankle and Carbin, 2019; Frankle et al., 2020). The ef- fectiveness of this approach is also substantiated with the use of RL (Yu et al., 2020). Instead of pruning individual units, structured-pruning prunes an entire group, such as attention heads in pre- trained models (Michel et al., 2019; Voita et al., 2019). Additionally, (Li et al., 2018) demonstrate the effectiveness of optimizing a model in a low- dimensional randomly oriented subspace. Later studies (Aghajanyan et al., 2021) have also shown that the intrinsic dimensionality decreases with pre- training larger models. (Hu et al., 2022) learns a low-rank factorization via projection matrix and ap- plies them to the self-attention weights. Recently, adding a small subset of parameters called adapters (Rebuffi et al., 2017) and compact adapters (Ma- habadi et al., 2021) which are model-specific (Stick- land and Murray, 2019). Pfeiffer et al. (2020) intro- duced a continuously evolving Adapter-Hub that stitches different pre-trained adapters for languages and tasks inspired from routing networks (Rosen- baum et al., 2019) optimized through reinforcement learning (Kirsch et al., 2018; Chang et al., 2019). Though these methods are efficient, they require access to the internal representation for model and gradient, which is not feasible for large models like GPT3 with limited access. Refinement. Recent work controls (L)LMs by re- fining a generated sequence into an improved one with a refinement module (Yasunaga and Liang, 2020; Saunders et al., 2022; Schick et al., 2022; Yang et al., 2022; Welleck et al., 2023; Madaan"}, {"question": " What strategy is commonly used in parameter-efficient fine-tuning?", "answer": " Pruning the model parameters and introducing sparsity.", "ref_chunk": "and for the frequency of exploration over the range [5, 20]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 18000 64 Adam 1e-8 1e-5 linear with warmup 800 0.05 8 Table 16: Hyperparameters for training policy adapter to reduce toxicity C.2.2 Lexically Constrained Generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 17. We performed a hyperparameter grid search for the number of training steps over the range [5k, 20k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. C.2.3 Open-ended generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 18. We performed a hyperparameter grid search for the number of training steps over the range [30k, 50k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 14000 64 Adam 1e-8 1e-5 linear with warmup 500 0.01 15 Table 17: Hyperparameters for training policy adapter to lexically constrained generation frequency of exploration over the range [15, 25]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 50000 64 Adam 1e-8 1e-5 linear with warmup 1500 0.05 25 Table 18: Hyperparameters for training policy adapter to open-ended generation C.2.4 Dialogue Safety Control We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 19. We performed a hyperparameter grid search for the number of training steps over the range [10k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. C.2.5 Knowledge-grounded Dialogue We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 20. We performed a hyperparameter grid search for the number of training steps over the range [7.5k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 15000 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 15 Table 19: Hyperparameters for training policy adapter to control dialogue safety for the frequency of exploration over the range [15, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 12500 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 25 Table 20: Hyperparameters for training policy adapter to improve dialogue faithfulness D Additional Related Works Parameter-Efficient Fine-Tuning Prompting and prefix-tuning (Li and Liang, 2021) adapt a very large model to a specific task. However, they are affected by sensitivity based on order of words or examples (Zhao et al., 2021; Webson and Pavlick, 2022), lack associative clarity (Min et al., 2022) and tuning prompts work for only very large mod- els (Mahabadi et al., 2021; Liu et al., 2022b). These methods compose the input to the model. In con- trast, parameter-efficient finetuning offers a clean way to compose parameters directly by adding or updating a smaller subset of model parameters. A common strategy is to prune the model parameters and introduce sparsity (Han et al., 2017; Frankle and Carbin, 2019; Frankle et al., 2020). The ef- fectiveness of this approach is also substantiated with the use of RL (Yu et al., 2020). Instead of pruning individual units, structured-pruning prunes an entire group, such as attention heads in pre- trained models (Michel et al., 2019; Voita et al., 2019). Additionally, (Li et al., 2018) demonstrate the effectiveness of optimizing a model in a low- dimensional randomly oriented subspace. Later studies (Aghajanyan et al., 2021) have also shown that the intrinsic dimensionality decreases with pre- training larger models. (Hu et al., 2022) learns a low-rank factorization via projection matrix and ap- plies them to the self-attention weights. Recently, adding a small subset of parameters called adapters (Rebuffi et al., 2017) and compact adapters (Ma- habadi et al., 2021) which are model-specific (Stick- land and Murray, 2019). Pfeiffer et al. (2020) intro- duced a continuously evolving Adapter-Hub that stitches different pre-trained adapters for languages and tasks inspired from routing networks (Rosen- baum et al., 2019) optimized through reinforcement learning (Kirsch et al., 2018; Chang et al., 2019). Though these methods are efficient, they require access to the internal representation for model and gradient, which is not feasible for large models like GPT3 with limited access. Refinement. Recent work controls (L)LMs by re- fining a generated sequence into an improved one with a refinement module (Yasunaga and Liang, 2020; Saunders et al., 2022; Schick et al., 2022; Yang et al., 2022; Welleck et al., 2023; Madaan"}, {"question": " What is the purpose of structured-pruning in model optimization?", "answer": " To prune an entire group, such as attention heads in pre-trained models.", "ref_chunk": "and for the frequency of exploration over the range [5, 20]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 18000 64 Adam 1e-8 1e-5 linear with warmup 800 0.05 8 Table 16: Hyperparameters for training policy adapter to reduce toxicity C.2.2 Lexically Constrained Generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 17. We performed a hyperparameter grid search for the number of training steps over the range [5k, 20k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. C.2.3 Open-ended generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 18. We performed a hyperparameter grid search for the number of training steps over the range [30k, 50k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 14000 64 Adam 1e-8 1e-5 linear with warmup 500 0.01 15 Table 17: Hyperparameters for training policy adapter to lexically constrained generation frequency of exploration over the range [15, 25]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 50000 64 Adam 1e-8 1e-5 linear with warmup 1500 0.05 25 Table 18: Hyperparameters for training policy adapter to open-ended generation C.2.4 Dialogue Safety Control We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 19. We performed a hyperparameter grid search for the number of training steps over the range [10k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. C.2.5 Knowledge-grounded Dialogue We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 20. We performed a hyperparameter grid search for the number of training steps over the range [7.5k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 15000 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 15 Table 19: Hyperparameters for training policy adapter to control dialogue safety for the frequency of exploration over the range [15, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 12500 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 25 Table 20: Hyperparameters for training policy adapter to improve dialogue faithfulness D Additional Related Works Parameter-Efficient Fine-Tuning Prompting and prefix-tuning (Li and Liang, 2021) adapt a very large model to a specific task. However, they are affected by sensitivity based on order of words or examples (Zhao et al., 2021; Webson and Pavlick, 2022), lack associative clarity (Min et al., 2022) and tuning prompts work for only very large mod- els (Mahabadi et al., 2021; Liu et al., 2022b). These methods compose the input to the model. In con- trast, parameter-efficient finetuning offers a clean way to compose parameters directly by adding or updating a smaller subset of model parameters. A common strategy is to prune the model parameters and introduce sparsity (Han et al., 2017; Frankle and Carbin, 2019; Frankle et al., 2020). The ef- fectiveness of this approach is also substantiated with the use of RL (Yu et al., 2020). Instead of pruning individual units, structured-pruning prunes an entire group, such as attention heads in pre- trained models (Michel et al., 2019; Voita et al., 2019). Additionally, (Li et al., 2018) demonstrate the effectiveness of optimizing a model in a low- dimensional randomly oriented subspace. Later studies (Aghajanyan et al., 2021) have also shown that the intrinsic dimensionality decreases with pre- training larger models. (Hu et al., 2022) learns a low-rank factorization via projection matrix and ap- plies them to the self-attention weights. Recently, adding a small subset of parameters called adapters (Rebuffi et al., 2017) and compact adapters (Ma- habadi et al., 2021) which are model-specific (Stick- land and Murray, 2019). Pfeiffer et al. (2020) intro- duced a continuously evolving Adapter-Hub that stitches different pre-trained adapters for languages and tasks inspired from routing networks (Rosen- baum et al., 2019) optimized through reinforcement learning (Kirsch et al., 2018; Chang et al., 2019). Though these methods are efficient, they require access to the internal representation for model and gradient, which is not feasible for large models like GPT3 with limited access. Refinement. Recent work controls (L)LMs by re- fining a generated sequence into an improved one with a refinement module (Yasunaga and Liang, 2020; Saunders et al., 2022; Schick et al., 2022; Yang et al., 2022; Welleck et al., 2023; Madaan"}, {"question": " What approach demonstrated the effectiveness of model optimization in a low-dimensional subspace?", "answer": " Optimizing a model in a low-dimensional randomly oriented subspace.", "ref_chunk": "and for the frequency of exploration over the range [5, 20]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 18000 64 Adam 1e-8 1e-5 linear with warmup 800 0.05 8 Table 16: Hyperparameters for training policy adapter to reduce toxicity C.2.2 Lexically Constrained Generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 17. We performed a hyperparameter grid search for the number of training steps over the range [5k, 20k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. C.2.3 Open-ended generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 18. We performed a hyperparameter grid search for the number of training steps over the range [30k, 50k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 14000 64 Adam 1e-8 1e-5 linear with warmup 500 0.01 15 Table 17: Hyperparameters for training policy adapter to lexically constrained generation frequency of exploration over the range [15, 25]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 50000 64 Adam 1e-8 1e-5 linear with warmup 1500 0.05 25 Table 18: Hyperparameters for training policy adapter to open-ended generation C.2.4 Dialogue Safety Control We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 19. We performed a hyperparameter grid search for the number of training steps over the range [10k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. C.2.5 Knowledge-grounded Dialogue We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 20. We performed a hyperparameter grid search for the number of training steps over the range [7.5k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 15000 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 15 Table 19: Hyperparameters for training policy adapter to control dialogue safety for the frequency of exploration over the range [15, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 12500 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 25 Table 20: Hyperparameters for training policy adapter to improve dialogue faithfulness D Additional Related Works Parameter-Efficient Fine-Tuning Prompting and prefix-tuning (Li and Liang, 2021) adapt a very large model to a specific task. However, they are affected by sensitivity based on order of words or examples (Zhao et al., 2021; Webson and Pavlick, 2022), lack associative clarity (Min et al., 2022) and tuning prompts work for only very large mod- els (Mahabadi et al., 2021; Liu et al., 2022b). These methods compose the input to the model. In con- trast, parameter-efficient finetuning offers a clean way to compose parameters directly by adding or updating a smaller subset of model parameters. A common strategy is to prune the model parameters and introduce sparsity (Han et al., 2017; Frankle and Carbin, 2019; Frankle et al., 2020). The ef- fectiveness of this approach is also substantiated with the use of RL (Yu et al., 2020). Instead of pruning individual units, structured-pruning prunes an entire group, such as attention heads in pre- trained models (Michel et al., 2019; Voita et al., 2019). Additionally, (Li et al., 2018) demonstrate the effectiveness of optimizing a model in a low- dimensional randomly oriented subspace. Later studies (Aghajanyan et al., 2021) have also shown that the intrinsic dimensionality decreases with pre- training larger models. (Hu et al., 2022) learns a low-rank factorization via projection matrix and ap- plies them to the self-attention weights. Recently, adding a small subset of parameters called adapters (Rebuffi et al., 2017) and compact adapters (Ma- habadi et al., 2021) which are model-specific (Stick- land and Murray, 2019). Pfeiffer et al. (2020) intro- duced a continuously evolving Adapter-Hub that stitches different pre-trained adapters for languages and tasks inspired from routing networks (Rosen- baum et al., 2019) optimized through reinforcement learning (Kirsch et al., 2018; Chang et al., 2019). Though these methods are efficient, they require access to the internal representation for model and gradient, which is not feasible for large models like GPT3 with limited access. Refinement. Recent work controls (L)LMs by re- fining a generated sequence into an improved one with a refinement module (Yasunaga and Liang, 2020; Saunders et al., 2022; Schick et al., 2022; Yang et al., 2022; Welleck et al., 2023; Madaan"}, {"question": " What is the function of adapters and compact adapters in model optimization?", "answer": " To add a small subset of parameters, which are model-specific.", "ref_chunk": "and for the frequency of exploration over the range [5, 20]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 18000 64 Adam 1e-8 1e-5 linear with warmup 800 0.05 8 Table 16: Hyperparameters for training policy adapter to reduce toxicity C.2.2 Lexically Constrained Generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 17. We performed a hyperparameter grid search for the number of training steps over the range [5k, 20k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. C.2.3 Open-ended generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 18. We performed a hyperparameter grid search for the number of training steps over the range [30k, 50k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 14000 64 Adam 1e-8 1e-5 linear with warmup 500 0.01 15 Table 17: Hyperparameters for training policy adapter to lexically constrained generation frequency of exploration over the range [15, 25]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 50000 64 Adam 1e-8 1e-5 linear with warmup 1500 0.05 25 Table 18: Hyperparameters for training policy adapter to open-ended generation C.2.4 Dialogue Safety Control We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 19. We performed a hyperparameter grid search for the number of training steps over the range [10k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. C.2.5 Knowledge-grounded Dialogue We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 20. We performed a hyperparameter grid search for the number of training steps over the range [7.5k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 15000 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 15 Table 19: Hyperparameters for training policy adapter to control dialogue safety for the frequency of exploration over the range [15, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 12500 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 25 Table 20: Hyperparameters for training policy adapter to improve dialogue faithfulness D Additional Related Works Parameter-Efficient Fine-Tuning Prompting and prefix-tuning (Li and Liang, 2021) adapt a very large model to a specific task. However, they are affected by sensitivity based on order of words or examples (Zhao et al., 2021; Webson and Pavlick, 2022), lack associative clarity (Min et al., 2022) and tuning prompts work for only very large mod- els (Mahabadi et al., 2021; Liu et al., 2022b). These methods compose the input to the model. In con- trast, parameter-efficient finetuning offers a clean way to compose parameters directly by adding or updating a smaller subset of model parameters. A common strategy is to prune the model parameters and introduce sparsity (Han et al., 2017; Frankle and Carbin, 2019; Frankle et al., 2020). The ef- fectiveness of this approach is also substantiated with the use of RL (Yu et al., 2020). Instead of pruning individual units, structured-pruning prunes an entire group, such as attention heads in pre- trained models (Michel et al., 2019; Voita et al., 2019). Additionally, (Li et al., 2018) demonstrate the effectiveness of optimizing a model in a low- dimensional randomly oriented subspace. Later studies (Aghajanyan et al., 2021) have also shown that the intrinsic dimensionality decreases with pre- training larger models. (Hu et al., 2022) learns a low-rank factorization via projection matrix and ap- plies them to the self-attention weights. Recently, adding a small subset of parameters called adapters (Rebuffi et al., 2017) and compact adapters (Ma- habadi et al., 2021) which are model-specific (Stick- land and Murray, 2019). Pfeiffer et al. (2020) intro- duced a continuously evolving Adapter-Hub that stitches different pre-trained adapters for languages and tasks inspired from routing networks (Rosen- baum et al., 2019) optimized through reinforcement learning (Kirsch et al., 2018; Chang et al., 2019). Though these methods are efficient, they require access to the internal representation for model and gradient, which is not feasible for large models like GPT3 with limited access. Refinement. Recent work controls (L)LMs by re- fining a generated sequence into an improved one with a refinement module (Yasunaga and Liang, 2020; Saunders et al., 2022; Schick et al., 2022; Yang et al., 2022; Welleck et al., 2023; Madaan"}, {"question": " What is the limitation of methods that control large models like GPT3?", "answer": " They require access to the internal representation for model and gradient, which is not feasible.", "ref_chunk": "and for the frequency of exploration over the range [5, 20]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 18000 64 Adam 1e-8 1e-5 linear with warmup 800 0.05 8 Table 16: Hyperparameters for training policy adapter to reduce toxicity C.2.2 Lexically Constrained Generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 17. We performed a hyperparameter grid search for the number of training steps over the range [5k, 20k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. C.2.3 Open-ended generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 18. We performed a hyperparameter grid search for the number of training steps over the range [30k, 50k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 14000 64 Adam 1e-8 1e-5 linear with warmup 500 0.01 15 Table 17: Hyperparameters for training policy adapter to lexically constrained generation frequency of exploration over the range [15, 25]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 50000 64 Adam 1e-8 1e-5 linear with warmup 1500 0.05 25 Table 18: Hyperparameters for training policy adapter to open-ended generation C.2.4 Dialogue Safety Control We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 19. We performed a hyperparameter grid search for the number of training steps over the range [10k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. C.2.5 Knowledge-grounded Dialogue We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 20. We performed a hyperparameter grid search for the number of training steps over the range [7.5k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 15000 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 15 Table 19: Hyperparameters for training policy adapter to control dialogue safety for the frequency of exploration over the range [15, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 12500 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 25 Table 20: Hyperparameters for training policy adapter to improve dialogue faithfulness D Additional Related Works Parameter-Efficient Fine-Tuning Prompting and prefix-tuning (Li and Liang, 2021) adapt a very large model to a specific task. However, they are affected by sensitivity based on order of words or examples (Zhao et al., 2021; Webson and Pavlick, 2022), lack associative clarity (Min et al., 2022) and tuning prompts work for only very large mod- els (Mahabadi et al., 2021; Liu et al., 2022b). These methods compose the input to the model. In con- trast, parameter-efficient finetuning offers a clean way to compose parameters directly by adding or updating a smaller subset of model parameters. A common strategy is to prune the model parameters and introduce sparsity (Han et al., 2017; Frankle and Carbin, 2019; Frankle et al., 2020). The ef- fectiveness of this approach is also substantiated with the use of RL (Yu et al., 2020). Instead of pruning individual units, structured-pruning prunes an entire group, such as attention heads in pre- trained models (Michel et al., 2019; Voita et al., 2019). Additionally, (Li et al., 2018) demonstrate the effectiveness of optimizing a model in a low- dimensional randomly oriented subspace. Later studies (Aghajanyan et al., 2021) have also shown that the intrinsic dimensionality decreases with pre- training larger models. (Hu et al., 2022) learns a low-rank factorization via projection matrix and ap- plies them to the self-attention weights. Recently, adding a small subset of parameters called adapters (Rebuffi et al., 2017) and compact adapters (Ma- habadi et al., 2021) which are model-specific (Stick- land and Murray, 2019). Pfeiffer et al. (2020) intro- duced a continuously evolving Adapter-Hub that stitches different pre-trained adapters for languages and tasks inspired from routing networks (Rosen- baum et al., 2019) optimized through reinforcement learning (Kirsch et al., 2018; Chang et al., 2019). Though these methods are efficient, they require access to the internal representation for model and gradient, which is not feasible for large models like GPT3 with limited access. Refinement. Recent work controls (L)LMs by re- fining a generated sequence into an improved one with a refinement module (Yasunaga and Liang, 2020; Saunders et al., 2022; Schick et al., 2022; Yang et al., 2022; Welleck et al., 2023; Madaan"}], "doc_text": "and for the frequency of exploration over the range [5, 20]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 18000 64 Adam 1e-8 1e-5 linear with warmup 800 0.05 8 Table 16: Hyperparameters for training policy adapter to reduce toxicity C.2.2 Lexically Constrained Generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 17. We performed a hyperparameter grid search for the number of training steps over the range [5k, 20k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. C.2.3 Open-ended generation We initialize the policy adapter with an off-the- shelf GPT2-L model and use QUARK as the RL al- gorithm for the adapter training. Hyperparameters for training are given in Table 18. We performed a hyperparameter grid search for the number of training steps over the range [30k, 50k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 14000 64 Adam 1e-8 1e-5 linear with warmup 500 0.01 15 Table 17: Hyperparameters for training policy adapter to lexically constrained generation frequency of exploration over the range [15, 25]. During inference, we use nucleus sampling with p = 0.9 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment GPT2-Large 774M 50000 64 Adam 1e-8 1e-5 linear with warmup 1500 0.05 25 Table 18: Hyperparameters for training policy adapter to open-ended generation C.2.4 Dialogue Safety Control We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 19. We performed a hyperparameter grid search for the number of training steps over the range [10k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and for the frequency of exploration over the range [10, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. C.2.5 Knowledge-grounded Dialogue We initialize the policy adapter with an off-the- shelf blenderbot-1B-distill model and use QUARK as the RL algorithm for the adapter training. Hy- perparameters for training are given in Table 20. We performed a hyperparameter grid search for the number of training steps over the range [7.5k, 15k], for the KL coefficient \u03b2 over the range [0, 0.3], and Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 15000 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 15 Table 19: Hyperparameters for training policy adapter to control dialogue safety for the frequency of exploration over the range [15, 30]. During inference, we use nucleus sampling with p = 0.6 and temperature 1.0. Hyperparameter model number of parameters number of steps batch size learning rate optimizer Adam epsilon Adam initial learning rate learning rate scheduler warmup steps KL coefficient \u03b2 frequency of exploration Assignment blenderbot-1B-distill 1B 12500 64 Adam 1e-8 1e-5 linear with warmup 300 0.1 25 Table 20: Hyperparameters for training policy adapter to improve dialogue faithfulness D Additional Related Works Parameter-Efficient Fine-Tuning Prompting and prefix-tuning (Li and Liang, 2021) adapt a very large model to a specific task. However, they are affected by sensitivity based on order of words or examples (Zhao et al., 2021; Webson and Pavlick, 2022), lack associative clarity (Min et al., 2022) and tuning prompts work for only very large mod- els (Mahabadi et al., 2021; Liu et al., 2022b). These methods compose the input to the model. In con- trast, parameter-efficient finetuning offers a clean way to compose parameters directly by adding or updating a smaller subset of model parameters. A common strategy is to prune the model parameters and introduce sparsity (Han et al., 2017; Frankle and Carbin, 2019; Frankle et al., 2020). The ef- fectiveness of this approach is also substantiated with the use of RL (Yu et al., 2020). Instead of pruning individual units, structured-pruning prunes an entire group, such as attention heads in pre- trained models (Michel et al., 2019; Voita et al., 2019). Additionally, (Li et al., 2018) demonstrate the effectiveness of optimizing a model in a low- dimensional randomly oriented subspace. Later studies (Aghajanyan et al., 2021) have also shown that the intrinsic dimensionality decreases with pre- training larger models. (Hu et al., 2022) learns a low-rank factorization via projection matrix and ap- plies them to the self-attention weights. Recently, adding a small subset of parameters called adapters (Rebuffi et al., 2017) and compact adapters (Ma- habadi et al., 2021) which are model-specific (Stick- land and Murray, 2019). Pfeiffer et al. (2020) intro- duced a continuously evolving Adapter-Hub that stitches different pre-trained adapters for languages and tasks inspired from routing networks (Rosen- baum et al., 2019) optimized through reinforcement learning (Kirsch et al., 2018; Chang et al., 2019). Though these methods are efficient, they require access to the internal representation for model and gradient, which is not feasible for large models like GPT3 with limited access. Refinement. Recent work controls (L)LMs by re- fining a generated sequence into an improved one with a refinement module (Yasunaga and Liang, 2020; Saunders et al., 2022; Schick et al., 2022; Yang et al., 2022; Welleck et al., 2023; Madaan"}