{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_The_Devil_Is_in_the_Errors:_Leveraging_Large_Language_Models_for_Fine-grained_Machine_Translation_Evaluation_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the text?,answer: The main focus of the text is on leveraging large language models for fine-grained machine translation evaluation.", "ref_chunk": "3 2 0 2 g u A 4 1 ] L C . s c [ 1 v 6 8 2 7 0 . 8 0 3 2 : v i X r a The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Patrick Fernandes\u2217,2,3,4 Daniel Deutsch1 Mara Finkelstein1 Parker Riley1 Andr\u00e9 F. T. Martins3,4,5 Graham Neubig2,6 Ankush Garg1 1Google Jonathan H. Clark1 Markus Freitag1 Orhan Firat1 2Carnegie Mellon University 3Instituto Superior T\u00e9cnico 4Instituto de Telecomunica\u00e7\u00f5es 5Unbabel 6Inspired Cognition pfernand@cs.cmu.edu Abstract Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iter- ative development of MT systems. While considerable progress has been made on esti- mating a single scalar quality score, current metrics lack the informativeness of more de- tailed schemes that annotate individual er- rors, such as Multidimensional Quality Met- rics (MQM). In this paper, we help fill this gap by proposing AUTOMQM, a prompt- ing technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in transla- tions. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AUTOMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particu- larly large gains for larger models) while providing interpretability through error spans that align with human annotations. 1 Introduction Evaluating natural language generation systems has always been challenging, and as the output qual- ity of these systems has improved, evaluation has become even more challenging and critical. For ex- ample, in Machine Translation (MT), a field where evaluation has garnered considerable attention, pre- vious standard automatic surface-level metrics such as BLEU (Papineni et al., 2002) are becoming less reliable as the quality of generation systems im- proves, with little remaining correlation with hu- man judgments (Freitag et al., 2022). To keep pace with the constantly improving qual- ity of MT output, the next generation of automatic Errors: \u2018easy\u2019 - major/accuracy; \u2018are\u2019 - minor/\ufb02uency Score: 25 MQM Identify the errors in the translationPortuguese: {source}; English:{candidate} Score Prediction A\u1d1c\u1d1b\u1d0fMQM Score: -5x1(major) - 1x1(minor) = -6 Candidate: \u201cEvaluating automatic translation are easy.\u201d Score the following translation from 0 to 100:Portuguese: {source}; English:{candidate} Source: \u201cAvaliar tradu\u00e7\u00e3o autom\u00e1tica \u00e9 dif\u00edcil.\u201d Figure 1: Illustration of how AUTOMQM uses LLMs to assess the quality of a translation. Rather than asking for a single quality score, AUTOMQM prompts models to identify and classify errors, and uses the MQM framework to produce a score. metrics is rapidly evolving. Learned automatic metrics that leverage human-judgments to finetune language models (Sellam et al., 2020; Rei et al., 2022a) currently represent the state-of-the-art in au- tomatic evaluation benchmarks like the WMT Met- rics task (Freitag et al., 2022), and show high corre- lation with human judgments. However, these met- rics typically output a single, uninterpretable qual- ity score, making it difficult to understand the type and extent of errors identified by them. The lack of insights makes it difficult for model developers to leverage these metrics to improve their systems. Unlike automatic metrics that only provide a single scalar value as quality score, state-of-the-art human evaluation methodologies like Multidi- mensional Quality Metrics (MQM; Lommel et al., 2014; Freitag et al., 2021a) ask professional annotators to identify and label error spans with a category and severity. This much richer feedback can be used to gain a better understanding of the current limitations of the model under evaluation \u2217 Work done while working part-time at Google. and improve it. In this paper, we ask whether large language models (LLMs) in combination with a few human annotations can be used to design an automatic metric that generates rich feedback similar to that generated by human experts in MQM. This work is motivated by recent papers that demonstrated that LLMs can be used as automatic metrics (Liu et al., 2023b) to generate a single quality score. In particular, Kocmi and Federmann (2023) showed that LLMs can be prompted to assess the quality of machine-generated translations, even achieving state-of-the-art performance on assessing system- level quality. However, previous work only pro- vides a limited view of the capabilities of LLMs for machine translation evaluation: the focus has pre- dominantly been on score prediction (i.e. predict- ing a numerical value for quality), without consid- ering the use of any annotated data (either through in-context learning or finetuning), and only in high- resource language pairs. We provide a large-scale study of the capabilities of LLMs (from the PaLM and PaLM-2 families; Chowdhery et al., 2022; Anil et al., 2023) for ma- chine translation evaluation (both with and without a reference translation), provide a novel compari- son between prompting and finetuning, and investi- gate the performance in the low-resource scenario. Inspired by findings that the performance of LLMs can be improved by prompting them for rationales of their predictions (Wei et al., 2022; Lu et al., 2023), we also propose AUTOMQM, a prompt- ing technique for MT evaluation that asks LLMs to identify error spans in a translation and to clas- sify these errors according to the MQM framework, with a quality score derived automatically from the identified errors. A key advantage of AUTOMQM is its interpretability, as users can inspect the errors responsible for a score (Figure 1). Our contributions can be summarized as follows: We confirm the finding of Kocmi and Feder- mann (2023) that LLMs are zero-shot state-of- the-art system-level evaluators, but show low correlation with human judgment compared to learned metrics at the segment-level. We show that finetuning an LLM with hu- man judgment mitigates its low segment-level performance (particularly for smaller LLMs), showing similar correlations with human judg- ment at both the system-level and segment- level to state-of-the-art learned metrics. We are the first to evaluate LLM-based evalu- ation methods on low-resource language pairs. We find that their performance is promising, but lags behind"}, {"question": " Why is evaluating natural language generation systems challenging?,answer: Evaluating natural language generation systems is challenging because as the output quality of these systems improves, evaluation becomes even more critical.", "ref_chunk": "3 2 0 2 g u A 4 1 ] L C . s c [ 1 v 6 8 2 7 0 . 8 0 3 2 : v i X r a The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Patrick Fernandes\u2217,2,3,4 Daniel Deutsch1 Mara Finkelstein1 Parker Riley1 Andr\u00e9 F. T. Martins3,4,5 Graham Neubig2,6 Ankush Garg1 1Google Jonathan H. Clark1 Markus Freitag1 Orhan Firat1 2Carnegie Mellon University 3Instituto Superior T\u00e9cnico 4Instituto de Telecomunica\u00e7\u00f5es 5Unbabel 6Inspired Cognition pfernand@cs.cmu.edu Abstract Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iter- ative development of MT systems. While considerable progress has been made on esti- mating a single scalar quality score, current metrics lack the informativeness of more de- tailed schemes that annotate individual er- rors, such as Multidimensional Quality Met- rics (MQM). In this paper, we help fill this gap by proposing AUTOMQM, a prompt- ing technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in transla- tions. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AUTOMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particu- larly large gains for larger models) while providing interpretability through error spans that align with human annotations. 1 Introduction Evaluating natural language generation systems has always been challenging, and as the output qual- ity of these systems has improved, evaluation has become even more challenging and critical. For ex- ample, in Machine Translation (MT), a field where evaluation has garnered considerable attention, pre- vious standard automatic surface-level metrics such as BLEU (Papineni et al., 2002) are becoming less reliable as the quality of generation systems im- proves, with little remaining correlation with hu- man judgments (Freitag et al., 2022). To keep pace with the constantly improving qual- ity of MT output, the next generation of automatic Errors: \u2018easy\u2019 - major/accuracy; \u2018are\u2019 - minor/\ufb02uency Score: 25 MQM Identify the errors in the translationPortuguese: {source}; English:{candidate} Score Prediction A\u1d1c\u1d1b\u1d0fMQM Score: -5x1(major) - 1x1(minor) = -6 Candidate: \u201cEvaluating automatic translation are easy.\u201d Score the following translation from 0 to 100:Portuguese: {source}; English:{candidate} Source: \u201cAvaliar tradu\u00e7\u00e3o autom\u00e1tica \u00e9 dif\u00edcil.\u201d Figure 1: Illustration of how AUTOMQM uses LLMs to assess the quality of a translation. Rather than asking for a single quality score, AUTOMQM prompts models to identify and classify errors, and uses the MQM framework to produce a score. metrics is rapidly evolving. Learned automatic metrics that leverage human-judgments to finetune language models (Sellam et al., 2020; Rei et al., 2022a) currently represent the state-of-the-art in au- tomatic evaluation benchmarks like the WMT Met- rics task (Freitag et al., 2022), and show high corre- lation with human judgments. However, these met- rics typically output a single, uninterpretable qual- ity score, making it difficult to understand the type and extent of errors identified by them. The lack of insights makes it difficult for model developers to leverage these metrics to improve their systems. Unlike automatic metrics that only provide a single scalar value as quality score, state-of-the-art human evaluation methodologies like Multidi- mensional Quality Metrics (MQM; Lommel et al., 2014; Freitag et al., 2021a) ask professional annotators to identify and label error spans with a category and severity. This much richer feedback can be used to gain a better understanding of the current limitations of the model under evaluation \u2217 Work done while working part-time at Google. and improve it. In this paper, we ask whether large language models (LLMs) in combination with a few human annotations can be used to design an automatic metric that generates rich feedback similar to that generated by human experts in MQM. This work is motivated by recent papers that demonstrated that LLMs can be used as automatic metrics (Liu et al., 2023b) to generate a single quality score. In particular, Kocmi and Federmann (2023) showed that LLMs can be prompted to assess the quality of machine-generated translations, even achieving state-of-the-art performance on assessing system- level quality. However, previous work only pro- vides a limited view of the capabilities of LLMs for machine translation evaluation: the focus has pre- dominantly been on score prediction (i.e. predict- ing a numerical value for quality), without consid- ering the use of any annotated data (either through in-context learning or finetuning), and only in high- resource language pairs. We provide a large-scale study of the capabilities of LLMs (from the PaLM and PaLM-2 families; Chowdhery et al., 2022; Anil et al., 2023) for ma- chine translation evaluation (both with and without a reference translation), provide a novel compari- son between prompting and finetuning, and investi- gate the performance in the low-resource scenario. Inspired by findings that the performance of LLMs can be improved by prompting them for rationales of their predictions (Wei et al., 2022; Lu et al., 2023), we also propose AUTOMQM, a prompt- ing technique for MT evaluation that asks LLMs to identify error spans in a translation and to clas- sify these errors according to the MQM framework, with a quality score derived automatically from the identified errors. A key advantage of AUTOMQM is its interpretability, as users can inspect the errors responsible for a score (Figure 1). Our contributions can be summarized as follows: We confirm the finding of Kocmi and Feder- mann (2023) that LLMs are zero-shot state-of- the-art system-level evaluators, but show low correlation with human judgment compared to learned metrics at the segment-level. We show that finetuning an LLM with hu- man judgment mitigates its low segment-level performance (particularly for smaller LLMs), showing similar correlations with human judg- ment at both the system-level and segment- level to state-of-the-art learned metrics. We are the first to evaluate LLM-based evalu- ation methods on low-resource language pairs. We find that their performance is promising, but lags behind"}, {"question": " Why are standard automatic surface-level metrics like BLEU becoming less reliable in Machine Translation?,answer: Standard automatic surface-level metrics like BLEU are becoming less reliable in Machine Translation because they have little remaining correlation with human judgments as the quality of generation systems improves.", "ref_chunk": "3 2 0 2 g u A 4 1 ] L C . s c [ 1 v 6 8 2 7 0 . 8 0 3 2 : v i X r a The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Patrick Fernandes\u2217,2,3,4 Daniel Deutsch1 Mara Finkelstein1 Parker Riley1 Andr\u00e9 F. T. Martins3,4,5 Graham Neubig2,6 Ankush Garg1 1Google Jonathan H. Clark1 Markus Freitag1 Orhan Firat1 2Carnegie Mellon University 3Instituto Superior T\u00e9cnico 4Instituto de Telecomunica\u00e7\u00f5es 5Unbabel 6Inspired Cognition pfernand@cs.cmu.edu Abstract Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iter- ative development of MT systems. While considerable progress has been made on esti- mating a single scalar quality score, current metrics lack the informativeness of more de- tailed schemes that annotate individual er- rors, such as Multidimensional Quality Met- rics (MQM). In this paper, we help fill this gap by proposing AUTOMQM, a prompt- ing technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in transla- tions. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AUTOMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particu- larly large gains for larger models) while providing interpretability through error spans that align with human annotations. 1 Introduction Evaluating natural language generation systems has always been challenging, and as the output qual- ity of these systems has improved, evaluation has become even more challenging and critical. For ex- ample, in Machine Translation (MT), a field where evaluation has garnered considerable attention, pre- vious standard automatic surface-level metrics such as BLEU (Papineni et al., 2002) are becoming less reliable as the quality of generation systems im- proves, with little remaining correlation with hu- man judgments (Freitag et al., 2022). To keep pace with the constantly improving qual- ity of MT output, the next generation of automatic Errors: \u2018easy\u2019 - major/accuracy; \u2018are\u2019 - minor/\ufb02uency Score: 25 MQM Identify the errors in the translationPortuguese: {source}; English:{candidate} Score Prediction A\u1d1c\u1d1b\u1d0fMQM Score: -5x1(major) - 1x1(minor) = -6 Candidate: \u201cEvaluating automatic translation are easy.\u201d Score the following translation from 0 to 100:Portuguese: {source}; English:{candidate} Source: \u201cAvaliar tradu\u00e7\u00e3o autom\u00e1tica \u00e9 dif\u00edcil.\u201d Figure 1: Illustration of how AUTOMQM uses LLMs to assess the quality of a translation. Rather than asking for a single quality score, AUTOMQM prompts models to identify and classify errors, and uses the MQM framework to produce a score. metrics is rapidly evolving. Learned automatic metrics that leverage human-judgments to finetune language models (Sellam et al., 2020; Rei et al., 2022a) currently represent the state-of-the-art in au- tomatic evaluation benchmarks like the WMT Met- rics task (Freitag et al., 2022), and show high corre- lation with human judgments. However, these met- rics typically output a single, uninterpretable qual- ity score, making it difficult to understand the type and extent of errors identified by them. The lack of insights makes it difficult for model developers to leverage these metrics to improve their systems. Unlike automatic metrics that only provide a single scalar value as quality score, state-of-the-art human evaluation methodologies like Multidi- mensional Quality Metrics (MQM; Lommel et al., 2014; Freitag et al., 2021a) ask professional annotators to identify and label error spans with a category and severity. This much richer feedback can be used to gain a better understanding of the current limitations of the model under evaluation \u2217 Work done while working part-time at Google. and improve it. In this paper, we ask whether large language models (LLMs) in combination with a few human annotations can be used to design an automatic metric that generates rich feedback similar to that generated by human experts in MQM. This work is motivated by recent papers that demonstrated that LLMs can be used as automatic metrics (Liu et al., 2023b) to generate a single quality score. In particular, Kocmi and Federmann (2023) showed that LLMs can be prompted to assess the quality of machine-generated translations, even achieving state-of-the-art performance on assessing system- level quality. However, previous work only pro- vides a limited view of the capabilities of LLMs for machine translation evaluation: the focus has pre- dominantly been on score prediction (i.e. predict- ing a numerical value for quality), without consid- ering the use of any annotated data (either through in-context learning or finetuning), and only in high- resource language pairs. We provide a large-scale study of the capabilities of LLMs (from the PaLM and PaLM-2 families; Chowdhery et al., 2022; Anil et al., 2023) for ma- chine translation evaluation (both with and without a reference translation), provide a novel compari- son between prompting and finetuning, and investi- gate the performance in the low-resource scenario. Inspired by findings that the performance of LLMs can be improved by prompting them for rationales of their predictions (Wei et al., 2022; Lu et al., 2023), we also propose AUTOMQM, a prompt- ing technique for MT evaluation that asks LLMs to identify error spans in a translation and to clas- sify these errors according to the MQM framework, with a quality score derived automatically from the identified errors. A key advantage of AUTOMQM is its interpretability, as users can inspect the errors responsible for a score (Figure 1). Our contributions can be summarized as follows: We confirm the finding of Kocmi and Feder- mann (2023) that LLMs are zero-shot state-of- the-art system-level evaluators, but show low correlation with human judgment compared to learned metrics at the segment-level. We show that finetuning an LLM with hu- man judgment mitigates its low segment-level performance (particularly for smaller LLMs), showing similar correlations with human judg- ment at both the system-level and segment- level to state-of-the-art learned metrics. We are the first to evaluate LLM-based evalu- ation methods on low-resource language pairs. We find that their performance is promising, but lags behind"}, {"question": " What is AUTOMQM and how does it function?,answer: AUTOMQM is a prompting technique that leverages the reasoning and in-context learning capabilities of large language models (LLMs) to identify and categorize errors in translations. It asks LLMs to identify error spans and classify errors according to the MQM framework, deriving a quality score automatically from the identified errors.", "ref_chunk": "3 2 0 2 g u A 4 1 ] L C . s c [ 1 v 6 8 2 7 0 . 8 0 3 2 : v i X r a The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Patrick Fernandes\u2217,2,3,4 Daniel Deutsch1 Mara Finkelstein1 Parker Riley1 Andr\u00e9 F. T. Martins3,4,5 Graham Neubig2,6 Ankush Garg1 1Google Jonathan H. Clark1 Markus Freitag1 Orhan Firat1 2Carnegie Mellon University 3Instituto Superior T\u00e9cnico 4Instituto de Telecomunica\u00e7\u00f5es 5Unbabel 6Inspired Cognition pfernand@cs.cmu.edu Abstract Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iter- ative development of MT systems. While considerable progress has been made on esti- mating a single scalar quality score, current metrics lack the informativeness of more de- tailed schemes that annotate individual er- rors, such as Multidimensional Quality Met- rics (MQM). In this paper, we help fill this gap by proposing AUTOMQM, a prompt- ing technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in transla- tions. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AUTOMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particu- larly large gains for larger models) while providing interpretability through error spans that align with human annotations. 1 Introduction Evaluating natural language generation systems has always been challenging, and as the output qual- ity of these systems has improved, evaluation has become even more challenging and critical. For ex- ample, in Machine Translation (MT), a field where evaluation has garnered considerable attention, pre- vious standard automatic surface-level metrics such as BLEU (Papineni et al., 2002) are becoming less reliable as the quality of generation systems im- proves, with little remaining correlation with hu- man judgments (Freitag et al., 2022). To keep pace with the constantly improving qual- ity of MT output, the next generation of automatic Errors: \u2018easy\u2019 - major/accuracy; \u2018are\u2019 - minor/\ufb02uency Score: 25 MQM Identify the errors in the translationPortuguese: {source}; English:{candidate} Score Prediction A\u1d1c\u1d1b\u1d0fMQM Score: -5x1(major) - 1x1(minor) = -6 Candidate: \u201cEvaluating automatic translation are easy.\u201d Score the following translation from 0 to 100:Portuguese: {source}; English:{candidate} Source: \u201cAvaliar tradu\u00e7\u00e3o autom\u00e1tica \u00e9 dif\u00edcil.\u201d Figure 1: Illustration of how AUTOMQM uses LLMs to assess the quality of a translation. Rather than asking for a single quality score, AUTOMQM prompts models to identify and classify errors, and uses the MQM framework to produce a score. metrics is rapidly evolving. Learned automatic metrics that leverage human-judgments to finetune language models (Sellam et al., 2020; Rei et al., 2022a) currently represent the state-of-the-art in au- tomatic evaluation benchmarks like the WMT Met- rics task (Freitag et al., 2022), and show high corre- lation with human judgments. However, these met- rics typically output a single, uninterpretable qual- ity score, making it difficult to understand the type and extent of errors identified by them. The lack of insights makes it difficult for model developers to leverage these metrics to improve their systems. Unlike automatic metrics that only provide a single scalar value as quality score, state-of-the-art human evaluation methodologies like Multidi- mensional Quality Metrics (MQM; Lommel et al., 2014; Freitag et al., 2021a) ask professional annotators to identify and label error spans with a category and severity. This much richer feedback can be used to gain a better understanding of the current limitations of the model under evaluation \u2217 Work done while working part-time at Google. and improve it. In this paper, we ask whether large language models (LLMs) in combination with a few human annotations can be used to design an automatic metric that generates rich feedback similar to that generated by human experts in MQM. This work is motivated by recent papers that demonstrated that LLMs can be used as automatic metrics (Liu et al., 2023b) to generate a single quality score. In particular, Kocmi and Federmann (2023) showed that LLMs can be prompted to assess the quality of machine-generated translations, even achieving state-of-the-art performance on assessing system- level quality. However, previous work only pro- vides a limited view of the capabilities of LLMs for machine translation evaluation: the focus has pre- dominantly been on score prediction (i.e. predict- ing a numerical value for quality), without consid- ering the use of any annotated data (either through in-context learning or finetuning), and only in high- resource language pairs. We provide a large-scale study of the capabilities of LLMs (from the PaLM and PaLM-2 families; Chowdhery et al., 2022; Anil et al., 2023) for ma- chine translation evaluation (both with and without a reference translation), provide a novel compari- son between prompting and finetuning, and investi- gate the performance in the low-resource scenario. Inspired by findings that the performance of LLMs can be improved by prompting them for rationales of their predictions (Wei et al., 2022; Lu et al., 2023), we also propose AUTOMQM, a prompt- ing technique for MT evaluation that asks LLMs to identify error spans in a translation and to clas- sify these errors according to the MQM framework, with a quality score derived automatically from the identified errors. A key advantage of AUTOMQM is its interpretability, as users can inspect the errors responsible for a score (Figure 1). Our contributions can be summarized as follows: We confirm the finding of Kocmi and Feder- mann (2023) that LLMs are zero-shot state-of- the-art system-level evaluators, but show low correlation with human judgment compared to learned metrics at the segment-level. We show that finetuning an LLM with hu- man judgment mitigates its low segment-level performance (particularly for smaller LLMs), showing similar correlations with human judg- ment at both the system-level and segment- level to state-of-the-art learned metrics. We are the first to evaluate LLM-based evalu- ation methods on low-resource language pairs. We find that their performance is promising, but lags behind"}, {"question": " What is the primary advantage of AUTOMQM over traditional automatic metrics for machine translation evaluation?,answer: The primary advantage of AUTOMQM is its interpretability, as users can inspect the errors responsible for a score, providing more detailed feedback than just a single scalar quality score.", "ref_chunk": "3 2 0 2 g u A 4 1 ] L C . s c [ 1 v 6 8 2 7 0 . 8 0 3 2 : v i X r a The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Patrick Fernandes\u2217,2,3,4 Daniel Deutsch1 Mara Finkelstein1 Parker Riley1 Andr\u00e9 F. T. Martins3,4,5 Graham Neubig2,6 Ankush Garg1 1Google Jonathan H. Clark1 Markus Freitag1 Orhan Firat1 2Carnegie Mellon University 3Instituto Superior T\u00e9cnico 4Instituto de Telecomunica\u00e7\u00f5es 5Unbabel 6Inspired Cognition pfernand@cs.cmu.edu Abstract Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iter- ative development of MT systems. While considerable progress has been made on esti- mating a single scalar quality score, current metrics lack the informativeness of more de- tailed schemes that annotate individual er- rors, such as Multidimensional Quality Met- rics (MQM). In this paper, we help fill this gap by proposing AUTOMQM, a prompt- ing technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in transla- tions. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AUTOMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particu- larly large gains for larger models) while providing interpretability through error spans that align with human annotations. 1 Introduction Evaluating natural language generation systems has always been challenging, and as the output qual- ity of these systems has improved, evaluation has become even more challenging and critical. For ex- ample, in Machine Translation (MT), a field where evaluation has garnered considerable attention, pre- vious standard automatic surface-level metrics such as BLEU (Papineni et al., 2002) are becoming less reliable as the quality of generation systems im- proves, with little remaining correlation with hu- man judgments (Freitag et al., 2022). To keep pace with the constantly improving qual- ity of MT output, the next generation of automatic Errors: \u2018easy\u2019 - major/accuracy; \u2018are\u2019 - minor/\ufb02uency Score: 25 MQM Identify the errors in the translationPortuguese: {source}; English:{candidate} Score Prediction A\u1d1c\u1d1b\u1d0fMQM Score: -5x1(major) - 1x1(minor) = -6 Candidate: \u201cEvaluating automatic translation are easy.\u201d Score the following translation from 0 to 100:Portuguese: {source}; English:{candidate} Source: \u201cAvaliar tradu\u00e7\u00e3o autom\u00e1tica \u00e9 dif\u00edcil.\u201d Figure 1: Illustration of how AUTOMQM uses LLMs to assess the quality of a translation. Rather than asking for a single quality score, AUTOMQM prompts models to identify and classify errors, and uses the MQM framework to produce a score. metrics is rapidly evolving. Learned automatic metrics that leverage human-judgments to finetune language models (Sellam et al., 2020; Rei et al., 2022a) currently represent the state-of-the-art in au- tomatic evaluation benchmarks like the WMT Met- rics task (Freitag et al., 2022), and show high corre- lation with human judgments. However, these met- rics typically output a single, uninterpretable qual- ity score, making it difficult to understand the type and extent of errors identified by them. The lack of insights makes it difficult for model developers to leverage these metrics to improve their systems. Unlike automatic metrics that only provide a single scalar value as quality score, state-of-the-art human evaluation methodologies like Multidi- mensional Quality Metrics (MQM; Lommel et al., 2014; Freitag et al., 2021a) ask professional annotators to identify and label error spans with a category and severity. This much richer feedback can be used to gain a better understanding of the current limitations of the model under evaluation \u2217 Work done while working part-time at Google. and improve it. In this paper, we ask whether large language models (LLMs) in combination with a few human annotations can be used to design an automatic metric that generates rich feedback similar to that generated by human experts in MQM. This work is motivated by recent papers that demonstrated that LLMs can be used as automatic metrics (Liu et al., 2023b) to generate a single quality score. In particular, Kocmi and Federmann (2023) showed that LLMs can be prompted to assess the quality of machine-generated translations, even achieving state-of-the-art performance on assessing system- level quality. However, previous work only pro- vides a limited view of the capabilities of LLMs for machine translation evaluation: the focus has pre- dominantly been on score prediction (i.e. predict- ing a numerical value for quality), without consid- ering the use of any annotated data (either through in-context learning or finetuning), and only in high- resource language pairs. We provide a large-scale study of the capabilities of LLMs (from the PaLM and PaLM-2 families; Chowdhery et al., 2022; Anil et al., 2023) for ma- chine translation evaluation (both with and without a reference translation), provide a novel compari- son between prompting and finetuning, and investi- gate the performance in the low-resource scenario. Inspired by findings that the performance of LLMs can be improved by prompting them for rationales of their predictions (Wei et al., 2022; Lu et al., 2023), we also propose AUTOMQM, a prompt- ing technique for MT evaluation that asks LLMs to identify error spans in a translation and to clas- sify these errors according to the MQM framework, with a quality score derived automatically from the identified errors. A key advantage of AUTOMQM is its interpretability, as users can inspect the errors responsible for a score (Figure 1). Our contributions can be summarized as follows: We confirm the finding of Kocmi and Feder- mann (2023) that LLMs are zero-shot state-of- the-art system-level evaluators, but show low correlation with human judgment compared to learned metrics at the segment-level. We show that finetuning an LLM with hu- man judgment mitigates its low segment-level performance (particularly for smaller LLMs), showing similar correlations with human judg- ment at both the system-level and segment- level to state-of-the-art learned metrics. We are the first to evaluate LLM-based evalu- ation methods on low-resource language pairs. We find that their performance is promising, but lags behind"}, {"question": " What is the significance of MQM in evaluating natural language generation systems?,answer: MQM (Multidimensional Quality Metrics) is significant in evaluating natural language generation systems as it asks professional annotators to identify and label error spans with a category and severity, providing rich feedback for understanding and improving systems.", "ref_chunk": "3 2 0 2 g u A 4 1 ] L C . s c [ 1 v 6 8 2 7 0 . 8 0 3 2 : v i X r a The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Patrick Fernandes\u2217,2,3,4 Daniel Deutsch1 Mara Finkelstein1 Parker Riley1 Andr\u00e9 F. T. Martins3,4,5 Graham Neubig2,6 Ankush Garg1 1Google Jonathan H. Clark1 Markus Freitag1 Orhan Firat1 2Carnegie Mellon University 3Instituto Superior T\u00e9cnico 4Instituto de Telecomunica\u00e7\u00f5es 5Unbabel 6Inspired Cognition pfernand@cs.cmu.edu Abstract Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iter- ative development of MT systems. While considerable progress has been made on esti- mating a single scalar quality score, current metrics lack the informativeness of more de- tailed schemes that annotate individual er- rors, such as Multidimensional Quality Met- rics (MQM). In this paper, we help fill this gap by proposing AUTOMQM, a prompt- ing technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in transla- tions. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AUTOMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particu- larly large gains for larger models) while providing interpretability through error spans that align with human annotations. 1 Introduction Evaluating natural language generation systems has always been challenging, and as the output qual- ity of these systems has improved, evaluation has become even more challenging and critical. For ex- ample, in Machine Translation (MT), a field where evaluation has garnered considerable attention, pre- vious standard automatic surface-level metrics such as BLEU (Papineni et al., 2002) are becoming less reliable as the quality of generation systems im- proves, with little remaining correlation with hu- man judgments (Freitag et al., 2022). To keep pace with the constantly improving qual- ity of MT output, the next generation of automatic Errors: \u2018easy\u2019 - major/accuracy; \u2018are\u2019 - minor/\ufb02uency Score: 25 MQM Identify the errors in the translationPortuguese: {source}; English:{candidate} Score Prediction A\u1d1c\u1d1b\u1d0fMQM Score: -5x1(major) - 1x1(minor) = -6 Candidate: \u201cEvaluating automatic translation are easy.\u201d Score the following translation from 0 to 100:Portuguese: {source}; English:{candidate} Source: \u201cAvaliar tradu\u00e7\u00e3o autom\u00e1tica \u00e9 dif\u00edcil.\u201d Figure 1: Illustration of how AUTOMQM uses LLMs to assess the quality of a translation. Rather than asking for a single quality score, AUTOMQM prompts models to identify and classify errors, and uses the MQM framework to produce a score. metrics is rapidly evolving. Learned automatic metrics that leverage human-judgments to finetune language models (Sellam et al., 2020; Rei et al., 2022a) currently represent the state-of-the-art in au- tomatic evaluation benchmarks like the WMT Met- rics task (Freitag et al., 2022), and show high corre- lation with human judgments. However, these met- rics typically output a single, uninterpretable qual- ity score, making it difficult to understand the type and extent of errors identified by them. The lack of insights makes it difficult for model developers to leverage these metrics to improve their systems. Unlike automatic metrics that only provide a single scalar value as quality score, state-of-the-art human evaluation methodologies like Multidi- mensional Quality Metrics (MQM; Lommel et al., 2014; Freitag et al., 2021a) ask professional annotators to identify and label error spans with a category and severity. This much richer feedback can be used to gain a better understanding of the current limitations of the model under evaluation \u2217 Work done while working part-time at Google. and improve it. In this paper, we ask whether large language models (LLMs) in combination with a few human annotations can be used to design an automatic metric that generates rich feedback similar to that generated by human experts in MQM. This work is motivated by recent papers that demonstrated that LLMs can be used as automatic metrics (Liu et al., 2023b) to generate a single quality score. In particular, Kocmi and Federmann (2023) showed that LLMs can be prompted to assess the quality of machine-generated translations, even achieving state-of-the-art performance on assessing system- level quality. However, previous work only pro- vides a limited view of the capabilities of LLMs for machine translation evaluation: the focus has pre- dominantly been on score prediction (i.e. predict- ing a numerical value for quality), without consid- ering the use of any annotated data (either through in-context learning or finetuning), and only in high- resource language pairs. We provide a large-scale study of the capabilities of LLMs (from the PaLM and PaLM-2 families; Chowdhery et al., 2022; Anil et al., 2023) for ma- chine translation evaluation (both with and without a reference translation), provide a novel compari- son between prompting and finetuning, and investi- gate the performance in the low-resource scenario. Inspired by findings that the performance of LLMs can be improved by prompting them for rationales of their predictions (Wei et al., 2022; Lu et al., 2023), we also propose AUTOMQM, a prompt- ing technique for MT evaluation that asks LLMs to identify error spans in a translation and to clas- sify these errors according to the MQM framework, with a quality score derived automatically from the identified errors. A key advantage of AUTOMQM is its interpretability, as users can inspect the errors responsible for a score (Figure 1). Our contributions can be summarized as follows: We confirm the finding of Kocmi and Feder- mann (2023) that LLMs are zero-shot state-of- the-art system-level evaluators, but show low correlation with human judgment compared to learned metrics at the segment-level. We show that finetuning an LLM with hu- man judgment mitigates its low segment-level performance (particularly for smaller LLMs), showing similar correlations with human judg- ment at both the system-level and segment- level to state-of-the-art learned metrics. We are the first to evaluate LLM-based evalu- ation methods on low-resource language pairs. We find that their performance is promising, but lags behind"}, {"question": " How does AUTOMQM aim to improve the evaluation of machine translation systems?,answer: AUTOMQM aims to improve the evaluation of machine translation systems by leveraging LLMs to provide detailed error identification and classification, leading to a more informative evaluation than traditional automatic metrics.", "ref_chunk": "3 2 0 2 g u A 4 1 ] L C . s c [ 1 v 6 8 2 7 0 . 8 0 3 2 : v i X r a The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Patrick Fernandes\u2217,2,3,4 Daniel Deutsch1 Mara Finkelstein1 Parker Riley1 Andr\u00e9 F. T. Martins3,4,5 Graham Neubig2,6 Ankush Garg1 1Google Jonathan H. Clark1 Markus Freitag1 Orhan Firat1 2Carnegie Mellon University 3Instituto Superior T\u00e9cnico 4Instituto de Telecomunica\u00e7\u00f5es 5Unbabel 6Inspired Cognition pfernand@cs.cmu.edu Abstract Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iter- ative development of MT systems. While considerable progress has been made on esti- mating a single scalar quality score, current metrics lack the informativeness of more de- tailed schemes that annotate individual er- rors, such as Multidimensional Quality Met- rics (MQM). In this paper, we help fill this gap by proposing AUTOMQM, a prompt- ing technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in transla- tions. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AUTOMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particu- larly large gains for larger models) while providing interpretability through error spans that align with human annotations. 1 Introduction Evaluating natural language generation systems has always been challenging, and as the output qual- ity of these systems has improved, evaluation has become even more challenging and critical. For ex- ample, in Machine Translation (MT), a field where evaluation has garnered considerable attention, pre- vious standard automatic surface-level metrics such as BLEU (Papineni et al., 2002) are becoming less reliable as the quality of generation systems im- proves, with little remaining correlation with hu- man judgments (Freitag et al., 2022). To keep pace with the constantly improving qual- ity of MT output, the next generation of automatic Errors: \u2018easy\u2019 - major/accuracy; \u2018are\u2019 - minor/\ufb02uency Score: 25 MQM Identify the errors in the translationPortuguese: {source}; English:{candidate} Score Prediction A\u1d1c\u1d1b\u1d0fMQM Score: -5x1(major) - 1x1(minor) = -6 Candidate: \u201cEvaluating automatic translation are easy.\u201d Score the following translation from 0 to 100:Portuguese: {source}; English:{candidate} Source: \u201cAvaliar tradu\u00e7\u00e3o autom\u00e1tica \u00e9 dif\u00edcil.\u201d Figure 1: Illustration of how AUTOMQM uses LLMs to assess the quality of a translation. Rather than asking for a single quality score, AUTOMQM prompts models to identify and classify errors, and uses the MQM framework to produce a score. metrics is rapidly evolving. Learned automatic metrics that leverage human-judgments to finetune language models (Sellam et al., 2020; Rei et al., 2022a) currently represent the state-of-the-art in au- tomatic evaluation benchmarks like the WMT Met- rics task (Freitag et al., 2022), and show high corre- lation with human judgments. However, these met- rics typically output a single, uninterpretable qual- ity score, making it difficult to understand the type and extent of errors identified by them. The lack of insights makes it difficult for model developers to leverage these metrics to improve their systems. Unlike automatic metrics that only provide a single scalar value as quality score, state-of-the-art human evaluation methodologies like Multidi- mensional Quality Metrics (MQM; Lommel et al., 2014; Freitag et al., 2021a) ask professional annotators to identify and label error spans with a category and severity. This much richer feedback can be used to gain a better understanding of the current limitations of the model under evaluation \u2217 Work done while working part-time at Google. and improve it. In this paper, we ask whether large language models (LLMs) in combination with a few human annotations can be used to design an automatic metric that generates rich feedback similar to that generated by human experts in MQM. This work is motivated by recent papers that demonstrated that LLMs can be used as automatic metrics (Liu et al., 2023b) to generate a single quality score. In particular, Kocmi and Federmann (2023) showed that LLMs can be prompted to assess the quality of machine-generated translations, even achieving state-of-the-art performance on assessing system- level quality. However, previous work only pro- vides a limited view of the capabilities of LLMs for machine translation evaluation: the focus has pre- dominantly been on score prediction (i.e. predict- ing a numerical value for quality), without consid- ering the use of any annotated data (either through in-context learning or finetuning), and only in high- resource language pairs. We provide a large-scale study of the capabilities of LLMs (from the PaLM and PaLM-2 families; Chowdhery et al., 2022; Anil et al., 2023) for ma- chine translation evaluation (both with and without a reference translation), provide a novel compari- son between prompting and finetuning, and investi- gate the performance in the low-resource scenario. Inspired by findings that the performance of LLMs can be improved by prompting them for rationales of their predictions (Wei et al., 2022; Lu et al., 2023), we also propose AUTOMQM, a prompt- ing technique for MT evaluation that asks LLMs to identify error spans in a translation and to clas- sify these errors according to the MQM framework, with a quality score derived automatically from the identified errors. A key advantage of AUTOMQM is its interpretability, as users can inspect the errors responsible for a score (Figure 1). Our contributions can be summarized as follows: We confirm the finding of Kocmi and Feder- mann (2023) that LLMs are zero-shot state-of- the-art system-level evaluators, but show low correlation with human judgment compared to learned metrics at the segment-level. We show that finetuning an LLM with hu- man judgment mitigates its low segment-level performance (particularly for smaller LLMs), showing similar correlations with human judg- ment at both the system-level and segment- level to state-of-the-art learned metrics. We are the first to evaluate LLM-based evalu- ation methods on low-resource language pairs. We find that their performance is promising, but lags behind"}, {"question": " What is the impact of finetuning LLMs with human judgment in machine translation evaluation?,answer: Finetuning LLMs with human judgment helps mitigate their low segment-level performance, particularly for smaller LLMs, and shows similar correlations with human judgment at both the system-level and segment-level as state-of-the-art learned metrics.", "ref_chunk": "3 2 0 2 g u A 4 1 ] L C . s c [ 1 v 6 8 2 7 0 . 8 0 3 2 : v i X r a The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Patrick Fernandes\u2217,2,3,4 Daniel Deutsch1 Mara Finkelstein1 Parker Riley1 Andr\u00e9 F. T. Martins3,4,5 Graham Neubig2,6 Ankush Garg1 1Google Jonathan H. Clark1 Markus Freitag1 Orhan Firat1 2Carnegie Mellon University 3Instituto Superior T\u00e9cnico 4Instituto de Telecomunica\u00e7\u00f5es 5Unbabel 6Inspired Cognition pfernand@cs.cmu.edu Abstract Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iter- ative development of MT systems. While considerable progress has been made on esti- mating a single scalar quality score, current metrics lack the informativeness of more de- tailed schemes that annotate individual er- rors, such as Multidimensional Quality Met- rics (MQM). In this paper, we help fill this gap by proposing AUTOMQM, a prompt- ing technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in transla- tions. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AUTOMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particu- larly large gains for larger models) while providing interpretability through error spans that align with human annotations. 1 Introduction Evaluating natural language generation systems has always been challenging, and as the output qual- ity of these systems has improved, evaluation has become even more challenging and critical. For ex- ample, in Machine Translation (MT), a field where evaluation has garnered considerable attention, pre- vious standard automatic surface-level metrics such as BLEU (Papineni et al., 2002) are becoming less reliable as the quality of generation systems im- proves, with little remaining correlation with hu- man judgments (Freitag et al., 2022). To keep pace with the constantly improving qual- ity of MT output, the next generation of automatic Errors: \u2018easy\u2019 - major/accuracy; \u2018are\u2019 - minor/\ufb02uency Score: 25 MQM Identify the errors in the translationPortuguese: {source}; English:{candidate} Score Prediction A\u1d1c\u1d1b\u1d0fMQM Score: -5x1(major) - 1x1(minor) = -6 Candidate: \u201cEvaluating automatic translation are easy.\u201d Score the following translation from 0 to 100:Portuguese: {source}; English:{candidate} Source: \u201cAvaliar tradu\u00e7\u00e3o autom\u00e1tica \u00e9 dif\u00edcil.\u201d Figure 1: Illustration of how AUTOMQM uses LLMs to assess the quality of a translation. Rather than asking for a single quality score, AUTOMQM prompts models to identify and classify errors, and uses the MQM framework to produce a score. metrics is rapidly evolving. Learned automatic metrics that leverage human-judgments to finetune language models (Sellam et al., 2020; Rei et al., 2022a) currently represent the state-of-the-art in au- tomatic evaluation benchmarks like the WMT Met- rics task (Freitag et al., 2022), and show high corre- lation with human judgments. However, these met- rics typically output a single, uninterpretable qual- ity score, making it difficult to understand the type and extent of errors identified by them. The lack of insights makes it difficult for model developers to leverage these metrics to improve their systems. Unlike automatic metrics that only provide a single scalar value as quality score, state-of-the-art human evaluation methodologies like Multidi- mensional Quality Metrics (MQM; Lommel et al., 2014; Freitag et al., 2021a) ask professional annotators to identify and label error spans with a category and severity. This much richer feedback can be used to gain a better understanding of the current limitations of the model under evaluation \u2217 Work done while working part-time at Google. and improve it. In this paper, we ask whether large language models (LLMs) in combination with a few human annotations can be used to design an automatic metric that generates rich feedback similar to that generated by human experts in MQM. This work is motivated by recent papers that demonstrated that LLMs can be used as automatic metrics (Liu et al., 2023b) to generate a single quality score. In particular, Kocmi and Federmann (2023) showed that LLMs can be prompted to assess the quality of machine-generated translations, even achieving state-of-the-art performance on assessing system- level quality. However, previous work only pro- vides a limited view of the capabilities of LLMs for machine translation evaluation: the focus has pre- dominantly been on score prediction (i.e. predict- ing a numerical value for quality), without consid- ering the use of any annotated data (either through in-context learning or finetuning), and only in high- resource language pairs. We provide a large-scale study of the capabilities of LLMs (from the PaLM and PaLM-2 families; Chowdhery et al., 2022; Anil et al., 2023) for ma- chine translation evaluation (both with and without a reference translation), provide a novel compari- son between prompting and finetuning, and investi- gate the performance in the low-resource scenario. Inspired by findings that the performance of LLMs can be improved by prompting them for rationales of their predictions (Wei et al., 2022; Lu et al., 2023), we also propose AUTOMQM, a prompt- ing technique for MT evaluation that asks LLMs to identify error spans in a translation and to clas- sify these errors according to the MQM framework, with a quality score derived automatically from the identified errors. A key advantage of AUTOMQM is its interpretability, as users can inspect the errors responsible for a score (Figure 1). Our contributions can be summarized as follows: We confirm the finding of Kocmi and Feder- mann (2023) that LLMs are zero-shot state-of- the-art system-level evaluators, but show low correlation with human judgment compared to learned metrics at the segment-level. We show that finetuning an LLM with hu- man judgment mitigates its low segment-level performance (particularly for smaller LLMs), showing similar correlations with human judg- ment at both the system-level and segment- level to state-of-the-art learned metrics. We are the first to evaluate LLM-based evalu- ation methods on low-resource language pairs. We find that their performance is promising, but lags behind"}, {"question": " What did recent papers demonstrate regarding the use of LLMs for automatic evaluation of machine-generated translations?,answer: Recent papers demonstrated that LLMs can be prompted to assess the quality of machine-generated translations, achieving state-of-the-art performance on system-level quality assessment.", "ref_chunk": "3 2 0 2 g u A 4 1 ] L C . s c [ 1 v 6 8 2 7 0 . 8 0 3 2 : v i X r a The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Patrick Fernandes\u2217,2,3,4 Daniel Deutsch1 Mara Finkelstein1 Parker Riley1 Andr\u00e9 F. T. Martins3,4,5 Graham Neubig2,6 Ankush Garg1 1Google Jonathan H. Clark1 Markus Freitag1 Orhan Firat1 2Carnegie Mellon University 3Instituto Superior T\u00e9cnico 4Instituto de Telecomunica\u00e7\u00f5es 5Unbabel 6Inspired Cognition pfernand@cs.cmu.edu Abstract Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iter- ative development of MT systems. While considerable progress has been made on esti- mating a single scalar quality score, current metrics lack the informativeness of more de- tailed schemes that annotate individual er- rors, such as Multidimensional Quality Met- rics (MQM). In this paper, we help fill this gap by proposing AUTOMQM, a prompt- ing technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in transla- tions. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AUTOMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particu- larly large gains for larger models) while providing interpretability through error spans that align with human annotations. 1 Introduction Evaluating natural language generation systems has always been challenging, and as the output qual- ity of these systems has improved, evaluation has become even more challenging and critical. For ex- ample, in Machine Translation (MT), a field where evaluation has garnered considerable attention, pre- vious standard automatic surface-level metrics such as BLEU (Papineni et al., 2002) are becoming less reliable as the quality of generation systems im- proves, with little remaining correlation with hu- man judgments (Freitag et al., 2022). To keep pace with the constantly improving qual- ity of MT output, the next generation of automatic Errors: \u2018easy\u2019 - major/accuracy; \u2018are\u2019 - minor/\ufb02uency Score: 25 MQM Identify the errors in the translationPortuguese: {source}; English:{candidate} Score Prediction A\u1d1c\u1d1b\u1d0fMQM Score: -5x1(major) - 1x1(minor) = -6 Candidate: \u201cEvaluating automatic translation are easy.\u201d Score the following translation from 0 to 100:Portuguese: {source}; English:{candidate} Source: \u201cAvaliar tradu\u00e7\u00e3o autom\u00e1tica \u00e9 dif\u00edcil.\u201d Figure 1: Illustration of how AUTOMQM uses LLMs to assess the quality of a translation. Rather than asking for a single quality score, AUTOMQM prompts models to identify and classify errors, and uses the MQM framework to produce a score. metrics is rapidly evolving. Learned automatic metrics that leverage human-judgments to finetune language models (Sellam et al., 2020; Rei et al., 2022a) currently represent the state-of-the-art in au- tomatic evaluation benchmarks like the WMT Met- rics task (Freitag et al., 2022), and show high corre- lation with human judgments. However, these met- rics typically output a single, uninterpretable qual- ity score, making it difficult to understand the type and extent of errors identified by them. The lack of insights makes it difficult for model developers to leverage these metrics to improve their systems. Unlike automatic metrics that only provide a single scalar value as quality score, state-of-the-art human evaluation methodologies like Multidi- mensional Quality Metrics (MQM; Lommel et al., 2014; Freitag et al., 2021a) ask professional annotators to identify and label error spans with a category and severity. This much richer feedback can be used to gain a better understanding of the current limitations of the model under evaluation \u2217 Work done while working part-time at Google. and improve it. In this paper, we ask whether large language models (LLMs) in combination with a few human annotations can be used to design an automatic metric that generates rich feedback similar to that generated by human experts in MQM. This work is motivated by recent papers that demonstrated that LLMs can be used as automatic metrics (Liu et al., 2023b) to generate a single quality score. In particular, Kocmi and Federmann (2023) showed that LLMs can be prompted to assess the quality of machine-generated translations, even achieving state-of-the-art performance on assessing system- level quality. However, previous work only pro- vides a limited view of the capabilities of LLMs for machine translation evaluation: the focus has pre- dominantly been on score prediction (i.e. predict- ing a numerical value for quality), without consid- ering the use of any annotated data (either through in-context learning or finetuning), and only in high- resource language pairs. We provide a large-scale study of the capabilities of LLMs (from the PaLM and PaLM-2 families; Chowdhery et al., 2022; Anil et al., 2023) for ma- chine translation evaluation (both with and without a reference translation), provide a novel compari- son between prompting and finetuning, and investi- gate the performance in the low-resource scenario. Inspired by findings that the performance of LLMs can be improved by prompting them for rationales of their predictions (Wei et al., 2022; Lu et al., 2023), we also propose AUTOMQM, a prompt- ing technique for MT evaluation that asks LLMs to identify error spans in a translation and to clas- sify these errors according to the MQM framework, with a quality score derived automatically from the identified errors. A key advantage of AUTOMQM is its interpretability, as users can inspect the errors responsible for a score (Figure 1). Our contributions can be summarized as follows: We confirm the finding of Kocmi and Feder- mann (2023) that LLMs are zero-shot state-of- the-art system-level evaluators, but show low correlation with human judgment compared to learned metrics at the segment-level. We show that finetuning an LLM with hu- man judgment mitigates its low segment-level performance (particularly for smaller LLMs), showing similar correlations with human judg- ment at both the system-level and segment- level to state-of-the-art learned metrics. We are the first to evaluate LLM-based evalu- ation methods on low-resource language pairs. We find that their performance is promising, but lags behind"}, {"question": " What is a key advantage of using LLMs for machine translation evaluation according to this text?,answer: A key advantage of using LLMs for machine translation evaluation is their ability to generate rich feedback similar to that generated by human experts in MQM, providing a better understanding of model limitations and areas for improvement.", "ref_chunk": "3 2 0 2 g u A 4 1 ] L C . s c [ 1 v 6 8 2 7 0 . 8 0 3 2 : v i X r a The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Patrick Fernandes\u2217,2,3,4 Daniel Deutsch1 Mara Finkelstein1 Parker Riley1 Andr\u00e9 F. T. Martins3,4,5 Graham Neubig2,6 Ankush Garg1 1Google Jonathan H. Clark1 Markus Freitag1 Orhan Firat1 2Carnegie Mellon University 3Instituto Superior T\u00e9cnico 4Instituto de Telecomunica\u00e7\u00f5es 5Unbabel 6Inspired Cognition pfernand@cs.cmu.edu Abstract Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iter- ative development of MT systems. While considerable progress has been made on esti- mating a single scalar quality score, current metrics lack the informativeness of more de- tailed schemes that annotate individual er- rors, such as Multidimensional Quality Met- rics (MQM). In this paper, we help fill this gap by proposing AUTOMQM, a prompt- ing technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in transla- tions. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AUTOMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particu- larly large gains for larger models) while providing interpretability through error spans that align with human annotations. 1 Introduction Evaluating natural language generation systems has always been challenging, and as the output qual- ity of these systems has improved, evaluation has become even more challenging and critical. For ex- ample, in Machine Translation (MT), a field where evaluation has garnered considerable attention, pre- vious standard automatic surface-level metrics such as BLEU (Papineni et al., 2002) are becoming less reliable as the quality of generation systems im- proves, with little remaining correlation with hu- man judgments (Freitag et al., 2022). To keep pace with the constantly improving qual- ity of MT output, the next generation of automatic Errors: \u2018easy\u2019 - major/accuracy; \u2018are\u2019 - minor/\ufb02uency Score: 25 MQM Identify the errors in the translationPortuguese: {source}; English:{candidate} Score Prediction A\u1d1c\u1d1b\u1d0fMQM Score: -5x1(major) - 1x1(minor) = -6 Candidate: \u201cEvaluating automatic translation are easy.\u201d Score the following translation from 0 to 100:Portuguese: {source}; English:{candidate} Source: \u201cAvaliar tradu\u00e7\u00e3o autom\u00e1tica \u00e9 dif\u00edcil.\u201d Figure 1: Illustration of how AUTOMQM uses LLMs to assess the quality of a translation. Rather than asking for a single quality score, AUTOMQM prompts models to identify and classify errors, and uses the MQM framework to produce a score. metrics is rapidly evolving. Learned automatic metrics that leverage human-judgments to finetune language models (Sellam et al., 2020; Rei et al., 2022a) currently represent the state-of-the-art in au- tomatic evaluation benchmarks like the WMT Met- rics task (Freitag et al., 2022), and show high corre- lation with human judgments. However, these met- rics typically output a single, uninterpretable qual- ity score, making it difficult to understand the type and extent of errors identified by them. The lack of insights makes it difficult for model developers to leverage these metrics to improve their systems. Unlike automatic metrics that only provide a single scalar value as quality score, state-of-the-art human evaluation methodologies like Multidi- mensional Quality Metrics (MQM; Lommel et al., 2014; Freitag et al., 2021a) ask professional annotators to identify and label error spans with a category and severity. This much richer feedback can be used to gain a better understanding of the current limitations of the model under evaluation \u2217 Work done while working part-time at Google. and improve it. In this paper, we ask whether large language models (LLMs) in combination with a few human annotations can be used to design an automatic metric that generates rich feedback similar to that generated by human experts in MQM. This work is motivated by recent papers that demonstrated that LLMs can be used as automatic metrics (Liu et al., 2023b) to generate a single quality score. In particular, Kocmi and Federmann (2023) showed that LLMs can be prompted to assess the quality of machine-generated translations, even achieving state-of-the-art performance on assessing system- level quality. However, previous work only pro- vides a limited view of the capabilities of LLMs for machine translation evaluation: the focus has pre- dominantly been on score prediction (i.e. predict- ing a numerical value for quality), without consid- ering the use of any annotated data (either through in-context learning or finetuning), and only in high- resource language pairs. We provide a large-scale study of the capabilities of LLMs (from the PaLM and PaLM-2 families; Chowdhery et al., 2022; Anil et al., 2023) for ma- chine translation evaluation (both with and without a reference translation), provide a novel compari- son between prompting and finetuning, and investi- gate the performance in the low-resource scenario. Inspired by findings that the performance of LLMs can be improved by prompting them for rationales of their predictions (Wei et al., 2022; Lu et al., 2023), we also propose AUTOMQM, a prompt- ing technique for MT evaluation that asks LLMs to identify error spans in a translation and to clas- sify these errors according to the MQM framework, with a quality score derived automatically from the identified errors. A key advantage of AUTOMQM is its interpretability, as users can inspect the errors responsible for a score (Figure 1). Our contributions can be summarized as follows: We confirm the finding of Kocmi and Feder- mann (2023) that LLMs are zero-shot state-of- the-art system-level evaluators, but show low correlation with human judgment compared to learned metrics at the segment-level. We show that finetuning an LLM with hu- man judgment mitigates its low segment-level performance (particularly for smaller LLMs), showing similar correlations with human judg- ment at both the system-level and segment- level to state-of-the-art learned metrics. We are the first to evaluate LLM-based evalu- ation methods on low-resource language pairs. We find that their performance is promising, but lags behind"}], "doc_text": "3 2 0 2 g u A 4 1 ] L C . s c [ 1 v 6 8 2 7 0 . 8 0 3 2 : v i X r a The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Patrick Fernandes\u2217,2,3,4 Daniel Deutsch1 Mara Finkelstein1 Parker Riley1 Andr\u00e9 F. T. Martins3,4,5 Graham Neubig2,6 Ankush Garg1 1Google Jonathan H. Clark1 Markus Freitag1 Orhan Firat1 2Carnegie Mellon University 3Instituto Superior T\u00e9cnico 4Instituto de Telecomunica\u00e7\u00f5es 5Unbabel 6Inspired Cognition pfernand@cs.cmu.edu Abstract Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iter- ative development of MT systems. While considerable progress has been made on esti- mating a single scalar quality score, current metrics lack the informativeness of more de- tailed schemes that annotate individual er- rors, such as Multidimensional Quality Met- rics (MQM). In this paper, we help fill this gap by proposing AUTOMQM, a prompt- ing technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in transla- tions. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AUTOMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particu- larly large gains for larger models) while providing interpretability through error spans that align with human annotations. 1 Introduction Evaluating natural language generation systems has always been challenging, and as the output qual- ity of these systems has improved, evaluation has become even more challenging and critical. For ex- ample, in Machine Translation (MT), a field where evaluation has garnered considerable attention, pre- vious standard automatic surface-level metrics such as BLEU (Papineni et al., 2002) are becoming less reliable as the quality of generation systems im- proves, with little remaining correlation with hu- man judgments (Freitag et al., 2022). To keep pace with the constantly improving qual- ity of MT output, the next generation of automatic Errors: \u2018easy\u2019 - major/accuracy; \u2018are\u2019 - minor/\ufb02uency Score: 25 MQM Identify the errors in the translationPortuguese: {source}; English:{candidate} Score Prediction A\u1d1c\u1d1b\u1d0fMQM Score: -5x1(major) - 1x1(minor) = -6 Candidate: \u201cEvaluating automatic translation are easy.\u201d Score the following translation from 0 to 100:Portuguese: {source}; English:{candidate} Source: \u201cAvaliar tradu\u00e7\u00e3o autom\u00e1tica \u00e9 dif\u00edcil.\u201d Figure 1: Illustration of how AUTOMQM uses LLMs to assess the quality of a translation. Rather than asking for a single quality score, AUTOMQM prompts models to identify and classify errors, and uses the MQM framework to produce a score. metrics is rapidly evolving. Learned automatic metrics that leverage human-judgments to finetune language models (Sellam et al., 2020; Rei et al., 2022a) currently represent the state-of-the-art in au- tomatic evaluation benchmarks like the WMT Met- rics task (Freitag et al., 2022), and show high corre- lation with human judgments. However, these met- rics typically output a single, uninterpretable qual- ity score, making it difficult to understand the type and extent of errors identified by them. The lack of insights makes it difficult for model developers to leverage these metrics to improve their systems. Unlike automatic metrics that only provide a single scalar value as quality score, state-of-the-art human evaluation methodologies like Multidi- mensional Quality Metrics (MQM; Lommel et al., 2014; Freitag et al., 2021a) ask professional annotators to identify and label error spans with a category and severity. This much richer feedback can be used to gain a better understanding of the current limitations of the model under evaluation \u2217 Work done while working part-time at Google. and improve it. In this paper, we ask whether large language models (LLMs) in combination with a few human annotations can be used to design an automatic metric that generates rich feedback similar to that generated by human experts in MQM. This work is motivated by recent papers that demonstrated that LLMs can be used as automatic metrics (Liu et al., 2023b) to generate a single quality score. In particular, Kocmi and Federmann (2023) showed that LLMs can be prompted to assess the quality of machine-generated translations, even achieving state-of-the-art performance on assessing system- level quality. However, previous work only pro- vides a limited view of the capabilities of LLMs for machine translation evaluation: the focus has pre- dominantly been on score prediction (i.e. predict- ing a numerical value for quality), without consid- ering the use of any annotated data (either through in-context learning or finetuning), and only in high- resource language pairs. We provide a large-scale study of the capabilities of LLMs (from the PaLM and PaLM-2 families; Chowdhery et al., 2022; Anil et al., 2023) for ma- chine translation evaluation (both with and without a reference translation), provide a novel compari- son between prompting and finetuning, and investi- gate the performance in the low-resource scenario. Inspired by findings that the performance of LLMs can be improved by prompting them for rationales of their predictions (Wei et al., 2022; Lu et al., 2023), we also propose AUTOMQM, a prompt- ing technique for MT evaluation that asks LLMs to identify error spans in a translation and to clas- sify these errors according to the MQM framework, with a quality score derived automatically from the identified errors. A key advantage of AUTOMQM is its interpretability, as users can inspect the errors responsible for a score (Figure 1). Our contributions can be summarized as follows: We confirm the finding of Kocmi and Feder- mann (2023) that LLMs are zero-shot state-of- the-art system-level evaluators, but show low correlation with human judgment compared to learned metrics at the segment-level. We show that finetuning an LLM with hu- man judgment mitigates its low segment-level performance (particularly for smaller LLMs), showing similar correlations with human judg- ment at both the system-level and segment- level to state-of-the-art learned metrics. We are the first to evaluate LLM-based evalu- ation methods on low-resource language pairs. We find that their performance is promising, but lags behind"}