{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_chunk_8.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the proposed training method to mitigate the issue of deciding the prediction order of dialog attributes?", "answer": " An order agnostic training method is proposed.", "ref_chunk": "joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1', ['Siddhant Arora', 'Hayato Futami', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization', '2023', ['arXiv.org', 'ArXiv'], 'Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.', ['A. Hussein', 'Brian Yan', 'Antonios Anastasopoulos', 'Shinji Watanabe', 'S. Khudanpur']] ['Challenges of Corporate Alliance CLOMA toward Plastic Litter', '2023', ['Oleoscience'], '', ['Shinji Watanabe']] ['The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction', '2023', ['arXiv.org', 'ArXiv'], 'Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.', ['Shilong Wu', 'Chenxi Wang', 'Hang Chen', 'Yusheng Dai', 'Chenyue Zhang', 'Ruoyu Wang', 'Hongbo Lan', 'Jun Du', 'Chin-Hui Lee', 'Jingdong Chen', 'Shinji Watanabe', 'Sabato Marco Siniscalchi', 'O. Scharenborg', 'Zhong-Qiu Wang', 'Jia Pan', 'Jianqing Gao']] ['Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data', '2023', ['Automatic Speech Recognition & Understanding', 'IEEE Automatic Speech Recognition and Understanding Workshop', 'Autom Speech Recognit Underst', 'ASRU', 'IEEE Autom Speech Recognit Underst Workshop'], 'Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet', ['Yifan Peng', 'Jinchuan Tian', 'Brian Yan', 'Dan Berrebbi', 'Xuankai Chang', 'Xinjian Li', 'Jiatong Shi', 'Siddhant Arora', 'William Chen', 'Roshan Sharma', 'Wangyou Zhang', 'Yui Sudo', 'Muhammad Shakeel', 'Jee-weon Jung', 'Soumi Maiti', 'Shinji Watanabe']] ['The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios', '2023', ['', ''], 'The CHiME challenges have played a significant role in the development and evaluation of robust automatic speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).', ['Samuele Cornell', 'Matthew Wiesner', 'Shinji Watanabe', 'Desh Raj', 'Xuankai Chang', 'Paola Garc\u00eda', 'Yoshiki Masuyama', 'Zhong-Qiu Wang', 'S. Squartini', 'S. Khudanpur']] ['Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing', '2023', ['arXiv.org', 'ArXiv'], 'Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.', ['Brian Yan', 'Xuankai Chang', 'Antonios Anastasopoulos',"}, {"question": " According to the text, what do the experiments show about the joint model compared to task-specific classifiers?", "answer": " The joint model achieves similar results to task-specific classifiers.", "ref_chunk": "joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1', ['Siddhant Arora', 'Hayato Futami', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization', '2023', ['arXiv.org', 'ArXiv'], 'Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.', ['A. Hussein', 'Brian Yan', 'Antonios Anastasopoulos', 'Shinji Watanabe', 'S. Khudanpur']] ['Challenges of Corporate Alliance CLOMA toward Plastic Litter', '2023', ['Oleoscience'], '', ['Shinji Watanabe']] ['The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction', '2023', ['arXiv.org', 'ArXiv'], 'Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.', ['Shilong Wu', 'Chenxi Wang', 'Hang Chen', 'Yusheng Dai', 'Chenyue Zhang', 'Ruoyu Wang', 'Hongbo Lan', 'Jun Du', 'Chin-Hui Lee', 'Jingdong Chen', 'Shinji Watanabe', 'Sabato Marco Siniscalchi', 'O. Scharenborg', 'Zhong-Qiu Wang', 'Jia Pan', 'Jianqing Gao']] ['Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data', '2023', ['Automatic Speech Recognition & Understanding', 'IEEE Automatic Speech Recognition and Understanding Workshop', 'Autom Speech Recognit Underst', 'ASRU', 'IEEE Autom Speech Recognit Underst Workshop'], 'Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet', ['Yifan Peng', 'Jinchuan Tian', 'Brian Yan', 'Dan Berrebbi', 'Xuankai Chang', 'Xinjian Li', 'Jiatong Shi', 'Siddhant Arora', 'William Chen', 'Roshan Sharma', 'Wangyou Zhang', 'Yui Sudo', 'Muhammad Shakeel', 'Jee-weon Jung', 'Soumi Maiti', 'Shinji Watanabe']] ['The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios', '2023', ['', ''], 'The CHiME challenges have played a significant role in the development and evaluation of robust automatic speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).', ['Samuele Cornell', 'Matthew Wiesner', 'Shinji Watanabe', 'Desh Raj', 'Xuankai Chang', 'Paola Garc\u00eda', 'Yoshiki Masuyama', 'Zhong-Qiu Wang', 'S. Squartini', 'S. Khudanpur']] ['Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing', '2023', ['arXiv.org', 'ArXiv'], 'Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.', ['Brian Yan', 'Xuankai Chang', 'Antonios Anastasopoulos',"}, {"question": " How does the joint model integrate dialog context to improve the SLU performance?", "answer": " The joint model can effectively integrate dialog content to further improve the SLU performance.", "ref_chunk": "joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1', ['Siddhant Arora', 'Hayato Futami', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization', '2023', ['arXiv.org', 'ArXiv'], 'Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.', ['A. Hussein', 'Brian Yan', 'Antonios Anastasopoulos', 'Shinji Watanabe', 'S. Khudanpur']] ['Challenges of Corporate Alliance CLOMA toward Plastic Litter', '2023', ['Oleoscience'], '', ['Shinji Watanabe']] ['The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction', '2023', ['arXiv.org', 'ArXiv'], 'Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.', ['Shilong Wu', 'Chenxi Wang', 'Hang Chen', 'Yusheng Dai', 'Chenyue Zhang', 'Ruoyu Wang', 'Hongbo Lan', 'Jun Du', 'Chin-Hui Lee', 'Jingdong Chen', 'Shinji Watanabe', 'Sabato Marco Siniscalchi', 'O. Scharenborg', 'Zhong-Qiu Wang', 'Jia Pan', 'Jianqing Gao']] ['Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data', '2023', ['Automatic Speech Recognition & Understanding', 'IEEE Automatic Speech Recognition and Understanding Workshop', 'Autom Speech Recognit Underst', 'ASRU', 'IEEE Autom Speech Recognit Underst Workshop'], 'Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet', ['Yifan Peng', 'Jinchuan Tian', 'Brian Yan', 'Dan Berrebbi', 'Xuankai Chang', 'Xinjian Li', 'Jiatong Shi', 'Siddhant Arora', 'William Chen', 'Roshan Sharma', 'Wangyou Zhang', 'Yui Sudo', 'Muhammad Shakeel', 'Jee-weon Jung', 'Soumi Maiti', 'Shinji Watanabe']] ['The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios', '2023', ['', ''], 'The CHiME challenges have played a significant role in the development and evaluation of robust automatic speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).', ['Samuele Cornell', 'Matthew Wiesner', 'Shinji Watanabe', 'Desh Raj', 'Xuankai Chang', 'Paola Garc\u00eda', 'Yoshiki Masuyama', 'Zhong-Qiu Wang', 'S. Squartini', 'S. Khudanpur']] ['Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing', '2023', ['arXiv.org', 'ArXiv'], 'Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.', ['Brian Yan', 'Xuankai Chang', 'Antonios Anastasopoulos',"}, {"question": " What gap does the introduction of target language context in E2E-ST aim to bridge?", "answer": " To bridge the gap of including context in end-to-end speech translation (E2E-ST).", "ref_chunk": "joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1', ['Siddhant Arora', 'Hayato Futami', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization', '2023', ['arXiv.org', 'ArXiv'], 'Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.', ['A. Hussein', 'Brian Yan', 'Antonios Anastasopoulos', 'Shinji Watanabe', 'S. Khudanpur']] ['Challenges of Corporate Alliance CLOMA toward Plastic Litter', '2023', ['Oleoscience'], '', ['Shinji Watanabe']] ['The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction', '2023', ['arXiv.org', 'ArXiv'], 'Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.', ['Shilong Wu', 'Chenxi Wang', 'Hang Chen', 'Yusheng Dai', 'Chenyue Zhang', 'Ruoyu Wang', 'Hongbo Lan', 'Jun Du', 'Chin-Hui Lee', 'Jingdong Chen', 'Shinji Watanabe', 'Sabato Marco Siniscalchi', 'O. Scharenborg', 'Zhong-Qiu Wang', 'Jia Pan', 'Jianqing Gao']] ['Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data', '2023', ['Automatic Speech Recognition & Understanding', 'IEEE Automatic Speech Recognition and Understanding Workshop', 'Autom Speech Recognit Underst', 'ASRU', 'IEEE Autom Speech Recognit Underst Workshop'], 'Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet', ['Yifan Peng', 'Jinchuan Tian', 'Brian Yan', 'Dan Berrebbi', 'Xuankai Chang', 'Xinjian Li', 'Jiatong Shi', 'Siddhant Arora', 'William Chen', 'Roshan Sharma', 'Wangyou Zhang', 'Yui Sudo', 'Muhammad Shakeel', 'Jee-weon Jung', 'Soumi Maiti', 'Shinji Watanabe']] ['The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios', '2023', ['', ''], 'The CHiME challenges have played a significant role in the development and evaluation of robust automatic speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).', ['Samuele Cornell', 'Matthew Wiesner', 'Shinji Watanabe', 'Desh Raj', 'Xuankai Chang', 'Paola Garc\u00eda', 'Yoshiki Masuyama', 'Zhong-Qiu Wang', 'S. Squartini', 'S. Khudanpur']] ['Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing', '2023', ['arXiv.org', 'ArXiv'], 'Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.', ['Brian Yan', 'Xuankai Chang', 'Antonios Anastasopoulos',"}, {"question": " In the MISP 2023 challenge, what task does the Audio-Visual Target Speaker Extraction (AVTSE) focus on?", "answer": " The MISP 2023 challenge focuses on Audio-Visual Target Speaker Extraction.", "ref_chunk": "joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1', ['Siddhant Arora', 'Hayato Futami', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization', '2023', ['arXiv.org', 'ArXiv'], 'Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.', ['A. Hussein', 'Brian Yan', 'Antonios Anastasopoulos', 'Shinji Watanabe', 'S. Khudanpur']] ['Challenges of Corporate Alliance CLOMA toward Plastic Litter', '2023', ['Oleoscience'], '', ['Shinji Watanabe']] ['The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction', '2023', ['arXiv.org', 'ArXiv'], 'Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.', ['Shilong Wu', 'Chenxi Wang', 'Hang Chen', 'Yusheng Dai', 'Chenyue Zhang', 'Ruoyu Wang', 'Hongbo Lan', 'Jun Du', 'Chin-Hui Lee', 'Jingdong Chen', 'Shinji Watanabe', 'Sabato Marco Siniscalchi', 'O. Scharenborg', 'Zhong-Qiu Wang', 'Jia Pan', 'Jianqing Gao']] ['Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data', '2023', ['Automatic Speech Recognition & Understanding', 'IEEE Automatic Speech Recognition and Understanding Workshop', 'Autom Speech Recognit Underst', 'ASRU', 'IEEE Autom Speech Recognit Underst Workshop'], 'Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet', ['Yifan Peng', 'Jinchuan Tian', 'Brian Yan', 'Dan Berrebbi', 'Xuankai Chang', 'Xinjian Li', 'Jiatong Shi', 'Siddhant Arora', 'William Chen', 'Roshan Sharma', 'Wangyou Zhang', 'Yui Sudo', 'Muhammad Shakeel', 'Jee-weon Jung', 'Soumi Maiti', 'Shinji Watanabe']] ['The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios', '2023', ['', ''], 'The CHiME challenges have played a significant role in the development and evaluation of robust automatic speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).', ['Samuele Cornell', 'Matthew Wiesner', 'Shinji Watanabe', 'Desh Raj', 'Xuankai Chang', 'Paola Garc\u00eda', 'Yoshiki Masuyama', 'Zhong-Qiu Wang', 'S. Squartini', 'S. Khudanpur']] ['Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing', '2023', ['arXiv.org', 'ArXiv'], 'Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.', ['Brian Yan', 'Xuankai Chang', 'Antonios Anastasopoulos',"}, {"question": " What does the work on OWSM aim to reproduce from the OpenAI Whisper model?", "answer": " Reproducing Whisperstyle training using an open-source toolkit and publicly available data.", "ref_chunk": "joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1', ['Siddhant Arora', 'Hayato Futami', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization', '2023', ['arXiv.org', 'ArXiv'], 'Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.', ['A. Hussein', 'Brian Yan', 'Antonios Anastasopoulos', 'Shinji Watanabe', 'S. Khudanpur']] ['Challenges of Corporate Alliance CLOMA toward Plastic Litter', '2023', ['Oleoscience'], '', ['Shinji Watanabe']] ['The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction', '2023', ['arXiv.org', 'ArXiv'], 'Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.', ['Shilong Wu', 'Chenxi Wang', 'Hang Chen', 'Yusheng Dai', 'Chenyue Zhang', 'Ruoyu Wang', 'Hongbo Lan', 'Jun Du', 'Chin-Hui Lee', 'Jingdong Chen', 'Shinji Watanabe', 'Sabato Marco Siniscalchi', 'O. Scharenborg', 'Zhong-Qiu Wang', 'Jia Pan', 'Jianqing Gao']] ['Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data', '2023', ['Automatic Speech Recognition & Understanding', 'IEEE Automatic Speech Recognition and Understanding Workshop', 'Autom Speech Recognit Underst', 'ASRU', 'IEEE Autom Speech Recognit Underst Workshop'], 'Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet', ['Yifan Peng', 'Jinchuan Tian', 'Brian Yan', 'Dan Berrebbi', 'Xuankai Chang', 'Xinjian Li', 'Jiatong Shi', 'Siddhant Arora', 'William Chen', 'Roshan Sharma', 'Wangyou Zhang', 'Yui Sudo', 'Muhammad Shakeel', 'Jee-weon Jung', 'Soumi Maiti', 'Shinji Watanabe']] ['The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios', '2023', ['', ''], 'The CHiME challenges have played a significant role in the development and evaluation of robust automatic speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).', ['Samuele Cornell', 'Matthew Wiesner', 'Shinji Watanabe', 'Desh Raj', 'Xuankai Chang', 'Paola Garc\u00eda', 'Yoshiki Masuyama', 'Zhong-Qiu Wang', 'S. Squartini', 'S. Khudanpur']] ['Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing', '2023', ['arXiv.org', 'ArXiv'], 'Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.', ['Brian Yan', 'Xuankai Chang', 'Antonios Anastasopoulos',"}, {"question": " What is the goal of participants in the CHiME-7 DASR Challenge?", "answer": " The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information.", "ref_chunk": "joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1', ['Siddhant Arora', 'Hayato Futami', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization', '2023', ['arXiv.org', 'ArXiv'], 'Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.', ['A. Hussein', 'Brian Yan', 'Antonios Anastasopoulos', 'Shinji Watanabe', 'S. Khudanpur']] ['Challenges of Corporate Alliance CLOMA toward Plastic Litter', '2023', ['Oleoscience'], '', ['Shinji Watanabe']] ['The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction', '2023', ['arXiv.org', 'ArXiv'], 'Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.', ['Shilong Wu', 'Chenxi Wang', 'Hang Chen', 'Yusheng Dai', 'Chenyue Zhang', 'Ruoyu Wang', 'Hongbo Lan', 'Jun Du', 'Chin-Hui Lee', 'Jingdong Chen', 'Shinji Watanabe', 'Sabato Marco Siniscalchi', 'O. Scharenborg', 'Zhong-Qiu Wang', 'Jia Pan', 'Jianqing Gao']] ['Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data', '2023', ['Automatic Speech Recognition & Understanding', 'IEEE Automatic Speech Recognition and Understanding Workshop', 'Autom Speech Recognit Underst', 'ASRU', 'IEEE Autom Speech Recognit Underst Workshop'], 'Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet', ['Yifan Peng', 'Jinchuan Tian', 'Brian Yan', 'Dan Berrebbi', 'Xuankai Chang', 'Xinjian Li', 'Jiatong Shi', 'Siddhant Arora', 'William Chen', 'Roshan Sharma', 'Wangyou Zhang', 'Yui Sudo', 'Muhammad Shakeel', 'Jee-weon Jung', 'Soumi Maiti', 'Shinji Watanabe']] ['The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios', '2023', ['', ''], 'The CHiME challenges have played a significant role in the development and evaluation of robust automatic speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).', ['Samuele Cornell', 'Matthew Wiesner', 'Shinji Watanabe', 'Desh Raj', 'Xuankai Chang', 'Paola Garc\u00eda', 'Yoshiki Masuyama', 'Zhong-Qiu Wang', 'S. Squartini', 'S. Khudanpur']] ['Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing', '2023', ['arXiv.org', 'ArXiv'], 'Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.', ['Brian Yan', 'Xuankai Chang', 'Antonios Anastasopoulos',"}, {"question": " What improvement does the ST/MT multi-tasking framework with hard parameter sharing propose in speech-to-text translation?", "answer": " The multi-tasking framework with hard parameter sharing proposes to reduce the speech-text modality gap and allows models to process both modalities using a joint vocabulary.", "ref_chunk": "joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1', ['Siddhant Arora', 'Hayato Futami', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization', '2023', ['arXiv.org', 'ArXiv'], 'Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.', ['A. Hussein', 'Brian Yan', 'Antonios Anastasopoulos', 'Shinji Watanabe', 'S. Khudanpur']] ['Challenges of Corporate Alliance CLOMA toward Plastic Litter', '2023', ['Oleoscience'], '', ['Shinji Watanabe']] ['The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction', '2023', ['arXiv.org', 'ArXiv'], 'Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.', ['Shilong Wu', 'Chenxi Wang', 'Hang Chen', 'Yusheng Dai', 'Chenyue Zhang', 'Ruoyu Wang', 'Hongbo Lan', 'Jun Du', 'Chin-Hui Lee', 'Jingdong Chen', 'Shinji Watanabe', 'Sabato Marco Siniscalchi', 'O. Scharenborg', 'Zhong-Qiu Wang', 'Jia Pan', 'Jianqing Gao']] ['Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data', '2023', ['Automatic Speech Recognition & Understanding', 'IEEE Automatic Speech Recognition and Understanding Workshop', 'Autom Speech Recognit Underst', 'ASRU', 'IEEE Autom Speech Recognit Underst Workshop'], 'Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet', ['Yifan Peng', 'Jinchuan Tian', 'Brian Yan', 'Dan Berrebbi', 'Xuankai Chang', 'Xinjian Li', 'Jiatong Shi', 'Siddhant Arora', 'William Chen', 'Roshan Sharma', 'Wangyou Zhang', 'Yui Sudo', 'Muhammad Shakeel', 'Jee-weon Jung', 'Soumi Maiti', 'Shinji Watanabe']] ['The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios', '2023', ['', ''], 'The CHiME challenges have played a significant role in the development and evaluation of robust automatic speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).', ['Samuele Cornell', 'Matthew Wiesner', 'Shinji Watanabe', 'Desh Raj', 'Xuankai Chang', 'Paola Garc\u00eda', 'Yoshiki Masuyama', 'Zhong-Qiu Wang', 'S. Squartini', 'S. Khudanpur']] ['Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing', '2023', ['arXiv.org', 'ArXiv'], 'Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.', ['Brian Yan', 'Xuankai Chang', 'Antonios Anastasopoulos',"}, {"question": " How does the context dropout method ensure robustness in E2E-ST?", "answer": " The context dropout method ensures robustness by handling the absence of context in E2E-ST.", "ref_chunk": "joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1', ['Siddhant Arora', 'Hayato Futami', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization', '2023', ['arXiv.org', 'ArXiv'], 'Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.', ['A. Hussein', 'Brian Yan', 'Antonios Anastasopoulos', 'Shinji Watanabe', 'S. Khudanpur']] ['Challenges of Corporate Alliance CLOMA toward Plastic Litter', '2023', ['Oleoscience'], '', ['Shinji Watanabe']] ['The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction', '2023', ['arXiv.org', 'ArXiv'], 'Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.', ['Shilong Wu', 'Chenxi Wang', 'Hang Chen', 'Yusheng Dai', 'Chenyue Zhang', 'Ruoyu Wang', 'Hongbo Lan', 'Jun Du', 'Chin-Hui Lee', 'Jingdong Chen', 'Shinji Watanabe', 'Sabato Marco Siniscalchi', 'O. Scharenborg', 'Zhong-Qiu Wang', 'Jia Pan', 'Jianqing Gao']] ['Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data', '2023', ['Automatic Speech Recognition & Understanding', 'IEEE Automatic Speech Recognition and Understanding Workshop', 'Autom Speech Recognit Underst', 'ASRU', 'IEEE Autom Speech Recognit Underst Workshop'], 'Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet', ['Yifan Peng', 'Jinchuan Tian', 'Brian Yan', 'Dan Berrebbi', 'Xuankai Chang', 'Xinjian Li', 'Jiatong Shi', 'Siddhant Arora', 'William Chen', 'Roshan Sharma', 'Wangyou Zhang', 'Yui Sudo', 'Muhammad Shakeel', 'Jee-weon Jung', 'Soumi Maiti', 'Shinji Watanabe']] ['The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios', '2023', ['', ''], 'The CHiME challenges have played a significant role in the development and evaluation of robust automatic speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).', ['Samuele Cornell', 'Matthew Wiesner', 'Shinji Watanabe', 'Desh Raj', 'Xuankai Chang', 'Paola Garc\u00eda', 'Yoshiki Masuyama', 'Zhong-Qiu Wang', 'S. Squartini', 'S. Khudanpur']] ['Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing', '2023', ['arXiv.org', 'ArXiv'], 'Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.', ['Brian Yan', 'Xuankai Chang', 'Antonios Anastasopoulos',"}, {"question": " What is the unique focus of the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges?", "answer": " The MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios.", "ref_chunk": "joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1', ['Siddhant Arora', 'Hayato Futami', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization', '2023', ['arXiv.org', 'ArXiv'], 'Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.', ['A. Hussein', 'Brian Yan', 'Antonios Anastasopoulos', 'Shinji Watanabe', 'S. Khudanpur']] ['Challenges of Corporate Alliance CLOMA toward Plastic Litter', '2023', ['Oleoscience'], '', ['Shinji Watanabe']] ['The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction', '2023', ['arXiv.org', 'ArXiv'], 'Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.', ['Shilong Wu', 'Chenxi Wang', 'Hang Chen', 'Yusheng Dai', 'Chenyue Zhang', 'Ruoyu Wang', 'Hongbo Lan', 'Jun Du', 'Chin-Hui Lee', 'Jingdong Chen', 'Shinji Watanabe', 'Sabato Marco Siniscalchi', 'O. Scharenborg', 'Zhong-Qiu Wang', 'Jia Pan', 'Jianqing Gao']] ['Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data', '2023', ['Automatic Speech Recognition & Understanding', 'IEEE Automatic Speech Recognition and Understanding Workshop', 'Autom Speech Recognit Underst', 'ASRU', 'IEEE Autom Speech Recognit Underst Workshop'], 'Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet', ['Yifan Peng', 'Jinchuan Tian', 'Brian Yan', 'Dan Berrebbi', 'Xuankai Chang', 'Xinjian Li', 'Jiatong Shi', 'Siddhant Arora', 'William Chen', 'Roshan Sharma', 'Wangyou Zhang', 'Yui Sudo', 'Muhammad Shakeel', 'Jee-weon Jung', 'Soumi Maiti', 'Shinji Watanabe']] ['The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios', '2023', ['', ''], 'The CHiME challenges have played a significant role in the development and evaluation of robust automatic speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).', ['Samuele Cornell', 'Matthew Wiesner', 'Shinji Watanabe', 'Desh Raj', 'Xuankai Chang', 'Paola Garc\u00eda', 'Yoshiki Masuyama', 'Zhong-Qiu Wang', 'S. Squartini', 'S. Khudanpur']] ['Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing', '2023', ['arXiv.org', 'ArXiv'], 'Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.', ['Brian Yan', 'Xuankai Chang', 'Antonios Anastasopoulos',"}], "doc_text": "joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1', ['Siddhant Arora', 'Hayato Futami', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']] ['Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization', '2023', ['arXiv.org', 'ArXiv'], 'Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.', ['A. Hussein', 'Brian Yan', 'Antonios Anastasopoulos', 'Shinji Watanabe', 'S. Khudanpur']] ['Challenges of Corporate Alliance CLOMA toward Plastic Litter', '2023', ['Oleoscience'], '', ['Shinji Watanabe']] ['The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction', '2023', ['arXiv.org', 'ArXiv'], 'Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.', ['Shilong Wu', 'Chenxi Wang', 'Hang Chen', 'Yusheng Dai', 'Chenyue Zhang', 'Ruoyu Wang', 'Hongbo Lan', 'Jun Du', 'Chin-Hui Lee', 'Jingdong Chen', 'Shinji Watanabe', 'Sabato Marco Siniscalchi', 'O. Scharenborg', 'Zhong-Qiu Wang', 'Jia Pan', 'Jianqing Gao']] ['Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data', '2023', ['Automatic Speech Recognition & Understanding', 'IEEE Automatic Speech Recognition and Understanding Workshop', 'Autom Speech Recognit Underst', 'ASRU', 'IEEE Autom Speech Recognit Underst Workshop'], 'Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet', ['Yifan Peng', 'Jinchuan Tian', 'Brian Yan', 'Dan Berrebbi', 'Xuankai Chang', 'Xinjian Li', 'Jiatong Shi', 'Siddhant Arora', 'William Chen', 'Roshan Sharma', 'Wangyou Zhang', 'Yui Sudo', 'Muhammad Shakeel', 'Jee-weon Jung', 'Soumi Maiti', 'Shinji Watanabe']] ['The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios', '2023', ['', ''], 'The CHiME challenges have played a significant role in the development and evaluation of robust automatic speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).', ['Samuele Cornell', 'Matthew Wiesner', 'Shinji Watanabe', 'Desh Raj', 'Xuankai Chang', 'Paola Garc\u00eda', 'Yoshiki Masuyama', 'Zhong-Qiu Wang', 'S. Squartini', 'S. Khudanpur']] ['Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing', '2023', ['arXiv.org', 'ArXiv'], 'Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.', ['Brian Yan', 'Xuankai Chang', 'Antonios Anastasopoulos',"}