{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Speech_collage:_code-switched_audio_generation_by_collaging_monolingual_corpora_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of extending the unit-segments by 0.05 seconds at the start and end of the segment?,answer: The extension provides an extra 0.05 second for overlap in the overlap-add process.", "ref_chunk": "unit-segments by 0.05 seconds at the start and end of the segment. This extension provides an extra 0.05 sec- ond which is utilized as overlap in overlap-add process. 2.2. Energy normalization Additionally, we normalize the synthesized utterance by the average of unit-segments energy to remove artifacts introduced by energy variations between segments. For a speech sequence X of length T , X = {xt \u2208 R|t = 1, \u00b7 \u00b7 \u00b7 , T }, the average energy is calculated as follows: X \u2032 = \uf8f1 \uf8f2 \uf8f3 (cid:113) 1 T xt (cid:80) t x2 t |t = 1, \u00b7 \u00b7 \u00b7 , T \uf8fc \uf8fd \uf8fe (1) return D 5: 6: function NORMALIZEENERGY(audioCut) e \u2190 AVGENERGY(audioCut) 7: return audioCut / 8: 9: function SAMPLEUNIT(D, w1:i) return random cut from D[w1:i] 10: 11: function GENERATECOLLAGE(TCS, A, n) 12: 13: 14: 15: 16: 17: 18: 19: 20: \u25b7 Eq 2 \u221a e \u25b7 n: size of n-gram D \u2190 SETUPSUPERVISIONS(A, n) DCS \u2190 \u2205 for texti \u2208 TCS do W \u2190 GETCONSECUNITS(texti) for wi \u2208 W do cut \u2190 SAMPLEUNIT(D[wi]) cutsi \u2190 OVERLAPADD(cutsi,cut) cutsi \u2190 NORMALIZEENERGY(cutsi) cutsi.text \u2190 cutsi.text + wi \u25b7 Append wi 21: DCS.append(cutsi) 22: return DCS 2.3. N-gram units To further enhance the quality of the generated CS we explore splic- ing consecutive units (n-grams), in alignment with selecting longer units in concatenated speech synthesis [29] Given a CS sentence our approach starts by matching the largest consecutive unit from monolingual alignments. If a specific n-gram is unavailable, the al- gorithm backs off to a smaller unit. It\u2019s worth noting that in this study, we only experimented with unigrams and bigrams. A detailed 2https://github.com/kaldi-asr/kaldi/tree/master/ egs/aishell/s5 https://github.com/kaldi-asr/kaldi/tree/master/ description of n-gram Speech Collage implementation is described in Algorithm 1. Using the alignments from monolingual data and maximum n-gram size, SETUPSUPERVISIONS(\u00b7) creates a collec- tion D of audio segments corresponding to each n-gram unit. Con- secutive n-gram units are matched from alignments, starting with n and progressing to unigrams. If an n-gram is absent, the algo- rithm backs off to an (n \u2212 1) unit. In GENERATECOLLAGE(\u00b7), the function GETCONSECUNITS(\u00b7) returns all consecutive (1 : n) units. Each n-gram unit is randomly drawn from its respective col- lection using SAMPLEUNIT(\u00b7). These segments are appended to the current spliced utterance with OVERLAPADD(\u00b7) described in \u00a72.1, and the resulting combined utterance undergoes energy normaliza- tion NORMALIZEENERGY(\u00b7) from Eq1. egs/mgb2_arabic/s5 2.4. Zero-shot CS framework In this case study we focus on generating Arabic-English code- switching (CS) data, operating under the assumption that no Arabic- English CS training data is available. To generate speech data using the Speech Collage method, we require CS text. We generate the CS text from monolingual resources using the lexicon-based (Random) replacements approach described in [13]. The approach entails the following steps: 1. Parallel Text Translation: We leverage a public Arabic- English Machine Translation System3 to generate the parallel English text from the Arabic transcription. 2. Word Level Alignments: After translation, we fine-tune multilingual BERT (mBERT) [30] to obtain the word-level alignments. 3. Random Replacement: Given the alignments, Arabic words are randomly substituted with their corresponding English words at a rate of 20%, as suggested by [13]. 2.5. End-to-End Speech Recognition In this work, we utilized the end-to-end (E2E) ASR conformer architecture [31], with the ESPNET toolkit [32]. The E2E-ASR implementation consists of a conformer encoder and a transformer decoder. Both are multiblocked self-attention architectures with the encoder further enhanced by an additional convolution module. The ASR task is formulated as Bayesian decision finding the most probable target word sequence \u02c6Y, from all possible outputs Y\u2217, by selecting the sequence which maximizes the posterior likelihood P (Y|X), given T-length sequence of D-dimensional speech fea- tures, X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. For text tokenization, we used word-piece byte-pair-encoding [33]. The total loss function Lasr is a multi-task learning objective that combines the decoder cross-entropy (CE) loss Lce and the CTC loss [34] Lctc. Lasr = \u03b1Lctc + (1 \u2212 \u03b1)Lce where \u03b1 is used for interpolation. In our approach, the conformer is initially pre-trained on monolingual data and subsequently fine- tuned on monolingual and synthetic CS speech combined. 2.6. Code-Mixing Index To quantify the amount of code-switching we use Code-Mixing In- dex (CMI) metric [35]. The CMI for an utterance is defined as: CM I = 2 \u2217 (N \u2212 maxi) + 1 1 N 2 P (x) Where maxi represents the number of words in the dominant lan- guage i, N is the total word count in the utterance, P is the number of code alternation points, with the constraint 0 \u2264 P < N . A low CMI score indicates monolingualism in the text whereas the high CMI score implies high degree of code-mixing in the text. 3. DATA AND EXPERIMENTAL SETUP In-domain: The target domain we are considering is the Mandarin- English code-switching, specifically SEAME [36]. In this scenario, we utilize monolingual training data from Chinese AISHELL-1 [37], 100h of English data randomly sampled from Tedlium3 [38] and 3API access available from https://mt.qcri.org/api (2) (3) SEAME text [36] to generate 62.2 hours of CS data. Evaluation is performed on SEAME test sets (devman and devsge), measur- ing mixed error-rate (MER) that considers word-level English and character-level Mandarin. We also report WER on monolingual En- glish and CER on monolingual Chinese subsets. Zero-shot: For this scenario, we use monolingual training data from MGB-2 [39] and Tedlium3. We generate 80 hours of CS data using synthetic CS text described in \u00a72.4. Evaluation is conducted on ES- CWA [8], which is a real Arabic-English CS dataset. Data pre-processing: All audios are augmented with speed pertur- bations (0.9, 1.0 and 1.1) and transformed into 83-dimensional fea- ture frames (80 log-mel filterbank coefficients plus 3 pitch features). Additionally, we augment the features with specaugment, with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. Models: the conformer encoder consists of 12 blocks, each with 2048 feed-forward dimensions, 256 attention dimensions,"}, {"question": " How is the synthesized utterance normalized in terms of energy?,answer: The synthesized utterance is normalized by the average of unit-segments energy to remove artifacts introduced by energy variations between segments.", "ref_chunk": "unit-segments by 0.05 seconds at the start and end of the segment. This extension provides an extra 0.05 sec- ond which is utilized as overlap in overlap-add process. 2.2. Energy normalization Additionally, we normalize the synthesized utterance by the average of unit-segments energy to remove artifacts introduced by energy variations between segments. For a speech sequence X of length T , X = {xt \u2208 R|t = 1, \u00b7 \u00b7 \u00b7 , T }, the average energy is calculated as follows: X \u2032 = \uf8f1 \uf8f2 \uf8f3 (cid:113) 1 T xt (cid:80) t x2 t |t = 1, \u00b7 \u00b7 \u00b7 , T \uf8fc \uf8fd \uf8fe (1) return D 5: 6: function NORMALIZEENERGY(audioCut) e \u2190 AVGENERGY(audioCut) 7: return audioCut / 8: 9: function SAMPLEUNIT(D, w1:i) return random cut from D[w1:i] 10: 11: function GENERATECOLLAGE(TCS, A, n) 12: 13: 14: 15: 16: 17: 18: 19: 20: \u25b7 Eq 2 \u221a e \u25b7 n: size of n-gram D \u2190 SETUPSUPERVISIONS(A, n) DCS \u2190 \u2205 for texti \u2208 TCS do W \u2190 GETCONSECUNITS(texti) for wi \u2208 W do cut \u2190 SAMPLEUNIT(D[wi]) cutsi \u2190 OVERLAPADD(cutsi,cut) cutsi \u2190 NORMALIZEENERGY(cutsi) cutsi.text \u2190 cutsi.text + wi \u25b7 Append wi 21: DCS.append(cutsi) 22: return DCS 2.3. N-gram units To further enhance the quality of the generated CS we explore splic- ing consecutive units (n-grams), in alignment with selecting longer units in concatenated speech synthesis [29] Given a CS sentence our approach starts by matching the largest consecutive unit from monolingual alignments. If a specific n-gram is unavailable, the al- gorithm backs off to a smaller unit. It\u2019s worth noting that in this study, we only experimented with unigrams and bigrams. A detailed 2https://github.com/kaldi-asr/kaldi/tree/master/ egs/aishell/s5 https://github.com/kaldi-asr/kaldi/tree/master/ description of n-gram Speech Collage implementation is described in Algorithm 1. Using the alignments from monolingual data and maximum n-gram size, SETUPSUPERVISIONS(\u00b7) creates a collec- tion D of audio segments corresponding to each n-gram unit. Con- secutive n-gram units are matched from alignments, starting with n and progressing to unigrams. If an n-gram is absent, the algo- rithm backs off to an (n \u2212 1) unit. In GENERATECOLLAGE(\u00b7), the function GETCONSECUNITS(\u00b7) returns all consecutive (1 : n) units. Each n-gram unit is randomly drawn from its respective col- lection using SAMPLEUNIT(\u00b7). These segments are appended to the current spliced utterance with OVERLAPADD(\u00b7) described in \u00a72.1, and the resulting combined utterance undergoes energy normaliza- tion NORMALIZEENERGY(\u00b7) from Eq1. egs/mgb2_arabic/s5 2.4. Zero-shot CS framework In this case study we focus on generating Arabic-English code- switching (CS) data, operating under the assumption that no Arabic- English CS training data is available. To generate speech data using the Speech Collage method, we require CS text. We generate the CS text from monolingual resources using the lexicon-based (Random) replacements approach described in [13]. The approach entails the following steps: 1. Parallel Text Translation: We leverage a public Arabic- English Machine Translation System3 to generate the parallel English text from the Arabic transcription. 2. Word Level Alignments: After translation, we fine-tune multilingual BERT (mBERT) [30] to obtain the word-level alignments. 3. Random Replacement: Given the alignments, Arabic words are randomly substituted with their corresponding English words at a rate of 20%, as suggested by [13]. 2.5. End-to-End Speech Recognition In this work, we utilized the end-to-end (E2E) ASR conformer architecture [31], with the ESPNET toolkit [32]. The E2E-ASR implementation consists of a conformer encoder and a transformer decoder. Both are multiblocked self-attention architectures with the encoder further enhanced by an additional convolution module. The ASR task is formulated as Bayesian decision finding the most probable target word sequence \u02c6Y, from all possible outputs Y\u2217, by selecting the sequence which maximizes the posterior likelihood P (Y|X), given T-length sequence of D-dimensional speech fea- tures, X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. For text tokenization, we used word-piece byte-pair-encoding [33]. The total loss function Lasr is a multi-task learning objective that combines the decoder cross-entropy (CE) loss Lce and the CTC loss [34] Lctc. Lasr = \u03b1Lctc + (1 \u2212 \u03b1)Lce where \u03b1 is used for interpolation. In our approach, the conformer is initially pre-trained on monolingual data and subsequently fine- tuned on monolingual and synthetic CS speech combined. 2.6. Code-Mixing Index To quantify the amount of code-switching we use Code-Mixing In- dex (CMI) metric [35]. The CMI for an utterance is defined as: CM I = 2 \u2217 (N \u2212 maxi) + 1 1 N 2 P (x) Where maxi represents the number of words in the dominant lan- guage i, N is the total word count in the utterance, P is the number of code alternation points, with the constraint 0 \u2264 P < N . A low CMI score indicates monolingualism in the text whereas the high CMI score implies high degree of code-mixing in the text. 3. DATA AND EXPERIMENTAL SETUP In-domain: The target domain we are considering is the Mandarin- English code-switching, specifically SEAME [36]. In this scenario, we utilize monolingual training data from Chinese AISHELL-1 [37], 100h of English data randomly sampled from Tedlium3 [38] and 3API access available from https://mt.qcri.org/api (2) (3) SEAME text [36] to generate 62.2 hours of CS data. Evaluation is performed on SEAME test sets (devman and devsge), measur- ing mixed error-rate (MER) that considers word-level English and character-level Mandarin. We also report WER on monolingual En- glish and CER on monolingual Chinese subsets. Zero-shot: For this scenario, we use monolingual training data from MGB-2 [39] and Tedlium3. We generate 80 hours of CS data using synthetic CS text described in \u00a72.4. Evaluation is conducted on ES- CWA [8], which is a real Arabic-English CS dataset. Data pre-processing: All audios are augmented with speed pertur- bations (0.9, 1.0 and 1.1) and transformed into 83-dimensional fea- ture frames (80 log-mel filterbank coefficients plus 3 pitch features). Additionally, we augment the features with specaugment, with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. Models: the conformer encoder consists of 12 blocks, each with 2048 feed-forward dimensions, 256 attention dimensions,"}, {"question": " What is the formula used to calculate the average energy of a speech sequence X of length T?,answer: The average energy is calculated as (1/T) * \u03a3(xt^2) for t = 1 to T.", "ref_chunk": "unit-segments by 0.05 seconds at the start and end of the segment. This extension provides an extra 0.05 sec- ond which is utilized as overlap in overlap-add process. 2.2. Energy normalization Additionally, we normalize the synthesized utterance by the average of unit-segments energy to remove artifacts introduced by energy variations between segments. For a speech sequence X of length T , X = {xt \u2208 R|t = 1, \u00b7 \u00b7 \u00b7 , T }, the average energy is calculated as follows: X \u2032 = \uf8f1 \uf8f2 \uf8f3 (cid:113) 1 T xt (cid:80) t x2 t |t = 1, \u00b7 \u00b7 \u00b7 , T \uf8fc \uf8fd \uf8fe (1) return D 5: 6: function NORMALIZEENERGY(audioCut) e \u2190 AVGENERGY(audioCut) 7: return audioCut / 8: 9: function SAMPLEUNIT(D, w1:i) return random cut from D[w1:i] 10: 11: function GENERATECOLLAGE(TCS, A, n) 12: 13: 14: 15: 16: 17: 18: 19: 20: \u25b7 Eq 2 \u221a e \u25b7 n: size of n-gram D \u2190 SETUPSUPERVISIONS(A, n) DCS \u2190 \u2205 for texti \u2208 TCS do W \u2190 GETCONSECUNITS(texti) for wi \u2208 W do cut \u2190 SAMPLEUNIT(D[wi]) cutsi \u2190 OVERLAPADD(cutsi,cut) cutsi \u2190 NORMALIZEENERGY(cutsi) cutsi.text \u2190 cutsi.text + wi \u25b7 Append wi 21: DCS.append(cutsi) 22: return DCS 2.3. N-gram units To further enhance the quality of the generated CS we explore splic- ing consecutive units (n-grams), in alignment with selecting longer units in concatenated speech synthesis [29] Given a CS sentence our approach starts by matching the largest consecutive unit from monolingual alignments. If a specific n-gram is unavailable, the al- gorithm backs off to a smaller unit. It\u2019s worth noting that in this study, we only experimented with unigrams and bigrams. A detailed 2https://github.com/kaldi-asr/kaldi/tree/master/ egs/aishell/s5 https://github.com/kaldi-asr/kaldi/tree/master/ description of n-gram Speech Collage implementation is described in Algorithm 1. Using the alignments from monolingual data and maximum n-gram size, SETUPSUPERVISIONS(\u00b7) creates a collec- tion D of audio segments corresponding to each n-gram unit. Con- secutive n-gram units are matched from alignments, starting with n and progressing to unigrams. If an n-gram is absent, the algo- rithm backs off to an (n \u2212 1) unit. In GENERATECOLLAGE(\u00b7), the function GETCONSECUNITS(\u00b7) returns all consecutive (1 : n) units. Each n-gram unit is randomly drawn from its respective col- lection using SAMPLEUNIT(\u00b7). These segments are appended to the current spliced utterance with OVERLAPADD(\u00b7) described in \u00a72.1, and the resulting combined utterance undergoes energy normaliza- tion NORMALIZEENERGY(\u00b7) from Eq1. egs/mgb2_arabic/s5 2.4. Zero-shot CS framework In this case study we focus on generating Arabic-English code- switching (CS) data, operating under the assumption that no Arabic- English CS training data is available. To generate speech data using the Speech Collage method, we require CS text. We generate the CS text from monolingual resources using the lexicon-based (Random) replacements approach described in [13]. The approach entails the following steps: 1. Parallel Text Translation: We leverage a public Arabic- English Machine Translation System3 to generate the parallel English text from the Arabic transcription. 2. Word Level Alignments: After translation, we fine-tune multilingual BERT (mBERT) [30] to obtain the word-level alignments. 3. Random Replacement: Given the alignments, Arabic words are randomly substituted with their corresponding English words at a rate of 20%, as suggested by [13]. 2.5. End-to-End Speech Recognition In this work, we utilized the end-to-end (E2E) ASR conformer architecture [31], with the ESPNET toolkit [32]. The E2E-ASR implementation consists of a conformer encoder and a transformer decoder. Both are multiblocked self-attention architectures with the encoder further enhanced by an additional convolution module. The ASR task is formulated as Bayesian decision finding the most probable target word sequence \u02c6Y, from all possible outputs Y\u2217, by selecting the sequence which maximizes the posterior likelihood P (Y|X), given T-length sequence of D-dimensional speech fea- tures, X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. For text tokenization, we used word-piece byte-pair-encoding [33]. The total loss function Lasr is a multi-task learning objective that combines the decoder cross-entropy (CE) loss Lce and the CTC loss [34] Lctc. Lasr = \u03b1Lctc + (1 \u2212 \u03b1)Lce where \u03b1 is used for interpolation. In our approach, the conformer is initially pre-trained on monolingual data and subsequently fine- tuned on monolingual and synthetic CS speech combined. 2.6. Code-Mixing Index To quantify the amount of code-switching we use Code-Mixing In- dex (CMI) metric [35]. The CMI for an utterance is defined as: CM I = 2 \u2217 (N \u2212 maxi) + 1 1 N 2 P (x) Where maxi represents the number of words in the dominant lan- guage i, N is the total word count in the utterance, P is the number of code alternation points, with the constraint 0 \u2264 P < N . A low CMI score indicates monolingualism in the text whereas the high CMI score implies high degree of code-mixing in the text. 3. DATA AND EXPERIMENTAL SETUP In-domain: The target domain we are considering is the Mandarin- English code-switching, specifically SEAME [36]. In this scenario, we utilize monolingual training data from Chinese AISHELL-1 [37], 100h of English data randomly sampled from Tedlium3 [38] and 3API access available from https://mt.qcri.org/api (2) (3) SEAME text [36] to generate 62.2 hours of CS data. Evaluation is performed on SEAME test sets (devman and devsge), measur- ing mixed error-rate (MER) that considers word-level English and character-level Mandarin. We also report WER on monolingual En- glish and CER on monolingual Chinese subsets. Zero-shot: For this scenario, we use monolingual training data from MGB-2 [39] and Tedlium3. We generate 80 hours of CS data using synthetic CS text described in \u00a72.4. Evaluation is conducted on ES- CWA [8], which is a real Arabic-English CS dataset. Data pre-processing: All audios are augmented with speed pertur- bations (0.9, 1.0 and 1.1) and transformed into 83-dimensional fea- ture frames (80 log-mel filterbank coefficients plus 3 pitch features). Additionally, we augment the features with specaugment, with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. Models: the conformer encoder consists of 12 blocks, each with 2048 feed-forward dimensions, 256 attention dimensions,"}, {"question": " What does the function NORMALIZEENERGY(audioCut) do?,answer: The function returns the audioCut normalized by the average energy.", "ref_chunk": "unit-segments by 0.05 seconds at the start and end of the segment. This extension provides an extra 0.05 sec- ond which is utilized as overlap in overlap-add process. 2.2. Energy normalization Additionally, we normalize the synthesized utterance by the average of unit-segments energy to remove artifacts introduced by energy variations between segments. For a speech sequence X of length T , X = {xt \u2208 R|t = 1, \u00b7 \u00b7 \u00b7 , T }, the average energy is calculated as follows: X \u2032 = \uf8f1 \uf8f2 \uf8f3 (cid:113) 1 T xt (cid:80) t x2 t |t = 1, \u00b7 \u00b7 \u00b7 , T \uf8fc \uf8fd \uf8fe (1) return D 5: 6: function NORMALIZEENERGY(audioCut) e \u2190 AVGENERGY(audioCut) 7: return audioCut / 8: 9: function SAMPLEUNIT(D, w1:i) return random cut from D[w1:i] 10: 11: function GENERATECOLLAGE(TCS, A, n) 12: 13: 14: 15: 16: 17: 18: 19: 20: \u25b7 Eq 2 \u221a e \u25b7 n: size of n-gram D \u2190 SETUPSUPERVISIONS(A, n) DCS \u2190 \u2205 for texti \u2208 TCS do W \u2190 GETCONSECUNITS(texti) for wi \u2208 W do cut \u2190 SAMPLEUNIT(D[wi]) cutsi \u2190 OVERLAPADD(cutsi,cut) cutsi \u2190 NORMALIZEENERGY(cutsi) cutsi.text \u2190 cutsi.text + wi \u25b7 Append wi 21: DCS.append(cutsi) 22: return DCS 2.3. N-gram units To further enhance the quality of the generated CS we explore splic- ing consecutive units (n-grams), in alignment with selecting longer units in concatenated speech synthesis [29] Given a CS sentence our approach starts by matching the largest consecutive unit from monolingual alignments. If a specific n-gram is unavailable, the al- gorithm backs off to a smaller unit. It\u2019s worth noting that in this study, we only experimented with unigrams and bigrams. A detailed 2https://github.com/kaldi-asr/kaldi/tree/master/ egs/aishell/s5 https://github.com/kaldi-asr/kaldi/tree/master/ description of n-gram Speech Collage implementation is described in Algorithm 1. Using the alignments from monolingual data and maximum n-gram size, SETUPSUPERVISIONS(\u00b7) creates a collec- tion D of audio segments corresponding to each n-gram unit. Con- secutive n-gram units are matched from alignments, starting with n and progressing to unigrams. If an n-gram is absent, the algo- rithm backs off to an (n \u2212 1) unit. In GENERATECOLLAGE(\u00b7), the function GETCONSECUNITS(\u00b7) returns all consecutive (1 : n) units. Each n-gram unit is randomly drawn from its respective col- lection using SAMPLEUNIT(\u00b7). These segments are appended to the current spliced utterance with OVERLAPADD(\u00b7) described in \u00a72.1, and the resulting combined utterance undergoes energy normaliza- tion NORMALIZEENERGY(\u00b7) from Eq1. egs/mgb2_arabic/s5 2.4. Zero-shot CS framework In this case study we focus on generating Arabic-English code- switching (CS) data, operating under the assumption that no Arabic- English CS training data is available. To generate speech data using the Speech Collage method, we require CS text. We generate the CS text from monolingual resources using the lexicon-based (Random) replacements approach described in [13]. The approach entails the following steps: 1. Parallel Text Translation: We leverage a public Arabic- English Machine Translation System3 to generate the parallel English text from the Arabic transcription. 2. Word Level Alignments: After translation, we fine-tune multilingual BERT (mBERT) [30] to obtain the word-level alignments. 3. Random Replacement: Given the alignments, Arabic words are randomly substituted with their corresponding English words at a rate of 20%, as suggested by [13]. 2.5. End-to-End Speech Recognition In this work, we utilized the end-to-end (E2E) ASR conformer architecture [31], with the ESPNET toolkit [32]. The E2E-ASR implementation consists of a conformer encoder and a transformer decoder. Both are multiblocked self-attention architectures with the encoder further enhanced by an additional convolution module. The ASR task is formulated as Bayesian decision finding the most probable target word sequence \u02c6Y, from all possible outputs Y\u2217, by selecting the sequence which maximizes the posterior likelihood P (Y|X), given T-length sequence of D-dimensional speech fea- tures, X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. For text tokenization, we used word-piece byte-pair-encoding [33]. The total loss function Lasr is a multi-task learning objective that combines the decoder cross-entropy (CE) loss Lce and the CTC loss [34] Lctc. Lasr = \u03b1Lctc + (1 \u2212 \u03b1)Lce where \u03b1 is used for interpolation. In our approach, the conformer is initially pre-trained on monolingual data and subsequently fine- tuned on monolingual and synthetic CS speech combined. 2.6. Code-Mixing Index To quantify the amount of code-switching we use Code-Mixing In- dex (CMI) metric [35]. The CMI for an utterance is defined as: CM I = 2 \u2217 (N \u2212 maxi) + 1 1 N 2 P (x) Where maxi represents the number of words in the dominant lan- guage i, N is the total word count in the utterance, P is the number of code alternation points, with the constraint 0 \u2264 P < N . A low CMI score indicates monolingualism in the text whereas the high CMI score implies high degree of code-mixing in the text. 3. DATA AND EXPERIMENTAL SETUP In-domain: The target domain we are considering is the Mandarin- English code-switching, specifically SEAME [36]. In this scenario, we utilize monolingual training data from Chinese AISHELL-1 [37], 100h of English data randomly sampled from Tedlium3 [38] and 3API access available from https://mt.qcri.org/api (2) (3) SEAME text [36] to generate 62.2 hours of CS data. Evaluation is performed on SEAME test sets (devman and devsge), measur- ing mixed error-rate (MER) that considers word-level English and character-level Mandarin. We also report WER on monolingual En- glish and CER on monolingual Chinese subsets. Zero-shot: For this scenario, we use monolingual training data from MGB-2 [39] and Tedlium3. We generate 80 hours of CS data using synthetic CS text described in \u00a72.4. Evaluation is conducted on ES- CWA [8], which is a real Arabic-English CS dataset. Data pre-processing: All audios are augmented with speed pertur- bations (0.9, 1.0 and 1.1) and transformed into 83-dimensional fea- ture frames (80 log-mel filterbank coefficients plus 3 pitch features). Additionally, we augment the features with specaugment, with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. Models: the conformer encoder consists of 12 blocks, each with 2048 feed-forward dimensions, 256 attention dimensions,"}, {"question": " What is the purpose of the N-gram units in enhancing the quality of the generated CS?,answer: N-gram units enhance the quality by splicing consecutive units in alignment with selecting longer units in concatenated speech synthesis.", "ref_chunk": "unit-segments by 0.05 seconds at the start and end of the segment. This extension provides an extra 0.05 sec- ond which is utilized as overlap in overlap-add process. 2.2. Energy normalization Additionally, we normalize the synthesized utterance by the average of unit-segments energy to remove artifacts introduced by energy variations between segments. For a speech sequence X of length T , X = {xt \u2208 R|t = 1, \u00b7 \u00b7 \u00b7 , T }, the average energy is calculated as follows: X \u2032 = \uf8f1 \uf8f2 \uf8f3 (cid:113) 1 T xt (cid:80) t x2 t |t = 1, \u00b7 \u00b7 \u00b7 , T \uf8fc \uf8fd \uf8fe (1) return D 5: 6: function NORMALIZEENERGY(audioCut) e \u2190 AVGENERGY(audioCut) 7: return audioCut / 8: 9: function SAMPLEUNIT(D, w1:i) return random cut from D[w1:i] 10: 11: function GENERATECOLLAGE(TCS, A, n) 12: 13: 14: 15: 16: 17: 18: 19: 20: \u25b7 Eq 2 \u221a e \u25b7 n: size of n-gram D \u2190 SETUPSUPERVISIONS(A, n) DCS \u2190 \u2205 for texti \u2208 TCS do W \u2190 GETCONSECUNITS(texti) for wi \u2208 W do cut \u2190 SAMPLEUNIT(D[wi]) cutsi \u2190 OVERLAPADD(cutsi,cut) cutsi \u2190 NORMALIZEENERGY(cutsi) cutsi.text \u2190 cutsi.text + wi \u25b7 Append wi 21: DCS.append(cutsi) 22: return DCS 2.3. N-gram units To further enhance the quality of the generated CS we explore splic- ing consecutive units (n-grams), in alignment with selecting longer units in concatenated speech synthesis [29] Given a CS sentence our approach starts by matching the largest consecutive unit from monolingual alignments. If a specific n-gram is unavailable, the al- gorithm backs off to a smaller unit. It\u2019s worth noting that in this study, we only experimented with unigrams and bigrams. A detailed 2https://github.com/kaldi-asr/kaldi/tree/master/ egs/aishell/s5 https://github.com/kaldi-asr/kaldi/tree/master/ description of n-gram Speech Collage implementation is described in Algorithm 1. Using the alignments from monolingual data and maximum n-gram size, SETUPSUPERVISIONS(\u00b7) creates a collec- tion D of audio segments corresponding to each n-gram unit. Con- secutive n-gram units are matched from alignments, starting with n and progressing to unigrams. If an n-gram is absent, the algo- rithm backs off to an (n \u2212 1) unit. In GENERATECOLLAGE(\u00b7), the function GETCONSECUNITS(\u00b7) returns all consecutive (1 : n) units. Each n-gram unit is randomly drawn from its respective col- lection using SAMPLEUNIT(\u00b7). These segments are appended to the current spliced utterance with OVERLAPADD(\u00b7) described in \u00a72.1, and the resulting combined utterance undergoes energy normaliza- tion NORMALIZEENERGY(\u00b7) from Eq1. egs/mgb2_arabic/s5 2.4. Zero-shot CS framework In this case study we focus on generating Arabic-English code- switching (CS) data, operating under the assumption that no Arabic- English CS training data is available. To generate speech data using the Speech Collage method, we require CS text. We generate the CS text from monolingual resources using the lexicon-based (Random) replacements approach described in [13]. The approach entails the following steps: 1. Parallel Text Translation: We leverage a public Arabic- English Machine Translation System3 to generate the parallel English text from the Arabic transcription. 2. Word Level Alignments: After translation, we fine-tune multilingual BERT (mBERT) [30] to obtain the word-level alignments. 3. Random Replacement: Given the alignments, Arabic words are randomly substituted with their corresponding English words at a rate of 20%, as suggested by [13]. 2.5. End-to-End Speech Recognition In this work, we utilized the end-to-end (E2E) ASR conformer architecture [31], with the ESPNET toolkit [32]. The E2E-ASR implementation consists of a conformer encoder and a transformer decoder. Both are multiblocked self-attention architectures with the encoder further enhanced by an additional convolution module. The ASR task is formulated as Bayesian decision finding the most probable target word sequence \u02c6Y, from all possible outputs Y\u2217, by selecting the sequence which maximizes the posterior likelihood P (Y|X), given T-length sequence of D-dimensional speech fea- tures, X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. For text tokenization, we used word-piece byte-pair-encoding [33]. The total loss function Lasr is a multi-task learning objective that combines the decoder cross-entropy (CE) loss Lce and the CTC loss [34] Lctc. Lasr = \u03b1Lctc + (1 \u2212 \u03b1)Lce where \u03b1 is used for interpolation. In our approach, the conformer is initially pre-trained on monolingual data and subsequently fine- tuned on monolingual and synthetic CS speech combined. 2.6. Code-Mixing Index To quantify the amount of code-switching we use Code-Mixing In- dex (CMI) metric [35]. The CMI for an utterance is defined as: CM I = 2 \u2217 (N \u2212 maxi) + 1 1 N 2 P (x) Where maxi represents the number of words in the dominant lan- guage i, N is the total word count in the utterance, P is the number of code alternation points, with the constraint 0 \u2264 P < N . A low CMI score indicates monolingualism in the text whereas the high CMI score implies high degree of code-mixing in the text. 3. DATA AND EXPERIMENTAL SETUP In-domain: The target domain we are considering is the Mandarin- English code-switching, specifically SEAME [36]. In this scenario, we utilize monolingual training data from Chinese AISHELL-1 [37], 100h of English data randomly sampled from Tedlium3 [38] and 3API access available from https://mt.qcri.org/api (2) (3) SEAME text [36] to generate 62.2 hours of CS data. Evaluation is performed on SEAME test sets (devman and devsge), measur- ing mixed error-rate (MER) that considers word-level English and character-level Mandarin. We also report WER on monolingual En- glish and CER on monolingual Chinese subsets. Zero-shot: For this scenario, we use monolingual training data from MGB-2 [39] and Tedlium3. We generate 80 hours of CS data using synthetic CS text described in \u00a72.4. Evaluation is conducted on ES- CWA [8], which is a real Arabic-English CS dataset. Data pre-processing: All audios are augmented with speed pertur- bations (0.9, 1.0 and 1.1) and transformed into 83-dimensional fea- ture frames (80 log-mel filterbank coefficients plus 3 pitch features). Additionally, we augment the features with specaugment, with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. Models: the conformer encoder consists of 12 blocks, each with 2048 feed-forward dimensions, 256 attention dimensions,"}, {"question": " How is a collection D of audio segments corresponding to each n-gram unit created?,answer: SETUPSUPERVISIONS(A, n) creates the collection D by using alignments from monolingual data and the maximum n-gram size.", "ref_chunk": "unit-segments by 0.05 seconds at the start and end of the segment. This extension provides an extra 0.05 sec- ond which is utilized as overlap in overlap-add process. 2.2. Energy normalization Additionally, we normalize the synthesized utterance by the average of unit-segments energy to remove artifacts introduced by energy variations between segments. For a speech sequence X of length T , X = {xt \u2208 R|t = 1, \u00b7 \u00b7 \u00b7 , T }, the average energy is calculated as follows: X \u2032 = \uf8f1 \uf8f2 \uf8f3 (cid:113) 1 T xt (cid:80) t x2 t |t = 1, \u00b7 \u00b7 \u00b7 , T \uf8fc \uf8fd \uf8fe (1) return D 5: 6: function NORMALIZEENERGY(audioCut) e \u2190 AVGENERGY(audioCut) 7: return audioCut / 8: 9: function SAMPLEUNIT(D, w1:i) return random cut from D[w1:i] 10: 11: function GENERATECOLLAGE(TCS, A, n) 12: 13: 14: 15: 16: 17: 18: 19: 20: \u25b7 Eq 2 \u221a e \u25b7 n: size of n-gram D \u2190 SETUPSUPERVISIONS(A, n) DCS \u2190 \u2205 for texti \u2208 TCS do W \u2190 GETCONSECUNITS(texti) for wi \u2208 W do cut \u2190 SAMPLEUNIT(D[wi]) cutsi \u2190 OVERLAPADD(cutsi,cut) cutsi \u2190 NORMALIZEENERGY(cutsi) cutsi.text \u2190 cutsi.text + wi \u25b7 Append wi 21: DCS.append(cutsi) 22: return DCS 2.3. N-gram units To further enhance the quality of the generated CS we explore splic- ing consecutive units (n-grams), in alignment with selecting longer units in concatenated speech synthesis [29] Given a CS sentence our approach starts by matching the largest consecutive unit from monolingual alignments. If a specific n-gram is unavailable, the al- gorithm backs off to a smaller unit. It\u2019s worth noting that in this study, we only experimented with unigrams and bigrams. A detailed 2https://github.com/kaldi-asr/kaldi/tree/master/ egs/aishell/s5 https://github.com/kaldi-asr/kaldi/tree/master/ description of n-gram Speech Collage implementation is described in Algorithm 1. Using the alignments from monolingual data and maximum n-gram size, SETUPSUPERVISIONS(\u00b7) creates a collec- tion D of audio segments corresponding to each n-gram unit. Con- secutive n-gram units are matched from alignments, starting with n and progressing to unigrams. If an n-gram is absent, the algo- rithm backs off to an (n \u2212 1) unit. In GENERATECOLLAGE(\u00b7), the function GETCONSECUNITS(\u00b7) returns all consecutive (1 : n) units. Each n-gram unit is randomly drawn from its respective col- lection using SAMPLEUNIT(\u00b7). These segments are appended to the current spliced utterance with OVERLAPADD(\u00b7) described in \u00a72.1, and the resulting combined utterance undergoes energy normaliza- tion NORMALIZEENERGY(\u00b7) from Eq1. egs/mgb2_arabic/s5 2.4. Zero-shot CS framework In this case study we focus on generating Arabic-English code- switching (CS) data, operating under the assumption that no Arabic- English CS training data is available. To generate speech data using the Speech Collage method, we require CS text. We generate the CS text from monolingual resources using the lexicon-based (Random) replacements approach described in [13]. The approach entails the following steps: 1. Parallel Text Translation: We leverage a public Arabic- English Machine Translation System3 to generate the parallel English text from the Arabic transcription. 2. Word Level Alignments: After translation, we fine-tune multilingual BERT (mBERT) [30] to obtain the word-level alignments. 3. Random Replacement: Given the alignments, Arabic words are randomly substituted with their corresponding English words at a rate of 20%, as suggested by [13]. 2.5. End-to-End Speech Recognition In this work, we utilized the end-to-end (E2E) ASR conformer architecture [31], with the ESPNET toolkit [32]. The E2E-ASR implementation consists of a conformer encoder and a transformer decoder. Both are multiblocked self-attention architectures with the encoder further enhanced by an additional convolution module. The ASR task is formulated as Bayesian decision finding the most probable target word sequence \u02c6Y, from all possible outputs Y\u2217, by selecting the sequence which maximizes the posterior likelihood P (Y|X), given T-length sequence of D-dimensional speech fea- tures, X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. For text tokenization, we used word-piece byte-pair-encoding [33]. The total loss function Lasr is a multi-task learning objective that combines the decoder cross-entropy (CE) loss Lce and the CTC loss [34] Lctc. Lasr = \u03b1Lctc + (1 \u2212 \u03b1)Lce where \u03b1 is used for interpolation. In our approach, the conformer is initially pre-trained on monolingual data and subsequently fine- tuned on monolingual and synthetic CS speech combined. 2.6. Code-Mixing Index To quantify the amount of code-switching we use Code-Mixing In- dex (CMI) metric [35]. The CMI for an utterance is defined as: CM I = 2 \u2217 (N \u2212 maxi) + 1 1 N 2 P (x) Where maxi represents the number of words in the dominant lan- guage i, N is the total word count in the utterance, P is the number of code alternation points, with the constraint 0 \u2264 P < N . A low CMI score indicates monolingualism in the text whereas the high CMI score implies high degree of code-mixing in the text. 3. DATA AND EXPERIMENTAL SETUP In-domain: The target domain we are considering is the Mandarin- English code-switching, specifically SEAME [36]. In this scenario, we utilize monolingual training data from Chinese AISHELL-1 [37], 100h of English data randomly sampled from Tedlium3 [38] and 3API access available from https://mt.qcri.org/api (2) (3) SEAME text [36] to generate 62.2 hours of CS data. Evaluation is performed on SEAME test sets (devman and devsge), measur- ing mixed error-rate (MER) that considers word-level English and character-level Mandarin. We also report WER on monolingual En- glish and CER on monolingual Chinese subsets. Zero-shot: For this scenario, we use monolingual training data from MGB-2 [39] and Tedlium3. We generate 80 hours of CS data using synthetic CS text described in \u00a72.4. Evaluation is conducted on ES- CWA [8], which is a real Arabic-English CS dataset. Data pre-processing: All audios are augmented with speed pertur- bations (0.9, 1.0 and 1.1) and transformed into 83-dimensional fea- ture frames (80 log-mel filterbank coefficients plus 3 pitch features). Additionally, we augment the features with specaugment, with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. Models: the conformer encoder consists of 12 blocks, each with 2048 feed-forward dimensions, 256 attention dimensions,"}, {"question": " What is the main objective of the zero-shot CS framework discussed in the text?,answer: The focus is on generating Arabic-English code-switching data without the availability of Arabic-English CS training data.", "ref_chunk": "unit-segments by 0.05 seconds at the start and end of the segment. This extension provides an extra 0.05 sec- ond which is utilized as overlap in overlap-add process. 2.2. Energy normalization Additionally, we normalize the synthesized utterance by the average of unit-segments energy to remove artifacts introduced by energy variations between segments. For a speech sequence X of length T , X = {xt \u2208 R|t = 1, \u00b7 \u00b7 \u00b7 , T }, the average energy is calculated as follows: X \u2032 = \uf8f1 \uf8f2 \uf8f3 (cid:113) 1 T xt (cid:80) t x2 t |t = 1, \u00b7 \u00b7 \u00b7 , T \uf8fc \uf8fd \uf8fe (1) return D 5: 6: function NORMALIZEENERGY(audioCut) e \u2190 AVGENERGY(audioCut) 7: return audioCut / 8: 9: function SAMPLEUNIT(D, w1:i) return random cut from D[w1:i] 10: 11: function GENERATECOLLAGE(TCS, A, n) 12: 13: 14: 15: 16: 17: 18: 19: 20: \u25b7 Eq 2 \u221a e \u25b7 n: size of n-gram D \u2190 SETUPSUPERVISIONS(A, n) DCS \u2190 \u2205 for texti \u2208 TCS do W \u2190 GETCONSECUNITS(texti) for wi \u2208 W do cut \u2190 SAMPLEUNIT(D[wi]) cutsi \u2190 OVERLAPADD(cutsi,cut) cutsi \u2190 NORMALIZEENERGY(cutsi) cutsi.text \u2190 cutsi.text + wi \u25b7 Append wi 21: DCS.append(cutsi) 22: return DCS 2.3. N-gram units To further enhance the quality of the generated CS we explore splic- ing consecutive units (n-grams), in alignment with selecting longer units in concatenated speech synthesis [29] Given a CS sentence our approach starts by matching the largest consecutive unit from monolingual alignments. If a specific n-gram is unavailable, the al- gorithm backs off to a smaller unit. It\u2019s worth noting that in this study, we only experimented with unigrams and bigrams. A detailed 2https://github.com/kaldi-asr/kaldi/tree/master/ egs/aishell/s5 https://github.com/kaldi-asr/kaldi/tree/master/ description of n-gram Speech Collage implementation is described in Algorithm 1. Using the alignments from monolingual data and maximum n-gram size, SETUPSUPERVISIONS(\u00b7) creates a collec- tion D of audio segments corresponding to each n-gram unit. Con- secutive n-gram units are matched from alignments, starting with n and progressing to unigrams. If an n-gram is absent, the algo- rithm backs off to an (n \u2212 1) unit. In GENERATECOLLAGE(\u00b7), the function GETCONSECUNITS(\u00b7) returns all consecutive (1 : n) units. Each n-gram unit is randomly drawn from its respective col- lection using SAMPLEUNIT(\u00b7). These segments are appended to the current spliced utterance with OVERLAPADD(\u00b7) described in \u00a72.1, and the resulting combined utterance undergoes energy normaliza- tion NORMALIZEENERGY(\u00b7) from Eq1. egs/mgb2_arabic/s5 2.4. Zero-shot CS framework In this case study we focus on generating Arabic-English code- switching (CS) data, operating under the assumption that no Arabic- English CS training data is available. To generate speech data using the Speech Collage method, we require CS text. We generate the CS text from monolingual resources using the lexicon-based (Random) replacements approach described in [13]. The approach entails the following steps: 1. Parallel Text Translation: We leverage a public Arabic- English Machine Translation System3 to generate the parallel English text from the Arabic transcription. 2. Word Level Alignments: After translation, we fine-tune multilingual BERT (mBERT) [30] to obtain the word-level alignments. 3. Random Replacement: Given the alignments, Arabic words are randomly substituted with their corresponding English words at a rate of 20%, as suggested by [13]. 2.5. End-to-End Speech Recognition In this work, we utilized the end-to-end (E2E) ASR conformer architecture [31], with the ESPNET toolkit [32]. The E2E-ASR implementation consists of a conformer encoder and a transformer decoder. Both are multiblocked self-attention architectures with the encoder further enhanced by an additional convolution module. The ASR task is formulated as Bayesian decision finding the most probable target word sequence \u02c6Y, from all possible outputs Y\u2217, by selecting the sequence which maximizes the posterior likelihood P (Y|X), given T-length sequence of D-dimensional speech fea- tures, X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. For text tokenization, we used word-piece byte-pair-encoding [33]. The total loss function Lasr is a multi-task learning objective that combines the decoder cross-entropy (CE) loss Lce and the CTC loss [34] Lctc. Lasr = \u03b1Lctc + (1 \u2212 \u03b1)Lce where \u03b1 is used for interpolation. In our approach, the conformer is initially pre-trained on monolingual data and subsequently fine- tuned on monolingual and synthetic CS speech combined. 2.6. Code-Mixing Index To quantify the amount of code-switching we use Code-Mixing In- dex (CMI) metric [35]. The CMI for an utterance is defined as: CM I = 2 \u2217 (N \u2212 maxi) + 1 1 N 2 P (x) Where maxi represents the number of words in the dominant lan- guage i, N is the total word count in the utterance, P is the number of code alternation points, with the constraint 0 \u2264 P < N . A low CMI score indicates monolingualism in the text whereas the high CMI score implies high degree of code-mixing in the text. 3. DATA AND EXPERIMENTAL SETUP In-domain: The target domain we are considering is the Mandarin- English code-switching, specifically SEAME [36]. In this scenario, we utilize monolingual training data from Chinese AISHELL-1 [37], 100h of English data randomly sampled from Tedlium3 [38] and 3API access available from https://mt.qcri.org/api (2) (3) SEAME text [36] to generate 62.2 hours of CS data. Evaluation is performed on SEAME test sets (devman and devsge), measur- ing mixed error-rate (MER) that considers word-level English and character-level Mandarin. We also report WER on monolingual En- glish and CER on monolingual Chinese subsets. Zero-shot: For this scenario, we use monolingual training data from MGB-2 [39] and Tedlium3. We generate 80 hours of CS data using synthetic CS text described in \u00a72.4. Evaluation is conducted on ES- CWA [8], which is a real Arabic-English CS dataset. Data pre-processing: All audios are augmented with speed pertur- bations (0.9, 1.0 and 1.1) and transformed into 83-dimensional fea- ture frames (80 log-mel filterbank coefficients plus 3 pitch features). Additionally, we augment the features with specaugment, with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. Models: the conformer encoder consists of 12 blocks, each with 2048 feed-forward dimensions, 256 attention dimensions,"}, {"question": " What is the architecture utilized for end-to-end (E2E) ASR in the text?,answer: The E2E ASR architecture consists of a conformer encoder and a transformer decoder, both being multi-blocked self-attention architectures.", "ref_chunk": "unit-segments by 0.05 seconds at the start and end of the segment. This extension provides an extra 0.05 sec- ond which is utilized as overlap in overlap-add process. 2.2. Energy normalization Additionally, we normalize the synthesized utterance by the average of unit-segments energy to remove artifacts introduced by energy variations between segments. For a speech sequence X of length T , X = {xt \u2208 R|t = 1, \u00b7 \u00b7 \u00b7 , T }, the average energy is calculated as follows: X \u2032 = \uf8f1 \uf8f2 \uf8f3 (cid:113) 1 T xt (cid:80) t x2 t |t = 1, \u00b7 \u00b7 \u00b7 , T \uf8fc \uf8fd \uf8fe (1) return D 5: 6: function NORMALIZEENERGY(audioCut) e \u2190 AVGENERGY(audioCut) 7: return audioCut / 8: 9: function SAMPLEUNIT(D, w1:i) return random cut from D[w1:i] 10: 11: function GENERATECOLLAGE(TCS, A, n) 12: 13: 14: 15: 16: 17: 18: 19: 20: \u25b7 Eq 2 \u221a e \u25b7 n: size of n-gram D \u2190 SETUPSUPERVISIONS(A, n) DCS \u2190 \u2205 for texti \u2208 TCS do W \u2190 GETCONSECUNITS(texti) for wi \u2208 W do cut \u2190 SAMPLEUNIT(D[wi]) cutsi \u2190 OVERLAPADD(cutsi,cut) cutsi \u2190 NORMALIZEENERGY(cutsi) cutsi.text \u2190 cutsi.text + wi \u25b7 Append wi 21: DCS.append(cutsi) 22: return DCS 2.3. N-gram units To further enhance the quality of the generated CS we explore splic- ing consecutive units (n-grams), in alignment with selecting longer units in concatenated speech synthesis [29] Given a CS sentence our approach starts by matching the largest consecutive unit from monolingual alignments. If a specific n-gram is unavailable, the al- gorithm backs off to a smaller unit. It\u2019s worth noting that in this study, we only experimented with unigrams and bigrams. A detailed 2https://github.com/kaldi-asr/kaldi/tree/master/ egs/aishell/s5 https://github.com/kaldi-asr/kaldi/tree/master/ description of n-gram Speech Collage implementation is described in Algorithm 1. Using the alignments from monolingual data and maximum n-gram size, SETUPSUPERVISIONS(\u00b7) creates a collec- tion D of audio segments corresponding to each n-gram unit. Con- secutive n-gram units are matched from alignments, starting with n and progressing to unigrams. If an n-gram is absent, the algo- rithm backs off to an (n \u2212 1) unit. In GENERATECOLLAGE(\u00b7), the function GETCONSECUNITS(\u00b7) returns all consecutive (1 : n) units. Each n-gram unit is randomly drawn from its respective col- lection using SAMPLEUNIT(\u00b7). These segments are appended to the current spliced utterance with OVERLAPADD(\u00b7) described in \u00a72.1, and the resulting combined utterance undergoes energy normaliza- tion NORMALIZEENERGY(\u00b7) from Eq1. egs/mgb2_arabic/s5 2.4. Zero-shot CS framework In this case study we focus on generating Arabic-English code- switching (CS) data, operating under the assumption that no Arabic- English CS training data is available. To generate speech data using the Speech Collage method, we require CS text. We generate the CS text from monolingual resources using the lexicon-based (Random) replacements approach described in [13]. The approach entails the following steps: 1. Parallel Text Translation: We leverage a public Arabic- English Machine Translation System3 to generate the parallel English text from the Arabic transcription. 2. Word Level Alignments: After translation, we fine-tune multilingual BERT (mBERT) [30] to obtain the word-level alignments. 3. Random Replacement: Given the alignments, Arabic words are randomly substituted with their corresponding English words at a rate of 20%, as suggested by [13]. 2.5. End-to-End Speech Recognition In this work, we utilized the end-to-end (E2E) ASR conformer architecture [31], with the ESPNET toolkit [32]. The E2E-ASR implementation consists of a conformer encoder and a transformer decoder. Both are multiblocked self-attention architectures with the encoder further enhanced by an additional convolution module. The ASR task is formulated as Bayesian decision finding the most probable target word sequence \u02c6Y, from all possible outputs Y\u2217, by selecting the sequence which maximizes the posterior likelihood P (Y|X), given T-length sequence of D-dimensional speech fea- tures, X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. For text tokenization, we used word-piece byte-pair-encoding [33]. The total loss function Lasr is a multi-task learning objective that combines the decoder cross-entropy (CE) loss Lce and the CTC loss [34] Lctc. Lasr = \u03b1Lctc + (1 \u2212 \u03b1)Lce where \u03b1 is used for interpolation. In our approach, the conformer is initially pre-trained on monolingual data and subsequently fine- tuned on monolingual and synthetic CS speech combined. 2.6. Code-Mixing Index To quantify the amount of code-switching we use Code-Mixing In- dex (CMI) metric [35]. The CMI for an utterance is defined as: CM I = 2 \u2217 (N \u2212 maxi) + 1 1 N 2 P (x) Where maxi represents the number of words in the dominant lan- guage i, N is the total word count in the utterance, P is the number of code alternation points, with the constraint 0 \u2264 P < N . A low CMI score indicates monolingualism in the text whereas the high CMI score implies high degree of code-mixing in the text. 3. DATA AND EXPERIMENTAL SETUP In-domain: The target domain we are considering is the Mandarin- English code-switching, specifically SEAME [36]. In this scenario, we utilize monolingual training data from Chinese AISHELL-1 [37], 100h of English data randomly sampled from Tedlium3 [38] and 3API access available from https://mt.qcri.org/api (2) (3) SEAME text [36] to generate 62.2 hours of CS data. Evaluation is performed on SEAME test sets (devman and devsge), measur- ing mixed error-rate (MER) that considers word-level English and character-level Mandarin. We also report WER on monolingual En- glish and CER on monolingual Chinese subsets. Zero-shot: For this scenario, we use monolingual training data from MGB-2 [39] and Tedlium3. We generate 80 hours of CS data using synthetic CS text described in \u00a72.4. Evaluation is conducted on ES- CWA [8], which is a real Arabic-English CS dataset. Data pre-processing: All audios are augmented with speed pertur- bations (0.9, 1.0 and 1.1) and transformed into 83-dimensional fea- ture frames (80 log-mel filterbank coefficients plus 3 pitch features). Additionally, we augment the features with specaugment, with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. Models: the conformer encoder consists of 12 blocks, each with 2048 feed-forward dimensions, 256 attention dimensions,"}, {"question": " How is the total loss function Lasr defined for the E2E ASR implementation?,answer: The total loss function combines the decoder cross-entropy loss Lce and the CTC loss Lctc, where Lasr = \u03b1Lctc + (1 \u2212 \u03b1)Lce.", "ref_chunk": "unit-segments by 0.05 seconds at the start and end of the segment. This extension provides an extra 0.05 sec- ond which is utilized as overlap in overlap-add process. 2.2. Energy normalization Additionally, we normalize the synthesized utterance by the average of unit-segments energy to remove artifacts introduced by energy variations between segments. For a speech sequence X of length T , X = {xt \u2208 R|t = 1, \u00b7 \u00b7 \u00b7 , T }, the average energy is calculated as follows: X \u2032 = \uf8f1 \uf8f2 \uf8f3 (cid:113) 1 T xt (cid:80) t x2 t |t = 1, \u00b7 \u00b7 \u00b7 , T \uf8fc \uf8fd \uf8fe (1) return D 5: 6: function NORMALIZEENERGY(audioCut) e \u2190 AVGENERGY(audioCut) 7: return audioCut / 8: 9: function SAMPLEUNIT(D, w1:i) return random cut from D[w1:i] 10: 11: function GENERATECOLLAGE(TCS, A, n) 12: 13: 14: 15: 16: 17: 18: 19: 20: \u25b7 Eq 2 \u221a e \u25b7 n: size of n-gram D \u2190 SETUPSUPERVISIONS(A, n) DCS \u2190 \u2205 for texti \u2208 TCS do W \u2190 GETCONSECUNITS(texti) for wi \u2208 W do cut \u2190 SAMPLEUNIT(D[wi]) cutsi \u2190 OVERLAPADD(cutsi,cut) cutsi \u2190 NORMALIZEENERGY(cutsi) cutsi.text \u2190 cutsi.text + wi \u25b7 Append wi 21: DCS.append(cutsi) 22: return DCS 2.3. N-gram units To further enhance the quality of the generated CS we explore splic- ing consecutive units (n-grams), in alignment with selecting longer units in concatenated speech synthesis [29] Given a CS sentence our approach starts by matching the largest consecutive unit from monolingual alignments. If a specific n-gram is unavailable, the al- gorithm backs off to a smaller unit. It\u2019s worth noting that in this study, we only experimented with unigrams and bigrams. A detailed 2https://github.com/kaldi-asr/kaldi/tree/master/ egs/aishell/s5 https://github.com/kaldi-asr/kaldi/tree/master/ description of n-gram Speech Collage implementation is described in Algorithm 1. Using the alignments from monolingual data and maximum n-gram size, SETUPSUPERVISIONS(\u00b7) creates a collec- tion D of audio segments corresponding to each n-gram unit. Con- secutive n-gram units are matched from alignments, starting with n and progressing to unigrams. If an n-gram is absent, the algo- rithm backs off to an (n \u2212 1) unit. In GENERATECOLLAGE(\u00b7), the function GETCONSECUNITS(\u00b7) returns all consecutive (1 : n) units. Each n-gram unit is randomly drawn from its respective col- lection using SAMPLEUNIT(\u00b7). These segments are appended to the current spliced utterance with OVERLAPADD(\u00b7) described in \u00a72.1, and the resulting combined utterance undergoes energy normaliza- tion NORMALIZEENERGY(\u00b7) from Eq1. egs/mgb2_arabic/s5 2.4. Zero-shot CS framework In this case study we focus on generating Arabic-English code- switching (CS) data, operating under the assumption that no Arabic- English CS training data is available. To generate speech data using the Speech Collage method, we require CS text. We generate the CS text from monolingual resources using the lexicon-based (Random) replacements approach described in [13]. The approach entails the following steps: 1. Parallel Text Translation: We leverage a public Arabic- English Machine Translation System3 to generate the parallel English text from the Arabic transcription. 2. Word Level Alignments: After translation, we fine-tune multilingual BERT (mBERT) [30] to obtain the word-level alignments. 3. Random Replacement: Given the alignments, Arabic words are randomly substituted with their corresponding English words at a rate of 20%, as suggested by [13]. 2.5. End-to-End Speech Recognition In this work, we utilized the end-to-end (E2E) ASR conformer architecture [31], with the ESPNET toolkit [32]. The E2E-ASR implementation consists of a conformer encoder and a transformer decoder. Both are multiblocked self-attention architectures with the encoder further enhanced by an additional convolution module. The ASR task is formulated as Bayesian decision finding the most probable target word sequence \u02c6Y, from all possible outputs Y\u2217, by selecting the sequence which maximizes the posterior likelihood P (Y|X), given T-length sequence of D-dimensional speech fea- tures, X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. For text tokenization, we used word-piece byte-pair-encoding [33]. The total loss function Lasr is a multi-task learning objective that combines the decoder cross-entropy (CE) loss Lce and the CTC loss [34] Lctc. Lasr = \u03b1Lctc + (1 \u2212 \u03b1)Lce where \u03b1 is used for interpolation. In our approach, the conformer is initially pre-trained on monolingual data and subsequently fine- tuned on monolingual and synthetic CS speech combined. 2.6. Code-Mixing Index To quantify the amount of code-switching we use Code-Mixing In- dex (CMI) metric [35]. The CMI for an utterance is defined as: CM I = 2 \u2217 (N \u2212 maxi) + 1 1 N 2 P (x) Where maxi represents the number of words in the dominant lan- guage i, N is the total word count in the utterance, P is the number of code alternation points, with the constraint 0 \u2264 P < N . A low CMI score indicates monolingualism in the text whereas the high CMI score implies high degree of code-mixing in the text. 3. DATA AND EXPERIMENTAL SETUP In-domain: The target domain we are considering is the Mandarin- English code-switching, specifically SEAME [36]. In this scenario, we utilize monolingual training data from Chinese AISHELL-1 [37], 100h of English data randomly sampled from Tedlium3 [38] and 3API access available from https://mt.qcri.org/api (2) (3) SEAME text [36] to generate 62.2 hours of CS data. Evaluation is performed on SEAME test sets (devman and devsge), measur- ing mixed error-rate (MER) that considers word-level English and character-level Mandarin. We also report WER on monolingual En- glish and CER on monolingual Chinese subsets. Zero-shot: For this scenario, we use monolingual training data from MGB-2 [39] and Tedlium3. We generate 80 hours of CS data using synthetic CS text described in \u00a72.4. Evaluation is conducted on ES- CWA [8], which is a real Arabic-English CS dataset. Data pre-processing: All audios are augmented with speed pertur- bations (0.9, 1.0 and 1.1) and transformed into 83-dimensional fea- ture frames (80 log-mel filterbank coefficients plus 3 pitch features). Additionally, we augment the features with specaugment, with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. Models: the conformer encoder consists of 12 blocks, each with 2048 feed-forward dimensions, 256 attention dimensions,"}, {"question": " What metric is used to quantify the amount of code-switching in an utterance?,answer: The Code-Mixing Index (CMI) metric is used, with a low score indicating monolingualism and a high score implying a high degree of code-mixing.", "ref_chunk": "unit-segments by 0.05 seconds at the start and end of the segment. This extension provides an extra 0.05 sec- ond which is utilized as overlap in overlap-add process. 2.2. Energy normalization Additionally, we normalize the synthesized utterance by the average of unit-segments energy to remove artifacts introduced by energy variations between segments. For a speech sequence X of length T , X = {xt \u2208 R|t = 1, \u00b7 \u00b7 \u00b7 , T }, the average energy is calculated as follows: X \u2032 = \uf8f1 \uf8f2 \uf8f3 (cid:113) 1 T xt (cid:80) t x2 t |t = 1, \u00b7 \u00b7 \u00b7 , T \uf8fc \uf8fd \uf8fe (1) return D 5: 6: function NORMALIZEENERGY(audioCut) e \u2190 AVGENERGY(audioCut) 7: return audioCut / 8: 9: function SAMPLEUNIT(D, w1:i) return random cut from D[w1:i] 10: 11: function GENERATECOLLAGE(TCS, A, n) 12: 13: 14: 15: 16: 17: 18: 19: 20: \u25b7 Eq 2 \u221a e \u25b7 n: size of n-gram D \u2190 SETUPSUPERVISIONS(A, n) DCS \u2190 \u2205 for texti \u2208 TCS do W \u2190 GETCONSECUNITS(texti) for wi \u2208 W do cut \u2190 SAMPLEUNIT(D[wi]) cutsi \u2190 OVERLAPADD(cutsi,cut) cutsi \u2190 NORMALIZEENERGY(cutsi) cutsi.text \u2190 cutsi.text + wi \u25b7 Append wi 21: DCS.append(cutsi) 22: return DCS 2.3. N-gram units To further enhance the quality of the generated CS we explore splic- ing consecutive units (n-grams), in alignment with selecting longer units in concatenated speech synthesis [29] Given a CS sentence our approach starts by matching the largest consecutive unit from monolingual alignments. If a specific n-gram is unavailable, the al- gorithm backs off to a smaller unit. It\u2019s worth noting that in this study, we only experimented with unigrams and bigrams. A detailed 2https://github.com/kaldi-asr/kaldi/tree/master/ egs/aishell/s5 https://github.com/kaldi-asr/kaldi/tree/master/ description of n-gram Speech Collage implementation is described in Algorithm 1. Using the alignments from monolingual data and maximum n-gram size, SETUPSUPERVISIONS(\u00b7) creates a collec- tion D of audio segments corresponding to each n-gram unit. Con- secutive n-gram units are matched from alignments, starting with n and progressing to unigrams. If an n-gram is absent, the algo- rithm backs off to an (n \u2212 1) unit. In GENERATECOLLAGE(\u00b7), the function GETCONSECUNITS(\u00b7) returns all consecutive (1 : n) units. Each n-gram unit is randomly drawn from its respective col- lection using SAMPLEUNIT(\u00b7). These segments are appended to the current spliced utterance with OVERLAPADD(\u00b7) described in \u00a72.1, and the resulting combined utterance undergoes energy normaliza- tion NORMALIZEENERGY(\u00b7) from Eq1. egs/mgb2_arabic/s5 2.4. Zero-shot CS framework In this case study we focus on generating Arabic-English code- switching (CS) data, operating under the assumption that no Arabic- English CS training data is available. To generate speech data using the Speech Collage method, we require CS text. We generate the CS text from monolingual resources using the lexicon-based (Random) replacements approach described in [13]. The approach entails the following steps: 1. Parallel Text Translation: We leverage a public Arabic- English Machine Translation System3 to generate the parallel English text from the Arabic transcription. 2. Word Level Alignments: After translation, we fine-tune multilingual BERT (mBERT) [30] to obtain the word-level alignments. 3. Random Replacement: Given the alignments, Arabic words are randomly substituted with their corresponding English words at a rate of 20%, as suggested by [13]. 2.5. End-to-End Speech Recognition In this work, we utilized the end-to-end (E2E) ASR conformer architecture [31], with the ESPNET toolkit [32]. The E2E-ASR implementation consists of a conformer encoder and a transformer decoder. Both are multiblocked self-attention architectures with the encoder further enhanced by an additional convolution module. The ASR task is formulated as Bayesian decision finding the most probable target word sequence \u02c6Y, from all possible outputs Y\u2217, by selecting the sequence which maximizes the posterior likelihood P (Y|X), given T-length sequence of D-dimensional speech fea- tures, X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. For text tokenization, we used word-piece byte-pair-encoding [33]. The total loss function Lasr is a multi-task learning objective that combines the decoder cross-entropy (CE) loss Lce and the CTC loss [34] Lctc. Lasr = \u03b1Lctc + (1 \u2212 \u03b1)Lce where \u03b1 is used for interpolation. In our approach, the conformer is initially pre-trained on monolingual data and subsequently fine- tuned on monolingual and synthetic CS speech combined. 2.6. Code-Mixing Index To quantify the amount of code-switching we use Code-Mixing In- dex (CMI) metric [35]. The CMI for an utterance is defined as: CM I = 2 \u2217 (N \u2212 maxi) + 1 1 N 2 P (x) Where maxi represents the number of words in the dominant lan- guage i, N is the total word count in the utterance, P is the number of code alternation points, with the constraint 0 \u2264 P < N . A low CMI score indicates monolingualism in the text whereas the high CMI score implies high degree of code-mixing in the text. 3. DATA AND EXPERIMENTAL SETUP In-domain: The target domain we are considering is the Mandarin- English code-switching, specifically SEAME [36]. In this scenario, we utilize monolingual training data from Chinese AISHELL-1 [37], 100h of English data randomly sampled from Tedlium3 [38] and 3API access available from https://mt.qcri.org/api (2) (3) SEAME text [36] to generate 62.2 hours of CS data. Evaluation is performed on SEAME test sets (devman and devsge), measur- ing mixed error-rate (MER) that considers word-level English and character-level Mandarin. We also report WER on monolingual En- glish and CER on monolingual Chinese subsets. Zero-shot: For this scenario, we use monolingual training data from MGB-2 [39] and Tedlium3. We generate 80 hours of CS data using synthetic CS text described in \u00a72.4. Evaluation is conducted on ES- CWA [8], which is a real Arabic-English CS dataset. Data pre-processing: All audios are augmented with speed pertur- bations (0.9, 1.0 and 1.1) and transformed into 83-dimensional fea- ture frames (80 log-mel filterbank coefficients plus 3 pitch features). Additionally, we augment the features with specaugment, with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. Models: the conformer encoder consists of 12 blocks, each with 2048 feed-forward dimensions, 256 attention dimensions,"}], "doc_text": "unit-segments by 0.05 seconds at the start and end of the segment. This extension provides an extra 0.05 sec- ond which is utilized as overlap in overlap-add process. 2.2. Energy normalization Additionally, we normalize the synthesized utterance by the average of unit-segments energy to remove artifacts introduced by energy variations between segments. For a speech sequence X of length T , X = {xt \u2208 R|t = 1, \u00b7 \u00b7 \u00b7 , T }, the average energy is calculated as follows: X \u2032 = \uf8f1 \uf8f2 \uf8f3 (cid:113) 1 T xt (cid:80) t x2 t |t = 1, \u00b7 \u00b7 \u00b7 , T \uf8fc \uf8fd \uf8fe (1) return D 5: 6: function NORMALIZEENERGY(audioCut) e \u2190 AVGENERGY(audioCut) 7: return audioCut / 8: 9: function SAMPLEUNIT(D, w1:i) return random cut from D[w1:i] 10: 11: function GENERATECOLLAGE(TCS, A, n) 12: 13: 14: 15: 16: 17: 18: 19: 20: \u25b7 Eq 2 \u221a e \u25b7 n: size of n-gram D \u2190 SETUPSUPERVISIONS(A, n) DCS \u2190 \u2205 for texti \u2208 TCS do W \u2190 GETCONSECUNITS(texti) for wi \u2208 W do cut \u2190 SAMPLEUNIT(D[wi]) cutsi \u2190 OVERLAPADD(cutsi,cut) cutsi \u2190 NORMALIZEENERGY(cutsi) cutsi.text \u2190 cutsi.text + wi \u25b7 Append wi 21: DCS.append(cutsi) 22: return DCS 2.3. N-gram units To further enhance the quality of the generated CS we explore splic- ing consecutive units (n-grams), in alignment with selecting longer units in concatenated speech synthesis [29] Given a CS sentence our approach starts by matching the largest consecutive unit from monolingual alignments. If a specific n-gram is unavailable, the al- gorithm backs off to a smaller unit. It\u2019s worth noting that in this study, we only experimented with unigrams and bigrams. A detailed 2https://github.com/kaldi-asr/kaldi/tree/master/ egs/aishell/s5 https://github.com/kaldi-asr/kaldi/tree/master/ description of n-gram Speech Collage implementation is described in Algorithm 1. Using the alignments from monolingual data and maximum n-gram size, SETUPSUPERVISIONS(\u00b7) creates a collec- tion D of audio segments corresponding to each n-gram unit. Con- secutive n-gram units are matched from alignments, starting with n and progressing to unigrams. If an n-gram is absent, the algo- rithm backs off to an (n \u2212 1) unit. In GENERATECOLLAGE(\u00b7), the function GETCONSECUNITS(\u00b7) returns all consecutive (1 : n) units. Each n-gram unit is randomly drawn from its respective col- lection using SAMPLEUNIT(\u00b7). These segments are appended to the current spliced utterance with OVERLAPADD(\u00b7) described in \u00a72.1, and the resulting combined utterance undergoes energy normaliza- tion NORMALIZEENERGY(\u00b7) from Eq1. egs/mgb2_arabic/s5 2.4. Zero-shot CS framework In this case study we focus on generating Arabic-English code- switching (CS) data, operating under the assumption that no Arabic- English CS training data is available. To generate speech data using the Speech Collage method, we require CS text. We generate the CS text from monolingual resources using the lexicon-based (Random) replacements approach described in [13]. The approach entails the following steps: 1. Parallel Text Translation: We leverage a public Arabic- English Machine Translation System3 to generate the parallel English text from the Arabic transcription. 2. Word Level Alignments: After translation, we fine-tune multilingual BERT (mBERT) [30] to obtain the word-level alignments. 3. Random Replacement: Given the alignments, Arabic words are randomly substituted with their corresponding English words at a rate of 20%, as suggested by [13]. 2.5. End-to-End Speech Recognition In this work, we utilized the end-to-end (E2E) ASR conformer architecture [31], with the ESPNET toolkit [32]. The E2E-ASR implementation consists of a conformer encoder and a transformer decoder. Both are multiblocked self-attention architectures with the encoder further enhanced by an additional convolution module. The ASR task is formulated as Bayesian decision finding the most probable target word sequence \u02c6Y, from all possible outputs Y\u2217, by selecting the sequence which maximizes the posterior likelihood P (Y|X), given T-length sequence of D-dimensional speech fea- tures, X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T }. For text tokenization, we used word-piece byte-pair-encoding [33]. The total loss function Lasr is a multi-task learning objective that combines the decoder cross-entropy (CE) loss Lce and the CTC loss [34] Lctc. Lasr = \u03b1Lctc + (1 \u2212 \u03b1)Lce where \u03b1 is used for interpolation. In our approach, the conformer is initially pre-trained on monolingual data and subsequently fine- tuned on monolingual and synthetic CS speech combined. 2.6. Code-Mixing Index To quantify the amount of code-switching we use Code-Mixing In- dex (CMI) metric [35]. The CMI for an utterance is defined as: CM I = 2 \u2217 (N \u2212 maxi) + 1 1 N 2 P (x) Where maxi represents the number of words in the dominant lan- guage i, N is the total word count in the utterance, P is the number of code alternation points, with the constraint 0 \u2264 P < N . A low CMI score indicates monolingualism in the text whereas the high CMI score implies high degree of code-mixing in the text. 3. DATA AND EXPERIMENTAL SETUP In-domain: The target domain we are considering is the Mandarin- English code-switching, specifically SEAME [36]. In this scenario, we utilize monolingual training data from Chinese AISHELL-1 [37], 100h of English data randomly sampled from Tedlium3 [38] and 3API access available from https://mt.qcri.org/api (2) (3) SEAME text [36] to generate 62.2 hours of CS data. Evaluation is performed on SEAME test sets (devman and devsge), measur- ing mixed error-rate (MER) that considers word-level English and character-level Mandarin. We also report WER on monolingual En- glish and CER on monolingual Chinese subsets. Zero-shot: For this scenario, we use monolingual training data from MGB-2 [39] and Tedlium3. We generate 80 hours of CS data using synthetic CS text described in \u00a72.4. Evaluation is conducted on ES- CWA [8], which is a real Arabic-English CS dataset. Data pre-processing: All audios are augmented with speed pertur- bations (0.9, 1.0 and 1.1) and transformed into 83-dimensional fea- ture frames (80 log-mel filterbank coefficients plus 3 pitch features). Additionally, we augment the features with specaugment, with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. Models: the conformer encoder consists of 12 blocks, each with 2048 feed-forward dimensions, 256 attention dimensions,"}