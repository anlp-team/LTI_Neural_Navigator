{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/David_R._Mortensen_Construction_Grammar_Provides_Unique_Insight_into_Neural_Language_Models_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " According to CxG, how is meaning encoded?", "answer": " In abstract constellations of linguistic units of different sizes.", "ref_chunk": "objective of language modelling to mean that LMs should necessarily achieve their goal the same way that humans do. Therefore, we do not argue that lan- guage models need to \u201cthink\u201d in terms of construc- tions because humans do. Rather, we consider con- structions an inherent property of human language, which makes it necessary for language models to understand them. There has recently been growing interest in devel- oping probing approaches for PLMs based on CxG. We see these approaches as coming from two differ- ent motivational standpoints, summarised below. 2.1 Constructions are Essential for Language Modelling According to CxG, meaning is encoded in abstract constellations of linguistic units of different sizes. This means that LMs, which the \ufb01eld of NLP is trying to develop to achieve human language com- petency, must also be able to assign meaning to these units to be full LMs. Their ability to assign meaning to words, or more speci\ufb01cally to subword units which are sometimes closer to morphemes than to words, has been shown at length (Wiede- mann et al., 2019; Reif et al., 2019; Schwartz et al., 2022). The question therefore remains: are PLMs able to retrieve and use meanings associated with patterns involving multiple tokens? We do not take this to only mean contiguous, \ufb01xed expressions, but much more importantly, non-contiguous patterns with slots that have varying constraints placed on them. To imitate and match human language be- haviour, models of human language need to learn how to recognise these patterns, retrieve their mean- ing, apply this meaning to the context, and use them when producing language. Simply put, there is no way around learning constructions if LMs are to advance. In addition, we believe that it is an in- dependently interesting question whether existing PLMs pick up on these abstract patterns using the current architectures and training setups, and if not, which change in architecture would be necessary to facilitate this. 2.2 Importance in Downstream Tasks Regardless of more fundamental questions about the long-term goals of LMs, we also \ufb01rmly be- lieve that probing for CxG is relevant for analysing Lang Reference Translation DeepL Translation German Italian Turkish Sie nieste den Schaum von ihrem Cappuccino runter. Lei ha starnutito via la schiuma dal suo cappuccino. Cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc hap\u00b8s\u0131rd\u0131. Sie nieste den Schaum von ihrem Cappuccino. Starnut\u00ec la schiuma del suo cappuccino. Hap\u00b8s\u0131rarak cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc u\u00e7urdu. Table 2: Translations of \u2018She sneezed the foam off her cappuccino.\u2019 given by DeepL1. Translated back to English by humans, they all mean \u201cShe sneezed her cappuccino\u2019s foam.\u201d, which does not correctly convey the resultative meaning component, i.e., that the foam is removed from the cappuccino by the sneeze (as opposed to put there). the challenges that face applied NLP, as evaluated on downstream tasks, at this point in time. Dis- cussion is increasingly focusing on diagnosing the speci\ufb01c scenarios that are challenging for current models. Srivastava et al. (2022) propose test suites that are designed to challenge LMs, and many of them are designed by looking for \u2018patterns\u2019 with a non-obvious, non-literal meaning that is more than the sum of the involved words. One example of such a failure can be found in Table 2, where we provide the DeepL1 translations for the famous instance of the caused-motion construction (Gold- berg, 1995, CMC;): \u2018She sneezed the foam off her cappuccino\u2019, where the unusual factor is that sneeze does not usually take a patient argument or cause a motion. For translation, this means that it either has to use the corresponding CMC in the target language, which might be quite different in form from the English CMC, or paraphrase in a way that conveys all meaning facets. For the languages we tested, DeepL did not achieve this: the resulting sentence sounds more like the foam was sneezed onto the cappuccino, or is ambiguous between this and the correct translation. Interestingly, for Rus- sian, the motion is conveyed in the translation, but not the fact that it is caused by a sneeze. Targeted adversarial test suites like this transla- tion example can be a useful resource to evaluate how well LMs perform on constructions, but more crucially, CxG theory and probing methods will inform the design of better and more systematic test suites, which in turn will be used to improve LMs (\u00a75.4). of human language processing. However, the dis- cussion about whether LMs are ready to be used as cognitive models is dominated by results of prob- ing studies based on Generative Grammar (GG), or more speci\ufb01cally Transformational Grammar. This means that GG is being used as the gold standard against which the cognitive plausibility of LMs is evaluated. Studies using GG assume a direct relationship between the models\u2019 performance on probing tasks and their linguistic competency. In- creased performance on GG probing tasks is seen as a sign it is becoming more reasonable to use LMs as cognitive models. Another linguistic rea- son for theoretical diversity is that if we could show that LMs conform better to CxG rather than GG, this might open up interesting discussions if they ever start being used as cognitive models. 3 Established Probing Methods Are Only Applicable to Some Aspects of CxG Established probing methods have focused on dif- ferent aspects of the syntactic and semantic knowl- edge of PLMs. In this section, we summarise the major approaches that were not designed specif- ically with constructions in mind. We show that although each of these methodologies deals with some aspect of CxG, and might even fully inves- tigate some simpler constructions, none of them fully covers constructional knowledge as de\ufb01ned in Section 1.1. 3.1 Probing Using Contextual Embeddings 2.3 Diversity in Linguistics for NLP Discussions about PLMs as models of human lan- guage processing have recently gained popularity. One forum for such discussions is the Neural Nets for Cognition Discussion Group at CogSci20222. The work is still very tentative, and most people agree that LMs are not ready to be used as models 1https://www.deepl.com/translator 2http://neural-nets-for-cognition.net Various probing studies (Garcia"}, {"question": " Why is it necessary for language models to understand constructions?", "answer": " Because constructions are an inherent property of human language.", "ref_chunk": "objective of language modelling to mean that LMs should necessarily achieve their goal the same way that humans do. Therefore, we do not argue that lan- guage models need to \u201cthink\u201d in terms of construc- tions because humans do. Rather, we consider con- structions an inherent property of human language, which makes it necessary for language models to understand them. There has recently been growing interest in devel- oping probing approaches for PLMs based on CxG. We see these approaches as coming from two differ- ent motivational standpoints, summarised below. 2.1 Constructions are Essential for Language Modelling According to CxG, meaning is encoded in abstract constellations of linguistic units of different sizes. This means that LMs, which the \ufb01eld of NLP is trying to develop to achieve human language com- petency, must also be able to assign meaning to these units to be full LMs. Their ability to assign meaning to words, or more speci\ufb01cally to subword units which are sometimes closer to morphemes than to words, has been shown at length (Wiede- mann et al., 2019; Reif et al., 2019; Schwartz et al., 2022). The question therefore remains: are PLMs able to retrieve and use meanings associated with patterns involving multiple tokens? We do not take this to only mean contiguous, \ufb01xed expressions, but much more importantly, non-contiguous patterns with slots that have varying constraints placed on them. To imitate and match human language be- haviour, models of human language need to learn how to recognise these patterns, retrieve their mean- ing, apply this meaning to the context, and use them when producing language. Simply put, there is no way around learning constructions if LMs are to advance. In addition, we believe that it is an in- dependently interesting question whether existing PLMs pick up on these abstract patterns using the current architectures and training setups, and if not, which change in architecture would be necessary to facilitate this. 2.2 Importance in Downstream Tasks Regardless of more fundamental questions about the long-term goals of LMs, we also \ufb01rmly be- lieve that probing for CxG is relevant for analysing Lang Reference Translation DeepL Translation German Italian Turkish Sie nieste den Schaum von ihrem Cappuccino runter. Lei ha starnutito via la schiuma dal suo cappuccino. Cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc hap\u00b8s\u0131rd\u0131. Sie nieste den Schaum von ihrem Cappuccino. Starnut\u00ec la schiuma del suo cappuccino. Hap\u00b8s\u0131rarak cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc u\u00e7urdu. Table 2: Translations of \u2018She sneezed the foam off her cappuccino.\u2019 given by DeepL1. Translated back to English by humans, they all mean \u201cShe sneezed her cappuccino\u2019s foam.\u201d, which does not correctly convey the resultative meaning component, i.e., that the foam is removed from the cappuccino by the sneeze (as opposed to put there). the challenges that face applied NLP, as evaluated on downstream tasks, at this point in time. Dis- cussion is increasingly focusing on diagnosing the speci\ufb01c scenarios that are challenging for current models. Srivastava et al. (2022) propose test suites that are designed to challenge LMs, and many of them are designed by looking for \u2018patterns\u2019 with a non-obvious, non-literal meaning that is more than the sum of the involved words. One example of such a failure can be found in Table 2, where we provide the DeepL1 translations for the famous instance of the caused-motion construction (Gold- berg, 1995, CMC;): \u2018She sneezed the foam off her cappuccino\u2019, where the unusual factor is that sneeze does not usually take a patient argument or cause a motion. For translation, this means that it either has to use the corresponding CMC in the target language, which might be quite different in form from the English CMC, or paraphrase in a way that conveys all meaning facets. For the languages we tested, DeepL did not achieve this: the resulting sentence sounds more like the foam was sneezed onto the cappuccino, or is ambiguous between this and the correct translation. Interestingly, for Rus- sian, the motion is conveyed in the translation, but not the fact that it is caused by a sneeze. Targeted adversarial test suites like this transla- tion example can be a useful resource to evaluate how well LMs perform on constructions, but more crucially, CxG theory and probing methods will inform the design of better and more systematic test suites, which in turn will be used to improve LMs (\u00a75.4). of human language processing. However, the dis- cussion about whether LMs are ready to be used as cognitive models is dominated by results of prob- ing studies based on Generative Grammar (GG), or more speci\ufb01cally Transformational Grammar. This means that GG is being used as the gold standard against which the cognitive plausibility of LMs is evaluated. Studies using GG assume a direct relationship between the models\u2019 performance on probing tasks and their linguistic competency. In- creased performance on GG probing tasks is seen as a sign it is becoming more reasonable to use LMs as cognitive models. Another linguistic rea- son for theoretical diversity is that if we could show that LMs conform better to CxG rather than GG, this might open up interesting discussions if they ever start being used as cognitive models. 3 Established Probing Methods Are Only Applicable to Some Aspects of CxG Established probing methods have focused on dif- ferent aspects of the syntactic and semantic knowl- edge of PLMs. In this section, we summarise the major approaches that were not designed specif- ically with constructions in mind. We show that although each of these methodologies deals with some aspect of CxG, and might even fully inves- tigate some simpler constructions, none of them fully covers constructional knowledge as de\ufb01ned in Section 1.1. 3.1 Probing Using Contextual Embeddings 2.3 Diversity in Linguistics for NLP Discussions about PLMs as models of human lan- guage processing have recently gained popularity. One forum for such discussions is the Neural Nets for Cognition Discussion Group at CogSci20222. The work is still very tentative, and most people agree that LMs are not ready to be used as models 1https://www.deepl.com/translator 2http://neural-nets-for-cognition.net Various probing studies (Garcia"}, {"question": " What is the significance of being able to assign meaning to subword units for language models?", "answer": " It is necessary for language models to be able to achieve human language competency.", "ref_chunk": "objective of language modelling to mean that LMs should necessarily achieve their goal the same way that humans do. Therefore, we do not argue that lan- guage models need to \u201cthink\u201d in terms of construc- tions because humans do. Rather, we consider con- structions an inherent property of human language, which makes it necessary for language models to understand them. There has recently been growing interest in devel- oping probing approaches for PLMs based on CxG. We see these approaches as coming from two differ- ent motivational standpoints, summarised below. 2.1 Constructions are Essential for Language Modelling According to CxG, meaning is encoded in abstract constellations of linguistic units of different sizes. This means that LMs, which the \ufb01eld of NLP is trying to develop to achieve human language com- petency, must also be able to assign meaning to these units to be full LMs. Their ability to assign meaning to words, or more speci\ufb01cally to subword units which are sometimes closer to morphemes than to words, has been shown at length (Wiede- mann et al., 2019; Reif et al., 2019; Schwartz et al., 2022). The question therefore remains: are PLMs able to retrieve and use meanings associated with patterns involving multiple tokens? We do not take this to only mean contiguous, \ufb01xed expressions, but much more importantly, non-contiguous patterns with slots that have varying constraints placed on them. To imitate and match human language be- haviour, models of human language need to learn how to recognise these patterns, retrieve their mean- ing, apply this meaning to the context, and use them when producing language. Simply put, there is no way around learning constructions if LMs are to advance. In addition, we believe that it is an in- dependently interesting question whether existing PLMs pick up on these abstract patterns using the current architectures and training setups, and if not, which change in architecture would be necessary to facilitate this. 2.2 Importance in Downstream Tasks Regardless of more fundamental questions about the long-term goals of LMs, we also \ufb01rmly be- lieve that probing for CxG is relevant for analysing Lang Reference Translation DeepL Translation German Italian Turkish Sie nieste den Schaum von ihrem Cappuccino runter. Lei ha starnutito via la schiuma dal suo cappuccino. Cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc hap\u00b8s\u0131rd\u0131. Sie nieste den Schaum von ihrem Cappuccino. Starnut\u00ec la schiuma del suo cappuccino. Hap\u00b8s\u0131rarak cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc u\u00e7urdu. Table 2: Translations of \u2018She sneezed the foam off her cappuccino.\u2019 given by DeepL1. Translated back to English by humans, they all mean \u201cShe sneezed her cappuccino\u2019s foam.\u201d, which does not correctly convey the resultative meaning component, i.e., that the foam is removed from the cappuccino by the sneeze (as opposed to put there). the challenges that face applied NLP, as evaluated on downstream tasks, at this point in time. Dis- cussion is increasingly focusing on diagnosing the speci\ufb01c scenarios that are challenging for current models. Srivastava et al. (2022) propose test suites that are designed to challenge LMs, and many of them are designed by looking for \u2018patterns\u2019 with a non-obvious, non-literal meaning that is more than the sum of the involved words. One example of such a failure can be found in Table 2, where we provide the DeepL1 translations for the famous instance of the caused-motion construction (Gold- berg, 1995, CMC;): \u2018She sneezed the foam off her cappuccino\u2019, where the unusual factor is that sneeze does not usually take a patient argument or cause a motion. For translation, this means that it either has to use the corresponding CMC in the target language, which might be quite different in form from the English CMC, or paraphrase in a way that conveys all meaning facets. For the languages we tested, DeepL did not achieve this: the resulting sentence sounds more like the foam was sneezed onto the cappuccino, or is ambiguous between this and the correct translation. Interestingly, for Rus- sian, the motion is conveyed in the translation, but not the fact that it is caused by a sneeze. Targeted adversarial test suites like this transla- tion example can be a useful resource to evaluate how well LMs perform on constructions, but more crucially, CxG theory and probing methods will inform the design of better and more systematic test suites, which in turn will be used to improve LMs (\u00a75.4). of human language processing. However, the dis- cussion about whether LMs are ready to be used as cognitive models is dominated by results of prob- ing studies based on Generative Grammar (GG), or more speci\ufb01cally Transformational Grammar. This means that GG is being used as the gold standard against which the cognitive plausibility of LMs is evaluated. Studies using GG assume a direct relationship between the models\u2019 performance on probing tasks and their linguistic competency. In- creased performance on GG probing tasks is seen as a sign it is becoming more reasonable to use LMs as cognitive models. Another linguistic rea- son for theoretical diversity is that if we could show that LMs conform better to CxG rather than GG, this might open up interesting discussions if they ever start being used as cognitive models. 3 Established Probing Methods Are Only Applicable to Some Aspects of CxG Established probing methods have focused on dif- ferent aspects of the syntactic and semantic knowl- edge of PLMs. In this section, we summarise the major approaches that were not designed specif- ically with constructions in mind. We show that although each of these methodologies deals with some aspect of CxG, and might even fully inves- tigate some simpler constructions, none of them fully covers constructional knowledge as de\ufb01ned in Section 1.1. 3.1 Probing Using Contextual Embeddings 2.3 Diversity in Linguistics for NLP Discussions about PLMs as models of human lan- guage processing have recently gained popularity. One forum for such discussions is the Neural Nets for Cognition Discussion Group at CogSci20222. The work is still very tentative, and most people agree that LMs are not ready to be used as models 1https://www.deepl.com/translator 2http://neural-nets-for-cognition.net Various probing studies (Garcia"}, {"question": " Why do models of human language need to learn how to recognize patterns?", "answer": " To imitate and match human language behavior.", "ref_chunk": "objective of language modelling to mean that LMs should necessarily achieve their goal the same way that humans do. Therefore, we do not argue that lan- guage models need to \u201cthink\u201d in terms of construc- tions because humans do. Rather, we consider con- structions an inherent property of human language, which makes it necessary for language models to understand them. There has recently been growing interest in devel- oping probing approaches for PLMs based on CxG. We see these approaches as coming from two differ- ent motivational standpoints, summarised below. 2.1 Constructions are Essential for Language Modelling According to CxG, meaning is encoded in abstract constellations of linguistic units of different sizes. This means that LMs, which the \ufb01eld of NLP is trying to develop to achieve human language com- petency, must also be able to assign meaning to these units to be full LMs. Their ability to assign meaning to words, or more speci\ufb01cally to subword units which are sometimes closer to morphemes than to words, has been shown at length (Wiede- mann et al., 2019; Reif et al., 2019; Schwartz et al., 2022). The question therefore remains: are PLMs able to retrieve and use meanings associated with patterns involving multiple tokens? We do not take this to only mean contiguous, \ufb01xed expressions, but much more importantly, non-contiguous patterns with slots that have varying constraints placed on them. To imitate and match human language be- haviour, models of human language need to learn how to recognise these patterns, retrieve their mean- ing, apply this meaning to the context, and use them when producing language. Simply put, there is no way around learning constructions if LMs are to advance. In addition, we believe that it is an in- dependently interesting question whether existing PLMs pick up on these abstract patterns using the current architectures and training setups, and if not, which change in architecture would be necessary to facilitate this. 2.2 Importance in Downstream Tasks Regardless of more fundamental questions about the long-term goals of LMs, we also \ufb01rmly be- lieve that probing for CxG is relevant for analysing Lang Reference Translation DeepL Translation German Italian Turkish Sie nieste den Schaum von ihrem Cappuccino runter. Lei ha starnutito via la schiuma dal suo cappuccino. Cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc hap\u00b8s\u0131rd\u0131. Sie nieste den Schaum von ihrem Cappuccino. Starnut\u00ec la schiuma del suo cappuccino. Hap\u00b8s\u0131rarak cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc u\u00e7urdu. Table 2: Translations of \u2018She sneezed the foam off her cappuccino.\u2019 given by DeepL1. Translated back to English by humans, they all mean \u201cShe sneezed her cappuccino\u2019s foam.\u201d, which does not correctly convey the resultative meaning component, i.e., that the foam is removed from the cappuccino by the sneeze (as opposed to put there). the challenges that face applied NLP, as evaluated on downstream tasks, at this point in time. Dis- cussion is increasingly focusing on diagnosing the speci\ufb01c scenarios that are challenging for current models. Srivastava et al. (2022) propose test suites that are designed to challenge LMs, and many of them are designed by looking for \u2018patterns\u2019 with a non-obvious, non-literal meaning that is more than the sum of the involved words. One example of such a failure can be found in Table 2, where we provide the DeepL1 translations for the famous instance of the caused-motion construction (Gold- berg, 1995, CMC;): \u2018She sneezed the foam off her cappuccino\u2019, where the unusual factor is that sneeze does not usually take a patient argument or cause a motion. For translation, this means that it either has to use the corresponding CMC in the target language, which might be quite different in form from the English CMC, or paraphrase in a way that conveys all meaning facets. For the languages we tested, DeepL did not achieve this: the resulting sentence sounds more like the foam was sneezed onto the cappuccino, or is ambiguous between this and the correct translation. Interestingly, for Rus- sian, the motion is conveyed in the translation, but not the fact that it is caused by a sneeze. Targeted adversarial test suites like this transla- tion example can be a useful resource to evaluate how well LMs perform on constructions, but more crucially, CxG theory and probing methods will inform the design of better and more systematic test suites, which in turn will be used to improve LMs (\u00a75.4). of human language processing. However, the dis- cussion about whether LMs are ready to be used as cognitive models is dominated by results of prob- ing studies based on Generative Grammar (GG), or more speci\ufb01cally Transformational Grammar. This means that GG is being used as the gold standard against which the cognitive plausibility of LMs is evaluated. Studies using GG assume a direct relationship between the models\u2019 performance on probing tasks and their linguistic competency. In- creased performance on GG probing tasks is seen as a sign it is becoming more reasonable to use LMs as cognitive models. Another linguistic rea- son for theoretical diversity is that if we could show that LMs conform better to CxG rather than GG, this might open up interesting discussions if they ever start being used as cognitive models. 3 Established Probing Methods Are Only Applicable to Some Aspects of CxG Established probing methods have focused on dif- ferent aspects of the syntactic and semantic knowl- edge of PLMs. In this section, we summarise the major approaches that were not designed specif- ically with constructions in mind. We show that although each of these methodologies deals with some aspect of CxG, and might even fully inves- tigate some simpler constructions, none of them fully covers constructional knowledge as de\ufb01ned in Section 1.1. 3.1 Probing Using Contextual Embeddings 2.3 Diversity in Linguistics for NLP Discussions about PLMs as models of human lan- guage processing have recently gained popularity. One forum for such discussions is the Neural Nets for Cognition Discussion Group at CogSci20222. The work is still very tentative, and most people agree that LMs are not ready to be used as models 1https://www.deepl.com/translator 2http://neural-nets-for-cognition.net Various probing studies (Garcia"}, {"question": " Why is it considered important for PLMs to learn constructions?", "answer": " There is no way around learning constructions if LMs are to advance.", "ref_chunk": "objective of language modelling to mean that LMs should necessarily achieve their goal the same way that humans do. Therefore, we do not argue that lan- guage models need to \u201cthink\u201d in terms of construc- tions because humans do. Rather, we consider con- structions an inherent property of human language, which makes it necessary for language models to understand them. There has recently been growing interest in devel- oping probing approaches for PLMs based on CxG. We see these approaches as coming from two differ- ent motivational standpoints, summarised below. 2.1 Constructions are Essential for Language Modelling According to CxG, meaning is encoded in abstract constellations of linguistic units of different sizes. This means that LMs, which the \ufb01eld of NLP is trying to develop to achieve human language com- petency, must also be able to assign meaning to these units to be full LMs. Their ability to assign meaning to words, or more speci\ufb01cally to subword units which are sometimes closer to morphemes than to words, has been shown at length (Wiede- mann et al., 2019; Reif et al., 2019; Schwartz et al., 2022). The question therefore remains: are PLMs able to retrieve and use meanings associated with patterns involving multiple tokens? We do not take this to only mean contiguous, \ufb01xed expressions, but much more importantly, non-contiguous patterns with slots that have varying constraints placed on them. To imitate and match human language be- haviour, models of human language need to learn how to recognise these patterns, retrieve their mean- ing, apply this meaning to the context, and use them when producing language. Simply put, there is no way around learning constructions if LMs are to advance. In addition, we believe that it is an in- dependently interesting question whether existing PLMs pick up on these abstract patterns using the current architectures and training setups, and if not, which change in architecture would be necessary to facilitate this. 2.2 Importance in Downstream Tasks Regardless of more fundamental questions about the long-term goals of LMs, we also \ufb01rmly be- lieve that probing for CxG is relevant for analysing Lang Reference Translation DeepL Translation German Italian Turkish Sie nieste den Schaum von ihrem Cappuccino runter. Lei ha starnutito via la schiuma dal suo cappuccino. Cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc hap\u00b8s\u0131rd\u0131. Sie nieste den Schaum von ihrem Cappuccino. Starnut\u00ec la schiuma del suo cappuccino. Hap\u00b8s\u0131rarak cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc u\u00e7urdu. Table 2: Translations of \u2018She sneezed the foam off her cappuccino.\u2019 given by DeepL1. Translated back to English by humans, they all mean \u201cShe sneezed her cappuccino\u2019s foam.\u201d, which does not correctly convey the resultative meaning component, i.e., that the foam is removed from the cappuccino by the sneeze (as opposed to put there). the challenges that face applied NLP, as evaluated on downstream tasks, at this point in time. Dis- cussion is increasingly focusing on diagnosing the speci\ufb01c scenarios that are challenging for current models. Srivastava et al. (2022) propose test suites that are designed to challenge LMs, and many of them are designed by looking for \u2018patterns\u2019 with a non-obvious, non-literal meaning that is more than the sum of the involved words. One example of such a failure can be found in Table 2, where we provide the DeepL1 translations for the famous instance of the caused-motion construction (Gold- berg, 1995, CMC;): \u2018She sneezed the foam off her cappuccino\u2019, where the unusual factor is that sneeze does not usually take a patient argument or cause a motion. For translation, this means that it either has to use the corresponding CMC in the target language, which might be quite different in form from the English CMC, or paraphrase in a way that conveys all meaning facets. For the languages we tested, DeepL did not achieve this: the resulting sentence sounds more like the foam was sneezed onto the cappuccino, or is ambiguous between this and the correct translation. Interestingly, for Rus- sian, the motion is conveyed in the translation, but not the fact that it is caused by a sneeze. Targeted adversarial test suites like this transla- tion example can be a useful resource to evaluate how well LMs perform on constructions, but more crucially, CxG theory and probing methods will inform the design of better and more systematic test suites, which in turn will be used to improve LMs (\u00a75.4). of human language processing. However, the dis- cussion about whether LMs are ready to be used as cognitive models is dominated by results of prob- ing studies based on Generative Grammar (GG), or more speci\ufb01cally Transformational Grammar. This means that GG is being used as the gold standard against which the cognitive plausibility of LMs is evaluated. Studies using GG assume a direct relationship between the models\u2019 performance on probing tasks and their linguistic competency. In- creased performance on GG probing tasks is seen as a sign it is becoming more reasonable to use LMs as cognitive models. Another linguistic rea- son for theoretical diversity is that if we could show that LMs conform better to CxG rather than GG, this might open up interesting discussions if they ever start being used as cognitive models. 3 Established Probing Methods Are Only Applicable to Some Aspects of CxG Established probing methods have focused on dif- ferent aspects of the syntactic and semantic knowl- edge of PLMs. In this section, we summarise the major approaches that were not designed specif- ically with constructions in mind. We show that although each of these methodologies deals with some aspect of CxG, and might even fully inves- tigate some simpler constructions, none of them fully covers constructional knowledge as de\ufb01ned in Section 1.1. 3.1 Probing Using Contextual Embeddings 2.3 Diversity in Linguistics for NLP Discussions about PLMs as models of human lan- guage processing have recently gained popularity. One forum for such discussions is the Neural Nets for Cognition Discussion Group at CogSci20222. The work is still very tentative, and most people agree that LMs are not ready to be used as models 1https://www.deepl.com/translator 2http://neural-nets-for-cognition.net Various probing studies (Garcia"}, {"question": " What is one aspect of the translations provided in Table 2 that does not convey the resultative meaning component?", "answer": " The translations do not correctly convey that the foam is removed from the cappuccino by the sneeze.", "ref_chunk": "objective of language modelling to mean that LMs should necessarily achieve their goal the same way that humans do. Therefore, we do not argue that lan- guage models need to \u201cthink\u201d in terms of construc- tions because humans do. Rather, we consider con- structions an inherent property of human language, which makes it necessary for language models to understand them. There has recently been growing interest in devel- oping probing approaches for PLMs based on CxG. We see these approaches as coming from two differ- ent motivational standpoints, summarised below. 2.1 Constructions are Essential for Language Modelling According to CxG, meaning is encoded in abstract constellations of linguistic units of different sizes. This means that LMs, which the \ufb01eld of NLP is trying to develop to achieve human language com- petency, must also be able to assign meaning to these units to be full LMs. Their ability to assign meaning to words, or more speci\ufb01cally to subword units which are sometimes closer to morphemes than to words, has been shown at length (Wiede- mann et al., 2019; Reif et al., 2019; Schwartz et al., 2022). The question therefore remains: are PLMs able to retrieve and use meanings associated with patterns involving multiple tokens? We do not take this to only mean contiguous, \ufb01xed expressions, but much more importantly, non-contiguous patterns with slots that have varying constraints placed on them. To imitate and match human language be- haviour, models of human language need to learn how to recognise these patterns, retrieve their mean- ing, apply this meaning to the context, and use them when producing language. Simply put, there is no way around learning constructions if LMs are to advance. In addition, we believe that it is an in- dependently interesting question whether existing PLMs pick up on these abstract patterns using the current architectures and training setups, and if not, which change in architecture would be necessary to facilitate this. 2.2 Importance in Downstream Tasks Regardless of more fundamental questions about the long-term goals of LMs, we also \ufb01rmly be- lieve that probing for CxG is relevant for analysing Lang Reference Translation DeepL Translation German Italian Turkish Sie nieste den Schaum von ihrem Cappuccino runter. Lei ha starnutito via la schiuma dal suo cappuccino. Cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc hap\u00b8s\u0131rd\u0131. Sie nieste den Schaum von ihrem Cappuccino. Starnut\u00ec la schiuma del suo cappuccino. Hap\u00b8s\u0131rarak cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc u\u00e7urdu. Table 2: Translations of \u2018She sneezed the foam off her cappuccino.\u2019 given by DeepL1. Translated back to English by humans, they all mean \u201cShe sneezed her cappuccino\u2019s foam.\u201d, which does not correctly convey the resultative meaning component, i.e., that the foam is removed from the cappuccino by the sneeze (as opposed to put there). the challenges that face applied NLP, as evaluated on downstream tasks, at this point in time. Dis- cussion is increasingly focusing on diagnosing the speci\ufb01c scenarios that are challenging for current models. Srivastava et al. (2022) propose test suites that are designed to challenge LMs, and many of them are designed by looking for \u2018patterns\u2019 with a non-obvious, non-literal meaning that is more than the sum of the involved words. One example of such a failure can be found in Table 2, where we provide the DeepL1 translations for the famous instance of the caused-motion construction (Gold- berg, 1995, CMC;): \u2018She sneezed the foam off her cappuccino\u2019, where the unusual factor is that sneeze does not usually take a patient argument or cause a motion. For translation, this means that it either has to use the corresponding CMC in the target language, which might be quite different in form from the English CMC, or paraphrase in a way that conveys all meaning facets. For the languages we tested, DeepL did not achieve this: the resulting sentence sounds more like the foam was sneezed onto the cappuccino, or is ambiguous between this and the correct translation. Interestingly, for Rus- sian, the motion is conveyed in the translation, but not the fact that it is caused by a sneeze. Targeted adversarial test suites like this transla- tion example can be a useful resource to evaluate how well LMs perform on constructions, but more crucially, CxG theory and probing methods will inform the design of better and more systematic test suites, which in turn will be used to improve LMs (\u00a75.4). of human language processing. However, the dis- cussion about whether LMs are ready to be used as cognitive models is dominated by results of prob- ing studies based on Generative Grammar (GG), or more speci\ufb01cally Transformational Grammar. This means that GG is being used as the gold standard against which the cognitive plausibility of LMs is evaluated. Studies using GG assume a direct relationship between the models\u2019 performance on probing tasks and their linguistic competency. In- creased performance on GG probing tasks is seen as a sign it is becoming more reasonable to use LMs as cognitive models. Another linguistic rea- son for theoretical diversity is that if we could show that LMs conform better to CxG rather than GG, this might open up interesting discussions if they ever start being used as cognitive models. 3 Established Probing Methods Are Only Applicable to Some Aspects of CxG Established probing methods have focused on dif- ferent aspects of the syntactic and semantic knowl- edge of PLMs. In this section, we summarise the major approaches that were not designed specif- ically with constructions in mind. We show that although each of these methodologies deals with some aspect of CxG, and might even fully inves- tigate some simpler constructions, none of them fully covers constructional knowledge as de\ufb01ned in Section 1.1. 3.1 Probing Using Contextual Embeddings 2.3 Diversity in Linguistics for NLP Discussions about PLMs as models of human lan- guage processing have recently gained popularity. One forum for such discussions is the Neural Nets for Cognition Discussion Group at CogSci20222. The work is still very tentative, and most people agree that LMs are not ready to be used as models 1https://www.deepl.com/translator 2http://neural-nets-for-cognition.net Various probing studies (Garcia"}, {"question": " What is the focus of discussions evaluating applied NLP on downstream tasks?", "answer": " Diagnosing the specific scenarios that are challenging for current models.", "ref_chunk": "objective of language modelling to mean that LMs should necessarily achieve their goal the same way that humans do. Therefore, we do not argue that lan- guage models need to \u201cthink\u201d in terms of construc- tions because humans do. Rather, we consider con- structions an inherent property of human language, which makes it necessary for language models to understand them. There has recently been growing interest in devel- oping probing approaches for PLMs based on CxG. We see these approaches as coming from two differ- ent motivational standpoints, summarised below. 2.1 Constructions are Essential for Language Modelling According to CxG, meaning is encoded in abstract constellations of linguistic units of different sizes. This means that LMs, which the \ufb01eld of NLP is trying to develop to achieve human language com- petency, must also be able to assign meaning to these units to be full LMs. Their ability to assign meaning to words, or more speci\ufb01cally to subword units which are sometimes closer to morphemes than to words, has been shown at length (Wiede- mann et al., 2019; Reif et al., 2019; Schwartz et al., 2022). The question therefore remains: are PLMs able to retrieve and use meanings associated with patterns involving multiple tokens? We do not take this to only mean contiguous, \ufb01xed expressions, but much more importantly, non-contiguous patterns with slots that have varying constraints placed on them. To imitate and match human language be- haviour, models of human language need to learn how to recognise these patterns, retrieve their mean- ing, apply this meaning to the context, and use them when producing language. Simply put, there is no way around learning constructions if LMs are to advance. In addition, we believe that it is an in- dependently interesting question whether existing PLMs pick up on these abstract patterns using the current architectures and training setups, and if not, which change in architecture would be necessary to facilitate this. 2.2 Importance in Downstream Tasks Regardless of more fundamental questions about the long-term goals of LMs, we also \ufb01rmly be- lieve that probing for CxG is relevant for analysing Lang Reference Translation DeepL Translation German Italian Turkish Sie nieste den Schaum von ihrem Cappuccino runter. Lei ha starnutito via la schiuma dal suo cappuccino. Cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc hap\u00b8s\u0131rd\u0131. Sie nieste den Schaum von ihrem Cappuccino. Starnut\u00ec la schiuma del suo cappuccino. Hap\u00b8s\u0131rarak cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc u\u00e7urdu. Table 2: Translations of \u2018She sneezed the foam off her cappuccino.\u2019 given by DeepL1. Translated back to English by humans, they all mean \u201cShe sneezed her cappuccino\u2019s foam.\u201d, which does not correctly convey the resultative meaning component, i.e., that the foam is removed from the cappuccino by the sneeze (as opposed to put there). the challenges that face applied NLP, as evaluated on downstream tasks, at this point in time. Dis- cussion is increasingly focusing on diagnosing the speci\ufb01c scenarios that are challenging for current models. Srivastava et al. (2022) propose test suites that are designed to challenge LMs, and many of them are designed by looking for \u2018patterns\u2019 with a non-obvious, non-literal meaning that is more than the sum of the involved words. One example of such a failure can be found in Table 2, where we provide the DeepL1 translations for the famous instance of the caused-motion construction (Gold- berg, 1995, CMC;): \u2018She sneezed the foam off her cappuccino\u2019, where the unusual factor is that sneeze does not usually take a patient argument or cause a motion. For translation, this means that it either has to use the corresponding CMC in the target language, which might be quite different in form from the English CMC, or paraphrase in a way that conveys all meaning facets. For the languages we tested, DeepL did not achieve this: the resulting sentence sounds more like the foam was sneezed onto the cappuccino, or is ambiguous between this and the correct translation. Interestingly, for Rus- sian, the motion is conveyed in the translation, but not the fact that it is caused by a sneeze. Targeted adversarial test suites like this transla- tion example can be a useful resource to evaluate how well LMs perform on constructions, but more crucially, CxG theory and probing methods will inform the design of better and more systematic test suites, which in turn will be used to improve LMs (\u00a75.4). of human language processing. However, the dis- cussion about whether LMs are ready to be used as cognitive models is dominated by results of prob- ing studies based on Generative Grammar (GG), or more speci\ufb01cally Transformational Grammar. This means that GG is being used as the gold standard against which the cognitive plausibility of LMs is evaluated. Studies using GG assume a direct relationship between the models\u2019 performance on probing tasks and their linguistic competency. In- creased performance on GG probing tasks is seen as a sign it is becoming more reasonable to use LMs as cognitive models. Another linguistic rea- son for theoretical diversity is that if we could show that LMs conform better to CxG rather than GG, this might open up interesting discussions if they ever start being used as cognitive models. 3 Established Probing Methods Are Only Applicable to Some Aspects of CxG Established probing methods have focused on dif- ferent aspects of the syntactic and semantic knowl- edge of PLMs. In this section, we summarise the major approaches that were not designed specif- ically with constructions in mind. We show that although each of these methodologies deals with some aspect of CxG, and might even fully inves- tigate some simpler constructions, none of them fully covers constructional knowledge as de\ufb01ned in Section 1.1. 3.1 Probing Using Contextual Embeddings 2.3 Diversity in Linguistics for NLP Discussions about PLMs as models of human lan- guage processing have recently gained popularity. One forum for such discussions is the Neural Nets for Cognition Discussion Group at CogSci20222. The work is still very tentative, and most people agree that LMs are not ready to be used as models 1https://www.deepl.com/translator 2http://neural-nets-for-cognition.net Various probing studies (Garcia"}, {"question": " What is the gold standard against which the cognitive plausibility of LMs is evaluated?", "answer": " Generative Grammar (GG) or Transformational Grammar.", "ref_chunk": "objective of language modelling to mean that LMs should necessarily achieve their goal the same way that humans do. Therefore, we do not argue that lan- guage models need to \u201cthink\u201d in terms of construc- tions because humans do. Rather, we consider con- structions an inherent property of human language, which makes it necessary for language models to understand them. There has recently been growing interest in devel- oping probing approaches for PLMs based on CxG. We see these approaches as coming from two differ- ent motivational standpoints, summarised below. 2.1 Constructions are Essential for Language Modelling According to CxG, meaning is encoded in abstract constellations of linguistic units of different sizes. This means that LMs, which the \ufb01eld of NLP is trying to develop to achieve human language com- petency, must also be able to assign meaning to these units to be full LMs. Their ability to assign meaning to words, or more speci\ufb01cally to subword units which are sometimes closer to morphemes than to words, has been shown at length (Wiede- mann et al., 2019; Reif et al., 2019; Schwartz et al., 2022). The question therefore remains: are PLMs able to retrieve and use meanings associated with patterns involving multiple tokens? We do not take this to only mean contiguous, \ufb01xed expressions, but much more importantly, non-contiguous patterns with slots that have varying constraints placed on them. To imitate and match human language be- haviour, models of human language need to learn how to recognise these patterns, retrieve their mean- ing, apply this meaning to the context, and use them when producing language. Simply put, there is no way around learning constructions if LMs are to advance. In addition, we believe that it is an in- dependently interesting question whether existing PLMs pick up on these abstract patterns using the current architectures and training setups, and if not, which change in architecture would be necessary to facilitate this. 2.2 Importance in Downstream Tasks Regardless of more fundamental questions about the long-term goals of LMs, we also \ufb01rmly be- lieve that probing for CxG is relevant for analysing Lang Reference Translation DeepL Translation German Italian Turkish Sie nieste den Schaum von ihrem Cappuccino runter. Lei ha starnutito via la schiuma dal suo cappuccino. Cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc hap\u00b8s\u0131rd\u0131. Sie nieste den Schaum von ihrem Cappuccino. Starnut\u00ec la schiuma del suo cappuccino. Hap\u00b8s\u0131rarak cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc u\u00e7urdu. Table 2: Translations of \u2018She sneezed the foam off her cappuccino.\u2019 given by DeepL1. Translated back to English by humans, they all mean \u201cShe sneezed her cappuccino\u2019s foam.\u201d, which does not correctly convey the resultative meaning component, i.e., that the foam is removed from the cappuccino by the sneeze (as opposed to put there). the challenges that face applied NLP, as evaluated on downstream tasks, at this point in time. Dis- cussion is increasingly focusing on diagnosing the speci\ufb01c scenarios that are challenging for current models. Srivastava et al. (2022) propose test suites that are designed to challenge LMs, and many of them are designed by looking for \u2018patterns\u2019 with a non-obvious, non-literal meaning that is more than the sum of the involved words. One example of such a failure can be found in Table 2, where we provide the DeepL1 translations for the famous instance of the caused-motion construction (Gold- berg, 1995, CMC;): \u2018She sneezed the foam off her cappuccino\u2019, where the unusual factor is that sneeze does not usually take a patient argument or cause a motion. For translation, this means that it either has to use the corresponding CMC in the target language, which might be quite different in form from the English CMC, or paraphrase in a way that conveys all meaning facets. For the languages we tested, DeepL did not achieve this: the resulting sentence sounds more like the foam was sneezed onto the cappuccino, or is ambiguous between this and the correct translation. Interestingly, for Rus- sian, the motion is conveyed in the translation, but not the fact that it is caused by a sneeze. Targeted adversarial test suites like this transla- tion example can be a useful resource to evaluate how well LMs perform on constructions, but more crucially, CxG theory and probing methods will inform the design of better and more systematic test suites, which in turn will be used to improve LMs (\u00a75.4). of human language processing. However, the dis- cussion about whether LMs are ready to be used as cognitive models is dominated by results of prob- ing studies based on Generative Grammar (GG), or more speci\ufb01cally Transformational Grammar. This means that GG is being used as the gold standard against which the cognitive plausibility of LMs is evaluated. Studies using GG assume a direct relationship between the models\u2019 performance on probing tasks and their linguistic competency. In- creased performance on GG probing tasks is seen as a sign it is becoming more reasonable to use LMs as cognitive models. Another linguistic rea- son for theoretical diversity is that if we could show that LMs conform better to CxG rather than GG, this might open up interesting discussions if they ever start being used as cognitive models. 3 Established Probing Methods Are Only Applicable to Some Aspects of CxG Established probing methods have focused on dif- ferent aspects of the syntactic and semantic knowl- edge of PLMs. In this section, we summarise the major approaches that were not designed specif- ically with constructions in mind. We show that although each of these methodologies deals with some aspect of CxG, and might even fully inves- tigate some simpler constructions, none of them fully covers constructional knowledge as de\ufb01ned in Section 1.1. 3.1 Probing Using Contextual Embeddings 2.3 Diversity in Linguistics for NLP Discussions about PLMs as models of human lan- guage processing have recently gained popularity. One forum for such discussions is the Neural Nets for Cognition Discussion Group at CogSci20222. The work is still very tentative, and most people agree that LMs are not ready to be used as models 1https://www.deepl.com/translator 2http://neural-nets-for-cognition.net Various probing studies (Garcia"}, {"question": " Why is it important to probe LMs based on CxG theory?", "answer": " To inform the design of better and more systematic test suites.", "ref_chunk": "objective of language modelling to mean that LMs should necessarily achieve their goal the same way that humans do. Therefore, we do not argue that lan- guage models need to \u201cthink\u201d in terms of construc- tions because humans do. Rather, we consider con- structions an inherent property of human language, which makes it necessary for language models to understand them. There has recently been growing interest in devel- oping probing approaches for PLMs based on CxG. We see these approaches as coming from two differ- ent motivational standpoints, summarised below. 2.1 Constructions are Essential for Language Modelling According to CxG, meaning is encoded in abstract constellations of linguistic units of different sizes. This means that LMs, which the \ufb01eld of NLP is trying to develop to achieve human language com- petency, must also be able to assign meaning to these units to be full LMs. Their ability to assign meaning to words, or more speci\ufb01cally to subword units which are sometimes closer to morphemes than to words, has been shown at length (Wiede- mann et al., 2019; Reif et al., 2019; Schwartz et al., 2022). The question therefore remains: are PLMs able to retrieve and use meanings associated with patterns involving multiple tokens? We do not take this to only mean contiguous, \ufb01xed expressions, but much more importantly, non-contiguous patterns with slots that have varying constraints placed on them. To imitate and match human language be- haviour, models of human language need to learn how to recognise these patterns, retrieve their mean- ing, apply this meaning to the context, and use them when producing language. Simply put, there is no way around learning constructions if LMs are to advance. In addition, we believe that it is an in- dependently interesting question whether existing PLMs pick up on these abstract patterns using the current architectures and training setups, and if not, which change in architecture would be necessary to facilitate this. 2.2 Importance in Downstream Tasks Regardless of more fundamental questions about the long-term goals of LMs, we also \ufb01rmly be- lieve that probing for CxG is relevant for analysing Lang Reference Translation DeepL Translation German Italian Turkish Sie nieste den Schaum von ihrem Cappuccino runter. Lei ha starnutito via la schiuma dal suo cappuccino. Cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc hap\u00b8s\u0131rd\u0131. Sie nieste den Schaum von ihrem Cappuccino. Starnut\u00ec la schiuma del suo cappuccino. Hap\u00b8s\u0131rarak cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc u\u00e7urdu. Table 2: Translations of \u2018She sneezed the foam off her cappuccino.\u2019 given by DeepL1. Translated back to English by humans, they all mean \u201cShe sneezed her cappuccino\u2019s foam.\u201d, which does not correctly convey the resultative meaning component, i.e., that the foam is removed from the cappuccino by the sneeze (as opposed to put there). the challenges that face applied NLP, as evaluated on downstream tasks, at this point in time. Dis- cussion is increasingly focusing on diagnosing the speci\ufb01c scenarios that are challenging for current models. Srivastava et al. (2022) propose test suites that are designed to challenge LMs, and many of them are designed by looking for \u2018patterns\u2019 with a non-obvious, non-literal meaning that is more than the sum of the involved words. One example of such a failure can be found in Table 2, where we provide the DeepL1 translations for the famous instance of the caused-motion construction (Gold- berg, 1995, CMC;): \u2018She sneezed the foam off her cappuccino\u2019, where the unusual factor is that sneeze does not usually take a patient argument or cause a motion. For translation, this means that it either has to use the corresponding CMC in the target language, which might be quite different in form from the English CMC, or paraphrase in a way that conveys all meaning facets. For the languages we tested, DeepL did not achieve this: the resulting sentence sounds more like the foam was sneezed onto the cappuccino, or is ambiguous between this and the correct translation. Interestingly, for Rus- sian, the motion is conveyed in the translation, but not the fact that it is caused by a sneeze. Targeted adversarial test suites like this transla- tion example can be a useful resource to evaluate how well LMs perform on constructions, but more crucially, CxG theory and probing methods will inform the design of better and more systematic test suites, which in turn will be used to improve LMs (\u00a75.4). of human language processing. However, the dis- cussion about whether LMs are ready to be used as cognitive models is dominated by results of prob- ing studies based on Generative Grammar (GG), or more speci\ufb01cally Transformational Grammar. This means that GG is being used as the gold standard against which the cognitive plausibility of LMs is evaluated. Studies using GG assume a direct relationship between the models\u2019 performance on probing tasks and their linguistic competency. In- creased performance on GG probing tasks is seen as a sign it is becoming more reasonable to use LMs as cognitive models. Another linguistic rea- son for theoretical diversity is that if we could show that LMs conform better to CxG rather than GG, this might open up interesting discussions if they ever start being used as cognitive models. 3 Established Probing Methods Are Only Applicable to Some Aspects of CxG Established probing methods have focused on dif- ferent aspects of the syntactic and semantic knowl- edge of PLMs. In this section, we summarise the major approaches that were not designed specif- ically with constructions in mind. We show that although each of these methodologies deals with some aspect of CxG, and might even fully inves- tigate some simpler constructions, none of them fully covers constructional knowledge as de\ufb01ned in Section 1.1. 3.1 Probing Using Contextual Embeddings 2.3 Diversity in Linguistics for NLP Discussions about PLMs as models of human lan- guage processing have recently gained popularity. One forum for such discussions is the Neural Nets for Cognition Discussion Group at CogSci20222. The work is still very tentative, and most people agree that LMs are not ready to be used as models 1https://www.deepl.com/translator 2http://neural-nets-for-cognition.net Various probing studies (Garcia"}, {"question": " Why are established probing methods not fully covering constructional knowledge?", "answer": " Because they were not designed specifically with constructions in mind.", "ref_chunk": "objective of language modelling to mean that LMs should necessarily achieve their goal the same way that humans do. Therefore, we do not argue that lan- guage models need to \u201cthink\u201d in terms of construc- tions because humans do. Rather, we consider con- structions an inherent property of human language, which makes it necessary for language models to understand them. There has recently been growing interest in devel- oping probing approaches for PLMs based on CxG. We see these approaches as coming from two differ- ent motivational standpoints, summarised below. 2.1 Constructions are Essential for Language Modelling According to CxG, meaning is encoded in abstract constellations of linguistic units of different sizes. This means that LMs, which the \ufb01eld of NLP is trying to develop to achieve human language com- petency, must also be able to assign meaning to these units to be full LMs. Their ability to assign meaning to words, or more speci\ufb01cally to subword units which are sometimes closer to morphemes than to words, has been shown at length (Wiede- mann et al., 2019; Reif et al., 2019; Schwartz et al., 2022). The question therefore remains: are PLMs able to retrieve and use meanings associated with patterns involving multiple tokens? We do not take this to only mean contiguous, \ufb01xed expressions, but much more importantly, non-contiguous patterns with slots that have varying constraints placed on them. To imitate and match human language be- haviour, models of human language need to learn how to recognise these patterns, retrieve their mean- ing, apply this meaning to the context, and use them when producing language. Simply put, there is no way around learning constructions if LMs are to advance. In addition, we believe that it is an in- dependently interesting question whether existing PLMs pick up on these abstract patterns using the current architectures and training setups, and if not, which change in architecture would be necessary to facilitate this. 2.2 Importance in Downstream Tasks Regardless of more fundamental questions about the long-term goals of LMs, we also \ufb01rmly be- lieve that probing for CxG is relevant for analysing Lang Reference Translation DeepL Translation German Italian Turkish Sie nieste den Schaum von ihrem Cappuccino runter. Lei ha starnutito via la schiuma dal suo cappuccino. Cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc hap\u00b8s\u0131rd\u0131. Sie nieste den Schaum von ihrem Cappuccino. Starnut\u00ec la schiuma del suo cappuccino. Hap\u00b8s\u0131rarak cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc u\u00e7urdu. Table 2: Translations of \u2018She sneezed the foam off her cappuccino.\u2019 given by DeepL1. Translated back to English by humans, they all mean \u201cShe sneezed her cappuccino\u2019s foam.\u201d, which does not correctly convey the resultative meaning component, i.e., that the foam is removed from the cappuccino by the sneeze (as opposed to put there). the challenges that face applied NLP, as evaluated on downstream tasks, at this point in time. Dis- cussion is increasingly focusing on diagnosing the speci\ufb01c scenarios that are challenging for current models. Srivastava et al. (2022) propose test suites that are designed to challenge LMs, and many of them are designed by looking for \u2018patterns\u2019 with a non-obvious, non-literal meaning that is more than the sum of the involved words. One example of such a failure can be found in Table 2, where we provide the DeepL1 translations for the famous instance of the caused-motion construction (Gold- berg, 1995, CMC;): \u2018She sneezed the foam off her cappuccino\u2019, where the unusual factor is that sneeze does not usually take a patient argument or cause a motion. For translation, this means that it either has to use the corresponding CMC in the target language, which might be quite different in form from the English CMC, or paraphrase in a way that conveys all meaning facets. For the languages we tested, DeepL did not achieve this: the resulting sentence sounds more like the foam was sneezed onto the cappuccino, or is ambiguous between this and the correct translation. Interestingly, for Rus- sian, the motion is conveyed in the translation, but not the fact that it is caused by a sneeze. Targeted adversarial test suites like this transla- tion example can be a useful resource to evaluate how well LMs perform on constructions, but more crucially, CxG theory and probing methods will inform the design of better and more systematic test suites, which in turn will be used to improve LMs (\u00a75.4). of human language processing. However, the dis- cussion about whether LMs are ready to be used as cognitive models is dominated by results of prob- ing studies based on Generative Grammar (GG), or more speci\ufb01cally Transformational Grammar. This means that GG is being used as the gold standard against which the cognitive plausibility of LMs is evaluated. Studies using GG assume a direct relationship between the models\u2019 performance on probing tasks and their linguistic competency. In- creased performance on GG probing tasks is seen as a sign it is becoming more reasonable to use LMs as cognitive models. Another linguistic rea- son for theoretical diversity is that if we could show that LMs conform better to CxG rather than GG, this might open up interesting discussions if they ever start being used as cognitive models. 3 Established Probing Methods Are Only Applicable to Some Aspects of CxG Established probing methods have focused on dif- ferent aspects of the syntactic and semantic knowl- edge of PLMs. In this section, we summarise the major approaches that were not designed specif- ically with constructions in mind. We show that although each of these methodologies deals with some aspect of CxG, and might even fully inves- tigate some simpler constructions, none of them fully covers constructional knowledge as de\ufb01ned in Section 1.1. 3.1 Probing Using Contextual Embeddings 2.3 Diversity in Linguistics for NLP Discussions about PLMs as models of human lan- guage processing have recently gained popularity. One forum for such discussions is the Neural Nets for Cognition Discussion Group at CogSci20222. The work is still very tentative, and most people agree that LMs are not ready to be used as models 1https://www.deepl.com/translator 2http://neural-nets-for-cognition.net Various probing studies (Garcia"}], "doc_text": "objective of language modelling to mean that LMs should necessarily achieve their goal the same way that humans do. Therefore, we do not argue that lan- guage models need to \u201cthink\u201d in terms of construc- tions because humans do. Rather, we consider con- structions an inherent property of human language, which makes it necessary for language models to understand them. There has recently been growing interest in devel- oping probing approaches for PLMs based on CxG. We see these approaches as coming from two differ- ent motivational standpoints, summarised below. 2.1 Constructions are Essential for Language Modelling According to CxG, meaning is encoded in abstract constellations of linguistic units of different sizes. This means that LMs, which the \ufb01eld of NLP is trying to develop to achieve human language com- petency, must also be able to assign meaning to these units to be full LMs. Their ability to assign meaning to words, or more speci\ufb01cally to subword units which are sometimes closer to morphemes than to words, has been shown at length (Wiede- mann et al., 2019; Reif et al., 2019; Schwartz et al., 2022). The question therefore remains: are PLMs able to retrieve and use meanings associated with patterns involving multiple tokens? We do not take this to only mean contiguous, \ufb01xed expressions, but much more importantly, non-contiguous patterns with slots that have varying constraints placed on them. To imitate and match human language be- haviour, models of human language need to learn how to recognise these patterns, retrieve their mean- ing, apply this meaning to the context, and use them when producing language. Simply put, there is no way around learning constructions if LMs are to advance. In addition, we believe that it is an in- dependently interesting question whether existing PLMs pick up on these abstract patterns using the current architectures and training setups, and if not, which change in architecture would be necessary to facilitate this. 2.2 Importance in Downstream Tasks Regardless of more fundamental questions about the long-term goals of LMs, we also \ufb01rmly be- lieve that probing for CxG is relevant for analysing Lang Reference Translation DeepL Translation German Italian Turkish Sie nieste den Schaum von ihrem Cappuccino runter. Lei ha starnutito via la schiuma dal suo cappuccino. Cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc hap\u00b8s\u0131rd\u0131. Sie nieste den Schaum von ihrem Cappuccino. Starnut\u00ec la schiuma del suo cappuccino. Hap\u00b8s\u0131rarak cappuccino\u2019sunun k\u00f6p\u00fc\u02d8g\u00fcn\u00fc u\u00e7urdu. Table 2: Translations of \u2018She sneezed the foam off her cappuccino.\u2019 given by DeepL1. Translated back to English by humans, they all mean \u201cShe sneezed her cappuccino\u2019s foam.\u201d, which does not correctly convey the resultative meaning component, i.e., that the foam is removed from the cappuccino by the sneeze (as opposed to put there). the challenges that face applied NLP, as evaluated on downstream tasks, at this point in time. Dis- cussion is increasingly focusing on diagnosing the speci\ufb01c scenarios that are challenging for current models. Srivastava et al. (2022) propose test suites that are designed to challenge LMs, and many of them are designed by looking for \u2018patterns\u2019 with a non-obvious, non-literal meaning that is more than the sum of the involved words. One example of such a failure can be found in Table 2, where we provide the DeepL1 translations for the famous instance of the caused-motion construction (Gold- berg, 1995, CMC;): \u2018She sneezed the foam off her cappuccino\u2019, where the unusual factor is that sneeze does not usually take a patient argument or cause a motion. For translation, this means that it either has to use the corresponding CMC in the target language, which might be quite different in form from the English CMC, or paraphrase in a way that conveys all meaning facets. For the languages we tested, DeepL did not achieve this: the resulting sentence sounds more like the foam was sneezed onto the cappuccino, or is ambiguous between this and the correct translation. Interestingly, for Rus- sian, the motion is conveyed in the translation, but not the fact that it is caused by a sneeze. Targeted adversarial test suites like this transla- tion example can be a useful resource to evaluate how well LMs perform on constructions, but more crucially, CxG theory and probing methods will inform the design of better and more systematic test suites, which in turn will be used to improve LMs (\u00a75.4). of human language processing. However, the dis- cussion about whether LMs are ready to be used as cognitive models is dominated by results of prob- ing studies based on Generative Grammar (GG), or more speci\ufb01cally Transformational Grammar. This means that GG is being used as the gold standard against which the cognitive plausibility of LMs is evaluated. Studies using GG assume a direct relationship between the models\u2019 performance on probing tasks and their linguistic competency. In- creased performance on GG probing tasks is seen as a sign it is becoming more reasonable to use LMs as cognitive models. Another linguistic rea- son for theoretical diversity is that if we could show that LMs conform better to CxG rather than GG, this might open up interesting discussions if they ever start being used as cognitive models. 3 Established Probing Methods Are Only Applicable to Some Aspects of CxG Established probing methods have focused on dif- ferent aspects of the syntactic and semantic knowl- edge of PLMs. In this section, we summarise the major approaches that were not designed specif- ically with constructions in mind. We show that although each of these methodologies deals with some aspect of CxG, and might even fully inves- tigate some simpler constructions, none of them fully covers constructional knowledge as de\ufb01ned in Section 1.1. 3.1 Probing Using Contextual Embeddings 2.3 Diversity in Linguistics for NLP Discussions about PLMs as models of human lan- guage processing have recently gained popularity. One forum for such discussions is the Neural Nets for Cognition Discussion Group at CogSci20222. The work is still very tentative, and most people agree that LMs are not ready to be used as models 1https://www.deepl.com/translator 2http://neural-nets-for-cognition.net Various probing studies (Garcia"}