{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_SantaCoder:_don't_reach_for_the_stars!_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What training algorithm was used for the Hugging Face Tokenizer?,answer: Byte-Pair Encoding (BPE) algorithm", "ref_chunk": "et al., 2021), MBPP (Austin et al., 2021) and MultiPL-E (Cassano et al., 2022). Tokenizer Seeing as the Santa models were the \ufb01rst models our community would train, our design choices for the tokenizer were modulated by a conservative approach, partly based on in- sights developed during the development of InCoder (Fried et al., 2022). We train a Hugging Face Tokenizer (MOI et al., 2022) using the Byte-Pair Encoding (BPE) algorithm on raw bytes with a vocabulary size of 49,152 tokens. This tokenizer was trained on 600,000 rows (Around 2.6 GB) of data\u2014200,000 for each language\u2014which were pre-tokenized using a digit splitter and the default GPT-2 pre-tokenizer regex before being converted to bytes. Training details Our base model is a 1.1B-parameter decoder-only transformer with FIM and MQA trained in float16. It has 24 layers, 16 heads and a hidden-size of 2048. The model is trained for 300K iterations with a global batch-size of 192 using Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, (cid:15) = 10\u22128 and a weight-decay of 0.1. A total of 118B tokens are seen in training. The learning-rate is set to 2 \u00d7 10\u22124 and follows a cosine decay after warming up for 2% of the training steps. Each training run takes 3.1 days to complete on 96 Tesla V100 GPUs for a total of 1.05 \u00d7 1021 FLOPs. The \ufb01nal model described in Section 6.2 uses twice the amount of compute. 5.2 ARCHITECTURE ABLATIONS We perform ablation experiments to de-risk the model architecture and training objective. Specif- ically, we investigate Fill-in-the-Middle (Bavarian et al., 2022) and Multi Query Attention (MQA) (Shazeer, 2019). FIM vs No-FIM Recent works (Fried et al., 2022; Bavarian et al., 2022) have shown that autore- gressive language-models can learn to in\ufb01ll code snippets by random transformation of the training 6 Preprint Language Base Stars Comments-to-code Near-dedup Tokenizer fertility Python Java JavaScript 75.6 GB 110 GB 82.7 GB 26.6 GB 35.8 GB 20.8 GB 65.6 GB 92.7 GB 57.5 GB 62.0 GB 88.4 GB 65.1 GB 72.5 GB 105.5 GB 76.4 GB Table 3: Data volume after additional \ufb01ltering of the Python, Java, JavaScript subsets of The Stack. data. Bavarian et al. (2022) argue that such data transformations do not harm the left-to-right gen- erative capabilities of the model. Following Bavarian et al. (2022), we implement FIM as a random transformation of the input sequence and split each training document into three parts uniformly at random: pre\ufb01x, middle and suf\ufb01x. Each part is prepended with a corresponding sentinel token, then documents are rearranged to put the middle part at the end of the sequence. The autoregressive training objective is unchanged. We use context-level FIM, apply transformations at the character level, use a FIM-rate of 0.5 and SPM+PSM joint training. We compare our base model to a model that was trained with the standard left-to-right objective only (No-FIM). Multi Query Attention vs Multi Head Attention Shazeer (2019) proposes Multi Query Atten- tion (MQA), an architectural change to transformer that shares key and value embeddings across attention heads. Compared to Multi Head Attention (MHA), this lowers the memory bandwidth requirements at generation time and results in faster inference. We compare our base model to a similar model using MHA instead, with the same hyper-parameters otherwise. Note that the MHA model has more parameters (1.3B) than the base model in this setting. 5.3 DATA FILTERING ABLATIONS We experiment with a number of preprocessing methods applied to the base dataset, described in Section 5.1. Note that the \ufb01lters are applied on top of the other \ufb01lters such as near-deduplication, line length \ufb01ltering, etc. GitHub stars Do popular repositories contain good quality code? We use GitHub stars as a proxy metric. We set the minimum threshold to 5 stars, as we believe that a lower number of stars would not be an indicator of popularity. This \ufb01lter removes more than 60% of the data (in terms of volume), see Table 3. Note that more than 40% of the \ufb01les do not have stars and that setting the threshold to 10 stars would remove an additional 5% of the data. Comment-to-code ratio Good code should be well documented. With this assumption, we \ufb01lter \ufb01les with a high comments-to-code ratio. We use the ast and tokenize modules to extract docstrings and comments from Python \ufb01les, and Pygments to extract comments from Java and JavaScript \ufb01les. We then analyze the comment-to-code character ratio. We \ufb01nd that about 20% of Python and Java \ufb01les and 45% of JavaScript \ufb01les have no comments. We use a minimum threshold of 1%, removing an additional 3% of \ufb01les in each language. We also \ufb01nd that \ufb01les with a ratio above 80% have poor quality, so we \ufb01lter them out, eliminating 2% of data in all languages. Overall, this comment-to-code \ufb01lter removes 20% of the data in terms of volume. More near-deduplication While exact-match deduplication is the most common preprocessing step for code LLMs (see Table 4), Kocetkov et al. (2022) showed that near-deduplication leads to additional performance gains. Their near-deduplication pipeline largely inherited the settings from CodeParrot (Tunstall et al., 2022): MinHash (Broder, 2000) + Locality Sensitive Hashing (LSH) based on datasketch9 with unigrams (non-alphanumeric tokens) and a 0.85 Jaccard similarity threshold. Additionally, it also recalculates the true unigram Jaccard similarity during the post- processing stage to weed out any false positives. In this paper, we investigate whether different deduplication settings can further improve performance. To this end, we conduct ablation experiments on a 200K subset of the raw python dataset from the Stack v1.1. We investigate the number of false positives and false negatives by comparing the clustered \ufb01les with their real Jaccard similarity. We \ufb01nd that: 1) Using unigrams during MinHash 9https://github.com/ekzhu/datasketch 7 Preprint Model Dataset Deduplication Method InCoder Fried et al. (2022) CodeGen (Nijkamp et al., 2022) AlphaCode (Li et al., 2022) PolyCoder (Xu et al., 2022a) PaLM Coder (Chowdhery et al., 2022) Near-deduplication (Levenshtein distance) CodeParrot (Tunstall et al., 2022) Codex (Chen et al., 2021) Exact"}, {"question": " How many tokens were in the vocabulary size of the Hugging Face Tokenizer?,answer: 49,152 tokens", "ref_chunk": "et al., 2021), MBPP (Austin et al., 2021) and MultiPL-E (Cassano et al., 2022). Tokenizer Seeing as the Santa models were the \ufb01rst models our community would train, our design choices for the tokenizer were modulated by a conservative approach, partly based on in- sights developed during the development of InCoder (Fried et al., 2022). We train a Hugging Face Tokenizer (MOI et al., 2022) using the Byte-Pair Encoding (BPE) algorithm on raw bytes with a vocabulary size of 49,152 tokens. This tokenizer was trained on 600,000 rows (Around 2.6 GB) of data\u2014200,000 for each language\u2014which were pre-tokenized using a digit splitter and the default GPT-2 pre-tokenizer regex before being converted to bytes. Training details Our base model is a 1.1B-parameter decoder-only transformer with FIM and MQA trained in float16. It has 24 layers, 16 heads and a hidden-size of 2048. The model is trained for 300K iterations with a global batch-size of 192 using Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, (cid:15) = 10\u22128 and a weight-decay of 0.1. A total of 118B tokens are seen in training. The learning-rate is set to 2 \u00d7 10\u22124 and follows a cosine decay after warming up for 2% of the training steps. Each training run takes 3.1 days to complete on 96 Tesla V100 GPUs for a total of 1.05 \u00d7 1021 FLOPs. The \ufb01nal model described in Section 6.2 uses twice the amount of compute. 5.2 ARCHITECTURE ABLATIONS We perform ablation experiments to de-risk the model architecture and training objective. Specif- ically, we investigate Fill-in-the-Middle (Bavarian et al., 2022) and Multi Query Attention (MQA) (Shazeer, 2019). FIM vs No-FIM Recent works (Fried et al., 2022; Bavarian et al., 2022) have shown that autore- gressive language-models can learn to in\ufb01ll code snippets by random transformation of the training 6 Preprint Language Base Stars Comments-to-code Near-dedup Tokenizer fertility Python Java JavaScript 75.6 GB 110 GB 82.7 GB 26.6 GB 35.8 GB 20.8 GB 65.6 GB 92.7 GB 57.5 GB 62.0 GB 88.4 GB 65.1 GB 72.5 GB 105.5 GB 76.4 GB Table 3: Data volume after additional \ufb01ltering of the Python, Java, JavaScript subsets of The Stack. data. Bavarian et al. (2022) argue that such data transformations do not harm the left-to-right gen- erative capabilities of the model. Following Bavarian et al. (2022), we implement FIM as a random transformation of the input sequence and split each training document into three parts uniformly at random: pre\ufb01x, middle and suf\ufb01x. Each part is prepended with a corresponding sentinel token, then documents are rearranged to put the middle part at the end of the sequence. The autoregressive training objective is unchanged. We use context-level FIM, apply transformations at the character level, use a FIM-rate of 0.5 and SPM+PSM joint training. We compare our base model to a model that was trained with the standard left-to-right objective only (No-FIM). Multi Query Attention vs Multi Head Attention Shazeer (2019) proposes Multi Query Atten- tion (MQA), an architectural change to transformer that shares key and value embeddings across attention heads. Compared to Multi Head Attention (MHA), this lowers the memory bandwidth requirements at generation time and results in faster inference. We compare our base model to a similar model using MHA instead, with the same hyper-parameters otherwise. Note that the MHA model has more parameters (1.3B) than the base model in this setting. 5.3 DATA FILTERING ABLATIONS We experiment with a number of preprocessing methods applied to the base dataset, described in Section 5.1. Note that the \ufb01lters are applied on top of the other \ufb01lters such as near-deduplication, line length \ufb01ltering, etc. GitHub stars Do popular repositories contain good quality code? We use GitHub stars as a proxy metric. We set the minimum threshold to 5 stars, as we believe that a lower number of stars would not be an indicator of popularity. This \ufb01lter removes more than 60% of the data (in terms of volume), see Table 3. Note that more than 40% of the \ufb01les do not have stars and that setting the threshold to 10 stars would remove an additional 5% of the data. Comment-to-code ratio Good code should be well documented. With this assumption, we \ufb01lter \ufb01les with a high comments-to-code ratio. We use the ast and tokenize modules to extract docstrings and comments from Python \ufb01les, and Pygments to extract comments from Java and JavaScript \ufb01les. We then analyze the comment-to-code character ratio. We \ufb01nd that about 20% of Python and Java \ufb01les and 45% of JavaScript \ufb01les have no comments. We use a minimum threshold of 1%, removing an additional 3% of \ufb01les in each language. We also \ufb01nd that \ufb01les with a ratio above 80% have poor quality, so we \ufb01lter them out, eliminating 2% of data in all languages. Overall, this comment-to-code \ufb01lter removes 20% of the data in terms of volume. More near-deduplication While exact-match deduplication is the most common preprocessing step for code LLMs (see Table 4), Kocetkov et al. (2022) showed that near-deduplication leads to additional performance gains. Their near-deduplication pipeline largely inherited the settings from CodeParrot (Tunstall et al., 2022): MinHash (Broder, 2000) + Locality Sensitive Hashing (LSH) based on datasketch9 with unigrams (non-alphanumeric tokens) and a 0.85 Jaccard similarity threshold. Additionally, it also recalculates the true unigram Jaccard similarity during the post- processing stage to weed out any false positives. In this paper, we investigate whether different deduplication settings can further improve performance. To this end, we conduct ablation experiments on a 200K subset of the raw python dataset from the Stack v1.1. We investigate the number of false positives and false negatives by comparing the clustered \ufb01les with their real Jaccard similarity. We \ufb01nd that: 1) Using unigrams during MinHash 9https://github.com/ekzhu/datasketch 7 Preprint Model Dataset Deduplication Method InCoder Fried et al. (2022) CodeGen (Nijkamp et al., 2022) AlphaCode (Li et al., 2022) PolyCoder (Xu et al., 2022a) PaLM Coder (Chowdhery et al., 2022) Near-deduplication (Levenshtein distance) CodeParrot (Tunstall et al., 2022) Codex (Chen et al., 2021) Exact"}, {"question": " How many layers does the base model transformer have?,answer: 24 layers", "ref_chunk": "et al., 2021), MBPP (Austin et al., 2021) and MultiPL-E (Cassano et al., 2022). Tokenizer Seeing as the Santa models were the \ufb01rst models our community would train, our design choices for the tokenizer were modulated by a conservative approach, partly based on in- sights developed during the development of InCoder (Fried et al., 2022). We train a Hugging Face Tokenizer (MOI et al., 2022) using the Byte-Pair Encoding (BPE) algorithm on raw bytes with a vocabulary size of 49,152 tokens. This tokenizer was trained on 600,000 rows (Around 2.6 GB) of data\u2014200,000 for each language\u2014which were pre-tokenized using a digit splitter and the default GPT-2 pre-tokenizer regex before being converted to bytes. Training details Our base model is a 1.1B-parameter decoder-only transformer with FIM and MQA trained in float16. It has 24 layers, 16 heads and a hidden-size of 2048. The model is trained for 300K iterations with a global batch-size of 192 using Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, (cid:15) = 10\u22128 and a weight-decay of 0.1. A total of 118B tokens are seen in training. The learning-rate is set to 2 \u00d7 10\u22124 and follows a cosine decay after warming up for 2% of the training steps. Each training run takes 3.1 days to complete on 96 Tesla V100 GPUs for a total of 1.05 \u00d7 1021 FLOPs. The \ufb01nal model described in Section 6.2 uses twice the amount of compute. 5.2 ARCHITECTURE ABLATIONS We perform ablation experiments to de-risk the model architecture and training objective. Specif- ically, we investigate Fill-in-the-Middle (Bavarian et al., 2022) and Multi Query Attention (MQA) (Shazeer, 2019). FIM vs No-FIM Recent works (Fried et al., 2022; Bavarian et al., 2022) have shown that autore- gressive language-models can learn to in\ufb01ll code snippets by random transformation of the training 6 Preprint Language Base Stars Comments-to-code Near-dedup Tokenizer fertility Python Java JavaScript 75.6 GB 110 GB 82.7 GB 26.6 GB 35.8 GB 20.8 GB 65.6 GB 92.7 GB 57.5 GB 62.0 GB 88.4 GB 65.1 GB 72.5 GB 105.5 GB 76.4 GB Table 3: Data volume after additional \ufb01ltering of the Python, Java, JavaScript subsets of The Stack. data. Bavarian et al. (2022) argue that such data transformations do not harm the left-to-right gen- erative capabilities of the model. Following Bavarian et al. (2022), we implement FIM as a random transformation of the input sequence and split each training document into three parts uniformly at random: pre\ufb01x, middle and suf\ufb01x. Each part is prepended with a corresponding sentinel token, then documents are rearranged to put the middle part at the end of the sequence. The autoregressive training objective is unchanged. We use context-level FIM, apply transformations at the character level, use a FIM-rate of 0.5 and SPM+PSM joint training. We compare our base model to a model that was trained with the standard left-to-right objective only (No-FIM). Multi Query Attention vs Multi Head Attention Shazeer (2019) proposes Multi Query Atten- tion (MQA), an architectural change to transformer that shares key and value embeddings across attention heads. Compared to Multi Head Attention (MHA), this lowers the memory bandwidth requirements at generation time and results in faster inference. We compare our base model to a similar model using MHA instead, with the same hyper-parameters otherwise. Note that the MHA model has more parameters (1.3B) than the base model in this setting. 5.3 DATA FILTERING ABLATIONS We experiment with a number of preprocessing methods applied to the base dataset, described in Section 5.1. Note that the \ufb01lters are applied on top of the other \ufb01lters such as near-deduplication, line length \ufb01ltering, etc. GitHub stars Do popular repositories contain good quality code? We use GitHub stars as a proxy metric. We set the minimum threshold to 5 stars, as we believe that a lower number of stars would not be an indicator of popularity. This \ufb01lter removes more than 60% of the data (in terms of volume), see Table 3. Note that more than 40% of the \ufb01les do not have stars and that setting the threshold to 10 stars would remove an additional 5% of the data. Comment-to-code ratio Good code should be well documented. With this assumption, we \ufb01lter \ufb01les with a high comments-to-code ratio. We use the ast and tokenize modules to extract docstrings and comments from Python \ufb01les, and Pygments to extract comments from Java and JavaScript \ufb01les. We then analyze the comment-to-code character ratio. We \ufb01nd that about 20% of Python and Java \ufb01les and 45% of JavaScript \ufb01les have no comments. We use a minimum threshold of 1%, removing an additional 3% of \ufb01les in each language. We also \ufb01nd that \ufb01les with a ratio above 80% have poor quality, so we \ufb01lter them out, eliminating 2% of data in all languages. Overall, this comment-to-code \ufb01lter removes 20% of the data in terms of volume. More near-deduplication While exact-match deduplication is the most common preprocessing step for code LLMs (see Table 4), Kocetkov et al. (2022) showed that near-deduplication leads to additional performance gains. Their near-deduplication pipeline largely inherited the settings from CodeParrot (Tunstall et al., 2022): MinHash (Broder, 2000) + Locality Sensitive Hashing (LSH) based on datasketch9 with unigrams (non-alphanumeric tokens) and a 0.85 Jaccard similarity threshold. Additionally, it also recalculates the true unigram Jaccard similarity during the post- processing stage to weed out any false positives. In this paper, we investigate whether different deduplication settings can further improve performance. To this end, we conduct ablation experiments on a 200K subset of the raw python dataset from the Stack v1.1. We investigate the number of false positives and false negatives by comparing the clustered \ufb01les with their real Jaccard similarity. We \ufb01nd that: 1) Using unigrams during MinHash 9https://github.com/ekzhu/datasketch 7 Preprint Model Dataset Deduplication Method InCoder Fried et al. (2022) CodeGen (Nijkamp et al., 2022) AlphaCode (Li et al., 2022) PolyCoder (Xu et al., 2022a) PaLM Coder (Chowdhery et al., 2022) Near-deduplication (Levenshtein distance) CodeParrot (Tunstall et al., 2022) Codex (Chen et al., 2021) Exact"}, {"question": " What is the hidden size of the base model transformer?,answer: 2048", "ref_chunk": "et al., 2021), MBPP (Austin et al., 2021) and MultiPL-E (Cassano et al., 2022). Tokenizer Seeing as the Santa models were the \ufb01rst models our community would train, our design choices for the tokenizer were modulated by a conservative approach, partly based on in- sights developed during the development of InCoder (Fried et al., 2022). We train a Hugging Face Tokenizer (MOI et al., 2022) using the Byte-Pair Encoding (BPE) algorithm on raw bytes with a vocabulary size of 49,152 tokens. This tokenizer was trained on 600,000 rows (Around 2.6 GB) of data\u2014200,000 for each language\u2014which were pre-tokenized using a digit splitter and the default GPT-2 pre-tokenizer regex before being converted to bytes. Training details Our base model is a 1.1B-parameter decoder-only transformer with FIM and MQA trained in float16. It has 24 layers, 16 heads and a hidden-size of 2048. The model is trained for 300K iterations with a global batch-size of 192 using Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, (cid:15) = 10\u22128 and a weight-decay of 0.1. A total of 118B tokens are seen in training. The learning-rate is set to 2 \u00d7 10\u22124 and follows a cosine decay after warming up for 2% of the training steps. Each training run takes 3.1 days to complete on 96 Tesla V100 GPUs for a total of 1.05 \u00d7 1021 FLOPs. The \ufb01nal model described in Section 6.2 uses twice the amount of compute. 5.2 ARCHITECTURE ABLATIONS We perform ablation experiments to de-risk the model architecture and training objective. Specif- ically, we investigate Fill-in-the-Middle (Bavarian et al., 2022) and Multi Query Attention (MQA) (Shazeer, 2019). FIM vs No-FIM Recent works (Fried et al., 2022; Bavarian et al., 2022) have shown that autore- gressive language-models can learn to in\ufb01ll code snippets by random transformation of the training 6 Preprint Language Base Stars Comments-to-code Near-dedup Tokenizer fertility Python Java JavaScript 75.6 GB 110 GB 82.7 GB 26.6 GB 35.8 GB 20.8 GB 65.6 GB 92.7 GB 57.5 GB 62.0 GB 88.4 GB 65.1 GB 72.5 GB 105.5 GB 76.4 GB Table 3: Data volume after additional \ufb01ltering of the Python, Java, JavaScript subsets of The Stack. data. Bavarian et al. (2022) argue that such data transformations do not harm the left-to-right gen- erative capabilities of the model. Following Bavarian et al. (2022), we implement FIM as a random transformation of the input sequence and split each training document into three parts uniformly at random: pre\ufb01x, middle and suf\ufb01x. Each part is prepended with a corresponding sentinel token, then documents are rearranged to put the middle part at the end of the sequence. The autoregressive training objective is unchanged. We use context-level FIM, apply transformations at the character level, use a FIM-rate of 0.5 and SPM+PSM joint training. We compare our base model to a model that was trained with the standard left-to-right objective only (No-FIM). Multi Query Attention vs Multi Head Attention Shazeer (2019) proposes Multi Query Atten- tion (MQA), an architectural change to transformer that shares key and value embeddings across attention heads. Compared to Multi Head Attention (MHA), this lowers the memory bandwidth requirements at generation time and results in faster inference. We compare our base model to a similar model using MHA instead, with the same hyper-parameters otherwise. Note that the MHA model has more parameters (1.3B) than the base model in this setting. 5.3 DATA FILTERING ABLATIONS We experiment with a number of preprocessing methods applied to the base dataset, described in Section 5.1. Note that the \ufb01lters are applied on top of the other \ufb01lters such as near-deduplication, line length \ufb01ltering, etc. GitHub stars Do popular repositories contain good quality code? We use GitHub stars as a proxy metric. We set the minimum threshold to 5 stars, as we believe that a lower number of stars would not be an indicator of popularity. This \ufb01lter removes more than 60% of the data (in terms of volume), see Table 3. Note that more than 40% of the \ufb01les do not have stars and that setting the threshold to 10 stars would remove an additional 5% of the data. Comment-to-code ratio Good code should be well documented. With this assumption, we \ufb01lter \ufb01les with a high comments-to-code ratio. We use the ast and tokenize modules to extract docstrings and comments from Python \ufb01les, and Pygments to extract comments from Java and JavaScript \ufb01les. We then analyze the comment-to-code character ratio. We \ufb01nd that about 20% of Python and Java \ufb01les and 45% of JavaScript \ufb01les have no comments. We use a minimum threshold of 1%, removing an additional 3% of \ufb01les in each language. We also \ufb01nd that \ufb01les with a ratio above 80% have poor quality, so we \ufb01lter them out, eliminating 2% of data in all languages. Overall, this comment-to-code \ufb01lter removes 20% of the data in terms of volume. More near-deduplication While exact-match deduplication is the most common preprocessing step for code LLMs (see Table 4), Kocetkov et al. (2022) showed that near-deduplication leads to additional performance gains. Their near-deduplication pipeline largely inherited the settings from CodeParrot (Tunstall et al., 2022): MinHash (Broder, 2000) + Locality Sensitive Hashing (LSH) based on datasketch9 with unigrams (non-alphanumeric tokens) and a 0.85 Jaccard similarity threshold. Additionally, it also recalculates the true unigram Jaccard similarity during the post- processing stage to weed out any false positives. In this paper, we investigate whether different deduplication settings can further improve performance. To this end, we conduct ablation experiments on a 200K subset of the raw python dataset from the Stack v1.1. We investigate the number of false positives and false negatives by comparing the clustered \ufb01les with their real Jaccard similarity. We \ufb01nd that: 1) Using unigrams during MinHash 9https://github.com/ekzhu/datasketch 7 Preprint Model Dataset Deduplication Method InCoder Fried et al. (2022) CodeGen (Nijkamp et al., 2022) AlphaCode (Li et al., 2022) PolyCoder (Xu et al., 2022a) PaLM Coder (Chowdhery et al., 2022) Near-deduplication (Levenshtein distance) CodeParrot (Tunstall et al., 2022) Codex (Chen et al., 2021) Exact"}, {"question": " How long does each training run take to complete on 96 Tesla V100 GPUs?,answer: 3.1 days", "ref_chunk": "et al., 2021), MBPP (Austin et al., 2021) and MultiPL-E (Cassano et al., 2022). Tokenizer Seeing as the Santa models were the \ufb01rst models our community would train, our design choices for the tokenizer were modulated by a conservative approach, partly based on in- sights developed during the development of InCoder (Fried et al., 2022). We train a Hugging Face Tokenizer (MOI et al., 2022) using the Byte-Pair Encoding (BPE) algorithm on raw bytes with a vocabulary size of 49,152 tokens. This tokenizer was trained on 600,000 rows (Around 2.6 GB) of data\u2014200,000 for each language\u2014which were pre-tokenized using a digit splitter and the default GPT-2 pre-tokenizer regex before being converted to bytes. Training details Our base model is a 1.1B-parameter decoder-only transformer with FIM and MQA trained in float16. It has 24 layers, 16 heads and a hidden-size of 2048. The model is trained for 300K iterations with a global batch-size of 192 using Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, (cid:15) = 10\u22128 and a weight-decay of 0.1. A total of 118B tokens are seen in training. The learning-rate is set to 2 \u00d7 10\u22124 and follows a cosine decay after warming up for 2% of the training steps. Each training run takes 3.1 days to complete on 96 Tesla V100 GPUs for a total of 1.05 \u00d7 1021 FLOPs. The \ufb01nal model described in Section 6.2 uses twice the amount of compute. 5.2 ARCHITECTURE ABLATIONS We perform ablation experiments to de-risk the model architecture and training objective. Specif- ically, we investigate Fill-in-the-Middle (Bavarian et al., 2022) and Multi Query Attention (MQA) (Shazeer, 2019). FIM vs No-FIM Recent works (Fried et al., 2022; Bavarian et al., 2022) have shown that autore- gressive language-models can learn to in\ufb01ll code snippets by random transformation of the training 6 Preprint Language Base Stars Comments-to-code Near-dedup Tokenizer fertility Python Java JavaScript 75.6 GB 110 GB 82.7 GB 26.6 GB 35.8 GB 20.8 GB 65.6 GB 92.7 GB 57.5 GB 62.0 GB 88.4 GB 65.1 GB 72.5 GB 105.5 GB 76.4 GB Table 3: Data volume after additional \ufb01ltering of the Python, Java, JavaScript subsets of The Stack. data. Bavarian et al. (2022) argue that such data transformations do not harm the left-to-right gen- erative capabilities of the model. Following Bavarian et al. (2022), we implement FIM as a random transformation of the input sequence and split each training document into three parts uniformly at random: pre\ufb01x, middle and suf\ufb01x. Each part is prepended with a corresponding sentinel token, then documents are rearranged to put the middle part at the end of the sequence. The autoregressive training objective is unchanged. We use context-level FIM, apply transformations at the character level, use a FIM-rate of 0.5 and SPM+PSM joint training. We compare our base model to a model that was trained with the standard left-to-right objective only (No-FIM). Multi Query Attention vs Multi Head Attention Shazeer (2019) proposes Multi Query Atten- tion (MQA), an architectural change to transformer that shares key and value embeddings across attention heads. Compared to Multi Head Attention (MHA), this lowers the memory bandwidth requirements at generation time and results in faster inference. We compare our base model to a similar model using MHA instead, with the same hyper-parameters otherwise. Note that the MHA model has more parameters (1.3B) than the base model in this setting. 5.3 DATA FILTERING ABLATIONS We experiment with a number of preprocessing methods applied to the base dataset, described in Section 5.1. Note that the \ufb01lters are applied on top of the other \ufb01lters such as near-deduplication, line length \ufb01ltering, etc. GitHub stars Do popular repositories contain good quality code? We use GitHub stars as a proxy metric. We set the minimum threshold to 5 stars, as we believe that a lower number of stars would not be an indicator of popularity. This \ufb01lter removes more than 60% of the data (in terms of volume), see Table 3. Note that more than 40% of the \ufb01les do not have stars and that setting the threshold to 10 stars would remove an additional 5% of the data. Comment-to-code ratio Good code should be well documented. With this assumption, we \ufb01lter \ufb01les with a high comments-to-code ratio. We use the ast and tokenize modules to extract docstrings and comments from Python \ufb01les, and Pygments to extract comments from Java and JavaScript \ufb01les. We then analyze the comment-to-code character ratio. We \ufb01nd that about 20% of Python and Java \ufb01les and 45% of JavaScript \ufb01les have no comments. We use a minimum threshold of 1%, removing an additional 3% of \ufb01les in each language. We also \ufb01nd that \ufb01les with a ratio above 80% have poor quality, so we \ufb01lter them out, eliminating 2% of data in all languages. Overall, this comment-to-code \ufb01lter removes 20% of the data in terms of volume. More near-deduplication While exact-match deduplication is the most common preprocessing step for code LLMs (see Table 4), Kocetkov et al. (2022) showed that near-deduplication leads to additional performance gains. Their near-deduplication pipeline largely inherited the settings from CodeParrot (Tunstall et al., 2022): MinHash (Broder, 2000) + Locality Sensitive Hashing (LSH) based on datasketch9 with unigrams (non-alphanumeric tokens) and a 0.85 Jaccard similarity threshold. Additionally, it also recalculates the true unigram Jaccard similarity during the post- processing stage to weed out any false positives. In this paper, we investigate whether different deduplication settings can further improve performance. To this end, we conduct ablation experiments on a 200K subset of the raw python dataset from the Stack v1.1. We investigate the number of false positives and false negatives by comparing the clustered \ufb01les with their real Jaccard similarity. We \ufb01nd that: 1) Using unigrams during MinHash 9https://github.com/ekzhu/datasketch 7 Preprint Model Dataset Deduplication Method InCoder Fried et al. (2022) CodeGen (Nijkamp et al., 2022) AlphaCode (Li et al., 2022) PolyCoder (Xu et al., 2022a) PaLM Coder (Chowdhery et al., 2022) Near-deduplication (Levenshtein distance) CodeParrot (Tunstall et al., 2022) Codex (Chen et al., 2021) Exact"}, {"question": " What does FIM stand for in the context of the model architecture ablations?,answer: Fill-in-the-Middle", "ref_chunk": "et al., 2021), MBPP (Austin et al., 2021) and MultiPL-E (Cassano et al., 2022). Tokenizer Seeing as the Santa models were the \ufb01rst models our community would train, our design choices for the tokenizer were modulated by a conservative approach, partly based on in- sights developed during the development of InCoder (Fried et al., 2022). We train a Hugging Face Tokenizer (MOI et al., 2022) using the Byte-Pair Encoding (BPE) algorithm on raw bytes with a vocabulary size of 49,152 tokens. This tokenizer was trained on 600,000 rows (Around 2.6 GB) of data\u2014200,000 for each language\u2014which were pre-tokenized using a digit splitter and the default GPT-2 pre-tokenizer regex before being converted to bytes. Training details Our base model is a 1.1B-parameter decoder-only transformer with FIM and MQA trained in float16. It has 24 layers, 16 heads and a hidden-size of 2048. The model is trained for 300K iterations with a global batch-size of 192 using Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, (cid:15) = 10\u22128 and a weight-decay of 0.1. A total of 118B tokens are seen in training. The learning-rate is set to 2 \u00d7 10\u22124 and follows a cosine decay after warming up for 2% of the training steps. Each training run takes 3.1 days to complete on 96 Tesla V100 GPUs for a total of 1.05 \u00d7 1021 FLOPs. The \ufb01nal model described in Section 6.2 uses twice the amount of compute. 5.2 ARCHITECTURE ABLATIONS We perform ablation experiments to de-risk the model architecture and training objective. Specif- ically, we investigate Fill-in-the-Middle (Bavarian et al., 2022) and Multi Query Attention (MQA) (Shazeer, 2019). FIM vs No-FIM Recent works (Fried et al., 2022; Bavarian et al., 2022) have shown that autore- gressive language-models can learn to in\ufb01ll code snippets by random transformation of the training 6 Preprint Language Base Stars Comments-to-code Near-dedup Tokenizer fertility Python Java JavaScript 75.6 GB 110 GB 82.7 GB 26.6 GB 35.8 GB 20.8 GB 65.6 GB 92.7 GB 57.5 GB 62.0 GB 88.4 GB 65.1 GB 72.5 GB 105.5 GB 76.4 GB Table 3: Data volume after additional \ufb01ltering of the Python, Java, JavaScript subsets of The Stack. data. Bavarian et al. (2022) argue that such data transformations do not harm the left-to-right gen- erative capabilities of the model. Following Bavarian et al. (2022), we implement FIM as a random transformation of the input sequence and split each training document into three parts uniformly at random: pre\ufb01x, middle and suf\ufb01x. Each part is prepended with a corresponding sentinel token, then documents are rearranged to put the middle part at the end of the sequence. The autoregressive training objective is unchanged. We use context-level FIM, apply transformations at the character level, use a FIM-rate of 0.5 and SPM+PSM joint training. We compare our base model to a model that was trained with the standard left-to-right objective only (No-FIM). Multi Query Attention vs Multi Head Attention Shazeer (2019) proposes Multi Query Atten- tion (MQA), an architectural change to transformer that shares key and value embeddings across attention heads. Compared to Multi Head Attention (MHA), this lowers the memory bandwidth requirements at generation time and results in faster inference. We compare our base model to a similar model using MHA instead, with the same hyper-parameters otherwise. Note that the MHA model has more parameters (1.3B) than the base model in this setting. 5.3 DATA FILTERING ABLATIONS We experiment with a number of preprocessing methods applied to the base dataset, described in Section 5.1. Note that the \ufb01lters are applied on top of the other \ufb01lters such as near-deduplication, line length \ufb01ltering, etc. GitHub stars Do popular repositories contain good quality code? We use GitHub stars as a proxy metric. We set the minimum threshold to 5 stars, as we believe that a lower number of stars would not be an indicator of popularity. This \ufb01lter removes more than 60% of the data (in terms of volume), see Table 3. Note that more than 40% of the \ufb01les do not have stars and that setting the threshold to 10 stars would remove an additional 5% of the data. Comment-to-code ratio Good code should be well documented. With this assumption, we \ufb01lter \ufb01les with a high comments-to-code ratio. We use the ast and tokenize modules to extract docstrings and comments from Python \ufb01les, and Pygments to extract comments from Java and JavaScript \ufb01les. We then analyze the comment-to-code character ratio. We \ufb01nd that about 20% of Python and Java \ufb01les and 45% of JavaScript \ufb01les have no comments. We use a minimum threshold of 1%, removing an additional 3% of \ufb01les in each language. We also \ufb01nd that \ufb01les with a ratio above 80% have poor quality, so we \ufb01lter them out, eliminating 2% of data in all languages. Overall, this comment-to-code \ufb01lter removes 20% of the data in terms of volume. More near-deduplication While exact-match deduplication is the most common preprocessing step for code LLMs (see Table 4), Kocetkov et al. (2022) showed that near-deduplication leads to additional performance gains. Their near-deduplication pipeline largely inherited the settings from CodeParrot (Tunstall et al., 2022): MinHash (Broder, 2000) + Locality Sensitive Hashing (LSH) based on datasketch9 with unigrams (non-alphanumeric tokens) and a 0.85 Jaccard similarity threshold. Additionally, it also recalculates the true unigram Jaccard similarity during the post- processing stage to weed out any false positives. In this paper, we investigate whether different deduplication settings can further improve performance. To this end, we conduct ablation experiments on a 200K subset of the raw python dataset from the Stack v1.1. We investigate the number of false positives and false negatives by comparing the clustered \ufb01les with their real Jaccard similarity. We \ufb01nd that: 1) Using unigrams during MinHash 9https://github.com/ekzhu/datasketch 7 Preprint Model Dataset Deduplication Method InCoder Fried et al. (2022) CodeGen (Nijkamp et al., 2022) AlphaCode (Li et al., 2022) PolyCoder (Xu et al., 2022a) PaLM Coder (Chowdhery et al., 2022) Near-deduplication (Levenshtein distance) CodeParrot (Tunstall et al., 2022) Codex (Chen et al., 2021) Exact"}, {"question": " What is the purpose of Multi Query Attention (MQA) according to Shazeer (2019)?,answer: To share key and value embeddings across attention heads", "ref_chunk": "et al., 2021), MBPP (Austin et al., 2021) and MultiPL-E (Cassano et al., 2022). Tokenizer Seeing as the Santa models were the \ufb01rst models our community would train, our design choices for the tokenizer were modulated by a conservative approach, partly based on in- sights developed during the development of InCoder (Fried et al., 2022). We train a Hugging Face Tokenizer (MOI et al., 2022) using the Byte-Pair Encoding (BPE) algorithm on raw bytes with a vocabulary size of 49,152 tokens. This tokenizer was trained on 600,000 rows (Around 2.6 GB) of data\u2014200,000 for each language\u2014which were pre-tokenized using a digit splitter and the default GPT-2 pre-tokenizer regex before being converted to bytes. Training details Our base model is a 1.1B-parameter decoder-only transformer with FIM and MQA trained in float16. It has 24 layers, 16 heads and a hidden-size of 2048. The model is trained for 300K iterations with a global batch-size of 192 using Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, (cid:15) = 10\u22128 and a weight-decay of 0.1. A total of 118B tokens are seen in training. The learning-rate is set to 2 \u00d7 10\u22124 and follows a cosine decay after warming up for 2% of the training steps. Each training run takes 3.1 days to complete on 96 Tesla V100 GPUs for a total of 1.05 \u00d7 1021 FLOPs. The \ufb01nal model described in Section 6.2 uses twice the amount of compute. 5.2 ARCHITECTURE ABLATIONS We perform ablation experiments to de-risk the model architecture and training objective. Specif- ically, we investigate Fill-in-the-Middle (Bavarian et al., 2022) and Multi Query Attention (MQA) (Shazeer, 2019). FIM vs No-FIM Recent works (Fried et al., 2022; Bavarian et al., 2022) have shown that autore- gressive language-models can learn to in\ufb01ll code snippets by random transformation of the training 6 Preprint Language Base Stars Comments-to-code Near-dedup Tokenizer fertility Python Java JavaScript 75.6 GB 110 GB 82.7 GB 26.6 GB 35.8 GB 20.8 GB 65.6 GB 92.7 GB 57.5 GB 62.0 GB 88.4 GB 65.1 GB 72.5 GB 105.5 GB 76.4 GB Table 3: Data volume after additional \ufb01ltering of the Python, Java, JavaScript subsets of The Stack. data. Bavarian et al. (2022) argue that such data transformations do not harm the left-to-right gen- erative capabilities of the model. Following Bavarian et al. (2022), we implement FIM as a random transformation of the input sequence and split each training document into three parts uniformly at random: pre\ufb01x, middle and suf\ufb01x. Each part is prepended with a corresponding sentinel token, then documents are rearranged to put the middle part at the end of the sequence. The autoregressive training objective is unchanged. We use context-level FIM, apply transformations at the character level, use a FIM-rate of 0.5 and SPM+PSM joint training. We compare our base model to a model that was trained with the standard left-to-right objective only (No-FIM). Multi Query Attention vs Multi Head Attention Shazeer (2019) proposes Multi Query Atten- tion (MQA), an architectural change to transformer that shares key and value embeddings across attention heads. Compared to Multi Head Attention (MHA), this lowers the memory bandwidth requirements at generation time and results in faster inference. We compare our base model to a similar model using MHA instead, with the same hyper-parameters otherwise. Note that the MHA model has more parameters (1.3B) than the base model in this setting. 5.3 DATA FILTERING ABLATIONS We experiment with a number of preprocessing methods applied to the base dataset, described in Section 5.1. Note that the \ufb01lters are applied on top of the other \ufb01lters such as near-deduplication, line length \ufb01ltering, etc. GitHub stars Do popular repositories contain good quality code? We use GitHub stars as a proxy metric. We set the minimum threshold to 5 stars, as we believe that a lower number of stars would not be an indicator of popularity. This \ufb01lter removes more than 60% of the data (in terms of volume), see Table 3. Note that more than 40% of the \ufb01les do not have stars and that setting the threshold to 10 stars would remove an additional 5% of the data. Comment-to-code ratio Good code should be well documented. With this assumption, we \ufb01lter \ufb01les with a high comments-to-code ratio. We use the ast and tokenize modules to extract docstrings and comments from Python \ufb01les, and Pygments to extract comments from Java and JavaScript \ufb01les. We then analyze the comment-to-code character ratio. We \ufb01nd that about 20% of Python and Java \ufb01les and 45% of JavaScript \ufb01les have no comments. We use a minimum threshold of 1%, removing an additional 3% of \ufb01les in each language. We also \ufb01nd that \ufb01les with a ratio above 80% have poor quality, so we \ufb01lter them out, eliminating 2% of data in all languages. Overall, this comment-to-code \ufb01lter removes 20% of the data in terms of volume. More near-deduplication While exact-match deduplication is the most common preprocessing step for code LLMs (see Table 4), Kocetkov et al. (2022) showed that near-deduplication leads to additional performance gains. Their near-deduplication pipeline largely inherited the settings from CodeParrot (Tunstall et al., 2022): MinHash (Broder, 2000) + Locality Sensitive Hashing (LSH) based on datasketch9 with unigrams (non-alphanumeric tokens) and a 0.85 Jaccard similarity threshold. Additionally, it also recalculates the true unigram Jaccard similarity during the post- processing stage to weed out any false positives. In this paper, we investigate whether different deduplication settings can further improve performance. To this end, we conduct ablation experiments on a 200K subset of the raw python dataset from the Stack v1.1. We investigate the number of false positives and false negatives by comparing the clustered \ufb01les with their real Jaccard similarity. We \ufb01nd that: 1) Using unigrams during MinHash 9https://github.com/ekzhu/datasketch 7 Preprint Model Dataset Deduplication Method InCoder Fried et al. (2022) CodeGen (Nijkamp et al., 2022) AlphaCode (Li et al., 2022) PolyCoder (Xu et al., 2022a) PaLM Coder (Chowdhery et al., 2022) Near-deduplication (Levenshtein distance) CodeParrot (Tunstall et al., 2022) Codex (Chen et al., 2021) Exact"}, {"question": " How much data is removed by filtering files with a high comments-to-code ratio?,answer: 20% of the data in terms of volume", "ref_chunk": "et al., 2021), MBPP (Austin et al., 2021) and MultiPL-E (Cassano et al., 2022). Tokenizer Seeing as the Santa models were the \ufb01rst models our community would train, our design choices for the tokenizer were modulated by a conservative approach, partly based on in- sights developed during the development of InCoder (Fried et al., 2022). We train a Hugging Face Tokenizer (MOI et al., 2022) using the Byte-Pair Encoding (BPE) algorithm on raw bytes with a vocabulary size of 49,152 tokens. This tokenizer was trained on 600,000 rows (Around 2.6 GB) of data\u2014200,000 for each language\u2014which were pre-tokenized using a digit splitter and the default GPT-2 pre-tokenizer regex before being converted to bytes. Training details Our base model is a 1.1B-parameter decoder-only transformer with FIM and MQA trained in float16. It has 24 layers, 16 heads and a hidden-size of 2048. The model is trained for 300K iterations with a global batch-size of 192 using Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, (cid:15) = 10\u22128 and a weight-decay of 0.1. A total of 118B tokens are seen in training. The learning-rate is set to 2 \u00d7 10\u22124 and follows a cosine decay after warming up for 2% of the training steps. Each training run takes 3.1 days to complete on 96 Tesla V100 GPUs for a total of 1.05 \u00d7 1021 FLOPs. The \ufb01nal model described in Section 6.2 uses twice the amount of compute. 5.2 ARCHITECTURE ABLATIONS We perform ablation experiments to de-risk the model architecture and training objective. Specif- ically, we investigate Fill-in-the-Middle (Bavarian et al., 2022) and Multi Query Attention (MQA) (Shazeer, 2019). FIM vs No-FIM Recent works (Fried et al., 2022; Bavarian et al., 2022) have shown that autore- gressive language-models can learn to in\ufb01ll code snippets by random transformation of the training 6 Preprint Language Base Stars Comments-to-code Near-dedup Tokenizer fertility Python Java JavaScript 75.6 GB 110 GB 82.7 GB 26.6 GB 35.8 GB 20.8 GB 65.6 GB 92.7 GB 57.5 GB 62.0 GB 88.4 GB 65.1 GB 72.5 GB 105.5 GB 76.4 GB Table 3: Data volume after additional \ufb01ltering of the Python, Java, JavaScript subsets of The Stack. data. Bavarian et al. (2022) argue that such data transformations do not harm the left-to-right gen- erative capabilities of the model. Following Bavarian et al. (2022), we implement FIM as a random transformation of the input sequence and split each training document into three parts uniformly at random: pre\ufb01x, middle and suf\ufb01x. Each part is prepended with a corresponding sentinel token, then documents are rearranged to put the middle part at the end of the sequence. The autoregressive training objective is unchanged. We use context-level FIM, apply transformations at the character level, use a FIM-rate of 0.5 and SPM+PSM joint training. We compare our base model to a model that was trained with the standard left-to-right objective only (No-FIM). Multi Query Attention vs Multi Head Attention Shazeer (2019) proposes Multi Query Atten- tion (MQA), an architectural change to transformer that shares key and value embeddings across attention heads. Compared to Multi Head Attention (MHA), this lowers the memory bandwidth requirements at generation time and results in faster inference. We compare our base model to a similar model using MHA instead, with the same hyper-parameters otherwise. Note that the MHA model has more parameters (1.3B) than the base model in this setting. 5.3 DATA FILTERING ABLATIONS We experiment with a number of preprocessing methods applied to the base dataset, described in Section 5.1. Note that the \ufb01lters are applied on top of the other \ufb01lters such as near-deduplication, line length \ufb01ltering, etc. GitHub stars Do popular repositories contain good quality code? We use GitHub stars as a proxy metric. We set the minimum threshold to 5 stars, as we believe that a lower number of stars would not be an indicator of popularity. This \ufb01lter removes more than 60% of the data (in terms of volume), see Table 3. Note that more than 40% of the \ufb01les do not have stars and that setting the threshold to 10 stars would remove an additional 5% of the data. Comment-to-code ratio Good code should be well documented. With this assumption, we \ufb01lter \ufb01les with a high comments-to-code ratio. We use the ast and tokenize modules to extract docstrings and comments from Python \ufb01les, and Pygments to extract comments from Java and JavaScript \ufb01les. We then analyze the comment-to-code character ratio. We \ufb01nd that about 20% of Python and Java \ufb01les and 45% of JavaScript \ufb01les have no comments. We use a minimum threshold of 1%, removing an additional 3% of \ufb01les in each language. We also \ufb01nd that \ufb01les with a ratio above 80% have poor quality, so we \ufb01lter them out, eliminating 2% of data in all languages. Overall, this comment-to-code \ufb01lter removes 20% of the data in terms of volume. More near-deduplication While exact-match deduplication is the most common preprocessing step for code LLMs (see Table 4), Kocetkov et al. (2022) showed that near-deduplication leads to additional performance gains. Their near-deduplication pipeline largely inherited the settings from CodeParrot (Tunstall et al., 2022): MinHash (Broder, 2000) + Locality Sensitive Hashing (LSH) based on datasketch9 with unigrams (non-alphanumeric tokens) and a 0.85 Jaccard similarity threshold. Additionally, it also recalculates the true unigram Jaccard similarity during the post- processing stage to weed out any false positives. In this paper, we investigate whether different deduplication settings can further improve performance. To this end, we conduct ablation experiments on a 200K subset of the raw python dataset from the Stack v1.1. We investigate the number of false positives and false negatives by comparing the clustered \ufb01les with their real Jaccard similarity. We \ufb01nd that: 1) Using unigrams during MinHash 9https://github.com/ekzhu/datasketch 7 Preprint Model Dataset Deduplication Method InCoder Fried et al. (2022) CodeGen (Nijkamp et al., 2022) AlphaCode (Li et al., 2022) PolyCoder (Xu et al., 2022a) PaLM Coder (Chowdhery et al., 2022) Near-deduplication (Levenshtein distance) CodeParrot (Tunstall et al., 2022) Codex (Chen et al., 2021) Exact"}, {"question": " What preprocessing step, according to Kocetkov et al. (2022), leads to additional performance gains for code LLMs?,answer: Near-deduplication", "ref_chunk": "et al., 2021), MBPP (Austin et al., 2021) and MultiPL-E (Cassano et al., 2022). Tokenizer Seeing as the Santa models were the \ufb01rst models our community would train, our design choices for the tokenizer were modulated by a conservative approach, partly based on in- sights developed during the development of InCoder (Fried et al., 2022). We train a Hugging Face Tokenizer (MOI et al., 2022) using the Byte-Pair Encoding (BPE) algorithm on raw bytes with a vocabulary size of 49,152 tokens. This tokenizer was trained on 600,000 rows (Around 2.6 GB) of data\u2014200,000 for each language\u2014which were pre-tokenized using a digit splitter and the default GPT-2 pre-tokenizer regex before being converted to bytes. Training details Our base model is a 1.1B-parameter decoder-only transformer with FIM and MQA trained in float16. It has 24 layers, 16 heads and a hidden-size of 2048. The model is trained for 300K iterations with a global batch-size of 192 using Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, (cid:15) = 10\u22128 and a weight-decay of 0.1. A total of 118B tokens are seen in training. The learning-rate is set to 2 \u00d7 10\u22124 and follows a cosine decay after warming up for 2% of the training steps. Each training run takes 3.1 days to complete on 96 Tesla V100 GPUs for a total of 1.05 \u00d7 1021 FLOPs. The \ufb01nal model described in Section 6.2 uses twice the amount of compute. 5.2 ARCHITECTURE ABLATIONS We perform ablation experiments to de-risk the model architecture and training objective. Specif- ically, we investigate Fill-in-the-Middle (Bavarian et al., 2022) and Multi Query Attention (MQA) (Shazeer, 2019). FIM vs No-FIM Recent works (Fried et al., 2022; Bavarian et al., 2022) have shown that autore- gressive language-models can learn to in\ufb01ll code snippets by random transformation of the training 6 Preprint Language Base Stars Comments-to-code Near-dedup Tokenizer fertility Python Java JavaScript 75.6 GB 110 GB 82.7 GB 26.6 GB 35.8 GB 20.8 GB 65.6 GB 92.7 GB 57.5 GB 62.0 GB 88.4 GB 65.1 GB 72.5 GB 105.5 GB 76.4 GB Table 3: Data volume after additional \ufb01ltering of the Python, Java, JavaScript subsets of The Stack. data. Bavarian et al. (2022) argue that such data transformations do not harm the left-to-right gen- erative capabilities of the model. Following Bavarian et al. (2022), we implement FIM as a random transformation of the input sequence and split each training document into three parts uniformly at random: pre\ufb01x, middle and suf\ufb01x. Each part is prepended with a corresponding sentinel token, then documents are rearranged to put the middle part at the end of the sequence. The autoregressive training objective is unchanged. We use context-level FIM, apply transformations at the character level, use a FIM-rate of 0.5 and SPM+PSM joint training. We compare our base model to a model that was trained with the standard left-to-right objective only (No-FIM). Multi Query Attention vs Multi Head Attention Shazeer (2019) proposes Multi Query Atten- tion (MQA), an architectural change to transformer that shares key and value embeddings across attention heads. Compared to Multi Head Attention (MHA), this lowers the memory bandwidth requirements at generation time and results in faster inference. We compare our base model to a similar model using MHA instead, with the same hyper-parameters otherwise. Note that the MHA model has more parameters (1.3B) than the base model in this setting. 5.3 DATA FILTERING ABLATIONS We experiment with a number of preprocessing methods applied to the base dataset, described in Section 5.1. Note that the \ufb01lters are applied on top of the other \ufb01lters such as near-deduplication, line length \ufb01ltering, etc. GitHub stars Do popular repositories contain good quality code? We use GitHub stars as a proxy metric. We set the minimum threshold to 5 stars, as we believe that a lower number of stars would not be an indicator of popularity. This \ufb01lter removes more than 60% of the data (in terms of volume), see Table 3. Note that more than 40% of the \ufb01les do not have stars and that setting the threshold to 10 stars would remove an additional 5% of the data. Comment-to-code ratio Good code should be well documented. With this assumption, we \ufb01lter \ufb01les with a high comments-to-code ratio. We use the ast and tokenize modules to extract docstrings and comments from Python \ufb01les, and Pygments to extract comments from Java and JavaScript \ufb01les. We then analyze the comment-to-code character ratio. We \ufb01nd that about 20% of Python and Java \ufb01les and 45% of JavaScript \ufb01les have no comments. We use a minimum threshold of 1%, removing an additional 3% of \ufb01les in each language. We also \ufb01nd that \ufb01les with a ratio above 80% have poor quality, so we \ufb01lter them out, eliminating 2% of data in all languages. Overall, this comment-to-code \ufb01lter removes 20% of the data in terms of volume. More near-deduplication While exact-match deduplication is the most common preprocessing step for code LLMs (see Table 4), Kocetkov et al. (2022) showed that near-deduplication leads to additional performance gains. Their near-deduplication pipeline largely inherited the settings from CodeParrot (Tunstall et al., 2022): MinHash (Broder, 2000) + Locality Sensitive Hashing (LSH) based on datasketch9 with unigrams (non-alphanumeric tokens) and a 0.85 Jaccard similarity threshold. Additionally, it also recalculates the true unigram Jaccard similarity during the post- processing stage to weed out any false positives. In this paper, we investigate whether different deduplication settings can further improve performance. To this end, we conduct ablation experiments on a 200K subset of the raw python dataset from the Stack v1.1. We investigate the number of false positives and false negatives by comparing the clustered \ufb01les with their real Jaccard similarity. We \ufb01nd that: 1) Using unigrams during MinHash 9https://github.com/ekzhu/datasketch 7 Preprint Model Dataset Deduplication Method InCoder Fried et al. (2022) CodeGen (Nijkamp et al., 2022) AlphaCode (Li et al., 2022) PolyCoder (Xu et al., 2022a) PaLM Coder (Chowdhery et al., 2022) Near-deduplication (Levenshtein distance) CodeParrot (Tunstall et al., 2022) Codex (Chen et al., 2021) Exact"}, {"question": " What tool is used for the near-deduplication pipeline based on datasketch according to Kocetkov et al. (2022)?,answer: Locality Sensitive Hashing (LSH)", "ref_chunk": "et al., 2021), MBPP (Austin et al., 2021) and MultiPL-E (Cassano et al., 2022). Tokenizer Seeing as the Santa models were the \ufb01rst models our community would train, our design choices for the tokenizer were modulated by a conservative approach, partly based on in- sights developed during the development of InCoder (Fried et al., 2022). We train a Hugging Face Tokenizer (MOI et al., 2022) using the Byte-Pair Encoding (BPE) algorithm on raw bytes with a vocabulary size of 49,152 tokens. This tokenizer was trained on 600,000 rows (Around 2.6 GB) of data\u2014200,000 for each language\u2014which were pre-tokenized using a digit splitter and the default GPT-2 pre-tokenizer regex before being converted to bytes. Training details Our base model is a 1.1B-parameter decoder-only transformer with FIM and MQA trained in float16. It has 24 layers, 16 heads and a hidden-size of 2048. The model is trained for 300K iterations with a global batch-size of 192 using Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, (cid:15) = 10\u22128 and a weight-decay of 0.1. A total of 118B tokens are seen in training. The learning-rate is set to 2 \u00d7 10\u22124 and follows a cosine decay after warming up for 2% of the training steps. Each training run takes 3.1 days to complete on 96 Tesla V100 GPUs for a total of 1.05 \u00d7 1021 FLOPs. The \ufb01nal model described in Section 6.2 uses twice the amount of compute. 5.2 ARCHITECTURE ABLATIONS We perform ablation experiments to de-risk the model architecture and training objective. Specif- ically, we investigate Fill-in-the-Middle (Bavarian et al., 2022) and Multi Query Attention (MQA) (Shazeer, 2019). FIM vs No-FIM Recent works (Fried et al., 2022; Bavarian et al., 2022) have shown that autore- gressive language-models can learn to in\ufb01ll code snippets by random transformation of the training 6 Preprint Language Base Stars Comments-to-code Near-dedup Tokenizer fertility Python Java JavaScript 75.6 GB 110 GB 82.7 GB 26.6 GB 35.8 GB 20.8 GB 65.6 GB 92.7 GB 57.5 GB 62.0 GB 88.4 GB 65.1 GB 72.5 GB 105.5 GB 76.4 GB Table 3: Data volume after additional \ufb01ltering of the Python, Java, JavaScript subsets of The Stack. data. Bavarian et al. (2022) argue that such data transformations do not harm the left-to-right gen- erative capabilities of the model. Following Bavarian et al. (2022), we implement FIM as a random transformation of the input sequence and split each training document into three parts uniformly at random: pre\ufb01x, middle and suf\ufb01x. Each part is prepended with a corresponding sentinel token, then documents are rearranged to put the middle part at the end of the sequence. The autoregressive training objective is unchanged. We use context-level FIM, apply transformations at the character level, use a FIM-rate of 0.5 and SPM+PSM joint training. We compare our base model to a model that was trained with the standard left-to-right objective only (No-FIM). Multi Query Attention vs Multi Head Attention Shazeer (2019) proposes Multi Query Atten- tion (MQA), an architectural change to transformer that shares key and value embeddings across attention heads. Compared to Multi Head Attention (MHA), this lowers the memory bandwidth requirements at generation time and results in faster inference. We compare our base model to a similar model using MHA instead, with the same hyper-parameters otherwise. Note that the MHA model has more parameters (1.3B) than the base model in this setting. 5.3 DATA FILTERING ABLATIONS We experiment with a number of preprocessing methods applied to the base dataset, described in Section 5.1. Note that the \ufb01lters are applied on top of the other \ufb01lters such as near-deduplication, line length \ufb01ltering, etc. GitHub stars Do popular repositories contain good quality code? We use GitHub stars as a proxy metric. We set the minimum threshold to 5 stars, as we believe that a lower number of stars would not be an indicator of popularity. This \ufb01lter removes more than 60% of the data (in terms of volume), see Table 3. Note that more than 40% of the \ufb01les do not have stars and that setting the threshold to 10 stars would remove an additional 5% of the data. Comment-to-code ratio Good code should be well documented. With this assumption, we \ufb01lter \ufb01les with a high comments-to-code ratio. We use the ast and tokenize modules to extract docstrings and comments from Python \ufb01les, and Pygments to extract comments from Java and JavaScript \ufb01les. We then analyze the comment-to-code character ratio. We \ufb01nd that about 20% of Python and Java \ufb01les and 45% of JavaScript \ufb01les have no comments. We use a minimum threshold of 1%, removing an additional 3% of \ufb01les in each language. We also \ufb01nd that \ufb01les with a ratio above 80% have poor quality, so we \ufb01lter them out, eliminating 2% of data in all languages. Overall, this comment-to-code \ufb01lter removes 20% of the data in terms of volume. More near-deduplication While exact-match deduplication is the most common preprocessing step for code LLMs (see Table 4), Kocetkov et al. (2022) showed that near-deduplication leads to additional performance gains. Their near-deduplication pipeline largely inherited the settings from CodeParrot (Tunstall et al., 2022): MinHash (Broder, 2000) + Locality Sensitive Hashing (LSH) based on datasketch9 with unigrams (non-alphanumeric tokens) and a 0.85 Jaccard similarity threshold. Additionally, it also recalculates the true unigram Jaccard similarity during the post- processing stage to weed out any false positives. In this paper, we investigate whether different deduplication settings can further improve performance. To this end, we conduct ablation experiments on a 200K subset of the raw python dataset from the Stack v1.1. We investigate the number of false positives and false negatives by comparing the clustered \ufb01les with their real Jaccard similarity. We \ufb01nd that: 1) Using unigrams during MinHash 9https://github.com/ekzhu/datasketch 7 Preprint Model Dataset Deduplication Method InCoder Fried et al. (2022) CodeGen (Nijkamp et al., 2022) AlphaCode (Li et al., 2022) PolyCoder (Xu et al., 2022a) PaLM Coder (Chowdhery et al., 2022) Near-deduplication (Levenshtein distance) CodeParrot (Tunstall et al., 2022) Codex (Chen et al., 2021) Exact"}], "doc_text": "et al., 2021), MBPP (Austin et al., 2021) and MultiPL-E (Cassano et al., 2022). Tokenizer Seeing as the Santa models were the \ufb01rst models our community would train, our design choices for the tokenizer were modulated by a conservative approach, partly based on in- sights developed during the development of InCoder (Fried et al., 2022). We train a Hugging Face Tokenizer (MOI et al., 2022) using the Byte-Pair Encoding (BPE) algorithm on raw bytes with a vocabulary size of 49,152 tokens. This tokenizer was trained on 600,000 rows (Around 2.6 GB) of data\u2014200,000 for each language\u2014which were pre-tokenized using a digit splitter and the default GPT-2 pre-tokenizer regex before being converted to bytes. Training details Our base model is a 1.1B-parameter decoder-only transformer with FIM and MQA trained in float16. It has 24 layers, 16 heads and a hidden-size of 2048. The model is trained for 300K iterations with a global batch-size of 192 using Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, (cid:15) = 10\u22128 and a weight-decay of 0.1. A total of 118B tokens are seen in training. The learning-rate is set to 2 \u00d7 10\u22124 and follows a cosine decay after warming up for 2% of the training steps. Each training run takes 3.1 days to complete on 96 Tesla V100 GPUs for a total of 1.05 \u00d7 1021 FLOPs. The \ufb01nal model described in Section 6.2 uses twice the amount of compute. 5.2 ARCHITECTURE ABLATIONS We perform ablation experiments to de-risk the model architecture and training objective. Specif- ically, we investigate Fill-in-the-Middle (Bavarian et al., 2022) and Multi Query Attention (MQA) (Shazeer, 2019). FIM vs No-FIM Recent works (Fried et al., 2022; Bavarian et al., 2022) have shown that autore- gressive language-models can learn to in\ufb01ll code snippets by random transformation of the training 6 Preprint Language Base Stars Comments-to-code Near-dedup Tokenizer fertility Python Java JavaScript 75.6 GB 110 GB 82.7 GB 26.6 GB 35.8 GB 20.8 GB 65.6 GB 92.7 GB 57.5 GB 62.0 GB 88.4 GB 65.1 GB 72.5 GB 105.5 GB 76.4 GB Table 3: Data volume after additional \ufb01ltering of the Python, Java, JavaScript subsets of The Stack. data. Bavarian et al. (2022) argue that such data transformations do not harm the left-to-right gen- erative capabilities of the model. Following Bavarian et al. (2022), we implement FIM as a random transformation of the input sequence and split each training document into three parts uniformly at random: pre\ufb01x, middle and suf\ufb01x. Each part is prepended with a corresponding sentinel token, then documents are rearranged to put the middle part at the end of the sequence. The autoregressive training objective is unchanged. We use context-level FIM, apply transformations at the character level, use a FIM-rate of 0.5 and SPM+PSM joint training. We compare our base model to a model that was trained with the standard left-to-right objective only (No-FIM). Multi Query Attention vs Multi Head Attention Shazeer (2019) proposes Multi Query Atten- tion (MQA), an architectural change to transformer that shares key and value embeddings across attention heads. Compared to Multi Head Attention (MHA), this lowers the memory bandwidth requirements at generation time and results in faster inference. We compare our base model to a similar model using MHA instead, with the same hyper-parameters otherwise. Note that the MHA model has more parameters (1.3B) than the base model in this setting. 5.3 DATA FILTERING ABLATIONS We experiment with a number of preprocessing methods applied to the base dataset, described in Section 5.1. Note that the \ufb01lters are applied on top of the other \ufb01lters such as near-deduplication, line length \ufb01ltering, etc. GitHub stars Do popular repositories contain good quality code? We use GitHub stars as a proxy metric. We set the minimum threshold to 5 stars, as we believe that a lower number of stars would not be an indicator of popularity. This \ufb01lter removes more than 60% of the data (in terms of volume), see Table 3. Note that more than 40% of the \ufb01les do not have stars and that setting the threshold to 10 stars would remove an additional 5% of the data. Comment-to-code ratio Good code should be well documented. With this assumption, we \ufb01lter \ufb01les with a high comments-to-code ratio. We use the ast and tokenize modules to extract docstrings and comments from Python \ufb01les, and Pygments to extract comments from Java and JavaScript \ufb01les. We then analyze the comment-to-code character ratio. We \ufb01nd that about 20% of Python and Java \ufb01les and 45% of JavaScript \ufb01les have no comments. We use a minimum threshold of 1%, removing an additional 3% of \ufb01les in each language. We also \ufb01nd that \ufb01les with a ratio above 80% have poor quality, so we \ufb01lter them out, eliminating 2% of data in all languages. Overall, this comment-to-code \ufb01lter removes 20% of the data in terms of volume. More near-deduplication While exact-match deduplication is the most common preprocessing step for code LLMs (see Table 4), Kocetkov et al. (2022) showed that near-deduplication leads to additional performance gains. Their near-deduplication pipeline largely inherited the settings from CodeParrot (Tunstall et al., 2022): MinHash (Broder, 2000) + Locality Sensitive Hashing (LSH) based on datasketch9 with unigrams (non-alphanumeric tokens) and a 0.85 Jaccard similarity threshold. Additionally, it also recalculates the true unigram Jaccard similarity during the post- processing stage to weed out any false positives. In this paper, we investigate whether different deduplication settings can further improve performance. To this end, we conduct ablation experiments on a 200K subset of the raw python dataset from the Stack v1.1. We investigate the number of false positives and false negatives by comparing the clustered \ufb01les with their real Jaccard similarity. We \ufb01nd that: 1) Using unigrams during MinHash 9https://github.com/ekzhu/datasketch 7 Preprint Model Dataset Deduplication Method InCoder Fried et al. (2022) CodeGen (Nijkamp et al., 2022) AlphaCode (Li et al., 2022) PolyCoder (Xu et al., 2022a) PaLM Coder (Chowdhery et al., 2022) Near-deduplication (Levenshtein distance) CodeParrot (Tunstall et al., 2022) Codex (Chen et al., 2021) Exact"}