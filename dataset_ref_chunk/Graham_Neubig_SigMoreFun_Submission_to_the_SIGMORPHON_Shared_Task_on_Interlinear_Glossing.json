{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_SigMoreFun_Submission_to_the_SIGMORPHON_Shared_Task_on_Interlinear_Glossing.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main task of the SigMoreFun submission to the SIGMORPHON 2023 Shared Task?", "answer": " The main task is interlinear glossing.,", "ref_chunk": "SigMoreFunSubmissiontotheSIGMORPHONSharedTaskonInterlinearGlossingTaiqiHe\u2217,LindiaTjuatja\u2217,NateRobinson,ShinjiWatanabe,DavidR.Mortensen,GrahamNeubig,LoriLevinLanguageTechnologiesInstituteCarnegieMellonUniversity{taiqih,ltjuatja,nrrobins,swatanab,dmortens,gneubig,lsl}@cs.cmu.eduAbstractInoursubmissiontotheSIGMORPHON2023SharedTaskoninterlinearglossing(IGT),weexploreapproachestodataaugmentationandmodelingacrosssevenlow-resourcelanguages.Fordataaugmentation,weexploretwoap-proaches:creatingartificialdatafromthepro-videdtrainingdataandutilizingexistingIGTresourcesinotherlanguages.Onthemodelingside,wetestanenhancedversionofthepro-videdtokenclassificationbaselineaswellasapretrainedmultilingualseq2seqmodel.Ad-ditionally,weapplypost-correctionusingadictionaryforGitksan,thelanguagewiththesmallestamountofdata.Wefindthatourtokenclassificationmodelsarethebestperforming,withthehighestword-levelaccuracyforAra-pahoandhighestmorpheme-levelaccuracyforGitksanoutofallsubmissions.Wealsoshowthatdataaugmentationisaneffectivestrategy,thoughapplyingartificialdatapretraininghasverydifferenteffectsacrossbothmodelstested.1IntroductionThispaperdescribestheSigMoreFunsubmissiontotheSIGMORPHON2023SharedTaskoninterlin-earglossing.Giveninputtextinatargetlanguage,thetaskistopredictthecorrespondinginterlineargloss(usingLeipzigglossingconventions).IGTisanimportantformoflinguisticannotationforthemorphologicalanalysisoflanguages,andalsoservesasanextremelyvaluableresourceforlan-guagedocumentationandeducationforspeakersoflow-resourcelanguages.Thereweretwotracksforthissharedtask,Track1(closed)andTrack2(open).ForTrack1,sys-temscouldonlybetrainedoninputsentencesandglosses;inTrack2,systemscouldmakeuseofthemorphologicalsegmentationoftheinputaswellasany(non-IGT)externalresources.SincetheTrack2settingbettermatchesthelong-termre-\n\u2217Theseauthorscontributedequallysearchgoalsofourteam,weonlyparticipateinthisopentrack.Inoursubmission,weinvestigatetwodifferentapproaches.First,weattemptdataaugmentationbyeithercreatingourownartificialglossdatabymanipulatingtheexistingtrainingdata,orbyuti-lizingexistingresourcescontainingIGTinotherlanguages(\u00a72).Second,weexploretwodifferentmodelsforglossgeneration(\u00a73).Thefirstbuildsoffthetokenclassificationbaseline,whilethesec-ondusesapretrainedmultilingualseq2seqmodel.Finally,wealsoattempttopost-correctmodeloutputswithadictionary.WeapplythistoGitk-sanandfindthatthis,combinedwithourotherapproaches,resultsinthehighestmorpheme-levelaccuracyforGitksaninTrack2.2DataAugmentationOnemajorchallengeforthissharedtaskisthescaleofdataprovided.Allofthelanguageshavelessthan40klinesoftrainingdata,andallbutArapahohavelessthan10k.Thesmallestdataset(Gitk-san)hasonly31linesofdata.Thus,oneobviousmethodtotryisdataaugmentation.Morespecif-ically,wetrypretrainingourmodelsondifferentformsofaugmenteddatabeforetrainingthemontheoriginaltargetlanguagedata.Weexploredtwoformsofdataaugmentation.First,wegeneratedartificialglossdatainthetar-getlanguagebyswappingwordsintheexistingtrainingdata.Second,weutilizeddatafromODIN(LewisandXia,2010;Xiaetal.,2014)toseeiftransferlearningfromdatainotherlanguagescanhelpimproveperformance.2.1ArtificialDataAchallengeourteamfacedwithrespecttodataaugmentationisfiguringouthowtoobtainaddi-tionaldatawhenwedonothavemuchknowledgeofthelanguages\u2019grammaticalsystems,alongwiththefactthattheselanguagesaregenerallyfrom\n209 Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 209\u2013216 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\ndigitallyunder-resourcedlanguagefamilies.Fur-thermore,wewantedoursolutiontobeeasilyim-plementedandrelativelylanguageagnosticduetotimeconstraintsandpracticalusabilityforre-searchersworkingonavarietyoflanguages.Thus,oneavenueofdataaugmentationwetriedwasbycreatingartificialdatafromtheprovidedtrainingdata.Thisrequiresnorule-writingorknowledgeofthegrammarofthelanguage,andthuscouldbeappliedquicklyandeasilytoallofthelanguagesinthesharedtask.Weusedanaiveword-swappingmethodtoran-domlyswapmorphemesthatoccurinsimilarcon-textstocreatenewsentences.Todothis,foreachglossline,wereplaceeachwordstem(thathasaglosslabelaffix)with\u201cSTEM\u201dtocreateaskeletongloss.Wenaivelydetermineifalabelisastembycheckingifitisinlowercase.Wedonotdothistowordsthatdonothaveaffixesas(withtheexceptionofUspanteko)wedonothaveaccesstopartsofspeech,anddonotwanttoswapwordsthatwouldcreateanungrammaticalsequence.Wecreateadictionarymappingeachskeletonwordglosstopossibleactualglosses,andmapeachactualglosstopossiblesurfaceforms(wemakenoassumptionsthatthesemappingsareone-to-one).Wethenrandomlysamplekrandomskeletonglosses(inthiscase,weusedkequaltoroughlythreetimestheamountoftrainingdata)andran-domlyfillinwordsthatmatchtheformatofskele-tonwordspresentintheline.(1)to(3)belowillustrateanexampleinthisprocess.Wecreateaskeletongloss(2)fromtheGitksansentencein(1)byreplacingtheallwordstemsthathaveanaffixwith\u201cSTEM\u201dinboththesegmentationandglosstiers\u2014inthiscase,only\u2019witxw-itappliestothisstep.Thentocreatetheartificialdatain(3),wereplacetheskeletonwordandcorrespondingglosswithanotherwordfromthetrainingdatathathasthesameskeletonform,inthiscasehahla\u2019lst-it.(1)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSP\u2019witxw-itcome-SX(2)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPSTEM-itSTEM-SX(3)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPhahla\u2019lst-itwork-SXWhilethismethodmaycreateasomewhatun-naturalinputsurfacesequence(asweareunabletocapturephonologicalchangesinthesurfaceformandcorrespondingtranslationsmaybenonsensi-cal),thismethodguaranteesthatthestructureoftheglossisanaturallyoccurringsequence(asweonlyuseglossskeletonsthatarepresentinthein-put).However,alimitationofthismethodisthatitdoesnotextendtoout-of-vocabularytokensorunseenglossstructures.Furthermore,aswecannotgenerateagold-standardtranslationfortheartifi-cialdata,wedonotmakeuseofatranslationintraining.2.2ODINAnotherpotentialavenuefordataaugmentationistransferlearningfromdatainotherlanguages,whichhasbeenshowntobeaneffectivemethodtoimproveperformanceinlow-resourcesettings(Ruderetal.,2019).TheavailableresourceweutilizeisODIN,ortheOnlineDatabaseforInterlinearText(LewisandXia,2010;Xiaetal.,2014).ODINcontains158,007linesofIGT,covering1,496languages.Weusethe2.1versionofODINdataandconvertthedatasettothesharedtaskformat,andfilteroutlanguageswithfewerthanfiveglossedsentences.However,thereremainssignificantnoiseinthedatasetthatcouldcausesignificantalignmentis-suesforthetokenclassificationmodels.ThereforeweopttoonlytraintheByT5modelsonODIN,inthehopethatthismodelislesssensitivetoalign-menterrors.Indeed,wefindthattheByT5modelfinetunedfirstonODINreceivesaperformanceboostwhenfinetunedagainonthesharedtaskdata.3ModelsWeexploretwomodelsforglossgeneration.Thefirstoneisbuiltuponthetokenclassificationbase-linewithsomeimprovements,andwetreatthismodelasourinternalbaseline.Thesecondmodelwedeploytestswhetherwecanachievecompeti-tiveperformancebyfinetuningapretrainedcharac-terbasedmultilingualandmultitaskmodel,ByT5.Forthismodel,weperformminimalpreprocess-inganduserawsegmentedmorphemesandfreetranslationsifavailable.3.1TokenClassificationTransformerWeusethebaselineTrack2modelprovidedbytheorganizersasastartingpoint.Theoriginalimple-mentationrandomlyinitializesatransformermodelfromthedefaultHuggingfaceRoBERTabasecon-figuration,andusesatokenclassificationobjective\n210\n\n\n1https://en.wikipedia.org/wiki/Lezgin_alphabets\nwithcross-entropyloss,whereeachglossistreatedasadistincttoken.Themorphemesandfreetrans-lationsaretokenizedbyspaceanddashes,withpunctuationspre-separated.TheyareconcatenatedandseparatedbytheSEPtokenandareusedastheinputstothemodel.WemodifytheoriginalTrack2baselinemodeltoobtainabetterbaseline.WeusepretrainedweightsfromXLM-RoBERTabase(Conneauetal.,2020),insteadofrandomlyinitializingtheweights.Wealsoslightlymodifythemorphemetokenizertoenforcethatthenumberofmorphemetokensmatchesthenumberofoutputglosstokensexactly.Additionally,weintroducetheCOPYtokentoreplacetheglossifitmatchesthecorrespondingmorphemeexactly.AnexamplefromNatuguisshowningloss(4):(4)67COPY.COPYmnc-xbe-1MINIMzloCOPYSkulCOPYWebelievethiswouldimproveperformancebyremovingtheneedtomemorizeglossedcode-switchingandpropernouns,thoughitisonlyef-fectiveifthecode-switchedlanguageisthesameasthematrixlanguage(e.g.Arapaho),andwouldhavenoeffectifthesourcelanguageusesadif-ferentorthographyoriscode-switchedtoanotherlanguage,wheretheglosswouldnotmatchedthemorphemeformexactly.Thismethodalsocom-pressesallpunctuationmarkersintoonetoken,buttheusefulnessofthissideeffectislessclear.Sinceweareusingpretrainedweights,itisthennaturaltoexploreintegratingthepretrainedtok-enizer.SinceXLM-RoBERTawasnottrainedonanyofthesourcelanguages,itmakesthemostsensetoonlyusethepretrainedtokenizertotok-enizefreetranslations,iftheyareavailable,andextendthevocabularytoincludemorphemes.3.2FinetunedByT5Multi-taskandmulti-lingualpretrainedlargelan-guagemodelshavebeenshowntobeeffectiveformanytasks.Weexplorewhethersuchmodelscanbeusedeffectivelyforglossing.Weconductex-perimentswithbothmT5(Xueetal.,2021)andByT5(Xueetal.,2022),butByT5ispreferredbecauseittakesrawtexts(bytesorcharacters)asinputsandintheoryshouldbemoreeffectiveforunseenlanguages.Weuseapromptbasedmultilin-gualsequencetosequenceobjectiveforbothmod-els.Theprompttemplateis:\u201cGenerateinterlin-earglossfrom[sourcelanguage]:[segmentedmorphemes]withits[matrixlanguage]trans-lation:[freetranslation]Answer:\u201d.Datafromalllanguagesaremixedtogetherandshuffled,withnoupordownsampling.Afterinitialexperi-ments,wefindByT5outperformsmT5acrossalllanguages,andthereforeweonlyconductsubse-quentexperimentsonByT5andreportthosere-sults.Uponinitialexperiments,wealsofindtheresultsforLezgitobelowerthanexpected.Wehypothe-sizethatthefactthatthedataareinCyrillicscriptcausesthisdeficiency,sinceByT5wastrainedonfarlessCyrillicdatathandataintheLatinscript.Thereforewecreateanautomaticromanizationtool,sourcedfromWikipedia1andintegratedintheEpitranpackage(Mortensenetal.,2018),andconvertallLezgidatatoLatinscriptforByT5fine-tuning.AfterinspectingtheoutputsoftheByT5models,wefindcaseswherepunctuationsareattachedtothepreviousglosses,insteadofbeingseparatedbyaspaceasisstandardinthetrainingsets.Thisisprobablyduetothefactthatthemodelwaspre-trainedonuntokenizeddataandthisbehaviorispreserveddespitefinetuningontokenizeddata.Wethereforeuseasimpleregularexpressionbasedtok-enizertofixtheinconsistencies.WenoticethattheprocedureonlygivesperformanceboostonGitk-san,Lezgi,Uspanteko,andNatugu,andsoweonlyapplytheproceduretothoselanguages,leavingtherestoftheoutputsunchanged.4DictionaryPost-correction:GitksanOneofthekeychallengesforextremelylowre-sourcelanguagesistheintegrationofstructuredlinguisticdatainotherforms,suchasadictionary,intomachinelearningpipelines.Wetestasimplepost-correctionmethodfromapre-existingdictio-naryonGitksanonly,duetoitsuniquecombinationoflowresourceandeasilyobtainabledictionaryinmachinereadableform.Weusethedictionarycom-piledbyForbesetal.(2021),withoutconsultingthemorphologicalanalyzersthattheyalsoprovided.Atinferencetime,ifamorphemeisunseendur-ingtraining,wesearchfortheexactforminthedictionary.Wealsoexpandthesearchtoallsubse-quencesofmorphemeswithintheenclosingword,plusthepreviouswholewordincaseswhereapar-ticleisincludedinthedictionaryform.Thefirst\n211\n\n\nmatcheddefinitionisusedastheglossandifnoneofthesearchyieldsanexactmatch,wefallbacktothemodelprediction.Weonlyapplythismethodtothetokenclassificationmodelsbecausethealign-mentbetweenmorphemesandglossesisdirectlyestablished,whereastheseq2seqmodelsdonotguaranteethatthenumberofglossesmatchesthenumberofmorphemes.5ResultsandDiscussionTables1and2showoursystems\u2019performance(aswellastheoriginalbaseline)onthetestdatawithre-specttoword-andmorpheme-levelmicro-averagedaccuracy,respectively.Overall,thetokenclassifica-tionmodeltrainedfirstontheartificiallygeneratedaugmenteddataperformthebest,withthemodeltrainedonthesharedtaskdataonlynotfarbehind.Meanwhile,ByT5modelsperformworse,withthemodelfinetunedfirstonODINtrailingourbestmodelbyafewpercentagepoints,whilethemodelfinetunedfirstonaugmenteddataperformsworsethanthebaseline.5.1DataAugmentationOverall,wefinddataaugmentationtobeuseful.Withartificiallygenerateddata,weseetheeffectsareperhapsgreatestforthemid-resourcelanguages(ddo,lez,ntu,nyb,usp),whilethehighestandlowestresourcedlanguagesdidnotreceivemuchbenefitfrompretrainingontheartificialdata.Wethinkthisisperhapsbecausethereisa\u201csweetspot\u201dwithrespecttotheamountofdatathatisrequiredtotrainamodel.Ifthereisenoughdataalready,inthecaseofArapaho,thenthenoisinessofartificialdatawouldout-weightthebenefitoftrainingonthem.Ontheotherendofthescale,Gitksanperhapsneedsmoresyntheticdatafordataaugmentationtoyieldmeaningfulimprovements.ForByT5models,artificiallygenerateddataseemtohavetheoppositeeffect,whereperfor-manceissignificantlydegraded.Aspeculationforthiseffectisthefactthepretrainedmodelismoresemanticallyaware,andsincetheartificiallygen-eratedsentencescouldbenonsensical,themodelcouldbecomeconfused.Ontheotherhand,pre-trainingonODINyieldsimprovementsforthema-jorityofthelanguages2.ThisisencouragingsincewedidnotperformmuchpreprocessingforODIN,\n2TsezistheonlylanguagethatappearedinODIN(68sentences).Wedidnotremoveitfromthecorpusbutthisshouldhavelittleinfluenceontheperformancebecausethesizeofthedatasetisverysmall.andthereisdefinitelystillroomtomakethedatacleanerandmoreinternallyconsistent,whichinturnshouldresultinabettermodel.5.2ChoiceofHyperparametersWefindthechoiceofhyperparametersofthetokenclassificationmodelstobenecessarilylanguageanddatasetspecific.ArapahoandGitksaninpar-ticularneedspecialattention,wherethenumberoftrainingepochsneedtobeadjustedfortheveryhighandlowdatasize.WealsodevelopedmostoftheoptimizationonthetokenclassificationmodelonArapaho.However,wedidnothavetimetopropagatethechanges(usingpretrainedtokenizer,savingthelastmodelinsteadofthemodelwiththelowestvalidationloss)totherestoflanguagessinceinitialexperimentshowedthatpretrainedtokeniz-ersdidnotimproveontheotherlanguages.How-ever,afterthesubmissiondeadlineisconcluded,weranmoreexperimentsanddiscoveredthataddingpretrainedtokenizersrequiresmoretrainingsteps,andthetrainingisbettercontrolledbyspecifyingthetrainingstepsinsteadofepochs.Wedonotin-cludethoselatestexperimentsinthispaper,butourtokenclassificationmodelshavethepotentialtoperformbetterwithmorehyperparametertuning.5.3In-VersusOut-of-VocabularyErrorsOnedimensionoferroranalysisweinvestigatedwaswhatproportionofoursystems\u2019errorscomefrommorphemesorwordsthatareeitherinoroutofthetrainingdatavocabulary.Wecountamor-phemeorwordasin-vocabularyifthesurfaceformanditscorrespondingglossco-occurinthepro-videdtrainingdata(notincludingthedevelopmentdata,asourmodelsareonlytrainedonthetrainset).NotethatthereisamuchlargerproportionofOOVwordsasopposedtomorphemesduetothefactthatanunseenwordcanbecomposedofdifferentcombinationsofseenmorphemes.Table3showstheproportionofmorphemesandwordsthatareout-of-vocab(OOV)withinthetestset.Whilenearlyallthelanguageshavelessthan10%oftheirmorphemesclassifiedasOOV,GitksannotablyhasarelativelylargeportionofOOVtestdata,with\u224845%ofmorphemesand\u224878%ofwordsbeingOOV.Tables4and5showourmodels\u2019performancesonin-versesout-of-vocabtokensatthemorphemeandwordlevels,respectively.Whilewewouldintuitivelyexpectthatword-levelOOVaccuracybeaboutthesameorworsethanmorpheme-levelOOV\n212\n\n\nTable2:Morpheme-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselinewithartificialpretraininganddictionarypost-correctionisthehighestGitksanaccuracyreportedoutofallsharedtasksubmissions.\nxlmr-base91.3684.3547.47/52.8280.1788.3585.8480.0880.42xlmr-aug89.3488.1546.89/52.3982.3685.5389.4983.0881.48\nTable3:ProportionofmorphemesandwordsthatareOOVwithinthetestset.accuracy,thisisnotthecaseduetothefactthatalargeportionofout-of-vocabwordsareformedwithin-vocabmorphemes.Formostlanguages,withtheexceptionofGitksan,thereappearstobeatrade-offbetweenbetterin-vocabmorphemeperformancewithXLMRandperformanceout-of-vocabwithByT5.6RelatedWorkTherehavebeenavarietyofapproachestotheprob-lemof(semi-)automaticallygeneratinginterlineargloss.BaldridgeandPalmer(2009)investigatetheefficacyofactivelearningforthetaskofinterlinearglossing,usingannotationtimerequiredbyexpertandnon-expertannotatorsastheirmetric.Thesys-temtheyusetogenerateglosslabelsuggestionsisastandardmaximumentropyclassifier.Arule-basedapproachbySnoeketal.(2014)utilizesanFSTtogenerateglossesforPlainsCree,focusingonnouns.Samard\u017ei\u00b4cetal.(2015)viewthetaskofglossingsegmentedtextasatwo-stepprocess,firsttreatingitasastandardPOStaggingtaskandthenaddinglexicalglossesfromadictio-nary.TheydemonstratethismethodonaChintangcorpusofabout1.2millionwords.Anumberofotherworksfocusingoninterlinearglossingutilizeconditionalrandomfield(CRF)models.MoellerandHulden(2018)testthreedifferentmodelsonaverysmallLezgidataset(<3000words):aCRF(thatoutputsBIOlabelswiththecorrespondingglosspercharacterinthein-put),asegmentationandlabellingpipelinethatuti-lizesaCRF(forBIOlabels)andSVM(forglossla-bels),andanLSTMseq2seqmodel.TheyfindthattheCRFthatjointlyproducestheBIOlabelsandtagsproducedthebestresults.McMillan-Major(2020)utilizestranslationsintheirtrainingdatabycreatingtwoCRFmodels,onethatpredictsglossfromthesegmentedinputandanotherthanpre-\nModelarpddogitlezntunybuspAVG\nModelarpddogitlezntunybuspAVG\nbyt5-base78.8680.3214.8460.72b76.6776.7377.2166.48byt5-aug73.2762.374.1738.6055.1169.2570.8553.38byt5-odin80.5682.7920.5763.7777.9782.5975.7269.14\nbaseline91.1185.3425.3351.8249.0388.7182.4867.69\nbaseline85.4475.7116.4134.5441.0884.3076.5559.14\nxlmr-base85.8773.7727.86/34.11a74.1582.9980.6173.4772.14xlmr-aug82.9280.0724.74/31.2577.7778.7285.5377.5173.39\naWereportbefore/afterdictionarybasedpost-correctionforGitksan.bWetrainedthismodelwithoutromanizingLezgi.Table1:Word-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselineisthehighestArapahoaccuracyreportedoutofallsharedtasksubmissions.\nbyt5-base78.8275.7712.5944.1062.4078.9774.2560.99byt5-aug72.1057.932.6026.2435.6270.0167.7347.46byt5-odin80.8178.2412.7450.0063.3985.3073.2563.39\nMorph0.0430.0090.4500.0560.0340.0190.070\narpddogitlezntunybusp\nWord0.2420.1550.7810.1690.2140.0840.200\n213\n\n\nbyt5-aug74.7658.243.4240.2736.5471.2770.56\n35.48\nTable4:Morpheme-levelaccuracyoveralltokensofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.\n24.67\n12.31\nxlmr-base95.2085.1282.8984.7990.8787.4686.05\n23.24\n45.07\nxlmr-aug93.7283.8594.0587.6489.2490.8191.11\n29.33\n24.10\n49.79\nbyt5-odin91.9387.6663.1073.7885.9387.6083.46\n59.51\n23.54\nbyt5-odin83.4778.5518.4262.9064.3886.8575.23\n18.26\n22.41\n49.17\nTable5:Word-levelaccuracyofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.dictsfromthetranslation,andthenusesheuristicstodeterminewhichmodeltoselectfromforeachmorpheme.BarrigaMart\u00ednezetal.(2021)usedaCRFmodeltoachieve>90%accuracyforgloss-ingOtomiandfindthatitworksbetterthananRNN,whichiscomputationallymoreexpensive.Otherworks,includingoursystems,haveturnedtoneuralmethods.Kondratyuk(2019)leveragespretrainedmultilingualBERTtoencodeinputsentences,thenapplyadditionalword-levelandcharacter-levelLSTMlayersbeforejointlydecod-inglemmasandmorphologytagsusingsimplese-quencetagginglayers.Furthermore,theyshowthattwo-stagetrainingbyfirsttrainingonalllan-guagesfollowedbytrainingonthetargetlanguageismoreeffectivethantrainingthesystemonthetargetlanguagealone.AnapproachbyZhaoetal.(2020),likeMcMillan-Major(2020),makesuseoftranslationsavailableinparallelcorpora,butdosobyusingamulti-sourcetransformermodel.Theyalsoincorporatelengthcontrolandalignmentdur-inginferencetoenhancetheirmodel,andtesttheirsystemonArapaho,Tsez,andLezgi.7ConclusionInoursharedtasksubmission,weexploredataaug-mentationmethodsandmodelingstrategiesforthetaskofinterlinearglossinginsevenlow-resourcelanguages.Ourbestperformingmodelsareto-kenclassificationmodelsusingXLMR.Wedemon-stratethatpretrainingonartificialdatawithXLMRisaneffectivetechniqueforthemid-resourcetestlanguages.Additionally,inourerroranalysiswefindthatwemayhaveactuallyundertrainedourtokenclassificationmodels,andthusoursystemsmayhavethepotentialtoperformbetterwithad-ditionalhyperparametertuning.WhileourByT5modelsdidnotperformaswellasourothersys-tems,weshowthatpretrainingonODINdataiseffective,despitethisdatabeingverynoisy.Finally,wealsodemonstrateimprovementsbyutilizingadictionarytopost-correctmodeloutputsforGitk-san.\n0.00\n0.00\n0.00\n0.00\n11.24\n0.82\n12.86\n14.52\n23.60\n4.97\n56.36\n45.65\n21.14\n43.37\n1.61\n47.52\n46.94\n29.69\n13.67\nxlmr-base95.9378.1895.2384.2493.1485.8586.27\n5.79\n28.04\n2.00\n17.00\n0.41\n16.08\n14.67\n19.35\nModelarpddogitlezntunybusp\nModelarpddogitlezntunybusp\n7.49\n28.09\n28.09\n44.81\n3.23\n3.23\n54.44\n2.33\nbyt5-aug87.2268.6910.7146.0665.1374.5981.44\n30.20\nxlmr-aug92.9888.9484.7487.1087.8891.1789.31\n48.70\n40.00\n28.63\n8.67\n2.60\n2.60\n9.68\n214\n\n\nAcknowledgementsThisworkwassupportedbyNSFCISERIgrantnumber2211951,FromAcousticSignaltoMor-phosyntacticAnalysisinoneEnd-to-EndNeuralSystem.ReferencesJasonBaldridgeandAlexisPalmer.2009.Howwelldoesactivelearningactuallywork?Time-basedeval-uationofcost-reductionstrategiesforlanguagedocu-mentation.InProceedingsofthe2009ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages296\u2013305,Singapore.AssociationforCompu-tationalLinguistics.DiegoBarrigaMart\u00ednez,VictorMijangos,andXi-menaGutierrez-Vasques.2021.Automaticinterlin-earglossingforOtomilanguage.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages34\u201343,Online.AssociationforComputationalLin-guistics.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.ClarissaForbes,GarrettNicolai,andMiikkaSilfverberg.2021.AnFSTmorphologicalanalyzerforthegitksanlanguage.InProceedingsofthe18thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages188\u2013197,Online.AssociationforComputationalLinguistics.DanKondratyuk.2019.Cross-linguallemmatizationandmorphologytaggingwithtwo-stagemultilin-gualBERTfine-tuning.InProceedingsofthe16thWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages12\u201318,Florence,Italy.AssociationforComputationalLinguistics.WilliamD.LewisandFeiXia.2010.DevelopingODIN:AMultilingualRepositoryofAnnotatedLanguageDataforHundredsoftheWorld\u2019sLanguages.Liter-aryandLinguisticComputing,25(3):303\u2013319.AngelinaMcMillan-Major.2020.Automatingglossgenerationininterlinearglossedtext.ProceedingsoftheSocietyforComputationinLinguistics,3(1):338\u2013349.SarahMoellerandMansHulden.2018.Automaticglossinginalow-resourcesettingforlanguagedoc-umentation.InProceedingsoftheWorkshoponComputationalModelingofPolysyntheticLanguages,pages84\u201393.DavidR.Mortensen,SiddharthDalmia,andPatrickLittell.2018.Epitran:PrecisionG2Pformanylan-guages.InProceedingsoftheEleventhInternationalConferenceonLanguageResourcesandEvaluation(LREC2018),Miyazaki,Japan.EuropeanLanguageResourcesAssociation(ELRA).SebastianRuder,MatthewEPeters,SwabhaSwayamdipta,andThomasWolf.2019.Transferlearninginnaturallanguageprocessing.InProceed-ingsofthe2019conferenceoftheNorthAmericanchapteroftheassociationforcomputationallinguistics:Tutorials,pages15\u201318.TanjaSamard\u017ei\u00b4c,RobertSchikowski,andSabineStoll.2015.Automaticinterlinearglossingastwo-levelsequenceclassification.NoamShazeerandMitchellStern.2018.Adafactor:Adaptivelearningrateswithsublinearmemorycost.InProceedingsofthe35thInternationalConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,pages4596\u20134604.PMLR.ConorSnoek,DorothyThunder,KaidiL\u00f5o,AnttiArppe,JordanLachler,SjurMoshagen,andTrondTrosterud.2014.ModelingthenounmorphologyofPlainsCree.InProceedingsofthe2014WorkshopontheUseofComputationalMethodsintheStudyofEndangeredLanguages,pages34\u201342,Baltimore,Maryland,USA.AssociationforComputationalLinguistics.FeiXia,WilliamLewis,MichaelWayneGoodman,JoshuaCrowgey,andEmilyM.Bender.2014.En-richingODIN.InProceedingsoftheNinthInter-nationalConferenceonLanguageResourcesandEvaluation(LREC\u201914),pages3151\u20133157,Reykjavik,Iceland.EuropeanLanguageResourcesAssociation(ELRA).LintingXue,AdityaBarua,NoahConstant,RamiAl-Rfou,SharanNarang,MihirKale,AdamRoberts,andColinRaffel.2022.ByT5:Towardsatoken-freefuturewithpre-trainedbyte-to-bytemodels.Transac-tionsoftheAssociationforComputationalLinguis-tics,10:291\u2013306.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.XingyuanZhao,SatoruOzaki,AntoniosAnastasopou-los,GrahamNeubig,andLoriLevin.2020.Auto-maticinterlinearglossingforunder-resourcedlan-guagesleveragingtranslations.InProceedingsofthe28thInternationalConferenceonComputationalLinguistics,pages5397\u20135408,Barcelona,Spain(On-line).InternationalCommitteeonComputationalLin-guistics.\n215\n\n\nAHyperparameterSettingsWeuseAdafactor(ShazeerandStern,2018)astheoptimizeracrossallexperiments,withthede-faultschedulerfromHuggingFaceTransformers,abatchsizeof32forRoBERTabasedmodelsandabatchsizeof4withagradientaccumulationstepof8forByT5basedmodels.Wetrainthetokenclassificationmodelsfor40epochsexceptforAra-paho,onwhichwetrain20epochs,andGitksan,onwhichwetrain2,000steps.WetraintheByT5basedmodelsfor20epochsonallofthedatamixedtogether.\n216"}, {"question": " How many low-resource languages were explored in the submission?", "answer": " Seven low-resource languages were explored.,", "ref_chunk": "SigMoreFunSubmissiontotheSIGMORPHONSharedTaskonInterlinearGlossingTaiqiHe\u2217,LindiaTjuatja\u2217,NateRobinson,ShinjiWatanabe,DavidR.Mortensen,GrahamNeubig,LoriLevinLanguageTechnologiesInstituteCarnegieMellonUniversity{taiqih,ltjuatja,nrrobins,swatanab,dmortens,gneubig,lsl}@cs.cmu.eduAbstractInoursubmissiontotheSIGMORPHON2023SharedTaskoninterlinearglossing(IGT),weexploreapproachestodataaugmentationandmodelingacrosssevenlow-resourcelanguages.Fordataaugmentation,weexploretwoap-proaches:creatingartificialdatafromthepro-videdtrainingdataandutilizingexistingIGTresourcesinotherlanguages.Onthemodelingside,wetestanenhancedversionofthepro-videdtokenclassificationbaselineaswellasapretrainedmultilingualseq2seqmodel.Ad-ditionally,weapplypost-correctionusingadictionaryforGitksan,thelanguagewiththesmallestamountofdata.Wefindthatourtokenclassificationmodelsarethebestperforming,withthehighestword-levelaccuracyforAra-pahoandhighestmorpheme-levelaccuracyforGitksanoutofallsubmissions.Wealsoshowthatdataaugmentationisaneffectivestrategy,thoughapplyingartificialdatapretraininghasverydifferenteffectsacrossbothmodelstested.1IntroductionThispaperdescribestheSigMoreFunsubmissiontotheSIGMORPHON2023SharedTaskoninterlin-earglossing.Giveninputtextinatargetlanguage,thetaskistopredictthecorrespondinginterlineargloss(usingLeipzigglossingconventions).IGTisanimportantformoflinguisticannotationforthemorphologicalanalysisoflanguages,andalsoservesasanextremelyvaluableresourceforlan-guagedocumentationandeducationforspeakersoflow-resourcelanguages.Thereweretwotracksforthissharedtask,Track1(closed)andTrack2(open).ForTrack1,sys-temscouldonlybetrainedoninputsentencesandglosses;inTrack2,systemscouldmakeuseofthemorphologicalsegmentationoftheinputaswellasany(non-IGT)externalresources.SincetheTrack2settingbettermatchesthelong-termre-\n\u2217Theseauthorscontributedequallysearchgoalsofourteam,weonlyparticipateinthisopentrack.Inoursubmission,weinvestigatetwodifferentapproaches.First,weattemptdataaugmentationbyeithercreatingourownartificialglossdatabymanipulatingtheexistingtrainingdata,orbyuti-lizingexistingresourcescontainingIGTinotherlanguages(\u00a72).Second,weexploretwodifferentmodelsforglossgeneration(\u00a73).Thefirstbuildsoffthetokenclassificationbaseline,whilethesec-ondusesapretrainedmultilingualseq2seqmodel.Finally,wealsoattempttopost-correctmodeloutputswithadictionary.WeapplythistoGitk-sanandfindthatthis,combinedwithourotherapproaches,resultsinthehighestmorpheme-levelaccuracyforGitksaninTrack2.2DataAugmentationOnemajorchallengeforthissharedtaskisthescaleofdataprovided.Allofthelanguageshavelessthan40klinesoftrainingdata,andallbutArapahohavelessthan10k.Thesmallestdataset(Gitk-san)hasonly31linesofdata.Thus,oneobviousmethodtotryisdataaugmentation.Morespecif-ically,wetrypretrainingourmodelsondifferentformsofaugmenteddatabeforetrainingthemontheoriginaltargetlanguagedata.Weexploredtwoformsofdataaugmentation.First,wegeneratedartificialglossdatainthetar-getlanguagebyswappingwordsintheexistingtrainingdata.Second,weutilizeddatafromODIN(LewisandXia,2010;Xiaetal.,2014)toseeiftransferlearningfromdatainotherlanguagescanhelpimproveperformance.2.1ArtificialDataAchallengeourteamfacedwithrespecttodataaugmentationisfiguringouthowtoobtainaddi-tionaldatawhenwedonothavemuchknowledgeofthelanguages\u2019grammaticalsystems,alongwiththefactthattheselanguagesaregenerallyfrom\n209 Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 209\u2013216 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\ndigitallyunder-resourcedlanguagefamilies.Fur-thermore,wewantedoursolutiontobeeasilyim-plementedandrelativelylanguageagnosticduetotimeconstraintsandpracticalusabilityforre-searchersworkingonavarietyoflanguages.Thus,oneavenueofdataaugmentationwetriedwasbycreatingartificialdatafromtheprovidedtrainingdata.Thisrequiresnorule-writingorknowledgeofthegrammarofthelanguage,andthuscouldbeappliedquicklyandeasilytoallofthelanguagesinthesharedtask.Weusedanaiveword-swappingmethodtoran-domlyswapmorphemesthatoccurinsimilarcon-textstocreatenewsentences.Todothis,foreachglossline,wereplaceeachwordstem(thathasaglosslabelaffix)with\u201cSTEM\u201dtocreateaskeletongloss.Wenaivelydetermineifalabelisastembycheckingifitisinlowercase.Wedonotdothistowordsthatdonothaveaffixesas(withtheexceptionofUspanteko)wedonothaveaccesstopartsofspeech,anddonotwanttoswapwordsthatwouldcreateanungrammaticalsequence.Wecreateadictionarymappingeachskeletonwordglosstopossibleactualglosses,andmapeachactualglosstopossiblesurfaceforms(wemakenoassumptionsthatthesemappingsareone-to-one).Wethenrandomlysamplekrandomskeletonglosses(inthiscase,weusedkequaltoroughlythreetimestheamountoftrainingdata)andran-domlyfillinwordsthatmatchtheformatofskele-tonwordspresentintheline.(1)to(3)belowillustrateanexampleinthisprocess.Wecreateaskeletongloss(2)fromtheGitksansentencein(1)byreplacingtheallwordstemsthathaveanaffixwith\u201cSTEM\u201dinboththesegmentationandglosstiers\u2014inthiscase,only\u2019witxw-itappliestothisstep.Thentocreatetheartificialdatain(3),wereplacetheskeletonwordandcorrespondingglosswithanotherwordfromthetrainingdatathathasthesameskeletonform,inthiscasehahla\u2019lst-it.(1)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSP\u2019witxw-itcome-SX(2)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPSTEM-itSTEM-SX(3)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPhahla\u2019lst-itwork-SXWhilethismethodmaycreateasomewhatun-naturalinputsurfacesequence(asweareunabletocapturephonologicalchangesinthesurfaceformandcorrespondingtranslationsmaybenonsensi-cal),thismethodguaranteesthatthestructureoftheglossisanaturallyoccurringsequence(asweonlyuseglossskeletonsthatarepresentinthein-put).However,alimitationofthismethodisthatitdoesnotextendtoout-of-vocabularytokensorunseenglossstructures.Furthermore,aswecannotgenerateagold-standardtranslationfortheartifi-cialdata,wedonotmakeuseofatranslationintraining.2.2ODINAnotherpotentialavenuefordataaugmentationistransferlearningfromdatainotherlanguages,whichhasbeenshowntobeaneffectivemethodtoimproveperformanceinlow-resourcesettings(Ruderetal.,2019).TheavailableresourceweutilizeisODIN,ortheOnlineDatabaseforInterlinearText(LewisandXia,2010;Xiaetal.,2014).ODINcontains158,007linesofIGT,covering1,496languages.Weusethe2.1versionofODINdataandconvertthedatasettothesharedtaskformat,andfilteroutlanguageswithfewerthanfiveglossedsentences.However,thereremainssignificantnoiseinthedatasetthatcouldcausesignificantalignmentis-suesforthetokenclassificationmodels.ThereforeweopttoonlytraintheByT5modelsonODIN,inthehopethatthismodelislesssensitivetoalign-menterrors.Indeed,wefindthattheByT5modelfinetunedfirstonODINreceivesaperformanceboostwhenfinetunedagainonthesharedtaskdata.3ModelsWeexploretwomodelsforglossgeneration.Thefirstoneisbuiltuponthetokenclassificationbase-linewithsomeimprovements,andwetreatthismodelasourinternalbaseline.Thesecondmodelwedeploytestswhetherwecanachievecompeti-tiveperformancebyfinetuningapretrainedcharac-terbasedmultilingualandmultitaskmodel,ByT5.Forthismodel,weperformminimalpreprocess-inganduserawsegmentedmorphemesandfreetranslationsifavailable.3.1TokenClassificationTransformerWeusethebaselineTrack2modelprovidedbytheorganizersasastartingpoint.Theoriginalimple-mentationrandomlyinitializesatransformermodelfromthedefaultHuggingfaceRoBERTabasecon-figuration,andusesatokenclassificationobjective\n210\n\n\n1https://en.wikipedia.org/wiki/Lezgin_alphabets\nwithcross-entropyloss,whereeachglossistreatedasadistincttoken.Themorphemesandfreetrans-lationsaretokenizedbyspaceanddashes,withpunctuationspre-separated.TheyareconcatenatedandseparatedbytheSEPtokenandareusedastheinputstothemodel.WemodifytheoriginalTrack2baselinemodeltoobtainabetterbaseline.WeusepretrainedweightsfromXLM-RoBERTabase(Conneauetal.,2020),insteadofrandomlyinitializingtheweights.Wealsoslightlymodifythemorphemetokenizertoenforcethatthenumberofmorphemetokensmatchesthenumberofoutputglosstokensexactly.Additionally,weintroducetheCOPYtokentoreplacetheglossifitmatchesthecorrespondingmorphemeexactly.AnexamplefromNatuguisshowningloss(4):(4)67COPY.COPYmnc-xbe-1MINIMzloCOPYSkulCOPYWebelievethiswouldimproveperformancebyremovingtheneedtomemorizeglossedcode-switchingandpropernouns,thoughitisonlyef-fectiveifthecode-switchedlanguageisthesameasthematrixlanguage(e.g.Arapaho),andwouldhavenoeffectifthesourcelanguageusesadif-ferentorthographyoriscode-switchedtoanotherlanguage,wheretheglosswouldnotmatchedthemorphemeformexactly.Thismethodalsocom-pressesallpunctuationmarkersintoonetoken,buttheusefulnessofthissideeffectislessclear.Sinceweareusingpretrainedweights,itisthennaturaltoexploreintegratingthepretrainedtok-enizer.SinceXLM-RoBERTawasnottrainedonanyofthesourcelanguages,itmakesthemostsensetoonlyusethepretrainedtokenizertotok-enizefreetranslations,iftheyareavailable,andextendthevocabularytoincludemorphemes.3.2FinetunedByT5Multi-taskandmulti-lingualpretrainedlargelan-guagemodelshavebeenshowntobeeffectiveformanytasks.Weexplorewhethersuchmodelscanbeusedeffectivelyforglossing.Weconductex-perimentswithbothmT5(Xueetal.,2021)andByT5(Xueetal.,2022),butByT5ispreferredbecauseittakesrawtexts(bytesorcharacters)asinputsandintheoryshouldbemoreeffectiveforunseenlanguages.Weuseapromptbasedmultilin-gualsequencetosequenceobjectiveforbothmod-els.Theprompttemplateis:\u201cGenerateinterlin-earglossfrom[sourcelanguage]:[segmentedmorphemes]withits[matrixlanguage]trans-lation:[freetranslation]Answer:\u201d.Datafromalllanguagesaremixedtogetherandshuffled,withnoupordownsampling.Afterinitialexperi-ments,wefindByT5outperformsmT5acrossalllanguages,andthereforeweonlyconductsubse-quentexperimentsonByT5andreportthosere-sults.Uponinitialexperiments,wealsofindtheresultsforLezgitobelowerthanexpected.Wehypothe-sizethatthefactthatthedataareinCyrillicscriptcausesthisdeficiency,sinceByT5wastrainedonfarlessCyrillicdatathandataintheLatinscript.Thereforewecreateanautomaticromanizationtool,sourcedfromWikipedia1andintegratedintheEpitranpackage(Mortensenetal.,2018),andconvertallLezgidatatoLatinscriptforByT5fine-tuning.AfterinspectingtheoutputsoftheByT5models,wefindcaseswherepunctuationsareattachedtothepreviousglosses,insteadofbeingseparatedbyaspaceasisstandardinthetrainingsets.Thisisprobablyduetothefactthatthemodelwaspre-trainedonuntokenizeddataandthisbehaviorispreserveddespitefinetuningontokenizeddata.Wethereforeuseasimpleregularexpressionbasedtok-enizertofixtheinconsistencies.WenoticethattheprocedureonlygivesperformanceboostonGitk-san,Lezgi,Uspanteko,andNatugu,andsoweonlyapplytheproceduretothoselanguages,leavingtherestoftheoutputsunchanged.4DictionaryPost-correction:GitksanOneofthekeychallengesforextremelylowre-sourcelanguagesistheintegrationofstructuredlinguisticdatainotherforms,suchasadictionary,intomachinelearningpipelines.Wetestasimplepost-correctionmethodfromapre-existingdictio-naryonGitksanonly,duetoitsuniquecombinationoflowresourceandeasilyobtainabledictionaryinmachinereadableform.Weusethedictionarycom-piledbyForbesetal.(2021),withoutconsultingthemorphologicalanalyzersthattheyalsoprovided.Atinferencetime,ifamorphemeisunseendur-ingtraining,wesearchfortheexactforminthedictionary.Wealsoexpandthesearchtoallsubse-quencesofmorphemeswithintheenclosingword,plusthepreviouswholewordincaseswhereapar-ticleisincludedinthedictionaryform.Thefirst\n211\n\n\nmatcheddefinitionisusedastheglossandifnoneofthesearchyieldsanexactmatch,wefallbacktothemodelprediction.Weonlyapplythismethodtothetokenclassificationmodelsbecausethealign-mentbetweenmorphemesandglossesisdirectlyestablished,whereastheseq2seqmodelsdonotguaranteethatthenumberofglossesmatchesthenumberofmorphemes.5ResultsandDiscussionTables1and2showoursystems\u2019performance(aswellastheoriginalbaseline)onthetestdatawithre-specttoword-andmorpheme-levelmicro-averagedaccuracy,respectively.Overall,thetokenclassifica-tionmodeltrainedfirstontheartificiallygeneratedaugmenteddataperformthebest,withthemodeltrainedonthesharedtaskdataonlynotfarbehind.Meanwhile,ByT5modelsperformworse,withthemodelfinetunedfirstonODINtrailingourbestmodelbyafewpercentagepoints,whilethemodelfinetunedfirstonaugmenteddataperformsworsethanthebaseline.5.1DataAugmentationOverall,wefinddataaugmentationtobeuseful.Withartificiallygenerateddata,weseetheeffectsareperhapsgreatestforthemid-resourcelanguages(ddo,lez,ntu,nyb,usp),whilethehighestandlowestresourcedlanguagesdidnotreceivemuchbenefitfrompretrainingontheartificialdata.Wethinkthisisperhapsbecausethereisa\u201csweetspot\u201dwithrespecttotheamountofdatathatisrequiredtotrainamodel.Ifthereisenoughdataalready,inthecaseofArapaho,thenthenoisinessofartificialdatawouldout-weightthebenefitoftrainingonthem.Ontheotherendofthescale,Gitksanperhapsneedsmoresyntheticdatafordataaugmentationtoyieldmeaningfulimprovements.ForByT5models,artificiallygenerateddataseemtohavetheoppositeeffect,whereperfor-manceissignificantlydegraded.Aspeculationforthiseffectisthefactthepretrainedmodelismoresemanticallyaware,andsincetheartificiallygen-eratedsentencescouldbenonsensical,themodelcouldbecomeconfused.Ontheotherhand,pre-trainingonODINyieldsimprovementsforthema-jorityofthelanguages2.ThisisencouragingsincewedidnotperformmuchpreprocessingforODIN,\n2TsezistheonlylanguagethatappearedinODIN(68sentences).Wedidnotremoveitfromthecorpusbutthisshouldhavelittleinfluenceontheperformancebecausethesizeofthedatasetisverysmall.andthereisdefinitelystillroomtomakethedatacleanerandmoreinternallyconsistent,whichinturnshouldresultinabettermodel.5.2ChoiceofHyperparametersWefindthechoiceofhyperparametersofthetokenclassificationmodelstobenecessarilylanguageanddatasetspecific.ArapahoandGitksaninpar-ticularneedspecialattention,wherethenumberoftrainingepochsneedtobeadjustedfortheveryhighandlowdatasize.WealsodevelopedmostoftheoptimizationonthetokenclassificationmodelonArapaho.However,wedidnothavetimetopropagatethechanges(usingpretrainedtokenizer,savingthelastmodelinsteadofthemodelwiththelowestvalidationloss)totherestoflanguagessinceinitialexperimentshowedthatpretrainedtokeniz-ersdidnotimproveontheotherlanguages.How-ever,afterthesubmissiondeadlineisconcluded,weranmoreexperimentsanddiscoveredthataddingpretrainedtokenizersrequiresmoretrainingsteps,andthetrainingisbettercontrolledbyspecifyingthetrainingstepsinsteadofepochs.Wedonotin-cludethoselatestexperimentsinthispaper,butourtokenclassificationmodelshavethepotentialtoperformbetterwithmorehyperparametertuning.5.3In-VersusOut-of-VocabularyErrorsOnedimensionoferroranalysisweinvestigatedwaswhatproportionofoursystems\u2019errorscomefrommorphemesorwordsthatareeitherinoroutofthetrainingdatavocabulary.Wecountamor-phemeorwordasin-vocabularyifthesurfaceformanditscorrespondingglossco-occurinthepro-videdtrainingdata(notincludingthedevelopmentdata,asourmodelsareonlytrainedonthetrainset).NotethatthereisamuchlargerproportionofOOVwordsasopposedtomorphemesduetothefactthatanunseenwordcanbecomposedofdifferentcombinationsofseenmorphemes.Table3showstheproportionofmorphemesandwordsthatareout-of-vocab(OOV)withinthetestset.Whilenearlyallthelanguageshavelessthan10%oftheirmorphemesclassifiedasOOV,GitksannotablyhasarelativelylargeportionofOOVtestdata,with\u224845%ofmorphemesand\u224878%ofwordsbeingOOV.Tables4and5showourmodels\u2019performancesonin-versesout-of-vocabtokensatthemorphemeandwordlevels,respectively.Whilewewouldintuitivelyexpectthatword-levelOOVaccuracybeaboutthesameorworsethanmorpheme-levelOOV\n212\n\n\nTable2:Morpheme-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselinewithartificialpretraininganddictionarypost-correctionisthehighestGitksanaccuracyreportedoutofallsharedtasksubmissions.\nxlmr-base91.3684.3547.47/52.8280.1788.3585.8480.0880.42xlmr-aug89.3488.1546.89/52.3982.3685.5389.4983.0881.48\nTable3:ProportionofmorphemesandwordsthatareOOVwithinthetestset.accuracy,thisisnotthecaseduetothefactthatalargeportionofout-of-vocabwordsareformedwithin-vocabmorphemes.Formostlanguages,withtheexceptionofGitksan,thereappearstobeatrade-offbetweenbetterin-vocabmorphemeperformancewithXLMRandperformanceout-of-vocabwithByT5.6RelatedWorkTherehavebeenavarietyofapproachestotheprob-lemof(semi-)automaticallygeneratinginterlineargloss.BaldridgeandPalmer(2009)investigatetheefficacyofactivelearningforthetaskofinterlinearglossing,usingannotationtimerequiredbyexpertandnon-expertannotatorsastheirmetric.Thesys-temtheyusetogenerateglosslabelsuggestionsisastandardmaximumentropyclassifier.Arule-basedapproachbySnoeketal.(2014)utilizesanFSTtogenerateglossesforPlainsCree,focusingonnouns.Samard\u017ei\u00b4cetal.(2015)viewthetaskofglossingsegmentedtextasatwo-stepprocess,firsttreatingitasastandardPOStaggingtaskandthenaddinglexicalglossesfromadictio-nary.TheydemonstratethismethodonaChintangcorpusofabout1.2millionwords.Anumberofotherworksfocusingoninterlinearglossingutilizeconditionalrandomfield(CRF)models.MoellerandHulden(2018)testthreedifferentmodelsonaverysmallLezgidataset(<3000words):aCRF(thatoutputsBIOlabelswiththecorrespondingglosspercharacterinthein-put),asegmentationandlabellingpipelinethatuti-lizesaCRF(forBIOlabels)andSVM(forglossla-bels),andanLSTMseq2seqmodel.TheyfindthattheCRFthatjointlyproducestheBIOlabelsandtagsproducedthebestresults.McMillan-Major(2020)utilizestranslationsintheirtrainingdatabycreatingtwoCRFmodels,onethatpredictsglossfromthesegmentedinputandanotherthanpre-\nModelarpddogitlezntunybuspAVG\nModelarpddogitlezntunybuspAVG\nbyt5-base78.8680.3214.8460.72b76.6776.7377.2166.48byt5-aug73.2762.374.1738.6055.1169.2570.8553.38byt5-odin80.5682.7920.5763.7777.9782.5975.7269.14\nbaseline91.1185.3425.3351.8249.0388.7182.4867.69\nbaseline85.4475.7116.4134.5441.0884.3076.5559.14\nxlmr-base85.8773.7727.86/34.11a74.1582.9980.6173.4772.14xlmr-aug82.9280.0724.74/31.2577.7778.7285.5377.5173.39\naWereportbefore/afterdictionarybasedpost-correctionforGitksan.bWetrainedthismodelwithoutromanizingLezgi.Table1:Word-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselineisthehighestArapahoaccuracyreportedoutofallsharedtasksubmissions.\nbyt5-base78.8275.7712.5944.1062.4078.9774.2560.99byt5-aug72.1057.932.6026.2435.6270.0167.7347.46byt5-odin80.8178.2412.7450.0063.3985.3073.2563.39\nMorph0.0430.0090.4500.0560.0340.0190.070\narpddogitlezntunybusp\nWord0.2420.1550.7810.1690.2140.0840.200\n213\n\n\nbyt5-aug74.7658.243.4240.2736.5471.2770.56\n35.48\nTable4:Morpheme-levelaccuracyoveralltokensofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.\n24.67\n12.31\nxlmr-base95.2085.1282.8984.7990.8787.4686.05\n23.24\n45.07\nxlmr-aug93.7283.8594.0587.6489.2490.8191.11\n29.33\n24.10\n49.79\nbyt5-odin91.9387.6663.1073.7885.9387.6083.46\n59.51\n23.54\nbyt5-odin83.4778.5518.4262.9064.3886.8575.23\n18.26\n22.41\n49.17\nTable5:Word-levelaccuracyofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.dictsfromthetranslation,andthenusesheuristicstodeterminewhichmodeltoselectfromforeachmorpheme.BarrigaMart\u00ednezetal.(2021)usedaCRFmodeltoachieve>90%accuracyforgloss-ingOtomiandfindthatitworksbetterthananRNN,whichiscomputationallymoreexpensive.Otherworks,includingoursystems,haveturnedtoneuralmethods.Kondratyuk(2019)leveragespretrainedmultilingualBERTtoencodeinputsentences,thenapplyadditionalword-levelandcharacter-levelLSTMlayersbeforejointlydecod-inglemmasandmorphologytagsusingsimplese-quencetagginglayers.Furthermore,theyshowthattwo-stagetrainingbyfirsttrainingonalllan-guagesfollowedbytrainingonthetargetlanguageismoreeffectivethantrainingthesystemonthetargetlanguagealone.AnapproachbyZhaoetal.(2020),likeMcMillan-Major(2020),makesuseoftranslationsavailableinparallelcorpora,butdosobyusingamulti-sourcetransformermodel.Theyalsoincorporatelengthcontrolandalignmentdur-inginferencetoenhancetheirmodel,andtesttheirsystemonArapaho,Tsez,andLezgi.7ConclusionInoursharedtasksubmission,weexploredataaug-mentationmethodsandmodelingstrategiesforthetaskofinterlinearglossinginsevenlow-resourcelanguages.Ourbestperformingmodelsareto-kenclassificationmodelsusingXLMR.Wedemon-stratethatpretrainingonartificialdatawithXLMRisaneffectivetechniqueforthemid-resourcetestlanguages.Additionally,inourerroranalysiswefindthatwemayhaveactuallyundertrainedourtokenclassificationmodels,andthusoursystemsmayhavethepotentialtoperformbetterwithad-ditionalhyperparametertuning.WhileourByT5modelsdidnotperformaswellasourothersys-tems,weshowthatpretrainingonODINdataiseffective,despitethisdatabeingverynoisy.Finally,wealsodemonstrateimprovementsbyutilizingadictionarytopost-correctmodeloutputsforGitk-san.\n0.00\n0.00\n0.00\n0.00\n11.24\n0.82\n12.86\n14.52\n23.60\n4.97\n56.36\n45.65\n21.14\n43.37\n1.61\n47.52\n46.94\n29.69\n13.67\nxlmr-base95.9378.1895.2384.2493.1485.8586.27\n5.79\n28.04\n2.00\n17.00\n0.41\n16.08\n14.67\n19.35\nModelarpddogitlezntunybusp\nModelarpddogitlezntunybusp\n7.49\n28.09\n28.09\n44.81\n3.23\n3.23\n54.44\n2.33\nbyt5-aug87.2268.6910.7146.0665.1374.5981.44\n30.20\nxlmr-aug92.9888.9484.7487.1087.8891.1789.31\n48.70\n40.00\n28.63\n8.67\n2.60\n2.60\n9.68\n214\n\n\nAcknowledgementsThisworkwassupportedbyNSFCISERIgrantnumber2211951,FromAcousticSignaltoMor-phosyntacticAnalysisinoneEnd-to-EndNeuralSystem.ReferencesJasonBaldridgeandAlexisPalmer.2009.Howwelldoesactivelearningactuallywork?Time-basedeval-uationofcost-reductionstrategiesforlanguagedocu-mentation.InProceedingsofthe2009ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages296\u2013305,Singapore.AssociationforCompu-tationalLinguistics.DiegoBarrigaMart\u00ednez,VictorMijangos,andXi-menaGutierrez-Vasques.2021.Automaticinterlin-earglossingforOtomilanguage.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages34\u201343,Online.AssociationforComputationalLin-guistics.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.ClarissaForbes,GarrettNicolai,andMiikkaSilfverberg.2021.AnFSTmorphologicalanalyzerforthegitksanlanguage.InProceedingsofthe18thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages188\u2013197,Online.AssociationforComputationalLinguistics.DanKondratyuk.2019.Cross-linguallemmatizationandmorphologytaggingwithtwo-stagemultilin-gualBERTfine-tuning.InProceedingsofthe16thWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages12\u201318,Florence,Italy.AssociationforComputationalLinguistics.WilliamD.LewisandFeiXia.2010.DevelopingODIN:AMultilingualRepositoryofAnnotatedLanguageDataforHundredsoftheWorld\u2019sLanguages.Liter-aryandLinguisticComputing,25(3):303\u2013319.AngelinaMcMillan-Major.2020.Automatingglossgenerationininterlinearglossedtext.ProceedingsoftheSocietyforComputationinLinguistics,3(1):338\u2013349.SarahMoellerandMansHulden.2018.Automaticglossinginalow-resourcesettingforlanguagedoc-umentation.InProceedingsoftheWorkshoponComputationalModelingofPolysyntheticLanguages,pages84\u201393.DavidR.Mortensen,SiddharthDalmia,andPatrickLittell.2018.Epitran:PrecisionG2Pformanylan-guages.InProceedingsoftheEleventhInternationalConferenceonLanguageResourcesandEvaluation(LREC2018),Miyazaki,Japan.EuropeanLanguageResourcesAssociation(ELRA).SebastianRuder,MatthewEPeters,SwabhaSwayamdipta,andThomasWolf.2019.Transferlearninginnaturallanguageprocessing.InProceed-ingsofthe2019conferenceoftheNorthAmericanchapteroftheassociationforcomputationallinguistics:Tutorials,pages15\u201318.TanjaSamard\u017ei\u00b4c,RobertSchikowski,andSabineStoll.2015.Automaticinterlinearglossingastwo-levelsequenceclassification.NoamShazeerandMitchellStern.2018.Adafactor:Adaptivelearningrateswithsublinearmemorycost.InProceedingsofthe35thInternationalConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,pages4596\u20134604.PMLR.ConorSnoek,DorothyThunder,KaidiL\u00f5o,AnttiArppe,JordanLachler,SjurMoshagen,andTrondTrosterud.2014.ModelingthenounmorphologyofPlainsCree.InProceedingsofthe2014WorkshopontheUseofComputationalMethodsintheStudyofEndangeredLanguages,pages34\u201342,Baltimore,Maryland,USA.AssociationforComputationalLinguistics.FeiXia,WilliamLewis,MichaelWayneGoodman,JoshuaCrowgey,andEmilyM.Bender.2014.En-richingODIN.InProceedingsoftheNinthInter-nationalConferenceonLanguageResourcesandEvaluation(LREC\u201914),pages3151\u20133157,Reykjavik,Iceland.EuropeanLanguageResourcesAssociation(ELRA).LintingXue,AdityaBarua,NoahConstant,RamiAl-Rfou,SharanNarang,MihirKale,AdamRoberts,andColinRaffel.2022.ByT5:Towardsatoken-freefuturewithpre-trainedbyte-to-bytemodels.Transac-tionsoftheAssociationforComputationalLinguis-tics,10:291\u2013306.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.XingyuanZhao,SatoruOzaki,AntoniosAnastasopou-los,GrahamNeubig,andLoriLevin.2020.Auto-maticinterlinearglossingforunder-resourcedlan-guagesleveragingtranslations.InProceedingsofthe28thInternationalConferenceonComputationalLinguistics,pages5397\u20135408,Barcelona,Spain(On-line).InternationalCommitteeonComputationalLin-guistics.\n215\n\n\nAHyperparameterSettingsWeuseAdafactor(ShazeerandStern,2018)astheoptimizeracrossallexperiments,withthede-faultschedulerfromHuggingFaceTransformers,abatchsizeof32forRoBERTabasedmodelsandabatchsizeof4withagradientaccumulationstepof8forByT5basedmodels.Wetrainthetokenclassificationmodelsfor40epochsexceptforAra-paho,onwhichwetrain20epochs,andGitksan,onwhichwetrain2,000steps.WetraintheByT5basedmodelsfor20epochsonallofthedatamixedtogether.\n216"}, {"question": " What were the two approaches to data augmentation explored in the submission?", "answer": " The two approaches were creating artificial data from the provided training data and utilizing existing IGT resources in other languages.,", "ref_chunk": "SigMoreFunSubmissiontotheSIGMORPHONSharedTaskonInterlinearGlossingTaiqiHe\u2217,LindiaTjuatja\u2217,NateRobinson,ShinjiWatanabe,DavidR.Mortensen,GrahamNeubig,LoriLevinLanguageTechnologiesInstituteCarnegieMellonUniversity{taiqih,ltjuatja,nrrobins,swatanab,dmortens,gneubig,lsl}@cs.cmu.eduAbstractInoursubmissiontotheSIGMORPHON2023SharedTaskoninterlinearglossing(IGT),weexploreapproachestodataaugmentationandmodelingacrosssevenlow-resourcelanguages.Fordataaugmentation,weexploretwoap-proaches:creatingartificialdatafromthepro-videdtrainingdataandutilizingexistingIGTresourcesinotherlanguages.Onthemodelingside,wetestanenhancedversionofthepro-videdtokenclassificationbaselineaswellasapretrainedmultilingualseq2seqmodel.Ad-ditionally,weapplypost-correctionusingadictionaryforGitksan,thelanguagewiththesmallestamountofdata.Wefindthatourtokenclassificationmodelsarethebestperforming,withthehighestword-levelaccuracyforAra-pahoandhighestmorpheme-levelaccuracyforGitksanoutofallsubmissions.Wealsoshowthatdataaugmentationisaneffectivestrategy,thoughapplyingartificialdatapretraininghasverydifferenteffectsacrossbothmodelstested.1IntroductionThispaperdescribestheSigMoreFunsubmissiontotheSIGMORPHON2023SharedTaskoninterlin-earglossing.Giveninputtextinatargetlanguage,thetaskistopredictthecorrespondinginterlineargloss(usingLeipzigglossingconventions).IGTisanimportantformoflinguisticannotationforthemorphologicalanalysisoflanguages,andalsoservesasanextremelyvaluableresourceforlan-guagedocumentationandeducationforspeakersoflow-resourcelanguages.Thereweretwotracksforthissharedtask,Track1(closed)andTrack2(open).ForTrack1,sys-temscouldonlybetrainedoninputsentencesandglosses;inTrack2,systemscouldmakeuseofthemorphologicalsegmentationoftheinputaswellasany(non-IGT)externalresources.SincetheTrack2settingbettermatchesthelong-termre-\n\u2217Theseauthorscontributedequallysearchgoalsofourteam,weonlyparticipateinthisopentrack.Inoursubmission,weinvestigatetwodifferentapproaches.First,weattemptdataaugmentationbyeithercreatingourownartificialglossdatabymanipulatingtheexistingtrainingdata,orbyuti-lizingexistingresourcescontainingIGTinotherlanguages(\u00a72).Second,weexploretwodifferentmodelsforglossgeneration(\u00a73).Thefirstbuildsoffthetokenclassificationbaseline,whilethesec-ondusesapretrainedmultilingualseq2seqmodel.Finally,wealsoattempttopost-correctmodeloutputswithadictionary.WeapplythistoGitk-sanandfindthatthis,combinedwithourotherapproaches,resultsinthehighestmorpheme-levelaccuracyforGitksaninTrack2.2DataAugmentationOnemajorchallengeforthissharedtaskisthescaleofdataprovided.Allofthelanguageshavelessthan40klinesoftrainingdata,andallbutArapahohavelessthan10k.Thesmallestdataset(Gitk-san)hasonly31linesofdata.Thus,oneobviousmethodtotryisdataaugmentation.Morespecif-ically,wetrypretrainingourmodelsondifferentformsofaugmenteddatabeforetrainingthemontheoriginaltargetlanguagedata.Weexploredtwoformsofdataaugmentation.First,wegeneratedartificialglossdatainthetar-getlanguagebyswappingwordsintheexistingtrainingdata.Second,weutilizeddatafromODIN(LewisandXia,2010;Xiaetal.,2014)toseeiftransferlearningfromdatainotherlanguagescanhelpimproveperformance.2.1ArtificialDataAchallengeourteamfacedwithrespecttodataaugmentationisfiguringouthowtoobtainaddi-tionaldatawhenwedonothavemuchknowledgeofthelanguages\u2019grammaticalsystems,alongwiththefactthattheselanguagesaregenerallyfrom\n209 Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 209\u2013216 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\ndigitallyunder-resourcedlanguagefamilies.Fur-thermore,wewantedoursolutiontobeeasilyim-plementedandrelativelylanguageagnosticduetotimeconstraintsandpracticalusabilityforre-searchersworkingonavarietyoflanguages.Thus,oneavenueofdataaugmentationwetriedwasbycreatingartificialdatafromtheprovidedtrainingdata.Thisrequiresnorule-writingorknowledgeofthegrammarofthelanguage,andthuscouldbeappliedquicklyandeasilytoallofthelanguagesinthesharedtask.Weusedanaiveword-swappingmethodtoran-domlyswapmorphemesthatoccurinsimilarcon-textstocreatenewsentences.Todothis,foreachglossline,wereplaceeachwordstem(thathasaglosslabelaffix)with\u201cSTEM\u201dtocreateaskeletongloss.Wenaivelydetermineifalabelisastembycheckingifitisinlowercase.Wedonotdothistowordsthatdonothaveaffixesas(withtheexceptionofUspanteko)wedonothaveaccesstopartsofspeech,anddonotwanttoswapwordsthatwouldcreateanungrammaticalsequence.Wecreateadictionarymappingeachskeletonwordglosstopossibleactualglosses,andmapeachactualglosstopossiblesurfaceforms(wemakenoassumptionsthatthesemappingsareone-to-one).Wethenrandomlysamplekrandomskeletonglosses(inthiscase,weusedkequaltoroughlythreetimestheamountoftrainingdata)andran-domlyfillinwordsthatmatchtheformatofskele-tonwordspresentintheline.(1)to(3)belowillustrateanexampleinthisprocess.Wecreateaskeletongloss(2)fromtheGitksansentencein(1)byreplacingtheallwordstemsthathaveanaffixwith\u201cSTEM\u201dinboththesegmentationandglosstiers\u2014inthiscase,only\u2019witxw-itappliestothisstep.Thentocreatetheartificialdatain(3),wereplacetheskeletonwordandcorrespondingglosswithanotherwordfromthetrainingdatathathasthesameskeletonform,inthiscasehahla\u2019lst-it.(1)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSP\u2019witxw-itcome-SX(2)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPSTEM-itSTEM-SX(3)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPhahla\u2019lst-itwork-SXWhilethismethodmaycreateasomewhatun-naturalinputsurfacesequence(asweareunabletocapturephonologicalchangesinthesurfaceformandcorrespondingtranslationsmaybenonsensi-cal),thismethodguaranteesthatthestructureoftheglossisanaturallyoccurringsequence(asweonlyuseglossskeletonsthatarepresentinthein-put).However,alimitationofthismethodisthatitdoesnotextendtoout-of-vocabularytokensorunseenglossstructures.Furthermore,aswecannotgenerateagold-standardtranslationfortheartifi-cialdata,wedonotmakeuseofatranslationintraining.2.2ODINAnotherpotentialavenuefordataaugmentationistransferlearningfromdatainotherlanguages,whichhasbeenshowntobeaneffectivemethodtoimproveperformanceinlow-resourcesettings(Ruderetal.,2019).TheavailableresourceweutilizeisODIN,ortheOnlineDatabaseforInterlinearText(LewisandXia,2010;Xiaetal.,2014).ODINcontains158,007linesofIGT,covering1,496languages.Weusethe2.1versionofODINdataandconvertthedatasettothesharedtaskformat,andfilteroutlanguageswithfewerthanfiveglossedsentences.However,thereremainssignificantnoiseinthedatasetthatcouldcausesignificantalignmentis-suesforthetokenclassificationmodels.ThereforeweopttoonlytraintheByT5modelsonODIN,inthehopethatthismodelislesssensitivetoalign-menterrors.Indeed,wefindthattheByT5modelfinetunedfirstonODINreceivesaperformanceboostwhenfinetunedagainonthesharedtaskdata.3ModelsWeexploretwomodelsforglossgeneration.Thefirstoneisbuiltuponthetokenclassificationbase-linewithsomeimprovements,andwetreatthismodelasourinternalbaseline.Thesecondmodelwedeploytestswhetherwecanachievecompeti-tiveperformancebyfinetuningapretrainedcharac-terbasedmultilingualandmultitaskmodel,ByT5.Forthismodel,weperformminimalpreprocess-inganduserawsegmentedmorphemesandfreetranslationsifavailable.3.1TokenClassificationTransformerWeusethebaselineTrack2modelprovidedbytheorganizersasastartingpoint.Theoriginalimple-mentationrandomlyinitializesatransformermodelfromthedefaultHuggingfaceRoBERTabasecon-figuration,andusesatokenclassificationobjective\n210\n\n\n1https://en.wikipedia.org/wiki/Lezgin_alphabets\nwithcross-entropyloss,whereeachglossistreatedasadistincttoken.Themorphemesandfreetrans-lationsaretokenizedbyspaceanddashes,withpunctuationspre-separated.TheyareconcatenatedandseparatedbytheSEPtokenandareusedastheinputstothemodel.WemodifytheoriginalTrack2baselinemodeltoobtainabetterbaseline.WeusepretrainedweightsfromXLM-RoBERTabase(Conneauetal.,2020),insteadofrandomlyinitializingtheweights.Wealsoslightlymodifythemorphemetokenizertoenforcethatthenumberofmorphemetokensmatchesthenumberofoutputglosstokensexactly.Additionally,weintroducetheCOPYtokentoreplacetheglossifitmatchesthecorrespondingmorphemeexactly.AnexamplefromNatuguisshowningloss(4):(4)67COPY.COPYmnc-xbe-1MINIMzloCOPYSkulCOPYWebelievethiswouldimproveperformancebyremovingtheneedtomemorizeglossedcode-switchingandpropernouns,thoughitisonlyef-fectiveifthecode-switchedlanguageisthesameasthematrixlanguage(e.g.Arapaho),andwouldhavenoeffectifthesourcelanguageusesadif-ferentorthographyoriscode-switchedtoanotherlanguage,wheretheglosswouldnotmatchedthemorphemeformexactly.Thismethodalsocom-pressesallpunctuationmarkersintoonetoken,buttheusefulnessofthissideeffectislessclear.Sinceweareusingpretrainedweights,itisthennaturaltoexploreintegratingthepretrainedtok-enizer.SinceXLM-RoBERTawasnottrainedonanyofthesourcelanguages,itmakesthemostsensetoonlyusethepretrainedtokenizertotok-enizefreetranslations,iftheyareavailable,andextendthevocabularytoincludemorphemes.3.2FinetunedByT5Multi-taskandmulti-lingualpretrainedlargelan-guagemodelshavebeenshowntobeeffectiveformanytasks.Weexplorewhethersuchmodelscanbeusedeffectivelyforglossing.Weconductex-perimentswithbothmT5(Xueetal.,2021)andByT5(Xueetal.,2022),butByT5ispreferredbecauseittakesrawtexts(bytesorcharacters)asinputsandintheoryshouldbemoreeffectiveforunseenlanguages.Weuseapromptbasedmultilin-gualsequencetosequenceobjectiveforbothmod-els.Theprompttemplateis:\u201cGenerateinterlin-earglossfrom[sourcelanguage]:[segmentedmorphemes]withits[matrixlanguage]trans-lation:[freetranslation]Answer:\u201d.Datafromalllanguagesaremixedtogetherandshuffled,withnoupordownsampling.Afterinitialexperi-ments,wefindByT5outperformsmT5acrossalllanguages,andthereforeweonlyconductsubse-quentexperimentsonByT5andreportthosere-sults.Uponinitialexperiments,wealsofindtheresultsforLezgitobelowerthanexpected.Wehypothe-sizethatthefactthatthedataareinCyrillicscriptcausesthisdeficiency,sinceByT5wastrainedonfarlessCyrillicdatathandataintheLatinscript.Thereforewecreateanautomaticromanizationtool,sourcedfromWikipedia1andintegratedintheEpitranpackage(Mortensenetal.,2018),andconvertallLezgidatatoLatinscriptforByT5fine-tuning.AfterinspectingtheoutputsoftheByT5models,wefindcaseswherepunctuationsareattachedtothepreviousglosses,insteadofbeingseparatedbyaspaceasisstandardinthetrainingsets.Thisisprobablyduetothefactthatthemodelwaspre-trainedonuntokenizeddataandthisbehaviorispreserveddespitefinetuningontokenizeddata.Wethereforeuseasimpleregularexpressionbasedtok-enizertofixtheinconsistencies.WenoticethattheprocedureonlygivesperformanceboostonGitk-san,Lezgi,Uspanteko,andNatugu,andsoweonlyapplytheproceduretothoselanguages,leavingtherestoftheoutputsunchanged.4DictionaryPost-correction:GitksanOneofthekeychallengesforextremelylowre-sourcelanguagesistheintegrationofstructuredlinguisticdatainotherforms,suchasadictionary,intomachinelearningpipelines.Wetestasimplepost-correctionmethodfromapre-existingdictio-naryonGitksanonly,duetoitsuniquecombinationoflowresourceandeasilyobtainabledictionaryinmachinereadableform.Weusethedictionarycom-piledbyForbesetal.(2021),withoutconsultingthemorphologicalanalyzersthattheyalsoprovided.Atinferencetime,ifamorphemeisunseendur-ingtraining,wesearchfortheexactforminthedictionary.Wealsoexpandthesearchtoallsubse-quencesofmorphemeswithintheenclosingword,plusthepreviouswholewordincaseswhereapar-ticleisincludedinthedictionaryform.Thefirst\n211\n\n\nmatcheddefinitionisusedastheglossandifnoneofthesearchyieldsanexactmatch,wefallbacktothemodelprediction.Weonlyapplythismethodtothetokenclassificationmodelsbecausethealign-mentbetweenmorphemesandglossesisdirectlyestablished,whereastheseq2seqmodelsdonotguaranteethatthenumberofglossesmatchesthenumberofmorphemes.5ResultsandDiscussionTables1and2showoursystems\u2019performance(aswellastheoriginalbaseline)onthetestdatawithre-specttoword-andmorpheme-levelmicro-averagedaccuracy,respectively.Overall,thetokenclassifica-tionmodeltrainedfirstontheartificiallygeneratedaugmenteddataperformthebest,withthemodeltrainedonthesharedtaskdataonlynotfarbehind.Meanwhile,ByT5modelsperformworse,withthemodelfinetunedfirstonODINtrailingourbestmodelbyafewpercentagepoints,whilethemodelfinetunedfirstonaugmenteddataperformsworsethanthebaseline.5.1DataAugmentationOverall,wefinddataaugmentationtobeuseful.Withartificiallygenerateddata,weseetheeffectsareperhapsgreatestforthemid-resourcelanguages(ddo,lez,ntu,nyb,usp),whilethehighestandlowestresourcedlanguagesdidnotreceivemuchbenefitfrompretrainingontheartificialdata.Wethinkthisisperhapsbecausethereisa\u201csweetspot\u201dwithrespecttotheamountofdatathatisrequiredtotrainamodel.Ifthereisenoughdataalready,inthecaseofArapaho,thenthenoisinessofartificialdatawouldout-weightthebenefitoftrainingonthem.Ontheotherendofthescale,Gitksanperhapsneedsmoresyntheticdatafordataaugmentationtoyieldmeaningfulimprovements.ForByT5models,artificiallygenerateddataseemtohavetheoppositeeffect,whereperfor-manceissignificantlydegraded.Aspeculationforthiseffectisthefactthepretrainedmodelismoresemanticallyaware,andsincetheartificiallygen-eratedsentencescouldbenonsensical,themodelcouldbecomeconfused.Ontheotherhand,pre-trainingonODINyieldsimprovementsforthema-jorityofthelanguages2.ThisisencouragingsincewedidnotperformmuchpreprocessingforODIN,\n2TsezistheonlylanguagethatappearedinODIN(68sentences).Wedidnotremoveitfromthecorpusbutthisshouldhavelittleinfluenceontheperformancebecausethesizeofthedatasetisverysmall.andthereisdefinitelystillroomtomakethedatacleanerandmoreinternallyconsistent,whichinturnshouldresultinabettermodel.5.2ChoiceofHyperparametersWefindthechoiceofhyperparametersofthetokenclassificationmodelstobenecessarilylanguageanddatasetspecific.ArapahoandGitksaninpar-ticularneedspecialattention,wherethenumberoftrainingepochsneedtobeadjustedfortheveryhighandlowdatasize.WealsodevelopedmostoftheoptimizationonthetokenclassificationmodelonArapaho.However,wedidnothavetimetopropagatethechanges(usingpretrainedtokenizer,savingthelastmodelinsteadofthemodelwiththelowestvalidationloss)totherestoflanguagessinceinitialexperimentshowedthatpretrainedtokeniz-ersdidnotimproveontheotherlanguages.How-ever,afterthesubmissiondeadlineisconcluded,weranmoreexperimentsanddiscoveredthataddingpretrainedtokenizersrequiresmoretrainingsteps,andthetrainingisbettercontrolledbyspecifyingthetrainingstepsinsteadofepochs.Wedonotin-cludethoselatestexperimentsinthispaper,butourtokenclassificationmodelshavethepotentialtoperformbetterwithmorehyperparametertuning.5.3In-VersusOut-of-VocabularyErrorsOnedimensionoferroranalysisweinvestigatedwaswhatproportionofoursystems\u2019errorscomefrommorphemesorwordsthatareeitherinoroutofthetrainingdatavocabulary.Wecountamor-phemeorwordasin-vocabularyifthesurfaceformanditscorrespondingglossco-occurinthepro-videdtrainingdata(notincludingthedevelopmentdata,asourmodelsareonlytrainedonthetrainset).NotethatthereisamuchlargerproportionofOOVwordsasopposedtomorphemesduetothefactthatanunseenwordcanbecomposedofdifferentcombinationsofseenmorphemes.Table3showstheproportionofmorphemesandwordsthatareout-of-vocab(OOV)withinthetestset.Whilenearlyallthelanguageshavelessthan10%oftheirmorphemesclassifiedasOOV,GitksannotablyhasarelativelylargeportionofOOVtestdata,with\u224845%ofmorphemesand\u224878%ofwordsbeingOOV.Tables4and5showourmodels\u2019performancesonin-versesout-of-vocabtokensatthemorphemeandwordlevels,respectively.Whilewewouldintuitivelyexpectthatword-levelOOVaccuracybeaboutthesameorworsethanmorpheme-levelOOV\n212\n\n\nTable2:Morpheme-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselinewithartificialpretraininganddictionarypost-correctionisthehighestGitksanaccuracyreportedoutofallsharedtasksubmissions.\nxlmr-base91.3684.3547.47/52.8280.1788.3585.8480.0880.42xlmr-aug89.3488.1546.89/52.3982.3685.5389.4983.0881.48\nTable3:ProportionofmorphemesandwordsthatareOOVwithinthetestset.accuracy,thisisnotthecaseduetothefactthatalargeportionofout-of-vocabwordsareformedwithin-vocabmorphemes.Formostlanguages,withtheexceptionofGitksan,thereappearstobeatrade-offbetweenbetterin-vocabmorphemeperformancewithXLMRandperformanceout-of-vocabwithByT5.6RelatedWorkTherehavebeenavarietyofapproachestotheprob-lemof(semi-)automaticallygeneratinginterlineargloss.BaldridgeandPalmer(2009)investigatetheefficacyofactivelearningforthetaskofinterlinearglossing,usingannotationtimerequiredbyexpertandnon-expertannotatorsastheirmetric.Thesys-temtheyusetogenerateglosslabelsuggestionsisastandardmaximumentropyclassifier.Arule-basedapproachbySnoeketal.(2014)utilizesanFSTtogenerateglossesforPlainsCree,focusingonnouns.Samard\u017ei\u00b4cetal.(2015)viewthetaskofglossingsegmentedtextasatwo-stepprocess,firsttreatingitasastandardPOStaggingtaskandthenaddinglexicalglossesfromadictio-nary.TheydemonstratethismethodonaChintangcorpusofabout1.2millionwords.Anumberofotherworksfocusingoninterlinearglossingutilizeconditionalrandomfield(CRF)models.MoellerandHulden(2018)testthreedifferentmodelsonaverysmallLezgidataset(<3000words):aCRF(thatoutputsBIOlabelswiththecorrespondingglosspercharacterinthein-put),asegmentationandlabellingpipelinethatuti-lizesaCRF(forBIOlabels)andSVM(forglossla-bels),andanLSTMseq2seqmodel.TheyfindthattheCRFthatjointlyproducestheBIOlabelsandtagsproducedthebestresults.McMillan-Major(2020)utilizestranslationsintheirtrainingdatabycreatingtwoCRFmodels,onethatpredictsglossfromthesegmentedinputandanotherthanpre-\nModelarpddogitlezntunybuspAVG\nModelarpddogitlezntunybuspAVG\nbyt5-base78.8680.3214.8460.72b76.6776.7377.2166.48byt5-aug73.2762.374.1738.6055.1169.2570.8553.38byt5-odin80.5682.7920.5763.7777.9782.5975.7269.14\nbaseline91.1185.3425.3351.8249.0388.7182.4867.69\nbaseline85.4475.7116.4134.5441.0884.3076.5559.14\nxlmr-base85.8773.7727.86/34.11a74.1582.9980.6173.4772.14xlmr-aug82.9280.0724.74/31.2577.7778.7285.5377.5173.39\naWereportbefore/afterdictionarybasedpost-correctionforGitksan.bWetrainedthismodelwithoutromanizingLezgi.Table1:Word-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselineisthehighestArapahoaccuracyreportedoutofallsharedtasksubmissions.\nbyt5-base78.8275.7712.5944.1062.4078.9774.2560.99byt5-aug72.1057.932.6026.2435.6270.0167.7347.46byt5-odin80.8178.2412.7450.0063.3985.3073.2563.39\nMorph0.0430.0090.4500.0560.0340.0190.070\narpddogitlezntunybusp\nWord0.2420.1550.7810.1690.2140.0840.200\n213\n\n\nbyt5-aug74.7658.243.4240.2736.5471.2770.56\n35.48\nTable4:Morpheme-levelaccuracyoveralltokensofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.\n24.67\n12.31\nxlmr-base95.2085.1282.8984.7990.8787.4686.05\n23.24\n45.07\nxlmr-aug93.7283.8594.0587.6489.2490.8191.11\n29.33\n24.10\n49.79\nbyt5-odin91.9387.6663.1073.7885.9387.6083.46\n59.51\n23.54\nbyt5-odin83.4778.5518.4262.9064.3886.8575.23\n18.26\n22.41\n49.17\nTable5:Word-levelaccuracyofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.dictsfromthetranslation,andthenusesheuristicstodeterminewhichmodeltoselectfromforeachmorpheme.BarrigaMart\u00ednezetal.(2021)usedaCRFmodeltoachieve>90%accuracyforgloss-ingOtomiandfindthatitworksbetterthananRNN,whichiscomputationallymoreexpensive.Otherworks,includingoursystems,haveturnedtoneuralmethods.Kondratyuk(2019)leveragespretrainedmultilingualBERTtoencodeinputsentences,thenapplyadditionalword-levelandcharacter-levelLSTMlayersbeforejointlydecod-inglemmasandmorphologytagsusingsimplese-quencetagginglayers.Furthermore,theyshowthattwo-stagetrainingbyfirsttrainingonalllan-guagesfollowedbytrainingonthetargetlanguageismoreeffectivethantrainingthesystemonthetargetlanguagealone.AnapproachbyZhaoetal.(2020),likeMcMillan-Major(2020),makesuseoftranslationsavailableinparallelcorpora,butdosobyusingamulti-sourcetransformermodel.Theyalsoincorporatelengthcontrolandalignmentdur-inginferencetoenhancetheirmodel,andtesttheirsystemonArapaho,Tsez,andLezgi.7ConclusionInoursharedtasksubmission,weexploredataaug-mentationmethodsandmodelingstrategiesforthetaskofinterlinearglossinginsevenlow-resourcelanguages.Ourbestperformingmodelsareto-kenclassificationmodelsusingXLMR.Wedemon-stratethatpretrainingonartificialdatawithXLMRisaneffectivetechniqueforthemid-resourcetestlanguages.Additionally,inourerroranalysiswefindthatwemayhaveactuallyundertrainedourtokenclassificationmodels,andthusoursystemsmayhavethepotentialtoperformbetterwithad-ditionalhyperparametertuning.WhileourByT5modelsdidnotperformaswellasourothersys-tems,weshowthatpretrainingonODINdataiseffective,despitethisdatabeingverynoisy.Finally,wealsodemonstrateimprovementsbyutilizingadictionarytopost-correctmodeloutputsforGitk-san.\n0.00\n0.00\n0.00\n0.00\n11.24\n0.82\n12.86\n14.52\n23.60\n4.97\n56.36\n45.65\n21.14\n43.37\n1.61\n47.52\n46.94\n29.69\n13.67\nxlmr-base95.9378.1895.2384.2493.1485.8586.27\n5.79\n28.04\n2.00\n17.00\n0.41\n16.08\n14.67\n19.35\nModelarpddogitlezntunybusp\nModelarpddogitlezntunybusp\n7.49\n28.09\n28.09\n44.81\n3.23\n3.23\n54.44\n2.33\nbyt5-aug87.2268.6910.7146.0665.1374.5981.44\n30.20\nxlmr-aug92.9888.9484.7487.1087.8891.1789.31\n48.70\n40.00\n28.63\n8.67\n2.60\n2.60\n9.68\n214\n\n\nAcknowledgementsThisworkwassupportedbyNSFCISERIgrantnumber2211951,FromAcousticSignaltoMor-phosyntacticAnalysisinoneEnd-to-EndNeuralSystem.ReferencesJasonBaldridgeandAlexisPalmer.2009.Howwelldoesactivelearningactuallywork?Time-basedeval-uationofcost-reductionstrategiesforlanguagedocu-mentation.InProceedingsofthe2009ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages296\u2013305,Singapore.AssociationforCompu-tationalLinguistics.DiegoBarrigaMart\u00ednez,VictorMijangos,andXi-menaGutierrez-Vasques.2021.Automaticinterlin-earglossingforOtomilanguage.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages34\u201343,Online.AssociationforComputationalLin-guistics.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.ClarissaForbes,GarrettNicolai,andMiikkaSilfverberg.2021.AnFSTmorphologicalanalyzerforthegitksanlanguage.InProceedingsofthe18thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages188\u2013197,Online.AssociationforComputationalLinguistics.DanKondratyuk.2019.Cross-linguallemmatizationandmorphologytaggingwithtwo-stagemultilin-gualBERTfine-tuning.InProceedingsofthe16thWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages12\u201318,Florence,Italy.AssociationforComputationalLinguistics.WilliamD.LewisandFeiXia.2010.DevelopingODIN:AMultilingualRepositoryofAnnotatedLanguageDataforHundredsoftheWorld\u2019sLanguages.Liter-aryandLinguisticComputing,25(3):303\u2013319.AngelinaMcMillan-Major.2020.Automatingglossgenerationininterlinearglossedtext.ProceedingsoftheSocietyforComputationinLinguistics,3(1):338\u2013349.SarahMoellerandMansHulden.2018.Automaticglossinginalow-resourcesettingforlanguagedoc-umentation.InProceedingsoftheWorkshoponComputationalModelingofPolysyntheticLanguages,pages84\u201393.DavidR.Mortensen,SiddharthDalmia,andPatrickLittell.2018.Epitran:PrecisionG2Pformanylan-guages.InProceedingsoftheEleventhInternationalConferenceonLanguageResourcesandEvaluation(LREC2018),Miyazaki,Japan.EuropeanLanguageResourcesAssociation(ELRA).SebastianRuder,MatthewEPeters,SwabhaSwayamdipta,andThomasWolf.2019.Transferlearninginnaturallanguageprocessing.InProceed-ingsofthe2019conferenceoftheNorthAmericanchapteroftheassociationforcomputationallinguistics:Tutorials,pages15\u201318.TanjaSamard\u017ei\u00b4c,RobertSchikowski,andSabineStoll.2015.Automaticinterlinearglossingastwo-levelsequenceclassification.NoamShazeerandMitchellStern.2018.Adafactor:Adaptivelearningrateswithsublinearmemorycost.InProceedingsofthe35thInternationalConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,pages4596\u20134604.PMLR.ConorSnoek,DorothyThunder,KaidiL\u00f5o,AnttiArppe,JordanLachler,SjurMoshagen,andTrondTrosterud.2014.ModelingthenounmorphologyofPlainsCree.InProceedingsofthe2014WorkshopontheUseofComputationalMethodsintheStudyofEndangeredLanguages,pages34\u201342,Baltimore,Maryland,USA.AssociationforComputationalLinguistics.FeiXia,WilliamLewis,MichaelWayneGoodman,JoshuaCrowgey,andEmilyM.Bender.2014.En-richingODIN.InProceedingsoftheNinthInter-nationalConferenceonLanguageResourcesandEvaluation(LREC\u201914),pages3151\u20133157,Reykjavik,Iceland.EuropeanLanguageResourcesAssociation(ELRA).LintingXue,AdityaBarua,NoahConstant,RamiAl-Rfou,SharanNarang,MihirKale,AdamRoberts,andColinRaffel.2022.ByT5:Towardsatoken-freefuturewithpre-trainedbyte-to-bytemodels.Transac-tionsoftheAssociationforComputationalLinguis-tics,10:291\u2013306.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.XingyuanZhao,SatoruOzaki,AntoniosAnastasopou-los,GrahamNeubig,andLoriLevin.2020.Auto-maticinterlinearglossingforunder-resourcedlan-guagesleveragingtranslations.InProceedingsofthe28thInternationalConferenceonComputationalLinguistics,pages5397\u20135408,Barcelona,Spain(On-line).InternationalCommitteeonComputationalLin-guistics.\n215\n\n\nAHyperparameterSettingsWeuseAdafactor(ShazeerandStern,2018)astheoptimizeracrossallexperiments,withthede-faultschedulerfromHuggingFaceTransformers,abatchsizeof32forRoBERTabasedmodelsandabatchsizeof4withagradientaccumulationstepof8forByT5basedmodels.Wetrainthetokenclassificationmodelsfor40epochsexceptforAra-paho,onwhichwetrain20epochs,andGitksan,onwhichwetrain2,000steps.WetraintheByT5basedmodelsfor20epochsonallofthedatamixedtogether.\n216"}, {"question": " Which model performed the best in the submission, and what was it based on?", "answer": " The token classification models using XLM-R performed the best in the submission.,", "ref_chunk": "SigMoreFunSubmissiontotheSIGMORPHONSharedTaskonInterlinearGlossingTaiqiHe\u2217,LindiaTjuatja\u2217,NateRobinson,ShinjiWatanabe,DavidR.Mortensen,GrahamNeubig,LoriLevinLanguageTechnologiesInstituteCarnegieMellonUniversity{taiqih,ltjuatja,nrrobins,swatanab,dmortens,gneubig,lsl}@cs.cmu.eduAbstractInoursubmissiontotheSIGMORPHON2023SharedTaskoninterlinearglossing(IGT),weexploreapproachestodataaugmentationandmodelingacrosssevenlow-resourcelanguages.Fordataaugmentation,weexploretwoap-proaches:creatingartificialdatafromthepro-videdtrainingdataandutilizingexistingIGTresourcesinotherlanguages.Onthemodelingside,wetestanenhancedversionofthepro-videdtokenclassificationbaselineaswellasapretrainedmultilingualseq2seqmodel.Ad-ditionally,weapplypost-correctionusingadictionaryforGitksan,thelanguagewiththesmallestamountofdata.Wefindthatourtokenclassificationmodelsarethebestperforming,withthehighestword-levelaccuracyforAra-pahoandhighestmorpheme-levelaccuracyforGitksanoutofallsubmissions.Wealsoshowthatdataaugmentationisaneffectivestrategy,thoughapplyingartificialdatapretraininghasverydifferenteffectsacrossbothmodelstested.1IntroductionThispaperdescribestheSigMoreFunsubmissiontotheSIGMORPHON2023SharedTaskoninterlin-earglossing.Giveninputtextinatargetlanguage,thetaskistopredictthecorrespondinginterlineargloss(usingLeipzigglossingconventions).IGTisanimportantformoflinguisticannotationforthemorphologicalanalysisoflanguages,andalsoservesasanextremelyvaluableresourceforlan-guagedocumentationandeducationforspeakersoflow-resourcelanguages.Thereweretwotracksforthissharedtask,Track1(closed)andTrack2(open).ForTrack1,sys-temscouldonlybetrainedoninputsentencesandglosses;inTrack2,systemscouldmakeuseofthemorphologicalsegmentationoftheinputaswellasany(non-IGT)externalresources.SincetheTrack2settingbettermatchesthelong-termre-\n\u2217Theseauthorscontributedequallysearchgoalsofourteam,weonlyparticipateinthisopentrack.Inoursubmission,weinvestigatetwodifferentapproaches.First,weattemptdataaugmentationbyeithercreatingourownartificialglossdatabymanipulatingtheexistingtrainingdata,orbyuti-lizingexistingresourcescontainingIGTinotherlanguages(\u00a72).Second,weexploretwodifferentmodelsforglossgeneration(\u00a73).Thefirstbuildsoffthetokenclassificationbaseline,whilethesec-ondusesapretrainedmultilingualseq2seqmodel.Finally,wealsoattempttopost-correctmodeloutputswithadictionary.WeapplythistoGitk-sanandfindthatthis,combinedwithourotherapproaches,resultsinthehighestmorpheme-levelaccuracyforGitksaninTrack2.2DataAugmentationOnemajorchallengeforthissharedtaskisthescaleofdataprovided.Allofthelanguageshavelessthan40klinesoftrainingdata,andallbutArapahohavelessthan10k.Thesmallestdataset(Gitk-san)hasonly31linesofdata.Thus,oneobviousmethodtotryisdataaugmentation.Morespecif-ically,wetrypretrainingourmodelsondifferentformsofaugmenteddatabeforetrainingthemontheoriginaltargetlanguagedata.Weexploredtwoformsofdataaugmentation.First,wegeneratedartificialglossdatainthetar-getlanguagebyswappingwordsintheexistingtrainingdata.Second,weutilizeddatafromODIN(LewisandXia,2010;Xiaetal.,2014)toseeiftransferlearningfromdatainotherlanguagescanhelpimproveperformance.2.1ArtificialDataAchallengeourteamfacedwithrespecttodataaugmentationisfiguringouthowtoobtainaddi-tionaldatawhenwedonothavemuchknowledgeofthelanguages\u2019grammaticalsystems,alongwiththefactthattheselanguagesaregenerallyfrom\n209 Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 209\u2013216 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\ndigitallyunder-resourcedlanguagefamilies.Fur-thermore,wewantedoursolutiontobeeasilyim-plementedandrelativelylanguageagnosticduetotimeconstraintsandpracticalusabilityforre-searchersworkingonavarietyoflanguages.Thus,oneavenueofdataaugmentationwetriedwasbycreatingartificialdatafromtheprovidedtrainingdata.Thisrequiresnorule-writingorknowledgeofthegrammarofthelanguage,andthuscouldbeappliedquicklyandeasilytoallofthelanguagesinthesharedtask.Weusedanaiveword-swappingmethodtoran-domlyswapmorphemesthatoccurinsimilarcon-textstocreatenewsentences.Todothis,foreachglossline,wereplaceeachwordstem(thathasaglosslabelaffix)with\u201cSTEM\u201dtocreateaskeletongloss.Wenaivelydetermineifalabelisastembycheckingifitisinlowercase.Wedonotdothistowordsthatdonothaveaffixesas(withtheexceptionofUspanteko)wedonothaveaccesstopartsofspeech,anddonotwanttoswapwordsthatwouldcreateanungrammaticalsequence.Wecreateadictionarymappingeachskeletonwordglosstopossibleactualglosses,andmapeachactualglosstopossiblesurfaceforms(wemakenoassumptionsthatthesemappingsareone-to-one).Wethenrandomlysamplekrandomskeletonglosses(inthiscase,weusedkequaltoroughlythreetimestheamountoftrainingdata)andran-domlyfillinwordsthatmatchtheformatofskele-tonwordspresentintheline.(1)to(3)belowillustrateanexampleinthisprocess.Wecreateaskeletongloss(2)fromtheGitksansentencein(1)byreplacingtheallwordstemsthathaveanaffixwith\u201cSTEM\u201dinboththesegmentationandglosstiers\u2014inthiscase,only\u2019witxw-itappliestothisstep.Thentocreatetheartificialdatain(3),wereplacetheskeletonwordandcorrespondingglosswithanotherwordfromthetrainingdatathathasthesameskeletonform,inthiscasehahla\u2019lst-it.(1)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSP\u2019witxw-itcome-SX(2)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPSTEM-itSTEM-SX(3)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPhahla\u2019lst-itwork-SXWhilethismethodmaycreateasomewhatun-naturalinputsurfacesequence(asweareunabletocapturephonologicalchangesinthesurfaceformandcorrespondingtranslationsmaybenonsensi-cal),thismethodguaranteesthatthestructureoftheglossisanaturallyoccurringsequence(asweonlyuseglossskeletonsthatarepresentinthein-put).However,alimitationofthismethodisthatitdoesnotextendtoout-of-vocabularytokensorunseenglossstructures.Furthermore,aswecannotgenerateagold-standardtranslationfortheartifi-cialdata,wedonotmakeuseofatranslationintraining.2.2ODINAnotherpotentialavenuefordataaugmentationistransferlearningfromdatainotherlanguages,whichhasbeenshowntobeaneffectivemethodtoimproveperformanceinlow-resourcesettings(Ruderetal.,2019).TheavailableresourceweutilizeisODIN,ortheOnlineDatabaseforInterlinearText(LewisandXia,2010;Xiaetal.,2014).ODINcontains158,007linesofIGT,covering1,496languages.Weusethe2.1versionofODINdataandconvertthedatasettothesharedtaskformat,andfilteroutlanguageswithfewerthanfiveglossedsentences.However,thereremainssignificantnoiseinthedatasetthatcouldcausesignificantalignmentis-suesforthetokenclassificationmodels.ThereforeweopttoonlytraintheByT5modelsonODIN,inthehopethatthismodelislesssensitivetoalign-menterrors.Indeed,wefindthattheByT5modelfinetunedfirstonODINreceivesaperformanceboostwhenfinetunedagainonthesharedtaskdata.3ModelsWeexploretwomodelsforglossgeneration.Thefirstoneisbuiltuponthetokenclassificationbase-linewithsomeimprovements,andwetreatthismodelasourinternalbaseline.Thesecondmodelwedeploytestswhetherwecanachievecompeti-tiveperformancebyfinetuningapretrainedcharac-terbasedmultilingualandmultitaskmodel,ByT5.Forthismodel,weperformminimalpreprocess-inganduserawsegmentedmorphemesandfreetranslationsifavailable.3.1TokenClassificationTransformerWeusethebaselineTrack2modelprovidedbytheorganizersasastartingpoint.Theoriginalimple-mentationrandomlyinitializesatransformermodelfromthedefaultHuggingfaceRoBERTabasecon-figuration,andusesatokenclassificationobjective\n210\n\n\n1https://en.wikipedia.org/wiki/Lezgin_alphabets\nwithcross-entropyloss,whereeachglossistreatedasadistincttoken.Themorphemesandfreetrans-lationsaretokenizedbyspaceanddashes,withpunctuationspre-separated.TheyareconcatenatedandseparatedbytheSEPtokenandareusedastheinputstothemodel.WemodifytheoriginalTrack2baselinemodeltoobtainabetterbaseline.WeusepretrainedweightsfromXLM-RoBERTabase(Conneauetal.,2020),insteadofrandomlyinitializingtheweights.Wealsoslightlymodifythemorphemetokenizertoenforcethatthenumberofmorphemetokensmatchesthenumberofoutputglosstokensexactly.Additionally,weintroducetheCOPYtokentoreplacetheglossifitmatchesthecorrespondingmorphemeexactly.AnexamplefromNatuguisshowningloss(4):(4)67COPY.COPYmnc-xbe-1MINIMzloCOPYSkulCOPYWebelievethiswouldimproveperformancebyremovingtheneedtomemorizeglossedcode-switchingandpropernouns,thoughitisonlyef-fectiveifthecode-switchedlanguageisthesameasthematrixlanguage(e.g.Arapaho),andwouldhavenoeffectifthesourcelanguageusesadif-ferentorthographyoriscode-switchedtoanotherlanguage,wheretheglosswouldnotmatchedthemorphemeformexactly.Thismethodalsocom-pressesallpunctuationmarkersintoonetoken,buttheusefulnessofthissideeffectislessclear.Sinceweareusingpretrainedweights,itisthennaturaltoexploreintegratingthepretrainedtok-enizer.SinceXLM-RoBERTawasnottrainedonanyofthesourcelanguages,itmakesthemostsensetoonlyusethepretrainedtokenizertotok-enizefreetranslations,iftheyareavailable,andextendthevocabularytoincludemorphemes.3.2FinetunedByT5Multi-taskandmulti-lingualpretrainedlargelan-guagemodelshavebeenshowntobeeffectiveformanytasks.Weexplorewhethersuchmodelscanbeusedeffectivelyforglossing.Weconductex-perimentswithbothmT5(Xueetal.,2021)andByT5(Xueetal.,2022),butByT5ispreferredbecauseittakesrawtexts(bytesorcharacters)asinputsandintheoryshouldbemoreeffectiveforunseenlanguages.Weuseapromptbasedmultilin-gualsequencetosequenceobjectiveforbothmod-els.Theprompttemplateis:\u201cGenerateinterlin-earglossfrom[sourcelanguage]:[segmentedmorphemes]withits[matrixlanguage]trans-lation:[freetranslation]Answer:\u201d.Datafromalllanguagesaremixedtogetherandshuffled,withnoupordownsampling.Afterinitialexperi-ments,wefindByT5outperformsmT5acrossalllanguages,andthereforeweonlyconductsubse-quentexperimentsonByT5andreportthosere-sults.Uponinitialexperiments,wealsofindtheresultsforLezgitobelowerthanexpected.Wehypothe-sizethatthefactthatthedataareinCyrillicscriptcausesthisdeficiency,sinceByT5wastrainedonfarlessCyrillicdatathandataintheLatinscript.Thereforewecreateanautomaticromanizationtool,sourcedfromWikipedia1andintegratedintheEpitranpackage(Mortensenetal.,2018),andconvertallLezgidatatoLatinscriptforByT5fine-tuning.AfterinspectingtheoutputsoftheByT5models,wefindcaseswherepunctuationsareattachedtothepreviousglosses,insteadofbeingseparatedbyaspaceasisstandardinthetrainingsets.Thisisprobablyduetothefactthatthemodelwaspre-trainedonuntokenizeddataandthisbehaviorispreserveddespitefinetuningontokenizeddata.Wethereforeuseasimpleregularexpressionbasedtok-enizertofixtheinconsistencies.WenoticethattheprocedureonlygivesperformanceboostonGitk-san,Lezgi,Uspanteko,andNatugu,andsoweonlyapplytheproceduretothoselanguages,leavingtherestoftheoutputsunchanged.4DictionaryPost-correction:GitksanOneofthekeychallengesforextremelylowre-sourcelanguagesistheintegrationofstructuredlinguisticdatainotherforms,suchasadictionary,intomachinelearningpipelines.Wetestasimplepost-correctionmethodfromapre-existingdictio-naryonGitksanonly,duetoitsuniquecombinationoflowresourceandeasilyobtainabledictionaryinmachinereadableform.Weusethedictionarycom-piledbyForbesetal.(2021),withoutconsultingthemorphologicalanalyzersthattheyalsoprovided.Atinferencetime,ifamorphemeisunseendur-ingtraining,wesearchfortheexactforminthedictionary.Wealsoexpandthesearchtoallsubse-quencesofmorphemeswithintheenclosingword,plusthepreviouswholewordincaseswhereapar-ticleisincludedinthedictionaryform.Thefirst\n211\n\n\nmatcheddefinitionisusedastheglossandifnoneofthesearchyieldsanexactmatch,wefallbacktothemodelprediction.Weonlyapplythismethodtothetokenclassificationmodelsbecausethealign-mentbetweenmorphemesandglossesisdirectlyestablished,whereastheseq2seqmodelsdonotguaranteethatthenumberofglossesmatchesthenumberofmorphemes.5ResultsandDiscussionTables1and2showoursystems\u2019performance(aswellastheoriginalbaseline)onthetestdatawithre-specttoword-andmorpheme-levelmicro-averagedaccuracy,respectively.Overall,thetokenclassifica-tionmodeltrainedfirstontheartificiallygeneratedaugmenteddataperformthebest,withthemodeltrainedonthesharedtaskdataonlynotfarbehind.Meanwhile,ByT5modelsperformworse,withthemodelfinetunedfirstonODINtrailingourbestmodelbyafewpercentagepoints,whilethemodelfinetunedfirstonaugmenteddataperformsworsethanthebaseline.5.1DataAugmentationOverall,wefinddataaugmentationtobeuseful.Withartificiallygenerateddata,weseetheeffectsareperhapsgreatestforthemid-resourcelanguages(ddo,lez,ntu,nyb,usp),whilethehighestandlowestresourcedlanguagesdidnotreceivemuchbenefitfrompretrainingontheartificialdata.Wethinkthisisperhapsbecausethereisa\u201csweetspot\u201dwithrespecttotheamountofdatathatisrequiredtotrainamodel.Ifthereisenoughdataalready,inthecaseofArapaho,thenthenoisinessofartificialdatawouldout-weightthebenefitoftrainingonthem.Ontheotherendofthescale,Gitksanperhapsneedsmoresyntheticdatafordataaugmentationtoyieldmeaningfulimprovements.ForByT5models,artificiallygenerateddataseemtohavetheoppositeeffect,whereperfor-manceissignificantlydegraded.Aspeculationforthiseffectisthefactthepretrainedmodelismoresemanticallyaware,andsincetheartificiallygen-eratedsentencescouldbenonsensical,themodelcouldbecomeconfused.Ontheotherhand,pre-trainingonODINyieldsimprovementsforthema-jorityofthelanguages2.ThisisencouragingsincewedidnotperformmuchpreprocessingforODIN,\n2TsezistheonlylanguagethatappearedinODIN(68sentences).Wedidnotremoveitfromthecorpusbutthisshouldhavelittleinfluenceontheperformancebecausethesizeofthedatasetisverysmall.andthereisdefinitelystillroomtomakethedatacleanerandmoreinternallyconsistent,whichinturnshouldresultinabettermodel.5.2ChoiceofHyperparametersWefindthechoiceofhyperparametersofthetokenclassificationmodelstobenecessarilylanguageanddatasetspecific.ArapahoandGitksaninpar-ticularneedspecialattention,wherethenumberoftrainingepochsneedtobeadjustedfortheveryhighandlowdatasize.WealsodevelopedmostoftheoptimizationonthetokenclassificationmodelonArapaho.However,wedidnothavetimetopropagatethechanges(usingpretrainedtokenizer,savingthelastmodelinsteadofthemodelwiththelowestvalidationloss)totherestoflanguagessinceinitialexperimentshowedthatpretrainedtokeniz-ersdidnotimproveontheotherlanguages.How-ever,afterthesubmissiondeadlineisconcluded,weranmoreexperimentsanddiscoveredthataddingpretrainedtokenizersrequiresmoretrainingsteps,andthetrainingisbettercontrolledbyspecifyingthetrainingstepsinsteadofepochs.Wedonotin-cludethoselatestexperimentsinthispaper,butourtokenclassificationmodelshavethepotentialtoperformbetterwithmorehyperparametertuning.5.3In-VersusOut-of-VocabularyErrorsOnedimensionoferroranalysisweinvestigatedwaswhatproportionofoursystems\u2019errorscomefrommorphemesorwordsthatareeitherinoroutofthetrainingdatavocabulary.Wecountamor-phemeorwordasin-vocabularyifthesurfaceformanditscorrespondingglossco-occurinthepro-videdtrainingdata(notincludingthedevelopmentdata,asourmodelsareonlytrainedonthetrainset).NotethatthereisamuchlargerproportionofOOVwordsasopposedtomorphemesduetothefactthatanunseenwordcanbecomposedofdifferentcombinationsofseenmorphemes.Table3showstheproportionofmorphemesandwordsthatareout-of-vocab(OOV)withinthetestset.Whilenearlyallthelanguageshavelessthan10%oftheirmorphemesclassifiedasOOV,GitksannotablyhasarelativelylargeportionofOOVtestdata,with\u224845%ofmorphemesand\u224878%ofwordsbeingOOV.Tables4and5showourmodels\u2019performancesonin-versesout-of-vocabtokensatthemorphemeandwordlevels,respectively.Whilewewouldintuitivelyexpectthatword-levelOOVaccuracybeaboutthesameorworsethanmorpheme-levelOOV\n212\n\n\nTable2:Morpheme-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselinewithartificialpretraininganddictionarypost-correctionisthehighestGitksanaccuracyreportedoutofallsharedtasksubmissions.\nxlmr-base91.3684.3547.47/52.8280.1788.3585.8480.0880.42xlmr-aug89.3488.1546.89/52.3982.3685.5389.4983.0881.48\nTable3:ProportionofmorphemesandwordsthatareOOVwithinthetestset.accuracy,thisisnotthecaseduetothefactthatalargeportionofout-of-vocabwordsareformedwithin-vocabmorphemes.Formostlanguages,withtheexceptionofGitksan,thereappearstobeatrade-offbetweenbetterin-vocabmorphemeperformancewithXLMRandperformanceout-of-vocabwithByT5.6RelatedWorkTherehavebeenavarietyofapproachestotheprob-lemof(semi-)automaticallygeneratinginterlineargloss.BaldridgeandPalmer(2009)investigatetheefficacyofactivelearningforthetaskofinterlinearglossing,usingannotationtimerequiredbyexpertandnon-expertannotatorsastheirmetric.Thesys-temtheyusetogenerateglosslabelsuggestionsisastandardmaximumentropyclassifier.Arule-basedapproachbySnoeketal.(2014)utilizesanFSTtogenerateglossesforPlainsCree,focusingonnouns.Samard\u017ei\u00b4cetal.(2015)viewthetaskofglossingsegmentedtextasatwo-stepprocess,firsttreatingitasastandardPOStaggingtaskandthenaddinglexicalglossesfromadictio-nary.TheydemonstratethismethodonaChintangcorpusofabout1.2millionwords.Anumberofotherworksfocusingoninterlinearglossingutilizeconditionalrandomfield(CRF)models.MoellerandHulden(2018)testthreedifferentmodelsonaverysmallLezgidataset(<3000words):aCRF(thatoutputsBIOlabelswiththecorrespondingglosspercharacterinthein-put),asegmentationandlabellingpipelinethatuti-lizesaCRF(forBIOlabels)andSVM(forglossla-bels),andanLSTMseq2seqmodel.TheyfindthattheCRFthatjointlyproducestheBIOlabelsandtagsproducedthebestresults.McMillan-Major(2020)utilizestranslationsintheirtrainingdatabycreatingtwoCRFmodels,onethatpredictsglossfromthesegmentedinputandanotherthanpre-\nModelarpddogitlezntunybuspAVG\nModelarpddogitlezntunybuspAVG\nbyt5-base78.8680.3214.8460.72b76.6776.7377.2166.48byt5-aug73.2762.374.1738.6055.1169.2570.8553.38byt5-odin80.5682.7920.5763.7777.9782.5975.7269.14\nbaseline91.1185.3425.3351.8249.0388.7182.4867.69\nbaseline85.4475.7116.4134.5441.0884.3076.5559.14\nxlmr-base85.8773.7727.86/34.11a74.1582.9980.6173.4772.14xlmr-aug82.9280.0724.74/31.2577.7778.7285.5377.5173.39\naWereportbefore/afterdictionarybasedpost-correctionforGitksan.bWetrainedthismodelwithoutromanizingLezgi.Table1:Word-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselineisthehighestArapahoaccuracyreportedoutofallsharedtasksubmissions.\nbyt5-base78.8275.7712.5944.1062.4078.9774.2560.99byt5-aug72.1057.932.6026.2435.6270.0167.7347.46byt5-odin80.8178.2412.7450.0063.3985.3073.2563.39\nMorph0.0430.0090.4500.0560.0340.0190.070\narpddogitlezntunybusp\nWord0.2420.1550.7810.1690.2140.0840.200\n213\n\n\nbyt5-aug74.7658.243.4240.2736.5471.2770.56\n35.48\nTable4:Morpheme-levelaccuracyoveralltokensofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.\n24.67\n12.31\nxlmr-base95.2085.1282.8984.7990.8787.4686.05\n23.24\n45.07\nxlmr-aug93.7283.8594.0587.6489.2490.8191.11\n29.33\n24.10\n49.79\nbyt5-odin91.9387.6663.1073.7885.9387.6083.46\n59.51\n23.54\nbyt5-odin83.4778.5518.4262.9064.3886.8575.23\n18.26\n22.41\n49.17\nTable5:Word-levelaccuracyofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.dictsfromthetranslation,andthenusesheuristicstodeterminewhichmodeltoselectfromforeachmorpheme.BarrigaMart\u00ednezetal.(2021)usedaCRFmodeltoachieve>90%accuracyforgloss-ingOtomiandfindthatitworksbetterthananRNN,whichiscomputationallymoreexpensive.Otherworks,includingoursystems,haveturnedtoneuralmethods.Kondratyuk(2019)leveragespretrainedmultilingualBERTtoencodeinputsentences,thenapplyadditionalword-levelandcharacter-levelLSTMlayersbeforejointlydecod-inglemmasandmorphologytagsusingsimplese-quencetagginglayers.Furthermore,theyshowthattwo-stagetrainingbyfirsttrainingonalllan-guagesfollowedbytrainingonthetargetlanguageismoreeffectivethantrainingthesystemonthetargetlanguagealone.AnapproachbyZhaoetal.(2020),likeMcMillan-Major(2020),makesuseoftranslationsavailableinparallelcorpora,butdosobyusingamulti-sourcetransformermodel.Theyalsoincorporatelengthcontrolandalignmentdur-inginferencetoenhancetheirmodel,andtesttheirsystemonArapaho,Tsez,andLezgi.7ConclusionInoursharedtasksubmission,weexploredataaug-mentationmethodsandmodelingstrategiesforthetaskofinterlinearglossinginsevenlow-resourcelanguages.Ourbestperformingmodelsareto-kenclassificationmodelsusingXLMR.Wedemon-stratethatpretrainingonartificialdatawithXLMRisaneffectivetechniqueforthemid-resourcetestlanguages.Additionally,inourerroranalysiswefindthatwemayhaveactuallyundertrainedourtokenclassificationmodels,andthusoursystemsmayhavethepotentialtoperformbetterwithad-ditionalhyperparametertuning.WhileourByT5modelsdidnotperformaswellasourothersys-tems,weshowthatpretrainingonODINdataiseffective,despitethisdatabeingverynoisy.Finally,wealsodemonstrateimprovementsbyutilizingadictionarytopost-correctmodeloutputsforGitk-san.\n0.00\n0.00\n0.00\n0.00\n11.24\n0.82\n12.86\n14.52\n23.60\n4.97\n56.36\n45.65\n21.14\n43.37\n1.61\n47.52\n46.94\n29.69\n13.67\nxlmr-base95.9378.1895.2384.2493.1485.8586.27\n5.79\n28.04\n2.00\n17.00\n0.41\n16.08\n14.67\n19.35\nModelarpddogitlezntunybusp\nModelarpddogitlezntunybusp\n7.49\n28.09\n28.09\n44.81\n3.23\n3.23\n54.44\n2.33\nbyt5-aug87.2268.6910.7146.0665.1374.5981.44\n30.20\nxlmr-aug92.9888.9484.7487.1087.8891.1789.31\n48.70\n40.00\n28.63\n8.67\n2.60\n2.60\n9.68\n214\n\n\nAcknowledgementsThisworkwassupportedbyNSFCISERIgrantnumber2211951,FromAcousticSignaltoMor-phosyntacticAnalysisinoneEnd-to-EndNeuralSystem.ReferencesJasonBaldridgeandAlexisPalmer.2009.Howwelldoesactivelearningactuallywork?Time-basedeval-uationofcost-reductionstrategiesforlanguagedocu-mentation.InProceedingsofthe2009ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages296\u2013305,Singapore.AssociationforCompu-tationalLinguistics.DiegoBarrigaMart\u00ednez,VictorMijangos,andXi-menaGutierrez-Vasques.2021.Automaticinterlin-earglossingforOtomilanguage.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages34\u201343,Online.AssociationforComputationalLin-guistics.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.ClarissaForbes,GarrettNicolai,andMiikkaSilfverberg.2021.AnFSTmorphologicalanalyzerforthegitksanlanguage.InProceedingsofthe18thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages188\u2013197,Online.AssociationforComputationalLinguistics.DanKondratyuk.2019.Cross-linguallemmatizationandmorphologytaggingwithtwo-stagemultilin-gualBERTfine-tuning.InProceedingsofthe16thWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages12\u201318,Florence,Italy.AssociationforComputationalLinguistics.WilliamD.LewisandFeiXia.2010.DevelopingODIN:AMultilingualRepositoryofAnnotatedLanguageDataforHundredsoftheWorld\u2019sLanguages.Liter-aryandLinguisticComputing,25(3):303\u2013319.AngelinaMcMillan-Major.2020.Automatingglossgenerationininterlinearglossedtext.ProceedingsoftheSocietyforComputationinLinguistics,3(1):338\u2013349.SarahMoellerandMansHulden.2018.Automaticglossinginalow-resourcesettingforlanguagedoc-umentation.InProceedingsoftheWorkshoponComputationalModelingofPolysyntheticLanguages,pages84\u201393.DavidR.Mortensen,SiddharthDalmia,andPatrickLittell.2018.Epitran:PrecisionG2Pformanylan-guages.InProceedingsoftheEleventhInternationalConferenceonLanguageResourcesandEvaluation(LREC2018),Miyazaki,Japan.EuropeanLanguageResourcesAssociation(ELRA).SebastianRuder,MatthewEPeters,SwabhaSwayamdipta,andThomasWolf.2019.Transferlearninginnaturallanguageprocessing.InProceed-ingsofthe2019conferenceoftheNorthAmericanchapteroftheassociationforcomputationallinguistics:Tutorials,pages15\u201318.TanjaSamard\u017ei\u00b4c,RobertSchikowski,andSabineStoll.2015.Automaticinterlinearglossingastwo-levelsequenceclassification.NoamShazeerandMitchellStern.2018.Adafactor:Adaptivelearningrateswithsublinearmemorycost.InProceedingsofthe35thInternationalConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,pages4596\u20134604.PMLR.ConorSnoek,DorothyThunder,KaidiL\u00f5o,AnttiArppe,JordanLachler,SjurMoshagen,andTrondTrosterud.2014.ModelingthenounmorphologyofPlainsCree.InProceedingsofthe2014WorkshopontheUseofComputationalMethodsintheStudyofEndangeredLanguages,pages34\u201342,Baltimore,Maryland,USA.AssociationforComputationalLinguistics.FeiXia,WilliamLewis,MichaelWayneGoodman,JoshuaCrowgey,andEmilyM.Bender.2014.En-richingODIN.InProceedingsoftheNinthInter-nationalConferenceonLanguageResourcesandEvaluation(LREC\u201914),pages3151\u20133157,Reykjavik,Iceland.EuropeanLanguageResourcesAssociation(ELRA).LintingXue,AdityaBarua,NoahConstant,RamiAl-Rfou,SharanNarang,MihirKale,AdamRoberts,andColinRaffel.2022.ByT5:Towardsatoken-freefuturewithpre-trainedbyte-to-bytemodels.Transac-tionsoftheAssociationforComputationalLinguis-tics,10:291\u2013306.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.XingyuanZhao,SatoruOzaki,AntoniosAnastasopou-los,GrahamNeubig,andLoriLevin.2020.Auto-maticinterlinearglossingforunder-resourcedlan-guagesleveragingtranslations.InProceedingsofthe28thInternationalConferenceonComputationalLinguistics,pages5397\u20135408,Barcelona,Spain(On-line).InternationalCommitteeonComputationalLin-guistics.\n215\n\n\nAHyperparameterSettingsWeuseAdafactor(ShazeerandStern,2018)astheoptimizeracrossallexperiments,withthede-faultschedulerfromHuggingFaceTransformers,abatchsizeof32forRoBERTabasedmodelsandabatchsizeof4withagradientaccumulationstepof8forByT5basedmodels.Wetrainthetokenclassificationmodelsfor40epochsexceptforAra-paho,onwhichwetrain20epochs,andGitksan,onwhichwetrain2,000steps.WetraintheByT5basedmodelsfor20epochsonallofthedatamixedtogether.\n216"}, {"question": " What was the major challenge for the shared task mentioned in the text?", "answer": " The scale of data provided, with all languages having less than 40k lines of training data and most having less than 10k lines.,", "ref_chunk": "SigMoreFunSubmissiontotheSIGMORPHONSharedTaskonInterlinearGlossingTaiqiHe\u2217,LindiaTjuatja\u2217,NateRobinson,ShinjiWatanabe,DavidR.Mortensen,GrahamNeubig,LoriLevinLanguageTechnologiesInstituteCarnegieMellonUniversity{taiqih,ltjuatja,nrrobins,swatanab,dmortens,gneubig,lsl}@cs.cmu.eduAbstractInoursubmissiontotheSIGMORPHON2023SharedTaskoninterlinearglossing(IGT),weexploreapproachestodataaugmentationandmodelingacrosssevenlow-resourcelanguages.Fordataaugmentation,weexploretwoap-proaches:creatingartificialdatafromthepro-videdtrainingdataandutilizingexistingIGTresourcesinotherlanguages.Onthemodelingside,wetestanenhancedversionofthepro-videdtokenclassificationbaselineaswellasapretrainedmultilingualseq2seqmodel.Ad-ditionally,weapplypost-correctionusingadictionaryforGitksan,thelanguagewiththesmallestamountofdata.Wefindthatourtokenclassificationmodelsarethebestperforming,withthehighestword-levelaccuracyforAra-pahoandhighestmorpheme-levelaccuracyforGitksanoutofallsubmissions.Wealsoshowthatdataaugmentationisaneffectivestrategy,thoughapplyingartificialdatapretraininghasverydifferenteffectsacrossbothmodelstested.1IntroductionThispaperdescribestheSigMoreFunsubmissiontotheSIGMORPHON2023SharedTaskoninterlin-earglossing.Giveninputtextinatargetlanguage,thetaskistopredictthecorrespondinginterlineargloss(usingLeipzigglossingconventions).IGTisanimportantformoflinguisticannotationforthemorphologicalanalysisoflanguages,andalsoservesasanextremelyvaluableresourceforlan-guagedocumentationandeducationforspeakersoflow-resourcelanguages.Thereweretwotracksforthissharedtask,Track1(closed)andTrack2(open).ForTrack1,sys-temscouldonlybetrainedoninputsentencesandglosses;inTrack2,systemscouldmakeuseofthemorphologicalsegmentationoftheinputaswellasany(non-IGT)externalresources.SincetheTrack2settingbettermatchesthelong-termre-\n\u2217Theseauthorscontributedequallysearchgoalsofourteam,weonlyparticipateinthisopentrack.Inoursubmission,weinvestigatetwodifferentapproaches.First,weattemptdataaugmentationbyeithercreatingourownartificialglossdatabymanipulatingtheexistingtrainingdata,orbyuti-lizingexistingresourcescontainingIGTinotherlanguages(\u00a72).Second,weexploretwodifferentmodelsforglossgeneration(\u00a73).Thefirstbuildsoffthetokenclassificationbaseline,whilethesec-ondusesapretrainedmultilingualseq2seqmodel.Finally,wealsoattempttopost-correctmodeloutputswithadictionary.WeapplythistoGitk-sanandfindthatthis,combinedwithourotherapproaches,resultsinthehighestmorpheme-levelaccuracyforGitksaninTrack2.2DataAugmentationOnemajorchallengeforthissharedtaskisthescaleofdataprovided.Allofthelanguageshavelessthan40klinesoftrainingdata,andallbutArapahohavelessthan10k.Thesmallestdataset(Gitk-san)hasonly31linesofdata.Thus,oneobviousmethodtotryisdataaugmentation.Morespecif-ically,wetrypretrainingourmodelsondifferentformsofaugmenteddatabeforetrainingthemontheoriginaltargetlanguagedata.Weexploredtwoformsofdataaugmentation.First,wegeneratedartificialglossdatainthetar-getlanguagebyswappingwordsintheexistingtrainingdata.Second,weutilizeddatafromODIN(LewisandXia,2010;Xiaetal.,2014)toseeiftransferlearningfromdatainotherlanguagescanhelpimproveperformance.2.1ArtificialDataAchallengeourteamfacedwithrespecttodataaugmentationisfiguringouthowtoobtainaddi-tionaldatawhenwedonothavemuchknowledgeofthelanguages\u2019grammaticalsystems,alongwiththefactthattheselanguagesaregenerallyfrom\n209 Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 209\u2013216 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\ndigitallyunder-resourcedlanguagefamilies.Fur-thermore,wewantedoursolutiontobeeasilyim-plementedandrelativelylanguageagnosticduetotimeconstraintsandpracticalusabilityforre-searchersworkingonavarietyoflanguages.Thus,oneavenueofdataaugmentationwetriedwasbycreatingartificialdatafromtheprovidedtrainingdata.Thisrequiresnorule-writingorknowledgeofthegrammarofthelanguage,andthuscouldbeappliedquicklyandeasilytoallofthelanguagesinthesharedtask.Weusedanaiveword-swappingmethodtoran-domlyswapmorphemesthatoccurinsimilarcon-textstocreatenewsentences.Todothis,foreachglossline,wereplaceeachwordstem(thathasaglosslabelaffix)with\u201cSTEM\u201dtocreateaskeletongloss.Wenaivelydetermineifalabelisastembycheckingifitisinlowercase.Wedonotdothistowordsthatdonothaveaffixesas(withtheexceptionofUspanteko)wedonothaveaccesstopartsofspeech,anddonotwanttoswapwordsthatwouldcreateanungrammaticalsequence.Wecreateadictionarymappingeachskeletonwordglosstopossibleactualglosses,andmapeachactualglosstopossiblesurfaceforms(wemakenoassumptionsthatthesemappingsareone-to-one).Wethenrandomlysamplekrandomskeletonglosses(inthiscase,weusedkequaltoroughlythreetimestheamountoftrainingdata)andran-domlyfillinwordsthatmatchtheformatofskele-tonwordspresentintheline.(1)to(3)belowillustrateanexampleinthisprocess.Wecreateaskeletongloss(2)fromtheGitksansentencein(1)byreplacingtheallwordstemsthathaveanaffixwith\u201cSTEM\u201dinboththesegmentationandglosstiers\u2014inthiscase,only\u2019witxw-itappliestothisstep.Thentocreatetheartificialdatain(3),wereplacetheskeletonwordandcorrespondingglosswithanotherwordfromthetrainingdatathathasthesameskeletonform,inthiscasehahla\u2019lst-it.(1)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSP\u2019witxw-itcome-SX(2)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPSTEM-itSTEM-SX(3)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPhahla\u2019lst-itwork-SXWhilethismethodmaycreateasomewhatun-naturalinputsurfacesequence(asweareunabletocapturephonologicalchangesinthesurfaceformandcorrespondingtranslationsmaybenonsensi-cal),thismethodguaranteesthatthestructureoftheglossisanaturallyoccurringsequence(asweonlyuseglossskeletonsthatarepresentinthein-put).However,alimitationofthismethodisthatitdoesnotextendtoout-of-vocabularytokensorunseenglossstructures.Furthermore,aswecannotgenerateagold-standardtranslationfortheartifi-cialdata,wedonotmakeuseofatranslationintraining.2.2ODINAnotherpotentialavenuefordataaugmentationistransferlearningfromdatainotherlanguages,whichhasbeenshowntobeaneffectivemethodtoimproveperformanceinlow-resourcesettings(Ruderetal.,2019).TheavailableresourceweutilizeisODIN,ortheOnlineDatabaseforInterlinearText(LewisandXia,2010;Xiaetal.,2014).ODINcontains158,007linesofIGT,covering1,496languages.Weusethe2.1versionofODINdataandconvertthedatasettothesharedtaskformat,andfilteroutlanguageswithfewerthanfiveglossedsentences.However,thereremainssignificantnoiseinthedatasetthatcouldcausesignificantalignmentis-suesforthetokenclassificationmodels.ThereforeweopttoonlytraintheByT5modelsonODIN,inthehopethatthismodelislesssensitivetoalign-menterrors.Indeed,wefindthattheByT5modelfinetunedfirstonODINreceivesaperformanceboostwhenfinetunedagainonthesharedtaskdata.3ModelsWeexploretwomodelsforglossgeneration.Thefirstoneisbuiltuponthetokenclassificationbase-linewithsomeimprovements,andwetreatthismodelasourinternalbaseline.Thesecondmodelwedeploytestswhetherwecanachievecompeti-tiveperformancebyfinetuningapretrainedcharac-terbasedmultilingualandmultitaskmodel,ByT5.Forthismodel,weperformminimalpreprocess-inganduserawsegmentedmorphemesandfreetranslationsifavailable.3.1TokenClassificationTransformerWeusethebaselineTrack2modelprovidedbytheorganizersasastartingpoint.Theoriginalimple-mentationrandomlyinitializesatransformermodelfromthedefaultHuggingfaceRoBERTabasecon-figuration,andusesatokenclassificationobjective\n210\n\n\n1https://en.wikipedia.org/wiki/Lezgin_alphabets\nwithcross-entropyloss,whereeachglossistreatedasadistincttoken.Themorphemesandfreetrans-lationsaretokenizedbyspaceanddashes,withpunctuationspre-separated.TheyareconcatenatedandseparatedbytheSEPtokenandareusedastheinputstothemodel.WemodifytheoriginalTrack2baselinemodeltoobtainabetterbaseline.WeusepretrainedweightsfromXLM-RoBERTabase(Conneauetal.,2020),insteadofrandomlyinitializingtheweights.Wealsoslightlymodifythemorphemetokenizertoenforcethatthenumberofmorphemetokensmatchesthenumberofoutputglosstokensexactly.Additionally,weintroducetheCOPYtokentoreplacetheglossifitmatchesthecorrespondingmorphemeexactly.AnexamplefromNatuguisshowningloss(4):(4)67COPY.COPYmnc-xbe-1MINIMzloCOPYSkulCOPYWebelievethiswouldimproveperformancebyremovingtheneedtomemorizeglossedcode-switchingandpropernouns,thoughitisonlyef-fectiveifthecode-switchedlanguageisthesameasthematrixlanguage(e.g.Arapaho),andwouldhavenoeffectifthesourcelanguageusesadif-ferentorthographyoriscode-switchedtoanotherlanguage,wheretheglosswouldnotmatchedthemorphemeformexactly.Thismethodalsocom-pressesallpunctuationmarkersintoonetoken,buttheusefulnessofthissideeffectislessclear.Sinceweareusingpretrainedweights,itisthennaturaltoexploreintegratingthepretrainedtok-enizer.SinceXLM-RoBERTawasnottrainedonanyofthesourcelanguages,itmakesthemostsensetoonlyusethepretrainedtokenizertotok-enizefreetranslations,iftheyareavailable,andextendthevocabularytoincludemorphemes.3.2FinetunedByT5Multi-taskandmulti-lingualpretrainedlargelan-guagemodelshavebeenshowntobeeffectiveformanytasks.Weexplorewhethersuchmodelscanbeusedeffectivelyforglossing.Weconductex-perimentswithbothmT5(Xueetal.,2021)andByT5(Xueetal.,2022),butByT5ispreferredbecauseittakesrawtexts(bytesorcharacters)asinputsandintheoryshouldbemoreeffectiveforunseenlanguages.Weuseapromptbasedmultilin-gualsequencetosequenceobjectiveforbothmod-els.Theprompttemplateis:\u201cGenerateinterlin-earglossfrom[sourcelanguage]:[segmentedmorphemes]withits[matrixlanguage]trans-lation:[freetranslation]Answer:\u201d.Datafromalllanguagesaremixedtogetherandshuffled,withnoupordownsampling.Afterinitialexperi-ments,wefindByT5outperformsmT5acrossalllanguages,andthereforeweonlyconductsubse-quentexperimentsonByT5andreportthosere-sults.Uponinitialexperiments,wealsofindtheresultsforLezgitobelowerthanexpected.Wehypothe-sizethatthefactthatthedataareinCyrillicscriptcausesthisdeficiency,sinceByT5wastrainedonfarlessCyrillicdatathandataintheLatinscript.Thereforewecreateanautomaticromanizationtool,sourcedfromWikipedia1andintegratedintheEpitranpackage(Mortensenetal.,2018),andconvertallLezgidatatoLatinscriptforByT5fine-tuning.AfterinspectingtheoutputsoftheByT5models,wefindcaseswherepunctuationsareattachedtothepreviousglosses,insteadofbeingseparatedbyaspaceasisstandardinthetrainingsets.Thisisprobablyduetothefactthatthemodelwaspre-trainedonuntokenizeddataandthisbehaviorispreserveddespitefinetuningontokenizeddata.Wethereforeuseasimpleregularexpressionbasedtok-enizertofixtheinconsistencies.WenoticethattheprocedureonlygivesperformanceboostonGitk-san,Lezgi,Uspanteko,andNatugu,andsoweonlyapplytheproceduretothoselanguages,leavingtherestoftheoutputsunchanged.4DictionaryPost-correction:GitksanOneofthekeychallengesforextremelylowre-sourcelanguagesistheintegrationofstructuredlinguisticdatainotherforms,suchasadictionary,intomachinelearningpipelines.Wetestasimplepost-correctionmethodfromapre-existingdictio-naryonGitksanonly,duetoitsuniquecombinationoflowresourceandeasilyobtainabledictionaryinmachinereadableform.Weusethedictionarycom-piledbyForbesetal.(2021),withoutconsultingthemorphologicalanalyzersthattheyalsoprovided.Atinferencetime,ifamorphemeisunseendur-ingtraining,wesearchfortheexactforminthedictionary.Wealsoexpandthesearchtoallsubse-quencesofmorphemeswithintheenclosingword,plusthepreviouswholewordincaseswhereapar-ticleisincludedinthedictionaryform.Thefirst\n211\n\n\nmatcheddefinitionisusedastheglossandifnoneofthesearchyieldsanexactmatch,wefallbacktothemodelprediction.Weonlyapplythismethodtothetokenclassificationmodelsbecausethealign-mentbetweenmorphemesandglossesisdirectlyestablished,whereastheseq2seqmodelsdonotguaranteethatthenumberofglossesmatchesthenumberofmorphemes.5ResultsandDiscussionTables1and2showoursystems\u2019performance(aswellastheoriginalbaseline)onthetestdatawithre-specttoword-andmorpheme-levelmicro-averagedaccuracy,respectively.Overall,thetokenclassifica-tionmodeltrainedfirstontheartificiallygeneratedaugmenteddataperformthebest,withthemodeltrainedonthesharedtaskdataonlynotfarbehind.Meanwhile,ByT5modelsperformworse,withthemodelfinetunedfirstonODINtrailingourbestmodelbyafewpercentagepoints,whilethemodelfinetunedfirstonaugmenteddataperformsworsethanthebaseline.5.1DataAugmentationOverall,wefinddataaugmentationtobeuseful.Withartificiallygenerateddata,weseetheeffectsareperhapsgreatestforthemid-resourcelanguages(ddo,lez,ntu,nyb,usp),whilethehighestandlowestresourcedlanguagesdidnotreceivemuchbenefitfrompretrainingontheartificialdata.Wethinkthisisperhapsbecausethereisa\u201csweetspot\u201dwithrespecttotheamountofdatathatisrequiredtotrainamodel.Ifthereisenoughdataalready,inthecaseofArapaho,thenthenoisinessofartificialdatawouldout-weightthebenefitoftrainingonthem.Ontheotherendofthescale,Gitksanperhapsneedsmoresyntheticdatafordataaugmentationtoyieldmeaningfulimprovements.ForByT5models,artificiallygenerateddataseemtohavetheoppositeeffect,whereperfor-manceissignificantlydegraded.Aspeculationforthiseffectisthefactthepretrainedmodelismoresemanticallyaware,andsincetheartificiallygen-eratedsentencescouldbenonsensical,themodelcouldbecomeconfused.Ontheotherhand,pre-trainingonODINyieldsimprovementsforthema-jorityofthelanguages2.ThisisencouragingsincewedidnotperformmuchpreprocessingforODIN,\n2TsezistheonlylanguagethatappearedinODIN(68sentences).Wedidnotremoveitfromthecorpusbutthisshouldhavelittleinfluenceontheperformancebecausethesizeofthedatasetisverysmall.andthereisdefinitelystillroomtomakethedatacleanerandmoreinternallyconsistent,whichinturnshouldresultinabettermodel.5.2ChoiceofHyperparametersWefindthechoiceofhyperparametersofthetokenclassificationmodelstobenecessarilylanguageanddatasetspecific.ArapahoandGitksaninpar-ticularneedspecialattention,wherethenumberoftrainingepochsneedtobeadjustedfortheveryhighandlowdatasize.WealsodevelopedmostoftheoptimizationonthetokenclassificationmodelonArapaho.However,wedidnothavetimetopropagatethechanges(usingpretrainedtokenizer,savingthelastmodelinsteadofthemodelwiththelowestvalidationloss)totherestoflanguagessinceinitialexperimentshowedthatpretrainedtokeniz-ersdidnotimproveontheotherlanguages.How-ever,afterthesubmissiondeadlineisconcluded,weranmoreexperimentsanddiscoveredthataddingpretrainedtokenizersrequiresmoretrainingsteps,andthetrainingisbettercontrolledbyspecifyingthetrainingstepsinsteadofepochs.Wedonotin-cludethoselatestexperimentsinthispaper,butourtokenclassificationmodelshavethepotentialtoperformbetterwithmorehyperparametertuning.5.3In-VersusOut-of-VocabularyErrorsOnedimensionoferroranalysisweinvestigatedwaswhatproportionofoursystems\u2019errorscomefrommorphemesorwordsthatareeitherinoroutofthetrainingdatavocabulary.Wecountamor-phemeorwordasin-vocabularyifthesurfaceformanditscorrespondingglossco-occurinthepro-videdtrainingdata(notincludingthedevelopmentdata,asourmodelsareonlytrainedonthetrainset).NotethatthereisamuchlargerproportionofOOVwordsasopposedtomorphemesduetothefactthatanunseenwordcanbecomposedofdifferentcombinationsofseenmorphemes.Table3showstheproportionofmorphemesandwordsthatareout-of-vocab(OOV)withinthetestset.Whilenearlyallthelanguageshavelessthan10%oftheirmorphemesclassifiedasOOV,GitksannotablyhasarelativelylargeportionofOOVtestdata,with\u224845%ofmorphemesand\u224878%ofwordsbeingOOV.Tables4and5showourmodels\u2019performancesonin-versesout-of-vocabtokensatthemorphemeandwordlevels,respectively.Whilewewouldintuitivelyexpectthatword-levelOOVaccuracybeaboutthesameorworsethanmorpheme-levelOOV\n212\n\n\nTable2:Morpheme-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselinewithartificialpretraininganddictionarypost-correctionisthehighestGitksanaccuracyreportedoutofallsharedtasksubmissions.\nxlmr-base91.3684.3547.47/52.8280.1788.3585.8480.0880.42xlmr-aug89.3488.1546.89/52.3982.3685.5389.4983.0881.48\nTable3:ProportionofmorphemesandwordsthatareOOVwithinthetestset.accuracy,thisisnotthecaseduetothefactthatalargeportionofout-of-vocabwordsareformedwithin-vocabmorphemes.Formostlanguages,withtheexceptionofGitksan,thereappearstobeatrade-offbetweenbetterin-vocabmorphemeperformancewithXLMRandperformanceout-of-vocabwithByT5.6RelatedWorkTherehavebeenavarietyofapproachestotheprob-lemof(semi-)automaticallygeneratinginterlineargloss.BaldridgeandPalmer(2009)investigatetheefficacyofactivelearningforthetaskofinterlinearglossing,usingannotationtimerequiredbyexpertandnon-expertannotatorsastheirmetric.Thesys-temtheyusetogenerateglosslabelsuggestionsisastandardmaximumentropyclassifier.Arule-basedapproachbySnoeketal.(2014)utilizesanFSTtogenerateglossesforPlainsCree,focusingonnouns.Samard\u017ei\u00b4cetal.(2015)viewthetaskofglossingsegmentedtextasatwo-stepprocess,firsttreatingitasastandardPOStaggingtaskandthenaddinglexicalglossesfromadictio-nary.TheydemonstratethismethodonaChintangcorpusofabout1.2millionwords.Anumberofotherworksfocusingoninterlinearglossingutilizeconditionalrandomfield(CRF)models.MoellerandHulden(2018)testthreedifferentmodelsonaverysmallLezgidataset(<3000words):aCRF(thatoutputsBIOlabelswiththecorrespondingglosspercharacterinthein-put),asegmentationandlabellingpipelinethatuti-lizesaCRF(forBIOlabels)andSVM(forglossla-bels),andanLSTMseq2seqmodel.TheyfindthattheCRFthatjointlyproducestheBIOlabelsandtagsproducedthebestresults.McMillan-Major(2020)utilizestranslationsintheirtrainingdatabycreatingtwoCRFmodels,onethatpredictsglossfromthesegmentedinputandanotherthanpre-\nModelarpddogitlezntunybuspAVG\nModelarpddogitlezntunybuspAVG\nbyt5-base78.8680.3214.8460.72b76.6776.7377.2166.48byt5-aug73.2762.374.1738.6055.1169.2570.8553.38byt5-odin80.5682.7920.5763.7777.9782.5975.7269.14\nbaseline91.1185.3425.3351.8249.0388.7182.4867.69\nbaseline85.4475.7116.4134.5441.0884.3076.5559.14\nxlmr-base85.8773.7727.86/34.11a74.1582.9980.6173.4772.14xlmr-aug82.9280.0724.74/31.2577.7778.7285.5377.5173.39\naWereportbefore/afterdictionarybasedpost-correctionforGitksan.bWetrainedthismodelwithoutromanizingLezgi.Table1:Word-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselineisthehighestArapahoaccuracyreportedoutofallsharedtasksubmissions.\nbyt5-base78.8275.7712.5944.1062.4078.9774.2560.99byt5-aug72.1057.932.6026.2435.6270.0167.7347.46byt5-odin80.8178.2412.7450.0063.3985.3073.2563.39\nMorph0.0430.0090.4500.0560.0340.0190.070\narpddogitlezntunybusp\nWord0.2420.1550.7810.1690.2140.0840.200\n213\n\n\nbyt5-aug74.7658.243.4240.2736.5471.2770.56\n35.48\nTable4:Morpheme-levelaccuracyoveralltokensofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.\n24.67\n12.31\nxlmr-base95.2085.1282.8984.7990.8787.4686.05\n23.24\n45.07\nxlmr-aug93.7283.8594.0587.6489.2490.8191.11\n29.33\n24.10\n49.79\nbyt5-odin91.9387.6663.1073.7885.9387.6083.46\n59.51\n23.54\nbyt5-odin83.4778.5518.4262.9064.3886.8575.23\n18.26\n22.41\n49.17\nTable5:Word-levelaccuracyofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.dictsfromthetranslation,andthenusesheuristicstodeterminewhichmodeltoselectfromforeachmorpheme.BarrigaMart\u00ednezetal.(2021)usedaCRFmodeltoachieve>90%accuracyforgloss-ingOtomiandfindthatitworksbetterthananRNN,whichiscomputationallymoreexpensive.Otherworks,includingoursystems,haveturnedtoneuralmethods.Kondratyuk(2019)leveragespretrainedmultilingualBERTtoencodeinputsentences,thenapplyadditionalword-levelandcharacter-levelLSTMlayersbeforejointlydecod-inglemmasandmorphologytagsusingsimplese-quencetagginglayers.Furthermore,theyshowthattwo-stagetrainingbyfirsttrainingonalllan-guagesfollowedbytrainingonthetargetlanguageismoreeffectivethantrainingthesystemonthetargetlanguagealone.AnapproachbyZhaoetal.(2020),likeMcMillan-Major(2020),makesuseoftranslationsavailableinparallelcorpora,butdosobyusingamulti-sourcetransformermodel.Theyalsoincorporatelengthcontrolandalignmentdur-inginferencetoenhancetheirmodel,andtesttheirsystemonArapaho,Tsez,andLezgi.7ConclusionInoursharedtasksubmission,weexploredataaug-mentationmethodsandmodelingstrategiesforthetaskofinterlinearglossinginsevenlow-resourcelanguages.Ourbestperformingmodelsareto-kenclassificationmodelsusingXLMR.Wedemon-stratethatpretrainingonartificialdatawithXLMRisaneffectivetechniqueforthemid-resourcetestlanguages.Additionally,inourerroranalysiswefindthatwemayhaveactuallyundertrainedourtokenclassificationmodels,andthusoursystemsmayhavethepotentialtoperformbetterwithad-ditionalhyperparametertuning.WhileourByT5modelsdidnotperformaswellasourothersys-tems,weshowthatpretrainingonODINdataiseffective,despitethisdatabeingverynoisy.Finally,wealsodemonstrateimprovementsbyutilizingadictionarytopost-correctmodeloutputsforGitk-san.\n0.00\n0.00\n0.00\n0.00\n11.24\n0.82\n12.86\n14.52\n23.60\n4.97\n56.36\n45.65\n21.14\n43.37\n1.61\n47.52\n46.94\n29.69\n13.67\nxlmr-base95.9378.1895.2384.2493.1485.8586.27\n5.79\n28.04\n2.00\n17.00\n0.41\n16.08\n14.67\n19.35\nModelarpddogitlezntunybusp\nModelarpddogitlezntunybusp\n7.49\n28.09\n28.09\n44.81\n3.23\n3.23\n54.44\n2.33\nbyt5-aug87.2268.6910.7146.0665.1374.5981.44\n30.20\nxlmr-aug92.9888.9484.7487.1087.8891.1789.31\n48.70\n40.00\n28.63\n8.67\n2.60\n2.60\n9.68\n214\n\n\nAcknowledgementsThisworkwassupportedbyNSFCISERIgrantnumber2211951,FromAcousticSignaltoMor-phosyntacticAnalysisinoneEnd-to-EndNeuralSystem.ReferencesJasonBaldridgeandAlexisPalmer.2009.Howwelldoesactivelearningactuallywork?Time-basedeval-uationofcost-reductionstrategiesforlanguagedocu-mentation.InProceedingsofthe2009ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages296\u2013305,Singapore.AssociationforCompu-tationalLinguistics.DiegoBarrigaMart\u00ednez,VictorMijangos,andXi-menaGutierrez-Vasques.2021.Automaticinterlin-earglossingforOtomilanguage.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages34\u201343,Online.AssociationforComputationalLin-guistics.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.ClarissaForbes,GarrettNicolai,andMiikkaSilfverberg.2021.AnFSTmorphologicalanalyzerforthegitksanlanguage.InProceedingsofthe18thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages188\u2013197,Online.AssociationforComputationalLinguistics.DanKondratyuk.2019.Cross-linguallemmatizationandmorphologytaggingwithtwo-stagemultilin-gualBERTfine-tuning.InProceedingsofthe16thWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages12\u201318,Florence,Italy.AssociationforComputationalLinguistics.WilliamD.LewisandFeiXia.2010.DevelopingODIN:AMultilingualRepositoryofAnnotatedLanguageDataforHundredsoftheWorld\u2019sLanguages.Liter-aryandLinguisticComputing,25(3):303\u2013319.AngelinaMcMillan-Major.2020.Automatingglossgenerationininterlinearglossedtext.ProceedingsoftheSocietyforComputationinLinguistics,3(1):338\u2013349.SarahMoellerandMansHulden.2018.Automaticglossinginalow-resourcesettingforlanguagedoc-umentation.InProceedingsoftheWorkshoponComputationalModelingofPolysyntheticLanguages,pages84\u201393.DavidR.Mortensen,SiddharthDalmia,andPatrickLittell.2018.Epitran:PrecisionG2Pformanylan-guages.InProceedingsoftheEleventhInternationalConferenceonLanguageResourcesandEvaluation(LREC2018),Miyazaki,Japan.EuropeanLanguageResourcesAssociation(ELRA).SebastianRuder,MatthewEPeters,SwabhaSwayamdipta,andThomasWolf.2019.Transferlearninginnaturallanguageprocessing.InProceed-ingsofthe2019conferenceoftheNorthAmericanchapteroftheassociationforcomputationallinguistics:Tutorials,pages15\u201318.TanjaSamard\u017ei\u00b4c,RobertSchikowski,andSabineStoll.2015.Automaticinterlinearglossingastwo-levelsequenceclassification.NoamShazeerandMitchellStern.2018.Adafactor:Adaptivelearningrateswithsublinearmemorycost.InProceedingsofthe35thInternationalConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,pages4596\u20134604.PMLR.ConorSnoek,DorothyThunder,KaidiL\u00f5o,AnttiArppe,JordanLachler,SjurMoshagen,andTrondTrosterud.2014.ModelingthenounmorphologyofPlainsCree.InProceedingsofthe2014WorkshopontheUseofComputationalMethodsintheStudyofEndangeredLanguages,pages34\u201342,Baltimore,Maryland,USA.AssociationforComputationalLinguistics.FeiXia,WilliamLewis,MichaelWayneGoodman,JoshuaCrowgey,andEmilyM.Bender.2014.En-richingODIN.InProceedingsoftheNinthInter-nationalConferenceonLanguageResourcesandEvaluation(LREC\u201914),pages3151\u20133157,Reykjavik,Iceland.EuropeanLanguageResourcesAssociation(ELRA).LintingXue,AdityaBarua,NoahConstant,RamiAl-Rfou,SharanNarang,MihirKale,AdamRoberts,andColinRaffel.2022.ByT5:Towardsatoken-freefuturewithpre-trainedbyte-to-bytemodels.Transac-tionsoftheAssociationforComputationalLinguis-tics,10:291\u2013306.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.XingyuanZhao,SatoruOzaki,AntoniosAnastasopou-los,GrahamNeubig,andLoriLevin.2020.Auto-maticinterlinearglossingforunder-resourcedlan-guagesleveragingtranslations.InProceedingsofthe28thInternationalConferenceonComputationalLinguistics,pages5397\u20135408,Barcelona,Spain(On-line).InternationalCommitteeonComputationalLin-guistics.\n215\n\n\nAHyperparameterSettingsWeuseAdafactor(ShazeerandStern,2018)astheoptimizeracrossallexperiments,withthede-faultschedulerfromHuggingFaceTransformers,abatchsizeof32forRoBERTabasedmodelsandabatchsizeof4withagradientaccumulationstepof8forByT5basedmodels.Wetrainthetokenclassificationmodelsfor40epochsexceptforAra-paho,onwhichwetrain20epochs,andGitksan,onwhichwetrain2,000steps.WetraintheByT5basedmodelsfor20epochsonallofthedatamixedtogether.\n216"}, {"question": " Why was data augmentation considered for this shared task?", "answer": " Data augmentation was considered due to the small scale of training data provided for the languages.,", "ref_chunk": "SigMoreFunSubmissiontotheSIGMORPHONSharedTaskonInterlinearGlossingTaiqiHe\u2217,LindiaTjuatja\u2217,NateRobinson,ShinjiWatanabe,DavidR.Mortensen,GrahamNeubig,LoriLevinLanguageTechnologiesInstituteCarnegieMellonUniversity{taiqih,ltjuatja,nrrobins,swatanab,dmortens,gneubig,lsl}@cs.cmu.eduAbstractInoursubmissiontotheSIGMORPHON2023SharedTaskoninterlinearglossing(IGT),weexploreapproachestodataaugmentationandmodelingacrosssevenlow-resourcelanguages.Fordataaugmentation,weexploretwoap-proaches:creatingartificialdatafromthepro-videdtrainingdataandutilizingexistingIGTresourcesinotherlanguages.Onthemodelingside,wetestanenhancedversionofthepro-videdtokenclassificationbaselineaswellasapretrainedmultilingualseq2seqmodel.Ad-ditionally,weapplypost-correctionusingadictionaryforGitksan,thelanguagewiththesmallestamountofdata.Wefindthatourtokenclassificationmodelsarethebestperforming,withthehighestword-levelaccuracyforAra-pahoandhighestmorpheme-levelaccuracyforGitksanoutofallsubmissions.Wealsoshowthatdataaugmentationisaneffectivestrategy,thoughapplyingartificialdatapretraininghasverydifferenteffectsacrossbothmodelstested.1IntroductionThispaperdescribestheSigMoreFunsubmissiontotheSIGMORPHON2023SharedTaskoninterlin-earglossing.Giveninputtextinatargetlanguage,thetaskistopredictthecorrespondinginterlineargloss(usingLeipzigglossingconventions).IGTisanimportantformoflinguisticannotationforthemorphologicalanalysisoflanguages,andalsoservesasanextremelyvaluableresourceforlan-guagedocumentationandeducationforspeakersoflow-resourcelanguages.Thereweretwotracksforthissharedtask,Track1(closed)andTrack2(open).ForTrack1,sys-temscouldonlybetrainedoninputsentencesandglosses;inTrack2,systemscouldmakeuseofthemorphologicalsegmentationoftheinputaswellasany(non-IGT)externalresources.SincetheTrack2settingbettermatchesthelong-termre-\n\u2217Theseauthorscontributedequallysearchgoalsofourteam,weonlyparticipateinthisopentrack.Inoursubmission,weinvestigatetwodifferentapproaches.First,weattemptdataaugmentationbyeithercreatingourownartificialglossdatabymanipulatingtheexistingtrainingdata,orbyuti-lizingexistingresourcescontainingIGTinotherlanguages(\u00a72).Second,weexploretwodifferentmodelsforglossgeneration(\u00a73).Thefirstbuildsoffthetokenclassificationbaseline,whilethesec-ondusesapretrainedmultilingualseq2seqmodel.Finally,wealsoattempttopost-correctmodeloutputswithadictionary.WeapplythistoGitk-sanandfindthatthis,combinedwithourotherapproaches,resultsinthehighestmorpheme-levelaccuracyforGitksaninTrack2.2DataAugmentationOnemajorchallengeforthissharedtaskisthescaleofdataprovided.Allofthelanguageshavelessthan40klinesoftrainingdata,andallbutArapahohavelessthan10k.Thesmallestdataset(Gitk-san)hasonly31linesofdata.Thus,oneobviousmethodtotryisdataaugmentation.Morespecif-ically,wetrypretrainingourmodelsondifferentformsofaugmenteddatabeforetrainingthemontheoriginaltargetlanguagedata.Weexploredtwoformsofdataaugmentation.First,wegeneratedartificialglossdatainthetar-getlanguagebyswappingwordsintheexistingtrainingdata.Second,weutilizeddatafromODIN(LewisandXia,2010;Xiaetal.,2014)toseeiftransferlearningfromdatainotherlanguagescanhelpimproveperformance.2.1ArtificialDataAchallengeourteamfacedwithrespecttodataaugmentationisfiguringouthowtoobtainaddi-tionaldatawhenwedonothavemuchknowledgeofthelanguages\u2019grammaticalsystems,alongwiththefactthattheselanguagesaregenerallyfrom\n209 Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 209\u2013216 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\ndigitallyunder-resourcedlanguagefamilies.Fur-thermore,wewantedoursolutiontobeeasilyim-plementedandrelativelylanguageagnosticduetotimeconstraintsandpracticalusabilityforre-searchersworkingonavarietyoflanguages.Thus,oneavenueofdataaugmentationwetriedwasbycreatingartificialdatafromtheprovidedtrainingdata.Thisrequiresnorule-writingorknowledgeofthegrammarofthelanguage,andthuscouldbeappliedquicklyandeasilytoallofthelanguagesinthesharedtask.Weusedanaiveword-swappingmethodtoran-domlyswapmorphemesthatoccurinsimilarcon-textstocreatenewsentences.Todothis,foreachglossline,wereplaceeachwordstem(thathasaglosslabelaffix)with\u201cSTEM\u201dtocreateaskeletongloss.Wenaivelydetermineifalabelisastembycheckingifitisinlowercase.Wedonotdothistowordsthatdonothaveaffixesas(withtheexceptionofUspanteko)wedonothaveaccesstopartsofspeech,anddonotwanttoswapwordsthatwouldcreateanungrammaticalsequence.Wecreateadictionarymappingeachskeletonwordglosstopossibleactualglosses,andmapeachactualglosstopossiblesurfaceforms(wemakenoassumptionsthatthesemappingsareone-to-one).Wethenrandomlysamplekrandomskeletonglosses(inthiscase,weusedkequaltoroughlythreetimestheamountoftrainingdata)andran-domlyfillinwordsthatmatchtheformatofskele-tonwordspresentintheline.(1)to(3)belowillustrateanexampleinthisprocess.Wecreateaskeletongloss(2)fromtheGitksansentencein(1)byreplacingtheallwordstemsthathaveanaffixwith\u201cSTEM\u201dinboththesegmentationandglosstiers\u2014inthiscase,only\u2019witxw-itappliestothisstep.Thentocreatetheartificialdatain(3),wereplacetheskeletonwordandcorrespondingglosswithanotherwordfromthetrainingdatathathasthesameskeletonform,inthiscasehahla\u2019lst-it.(1)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSP\u2019witxw-itcome-SX(2)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPSTEM-itSTEM-SX(3)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPhahla\u2019lst-itwork-SXWhilethismethodmaycreateasomewhatun-naturalinputsurfacesequence(asweareunabletocapturephonologicalchangesinthesurfaceformandcorrespondingtranslationsmaybenonsensi-cal),thismethodguaranteesthatthestructureoftheglossisanaturallyoccurringsequence(asweonlyuseglossskeletonsthatarepresentinthein-put).However,alimitationofthismethodisthatitdoesnotextendtoout-of-vocabularytokensorunseenglossstructures.Furthermore,aswecannotgenerateagold-standardtranslationfortheartifi-cialdata,wedonotmakeuseofatranslationintraining.2.2ODINAnotherpotentialavenuefordataaugmentationistransferlearningfromdatainotherlanguages,whichhasbeenshowntobeaneffectivemethodtoimproveperformanceinlow-resourcesettings(Ruderetal.,2019).TheavailableresourceweutilizeisODIN,ortheOnlineDatabaseforInterlinearText(LewisandXia,2010;Xiaetal.,2014).ODINcontains158,007linesofIGT,covering1,496languages.Weusethe2.1versionofODINdataandconvertthedatasettothesharedtaskformat,andfilteroutlanguageswithfewerthanfiveglossedsentences.However,thereremainssignificantnoiseinthedatasetthatcouldcausesignificantalignmentis-suesforthetokenclassificationmodels.ThereforeweopttoonlytraintheByT5modelsonODIN,inthehopethatthismodelislesssensitivetoalign-menterrors.Indeed,wefindthattheByT5modelfinetunedfirstonODINreceivesaperformanceboostwhenfinetunedagainonthesharedtaskdata.3ModelsWeexploretwomodelsforglossgeneration.Thefirstoneisbuiltuponthetokenclassificationbase-linewithsomeimprovements,andwetreatthismodelasourinternalbaseline.Thesecondmodelwedeploytestswhetherwecanachievecompeti-tiveperformancebyfinetuningapretrainedcharac-terbasedmultilingualandmultitaskmodel,ByT5.Forthismodel,weperformminimalpreprocess-inganduserawsegmentedmorphemesandfreetranslationsifavailable.3.1TokenClassificationTransformerWeusethebaselineTrack2modelprovidedbytheorganizersasastartingpoint.Theoriginalimple-mentationrandomlyinitializesatransformermodelfromthedefaultHuggingfaceRoBERTabasecon-figuration,andusesatokenclassificationobjective\n210\n\n\n1https://en.wikipedia.org/wiki/Lezgin_alphabets\nwithcross-entropyloss,whereeachglossistreatedasadistincttoken.Themorphemesandfreetrans-lationsaretokenizedbyspaceanddashes,withpunctuationspre-separated.TheyareconcatenatedandseparatedbytheSEPtokenandareusedastheinputstothemodel.WemodifytheoriginalTrack2baselinemodeltoobtainabetterbaseline.WeusepretrainedweightsfromXLM-RoBERTabase(Conneauetal.,2020),insteadofrandomlyinitializingtheweights.Wealsoslightlymodifythemorphemetokenizertoenforcethatthenumberofmorphemetokensmatchesthenumberofoutputglosstokensexactly.Additionally,weintroducetheCOPYtokentoreplacetheglossifitmatchesthecorrespondingmorphemeexactly.AnexamplefromNatuguisshowningloss(4):(4)67COPY.COPYmnc-xbe-1MINIMzloCOPYSkulCOPYWebelievethiswouldimproveperformancebyremovingtheneedtomemorizeglossedcode-switchingandpropernouns,thoughitisonlyef-fectiveifthecode-switchedlanguageisthesameasthematrixlanguage(e.g.Arapaho),andwouldhavenoeffectifthesourcelanguageusesadif-ferentorthographyoriscode-switchedtoanotherlanguage,wheretheglosswouldnotmatchedthemorphemeformexactly.Thismethodalsocom-pressesallpunctuationmarkersintoonetoken,buttheusefulnessofthissideeffectislessclear.Sinceweareusingpretrainedweights,itisthennaturaltoexploreintegratingthepretrainedtok-enizer.SinceXLM-RoBERTawasnottrainedonanyofthesourcelanguages,itmakesthemostsensetoonlyusethepretrainedtokenizertotok-enizefreetranslations,iftheyareavailable,andextendthevocabularytoincludemorphemes.3.2FinetunedByT5Multi-taskandmulti-lingualpretrainedlargelan-guagemodelshavebeenshowntobeeffectiveformanytasks.Weexplorewhethersuchmodelscanbeusedeffectivelyforglossing.Weconductex-perimentswithbothmT5(Xueetal.,2021)andByT5(Xueetal.,2022),butByT5ispreferredbecauseittakesrawtexts(bytesorcharacters)asinputsandintheoryshouldbemoreeffectiveforunseenlanguages.Weuseapromptbasedmultilin-gualsequencetosequenceobjectiveforbothmod-els.Theprompttemplateis:\u201cGenerateinterlin-earglossfrom[sourcelanguage]:[segmentedmorphemes]withits[matrixlanguage]trans-lation:[freetranslation]Answer:\u201d.Datafromalllanguagesaremixedtogetherandshuffled,withnoupordownsampling.Afterinitialexperi-ments,wefindByT5outperformsmT5acrossalllanguages,andthereforeweonlyconductsubse-quentexperimentsonByT5andreportthosere-sults.Uponinitialexperiments,wealsofindtheresultsforLezgitobelowerthanexpected.Wehypothe-sizethatthefactthatthedataareinCyrillicscriptcausesthisdeficiency,sinceByT5wastrainedonfarlessCyrillicdatathandataintheLatinscript.Thereforewecreateanautomaticromanizationtool,sourcedfromWikipedia1andintegratedintheEpitranpackage(Mortensenetal.,2018),andconvertallLezgidatatoLatinscriptforByT5fine-tuning.AfterinspectingtheoutputsoftheByT5models,wefindcaseswherepunctuationsareattachedtothepreviousglosses,insteadofbeingseparatedbyaspaceasisstandardinthetrainingsets.Thisisprobablyduetothefactthatthemodelwaspre-trainedonuntokenizeddataandthisbehaviorispreserveddespitefinetuningontokenizeddata.Wethereforeuseasimpleregularexpressionbasedtok-enizertofixtheinconsistencies.WenoticethattheprocedureonlygivesperformanceboostonGitk-san,Lezgi,Uspanteko,andNatugu,andsoweonlyapplytheproceduretothoselanguages,leavingtherestoftheoutputsunchanged.4DictionaryPost-correction:GitksanOneofthekeychallengesforextremelylowre-sourcelanguagesistheintegrationofstructuredlinguisticdatainotherforms,suchasadictionary,intomachinelearningpipelines.Wetestasimplepost-correctionmethodfromapre-existingdictio-naryonGitksanonly,duetoitsuniquecombinationoflowresourceandeasilyobtainabledictionaryinmachinereadableform.Weusethedictionarycom-piledbyForbesetal.(2021),withoutconsultingthemorphologicalanalyzersthattheyalsoprovided.Atinferencetime,ifamorphemeisunseendur-ingtraining,wesearchfortheexactforminthedictionary.Wealsoexpandthesearchtoallsubse-quencesofmorphemeswithintheenclosingword,plusthepreviouswholewordincaseswhereapar-ticleisincludedinthedictionaryform.Thefirst\n211\n\n\nmatcheddefinitionisusedastheglossandifnoneofthesearchyieldsanexactmatch,wefallbacktothemodelprediction.Weonlyapplythismethodtothetokenclassificationmodelsbecausethealign-mentbetweenmorphemesandglossesisdirectlyestablished,whereastheseq2seqmodelsdonotguaranteethatthenumberofglossesmatchesthenumberofmorphemes.5ResultsandDiscussionTables1and2showoursystems\u2019performance(aswellastheoriginalbaseline)onthetestdatawithre-specttoword-andmorpheme-levelmicro-averagedaccuracy,respectively.Overall,thetokenclassifica-tionmodeltrainedfirstontheartificiallygeneratedaugmenteddataperformthebest,withthemodeltrainedonthesharedtaskdataonlynotfarbehind.Meanwhile,ByT5modelsperformworse,withthemodelfinetunedfirstonODINtrailingourbestmodelbyafewpercentagepoints,whilethemodelfinetunedfirstonaugmenteddataperformsworsethanthebaseline.5.1DataAugmentationOverall,wefinddataaugmentationtobeuseful.Withartificiallygenerateddata,weseetheeffectsareperhapsgreatestforthemid-resourcelanguages(ddo,lez,ntu,nyb,usp),whilethehighestandlowestresourcedlanguagesdidnotreceivemuchbenefitfrompretrainingontheartificialdata.Wethinkthisisperhapsbecausethereisa\u201csweetspot\u201dwithrespecttotheamountofdatathatisrequiredtotrainamodel.Ifthereisenoughdataalready,inthecaseofArapaho,thenthenoisinessofartificialdatawouldout-weightthebenefitoftrainingonthem.Ontheotherendofthescale,Gitksanperhapsneedsmoresyntheticdatafordataaugmentationtoyieldmeaningfulimprovements.ForByT5models,artificiallygenerateddataseemtohavetheoppositeeffect,whereperfor-manceissignificantlydegraded.Aspeculationforthiseffectisthefactthepretrainedmodelismoresemanticallyaware,andsincetheartificiallygen-eratedsentencescouldbenonsensical,themodelcouldbecomeconfused.Ontheotherhand,pre-trainingonODINyieldsimprovementsforthema-jorityofthelanguages2.ThisisencouragingsincewedidnotperformmuchpreprocessingforODIN,\n2TsezistheonlylanguagethatappearedinODIN(68sentences).Wedidnotremoveitfromthecorpusbutthisshouldhavelittleinfluenceontheperformancebecausethesizeofthedatasetisverysmall.andthereisdefinitelystillroomtomakethedatacleanerandmoreinternallyconsistent,whichinturnshouldresultinabettermodel.5.2ChoiceofHyperparametersWefindthechoiceofhyperparametersofthetokenclassificationmodelstobenecessarilylanguageanddatasetspecific.ArapahoandGitksaninpar-ticularneedspecialattention,wherethenumberoftrainingepochsneedtobeadjustedfortheveryhighandlowdatasize.WealsodevelopedmostoftheoptimizationonthetokenclassificationmodelonArapaho.However,wedidnothavetimetopropagatethechanges(usingpretrainedtokenizer,savingthelastmodelinsteadofthemodelwiththelowestvalidationloss)totherestoflanguagessinceinitialexperimentshowedthatpretrainedtokeniz-ersdidnotimproveontheotherlanguages.How-ever,afterthesubmissiondeadlineisconcluded,weranmoreexperimentsanddiscoveredthataddingpretrainedtokenizersrequiresmoretrainingsteps,andthetrainingisbettercontrolledbyspecifyingthetrainingstepsinsteadofepochs.Wedonotin-cludethoselatestexperimentsinthispaper,butourtokenclassificationmodelshavethepotentialtoperformbetterwithmorehyperparametertuning.5.3In-VersusOut-of-VocabularyErrorsOnedimensionoferroranalysisweinvestigatedwaswhatproportionofoursystems\u2019errorscomefrommorphemesorwordsthatareeitherinoroutofthetrainingdatavocabulary.Wecountamor-phemeorwordasin-vocabularyifthesurfaceformanditscorrespondingglossco-occurinthepro-videdtrainingdata(notincludingthedevelopmentdata,asourmodelsareonlytrainedonthetrainset).NotethatthereisamuchlargerproportionofOOVwordsasopposedtomorphemesduetothefactthatanunseenwordcanbecomposedofdifferentcombinationsofseenmorphemes.Table3showstheproportionofmorphemesandwordsthatareout-of-vocab(OOV)withinthetestset.Whilenearlyallthelanguageshavelessthan10%oftheirmorphemesclassifiedasOOV,GitksannotablyhasarelativelylargeportionofOOVtestdata,with\u224845%ofmorphemesand\u224878%ofwordsbeingOOV.Tables4and5showourmodels\u2019performancesonin-versesout-of-vocabtokensatthemorphemeandwordlevels,respectively.Whilewewouldintuitivelyexpectthatword-levelOOVaccuracybeaboutthesameorworsethanmorpheme-levelOOV\n212\n\n\nTable2:Morpheme-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselinewithartificialpretraininganddictionarypost-correctionisthehighestGitksanaccuracyreportedoutofallsharedtasksubmissions.\nxlmr-base91.3684.3547.47/52.8280.1788.3585.8480.0880.42xlmr-aug89.3488.1546.89/52.3982.3685.5389.4983.0881.48\nTable3:ProportionofmorphemesandwordsthatareOOVwithinthetestset.accuracy,thisisnotthecaseduetothefactthatalargeportionofout-of-vocabwordsareformedwithin-vocabmorphemes.Formostlanguages,withtheexceptionofGitksan,thereappearstobeatrade-offbetweenbetterin-vocabmorphemeperformancewithXLMRandperformanceout-of-vocabwithByT5.6RelatedWorkTherehavebeenavarietyofapproachestotheprob-lemof(semi-)automaticallygeneratinginterlineargloss.BaldridgeandPalmer(2009)investigatetheefficacyofactivelearningforthetaskofinterlinearglossing,usingannotationtimerequiredbyexpertandnon-expertannotatorsastheirmetric.Thesys-temtheyusetogenerateglosslabelsuggestionsisastandardmaximumentropyclassifier.Arule-basedapproachbySnoeketal.(2014)utilizesanFSTtogenerateglossesforPlainsCree,focusingonnouns.Samard\u017ei\u00b4cetal.(2015)viewthetaskofglossingsegmentedtextasatwo-stepprocess,firsttreatingitasastandardPOStaggingtaskandthenaddinglexicalglossesfromadictio-nary.TheydemonstratethismethodonaChintangcorpusofabout1.2millionwords.Anumberofotherworksfocusingoninterlinearglossingutilizeconditionalrandomfield(CRF)models.MoellerandHulden(2018)testthreedifferentmodelsonaverysmallLezgidataset(<3000words):aCRF(thatoutputsBIOlabelswiththecorrespondingglosspercharacterinthein-put),asegmentationandlabellingpipelinethatuti-lizesaCRF(forBIOlabels)andSVM(forglossla-bels),andanLSTMseq2seqmodel.TheyfindthattheCRFthatjointlyproducestheBIOlabelsandtagsproducedthebestresults.McMillan-Major(2020)utilizestranslationsintheirtrainingdatabycreatingtwoCRFmodels,onethatpredictsglossfromthesegmentedinputandanotherthanpre-\nModelarpddogitlezntunybuspAVG\nModelarpddogitlezntunybuspAVG\nbyt5-base78.8680.3214.8460.72b76.6776.7377.2166.48byt5-aug73.2762.374.1738.6055.1169.2570.8553.38byt5-odin80.5682.7920.5763.7777.9782.5975.7269.14\nbaseline91.1185.3425.3351.8249.0388.7182.4867.69\nbaseline85.4475.7116.4134.5441.0884.3076.5559.14\nxlmr-base85.8773.7727.86/34.11a74.1582.9980.6173.4772.14xlmr-aug82.9280.0724.74/31.2577.7778.7285.5377.5173.39\naWereportbefore/afterdictionarybasedpost-correctionforGitksan.bWetrainedthismodelwithoutromanizingLezgi.Table1:Word-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselineisthehighestArapahoaccuracyreportedoutofallsharedtasksubmissions.\nbyt5-base78.8275.7712.5944.1062.4078.9774.2560.99byt5-aug72.1057.932.6026.2435.6270.0167.7347.46byt5-odin80.8178.2412.7450.0063.3985.3073.2563.39\nMorph0.0430.0090.4500.0560.0340.0190.070\narpddogitlezntunybusp\nWord0.2420.1550.7810.1690.2140.0840.200\n213\n\n\nbyt5-aug74.7658.243.4240.2736.5471.2770.56\n35.48\nTable4:Morpheme-levelaccuracyoveralltokensofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.\n24.67\n12.31\nxlmr-base95.2085.1282.8984.7990.8787.4686.05\n23.24\n45.07\nxlmr-aug93.7283.8594.0587.6489.2490.8191.11\n29.33\n24.10\n49.79\nbyt5-odin91.9387.6663.1073.7885.9387.6083.46\n59.51\n23.54\nbyt5-odin83.4778.5518.4262.9064.3886.8575.23\n18.26\n22.41\n49.17\nTable5:Word-levelaccuracyofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.dictsfromthetranslation,andthenusesheuristicstodeterminewhichmodeltoselectfromforeachmorpheme.BarrigaMart\u00ednezetal.(2021)usedaCRFmodeltoachieve>90%accuracyforgloss-ingOtomiandfindthatitworksbetterthananRNN,whichiscomputationallymoreexpensive.Otherworks,includingoursystems,haveturnedtoneuralmethods.Kondratyuk(2019)leveragespretrainedmultilingualBERTtoencodeinputsentences,thenapplyadditionalword-levelandcharacter-levelLSTMlayersbeforejointlydecod-inglemmasandmorphologytagsusingsimplese-quencetagginglayers.Furthermore,theyshowthattwo-stagetrainingbyfirsttrainingonalllan-guagesfollowedbytrainingonthetargetlanguageismoreeffectivethantrainingthesystemonthetargetlanguagealone.AnapproachbyZhaoetal.(2020),likeMcMillan-Major(2020),makesuseoftranslationsavailableinparallelcorpora,butdosobyusingamulti-sourcetransformermodel.Theyalsoincorporatelengthcontrolandalignmentdur-inginferencetoenhancetheirmodel,andtesttheirsystemonArapaho,Tsez,andLezgi.7ConclusionInoursharedtasksubmission,weexploredataaug-mentationmethodsandmodelingstrategiesforthetaskofinterlinearglossinginsevenlow-resourcelanguages.Ourbestperformingmodelsareto-kenclassificationmodelsusingXLMR.Wedemon-stratethatpretrainingonartificialdatawithXLMRisaneffectivetechniqueforthemid-resourcetestlanguages.Additionally,inourerroranalysiswefindthatwemayhaveactuallyundertrainedourtokenclassificationmodels,andthusoursystemsmayhavethepotentialtoperformbetterwithad-ditionalhyperparametertuning.WhileourByT5modelsdidnotperformaswellasourothersys-tems,weshowthatpretrainingonODINdataiseffective,despitethisdatabeingverynoisy.Finally,wealsodemonstrateimprovementsbyutilizingadictionarytopost-correctmodeloutputsforGitk-san.\n0.00\n0.00\n0.00\n0.00\n11.24\n0.82\n12.86\n14.52\n23.60\n4.97\n56.36\n45.65\n21.14\n43.37\n1.61\n47.52\n46.94\n29.69\n13.67\nxlmr-base95.9378.1895.2384.2493.1485.8586.27\n5.79\n28.04\n2.00\n17.00\n0.41\n16.08\n14.67\n19.35\nModelarpddogitlezntunybusp\nModelarpddogitlezntunybusp\n7.49\n28.09\n28.09\n44.81\n3.23\n3.23\n54.44\n2.33\nbyt5-aug87.2268.6910.7146.0665.1374.5981.44\n30.20\nxlmr-aug92.9888.9484.7487.1087.8891.1789.31\n48.70\n40.00\n28.63\n8.67\n2.60\n2.60\n9.68\n214\n\n\nAcknowledgementsThisworkwassupportedbyNSFCISERIgrantnumber2211951,FromAcousticSignaltoMor-phosyntacticAnalysisinoneEnd-to-EndNeuralSystem.ReferencesJasonBaldridgeandAlexisPalmer.2009.Howwelldoesactivelearningactuallywork?Time-basedeval-uationofcost-reductionstrategiesforlanguagedocu-mentation.InProceedingsofthe2009ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages296\u2013305,Singapore.AssociationforCompu-tationalLinguistics.DiegoBarrigaMart\u00ednez,VictorMijangos,andXi-menaGutierrez-Vasques.2021.Automaticinterlin-earglossingforOtomilanguage.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages34\u201343,Online.AssociationforComputationalLin-guistics.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.ClarissaForbes,GarrettNicolai,andMiikkaSilfverberg.2021.AnFSTmorphologicalanalyzerforthegitksanlanguage.InProceedingsofthe18thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages188\u2013197,Online.AssociationforComputationalLinguistics.DanKondratyuk.2019.Cross-linguallemmatizationandmorphologytaggingwithtwo-stagemultilin-gualBERTfine-tuning.InProceedingsofthe16thWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages12\u201318,Florence,Italy.AssociationforComputationalLinguistics.WilliamD.LewisandFeiXia.2010.DevelopingODIN:AMultilingualRepositoryofAnnotatedLanguageDataforHundredsoftheWorld\u2019sLanguages.Liter-aryandLinguisticComputing,25(3):303\u2013319.AngelinaMcMillan-Major.2020.Automatingglossgenerationininterlinearglossedtext.ProceedingsoftheSocietyforComputationinLinguistics,3(1):338\u2013349.SarahMoellerandMansHulden.2018.Automaticglossinginalow-resourcesettingforlanguagedoc-umentation.InProceedingsoftheWorkshoponComputationalModelingofPolysyntheticLanguages,pages84\u201393.DavidR.Mortensen,SiddharthDalmia,andPatrickLittell.2018.Epitran:PrecisionG2Pformanylan-guages.InProceedingsoftheEleventhInternationalConferenceonLanguageResourcesandEvaluation(LREC2018),Miyazaki,Japan.EuropeanLanguageResourcesAssociation(ELRA).SebastianRuder,MatthewEPeters,SwabhaSwayamdipta,andThomasWolf.2019.Transferlearninginnaturallanguageprocessing.InProceed-ingsofthe2019conferenceoftheNorthAmericanchapteroftheassociationforcomputationallinguistics:Tutorials,pages15\u201318.TanjaSamard\u017ei\u00b4c,RobertSchikowski,andSabineStoll.2015.Automaticinterlinearglossingastwo-levelsequenceclassification.NoamShazeerandMitchellStern.2018.Adafactor:Adaptivelearningrateswithsublinearmemorycost.InProceedingsofthe35thInternationalConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,pages4596\u20134604.PMLR.ConorSnoek,DorothyThunder,KaidiL\u00f5o,AnttiArppe,JordanLachler,SjurMoshagen,andTrondTrosterud.2014.ModelingthenounmorphologyofPlainsCree.InProceedingsofthe2014WorkshopontheUseofComputationalMethodsintheStudyofEndangeredLanguages,pages34\u201342,Baltimore,Maryland,USA.AssociationforComputationalLinguistics.FeiXia,WilliamLewis,MichaelWayneGoodman,JoshuaCrowgey,andEmilyM.Bender.2014.En-richingODIN.InProceedingsoftheNinthInter-nationalConferenceonLanguageResourcesandEvaluation(LREC\u201914),pages3151\u20133157,Reykjavik,Iceland.EuropeanLanguageResourcesAssociation(ELRA).LintingXue,AdityaBarua,NoahConstant,RamiAl-Rfou,SharanNarang,MihirKale,AdamRoberts,andColinRaffel.2022.ByT5:Towardsatoken-freefuturewithpre-trainedbyte-to-bytemodels.Transac-tionsoftheAssociationforComputationalLinguis-tics,10:291\u2013306.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.XingyuanZhao,SatoruOzaki,AntoniosAnastasopou-los,GrahamNeubig,andLoriLevin.2020.Auto-maticinterlinearglossingforunder-resourcedlan-guagesleveragingtranslations.InProceedingsofthe28thInternationalConferenceonComputationalLinguistics,pages5397\u20135408,Barcelona,Spain(On-line).InternationalCommitteeonComputationalLin-guistics.\n215\n\n\nAHyperparameterSettingsWeuseAdafactor(ShazeerandStern,2018)astheoptimizeracrossallexperiments,withthede-faultschedulerfromHuggingFaceTransformers,abatchsizeof32forRoBERTabasedmodelsandabatchsizeof4withagradientaccumulationstepof8forByT5basedmodels.Wetrainthetokenclassificationmodelsfor40epochsexceptforAra-paho,onwhichwetrain20epochs,andGitksan,onwhichwetrain2,000steps.WetraintheByT5basedmodelsfor20epochsonallofthedatamixedtogether.\n216"}, {"question": " What post-correction method was applied to Gitksan in the submission?", "answer": " A post-correction method using a dictionary was applied to Gitksan.,", "ref_chunk": "SigMoreFunSubmissiontotheSIGMORPHONSharedTaskonInterlinearGlossingTaiqiHe\u2217,LindiaTjuatja\u2217,NateRobinson,ShinjiWatanabe,DavidR.Mortensen,GrahamNeubig,LoriLevinLanguageTechnologiesInstituteCarnegieMellonUniversity{taiqih,ltjuatja,nrrobins,swatanab,dmortens,gneubig,lsl}@cs.cmu.eduAbstractInoursubmissiontotheSIGMORPHON2023SharedTaskoninterlinearglossing(IGT),weexploreapproachestodataaugmentationandmodelingacrosssevenlow-resourcelanguages.Fordataaugmentation,weexploretwoap-proaches:creatingartificialdatafromthepro-videdtrainingdataandutilizingexistingIGTresourcesinotherlanguages.Onthemodelingside,wetestanenhancedversionofthepro-videdtokenclassificationbaselineaswellasapretrainedmultilingualseq2seqmodel.Ad-ditionally,weapplypost-correctionusingadictionaryforGitksan,thelanguagewiththesmallestamountofdata.Wefindthatourtokenclassificationmodelsarethebestperforming,withthehighestword-levelaccuracyforAra-pahoandhighestmorpheme-levelaccuracyforGitksanoutofallsubmissions.Wealsoshowthatdataaugmentationisaneffectivestrategy,thoughapplyingartificialdatapretraininghasverydifferenteffectsacrossbothmodelstested.1IntroductionThispaperdescribestheSigMoreFunsubmissiontotheSIGMORPHON2023SharedTaskoninterlin-earglossing.Giveninputtextinatargetlanguage,thetaskistopredictthecorrespondinginterlineargloss(usingLeipzigglossingconventions).IGTisanimportantformoflinguisticannotationforthemorphologicalanalysisoflanguages,andalsoservesasanextremelyvaluableresourceforlan-guagedocumentationandeducationforspeakersoflow-resourcelanguages.Thereweretwotracksforthissharedtask,Track1(closed)andTrack2(open).ForTrack1,sys-temscouldonlybetrainedoninputsentencesandglosses;inTrack2,systemscouldmakeuseofthemorphologicalsegmentationoftheinputaswellasany(non-IGT)externalresources.SincetheTrack2settingbettermatchesthelong-termre-\n\u2217Theseauthorscontributedequallysearchgoalsofourteam,weonlyparticipateinthisopentrack.Inoursubmission,weinvestigatetwodifferentapproaches.First,weattemptdataaugmentationbyeithercreatingourownartificialglossdatabymanipulatingtheexistingtrainingdata,orbyuti-lizingexistingresourcescontainingIGTinotherlanguages(\u00a72).Second,weexploretwodifferentmodelsforglossgeneration(\u00a73).Thefirstbuildsoffthetokenclassificationbaseline,whilethesec-ondusesapretrainedmultilingualseq2seqmodel.Finally,wealsoattempttopost-correctmodeloutputswithadictionary.WeapplythistoGitk-sanandfindthatthis,combinedwithourotherapproaches,resultsinthehighestmorpheme-levelaccuracyforGitksaninTrack2.2DataAugmentationOnemajorchallengeforthissharedtaskisthescaleofdataprovided.Allofthelanguageshavelessthan40klinesoftrainingdata,andallbutArapahohavelessthan10k.Thesmallestdataset(Gitk-san)hasonly31linesofdata.Thus,oneobviousmethodtotryisdataaugmentation.Morespecif-ically,wetrypretrainingourmodelsondifferentformsofaugmenteddatabeforetrainingthemontheoriginaltargetlanguagedata.Weexploredtwoformsofdataaugmentation.First,wegeneratedartificialglossdatainthetar-getlanguagebyswappingwordsintheexistingtrainingdata.Second,weutilizeddatafromODIN(LewisandXia,2010;Xiaetal.,2014)toseeiftransferlearningfromdatainotherlanguagescanhelpimproveperformance.2.1ArtificialDataAchallengeourteamfacedwithrespecttodataaugmentationisfiguringouthowtoobtainaddi-tionaldatawhenwedonothavemuchknowledgeofthelanguages\u2019grammaticalsystems,alongwiththefactthattheselanguagesaregenerallyfrom\n209 Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 209\u2013216 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\ndigitallyunder-resourcedlanguagefamilies.Fur-thermore,wewantedoursolutiontobeeasilyim-plementedandrelativelylanguageagnosticduetotimeconstraintsandpracticalusabilityforre-searchersworkingonavarietyoflanguages.Thus,oneavenueofdataaugmentationwetriedwasbycreatingartificialdatafromtheprovidedtrainingdata.Thisrequiresnorule-writingorknowledgeofthegrammarofthelanguage,andthuscouldbeappliedquicklyandeasilytoallofthelanguagesinthesharedtask.Weusedanaiveword-swappingmethodtoran-domlyswapmorphemesthatoccurinsimilarcon-textstocreatenewsentences.Todothis,foreachglossline,wereplaceeachwordstem(thathasaglosslabelaffix)with\u201cSTEM\u201dtocreateaskeletongloss.Wenaivelydetermineifalabelisastembycheckingifitisinlowercase.Wedonotdothistowordsthatdonothaveaffixesas(withtheexceptionofUspanteko)wedonothaveaccesstopartsofspeech,anddonotwanttoswapwordsthatwouldcreateanungrammaticalsequence.Wecreateadictionarymappingeachskeletonwordglosstopossibleactualglosses,andmapeachactualglosstopossiblesurfaceforms(wemakenoassumptionsthatthesemappingsareone-to-one).Wethenrandomlysamplekrandomskeletonglosses(inthiscase,weusedkequaltoroughlythreetimestheamountoftrainingdata)andran-domlyfillinwordsthatmatchtheformatofskele-tonwordspresentintheline.(1)to(3)belowillustrateanexampleinthisprocess.Wecreateaskeletongloss(2)fromtheGitksansentencein(1)byreplacingtheallwordstemsthathaveanaffixwith\u201cSTEM\u201dinboththesegmentationandglosstiers\u2014inthiscase,only\u2019witxw-itappliestothisstep.Thentocreatetheartificialdatain(3),wereplacetheskeletonwordandcorrespondingglosswithanotherwordfromthetrainingdatathathasthesameskeletonform,inthiscasehahla\u2019lst-it.(1)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSP\u2019witxw-itcome-SX(2)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPSTEM-itSTEM-SX(3)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPhahla\u2019lst-itwork-SXWhilethismethodmaycreateasomewhatun-naturalinputsurfacesequence(asweareunabletocapturephonologicalchangesinthesurfaceformandcorrespondingtranslationsmaybenonsensi-cal),thismethodguaranteesthatthestructureoftheglossisanaturallyoccurringsequence(asweonlyuseglossskeletonsthatarepresentinthein-put).However,alimitationofthismethodisthatitdoesnotextendtoout-of-vocabularytokensorunseenglossstructures.Furthermore,aswecannotgenerateagold-standardtranslationfortheartifi-cialdata,wedonotmakeuseofatranslationintraining.2.2ODINAnotherpotentialavenuefordataaugmentationistransferlearningfromdatainotherlanguages,whichhasbeenshowntobeaneffectivemethodtoimproveperformanceinlow-resourcesettings(Ruderetal.,2019).TheavailableresourceweutilizeisODIN,ortheOnlineDatabaseforInterlinearText(LewisandXia,2010;Xiaetal.,2014).ODINcontains158,007linesofIGT,covering1,496languages.Weusethe2.1versionofODINdataandconvertthedatasettothesharedtaskformat,andfilteroutlanguageswithfewerthanfiveglossedsentences.However,thereremainssignificantnoiseinthedatasetthatcouldcausesignificantalignmentis-suesforthetokenclassificationmodels.ThereforeweopttoonlytraintheByT5modelsonODIN,inthehopethatthismodelislesssensitivetoalign-menterrors.Indeed,wefindthattheByT5modelfinetunedfirstonODINreceivesaperformanceboostwhenfinetunedagainonthesharedtaskdata.3ModelsWeexploretwomodelsforglossgeneration.Thefirstoneisbuiltuponthetokenclassificationbase-linewithsomeimprovements,andwetreatthismodelasourinternalbaseline.Thesecondmodelwedeploytestswhetherwecanachievecompeti-tiveperformancebyfinetuningapretrainedcharac-terbasedmultilingualandmultitaskmodel,ByT5.Forthismodel,weperformminimalpreprocess-inganduserawsegmentedmorphemesandfreetranslationsifavailable.3.1TokenClassificationTransformerWeusethebaselineTrack2modelprovidedbytheorganizersasastartingpoint.Theoriginalimple-mentationrandomlyinitializesatransformermodelfromthedefaultHuggingfaceRoBERTabasecon-figuration,andusesatokenclassificationobjective\n210\n\n\n1https://en.wikipedia.org/wiki/Lezgin_alphabets\nwithcross-entropyloss,whereeachglossistreatedasadistincttoken.Themorphemesandfreetrans-lationsaretokenizedbyspaceanddashes,withpunctuationspre-separated.TheyareconcatenatedandseparatedbytheSEPtokenandareusedastheinputstothemodel.WemodifytheoriginalTrack2baselinemodeltoobtainabetterbaseline.WeusepretrainedweightsfromXLM-RoBERTabase(Conneauetal.,2020),insteadofrandomlyinitializingtheweights.Wealsoslightlymodifythemorphemetokenizertoenforcethatthenumberofmorphemetokensmatchesthenumberofoutputglosstokensexactly.Additionally,weintroducetheCOPYtokentoreplacetheglossifitmatchesthecorrespondingmorphemeexactly.AnexamplefromNatuguisshowningloss(4):(4)67COPY.COPYmnc-xbe-1MINIMzloCOPYSkulCOPYWebelievethiswouldimproveperformancebyremovingtheneedtomemorizeglossedcode-switchingandpropernouns,thoughitisonlyef-fectiveifthecode-switchedlanguageisthesameasthematrixlanguage(e.g.Arapaho),andwouldhavenoeffectifthesourcelanguageusesadif-ferentorthographyoriscode-switchedtoanotherlanguage,wheretheglosswouldnotmatchedthemorphemeformexactly.Thismethodalsocom-pressesallpunctuationmarkersintoonetoken,buttheusefulnessofthissideeffectislessclear.Sinceweareusingpretrainedweights,itisthennaturaltoexploreintegratingthepretrainedtok-enizer.SinceXLM-RoBERTawasnottrainedonanyofthesourcelanguages,itmakesthemostsensetoonlyusethepretrainedtokenizertotok-enizefreetranslations,iftheyareavailable,andextendthevocabularytoincludemorphemes.3.2FinetunedByT5Multi-taskandmulti-lingualpretrainedlargelan-guagemodelshavebeenshowntobeeffectiveformanytasks.Weexplorewhethersuchmodelscanbeusedeffectivelyforglossing.Weconductex-perimentswithbothmT5(Xueetal.,2021)andByT5(Xueetal.,2022),butByT5ispreferredbecauseittakesrawtexts(bytesorcharacters)asinputsandintheoryshouldbemoreeffectiveforunseenlanguages.Weuseapromptbasedmultilin-gualsequencetosequenceobjectiveforbothmod-els.Theprompttemplateis:\u201cGenerateinterlin-earglossfrom[sourcelanguage]:[segmentedmorphemes]withits[matrixlanguage]trans-lation:[freetranslation]Answer:\u201d.Datafromalllanguagesaremixedtogetherandshuffled,withnoupordownsampling.Afterinitialexperi-ments,wefindByT5outperformsmT5acrossalllanguages,andthereforeweonlyconductsubse-quentexperimentsonByT5andreportthosere-sults.Uponinitialexperiments,wealsofindtheresultsforLezgitobelowerthanexpected.Wehypothe-sizethatthefactthatthedataareinCyrillicscriptcausesthisdeficiency,sinceByT5wastrainedonfarlessCyrillicdatathandataintheLatinscript.Thereforewecreateanautomaticromanizationtool,sourcedfromWikipedia1andintegratedintheEpitranpackage(Mortensenetal.,2018),andconvertallLezgidatatoLatinscriptforByT5fine-tuning.AfterinspectingtheoutputsoftheByT5models,wefindcaseswherepunctuationsareattachedtothepreviousglosses,insteadofbeingseparatedbyaspaceasisstandardinthetrainingsets.Thisisprobablyduetothefactthatthemodelwaspre-trainedonuntokenizeddataandthisbehaviorispreserveddespitefinetuningontokenizeddata.Wethereforeuseasimpleregularexpressionbasedtok-enizertofixtheinconsistencies.WenoticethattheprocedureonlygivesperformanceboostonGitk-san,Lezgi,Uspanteko,andNatugu,andsoweonlyapplytheproceduretothoselanguages,leavingtherestoftheoutputsunchanged.4DictionaryPost-correction:GitksanOneofthekeychallengesforextremelylowre-sourcelanguagesistheintegrationofstructuredlinguisticdatainotherforms,suchasadictionary,intomachinelearningpipelines.Wetestasimplepost-correctionmethodfromapre-existingdictio-naryonGitksanonly,duetoitsuniquecombinationoflowresourceandeasilyobtainabledictionaryinmachinereadableform.Weusethedictionarycom-piledbyForbesetal.(2021),withoutconsultingthemorphologicalanalyzersthattheyalsoprovided.Atinferencetime,ifamorphemeisunseendur-ingtraining,wesearchfortheexactforminthedictionary.Wealsoexpandthesearchtoallsubse-quencesofmorphemeswithintheenclosingword,plusthepreviouswholewordincaseswhereapar-ticleisincludedinthedictionaryform.Thefirst\n211\n\n\nmatcheddefinitionisusedastheglossandifnoneofthesearchyieldsanexactmatch,wefallbacktothemodelprediction.Weonlyapplythismethodtothetokenclassificationmodelsbecausethealign-mentbetweenmorphemesandglossesisdirectlyestablished,whereastheseq2seqmodelsdonotguaranteethatthenumberofglossesmatchesthenumberofmorphemes.5ResultsandDiscussionTables1and2showoursystems\u2019performance(aswellastheoriginalbaseline)onthetestdatawithre-specttoword-andmorpheme-levelmicro-averagedaccuracy,respectively.Overall,thetokenclassifica-tionmodeltrainedfirstontheartificiallygeneratedaugmenteddataperformthebest,withthemodeltrainedonthesharedtaskdataonlynotfarbehind.Meanwhile,ByT5modelsperformworse,withthemodelfinetunedfirstonODINtrailingourbestmodelbyafewpercentagepoints,whilethemodelfinetunedfirstonaugmenteddataperformsworsethanthebaseline.5.1DataAugmentationOverall,wefinddataaugmentationtobeuseful.Withartificiallygenerateddata,weseetheeffectsareperhapsgreatestforthemid-resourcelanguages(ddo,lez,ntu,nyb,usp),whilethehighestandlowestresourcedlanguagesdidnotreceivemuchbenefitfrompretrainingontheartificialdata.Wethinkthisisperhapsbecausethereisa\u201csweetspot\u201dwithrespecttotheamountofdatathatisrequiredtotrainamodel.Ifthereisenoughdataalready,inthecaseofArapaho,thenthenoisinessofartificialdatawouldout-weightthebenefitoftrainingonthem.Ontheotherendofthescale,Gitksanperhapsneedsmoresyntheticdatafordataaugmentationtoyieldmeaningfulimprovements.ForByT5models,artificiallygenerateddataseemtohavetheoppositeeffect,whereperfor-manceissignificantlydegraded.Aspeculationforthiseffectisthefactthepretrainedmodelismoresemanticallyaware,andsincetheartificiallygen-eratedsentencescouldbenonsensical,themodelcouldbecomeconfused.Ontheotherhand,pre-trainingonODINyieldsimprovementsforthema-jorityofthelanguages2.ThisisencouragingsincewedidnotperformmuchpreprocessingforODIN,\n2TsezistheonlylanguagethatappearedinODIN(68sentences).Wedidnotremoveitfromthecorpusbutthisshouldhavelittleinfluenceontheperformancebecausethesizeofthedatasetisverysmall.andthereisdefinitelystillroomtomakethedatacleanerandmoreinternallyconsistent,whichinturnshouldresultinabettermodel.5.2ChoiceofHyperparametersWefindthechoiceofhyperparametersofthetokenclassificationmodelstobenecessarilylanguageanddatasetspecific.ArapahoandGitksaninpar-ticularneedspecialattention,wherethenumberoftrainingepochsneedtobeadjustedfortheveryhighandlowdatasize.WealsodevelopedmostoftheoptimizationonthetokenclassificationmodelonArapaho.However,wedidnothavetimetopropagatethechanges(usingpretrainedtokenizer,savingthelastmodelinsteadofthemodelwiththelowestvalidationloss)totherestoflanguagessinceinitialexperimentshowedthatpretrainedtokeniz-ersdidnotimproveontheotherlanguages.How-ever,afterthesubmissiondeadlineisconcluded,weranmoreexperimentsanddiscoveredthataddingpretrainedtokenizersrequiresmoretrainingsteps,andthetrainingisbettercontrolledbyspecifyingthetrainingstepsinsteadofepochs.Wedonotin-cludethoselatestexperimentsinthispaper,butourtokenclassificationmodelshavethepotentialtoperformbetterwithmorehyperparametertuning.5.3In-VersusOut-of-VocabularyErrorsOnedimensionoferroranalysisweinvestigatedwaswhatproportionofoursystems\u2019errorscomefrommorphemesorwordsthatareeitherinoroutofthetrainingdatavocabulary.Wecountamor-phemeorwordasin-vocabularyifthesurfaceformanditscorrespondingglossco-occurinthepro-videdtrainingdata(notincludingthedevelopmentdata,asourmodelsareonlytrainedonthetrainset).NotethatthereisamuchlargerproportionofOOVwordsasopposedtomorphemesduetothefactthatanunseenwordcanbecomposedofdifferentcombinationsofseenmorphemes.Table3showstheproportionofmorphemesandwordsthatareout-of-vocab(OOV)withinthetestset.Whilenearlyallthelanguageshavelessthan10%oftheirmorphemesclassifiedasOOV,GitksannotablyhasarelativelylargeportionofOOVtestdata,with\u224845%ofmorphemesand\u224878%ofwordsbeingOOV.Tables4and5showourmodels\u2019performancesonin-versesout-of-vocabtokensatthemorphemeandwordlevels,respectively.Whilewewouldintuitivelyexpectthatword-levelOOVaccuracybeaboutthesameorworsethanmorpheme-levelOOV\n212\n\n\nTable2:Morpheme-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselinewithartificialpretraininganddictionarypost-correctionisthehighestGitksanaccuracyreportedoutofallsharedtasksubmissions.\nxlmr-base91.3684.3547.47/52.8280.1788.3585.8480.0880.42xlmr-aug89.3488.1546.89/52.3982.3685.5389.4983.0881.48\nTable3:ProportionofmorphemesandwordsthatareOOVwithinthetestset.accuracy,thisisnotthecaseduetothefactthatalargeportionofout-of-vocabwordsareformedwithin-vocabmorphemes.Formostlanguages,withtheexceptionofGitksan,thereappearstobeatrade-offbetweenbetterin-vocabmorphemeperformancewithXLMRandperformanceout-of-vocabwithByT5.6RelatedWorkTherehavebeenavarietyofapproachestotheprob-lemof(semi-)automaticallygeneratinginterlineargloss.BaldridgeandPalmer(2009)investigatetheefficacyofactivelearningforthetaskofinterlinearglossing,usingannotationtimerequiredbyexpertandnon-expertannotatorsastheirmetric.Thesys-temtheyusetogenerateglosslabelsuggestionsisastandardmaximumentropyclassifier.Arule-basedapproachbySnoeketal.(2014)utilizesanFSTtogenerateglossesforPlainsCree,focusingonnouns.Samard\u017ei\u00b4cetal.(2015)viewthetaskofglossingsegmentedtextasatwo-stepprocess,firsttreatingitasastandardPOStaggingtaskandthenaddinglexicalglossesfromadictio-nary.TheydemonstratethismethodonaChintangcorpusofabout1.2millionwords.Anumberofotherworksfocusingoninterlinearglossingutilizeconditionalrandomfield(CRF)models.MoellerandHulden(2018)testthreedifferentmodelsonaverysmallLezgidataset(<3000words):aCRF(thatoutputsBIOlabelswiththecorrespondingglosspercharacterinthein-put),asegmentationandlabellingpipelinethatuti-lizesaCRF(forBIOlabels)andSVM(forglossla-bels),andanLSTMseq2seqmodel.TheyfindthattheCRFthatjointlyproducestheBIOlabelsandtagsproducedthebestresults.McMillan-Major(2020)utilizestranslationsintheirtrainingdatabycreatingtwoCRFmodels,onethatpredictsglossfromthesegmentedinputandanotherthanpre-\nModelarpddogitlezntunybuspAVG\nModelarpddogitlezntunybuspAVG\nbyt5-base78.8680.3214.8460.72b76.6776.7377.2166.48byt5-aug73.2762.374.1738.6055.1169.2570.8553.38byt5-odin80.5682.7920.5763.7777.9782.5975.7269.14\nbaseline91.1185.3425.3351.8249.0388.7182.4867.69\nbaseline85.4475.7116.4134.5441.0884.3076.5559.14\nxlmr-base85.8773.7727.86/34.11a74.1582.9980.6173.4772.14xlmr-aug82.9280.0724.74/31.2577.7778.7285.5377.5173.39\naWereportbefore/afterdictionarybasedpost-correctionforGitksan.bWetrainedthismodelwithoutromanizingLezgi.Table1:Word-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselineisthehighestArapahoaccuracyreportedoutofallsharedtasksubmissions.\nbyt5-base78.8275.7712.5944.1062.4078.9774.2560.99byt5-aug72.1057.932.6026.2435.6270.0167.7347.46byt5-odin80.8178.2412.7450.0063.3985.3073.2563.39\nMorph0.0430.0090.4500.0560.0340.0190.070\narpddogitlezntunybusp\nWord0.2420.1550.7810.1690.2140.0840.200\n213\n\n\nbyt5-aug74.7658.243.4240.2736.5471.2770.56\n35.48\nTable4:Morpheme-levelaccuracyoveralltokensofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.\n24.67\n12.31\nxlmr-base95.2085.1282.8984.7990.8787.4686.05\n23.24\n45.07\nxlmr-aug93.7283.8594.0587.6489.2490.8191.11\n29.33\n24.10\n49.79\nbyt5-odin91.9387.6663.1073.7885.9387.6083.46\n59.51\n23.54\nbyt5-odin83.4778.5518.4262.9064.3886.8575.23\n18.26\n22.41\n49.17\nTable5:Word-levelaccuracyofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.dictsfromthetranslation,andthenusesheuristicstodeterminewhichmodeltoselectfromforeachmorpheme.BarrigaMart\u00ednezetal.(2021)usedaCRFmodeltoachieve>90%accuracyforgloss-ingOtomiandfindthatitworksbetterthananRNN,whichiscomputationallymoreexpensive.Otherworks,includingoursystems,haveturnedtoneuralmethods.Kondratyuk(2019)leveragespretrainedmultilingualBERTtoencodeinputsentences,thenapplyadditionalword-levelandcharacter-levelLSTMlayersbeforejointlydecod-inglemmasandmorphologytagsusingsimplese-quencetagginglayers.Furthermore,theyshowthattwo-stagetrainingbyfirsttrainingonalllan-guagesfollowedbytrainingonthetargetlanguageismoreeffectivethantrainingthesystemonthetargetlanguagealone.AnapproachbyZhaoetal.(2020),likeMcMillan-Major(2020),makesuseoftranslationsavailableinparallelcorpora,butdosobyusingamulti-sourcetransformermodel.Theyalsoincorporatelengthcontrolandalignmentdur-inginferencetoenhancetheirmodel,andtesttheirsystemonArapaho,Tsez,andLezgi.7ConclusionInoursharedtasksubmission,weexploredataaug-mentationmethodsandmodelingstrategiesforthetaskofinterlinearglossinginsevenlow-resourcelanguages.Ourbestperformingmodelsareto-kenclassificationmodelsusingXLMR.Wedemon-stratethatpretrainingonartificialdatawithXLMRisaneffectivetechniqueforthemid-resourcetestlanguages.Additionally,inourerroranalysiswefindthatwemayhaveactuallyundertrainedourtokenclassificationmodels,andthusoursystemsmayhavethepotentialtoperformbetterwithad-ditionalhyperparametertuning.WhileourByT5modelsdidnotperformaswellasourothersys-tems,weshowthatpretrainingonODINdataiseffective,despitethisdatabeingverynoisy.Finally,wealsodemonstrateimprovementsbyutilizingadictionarytopost-correctmodeloutputsforGitk-san.\n0.00\n0.00\n0.00\n0.00\n11.24\n0.82\n12.86\n14.52\n23.60\n4.97\n56.36\n45.65\n21.14\n43.37\n1.61\n47.52\n46.94\n29.69\n13.67\nxlmr-base95.9378.1895.2384.2493.1485.8586.27\n5.79\n28.04\n2.00\n17.00\n0.41\n16.08\n14.67\n19.35\nModelarpddogitlezntunybusp\nModelarpddogitlezntunybusp\n7.49\n28.09\n28.09\n44.81\n3.23\n3.23\n54.44\n2.33\nbyt5-aug87.2268.6910.7146.0665.1374.5981.44\n30.20\nxlmr-aug92.9888.9484.7487.1087.8891.1789.31\n48.70\n40.00\n28.63\n8.67\n2.60\n2.60\n9.68\n214\n\n\nAcknowledgementsThisworkwassupportedbyNSFCISERIgrantnumber2211951,FromAcousticSignaltoMor-phosyntacticAnalysisinoneEnd-to-EndNeuralSystem.ReferencesJasonBaldridgeandAlexisPalmer.2009.Howwelldoesactivelearningactuallywork?Time-basedeval-uationofcost-reductionstrategiesforlanguagedocu-mentation.InProceedingsofthe2009ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages296\u2013305,Singapore.AssociationforCompu-tationalLinguistics.DiegoBarrigaMart\u00ednez,VictorMijangos,andXi-menaGutierrez-Vasques.2021.Automaticinterlin-earglossingforOtomilanguage.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages34\u201343,Online.AssociationforComputationalLin-guistics.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.ClarissaForbes,GarrettNicolai,andMiikkaSilfverberg.2021.AnFSTmorphologicalanalyzerforthegitksanlanguage.InProceedingsofthe18thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages188\u2013197,Online.AssociationforComputationalLinguistics.DanKondratyuk.2019.Cross-linguallemmatizationandmorphologytaggingwithtwo-stagemultilin-gualBERTfine-tuning.InProceedingsofthe16thWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages12\u201318,Florence,Italy.AssociationforComputationalLinguistics.WilliamD.LewisandFeiXia.2010.DevelopingODIN:AMultilingualRepositoryofAnnotatedLanguageDataforHundredsoftheWorld\u2019sLanguages.Liter-aryandLinguisticComputing,25(3):303\u2013319.AngelinaMcMillan-Major.2020.Automatingglossgenerationininterlinearglossedtext.ProceedingsoftheSocietyforComputationinLinguistics,3(1):338\u2013349.SarahMoellerandMansHulden.2018.Automaticglossinginalow-resourcesettingforlanguagedoc-umentation.InProceedingsoftheWorkshoponComputationalModelingofPolysyntheticLanguages,pages84\u201393.DavidR.Mortensen,SiddharthDalmia,andPatrickLittell.2018.Epitran:PrecisionG2Pformanylan-guages.InProceedingsoftheEleventhInternationalConferenceonLanguageResourcesandEvaluation(LREC2018),Miyazaki,Japan.EuropeanLanguageResourcesAssociation(ELRA).SebastianRuder,MatthewEPeters,SwabhaSwayamdipta,andThomasWolf.2019.Transferlearninginnaturallanguageprocessing.InProceed-ingsofthe2019conferenceoftheNorthAmericanchapteroftheassociationforcomputationallinguistics:Tutorials,pages15\u201318.TanjaSamard\u017ei\u00b4c,RobertSchikowski,andSabineStoll.2015.Automaticinterlinearglossingastwo-levelsequenceclassification.NoamShazeerandMitchellStern.2018.Adafactor:Adaptivelearningrateswithsublinearmemorycost.InProceedingsofthe35thInternationalConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,pages4596\u20134604.PMLR.ConorSnoek,DorothyThunder,KaidiL\u00f5o,AnttiArppe,JordanLachler,SjurMoshagen,andTrondTrosterud.2014.ModelingthenounmorphologyofPlainsCree.InProceedingsofthe2014WorkshopontheUseofComputationalMethodsintheStudyofEndangeredLanguages,pages34\u201342,Baltimore,Maryland,USA.AssociationforComputationalLinguistics.FeiXia,WilliamLewis,MichaelWayneGoodman,JoshuaCrowgey,andEmilyM.Bender.2014.En-richingODIN.InProceedingsoftheNinthInter-nationalConferenceonLanguageResourcesandEvaluation(LREC\u201914),pages3151\u20133157,Reykjavik,Iceland.EuropeanLanguageResourcesAssociation(ELRA).LintingXue,AdityaBarua,NoahConstant,RamiAl-Rfou,SharanNarang,MihirKale,AdamRoberts,andColinRaffel.2022.ByT5:Towardsatoken-freefuturewithpre-trainedbyte-to-bytemodels.Transac-tionsoftheAssociationforComputationalLinguis-tics,10:291\u2013306.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.XingyuanZhao,SatoruOzaki,AntoniosAnastasopou-los,GrahamNeubig,andLoriLevin.2020.Auto-maticinterlinearglossingforunder-resourcedlan-guagesleveragingtranslations.InProceedingsofthe28thInternationalConferenceonComputationalLinguistics,pages5397\u20135408,Barcelona,Spain(On-line).InternationalCommitteeonComputationalLin-guistics.\n215\n\n\nAHyperparameterSettingsWeuseAdafactor(ShazeerandStern,2018)astheoptimizeracrossallexperiments,withthede-faultschedulerfromHuggingFaceTransformers,abatchsizeof32forRoBERTabasedmodelsandabatchsizeof4withagradientaccumulationstepof8forByT5basedmodels.Wetrainthetokenclassificationmodelsfor40epochsexceptforAra-paho,onwhichwetrain20epochs,andGitksan,onwhichwetrain2,000steps.WetraintheByT5basedmodelsfor20epochsonallofthedatamixedtogether.\n216"}, {"question": " What is the significance of the Leipzig glossing conventions mentioned in the text?", "answer": " IGT is an important form of linguistic annotation for morphological analysis of languages using the Leipzig glossing conventions.,", "ref_chunk": "SigMoreFunSubmissiontotheSIGMORPHONSharedTaskonInterlinearGlossingTaiqiHe\u2217,LindiaTjuatja\u2217,NateRobinson,ShinjiWatanabe,DavidR.Mortensen,GrahamNeubig,LoriLevinLanguageTechnologiesInstituteCarnegieMellonUniversity{taiqih,ltjuatja,nrrobins,swatanab,dmortens,gneubig,lsl}@cs.cmu.eduAbstractInoursubmissiontotheSIGMORPHON2023SharedTaskoninterlinearglossing(IGT),weexploreapproachestodataaugmentationandmodelingacrosssevenlow-resourcelanguages.Fordataaugmentation,weexploretwoap-proaches:creatingartificialdatafromthepro-videdtrainingdataandutilizingexistingIGTresourcesinotherlanguages.Onthemodelingside,wetestanenhancedversionofthepro-videdtokenclassificationbaselineaswellasapretrainedmultilingualseq2seqmodel.Ad-ditionally,weapplypost-correctionusingadictionaryforGitksan,thelanguagewiththesmallestamountofdata.Wefindthatourtokenclassificationmodelsarethebestperforming,withthehighestword-levelaccuracyforAra-pahoandhighestmorpheme-levelaccuracyforGitksanoutofallsubmissions.Wealsoshowthatdataaugmentationisaneffectivestrategy,thoughapplyingartificialdatapretraininghasverydifferenteffectsacrossbothmodelstested.1IntroductionThispaperdescribestheSigMoreFunsubmissiontotheSIGMORPHON2023SharedTaskoninterlin-earglossing.Giveninputtextinatargetlanguage,thetaskistopredictthecorrespondinginterlineargloss(usingLeipzigglossingconventions).IGTisanimportantformoflinguisticannotationforthemorphologicalanalysisoflanguages,andalsoservesasanextremelyvaluableresourceforlan-guagedocumentationandeducationforspeakersoflow-resourcelanguages.Thereweretwotracksforthissharedtask,Track1(closed)andTrack2(open).ForTrack1,sys-temscouldonlybetrainedoninputsentencesandglosses;inTrack2,systemscouldmakeuseofthemorphologicalsegmentationoftheinputaswellasany(non-IGT)externalresources.SincetheTrack2settingbettermatchesthelong-termre-\n\u2217Theseauthorscontributedequallysearchgoalsofourteam,weonlyparticipateinthisopentrack.Inoursubmission,weinvestigatetwodifferentapproaches.First,weattemptdataaugmentationbyeithercreatingourownartificialglossdatabymanipulatingtheexistingtrainingdata,orbyuti-lizingexistingresourcescontainingIGTinotherlanguages(\u00a72).Second,weexploretwodifferentmodelsforglossgeneration(\u00a73).Thefirstbuildsoffthetokenclassificationbaseline,whilethesec-ondusesapretrainedmultilingualseq2seqmodel.Finally,wealsoattempttopost-correctmodeloutputswithadictionary.WeapplythistoGitk-sanandfindthatthis,combinedwithourotherapproaches,resultsinthehighestmorpheme-levelaccuracyforGitksaninTrack2.2DataAugmentationOnemajorchallengeforthissharedtaskisthescaleofdataprovided.Allofthelanguageshavelessthan40klinesoftrainingdata,andallbutArapahohavelessthan10k.Thesmallestdataset(Gitk-san)hasonly31linesofdata.Thus,oneobviousmethodtotryisdataaugmentation.Morespecif-ically,wetrypretrainingourmodelsondifferentformsofaugmenteddatabeforetrainingthemontheoriginaltargetlanguagedata.Weexploredtwoformsofdataaugmentation.First,wegeneratedartificialglossdatainthetar-getlanguagebyswappingwordsintheexistingtrainingdata.Second,weutilizeddatafromODIN(LewisandXia,2010;Xiaetal.,2014)toseeiftransferlearningfromdatainotherlanguagescanhelpimproveperformance.2.1ArtificialDataAchallengeourteamfacedwithrespecttodataaugmentationisfiguringouthowtoobtainaddi-tionaldatawhenwedonothavemuchknowledgeofthelanguages\u2019grammaticalsystems,alongwiththefactthattheselanguagesaregenerallyfrom\n209 Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 209\u2013216 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\ndigitallyunder-resourcedlanguagefamilies.Fur-thermore,wewantedoursolutiontobeeasilyim-plementedandrelativelylanguageagnosticduetotimeconstraintsandpracticalusabilityforre-searchersworkingonavarietyoflanguages.Thus,oneavenueofdataaugmentationwetriedwasbycreatingartificialdatafromtheprovidedtrainingdata.Thisrequiresnorule-writingorknowledgeofthegrammarofthelanguage,andthuscouldbeappliedquicklyandeasilytoallofthelanguagesinthesharedtask.Weusedanaiveword-swappingmethodtoran-domlyswapmorphemesthatoccurinsimilarcon-textstocreatenewsentences.Todothis,foreachglossline,wereplaceeachwordstem(thathasaglosslabelaffix)with\u201cSTEM\u201dtocreateaskeletongloss.Wenaivelydetermineifalabelisastembycheckingifitisinlowercase.Wedonotdothistowordsthatdonothaveaffixesas(withtheexceptionofUspanteko)wedonothaveaccesstopartsofspeech,anddonotwanttoswapwordsthatwouldcreateanungrammaticalsequence.Wecreateadictionarymappingeachskeletonwordglosstopossibleactualglosses,andmapeachactualglosstopossiblesurfaceforms(wemakenoassumptionsthatthesemappingsareone-to-one).Wethenrandomlysamplekrandomskeletonglosses(inthiscase,weusedkequaltoroughlythreetimestheamountoftrainingdata)andran-domlyfillinwordsthatmatchtheformatofskele-tonwordspresentintheline.(1)to(3)belowillustrateanexampleinthisprocess.Wecreateaskeletongloss(2)fromtheGitksansentencein(1)byreplacingtheallwordstemsthathaveanaffixwith\u201cSTEM\u201dinboththesegmentationandglosstiers\u2014inthiscase,only\u2019witxw-itappliestothisstep.Thentocreatetheartificialdatain(3),wereplacetheskeletonwordandcorrespondingglosswithanotherwordfromthetrainingdatathathasthesameskeletonform,inthiscasehahla\u2019lst-it.(1)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSP\u2019witxw-itcome-SX(2)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPSTEM-itSTEM-SX(3)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPhahla\u2019lst-itwork-SXWhilethismethodmaycreateasomewhatun-naturalinputsurfacesequence(asweareunabletocapturephonologicalchangesinthesurfaceformandcorrespondingtranslationsmaybenonsensi-cal),thismethodguaranteesthatthestructureoftheglossisanaturallyoccurringsequence(asweonlyuseglossskeletonsthatarepresentinthein-put).However,alimitationofthismethodisthatitdoesnotextendtoout-of-vocabularytokensorunseenglossstructures.Furthermore,aswecannotgenerateagold-standardtranslationfortheartifi-cialdata,wedonotmakeuseofatranslationintraining.2.2ODINAnotherpotentialavenuefordataaugmentationistransferlearningfromdatainotherlanguages,whichhasbeenshowntobeaneffectivemethodtoimproveperformanceinlow-resourcesettings(Ruderetal.,2019).TheavailableresourceweutilizeisODIN,ortheOnlineDatabaseforInterlinearText(LewisandXia,2010;Xiaetal.,2014).ODINcontains158,007linesofIGT,covering1,496languages.Weusethe2.1versionofODINdataandconvertthedatasettothesharedtaskformat,andfilteroutlanguageswithfewerthanfiveglossedsentences.However,thereremainssignificantnoiseinthedatasetthatcouldcausesignificantalignmentis-suesforthetokenclassificationmodels.ThereforeweopttoonlytraintheByT5modelsonODIN,inthehopethatthismodelislesssensitivetoalign-menterrors.Indeed,wefindthattheByT5modelfinetunedfirstonODINreceivesaperformanceboostwhenfinetunedagainonthesharedtaskdata.3ModelsWeexploretwomodelsforglossgeneration.Thefirstoneisbuiltuponthetokenclassificationbase-linewithsomeimprovements,andwetreatthismodelasourinternalbaseline.Thesecondmodelwedeploytestswhetherwecanachievecompeti-tiveperformancebyfinetuningapretrainedcharac-terbasedmultilingualandmultitaskmodel,ByT5.Forthismodel,weperformminimalpreprocess-inganduserawsegmentedmorphemesandfreetranslationsifavailable.3.1TokenClassificationTransformerWeusethebaselineTrack2modelprovidedbytheorganizersasastartingpoint.Theoriginalimple-mentationrandomlyinitializesatransformermodelfromthedefaultHuggingfaceRoBERTabasecon-figuration,andusesatokenclassificationobjective\n210\n\n\n1https://en.wikipedia.org/wiki/Lezgin_alphabets\nwithcross-entropyloss,whereeachglossistreatedasadistincttoken.Themorphemesandfreetrans-lationsaretokenizedbyspaceanddashes,withpunctuationspre-separated.TheyareconcatenatedandseparatedbytheSEPtokenandareusedastheinputstothemodel.WemodifytheoriginalTrack2baselinemodeltoobtainabetterbaseline.WeusepretrainedweightsfromXLM-RoBERTabase(Conneauetal.,2020),insteadofrandomlyinitializingtheweights.Wealsoslightlymodifythemorphemetokenizertoenforcethatthenumberofmorphemetokensmatchesthenumberofoutputglosstokensexactly.Additionally,weintroducetheCOPYtokentoreplacetheglossifitmatchesthecorrespondingmorphemeexactly.AnexamplefromNatuguisshowningloss(4):(4)67COPY.COPYmnc-xbe-1MINIMzloCOPYSkulCOPYWebelievethiswouldimproveperformancebyremovingtheneedtomemorizeglossedcode-switchingandpropernouns,thoughitisonlyef-fectiveifthecode-switchedlanguageisthesameasthematrixlanguage(e.g.Arapaho),andwouldhavenoeffectifthesourcelanguageusesadif-ferentorthographyoriscode-switchedtoanotherlanguage,wheretheglosswouldnotmatchedthemorphemeformexactly.Thismethodalsocom-pressesallpunctuationmarkersintoonetoken,buttheusefulnessofthissideeffectislessclear.Sinceweareusingpretrainedweights,itisthennaturaltoexploreintegratingthepretrainedtok-enizer.SinceXLM-RoBERTawasnottrainedonanyofthesourcelanguages,itmakesthemostsensetoonlyusethepretrainedtokenizertotok-enizefreetranslations,iftheyareavailable,andextendthevocabularytoincludemorphemes.3.2FinetunedByT5Multi-taskandmulti-lingualpretrainedlargelan-guagemodelshavebeenshowntobeeffectiveformanytasks.Weexplorewhethersuchmodelscanbeusedeffectivelyforglossing.Weconductex-perimentswithbothmT5(Xueetal.,2021)andByT5(Xueetal.,2022),butByT5ispreferredbecauseittakesrawtexts(bytesorcharacters)asinputsandintheoryshouldbemoreeffectiveforunseenlanguages.Weuseapromptbasedmultilin-gualsequencetosequenceobjectiveforbothmod-els.Theprompttemplateis:\u201cGenerateinterlin-earglossfrom[sourcelanguage]:[segmentedmorphemes]withits[matrixlanguage]trans-lation:[freetranslation]Answer:\u201d.Datafromalllanguagesaremixedtogetherandshuffled,withnoupordownsampling.Afterinitialexperi-ments,wefindByT5outperformsmT5acrossalllanguages,andthereforeweonlyconductsubse-quentexperimentsonByT5andreportthosere-sults.Uponinitialexperiments,wealsofindtheresultsforLezgitobelowerthanexpected.Wehypothe-sizethatthefactthatthedataareinCyrillicscriptcausesthisdeficiency,sinceByT5wastrainedonfarlessCyrillicdatathandataintheLatinscript.Thereforewecreateanautomaticromanizationtool,sourcedfromWikipedia1andintegratedintheEpitranpackage(Mortensenetal.,2018),andconvertallLezgidatatoLatinscriptforByT5fine-tuning.AfterinspectingtheoutputsoftheByT5models,wefindcaseswherepunctuationsareattachedtothepreviousglosses,insteadofbeingseparatedbyaspaceasisstandardinthetrainingsets.Thisisprobablyduetothefactthatthemodelwaspre-trainedonuntokenizeddataandthisbehaviorispreserveddespitefinetuningontokenizeddata.Wethereforeuseasimpleregularexpressionbasedtok-enizertofixtheinconsistencies.WenoticethattheprocedureonlygivesperformanceboostonGitk-san,Lezgi,Uspanteko,andNatugu,andsoweonlyapplytheproceduretothoselanguages,leavingtherestoftheoutputsunchanged.4DictionaryPost-correction:GitksanOneofthekeychallengesforextremelylowre-sourcelanguagesistheintegrationofstructuredlinguisticdatainotherforms,suchasadictionary,intomachinelearningpipelines.Wetestasimplepost-correctionmethodfromapre-existingdictio-naryonGitksanonly,duetoitsuniquecombinationoflowresourceandeasilyobtainabledictionaryinmachinereadableform.Weusethedictionarycom-piledbyForbesetal.(2021),withoutconsultingthemorphologicalanalyzersthattheyalsoprovided.Atinferencetime,ifamorphemeisunseendur-ingtraining,wesearchfortheexactforminthedictionary.Wealsoexpandthesearchtoallsubse-quencesofmorphemeswithintheenclosingword,plusthepreviouswholewordincaseswhereapar-ticleisincludedinthedictionaryform.Thefirst\n211\n\n\nmatcheddefinitionisusedastheglossandifnoneofthesearchyieldsanexactmatch,wefallbacktothemodelprediction.Weonlyapplythismethodtothetokenclassificationmodelsbecausethealign-mentbetweenmorphemesandglossesisdirectlyestablished,whereastheseq2seqmodelsdonotguaranteethatthenumberofglossesmatchesthenumberofmorphemes.5ResultsandDiscussionTables1and2showoursystems\u2019performance(aswellastheoriginalbaseline)onthetestdatawithre-specttoword-andmorpheme-levelmicro-averagedaccuracy,respectively.Overall,thetokenclassifica-tionmodeltrainedfirstontheartificiallygeneratedaugmenteddataperformthebest,withthemodeltrainedonthesharedtaskdataonlynotfarbehind.Meanwhile,ByT5modelsperformworse,withthemodelfinetunedfirstonODINtrailingourbestmodelbyafewpercentagepoints,whilethemodelfinetunedfirstonaugmenteddataperformsworsethanthebaseline.5.1DataAugmentationOverall,wefinddataaugmentationtobeuseful.Withartificiallygenerateddata,weseetheeffectsareperhapsgreatestforthemid-resourcelanguages(ddo,lez,ntu,nyb,usp),whilethehighestandlowestresourcedlanguagesdidnotreceivemuchbenefitfrompretrainingontheartificialdata.Wethinkthisisperhapsbecausethereisa\u201csweetspot\u201dwithrespecttotheamountofdatathatisrequiredtotrainamodel.Ifthereisenoughdataalready,inthecaseofArapaho,thenthenoisinessofartificialdatawouldout-weightthebenefitoftrainingonthem.Ontheotherendofthescale,Gitksanperhapsneedsmoresyntheticdatafordataaugmentationtoyieldmeaningfulimprovements.ForByT5models,artificiallygenerateddataseemtohavetheoppositeeffect,whereperfor-manceissignificantlydegraded.Aspeculationforthiseffectisthefactthepretrainedmodelismoresemanticallyaware,andsincetheartificiallygen-eratedsentencescouldbenonsensical,themodelcouldbecomeconfused.Ontheotherhand,pre-trainingonODINyieldsimprovementsforthema-jorityofthelanguages2.ThisisencouragingsincewedidnotperformmuchpreprocessingforODIN,\n2TsezistheonlylanguagethatappearedinODIN(68sentences).Wedidnotremoveitfromthecorpusbutthisshouldhavelittleinfluenceontheperformancebecausethesizeofthedatasetisverysmall.andthereisdefinitelystillroomtomakethedatacleanerandmoreinternallyconsistent,whichinturnshouldresultinabettermodel.5.2ChoiceofHyperparametersWefindthechoiceofhyperparametersofthetokenclassificationmodelstobenecessarilylanguageanddatasetspecific.ArapahoandGitksaninpar-ticularneedspecialattention,wherethenumberoftrainingepochsneedtobeadjustedfortheveryhighandlowdatasize.WealsodevelopedmostoftheoptimizationonthetokenclassificationmodelonArapaho.However,wedidnothavetimetopropagatethechanges(usingpretrainedtokenizer,savingthelastmodelinsteadofthemodelwiththelowestvalidationloss)totherestoflanguagessinceinitialexperimentshowedthatpretrainedtokeniz-ersdidnotimproveontheotherlanguages.How-ever,afterthesubmissiondeadlineisconcluded,weranmoreexperimentsanddiscoveredthataddingpretrainedtokenizersrequiresmoretrainingsteps,andthetrainingisbettercontrolledbyspecifyingthetrainingstepsinsteadofepochs.Wedonotin-cludethoselatestexperimentsinthispaper,butourtokenclassificationmodelshavethepotentialtoperformbetterwithmorehyperparametertuning.5.3In-VersusOut-of-VocabularyErrorsOnedimensionoferroranalysisweinvestigatedwaswhatproportionofoursystems\u2019errorscomefrommorphemesorwordsthatareeitherinoroutofthetrainingdatavocabulary.Wecountamor-phemeorwordasin-vocabularyifthesurfaceformanditscorrespondingglossco-occurinthepro-videdtrainingdata(notincludingthedevelopmentdata,asourmodelsareonlytrainedonthetrainset).NotethatthereisamuchlargerproportionofOOVwordsasopposedtomorphemesduetothefactthatanunseenwordcanbecomposedofdifferentcombinationsofseenmorphemes.Table3showstheproportionofmorphemesandwordsthatareout-of-vocab(OOV)withinthetestset.Whilenearlyallthelanguageshavelessthan10%oftheirmorphemesclassifiedasOOV,GitksannotablyhasarelativelylargeportionofOOVtestdata,with\u224845%ofmorphemesand\u224878%ofwordsbeingOOV.Tables4and5showourmodels\u2019performancesonin-versesout-of-vocabtokensatthemorphemeandwordlevels,respectively.Whilewewouldintuitivelyexpectthatword-levelOOVaccuracybeaboutthesameorworsethanmorpheme-levelOOV\n212\n\n\nTable2:Morpheme-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselinewithartificialpretraininganddictionarypost-correctionisthehighestGitksanaccuracyreportedoutofallsharedtasksubmissions.\nxlmr-base91.3684.3547.47/52.8280.1788.3585.8480.0880.42xlmr-aug89.3488.1546.89/52.3982.3685.5389.4983.0881.48\nTable3:ProportionofmorphemesandwordsthatareOOVwithinthetestset.accuracy,thisisnotthecaseduetothefactthatalargeportionofout-of-vocabwordsareformedwithin-vocabmorphemes.Formostlanguages,withtheexceptionofGitksan,thereappearstobeatrade-offbetweenbetterin-vocabmorphemeperformancewithXLMRandperformanceout-of-vocabwithByT5.6RelatedWorkTherehavebeenavarietyofapproachestotheprob-lemof(semi-)automaticallygeneratinginterlineargloss.BaldridgeandPalmer(2009)investigatetheefficacyofactivelearningforthetaskofinterlinearglossing,usingannotationtimerequiredbyexpertandnon-expertannotatorsastheirmetric.Thesys-temtheyusetogenerateglosslabelsuggestionsisastandardmaximumentropyclassifier.Arule-basedapproachbySnoeketal.(2014)utilizesanFSTtogenerateglossesforPlainsCree,focusingonnouns.Samard\u017ei\u00b4cetal.(2015)viewthetaskofglossingsegmentedtextasatwo-stepprocess,firsttreatingitasastandardPOStaggingtaskandthenaddinglexicalglossesfromadictio-nary.TheydemonstratethismethodonaChintangcorpusofabout1.2millionwords.Anumberofotherworksfocusingoninterlinearglossingutilizeconditionalrandomfield(CRF)models.MoellerandHulden(2018)testthreedifferentmodelsonaverysmallLezgidataset(<3000words):aCRF(thatoutputsBIOlabelswiththecorrespondingglosspercharacterinthein-put),asegmentationandlabellingpipelinethatuti-lizesaCRF(forBIOlabels)andSVM(forglossla-bels),andanLSTMseq2seqmodel.TheyfindthattheCRFthatjointlyproducestheBIOlabelsandtagsproducedthebestresults.McMillan-Major(2020)utilizestranslationsintheirtrainingdatabycreatingtwoCRFmodels,onethatpredictsglossfromthesegmentedinputandanotherthanpre-\nModelarpddogitlezntunybuspAVG\nModelarpddogitlezntunybuspAVG\nbyt5-base78.8680.3214.8460.72b76.6776.7377.2166.48byt5-aug73.2762.374.1738.6055.1169.2570.8553.38byt5-odin80.5682.7920.5763.7777.9782.5975.7269.14\nbaseline91.1185.3425.3351.8249.0388.7182.4867.69\nbaseline85.4475.7116.4134.5441.0884.3076.5559.14\nxlmr-base85.8773.7727.86/34.11a74.1582.9980.6173.4772.14xlmr-aug82.9280.0724.74/31.2577.7778.7285.5377.5173.39\naWereportbefore/afterdictionarybasedpost-correctionforGitksan.bWetrainedthismodelwithoutromanizingLezgi.Table1:Word-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselineisthehighestArapahoaccuracyreportedoutofallsharedtasksubmissions.\nbyt5-base78.8275.7712.5944.1062.4078.9774.2560.99byt5-aug72.1057.932.6026.2435.6270.0167.7347.46byt5-odin80.8178.2412.7450.0063.3985.3073.2563.39\nMorph0.0430.0090.4500.0560.0340.0190.070\narpddogitlezntunybusp\nWord0.2420.1550.7810.1690.2140.0840.200\n213\n\n\nbyt5-aug74.7658.243.4240.2736.5471.2770.56\n35.48\nTable4:Morpheme-levelaccuracyoveralltokensofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.\n24.67\n12.31\nxlmr-base95.2085.1282.8984.7990.8787.4686.05\n23.24\n45.07\nxlmr-aug93.7283.8594.0587.6489.2490.8191.11\n29.33\n24.10\n49.79\nbyt5-odin91.9387.6663.1073.7885.9387.6083.46\n59.51\n23.54\nbyt5-odin83.4778.5518.4262.9064.3886.8575.23\n18.26\n22.41\n49.17\nTable5:Word-levelaccuracyofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.dictsfromthetranslation,andthenusesheuristicstodeterminewhichmodeltoselectfromforeachmorpheme.BarrigaMart\u00ednezetal.(2021)usedaCRFmodeltoachieve>90%accuracyforgloss-ingOtomiandfindthatitworksbetterthananRNN,whichiscomputationallymoreexpensive.Otherworks,includingoursystems,haveturnedtoneuralmethods.Kondratyuk(2019)leveragespretrainedmultilingualBERTtoencodeinputsentences,thenapplyadditionalword-levelandcharacter-levelLSTMlayersbeforejointlydecod-inglemmasandmorphologytagsusingsimplese-quencetagginglayers.Furthermore,theyshowthattwo-stagetrainingbyfirsttrainingonalllan-guagesfollowedbytrainingonthetargetlanguageismoreeffectivethantrainingthesystemonthetargetlanguagealone.AnapproachbyZhaoetal.(2020),likeMcMillan-Major(2020),makesuseoftranslationsavailableinparallelcorpora,butdosobyusingamulti-sourcetransformermodel.Theyalsoincorporatelengthcontrolandalignmentdur-inginferencetoenhancetheirmodel,andtesttheirsystemonArapaho,Tsez,andLezgi.7ConclusionInoursharedtasksubmission,weexploredataaug-mentationmethodsandmodelingstrategiesforthetaskofinterlinearglossinginsevenlow-resourcelanguages.Ourbestperformingmodelsareto-kenclassificationmodelsusingXLMR.Wedemon-stratethatpretrainingonartificialdatawithXLMRisaneffectivetechniqueforthemid-resourcetestlanguages.Additionally,inourerroranalysiswefindthatwemayhaveactuallyundertrainedourtokenclassificationmodels,andthusoursystemsmayhavethepotentialtoperformbetterwithad-ditionalhyperparametertuning.WhileourByT5modelsdidnotperformaswellasourothersys-tems,weshowthatpretrainingonODINdataiseffective,despitethisdatabeingverynoisy.Finally,wealsodemonstrateimprovementsbyutilizingadictionarytopost-correctmodeloutputsforGitk-san.\n0.00\n0.00\n0.00\n0.00\n11.24\n0.82\n12.86\n14.52\n23.60\n4.97\n56.36\n45.65\n21.14\n43.37\n1.61\n47.52\n46.94\n29.69\n13.67\nxlmr-base95.9378.1895.2384.2493.1485.8586.27\n5.79\n28.04\n2.00\n17.00\n0.41\n16.08\n14.67\n19.35\nModelarpddogitlezntunybusp\nModelarpddogitlezntunybusp\n7.49\n28.09\n28.09\n44.81\n3.23\n3.23\n54.44\n2.33\nbyt5-aug87.2268.6910.7146.0665.1374.5981.44\n30.20\nxlmr-aug92.9888.9484.7487.1087.8891.1789.31\n48.70\n40.00\n28.63\n8.67\n2.60\n2.60\n9.68\n214\n\n\nAcknowledgementsThisworkwassupportedbyNSFCISERIgrantnumber2211951,FromAcousticSignaltoMor-phosyntacticAnalysisinoneEnd-to-EndNeuralSystem.ReferencesJasonBaldridgeandAlexisPalmer.2009.Howwelldoesactivelearningactuallywork?Time-basedeval-uationofcost-reductionstrategiesforlanguagedocu-mentation.InProceedingsofthe2009ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages296\u2013305,Singapore.AssociationforCompu-tationalLinguistics.DiegoBarrigaMart\u00ednez,VictorMijangos,andXi-menaGutierrez-Vasques.2021.Automaticinterlin-earglossingforOtomilanguage.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages34\u201343,Online.AssociationforComputationalLin-guistics.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.ClarissaForbes,GarrettNicolai,andMiikkaSilfverberg.2021.AnFSTmorphologicalanalyzerforthegitksanlanguage.InProceedingsofthe18thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages188\u2013197,Online.AssociationforComputationalLinguistics.DanKondratyuk.2019.Cross-linguallemmatizationandmorphologytaggingwithtwo-stagemultilin-gualBERTfine-tuning.InProceedingsofthe16thWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages12\u201318,Florence,Italy.AssociationforComputationalLinguistics.WilliamD.LewisandFeiXia.2010.DevelopingODIN:AMultilingualRepositoryofAnnotatedLanguageDataforHundredsoftheWorld\u2019sLanguages.Liter-aryandLinguisticComputing,25(3):303\u2013319.AngelinaMcMillan-Major.2020.Automatingglossgenerationininterlinearglossedtext.ProceedingsoftheSocietyforComputationinLinguistics,3(1):338\u2013349.SarahMoellerandMansHulden.2018.Automaticglossinginalow-resourcesettingforlanguagedoc-umentation.InProceedingsoftheWorkshoponComputationalModelingofPolysyntheticLanguages,pages84\u201393.DavidR.Mortensen,SiddharthDalmia,andPatrickLittell.2018.Epitran:PrecisionG2Pformanylan-guages.InProceedingsoftheEleventhInternationalConferenceonLanguageResourcesandEvaluation(LREC2018),Miyazaki,Japan.EuropeanLanguageResourcesAssociation(ELRA).SebastianRuder,MatthewEPeters,SwabhaSwayamdipta,andThomasWolf.2019.Transferlearninginnaturallanguageprocessing.InProceed-ingsofthe2019conferenceoftheNorthAmericanchapteroftheassociationforcomputationallinguistics:Tutorials,pages15\u201318.TanjaSamard\u017ei\u00b4c,RobertSchikowski,andSabineStoll.2015.Automaticinterlinearglossingastwo-levelsequenceclassification.NoamShazeerandMitchellStern.2018.Adafactor:Adaptivelearningrateswithsublinearmemorycost.InProceedingsofthe35thInternationalConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,pages4596\u20134604.PMLR.ConorSnoek,DorothyThunder,KaidiL\u00f5o,AnttiArppe,JordanLachler,SjurMoshagen,andTrondTrosterud.2014.ModelingthenounmorphologyofPlainsCree.InProceedingsofthe2014WorkshopontheUseofComputationalMethodsintheStudyofEndangeredLanguages,pages34\u201342,Baltimore,Maryland,USA.AssociationforComputationalLinguistics.FeiXia,WilliamLewis,MichaelWayneGoodman,JoshuaCrowgey,andEmilyM.Bender.2014.En-richingODIN.InProceedingsoftheNinthInter-nationalConferenceonLanguageResourcesandEvaluation(LREC\u201914),pages3151\u20133157,Reykjavik,Iceland.EuropeanLanguageResourcesAssociation(ELRA).LintingXue,AdityaBarua,NoahConstant,RamiAl-Rfou,SharanNarang,MihirKale,AdamRoberts,andColinRaffel.2022.ByT5:Towardsatoken-freefuturewithpre-trainedbyte-to-bytemodels.Transac-tionsoftheAssociationforComputationalLinguis-tics,10:291\u2013306.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.XingyuanZhao,SatoruOzaki,AntoniosAnastasopou-los,GrahamNeubig,andLoriLevin.2020.Auto-maticinterlinearglossingforunder-resourcedlan-guagesleveragingtranslations.InProceedingsofthe28thInternationalConferenceonComputationalLinguistics,pages5397\u20135408,Barcelona,Spain(On-line).InternationalCommitteeonComputationalLin-guistics.\n215\n\n\nAHyperparameterSettingsWeuseAdafactor(ShazeerandStern,2018)astheoptimizeracrossallexperiments,withthede-faultschedulerfromHuggingFaceTransformers,abatchsizeof32forRoBERTabasedmodelsandabatchsizeof4withagradientaccumulationstepof8forByT5basedmodels.Wetrainthetokenclassificationmodelsfor40epochsexceptforAra-paho,onwhichwetrain20epochs,andGitksan,onwhichwetrain2,000steps.WetraintheByT5basedmodelsfor20epochsonallofthedatamixedtogether.\n216"}, {"question": " Which language had the smallest amount of data in the submission?", "answer": " Gitksan had the smallest dataset with only 31 lines of data.,", "ref_chunk": "SigMoreFunSubmissiontotheSIGMORPHONSharedTaskonInterlinearGlossingTaiqiHe\u2217,LindiaTjuatja\u2217,NateRobinson,ShinjiWatanabe,DavidR.Mortensen,GrahamNeubig,LoriLevinLanguageTechnologiesInstituteCarnegieMellonUniversity{taiqih,ltjuatja,nrrobins,swatanab,dmortens,gneubig,lsl}@cs.cmu.eduAbstractInoursubmissiontotheSIGMORPHON2023SharedTaskoninterlinearglossing(IGT),weexploreapproachestodataaugmentationandmodelingacrosssevenlow-resourcelanguages.Fordataaugmentation,weexploretwoap-proaches:creatingartificialdatafromthepro-videdtrainingdataandutilizingexistingIGTresourcesinotherlanguages.Onthemodelingside,wetestanenhancedversionofthepro-videdtokenclassificationbaselineaswellasapretrainedmultilingualseq2seqmodel.Ad-ditionally,weapplypost-correctionusingadictionaryforGitksan,thelanguagewiththesmallestamountofdata.Wefindthatourtokenclassificationmodelsarethebestperforming,withthehighestword-levelaccuracyforAra-pahoandhighestmorpheme-levelaccuracyforGitksanoutofallsubmissions.Wealsoshowthatdataaugmentationisaneffectivestrategy,thoughapplyingartificialdatapretraininghasverydifferenteffectsacrossbothmodelstested.1IntroductionThispaperdescribestheSigMoreFunsubmissiontotheSIGMORPHON2023SharedTaskoninterlin-earglossing.Giveninputtextinatargetlanguage,thetaskistopredictthecorrespondinginterlineargloss(usingLeipzigglossingconventions).IGTisanimportantformoflinguisticannotationforthemorphologicalanalysisoflanguages,andalsoservesasanextremelyvaluableresourceforlan-guagedocumentationandeducationforspeakersoflow-resourcelanguages.Thereweretwotracksforthissharedtask,Track1(closed)andTrack2(open).ForTrack1,sys-temscouldonlybetrainedoninputsentencesandglosses;inTrack2,systemscouldmakeuseofthemorphologicalsegmentationoftheinputaswellasany(non-IGT)externalresources.SincetheTrack2settingbettermatchesthelong-termre-\n\u2217Theseauthorscontributedequallysearchgoalsofourteam,weonlyparticipateinthisopentrack.Inoursubmission,weinvestigatetwodifferentapproaches.First,weattemptdataaugmentationbyeithercreatingourownartificialglossdatabymanipulatingtheexistingtrainingdata,orbyuti-lizingexistingresourcescontainingIGTinotherlanguages(\u00a72).Second,weexploretwodifferentmodelsforglossgeneration(\u00a73).Thefirstbuildsoffthetokenclassificationbaseline,whilethesec-ondusesapretrainedmultilingualseq2seqmodel.Finally,wealsoattempttopost-correctmodeloutputswithadictionary.WeapplythistoGitk-sanandfindthatthis,combinedwithourotherapproaches,resultsinthehighestmorpheme-levelaccuracyforGitksaninTrack2.2DataAugmentationOnemajorchallengeforthissharedtaskisthescaleofdataprovided.Allofthelanguageshavelessthan40klinesoftrainingdata,andallbutArapahohavelessthan10k.Thesmallestdataset(Gitk-san)hasonly31linesofdata.Thus,oneobviousmethodtotryisdataaugmentation.Morespecif-ically,wetrypretrainingourmodelsondifferentformsofaugmenteddatabeforetrainingthemontheoriginaltargetlanguagedata.Weexploredtwoformsofdataaugmentation.First,wegeneratedartificialglossdatainthetar-getlanguagebyswappingwordsintheexistingtrainingdata.Second,weutilizeddatafromODIN(LewisandXia,2010;Xiaetal.,2014)toseeiftransferlearningfromdatainotherlanguagescanhelpimproveperformance.2.1ArtificialDataAchallengeourteamfacedwithrespecttodataaugmentationisfiguringouthowtoobtainaddi-tionaldatawhenwedonothavemuchknowledgeofthelanguages\u2019grammaticalsystems,alongwiththefactthattheselanguagesaregenerallyfrom\n209 Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 209\u2013216 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\ndigitallyunder-resourcedlanguagefamilies.Fur-thermore,wewantedoursolutiontobeeasilyim-plementedandrelativelylanguageagnosticduetotimeconstraintsandpracticalusabilityforre-searchersworkingonavarietyoflanguages.Thus,oneavenueofdataaugmentationwetriedwasbycreatingartificialdatafromtheprovidedtrainingdata.Thisrequiresnorule-writingorknowledgeofthegrammarofthelanguage,andthuscouldbeappliedquicklyandeasilytoallofthelanguagesinthesharedtask.Weusedanaiveword-swappingmethodtoran-domlyswapmorphemesthatoccurinsimilarcon-textstocreatenewsentences.Todothis,foreachglossline,wereplaceeachwordstem(thathasaglosslabelaffix)with\u201cSTEM\u201dtocreateaskeletongloss.Wenaivelydetermineifalabelisastembycheckingifitisinlowercase.Wedonotdothistowordsthatdonothaveaffixesas(withtheexceptionofUspanteko)wedonothaveaccesstopartsofspeech,anddonotwanttoswapwordsthatwouldcreateanungrammaticalsequence.Wecreateadictionarymappingeachskeletonwordglosstopossibleactualglosses,andmapeachactualglosstopossiblesurfaceforms(wemakenoassumptionsthatthesemappingsareone-to-one).Wethenrandomlysamplekrandomskeletonglosses(inthiscase,weusedkequaltoroughlythreetimestheamountoftrainingdata)andran-domlyfillinwordsthatmatchtheformatofskele-tonwordspresentintheline.(1)to(3)belowillustrateanexampleinthisprocess.Wecreateaskeletongloss(2)fromtheGitksansentencein(1)byreplacingtheallwordstemsthathaveanaffixwith\u201cSTEM\u201dinboththesegmentationandglosstiers\u2014inthiscase,only\u2019witxw-itappliestothisstep.Thentocreatetheartificialdatain(3),wereplacetheskeletonwordandcorrespondingglosswithanotherwordfromthetrainingdatathathasthesameskeletonform,inthiscasehahla\u2019lst-it.(1)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSP\u2019witxw-itcome-SX(2)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPSTEM-itSTEM-SX(3)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPhahla\u2019lst-itwork-SXWhilethismethodmaycreateasomewhatun-naturalinputsurfacesequence(asweareunabletocapturephonologicalchangesinthesurfaceformandcorrespondingtranslationsmaybenonsensi-cal),thismethodguaranteesthatthestructureoftheglossisanaturallyoccurringsequence(asweonlyuseglossskeletonsthatarepresentinthein-put).However,alimitationofthismethodisthatitdoesnotextendtoout-of-vocabularytokensorunseenglossstructures.Furthermore,aswecannotgenerateagold-standardtranslationfortheartifi-cialdata,wedonotmakeuseofatranslationintraining.2.2ODINAnotherpotentialavenuefordataaugmentationistransferlearningfromdatainotherlanguages,whichhasbeenshowntobeaneffectivemethodtoimproveperformanceinlow-resourcesettings(Ruderetal.,2019).TheavailableresourceweutilizeisODIN,ortheOnlineDatabaseforInterlinearText(LewisandXia,2010;Xiaetal.,2014).ODINcontains158,007linesofIGT,covering1,496languages.Weusethe2.1versionofODINdataandconvertthedatasettothesharedtaskformat,andfilteroutlanguageswithfewerthanfiveglossedsentences.However,thereremainssignificantnoiseinthedatasetthatcouldcausesignificantalignmentis-suesforthetokenclassificationmodels.ThereforeweopttoonlytraintheByT5modelsonODIN,inthehopethatthismodelislesssensitivetoalign-menterrors.Indeed,wefindthattheByT5modelfinetunedfirstonODINreceivesaperformanceboostwhenfinetunedagainonthesharedtaskdata.3ModelsWeexploretwomodelsforglossgeneration.Thefirstoneisbuiltuponthetokenclassificationbase-linewithsomeimprovements,andwetreatthismodelasourinternalbaseline.Thesecondmodelwedeploytestswhetherwecanachievecompeti-tiveperformancebyfinetuningapretrainedcharac-terbasedmultilingualandmultitaskmodel,ByT5.Forthismodel,weperformminimalpreprocess-inganduserawsegmentedmorphemesandfreetranslationsifavailable.3.1TokenClassificationTransformerWeusethebaselineTrack2modelprovidedbytheorganizersasastartingpoint.Theoriginalimple-mentationrandomlyinitializesatransformermodelfromthedefaultHuggingfaceRoBERTabasecon-figuration,andusesatokenclassificationobjective\n210\n\n\n1https://en.wikipedia.org/wiki/Lezgin_alphabets\nwithcross-entropyloss,whereeachglossistreatedasadistincttoken.Themorphemesandfreetrans-lationsaretokenizedbyspaceanddashes,withpunctuationspre-separated.TheyareconcatenatedandseparatedbytheSEPtokenandareusedastheinputstothemodel.WemodifytheoriginalTrack2baselinemodeltoobtainabetterbaseline.WeusepretrainedweightsfromXLM-RoBERTabase(Conneauetal.,2020),insteadofrandomlyinitializingtheweights.Wealsoslightlymodifythemorphemetokenizertoenforcethatthenumberofmorphemetokensmatchesthenumberofoutputglosstokensexactly.Additionally,weintroducetheCOPYtokentoreplacetheglossifitmatchesthecorrespondingmorphemeexactly.AnexamplefromNatuguisshowningloss(4):(4)67COPY.COPYmnc-xbe-1MINIMzloCOPYSkulCOPYWebelievethiswouldimproveperformancebyremovingtheneedtomemorizeglossedcode-switchingandpropernouns,thoughitisonlyef-fectiveifthecode-switchedlanguageisthesameasthematrixlanguage(e.g.Arapaho),andwouldhavenoeffectifthesourcelanguageusesadif-ferentorthographyoriscode-switchedtoanotherlanguage,wheretheglosswouldnotmatchedthemorphemeformexactly.Thismethodalsocom-pressesallpunctuationmarkersintoonetoken,buttheusefulnessofthissideeffectislessclear.Sinceweareusingpretrainedweights,itisthennaturaltoexploreintegratingthepretrainedtok-enizer.SinceXLM-RoBERTawasnottrainedonanyofthesourcelanguages,itmakesthemostsensetoonlyusethepretrainedtokenizertotok-enizefreetranslations,iftheyareavailable,andextendthevocabularytoincludemorphemes.3.2FinetunedByT5Multi-taskandmulti-lingualpretrainedlargelan-guagemodelshavebeenshowntobeeffectiveformanytasks.Weexplorewhethersuchmodelscanbeusedeffectivelyforglossing.Weconductex-perimentswithbothmT5(Xueetal.,2021)andByT5(Xueetal.,2022),butByT5ispreferredbecauseittakesrawtexts(bytesorcharacters)asinputsandintheoryshouldbemoreeffectiveforunseenlanguages.Weuseapromptbasedmultilin-gualsequencetosequenceobjectiveforbothmod-els.Theprompttemplateis:\u201cGenerateinterlin-earglossfrom[sourcelanguage]:[segmentedmorphemes]withits[matrixlanguage]trans-lation:[freetranslation]Answer:\u201d.Datafromalllanguagesaremixedtogetherandshuffled,withnoupordownsampling.Afterinitialexperi-ments,wefindByT5outperformsmT5acrossalllanguages,andthereforeweonlyconductsubse-quentexperimentsonByT5andreportthosere-sults.Uponinitialexperiments,wealsofindtheresultsforLezgitobelowerthanexpected.Wehypothe-sizethatthefactthatthedataareinCyrillicscriptcausesthisdeficiency,sinceByT5wastrainedonfarlessCyrillicdatathandataintheLatinscript.Thereforewecreateanautomaticromanizationtool,sourcedfromWikipedia1andintegratedintheEpitranpackage(Mortensenetal.,2018),andconvertallLezgidatatoLatinscriptforByT5fine-tuning.AfterinspectingtheoutputsoftheByT5models,wefindcaseswherepunctuationsareattachedtothepreviousglosses,insteadofbeingseparatedbyaspaceasisstandardinthetrainingsets.Thisisprobablyduetothefactthatthemodelwaspre-trainedonuntokenizeddataandthisbehaviorispreserveddespitefinetuningontokenizeddata.Wethereforeuseasimpleregularexpressionbasedtok-enizertofixtheinconsistencies.WenoticethattheprocedureonlygivesperformanceboostonGitk-san,Lezgi,Uspanteko,andNatugu,andsoweonlyapplytheproceduretothoselanguages,leavingtherestoftheoutputsunchanged.4DictionaryPost-correction:GitksanOneofthekeychallengesforextremelylowre-sourcelanguagesistheintegrationofstructuredlinguisticdatainotherforms,suchasadictionary,intomachinelearningpipelines.Wetestasimplepost-correctionmethodfromapre-existingdictio-naryonGitksanonly,duetoitsuniquecombinationoflowresourceandeasilyobtainabledictionaryinmachinereadableform.Weusethedictionarycom-piledbyForbesetal.(2021),withoutconsultingthemorphologicalanalyzersthattheyalsoprovided.Atinferencetime,ifamorphemeisunseendur-ingtraining,wesearchfortheexactforminthedictionary.Wealsoexpandthesearchtoallsubse-quencesofmorphemeswithintheenclosingword,plusthepreviouswholewordincaseswhereapar-ticleisincludedinthedictionaryform.Thefirst\n211\n\n\nmatcheddefinitionisusedastheglossandifnoneofthesearchyieldsanexactmatch,wefallbacktothemodelprediction.Weonlyapplythismethodtothetokenclassificationmodelsbecausethealign-mentbetweenmorphemesandglossesisdirectlyestablished,whereastheseq2seqmodelsdonotguaranteethatthenumberofglossesmatchesthenumberofmorphemes.5ResultsandDiscussionTables1and2showoursystems\u2019performance(aswellastheoriginalbaseline)onthetestdatawithre-specttoword-andmorpheme-levelmicro-averagedaccuracy,respectively.Overall,thetokenclassifica-tionmodeltrainedfirstontheartificiallygeneratedaugmenteddataperformthebest,withthemodeltrainedonthesharedtaskdataonlynotfarbehind.Meanwhile,ByT5modelsperformworse,withthemodelfinetunedfirstonODINtrailingourbestmodelbyafewpercentagepoints,whilethemodelfinetunedfirstonaugmenteddataperformsworsethanthebaseline.5.1DataAugmentationOverall,wefinddataaugmentationtobeuseful.Withartificiallygenerateddata,weseetheeffectsareperhapsgreatestforthemid-resourcelanguages(ddo,lez,ntu,nyb,usp),whilethehighestandlowestresourcedlanguagesdidnotreceivemuchbenefitfrompretrainingontheartificialdata.Wethinkthisisperhapsbecausethereisa\u201csweetspot\u201dwithrespecttotheamountofdatathatisrequiredtotrainamodel.Ifthereisenoughdataalready,inthecaseofArapaho,thenthenoisinessofartificialdatawouldout-weightthebenefitoftrainingonthem.Ontheotherendofthescale,Gitksanperhapsneedsmoresyntheticdatafordataaugmentationtoyieldmeaningfulimprovements.ForByT5models,artificiallygenerateddataseemtohavetheoppositeeffect,whereperfor-manceissignificantlydegraded.Aspeculationforthiseffectisthefactthepretrainedmodelismoresemanticallyaware,andsincetheartificiallygen-eratedsentencescouldbenonsensical,themodelcouldbecomeconfused.Ontheotherhand,pre-trainingonODINyieldsimprovementsforthema-jorityofthelanguages2.ThisisencouragingsincewedidnotperformmuchpreprocessingforODIN,\n2TsezistheonlylanguagethatappearedinODIN(68sentences).Wedidnotremoveitfromthecorpusbutthisshouldhavelittleinfluenceontheperformancebecausethesizeofthedatasetisverysmall.andthereisdefinitelystillroomtomakethedatacleanerandmoreinternallyconsistent,whichinturnshouldresultinabettermodel.5.2ChoiceofHyperparametersWefindthechoiceofhyperparametersofthetokenclassificationmodelstobenecessarilylanguageanddatasetspecific.ArapahoandGitksaninpar-ticularneedspecialattention,wherethenumberoftrainingepochsneedtobeadjustedfortheveryhighandlowdatasize.WealsodevelopedmostoftheoptimizationonthetokenclassificationmodelonArapaho.However,wedidnothavetimetopropagatethechanges(usingpretrainedtokenizer,savingthelastmodelinsteadofthemodelwiththelowestvalidationloss)totherestoflanguagessinceinitialexperimentshowedthatpretrainedtokeniz-ersdidnotimproveontheotherlanguages.How-ever,afterthesubmissiondeadlineisconcluded,weranmoreexperimentsanddiscoveredthataddingpretrainedtokenizersrequiresmoretrainingsteps,andthetrainingisbettercontrolledbyspecifyingthetrainingstepsinsteadofepochs.Wedonotin-cludethoselatestexperimentsinthispaper,butourtokenclassificationmodelshavethepotentialtoperformbetterwithmorehyperparametertuning.5.3In-VersusOut-of-VocabularyErrorsOnedimensionoferroranalysisweinvestigatedwaswhatproportionofoursystems\u2019errorscomefrommorphemesorwordsthatareeitherinoroutofthetrainingdatavocabulary.Wecountamor-phemeorwordasin-vocabularyifthesurfaceformanditscorrespondingglossco-occurinthepro-videdtrainingdata(notincludingthedevelopmentdata,asourmodelsareonlytrainedonthetrainset).NotethatthereisamuchlargerproportionofOOVwordsasopposedtomorphemesduetothefactthatanunseenwordcanbecomposedofdifferentcombinationsofseenmorphemes.Table3showstheproportionofmorphemesandwordsthatareout-of-vocab(OOV)withinthetestset.Whilenearlyallthelanguageshavelessthan10%oftheirmorphemesclassifiedasOOV,GitksannotablyhasarelativelylargeportionofOOVtestdata,with\u224845%ofmorphemesand\u224878%ofwordsbeingOOV.Tables4and5showourmodels\u2019performancesonin-versesout-of-vocabtokensatthemorphemeandwordlevels,respectively.Whilewewouldintuitivelyexpectthatword-levelOOVaccuracybeaboutthesameorworsethanmorpheme-levelOOV\n212\n\n\nTable2:Morpheme-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselinewithartificialpretraininganddictionarypost-correctionisthehighestGitksanaccuracyreportedoutofallsharedtasksubmissions.\nxlmr-base91.3684.3547.47/52.8280.1788.3585.8480.0880.42xlmr-aug89.3488.1546.89/52.3982.3685.5389.4983.0881.48\nTable3:ProportionofmorphemesandwordsthatareOOVwithinthetestset.accuracy,thisisnotthecaseduetothefactthatalargeportionofout-of-vocabwordsareformedwithin-vocabmorphemes.Formostlanguages,withtheexceptionofGitksan,thereappearstobeatrade-offbetweenbetterin-vocabmorphemeperformancewithXLMRandperformanceout-of-vocabwithByT5.6RelatedWorkTherehavebeenavarietyofapproachestotheprob-lemof(semi-)automaticallygeneratinginterlineargloss.BaldridgeandPalmer(2009)investigatetheefficacyofactivelearningforthetaskofinterlinearglossing,usingannotationtimerequiredbyexpertandnon-expertannotatorsastheirmetric.Thesys-temtheyusetogenerateglosslabelsuggestionsisastandardmaximumentropyclassifier.Arule-basedapproachbySnoeketal.(2014)utilizesanFSTtogenerateglossesforPlainsCree,focusingonnouns.Samard\u017ei\u00b4cetal.(2015)viewthetaskofglossingsegmentedtextasatwo-stepprocess,firsttreatingitasastandardPOStaggingtaskandthenaddinglexicalglossesfromadictio-nary.TheydemonstratethismethodonaChintangcorpusofabout1.2millionwords.Anumberofotherworksfocusingoninterlinearglossingutilizeconditionalrandomfield(CRF)models.MoellerandHulden(2018)testthreedifferentmodelsonaverysmallLezgidataset(<3000words):aCRF(thatoutputsBIOlabelswiththecorrespondingglosspercharacterinthein-put),asegmentationandlabellingpipelinethatuti-lizesaCRF(forBIOlabels)andSVM(forglossla-bels),andanLSTMseq2seqmodel.TheyfindthattheCRFthatjointlyproducestheBIOlabelsandtagsproducedthebestresults.McMillan-Major(2020)utilizestranslationsintheirtrainingdatabycreatingtwoCRFmodels,onethatpredictsglossfromthesegmentedinputandanotherthanpre-\nModelarpddogitlezntunybuspAVG\nModelarpddogitlezntunybuspAVG\nbyt5-base78.8680.3214.8460.72b76.6776.7377.2166.48byt5-aug73.2762.374.1738.6055.1169.2570.8553.38byt5-odin80.5682.7920.5763.7777.9782.5975.7269.14\nbaseline91.1185.3425.3351.8249.0388.7182.4867.69\nbaseline85.4475.7116.4134.5441.0884.3076.5559.14\nxlmr-base85.8773.7727.86/34.11a74.1582.9980.6173.4772.14xlmr-aug82.9280.0724.74/31.2577.7778.7285.5377.5173.39\naWereportbefore/afterdictionarybasedpost-correctionforGitksan.bWetrainedthismodelwithoutromanizingLezgi.Table1:Word-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselineisthehighestArapahoaccuracyreportedoutofallsharedtasksubmissions.\nbyt5-base78.8275.7712.5944.1062.4078.9774.2560.99byt5-aug72.1057.932.6026.2435.6270.0167.7347.46byt5-odin80.8178.2412.7450.0063.3985.3073.2563.39\nMorph0.0430.0090.4500.0560.0340.0190.070\narpddogitlezntunybusp\nWord0.2420.1550.7810.1690.2140.0840.200\n213\n\n\nbyt5-aug74.7658.243.4240.2736.5471.2770.56\n35.48\nTable4:Morpheme-levelaccuracyoveralltokensofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.\n24.67\n12.31\nxlmr-base95.2085.1282.8984.7990.8787.4686.05\n23.24\n45.07\nxlmr-aug93.7283.8594.0587.6489.2490.8191.11\n29.33\n24.10\n49.79\nbyt5-odin91.9387.6663.1073.7885.9387.6083.46\n59.51\n23.54\nbyt5-odin83.4778.5518.4262.9064.3886.8575.23\n18.26\n22.41\n49.17\nTable5:Word-levelaccuracyofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.dictsfromthetranslation,andthenusesheuristicstodeterminewhichmodeltoselectfromforeachmorpheme.BarrigaMart\u00ednezetal.(2021)usedaCRFmodeltoachieve>90%accuracyforgloss-ingOtomiandfindthatitworksbetterthananRNN,whichiscomputationallymoreexpensive.Otherworks,includingoursystems,haveturnedtoneuralmethods.Kondratyuk(2019)leveragespretrainedmultilingualBERTtoencodeinputsentences,thenapplyadditionalword-levelandcharacter-levelLSTMlayersbeforejointlydecod-inglemmasandmorphologytagsusingsimplese-quencetagginglayers.Furthermore,theyshowthattwo-stagetrainingbyfirsttrainingonalllan-guagesfollowedbytrainingonthetargetlanguageismoreeffectivethantrainingthesystemonthetargetlanguagealone.AnapproachbyZhaoetal.(2020),likeMcMillan-Major(2020),makesuseoftranslationsavailableinparallelcorpora,butdosobyusingamulti-sourcetransformermodel.Theyalsoincorporatelengthcontrolandalignmentdur-inginferencetoenhancetheirmodel,andtesttheirsystemonArapaho,Tsez,andLezgi.7ConclusionInoursharedtasksubmission,weexploredataaug-mentationmethodsandmodelingstrategiesforthetaskofinterlinearglossinginsevenlow-resourcelanguages.Ourbestperformingmodelsareto-kenclassificationmodelsusingXLMR.Wedemon-stratethatpretrainingonartificialdatawithXLMRisaneffectivetechniqueforthemid-resourcetestlanguages.Additionally,inourerroranalysiswefindthatwemayhaveactuallyundertrainedourtokenclassificationmodels,andthusoursystemsmayhavethepotentialtoperformbetterwithad-ditionalhyperparametertuning.WhileourByT5modelsdidnotperformaswellasourothersys-tems,weshowthatpretrainingonODINdataiseffective,despitethisdatabeingverynoisy.Finally,wealsodemonstrateimprovementsbyutilizingadictionarytopost-correctmodeloutputsforGitk-san.\n0.00\n0.00\n0.00\n0.00\n11.24\n0.82\n12.86\n14.52\n23.60\n4.97\n56.36\n45.65\n21.14\n43.37\n1.61\n47.52\n46.94\n29.69\n13.67\nxlmr-base95.9378.1895.2384.2493.1485.8586.27\n5.79\n28.04\n2.00\n17.00\n0.41\n16.08\n14.67\n19.35\nModelarpddogitlezntunybusp\nModelarpddogitlezntunybusp\n7.49\n28.09\n28.09\n44.81\n3.23\n3.23\n54.44\n2.33\nbyt5-aug87.2268.6910.7146.0665.1374.5981.44\n30.20\nxlmr-aug92.9888.9484.7487.1087.8891.1789.31\n48.70\n40.00\n28.63\n8.67\n2.60\n2.60\n9.68\n214\n\n\nAcknowledgementsThisworkwassupportedbyNSFCISERIgrantnumber2211951,FromAcousticSignaltoMor-phosyntacticAnalysisinoneEnd-to-EndNeuralSystem.ReferencesJasonBaldridgeandAlexisPalmer.2009.Howwelldoesactivelearningactuallywork?Time-basedeval-uationofcost-reductionstrategiesforlanguagedocu-mentation.InProceedingsofthe2009ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages296\u2013305,Singapore.AssociationforCompu-tationalLinguistics.DiegoBarrigaMart\u00ednez,VictorMijangos,andXi-menaGutierrez-Vasques.2021.Automaticinterlin-earglossingforOtomilanguage.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages34\u201343,Online.AssociationforComputationalLin-guistics.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.ClarissaForbes,GarrettNicolai,andMiikkaSilfverberg.2021.AnFSTmorphologicalanalyzerforthegitksanlanguage.InProceedingsofthe18thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages188\u2013197,Online.AssociationforComputationalLinguistics.DanKondratyuk.2019.Cross-linguallemmatizationandmorphologytaggingwithtwo-stagemultilin-gualBERTfine-tuning.InProceedingsofthe16thWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages12\u201318,Florence,Italy.AssociationforComputationalLinguistics.WilliamD.LewisandFeiXia.2010.DevelopingODIN:AMultilingualRepositoryofAnnotatedLanguageDataforHundredsoftheWorld\u2019sLanguages.Liter-aryandLinguisticComputing,25(3):303\u2013319.AngelinaMcMillan-Major.2020.Automatingglossgenerationininterlinearglossedtext.ProceedingsoftheSocietyforComputationinLinguistics,3(1):338\u2013349.SarahMoellerandMansHulden.2018.Automaticglossinginalow-resourcesettingforlanguagedoc-umentation.InProceedingsoftheWorkshoponComputationalModelingofPolysyntheticLanguages,pages84\u201393.DavidR.Mortensen,SiddharthDalmia,andPatrickLittell.2018.Epitran:PrecisionG2Pformanylan-guages.InProceedingsoftheEleventhInternationalConferenceonLanguageResourcesandEvaluation(LREC2018),Miyazaki,Japan.EuropeanLanguageResourcesAssociation(ELRA).SebastianRuder,MatthewEPeters,SwabhaSwayamdipta,andThomasWolf.2019.Transferlearninginnaturallanguageprocessing.InProceed-ingsofthe2019conferenceoftheNorthAmericanchapteroftheassociationforcomputationallinguistics:Tutorials,pages15\u201318.TanjaSamard\u017ei\u00b4c,RobertSchikowski,andSabineStoll.2015.Automaticinterlinearglossingastwo-levelsequenceclassification.NoamShazeerandMitchellStern.2018.Adafactor:Adaptivelearningrateswithsublinearmemorycost.InProceedingsofthe35thInternationalConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,pages4596\u20134604.PMLR.ConorSnoek,DorothyThunder,KaidiL\u00f5o,AnttiArppe,JordanLachler,SjurMoshagen,andTrondTrosterud.2014.ModelingthenounmorphologyofPlainsCree.InProceedingsofthe2014WorkshopontheUseofComputationalMethodsintheStudyofEndangeredLanguages,pages34\u201342,Baltimore,Maryland,USA.AssociationforComputationalLinguistics.FeiXia,WilliamLewis,MichaelWayneGoodman,JoshuaCrowgey,andEmilyM.Bender.2014.En-richingODIN.InProceedingsoftheNinthInter-nationalConferenceonLanguageResourcesandEvaluation(LREC\u201914),pages3151\u20133157,Reykjavik,Iceland.EuropeanLanguageResourcesAssociation(ELRA).LintingXue,AdityaBarua,NoahConstant,RamiAl-Rfou,SharanNarang,MihirKale,AdamRoberts,andColinRaffel.2022.ByT5:Towardsatoken-freefuturewithpre-trainedbyte-to-bytemodels.Transac-tionsoftheAssociationforComputationalLinguis-tics,10:291\u2013306.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.XingyuanZhao,SatoruOzaki,AntoniosAnastasopou-los,GrahamNeubig,andLoriLevin.2020.Auto-maticinterlinearglossingforunder-resourcedlan-guagesleveragingtranslations.InProceedingsofthe28thInternationalConferenceonComputationalLinguistics,pages5397\u20135408,Barcelona,Spain(On-line).InternationalCommitteeonComputationalLin-guistics.\n215\n\n\nAHyperparameterSettingsWeuseAdafactor(ShazeerandStern,2018)astheoptimizeracrossallexperiments,withthede-faultschedulerfromHuggingFaceTransformers,abatchsizeof32forRoBERTabasedmodelsandabatchsizeof4withagradientaccumulationstepof8forByT5basedmodels.Wetrainthetokenclassificationmodelsfor40epochsexceptforAra-paho,onwhichwetrain20epochs,andGitksan,onwhichwetrain2,000steps.WetraintheByT5basedmodelsfor20epochsonallofthedatamixedtogether.\n216"}, {"question": " What was the choice of optimizer used in the experiments described in the text?", "answer": " Adafactor was used as the optimizer in the experiments.,", "ref_chunk": "SigMoreFunSubmissiontotheSIGMORPHONSharedTaskonInterlinearGlossingTaiqiHe\u2217,LindiaTjuatja\u2217,NateRobinson,ShinjiWatanabe,DavidR.Mortensen,GrahamNeubig,LoriLevinLanguageTechnologiesInstituteCarnegieMellonUniversity{taiqih,ltjuatja,nrrobins,swatanab,dmortens,gneubig,lsl}@cs.cmu.eduAbstractInoursubmissiontotheSIGMORPHON2023SharedTaskoninterlinearglossing(IGT),weexploreapproachestodataaugmentationandmodelingacrosssevenlow-resourcelanguages.Fordataaugmentation,weexploretwoap-proaches:creatingartificialdatafromthepro-videdtrainingdataandutilizingexistingIGTresourcesinotherlanguages.Onthemodelingside,wetestanenhancedversionofthepro-videdtokenclassificationbaselineaswellasapretrainedmultilingualseq2seqmodel.Ad-ditionally,weapplypost-correctionusingadictionaryforGitksan,thelanguagewiththesmallestamountofdata.Wefindthatourtokenclassificationmodelsarethebestperforming,withthehighestword-levelaccuracyforAra-pahoandhighestmorpheme-levelaccuracyforGitksanoutofallsubmissions.Wealsoshowthatdataaugmentationisaneffectivestrategy,thoughapplyingartificialdatapretraininghasverydifferenteffectsacrossbothmodelstested.1IntroductionThispaperdescribestheSigMoreFunsubmissiontotheSIGMORPHON2023SharedTaskoninterlin-earglossing.Giveninputtextinatargetlanguage,thetaskistopredictthecorrespondinginterlineargloss(usingLeipzigglossingconventions).IGTisanimportantformoflinguisticannotationforthemorphologicalanalysisoflanguages,andalsoservesasanextremelyvaluableresourceforlan-guagedocumentationandeducationforspeakersoflow-resourcelanguages.Thereweretwotracksforthissharedtask,Track1(closed)andTrack2(open).ForTrack1,sys-temscouldonlybetrainedoninputsentencesandglosses;inTrack2,systemscouldmakeuseofthemorphologicalsegmentationoftheinputaswellasany(non-IGT)externalresources.SincetheTrack2settingbettermatchesthelong-termre-\n\u2217Theseauthorscontributedequallysearchgoalsofourteam,weonlyparticipateinthisopentrack.Inoursubmission,weinvestigatetwodifferentapproaches.First,weattemptdataaugmentationbyeithercreatingourownartificialglossdatabymanipulatingtheexistingtrainingdata,orbyuti-lizingexistingresourcescontainingIGTinotherlanguages(\u00a72).Second,weexploretwodifferentmodelsforglossgeneration(\u00a73).Thefirstbuildsoffthetokenclassificationbaseline,whilethesec-ondusesapretrainedmultilingualseq2seqmodel.Finally,wealsoattempttopost-correctmodeloutputswithadictionary.WeapplythistoGitk-sanandfindthatthis,combinedwithourotherapproaches,resultsinthehighestmorpheme-levelaccuracyforGitksaninTrack2.2DataAugmentationOnemajorchallengeforthissharedtaskisthescaleofdataprovided.Allofthelanguageshavelessthan40klinesoftrainingdata,andallbutArapahohavelessthan10k.Thesmallestdataset(Gitk-san)hasonly31linesofdata.Thus,oneobviousmethodtotryisdataaugmentation.Morespecif-ically,wetrypretrainingourmodelsondifferentformsofaugmenteddatabeforetrainingthemontheoriginaltargetlanguagedata.Weexploredtwoformsofdataaugmentation.First,wegeneratedartificialglossdatainthetar-getlanguagebyswappingwordsintheexistingtrainingdata.Second,weutilizeddatafromODIN(LewisandXia,2010;Xiaetal.,2014)toseeiftransferlearningfromdatainotherlanguagescanhelpimproveperformance.2.1ArtificialDataAchallengeourteamfacedwithrespecttodataaugmentationisfiguringouthowtoobtainaddi-tionaldatawhenwedonothavemuchknowledgeofthelanguages\u2019grammaticalsystems,alongwiththefactthattheselanguagesaregenerallyfrom\n209 Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 209\u2013216 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\ndigitallyunder-resourcedlanguagefamilies.Fur-thermore,wewantedoursolutiontobeeasilyim-plementedandrelativelylanguageagnosticduetotimeconstraintsandpracticalusabilityforre-searchersworkingonavarietyoflanguages.Thus,oneavenueofdataaugmentationwetriedwasbycreatingartificialdatafromtheprovidedtrainingdata.Thisrequiresnorule-writingorknowledgeofthegrammarofthelanguage,andthuscouldbeappliedquicklyandeasilytoallofthelanguagesinthesharedtask.Weusedanaiveword-swappingmethodtoran-domlyswapmorphemesthatoccurinsimilarcon-textstocreatenewsentences.Todothis,foreachglossline,wereplaceeachwordstem(thathasaglosslabelaffix)with\u201cSTEM\u201dtocreateaskeletongloss.Wenaivelydetermineifalabelisastembycheckingifitisinlowercase.Wedonotdothistowordsthatdonothaveaffixesas(withtheexceptionofUspanteko)wedonothaveaccesstopartsofspeech,anddonotwanttoswapwordsthatwouldcreateanungrammaticalsequence.Wecreateadictionarymappingeachskeletonwordglosstopossibleactualglosses,andmapeachactualglosstopossiblesurfaceforms(wemakenoassumptionsthatthesemappingsareone-to-one).Wethenrandomlysamplekrandomskeletonglosses(inthiscase,weusedkequaltoroughlythreetimestheamountoftrainingdata)andran-domlyfillinwordsthatmatchtheformatofskele-tonwordspresentintheline.(1)to(3)belowillustrateanexampleinthisprocess.Wecreateaskeletongloss(2)fromtheGitksansentencein(1)byreplacingtheallwordstemsthathaveanaffixwith\u201cSTEM\u201dinboththesegmentationandglosstiers\u2014inthiscase,only\u2019witxw-itappliestothisstep.Thentocreatetheartificialdatain(3),wereplacetheskeletonwordandcorrespondingglosswithanotherwordfromthetrainingdatathathasthesameskeletonform,inthiscasehahla\u2019lst-it.(1)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSP\u2019witxw-itcome-SX(2)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPSTEM-itSTEM-SX(3)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPhahla\u2019lst-itwork-SXWhilethismethodmaycreateasomewhatun-naturalinputsurfacesequence(asweareunabletocapturephonologicalchangesinthesurfaceformandcorrespondingtranslationsmaybenonsensi-cal),thismethodguaranteesthatthestructureoftheglossisanaturallyoccurringsequence(asweonlyuseglossskeletonsthatarepresentinthein-put).However,alimitationofthismethodisthatitdoesnotextendtoout-of-vocabularytokensorunseenglossstructures.Furthermore,aswecannotgenerateagold-standardtranslationfortheartifi-cialdata,wedonotmakeuseofatranslationintraining.2.2ODINAnotherpotentialavenuefordataaugmentationistransferlearningfromdatainotherlanguages,whichhasbeenshowntobeaneffectivemethodtoimproveperformanceinlow-resourcesettings(Ruderetal.,2019).TheavailableresourceweutilizeisODIN,ortheOnlineDatabaseforInterlinearText(LewisandXia,2010;Xiaetal.,2014).ODINcontains158,007linesofIGT,covering1,496languages.Weusethe2.1versionofODINdataandconvertthedatasettothesharedtaskformat,andfilteroutlanguageswithfewerthanfiveglossedsentences.However,thereremainssignificantnoiseinthedatasetthatcouldcausesignificantalignmentis-suesforthetokenclassificationmodels.ThereforeweopttoonlytraintheByT5modelsonODIN,inthehopethatthismodelislesssensitivetoalign-menterrors.Indeed,wefindthattheByT5modelfinetunedfirstonODINreceivesaperformanceboostwhenfinetunedagainonthesharedtaskdata.3ModelsWeexploretwomodelsforglossgeneration.Thefirstoneisbuiltuponthetokenclassificationbase-linewithsomeimprovements,andwetreatthismodelasourinternalbaseline.Thesecondmodelwedeploytestswhetherwecanachievecompeti-tiveperformancebyfinetuningapretrainedcharac-terbasedmultilingualandmultitaskmodel,ByT5.Forthismodel,weperformminimalpreprocess-inganduserawsegmentedmorphemesandfreetranslationsifavailable.3.1TokenClassificationTransformerWeusethebaselineTrack2modelprovidedbytheorganizersasastartingpoint.Theoriginalimple-mentationrandomlyinitializesatransformermodelfromthedefaultHuggingfaceRoBERTabasecon-figuration,andusesatokenclassificationobjective\n210\n\n\n1https://en.wikipedia.org/wiki/Lezgin_alphabets\nwithcross-entropyloss,whereeachglossistreatedasadistincttoken.Themorphemesandfreetrans-lationsaretokenizedbyspaceanddashes,withpunctuationspre-separated.TheyareconcatenatedandseparatedbytheSEPtokenandareusedastheinputstothemodel.WemodifytheoriginalTrack2baselinemodeltoobtainabetterbaseline.WeusepretrainedweightsfromXLM-RoBERTabase(Conneauetal.,2020),insteadofrandomlyinitializingtheweights.Wealsoslightlymodifythemorphemetokenizertoenforcethatthenumberofmorphemetokensmatchesthenumberofoutputglosstokensexactly.Additionally,weintroducetheCOPYtokentoreplacetheglossifitmatchesthecorrespondingmorphemeexactly.AnexamplefromNatuguisshowningloss(4):(4)67COPY.COPYmnc-xbe-1MINIMzloCOPYSkulCOPYWebelievethiswouldimproveperformancebyremovingtheneedtomemorizeglossedcode-switchingandpropernouns,thoughitisonlyef-fectiveifthecode-switchedlanguageisthesameasthematrixlanguage(e.g.Arapaho),andwouldhavenoeffectifthesourcelanguageusesadif-ferentorthographyoriscode-switchedtoanotherlanguage,wheretheglosswouldnotmatchedthemorphemeformexactly.Thismethodalsocom-pressesallpunctuationmarkersintoonetoken,buttheusefulnessofthissideeffectislessclear.Sinceweareusingpretrainedweights,itisthennaturaltoexploreintegratingthepretrainedtok-enizer.SinceXLM-RoBERTawasnottrainedonanyofthesourcelanguages,itmakesthemostsensetoonlyusethepretrainedtokenizertotok-enizefreetranslations,iftheyareavailable,andextendthevocabularytoincludemorphemes.3.2FinetunedByT5Multi-taskandmulti-lingualpretrainedlargelan-guagemodelshavebeenshowntobeeffectiveformanytasks.Weexplorewhethersuchmodelscanbeusedeffectivelyforglossing.Weconductex-perimentswithbothmT5(Xueetal.,2021)andByT5(Xueetal.,2022),butByT5ispreferredbecauseittakesrawtexts(bytesorcharacters)asinputsandintheoryshouldbemoreeffectiveforunseenlanguages.Weuseapromptbasedmultilin-gualsequencetosequenceobjectiveforbothmod-els.Theprompttemplateis:\u201cGenerateinterlin-earglossfrom[sourcelanguage]:[segmentedmorphemes]withits[matrixlanguage]trans-lation:[freetranslation]Answer:\u201d.Datafromalllanguagesaremixedtogetherandshuffled,withnoupordownsampling.Afterinitialexperi-ments,wefindByT5outperformsmT5acrossalllanguages,andthereforeweonlyconductsubse-quentexperimentsonByT5andreportthosere-sults.Uponinitialexperiments,wealsofindtheresultsforLezgitobelowerthanexpected.Wehypothe-sizethatthefactthatthedataareinCyrillicscriptcausesthisdeficiency,sinceByT5wastrainedonfarlessCyrillicdatathandataintheLatinscript.Thereforewecreateanautomaticromanizationtool,sourcedfromWikipedia1andintegratedintheEpitranpackage(Mortensenetal.,2018),andconvertallLezgidatatoLatinscriptforByT5fine-tuning.AfterinspectingtheoutputsoftheByT5models,wefindcaseswherepunctuationsareattachedtothepreviousglosses,insteadofbeingseparatedbyaspaceasisstandardinthetrainingsets.Thisisprobablyduetothefactthatthemodelwaspre-trainedonuntokenizeddataandthisbehaviorispreserveddespitefinetuningontokenizeddata.Wethereforeuseasimpleregularexpressionbasedtok-enizertofixtheinconsistencies.WenoticethattheprocedureonlygivesperformanceboostonGitk-san,Lezgi,Uspanteko,andNatugu,andsoweonlyapplytheproceduretothoselanguages,leavingtherestoftheoutputsunchanged.4DictionaryPost-correction:GitksanOneofthekeychallengesforextremelylowre-sourcelanguagesistheintegrationofstructuredlinguisticdatainotherforms,suchasadictionary,intomachinelearningpipelines.Wetestasimplepost-correctionmethodfromapre-existingdictio-naryonGitksanonly,duetoitsuniquecombinationoflowresourceandeasilyobtainabledictionaryinmachinereadableform.Weusethedictionarycom-piledbyForbesetal.(2021),withoutconsultingthemorphologicalanalyzersthattheyalsoprovided.Atinferencetime,ifamorphemeisunseendur-ingtraining,wesearchfortheexactforminthedictionary.Wealsoexpandthesearchtoallsubse-quencesofmorphemeswithintheenclosingword,plusthepreviouswholewordincaseswhereapar-ticleisincludedinthedictionaryform.Thefirst\n211\n\n\nmatcheddefinitionisusedastheglossandifnoneofthesearchyieldsanexactmatch,wefallbacktothemodelprediction.Weonlyapplythismethodtothetokenclassificationmodelsbecausethealign-mentbetweenmorphemesandglossesisdirectlyestablished,whereastheseq2seqmodelsdonotguaranteethatthenumberofglossesmatchesthenumberofmorphemes.5ResultsandDiscussionTables1and2showoursystems\u2019performance(aswellastheoriginalbaseline)onthetestdatawithre-specttoword-andmorpheme-levelmicro-averagedaccuracy,respectively.Overall,thetokenclassifica-tionmodeltrainedfirstontheartificiallygeneratedaugmenteddataperformthebest,withthemodeltrainedonthesharedtaskdataonlynotfarbehind.Meanwhile,ByT5modelsperformworse,withthemodelfinetunedfirstonODINtrailingourbestmodelbyafewpercentagepoints,whilethemodelfinetunedfirstonaugmenteddataperformsworsethanthebaseline.5.1DataAugmentationOverall,wefinddataaugmentationtobeuseful.Withartificiallygenerateddata,weseetheeffectsareperhapsgreatestforthemid-resourcelanguages(ddo,lez,ntu,nyb,usp),whilethehighestandlowestresourcedlanguagesdidnotreceivemuchbenefitfrompretrainingontheartificialdata.Wethinkthisisperhapsbecausethereisa\u201csweetspot\u201dwithrespecttotheamountofdatathatisrequiredtotrainamodel.Ifthereisenoughdataalready,inthecaseofArapaho,thenthenoisinessofartificialdatawouldout-weightthebenefitoftrainingonthem.Ontheotherendofthescale,Gitksanperhapsneedsmoresyntheticdatafordataaugmentationtoyieldmeaningfulimprovements.ForByT5models,artificiallygenerateddataseemtohavetheoppositeeffect,whereperfor-manceissignificantlydegraded.Aspeculationforthiseffectisthefactthepretrainedmodelismoresemanticallyaware,andsincetheartificiallygen-eratedsentencescouldbenonsensical,themodelcouldbecomeconfused.Ontheotherhand,pre-trainingonODINyieldsimprovementsforthema-jorityofthelanguages2.ThisisencouragingsincewedidnotperformmuchpreprocessingforODIN,\n2TsezistheonlylanguagethatappearedinODIN(68sentences).Wedidnotremoveitfromthecorpusbutthisshouldhavelittleinfluenceontheperformancebecausethesizeofthedatasetisverysmall.andthereisdefinitelystillroomtomakethedatacleanerandmoreinternallyconsistent,whichinturnshouldresultinabettermodel.5.2ChoiceofHyperparametersWefindthechoiceofhyperparametersofthetokenclassificationmodelstobenecessarilylanguageanddatasetspecific.ArapahoandGitksaninpar-ticularneedspecialattention,wherethenumberoftrainingepochsneedtobeadjustedfortheveryhighandlowdatasize.WealsodevelopedmostoftheoptimizationonthetokenclassificationmodelonArapaho.However,wedidnothavetimetopropagatethechanges(usingpretrainedtokenizer,savingthelastmodelinsteadofthemodelwiththelowestvalidationloss)totherestoflanguagessinceinitialexperimentshowedthatpretrainedtokeniz-ersdidnotimproveontheotherlanguages.How-ever,afterthesubmissiondeadlineisconcluded,weranmoreexperimentsanddiscoveredthataddingpretrainedtokenizersrequiresmoretrainingsteps,andthetrainingisbettercontrolledbyspecifyingthetrainingstepsinsteadofepochs.Wedonotin-cludethoselatestexperimentsinthispaper,butourtokenclassificationmodelshavethepotentialtoperformbetterwithmorehyperparametertuning.5.3In-VersusOut-of-VocabularyErrorsOnedimensionoferroranalysisweinvestigatedwaswhatproportionofoursystems\u2019errorscomefrommorphemesorwordsthatareeitherinoroutofthetrainingdatavocabulary.Wecountamor-phemeorwordasin-vocabularyifthesurfaceformanditscorrespondingglossco-occurinthepro-videdtrainingdata(notincludingthedevelopmentdata,asourmodelsareonlytrainedonthetrainset).NotethatthereisamuchlargerproportionofOOVwordsasopposedtomorphemesduetothefactthatanunseenwordcanbecomposedofdifferentcombinationsofseenmorphemes.Table3showstheproportionofmorphemesandwordsthatareout-of-vocab(OOV)withinthetestset.Whilenearlyallthelanguageshavelessthan10%oftheirmorphemesclassifiedasOOV,GitksannotablyhasarelativelylargeportionofOOVtestdata,with\u224845%ofmorphemesand\u224878%ofwordsbeingOOV.Tables4and5showourmodels\u2019performancesonin-versesout-of-vocabtokensatthemorphemeandwordlevels,respectively.Whilewewouldintuitivelyexpectthatword-levelOOVaccuracybeaboutthesameorworsethanmorpheme-levelOOV\n212\n\n\nTable2:Morpheme-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselinewithartificialpretraininganddictionarypost-correctionisthehighestGitksanaccuracyreportedoutofallsharedtasksubmissions.\nxlmr-base91.3684.3547.47/52.8280.1788.3585.8480.0880.42xlmr-aug89.3488.1546.89/52.3982.3685.5389.4983.0881.48\nTable3:ProportionofmorphemesandwordsthatareOOVwithinthetestset.accuracy,thisisnotthecaseduetothefactthatalargeportionofout-of-vocabwordsareformedwithin-vocabmorphemes.Formostlanguages,withtheexceptionofGitksan,thereappearstobeatrade-offbetweenbetterin-vocabmorphemeperformancewithXLMRandperformanceout-of-vocabwithByT5.6RelatedWorkTherehavebeenavarietyofapproachestotheprob-lemof(semi-)automaticallygeneratinginterlineargloss.BaldridgeandPalmer(2009)investigatetheefficacyofactivelearningforthetaskofinterlinearglossing,usingannotationtimerequiredbyexpertandnon-expertannotatorsastheirmetric.Thesys-temtheyusetogenerateglosslabelsuggestionsisastandardmaximumentropyclassifier.Arule-basedapproachbySnoeketal.(2014)utilizesanFSTtogenerateglossesforPlainsCree,focusingonnouns.Samard\u017ei\u00b4cetal.(2015)viewthetaskofglossingsegmentedtextasatwo-stepprocess,firsttreatingitasastandardPOStaggingtaskandthenaddinglexicalglossesfromadictio-nary.TheydemonstratethismethodonaChintangcorpusofabout1.2millionwords.Anumberofotherworksfocusingoninterlinearglossingutilizeconditionalrandomfield(CRF)models.MoellerandHulden(2018)testthreedifferentmodelsonaverysmallLezgidataset(<3000words):aCRF(thatoutputsBIOlabelswiththecorrespondingglosspercharacterinthein-put),asegmentationandlabellingpipelinethatuti-lizesaCRF(forBIOlabels)andSVM(forglossla-bels),andanLSTMseq2seqmodel.TheyfindthattheCRFthatjointlyproducestheBIOlabelsandtagsproducedthebestresults.McMillan-Major(2020)utilizestranslationsintheirtrainingdatabycreatingtwoCRFmodels,onethatpredictsglossfromthesegmentedinputandanotherthanpre-\nModelarpddogitlezntunybuspAVG\nModelarpddogitlezntunybuspAVG\nbyt5-base78.8680.3214.8460.72b76.6776.7377.2166.48byt5-aug73.2762.374.1738.6055.1169.2570.8553.38byt5-odin80.5682.7920.5763.7777.9782.5975.7269.14\nbaseline91.1185.3425.3351.8249.0388.7182.4867.69\nbaseline85.4475.7116.4134.5441.0884.3076.5559.14\nxlmr-base85.8773.7727.86/34.11a74.1582.9980.6173.4772.14xlmr-aug82.9280.0724.74/31.2577.7778.7285.5377.5173.39\naWereportbefore/afterdictionarybasedpost-correctionforGitksan.bWetrainedthismodelwithoutromanizingLezgi.Table1:Word-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselineisthehighestArapahoaccuracyreportedoutofallsharedtasksubmissions.\nbyt5-base78.8275.7712.5944.1062.4078.9774.2560.99byt5-aug72.1057.932.6026.2435.6270.0167.7347.46byt5-odin80.8178.2412.7450.0063.3985.3073.2563.39\nMorph0.0430.0090.4500.0560.0340.0190.070\narpddogitlezntunybusp\nWord0.2420.1550.7810.1690.2140.0840.200\n213\n\n\nbyt5-aug74.7658.243.4240.2736.5471.2770.56\n35.48\nTable4:Morpheme-levelaccuracyoveralltokensofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.\n24.67\n12.31\nxlmr-base95.2085.1282.8984.7990.8787.4686.05\n23.24\n45.07\nxlmr-aug93.7283.8594.0587.6489.2490.8191.11\n29.33\n24.10\n49.79\nbyt5-odin91.9387.6663.1073.7885.9387.6083.46\n59.51\n23.54\nbyt5-odin83.4778.5518.4262.9064.3886.8575.23\n18.26\n22.41\n49.17\nTable5:Word-levelaccuracyofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.dictsfromthetranslation,andthenusesheuristicstodeterminewhichmodeltoselectfromforeachmorpheme.BarrigaMart\u00ednezetal.(2021)usedaCRFmodeltoachieve>90%accuracyforgloss-ingOtomiandfindthatitworksbetterthananRNN,whichiscomputationallymoreexpensive.Otherworks,includingoursystems,haveturnedtoneuralmethods.Kondratyuk(2019)leveragespretrainedmultilingualBERTtoencodeinputsentences,thenapplyadditionalword-levelandcharacter-levelLSTMlayersbeforejointlydecod-inglemmasandmorphologytagsusingsimplese-quencetagginglayers.Furthermore,theyshowthattwo-stagetrainingbyfirsttrainingonalllan-guagesfollowedbytrainingonthetargetlanguageismoreeffectivethantrainingthesystemonthetargetlanguagealone.AnapproachbyZhaoetal.(2020),likeMcMillan-Major(2020),makesuseoftranslationsavailableinparallelcorpora,butdosobyusingamulti-sourcetransformermodel.Theyalsoincorporatelengthcontrolandalignmentdur-inginferencetoenhancetheirmodel,andtesttheirsystemonArapaho,Tsez,andLezgi.7ConclusionInoursharedtasksubmission,weexploredataaug-mentationmethodsandmodelingstrategiesforthetaskofinterlinearglossinginsevenlow-resourcelanguages.Ourbestperformingmodelsareto-kenclassificationmodelsusingXLMR.Wedemon-stratethatpretrainingonartificialdatawithXLMRisaneffectivetechniqueforthemid-resourcetestlanguages.Additionally,inourerroranalysiswefindthatwemayhaveactuallyundertrainedourtokenclassificationmodels,andthusoursystemsmayhavethepotentialtoperformbetterwithad-ditionalhyperparametertuning.WhileourByT5modelsdidnotperformaswellasourothersys-tems,weshowthatpretrainingonODINdataiseffective,despitethisdatabeingverynoisy.Finally,wealsodemonstrateimprovementsbyutilizingadictionarytopost-correctmodeloutputsforGitk-san.\n0.00\n0.00\n0.00\n0.00\n11.24\n0.82\n12.86\n14.52\n23.60\n4.97\n56.36\n45.65\n21.14\n43.37\n1.61\n47.52\n46.94\n29.69\n13.67\nxlmr-base95.9378.1895.2384.2493.1485.8586.27\n5.79\n28.04\n2.00\n17.00\n0.41\n16.08\n14.67\n19.35\nModelarpddogitlezntunybusp\nModelarpddogitlezntunybusp\n7.49\n28.09\n28.09\n44.81\n3.23\n3.23\n54.44\n2.33\nbyt5-aug87.2268.6910.7146.0665.1374.5981.44\n30.20\nxlmr-aug92.9888.9484.7487.1087.8891.1789.31\n48.70\n40.00\n28.63\n8.67\n2.60\n2.60\n9.68\n214\n\n\nAcknowledgementsThisworkwassupportedbyNSFCISERIgrantnumber2211951,FromAcousticSignaltoMor-phosyntacticAnalysisinoneEnd-to-EndNeuralSystem.ReferencesJasonBaldridgeandAlexisPalmer.2009.Howwelldoesactivelearningactuallywork?Time-basedeval-uationofcost-reductionstrategiesforlanguagedocu-mentation.InProceedingsofthe2009ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages296\u2013305,Singapore.AssociationforCompu-tationalLinguistics.DiegoBarrigaMart\u00ednez,VictorMijangos,andXi-menaGutierrez-Vasques.2021.Automaticinterlin-earglossingforOtomilanguage.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages34\u201343,Online.AssociationforComputationalLin-guistics.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.ClarissaForbes,GarrettNicolai,andMiikkaSilfverberg.2021.AnFSTmorphologicalanalyzerforthegitksanlanguage.InProceedingsofthe18thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages188\u2013197,Online.AssociationforComputationalLinguistics.DanKondratyuk.2019.Cross-linguallemmatizationandmorphologytaggingwithtwo-stagemultilin-gualBERTfine-tuning.InProceedingsofthe16thWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages12\u201318,Florence,Italy.AssociationforComputationalLinguistics.WilliamD.LewisandFeiXia.2010.DevelopingODIN:AMultilingualRepositoryofAnnotatedLanguageDataforHundredsoftheWorld\u2019sLanguages.Liter-aryandLinguisticComputing,25(3):303\u2013319.AngelinaMcMillan-Major.2020.Automatingglossgenerationininterlinearglossedtext.ProceedingsoftheSocietyforComputationinLinguistics,3(1):338\u2013349.SarahMoellerandMansHulden.2018.Automaticglossinginalow-resourcesettingforlanguagedoc-umentation.InProceedingsoftheWorkshoponComputationalModelingofPolysyntheticLanguages,pages84\u201393.DavidR.Mortensen,SiddharthDalmia,andPatrickLittell.2018.Epitran:PrecisionG2Pformanylan-guages.InProceedingsoftheEleventhInternationalConferenceonLanguageResourcesandEvaluation(LREC2018),Miyazaki,Japan.EuropeanLanguageResourcesAssociation(ELRA).SebastianRuder,MatthewEPeters,SwabhaSwayamdipta,andThomasWolf.2019.Transferlearninginnaturallanguageprocessing.InProceed-ingsofthe2019conferenceoftheNorthAmericanchapteroftheassociationforcomputationallinguistics:Tutorials,pages15\u201318.TanjaSamard\u017ei\u00b4c,RobertSchikowski,andSabineStoll.2015.Automaticinterlinearglossingastwo-levelsequenceclassification.NoamShazeerandMitchellStern.2018.Adafactor:Adaptivelearningrateswithsublinearmemorycost.InProceedingsofthe35thInternationalConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,pages4596\u20134604.PMLR.ConorSnoek,DorothyThunder,KaidiL\u00f5o,AnttiArppe,JordanLachler,SjurMoshagen,andTrondTrosterud.2014.ModelingthenounmorphologyofPlainsCree.InProceedingsofthe2014WorkshopontheUseofComputationalMethodsintheStudyofEndangeredLanguages,pages34\u201342,Baltimore,Maryland,USA.AssociationforComputationalLinguistics.FeiXia,WilliamLewis,MichaelWayneGoodman,JoshuaCrowgey,andEmilyM.Bender.2014.En-richingODIN.InProceedingsoftheNinthInter-nationalConferenceonLanguageResourcesandEvaluation(LREC\u201914),pages3151\u20133157,Reykjavik,Iceland.EuropeanLanguageResourcesAssociation(ELRA).LintingXue,AdityaBarua,NoahConstant,RamiAl-Rfou,SharanNarang,MihirKale,AdamRoberts,andColinRaffel.2022.ByT5:Towardsatoken-freefuturewithpre-trainedbyte-to-bytemodels.Transac-tionsoftheAssociationforComputationalLinguis-tics,10:291\u2013306.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.XingyuanZhao,SatoruOzaki,AntoniosAnastasopou-los,GrahamNeubig,andLoriLevin.2020.Auto-maticinterlinearglossingforunder-resourcedlan-guagesleveragingtranslations.InProceedingsofthe28thInternationalConferenceonComputationalLinguistics,pages5397\u20135408,Barcelona,Spain(On-line).InternationalCommitteeonComputationalLin-guistics.\n215\n\n\nAHyperparameterSettingsWeuseAdafactor(ShazeerandStern,2018)astheoptimizeracrossallexperiments,withthede-faultschedulerfromHuggingFaceTransformers,abatchsizeof32forRoBERTabasedmodelsandabatchsizeof4withagradientaccumulationstepof8forByT5basedmodels.Wetrainthetokenclassificationmodelsfor40epochsexceptforAra-paho,onwhichwetrain20epochs,andGitksan,onwhichwetrain2,000steps.WetraintheByT5basedmodelsfor20epochsonallofthedatamixedtogether.\n216"}], "doc_text": "SigMoreFunSubmissiontotheSIGMORPHONSharedTaskonInterlinearGlossingTaiqiHe\u2217,LindiaTjuatja\u2217,NateRobinson,ShinjiWatanabe,DavidR.Mortensen,GrahamNeubig,LoriLevinLanguageTechnologiesInstituteCarnegieMellonUniversity{taiqih,ltjuatja,nrrobins,swatanab,dmortens,gneubig,lsl}@cs.cmu.eduAbstractInoursubmissiontotheSIGMORPHON2023SharedTaskoninterlinearglossing(IGT),weexploreapproachestodataaugmentationandmodelingacrosssevenlow-resourcelanguages.Fordataaugmentation,weexploretwoap-proaches:creatingartificialdatafromthepro-videdtrainingdataandutilizingexistingIGTresourcesinotherlanguages.Onthemodelingside,wetestanenhancedversionofthepro-videdtokenclassificationbaselineaswellasapretrainedmultilingualseq2seqmodel.Ad-ditionally,weapplypost-correctionusingadictionaryforGitksan,thelanguagewiththesmallestamountofdata.Wefindthatourtokenclassificationmodelsarethebestperforming,withthehighestword-levelaccuracyforAra-pahoandhighestmorpheme-levelaccuracyforGitksanoutofallsubmissions.Wealsoshowthatdataaugmentationisaneffectivestrategy,thoughapplyingartificialdatapretraininghasverydifferenteffectsacrossbothmodelstested.1IntroductionThispaperdescribestheSigMoreFunsubmissiontotheSIGMORPHON2023SharedTaskoninterlin-earglossing.Giveninputtextinatargetlanguage,thetaskistopredictthecorrespondinginterlineargloss(usingLeipzigglossingconventions).IGTisanimportantformoflinguisticannotationforthemorphologicalanalysisoflanguages,andalsoservesasanextremelyvaluableresourceforlan-guagedocumentationandeducationforspeakersoflow-resourcelanguages.Thereweretwotracksforthissharedtask,Track1(closed)andTrack2(open).ForTrack1,sys-temscouldonlybetrainedoninputsentencesandglosses;inTrack2,systemscouldmakeuseofthemorphologicalsegmentationoftheinputaswellasany(non-IGT)externalresources.SincetheTrack2settingbettermatchesthelong-termre-\n\u2217Theseauthorscontributedequallysearchgoalsofourteam,weonlyparticipateinthisopentrack.Inoursubmission,weinvestigatetwodifferentapproaches.First,weattemptdataaugmentationbyeithercreatingourownartificialglossdatabymanipulatingtheexistingtrainingdata,orbyuti-lizingexistingresourcescontainingIGTinotherlanguages(\u00a72).Second,weexploretwodifferentmodelsforglossgeneration(\u00a73).Thefirstbuildsoffthetokenclassificationbaseline,whilethesec-ondusesapretrainedmultilingualseq2seqmodel.Finally,wealsoattempttopost-correctmodeloutputswithadictionary.WeapplythistoGitk-sanandfindthatthis,combinedwithourotherapproaches,resultsinthehighestmorpheme-levelaccuracyforGitksaninTrack2.2DataAugmentationOnemajorchallengeforthissharedtaskisthescaleofdataprovided.Allofthelanguageshavelessthan40klinesoftrainingdata,andallbutArapahohavelessthan10k.Thesmallestdataset(Gitk-san)hasonly31linesofdata.Thus,oneobviousmethodtotryisdataaugmentation.Morespecif-ically,wetrypretrainingourmodelsondifferentformsofaugmenteddatabeforetrainingthemontheoriginaltargetlanguagedata.Weexploredtwoformsofdataaugmentation.First,wegeneratedartificialglossdatainthetar-getlanguagebyswappingwordsintheexistingtrainingdata.Second,weutilizeddatafromODIN(LewisandXia,2010;Xiaetal.,2014)toseeiftransferlearningfromdatainotherlanguagescanhelpimproveperformance.2.1ArtificialDataAchallengeourteamfacedwithrespecttodataaugmentationisfiguringouthowtoobtainaddi-tionaldatawhenwedonothavemuchknowledgeofthelanguages\u2019grammaticalsystems,alongwiththefactthattheselanguagesaregenerallyfrom\n209 Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 209\u2013216 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\ndigitallyunder-resourcedlanguagefamilies.Fur-thermore,wewantedoursolutiontobeeasilyim-plementedandrelativelylanguageagnosticduetotimeconstraintsandpracticalusabilityforre-searchersworkingonavarietyoflanguages.Thus,oneavenueofdataaugmentationwetriedwasbycreatingartificialdatafromtheprovidedtrainingdata.Thisrequiresnorule-writingorknowledgeofthegrammarofthelanguage,andthuscouldbeappliedquicklyandeasilytoallofthelanguagesinthesharedtask.Weusedanaiveword-swappingmethodtoran-domlyswapmorphemesthatoccurinsimilarcon-textstocreatenewsentences.Todothis,foreachglossline,wereplaceeachwordstem(thathasaglosslabelaffix)with\u201cSTEM\u201dtocreateaskeletongloss.Wenaivelydetermineifalabelisastembycheckingifitisinlowercase.Wedonotdothistowordsthatdonothaveaffixesas(withtheexceptionofUspanteko)wedonothaveaccesstopartsofspeech,anddonotwanttoswapwordsthatwouldcreateanungrammaticalsequence.Wecreateadictionarymappingeachskeletonwordglosstopossibleactualglosses,andmapeachactualglosstopossiblesurfaceforms(wemakenoassumptionsthatthesemappingsareone-to-one).Wethenrandomlysamplekrandomskeletonglosses(inthiscase,weusedkequaltoroughlythreetimestheamountoftrainingdata)andran-domlyfillinwordsthatmatchtheformatofskele-tonwordspresentintheline.(1)to(3)belowillustrateanexampleinthisprocess.Wecreateaskeletongloss(2)fromtheGitksansentencein(1)byreplacingtheallwordstemsthathaveanaffixwith\u201cSTEM\u201dinboththesegmentationandglosstiers\u2014inthiscase,only\u2019witxw-itappliestothisstep.Thentocreatetheartificialdatain(3),wereplacetheskeletonwordandcorrespondingglosswithanotherwordfromthetrainingdatathathasthesameskeletonform,inthiscasehahla\u2019lst-it.(1)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSP\u2019witxw-itcome-SX(2)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPSTEM-itSTEM-SX(3)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPhahla\u2019lst-itwork-SXWhilethismethodmaycreateasomewhatun-naturalinputsurfacesequence(asweareunabletocapturephonologicalchangesinthesurfaceformandcorrespondingtranslationsmaybenonsensi-cal),thismethodguaranteesthatthestructureoftheglossisanaturallyoccurringsequence(asweonlyuseglossskeletonsthatarepresentinthein-put).However,alimitationofthismethodisthatitdoesnotextendtoout-of-vocabularytokensorunseenglossstructures.Furthermore,aswecannotgenerateagold-standardtranslationfortheartifi-cialdata,wedonotmakeuseofatranslationintraining.2.2ODINAnotherpotentialavenuefordataaugmentationistransferlearningfromdatainotherlanguages,whichhasbeenshowntobeaneffectivemethodtoimproveperformanceinlow-resourcesettings(Ruderetal.,2019).TheavailableresourceweutilizeisODIN,ortheOnlineDatabaseforInterlinearText(LewisandXia,2010;Xiaetal.,2014).ODINcontains158,007linesofIGT,covering1,496languages.Weusethe2.1versionofODINdataandconvertthedatasettothesharedtaskformat,andfilteroutlanguageswithfewerthanfiveglossedsentences.However,thereremainssignificantnoiseinthedatasetthatcouldcausesignificantalignmentis-suesforthetokenclassificationmodels.ThereforeweopttoonlytraintheByT5modelsonODIN,inthehopethatthismodelislesssensitivetoalign-menterrors.Indeed,wefindthattheByT5modelfinetunedfirstonODINreceivesaperformanceboostwhenfinetunedagainonthesharedtaskdata.3ModelsWeexploretwomodelsforglossgeneration.Thefirstoneisbuiltuponthetokenclassificationbase-linewithsomeimprovements,andwetreatthismodelasourinternalbaseline.Thesecondmodelwedeploytestswhetherwecanachievecompeti-tiveperformancebyfinetuningapretrainedcharac-terbasedmultilingualandmultitaskmodel,ByT5.Forthismodel,weperformminimalpreprocess-inganduserawsegmentedmorphemesandfreetranslationsifavailable.3.1TokenClassificationTransformerWeusethebaselineTrack2modelprovidedbytheorganizersasastartingpoint.Theoriginalimple-mentationrandomlyinitializesatransformermodelfromthedefaultHuggingfaceRoBERTabasecon-figuration,andusesatokenclassificationobjective\n210\n\n\n1https://en.wikipedia.org/wiki/Lezgin_alphabets\nwithcross-entropyloss,whereeachglossistreatedasadistincttoken.Themorphemesandfreetrans-lationsaretokenizedbyspaceanddashes,withpunctuationspre-separated.TheyareconcatenatedandseparatedbytheSEPtokenandareusedastheinputstothemodel.WemodifytheoriginalTrack2baselinemodeltoobtainabetterbaseline.WeusepretrainedweightsfromXLM-RoBERTabase(Conneauetal.,2020),insteadofrandomlyinitializingtheweights.Wealsoslightlymodifythemorphemetokenizertoenforcethatthenumberofmorphemetokensmatchesthenumberofoutputglosstokensexactly.Additionally,weintroducetheCOPYtokentoreplacetheglossifitmatchesthecorrespondingmorphemeexactly.AnexamplefromNatuguisshowningloss(4):(4)67COPY.COPYmnc-xbe-1MINIMzloCOPYSkulCOPYWebelievethiswouldimproveperformancebyremovingtheneedtomemorizeglossedcode-switchingandpropernouns,thoughitisonlyef-fectiveifthecode-switchedlanguageisthesameasthematrixlanguage(e.g.Arapaho),andwouldhavenoeffectifthesourcelanguageusesadif-ferentorthographyoriscode-switchedtoanotherlanguage,wheretheglosswouldnotmatchedthemorphemeformexactly.Thismethodalsocom-pressesallpunctuationmarkersintoonetoken,buttheusefulnessofthissideeffectislessclear.Sinceweareusingpretrainedweights,itisthennaturaltoexploreintegratingthepretrainedtok-enizer.SinceXLM-RoBERTawasnottrainedonanyofthesourcelanguages,itmakesthemostsensetoonlyusethepretrainedtokenizertotok-enizefreetranslations,iftheyareavailable,andextendthevocabularytoincludemorphemes.3.2FinetunedByT5Multi-taskandmulti-lingualpretrainedlargelan-guagemodelshavebeenshowntobeeffectiveformanytasks.Weexplorewhethersuchmodelscanbeusedeffectivelyforglossing.Weconductex-perimentswithbothmT5(Xueetal.,2021)andByT5(Xueetal.,2022),butByT5ispreferredbecauseittakesrawtexts(bytesorcharacters)asinputsandintheoryshouldbemoreeffectiveforunseenlanguages.Weuseapromptbasedmultilin-gualsequencetosequenceobjectiveforbothmod-els.Theprompttemplateis:\u201cGenerateinterlin-earglossfrom[sourcelanguage]:[segmentedmorphemes]withits[matrixlanguage]trans-lation:[freetranslation]Answer:\u201d.Datafromalllanguagesaremixedtogetherandshuffled,withnoupordownsampling.Afterinitialexperi-ments,wefindByT5outperformsmT5acrossalllanguages,andthereforeweonlyconductsubse-quentexperimentsonByT5andreportthosere-sults.Uponinitialexperiments,wealsofindtheresultsforLezgitobelowerthanexpected.Wehypothe-sizethatthefactthatthedataareinCyrillicscriptcausesthisdeficiency,sinceByT5wastrainedonfarlessCyrillicdatathandataintheLatinscript.Thereforewecreateanautomaticromanizationtool,sourcedfromWikipedia1andintegratedintheEpitranpackage(Mortensenetal.,2018),andconvertallLezgidatatoLatinscriptforByT5fine-tuning.AfterinspectingtheoutputsoftheByT5models,wefindcaseswherepunctuationsareattachedtothepreviousglosses,insteadofbeingseparatedbyaspaceasisstandardinthetrainingsets.Thisisprobablyduetothefactthatthemodelwaspre-trainedonuntokenizeddataandthisbehaviorispreserveddespitefinetuningontokenizeddata.Wethereforeuseasimpleregularexpressionbasedtok-enizertofixtheinconsistencies.WenoticethattheprocedureonlygivesperformanceboostonGitk-san,Lezgi,Uspanteko,andNatugu,andsoweonlyapplytheproceduretothoselanguages,leavingtherestoftheoutputsunchanged.4DictionaryPost-correction:GitksanOneofthekeychallengesforextremelylowre-sourcelanguagesistheintegrationofstructuredlinguisticdatainotherforms,suchasadictionary,intomachinelearningpipelines.Wetestasimplepost-correctionmethodfromapre-existingdictio-naryonGitksanonly,duetoitsuniquecombinationoflowresourceandeasilyobtainabledictionaryinmachinereadableform.Weusethedictionarycom-piledbyForbesetal.(2021),withoutconsultingthemorphologicalanalyzersthattheyalsoprovided.Atinferencetime,ifamorphemeisunseendur-ingtraining,wesearchfortheexactforminthedictionary.Wealsoexpandthesearchtoallsubse-quencesofmorphemeswithintheenclosingword,plusthepreviouswholewordincaseswhereapar-ticleisincludedinthedictionaryform.Thefirst\n211\n\n\nmatcheddefinitionisusedastheglossandifnoneofthesearchyieldsanexactmatch,wefallbacktothemodelprediction.Weonlyapplythismethodtothetokenclassificationmodelsbecausethealign-mentbetweenmorphemesandglossesisdirectlyestablished,whereastheseq2seqmodelsdonotguaranteethatthenumberofglossesmatchesthenumberofmorphemes.5ResultsandDiscussionTables1and2showoursystems\u2019performance(aswellastheoriginalbaseline)onthetestdatawithre-specttoword-andmorpheme-levelmicro-averagedaccuracy,respectively.Overall,thetokenclassifica-tionmodeltrainedfirstontheartificiallygeneratedaugmenteddataperformthebest,withthemodeltrainedonthesharedtaskdataonlynotfarbehind.Meanwhile,ByT5modelsperformworse,withthemodelfinetunedfirstonODINtrailingourbestmodelbyafewpercentagepoints,whilethemodelfinetunedfirstonaugmenteddataperformsworsethanthebaseline.5.1DataAugmentationOverall,wefinddataaugmentationtobeuseful.Withartificiallygenerateddata,weseetheeffectsareperhapsgreatestforthemid-resourcelanguages(ddo,lez,ntu,nyb,usp),whilethehighestandlowestresourcedlanguagesdidnotreceivemuchbenefitfrompretrainingontheartificialdata.Wethinkthisisperhapsbecausethereisa\u201csweetspot\u201dwithrespecttotheamountofdatathatisrequiredtotrainamodel.Ifthereisenoughdataalready,inthecaseofArapaho,thenthenoisinessofartificialdatawouldout-weightthebenefitoftrainingonthem.Ontheotherendofthescale,Gitksanperhapsneedsmoresyntheticdatafordataaugmentationtoyieldmeaningfulimprovements.ForByT5models,artificiallygenerateddataseemtohavetheoppositeeffect,whereperfor-manceissignificantlydegraded.Aspeculationforthiseffectisthefactthepretrainedmodelismoresemanticallyaware,andsincetheartificiallygen-eratedsentencescouldbenonsensical,themodelcouldbecomeconfused.Ontheotherhand,pre-trainingonODINyieldsimprovementsforthema-jorityofthelanguages2.ThisisencouragingsincewedidnotperformmuchpreprocessingforODIN,\n2TsezistheonlylanguagethatappearedinODIN(68sentences).Wedidnotremoveitfromthecorpusbutthisshouldhavelittleinfluenceontheperformancebecausethesizeofthedatasetisverysmall.andthereisdefinitelystillroomtomakethedatacleanerandmoreinternallyconsistent,whichinturnshouldresultinabettermodel.5.2ChoiceofHyperparametersWefindthechoiceofhyperparametersofthetokenclassificationmodelstobenecessarilylanguageanddatasetspecific.ArapahoandGitksaninpar-ticularneedspecialattention,wherethenumberoftrainingepochsneedtobeadjustedfortheveryhighandlowdatasize.WealsodevelopedmostoftheoptimizationonthetokenclassificationmodelonArapaho.However,wedidnothavetimetopropagatethechanges(usingpretrainedtokenizer,savingthelastmodelinsteadofthemodelwiththelowestvalidationloss)totherestoflanguagessinceinitialexperimentshowedthatpretrainedtokeniz-ersdidnotimproveontheotherlanguages.How-ever,afterthesubmissiondeadlineisconcluded,weranmoreexperimentsanddiscoveredthataddingpretrainedtokenizersrequiresmoretrainingsteps,andthetrainingisbettercontrolledbyspecifyingthetrainingstepsinsteadofepochs.Wedonotin-cludethoselatestexperimentsinthispaper,butourtokenclassificationmodelshavethepotentialtoperformbetterwithmorehyperparametertuning.5.3In-VersusOut-of-VocabularyErrorsOnedimensionoferroranalysisweinvestigatedwaswhatproportionofoursystems\u2019errorscomefrommorphemesorwordsthatareeitherinoroutofthetrainingdatavocabulary.Wecountamor-phemeorwordasin-vocabularyifthesurfaceformanditscorrespondingglossco-occurinthepro-videdtrainingdata(notincludingthedevelopmentdata,asourmodelsareonlytrainedonthetrainset).NotethatthereisamuchlargerproportionofOOVwordsasopposedtomorphemesduetothefactthatanunseenwordcanbecomposedofdifferentcombinationsofseenmorphemes.Table3showstheproportionofmorphemesandwordsthatareout-of-vocab(OOV)withinthetestset.Whilenearlyallthelanguageshavelessthan10%oftheirmorphemesclassifiedasOOV,GitksannotablyhasarelativelylargeportionofOOVtestdata,with\u224845%ofmorphemesand\u224878%ofwordsbeingOOV.Tables4and5showourmodels\u2019performancesonin-versesout-of-vocabtokensatthemorphemeandwordlevels,respectively.Whilewewouldintuitivelyexpectthatword-levelOOVaccuracybeaboutthesameorworsethanmorpheme-levelOOV\n212\n\n\nTable2:Morpheme-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselinewithartificialpretraininganddictionarypost-correctionisthehighestGitksanaccuracyreportedoutofallsharedtasksubmissions.\nxlmr-base91.3684.3547.47/52.8280.1788.3585.8480.0880.42xlmr-aug89.3488.1546.89/52.3982.3685.5389.4983.0881.48\nTable3:ProportionofmorphemesandwordsthatareOOVwithinthetestset.accuracy,thisisnotthecaseduetothefactthatalargeportionofout-of-vocabwordsareformedwithin-vocabmorphemes.Formostlanguages,withtheexceptionofGitksan,thereappearstobeatrade-offbetweenbetterin-vocabmorphemeperformancewithXLMRandperformanceout-of-vocabwithByT5.6RelatedWorkTherehavebeenavarietyofapproachestotheprob-lemof(semi-)automaticallygeneratinginterlineargloss.BaldridgeandPalmer(2009)investigatetheefficacyofactivelearningforthetaskofinterlinearglossing,usingannotationtimerequiredbyexpertandnon-expertannotatorsastheirmetric.Thesys-temtheyusetogenerateglosslabelsuggestionsisastandardmaximumentropyclassifier.Arule-basedapproachbySnoeketal.(2014)utilizesanFSTtogenerateglossesforPlainsCree,focusingonnouns.Samard\u017ei\u00b4cetal.(2015)viewthetaskofglossingsegmentedtextasatwo-stepprocess,firsttreatingitasastandardPOStaggingtaskandthenaddinglexicalglossesfromadictio-nary.TheydemonstratethismethodonaChintangcorpusofabout1.2millionwords.Anumberofotherworksfocusingoninterlinearglossingutilizeconditionalrandomfield(CRF)models.MoellerandHulden(2018)testthreedifferentmodelsonaverysmallLezgidataset(<3000words):aCRF(thatoutputsBIOlabelswiththecorrespondingglosspercharacterinthein-put),asegmentationandlabellingpipelinethatuti-lizesaCRF(forBIOlabels)andSVM(forglossla-bels),andanLSTMseq2seqmodel.TheyfindthattheCRFthatjointlyproducestheBIOlabelsandtagsproducedthebestresults.McMillan-Major(2020)utilizestranslationsintheirtrainingdatabycreatingtwoCRFmodels,onethatpredictsglossfromthesegmentedinputandanotherthanpre-\nModelarpddogitlezntunybuspAVG\nModelarpddogitlezntunybuspAVG\nbyt5-base78.8680.3214.8460.72b76.6776.7377.2166.48byt5-aug73.2762.374.1738.6055.1169.2570.8553.38byt5-odin80.5682.7920.5763.7777.9782.5975.7269.14\nbaseline91.1185.3425.3351.8249.0388.7182.4867.69\nbaseline85.4475.7116.4134.5441.0884.3076.5559.14\nxlmr-base85.8773.7727.86/34.11a74.1582.9980.6173.4772.14xlmr-aug82.9280.0724.74/31.2577.7778.7285.5377.5173.39\naWereportbefore/afterdictionarybasedpost-correctionforGitksan.bWetrainedthismodelwithoutromanizingLezgi.Table1:Word-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselineisthehighestArapahoaccuracyreportedoutofallsharedtasksubmissions.\nbyt5-base78.8275.7712.5944.1062.4078.9774.2560.99byt5-aug72.1057.932.6026.2435.6270.0167.7347.46byt5-odin80.8178.2412.7450.0063.3985.3073.2563.39\nMorph0.0430.0090.4500.0560.0340.0190.070\narpddogitlezntunybusp\nWord0.2420.1550.7810.1690.2140.0840.200\n213\n\n\nbyt5-aug74.7658.243.4240.2736.5471.2770.56\n35.48\nTable4:Morpheme-levelaccuracyoveralltokensofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.\n24.67\n12.31\nxlmr-base95.2085.1282.8984.7990.8787.4686.05\n23.24\n45.07\nxlmr-aug93.7283.8594.0587.6489.2490.8191.11\n29.33\n24.10\n49.79\nbyt5-odin91.9387.6663.1073.7885.9387.6083.46\n59.51\n23.54\nbyt5-odin83.4778.5518.4262.9064.3886.8575.23\n18.26\n22.41\n49.17\nTable5:Word-levelaccuracyofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.dictsfromthetranslation,andthenusesheuristicstodeterminewhichmodeltoselectfromforeachmorpheme.BarrigaMart\u00ednezetal.(2021)usedaCRFmodeltoachieve>90%accuracyforgloss-ingOtomiandfindthatitworksbetterthananRNN,whichiscomputationallymoreexpensive.Otherworks,includingoursystems,haveturnedtoneuralmethods.Kondratyuk(2019)leveragespretrainedmultilingualBERTtoencodeinputsentences,thenapplyadditionalword-levelandcharacter-levelLSTMlayersbeforejointlydecod-inglemmasandmorphologytagsusingsimplese-quencetagginglayers.Furthermore,theyshowthattwo-stagetrainingbyfirsttrainingonalllan-guagesfollowedbytrainingonthetargetlanguageismoreeffectivethantrainingthesystemonthetargetlanguagealone.AnapproachbyZhaoetal.(2020),likeMcMillan-Major(2020),makesuseoftranslationsavailableinparallelcorpora,butdosobyusingamulti-sourcetransformermodel.Theyalsoincorporatelengthcontrolandalignmentdur-inginferencetoenhancetheirmodel,andtesttheirsystemonArapaho,Tsez,andLezgi.7ConclusionInoursharedtasksubmission,weexploredataaug-mentationmethodsandmodelingstrategiesforthetaskofinterlinearglossinginsevenlow-resourcelanguages.Ourbestperformingmodelsareto-kenclassificationmodelsusingXLMR.Wedemon-stratethatpretrainingonartificialdatawithXLMRisaneffectivetechniqueforthemid-resourcetestlanguages.Additionally,inourerroranalysiswefindthatwemayhaveactuallyundertrainedourtokenclassificationmodels,andthusoursystemsmayhavethepotentialtoperformbetterwithad-ditionalhyperparametertuning.WhileourByT5modelsdidnotperformaswellasourothersys-tems,weshowthatpretrainingonODINdataiseffective,despitethisdatabeingverynoisy.Finally,wealsodemonstrateimprovementsbyutilizingadictionarytopost-correctmodeloutputsforGitk-san.\n0.00\n0.00\n0.00\n0.00\n11.24\n0.82\n12.86\n14.52\n23.60\n4.97\n56.36\n45.65\n21.14\n43.37\n1.61\n47.52\n46.94\n29.69\n13.67\nxlmr-base95.9378.1895.2384.2493.1485.8586.27\n5.79\n28.04\n2.00\n17.00\n0.41\n16.08\n14.67\n19.35\nModelarpddogitlezntunybusp\nModelarpddogitlezntunybusp\n7.49\n28.09\n28.09\n44.81\n3.23\n3.23\n54.44\n2.33\nbyt5-aug87.2268.6910.7146.0665.1374.5981.44\n30.20\nxlmr-aug92.9888.9484.7487.1087.8891.1789.31\n48.70\n40.00\n28.63\n8.67\n2.60\n2.60\n9.68\n214\n\n\nAcknowledgementsThisworkwassupportedbyNSFCISERIgrantnumber2211951,FromAcousticSignaltoMor-phosyntacticAnalysisinoneEnd-to-EndNeuralSystem.ReferencesJasonBaldridgeandAlexisPalmer.2009.Howwelldoesactivelearningactuallywork?Time-basedeval-uationofcost-reductionstrategiesforlanguagedocu-mentation.InProceedingsofthe2009ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages296\u2013305,Singapore.AssociationforCompu-tationalLinguistics.DiegoBarrigaMart\u00ednez,VictorMijangos,andXi-menaGutierrez-Vasques.2021.Automaticinterlin-earglossingforOtomilanguage.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages34\u201343,Online.AssociationforComputationalLin-guistics.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm\u00e1n,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.ClarissaForbes,GarrettNicolai,andMiikkaSilfverberg.2021.AnFSTmorphologicalanalyzerforthegitksanlanguage.InProceedingsofthe18thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages188\u2013197,Online.AssociationforComputationalLinguistics.DanKondratyuk.2019.Cross-linguallemmatizationandmorphologytaggingwithtwo-stagemultilin-gualBERTfine-tuning.InProceedingsofthe16thWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages12\u201318,Florence,Italy.AssociationforComputationalLinguistics.WilliamD.LewisandFeiXia.2010.DevelopingODIN:AMultilingualRepositoryofAnnotatedLanguageDataforHundredsoftheWorld\u2019sLanguages.Liter-aryandLinguisticComputing,25(3):303\u2013319.AngelinaMcMillan-Major.2020.Automatingglossgenerationininterlinearglossedtext.ProceedingsoftheSocietyforComputationinLinguistics,3(1):338\u2013349.SarahMoellerandMansHulden.2018.Automaticglossinginalow-resourcesettingforlanguagedoc-umentation.InProceedingsoftheWorkshoponComputationalModelingofPolysyntheticLanguages,pages84\u201393.DavidR.Mortensen,SiddharthDalmia,andPatrickLittell.2018.Epitran:PrecisionG2Pformanylan-guages.InProceedingsoftheEleventhInternationalConferenceonLanguageResourcesandEvaluation(LREC2018),Miyazaki,Japan.EuropeanLanguageResourcesAssociation(ELRA).SebastianRuder,MatthewEPeters,SwabhaSwayamdipta,andThomasWolf.2019.Transferlearninginnaturallanguageprocessing.InProceed-ingsofthe2019conferenceoftheNorthAmericanchapteroftheassociationforcomputationallinguistics:Tutorials,pages15\u201318.TanjaSamard\u017ei\u00b4c,RobertSchikowski,andSabineStoll.2015.Automaticinterlinearglossingastwo-levelsequenceclassification.NoamShazeerandMitchellStern.2018.Adafactor:Adaptivelearningrateswithsublinearmemorycost.InProceedingsofthe35thInternationalConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,pages4596\u20134604.PMLR.ConorSnoek,DorothyThunder,KaidiL\u00f5o,AnttiArppe,JordanLachler,SjurMoshagen,andTrondTrosterud.2014.ModelingthenounmorphologyofPlainsCree.InProceedingsofthe2014WorkshopontheUseofComputationalMethodsintheStudyofEndangeredLanguages,pages34\u201342,Baltimore,Maryland,USA.AssociationforComputationalLinguistics.FeiXia,WilliamLewis,MichaelWayneGoodman,JoshuaCrowgey,andEmilyM.Bender.2014.En-richingODIN.InProceedingsoftheNinthInter-nationalConferenceonLanguageResourcesandEvaluation(LREC\u201914),pages3151\u20133157,Reykjavik,Iceland.EuropeanLanguageResourcesAssociation(ELRA).LintingXue,AdityaBarua,NoahConstant,RamiAl-Rfou,SharanNarang,MihirKale,AdamRoberts,andColinRaffel.2022.ByT5:Towardsatoken-freefuturewithpre-trainedbyte-to-bytemodels.Transac-tionsoftheAssociationforComputationalLinguis-tics,10:291\u2013306.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483\u2013498,On-line.AssociationforComputationalLinguistics.XingyuanZhao,SatoruOzaki,AntoniosAnastasopou-los,GrahamNeubig,andLoriLevin.2020.Auto-maticinterlinearglossingforunder-resourcedlan-guagesleveragingtranslations.InProceedingsofthe28thInternationalConferenceonComputationalLinguistics,pages5397\u20135408,Barcelona,Spain(On-line).InternationalCommitteeonComputationalLin-guistics.\n215\n\n\nAHyperparameterSettingsWeuseAdafactor(ShazeerandStern,2018)astheoptimizeracrossallexperiments,withthede-faultschedulerfromHuggingFaceTransformers,abatchsizeof32forRoBERTabasedmodelsandabatchsizeof4withagradientaccumulationstepof8forByT5basedmodels.Wetrainthetokenclassificationmodelsfor40epochsexceptforAra-paho,onwhichwetrain20epochs,andGitksan,onwhichwetrain2,000steps.WetraintheByT5basedmodelsfor20epochsonallofthedatamixedtogether.\n216"}