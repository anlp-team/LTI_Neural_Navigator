{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_The_Devil_Is_in_the_Errors:_Leveraging_Large_Language_Models_for_Fine-grained_Machine_Translation_Evaluation_chunk_7.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the text suggest about LLM evaluators when prompted for AUTOMQM?", "answer": " The text suggests that LLM evaluators are much more robust to the choice of in-context examples when prompted for AUTOMQM rather than for score prediction.", "ref_chunk": "to be lower, with most example sets exhibiting less than 0.05 Pearson dif- ference from the best-performing sets. All this sug- gests that LLM evaluators are much more robust to the choice of in-context examples when prompted for AUTOMQM rather than for score prediction. We also find that the behavior of in-context learn- ing is quite similar for both reference-based and reference-less evaluation tasks. Finally, we observe PaLM-2 (Bison) PaLM-2 (Bison) 0.30 0.15 PaLM-2 (Bison) ref-free 0.30Pearson (EN-DE) 0.05 0.15 PaLM-2 (Bison) ref-free 0.10 3 0.00 1 0 0 0.00 3 2 2 6# of in-context examples 1 0.35Pearson (ZH-EN) 4 4 6# of in-context examples 5 5 0.05 0.25 0.25 0.20 0.20 0.10 Figure 8: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the AUTOMQM prompt, for EN-DE (left) and ZH-EN (right). System-Level Segment-Level All (2 LPs) EN-DE ZH-EN Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL MATESE COMET-QE MATESE-QE COMET-WL \u2713 \u2713 \u2717 \u2717 \u2717 81.1% 79.9% 76.9% 73.4% 71.6% 0.549 0.391 0.419 0.298 0.418 61.1% 0.581 58.8% 0.528 56.3% 0.505 57.9% 0.468 57.1% 0.406 54.6% 51.5% 48.8% 50.1% 51.5% Score Prediction PaLM-2 BISON PaLM-2 UNICORN PaLM-2 BISON PaLM-2 UNICORN \u2713 \u2713 \u2717 \u2717 86.4% 86.4% 84.0% 80.5% 0.394 0.401 0.355 0.275 56.8% 0.322 56.3% 0.349 57.0% 0.299 56.1% 0.252 49.3% 51.1% 48.6% 48.3% AutoMQM PaLM-2 BISON PaLM-2 UNICORN PaLM 2 BISON PaLM 2 UNICORN \u2713 \u2713 \u2717 \u2717 84.0% 87.6% 87.6% 83.4% 0.369 0.432 0.297 0.368 59.2% 0.355 59.1% 0.442 55.2% 0.331 56.4% 0.429 48.4% 51.8% 48.0% 50.2% Table 5: Meta-evaluation results for PaLM-2 models using AutoMQM and score prediction, at the system and segment levels for multiple language pairs. that the example sets that perform well for one task generally work well for the other, with performance on both settings given a fixed in-context set being highly correlated, as shown in Figure 9. 0.30 0.20 5 0.25 0.3Pearson w/o Reference 2 0.10 0.0 0.05 0.2 6Number of Examples 4 0.1 0.00 0.15 0.35Pearson with Reference 1 Corr = 0.941 3 Table 5 shows the meta-evaluation results for PaLM-2 BISON and UNICORN prompted with AUTOMQM (using the best-performing in-context learning sets in Figure 8). For ease of comparison, we also report their performance when prompted for score prediction, as well as the performance of the baselines. Overall, prompting LLMs with AUTOMQM seems to lead to significant improve- ments in evaluating machine translation quality, particularly for larger models: UNICORN achieves better performance (across all meta evaluations) with it than when prompted for score prediction, and its reference-less version is competitive with the best learned metric even at the segment level. However, for the smaller BISON, the benefits of AUTOMQM are less clear, with both techniques performing comparably. This hints that scale is necessary for zero- and few- shot fine-grained evalu- ation (like with AUTOMQM). We also find that the Figure 9: Scatter plot of the Pearson of PaLM-2 (BISON) models, with/without including the reference in the prompt, for each in-context learning setting tried. distribution of scores produced by LLMs prompted with AUTOMQM is much closer to the gold MQM distribution, with models outputting a much larger set of scores, and in the same ranges as annotators do (see Figure 10). 10 10000 1000 1 30 0 PaLM-2 (Unicorn)Score# occurrences (en-de) PaLM-2 (Bison) 35 100 25 5 15 40 10 Gold MQM 20 Figure 10: Distribution of scores for PaLM-2 models using AUTOMQM, on WMT22 EN-DE EN-DE ZH-EN Model R? SP MR MCC SP MR MCC Baselines COMET-WL \u2717 0.267 0.250 0.161 0.364 0.178 0.152 AutoMQM BISON UNICORN BISON UNICORN \u2713 0.095 0.749 0.060 0.252 0.255 0.109 \u2713 0.175 0.628 0.193 0.238 0.476 0.143 \u2717 0.119 0.520 0.092 0.224 0.311 0.091 \u2717 0.150 0.580 0.150 0.229 0.488 0.133 Table 6: Span-level meta-evaluation on WMT22 for PaLM-2 models using AutoMQM. SR and MR represent span precision and major recall, respectively. Finally, when evaluating the error spans pro- duced by LLMs prompted with AUTOMQM (Ta- ble 6), we find that PaLM-2 models are able to identify most of the major errors. However, it does seem to over-predict errors (with errors predicted by UNICORN having on average \u223c5 words per span vs \u223c2 words in the ground truth) and have overall low span precision. Similarly to overall score cor- relations, scale also seems to be important for the quality of spans produced by AUTOMQM, with UNICORN outperforming BISON at most metrics. Additionally, UNICORN prompted with AutoMQM predicts spans of comparable quality to the ones produced by current state-of-the-art learned word- level evaluators (trained on a considerable number of fine-grained annotations derived from MQM): while word-level models are more precise, their overall span correlation (MCC) is comparable, and they miss considerably more major errors than LLMs (despite only leveraging a handful of an- notations). 7 Conclusion In this study, we have systematically investi- gated the capabilities of large language models for machine translation evaluation through score prediction, and proposed AUTOMQM, a novel prompting technique that leverages the Multidi- mensional Quality Metrics (MQM) framework for interpretable MT evaluation using LLMs. We demonstrated that just prompting LLMs for score prediction leads to state-of-the-art system- level evaluators, but still falls short of the best learned metrics at the segment-level (with fine- tuning being necessary to close this gap). Then we showed that AUTOMQM can further improve the performance of LLMs without finetuning while providing interpretability through error spans that align with human annotations. Our findings surrounding finetuning LLMs for score prediction hint that LLMs\u2019 performance in machine translation evaluation could be further im- proved by finetuning these models on fine-grained human judgment data (like MQM) and is a direc- tion we are actively pursuing. Additionally, the general-purpose nature of LLMs may enable the application of similar prompting techniques (lever- aging some fine-grained evaluation schemes) to other evaluation problems (Wu et al., 2023). Acknowledgements We would like to thank Ricardo Rei, Marcos Treviso and Chryssa Zerva for helping run the word-level QE baselines, and George Foster who provided"}, {"question": " According to the text, how is the behavior of in-context learning for reference-based and reference-less evaluation tasks?", "answer": " The text states that the behavior of in-context learning is quite similar for both reference-based and reference-less evaluation tasks.", "ref_chunk": "to be lower, with most example sets exhibiting less than 0.05 Pearson dif- ference from the best-performing sets. All this sug- gests that LLM evaluators are much more robust to the choice of in-context examples when prompted for AUTOMQM rather than for score prediction. We also find that the behavior of in-context learn- ing is quite similar for both reference-based and reference-less evaluation tasks. Finally, we observe PaLM-2 (Bison) PaLM-2 (Bison) 0.30 0.15 PaLM-2 (Bison) ref-free 0.30Pearson (EN-DE) 0.05 0.15 PaLM-2 (Bison) ref-free 0.10 3 0.00 1 0 0 0.00 3 2 2 6# of in-context examples 1 0.35Pearson (ZH-EN) 4 4 6# of in-context examples 5 5 0.05 0.25 0.25 0.20 0.20 0.10 Figure 8: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the AUTOMQM prompt, for EN-DE (left) and ZH-EN (right). System-Level Segment-Level All (2 LPs) EN-DE ZH-EN Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL MATESE COMET-QE MATESE-QE COMET-WL \u2713 \u2713 \u2717 \u2717 \u2717 81.1% 79.9% 76.9% 73.4% 71.6% 0.549 0.391 0.419 0.298 0.418 61.1% 0.581 58.8% 0.528 56.3% 0.505 57.9% 0.468 57.1% 0.406 54.6% 51.5% 48.8% 50.1% 51.5% Score Prediction PaLM-2 BISON PaLM-2 UNICORN PaLM-2 BISON PaLM-2 UNICORN \u2713 \u2713 \u2717 \u2717 86.4% 86.4% 84.0% 80.5% 0.394 0.401 0.355 0.275 56.8% 0.322 56.3% 0.349 57.0% 0.299 56.1% 0.252 49.3% 51.1% 48.6% 48.3% AutoMQM PaLM-2 BISON PaLM-2 UNICORN PaLM 2 BISON PaLM 2 UNICORN \u2713 \u2713 \u2717 \u2717 84.0% 87.6% 87.6% 83.4% 0.369 0.432 0.297 0.368 59.2% 0.355 59.1% 0.442 55.2% 0.331 56.4% 0.429 48.4% 51.8% 48.0% 50.2% Table 5: Meta-evaluation results for PaLM-2 models using AutoMQM and score prediction, at the system and segment levels for multiple language pairs. that the example sets that perform well for one task generally work well for the other, with performance on both settings given a fixed in-context set being highly correlated, as shown in Figure 9. 0.30 0.20 5 0.25 0.3Pearson w/o Reference 2 0.10 0.0 0.05 0.2 6Number of Examples 4 0.1 0.00 0.15 0.35Pearson with Reference 1 Corr = 0.941 3 Table 5 shows the meta-evaluation results for PaLM-2 BISON and UNICORN prompted with AUTOMQM (using the best-performing in-context learning sets in Figure 8). For ease of comparison, we also report their performance when prompted for score prediction, as well as the performance of the baselines. Overall, prompting LLMs with AUTOMQM seems to lead to significant improve- ments in evaluating machine translation quality, particularly for larger models: UNICORN achieves better performance (across all meta evaluations) with it than when prompted for score prediction, and its reference-less version is competitive with the best learned metric even at the segment level. However, for the smaller BISON, the benefits of AUTOMQM are less clear, with both techniques performing comparably. This hints that scale is necessary for zero- and few- shot fine-grained evalu- ation (like with AUTOMQM). We also find that the Figure 9: Scatter plot of the Pearson of PaLM-2 (BISON) models, with/without including the reference in the prompt, for each in-context learning setting tried. distribution of scores produced by LLMs prompted with AUTOMQM is much closer to the gold MQM distribution, with models outputting a much larger set of scores, and in the same ranges as annotators do (see Figure 10). 10 10000 1000 1 30 0 PaLM-2 (Unicorn)Score# occurrences (en-de) PaLM-2 (Bison) 35 100 25 5 15 40 10 Gold MQM 20 Figure 10: Distribution of scores for PaLM-2 models using AUTOMQM, on WMT22 EN-DE EN-DE ZH-EN Model R? SP MR MCC SP MR MCC Baselines COMET-WL \u2717 0.267 0.250 0.161 0.364 0.178 0.152 AutoMQM BISON UNICORN BISON UNICORN \u2713 0.095 0.749 0.060 0.252 0.255 0.109 \u2713 0.175 0.628 0.193 0.238 0.476 0.143 \u2717 0.119 0.520 0.092 0.224 0.311 0.091 \u2717 0.150 0.580 0.150 0.229 0.488 0.133 Table 6: Span-level meta-evaluation on WMT22 for PaLM-2 models using AutoMQM. SR and MR represent span precision and major recall, respectively. Finally, when evaluating the error spans pro- duced by LLMs prompted with AUTOMQM (Ta- ble 6), we find that PaLM-2 models are able to identify most of the major errors. However, it does seem to over-predict errors (with errors predicted by UNICORN having on average \u223c5 words per span vs \u223c2 words in the ground truth) and have overall low span precision. Similarly to overall score cor- relations, scale also seems to be important for the quality of spans produced by AUTOMQM, with UNICORN outperforming BISON at most metrics. Additionally, UNICORN prompted with AutoMQM predicts spans of comparable quality to the ones produced by current state-of-the-art learned word- level evaluators (trained on a considerable number of fine-grained annotations derived from MQM): while word-level models are more precise, their overall span correlation (MCC) is comparable, and they miss considerably more major errors than LLMs (despite only leveraging a handful of an- notations). 7 Conclusion In this study, we have systematically investi- gated the capabilities of large language models for machine translation evaluation through score prediction, and proposed AUTOMQM, a novel prompting technique that leverages the Multidi- mensional Quality Metrics (MQM) framework for interpretable MT evaluation using LLMs. We demonstrated that just prompting LLMs for score prediction leads to state-of-the-art system- level evaluators, but still falls short of the best learned metrics at the segment-level (with fine- tuning being necessary to close this gap). Then we showed that AUTOMQM can further improve the performance of LLMs without finetuning while providing interpretability through error spans that align with human annotations. Our findings surrounding finetuning LLMs for score prediction hint that LLMs\u2019 performance in machine translation evaluation could be further im- proved by finetuning these models on fine-grained human judgment data (like MQM) and is a direc- tion we are actively pursuing. Additionally, the general-purpose nature of LLMs may enable the application of similar prompting techniques (lever- aging some fine-grained evaluation schemes) to other evaluation problems (Wu et al., 2023). Acknowledgements We would like to thank Ricardo Rei, Marcos Treviso and Chryssa Zerva for helping run the word-level QE baselines, and George Foster who provided"}, {"question": " What do the meta-evaluation results in Table 5 show for PaLM-2 BISON and UNICORN prompted with AUTOMQM?", "answer": " The meta-evaluation results in Table 5 show that using AUTOMQM leads to significant improvements in evaluating machine translation quality, especially for larger models like UNICORN.", "ref_chunk": "to be lower, with most example sets exhibiting less than 0.05 Pearson dif- ference from the best-performing sets. All this sug- gests that LLM evaluators are much more robust to the choice of in-context examples when prompted for AUTOMQM rather than for score prediction. We also find that the behavior of in-context learn- ing is quite similar for both reference-based and reference-less evaluation tasks. Finally, we observe PaLM-2 (Bison) PaLM-2 (Bison) 0.30 0.15 PaLM-2 (Bison) ref-free 0.30Pearson (EN-DE) 0.05 0.15 PaLM-2 (Bison) ref-free 0.10 3 0.00 1 0 0 0.00 3 2 2 6# of in-context examples 1 0.35Pearson (ZH-EN) 4 4 6# of in-context examples 5 5 0.05 0.25 0.25 0.20 0.20 0.10 Figure 8: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the AUTOMQM prompt, for EN-DE (left) and ZH-EN (right). System-Level Segment-Level All (2 LPs) EN-DE ZH-EN Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL MATESE COMET-QE MATESE-QE COMET-WL \u2713 \u2713 \u2717 \u2717 \u2717 81.1% 79.9% 76.9% 73.4% 71.6% 0.549 0.391 0.419 0.298 0.418 61.1% 0.581 58.8% 0.528 56.3% 0.505 57.9% 0.468 57.1% 0.406 54.6% 51.5% 48.8% 50.1% 51.5% Score Prediction PaLM-2 BISON PaLM-2 UNICORN PaLM-2 BISON PaLM-2 UNICORN \u2713 \u2713 \u2717 \u2717 86.4% 86.4% 84.0% 80.5% 0.394 0.401 0.355 0.275 56.8% 0.322 56.3% 0.349 57.0% 0.299 56.1% 0.252 49.3% 51.1% 48.6% 48.3% AutoMQM PaLM-2 BISON PaLM-2 UNICORN PaLM 2 BISON PaLM 2 UNICORN \u2713 \u2713 \u2717 \u2717 84.0% 87.6% 87.6% 83.4% 0.369 0.432 0.297 0.368 59.2% 0.355 59.1% 0.442 55.2% 0.331 56.4% 0.429 48.4% 51.8% 48.0% 50.2% Table 5: Meta-evaluation results for PaLM-2 models using AutoMQM and score prediction, at the system and segment levels for multiple language pairs. that the example sets that perform well for one task generally work well for the other, with performance on both settings given a fixed in-context set being highly correlated, as shown in Figure 9. 0.30 0.20 5 0.25 0.3Pearson w/o Reference 2 0.10 0.0 0.05 0.2 6Number of Examples 4 0.1 0.00 0.15 0.35Pearson with Reference 1 Corr = 0.941 3 Table 5 shows the meta-evaluation results for PaLM-2 BISON and UNICORN prompted with AUTOMQM (using the best-performing in-context learning sets in Figure 8). For ease of comparison, we also report their performance when prompted for score prediction, as well as the performance of the baselines. Overall, prompting LLMs with AUTOMQM seems to lead to significant improve- ments in evaluating machine translation quality, particularly for larger models: UNICORN achieves better performance (across all meta evaluations) with it than when prompted for score prediction, and its reference-less version is competitive with the best learned metric even at the segment level. However, for the smaller BISON, the benefits of AUTOMQM are less clear, with both techniques performing comparably. This hints that scale is necessary for zero- and few- shot fine-grained evalu- ation (like with AUTOMQM). We also find that the Figure 9: Scatter plot of the Pearson of PaLM-2 (BISON) models, with/without including the reference in the prompt, for each in-context learning setting tried. distribution of scores produced by LLMs prompted with AUTOMQM is much closer to the gold MQM distribution, with models outputting a much larger set of scores, and in the same ranges as annotators do (see Figure 10). 10 10000 1000 1 30 0 PaLM-2 (Unicorn)Score# occurrences (en-de) PaLM-2 (Bison) 35 100 25 5 15 40 10 Gold MQM 20 Figure 10: Distribution of scores for PaLM-2 models using AUTOMQM, on WMT22 EN-DE EN-DE ZH-EN Model R? SP MR MCC SP MR MCC Baselines COMET-WL \u2717 0.267 0.250 0.161 0.364 0.178 0.152 AutoMQM BISON UNICORN BISON UNICORN \u2713 0.095 0.749 0.060 0.252 0.255 0.109 \u2713 0.175 0.628 0.193 0.238 0.476 0.143 \u2717 0.119 0.520 0.092 0.224 0.311 0.091 \u2717 0.150 0.580 0.150 0.229 0.488 0.133 Table 6: Span-level meta-evaluation on WMT22 for PaLM-2 models using AutoMQM. SR and MR represent span precision and major recall, respectively. Finally, when evaluating the error spans pro- duced by LLMs prompted with AUTOMQM (Ta- ble 6), we find that PaLM-2 models are able to identify most of the major errors. However, it does seem to over-predict errors (with errors predicted by UNICORN having on average \u223c5 words per span vs \u223c2 words in the ground truth) and have overall low span precision. Similarly to overall score cor- relations, scale also seems to be important for the quality of spans produced by AUTOMQM, with UNICORN outperforming BISON at most metrics. Additionally, UNICORN prompted with AutoMQM predicts spans of comparable quality to the ones produced by current state-of-the-art learned word- level evaluators (trained on a considerable number of fine-grained annotations derived from MQM): while word-level models are more precise, their overall span correlation (MCC) is comparable, and they miss considerably more major errors than LLMs (despite only leveraging a handful of an- notations). 7 Conclusion In this study, we have systematically investi- gated the capabilities of large language models for machine translation evaluation through score prediction, and proposed AUTOMQM, a novel prompting technique that leverages the Multidi- mensional Quality Metrics (MQM) framework for interpretable MT evaluation using LLMs. We demonstrated that just prompting LLMs for score prediction leads to state-of-the-art system- level evaluators, but still falls short of the best learned metrics at the segment-level (with fine- tuning being necessary to close this gap). Then we showed that AUTOMQM can further improve the performance of LLMs without finetuning while providing interpretability through error spans that align with human annotations. Our findings surrounding finetuning LLMs for score prediction hint that LLMs\u2019 performance in machine translation evaluation could be further im- proved by finetuning these models on fine-grained human judgment data (like MQM) and is a direc- tion we are actively pursuing. Additionally, the general-purpose nature of LLMs may enable the application of similar prompting techniques (lever- aging some fine-grained evaluation schemes) to other evaluation problems (Wu et al., 2023). Acknowledgements We would like to thank Ricardo Rei, Marcos Treviso and Chryssa Zerva for helping run the word-level QE baselines, and George Foster who provided"}, {"question": " What does the text suggest about the benefits of AUTOMQM for smaller models like BISON?", "answer": " The text suggests that the benefits of AUTOMQM are less clear for smaller models like BISON, with both techniques (AUTOMQM and score prediction) performing comparably.", "ref_chunk": "to be lower, with most example sets exhibiting less than 0.05 Pearson dif- ference from the best-performing sets. All this sug- gests that LLM evaluators are much more robust to the choice of in-context examples when prompted for AUTOMQM rather than for score prediction. We also find that the behavior of in-context learn- ing is quite similar for both reference-based and reference-less evaluation tasks. Finally, we observe PaLM-2 (Bison) PaLM-2 (Bison) 0.30 0.15 PaLM-2 (Bison) ref-free 0.30Pearson (EN-DE) 0.05 0.15 PaLM-2 (Bison) ref-free 0.10 3 0.00 1 0 0 0.00 3 2 2 6# of in-context examples 1 0.35Pearson (ZH-EN) 4 4 6# of in-context examples 5 5 0.05 0.25 0.25 0.20 0.20 0.10 Figure 8: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the AUTOMQM prompt, for EN-DE (left) and ZH-EN (right). System-Level Segment-Level All (2 LPs) EN-DE ZH-EN Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL MATESE COMET-QE MATESE-QE COMET-WL \u2713 \u2713 \u2717 \u2717 \u2717 81.1% 79.9% 76.9% 73.4% 71.6% 0.549 0.391 0.419 0.298 0.418 61.1% 0.581 58.8% 0.528 56.3% 0.505 57.9% 0.468 57.1% 0.406 54.6% 51.5% 48.8% 50.1% 51.5% Score Prediction PaLM-2 BISON PaLM-2 UNICORN PaLM-2 BISON PaLM-2 UNICORN \u2713 \u2713 \u2717 \u2717 86.4% 86.4% 84.0% 80.5% 0.394 0.401 0.355 0.275 56.8% 0.322 56.3% 0.349 57.0% 0.299 56.1% 0.252 49.3% 51.1% 48.6% 48.3% AutoMQM PaLM-2 BISON PaLM-2 UNICORN PaLM 2 BISON PaLM 2 UNICORN \u2713 \u2713 \u2717 \u2717 84.0% 87.6% 87.6% 83.4% 0.369 0.432 0.297 0.368 59.2% 0.355 59.1% 0.442 55.2% 0.331 56.4% 0.429 48.4% 51.8% 48.0% 50.2% Table 5: Meta-evaluation results for PaLM-2 models using AutoMQM and score prediction, at the system and segment levels for multiple language pairs. that the example sets that perform well for one task generally work well for the other, with performance on both settings given a fixed in-context set being highly correlated, as shown in Figure 9. 0.30 0.20 5 0.25 0.3Pearson w/o Reference 2 0.10 0.0 0.05 0.2 6Number of Examples 4 0.1 0.00 0.15 0.35Pearson with Reference 1 Corr = 0.941 3 Table 5 shows the meta-evaluation results for PaLM-2 BISON and UNICORN prompted with AUTOMQM (using the best-performing in-context learning sets in Figure 8). For ease of comparison, we also report their performance when prompted for score prediction, as well as the performance of the baselines. Overall, prompting LLMs with AUTOMQM seems to lead to significant improve- ments in evaluating machine translation quality, particularly for larger models: UNICORN achieves better performance (across all meta evaluations) with it than when prompted for score prediction, and its reference-less version is competitive with the best learned metric even at the segment level. However, for the smaller BISON, the benefits of AUTOMQM are less clear, with both techniques performing comparably. This hints that scale is necessary for zero- and few- shot fine-grained evalu- ation (like with AUTOMQM). We also find that the Figure 9: Scatter plot of the Pearson of PaLM-2 (BISON) models, with/without including the reference in the prompt, for each in-context learning setting tried. distribution of scores produced by LLMs prompted with AUTOMQM is much closer to the gold MQM distribution, with models outputting a much larger set of scores, and in the same ranges as annotators do (see Figure 10). 10 10000 1000 1 30 0 PaLM-2 (Unicorn)Score# occurrences (en-de) PaLM-2 (Bison) 35 100 25 5 15 40 10 Gold MQM 20 Figure 10: Distribution of scores for PaLM-2 models using AUTOMQM, on WMT22 EN-DE EN-DE ZH-EN Model R? SP MR MCC SP MR MCC Baselines COMET-WL \u2717 0.267 0.250 0.161 0.364 0.178 0.152 AutoMQM BISON UNICORN BISON UNICORN \u2713 0.095 0.749 0.060 0.252 0.255 0.109 \u2713 0.175 0.628 0.193 0.238 0.476 0.143 \u2717 0.119 0.520 0.092 0.224 0.311 0.091 \u2717 0.150 0.580 0.150 0.229 0.488 0.133 Table 6: Span-level meta-evaluation on WMT22 for PaLM-2 models using AutoMQM. SR and MR represent span precision and major recall, respectively. Finally, when evaluating the error spans pro- duced by LLMs prompted with AUTOMQM (Ta- ble 6), we find that PaLM-2 models are able to identify most of the major errors. However, it does seem to over-predict errors (with errors predicted by UNICORN having on average \u223c5 words per span vs \u223c2 words in the ground truth) and have overall low span precision. Similarly to overall score cor- relations, scale also seems to be important for the quality of spans produced by AUTOMQM, with UNICORN outperforming BISON at most metrics. Additionally, UNICORN prompted with AutoMQM predicts spans of comparable quality to the ones produced by current state-of-the-art learned word- level evaluators (trained on a considerable number of fine-grained annotations derived from MQM): while word-level models are more precise, their overall span correlation (MCC) is comparable, and they miss considerably more major errors than LLMs (despite only leveraging a handful of an- notations). 7 Conclusion In this study, we have systematically investi- gated the capabilities of large language models for machine translation evaluation through score prediction, and proposed AUTOMQM, a novel prompting technique that leverages the Multidi- mensional Quality Metrics (MQM) framework for interpretable MT evaluation using LLMs. We demonstrated that just prompting LLMs for score prediction leads to state-of-the-art system- level evaluators, but still falls short of the best learned metrics at the segment-level (with fine- tuning being necessary to close this gap). Then we showed that AUTOMQM can further improve the performance of LLMs without finetuning while providing interpretability through error spans that align with human annotations. Our findings surrounding finetuning LLMs for score prediction hint that LLMs\u2019 performance in machine translation evaluation could be further im- proved by finetuning these models on fine-grained human judgment data (like MQM) and is a direc- tion we are actively pursuing. Additionally, the general-purpose nature of LLMs may enable the application of similar prompting techniques (lever- aging some fine-grained evaluation schemes) to other evaluation problems (Wu et al., 2023). Acknowledgements We would like to thank Ricardo Rei, Marcos Treviso and Chryssa Zerva for helping run the word-level QE baselines, and George Foster who provided"}, {"question": " How does the distribution of scores produced by LLMs prompted with AUTOMQM compare to the gold MQM distribution?", "answer": " The text mentions that the distribution of scores produced by LLMs prompted with AUTOMQM is much closer to the gold MQM distribution.", "ref_chunk": "to be lower, with most example sets exhibiting less than 0.05 Pearson dif- ference from the best-performing sets. All this sug- gests that LLM evaluators are much more robust to the choice of in-context examples when prompted for AUTOMQM rather than for score prediction. We also find that the behavior of in-context learn- ing is quite similar for both reference-based and reference-less evaluation tasks. Finally, we observe PaLM-2 (Bison) PaLM-2 (Bison) 0.30 0.15 PaLM-2 (Bison) ref-free 0.30Pearson (EN-DE) 0.05 0.15 PaLM-2 (Bison) ref-free 0.10 3 0.00 1 0 0 0.00 3 2 2 6# of in-context examples 1 0.35Pearson (ZH-EN) 4 4 6# of in-context examples 5 5 0.05 0.25 0.25 0.20 0.20 0.10 Figure 8: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the AUTOMQM prompt, for EN-DE (left) and ZH-EN (right). System-Level Segment-Level All (2 LPs) EN-DE ZH-EN Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL MATESE COMET-QE MATESE-QE COMET-WL \u2713 \u2713 \u2717 \u2717 \u2717 81.1% 79.9% 76.9% 73.4% 71.6% 0.549 0.391 0.419 0.298 0.418 61.1% 0.581 58.8% 0.528 56.3% 0.505 57.9% 0.468 57.1% 0.406 54.6% 51.5% 48.8% 50.1% 51.5% Score Prediction PaLM-2 BISON PaLM-2 UNICORN PaLM-2 BISON PaLM-2 UNICORN \u2713 \u2713 \u2717 \u2717 86.4% 86.4% 84.0% 80.5% 0.394 0.401 0.355 0.275 56.8% 0.322 56.3% 0.349 57.0% 0.299 56.1% 0.252 49.3% 51.1% 48.6% 48.3% AutoMQM PaLM-2 BISON PaLM-2 UNICORN PaLM 2 BISON PaLM 2 UNICORN \u2713 \u2713 \u2717 \u2717 84.0% 87.6% 87.6% 83.4% 0.369 0.432 0.297 0.368 59.2% 0.355 59.1% 0.442 55.2% 0.331 56.4% 0.429 48.4% 51.8% 48.0% 50.2% Table 5: Meta-evaluation results for PaLM-2 models using AutoMQM and score prediction, at the system and segment levels for multiple language pairs. that the example sets that perform well for one task generally work well for the other, with performance on both settings given a fixed in-context set being highly correlated, as shown in Figure 9. 0.30 0.20 5 0.25 0.3Pearson w/o Reference 2 0.10 0.0 0.05 0.2 6Number of Examples 4 0.1 0.00 0.15 0.35Pearson with Reference 1 Corr = 0.941 3 Table 5 shows the meta-evaluation results for PaLM-2 BISON and UNICORN prompted with AUTOMQM (using the best-performing in-context learning sets in Figure 8). For ease of comparison, we also report their performance when prompted for score prediction, as well as the performance of the baselines. Overall, prompting LLMs with AUTOMQM seems to lead to significant improve- ments in evaluating machine translation quality, particularly for larger models: UNICORN achieves better performance (across all meta evaluations) with it than when prompted for score prediction, and its reference-less version is competitive with the best learned metric even at the segment level. However, for the smaller BISON, the benefits of AUTOMQM are less clear, with both techniques performing comparably. This hints that scale is necessary for zero- and few- shot fine-grained evalu- ation (like with AUTOMQM). We also find that the Figure 9: Scatter plot of the Pearson of PaLM-2 (BISON) models, with/without including the reference in the prompt, for each in-context learning setting tried. distribution of scores produced by LLMs prompted with AUTOMQM is much closer to the gold MQM distribution, with models outputting a much larger set of scores, and in the same ranges as annotators do (see Figure 10). 10 10000 1000 1 30 0 PaLM-2 (Unicorn)Score# occurrences (en-de) PaLM-2 (Bison) 35 100 25 5 15 40 10 Gold MQM 20 Figure 10: Distribution of scores for PaLM-2 models using AUTOMQM, on WMT22 EN-DE EN-DE ZH-EN Model R? SP MR MCC SP MR MCC Baselines COMET-WL \u2717 0.267 0.250 0.161 0.364 0.178 0.152 AutoMQM BISON UNICORN BISON UNICORN \u2713 0.095 0.749 0.060 0.252 0.255 0.109 \u2713 0.175 0.628 0.193 0.238 0.476 0.143 \u2717 0.119 0.520 0.092 0.224 0.311 0.091 \u2717 0.150 0.580 0.150 0.229 0.488 0.133 Table 6: Span-level meta-evaluation on WMT22 for PaLM-2 models using AutoMQM. SR and MR represent span precision and major recall, respectively. Finally, when evaluating the error spans pro- duced by LLMs prompted with AUTOMQM (Ta- ble 6), we find that PaLM-2 models are able to identify most of the major errors. However, it does seem to over-predict errors (with errors predicted by UNICORN having on average \u223c5 words per span vs \u223c2 words in the ground truth) and have overall low span precision. Similarly to overall score cor- relations, scale also seems to be important for the quality of spans produced by AUTOMQM, with UNICORN outperforming BISON at most metrics. Additionally, UNICORN prompted with AutoMQM predicts spans of comparable quality to the ones produced by current state-of-the-art learned word- level evaluators (trained on a considerable number of fine-grained annotations derived from MQM): while word-level models are more precise, their overall span correlation (MCC) is comparable, and they miss considerably more major errors than LLMs (despite only leveraging a handful of an- notations). 7 Conclusion In this study, we have systematically investi- gated the capabilities of large language models for machine translation evaluation through score prediction, and proposed AUTOMQM, a novel prompting technique that leverages the Multidi- mensional Quality Metrics (MQM) framework for interpretable MT evaluation using LLMs. We demonstrated that just prompting LLMs for score prediction leads to state-of-the-art system- level evaluators, but still falls short of the best learned metrics at the segment-level (with fine- tuning being necessary to close this gap). Then we showed that AUTOMQM can further improve the performance of LLMs without finetuning while providing interpretability through error spans that align with human annotations. Our findings surrounding finetuning LLMs for score prediction hint that LLMs\u2019 performance in machine translation evaluation could be further im- proved by finetuning these models on fine-grained human judgment data (like MQM) and is a direc- tion we are actively pursuing. Additionally, the general-purpose nature of LLMs may enable the application of similar prompting techniques (lever- aging some fine-grained evaluation schemes) to other evaluation problems (Wu et al., 2023). Acknowledgements We would like to thank Ricardo Rei, Marcos Treviso and Chryssa Zerva for helping run the word-level QE baselines, and George Foster who provided"}, {"question": " What are SR and MR in Table 6 used for in the evaluation of error spans produced by LLMs?", "answer": " SR represents span precision and MR represents major recall in the evaluation of error spans produced by LLMs.", "ref_chunk": "to be lower, with most example sets exhibiting less than 0.05 Pearson dif- ference from the best-performing sets. All this sug- gests that LLM evaluators are much more robust to the choice of in-context examples when prompted for AUTOMQM rather than for score prediction. We also find that the behavior of in-context learn- ing is quite similar for both reference-based and reference-less evaluation tasks. Finally, we observe PaLM-2 (Bison) PaLM-2 (Bison) 0.30 0.15 PaLM-2 (Bison) ref-free 0.30Pearson (EN-DE) 0.05 0.15 PaLM-2 (Bison) ref-free 0.10 3 0.00 1 0 0 0.00 3 2 2 6# of in-context examples 1 0.35Pearson (ZH-EN) 4 4 6# of in-context examples 5 5 0.05 0.25 0.25 0.20 0.20 0.10 Figure 8: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the AUTOMQM prompt, for EN-DE (left) and ZH-EN (right). System-Level Segment-Level All (2 LPs) EN-DE ZH-EN Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL MATESE COMET-QE MATESE-QE COMET-WL \u2713 \u2713 \u2717 \u2717 \u2717 81.1% 79.9% 76.9% 73.4% 71.6% 0.549 0.391 0.419 0.298 0.418 61.1% 0.581 58.8% 0.528 56.3% 0.505 57.9% 0.468 57.1% 0.406 54.6% 51.5% 48.8% 50.1% 51.5% Score Prediction PaLM-2 BISON PaLM-2 UNICORN PaLM-2 BISON PaLM-2 UNICORN \u2713 \u2713 \u2717 \u2717 86.4% 86.4% 84.0% 80.5% 0.394 0.401 0.355 0.275 56.8% 0.322 56.3% 0.349 57.0% 0.299 56.1% 0.252 49.3% 51.1% 48.6% 48.3% AutoMQM PaLM-2 BISON PaLM-2 UNICORN PaLM 2 BISON PaLM 2 UNICORN \u2713 \u2713 \u2717 \u2717 84.0% 87.6% 87.6% 83.4% 0.369 0.432 0.297 0.368 59.2% 0.355 59.1% 0.442 55.2% 0.331 56.4% 0.429 48.4% 51.8% 48.0% 50.2% Table 5: Meta-evaluation results for PaLM-2 models using AutoMQM and score prediction, at the system and segment levels for multiple language pairs. that the example sets that perform well for one task generally work well for the other, with performance on both settings given a fixed in-context set being highly correlated, as shown in Figure 9. 0.30 0.20 5 0.25 0.3Pearson w/o Reference 2 0.10 0.0 0.05 0.2 6Number of Examples 4 0.1 0.00 0.15 0.35Pearson with Reference 1 Corr = 0.941 3 Table 5 shows the meta-evaluation results for PaLM-2 BISON and UNICORN prompted with AUTOMQM (using the best-performing in-context learning sets in Figure 8). For ease of comparison, we also report their performance when prompted for score prediction, as well as the performance of the baselines. Overall, prompting LLMs with AUTOMQM seems to lead to significant improve- ments in evaluating machine translation quality, particularly for larger models: UNICORN achieves better performance (across all meta evaluations) with it than when prompted for score prediction, and its reference-less version is competitive with the best learned metric even at the segment level. However, for the smaller BISON, the benefits of AUTOMQM are less clear, with both techniques performing comparably. This hints that scale is necessary for zero- and few- shot fine-grained evalu- ation (like with AUTOMQM). We also find that the Figure 9: Scatter plot of the Pearson of PaLM-2 (BISON) models, with/without including the reference in the prompt, for each in-context learning setting tried. distribution of scores produced by LLMs prompted with AUTOMQM is much closer to the gold MQM distribution, with models outputting a much larger set of scores, and in the same ranges as annotators do (see Figure 10). 10 10000 1000 1 30 0 PaLM-2 (Unicorn)Score# occurrences (en-de) PaLM-2 (Bison) 35 100 25 5 15 40 10 Gold MQM 20 Figure 10: Distribution of scores for PaLM-2 models using AUTOMQM, on WMT22 EN-DE EN-DE ZH-EN Model R? SP MR MCC SP MR MCC Baselines COMET-WL \u2717 0.267 0.250 0.161 0.364 0.178 0.152 AutoMQM BISON UNICORN BISON UNICORN \u2713 0.095 0.749 0.060 0.252 0.255 0.109 \u2713 0.175 0.628 0.193 0.238 0.476 0.143 \u2717 0.119 0.520 0.092 0.224 0.311 0.091 \u2717 0.150 0.580 0.150 0.229 0.488 0.133 Table 6: Span-level meta-evaluation on WMT22 for PaLM-2 models using AutoMQM. SR and MR represent span precision and major recall, respectively. Finally, when evaluating the error spans pro- duced by LLMs prompted with AUTOMQM (Ta- ble 6), we find that PaLM-2 models are able to identify most of the major errors. However, it does seem to over-predict errors (with errors predicted by UNICORN having on average \u223c5 words per span vs \u223c2 words in the ground truth) and have overall low span precision. Similarly to overall score cor- relations, scale also seems to be important for the quality of spans produced by AUTOMQM, with UNICORN outperforming BISON at most metrics. Additionally, UNICORN prompted with AutoMQM predicts spans of comparable quality to the ones produced by current state-of-the-art learned word- level evaluators (trained on a considerable number of fine-grained annotations derived from MQM): while word-level models are more precise, their overall span correlation (MCC) is comparable, and they miss considerably more major errors than LLMs (despite only leveraging a handful of an- notations). 7 Conclusion In this study, we have systematically investi- gated the capabilities of large language models for machine translation evaluation through score prediction, and proposed AUTOMQM, a novel prompting technique that leverages the Multidi- mensional Quality Metrics (MQM) framework for interpretable MT evaluation using LLMs. We demonstrated that just prompting LLMs for score prediction leads to state-of-the-art system- level evaluators, but still falls short of the best learned metrics at the segment-level (with fine- tuning being necessary to close this gap). Then we showed that AUTOMQM can further improve the performance of LLMs without finetuning while providing interpretability through error spans that align with human annotations. Our findings surrounding finetuning LLMs for score prediction hint that LLMs\u2019 performance in machine translation evaluation could be further im- proved by finetuning these models on fine-grained human judgment data (like MQM) and is a direc- tion we are actively pursuing. Additionally, the general-purpose nature of LLMs may enable the application of similar prompting techniques (lever- aging some fine-grained evaluation schemes) to other evaluation problems (Wu et al., 2023). Acknowledgements We would like to thank Ricardo Rei, Marcos Treviso and Chryssa Zerva for helping run the word-level QE baselines, and George Foster who provided"}, {"question": " What does the text indicate about the precision of error spans predicted by UNICORN compared to the ground truth?", "answer": " The text indicates that errors predicted by UNICORN have on average approximately 5 words per span, compared to approximately 2 words in the ground truth.", "ref_chunk": "to be lower, with most example sets exhibiting less than 0.05 Pearson dif- ference from the best-performing sets. All this sug- gests that LLM evaluators are much more robust to the choice of in-context examples when prompted for AUTOMQM rather than for score prediction. We also find that the behavior of in-context learn- ing is quite similar for both reference-based and reference-less evaluation tasks. Finally, we observe PaLM-2 (Bison) PaLM-2 (Bison) 0.30 0.15 PaLM-2 (Bison) ref-free 0.30Pearson (EN-DE) 0.05 0.15 PaLM-2 (Bison) ref-free 0.10 3 0.00 1 0 0 0.00 3 2 2 6# of in-context examples 1 0.35Pearson (ZH-EN) 4 4 6# of in-context examples 5 5 0.05 0.25 0.25 0.20 0.20 0.10 Figure 8: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the AUTOMQM prompt, for EN-DE (left) and ZH-EN (right). System-Level Segment-Level All (2 LPs) EN-DE ZH-EN Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL MATESE COMET-QE MATESE-QE COMET-WL \u2713 \u2713 \u2717 \u2717 \u2717 81.1% 79.9% 76.9% 73.4% 71.6% 0.549 0.391 0.419 0.298 0.418 61.1% 0.581 58.8% 0.528 56.3% 0.505 57.9% 0.468 57.1% 0.406 54.6% 51.5% 48.8% 50.1% 51.5% Score Prediction PaLM-2 BISON PaLM-2 UNICORN PaLM-2 BISON PaLM-2 UNICORN \u2713 \u2713 \u2717 \u2717 86.4% 86.4% 84.0% 80.5% 0.394 0.401 0.355 0.275 56.8% 0.322 56.3% 0.349 57.0% 0.299 56.1% 0.252 49.3% 51.1% 48.6% 48.3% AutoMQM PaLM-2 BISON PaLM-2 UNICORN PaLM 2 BISON PaLM 2 UNICORN \u2713 \u2713 \u2717 \u2717 84.0% 87.6% 87.6% 83.4% 0.369 0.432 0.297 0.368 59.2% 0.355 59.1% 0.442 55.2% 0.331 56.4% 0.429 48.4% 51.8% 48.0% 50.2% Table 5: Meta-evaluation results for PaLM-2 models using AutoMQM and score prediction, at the system and segment levels for multiple language pairs. that the example sets that perform well for one task generally work well for the other, with performance on both settings given a fixed in-context set being highly correlated, as shown in Figure 9. 0.30 0.20 5 0.25 0.3Pearson w/o Reference 2 0.10 0.0 0.05 0.2 6Number of Examples 4 0.1 0.00 0.15 0.35Pearson with Reference 1 Corr = 0.941 3 Table 5 shows the meta-evaluation results for PaLM-2 BISON and UNICORN prompted with AUTOMQM (using the best-performing in-context learning sets in Figure 8). For ease of comparison, we also report their performance when prompted for score prediction, as well as the performance of the baselines. Overall, prompting LLMs with AUTOMQM seems to lead to significant improve- ments in evaluating machine translation quality, particularly for larger models: UNICORN achieves better performance (across all meta evaluations) with it than when prompted for score prediction, and its reference-less version is competitive with the best learned metric even at the segment level. However, for the smaller BISON, the benefits of AUTOMQM are less clear, with both techniques performing comparably. This hints that scale is necessary for zero- and few- shot fine-grained evalu- ation (like with AUTOMQM). We also find that the Figure 9: Scatter plot of the Pearson of PaLM-2 (BISON) models, with/without including the reference in the prompt, for each in-context learning setting tried. distribution of scores produced by LLMs prompted with AUTOMQM is much closer to the gold MQM distribution, with models outputting a much larger set of scores, and in the same ranges as annotators do (see Figure 10). 10 10000 1000 1 30 0 PaLM-2 (Unicorn)Score# occurrences (en-de) PaLM-2 (Bison) 35 100 25 5 15 40 10 Gold MQM 20 Figure 10: Distribution of scores for PaLM-2 models using AUTOMQM, on WMT22 EN-DE EN-DE ZH-EN Model R? SP MR MCC SP MR MCC Baselines COMET-WL \u2717 0.267 0.250 0.161 0.364 0.178 0.152 AutoMQM BISON UNICORN BISON UNICORN \u2713 0.095 0.749 0.060 0.252 0.255 0.109 \u2713 0.175 0.628 0.193 0.238 0.476 0.143 \u2717 0.119 0.520 0.092 0.224 0.311 0.091 \u2717 0.150 0.580 0.150 0.229 0.488 0.133 Table 6: Span-level meta-evaluation on WMT22 for PaLM-2 models using AutoMQM. SR and MR represent span precision and major recall, respectively. Finally, when evaluating the error spans pro- duced by LLMs prompted with AUTOMQM (Ta- ble 6), we find that PaLM-2 models are able to identify most of the major errors. However, it does seem to over-predict errors (with errors predicted by UNICORN having on average \u223c5 words per span vs \u223c2 words in the ground truth) and have overall low span precision. Similarly to overall score cor- relations, scale also seems to be important for the quality of spans produced by AUTOMQM, with UNICORN outperforming BISON at most metrics. Additionally, UNICORN prompted with AutoMQM predicts spans of comparable quality to the ones produced by current state-of-the-art learned word- level evaluators (trained on a considerable number of fine-grained annotations derived from MQM): while word-level models are more precise, their overall span correlation (MCC) is comparable, and they miss considerably more major errors than LLMs (despite only leveraging a handful of an- notations). 7 Conclusion In this study, we have systematically investi- gated the capabilities of large language models for machine translation evaluation through score prediction, and proposed AUTOMQM, a novel prompting technique that leverages the Multidi- mensional Quality Metrics (MQM) framework for interpretable MT evaluation using LLMs. We demonstrated that just prompting LLMs for score prediction leads to state-of-the-art system- level evaluators, but still falls short of the best learned metrics at the segment-level (with fine- tuning being necessary to close this gap). Then we showed that AUTOMQM can further improve the performance of LLMs without finetuning while providing interpretability through error spans that align with human annotations. Our findings surrounding finetuning LLMs for score prediction hint that LLMs\u2019 performance in machine translation evaluation could be further im- proved by finetuning these models on fine-grained human judgment data (like MQM) and is a direc- tion we are actively pursuing. Additionally, the general-purpose nature of LLMs may enable the application of similar prompting techniques (lever- aging some fine-grained evaluation schemes) to other evaluation problems (Wu et al., 2023). Acknowledgements We would like to thank Ricardo Rei, Marcos Treviso and Chryssa Zerva for helping run the word-level QE baselines, and George Foster who provided"}, {"question": " What is a conclusion drawn from the study regarding the capabilities of large language models (LLMs) for machine translation evaluation?", "answer": " The conclusion drawn from the study is that large language models can be further improved for machine translation evaluation by fine-tuning these models on fine-grained human judgment data like Multidimensional Quality Metrics (MQM).", "ref_chunk": "to be lower, with most example sets exhibiting less than 0.05 Pearson dif- ference from the best-performing sets. All this sug- gests that LLM evaluators are much more robust to the choice of in-context examples when prompted for AUTOMQM rather than for score prediction. We also find that the behavior of in-context learn- ing is quite similar for both reference-based and reference-less evaluation tasks. Finally, we observe PaLM-2 (Bison) PaLM-2 (Bison) 0.30 0.15 PaLM-2 (Bison) ref-free 0.30Pearson (EN-DE) 0.05 0.15 PaLM-2 (Bison) ref-free 0.10 3 0.00 1 0 0 0.00 3 2 2 6# of in-context examples 1 0.35Pearson (ZH-EN) 4 4 6# of in-context examples 5 5 0.05 0.25 0.25 0.20 0.20 0.10 Figure 8: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the AUTOMQM prompt, for EN-DE (left) and ZH-EN (right). System-Level Segment-Level All (2 LPs) EN-DE ZH-EN Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL MATESE COMET-QE MATESE-QE COMET-WL \u2713 \u2713 \u2717 \u2717 \u2717 81.1% 79.9% 76.9% 73.4% 71.6% 0.549 0.391 0.419 0.298 0.418 61.1% 0.581 58.8% 0.528 56.3% 0.505 57.9% 0.468 57.1% 0.406 54.6% 51.5% 48.8% 50.1% 51.5% Score Prediction PaLM-2 BISON PaLM-2 UNICORN PaLM-2 BISON PaLM-2 UNICORN \u2713 \u2713 \u2717 \u2717 86.4% 86.4% 84.0% 80.5% 0.394 0.401 0.355 0.275 56.8% 0.322 56.3% 0.349 57.0% 0.299 56.1% 0.252 49.3% 51.1% 48.6% 48.3% AutoMQM PaLM-2 BISON PaLM-2 UNICORN PaLM 2 BISON PaLM 2 UNICORN \u2713 \u2713 \u2717 \u2717 84.0% 87.6% 87.6% 83.4% 0.369 0.432 0.297 0.368 59.2% 0.355 59.1% 0.442 55.2% 0.331 56.4% 0.429 48.4% 51.8% 48.0% 50.2% Table 5: Meta-evaluation results for PaLM-2 models using AutoMQM and score prediction, at the system and segment levels for multiple language pairs. that the example sets that perform well for one task generally work well for the other, with performance on both settings given a fixed in-context set being highly correlated, as shown in Figure 9. 0.30 0.20 5 0.25 0.3Pearson w/o Reference 2 0.10 0.0 0.05 0.2 6Number of Examples 4 0.1 0.00 0.15 0.35Pearson with Reference 1 Corr = 0.941 3 Table 5 shows the meta-evaluation results for PaLM-2 BISON and UNICORN prompted with AUTOMQM (using the best-performing in-context learning sets in Figure 8). For ease of comparison, we also report their performance when prompted for score prediction, as well as the performance of the baselines. Overall, prompting LLMs with AUTOMQM seems to lead to significant improve- ments in evaluating machine translation quality, particularly for larger models: UNICORN achieves better performance (across all meta evaluations) with it than when prompted for score prediction, and its reference-less version is competitive with the best learned metric even at the segment level. However, for the smaller BISON, the benefits of AUTOMQM are less clear, with both techniques performing comparably. This hints that scale is necessary for zero- and few- shot fine-grained evalu- ation (like with AUTOMQM). We also find that the Figure 9: Scatter plot of the Pearson of PaLM-2 (BISON) models, with/without including the reference in the prompt, for each in-context learning setting tried. distribution of scores produced by LLMs prompted with AUTOMQM is much closer to the gold MQM distribution, with models outputting a much larger set of scores, and in the same ranges as annotators do (see Figure 10). 10 10000 1000 1 30 0 PaLM-2 (Unicorn)Score# occurrences (en-de) PaLM-2 (Bison) 35 100 25 5 15 40 10 Gold MQM 20 Figure 10: Distribution of scores for PaLM-2 models using AUTOMQM, on WMT22 EN-DE EN-DE ZH-EN Model R? SP MR MCC SP MR MCC Baselines COMET-WL \u2717 0.267 0.250 0.161 0.364 0.178 0.152 AutoMQM BISON UNICORN BISON UNICORN \u2713 0.095 0.749 0.060 0.252 0.255 0.109 \u2713 0.175 0.628 0.193 0.238 0.476 0.143 \u2717 0.119 0.520 0.092 0.224 0.311 0.091 \u2717 0.150 0.580 0.150 0.229 0.488 0.133 Table 6: Span-level meta-evaluation on WMT22 for PaLM-2 models using AutoMQM. SR and MR represent span precision and major recall, respectively. Finally, when evaluating the error spans pro- duced by LLMs prompted with AUTOMQM (Ta- ble 6), we find that PaLM-2 models are able to identify most of the major errors. However, it does seem to over-predict errors (with errors predicted by UNICORN having on average \u223c5 words per span vs \u223c2 words in the ground truth) and have overall low span precision. Similarly to overall score cor- relations, scale also seems to be important for the quality of spans produced by AUTOMQM, with UNICORN outperforming BISON at most metrics. Additionally, UNICORN prompted with AutoMQM predicts spans of comparable quality to the ones produced by current state-of-the-art learned word- level evaluators (trained on a considerable number of fine-grained annotations derived from MQM): while word-level models are more precise, their overall span correlation (MCC) is comparable, and they miss considerably more major errors than LLMs (despite only leveraging a handful of an- notations). 7 Conclusion In this study, we have systematically investi- gated the capabilities of large language models for machine translation evaluation through score prediction, and proposed AUTOMQM, a novel prompting technique that leverages the Multidi- mensional Quality Metrics (MQM) framework for interpretable MT evaluation using LLMs. We demonstrated that just prompting LLMs for score prediction leads to state-of-the-art system- level evaluators, but still falls short of the best learned metrics at the segment-level (with fine- tuning being necessary to close this gap). Then we showed that AUTOMQM can further improve the performance of LLMs without finetuning while providing interpretability through error spans that align with human annotations. Our findings surrounding finetuning LLMs for score prediction hint that LLMs\u2019 performance in machine translation evaluation could be further im- proved by finetuning these models on fine-grained human judgment data (like MQM) and is a direc- tion we are actively pursuing. Additionally, the general-purpose nature of LLMs may enable the application of similar prompting techniques (lever- aging some fine-grained evaluation schemes) to other evaluation problems (Wu et al., 2023). Acknowledgements We would like to thank Ricardo Rei, Marcos Treviso and Chryssa Zerva for helping run the word-level QE baselines, and George Foster who provided"}, {"question": " According to the text, what is proposed as a novel prompting technique for interpretable machine translation evaluation using LLMs?", "answer": " The text proposes AUTOMQM as a novel prompting technique that leverages the MQM framework for interpretable machine translation evaluation using LLMs.", "ref_chunk": "to be lower, with most example sets exhibiting less than 0.05 Pearson dif- ference from the best-performing sets. All this sug- gests that LLM evaluators are much more robust to the choice of in-context examples when prompted for AUTOMQM rather than for score prediction. We also find that the behavior of in-context learn- ing is quite similar for both reference-based and reference-less evaluation tasks. Finally, we observe PaLM-2 (Bison) PaLM-2 (Bison) 0.30 0.15 PaLM-2 (Bison) ref-free 0.30Pearson (EN-DE) 0.05 0.15 PaLM-2 (Bison) ref-free 0.10 3 0.00 1 0 0 0.00 3 2 2 6# of in-context examples 1 0.35Pearson (ZH-EN) 4 4 6# of in-context examples 5 5 0.05 0.25 0.25 0.20 0.20 0.10 Figure 8: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the AUTOMQM prompt, for EN-DE (left) and ZH-EN (right). System-Level Segment-Level All (2 LPs) EN-DE ZH-EN Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL MATESE COMET-QE MATESE-QE COMET-WL \u2713 \u2713 \u2717 \u2717 \u2717 81.1% 79.9% 76.9% 73.4% 71.6% 0.549 0.391 0.419 0.298 0.418 61.1% 0.581 58.8% 0.528 56.3% 0.505 57.9% 0.468 57.1% 0.406 54.6% 51.5% 48.8% 50.1% 51.5% Score Prediction PaLM-2 BISON PaLM-2 UNICORN PaLM-2 BISON PaLM-2 UNICORN \u2713 \u2713 \u2717 \u2717 86.4% 86.4% 84.0% 80.5% 0.394 0.401 0.355 0.275 56.8% 0.322 56.3% 0.349 57.0% 0.299 56.1% 0.252 49.3% 51.1% 48.6% 48.3% AutoMQM PaLM-2 BISON PaLM-2 UNICORN PaLM 2 BISON PaLM 2 UNICORN \u2713 \u2713 \u2717 \u2717 84.0% 87.6% 87.6% 83.4% 0.369 0.432 0.297 0.368 59.2% 0.355 59.1% 0.442 55.2% 0.331 56.4% 0.429 48.4% 51.8% 48.0% 50.2% Table 5: Meta-evaluation results for PaLM-2 models using AutoMQM and score prediction, at the system and segment levels for multiple language pairs. that the example sets that perform well for one task generally work well for the other, with performance on both settings given a fixed in-context set being highly correlated, as shown in Figure 9. 0.30 0.20 5 0.25 0.3Pearson w/o Reference 2 0.10 0.0 0.05 0.2 6Number of Examples 4 0.1 0.00 0.15 0.35Pearson with Reference 1 Corr = 0.941 3 Table 5 shows the meta-evaluation results for PaLM-2 BISON and UNICORN prompted with AUTOMQM (using the best-performing in-context learning sets in Figure 8). For ease of comparison, we also report their performance when prompted for score prediction, as well as the performance of the baselines. Overall, prompting LLMs with AUTOMQM seems to lead to significant improve- ments in evaluating machine translation quality, particularly for larger models: UNICORN achieves better performance (across all meta evaluations) with it than when prompted for score prediction, and its reference-less version is competitive with the best learned metric even at the segment level. However, for the smaller BISON, the benefits of AUTOMQM are less clear, with both techniques performing comparably. This hints that scale is necessary for zero- and few- shot fine-grained evalu- ation (like with AUTOMQM). We also find that the Figure 9: Scatter plot of the Pearson of PaLM-2 (BISON) models, with/without including the reference in the prompt, for each in-context learning setting tried. distribution of scores produced by LLMs prompted with AUTOMQM is much closer to the gold MQM distribution, with models outputting a much larger set of scores, and in the same ranges as annotators do (see Figure 10). 10 10000 1000 1 30 0 PaLM-2 (Unicorn)Score# occurrences (en-de) PaLM-2 (Bison) 35 100 25 5 15 40 10 Gold MQM 20 Figure 10: Distribution of scores for PaLM-2 models using AUTOMQM, on WMT22 EN-DE EN-DE ZH-EN Model R? SP MR MCC SP MR MCC Baselines COMET-WL \u2717 0.267 0.250 0.161 0.364 0.178 0.152 AutoMQM BISON UNICORN BISON UNICORN \u2713 0.095 0.749 0.060 0.252 0.255 0.109 \u2713 0.175 0.628 0.193 0.238 0.476 0.143 \u2717 0.119 0.520 0.092 0.224 0.311 0.091 \u2717 0.150 0.580 0.150 0.229 0.488 0.133 Table 6: Span-level meta-evaluation on WMT22 for PaLM-2 models using AutoMQM. SR and MR represent span precision and major recall, respectively. Finally, when evaluating the error spans pro- duced by LLMs prompted with AUTOMQM (Ta- ble 6), we find that PaLM-2 models are able to identify most of the major errors. However, it does seem to over-predict errors (with errors predicted by UNICORN having on average \u223c5 words per span vs \u223c2 words in the ground truth) and have overall low span precision. Similarly to overall score cor- relations, scale also seems to be important for the quality of spans produced by AUTOMQM, with UNICORN outperforming BISON at most metrics. Additionally, UNICORN prompted with AutoMQM predicts spans of comparable quality to the ones produced by current state-of-the-art learned word- level evaluators (trained on a considerable number of fine-grained annotations derived from MQM): while word-level models are more precise, their overall span correlation (MCC) is comparable, and they miss considerably more major errors than LLMs (despite only leveraging a handful of an- notations). 7 Conclusion In this study, we have systematically investi- gated the capabilities of large language models for machine translation evaluation through score prediction, and proposed AUTOMQM, a novel prompting technique that leverages the Multidi- mensional Quality Metrics (MQM) framework for interpretable MT evaluation using LLMs. We demonstrated that just prompting LLMs for score prediction leads to state-of-the-art system- level evaluators, but still falls short of the best learned metrics at the segment-level (with fine- tuning being necessary to close this gap). Then we showed that AUTOMQM can further improve the performance of LLMs without finetuning while providing interpretability through error spans that align with human annotations. Our findings surrounding finetuning LLMs for score prediction hint that LLMs\u2019 performance in machine translation evaluation could be further im- proved by finetuning these models on fine-grained human judgment data (like MQM) and is a direc- tion we are actively pursuing. Additionally, the general-purpose nature of LLMs may enable the application of similar prompting techniques (lever- aging some fine-grained evaluation schemes) to other evaluation problems (Wu et al., 2023). Acknowledgements We would like to thank Ricardo Rei, Marcos Treviso and Chryssa Zerva for helping run the word-level QE baselines, and George Foster who provided"}, {"question": " Who are some individuals acknowledged in the text for their contributions?", "answer": " Ricardo Rei, Marcos Treviso, Chryssa Zerva, and George Foster are acknowledged in the text for their contributions.", "ref_chunk": "to be lower, with most example sets exhibiting less than 0.05 Pearson dif- ference from the best-performing sets. All this sug- gests that LLM evaluators are much more robust to the choice of in-context examples when prompted for AUTOMQM rather than for score prediction. We also find that the behavior of in-context learn- ing is quite similar for both reference-based and reference-less evaluation tasks. Finally, we observe PaLM-2 (Bison) PaLM-2 (Bison) 0.30 0.15 PaLM-2 (Bison) ref-free 0.30Pearson (EN-DE) 0.05 0.15 PaLM-2 (Bison) ref-free 0.10 3 0.00 1 0 0 0.00 3 2 2 6# of in-context examples 1 0.35Pearson (ZH-EN) 4 4 6# of in-context examples 5 5 0.05 0.25 0.25 0.20 0.20 0.10 Figure 8: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the AUTOMQM prompt, for EN-DE (left) and ZH-EN (right). System-Level Segment-Level All (2 LPs) EN-DE ZH-EN Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL MATESE COMET-QE MATESE-QE COMET-WL \u2713 \u2713 \u2717 \u2717 \u2717 81.1% 79.9% 76.9% 73.4% 71.6% 0.549 0.391 0.419 0.298 0.418 61.1% 0.581 58.8% 0.528 56.3% 0.505 57.9% 0.468 57.1% 0.406 54.6% 51.5% 48.8% 50.1% 51.5% Score Prediction PaLM-2 BISON PaLM-2 UNICORN PaLM-2 BISON PaLM-2 UNICORN \u2713 \u2713 \u2717 \u2717 86.4% 86.4% 84.0% 80.5% 0.394 0.401 0.355 0.275 56.8% 0.322 56.3% 0.349 57.0% 0.299 56.1% 0.252 49.3% 51.1% 48.6% 48.3% AutoMQM PaLM-2 BISON PaLM-2 UNICORN PaLM 2 BISON PaLM 2 UNICORN \u2713 \u2713 \u2717 \u2717 84.0% 87.6% 87.6% 83.4% 0.369 0.432 0.297 0.368 59.2% 0.355 59.1% 0.442 55.2% 0.331 56.4% 0.429 48.4% 51.8% 48.0% 50.2% Table 5: Meta-evaluation results for PaLM-2 models using AutoMQM and score prediction, at the system and segment levels for multiple language pairs. that the example sets that perform well for one task generally work well for the other, with performance on both settings given a fixed in-context set being highly correlated, as shown in Figure 9. 0.30 0.20 5 0.25 0.3Pearson w/o Reference 2 0.10 0.0 0.05 0.2 6Number of Examples 4 0.1 0.00 0.15 0.35Pearson with Reference 1 Corr = 0.941 3 Table 5 shows the meta-evaluation results for PaLM-2 BISON and UNICORN prompted with AUTOMQM (using the best-performing in-context learning sets in Figure 8). For ease of comparison, we also report their performance when prompted for score prediction, as well as the performance of the baselines. Overall, prompting LLMs with AUTOMQM seems to lead to significant improve- ments in evaluating machine translation quality, particularly for larger models: UNICORN achieves better performance (across all meta evaluations) with it than when prompted for score prediction, and its reference-less version is competitive with the best learned metric even at the segment level. However, for the smaller BISON, the benefits of AUTOMQM are less clear, with both techniques performing comparably. This hints that scale is necessary for zero- and few- shot fine-grained evalu- ation (like with AUTOMQM). We also find that the Figure 9: Scatter plot of the Pearson of PaLM-2 (BISON) models, with/without including the reference in the prompt, for each in-context learning setting tried. distribution of scores produced by LLMs prompted with AUTOMQM is much closer to the gold MQM distribution, with models outputting a much larger set of scores, and in the same ranges as annotators do (see Figure 10). 10 10000 1000 1 30 0 PaLM-2 (Unicorn)Score# occurrences (en-de) PaLM-2 (Bison) 35 100 25 5 15 40 10 Gold MQM 20 Figure 10: Distribution of scores for PaLM-2 models using AUTOMQM, on WMT22 EN-DE EN-DE ZH-EN Model R? SP MR MCC SP MR MCC Baselines COMET-WL \u2717 0.267 0.250 0.161 0.364 0.178 0.152 AutoMQM BISON UNICORN BISON UNICORN \u2713 0.095 0.749 0.060 0.252 0.255 0.109 \u2713 0.175 0.628 0.193 0.238 0.476 0.143 \u2717 0.119 0.520 0.092 0.224 0.311 0.091 \u2717 0.150 0.580 0.150 0.229 0.488 0.133 Table 6: Span-level meta-evaluation on WMT22 for PaLM-2 models using AutoMQM. SR and MR represent span precision and major recall, respectively. Finally, when evaluating the error spans pro- duced by LLMs prompted with AUTOMQM (Ta- ble 6), we find that PaLM-2 models are able to identify most of the major errors. However, it does seem to over-predict errors (with errors predicted by UNICORN having on average \u223c5 words per span vs \u223c2 words in the ground truth) and have overall low span precision. Similarly to overall score cor- relations, scale also seems to be important for the quality of spans produced by AUTOMQM, with UNICORN outperforming BISON at most metrics. Additionally, UNICORN prompted with AutoMQM predicts spans of comparable quality to the ones produced by current state-of-the-art learned word- level evaluators (trained on a considerable number of fine-grained annotations derived from MQM): while word-level models are more precise, their overall span correlation (MCC) is comparable, and they miss considerably more major errors than LLMs (despite only leveraging a handful of an- notations). 7 Conclusion In this study, we have systematically investi- gated the capabilities of large language models for machine translation evaluation through score prediction, and proposed AUTOMQM, a novel prompting technique that leverages the Multidi- mensional Quality Metrics (MQM) framework for interpretable MT evaluation using LLMs. We demonstrated that just prompting LLMs for score prediction leads to state-of-the-art system- level evaluators, but still falls short of the best learned metrics at the segment-level (with fine- tuning being necessary to close this gap). Then we showed that AUTOMQM can further improve the performance of LLMs without finetuning while providing interpretability through error spans that align with human annotations. Our findings surrounding finetuning LLMs for score prediction hint that LLMs\u2019 performance in machine translation evaluation could be further im- proved by finetuning these models on fine-grained human judgment data (like MQM) and is a direc- tion we are actively pursuing. Additionally, the general-purpose nature of LLMs may enable the application of similar prompting techniques (lever- aging some fine-grained evaluation schemes) to other evaluation problems (Wu et al., 2023). Acknowledgements We would like to thank Ricardo Rei, Marcos Treviso and Chryssa Zerva for helping run the word-level QE baselines, and George Foster who provided"}], "doc_text": "to be lower, with most example sets exhibiting less than 0.05 Pearson dif- ference from the best-performing sets. All this sug- gests that LLM evaluators are much more robust to the choice of in-context examples when prompted for AUTOMQM rather than for score prediction. We also find that the behavior of in-context learn- ing is quite similar for both reference-based and reference-less evaluation tasks. Finally, we observe PaLM-2 (Bison) PaLM-2 (Bison) 0.30 0.15 PaLM-2 (Bison) ref-free 0.30Pearson (EN-DE) 0.05 0.15 PaLM-2 (Bison) ref-free 0.10 3 0.00 1 0 0 0.00 3 2 2 6# of in-context examples 1 0.35Pearson (ZH-EN) 4 4 6# of in-context examples 5 5 0.05 0.25 0.25 0.20 0.20 0.10 Figure 8: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the AUTOMQM prompt, for EN-DE (left) and ZH-EN (right). System-Level Segment-Level All (2 LPs) EN-DE ZH-EN Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL MATESE COMET-QE MATESE-QE COMET-WL \u2713 \u2713 \u2717 \u2717 \u2717 81.1% 79.9% 76.9% 73.4% 71.6% 0.549 0.391 0.419 0.298 0.418 61.1% 0.581 58.8% 0.528 56.3% 0.505 57.9% 0.468 57.1% 0.406 54.6% 51.5% 48.8% 50.1% 51.5% Score Prediction PaLM-2 BISON PaLM-2 UNICORN PaLM-2 BISON PaLM-2 UNICORN \u2713 \u2713 \u2717 \u2717 86.4% 86.4% 84.0% 80.5% 0.394 0.401 0.355 0.275 56.8% 0.322 56.3% 0.349 57.0% 0.299 56.1% 0.252 49.3% 51.1% 48.6% 48.3% AutoMQM PaLM-2 BISON PaLM-2 UNICORN PaLM 2 BISON PaLM 2 UNICORN \u2713 \u2713 \u2717 \u2717 84.0% 87.6% 87.6% 83.4% 0.369 0.432 0.297 0.368 59.2% 0.355 59.1% 0.442 55.2% 0.331 56.4% 0.429 48.4% 51.8% 48.0% 50.2% Table 5: Meta-evaluation results for PaLM-2 models using AutoMQM and score prediction, at the system and segment levels for multiple language pairs. that the example sets that perform well for one task generally work well for the other, with performance on both settings given a fixed in-context set being highly correlated, as shown in Figure 9. 0.30 0.20 5 0.25 0.3Pearson w/o Reference 2 0.10 0.0 0.05 0.2 6Number of Examples 4 0.1 0.00 0.15 0.35Pearson with Reference 1 Corr = 0.941 3 Table 5 shows the meta-evaluation results for PaLM-2 BISON and UNICORN prompted with AUTOMQM (using the best-performing in-context learning sets in Figure 8). For ease of comparison, we also report their performance when prompted for score prediction, as well as the performance of the baselines. Overall, prompting LLMs with AUTOMQM seems to lead to significant improve- ments in evaluating machine translation quality, particularly for larger models: UNICORN achieves better performance (across all meta evaluations) with it than when prompted for score prediction, and its reference-less version is competitive with the best learned metric even at the segment level. However, for the smaller BISON, the benefits of AUTOMQM are less clear, with both techniques performing comparably. This hints that scale is necessary for zero- and few- shot fine-grained evalu- ation (like with AUTOMQM). We also find that the Figure 9: Scatter plot of the Pearson of PaLM-2 (BISON) models, with/without including the reference in the prompt, for each in-context learning setting tried. distribution of scores produced by LLMs prompted with AUTOMQM is much closer to the gold MQM distribution, with models outputting a much larger set of scores, and in the same ranges as annotators do (see Figure 10). 10 10000 1000 1 30 0 PaLM-2 (Unicorn)Score# occurrences (en-de) PaLM-2 (Bison) 35 100 25 5 15 40 10 Gold MQM 20 Figure 10: Distribution of scores for PaLM-2 models using AUTOMQM, on WMT22 EN-DE EN-DE ZH-EN Model R? SP MR MCC SP MR MCC Baselines COMET-WL \u2717 0.267 0.250 0.161 0.364 0.178 0.152 AutoMQM BISON UNICORN BISON UNICORN \u2713 0.095 0.749 0.060 0.252 0.255 0.109 \u2713 0.175 0.628 0.193 0.238 0.476 0.143 \u2717 0.119 0.520 0.092 0.224 0.311 0.091 \u2717 0.150 0.580 0.150 0.229 0.488 0.133 Table 6: Span-level meta-evaluation on WMT22 for PaLM-2 models using AutoMQM. SR and MR represent span precision and major recall, respectively. Finally, when evaluating the error spans pro- duced by LLMs prompted with AUTOMQM (Ta- ble 6), we find that PaLM-2 models are able to identify most of the major errors. However, it does seem to over-predict errors (with errors predicted by UNICORN having on average \u223c5 words per span vs \u223c2 words in the ground truth) and have overall low span precision. Similarly to overall score cor- relations, scale also seems to be important for the quality of spans produced by AUTOMQM, with UNICORN outperforming BISON at most metrics. Additionally, UNICORN prompted with AutoMQM predicts spans of comparable quality to the ones produced by current state-of-the-art learned word- level evaluators (trained on a considerable number of fine-grained annotations derived from MQM): while word-level models are more precise, their overall span correlation (MCC) is comparable, and they miss considerably more major errors than LLMs (despite only leveraging a handful of an- notations). 7 Conclusion In this study, we have systematically investi- gated the capabilities of large language models for machine translation evaluation through score prediction, and proposed AUTOMQM, a novel prompting technique that leverages the Multidi- mensional Quality Metrics (MQM) framework for interpretable MT evaluation using LLMs. We demonstrated that just prompting LLMs for score prediction leads to state-of-the-art system- level evaluators, but still falls short of the best learned metrics at the segment-level (with fine- tuning being necessary to close this gap). Then we showed that AUTOMQM can further improve the performance of LLMs without finetuning while providing interpretability through error spans that align with human annotations. Our findings surrounding finetuning LLMs for score prediction hint that LLMs\u2019 performance in machine translation evaluation could be further im- proved by finetuning these models on fine-grained human judgment data (like MQM) and is a direc- tion we are actively pursuing. Additionally, the general-purpose nature of LLMs may enable the application of similar prompting techniques (lever- aging some fine-grained evaluation schemes) to other evaluation problems (Wu et al., 2023). Acknowledgements We would like to thank Ricardo Rei, Marcos Treviso and Chryssa Zerva for helping run the word-level QE baselines, and George Foster who provided"}