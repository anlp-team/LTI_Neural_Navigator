{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Alexander_I._Rudnicky_Overview_of_Robust_and_Multilingual_Automatic_Evaluation_Metrics\n\nfor_Open-Domain_Dialogue_Systems_at_DSTC_11_Track_4_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the article?,        answer: The main focus of the article is on robust and multilingual automatic evaluation metrics for open-domain dialogue systems.    ", "ref_chunk": "3 2 0 2 p e S 4 1 ] L C . s c [ 3 v 4 9 7 2 1 . 6 0 3 2 : v i X r a Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4 Mario Rodr\u00edguez-Cantelar1, Chen Zhang2, Chengguang Tang3, Ke Shi3, Sarik Ghazarian4, Jo\u00e3o Sedoc5, Luis Fernando D\u2019Haro1 and Alexander Rudnicky6 1Speech Technology and Machine Learning Group - Universidad Polit\u00e9cnica de Madrid, Spain 2National University of Singapore, Singapore 3Tencent AI Lab, China 4University of Southern California, USA 5Department of Technology, Operations, and Statistics, New York University, USA 6Carnegie Mellon University, USA 1 Abstract The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have trig- gered various challenges regarding their au- tomatic evaluation. Automatic evaluation of open-domain dialogue systems as an open chal- lenge has been the center of the attention of many researchers. Despite the consistent ef- forts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technol- ogy Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual auto- matic evaluation metrics. This article describes the datasets and baselines provided to partici- pants and discusses the submission and result details of the two proposed subtasks. Introduction dialogue systems. Common metrics are based on word overlap, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which mainly focus on matching syntactic information with a set of golden references. Unfortunately, such metrics correlate poorly with human judgments (Liu et al., 2016) as in open-domain dialogue, there can be limitless feasible responses w.r.t. a dialogue context. Alternatively, recently developed model-based metrics such as BERTscore (Sun et al., 2022), BLEURT (Sellam et al., 2020), FED (Mehri and Eskenazi, 2020a), and MDD-Eval (Zhang et al., 2022a), which take advantage of the strong seman- tic representation capability of pre-trained trans- former language models, perform the evaluation at semantic and partially pragmatic levels. Some of them do not even need golden references as input. Regrettably, despite their improvement over the word-overlap metrics, these metrics are not perfect; that is, their correlation with human evaluation is still not strong. Moreover, most of them perform well only on a particular dimension (e.g., engaging- ness or coherence) (Zhang et al., 2022b), or specific to a single domain. In addition, their performance may be highly dependent on the datasets used for training and evaluation (Yeh et al., 2021). Recent advances in large-scale neural language models (Devlin et al., 2019; Radford et al., 2019; Zhang et al., 2020) have led to significant attention in dialogue systems, especially in the open domain category. Significant research efforts are dedicated to boost the robustness of dialogue systems, that is, improving their capability to perform well across multiple domains, dimensions, and handling hu- mans\u2019 diverse expressions of the same ideas (e.g., paraphrasing or back-translation). Automatic evaluation is an indispensable com- ponent for speeding up the development of robust Due to the lack of robust automatic evaluation metrics (Mehri and Eskenazi, 2020a), researchers have to resort to the time-consuming and cost- intensive human evaluation process to analyze the performance of their model and benchmark their proposed methods against baselines. Furthermore, to the best of our knowledge, none of the existing metrics have been thoroughly tested in a multilingual setting. Metric generalization across different languages is highly desirable, as it helps the transformation of state-of-the-art English- only dialogue systems into highly capable multi- lingual systems. Although multilingual pre-trained language models may exist and can be poten- tially used for training multilingual dialogue sys- tems, human-annotations or high-quality dialogue datasets for languages other than English are very scarce or even nonexistent in the case of some low- resource languages. To address this problem, we take advantage of recent advances in neural ma- chine translation and paraphrasing systems. Using existing high-quality services and models, it is pos- sible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics. To this end, we propose two subtasks in our track, and their details are listed as follows: 1.1 Track Details This track consists of two tasks which are explained in more detail below. Participants will develop effective open-ended and multilingual automatic dialogue evaluation metrics that perform similarly when evaluated in a new language. Participants will develop effec- tive open-ended automatic dialogue evaluation met- rics that perform robustly when evaluated over paraphrased/back-translated sentences in English. For both tasks, proposed metrics are expected to show the following two important properties, as indicated in (Deriu et al., 2021): 1. Correlated to human judgments - the metrics should produce evaluation scores that well correlate to human judgments (scores) across multiple languages or alternative responses (i.e., back-translated or paraphrased). 2. Explainable - the metrics should provide con- structive and explicit feedback to the gener- ative models in terms of the quality of their generated responses. For instance, if a gener- ative model contradicts itself, the evaluation metrics should signal such behavior. Participants can propose their own metrics or optionally improve the deep AM-FM (Zhang et al., 2021) baseline evaluation model provided by us. A leaderboard on the ChatEval platform1 was pro- vided to check the performance of their different 1https://chateval.org/dstc11 proposed models compared to those submitted by other researchers. For each evaluation task, Spearman\u2019s correla- tion was used to compare the proposed evaluation metrics against human judgments. A final average score was calculated to rank the submitted met- ric models. Additional instructions to participants were provided through the Github repository2 and by email on the main DSTC distribution list. 2 Task 1: Multilingual Automatic Metrics In"}, {"question": " Why have neural networks revolutionized the research on dialogue systems?,        answer: Neural networks have revolutionized the research on dialogue systems because they have triggered various challenges regarding automatic evaluation.    ", "ref_chunk": "3 2 0 2 p e S 4 1 ] L C . s c [ 3 v 4 9 7 2 1 . 6 0 3 2 : v i X r a Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4 Mario Rodr\u00edguez-Cantelar1, Chen Zhang2, Chengguang Tang3, Ke Shi3, Sarik Ghazarian4, Jo\u00e3o Sedoc5, Luis Fernando D\u2019Haro1 and Alexander Rudnicky6 1Speech Technology and Machine Learning Group - Universidad Polit\u00e9cnica de Madrid, Spain 2National University of Singapore, Singapore 3Tencent AI Lab, China 4University of Southern California, USA 5Department of Technology, Operations, and Statistics, New York University, USA 6Carnegie Mellon University, USA 1 Abstract The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have trig- gered various challenges regarding their au- tomatic evaluation. Automatic evaluation of open-domain dialogue systems as an open chal- lenge has been the center of the attention of many researchers. Despite the consistent ef- forts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technol- ogy Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual auto- matic evaluation metrics. This article describes the datasets and baselines provided to partici- pants and discusses the submission and result details of the two proposed subtasks. Introduction dialogue systems. Common metrics are based on word overlap, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which mainly focus on matching syntactic information with a set of golden references. Unfortunately, such metrics correlate poorly with human judgments (Liu et al., 2016) as in open-domain dialogue, there can be limitless feasible responses w.r.t. a dialogue context. Alternatively, recently developed model-based metrics such as BERTscore (Sun et al., 2022), BLEURT (Sellam et al., 2020), FED (Mehri and Eskenazi, 2020a), and MDD-Eval (Zhang et al., 2022a), which take advantage of the strong seman- tic representation capability of pre-trained trans- former language models, perform the evaluation at semantic and partially pragmatic levels. Some of them do not even need golden references as input. Regrettably, despite their improvement over the word-overlap metrics, these metrics are not perfect; that is, their correlation with human evaluation is still not strong. Moreover, most of them perform well only on a particular dimension (e.g., engaging- ness or coherence) (Zhang et al., 2022b), or specific to a single domain. In addition, their performance may be highly dependent on the datasets used for training and evaluation (Yeh et al., 2021). Recent advances in large-scale neural language models (Devlin et al., 2019; Radford et al., 2019; Zhang et al., 2020) have led to significant attention in dialogue systems, especially in the open domain category. Significant research efforts are dedicated to boost the robustness of dialogue systems, that is, improving their capability to perform well across multiple domains, dimensions, and handling hu- mans\u2019 diverse expressions of the same ideas (e.g., paraphrasing or back-translation). Automatic evaluation is an indispensable com- ponent for speeding up the development of robust Due to the lack of robust automatic evaluation metrics (Mehri and Eskenazi, 2020a), researchers have to resort to the time-consuming and cost- intensive human evaluation process to analyze the performance of their model and benchmark their proposed methods against baselines. Furthermore, to the best of our knowledge, none of the existing metrics have been thoroughly tested in a multilingual setting. Metric generalization across different languages is highly desirable, as it helps the transformation of state-of-the-art English- only dialogue systems into highly capable multi- lingual systems. Although multilingual pre-trained language models may exist and can be poten- tially used for training multilingual dialogue sys- tems, human-annotations or high-quality dialogue datasets for languages other than English are very scarce or even nonexistent in the case of some low- resource languages. To address this problem, we take advantage of recent advances in neural ma- chine translation and paraphrasing systems. Using existing high-quality services and models, it is pos- sible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics. To this end, we propose two subtasks in our track, and their details are listed as follows: 1.1 Track Details This track consists of two tasks which are explained in more detail below. Participants will develop effective open-ended and multilingual automatic dialogue evaluation metrics that perform similarly when evaluated in a new language. Participants will develop effec- tive open-ended automatic dialogue evaluation met- rics that perform robustly when evaluated over paraphrased/back-translated sentences in English. For both tasks, proposed metrics are expected to show the following two important properties, as indicated in (Deriu et al., 2021): 1. Correlated to human judgments - the metrics should produce evaluation scores that well correlate to human judgments (scores) across multiple languages or alternative responses (i.e., back-translated or paraphrased). 2. Explainable - the metrics should provide con- structive and explicit feedback to the gener- ative models in terms of the quality of their generated responses. For instance, if a gener- ative model contradicts itself, the evaluation metrics should signal such behavior. Participants can propose their own metrics or optionally improve the deep AM-FM (Zhang et al., 2021) baseline evaluation model provided by us. A leaderboard on the ChatEval platform1 was pro- vided to check the performance of their different 1https://chateval.org/dstc11 proposed models compared to those submitted by other researchers. For each evaluation task, Spearman\u2019s correla- tion was used to compare the proposed evaluation metrics against human judgments. A final average score was calculated to rank the submitted met- ric models. Additional instructions to participants were provided through the Github repository2 and by email on the main DSTC distribution list. 2 Task 1: Multilingual Automatic Metrics In"}, {"question": " What are some common metrics used for evaluating dialogue systems based on?,        answer: Common metrics used for evaluating dialogue systems are based on word overlap, such as BLEU and ROUGE, which focus on matching syntactic information with a set of golden references.    ", "ref_chunk": "3 2 0 2 p e S 4 1 ] L C . s c [ 3 v 4 9 7 2 1 . 6 0 3 2 : v i X r a Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4 Mario Rodr\u00edguez-Cantelar1, Chen Zhang2, Chengguang Tang3, Ke Shi3, Sarik Ghazarian4, Jo\u00e3o Sedoc5, Luis Fernando D\u2019Haro1 and Alexander Rudnicky6 1Speech Technology and Machine Learning Group - Universidad Polit\u00e9cnica de Madrid, Spain 2National University of Singapore, Singapore 3Tencent AI Lab, China 4University of Southern California, USA 5Department of Technology, Operations, and Statistics, New York University, USA 6Carnegie Mellon University, USA 1 Abstract The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have trig- gered various challenges regarding their au- tomatic evaluation. Automatic evaluation of open-domain dialogue systems as an open chal- lenge has been the center of the attention of many researchers. Despite the consistent ef- forts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technol- ogy Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual auto- matic evaluation metrics. This article describes the datasets and baselines provided to partici- pants and discusses the submission and result details of the two proposed subtasks. Introduction dialogue systems. Common metrics are based on word overlap, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which mainly focus on matching syntactic information with a set of golden references. Unfortunately, such metrics correlate poorly with human judgments (Liu et al., 2016) as in open-domain dialogue, there can be limitless feasible responses w.r.t. a dialogue context. Alternatively, recently developed model-based metrics such as BERTscore (Sun et al., 2022), BLEURT (Sellam et al., 2020), FED (Mehri and Eskenazi, 2020a), and MDD-Eval (Zhang et al., 2022a), which take advantage of the strong seman- tic representation capability of pre-trained trans- former language models, perform the evaluation at semantic and partially pragmatic levels. Some of them do not even need golden references as input. Regrettably, despite their improvement over the word-overlap metrics, these metrics are not perfect; that is, their correlation with human evaluation is still not strong. Moreover, most of them perform well only on a particular dimension (e.g., engaging- ness or coherence) (Zhang et al., 2022b), or specific to a single domain. In addition, their performance may be highly dependent on the datasets used for training and evaluation (Yeh et al., 2021). Recent advances in large-scale neural language models (Devlin et al., 2019; Radford et al., 2019; Zhang et al., 2020) have led to significant attention in dialogue systems, especially in the open domain category. Significant research efforts are dedicated to boost the robustness of dialogue systems, that is, improving their capability to perform well across multiple domains, dimensions, and handling hu- mans\u2019 diverse expressions of the same ideas (e.g., paraphrasing or back-translation). Automatic evaluation is an indispensable com- ponent for speeding up the development of robust Due to the lack of robust automatic evaluation metrics (Mehri and Eskenazi, 2020a), researchers have to resort to the time-consuming and cost- intensive human evaluation process to analyze the performance of their model and benchmark their proposed methods against baselines. Furthermore, to the best of our knowledge, none of the existing metrics have been thoroughly tested in a multilingual setting. Metric generalization across different languages is highly desirable, as it helps the transformation of state-of-the-art English- only dialogue systems into highly capable multi- lingual systems. Although multilingual pre-trained language models may exist and can be poten- tially used for training multilingual dialogue sys- tems, human-annotations or high-quality dialogue datasets for languages other than English are very scarce or even nonexistent in the case of some low- resource languages. To address this problem, we take advantage of recent advances in neural ma- chine translation and paraphrasing systems. Using existing high-quality services and models, it is pos- sible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics. To this end, we propose two subtasks in our track, and their details are listed as follows: 1.1 Track Details This track consists of two tasks which are explained in more detail below. Participants will develop effective open-ended and multilingual automatic dialogue evaluation metrics that perform similarly when evaluated in a new language. Participants will develop effec- tive open-ended automatic dialogue evaluation met- rics that perform robustly when evaluated over paraphrased/back-translated sentences in English. For both tasks, proposed metrics are expected to show the following two important properties, as indicated in (Deriu et al., 2021): 1. Correlated to human judgments - the metrics should produce evaluation scores that well correlate to human judgments (scores) across multiple languages or alternative responses (i.e., back-translated or paraphrased). 2. Explainable - the metrics should provide con- structive and explicit feedback to the gener- ative models in terms of the quality of their generated responses. For instance, if a gener- ative model contradicts itself, the evaluation metrics should signal such behavior. Participants can propose their own metrics or optionally improve the deep AM-FM (Zhang et al., 2021) baseline evaluation model provided by us. A leaderboard on the ChatEval platform1 was pro- vided to check the performance of their different 1https://chateval.org/dstc11 proposed models compared to those submitted by other researchers. For each evaluation task, Spearman\u2019s correla- tion was used to compare the proposed evaluation metrics against human judgments. A final average score was calculated to rank the submitted met- ric models. Additional instructions to participants were provided through the Github repository2 and by email on the main DSTC distribution list. 2 Task 1: Multilingual Automatic Metrics In"}, {"question": " Why do word-overlap metrics like BLEU and ROUGE correlate poorly with human judgments in open-domain dialogues?,        answer: Word-overlap metrics like BLEU and ROUGE correlate poorly with human judgments in open-domain dialogues because there can be limitless feasible responses with respect to a dialogue context.    ", "ref_chunk": "3 2 0 2 p e S 4 1 ] L C . s c [ 3 v 4 9 7 2 1 . 6 0 3 2 : v i X r a Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4 Mario Rodr\u00edguez-Cantelar1, Chen Zhang2, Chengguang Tang3, Ke Shi3, Sarik Ghazarian4, Jo\u00e3o Sedoc5, Luis Fernando D\u2019Haro1 and Alexander Rudnicky6 1Speech Technology and Machine Learning Group - Universidad Polit\u00e9cnica de Madrid, Spain 2National University of Singapore, Singapore 3Tencent AI Lab, China 4University of Southern California, USA 5Department of Technology, Operations, and Statistics, New York University, USA 6Carnegie Mellon University, USA 1 Abstract The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have trig- gered various challenges regarding their au- tomatic evaluation. Automatic evaluation of open-domain dialogue systems as an open chal- lenge has been the center of the attention of many researchers. Despite the consistent ef- forts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technol- ogy Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual auto- matic evaluation metrics. This article describes the datasets and baselines provided to partici- pants and discusses the submission and result details of the two proposed subtasks. Introduction dialogue systems. Common metrics are based on word overlap, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which mainly focus on matching syntactic information with a set of golden references. Unfortunately, such metrics correlate poorly with human judgments (Liu et al., 2016) as in open-domain dialogue, there can be limitless feasible responses w.r.t. a dialogue context. Alternatively, recently developed model-based metrics such as BERTscore (Sun et al., 2022), BLEURT (Sellam et al., 2020), FED (Mehri and Eskenazi, 2020a), and MDD-Eval (Zhang et al., 2022a), which take advantage of the strong seman- tic representation capability of pre-trained trans- former language models, perform the evaluation at semantic and partially pragmatic levels. Some of them do not even need golden references as input. Regrettably, despite their improvement over the word-overlap metrics, these metrics are not perfect; that is, their correlation with human evaluation is still not strong. Moreover, most of them perform well only on a particular dimension (e.g., engaging- ness or coherence) (Zhang et al., 2022b), or specific to a single domain. In addition, their performance may be highly dependent on the datasets used for training and evaluation (Yeh et al., 2021). Recent advances in large-scale neural language models (Devlin et al., 2019; Radford et al., 2019; Zhang et al., 2020) have led to significant attention in dialogue systems, especially in the open domain category. Significant research efforts are dedicated to boost the robustness of dialogue systems, that is, improving their capability to perform well across multiple domains, dimensions, and handling hu- mans\u2019 diverse expressions of the same ideas (e.g., paraphrasing or back-translation). Automatic evaluation is an indispensable com- ponent for speeding up the development of robust Due to the lack of robust automatic evaluation metrics (Mehri and Eskenazi, 2020a), researchers have to resort to the time-consuming and cost- intensive human evaluation process to analyze the performance of their model and benchmark their proposed methods against baselines. Furthermore, to the best of our knowledge, none of the existing metrics have been thoroughly tested in a multilingual setting. Metric generalization across different languages is highly desirable, as it helps the transformation of state-of-the-art English- only dialogue systems into highly capable multi- lingual systems. Although multilingual pre-trained language models may exist and can be poten- tially used for training multilingual dialogue sys- tems, human-annotations or high-quality dialogue datasets for languages other than English are very scarce or even nonexistent in the case of some low- resource languages. To address this problem, we take advantage of recent advances in neural ma- chine translation and paraphrasing systems. Using existing high-quality services and models, it is pos- sible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics. To this end, we propose two subtasks in our track, and their details are listed as follows: 1.1 Track Details This track consists of two tasks which are explained in more detail below. Participants will develop effective open-ended and multilingual automatic dialogue evaluation metrics that perform similarly when evaluated in a new language. Participants will develop effec- tive open-ended automatic dialogue evaluation met- rics that perform robustly when evaluated over paraphrased/back-translated sentences in English. For both tasks, proposed metrics are expected to show the following two important properties, as indicated in (Deriu et al., 2021): 1. Correlated to human judgments - the metrics should produce evaluation scores that well correlate to human judgments (scores) across multiple languages or alternative responses (i.e., back-translated or paraphrased). 2. Explainable - the metrics should provide con- structive and explicit feedback to the gener- ative models in terms of the quality of their generated responses. For instance, if a gener- ative model contradicts itself, the evaluation metrics should signal such behavior. Participants can propose their own metrics or optionally improve the deep AM-FM (Zhang et al., 2021) baseline evaluation model provided by us. A leaderboard on the ChatEval platform1 was pro- vided to check the performance of their different 1https://chateval.org/dstc11 proposed models compared to those submitted by other researchers. For each evaluation task, Spearman\u2019s correla- tion was used to compare the proposed evaluation metrics against human judgments. A final average score was calculated to rank the submitted met- ric models. Additional instructions to participants were provided through the Github repository2 and by email on the main DSTC distribution list. 2 Task 1: Multilingual Automatic Metrics In"}, {"question": " What is an alternative to word-overlap metrics that have been recently developed?,        answer: Recently developed model-based metrics such as BERTscore, BLEURT, FED, and MDD-Eval perform evaluation at semantic and partially pragmatic levels, taking advantage of the strong semantic representation capability of pre-trained transformer language models.    ", "ref_chunk": "3 2 0 2 p e S 4 1 ] L C . s c [ 3 v 4 9 7 2 1 . 6 0 3 2 : v i X r a Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4 Mario Rodr\u00edguez-Cantelar1, Chen Zhang2, Chengguang Tang3, Ke Shi3, Sarik Ghazarian4, Jo\u00e3o Sedoc5, Luis Fernando D\u2019Haro1 and Alexander Rudnicky6 1Speech Technology and Machine Learning Group - Universidad Polit\u00e9cnica de Madrid, Spain 2National University of Singapore, Singapore 3Tencent AI Lab, China 4University of Southern California, USA 5Department of Technology, Operations, and Statistics, New York University, USA 6Carnegie Mellon University, USA 1 Abstract The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have trig- gered various challenges regarding their au- tomatic evaluation. Automatic evaluation of open-domain dialogue systems as an open chal- lenge has been the center of the attention of many researchers. Despite the consistent ef- forts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technol- ogy Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual auto- matic evaluation metrics. This article describes the datasets and baselines provided to partici- pants and discusses the submission and result details of the two proposed subtasks. Introduction dialogue systems. Common metrics are based on word overlap, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which mainly focus on matching syntactic information with a set of golden references. Unfortunately, such metrics correlate poorly with human judgments (Liu et al., 2016) as in open-domain dialogue, there can be limitless feasible responses w.r.t. a dialogue context. Alternatively, recently developed model-based metrics such as BERTscore (Sun et al., 2022), BLEURT (Sellam et al., 2020), FED (Mehri and Eskenazi, 2020a), and MDD-Eval (Zhang et al., 2022a), which take advantage of the strong seman- tic representation capability of pre-trained trans- former language models, perform the evaluation at semantic and partially pragmatic levels. Some of them do not even need golden references as input. Regrettably, despite their improvement over the word-overlap metrics, these metrics are not perfect; that is, their correlation with human evaluation is still not strong. Moreover, most of them perform well only on a particular dimension (e.g., engaging- ness or coherence) (Zhang et al., 2022b), or specific to a single domain. In addition, their performance may be highly dependent on the datasets used for training and evaluation (Yeh et al., 2021). Recent advances in large-scale neural language models (Devlin et al., 2019; Radford et al., 2019; Zhang et al., 2020) have led to significant attention in dialogue systems, especially in the open domain category. Significant research efforts are dedicated to boost the robustness of dialogue systems, that is, improving their capability to perform well across multiple domains, dimensions, and handling hu- mans\u2019 diverse expressions of the same ideas (e.g., paraphrasing or back-translation). Automatic evaluation is an indispensable com- ponent for speeding up the development of robust Due to the lack of robust automatic evaluation metrics (Mehri and Eskenazi, 2020a), researchers have to resort to the time-consuming and cost- intensive human evaluation process to analyze the performance of their model and benchmark their proposed methods against baselines. Furthermore, to the best of our knowledge, none of the existing metrics have been thoroughly tested in a multilingual setting. Metric generalization across different languages is highly desirable, as it helps the transformation of state-of-the-art English- only dialogue systems into highly capable multi- lingual systems. Although multilingual pre-trained language models may exist and can be poten- tially used for training multilingual dialogue sys- tems, human-annotations or high-quality dialogue datasets for languages other than English are very scarce or even nonexistent in the case of some low- resource languages. To address this problem, we take advantage of recent advances in neural ma- chine translation and paraphrasing systems. Using existing high-quality services and models, it is pos- sible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics. To this end, we propose two subtasks in our track, and their details are listed as follows: 1.1 Track Details This track consists of two tasks which are explained in more detail below. Participants will develop effective open-ended and multilingual automatic dialogue evaluation metrics that perform similarly when evaluated in a new language. Participants will develop effec- tive open-ended automatic dialogue evaluation met- rics that perform robustly when evaluated over paraphrased/back-translated sentences in English. For both tasks, proposed metrics are expected to show the following two important properties, as indicated in (Deriu et al., 2021): 1. Correlated to human judgments - the metrics should produce evaluation scores that well correlate to human judgments (scores) across multiple languages or alternative responses (i.e., back-translated or paraphrased). 2. Explainable - the metrics should provide con- structive and explicit feedback to the gener- ative models in terms of the quality of their generated responses. For instance, if a gener- ative model contradicts itself, the evaluation metrics should signal such behavior. Participants can propose their own metrics or optionally improve the deep AM-FM (Zhang et al., 2021) baseline evaluation model provided by us. A leaderboard on the ChatEval platform1 was pro- vided to check the performance of their different 1https://chateval.org/dstc11 proposed models compared to those submitted by other researchers. For each evaluation task, Spearman\u2019s correla- tion was used to compare the proposed evaluation metrics against human judgments. A final average score was calculated to rank the submitted met- ric models. Additional instructions to participants were provided through the Github repository2 and by email on the main DSTC distribution list. 2 Task 1: Multilingual Automatic Metrics In"}, {"question": " What is the significance of robust automatic evaluation metrics for dialogue systems?,        answer: Robust automatic evaluation metrics for dialogue systems are significant because they help in developing systems that can perform well across multiple domains, dimensions, and handle diverse human expressions of the same ideas.    ", "ref_chunk": "3 2 0 2 p e S 4 1 ] L C . s c [ 3 v 4 9 7 2 1 . 6 0 3 2 : v i X r a Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4 Mario Rodr\u00edguez-Cantelar1, Chen Zhang2, Chengguang Tang3, Ke Shi3, Sarik Ghazarian4, Jo\u00e3o Sedoc5, Luis Fernando D\u2019Haro1 and Alexander Rudnicky6 1Speech Technology and Machine Learning Group - Universidad Polit\u00e9cnica de Madrid, Spain 2National University of Singapore, Singapore 3Tencent AI Lab, China 4University of Southern California, USA 5Department of Technology, Operations, and Statistics, New York University, USA 6Carnegie Mellon University, USA 1 Abstract The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have trig- gered various challenges regarding their au- tomatic evaluation. Automatic evaluation of open-domain dialogue systems as an open chal- lenge has been the center of the attention of many researchers. Despite the consistent ef- forts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technol- ogy Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual auto- matic evaluation metrics. This article describes the datasets and baselines provided to partici- pants and discusses the submission and result details of the two proposed subtasks. Introduction dialogue systems. Common metrics are based on word overlap, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which mainly focus on matching syntactic information with a set of golden references. Unfortunately, such metrics correlate poorly with human judgments (Liu et al., 2016) as in open-domain dialogue, there can be limitless feasible responses w.r.t. a dialogue context. Alternatively, recently developed model-based metrics such as BERTscore (Sun et al., 2022), BLEURT (Sellam et al., 2020), FED (Mehri and Eskenazi, 2020a), and MDD-Eval (Zhang et al., 2022a), which take advantage of the strong seman- tic representation capability of pre-trained trans- former language models, perform the evaluation at semantic and partially pragmatic levels. Some of them do not even need golden references as input. Regrettably, despite their improvement over the word-overlap metrics, these metrics are not perfect; that is, their correlation with human evaluation is still not strong. Moreover, most of them perform well only on a particular dimension (e.g., engaging- ness or coherence) (Zhang et al., 2022b), or specific to a single domain. In addition, their performance may be highly dependent on the datasets used for training and evaluation (Yeh et al., 2021). Recent advances in large-scale neural language models (Devlin et al., 2019; Radford et al., 2019; Zhang et al., 2020) have led to significant attention in dialogue systems, especially in the open domain category. Significant research efforts are dedicated to boost the robustness of dialogue systems, that is, improving their capability to perform well across multiple domains, dimensions, and handling hu- mans\u2019 diverse expressions of the same ideas (e.g., paraphrasing or back-translation). Automatic evaluation is an indispensable com- ponent for speeding up the development of robust Due to the lack of robust automatic evaluation metrics (Mehri and Eskenazi, 2020a), researchers have to resort to the time-consuming and cost- intensive human evaluation process to analyze the performance of their model and benchmark their proposed methods against baselines. Furthermore, to the best of our knowledge, none of the existing metrics have been thoroughly tested in a multilingual setting. Metric generalization across different languages is highly desirable, as it helps the transformation of state-of-the-art English- only dialogue systems into highly capable multi- lingual systems. Although multilingual pre-trained language models may exist and can be poten- tially used for training multilingual dialogue sys- tems, human-annotations or high-quality dialogue datasets for languages other than English are very scarce or even nonexistent in the case of some low- resource languages. To address this problem, we take advantage of recent advances in neural ma- chine translation and paraphrasing systems. Using existing high-quality services and models, it is pos- sible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics. To this end, we propose two subtasks in our track, and their details are listed as follows: 1.1 Track Details This track consists of two tasks which are explained in more detail below. Participants will develop effective open-ended and multilingual automatic dialogue evaluation metrics that perform similarly when evaluated in a new language. Participants will develop effec- tive open-ended automatic dialogue evaluation met- rics that perform robustly when evaluated over paraphrased/back-translated sentences in English. For both tasks, proposed metrics are expected to show the following two important properties, as indicated in (Deriu et al., 2021): 1. Correlated to human judgments - the metrics should produce evaluation scores that well correlate to human judgments (scores) across multiple languages or alternative responses (i.e., back-translated or paraphrased). 2. Explainable - the metrics should provide con- structive and explicit feedback to the gener- ative models in terms of the quality of their generated responses. For instance, if a gener- ative model contradicts itself, the evaluation metrics should signal such behavior. Participants can propose their own metrics or optionally improve the deep AM-FM (Zhang et al., 2021) baseline evaluation model provided by us. A leaderboard on the ChatEval platform1 was pro- vided to check the performance of their different 1https://chateval.org/dstc11 proposed models compared to those submitted by other researchers. For each evaluation task, Spearman\u2019s correla- tion was used to compare the proposed evaluation metrics against human judgments. A final average score was calculated to rank the submitted met- ric models. Additional instructions to participants were provided through the Github repository2 and by email on the main DSTC distribution list. 2 Task 1: Multilingual Automatic Metrics In"}, {"question": " Why do researchers have to resort to human evaluation processes in the absence of robust automatic evaluation metrics?,        answer: Researchers have to resort to human evaluation processes in the absence of robust automatic evaluation metrics to analyze the performance of their models and benchmark their proposed methods against baselines.    ", "ref_chunk": "3 2 0 2 p e S 4 1 ] L C . s c [ 3 v 4 9 7 2 1 . 6 0 3 2 : v i X r a Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4 Mario Rodr\u00edguez-Cantelar1, Chen Zhang2, Chengguang Tang3, Ke Shi3, Sarik Ghazarian4, Jo\u00e3o Sedoc5, Luis Fernando D\u2019Haro1 and Alexander Rudnicky6 1Speech Technology and Machine Learning Group - Universidad Polit\u00e9cnica de Madrid, Spain 2National University of Singapore, Singapore 3Tencent AI Lab, China 4University of Southern California, USA 5Department of Technology, Operations, and Statistics, New York University, USA 6Carnegie Mellon University, USA 1 Abstract The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have trig- gered various challenges regarding their au- tomatic evaluation. Automatic evaluation of open-domain dialogue systems as an open chal- lenge has been the center of the attention of many researchers. Despite the consistent ef- forts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technol- ogy Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual auto- matic evaluation metrics. This article describes the datasets and baselines provided to partici- pants and discusses the submission and result details of the two proposed subtasks. Introduction dialogue systems. Common metrics are based on word overlap, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which mainly focus on matching syntactic information with a set of golden references. Unfortunately, such metrics correlate poorly with human judgments (Liu et al., 2016) as in open-domain dialogue, there can be limitless feasible responses w.r.t. a dialogue context. Alternatively, recently developed model-based metrics such as BERTscore (Sun et al., 2022), BLEURT (Sellam et al., 2020), FED (Mehri and Eskenazi, 2020a), and MDD-Eval (Zhang et al., 2022a), which take advantage of the strong seman- tic representation capability of pre-trained trans- former language models, perform the evaluation at semantic and partially pragmatic levels. Some of them do not even need golden references as input. Regrettably, despite their improvement over the word-overlap metrics, these metrics are not perfect; that is, their correlation with human evaluation is still not strong. Moreover, most of them perform well only on a particular dimension (e.g., engaging- ness or coherence) (Zhang et al., 2022b), or specific to a single domain. In addition, their performance may be highly dependent on the datasets used for training and evaluation (Yeh et al., 2021). Recent advances in large-scale neural language models (Devlin et al., 2019; Radford et al., 2019; Zhang et al., 2020) have led to significant attention in dialogue systems, especially in the open domain category. Significant research efforts are dedicated to boost the robustness of dialogue systems, that is, improving their capability to perform well across multiple domains, dimensions, and handling hu- mans\u2019 diverse expressions of the same ideas (e.g., paraphrasing or back-translation). Automatic evaluation is an indispensable com- ponent for speeding up the development of robust Due to the lack of robust automatic evaluation metrics (Mehri and Eskenazi, 2020a), researchers have to resort to the time-consuming and cost- intensive human evaluation process to analyze the performance of their model and benchmark their proposed methods against baselines. Furthermore, to the best of our knowledge, none of the existing metrics have been thoroughly tested in a multilingual setting. Metric generalization across different languages is highly desirable, as it helps the transformation of state-of-the-art English- only dialogue systems into highly capable multi- lingual systems. Although multilingual pre-trained language models may exist and can be poten- tially used for training multilingual dialogue sys- tems, human-annotations or high-quality dialogue datasets for languages other than English are very scarce or even nonexistent in the case of some low- resource languages. To address this problem, we take advantage of recent advances in neural ma- chine translation and paraphrasing systems. Using existing high-quality services and models, it is pos- sible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics. To this end, we propose two subtasks in our track, and their details are listed as follows: 1.1 Track Details This track consists of two tasks which are explained in more detail below. Participants will develop effective open-ended and multilingual automatic dialogue evaluation metrics that perform similarly when evaluated in a new language. Participants will develop effec- tive open-ended automatic dialogue evaluation met- rics that perform robustly when evaluated over paraphrased/back-translated sentences in English. For both tasks, proposed metrics are expected to show the following two important properties, as indicated in (Deriu et al., 2021): 1. Correlated to human judgments - the metrics should produce evaluation scores that well correlate to human judgments (scores) across multiple languages or alternative responses (i.e., back-translated or paraphrased). 2. Explainable - the metrics should provide con- structive and explicit feedback to the gener- ative models in terms of the quality of their generated responses. For instance, if a gener- ative model contradicts itself, the evaluation metrics should signal such behavior. Participants can propose their own metrics or optionally improve the deep AM-FM (Zhang et al., 2021) baseline evaluation model provided by us. A leaderboard on the ChatEval platform1 was pro- vided to check the performance of their different 1https://chateval.org/dstc11 proposed models compared to those submitted by other researchers. For each evaluation task, Spearman\u2019s correla- tion was used to compare the proposed evaluation metrics against human judgments. A final average score was calculated to rank the submitted met- ric models. Additional instructions to participants were provided through the Github repository2 and by email on the main DSTC distribution list. 2 Task 1: Multilingual Automatic Metrics In"}, {"question": " What are the important properties expected from proposed metrics for effective dialogue evaluation?,        answer: Two important properties expected from proposed metrics for effective dialogue evaluation are being correlated to human judgments and providing explicit feedback to generative models in terms of the quality of their responses.    ", "ref_chunk": "3 2 0 2 p e S 4 1 ] L C . s c [ 3 v 4 9 7 2 1 . 6 0 3 2 : v i X r a Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4 Mario Rodr\u00edguez-Cantelar1, Chen Zhang2, Chengguang Tang3, Ke Shi3, Sarik Ghazarian4, Jo\u00e3o Sedoc5, Luis Fernando D\u2019Haro1 and Alexander Rudnicky6 1Speech Technology and Machine Learning Group - Universidad Polit\u00e9cnica de Madrid, Spain 2National University of Singapore, Singapore 3Tencent AI Lab, China 4University of Southern California, USA 5Department of Technology, Operations, and Statistics, New York University, USA 6Carnegie Mellon University, USA 1 Abstract The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have trig- gered various challenges regarding their au- tomatic evaluation. Automatic evaluation of open-domain dialogue systems as an open chal- lenge has been the center of the attention of many researchers. Despite the consistent ef- forts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technol- ogy Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual auto- matic evaluation metrics. This article describes the datasets and baselines provided to partici- pants and discusses the submission and result details of the two proposed subtasks. Introduction dialogue systems. Common metrics are based on word overlap, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which mainly focus on matching syntactic information with a set of golden references. Unfortunately, such metrics correlate poorly with human judgments (Liu et al., 2016) as in open-domain dialogue, there can be limitless feasible responses w.r.t. a dialogue context. Alternatively, recently developed model-based metrics such as BERTscore (Sun et al., 2022), BLEURT (Sellam et al., 2020), FED (Mehri and Eskenazi, 2020a), and MDD-Eval (Zhang et al., 2022a), which take advantage of the strong seman- tic representation capability of pre-trained trans- former language models, perform the evaluation at semantic and partially pragmatic levels. Some of them do not even need golden references as input. Regrettably, despite their improvement over the word-overlap metrics, these metrics are not perfect; that is, their correlation with human evaluation is still not strong. Moreover, most of them perform well only on a particular dimension (e.g., engaging- ness or coherence) (Zhang et al., 2022b), or specific to a single domain. In addition, their performance may be highly dependent on the datasets used for training and evaluation (Yeh et al., 2021). Recent advances in large-scale neural language models (Devlin et al., 2019; Radford et al., 2019; Zhang et al., 2020) have led to significant attention in dialogue systems, especially in the open domain category. Significant research efforts are dedicated to boost the robustness of dialogue systems, that is, improving their capability to perform well across multiple domains, dimensions, and handling hu- mans\u2019 diverse expressions of the same ideas (e.g., paraphrasing or back-translation). Automatic evaluation is an indispensable com- ponent for speeding up the development of robust Due to the lack of robust automatic evaluation metrics (Mehri and Eskenazi, 2020a), researchers have to resort to the time-consuming and cost- intensive human evaluation process to analyze the performance of their model and benchmark their proposed methods against baselines. Furthermore, to the best of our knowledge, none of the existing metrics have been thoroughly tested in a multilingual setting. Metric generalization across different languages is highly desirable, as it helps the transformation of state-of-the-art English- only dialogue systems into highly capable multi- lingual systems. Although multilingual pre-trained language models may exist and can be poten- tially used for training multilingual dialogue sys- tems, human-annotations or high-quality dialogue datasets for languages other than English are very scarce or even nonexistent in the case of some low- resource languages. To address this problem, we take advantage of recent advances in neural ma- chine translation and paraphrasing systems. Using existing high-quality services and models, it is pos- sible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics. To this end, we propose two subtasks in our track, and their details are listed as follows: 1.1 Track Details This track consists of two tasks which are explained in more detail below. Participants will develop effective open-ended and multilingual automatic dialogue evaluation metrics that perform similarly when evaluated in a new language. Participants will develop effec- tive open-ended automatic dialogue evaluation met- rics that perform robustly when evaluated over paraphrased/back-translated sentences in English. For both tasks, proposed metrics are expected to show the following two important properties, as indicated in (Deriu et al., 2021): 1. Correlated to human judgments - the metrics should produce evaluation scores that well correlate to human judgments (scores) across multiple languages or alternative responses (i.e., back-translated or paraphrased). 2. Explainable - the metrics should provide con- structive and explicit feedback to the gener- ative models in terms of the quality of their generated responses. For instance, if a gener- ative model contradicts itself, the evaluation metrics should signal such behavior. Participants can propose their own metrics or optionally improve the deep AM-FM (Zhang et al., 2021) baseline evaluation model provided by us. A leaderboard on the ChatEval platform1 was pro- vided to check the performance of their different 1https://chateval.org/dstc11 proposed models compared to those submitted by other researchers. For each evaluation task, Spearman\u2019s correla- tion was used to compare the proposed evaluation metrics against human judgments. A final average score was calculated to rank the submitted met- ric models. Additional instructions to participants were provided through the Github repository2 and by email on the main DSTC distribution list. 2 Task 1: Multilingual Automatic Metrics In"}, {"question": " What is the purpose of the proposed two subtasks in the track?,        answer: The proposed two subtasks in the track aim to develop effective open-ended and multilingual automatic dialogue evaluation metrics that perform similarly when evaluated in a new language, and to develop effective open-ended automatic dialogue evaluation metrics that perform robustly when evaluated over paraphrased/back-translated sentences in English.    ", "ref_chunk": "3 2 0 2 p e S 4 1 ] L C . s c [ 3 v 4 9 7 2 1 . 6 0 3 2 : v i X r a Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4 Mario Rodr\u00edguez-Cantelar1, Chen Zhang2, Chengguang Tang3, Ke Shi3, Sarik Ghazarian4, Jo\u00e3o Sedoc5, Luis Fernando D\u2019Haro1 and Alexander Rudnicky6 1Speech Technology and Machine Learning Group - Universidad Polit\u00e9cnica de Madrid, Spain 2National University of Singapore, Singapore 3Tencent AI Lab, China 4University of Southern California, USA 5Department of Technology, Operations, and Statistics, New York University, USA 6Carnegie Mellon University, USA 1 Abstract The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have trig- gered various challenges regarding their au- tomatic evaluation. Automatic evaluation of open-domain dialogue systems as an open chal- lenge has been the center of the attention of many researchers. Despite the consistent ef- forts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technol- ogy Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual auto- matic evaluation metrics. This article describes the datasets and baselines provided to partici- pants and discusses the submission and result details of the two proposed subtasks. Introduction dialogue systems. Common metrics are based on word overlap, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which mainly focus on matching syntactic information with a set of golden references. Unfortunately, such metrics correlate poorly with human judgments (Liu et al., 2016) as in open-domain dialogue, there can be limitless feasible responses w.r.t. a dialogue context. Alternatively, recently developed model-based metrics such as BERTscore (Sun et al., 2022), BLEURT (Sellam et al., 2020), FED (Mehri and Eskenazi, 2020a), and MDD-Eval (Zhang et al., 2022a), which take advantage of the strong seman- tic representation capability of pre-trained trans- former language models, perform the evaluation at semantic and partially pragmatic levels. Some of them do not even need golden references as input. Regrettably, despite their improvement over the word-overlap metrics, these metrics are not perfect; that is, their correlation with human evaluation is still not strong. Moreover, most of them perform well only on a particular dimension (e.g., engaging- ness or coherence) (Zhang et al., 2022b), or specific to a single domain. In addition, their performance may be highly dependent on the datasets used for training and evaluation (Yeh et al., 2021). Recent advances in large-scale neural language models (Devlin et al., 2019; Radford et al., 2019; Zhang et al., 2020) have led to significant attention in dialogue systems, especially in the open domain category. Significant research efforts are dedicated to boost the robustness of dialogue systems, that is, improving their capability to perform well across multiple domains, dimensions, and handling hu- mans\u2019 diverse expressions of the same ideas (e.g., paraphrasing or back-translation). Automatic evaluation is an indispensable com- ponent for speeding up the development of robust Due to the lack of robust automatic evaluation metrics (Mehri and Eskenazi, 2020a), researchers have to resort to the time-consuming and cost- intensive human evaluation process to analyze the performance of their model and benchmark their proposed methods against baselines. Furthermore, to the best of our knowledge, none of the existing metrics have been thoroughly tested in a multilingual setting. Metric generalization across different languages is highly desirable, as it helps the transformation of state-of-the-art English- only dialogue systems into highly capable multi- lingual systems. Although multilingual pre-trained language models may exist and can be poten- tially used for training multilingual dialogue sys- tems, human-annotations or high-quality dialogue datasets for languages other than English are very scarce or even nonexistent in the case of some low- resource languages. To address this problem, we take advantage of recent advances in neural ma- chine translation and paraphrasing systems. Using existing high-quality services and models, it is pos- sible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics. To this end, we propose two subtasks in our track, and their details are listed as follows: 1.1 Track Details This track consists of two tasks which are explained in more detail below. Participants will develop effective open-ended and multilingual automatic dialogue evaluation metrics that perform similarly when evaluated in a new language. Participants will develop effec- tive open-ended automatic dialogue evaluation met- rics that perform robustly when evaluated over paraphrased/back-translated sentences in English. For both tasks, proposed metrics are expected to show the following two important properties, as indicated in (Deriu et al., 2021): 1. Correlated to human judgments - the metrics should produce evaluation scores that well correlate to human judgments (scores) across multiple languages or alternative responses (i.e., back-translated or paraphrased). 2. Explainable - the metrics should provide con- structive and explicit feedback to the gener- ative models in terms of the quality of their generated responses. For instance, if a gener- ative model contradicts itself, the evaluation metrics should signal such behavior. Participants can propose their own metrics or optionally improve the deep AM-FM (Zhang et al., 2021) baseline evaluation model provided by us. A leaderboard on the ChatEval platform1 was pro- vided to check the performance of their different 1https://chateval.org/dstc11 proposed models compared to those submitted by other researchers. For each evaluation task, Spearman\u2019s correla- tion was used to compare the proposed evaluation metrics against human judgments. A final average score was calculated to rank the submitted met- ric models. Additional instructions to participants were provided through the Github repository2 and by email on the main DSTC distribution list. 2 Task 1: Multilingual Automatic Metrics In"}, {"question": " How is the performance of proposed evaluation metrics compared in the track?,        answer: The performance of proposed evaluation metrics is compared using Spearman\u2019s correlation against human judgments, and a final average score is calculated to rank the submitted metric models.    ", "ref_chunk": "3 2 0 2 p e S 4 1 ] L C . s c [ 3 v 4 9 7 2 1 . 6 0 3 2 : v i X r a Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4 Mario Rodr\u00edguez-Cantelar1, Chen Zhang2, Chengguang Tang3, Ke Shi3, Sarik Ghazarian4, Jo\u00e3o Sedoc5, Luis Fernando D\u2019Haro1 and Alexander Rudnicky6 1Speech Technology and Machine Learning Group - Universidad Polit\u00e9cnica de Madrid, Spain 2National University of Singapore, Singapore 3Tencent AI Lab, China 4University of Southern California, USA 5Department of Technology, Operations, and Statistics, New York University, USA 6Carnegie Mellon University, USA 1 Abstract The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have trig- gered various challenges regarding their au- tomatic evaluation. Automatic evaluation of open-domain dialogue systems as an open chal- lenge has been the center of the attention of many researchers. Despite the consistent ef- forts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technol- ogy Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual auto- matic evaluation metrics. This article describes the datasets and baselines provided to partici- pants and discusses the submission and result details of the two proposed subtasks. Introduction dialogue systems. Common metrics are based on word overlap, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which mainly focus on matching syntactic information with a set of golden references. Unfortunately, such metrics correlate poorly with human judgments (Liu et al., 2016) as in open-domain dialogue, there can be limitless feasible responses w.r.t. a dialogue context. Alternatively, recently developed model-based metrics such as BERTscore (Sun et al., 2022), BLEURT (Sellam et al., 2020), FED (Mehri and Eskenazi, 2020a), and MDD-Eval (Zhang et al., 2022a), which take advantage of the strong seman- tic representation capability of pre-trained trans- former language models, perform the evaluation at semantic and partially pragmatic levels. Some of them do not even need golden references as input. Regrettably, despite their improvement over the word-overlap metrics, these metrics are not perfect; that is, their correlation with human evaluation is still not strong. Moreover, most of them perform well only on a particular dimension (e.g., engaging- ness or coherence) (Zhang et al., 2022b), or specific to a single domain. In addition, their performance may be highly dependent on the datasets used for training and evaluation (Yeh et al., 2021). Recent advances in large-scale neural language models (Devlin et al., 2019; Radford et al., 2019; Zhang et al., 2020) have led to significant attention in dialogue systems, especially in the open domain category. Significant research efforts are dedicated to boost the robustness of dialogue systems, that is, improving their capability to perform well across multiple domains, dimensions, and handling hu- mans\u2019 diverse expressions of the same ideas (e.g., paraphrasing or back-translation). Automatic evaluation is an indispensable com- ponent for speeding up the development of robust Due to the lack of robust automatic evaluation metrics (Mehri and Eskenazi, 2020a), researchers have to resort to the time-consuming and cost- intensive human evaluation process to analyze the performance of their model and benchmark their proposed methods against baselines. Furthermore, to the best of our knowledge, none of the existing metrics have been thoroughly tested in a multilingual setting. Metric generalization across different languages is highly desirable, as it helps the transformation of state-of-the-art English- only dialogue systems into highly capable multi- lingual systems. Although multilingual pre-trained language models may exist and can be poten- tially used for training multilingual dialogue sys- tems, human-annotations or high-quality dialogue datasets for languages other than English are very scarce or even nonexistent in the case of some low- resource languages. To address this problem, we take advantage of recent advances in neural ma- chine translation and paraphrasing systems. Using existing high-quality services and models, it is pos- sible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics. To this end, we propose two subtasks in our track, and their details are listed as follows: 1.1 Track Details This track consists of two tasks which are explained in more detail below. Participants will develop effective open-ended and multilingual automatic dialogue evaluation metrics that perform similarly when evaluated in a new language. Participants will develop effec- tive open-ended automatic dialogue evaluation met- rics that perform robustly when evaluated over paraphrased/back-translated sentences in English. For both tasks, proposed metrics are expected to show the following two important properties, as indicated in (Deriu et al., 2021): 1. Correlated to human judgments - the metrics should produce evaluation scores that well correlate to human judgments (scores) across multiple languages or alternative responses (i.e., back-translated or paraphrased). 2. Explainable - the metrics should provide con- structive and explicit feedback to the gener- ative models in terms of the quality of their generated responses. For instance, if a gener- ative model contradicts itself, the evaluation metrics should signal such behavior. Participants can propose their own metrics or optionally improve the deep AM-FM (Zhang et al., 2021) baseline evaluation model provided by us. A leaderboard on the ChatEval platform1 was pro- vided to check the performance of their different 1https://chateval.org/dstc11 proposed models compared to those submitted by other researchers. For each evaluation task, Spearman\u2019s correla- tion was used to compare the proposed evaluation metrics against human judgments. A final average score was calculated to rank the submitted met- ric models. Additional instructions to participants were provided through the Github repository2 and by email on the main DSTC distribution list. 2 Task 1: Multilingual Automatic Metrics In"}], "doc_text": "3 2 0 2 p e S 4 1 ] L C . s c [ 3 v 4 9 7 2 1 . 6 0 3 2 : v i X r a Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4 Mario Rodr\u00edguez-Cantelar1, Chen Zhang2, Chengguang Tang3, Ke Shi3, Sarik Ghazarian4, Jo\u00e3o Sedoc5, Luis Fernando D\u2019Haro1 and Alexander Rudnicky6 1Speech Technology and Machine Learning Group - Universidad Polit\u00e9cnica de Madrid, Spain 2National University of Singapore, Singapore 3Tencent AI Lab, China 4University of Southern California, USA 5Department of Technology, Operations, and Statistics, New York University, USA 6Carnegie Mellon University, USA 1 Abstract The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have trig- gered various challenges regarding their au- tomatic evaluation. Automatic evaluation of open-domain dialogue systems as an open chal- lenge has been the center of the attention of many researchers. Despite the consistent ef- forts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technol- ogy Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual auto- matic evaluation metrics. This article describes the datasets and baselines provided to partici- pants and discusses the submission and result details of the two proposed subtasks. Introduction dialogue systems. Common metrics are based on word overlap, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which mainly focus on matching syntactic information with a set of golden references. Unfortunately, such metrics correlate poorly with human judgments (Liu et al., 2016) as in open-domain dialogue, there can be limitless feasible responses w.r.t. a dialogue context. Alternatively, recently developed model-based metrics such as BERTscore (Sun et al., 2022), BLEURT (Sellam et al., 2020), FED (Mehri and Eskenazi, 2020a), and MDD-Eval (Zhang et al., 2022a), which take advantage of the strong seman- tic representation capability of pre-trained trans- former language models, perform the evaluation at semantic and partially pragmatic levels. Some of them do not even need golden references as input. Regrettably, despite their improvement over the word-overlap metrics, these metrics are not perfect; that is, their correlation with human evaluation is still not strong. Moreover, most of them perform well only on a particular dimension (e.g., engaging- ness or coherence) (Zhang et al., 2022b), or specific to a single domain. In addition, their performance may be highly dependent on the datasets used for training and evaluation (Yeh et al., 2021). Recent advances in large-scale neural language models (Devlin et al., 2019; Radford et al., 2019; Zhang et al., 2020) have led to significant attention in dialogue systems, especially in the open domain category. Significant research efforts are dedicated to boost the robustness of dialogue systems, that is, improving their capability to perform well across multiple domains, dimensions, and handling hu- mans\u2019 diverse expressions of the same ideas (e.g., paraphrasing or back-translation). Automatic evaluation is an indispensable com- ponent for speeding up the development of robust Due to the lack of robust automatic evaluation metrics (Mehri and Eskenazi, 2020a), researchers have to resort to the time-consuming and cost- intensive human evaluation process to analyze the performance of their model and benchmark their proposed methods against baselines. Furthermore, to the best of our knowledge, none of the existing metrics have been thoroughly tested in a multilingual setting. Metric generalization across different languages is highly desirable, as it helps the transformation of state-of-the-art English- only dialogue systems into highly capable multi- lingual systems. Although multilingual pre-trained language models may exist and can be poten- tially used for training multilingual dialogue sys- tems, human-annotations or high-quality dialogue datasets for languages other than English are very scarce or even nonexistent in the case of some low- resource languages. To address this problem, we take advantage of recent advances in neural ma- chine translation and paraphrasing systems. Using existing high-quality services and models, it is pos- sible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics. To this end, we propose two subtasks in our track, and their details are listed as follows: 1.1 Track Details This track consists of two tasks which are explained in more detail below. Participants will develop effective open-ended and multilingual automatic dialogue evaluation metrics that perform similarly when evaluated in a new language. Participants will develop effec- tive open-ended automatic dialogue evaluation met- rics that perform robustly when evaluated over paraphrased/back-translated sentences in English. For both tasks, proposed metrics are expected to show the following two important properties, as indicated in (Deriu et al., 2021): 1. Correlated to human judgments - the metrics should produce evaluation scores that well correlate to human judgments (scores) across multiple languages or alternative responses (i.e., back-translated or paraphrased). 2. Explainable - the metrics should provide con- structive and explicit feedback to the gener- ative models in terms of the quality of their generated responses. For instance, if a gener- ative model contradicts itself, the evaluation metrics should signal such behavior. Participants can propose their own metrics or optionally improve the deep AM-FM (Zhang et al., 2021) baseline evaluation model provided by us. A leaderboard on the ChatEval platform1 was pro- vided to check the performance of their different 1https://chateval.org/dstc11 proposed models compared to those submitted by other researchers. For each evaluation task, Spearman\u2019s correla- tion was used to compare the proposed evaluation metrics against human judgments. A final average score was calculated to rank the submitted met- ric models. Additional instructions to participants were provided through the Github repository2 and by email on the main DSTC distribution list. 2 Task 1: Multilingual Automatic Metrics In"}