{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Enhancing_End-to-End_Conversational_Speech_Translation_Through_Target_Language_Context_Utilization_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the unique characteristic of the approach described in the text?", "answer": " The unique characteristic is the use of context also during training.", "ref_chunk": "encoder DECst(\u00b7) generates \u02c6yl at each time step that is conditionally dependent on the hidden represen- tation HE st, previous target sequence Y1:t\u22121 and the context Ycntx of previous sentences shown in Eq. (5, 6). asr) H D st = DECst(H E st , Ycntx 1:K , Y1:l\u22121) P (yl|Y1:l\u22121, X, Ycntx) = Softmax(H D st) The unique characteristic of our approach lies in the use of context also during training. While other methods may attempt to predict the (2) (3) (5) (6) context, our model uses the context exclusively as an initial condition for the decoder, overcoming memory constraints of extended audio. During model training, the ST cross-entropy loss is computed solely for the target translation, excluding the context. The model is opti- mized with a multi-task learning objective combining hybrid ASR attention Lasr att and CTC Lst att and CTC Lasr ctc as well as hybrid ST attention Lst ctc losses: L =\u03b13((1 \u2212 \u03b11)Lasr att + \u03b11Lasr ctc) att + \u03b12Lst + (1 \u2212 \u03b13)((1 \u2212 \u03b12)Lst ctc) where \u03b1\u2032s are used for interpolation. 3. EXPERIMENTS 3.1. Dataset We demonstrate the efficacy of our proposed approach through evaluations on three conversational datasets (their statistics are summarized in Table 1): Fisher-CallHome Spanish English [33], IWSLT22 Tunisian Arabic-English [34], and BOLT Chinese-English [35]. These datasets contain 3-way data comprising telephone speech, source language transcriptions, and corresponding English translations. We use separated source and target vocabularies, each Table 1. Statistics for the conversational ST corpora. Corpus Lang #Hours Train Dev Test Spanish Fisher/Callhome Tunisian IWSLT22 Chinese BOLT Sp-En Ar-En Zh-En 186.3 161.0 110.6 9.3 6.3 8.5 4.5/1.8 3.6 8.5 consisting of 4K byte-pair-encoding [36] (BPE) units. All audios are resampled from 8kHz to 16kHz, augmented with speed perturbations (0.9, 1.0 and 1.1) and transformed into 83-dimensional feature frames (80 log-mel filterbank coefficients plus 3 pitch features). Ad- ditionally, we augment the features with specaugment [37], with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. During scoring we report the results in terms of case- sensitive BLEU with punctuation. We also measure the statistical significance of improvement using paired bootstrap resampling with sacreBLEU[38]. 3.2. Baseline configuration We conduct all experiments by customizing the ESPnet toolkit [13]. For the encoders ENCasr in Eq. (2) and ENCst in Eq. (3), we employ the conformer architecture [39] consisting of 12 blocks for ENCasr and 6 blocks for ENCst. Both encoders are configured with 2048 feed-forward dimensions, 256 attention dimensions, and 4 attention heads. The transformer architecture is employed for DECasr and DECst in Eq. (4), each with 6 decoder blocks and the same configu- ration as the encoders. The model has 72M parameters. We follow ST best practices [13], we first pretrain the ASR module followed by fine-tuning of the entire E2E-ST model for the translation task. Our training configuration remains consistent for both pretraining and fine-tuning, employing Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight (\u03b11, \u03b12) in Eq. (7) to 0.3 and the weight that combines ASR and ST losses (7) Table 2. Comparison of the BLEU scores according to contextual information quality using one previous utterrance as context. \u2020: de- notes a statistically significant difference (p < 0.01) compared to the no-context baseline. Context Type Fisher CallHome IWSLT22 No-context Random Gold 29.8 29.9 31.3\u2020 25.9 25.2 26.0 19.7 19.5 19.9 (\u03b13) to 0.3. During inference, we use a beam size of 10 and length penalty of 0.3. 3.3. Contextual E2E-ST configuration In the Contextual E2E-ST model (described in \u00a72), we retain the same configuration as the baseline. The only difference is that during training with teacher-forcing, the decoder initial condition is the pre- ceding sentences alongside a start-of-sentence token. Heuristically, we limit any long contextual sentences to the last 50 tokens. Upon visual inspection, we found that these truncated sentences effectively capture the contextually relevant information. The previous sentence considered a part of context only if they are from the same recording.1 We will refer to (Gold) context when employing ground-truth trans- lations and to (Hyp) when utilizing the model\u2019s predictions. During training we only use the Gold context, however during inference we explore both Gold and Hyp context, simulating both an oracle and a more realistic scenario. 4. RESULTS 4.1. Contextual ST results We examined the effect of preceding context on model performance by comparing Gold previous context with the no-context baseline and randomly selected sentences unrelated to the ground truth. The results, shown in Table 2, use a context size of one preceding sentence. The analysis indicates that, compared to the no-context baseline, gold context consistently improves performance, with a maximum BLEU increase of +1.5. Random context, on the other hand, consistently lowers performance up to a maximum BLEU reduction of \u22120.7. This outcome affirms that incorporating even a single sentence as context yields improvements, which stem exactly from high-quality context rather than other artifacts. 4.2. Context dropout Next we explore the bias resulting from training with gold context when no context is available during inference (results in Table 3). A model trained with context but applied to inference without context (third row) shows a degradation of almost \u22121 BLEU points even compared to the non-contextual baseline (first row). This clearly demonstrates the context-trained model\u2019s strong inclination to depend on context during inference. To overcome this limitation we propose to use context dropout: during training, target-side context is proba- bilistically included or not. We experiment with various percentages of context dropout ([0.2\u20130.7]) and find that 0.2 yields the best results across all datasets. Now, using context dropout (row four) leads to an improvement of up to +0.6 in BLEU score when context is avail- able during inference. But more importantly, row five shows that the 1The first utterance of each conversation has no associated context. Table 3. Comparison of the BLEU scores according to the bias towards contextual information using context"}, {"question": " How is the model training process described in the text?", "answer": " During model training, the ST cross-entropy loss is computed solely for the target translation, excluding the context.", "ref_chunk": "encoder DECst(\u00b7) generates \u02c6yl at each time step that is conditionally dependent on the hidden represen- tation HE st, previous target sequence Y1:t\u22121 and the context Ycntx of previous sentences shown in Eq. (5, 6). asr) H D st = DECst(H E st , Ycntx 1:K , Y1:l\u22121) P (yl|Y1:l\u22121, X, Ycntx) = Softmax(H D st) The unique characteristic of our approach lies in the use of context also during training. While other methods may attempt to predict the (2) (3) (5) (6) context, our model uses the context exclusively as an initial condition for the decoder, overcoming memory constraints of extended audio. During model training, the ST cross-entropy loss is computed solely for the target translation, excluding the context. The model is opti- mized with a multi-task learning objective combining hybrid ASR attention Lasr att and CTC Lst att and CTC Lasr ctc as well as hybrid ST attention Lst ctc losses: L =\u03b13((1 \u2212 \u03b11)Lasr att + \u03b11Lasr ctc) att + \u03b12Lst + (1 \u2212 \u03b13)((1 \u2212 \u03b12)Lst ctc) where \u03b1\u2032s are used for interpolation. 3. EXPERIMENTS 3.1. Dataset We demonstrate the efficacy of our proposed approach through evaluations on three conversational datasets (their statistics are summarized in Table 1): Fisher-CallHome Spanish English [33], IWSLT22 Tunisian Arabic-English [34], and BOLT Chinese-English [35]. These datasets contain 3-way data comprising telephone speech, source language transcriptions, and corresponding English translations. We use separated source and target vocabularies, each Table 1. Statistics for the conversational ST corpora. Corpus Lang #Hours Train Dev Test Spanish Fisher/Callhome Tunisian IWSLT22 Chinese BOLT Sp-En Ar-En Zh-En 186.3 161.0 110.6 9.3 6.3 8.5 4.5/1.8 3.6 8.5 consisting of 4K byte-pair-encoding [36] (BPE) units. All audios are resampled from 8kHz to 16kHz, augmented with speed perturbations (0.9, 1.0 and 1.1) and transformed into 83-dimensional feature frames (80 log-mel filterbank coefficients plus 3 pitch features). Ad- ditionally, we augment the features with specaugment [37], with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. During scoring we report the results in terms of case- sensitive BLEU with punctuation. We also measure the statistical significance of improvement using paired bootstrap resampling with sacreBLEU[38]. 3.2. Baseline configuration We conduct all experiments by customizing the ESPnet toolkit [13]. For the encoders ENCasr in Eq. (2) and ENCst in Eq. (3), we employ the conformer architecture [39] consisting of 12 blocks for ENCasr and 6 blocks for ENCst. Both encoders are configured with 2048 feed-forward dimensions, 256 attention dimensions, and 4 attention heads. The transformer architecture is employed for DECasr and DECst in Eq. (4), each with 6 decoder blocks and the same configu- ration as the encoders. The model has 72M parameters. We follow ST best practices [13], we first pretrain the ASR module followed by fine-tuning of the entire E2E-ST model for the translation task. Our training configuration remains consistent for both pretraining and fine-tuning, employing Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight (\u03b11, \u03b12) in Eq. (7) to 0.3 and the weight that combines ASR and ST losses (7) Table 2. Comparison of the BLEU scores according to contextual information quality using one previous utterrance as context. \u2020: de- notes a statistically significant difference (p < 0.01) compared to the no-context baseline. Context Type Fisher CallHome IWSLT22 No-context Random Gold 29.8 29.9 31.3\u2020 25.9 25.2 26.0 19.7 19.5 19.9 (\u03b13) to 0.3. During inference, we use a beam size of 10 and length penalty of 0.3. 3.3. Contextual E2E-ST configuration In the Contextual E2E-ST model (described in \u00a72), we retain the same configuration as the baseline. The only difference is that during training with teacher-forcing, the decoder initial condition is the pre- ceding sentences alongside a start-of-sentence token. Heuristically, we limit any long contextual sentences to the last 50 tokens. Upon visual inspection, we found that these truncated sentences effectively capture the contextually relevant information. The previous sentence considered a part of context only if they are from the same recording.1 We will refer to (Gold) context when employing ground-truth trans- lations and to (Hyp) when utilizing the model\u2019s predictions. During training we only use the Gold context, however during inference we explore both Gold and Hyp context, simulating both an oracle and a more realistic scenario. 4. RESULTS 4.1. Contextual ST results We examined the effect of preceding context on model performance by comparing Gold previous context with the no-context baseline and randomly selected sentences unrelated to the ground truth. The results, shown in Table 2, use a context size of one preceding sentence. The analysis indicates that, compared to the no-context baseline, gold context consistently improves performance, with a maximum BLEU increase of +1.5. Random context, on the other hand, consistently lowers performance up to a maximum BLEU reduction of \u22120.7. This outcome affirms that incorporating even a single sentence as context yields improvements, which stem exactly from high-quality context rather than other artifacts. 4.2. Context dropout Next we explore the bias resulting from training with gold context when no context is available during inference (results in Table 3). A model trained with context but applied to inference without context (third row) shows a degradation of almost \u22121 BLEU points even compared to the non-contextual baseline (first row). This clearly demonstrates the context-trained model\u2019s strong inclination to depend on context during inference. To overcome this limitation we propose to use context dropout: during training, target-side context is proba- bilistically included or not. We experiment with various percentages of context dropout ([0.2\u20130.7]) and find that 0.2 yields the best results across all datasets. Now, using context dropout (row four) leads to an improvement of up to +0.6 in BLEU score when context is avail- able during inference. But more importantly, row five shows that the 1The first utterance of each conversation has no associated context. Table 3. Comparison of the BLEU scores according to the bias towards contextual information using context"}, {"question": " What is the model optimized with during training?", "answer": " The model is optimized with a multi-task learning objective combining hybrid ASR attention, CTC, and ST losses.", "ref_chunk": "encoder DECst(\u00b7) generates \u02c6yl at each time step that is conditionally dependent on the hidden represen- tation HE st, previous target sequence Y1:t\u22121 and the context Ycntx of previous sentences shown in Eq. (5, 6). asr) H D st = DECst(H E st , Ycntx 1:K , Y1:l\u22121) P (yl|Y1:l\u22121, X, Ycntx) = Softmax(H D st) The unique characteristic of our approach lies in the use of context also during training. While other methods may attempt to predict the (2) (3) (5) (6) context, our model uses the context exclusively as an initial condition for the decoder, overcoming memory constraints of extended audio. During model training, the ST cross-entropy loss is computed solely for the target translation, excluding the context. The model is opti- mized with a multi-task learning objective combining hybrid ASR attention Lasr att and CTC Lst att and CTC Lasr ctc as well as hybrid ST attention Lst ctc losses: L =\u03b13((1 \u2212 \u03b11)Lasr att + \u03b11Lasr ctc) att + \u03b12Lst + (1 \u2212 \u03b13)((1 \u2212 \u03b12)Lst ctc) where \u03b1\u2032s are used for interpolation. 3. EXPERIMENTS 3.1. Dataset We demonstrate the efficacy of our proposed approach through evaluations on three conversational datasets (their statistics are summarized in Table 1): Fisher-CallHome Spanish English [33], IWSLT22 Tunisian Arabic-English [34], and BOLT Chinese-English [35]. These datasets contain 3-way data comprising telephone speech, source language transcriptions, and corresponding English translations. We use separated source and target vocabularies, each Table 1. Statistics for the conversational ST corpora. Corpus Lang #Hours Train Dev Test Spanish Fisher/Callhome Tunisian IWSLT22 Chinese BOLT Sp-En Ar-En Zh-En 186.3 161.0 110.6 9.3 6.3 8.5 4.5/1.8 3.6 8.5 consisting of 4K byte-pair-encoding [36] (BPE) units. All audios are resampled from 8kHz to 16kHz, augmented with speed perturbations (0.9, 1.0 and 1.1) and transformed into 83-dimensional feature frames (80 log-mel filterbank coefficients plus 3 pitch features). Ad- ditionally, we augment the features with specaugment [37], with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. During scoring we report the results in terms of case- sensitive BLEU with punctuation. We also measure the statistical significance of improvement using paired bootstrap resampling with sacreBLEU[38]. 3.2. Baseline configuration We conduct all experiments by customizing the ESPnet toolkit [13]. For the encoders ENCasr in Eq. (2) and ENCst in Eq. (3), we employ the conformer architecture [39] consisting of 12 blocks for ENCasr and 6 blocks for ENCst. Both encoders are configured with 2048 feed-forward dimensions, 256 attention dimensions, and 4 attention heads. The transformer architecture is employed for DECasr and DECst in Eq. (4), each with 6 decoder blocks and the same configu- ration as the encoders. The model has 72M parameters. We follow ST best practices [13], we first pretrain the ASR module followed by fine-tuning of the entire E2E-ST model for the translation task. Our training configuration remains consistent for both pretraining and fine-tuning, employing Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight (\u03b11, \u03b12) in Eq. (7) to 0.3 and the weight that combines ASR and ST losses (7) Table 2. Comparison of the BLEU scores according to contextual information quality using one previous utterrance as context. \u2020: de- notes a statistically significant difference (p < 0.01) compared to the no-context baseline. Context Type Fisher CallHome IWSLT22 No-context Random Gold 29.8 29.9 31.3\u2020 25.9 25.2 26.0 19.7 19.5 19.9 (\u03b13) to 0.3. During inference, we use a beam size of 10 and length penalty of 0.3. 3.3. Contextual E2E-ST configuration In the Contextual E2E-ST model (described in \u00a72), we retain the same configuration as the baseline. The only difference is that during training with teacher-forcing, the decoder initial condition is the pre- ceding sentences alongside a start-of-sentence token. Heuristically, we limit any long contextual sentences to the last 50 tokens. Upon visual inspection, we found that these truncated sentences effectively capture the contextually relevant information. The previous sentence considered a part of context only if they are from the same recording.1 We will refer to (Gold) context when employing ground-truth trans- lations and to (Hyp) when utilizing the model\u2019s predictions. During training we only use the Gold context, however during inference we explore both Gold and Hyp context, simulating both an oracle and a more realistic scenario. 4. RESULTS 4.1. Contextual ST results We examined the effect of preceding context on model performance by comparing Gold previous context with the no-context baseline and randomly selected sentences unrelated to the ground truth. The results, shown in Table 2, use a context size of one preceding sentence. The analysis indicates that, compared to the no-context baseline, gold context consistently improves performance, with a maximum BLEU increase of +1.5. Random context, on the other hand, consistently lowers performance up to a maximum BLEU reduction of \u22120.7. This outcome affirms that incorporating even a single sentence as context yields improvements, which stem exactly from high-quality context rather than other artifacts. 4.2. Context dropout Next we explore the bias resulting from training with gold context when no context is available during inference (results in Table 3). A model trained with context but applied to inference without context (third row) shows a degradation of almost \u22121 BLEU points even compared to the non-contextual baseline (first row). This clearly demonstrates the context-trained model\u2019s strong inclination to depend on context during inference. To overcome this limitation we propose to use context dropout: during training, target-side context is proba- bilistically included or not. We experiment with various percentages of context dropout ([0.2\u20130.7]) and find that 0.2 yields the best results across all datasets. Now, using context dropout (row four) leads to an improvement of up to +0.6 in BLEU score when context is avail- able during inference. But more importantly, row five shows that the 1The first utterance of each conversation has no associated context. Table 3. Comparison of the BLEU scores according to the bias towards contextual information using context"}, {"question": " How many blocks does the conformer architecture consist of for the ENCst encoder?", "answer": " The conformer architecture consists of 6 blocks for the ENCst encoder.", "ref_chunk": "encoder DECst(\u00b7) generates \u02c6yl at each time step that is conditionally dependent on the hidden represen- tation HE st, previous target sequence Y1:t\u22121 and the context Ycntx of previous sentences shown in Eq. (5, 6). asr) H D st = DECst(H E st , Ycntx 1:K , Y1:l\u22121) P (yl|Y1:l\u22121, X, Ycntx) = Softmax(H D st) The unique characteristic of our approach lies in the use of context also during training. While other methods may attempt to predict the (2) (3) (5) (6) context, our model uses the context exclusively as an initial condition for the decoder, overcoming memory constraints of extended audio. During model training, the ST cross-entropy loss is computed solely for the target translation, excluding the context. The model is opti- mized with a multi-task learning objective combining hybrid ASR attention Lasr att and CTC Lst att and CTC Lasr ctc as well as hybrid ST attention Lst ctc losses: L =\u03b13((1 \u2212 \u03b11)Lasr att + \u03b11Lasr ctc) att + \u03b12Lst + (1 \u2212 \u03b13)((1 \u2212 \u03b12)Lst ctc) where \u03b1\u2032s are used for interpolation. 3. EXPERIMENTS 3.1. Dataset We demonstrate the efficacy of our proposed approach through evaluations on three conversational datasets (their statistics are summarized in Table 1): Fisher-CallHome Spanish English [33], IWSLT22 Tunisian Arabic-English [34], and BOLT Chinese-English [35]. These datasets contain 3-way data comprising telephone speech, source language transcriptions, and corresponding English translations. We use separated source and target vocabularies, each Table 1. Statistics for the conversational ST corpora. Corpus Lang #Hours Train Dev Test Spanish Fisher/Callhome Tunisian IWSLT22 Chinese BOLT Sp-En Ar-En Zh-En 186.3 161.0 110.6 9.3 6.3 8.5 4.5/1.8 3.6 8.5 consisting of 4K byte-pair-encoding [36] (BPE) units. All audios are resampled from 8kHz to 16kHz, augmented with speed perturbations (0.9, 1.0 and 1.1) and transformed into 83-dimensional feature frames (80 log-mel filterbank coefficients plus 3 pitch features). Ad- ditionally, we augment the features with specaugment [37], with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. During scoring we report the results in terms of case- sensitive BLEU with punctuation. We also measure the statistical significance of improvement using paired bootstrap resampling with sacreBLEU[38]. 3.2. Baseline configuration We conduct all experiments by customizing the ESPnet toolkit [13]. For the encoders ENCasr in Eq. (2) and ENCst in Eq. (3), we employ the conformer architecture [39] consisting of 12 blocks for ENCasr and 6 blocks for ENCst. Both encoders are configured with 2048 feed-forward dimensions, 256 attention dimensions, and 4 attention heads. The transformer architecture is employed for DECasr and DECst in Eq. (4), each with 6 decoder blocks and the same configu- ration as the encoders. The model has 72M parameters. We follow ST best practices [13], we first pretrain the ASR module followed by fine-tuning of the entire E2E-ST model for the translation task. Our training configuration remains consistent for both pretraining and fine-tuning, employing Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight (\u03b11, \u03b12) in Eq. (7) to 0.3 and the weight that combines ASR and ST losses (7) Table 2. Comparison of the BLEU scores according to contextual information quality using one previous utterrance as context. \u2020: de- notes a statistically significant difference (p < 0.01) compared to the no-context baseline. Context Type Fisher CallHome IWSLT22 No-context Random Gold 29.8 29.9 31.3\u2020 25.9 25.2 26.0 19.7 19.5 19.9 (\u03b13) to 0.3. During inference, we use a beam size of 10 and length penalty of 0.3. 3.3. Contextual E2E-ST configuration In the Contextual E2E-ST model (described in \u00a72), we retain the same configuration as the baseline. The only difference is that during training with teacher-forcing, the decoder initial condition is the pre- ceding sentences alongside a start-of-sentence token. Heuristically, we limit any long contextual sentences to the last 50 tokens. Upon visual inspection, we found that these truncated sentences effectively capture the contextually relevant information. The previous sentence considered a part of context only if they are from the same recording.1 We will refer to (Gold) context when employing ground-truth trans- lations and to (Hyp) when utilizing the model\u2019s predictions. During training we only use the Gold context, however during inference we explore both Gold and Hyp context, simulating both an oracle and a more realistic scenario. 4. RESULTS 4.1. Contextual ST results We examined the effect of preceding context on model performance by comparing Gold previous context with the no-context baseline and randomly selected sentences unrelated to the ground truth. The results, shown in Table 2, use a context size of one preceding sentence. The analysis indicates that, compared to the no-context baseline, gold context consistently improves performance, with a maximum BLEU increase of +1.5. Random context, on the other hand, consistently lowers performance up to a maximum BLEU reduction of \u22120.7. This outcome affirms that incorporating even a single sentence as context yields improvements, which stem exactly from high-quality context rather than other artifacts. 4.2. Context dropout Next we explore the bias resulting from training with gold context when no context is available during inference (results in Table 3). A model trained with context but applied to inference without context (third row) shows a degradation of almost \u22121 BLEU points even compared to the non-contextual baseline (first row). This clearly demonstrates the context-trained model\u2019s strong inclination to depend on context during inference. To overcome this limitation we propose to use context dropout: during training, target-side context is proba- bilistically included or not. We experiment with various percentages of context dropout ([0.2\u20130.7]) and find that 0.2 yields the best results across all datasets. Now, using context dropout (row four) leads to an improvement of up to +0.6 in BLEU score when context is avail- able during inference. But more importantly, row five shows that the 1The first utterance of each conversation has no associated context. Table 3. Comparison of the BLEU scores according to the bias towards contextual information using context"}, {"question": " What is the learning rate used in the training configuration?", "answer": " The learning rate used is 0.001.", "ref_chunk": "encoder DECst(\u00b7) generates \u02c6yl at each time step that is conditionally dependent on the hidden represen- tation HE st, previous target sequence Y1:t\u22121 and the context Ycntx of previous sentences shown in Eq. (5, 6). asr) H D st = DECst(H E st , Ycntx 1:K , Y1:l\u22121) P (yl|Y1:l\u22121, X, Ycntx) = Softmax(H D st) The unique characteristic of our approach lies in the use of context also during training. While other methods may attempt to predict the (2) (3) (5) (6) context, our model uses the context exclusively as an initial condition for the decoder, overcoming memory constraints of extended audio. During model training, the ST cross-entropy loss is computed solely for the target translation, excluding the context. The model is opti- mized with a multi-task learning objective combining hybrid ASR attention Lasr att and CTC Lst att and CTC Lasr ctc as well as hybrid ST attention Lst ctc losses: L =\u03b13((1 \u2212 \u03b11)Lasr att + \u03b11Lasr ctc) att + \u03b12Lst + (1 \u2212 \u03b13)((1 \u2212 \u03b12)Lst ctc) where \u03b1\u2032s are used for interpolation. 3. EXPERIMENTS 3.1. Dataset We demonstrate the efficacy of our proposed approach through evaluations on three conversational datasets (their statistics are summarized in Table 1): Fisher-CallHome Spanish English [33], IWSLT22 Tunisian Arabic-English [34], and BOLT Chinese-English [35]. These datasets contain 3-way data comprising telephone speech, source language transcriptions, and corresponding English translations. We use separated source and target vocabularies, each Table 1. Statistics for the conversational ST corpora. Corpus Lang #Hours Train Dev Test Spanish Fisher/Callhome Tunisian IWSLT22 Chinese BOLT Sp-En Ar-En Zh-En 186.3 161.0 110.6 9.3 6.3 8.5 4.5/1.8 3.6 8.5 consisting of 4K byte-pair-encoding [36] (BPE) units. All audios are resampled from 8kHz to 16kHz, augmented with speed perturbations (0.9, 1.0 and 1.1) and transformed into 83-dimensional feature frames (80 log-mel filterbank coefficients plus 3 pitch features). Ad- ditionally, we augment the features with specaugment [37], with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. During scoring we report the results in terms of case- sensitive BLEU with punctuation. We also measure the statistical significance of improvement using paired bootstrap resampling with sacreBLEU[38]. 3.2. Baseline configuration We conduct all experiments by customizing the ESPnet toolkit [13]. For the encoders ENCasr in Eq. (2) and ENCst in Eq. (3), we employ the conformer architecture [39] consisting of 12 blocks for ENCasr and 6 blocks for ENCst. Both encoders are configured with 2048 feed-forward dimensions, 256 attention dimensions, and 4 attention heads. The transformer architecture is employed for DECasr and DECst in Eq. (4), each with 6 decoder blocks and the same configu- ration as the encoders. The model has 72M parameters. We follow ST best practices [13], we first pretrain the ASR module followed by fine-tuning of the entire E2E-ST model for the translation task. Our training configuration remains consistent for both pretraining and fine-tuning, employing Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight (\u03b11, \u03b12) in Eq. (7) to 0.3 and the weight that combines ASR and ST losses (7) Table 2. Comparison of the BLEU scores according to contextual information quality using one previous utterrance as context. \u2020: de- notes a statistically significant difference (p < 0.01) compared to the no-context baseline. Context Type Fisher CallHome IWSLT22 No-context Random Gold 29.8 29.9 31.3\u2020 25.9 25.2 26.0 19.7 19.5 19.9 (\u03b13) to 0.3. During inference, we use a beam size of 10 and length penalty of 0.3. 3.3. Contextual E2E-ST configuration In the Contextual E2E-ST model (described in \u00a72), we retain the same configuration as the baseline. The only difference is that during training with teacher-forcing, the decoder initial condition is the pre- ceding sentences alongside a start-of-sentence token. Heuristically, we limit any long contextual sentences to the last 50 tokens. Upon visual inspection, we found that these truncated sentences effectively capture the contextually relevant information. The previous sentence considered a part of context only if they are from the same recording.1 We will refer to (Gold) context when employing ground-truth trans- lations and to (Hyp) when utilizing the model\u2019s predictions. During training we only use the Gold context, however during inference we explore both Gold and Hyp context, simulating both an oracle and a more realistic scenario. 4. RESULTS 4.1. Contextual ST results We examined the effect of preceding context on model performance by comparing Gold previous context with the no-context baseline and randomly selected sentences unrelated to the ground truth. The results, shown in Table 2, use a context size of one preceding sentence. The analysis indicates that, compared to the no-context baseline, gold context consistently improves performance, with a maximum BLEU increase of +1.5. Random context, on the other hand, consistently lowers performance up to a maximum BLEU reduction of \u22120.7. This outcome affirms that incorporating even a single sentence as context yields improvements, which stem exactly from high-quality context rather than other artifacts. 4.2. Context dropout Next we explore the bias resulting from training with gold context when no context is available during inference (results in Table 3). A model trained with context but applied to inference without context (third row) shows a degradation of almost \u22121 BLEU points even compared to the non-contextual baseline (first row). This clearly demonstrates the context-trained model\u2019s strong inclination to depend on context during inference. To overcome this limitation we propose to use context dropout: during training, target-side context is proba- bilistically included or not. We experiment with various percentages of context dropout ([0.2\u20130.7]) and find that 0.2 yields the best results across all datasets. Now, using context dropout (row four) leads to an improvement of up to +0.6 in BLEU score when context is avail- able during inference. But more importantly, row five shows that the 1The first utterance of each conversation has no associated context. Table 3. Comparison of the BLEU scores according to the bias towards contextual information using context"}, {"question": " What is the beam size used during inference?", "answer": " A beam size of 10 is used during inference.", "ref_chunk": "encoder DECst(\u00b7) generates \u02c6yl at each time step that is conditionally dependent on the hidden represen- tation HE st, previous target sequence Y1:t\u22121 and the context Ycntx of previous sentences shown in Eq. (5, 6). asr) H D st = DECst(H E st , Ycntx 1:K , Y1:l\u22121) P (yl|Y1:l\u22121, X, Ycntx) = Softmax(H D st) The unique characteristic of our approach lies in the use of context also during training. While other methods may attempt to predict the (2) (3) (5) (6) context, our model uses the context exclusively as an initial condition for the decoder, overcoming memory constraints of extended audio. During model training, the ST cross-entropy loss is computed solely for the target translation, excluding the context. The model is opti- mized with a multi-task learning objective combining hybrid ASR attention Lasr att and CTC Lst att and CTC Lasr ctc as well as hybrid ST attention Lst ctc losses: L =\u03b13((1 \u2212 \u03b11)Lasr att + \u03b11Lasr ctc) att + \u03b12Lst + (1 \u2212 \u03b13)((1 \u2212 \u03b12)Lst ctc) where \u03b1\u2032s are used for interpolation. 3. EXPERIMENTS 3.1. Dataset We demonstrate the efficacy of our proposed approach through evaluations on three conversational datasets (their statistics are summarized in Table 1): Fisher-CallHome Spanish English [33], IWSLT22 Tunisian Arabic-English [34], and BOLT Chinese-English [35]. These datasets contain 3-way data comprising telephone speech, source language transcriptions, and corresponding English translations. We use separated source and target vocabularies, each Table 1. Statistics for the conversational ST corpora. Corpus Lang #Hours Train Dev Test Spanish Fisher/Callhome Tunisian IWSLT22 Chinese BOLT Sp-En Ar-En Zh-En 186.3 161.0 110.6 9.3 6.3 8.5 4.5/1.8 3.6 8.5 consisting of 4K byte-pair-encoding [36] (BPE) units. All audios are resampled from 8kHz to 16kHz, augmented with speed perturbations (0.9, 1.0 and 1.1) and transformed into 83-dimensional feature frames (80 log-mel filterbank coefficients plus 3 pitch features). Ad- ditionally, we augment the features with specaugment [37], with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. During scoring we report the results in terms of case- sensitive BLEU with punctuation. We also measure the statistical significance of improvement using paired bootstrap resampling with sacreBLEU[38]. 3.2. Baseline configuration We conduct all experiments by customizing the ESPnet toolkit [13]. For the encoders ENCasr in Eq. (2) and ENCst in Eq. (3), we employ the conformer architecture [39] consisting of 12 blocks for ENCasr and 6 blocks for ENCst. Both encoders are configured with 2048 feed-forward dimensions, 256 attention dimensions, and 4 attention heads. The transformer architecture is employed for DECasr and DECst in Eq. (4), each with 6 decoder blocks and the same configu- ration as the encoders. The model has 72M parameters. We follow ST best practices [13], we first pretrain the ASR module followed by fine-tuning of the entire E2E-ST model for the translation task. Our training configuration remains consistent for both pretraining and fine-tuning, employing Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight (\u03b11, \u03b12) in Eq. (7) to 0.3 and the weight that combines ASR and ST losses (7) Table 2. Comparison of the BLEU scores according to contextual information quality using one previous utterrance as context. \u2020: de- notes a statistically significant difference (p < 0.01) compared to the no-context baseline. Context Type Fisher CallHome IWSLT22 No-context Random Gold 29.8 29.9 31.3\u2020 25.9 25.2 26.0 19.7 19.5 19.9 (\u03b13) to 0.3. During inference, we use a beam size of 10 and length penalty of 0.3. 3.3. Contextual E2E-ST configuration In the Contextual E2E-ST model (described in \u00a72), we retain the same configuration as the baseline. The only difference is that during training with teacher-forcing, the decoder initial condition is the pre- ceding sentences alongside a start-of-sentence token. Heuristically, we limit any long contextual sentences to the last 50 tokens. Upon visual inspection, we found that these truncated sentences effectively capture the contextually relevant information. The previous sentence considered a part of context only if they are from the same recording.1 We will refer to (Gold) context when employing ground-truth trans- lations and to (Hyp) when utilizing the model\u2019s predictions. During training we only use the Gold context, however during inference we explore both Gold and Hyp context, simulating both an oracle and a more realistic scenario. 4. RESULTS 4.1. Contextual ST results We examined the effect of preceding context on model performance by comparing Gold previous context with the no-context baseline and randomly selected sentences unrelated to the ground truth. The results, shown in Table 2, use a context size of one preceding sentence. The analysis indicates that, compared to the no-context baseline, gold context consistently improves performance, with a maximum BLEU increase of +1.5. Random context, on the other hand, consistently lowers performance up to a maximum BLEU reduction of \u22120.7. This outcome affirms that incorporating even a single sentence as context yields improvements, which stem exactly from high-quality context rather than other artifacts. 4.2. Context dropout Next we explore the bias resulting from training with gold context when no context is available during inference (results in Table 3). A model trained with context but applied to inference without context (third row) shows a degradation of almost \u22121 BLEU points even compared to the non-contextual baseline (first row). This clearly demonstrates the context-trained model\u2019s strong inclination to depend on context during inference. To overcome this limitation we propose to use context dropout: during training, target-side context is proba- bilistically included or not. We experiment with various percentages of context dropout ([0.2\u20130.7]) and find that 0.2 yields the best results across all datasets. Now, using context dropout (row four) leads to an improvement of up to +0.6 in BLEU score when context is avail- able during inference. But more importantly, row five shows that the 1The first utterance of each conversation has no associated context. Table 3. Comparison of the BLEU scores according to the bias towards contextual information using context"}, {"question": " How is the context dropout introduced during training?", "answer": " During training, target-side context is probabilistically included or not, which is known as context dropout.", "ref_chunk": "encoder DECst(\u00b7) generates \u02c6yl at each time step that is conditionally dependent on the hidden represen- tation HE st, previous target sequence Y1:t\u22121 and the context Ycntx of previous sentences shown in Eq. (5, 6). asr) H D st = DECst(H E st , Ycntx 1:K , Y1:l\u22121) P (yl|Y1:l\u22121, X, Ycntx) = Softmax(H D st) The unique characteristic of our approach lies in the use of context also during training. While other methods may attempt to predict the (2) (3) (5) (6) context, our model uses the context exclusively as an initial condition for the decoder, overcoming memory constraints of extended audio. During model training, the ST cross-entropy loss is computed solely for the target translation, excluding the context. The model is opti- mized with a multi-task learning objective combining hybrid ASR attention Lasr att and CTC Lst att and CTC Lasr ctc as well as hybrid ST attention Lst ctc losses: L =\u03b13((1 \u2212 \u03b11)Lasr att + \u03b11Lasr ctc) att + \u03b12Lst + (1 \u2212 \u03b13)((1 \u2212 \u03b12)Lst ctc) where \u03b1\u2032s are used for interpolation. 3. EXPERIMENTS 3.1. Dataset We demonstrate the efficacy of our proposed approach through evaluations on three conversational datasets (their statistics are summarized in Table 1): Fisher-CallHome Spanish English [33], IWSLT22 Tunisian Arabic-English [34], and BOLT Chinese-English [35]. These datasets contain 3-way data comprising telephone speech, source language transcriptions, and corresponding English translations. We use separated source and target vocabularies, each Table 1. Statistics for the conversational ST corpora. Corpus Lang #Hours Train Dev Test Spanish Fisher/Callhome Tunisian IWSLT22 Chinese BOLT Sp-En Ar-En Zh-En 186.3 161.0 110.6 9.3 6.3 8.5 4.5/1.8 3.6 8.5 consisting of 4K byte-pair-encoding [36] (BPE) units. All audios are resampled from 8kHz to 16kHz, augmented with speed perturbations (0.9, 1.0 and 1.1) and transformed into 83-dimensional feature frames (80 log-mel filterbank coefficients plus 3 pitch features). Ad- ditionally, we augment the features with specaugment [37], with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. During scoring we report the results in terms of case- sensitive BLEU with punctuation. We also measure the statistical significance of improvement using paired bootstrap resampling with sacreBLEU[38]. 3.2. Baseline configuration We conduct all experiments by customizing the ESPnet toolkit [13]. For the encoders ENCasr in Eq. (2) and ENCst in Eq. (3), we employ the conformer architecture [39] consisting of 12 blocks for ENCasr and 6 blocks for ENCst. Both encoders are configured with 2048 feed-forward dimensions, 256 attention dimensions, and 4 attention heads. The transformer architecture is employed for DECasr and DECst in Eq. (4), each with 6 decoder blocks and the same configu- ration as the encoders. The model has 72M parameters. We follow ST best practices [13], we first pretrain the ASR module followed by fine-tuning of the entire E2E-ST model for the translation task. Our training configuration remains consistent for both pretraining and fine-tuning, employing Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight (\u03b11, \u03b12) in Eq. (7) to 0.3 and the weight that combines ASR and ST losses (7) Table 2. Comparison of the BLEU scores according to contextual information quality using one previous utterrance as context. \u2020: de- notes a statistically significant difference (p < 0.01) compared to the no-context baseline. Context Type Fisher CallHome IWSLT22 No-context Random Gold 29.8 29.9 31.3\u2020 25.9 25.2 26.0 19.7 19.5 19.9 (\u03b13) to 0.3. During inference, we use a beam size of 10 and length penalty of 0.3. 3.3. Contextual E2E-ST configuration In the Contextual E2E-ST model (described in \u00a72), we retain the same configuration as the baseline. The only difference is that during training with teacher-forcing, the decoder initial condition is the pre- ceding sentences alongside a start-of-sentence token. Heuristically, we limit any long contextual sentences to the last 50 tokens. Upon visual inspection, we found that these truncated sentences effectively capture the contextually relevant information. The previous sentence considered a part of context only if they are from the same recording.1 We will refer to (Gold) context when employing ground-truth trans- lations and to (Hyp) when utilizing the model\u2019s predictions. During training we only use the Gold context, however during inference we explore both Gold and Hyp context, simulating both an oracle and a more realistic scenario. 4. RESULTS 4.1. Contextual ST results We examined the effect of preceding context on model performance by comparing Gold previous context with the no-context baseline and randomly selected sentences unrelated to the ground truth. The results, shown in Table 2, use a context size of one preceding sentence. The analysis indicates that, compared to the no-context baseline, gold context consistently improves performance, with a maximum BLEU increase of +1.5. Random context, on the other hand, consistently lowers performance up to a maximum BLEU reduction of \u22120.7. This outcome affirms that incorporating even a single sentence as context yields improvements, which stem exactly from high-quality context rather than other artifacts. 4.2. Context dropout Next we explore the bias resulting from training with gold context when no context is available during inference (results in Table 3). A model trained with context but applied to inference without context (third row) shows a degradation of almost \u22121 BLEU points even compared to the non-contextual baseline (first row). This clearly demonstrates the context-trained model\u2019s strong inclination to depend on context during inference. To overcome this limitation we propose to use context dropout: during training, target-side context is proba- bilistically included or not. We experiment with various percentages of context dropout ([0.2\u20130.7]) and find that 0.2 yields the best results across all datasets. Now, using context dropout (row four) leads to an improvement of up to +0.6 in BLEU score when context is avail- able during inference. But more importantly, row five shows that the 1The first utterance of each conversation has no associated context. Table 3. Comparison of the BLEU scores according to the bias towards contextual information using context"}, {"question": " What is the best percentage of context dropout found during experiments?", "answer": " The best results were found with a context dropout of 0.2.", "ref_chunk": "encoder DECst(\u00b7) generates \u02c6yl at each time step that is conditionally dependent on the hidden represen- tation HE st, previous target sequence Y1:t\u22121 and the context Ycntx of previous sentences shown in Eq. (5, 6). asr) H D st = DECst(H E st , Ycntx 1:K , Y1:l\u22121) P (yl|Y1:l\u22121, X, Ycntx) = Softmax(H D st) The unique characteristic of our approach lies in the use of context also during training. While other methods may attempt to predict the (2) (3) (5) (6) context, our model uses the context exclusively as an initial condition for the decoder, overcoming memory constraints of extended audio. During model training, the ST cross-entropy loss is computed solely for the target translation, excluding the context. The model is opti- mized with a multi-task learning objective combining hybrid ASR attention Lasr att and CTC Lst att and CTC Lasr ctc as well as hybrid ST attention Lst ctc losses: L =\u03b13((1 \u2212 \u03b11)Lasr att + \u03b11Lasr ctc) att + \u03b12Lst + (1 \u2212 \u03b13)((1 \u2212 \u03b12)Lst ctc) where \u03b1\u2032s are used for interpolation. 3. EXPERIMENTS 3.1. Dataset We demonstrate the efficacy of our proposed approach through evaluations on three conversational datasets (their statistics are summarized in Table 1): Fisher-CallHome Spanish English [33], IWSLT22 Tunisian Arabic-English [34], and BOLT Chinese-English [35]. These datasets contain 3-way data comprising telephone speech, source language transcriptions, and corresponding English translations. We use separated source and target vocabularies, each Table 1. Statistics for the conversational ST corpora. Corpus Lang #Hours Train Dev Test Spanish Fisher/Callhome Tunisian IWSLT22 Chinese BOLT Sp-En Ar-En Zh-En 186.3 161.0 110.6 9.3 6.3 8.5 4.5/1.8 3.6 8.5 consisting of 4K byte-pair-encoding [36] (BPE) units. All audios are resampled from 8kHz to 16kHz, augmented with speed perturbations (0.9, 1.0 and 1.1) and transformed into 83-dimensional feature frames (80 log-mel filterbank coefficients plus 3 pitch features). Ad- ditionally, we augment the features with specaugment [37], with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. During scoring we report the results in terms of case- sensitive BLEU with punctuation. We also measure the statistical significance of improvement using paired bootstrap resampling with sacreBLEU[38]. 3.2. Baseline configuration We conduct all experiments by customizing the ESPnet toolkit [13]. For the encoders ENCasr in Eq. (2) and ENCst in Eq. (3), we employ the conformer architecture [39] consisting of 12 blocks for ENCasr and 6 blocks for ENCst. Both encoders are configured with 2048 feed-forward dimensions, 256 attention dimensions, and 4 attention heads. The transformer architecture is employed for DECasr and DECst in Eq. (4), each with 6 decoder blocks and the same configu- ration as the encoders. The model has 72M parameters. We follow ST best practices [13], we first pretrain the ASR module followed by fine-tuning of the entire E2E-ST model for the translation task. Our training configuration remains consistent for both pretraining and fine-tuning, employing Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight (\u03b11, \u03b12) in Eq. (7) to 0.3 and the weight that combines ASR and ST losses (7) Table 2. Comparison of the BLEU scores according to contextual information quality using one previous utterrance as context. \u2020: de- notes a statistically significant difference (p < 0.01) compared to the no-context baseline. Context Type Fisher CallHome IWSLT22 No-context Random Gold 29.8 29.9 31.3\u2020 25.9 25.2 26.0 19.7 19.5 19.9 (\u03b13) to 0.3. During inference, we use a beam size of 10 and length penalty of 0.3. 3.3. Contextual E2E-ST configuration In the Contextual E2E-ST model (described in \u00a72), we retain the same configuration as the baseline. The only difference is that during training with teacher-forcing, the decoder initial condition is the pre- ceding sentences alongside a start-of-sentence token. Heuristically, we limit any long contextual sentences to the last 50 tokens. Upon visual inspection, we found that these truncated sentences effectively capture the contextually relevant information. The previous sentence considered a part of context only if they are from the same recording.1 We will refer to (Gold) context when employing ground-truth trans- lations and to (Hyp) when utilizing the model\u2019s predictions. During training we only use the Gold context, however during inference we explore both Gold and Hyp context, simulating both an oracle and a more realistic scenario. 4. RESULTS 4.1. Contextual ST results We examined the effect of preceding context on model performance by comparing Gold previous context with the no-context baseline and randomly selected sentences unrelated to the ground truth. The results, shown in Table 2, use a context size of one preceding sentence. The analysis indicates that, compared to the no-context baseline, gold context consistently improves performance, with a maximum BLEU increase of +1.5. Random context, on the other hand, consistently lowers performance up to a maximum BLEU reduction of \u22120.7. This outcome affirms that incorporating even a single sentence as context yields improvements, which stem exactly from high-quality context rather than other artifacts. 4.2. Context dropout Next we explore the bias resulting from training with gold context when no context is available during inference (results in Table 3). A model trained with context but applied to inference without context (third row) shows a degradation of almost \u22121 BLEU points even compared to the non-contextual baseline (first row). This clearly demonstrates the context-trained model\u2019s strong inclination to depend on context during inference. To overcome this limitation we propose to use context dropout: during training, target-side context is proba- bilistically included or not. We experiment with various percentages of context dropout ([0.2\u20130.7]) and find that 0.2 yields the best results across all datasets. Now, using context dropout (row four) leads to an improvement of up to +0.6 in BLEU score when context is avail- able during inference. But more importantly, row five shows that the 1The first utterance of each conversation has no associated context. Table 3. Comparison of the BLEU scores according to the bias towards contextual information using context"}, {"question": " What type of sentences are considered part of the context?", "answer": " Previous sentences are considered part of the context only if they are from the same recording.", "ref_chunk": "encoder DECst(\u00b7) generates \u02c6yl at each time step that is conditionally dependent on the hidden represen- tation HE st, previous target sequence Y1:t\u22121 and the context Ycntx of previous sentences shown in Eq. (5, 6). asr) H D st = DECst(H E st , Ycntx 1:K , Y1:l\u22121) P (yl|Y1:l\u22121, X, Ycntx) = Softmax(H D st) The unique characteristic of our approach lies in the use of context also during training. While other methods may attempt to predict the (2) (3) (5) (6) context, our model uses the context exclusively as an initial condition for the decoder, overcoming memory constraints of extended audio. During model training, the ST cross-entropy loss is computed solely for the target translation, excluding the context. The model is opti- mized with a multi-task learning objective combining hybrid ASR attention Lasr att and CTC Lst att and CTC Lasr ctc as well as hybrid ST attention Lst ctc losses: L =\u03b13((1 \u2212 \u03b11)Lasr att + \u03b11Lasr ctc) att + \u03b12Lst + (1 \u2212 \u03b13)((1 \u2212 \u03b12)Lst ctc) where \u03b1\u2032s are used for interpolation. 3. EXPERIMENTS 3.1. Dataset We demonstrate the efficacy of our proposed approach through evaluations on three conversational datasets (their statistics are summarized in Table 1): Fisher-CallHome Spanish English [33], IWSLT22 Tunisian Arabic-English [34], and BOLT Chinese-English [35]. These datasets contain 3-way data comprising telephone speech, source language transcriptions, and corresponding English translations. We use separated source and target vocabularies, each Table 1. Statistics for the conversational ST corpora. Corpus Lang #Hours Train Dev Test Spanish Fisher/Callhome Tunisian IWSLT22 Chinese BOLT Sp-En Ar-En Zh-En 186.3 161.0 110.6 9.3 6.3 8.5 4.5/1.8 3.6 8.5 consisting of 4K byte-pair-encoding [36] (BPE) units. All audios are resampled from 8kHz to 16kHz, augmented with speed perturbations (0.9, 1.0 and 1.1) and transformed into 83-dimensional feature frames (80 log-mel filterbank coefficients plus 3 pitch features). Ad- ditionally, we augment the features with specaugment [37], with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. During scoring we report the results in terms of case- sensitive BLEU with punctuation. We also measure the statistical significance of improvement using paired bootstrap resampling with sacreBLEU[38]. 3.2. Baseline configuration We conduct all experiments by customizing the ESPnet toolkit [13]. For the encoders ENCasr in Eq. (2) and ENCst in Eq. (3), we employ the conformer architecture [39] consisting of 12 blocks for ENCasr and 6 blocks for ENCst. Both encoders are configured with 2048 feed-forward dimensions, 256 attention dimensions, and 4 attention heads. The transformer architecture is employed for DECasr and DECst in Eq. (4), each with 6 decoder blocks and the same configu- ration as the encoders. The model has 72M parameters. We follow ST best practices [13], we first pretrain the ASR module followed by fine-tuning of the entire E2E-ST model for the translation task. Our training configuration remains consistent for both pretraining and fine-tuning, employing Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight (\u03b11, \u03b12) in Eq. (7) to 0.3 and the weight that combines ASR and ST losses (7) Table 2. Comparison of the BLEU scores according to contextual information quality using one previous utterrance as context. \u2020: de- notes a statistically significant difference (p < 0.01) compared to the no-context baseline. Context Type Fisher CallHome IWSLT22 No-context Random Gold 29.8 29.9 31.3\u2020 25.9 25.2 26.0 19.7 19.5 19.9 (\u03b13) to 0.3. During inference, we use a beam size of 10 and length penalty of 0.3. 3.3. Contextual E2E-ST configuration In the Contextual E2E-ST model (described in \u00a72), we retain the same configuration as the baseline. The only difference is that during training with teacher-forcing, the decoder initial condition is the pre- ceding sentences alongside a start-of-sentence token. Heuristically, we limit any long contextual sentences to the last 50 tokens. Upon visual inspection, we found that these truncated sentences effectively capture the contextually relevant information. The previous sentence considered a part of context only if they are from the same recording.1 We will refer to (Gold) context when employing ground-truth trans- lations and to (Hyp) when utilizing the model\u2019s predictions. During training we only use the Gold context, however during inference we explore both Gold and Hyp context, simulating both an oracle and a more realistic scenario. 4. RESULTS 4.1. Contextual ST results We examined the effect of preceding context on model performance by comparing Gold previous context with the no-context baseline and randomly selected sentences unrelated to the ground truth. The results, shown in Table 2, use a context size of one preceding sentence. The analysis indicates that, compared to the no-context baseline, gold context consistently improves performance, with a maximum BLEU increase of +1.5. Random context, on the other hand, consistently lowers performance up to a maximum BLEU reduction of \u22120.7. This outcome affirms that incorporating even a single sentence as context yields improvements, which stem exactly from high-quality context rather than other artifacts. 4.2. Context dropout Next we explore the bias resulting from training with gold context when no context is available during inference (results in Table 3). A model trained with context but applied to inference without context (third row) shows a degradation of almost \u22121 BLEU points even compared to the non-contextual baseline (first row). This clearly demonstrates the context-trained model\u2019s strong inclination to depend on context during inference. To overcome this limitation we propose to use context dropout: during training, target-side context is proba- bilistically included or not. We experiment with various percentages of context dropout ([0.2\u20130.7]) and find that 0.2 yields the best results across all datasets. Now, using context dropout (row four) leads to an improvement of up to +0.6 in BLEU score when context is avail- able during inference. But more importantly, row five shows that the 1The first utterance of each conversation has no associated context. Table 3. Comparison of the BLEU scores according to the bias towards contextual information using context"}, {"question": " What is the effect of using gold context during training but no context during inference?", "answer": " There is a degradation of almost -1 BLEU points compared to the non-contextual baseline.", "ref_chunk": "encoder DECst(\u00b7) generates \u02c6yl at each time step that is conditionally dependent on the hidden represen- tation HE st, previous target sequence Y1:t\u22121 and the context Ycntx of previous sentences shown in Eq. (5, 6). asr) H D st = DECst(H E st , Ycntx 1:K , Y1:l\u22121) P (yl|Y1:l\u22121, X, Ycntx) = Softmax(H D st) The unique characteristic of our approach lies in the use of context also during training. While other methods may attempt to predict the (2) (3) (5) (6) context, our model uses the context exclusively as an initial condition for the decoder, overcoming memory constraints of extended audio. During model training, the ST cross-entropy loss is computed solely for the target translation, excluding the context. The model is opti- mized with a multi-task learning objective combining hybrid ASR attention Lasr att and CTC Lst att and CTC Lasr ctc as well as hybrid ST attention Lst ctc losses: L =\u03b13((1 \u2212 \u03b11)Lasr att + \u03b11Lasr ctc) att + \u03b12Lst + (1 \u2212 \u03b13)((1 \u2212 \u03b12)Lst ctc) where \u03b1\u2032s are used for interpolation. 3. EXPERIMENTS 3.1. Dataset We demonstrate the efficacy of our proposed approach through evaluations on three conversational datasets (their statistics are summarized in Table 1): Fisher-CallHome Spanish English [33], IWSLT22 Tunisian Arabic-English [34], and BOLT Chinese-English [35]. These datasets contain 3-way data comprising telephone speech, source language transcriptions, and corresponding English translations. We use separated source and target vocabularies, each Table 1. Statistics for the conversational ST corpora. Corpus Lang #Hours Train Dev Test Spanish Fisher/Callhome Tunisian IWSLT22 Chinese BOLT Sp-En Ar-En Zh-En 186.3 161.0 110.6 9.3 6.3 8.5 4.5/1.8 3.6 8.5 consisting of 4K byte-pair-encoding [36] (BPE) units. All audios are resampled from 8kHz to 16kHz, augmented with speed perturbations (0.9, 1.0 and 1.1) and transformed into 83-dimensional feature frames (80 log-mel filterbank coefficients plus 3 pitch features). Ad- ditionally, we augment the features with specaugment [37], with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. During scoring we report the results in terms of case- sensitive BLEU with punctuation. We also measure the statistical significance of improvement using paired bootstrap resampling with sacreBLEU[38]. 3.2. Baseline configuration We conduct all experiments by customizing the ESPnet toolkit [13]. For the encoders ENCasr in Eq. (2) and ENCst in Eq. (3), we employ the conformer architecture [39] consisting of 12 blocks for ENCasr and 6 blocks for ENCst. Both encoders are configured with 2048 feed-forward dimensions, 256 attention dimensions, and 4 attention heads. The transformer architecture is employed for DECasr and DECst in Eq. (4), each with 6 decoder blocks and the same configu- ration as the encoders. The model has 72M parameters. We follow ST best practices [13], we first pretrain the ASR module followed by fine-tuning of the entire E2E-ST model for the translation task. Our training configuration remains consistent for both pretraining and fine-tuning, employing Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight (\u03b11, \u03b12) in Eq. (7) to 0.3 and the weight that combines ASR and ST losses (7) Table 2. Comparison of the BLEU scores according to contextual information quality using one previous utterrance as context. \u2020: de- notes a statistically significant difference (p < 0.01) compared to the no-context baseline. Context Type Fisher CallHome IWSLT22 No-context Random Gold 29.8 29.9 31.3\u2020 25.9 25.2 26.0 19.7 19.5 19.9 (\u03b13) to 0.3. During inference, we use a beam size of 10 and length penalty of 0.3. 3.3. Contextual E2E-ST configuration In the Contextual E2E-ST model (described in \u00a72), we retain the same configuration as the baseline. The only difference is that during training with teacher-forcing, the decoder initial condition is the pre- ceding sentences alongside a start-of-sentence token. Heuristically, we limit any long contextual sentences to the last 50 tokens. Upon visual inspection, we found that these truncated sentences effectively capture the contextually relevant information. The previous sentence considered a part of context only if they are from the same recording.1 We will refer to (Gold) context when employing ground-truth trans- lations and to (Hyp) when utilizing the model\u2019s predictions. During training we only use the Gold context, however during inference we explore both Gold and Hyp context, simulating both an oracle and a more realistic scenario. 4. RESULTS 4.1. Contextual ST results We examined the effect of preceding context on model performance by comparing Gold previous context with the no-context baseline and randomly selected sentences unrelated to the ground truth. The results, shown in Table 2, use a context size of one preceding sentence. The analysis indicates that, compared to the no-context baseline, gold context consistently improves performance, with a maximum BLEU increase of +1.5. Random context, on the other hand, consistently lowers performance up to a maximum BLEU reduction of \u22120.7. This outcome affirms that incorporating even a single sentence as context yields improvements, which stem exactly from high-quality context rather than other artifacts. 4.2. Context dropout Next we explore the bias resulting from training with gold context when no context is available during inference (results in Table 3). A model trained with context but applied to inference without context (third row) shows a degradation of almost \u22121 BLEU points even compared to the non-contextual baseline (first row). This clearly demonstrates the context-trained model\u2019s strong inclination to depend on context during inference. To overcome this limitation we propose to use context dropout: during training, target-side context is proba- bilistically included or not. We experiment with various percentages of context dropout ([0.2\u20130.7]) and find that 0.2 yields the best results across all datasets. Now, using context dropout (row four) leads to an improvement of up to +0.6 in BLEU score when context is avail- able during inference. But more importantly, row five shows that the 1The first utterance of each conversation has no associated context. Table 3. Comparison of the BLEU scores according to the bias towards contextual information using context"}], "doc_text": "encoder DECst(\u00b7) generates \u02c6yl at each time step that is conditionally dependent on the hidden represen- tation HE st, previous target sequence Y1:t\u22121 and the context Ycntx of previous sentences shown in Eq. (5, 6). asr) H D st = DECst(H E st , Ycntx 1:K , Y1:l\u22121) P (yl|Y1:l\u22121, X, Ycntx) = Softmax(H D st) The unique characteristic of our approach lies in the use of context also during training. While other methods may attempt to predict the (2) (3) (5) (6) context, our model uses the context exclusively as an initial condition for the decoder, overcoming memory constraints of extended audio. During model training, the ST cross-entropy loss is computed solely for the target translation, excluding the context. The model is opti- mized with a multi-task learning objective combining hybrid ASR attention Lasr att and CTC Lst att and CTC Lasr ctc as well as hybrid ST attention Lst ctc losses: L =\u03b13((1 \u2212 \u03b11)Lasr att + \u03b11Lasr ctc) att + \u03b12Lst + (1 \u2212 \u03b13)((1 \u2212 \u03b12)Lst ctc) where \u03b1\u2032s are used for interpolation. 3. EXPERIMENTS 3.1. Dataset We demonstrate the efficacy of our proposed approach through evaluations on three conversational datasets (their statistics are summarized in Table 1): Fisher-CallHome Spanish English [33], IWSLT22 Tunisian Arabic-English [34], and BOLT Chinese-English [35]. These datasets contain 3-way data comprising telephone speech, source language transcriptions, and corresponding English translations. We use separated source and target vocabularies, each Table 1. Statistics for the conversational ST corpora. Corpus Lang #Hours Train Dev Test Spanish Fisher/Callhome Tunisian IWSLT22 Chinese BOLT Sp-En Ar-En Zh-En 186.3 161.0 110.6 9.3 6.3 8.5 4.5/1.8 3.6 8.5 consisting of 4K byte-pair-encoding [36] (BPE) units. All audios are resampled from 8kHz to 16kHz, augmented with speed perturbations (0.9, 1.0 and 1.1) and transformed into 83-dimensional feature frames (80 log-mel filterbank coefficients plus 3 pitch features). Ad- ditionally, we augment the features with specaugment [37], with mask parameters (mT, mF, T, F ) = (5, 2, 27, 0.05) and bi-cubic time-warping. During scoring we report the results in terms of case- sensitive BLEU with punctuation. We also measure the statistical significance of improvement using paired bootstrap resampling with sacreBLEU[38]. 3.2. Baseline configuration We conduct all experiments by customizing the ESPnet toolkit [13]. For the encoders ENCasr in Eq. (2) and ENCst in Eq. (3), we employ the conformer architecture [39] consisting of 12 blocks for ENCasr and 6 blocks for ENCst. Both encoders are configured with 2048 feed-forward dimensions, 256 attention dimensions, and 4 attention heads. The transformer architecture is employed for DECasr and DECst in Eq. (4), each with 6 decoder blocks and the same configu- ration as the encoders. The model has 72M parameters. We follow ST best practices [13], we first pretrain the ASR module followed by fine-tuning of the entire E2E-ST model for the translation task. Our training configuration remains consistent for both pretraining and fine-tuning, employing Adam optimizer with a learning rate of 0.001, warmup-steps of 25K, a dropout-rate of 0.1 and 40 epochs. We use joint training with hybrid CTC/attention by setting CTC weight (\u03b11, \u03b12) in Eq. (7) to 0.3 and the weight that combines ASR and ST losses (7) Table 2. Comparison of the BLEU scores according to contextual information quality using one previous utterrance as context. \u2020: de- notes a statistically significant difference (p < 0.01) compared to the no-context baseline. Context Type Fisher CallHome IWSLT22 No-context Random Gold 29.8 29.9 31.3\u2020 25.9 25.2 26.0 19.7 19.5 19.9 (\u03b13) to 0.3. During inference, we use a beam size of 10 and length penalty of 0.3. 3.3. Contextual E2E-ST configuration In the Contextual E2E-ST model (described in \u00a72), we retain the same configuration as the baseline. The only difference is that during training with teacher-forcing, the decoder initial condition is the pre- ceding sentences alongside a start-of-sentence token. Heuristically, we limit any long contextual sentences to the last 50 tokens. Upon visual inspection, we found that these truncated sentences effectively capture the contextually relevant information. The previous sentence considered a part of context only if they are from the same recording.1 We will refer to (Gold) context when employing ground-truth trans- lations and to (Hyp) when utilizing the model\u2019s predictions. During training we only use the Gold context, however during inference we explore both Gold and Hyp context, simulating both an oracle and a more realistic scenario. 4. RESULTS 4.1. Contextual ST results We examined the effect of preceding context on model performance by comparing Gold previous context with the no-context baseline and randomly selected sentences unrelated to the ground truth. The results, shown in Table 2, use a context size of one preceding sentence. The analysis indicates that, compared to the no-context baseline, gold context consistently improves performance, with a maximum BLEU increase of +1.5. Random context, on the other hand, consistently lowers performance up to a maximum BLEU reduction of \u22120.7. This outcome affirms that incorporating even a single sentence as context yields improvements, which stem exactly from high-quality context rather than other artifacts. 4.2. Context dropout Next we explore the bias resulting from training with gold context when no context is available during inference (results in Table 3). A model trained with context but applied to inference without context (third row) shows a degradation of almost \u22121 BLEU points even compared to the non-contextual baseline (first row). This clearly demonstrates the context-trained model\u2019s strong inclination to depend on context during inference. To overcome this limitation we propose to use context dropout: during training, target-side context is proba- bilistically included or not. We experiment with various percentages of context dropout ([0.2\u20130.7]) and find that 0.2 yields the best results across all datasets. Now, using context dropout (row four) leads to an improvement of up to +0.6 in BLEU score when context is avail- able during inference. But more importantly, row five shows that the 1The first utterance of each conversation has no associated context. Table 3. Comparison of the BLEU scores according to the bias towards contextual information using context"}