{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Learning_to_Speak_from_Text:_Zero-Shot_Multilingual_Text-to-Speech_with_Unsupervised_Text_Pretraining_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the evaluation metric used to measure synthetic speech quality?,answer: Mel cepstral distortion (MCD)", "ref_chunk": "a multilingual Transformer TTS model with the paired data of the unseen language. It also used the language IDs. Unseen language We compared Fully zero-shot TTS and Text-seen zero-shot TTS defined in \u00a7 2.3. In Oracle, we used the Monolingual and Multilingual w/ LIDs, which used the paired data of the unseen language. In Fully zero-shot TTS, we used Multilingual w/o LIDs to synthesize speech from text tokens in the unseen language. This method corresponds to the conventional multilingual TTS model using bytes [He et al., 2021] or IPA symbols [Staib et al., 2020]. Evaluation Metrics To objectively measure the synthetic speech quality, we used mel cepstral distortion (MCD) [Fukada et al., 1992] with the mel cepstrum dimension set to 25. We also evaluated the intelligibility using CERs computed with a multilingual ASR model [Radford et al., 2022]. We used a pretrained large model that is publicly available6. To evaluate the nat- uralness, we carried out listening tests to calculate five-scale mean opinion scores (MOS) of synthesized speech for each method. Forty native speakers were recruited through Ama- zon Mechanical Turk [Paolacci et al., 2010] for each of the tests. Furthermore, we leveraged a publicly available auto- matic MOS (AMOS) prediction model [Saeki et al., 2022a] to evaluate the naturalness. Note that the model was trained on English and Chinese datasets, but previous work [Seki et al., 2022] has reported that it also showed a correlation coef- ficient higher than 0.8 for another language (Japanese). 3.2 Evaluation Results on Seen Languages We evaluated our framework on the seen languages included in the paired data, as defined in \u00a7 2.3. Table 2 lists the results in MCD and CER. Lower values are better for both metrics. As we can see, the byte-based or IPA-based models with the proposed multilingual pretraining performed the best across all languages and metrics. Among the baselines, byte-based monolingual and multilingual models tended to have higher MCD and CER than IPA-based models, and failed to synthe- size intelligible speech in some languages. For example, the baseline byte-based models showed the high CER values for French, which has a deep orthography, meaning that a single character has different pronunciations depending on the con- text. We observed that our method improved the byte-based models and they outperformed the IPA-based baseline models for all the metrics and languages. It is worth noting that the proposed byte-based models even outperformed the proposed IPA-based models except for el and ru. These results suggest that our framework is effective in building a TTS model for languages without G2P modules. 3.3 Evaluation Results on Unseen Language We evaluated our method on zero-shot TTS for the unseen language defined in \u00a7 2.3. As described in \u00a7 2.4, we first used the x-vector from the es speaker to compute the MCD. Table 3 lists the results. The baseline models showed the CERs of over 40% and MCDs of over 10.0. However, our 6https://github.com/openai/whisper Method fi MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER de fr ru hu nl el Natural 2.75 4.52 2.12 4.73 4.86 6.22 7.14 Baseline (Monolingual) Bytes monolingual IPA monolingual 7.70 7.38 8.61 4.07 11.76 8.96 91.82 17.86 11.43 >100 25.30 11.89 8.33 7.23 56.03 27.62 10.22 7.59 93.05 24.62 7.49 7.80 15.33 19.20 10.20 8.16 85.98 21.79 Baseline (Multilingual) Bytes multilingual w/o LIDs Bytes multilingual w/ LIDs IPA multilingual w/o LIDs IPA multilingual w/ LIDs 7.68 6.51 6.31 6.16 37.46 13.19 10.64 9.76 8.71 10.84 7.44 6.88 41.35 55.79 20.86 14.97 9.38 45.92 12.89 >100 35.32 8.10 23.54 7.63 6.26 6.78 5.53 5.17 29.19 27.22 19.56 10.63 6.48 9.09 5.59 5.28 33.82 42.97 14.03 9.11 8.46 8.47 7.76 6.95 46.33 39.37 34.49 19.48 7.64 7.25 6.90 6.90 36.24 23.56 19.33 16.97 Proposed (Unsupervised text pretraining) Bytes multilingual IPA multilingual 5.65 5.88 3.79 5.52 6.48 6.61 7.15 7.72 7.38 7.25 10.62 15.85 4.99 5.18 5.28 8.62 5.01 5.30 6.05 7.37 6.52 7.00 13.74 14.42 6.57 6.53 11.75 11.06 Table 2: Evaluation results for seen languages. Bold indicates best scores in baseline and proposed methods. es Method es x-vector MCD CER Natural 2.71 Oracle Bytes monolingual IPA monolingual IPA multilingual 8.65 8.47 6.20 10.70 5.28 5.32 Baseline (Fully zero-shot TTS) Bytes multilingual IPA multilingual 11.22 10.75 64.07 44.75 fr x-vector CER 2.71 - 6.99 66.45 44.37 (a) Token embedding \ud835\udc4d(b) Encoder inputs \ud835\udc3b!\" Proposed (Text-seen zero-shot TTS) Bytes multilingual IPA multilingual 9.05 9.44 18.27 11.69 13.74 13.33 Figure 3: Visualization of token and language embedding. Pairs of similar languages (es\u2013fr and de\u2013nl) are overlapping in token embed- ding space, while output of bottleneck layer separates them. Table 3: Evaluation results for unseen language. proposed text preraining improved the metrics, resulting in CERs of less than half for both byte and IPA-based methods. Also, in contrast to the results for the seen languages, the IPA-based model outperformed the byte-based one in terms of CER. Compared with the oracle case with the paired data of the unseen language, our proposed zero-shot TTS showed higher MCD and CER but achieved only 1% difference in CER compared to the oracle byte-based monolingual model. These results demonstrate the effectiveness of our method in achieving intelligible zero-shot TTS for the unseen language. To investigate the case where the target speaker informa- tion is completely unavailable, we also used the x-vector from a seen language. We chose the fr speaker because es and fr are both categorized as Western Romance in Glottolog [Ham- marstr\u00a8om et al., 2021]. Table 3 lists the results. Note that this case does not have the MCD results, since a different speaker than the ground-truth speech was used. We can see that the unsupervised text pretraining also improved the zero- shot performance when using the x-vector from the fr speaker. In the proposed byte-based model, the cross-lingual x-vector showed the lower CER. This might result from that the es x-vector was not present in the training data whereas the fr x-vector was present in the training data. 3.4 Ablation Study To"}, {"question": " How was intelligibility evaluated in the study?,answer: Using Character Error Rates (CERs) computed with a multilingual ASR model", "ref_chunk": "a multilingual Transformer TTS model with the paired data of the unseen language. It also used the language IDs. Unseen language We compared Fully zero-shot TTS and Text-seen zero-shot TTS defined in \u00a7 2.3. In Oracle, we used the Monolingual and Multilingual w/ LIDs, which used the paired data of the unseen language. In Fully zero-shot TTS, we used Multilingual w/o LIDs to synthesize speech from text tokens in the unseen language. This method corresponds to the conventional multilingual TTS model using bytes [He et al., 2021] or IPA symbols [Staib et al., 2020]. Evaluation Metrics To objectively measure the synthetic speech quality, we used mel cepstral distortion (MCD) [Fukada et al., 1992] with the mel cepstrum dimension set to 25. We also evaluated the intelligibility using CERs computed with a multilingual ASR model [Radford et al., 2022]. We used a pretrained large model that is publicly available6. To evaluate the nat- uralness, we carried out listening tests to calculate five-scale mean opinion scores (MOS) of synthesized speech for each method. Forty native speakers were recruited through Ama- zon Mechanical Turk [Paolacci et al., 2010] for each of the tests. Furthermore, we leveraged a publicly available auto- matic MOS (AMOS) prediction model [Saeki et al., 2022a] to evaluate the naturalness. Note that the model was trained on English and Chinese datasets, but previous work [Seki et al., 2022] has reported that it also showed a correlation coef- ficient higher than 0.8 for another language (Japanese). 3.2 Evaluation Results on Seen Languages We evaluated our framework on the seen languages included in the paired data, as defined in \u00a7 2.3. Table 2 lists the results in MCD and CER. Lower values are better for both metrics. As we can see, the byte-based or IPA-based models with the proposed multilingual pretraining performed the best across all languages and metrics. Among the baselines, byte-based monolingual and multilingual models tended to have higher MCD and CER than IPA-based models, and failed to synthe- size intelligible speech in some languages. For example, the baseline byte-based models showed the high CER values for French, which has a deep orthography, meaning that a single character has different pronunciations depending on the con- text. We observed that our method improved the byte-based models and they outperformed the IPA-based baseline models for all the metrics and languages. It is worth noting that the proposed byte-based models even outperformed the proposed IPA-based models except for el and ru. These results suggest that our framework is effective in building a TTS model for languages without G2P modules. 3.3 Evaluation Results on Unseen Language We evaluated our method on zero-shot TTS for the unseen language defined in \u00a7 2.3. As described in \u00a7 2.4, we first used the x-vector from the es speaker to compute the MCD. Table 3 lists the results. The baseline models showed the CERs of over 40% and MCDs of over 10.0. However, our 6https://github.com/openai/whisper Method fi MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER de fr ru hu nl el Natural 2.75 4.52 2.12 4.73 4.86 6.22 7.14 Baseline (Monolingual) Bytes monolingual IPA monolingual 7.70 7.38 8.61 4.07 11.76 8.96 91.82 17.86 11.43 >100 25.30 11.89 8.33 7.23 56.03 27.62 10.22 7.59 93.05 24.62 7.49 7.80 15.33 19.20 10.20 8.16 85.98 21.79 Baseline (Multilingual) Bytes multilingual w/o LIDs Bytes multilingual w/ LIDs IPA multilingual w/o LIDs IPA multilingual w/ LIDs 7.68 6.51 6.31 6.16 37.46 13.19 10.64 9.76 8.71 10.84 7.44 6.88 41.35 55.79 20.86 14.97 9.38 45.92 12.89 >100 35.32 8.10 23.54 7.63 6.26 6.78 5.53 5.17 29.19 27.22 19.56 10.63 6.48 9.09 5.59 5.28 33.82 42.97 14.03 9.11 8.46 8.47 7.76 6.95 46.33 39.37 34.49 19.48 7.64 7.25 6.90 6.90 36.24 23.56 19.33 16.97 Proposed (Unsupervised text pretraining) Bytes multilingual IPA multilingual 5.65 5.88 3.79 5.52 6.48 6.61 7.15 7.72 7.38 7.25 10.62 15.85 4.99 5.18 5.28 8.62 5.01 5.30 6.05 7.37 6.52 7.00 13.74 14.42 6.57 6.53 11.75 11.06 Table 2: Evaluation results for seen languages. Bold indicates best scores in baseline and proposed methods. es Method es x-vector MCD CER Natural 2.71 Oracle Bytes monolingual IPA monolingual IPA multilingual 8.65 8.47 6.20 10.70 5.28 5.32 Baseline (Fully zero-shot TTS) Bytes multilingual IPA multilingual 11.22 10.75 64.07 44.75 fr x-vector CER 2.71 - 6.99 66.45 44.37 (a) Token embedding \ud835\udc4d(b) Encoder inputs \ud835\udc3b!\" Proposed (Text-seen zero-shot TTS) Bytes multilingual IPA multilingual 9.05 9.44 18.27 11.69 13.74 13.33 Figure 3: Visualization of token and language embedding. Pairs of similar languages (es\u2013fr and de\u2013nl) are overlapping in token embed- ding space, while output of bottleneck layer separates them. Table 3: Evaluation results for unseen language. proposed text preraining improved the metrics, resulting in CERs of less than half for both byte and IPA-based methods. Also, in contrast to the results for the seen languages, the IPA-based model outperformed the byte-based one in terms of CER. Compared with the oracle case with the paired data of the unseen language, our proposed zero-shot TTS showed higher MCD and CER but achieved only 1% difference in CER compared to the oracle byte-based monolingual model. These results demonstrate the effectiveness of our method in achieving intelligible zero-shot TTS for the unseen language. To investigate the case where the target speaker informa- tion is completely unavailable, we also used the x-vector from a seen language. We chose the fr speaker because es and fr are both categorized as Western Romance in Glottolog [Ham- marstr\u00a8om et al., 2021]. Table 3 lists the results. Note that this case does not have the MCD results, since a different speaker than the ground-truth speech was used. We can see that the unsupervised text pretraining also improved the zero- shot performance when using the x-vector from the fr speaker. In the proposed byte-based model, the cross-lingual x-vector showed the lower CER. This might result from that the es x-vector was not present in the training data whereas the fr x-vector was present in the training data. 3.4 Ablation Study To"}, {"question": " How many native speakers were recruited for the listening tests to evaluate the naturalness of synthesized speech?,answer: Forty native speakers", "ref_chunk": "a multilingual Transformer TTS model with the paired data of the unseen language. It also used the language IDs. Unseen language We compared Fully zero-shot TTS and Text-seen zero-shot TTS defined in \u00a7 2.3. In Oracle, we used the Monolingual and Multilingual w/ LIDs, which used the paired data of the unseen language. In Fully zero-shot TTS, we used Multilingual w/o LIDs to synthesize speech from text tokens in the unseen language. This method corresponds to the conventional multilingual TTS model using bytes [He et al., 2021] or IPA symbols [Staib et al., 2020]. Evaluation Metrics To objectively measure the synthetic speech quality, we used mel cepstral distortion (MCD) [Fukada et al., 1992] with the mel cepstrum dimension set to 25. We also evaluated the intelligibility using CERs computed with a multilingual ASR model [Radford et al., 2022]. We used a pretrained large model that is publicly available6. To evaluate the nat- uralness, we carried out listening tests to calculate five-scale mean opinion scores (MOS) of synthesized speech for each method. Forty native speakers were recruited through Ama- zon Mechanical Turk [Paolacci et al., 2010] for each of the tests. Furthermore, we leveraged a publicly available auto- matic MOS (AMOS) prediction model [Saeki et al., 2022a] to evaluate the naturalness. Note that the model was trained on English and Chinese datasets, but previous work [Seki et al., 2022] has reported that it also showed a correlation coef- ficient higher than 0.8 for another language (Japanese). 3.2 Evaluation Results on Seen Languages We evaluated our framework on the seen languages included in the paired data, as defined in \u00a7 2.3. Table 2 lists the results in MCD and CER. Lower values are better for both metrics. As we can see, the byte-based or IPA-based models with the proposed multilingual pretraining performed the best across all languages and metrics. Among the baselines, byte-based monolingual and multilingual models tended to have higher MCD and CER than IPA-based models, and failed to synthe- size intelligible speech in some languages. For example, the baseline byte-based models showed the high CER values for French, which has a deep orthography, meaning that a single character has different pronunciations depending on the con- text. We observed that our method improved the byte-based models and they outperformed the IPA-based baseline models for all the metrics and languages. It is worth noting that the proposed byte-based models even outperformed the proposed IPA-based models except for el and ru. These results suggest that our framework is effective in building a TTS model for languages without G2P modules. 3.3 Evaluation Results on Unseen Language We evaluated our method on zero-shot TTS for the unseen language defined in \u00a7 2.3. As described in \u00a7 2.4, we first used the x-vector from the es speaker to compute the MCD. Table 3 lists the results. The baseline models showed the CERs of over 40% and MCDs of over 10.0. However, our 6https://github.com/openai/whisper Method fi MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER de fr ru hu nl el Natural 2.75 4.52 2.12 4.73 4.86 6.22 7.14 Baseline (Monolingual) Bytes monolingual IPA monolingual 7.70 7.38 8.61 4.07 11.76 8.96 91.82 17.86 11.43 >100 25.30 11.89 8.33 7.23 56.03 27.62 10.22 7.59 93.05 24.62 7.49 7.80 15.33 19.20 10.20 8.16 85.98 21.79 Baseline (Multilingual) Bytes multilingual w/o LIDs Bytes multilingual w/ LIDs IPA multilingual w/o LIDs IPA multilingual w/ LIDs 7.68 6.51 6.31 6.16 37.46 13.19 10.64 9.76 8.71 10.84 7.44 6.88 41.35 55.79 20.86 14.97 9.38 45.92 12.89 >100 35.32 8.10 23.54 7.63 6.26 6.78 5.53 5.17 29.19 27.22 19.56 10.63 6.48 9.09 5.59 5.28 33.82 42.97 14.03 9.11 8.46 8.47 7.76 6.95 46.33 39.37 34.49 19.48 7.64 7.25 6.90 6.90 36.24 23.56 19.33 16.97 Proposed (Unsupervised text pretraining) Bytes multilingual IPA multilingual 5.65 5.88 3.79 5.52 6.48 6.61 7.15 7.72 7.38 7.25 10.62 15.85 4.99 5.18 5.28 8.62 5.01 5.30 6.05 7.37 6.52 7.00 13.74 14.42 6.57 6.53 11.75 11.06 Table 2: Evaluation results for seen languages. Bold indicates best scores in baseline and proposed methods. es Method es x-vector MCD CER Natural 2.71 Oracle Bytes monolingual IPA monolingual IPA multilingual 8.65 8.47 6.20 10.70 5.28 5.32 Baseline (Fully zero-shot TTS) Bytes multilingual IPA multilingual 11.22 10.75 64.07 44.75 fr x-vector CER 2.71 - 6.99 66.45 44.37 (a) Token embedding \ud835\udc4d(b) Encoder inputs \ud835\udc3b!\" Proposed (Text-seen zero-shot TTS) Bytes multilingual IPA multilingual 9.05 9.44 18.27 11.69 13.74 13.33 Figure 3: Visualization of token and language embedding. Pairs of similar languages (es\u2013fr and de\u2013nl) are overlapping in token embed- ding space, while output of bottleneck layer separates them. Table 3: Evaluation results for unseen language. proposed text preraining improved the metrics, resulting in CERs of less than half for both byte and IPA-based methods. Also, in contrast to the results for the seen languages, the IPA-based model outperformed the byte-based one in terms of CER. Compared with the oracle case with the paired data of the unseen language, our proposed zero-shot TTS showed higher MCD and CER but achieved only 1% difference in CER compared to the oracle byte-based monolingual model. These results demonstrate the effectiveness of our method in achieving intelligible zero-shot TTS for the unseen language. To investigate the case where the target speaker informa- tion is completely unavailable, we also used the x-vector from a seen language. We chose the fr speaker because es and fr are both categorized as Western Romance in Glottolog [Ham- marstr\u00a8om et al., 2021]. Table 3 lists the results. Note that this case does not have the MCD results, since a different speaker than the ground-truth speech was used. We can see that the unsupervised text pretraining also improved the zero- shot performance when using the x-vector from the fr speaker. In the proposed byte-based model, the cross-lingual x-vector showed the lower CER. This might result from that the es x-vector was not present in the training data whereas the fr x-vector was present in the training data. 3.4 Ablation Study To"}, {"question": " What publicly available model was leveraged to evaluate the naturalness of the synthesized speech?,answer: An automatic MOS (AMOS) prediction model", "ref_chunk": "a multilingual Transformer TTS model with the paired data of the unseen language. It also used the language IDs. Unseen language We compared Fully zero-shot TTS and Text-seen zero-shot TTS defined in \u00a7 2.3. In Oracle, we used the Monolingual and Multilingual w/ LIDs, which used the paired data of the unseen language. In Fully zero-shot TTS, we used Multilingual w/o LIDs to synthesize speech from text tokens in the unseen language. This method corresponds to the conventional multilingual TTS model using bytes [He et al., 2021] or IPA symbols [Staib et al., 2020]. Evaluation Metrics To objectively measure the synthetic speech quality, we used mel cepstral distortion (MCD) [Fukada et al., 1992] with the mel cepstrum dimension set to 25. We also evaluated the intelligibility using CERs computed with a multilingual ASR model [Radford et al., 2022]. We used a pretrained large model that is publicly available6. To evaluate the nat- uralness, we carried out listening tests to calculate five-scale mean opinion scores (MOS) of synthesized speech for each method. Forty native speakers were recruited through Ama- zon Mechanical Turk [Paolacci et al., 2010] for each of the tests. Furthermore, we leveraged a publicly available auto- matic MOS (AMOS) prediction model [Saeki et al., 2022a] to evaluate the naturalness. Note that the model was trained on English and Chinese datasets, but previous work [Seki et al., 2022] has reported that it also showed a correlation coef- ficient higher than 0.8 for another language (Japanese). 3.2 Evaluation Results on Seen Languages We evaluated our framework on the seen languages included in the paired data, as defined in \u00a7 2.3. Table 2 lists the results in MCD and CER. Lower values are better for both metrics. As we can see, the byte-based or IPA-based models with the proposed multilingual pretraining performed the best across all languages and metrics. Among the baselines, byte-based monolingual and multilingual models tended to have higher MCD and CER than IPA-based models, and failed to synthe- size intelligible speech in some languages. For example, the baseline byte-based models showed the high CER values for French, which has a deep orthography, meaning that a single character has different pronunciations depending on the con- text. We observed that our method improved the byte-based models and they outperformed the IPA-based baseline models for all the metrics and languages. It is worth noting that the proposed byte-based models even outperformed the proposed IPA-based models except for el and ru. These results suggest that our framework is effective in building a TTS model for languages without G2P modules. 3.3 Evaluation Results on Unseen Language We evaluated our method on zero-shot TTS for the unseen language defined in \u00a7 2.3. As described in \u00a7 2.4, we first used the x-vector from the es speaker to compute the MCD. Table 3 lists the results. The baseline models showed the CERs of over 40% and MCDs of over 10.0. However, our 6https://github.com/openai/whisper Method fi MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER de fr ru hu nl el Natural 2.75 4.52 2.12 4.73 4.86 6.22 7.14 Baseline (Monolingual) Bytes monolingual IPA monolingual 7.70 7.38 8.61 4.07 11.76 8.96 91.82 17.86 11.43 >100 25.30 11.89 8.33 7.23 56.03 27.62 10.22 7.59 93.05 24.62 7.49 7.80 15.33 19.20 10.20 8.16 85.98 21.79 Baseline (Multilingual) Bytes multilingual w/o LIDs Bytes multilingual w/ LIDs IPA multilingual w/o LIDs IPA multilingual w/ LIDs 7.68 6.51 6.31 6.16 37.46 13.19 10.64 9.76 8.71 10.84 7.44 6.88 41.35 55.79 20.86 14.97 9.38 45.92 12.89 >100 35.32 8.10 23.54 7.63 6.26 6.78 5.53 5.17 29.19 27.22 19.56 10.63 6.48 9.09 5.59 5.28 33.82 42.97 14.03 9.11 8.46 8.47 7.76 6.95 46.33 39.37 34.49 19.48 7.64 7.25 6.90 6.90 36.24 23.56 19.33 16.97 Proposed (Unsupervised text pretraining) Bytes multilingual IPA multilingual 5.65 5.88 3.79 5.52 6.48 6.61 7.15 7.72 7.38 7.25 10.62 15.85 4.99 5.18 5.28 8.62 5.01 5.30 6.05 7.37 6.52 7.00 13.74 14.42 6.57 6.53 11.75 11.06 Table 2: Evaluation results for seen languages. Bold indicates best scores in baseline and proposed methods. es Method es x-vector MCD CER Natural 2.71 Oracle Bytes monolingual IPA monolingual IPA multilingual 8.65 8.47 6.20 10.70 5.28 5.32 Baseline (Fully zero-shot TTS) Bytes multilingual IPA multilingual 11.22 10.75 64.07 44.75 fr x-vector CER 2.71 - 6.99 66.45 44.37 (a) Token embedding \ud835\udc4d(b) Encoder inputs \ud835\udc3b!\" Proposed (Text-seen zero-shot TTS) Bytes multilingual IPA multilingual 9.05 9.44 18.27 11.69 13.74 13.33 Figure 3: Visualization of token and language embedding. Pairs of similar languages (es\u2013fr and de\u2013nl) are overlapping in token embed- ding space, while output of bottleneck layer separates them. Table 3: Evaluation results for unseen language. proposed text preraining improved the metrics, resulting in CERs of less than half for both byte and IPA-based methods. Also, in contrast to the results for the seen languages, the IPA-based model outperformed the byte-based one in terms of CER. Compared with the oracle case with the paired data of the unseen language, our proposed zero-shot TTS showed higher MCD and CER but achieved only 1% difference in CER compared to the oracle byte-based monolingual model. These results demonstrate the effectiveness of our method in achieving intelligible zero-shot TTS for the unseen language. To investigate the case where the target speaker informa- tion is completely unavailable, we also used the x-vector from a seen language. We chose the fr speaker because es and fr are both categorized as Western Romance in Glottolog [Ham- marstr\u00a8om et al., 2021]. Table 3 lists the results. Note that this case does not have the MCD results, since a different speaker than the ground-truth speech was used. We can see that the unsupervised text pretraining also improved the zero- shot performance when using the x-vector from the fr speaker. In the proposed byte-based model, the cross-lingual x-vector showed the lower CER. This might result from that the es x-vector was not present in the training data whereas the fr x-vector was present in the training data. 3.4 Ablation Study To"}, {"question": " What correlation coefficient was reported for the model trained on English and Chinese datasets regarding another language (Japanese)?,answer: Higher than 0.8", "ref_chunk": "a multilingual Transformer TTS model with the paired data of the unseen language. It also used the language IDs. Unseen language We compared Fully zero-shot TTS and Text-seen zero-shot TTS defined in \u00a7 2.3. In Oracle, we used the Monolingual and Multilingual w/ LIDs, which used the paired data of the unseen language. In Fully zero-shot TTS, we used Multilingual w/o LIDs to synthesize speech from text tokens in the unseen language. This method corresponds to the conventional multilingual TTS model using bytes [He et al., 2021] or IPA symbols [Staib et al., 2020]. Evaluation Metrics To objectively measure the synthetic speech quality, we used mel cepstral distortion (MCD) [Fukada et al., 1992] with the mel cepstrum dimension set to 25. We also evaluated the intelligibility using CERs computed with a multilingual ASR model [Radford et al., 2022]. We used a pretrained large model that is publicly available6. To evaluate the nat- uralness, we carried out listening tests to calculate five-scale mean opinion scores (MOS) of synthesized speech for each method. Forty native speakers were recruited through Ama- zon Mechanical Turk [Paolacci et al., 2010] for each of the tests. Furthermore, we leveraged a publicly available auto- matic MOS (AMOS) prediction model [Saeki et al., 2022a] to evaluate the naturalness. Note that the model was trained on English and Chinese datasets, but previous work [Seki et al., 2022] has reported that it also showed a correlation coef- ficient higher than 0.8 for another language (Japanese). 3.2 Evaluation Results on Seen Languages We evaluated our framework on the seen languages included in the paired data, as defined in \u00a7 2.3. Table 2 lists the results in MCD and CER. Lower values are better for both metrics. As we can see, the byte-based or IPA-based models with the proposed multilingual pretraining performed the best across all languages and metrics. Among the baselines, byte-based monolingual and multilingual models tended to have higher MCD and CER than IPA-based models, and failed to synthe- size intelligible speech in some languages. For example, the baseline byte-based models showed the high CER values for French, which has a deep orthography, meaning that a single character has different pronunciations depending on the con- text. We observed that our method improved the byte-based models and they outperformed the IPA-based baseline models for all the metrics and languages. It is worth noting that the proposed byte-based models even outperformed the proposed IPA-based models except for el and ru. These results suggest that our framework is effective in building a TTS model for languages without G2P modules. 3.3 Evaluation Results on Unseen Language We evaluated our method on zero-shot TTS for the unseen language defined in \u00a7 2.3. As described in \u00a7 2.4, we first used the x-vector from the es speaker to compute the MCD. Table 3 lists the results. The baseline models showed the CERs of over 40% and MCDs of over 10.0. However, our 6https://github.com/openai/whisper Method fi MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER de fr ru hu nl el Natural 2.75 4.52 2.12 4.73 4.86 6.22 7.14 Baseline (Monolingual) Bytes monolingual IPA monolingual 7.70 7.38 8.61 4.07 11.76 8.96 91.82 17.86 11.43 >100 25.30 11.89 8.33 7.23 56.03 27.62 10.22 7.59 93.05 24.62 7.49 7.80 15.33 19.20 10.20 8.16 85.98 21.79 Baseline (Multilingual) Bytes multilingual w/o LIDs Bytes multilingual w/ LIDs IPA multilingual w/o LIDs IPA multilingual w/ LIDs 7.68 6.51 6.31 6.16 37.46 13.19 10.64 9.76 8.71 10.84 7.44 6.88 41.35 55.79 20.86 14.97 9.38 45.92 12.89 >100 35.32 8.10 23.54 7.63 6.26 6.78 5.53 5.17 29.19 27.22 19.56 10.63 6.48 9.09 5.59 5.28 33.82 42.97 14.03 9.11 8.46 8.47 7.76 6.95 46.33 39.37 34.49 19.48 7.64 7.25 6.90 6.90 36.24 23.56 19.33 16.97 Proposed (Unsupervised text pretraining) Bytes multilingual IPA multilingual 5.65 5.88 3.79 5.52 6.48 6.61 7.15 7.72 7.38 7.25 10.62 15.85 4.99 5.18 5.28 8.62 5.01 5.30 6.05 7.37 6.52 7.00 13.74 14.42 6.57 6.53 11.75 11.06 Table 2: Evaluation results for seen languages. Bold indicates best scores in baseline and proposed methods. es Method es x-vector MCD CER Natural 2.71 Oracle Bytes monolingual IPA monolingual IPA multilingual 8.65 8.47 6.20 10.70 5.28 5.32 Baseline (Fully zero-shot TTS) Bytes multilingual IPA multilingual 11.22 10.75 64.07 44.75 fr x-vector CER 2.71 - 6.99 66.45 44.37 (a) Token embedding \ud835\udc4d(b) Encoder inputs \ud835\udc3b!\" Proposed (Text-seen zero-shot TTS) Bytes multilingual IPA multilingual 9.05 9.44 18.27 11.69 13.74 13.33 Figure 3: Visualization of token and language embedding. Pairs of similar languages (es\u2013fr and de\u2013nl) are overlapping in token embed- ding space, while output of bottleneck layer separates them. Table 3: Evaluation results for unseen language. proposed text preraining improved the metrics, resulting in CERs of less than half for both byte and IPA-based methods. Also, in contrast to the results for the seen languages, the IPA-based model outperformed the byte-based one in terms of CER. Compared with the oracle case with the paired data of the unseen language, our proposed zero-shot TTS showed higher MCD and CER but achieved only 1% difference in CER compared to the oracle byte-based monolingual model. These results demonstrate the effectiveness of our method in achieving intelligible zero-shot TTS for the unseen language. To investigate the case where the target speaker informa- tion is completely unavailable, we also used the x-vector from a seen language. We chose the fr speaker because es and fr are both categorized as Western Romance in Glottolog [Ham- marstr\u00a8om et al., 2021]. Table 3 lists the results. Note that this case does not have the MCD results, since a different speaker than the ground-truth speech was used. We can see that the unsupervised text pretraining also improved the zero- shot performance when using the x-vector from the fr speaker. In the proposed byte-based model, the cross-lingual x-vector showed the lower CER. This might result from that the es x-vector was not present in the training data whereas the fr x-vector was present in the training data. 3.4 Ablation Study To"}, {"question": " Which models performed the best across all languages and metrics according to the evaluation results on seen languages?,answer: Byte-based or IPA-based models with the proposed multilingual pretraining", "ref_chunk": "a multilingual Transformer TTS model with the paired data of the unseen language. It also used the language IDs. Unseen language We compared Fully zero-shot TTS and Text-seen zero-shot TTS defined in \u00a7 2.3. In Oracle, we used the Monolingual and Multilingual w/ LIDs, which used the paired data of the unseen language. In Fully zero-shot TTS, we used Multilingual w/o LIDs to synthesize speech from text tokens in the unseen language. This method corresponds to the conventional multilingual TTS model using bytes [He et al., 2021] or IPA symbols [Staib et al., 2020]. Evaluation Metrics To objectively measure the synthetic speech quality, we used mel cepstral distortion (MCD) [Fukada et al., 1992] with the mel cepstrum dimension set to 25. We also evaluated the intelligibility using CERs computed with a multilingual ASR model [Radford et al., 2022]. We used a pretrained large model that is publicly available6. To evaluate the nat- uralness, we carried out listening tests to calculate five-scale mean opinion scores (MOS) of synthesized speech for each method. Forty native speakers were recruited through Ama- zon Mechanical Turk [Paolacci et al., 2010] for each of the tests. Furthermore, we leveraged a publicly available auto- matic MOS (AMOS) prediction model [Saeki et al., 2022a] to evaluate the naturalness. Note that the model was trained on English and Chinese datasets, but previous work [Seki et al., 2022] has reported that it also showed a correlation coef- ficient higher than 0.8 for another language (Japanese). 3.2 Evaluation Results on Seen Languages We evaluated our framework on the seen languages included in the paired data, as defined in \u00a7 2.3. Table 2 lists the results in MCD and CER. Lower values are better for both metrics. As we can see, the byte-based or IPA-based models with the proposed multilingual pretraining performed the best across all languages and metrics. Among the baselines, byte-based monolingual and multilingual models tended to have higher MCD and CER than IPA-based models, and failed to synthe- size intelligible speech in some languages. For example, the baseline byte-based models showed the high CER values for French, which has a deep orthography, meaning that a single character has different pronunciations depending on the con- text. We observed that our method improved the byte-based models and they outperformed the IPA-based baseline models for all the metrics and languages. It is worth noting that the proposed byte-based models even outperformed the proposed IPA-based models except for el and ru. These results suggest that our framework is effective in building a TTS model for languages without G2P modules. 3.3 Evaluation Results on Unseen Language We evaluated our method on zero-shot TTS for the unseen language defined in \u00a7 2.3. As described in \u00a7 2.4, we first used the x-vector from the es speaker to compute the MCD. Table 3 lists the results. The baseline models showed the CERs of over 40% and MCDs of over 10.0. However, our 6https://github.com/openai/whisper Method fi MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER de fr ru hu nl el Natural 2.75 4.52 2.12 4.73 4.86 6.22 7.14 Baseline (Monolingual) Bytes monolingual IPA monolingual 7.70 7.38 8.61 4.07 11.76 8.96 91.82 17.86 11.43 >100 25.30 11.89 8.33 7.23 56.03 27.62 10.22 7.59 93.05 24.62 7.49 7.80 15.33 19.20 10.20 8.16 85.98 21.79 Baseline (Multilingual) Bytes multilingual w/o LIDs Bytes multilingual w/ LIDs IPA multilingual w/o LIDs IPA multilingual w/ LIDs 7.68 6.51 6.31 6.16 37.46 13.19 10.64 9.76 8.71 10.84 7.44 6.88 41.35 55.79 20.86 14.97 9.38 45.92 12.89 >100 35.32 8.10 23.54 7.63 6.26 6.78 5.53 5.17 29.19 27.22 19.56 10.63 6.48 9.09 5.59 5.28 33.82 42.97 14.03 9.11 8.46 8.47 7.76 6.95 46.33 39.37 34.49 19.48 7.64 7.25 6.90 6.90 36.24 23.56 19.33 16.97 Proposed (Unsupervised text pretraining) Bytes multilingual IPA multilingual 5.65 5.88 3.79 5.52 6.48 6.61 7.15 7.72 7.38 7.25 10.62 15.85 4.99 5.18 5.28 8.62 5.01 5.30 6.05 7.37 6.52 7.00 13.74 14.42 6.57 6.53 11.75 11.06 Table 2: Evaluation results for seen languages. Bold indicates best scores in baseline and proposed methods. es Method es x-vector MCD CER Natural 2.71 Oracle Bytes monolingual IPA monolingual IPA multilingual 8.65 8.47 6.20 10.70 5.28 5.32 Baseline (Fully zero-shot TTS) Bytes multilingual IPA multilingual 11.22 10.75 64.07 44.75 fr x-vector CER 2.71 - 6.99 66.45 44.37 (a) Token embedding \ud835\udc4d(b) Encoder inputs \ud835\udc3b!\" Proposed (Text-seen zero-shot TTS) Bytes multilingual IPA multilingual 9.05 9.44 18.27 11.69 13.74 13.33 Figure 3: Visualization of token and language embedding. Pairs of similar languages (es\u2013fr and de\u2013nl) are overlapping in token embed- ding space, while output of bottleneck layer separates them. Table 3: Evaluation results for unseen language. proposed text preraining improved the metrics, resulting in CERs of less than half for both byte and IPA-based methods. Also, in contrast to the results for the seen languages, the IPA-based model outperformed the byte-based one in terms of CER. Compared with the oracle case with the paired data of the unseen language, our proposed zero-shot TTS showed higher MCD and CER but achieved only 1% difference in CER compared to the oracle byte-based monolingual model. These results demonstrate the effectiveness of our method in achieving intelligible zero-shot TTS for the unseen language. To investigate the case where the target speaker informa- tion is completely unavailable, we also used the x-vector from a seen language. We chose the fr speaker because es and fr are both categorized as Western Romance in Glottolog [Ham- marstr\u00a8om et al., 2021]. Table 3 lists the results. Note that this case does not have the MCD results, since a different speaker than the ground-truth speech was used. We can see that the unsupervised text pretraining also improved the zero- shot performance when using the x-vector from the fr speaker. In the proposed byte-based model, the cross-lingual x-vector showed the lower CER. This might result from that the es x-vector was not present in the training data whereas the fr x-vector was present in the training data. 3.4 Ablation Study To"}, {"question": " In terms of intelligibility, what issue did the byte-based models face in some languages like French?,answer: High CER values due to deep orthography (single character having different pronunciations)", "ref_chunk": "a multilingual Transformer TTS model with the paired data of the unseen language. It also used the language IDs. Unseen language We compared Fully zero-shot TTS and Text-seen zero-shot TTS defined in \u00a7 2.3. In Oracle, we used the Monolingual and Multilingual w/ LIDs, which used the paired data of the unseen language. In Fully zero-shot TTS, we used Multilingual w/o LIDs to synthesize speech from text tokens in the unseen language. This method corresponds to the conventional multilingual TTS model using bytes [He et al., 2021] or IPA symbols [Staib et al., 2020]. Evaluation Metrics To objectively measure the synthetic speech quality, we used mel cepstral distortion (MCD) [Fukada et al., 1992] with the mel cepstrum dimension set to 25. We also evaluated the intelligibility using CERs computed with a multilingual ASR model [Radford et al., 2022]. We used a pretrained large model that is publicly available6. To evaluate the nat- uralness, we carried out listening tests to calculate five-scale mean opinion scores (MOS) of synthesized speech for each method. Forty native speakers were recruited through Ama- zon Mechanical Turk [Paolacci et al., 2010] for each of the tests. Furthermore, we leveraged a publicly available auto- matic MOS (AMOS) prediction model [Saeki et al., 2022a] to evaluate the naturalness. Note that the model was trained on English and Chinese datasets, but previous work [Seki et al., 2022] has reported that it also showed a correlation coef- ficient higher than 0.8 for another language (Japanese). 3.2 Evaluation Results on Seen Languages We evaluated our framework on the seen languages included in the paired data, as defined in \u00a7 2.3. Table 2 lists the results in MCD and CER. Lower values are better for both metrics. As we can see, the byte-based or IPA-based models with the proposed multilingual pretraining performed the best across all languages and metrics. Among the baselines, byte-based monolingual and multilingual models tended to have higher MCD and CER than IPA-based models, and failed to synthe- size intelligible speech in some languages. For example, the baseline byte-based models showed the high CER values for French, which has a deep orthography, meaning that a single character has different pronunciations depending on the con- text. We observed that our method improved the byte-based models and they outperformed the IPA-based baseline models for all the metrics and languages. It is worth noting that the proposed byte-based models even outperformed the proposed IPA-based models except for el and ru. These results suggest that our framework is effective in building a TTS model for languages without G2P modules. 3.3 Evaluation Results on Unseen Language We evaluated our method on zero-shot TTS for the unseen language defined in \u00a7 2.3. As described in \u00a7 2.4, we first used the x-vector from the es speaker to compute the MCD. Table 3 lists the results. The baseline models showed the CERs of over 40% and MCDs of over 10.0. However, our 6https://github.com/openai/whisper Method fi MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER de fr ru hu nl el Natural 2.75 4.52 2.12 4.73 4.86 6.22 7.14 Baseline (Monolingual) Bytes monolingual IPA monolingual 7.70 7.38 8.61 4.07 11.76 8.96 91.82 17.86 11.43 >100 25.30 11.89 8.33 7.23 56.03 27.62 10.22 7.59 93.05 24.62 7.49 7.80 15.33 19.20 10.20 8.16 85.98 21.79 Baseline (Multilingual) Bytes multilingual w/o LIDs Bytes multilingual w/ LIDs IPA multilingual w/o LIDs IPA multilingual w/ LIDs 7.68 6.51 6.31 6.16 37.46 13.19 10.64 9.76 8.71 10.84 7.44 6.88 41.35 55.79 20.86 14.97 9.38 45.92 12.89 >100 35.32 8.10 23.54 7.63 6.26 6.78 5.53 5.17 29.19 27.22 19.56 10.63 6.48 9.09 5.59 5.28 33.82 42.97 14.03 9.11 8.46 8.47 7.76 6.95 46.33 39.37 34.49 19.48 7.64 7.25 6.90 6.90 36.24 23.56 19.33 16.97 Proposed (Unsupervised text pretraining) Bytes multilingual IPA multilingual 5.65 5.88 3.79 5.52 6.48 6.61 7.15 7.72 7.38 7.25 10.62 15.85 4.99 5.18 5.28 8.62 5.01 5.30 6.05 7.37 6.52 7.00 13.74 14.42 6.57 6.53 11.75 11.06 Table 2: Evaluation results for seen languages. Bold indicates best scores in baseline and proposed methods. es Method es x-vector MCD CER Natural 2.71 Oracle Bytes monolingual IPA monolingual IPA multilingual 8.65 8.47 6.20 10.70 5.28 5.32 Baseline (Fully zero-shot TTS) Bytes multilingual IPA multilingual 11.22 10.75 64.07 44.75 fr x-vector CER 2.71 - 6.99 66.45 44.37 (a) Token embedding \ud835\udc4d(b) Encoder inputs \ud835\udc3b!\" Proposed (Text-seen zero-shot TTS) Bytes multilingual IPA multilingual 9.05 9.44 18.27 11.69 13.74 13.33 Figure 3: Visualization of token and language embedding. Pairs of similar languages (es\u2013fr and de\u2013nl) are overlapping in token embed- ding space, while output of bottleneck layer separates them. Table 3: Evaluation results for unseen language. proposed text preraining improved the metrics, resulting in CERs of less than half for both byte and IPA-based methods. Also, in contrast to the results for the seen languages, the IPA-based model outperformed the byte-based one in terms of CER. Compared with the oracle case with the paired data of the unseen language, our proposed zero-shot TTS showed higher MCD and CER but achieved only 1% difference in CER compared to the oracle byte-based monolingual model. These results demonstrate the effectiveness of our method in achieving intelligible zero-shot TTS for the unseen language. To investigate the case where the target speaker informa- tion is completely unavailable, we also used the x-vector from a seen language. We chose the fr speaker because es and fr are both categorized as Western Romance in Glottolog [Ham- marstr\u00a8om et al., 2021]. Table 3 lists the results. Note that this case does not have the MCD results, since a different speaker than the ground-truth speech was used. We can see that the unsupervised text pretraining also improved the zero- shot performance when using the x-vector from the fr speaker. In the proposed byte-based model, the cross-lingual x-vector showed the lower CER. This might result from that the es x-vector was not present in the training data whereas the fr x-vector was present in the training data. 3.4 Ablation Study To"}, {"question": " In the evaluation results for unseen language, which model outperformed the byte-based one in terms of CER?,answer: IPA-based model", "ref_chunk": "a multilingual Transformer TTS model with the paired data of the unseen language. It also used the language IDs. Unseen language We compared Fully zero-shot TTS and Text-seen zero-shot TTS defined in \u00a7 2.3. In Oracle, we used the Monolingual and Multilingual w/ LIDs, which used the paired data of the unseen language. In Fully zero-shot TTS, we used Multilingual w/o LIDs to synthesize speech from text tokens in the unseen language. This method corresponds to the conventional multilingual TTS model using bytes [He et al., 2021] or IPA symbols [Staib et al., 2020]. Evaluation Metrics To objectively measure the synthetic speech quality, we used mel cepstral distortion (MCD) [Fukada et al., 1992] with the mel cepstrum dimension set to 25. We also evaluated the intelligibility using CERs computed with a multilingual ASR model [Radford et al., 2022]. We used a pretrained large model that is publicly available6. To evaluate the nat- uralness, we carried out listening tests to calculate five-scale mean opinion scores (MOS) of synthesized speech for each method. Forty native speakers were recruited through Ama- zon Mechanical Turk [Paolacci et al., 2010] for each of the tests. Furthermore, we leveraged a publicly available auto- matic MOS (AMOS) prediction model [Saeki et al., 2022a] to evaluate the naturalness. Note that the model was trained on English and Chinese datasets, but previous work [Seki et al., 2022] has reported that it also showed a correlation coef- ficient higher than 0.8 for another language (Japanese). 3.2 Evaluation Results on Seen Languages We evaluated our framework on the seen languages included in the paired data, as defined in \u00a7 2.3. Table 2 lists the results in MCD and CER. Lower values are better for both metrics. As we can see, the byte-based or IPA-based models with the proposed multilingual pretraining performed the best across all languages and metrics. Among the baselines, byte-based monolingual and multilingual models tended to have higher MCD and CER than IPA-based models, and failed to synthe- size intelligible speech in some languages. For example, the baseline byte-based models showed the high CER values for French, which has a deep orthography, meaning that a single character has different pronunciations depending on the con- text. We observed that our method improved the byte-based models and they outperformed the IPA-based baseline models for all the metrics and languages. It is worth noting that the proposed byte-based models even outperformed the proposed IPA-based models except for el and ru. These results suggest that our framework is effective in building a TTS model for languages without G2P modules. 3.3 Evaluation Results on Unseen Language We evaluated our method on zero-shot TTS for the unseen language defined in \u00a7 2.3. As described in \u00a7 2.4, we first used the x-vector from the es speaker to compute the MCD. Table 3 lists the results. The baseline models showed the CERs of over 40% and MCDs of over 10.0. However, our 6https://github.com/openai/whisper Method fi MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER de fr ru hu nl el Natural 2.75 4.52 2.12 4.73 4.86 6.22 7.14 Baseline (Monolingual) Bytes monolingual IPA monolingual 7.70 7.38 8.61 4.07 11.76 8.96 91.82 17.86 11.43 >100 25.30 11.89 8.33 7.23 56.03 27.62 10.22 7.59 93.05 24.62 7.49 7.80 15.33 19.20 10.20 8.16 85.98 21.79 Baseline (Multilingual) Bytes multilingual w/o LIDs Bytes multilingual w/ LIDs IPA multilingual w/o LIDs IPA multilingual w/ LIDs 7.68 6.51 6.31 6.16 37.46 13.19 10.64 9.76 8.71 10.84 7.44 6.88 41.35 55.79 20.86 14.97 9.38 45.92 12.89 >100 35.32 8.10 23.54 7.63 6.26 6.78 5.53 5.17 29.19 27.22 19.56 10.63 6.48 9.09 5.59 5.28 33.82 42.97 14.03 9.11 8.46 8.47 7.76 6.95 46.33 39.37 34.49 19.48 7.64 7.25 6.90 6.90 36.24 23.56 19.33 16.97 Proposed (Unsupervised text pretraining) Bytes multilingual IPA multilingual 5.65 5.88 3.79 5.52 6.48 6.61 7.15 7.72 7.38 7.25 10.62 15.85 4.99 5.18 5.28 8.62 5.01 5.30 6.05 7.37 6.52 7.00 13.74 14.42 6.57 6.53 11.75 11.06 Table 2: Evaluation results for seen languages. Bold indicates best scores in baseline and proposed methods. es Method es x-vector MCD CER Natural 2.71 Oracle Bytes monolingual IPA monolingual IPA multilingual 8.65 8.47 6.20 10.70 5.28 5.32 Baseline (Fully zero-shot TTS) Bytes multilingual IPA multilingual 11.22 10.75 64.07 44.75 fr x-vector CER 2.71 - 6.99 66.45 44.37 (a) Token embedding \ud835\udc4d(b) Encoder inputs \ud835\udc3b!\" Proposed (Text-seen zero-shot TTS) Bytes multilingual IPA multilingual 9.05 9.44 18.27 11.69 13.74 13.33 Figure 3: Visualization of token and language embedding. Pairs of similar languages (es\u2013fr and de\u2013nl) are overlapping in token embed- ding space, while output of bottleneck layer separates them. Table 3: Evaluation results for unseen language. proposed text preraining improved the metrics, resulting in CERs of less than half for both byte and IPA-based methods. Also, in contrast to the results for the seen languages, the IPA-based model outperformed the byte-based one in terms of CER. Compared with the oracle case with the paired data of the unseen language, our proposed zero-shot TTS showed higher MCD and CER but achieved only 1% difference in CER compared to the oracle byte-based monolingual model. These results demonstrate the effectiveness of our method in achieving intelligible zero-shot TTS for the unseen language. To investigate the case where the target speaker informa- tion is completely unavailable, we also used the x-vector from a seen language. We chose the fr speaker because es and fr are both categorized as Western Romance in Glottolog [Ham- marstr\u00a8om et al., 2021]. Table 3 lists the results. Note that this case does not have the MCD results, since a different speaker than the ground-truth speech was used. We can see that the unsupervised text pretraining also improved the zero- shot performance when using the x-vector from the fr speaker. In the proposed byte-based model, the cross-lingual x-vector showed the lower CER. This might result from that the es x-vector was not present in the training data whereas the fr x-vector was present in the training data. 3.4 Ablation Study To"}, {"question": " How did the proposed zero-shot TTS compare to the oracle case with paired data of the unseen language?,answer: Higher MCD and CER values, but achieved only 1% difference in CER", "ref_chunk": "a multilingual Transformer TTS model with the paired data of the unseen language. It also used the language IDs. Unseen language We compared Fully zero-shot TTS and Text-seen zero-shot TTS defined in \u00a7 2.3. In Oracle, we used the Monolingual and Multilingual w/ LIDs, which used the paired data of the unseen language. In Fully zero-shot TTS, we used Multilingual w/o LIDs to synthesize speech from text tokens in the unseen language. This method corresponds to the conventional multilingual TTS model using bytes [He et al., 2021] or IPA symbols [Staib et al., 2020]. Evaluation Metrics To objectively measure the synthetic speech quality, we used mel cepstral distortion (MCD) [Fukada et al., 1992] with the mel cepstrum dimension set to 25. We also evaluated the intelligibility using CERs computed with a multilingual ASR model [Radford et al., 2022]. We used a pretrained large model that is publicly available6. To evaluate the nat- uralness, we carried out listening tests to calculate five-scale mean opinion scores (MOS) of synthesized speech for each method. Forty native speakers were recruited through Ama- zon Mechanical Turk [Paolacci et al., 2010] for each of the tests. Furthermore, we leveraged a publicly available auto- matic MOS (AMOS) prediction model [Saeki et al., 2022a] to evaluate the naturalness. Note that the model was trained on English and Chinese datasets, but previous work [Seki et al., 2022] has reported that it also showed a correlation coef- ficient higher than 0.8 for another language (Japanese). 3.2 Evaluation Results on Seen Languages We evaluated our framework on the seen languages included in the paired data, as defined in \u00a7 2.3. Table 2 lists the results in MCD and CER. Lower values are better for both metrics. As we can see, the byte-based or IPA-based models with the proposed multilingual pretraining performed the best across all languages and metrics. Among the baselines, byte-based monolingual and multilingual models tended to have higher MCD and CER than IPA-based models, and failed to synthe- size intelligible speech in some languages. For example, the baseline byte-based models showed the high CER values for French, which has a deep orthography, meaning that a single character has different pronunciations depending on the con- text. We observed that our method improved the byte-based models and they outperformed the IPA-based baseline models for all the metrics and languages. It is worth noting that the proposed byte-based models even outperformed the proposed IPA-based models except for el and ru. These results suggest that our framework is effective in building a TTS model for languages without G2P modules. 3.3 Evaluation Results on Unseen Language We evaluated our method on zero-shot TTS for the unseen language defined in \u00a7 2.3. As described in \u00a7 2.4, we first used the x-vector from the es speaker to compute the MCD. Table 3 lists the results. The baseline models showed the CERs of over 40% and MCDs of over 10.0. However, our 6https://github.com/openai/whisper Method fi MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER de fr ru hu nl el Natural 2.75 4.52 2.12 4.73 4.86 6.22 7.14 Baseline (Monolingual) Bytes monolingual IPA monolingual 7.70 7.38 8.61 4.07 11.76 8.96 91.82 17.86 11.43 >100 25.30 11.89 8.33 7.23 56.03 27.62 10.22 7.59 93.05 24.62 7.49 7.80 15.33 19.20 10.20 8.16 85.98 21.79 Baseline (Multilingual) Bytes multilingual w/o LIDs Bytes multilingual w/ LIDs IPA multilingual w/o LIDs IPA multilingual w/ LIDs 7.68 6.51 6.31 6.16 37.46 13.19 10.64 9.76 8.71 10.84 7.44 6.88 41.35 55.79 20.86 14.97 9.38 45.92 12.89 >100 35.32 8.10 23.54 7.63 6.26 6.78 5.53 5.17 29.19 27.22 19.56 10.63 6.48 9.09 5.59 5.28 33.82 42.97 14.03 9.11 8.46 8.47 7.76 6.95 46.33 39.37 34.49 19.48 7.64 7.25 6.90 6.90 36.24 23.56 19.33 16.97 Proposed (Unsupervised text pretraining) Bytes multilingual IPA multilingual 5.65 5.88 3.79 5.52 6.48 6.61 7.15 7.72 7.38 7.25 10.62 15.85 4.99 5.18 5.28 8.62 5.01 5.30 6.05 7.37 6.52 7.00 13.74 14.42 6.57 6.53 11.75 11.06 Table 2: Evaluation results for seen languages. Bold indicates best scores in baseline and proposed methods. es Method es x-vector MCD CER Natural 2.71 Oracle Bytes monolingual IPA monolingual IPA multilingual 8.65 8.47 6.20 10.70 5.28 5.32 Baseline (Fully zero-shot TTS) Bytes multilingual IPA multilingual 11.22 10.75 64.07 44.75 fr x-vector CER 2.71 - 6.99 66.45 44.37 (a) Token embedding \ud835\udc4d(b) Encoder inputs \ud835\udc3b!\" Proposed (Text-seen zero-shot TTS) Bytes multilingual IPA multilingual 9.05 9.44 18.27 11.69 13.74 13.33 Figure 3: Visualization of token and language embedding. Pairs of similar languages (es\u2013fr and de\u2013nl) are overlapping in token embed- ding space, while output of bottleneck layer separates them. Table 3: Evaluation results for unseen language. proposed text preraining improved the metrics, resulting in CERs of less than half for both byte and IPA-based methods. Also, in contrast to the results for the seen languages, the IPA-based model outperformed the byte-based one in terms of CER. Compared with the oracle case with the paired data of the unseen language, our proposed zero-shot TTS showed higher MCD and CER but achieved only 1% difference in CER compared to the oracle byte-based monolingual model. These results demonstrate the effectiveness of our method in achieving intelligible zero-shot TTS for the unseen language. To investigate the case where the target speaker informa- tion is completely unavailable, we also used the x-vector from a seen language. We chose the fr speaker because es and fr are both categorized as Western Romance in Glottolog [Ham- marstr\u00a8om et al., 2021]. Table 3 lists the results. Note that this case does not have the MCD results, since a different speaker than the ground-truth speech was used. We can see that the unsupervised text pretraining also improved the zero- shot performance when using the x-vector from the fr speaker. In the proposed byte-based model, the cross-lingual x-vector showed the lower CER. This might result from that the es x-vector was not present in the training data whereas the fr x-vector was present in the training data. 3.4 Ablation Study To"}, {"question": " What case was investigated to study the effectiveness of achieving intelligible zero-shot TTS for the unseen language?,answer: Where the target speaker information is completely unavailable", "ref_chunk": "a multilingual Transformer TTS model with the paired data of the unseen language. It also used the language IDs. Unseen language We compared Fully zero-shot TTS and Text-seen zero-shot TTS defined in \u00a7 2.3. In Oracle, we used the Monolingual and Multilingual w/ LIDs, which used the paired data of the unseen language. In Fully zero-shot TTS, we used Multilingual w/o LIDs to synthesize speech from text tokens in the unseen language. This method corresponds to the conventional multilingual TTS model using bytes [He et al., 2021] or IPA symbols [Staib et al., 2020]. Evaluation Metrics To objectively measure the synthetic speech quality, we used mel cepstral distortion (MCD) [Fukada et al., 1992] with the mel cepstrum dimension set to 25. We also evaluated the intelligibility using CERs computed with a multilingual ASR model [Radford et al., 2022]. We used a pretrained large model that is publicly available6. To evaluate the nat- uralness, we carried out listening tests to calculate five-scale mean opinion scores (MOS) of synthesized speech for each method. Forty native speakers were recruited through Ama- zon Mechanical Turk [Paolacci et al., 2010] for each of the tests. Furthermore, we leveraged a publicly available auto- matic MOS (AMOS) prediction model [Saeki et al., 2022a] to evaluate the naturalness. Note that the model was trained on English and Chinese datasets, but previous work [Seki et al., 2022] has reported that it also showed a correlation coef- ficient higher than 0.8 for another language (Japanese). 3.2 Evaluation Results on Seen Languages We evaluated our framework on the seen languages included in the paired data, as defined in \u00a7 2.3. Table 2 lists the results in MCD and CER. Lower values are better for both metrics. As we can see, the byte-based or IPA-based models with the proposed multilingual pretraining performed the best across all languages and metrics. Among the baselines, byte-based monolingual and multilingual models tended to have higher MCD and CER than IPA-based models, and failed to synthe- size intelligible speech in some languages. For example, the baseline byte-based models showed the high CER values for French, which has a deep orthography, meaning that a single character has different pronunciations depending on the con- text. We observed that our method improved the byte-based models and they outperformed the IPA-based baseline models for all the metrics and languages. It is worth noting that the proposed byte-based models even outperformed the proposed IPA-based models except for el and ru. These results suggest that our framework is effective in building a TTS model for languages without G2P modules. 3.3 Evaluation Results on Unseen Language We evaluated our method on zero-shot TTS for the unseen language defined in \u00a7 2.3. As described in \u00a7 2.4, we first used the x-vector from the es speaker to compute the MCD. Table 3 lists the results. The baseline models showed the CERs of over 40% and MCDs of over 10.0. However, our 6https://github.com/openai/whisper Method fi MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER de fr ru hu nl el Natural 2.75 4.52 2.12 4.73 4.86 6.22 7.14 Baseline (Monolingual) Bytes monolingual IPA monolingual 7.70 7.38 8.61 4.07 11.76 8.96 91.82 17.86 11.43 >100 25.30 11.89 8.33 7.23 56.03 27.62 10.22 7.59 93.05 24.62 7.49 7.80 15.33 19.20 10.20 8.16 85.98 21.79 Baseline (Multilingual) Bytes multilingual w/o LIDs Bytes multilingual w/ LIDs IPA multilingual w/o LIDs IPA multilingual w/ LIDs 7.68 6.51 6.31 6.16 37.46 13.19 10.64 9.76 8.71 10.84 7.44 6.88 41.35 55.79 20.86 14.97 9.38 45.92 12.89 >100 35.32 8.10 23.54 7.63 6.26 6.78 5.53 5.17 29.19 27.22 19.56 10.63 6.48 9.09 5.59 5.28 33.82 42.97 14.03 9.11 8.46 8.47 7.76 6.95 46.33 39.37 34.49 19.48 7.64 7.25 6.90 6.90 36.24 23.56 19.33 16.97 Proposed (Unsupervised text pretraining) Bytes multilingual IPA multilingual 5.65 5.88 3.79 5.52 6.48 6.61 7.15 7.72 7.38 7.25 10.62 15.85 4.99 5.18 5.28 8.62 5.01 5.30 6.05 7.37 6.52 7.00 13.74 14.42 6.57 6.53 11.75 11.06 Table 2: Evaluation results for seen languages. Bold indicates best scores in baseline and proposed methods. es Method es x-vector MCD CER Natural 2.71 Oracle Bytes monolingual IPA monolingual IPA multilingual 8.65 8.47 6.20 10.70 5.28 5.32 Baseline (Fully zero-shot TTS) Bytes multilingual IPA multilingual 11.22 10.75 64.07 44.75 fr x-vector CER 2.71 - 6.99 66.45 44.37 (a) Token embedding \ud835\udc4d(b) Encoder inputs \ud835\udc3b!\" Proposed (Text-seen zero-shot TTS) Bytes multilingual IPA multilingual 9.05 9.44 18.27 11.69 13.74 13.33 Figure 3: Visualization of token and language embedding. Pairs of similar languages (es\u2013fr and de\u2013nl) are overlapping in token embed- ding space, while output of bottleneck layer separates them. Table 3: Evaluation results for unseen language. proposed text preraining improved the metrics, resulting in CERs of less than half for both byte and IPA-based methods. Also, in contrast to the results for the seen languages, the IPA-based model outperformed the byte-based one in terms of CER. Compared with the oracle case with the paired data of the unseen language, our proposed zero-shot TTS showed higher MCD and CER but achieved only 1% difference in CER compared to the oracle byte-based monolingual model. These results demonstrate the effectiveness of our method in achieving intelligible zero-shot TTS for the unseen language. To investigate the case where the target speaker informa- tion is completely unavailable, we also used the x-vector from a seen language. We chose the fr speaker because es and fr are both categorized as Western Romance in Glottolog [Ham- marstr\u00a8om et al., 2021]. Table 3 lists the results. Note that this case does not have the MCD results, since a different speaker than the ground-truth speech was used. We can see that the unsupervised text pretraining also improved the zero- shot performance when using the x-vector from the fr speaker. In the proposed byte-based model, the cross-lingual x-vector showed the lower CER. This might result from that the es x-vector was not present in the training data whereas the fr x-vector was present in the training data. 3.4 Ablation Study To"}], "doc_text": "a multilingual Transformer TTS model with the paired data of the unseen language. It also used the language IDs. Unseen language We compared Fully zero-shot TTS and Text-seen zero-shot TTS defined in \u00a7 2.3. In Oracle, we used the Monolingual and Multilingual w/ LIDs, which used the paired data of the unseen language. In Fully zero-shot TTS, we used Multilingual w/o LIDs to synthesize speech from text tokens in the unseen language. This method corresponds to the conventional multilingual TTS model using bytes [He et al., 2021] or IPA symbols [Staib et al., 2020]. Evaluation Metrics To objectively measure the synthetic speech quality, we used mel cepstral distortion (MCD) [Fukada et al., 1992] with the mel cepstrum dimension set to 25. We also evaluated the intelligibility using CERs computed with a multilingual ASR model [Radford et al., 2022]. We used a pretrained large model that is publicly available6. To evaluate the nat- uralness, we carried out listening tests to calculate five-scale mean opinion scores (MOS) of synthesized speech for each method. Forty native speakers were recruited through Ama- zon Mechanical Turk [Paolacci et al., 2010] for each of the tests. Furthermore, we leveraged a publicly available auto- matic MOS (AMOS) prediction model [Saeki et al., 2022a] to evaluate the naturalness. Note that the model was trained on English and Chinese datasets, but previous work [Seki et al., 2022] has reported that it also showed a correlation coef- ficient higher than 0.8 for another language (Japanese). 3.2 Evaluation Results on Seen Languages We evaluated our framework on the seen languages included in the paired data, as defined in \u00a7 2.3. Table 2 lists the results in MCD and CER. Lower values are better for both metrics. As we can see, the byte-based or IPA-based models with the proposed multilingual pretraining performed the best across all languages and metrics. Among the baselines, byte-based monolingual and multilingual models tended to have higher MCD and CER than IPA-based models, and failed to synthe- size intelligible speech in some languages. For example, the baseline byte-based models showed the high CER values for French, which has a deep orthography, meaning that a single character has different pronunciations depending on the con- text. We observed that our method improved the byte-based models and they outperformed the IPA-based baseline models for all the metrics and languages. It is worth noting that the proposed byte-based models even outperformed the proposed IPA-based models except for el and ru. These results suggest that our framework is effective in building a TTS model for languages without G2P modules. 3.3 Evaluation Results on Unseen Language We evaluated our method on zero-shot TTS for the unseen language defined in \u00a7 2.3. As described in \u00a7 2.4, we first used the x-vector from the es speaker to compute the MCD. Table 3 lists the results. The baseline models showed the CERs of over 40% and MCDs of over 10.0. However, our 6https://github.com/openai/whisper Method fi MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER de fr ru hu nl el Natural 2.75 4.52 2.12 4.73 4.86 6.22 7.14 Baseline (Monolingual) Bytes monolingual IPA monolingual 7.70 7.38 8.61 4.07 11.76 8.96 91.82 17.86 11.43 >100 25.30 11.89 8.33 7.23 56.03 27.62 10.22 7.59 93.05 24.62 7.49 7.80 15.33 19.20 10.20 8.16 85.98 21.79 Baseline (Multilingual) Bytes multilingual w/o LIDs Bytes multilingual w/ LIDs IPA multilingual w/o LIDs IPA multilingual w/ LIDs 7.68 6.51 6.31 6.16 37.46 13.19 10.64 9.76 8.71 10.84 7.44 6.88 41.35 55.79 20.86 14.97 9.38 45.92 12.89 >100 35.32 8.10 23.54 7.63 6.26 6.78 5.53 5.17 29.19 27.22 19.56 10.63 6.48 9.09 5.59 5.28 33.82 42.97 14.03 9.11 8.46 8.47 7.76 6.95 46.33 39.37 34.49 19.48 7.64 7.25 6.90 6.90 36.24 23.56 19.33 16.97 Proposed (Unsupervised text pretraining) Bytes multilingual IPA multilingual 5.65 5.88 3.79 5.52 6.48 6.61 7.15 7.72 7.38 7.25 10.62 15.85 4.99 5.18 5.28 8.62 5.01 5.30 6.05 7.37 6.52 7.00 13.74 14.42 6.57 6.53 11.75 11.06 Table 2: Evaluation results for seen languages. Bold indicates best scores in baseline and proposed methods. es Method es x-vector MCD CER Natural 2.71 Oracle Bytes monolingual IPA monolingual IPA multilingual 8.65 8.47 6.20 10.70 5.28 5.32 Baseline (Fully zero-shot TTS) Bytes multilingual IPA multilingual 11.22 10.75 64.07 44.75 fr x-vector CER 2.71 - 6.99 66.45 44.37 (a) Token embedding \ud835\udc4d(b) Encoder inputs \ud835\udc3b!\" Proposed (Text-seen zero-shot TTS) Bytes multilingual IPA multilingual 9.05 9.44 18.27 11.69 13.74 13.33 Figure 3: Visualization of token and language embedding. Pairs of similar languages (es\u2013fr and de\u2013nl) are overlapping in token embed- ding space, while output of bottleneck layer separates them. Table 3: Evaluation results for unseen language. proposed text preraining improved the metrics, resulting in CERs of less than half for both byte and IPA-based methods. Also, in contrast to the results for the seen languages, the IPA-based model outperformed the byte-based one in terms of CER. Compared with the oracle case with the paired data of the unseen language, our proposed zero-shot TTS showed higher MCD and CER but achieved only 1% difference in CER compared to the oracle byte-based monolingual model. These results demonstrate the effectiveness of our method in achieving intelligible zero-shot TTS for the unseen language. To investigate the case where the target speaker informa- tion is completely unavailable, we also used the x-vector from a seen language. We chose the fr speaker because es and fr are both categorized as Western Romance in Glottolog [Ham- marstr\u00a8om et al., 2021]. Table 3 lists the results. Note that this case does not have the MCD results, since a different speaker than the ground-truth speech was used. We can see that the unsupervised text pretraining also improved the zero- shot performance when using the x-vector from the fr speaker. In the proposed byte-based model, the cross-lingual x-vector showed the lower CER. This might result from that the es x-vector was not present in the training data whereas the fr x-vector was present in the training data. 3.4 Ablation Study To"}