{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_Pragmatic_Inference_with_a_CLIP_Listener_for_Contrastive_Captioning_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the proposed method for contrastive captioning?", "answer": " Generating discriminative captions that distinguish target images from very similar alternative distractor images.", "ref_chunk": "3 2 0 2 n u J 5 1 ] L C . s c [ 1 v 8 1 8 8 0 . 6 0 3 2 : v i X r a Pragmatic Inference with a CLIP Listener for Contrastive Captioning Jiefu Ou1 Benno Krojer2 Daniel Fried1 Carnegie Mellon University1 Mila/McGill University2 jiefuo@andrew.cmu.edu Abstract We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic in- ference procedure that formulates captioning as a reference game between a speaker, which pro- duces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off- the-shelf CLIP model to parameterize the lis- tener. Compared with captioner-only pragmatic models, our method benefits from rich vision- language alignment representations from CLIP when reasoning over distractors. Like previ- ous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to dis- criminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which al- lows us to automatically optimize the captions for informativity \u2014 outperforming past meth- ods for discriminative captioning by 11% to 15% accuracy in human evaluations.1 Figure 1: Illustration of the contrastive captioning task with a random example from the ImageCoDe dataset. Models are tasked with generating captions that dis- tinguish the target image (a) from other very similar distractors images (b) to (d). (There are a total of 9 dis- tractors in each set of images, we omit the rest of them for simplicity of illustration.) Compared with baselines from previous work, our proposed approach, PICL, gen- erates informative captions that help clearly identify the target out of the distractors, while remaining natural and fluent. 1 Introduction Discriminative captioning provides a challenging testbed for generating context-sensitive grounded language. In this task, a model must produce a description of a target image (e.g., the green high- lighted image in Figure 1) that allows a person to correctly identify the target image from among a set of similar distractor images (e.g., the red highlighted images). Good captions must strike a balance between two criteria: (1) being fluent 1The code is available at https://github.com/ descriptions of the target image and (2) being dis- criminative in context: allowing a person to pick out the target image from the set. Past work on discriminative captioning has suc- cessfully applied techniques from computational pragmatics to trade off between the two criteria above (Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018). Possible cap- tions are selected using a combination of two scor- ing functions: (1) the caption\u2019s probability under a standard image captioning model, or base speaker score, which measures the caption\u2019s fluency and JefferyO/prag_clip_contra_caption faithfulness to the image, and (2) a base listener score, which predicts how likely a human listener would be to correctly identify the target image given the caption, i.e. measuring discriminative- ness.These past works typically obtain the listener scores from the image captioning (speaker) model itself, for example using Bayesian inference over the set of possible images (Cohn-Gordon et al., 2018). The relative weight of these two scores is controlled using a informativity hyperparameter,2 whose value affects the tradeoff between produc- ing captions that are predicted to be fluent and faithful, versus captions that are predicted to be discriminative. It is challenging to automatically choose a value for this hyperparameter, as captions that appear to be discriminative under a caption- ing model are frequently uninformative for peo- ple (Dess\u00ec et al., 2022). Our approach, PICL (Pragmatic Inference with a CLIP Listener) follows this same pragmatic framework, but scores discriminativeness using a listener model separate from the speaker. We implement the listener model using CLIP (Rad- ford et al., 2021). As shown in previous work, the rich vision-language representation learned in CLIP (1) provides robust assessments of model- generated captions that highly correlate with hu- man judgments (Hessel et al., 2021), and (2) ef- fectively quantifies the degree of discriminative- ness/informativeness of visual referring expres- sions (Takmaz et al., 2022). To evaluate PICL, we conduct experiments with sets of images from ImageCoDe (Krojer et al., 2022), a challenging dataset originally designed for contrastive retrieval: retrieving target images among a set of distractors given contextual descrip- tions. We perform contrastive captioning on this dataset for the first time. We compare PICL to past work on two criteria: (1) informativeness and (2) fluency, evaluating both metrics using automatic as well as human evaluations. Results show that our approach typically outper- forms past methods on both criteria, and is substan- tially more robust to the value of the informativ- ity hyperparameter. In particular, we are able to choose this hyperparameter automatically by maxi- mizing how informative the captions are predicted to be to human evaluators. In contrast, we find that maximizing predicted informativity leads past 2This parameter is also sometimes referred to as a ratio- nality parameter. methods to produce captions that are so disfluent that they are misleading for people. In this auto- matic hyperparameter selection setting, our method produces captions that are 11% to 15% easier for human annotators to interpret correctly than past work. 2 Related Work Contrastive Captioning A variety of methods for contrastive captioning generate captions that optimize for discriminative objectives, e.g., mini- mizing the textual similarity between captions for the target and distractor images (Wang et al., 2020), using generated captions as input to image retrieval models (Luo et al., 2018; Liu et al., 2018), and com- puting CLIP similarity scores between captions and target images (Cho et al., 2022). Other meth- ods involve leveraging fine-grained image regional features to generate distinctive captions based on similar and/or unique objects"}, {"question": " How is the approach for contrastive captioning in the text different from previous methods?", "answer": " It leverages an off-the-shelf CLIP model to parameterize the listener instead of deriving both the speaker and listener distributions from a single captioning model.", "ref_chunk": "3 2 0 2 n u J 5 1 ] L C . s c [ 1 v 8 1 8 8 0 . 6 0 3 2 : v i X r a Pragmatic Inference with a CLIP Listener for Contrastive Captioning Jiefu Ou1 Benno Krojer2 Daniel Fried1 Carnegie Mellon University1 Mila/McGill University2 jiefuo@andrew.cmu.edu Abstract We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic in- ference procedure that formulates captioning as a reference game between a speaker, which pro- duces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off- the-shelf CLIP model to parameterize the lis- tener. Compared with captioner-only pragmatic models, our method benefits from rich vision- language alignment representations from CLIP when reasoning over distractors. Like previ- ous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to dis- criminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which al- lows us to automatically optimize the captions for informativity \u2014 outperforming past meth- ods for discriminative captioning by 11% to 15% accuracy in human evaluations.1 Figure 1: Illustration of the contrastive captioning task with a random example from the ImageCoDe dataset. Models are tasked with generating captions that dis- tinguish the target image (a) from other very similar distractors images (b) to (d). (There are a total of 9 dis- tractors in each set of images, we omit the rest of them for simplicity of illustration.) Compared with baselines from previous work, our proposed approach, PICL, gen- erates informative captions that help clearly identify the target out of the distractors, while remaining natural and fluent. 1 Introduction Discriminative captioning provides a challenging testbed for generating context-sensitive grounded language. In this task, a model must produce a description of a target image (e.g., the green high- lighted image in Figure 1) that allows a person to correctly identify the target image from among a set of similar distractor images (e.g., the red highlighted images). Good captions must strike a balance between two criteria: (1) being fluent 1The code is available at https://github.com/ descriptions of the target image and (2) being dis- criminative in context: allowing a person to pick out the target image from the set. Past work on discriminative captioning has suc- cessfully applied techniques from computational pragmatics to trade off between the two criteria above (Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018). Possible cap- tions are selected using a combination of two scor- ing functions: (1) the caption\u2019s probability under a standard image captioning model, or base speaker score, which measures the caption\u2019s fluency and JefferyO/prag_clip_contra_caption faithfulness to the image, and (2) a base listener score, which predicts how likely a human listener would be to correctly identify the target image given the caption, i.e. measuring discriminative- ness.These past works typically obtain the listener scores from the image captioning (speaker) model itself, for example using Bayesian inference over the set of possible images (Cohn-Gordon et al., 2018). The relative weight of these two scores is controlled using a informativity hyperparameter,2 whose value affects the tradeoff between produc- ing captions that are predicted to be fluent and faithful, versus captions that are predicted to be discriminative. It is challenging to automatically choose a value for this hyperparameter, as captions that appear to be discriminative under a caption- ing model are frequently uninformative for peo- ple (Dess\u00ec et al., 2022). Our approach, PICL (Pragmatic Inference with a CLIP Listener) follows this same pragmatic framework, but scores discriminativeness using a listener model separate from the speaker. We implement the listener model using CLIP (Rad- ford et al., 2021). As shown in previous work, the rich vision-language representation learned in CLIP (1) provides robust assessments of model- generated captions that highly correlate with hu- man judgments (Hessel et al., 2021), and (2) ef- fectively quantifies the degree of discriminative- ness/informativeness of visual referring expres- sions (Takmaz et al., 2022). To evaluate PICL, we conduct experiments with sets of images from ImageCoDe (Krojer et al., 2022), a challenging dataset originally designed for contrastive retrieval: retrieving target images among a set of distractors given contextual descrip- tions. We perform contrastive captioning on this dataset for the first time. We compare PICL to past work on two criteria: (1) informativeness and (2) fluency, evaluating both metrics using automatic as well as human evaluations. Results show that our approach typically outper- forms past methods on both criteria, and is substan- tially more robust to the value of the informativ- ity hyperparameter. In particular, we are able to choose this hyperparameter automatically by maxi- mizing how informative the captions are predicted to be to human evaluators. In contrast, we find that maximizing predicted informativity leads past 2This parameter is also sometimes referred to as a ratio- nality parameter. methods to produce captions that are so disfluent that they are misleading for people. In this auto- matic hyperparameter selection setting, our method produces captions that are 11% to 15% easier for human annotators to interpret correctly than past work. 2 Related Work Contrastive Captioning A variety of methods for contrastive captioning generate captions that optimize for discriminative objectives, e.g., mini- mizing the textual similarity between captions for the target and distractor images (Wang et al., 2020), using generated captions as input to image retrieval models (Luo et al., 2018; Liu et al., 2018), and com- puting CLIP similarity scores between captions and target images (Cho et al., 2022). Other meth- ods involve leveraging fine-grained image regional features to generate distinctive captions based on similar and/or unique objects"}, {"question": " How is the tradeoff between informativity and fluency of the captions controlled in the proposed method?", "answer": " Using a hyperparameter.", "ref_chunk": "3 2 0 2 n u J 5 1 ] L C . s c [ 1 v 8 1 8 8 0 . 6 0 3 2 : v i X r a Pragmatic Inference with a CLIP Listener for Contrastive Captioning Jiefu Ou1 Benno Krojer2 Daniel Fried1 Carnegie Mellon University1 Mila/McGill University2 jiefuo@andrew.cmu.edu Abstract We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic in- ference procedure that formulates captioning as a reference game between a speaker, which pro- duces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off- the-shelf CLIP model to parameterize the lis- tener. Compared with captioner-only pragmatic models, our method benefits from rich vision- language alignment representations from CLIP when reasoning over distractors. Like previ- ous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to dis- criminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which al- lows us to automatically optimize the captions for informativity \u2014 outperforming past meth- ods for discriminative captioning by 11% to 15% accuracy in human evaluations.1 Figure 1: Illustration of the contrastive captioning task with a random example from the ImageCoDe dataset. Models are tasked with generating captions that dis- tinguish the target image (a) from other very similar distractors images (b) to (d). (There are a total of 9 dis- tractors in each set of images, we omit the rest of them for simplicity of illustration.) Compared with baselines from previous work, our proposed approach, PICL, gen- erates informative captions that help clearly identify the target out of the distractors, while remaining natural and fluent. 1 Introduction Discriminative captioning provides a challenging testbed for generating context-sensitive grounded language. In this task, a model must produce a description of a target image (e.g., the green high- lighted image in Figure 1) that allows a person to correctly identify the target image from among a set of similar distractor images (e.g., the red highlighted images). Good captions must strike a balance between two criteria: (1) being fluent 1The code is available at https://github.com/ descriptions of the target image and (2) being dis- criminative in context: allowing a person to pick out the target image from the set. Past work on discriminative captioning has suc- cessfully applied techniques from computational pragmatics to trade off between the two criteria above (Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018). Possible cap- tions are selected using a combination of two scor- ing functions: (1) the caption\u2019s probability under a standard image captioning model, or base speaker score, which measures the caption\u2019s fluency and JefferyO/prag_clip_contra_caption faithfulness to the image, and (2) a base listener score, which predicts how likely a human listener would be to correctly identify the target image given the caption, i.e. measuring discriminative- ness.These past works typically obtain the listener scores from the image captioning (speaker) model itself, for example using Bayesian inference over the set of possible images (Cohn-Gordon et al., 2018). The relative weight of these two scores is controlled using a informativity hyperparameter,2 whose value affects the tradeoff between produc- ing captions that are predicted to be fluent and faithful, versus captions that are predicted to be discriminative. It is challenging to automatically choose a value for this hyperparameter, as captions that appear to be discriminative under a caption- ing model are frequently uninformative for peo- ple (Dess\u00ec et al., 2022). Our approach, PICL (Pragmatic Inference with a CLIP Listener) follows this same pragmatic framework, but scores discriminativeness using a listener model separate from the speaker. We implement the listener model using CLIP (Rad- ford et al., 2021). As shown in previous work, the rich vision-language representation learned in CLIP (1) provides robust assessments of model- generated captions that highly correlate with hu- man judgments (Hessel et al., 2021), and (2) ef- fectively quantifies the degree of discriminative- ness/informativeness of visual referring expres- sions (Takmaz et al., 2022). To evaluate PICL, we conduct experiments with sets of images from ImageCoDe (Krojer et al., 2022), a challenging dataset originally designed for contrastive retrieval: retrieving target images among a set of distractors given contextual descrip- tions. We perform contrastive captioning on this dataset for the first time. We compare PICL to past work on two criteria: (1) informativeness and (2) fluency, evaluating both metrics using automatic as well as human evaluations. Results show that our approach typically outper- forms past methods on both criteria, and is substan- tially more robust to the value of the informativ- ity hyperparameter. In particular, we are able to choose this hyperparameter automatically by maxi- mizing how informative the captions are predicted to be to human evaluators. In contrast, we find that maximizing predicted informativity leads past 2This parameter is also sometimes referred to as a ratio- nality parameter. methods to produce captions that are so disfluent that they are misleading for people. In this auto- matic hyperparameter selection setting, our method produces captions that are 11% to 15% easier for human annotators to interpret correctly than past work. 2 Related Work Contrastive Captioning A variety of methods for contrastive captioning generate captions that optimize for discriminative objectives, e.g., mini- mizing the textual similarity between captions for the target and distractor images (Wang et al., 2020), using generated captions as input to image retrieval models (Luo et al., 2018; Liu et al., 2018), and com- puting CLIP similarity scores between captions and target images (Cho et al., 2022). Other meth- ods involve leveraging fine-grained image regional features to generate distinctive captions based on similar and/or unique objects"}, {"question": " What percentage improvement in accuracy in human evaluations did the proposed method achieve in comparison to past methods for discriminative captioning?", "answer": " 11% to 15%.", "ref_chunk": "3 2 0 2 n u J 5 1 ] L C . s c [ 1 v 8 1 8 8 0 . 6 0 3 2 : v i X r a Pragmatic Inference with a CLIP Listener for Contrastive Captioning Jiefu Ou1 Benno Krojer2 Daniel Fried1 Carnegie Mellon University1 Mila/McGill University2 jiefuo@andrew.cmu.edu Abstract We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic in- ference procedure that formulates captioning as a reference game between a speaker, which pro- duces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off- the-shelf CLIP model to parameterize the lis- tener. Compared with captioner-only pragmatic models, our method benefits from rich vision- language alignment representations from CLIP when reasoning over distractors. Like previ- ous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to dis- criminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which al- lows us to automatically optimize the captions for informativity \u2014 outperforming past meth- ods for discriminative captioning by 11% to 15% accuracy in human evaluations.1 Figure 1: Illustration of the contrastive captioning task with a random example from the ImageCoDe dataset. Models are tasked with generating captions that dis- tinguish the target image (a) from other very similar distractors images (b) to (d). (There are a total of 9 dis- tractors in each set of images, we omit the rest of them for simplicity of illustration.) Compared with baselines from previous work, our proposed approach, PICL, gen- erates informative captions that help clearly identify the target out of the distractors, while remaining natural and fluent. 1 Introduction Discriminative captioning provides a challenging testbed for generating context-sensitive grounded language. In this task, a model must produce a description of a target image (e.g., the green high- lighted image in Figure 1) that allows a person to correctly identify the target image from among a set of similar distractor images (e.g., the red highlighted images). Good captions must strike a balance between two criteria: (1) being fluent 1The code is available at https://github.com/ descriptions of the target image and (2) being dis- criminative in context: allowing a person to pick out the target image from the set. Past work on discriminative captioning has suc- cessfully applied techniques from computational pragmatics to trade off between the two criteria above (Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018). Possible cap- tions are selected using a combination of two scor- ing functions: (1) the caption\u2019s probability under a standard image captioning model, or base speaker score, which measures the caption\u2019s fluency and JefferyO/prag_clip_contra_caption faithfulness to the image, and (2) a base listener score, which predicts how likely a human listener would be to correctly identify the target image given the caption, i.e. measuring discriminative- ness.These past works typically obtain the listener scores from the image captioning (speaker) model itself, for example using Bayesian inference over the set of possible images (Cohn-Gordon et al., 2018). The relative weight of these two scores is controlled using a informativity hyperparameter,2 whose value affects the tradeoff between produc- ing captions that are predicted to be fluent and faithful, versus captions that are predicted to be discriminative. It is challenging to automatically choose a value for this hyperparameter, as captions that appear to be discriminative under a caption- ing model are frequently uninformative for peo- ple (Dess\u00ec et al., 2022). Our approach, PICL (Pragmatic Inference with a CLIP Listener) follows this same pragmatic framework, but scores discriminativeness using a listener model separate from the speaker. We implement the listener model using CLIP (Rad- ford et al., 2021). As shown in previous work, the rich vision-language representation learned in CLIP (1) provides robust assessments of model- generated captions that highly correlate with hu- man judgments (Hessel et al., 2021), and (2) ef- fectively quantifies the degree of discriminative- ness/informativeness of visual referring expres- sions (Takmaz et al., 2022). To evaluate PICL, we conduct experiments with sets of images from ImageCoDe (Krojer et al., 2022), a challenging dataset originally designed for contrastive retrieval: retrieving target images among a set of distractors given contextual descrip- tions. We perform contrastive captioning on this dataset for the first time. We compare PICL to past work on two criteria: (1) informativeness and (2) fluency, evaluating both metrics using automatic as well as human evaluations. Results show that our approach typically outper- forms past methods on both criteria, and is substan- tially more robust to the value of the informativ- ity hyperparameter. In particular, we are able to choose this hyperparameter automatically by maxi- mizing how informative the captions are predicted to be to human evaluators. In contrast, we find that maximizing predicted informativity leads past 2This parameter is also sometimes referred to as a ratio- nality parameter. methods to produce captions that are so disfluent that they are misleading for people. In this auto- matic hyperparameter selection setting, our method produces captions that are 11% to 15% easier for human annotators to interpret correctly than past work. 2 Related Work Contrastive Captioning A variety of methods for contrastive captioning generate captions that optimize for discriminative objectives, e.g., mini- mizing the textual similarity between captions for the target and distractor images (Wang et al., 2020), using generated captions as input to image retrieval models (Luo et al., 2018; Liu et al., 2018), and com- puting CLIP similarity scores between captions and target images (Cho et al., 2022). Other meth- ods involve leveraging fine-grained image regional features to generate distinctive captions based on similar and/or unique objects"}, {"question": " What role does CLIP play in the proposed approach, PICL?", "answer": " It is used to score discriminativeness by implementing the listener model.", "ref_chunk": "3 2 0 2 n u J 5 1 ] L C . s c [ 1 v 8 1 8 8 0 . 6 0 3 2 : v i X r a Pragmatic Inference with a CLIP Listener for Contrastive Captioning Jiefu Ou1 Benno Krojer2 Daniel Fried1 Carnegie Mellon University1 Mila/McGill University2 jiefuo@andrew.cmu.edu Abstract We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic in- ference procedure that formulates captioning as a reference game between a speaker, which pro- duces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off- the-shelf CLIP model to parameterize the lis- tener. Compared with captioner-only pragmatic models, our method benefits from rich vision- language alignment representations from CLIP when reasoning over distractors. Like previ- ous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to dis- criminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which al- lows us to automatically optimize the captions for informativity \u2014 outperforming past meth- ods for discriminative captioning by 11% to 15% accuracy in human evaluations.1 Figure 1: Illustration of the contrastive captioning task with a random example from the ImageCoDe dataset. Models are tasked with generating captions that dis- tinguish the target image (a) from other very similar distractors images (b) to (d). (There are a total of 9 dis- tractors in each set of images, we omit the rest of them for simplicity of illustration.) Compared with baselines from previous work, our proposed approach, PICL, gen- erates informative captions that help clearly identify the target out of the distractors, while remaining natural and fluent. 1 Introduction Discriminative captioning provides a challenging testbed for generating context-sensitive grounded language. In this task, a model must produce a description of a target image (e.g., the green high- lighted image in Figure 1) that allows a person to correctly identify the target image from among a set of similar distractor images (e.g., the red highlighted images). Good captions must strike a balance between two criteria: (1) being fluent 1The code is available at https://github.com/ descriptions of the target image and (2) being dis- criminative in context: allowing a person to pick out the target image from the set. Past work on discriminative captioning has suc- cessfully applied techniques from computational pragmatics to trade off between the two criteria above (Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018). Possible cap- tions are selected using a combination of two scor- ing functions: (1) the caption\u2019s probability under a standard image captioning model, or base speaker score, which measures the caption\u2019s fluency and JefferyO/prag_clip_contra_caption faithfulness to the image, and (2) a base listener score, which predicts how likely a human listener would be to correctly identify the target image given the caption, i.e. measuring discriminative- ness.These past works typically obtain the listener scores from the image captioning (speaker) model itself, for example using Bayesian inference over the set of possible images (Cohn-Gordon et al., 2018). The relative weight of these two scores is controlled using a informativity hyperparameter,2 whose value affects the tradeoff between produc- ing captions that are predicted to be fluent and faithful, versus captions that are predicted to be discriminative. It is challenging to automatically choose a value for this hyperparameter, as captions that appear to be discriminative under a caption- ing model are frequently uninformative for peo- ple (Dess\u00ec et al., 2022). Our approach, PICL (Pragmatic Inference with a CLIP Listener) follows this same pragmatic framework, but scores discriminativeness using a listener model separate from the speaker. We implement the listener model using CLIP (Rad- ford et al., 2021). As shown in previous work, the rich vision-language representation learned in CLIP (1) provides robust assessments of model- generated captions that highly correlate with hu- man judgments (Hessel et al., 2021), and (2) ef- fectively quantifies the degree of discriminative- ness/informativeness of visual referring expres- sions (Takmaz et al., 2022). To evaluate PICL, we conduct experiments with sets of images from ImageCoDe (Krojer et al., 2022), a challenging dataset originally designed for contrastive retrieval: retrieving target images among a set of distractors given contextual descrip- tions. We perform contrastive captioning on this dataset for the first time. We compare PICL to past work on two criteria: (1) informativeness and (2) fluency, evaluating both metrics using automatic as well as human evaluations. Results show that our approach typically outper- forms past methods on both criteria, and is substan- tially more robust to the value of the informativ- ity hyperparameter. In particular, we are able to choose this hyperparameter automatically by maxi- mizing how informative the captions are predicted to be to human evaluators. In contrast, we find that maximizing predicted informativity leads past 2This parameter is also sometimes referred to as a ratio- nality parameter. methods to produce captions that are so disfluent that they are misleading for people. In this auto- matic hyperparameter selection setting, our method produces captions that are 11% to 15% easier for human annotators to interpret correctly than past work. 2 Related Work Contrastive Captioning A variety of methods for contrastive captioning generate captions that optimize for discriminative objectives, e.g., mini- mizing the textual similarity between captions for the target and distractor images (Wang et al., 2020), using generated captions as input to image retrieval models (Luo et al., 2018; Liu et al., 2018), and com- puting CLIP similarity scores between captions and target images (Cho et al., 2022). Other meth- ods involve leveraging fine-grained image regional features to generate distinctive captions based on similar and/or unique objects"}, {"question": " What is the dataset used to evaluate PICL in the experiments mentioned in the text?", "answer": " ImageCoDe.", "ref_chunk": "3 2 0 2 n u J 5 1 ] L C . s c [ 1 v 8 1 8 8 0 . 6 0 3 2 : v i X r a Pragmatic Inference with a CLIP Listener for Contrastive Captioning Jiefu Ou1 Benno Krojer2 Daniel Fried1 Carnegie Mellon University1 Mila/McGill University2 jiefuo@andrew.cmu.edu Abstract We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic in- ference procedure that formulates captioning as a reference game between a speaker, which pro- duces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off- the-shelf CLIP model to parameterize the lis- tener. Compared with captioner-only pragmatic models, our method benefits from rich vision- language alignment representations from CLIP when reasoning over distractors. Like previ- ous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to dis- criminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which al- lows us to automatically optimize the captions for informativity \u2014 outperforming past meth- ods for discriminative captioning by 11% to 15% accuracy in human evaluations.1 Figure 1: Illustration of the contrastive captioning task with a random example from the ImageCoDe dataset. Models are tasked with generating captions that dis- tinguish the target image (a) from other very similar distractors images (b) to (d). (There are a total of 9 dis- tractors in each set of images, we omit the rest of them for simplicity of illustration.) Compared with baselines from previous work, our proposed approach, PICL, gen- erates informative captions that help clearly identify the target out of the distractors, while remaining natural and fluent. 1 Introduction Discriminative captioning provides a challenging testbed for generating context-sensitive grounded language. In this task, a model must produce a description of a target image (e.g., the green high- lighted image in Figure 1) that allows a person to correctly identify the target image from among a set of similar distractor images (e.g., the red highlighted images). Good captions must strike a balance between two criteria: (1) being fluent 1The code is available at https://github.com/ descriptions of the target image and (2) being dis- criminative in context: allowing a person to pick out the target image from the set. Past work on discriminative captioning has suc- cessfully applied techniques from computational pragmatics to trade off between the two criteria above (Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018). Possible cap- tions are selected using a combination of two scor- ing functions: (1) the caption\u2019s probability under a standard image captioning model, or base speaker score, which measures the caption\u2019s fluency and JefferyO/prag_clip_contra_caption faithfulness to the image, and (2) a base listener score, which predicts how likely a human listener would be to correctly identify the target image given the caption, i.e. measuring discriminative- ness.These past works typically obtain the listener scores from the image captioning (speaker) model itself, for example using Bayesian inference over the set of possible images (Cohn-Gordon et al., 2018). The relative weight of these two scores is controlled using a informativity hyperparameter,2 whose value affects the tradeoff between produc- ing captions that are predicted to be fluent and faithful, versus captions that are predicted to be discriminative. It is challenging to automatically choose a value for this hyperparameter, as captions that appear to be discriminative under a caption- ing model are frequently uninformative for peo- ple (Dess\u00ec et al., 2022). Our approach, PICL (Pragmatic Inference with a CLIP Listener) follows this same pragmatic framework, but scores discriminativeness using a listener model separate from the speaker. We implement the listener model using CLIP (Rad- ford et al., 2021). As shown in previous work, the rich vision-language representation learned in CLIP (1) provides robust assessments of model- generated captions that highly correlate with hu- man judgments (Hessel et al., 2021), and (2) ef- fectively quantifies the degree of discriminative- ness/informativeness of visual referring expres- sions (Takmaz et al., 2022). To evaluate PICL, we conduct experiments with sets of images from ImageCoDe (Krojer et al., 2022), a challenging dataset originally designed for contrastive retrieval: retrieving target images among a set of distractors given contextual descrip- tions. We perform contrastive captioning on this dataset for the first time. We compare PICL to past work on two criteria: (1) informativeness and (2) fluency, evaluating both metrics using automatic as well as human evaluations. Results show that our approach typically outper- forms past methods on both criteria, and is substan- tially more robust to the value of the informativ- ity hyperparameter. In particular, we are able to choose this hyperparameter automatically by maxi- mizing how informative the captions are predicted to be to human evaluators. In contrast, we find that maximizing predicted informativity leads past 2This parameter is also sometimes referred to as a ratio- nality parameter. methods to produce captions that are so disfluent that they are misleading for people. In this auto- matic hyperparameter selection setting, our method produces captions that are 11% to 15% easier for human annotators to interpret correctly than past work. 2 Related Work Contrastive Captioning A variety of methods for contrastive captioning generate captions that optimize for discriminative objectives, e.g., mini- mizing the textual similarity between captions for the target and distractor images (Wang et al., 2020), using generated captions as input to image retrieval models (Luo et al., 2018; Liu et al., 2018), and com- puting CLIP similarity scores between captions and target images (Cho et al., 2022). Other meth- ods involve leveraging fine-grained image regional features to generate distinctive captions based on similar and/or unique objects"}, {"question": " How does PICL compare to past methods in terms of informativeness and fluency?", "answer": " It typically outperforms past methods on both criteria.", "ref_chunk": "3 2 0 2 n u J 5 1 ] L C . s c [ 1 v 8 1 8 8 0 . 6 0 3 2 : v i X r a Pragmatic Inference with a CLIP Listener for Contrastive Captioning Jiefu Ou1 Benno Krojer2 Daniel Fried1 Carnegie Mellon University1 Mila/McGill University2 jiefuo@andrew.cmu.edu Abstract We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic in- ference procedure that formulates captioning as a reference game between a speaker, which pro- duces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off- the-shelf CLIP model to parameterize the lis- tener. Compared with captioner-only pragmatic models, our method benefits from rich vision- language alignment representations from CLIP when reasoning over distractors. Like previ- ous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to dis- criminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which al- lows us to automatically optimize the captions for informativity \u2014 outperforming past meth- ods for discriminative captioning by 11% to 15% accuracy in human evaluations.1 Figure 1: Illustration of the contrastive captioning task with a random example from the ImageCoDe dataset. Models are tasked with generating captions that dis- tinguish the target image (a) from other very similar distractors images (b) to (d). (There are a total of 9 dis- tractors in each set of images, we omit the rest of them for simplicity of illustration.) Compared with baselines from previous work, our proposed approach, PICL, gen- erates informative captions that help clearly identify the target out of the distractors, while remaining natural and fluent. 1 Introduction Discriminative captioning provides a challenging testbed for generating context-sensitive grounded language. In this task, a model must produce a description of a target image (e.g., the green high- lighted image in Figure 1) that allows a person to correctly identify the target image from among a set of similar distractor images (e.g., the red highlighted images). Good captions must strike a balance between two criteria: (1) being fluent 1The code is available at https://github.com/ descriptions of the target image and (2) being dis- criminative in context: allowing a person to pick out the target image from the set. Past work on discriminative captioning has suc- cessfully applied techniques from computational pragmatics to trade off between the two criteria above (Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018). Possible cap- tions are selected using a combination of two scor- ing functions: (1) the caption\u2019s probability under a standard image captioning model, or base speaker score, which measures the caption\u2019s fluency and JefferyO/prag_clip_contra_caption faithfulness to the image, and (2) a base listener score, which predicts how likely a human listener would be to correctly identify the target image given the caption, i.e. measuring discriminative- ness.These past works typically obtain the listener scores from the image captioning (speaker) model itself, for example using Bayesian inference over the set of possible images (Cohn-Gordon et al., 2018). The relative weight of these two scores is controlled using a informativity hyperparameter,2 whose value affects the tradeoff between produc- ing captions that are predicted to be fluent and faithful, versus captions that are predicted to be discriminative. It is challenging to automatically choose a value for this hyperparameter, as captions that appear to be discriminative under a caption- ing model are frequently uninformative for peo- ple (Dess\u00ec et al., 2022). Our approach, PICL (Pragmatic Inference with a CLIP Listener) follows this same pragmatic framework, but scores discriminativeness using a listener model separate from the speaker. We implement the listener model using CLIP (Rad- ford et al., 2021). As shown in previous work, the rich vision-language representation learned in CLIP (1) provides robust assessments of model- generated captions that highly correlate with hu- man judgments (Hessel et al., 2021), and (2) ef- fectively quantifies the degree of discriminative- ness/informativeness of visual referring expres- sions (Takmaz et al., 2022). To evaluate PICL, we conduct experiments with sets of images from ImageCoDe (Krojer et al., 2022), a challenging dataset originally designed for contrastive retrieval: retrieving target images among a set of distractors given contextual descrip- tions. We perform contrastive captioning on this dataset for the first time. We compare PICL to past work on two criteria: (1) informativeness and (2) fluency, evaluating both metrics using automatic as well as human evaluations. Results show that our approach typically outper- forms past methods on both criteria, and is substan- tially more robust to the value of the informativ- ity hyperparameter. In particular, we are able to choose this hyperparameter automatically by maxi- mizing how informative the captions are predicted to be to human evaluators. In contrast, we find that maximizing predicted informativity leads past 2This parameter is also sometimes referred to as a ratio- nality parameter. methods to produce captions that are so disfluent that they are misleading for people. In this auto- matic hyperparameter selection setting, our method produces captions that are 11% to 15% easier for human annotators to interpret correctly than past work. 2 Related Work Contrastive Captioning A variety of methods for contrastive captioning generate captions that optimize for discriminative objectives, e.g., mini- mizing the textual similarity between captions for the target and distractor images (Wang et al., 2020), using generated captions as input to image retrieval models (Luo et al., 2018; Liu et al., 2018), and com- puting CLIP similarity scores between captions and target images (Cho et al., 2022). Other meth- ods involve leveraging fine-grained image regional features to generate distinctive captions based on similar and/or unique objects"}, {"question": " How does the method in the text automatically optimize the captions for informativity?", "answer": " By choosing the hyperparameter value that maximizes how informative the captions are predicted to be to human evaluators.", "ref_chunk": "3 2 0 2 n u J 5 1 ] L C . s c [ 1 v 8 1 8 8 0 . 6 0 3 2 : v i X r a Pragmatic Inference with a CLIP Listener for Contrastive Captioning Jiefu Ou1 Benno Krojer2 Daniel Fried1 Carnegie Mellon University1 Mila/McGill University2 jiefuo@andrew.cmu.edu Abstract We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic in- ference procedure that formulates captioning as a reference game between a speaker, which pro- duces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off- the-shelf CLIP model to parameterize the lis- tener. Compared with captioner-only pragmatic models, our method benefits from rich vision- language alignment representations from CLIP when reasoning over distractors. Like previ- ous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to dis- criminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which al- lows us to automatically optimize the captions for informativity \u2014 outperforming past meth- ods for discriminative captioning by 11% to 15% accuracy in human evaluations.1 Figure 1: Illustration of the contrastive captioning task with a random example from the ImageCoDe dataset. Models are tasked with generating captions that dis- tinguish the target image (a) from other very similar distractors images (b) to (d). (There are a total of 9 dis- tractors in each set of images, we omit the rest of them for simplicity of illustration.) Compared with baselines from previous work, our proposed approach, PICL, gen- erates informative captions that help clearly identify the target out of the distractors, while remaining natural and fluent. 1 Introduction Discriminative captioning provides a challenging testbed for generating context-sensitive grounded language. In this task, a model must produce a description of a target image (e.g., the green high- lighted image in Figure 1) that allows a person to correctly identify the target image from among a set of similar distractor images (e.g., the red highlighted images). Good captions must strike a balance between two criteria: (1) being fluent 1The code is available at https://github.com/ descriptions of the target image and (2) being dis- criminative in context: allowing a person to pick out the target image from the set. Past work on discriminative captioning has suc- cessfully applied techniques from computational pragmatics to trade off between the two criteria above (Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018). Possible cap- tions are selected using a combination of two scor- ing functions: (1) the caption\u2019s probability under a standard image captioning model, or base speaker score, which measures the caption\u2019s fluency and JefferyO/prag_clip_contra_caption faithfulness to the image, and (2) a base listener score, which predicts how likely a human listener would be to correctly identify the target image given the caption, i.e. measuring discriminative- ness.These past works typically obtain the listener scores from the image captioning (speaker) model itself, for example using Bayesian inference over the set of possible images (Cohn-Gordon et al., 2018). The relative weight of these two scores is controlled using a informativity hyperparameter,2 whose value affects the tradeoff between produc- ing captions that are predicted to be fluent and faithful, versus captions that are predicted to be discriminative. It is challenging to automatically choose a value for this hyperparameter, as captions that appear to be discriminative under a caption- ing model are frequently uninformative for peo- ple (Dess\u00ec et al., 2022). Our approach, PICL (Pragmatic Inference with a CLIP Listener) follows this same pragmatic framework, but scores discriminativeness using a listener model separate from the speaker. We implement the listener model using CLIP (Rad- ford et al., 2021). As shown in previous work, the rich vision-language representation learned in CLIP (1) provides robust assessments of model- generated captions that highly correlate with hu- man judgments (Hessel et al., 2021), and (2) ef- fectively quantifies the degree of discriminative- ness/informativeness of visual referring expres- sions (Takmaz et al., 2022). To evaluate PICL, we conduct experiments with sets of images from ImageCoDe (Krojer et al., 2022), a challenging dataset originally designed for contrastive retrieval: retrieving target images among a set of distractors given contextual descrip- tions. We perform contrastive captioning on this dataset for the first time. We compare PICL to past work on two criteria: (1) informativeness and (2) fluency, evaluating both metrics using automatic as well as human evaluations. Results show that our approach typically outper- forms past methods on both criteria, and is substan- tially more robust to the value of the informativ- ity hyperparameter. In particular, we are able to choose this hyperparameter automatically by maxi- mizing how informative the captions are predicted to be to human evaluators. In contrast, we find that maximizing predicted informativity leads past 2This parameter is also sometimes referred to as a ratio- nality parameter. methods to produce captions that are so disfluent that they are misleading for people. In this auto- matic hyperparameter selection setting, our method produces captions that are 11% to 15% easier for human annotators to interpret correctly than past work. 2 Related Work Contrastive Captioning A variety of methods for contrastive captioning generate captions that optimize for discriminative objectives, e.g., mini- mizing the textual similarity between captions for the target and distractor images (Wang et al., 2020), using generated captions as input to image retrieval models (Luo et al., 2018; Liu et al., 2018), and com- puting CLIP similarity scores between captions and target images (Cho et al., 2022). Other meth- ods involve leveraging fine-grained image regional features to generate distinctive captions based on similar and/or unique objects"}, {"question": " What is the challenge in automatically choosing a value for the hyperparameter in discriminative captioning?", "answer": " Captions that appear to be discriminative under a captioning model are frequently uninformative for people.", "ref_chunk": "3 2 0 2 n u J 5 1 ] L C . s c [ 1 v 8 1 8 8 0 . 6 0 3 2 : v i X r a Pragmatic Inference with a CLIP Listener for Contrastive Captioning Jiefu Ou1 Benno Krojer2 Daniel Fried1 Carnegie Mellon University1 Mila/McGill University2 jiefuo@andrew.cmu.edu Abstract We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic in- ference procedure that formulates captioning as a reference game between a speaker, which pro- duces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off- the-shelf CLIP model to parameterize the lis- tener. Compared with captioner-only pragmatic models, our method benefits from rich vision- language alignment representations from CLIP when reasoning over distractors. Like previ- ous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to dis- criminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which al- lows us to automatically optimize the captions for informativity \u2014 outperforming past meth- ods for discriminative captioning by 11% to 15% accuracy in human evaluations.1 Figure 1: Illustration of the contrastive captioning task with a random example from the ImageCoDe dataset. Models are tasked with generating captions that dis- tinguish the target image (a) from other very similar distractors images (b) to (d). (There are a total of 9 dis- tractors in each set of images, we omit the rest of them for simplicity of illustration.) Compared with baselines from previous work, our proposed approach, PICL, gen- erates informative captions that help clearly identify the target out of the distractors, while remaining natural and fluent. 1 Introduction Discriminative captioning provides a challenging testbed for generating context-sensitive grounded language. In this task, a model must produce a description of a target image (e.g., the green high- lighted image in Figure 1) that allows a person to correctly identify the target image from among a set of similar distractor images (e.g., the red highlighted images). Good captions must strike a balance between two criteria: (1) being fluent 1The code is available at https://github.com/ descriptions of the target image and (2) being dis- criminative in context: allowing a person to pick out the target image from the set. Past work on discriminative captioning has suc- cessfully applied techniques from computational pragmatics to trade off between the two criteria above (Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018). Possible cap- tions are selected using a combination of two scor- ing functions: (1) the caption\u2019s probability under a standard image captioning model, or base speaker score, which measures the caption\u2019s fluency and JefferyO/prag_clip_contra_caption faithfulness to the image, and (2) a base listener score, which predicts how likely a human listener would be to correctly identify the target image given the caption, i.e. measuring discriminative- ness.These past works typically obtain the listener scores from the image captioning (speaker) model itself, for example using Bayesian inference over the set of possible images (Cohn-Gordon et al., 2018). The relative weight of these two scores is controlled using a informativity hyperparameter,2 whose value affects the tradeoff between produc- ing captions that are predicted to be fluent and faithful, versus captions that are predicted to be discriminative. It is challenging to automatically choose a value for this hyperparameter, as captions that appear to be discriminative under a caption- ing model are frequently uninformative for peo- ple (Dess\u00ec et al., 2022). Our approach, PICL (Pragmatic Inference with a CLIP Listener) follows this same pragmatic framework, but scores discriminativeness using a listener model separate from the speaker. We implement the listener model using CLIP (Rad- ford et al., 2021). As shown in previous work, the rich vision-language representation learned in CLIP (1) provides robust assessments of model- generated captions that highly correlate with hu- man judgments (Hessel et al., 2021), and (2) ef- fectively quantifies the degree of discriminative- ness/informativeness of visual referring expres- sions (Takmaz et al., 2022). To evaluate PICL, we conduct experiments with sets of images from ImageCoDe (Krojer et al., 2022), a challenging dataset originally designed for contrastive retrieval: retrieving target images among a set of distractors given contextual descrip- tions. We perform contrastive captioning on this dataset for the first time. We compare PICL to past work on two criteria: (1) informativeness and (2) fluency, evaluating both metrics using automatic as well as human evaluations. Results show that our approach typically outper- forms past methods on both criteria, and is substan- tially more robust to the value of the informativ- ity hyperparameter. In particular, we are able to choose this hyperparameter automatically by maxi- mizing how informative the captions are predicted to be to human evaluators. In contrast, we find that maximizing predicted informativity leads past 2This parameter is also sometimes referred to as a ratio- nality parameter. methods to produce captions that are so disfluent that they are misleading for people. In this auto- matic hyperparameter selection setting, our method produces captions that are 11% to 15% easier for human annotators to interpret correctly than past work. 2 Related Work Contrastive Captioning A variety of methods for contrastive captioning generate captions that optimize for discriminative objectives, e.g., mini- mizing the textual similarity between captions for the target and distractor images (Wang et al., 2020), using generated captions as input to image retrieval models (Luo et al., 2018; Liu et al., 2018), and com- puting CLIP similarity scores between captions and target images (Cho et al., 2022). Other meth- ods involve leveraging fine-grained image regional features to generate distinctive captions based on similar and/or unique objects"}, {"question": " What is the main benefit of using CLIP in the proposed approach for contrastive captioning?", "answer": " It provides robust assessments of model-generated captions that highly correlate with human judgments.", "ref_chunk": "3 2 0 2 n u J 5 1 ] L C . s c [ 1 v 8 1 8 8 0 . 6 0 3 2 : v i X r a Pragmatic Inference with a CLIP Listener for Contrastive Captioning Jiefu Ou1 Benno Krojer2 Daniel Fried1 Carnegie Mellon University1 Mila/McGill University2 jiefuo@andrew.cmu.edu Abstract We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic in- ference procedure that formulates captioning as a reference game between a speaker, which pro- duces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off- the-shelf CLIP model to parameterize the lis- tener. Compared with captioner-only pragmatic models, our method benefits from rich vision- language alignment representations from CLIP when reasoning over distractors. Like previ- ous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to dis- criminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which al- lows us to automatically optimize the captions for informativity \u2014 outperforming past meth- ods for discriminative captioning by 11% to 15% accuracy in human evaluations.1 Figure 1: Illustration of the contrastive captioning task with a random example from the ImageCoDe dataset. Models are tasked with generating captions that dis- tinguish the target image (a) from other very similar distractors images (b) to (d). (There are a total of 9 dis- tractors in each set of images, we omit the rest of them for simplicity of illustration.) Compared with baselines from previous work, our proposed approach, PICL, gen- erates informative captions that help clearly identify the target out of the distractors, while remaining natural and fluent. 1 Introduction Discriminative captioning provides a challenging testbed for generating context-sensitive grounded language. In this task, a model must produce a description of a target image (e.g., the green high- lighted image in Figure 1) that allows a person to correctly identify the target image from among a set of similar distractor images (e.g., the red highlighted images). Good captions must strike a balance between two criteria: (1) being fluent 1The code is available at https://github.com/ descriptions of the target image and (2) being dis- criminative in context: allowing a person to pick out the target image from the set. Past work on discriminative captioning has suc- cessfully applied techniques from computational pragmatics to trade off between the two criteria above (Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018). Possible cap- tions are selected using a combination of two scor- ing functions: (1) the caption\u2019s probability under a standard image captioning model, or base speaker score, which measures the caption\u2019s fluency and JefferyO/prag_clip_contra_caption faithfulness to the image, and (2) a base listener score, which predicts how likely a human listener would be to correctly identify the target image given the caption, i.e. measuring discriminative- ness.These past works typically obtain the listener scores from the image captioning (speaker) model itself, for example using Bayesian inference over the set of possible images (Cohn-Gordon et al., 2018). The relative weight of these two scores is controlled using a informativity hyperparameter,2 whose value affects the tradeoff between produc- ing captions that are predicted to be fluent and faithful, versus captions that are predicted to be discriminative. It is challenging to automatically choose a value for this hyperparameter, as captions that appear to be discriminative under a caption- ing model are frequently uninformative for peo- ple (Dess\u00ec et al., 2022). Our approach, PICL (Pragmatic Inference with a CLIP Listener) follows this same pragmatic framework, but scores discriminativeness using a listener model separate from the speaker. We implement the listener model using CLIP (Rad- ford et al., 2021). As shown in previous work, the rich vision-language representation learned in CLIP (1) provides robust assessments of model- generated captions that highly correlate with hu- man judgments (Hessel et al., 2021), and (2) ef- fectively quantifies the degree of discriminative- ness/informativeness of visual referring expres- sions (Takmaz et al., 2022). To evaluate PICL, we conduct experiments with sets of images from ImageCoDe (Krojer et al., 2022), a challenging dataset originally designed for contrastive retrieval: retrieving target images among a set of distractors given contextual descrip- tions. We perform contrastive captioning on this dataset for the first time. We compare PICL to past work on two criteria: (1) informativeness and (2) fluency, evaluating both metrics using automatic as well as human evaluations. Results show that our approach typically outper- forms past methods on both criteria, and is substan- tially more robust to the value of the informativ- ity hyperparameter. In particular, we are able to choose this hyperparameter automatically by maxi- mizing how informative the captions are predicted to be to human evaluators. In contrast, we find that maximizing predicted informativity leads past 2This parameter is also sometimes referred to as a ratio- nality parameter. methods to produce captions that are so disfluent that they are misleading for people. In this auto- matic hyperparameter selection setting, our method produces captions that are 11% to 15% easier for human annotators to interpret correctly than past work. 2 Related Work Contrastive Captioning A variety of methods for contrastive captioning generate captions that optimize for discriminative objectives, e.g., mini- mizing the textual similarity between captions for the target and distractor images (Wang et al., 2020), using generated captions as input to image retrieval models (Luo et al., 2018; Liu et al., 2018), and com- puting CLIP similarity scores between captions and target images (Cho et al., 2022). Other meth- ods involve leveraging fine-grained image regional features to generate distinctive captions based on similar and/or unique objects"}], "doc_text": "3 2 0 2 n u J 5 1 ] L C . s c [ 1 v 8 1 8 8 0 . 6 0 3 2 : v i X r a Pragmatic Inference with a CLIP Listener for Contrastive Captioning Jiefu Ou1 Benno Krojer2 Daniel Fried1 Carnegie Mellon University1 Mila/McGill University2 jiefuo@andrew.cmu.edu Abstract We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic in- ference procedure that formulates captioning as a reference game between a speaker, which pro- duces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off- the-shelf CLIP model to parameterize the lis- tener. Compared with captioner-only pragmatic models, our method benefits from rich vision- language alignment representations from CLIP when reasoning over distractors. Like previ- ous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to dis- criminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which al- lows us to automatically optimize the captions for informativity \u2014 outperforming past meth- ods for discriminative captioning by 11% to 15% accuracy in human evaluations.1 Figure 1: Illustration of the contrastive captioning task with a random example from the ImageCoDe dataset. Models are tasked with generating captions that dis- tinguish the target image (a) from other very similar distractors images (b) to (d). (There are a total of 9 dis- tractors in each set of images, we omit the rest of them for simplicity of illustration.) Compared with baselines from previous work, our proposed approach, PICL, gen- erates informative captions that help clearly identify the target out of the distractors, while remaining natural and fluent. 1 Introduction Discriminative captioning provides a challenging testbed for generating context-sensitive grounded language. In this task, a model must produce a description of a target image (e.g., the green high- lighted image in Figure 1) that allows a person to correctly identify the target image from among a set of similar distractor images (e.g., the red highlighted images). Good captions must strike a balance between two criteria: (1) being fluent 1The code is available at https://github.com/ descriptions of the target image and (2) being dis- criminative in context: allowing a person to pick out the target image from the set. Past work on discriminative captioning has suc- cessfully applied techniques from computational pragmatics to trade off between the two criteria above (Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018). Possible cap- tions are selected using a combination of two scor- ing functions: (1) the caption\u2019s probability under a standard image captioning model, or base speaker score, which measures the caption\u2019s fluency and JefferyO/prag_clip_contra_caption faithfulness to the image, and (2) a base listener score, which predicts how likely a human listener would be to correctly identify the target image given the caption, i.e. measuring discriminative- ness.These past works typically obtain the listener scores from the image captioning (speaker) model itself, for example using Bayesian inference over the set of possible images (Cohn-Gordon et al., 2018). The relative weight of these two scores is controlled using a informativity hyperparameter,2 whose value affects the tradeoff between produc- ing captions that are predicted to be fluent and faithful, versus captions that are predicted to be discriminative. It is challenging to automatically choose a value for this hyperparameter, as captions that appear to be discriminative under a caption- ing model are frequently uninformative for peo- ple (Dess\u00ec et al., 2022). Our approach, PICL (Pragmatic Inference with a CLIP Listener) follows this same pragmatic framework, but scores discriminativeness using a listener model separate from the speaker. We implement the listener model using CLIP (Rad- ford et al., 2021). As shown in previous work, the rich vision-language representation learned in CLIP (1) provides robust assessments of model- generated captions that highly correlate with hu- man judgments (Hessel et al., 2021), and (2) ef- fectively quantifies the degree of discriminative- ness/informativeness of visual referring expres- sions (Takmaz et al., 2022). To evaluate PICL, we conduct experiments with sets of images from ImageCoDe (Krojer et al., 2022), a challenging dataset originally designed for contrastive retrieval: retrieving target images among a set of distractors given contextual descrip- tions. We perform contrastive captioning on this dataset for the first time. We compare PICL to past work on two criteria: (1) informativeness and (2) fluency, evaluating both metrics using automatic as well as human evaluations. Results show that our approach typically outper- forms past methods on both criteria, and is substan- tially more robust to the value of the informativ- ity hyperparameter. In particular, we are able to choose this hyperparameter automatically by maxi- mizing how informative the captions are predicted to be to human evaluators. In contrast, we find that maximizing predicted informativity leads past 2This parameter is also sometimes referred to as a ratio- nality parameter. methods to produce captions that are so disfluent that they are misleading for people. In this auto- matic hyperparameter selection setting, our method produces captions that are 11% to 15% easier for human annotators to interpret correctly than past work. 2 Related Work Contrastive Captioning A variety of methods for contrastive captioning generate captions that optimize for discriminative objectives, e.g., mini- mizing the textual similarity between captions for the target and distractor images (Wang et al., 2020), using generated captions as input to image retrieval models (Luo et al., 2018; Liu et al., 2018), and com- puting CLIP similarity scores between captions and target images (Cho et al., 2022). Other meth- ods involve leveraging fine-grained image regional features to generate distinctive captions based on similar and/or unique objects"}