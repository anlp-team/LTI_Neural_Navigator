{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Multi-lingual_and_Multi-cultural_Figurative_Language_Understanding_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the two contributors to the performance gap in the seven languages compared to English?", "answer": " The two contributors are cross-lingual transfer and concept shift.", "ref_chunk": "that there can be two contributors to the gap in performance in these seven languages as com- pared to English. First, since our fine-tuning lan- guage is English, there can be a drop in perfor- mance simply due to cross-lingual transfer. Sec- ond, there is a concept shift in these metaphors, as evidenced by our analysis in Section 4. To discern the contribution of both, we machine-translate the target test sets to en (we refer to this as translate- test). The difference between translate-test and zero-shot, can be thought of as the cross-lingual transfer gap, while the rest of the difference be- tween translate-test and en test performance can be attributed to the concept shift. Due to possible MT errors, the results here represent upper bounds for concept shift and cross-lingual shift, which is References to weather/season References to food References to friendship su The Indian Ocean is sparkling like a Peacock this Christmas season. kn That food is as sweet as Neem jv My friend\u2019s father is like a raden werkudara. kn The weather is also warm like the rainy season. hi Hotel food was like tamarind. hi He guided his friend like Krishna. su The weather looks like you can fry fish on the asphalt. sw His waist is the width of a baobab. sw His friend is abunuwasi. hi Tina and Ravi\u2019s love is like monsoon season. jv The taste of this food is like boiled tempeh. id He asks the help of his friends just like the king of Tanah Djawo Kingdom. Table 7: Translated examples with cultural references specific to regions where these languages are spoken. further discussed in Section 6.1. The concept shift gap is generally greater than the cross-lingual gap. As reported in Ta- ble 8, the concept shift gap is greater than the cross-lingual transfer gap for all languages except Swahili, across all models. This result for sw cor- roborates our findings in Section 4, where we ob- serve that en shares the greatest proportion of ob- ject concepts with sw. Given Swahili\u2019s extremely low-representation in MPLMs (Table 5), and its high concept overlap with English, we cover most of the gap by simply translating sw to en. For Indonesian (id), we observe that zero-shot perfor- mance itself is close to en performance (83.6%) for XLM-R, since id is well-represented in this model (Table 5). Hence, translating to en does not help, and the model needs to be competent in better un- derstanding the cultural references specific to id. In mBERT however, id is poorly represented, and translating to en does help improve performance. Performance increases as model and train- ing data size increase, but moreso for higher resource languages. The smallest model exam- ined, mBERT, has relatively poor performance for all languages, as all languages have < 60% ac- curacy. Hindi and Indonesian, the two highest- resource languages in our dataset, show a high gain in performance when using a larger model, increas- ing to 67.58% and 78.09% accuracy respectively. This is especially true for Indonesian, which has a relatively high amount of training data as shown in Table 5. However, lower resource languages tend to show a more modest gain in performance. 5.2 Few-shot 5.2.1 Few-shot evaluation While it is common to fine-tune MPLMs on En- glish, given its widespread use and availability, several past works have shown how this is sub- optimal (Lin et al., 2019\u037e Debnath et al., 2021) and choosing optimal transfer languages is an important research question in itself (Dhamecha et al., 2021). While the design of an ideal al- location of annotation resources is still unknown, Lauscher et al. (2020) demonstrate the effective- ness of investing in few-shot (5-10) in-language task-specific examples, which provides vast im- provements over the zero-shot setup. We include between 2-50 labelled pairs of sen- tences from each target language, in addition to the English labelled data, for fine-tuning the model. Training details for all models can be found in Ap- pendix E. 5.2.2 Few-shot results Figure 3 presents the effects of few-shot transfer for each language. Generally, the performance gain is modest. This aligns with results from Lauscher et al. (2020), who found that perfor- mance gains were quite small on XNLI. As our task is also an NLI task, we may expect simi- lar improvements. However, we find collecting some cultural examples could disproportionately help low-resource languages. Augmenting with few examples usually does not help much We observed that with a few excep- tions, the increase in accuracy on the test set gained was small (< 1%). This is likely because of the di- versity of facts needed in order to improve perfor- mance. As noted in Section 4.1 and Table 1, this dataset contains many unique cultural references that do not repeat, limiting the utility of seeing a few examples. languages benefit more greatly from augmentation However, there are a few exceptions to this trend. In particular, adding 50 paired Kannada examples to XLM-Rlarge improved performance by 3.83%. Swahili also improves by 1.10% with 50 additional examples for XLM-Rbase, and Sundanese improves by 2.33% with 50 examples for mBERTbase. Lower-resource 5.3 Evaluation of Large Language Models In addition to the three MPLMs we examine in de- tail, we also examine the zero-shot performance of large pretrained language models. We choose to Model Language Zero-shot Performance Translate-test Cross-Lingual Concept Shift Transfer Gap (to EN) Gap XLM-Rlarge en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 81.50 \u00b12.41 67.58 \u00b11.38 78.09 \u00b11.14 60.93 \u00b11.95 58.08 \u00b12.10 60.40 \u00b11.98 58.16 \u00b10.73 - 81.50 \u00b12.41 67.82 \u00b11.52 77.51 \u00b10.91 68.13 \u00b11.66 63.67 \u00b10.98 70.07 \u00b10.92 75.29 \u00b12.05 - 0.00 0.24 -0.58 7.20 5.59 9.67 17.13 - 0.00 13.68 3.99 13.37 17.83 11.43 6.21 - XLM-Rbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 75.26 \u00b10.95 62.48 \u00b10.31 68.88 \u00b10.71 53.67 \u00b10.54 54.67 \u00b11.31 52.41 \u00b11.79 52.73 \u00b11.38 - 75.26 \u00b10.95 63.29 \u00b10.84 66.54 \u00b11.22 58.17 \u00b10.82 57.86 \u00b11.10 61.33 \u00b10.68 65.77 \u00b11.82 - 0.00 0.81 -2.34 4.50 3.20 8.93 13.04 - 0.00 11.97"}, {"question": " How is the difference between translate-test and zero-shot performance explained?", "answer": " The cross-lingual transfer gap is thought of as the difference, while the remaining difference is attributed to the concept shift.", "ref_chunk": "that there can be two contributors to the gap in performance in these seven languages as com- pared to English. First, since our fine-tuning lan- guage is English, there can be a drop in perfor- mance simply due to cross-lingual transfer. Sec- ond, there is a concept shift in these metaphors, as evidenced by our analysis in Section 4. To discern the contribution of both, we machine-translate the target test sets to en (we refer to this as translate- test). The difference between translate-test and zero-shot, can be thought of as the cross-lingual transfer gap, while the rest of the difference be- tween translate-test and en test performance can be attributed to the concept shift. Due to possible MT errors, the results here represent upper bounds for concept shift and cross-lingual shift, which is References to weather/season References to food References to friendship su The Indian Ocean is sparkling like a Peacock this Christmas season. kn That food is as sweet as Neem jv My friend\u2019s father is like a raden werkudara. kn The weather is also warm like the rainy season. hi Hotel food was like tamarind. hi He guided his friend like Krishna. su The weather looks like you can fry fish on the asphalt. sw His waist is the width of a baobab. sw His friend is abunuwasi. hi Tina and Ravi\u2019s love is like monsoon season. jv The taste of this food is like boiled tempeh. id He asks the help of his friends just like the king of Tanah Djawo Kingdom. Table 7: Translated examples with cultural references specific to regions where these languages are spoken. further discussed in Section 6.1. The concept shift gap is generally greater than the cross-lingual gap. As reported in Ta- ble 8, the concept shift gap is greater than the cross-lingual transfer gap for all languages except Swahili, across all models. This result for sw cor- roborates our findings in Section 4, where we ob- serve that en shares the greatest proportion of ob- ject concepts with sw. Given Swahili\u2019s extremely low-representation in MPLMs (Table 5), and its high concept overlap with English, we cover most of the gap by simply translating sw to en. For Indonesian (id), we observe that zero-shot perfor- mance itself is close to en performance (83.6%) for XLM-R, since id is well-represented in this model (Table 5). Hence, translating to en does not help, and the model needs to be competent in better un- derstanding the cultural references specific to id. In mBERT however, id is poorly represented, and translating to en does help improve performance. Performance increases as model and train- ing data size increase, but moreso for higher resource languages. The smallest model exam- ined, mBERT, has relatively poor performance for all languages, as all languages have < 60% ac- curacy. Hindi and Indonesian, the two highest- resource languages in our dataset, show a high gain in performance when using a larger model, increas- ing to 67.58% and 78.09% accuracy respectively. This is especially true for Indonesian, which has a relatively high amount of training data as shown in Table 5. However, lower resource languages tend to show a more modest gain in performance. 5.2 Few-shot 5.2.1 Few-shot evaluation While it is common to fine-tune MPLMs on En- glish, given its widespread use and availability, several past works have shown how this is sub- optimal (Lin et al., 2019\u037e Debnath et al., 2021) and choosing optimal transfer languages is an important research question in itself (Dhamecha et al., 2021). While the design of an ideal al- location of annotation resources is still unknown, Lauscher et al. (2020) demonstrate the effective- ness of investing in few-shot (5-10) in-language task-specific examples, which provides vast im- provements over the zero-shot setup. We include between 2-50 labelled pairs of sen- tences from each target language, in addition to the English labelled data, for fine-tuning the model. Training details for all models can be found in Ap- pendix E. 5.2.2 Few-shot results Figure 3 presents the effects of few-shot transfer for each language. Generally, the performance gain is modest. This aligns with results from Lauscher et al. (2020), who found that perfor- mance gains were quite small on XNLI. As our task is also an NLI task, we may expect simi- lar improvements. However, we find collecting some cultural examples could disproportionately help low-resource languages. Augmenting with few examples usually does not help much We observed that with a few excep- tions, the increase in accuracy on the test set gained was small (< 1%). This is likely because of the di- versity of facts needed in order to improve perfor- mance. As noted in Section 4.1 and Table 1, this dataset contains many unique cultural references that do not repeat, limiting the utility of seeing a few examples. languages benefit more greatly from augmentation However, there are a few exceptions to this trend. In particular, adding 50 paired Kannada examples to XLM-Rlarge improved performance by 3.83%. Swahili also improves by 1.10% with 50 additional examples for XLM-Rbase, and Sundanese improves by 2.33% with 50 examples for mBERTbase. Lower-resource 5.3 Evaluation of Large Language Models In addition to the three MPLMs we examine in de- tail, we also examine the zero-shot performance of large pretrained language models. We choose to Model Language Zero-shot Performance Translate-test Cross-Lingual Concept Shift Transfer Gap (to EN) Gap XLM-Rlarge en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 81.50 \u00b12.41 67.58 \u00b11.38 78.09 \u00b11.14 60.93 \u00b11.95 58.08 \u00b12.10 60.40 \u00b11.98 58.16 \u00b10.73 - 81.50 \u00b12.41 67.82 \u00b11.52 77.51 \u00b10.91 68.13 \u00b11.66 63.67 \u00b10.98 70.07 \u00b10.92 75.29 \u00b12.05 - 0.00 0.24 -0.58 7.20 5.59 9.67 17.13 - 0.00 13.68 3.99 13.37 17.83 11.43 6.21 - XLM-Rbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 75.26 \u00b10.95 62.48 \u00b10.31 68.88 \u00b10.71 53.67 \u00b10.54 54.67 \u00b11.31 52.41 \u00b11.79 52.73 \u00b11.38 - 75.26 \u00b10.95 63.29 \u00b10.84 66.54 \u00b11.22 58.17 \u00b10.82 57.86 \u00b11.10 61.33 \u00b10.68 65.77 \u00b11.82 - 0.00 0.81 -2.34 4.50 3.20 8.93 13.04 - 0.00 11.97"}, {"question": " What do the results represent due to possible machine translation errors?", "answer": " The results represent upper bounds for concept shift and cross-lingual shift.", "ref_chunk": "that there can be two contributors to the gap in performance in these seven languages as com- pared to English. First, since our fine-tuning lan- guage is English, there can be a drop in perfor- mance simply due to cross-lingual transfer. Sec- ond, there is a concept shift in these metaphors, as evidenced by our analysis in Section 4. To discern the contribution of both, we machine-translate the target test sets to en (we refer to this as translate- test). The difference between translate-test and zero-shot, can be thought of as the cross-lingual transfer gap, while the rest of the difference be- tween translate-test and en test performance can be attributed to the concept shift. Due to possible MT errors, the results here represent upper bounds for concept shift and cross-lingual shift, which is References to weather/season References to food References to friendship su The Indian Ocean is sparkling like a Peacock this Christmas season. kn That food is as sweet as Neem jv My friend\u2019s father is like a raden werkudara. kn The weather is also warm like the rainy season. hi Hotel food was like tamarind. hi He guided his friend like Krishna. su The weather looks like you can fry fish on the asphalt. sw His waist is the width of a baobab. sw His friend is abunuwasi. hi Tina and Ravi\u2019s love is like monsoon season. jv The taste of this food is like boiled tempeh. id He asks the help of his friends just like the king of Tanah Djawo Kingdom. Table 7: Translated examples with cultural references specific to regions where these languages are spoken. further discussed in Section 6.1. The concept shift gap is generally greater than the cross-lingual gap. As reported in Ta- ble 8, the concept shift gap is greater than the cross-lingual transfer gap for all languages except Swahili, across all models. This result for sw cor- roborates our findings in Section 4, where we ob- serve that en shares the greatest proportion of ob- ject concepts with sw. Given Swahili\u2019s extremely low-representation in MPLMs (Table 5), and its high concept overlap with English, we cover most of the gap by simply translating sw to en. For Indonesian (id), we observe that zero-shot perfor- mance itself is close to en performance (83.6%) for XLM-R, since id is well-represented in this model (Table 5). Hence, translating to en does not help, and the model needs to be competent in better un- derstanding the cultural references specific to id. In mBERT however, id is poorly represented, and translating to en does help improve performance. Performance increases as model and train- ing data size increase, but moreso for higher resource languages. The smallest model exam- ined, mBERT, has relatively poor performance for all languages, as all languages have < 60% ac- curacy. Hindi and Indonesian, the two highest- resource languages in our dataset, show a high gain in performance when using a larger model, increas- ing to 67.58% and 78.09% accuracy respectively. This is especially true for Indonesian, which has a relatively high amount of training data as shown in Table 5. However, lower resource languages tend to show a more modest gain in performance. 5.2 Few-shot 5.2.1 Few-shot evaluation While it is common to fine-tune MPLMs on En- glish, given its widespread use and availability, several past works have shown how this is sub- optimal (Lin et al., 2019\u037e Debnath et al., 2021) and choosing optimal transfer languages is an important research question in itself (Dhamecha et al., 2021). While the design of an ideal al- location of annotation resources is still unknown, Lauscher et al. (2020) demonstrate the effective- ness of investing in few-shot (5-10) in-language task-specific examples, which provides vast im- provements over the zero-shot setup. We include between 2-50 labelled pairs of sen- tences from each target language, in addition to the English labelled data, for fine-tuning the model. Training details for all models can be found in Ap- pendix E. 5.2.2 Few-shot results Figure 3 presents the effects of few-shot transfer for each language. Generally, the performance gain is modest. This aligns with results from Lauscher et al. (2020), who found that perfor- mance gains were quite small on XNLI. As our task is also an NLI task, we may expect simi- lar improvements. However, we find collecting some cultural examples could disproportionately help low-resource languages. Augmenting with few examples usually does not help much We observed that with a few excep- tions, the increase in accuracy on the test set gained was small (< 1%). This is likely because of the di- versity of facts needed in order to improve perfor- mance. As noted in Section 4.1 and Table 1, this dataset contains many unique cultural references that do not repeat, limiting the utility of seeing a few examples. languages benefit more greatly from augmentation However, there are a few exceptions to this trend. In particular, adding 50 paired Kannada examples to XLM-Rlarge improved performance by 3.83%. Swahili also improves by 1.10% with 50 additional examples for XLM-Rbase, and Sundanese improves by 2.33% with 50 examples for mBERTbase. Lower-resource 5.3 Evaluation of Large Language Models In addition to the three MPLMs we examine in de- tail, we also examine the zero-shot performance of large pretrained language models. We choose to Model Language Zero-shot Performance Translate-test Cross-Lingual Concept Shift Transfer Gap (to EN) Gap XLM-Rlarge en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 81.50 \u00b12.41 67.58 \u00b11.38 78.09 \u00b11.14 60.93 \u00b11.95 58.08 \u00b12.10 60.40 \u00b11.98 58.16 \u00b10.73 - 81.50 \u00b12.41 67.82 \u00b11.52 77.51 \u00b10.91 68.13 \u00b11.66 63.67 \u00b10.98 70.07 \u00b10.92 75.29 \u00b12.05 - 0.00 0.24 -0.58 7.20 5.59 9.67 17.13 - 0.00 13.68 3.99 13.37 17.83 11.43 6.21 - XLM-Rbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 75.26 \u00b10.95 62.48 \u00b10.31 68.88 \u00b10.71 53.67 \u00b10.54 54.67 \u00b11.31 52.41 \u00b11.79 52.73 \u00b11.38 - 75.26 \u00b10.95 63.29 \u00b10.84 66.54 \u00b11.22 58.17 \u00b10.82 57.86 \u00b11.10 61.33 \u00b10.68 65.77 \u00b11.82 - 0.00 0.81 -2.34 4.50 3.20 8.93 13.04 - 0.00 11.97"}, {"question": " Which gap is generally greater, the concept shift gap or the cross-lingual gap?", "answer": " The concept shift gap is generally greater.", "ref_chunk": "that there can be two contributors to the gap in performance in these seven languages as com- pared to English. First, since our fine-tuning lan- guage is English, there can be a drop in perfor- mance simply due to cross-lingual transfer. Sec- ond, there is a concept shift in these metaphors, as evidenced by our analysis in Section 4. To discern the contribution of both, we machine-translate the target test sets to en (we refer to this as translate- test). The difference between translate-test and zero-shot, can be thought of as the cross-lingual transfer gap, while the rest of the difference be- tween translate-test and en test performance can be attributed to the concept shift. Due to possible MT errors, the results here represent upper bounds for concept shift and cross-lingual shift, which is References to weather/season References to food References to friendship su The Indian Ocean is sparkling like a Peacock this Christmas season. kn That food is as sweet as Neem jv My friend\u2019s father is like a raden werkudara. kn The weather is also warm like the rainy season. hi Hotel food was like tamarind. hi He guided his friend like Krishna. su The weather looks like you can fry fish on the asphalt. sw His waist is the width of a baobab. sw His friend is abunuwasi. hi Tina and Ravi\u2019s love is like monsoon season. jv The taste of this food is like boiled tempeh. id He asks the help of his friends just like the king of Tanah Djawo Kingdom. Table 7: Translated examples with cultural references specific to regions where these languages are spoken. further discussed in Section 6.1. The concept shift gap is generally greater than the cross-lingual gap. As reported in Ta- ble 8, the concept shift gap is greater than the cross-lingual transfer gap for all languages except Swahili, across all models. This result for sw cor- roborates our findings in Section 4, where we ob- serve that en shares the greatest proportion of ob- ject concepts with sw. Given Swahili\u2019s extremely low-representation in MPLMs (Table 5), and its high concept overlap with English, we cover most of the gap by simply translating sw to en. For Indonesian (id), we observe that zero-shot perfor- mance itself is close to en performance (83.6%) for XLM-R, since id is well-represented in this model (Table 5). Hence, translating to en does not help, and the model needs to be competent in better un- derstanding the cultural references specific to id. In mBERT however, id is poorly represented, and translating to en does help improve performance. Performance increases as model and train- ing data size increase, but moreso for higher resource languages. The smallest model exam- ined, mBERT, has relatively poor performance for all languages, as all languages have < 60% ac- curacy. Hindi and Indonesian, the two highest- resource languages in our dataset, show a high gain in performance when using a larger model, increas- ing to 67.58% and 78.09% accuracy respectively. This is especially true for Indonesian, which has a relatively high amount of training data as shown in Table 5. However, lower resource languages tend to show a more modest gain in performance. 5.2 Few-shot 5.2.1 Few-shot evaluation While it is common to fine-tune MPLMs on En- glish, given its widespread use and availability, several past works have shown how this is sub- optimal (Lin et al., 2019\u037e Debnath et al., 2021) and choosing optimal transfer languages is an important research question in itself (Dhamecha et al., 2021). While the design of an ideal al- location of annotation resources is still unknown, Lauscher et al. (2020) demonstrate the effective- ness of investing in few-shot (5-10) in-language task-specific examples, which provides vast im- provements over the zero-shot setup. We include between 2-50 labelled pairs of sen- tences from each target language, in addition to the English labelled data, for fine-tuning the model. Training details for all models can be found in Ap- pendix E. 5.2.2 Few-shot results Figure 3 presents the effects of few-shot transfer for each language. Generally, the performance gain is modest. This aligns with results from Lauscher et al. (2020), who found that perfor- mance gains were quite small on XNLI. As our task is also an NLI task, we may expect simi- lar improvements. However, we find collecting some cultural examples could disproportionately help low-resource languages. Augmenting with few examples usually does not help much We observed that with a few excep- tions, the increase in accuracy on the test set gained was small (< 1%). This is likely because of the di- versity of facts needed in order to improve perfor- mance. As noted in Section 4.1 and Table 1, this dataset contains many unique cultural references that do not repeat, limiting the utility of seeing a few examples. languages benefit more greatly from augmentation However, there are a few exceptions to this trend. In particular, adding 50 paired Kannada examples to XLM-Rlarge improved performance by 3.83%. Swahili also improves by 1.10% with 50 additional examples for XLM-Rbase, and Sundanese improves by 2.33% with 50 examples for mBERTbase. Lower-resource 5.3 Evaluation of Large Language Models In addition to the three MPLMs we examine in de- tail, we also examine the zero-shot performance of large pretrained language models. We choose to Model Language Zero-shot Performance Translate-test Cross-Lingual Concept Shift Transfer Gap (to EN) Gap XLM-Rlarge en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 81.50 \u00b12.41 67.58 \u00b11.38 78.09 \u00b11.14 60.93 \u00b11.95 58.08 \u00b12.10 60.40 \u00b11.98 58.16 \u00b10.73 - 81.50 \u00b12.41 67.82 \u00b11.52 77.51 \u00b10.91 68.13 \u00b11.66 63.67 \u00b10.98 70.07 \u00b10.92 75.29 \u00b12.05 - 0.00 0.24 -0.58 7.20 5.59 9.67 17.13 - 0.00 13.68 3.99 13.37 17.83 11.43 6.21 - XLM-Rbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 75.26 \u00b10.95 62.48 \u00b10.31 68.88 \u00b10.71 53.67 \u00b10.54 54.67 \u00b11.31 52.41 \u00b11.79 52.73 \u00b11.38 - 75.26 \u00b10.95 63.29 \u00b10.84 66.54 \u00b11.22 58.17 \u00b10.82 57.86 \u00b11.10 61.33 \u00b10.68 65.77 \u00b11.82 - 0.00 0.81 -2.34 4.50 3.20 8.93 13.04 - 0.00 11.97"}, {"question": " In which language does the concept shift gap exceed the cross-lingual transfer gap for all languages except Swahili?", "answer": " The concept shift gap exceeds the cross-lingual transfer gap for all languages except Swahili.", "ref_chunk": "that there can be two contributors to the gap in performance in these seven languages as com- pared to English. First, since our fine-tuning lan- guage is English, there can be a drop in perfor- mance simply due to cross-lingual transfer. Sec- ond, there is a concept shift in these metaphors, as evidenced by our analysis in Section 4. To discern the contribution of both, we machine-translate the target test sets to en (we refer to this as translate- test). The difference between translate-test and zero-shot, can be thought of as the cross-lingual transfer gap, while the rest of the difference be- tween translate-test and en test performance can be attributed to the concept shift. Due to possible MT errors, the results here represent upper bounds for concept shift and cross-lingual shift, which is References to weather/season References to food References to friendship su The Indian Ocean is sparkling like a Peacock this Christmas season. kn That food is as sweet as Neem jv My friend\u2019s father is like a raden werkudara. kn The weather is also warm like the rainy season. hi Hotel food was like tamarind. hi He guided his friend like Krishna. su The weather looks like you can fry fish on the asphalt. sw His waist is the width of a baobab. sw His friend is abunuwasi. hi Tina and Ravi\u2019s love is like monsoon season. jv The taste of this food is like boiled tempeh. id He asks the help of his friends just like the king of Tanah Djawo Kingdom. Table 7: Translated examples with cultural references specific to regions where these languages are spoken. further discussed in Section 6.1. The concept shift gap is generally greater than the cross-lingual gap. As reported in Ta- ble 8, the concept shift gap is greater than the cross-lingual transfer gap for all languages except Swahili, across all models. This result for sw cor- roborates our findings in Section 4, where we ob- serve that en shares the greatest proportion of ob- ject concepts with sw. Given Swahili\u2019s extremely low-representation in MPLMs (Table 5), and its high concept overlap with English, we cover most of the gap by simply translating sw to en. For Indonesian (id), we observe that zero-shot perfor- mance itself is close to en performance (83.6%) for XLM-R, since id is well-represented in this model (Table 5). Hence, translating to en does not help, and the model needs to be competent in better un- derstanding the cultural references specific to id. In mBERT however, id is poorly represented, and translating to en does help improve performance. Performance increases as model and train- ing data size increase, but moreso for higher resource languages. The smallest model exam- ined, mBERT, has relatively poor performance for all languages, as all languages have < 60% ac- curacy. Hindi and Indonesian, the two highest- resource languages in our dataset, show a high gain in performance when using a larger model, increas- ing to 67.58% and 78.09% accuracy respectively. This is especially true for Indonesian, which has a relatively high amount of training data as shown in Table 5. However, lower resource languages tend to show a more modest gain in performance. 5.2 Few-shot 5.2.1 Few-shot evaluation While it is common to fine-tune MPLMs on En- glish, given its widespread use and availability, several past works have shown how this is sub- optimal (Lin et al., 2019\u037e Debnath et al., 2021) and choosing optimal transfer languages is an important research question in itself (Dhamecha et al., 2021). While the design of an ideal al- location of annotation resources is still unknown, Lauscher et al. (2020) demonstrate the effective- ness of investing in few-shot (5-10) in-language task-specific examples, which provides vast im- provements over the zero-shot setup. We include between 2-50 labelled pairs of sen- tences from each target language, in addition to the English labelled data, for fine-tuning the model. Training details for all models can be found in Ap- pendix E. 5.2.2 Few-shot results Figure 3 presents the effects of few-shot transfer for each language. Generally, the performance gain is modest. This aligns with results from Lauscher et al. (2020), who found that perfor- mance gains were quite small on XNLI. As our task is also an NLI task, we may expect simi- lar improvements. However, we find collecting some cultural examples could disproportionately help low-resource languages. Augmenting with few examples usually does not help much We observed that with a few excep- tions, the increase in accuracy on the test set gained was small (< 1%). This is likely because of the di- versity of facts needed in order to improve perfor- mance. As noted in Section 4.1 and Table 1, this dataset contains many unique cultural references that do not repeat, limiting the utility of seeing a few examples. languages benefit more greatly from augmentation However, there are a few exceptions to this trend. In particular, adding 50 paired Kannada examples to XLM-Rlarge improved performance by 3.83%. Swahili also improves by 1.10% with 50 additional examples for XLM-Rbase, and Sundanese improves by 2.33% with 50 examples for mBERTbase. Lower-resource 5.3 Evaluation of Large Language Models In addition to the three MPLMs we examine in de- tail, we also examine the zero-shot performance of large pretrained language models. We choose to Model Language Zero-shot Performance Translate-test Cross-Lingual Concept Shift Transfer Gap (to EN) Gap XLM-Rlarge en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 81.50 \u00b12.41 67.58 \u00b11.38 78.09 \u00b11.14 60.93 \u00b11.95 58.08 \u00b12.10 60.40 \u00b11.98 58.16 \u00b10.73 - 81.50 \u00b12.41 67.82 \u00b11.52 77.51 \u00b10.91 68.13 \u00b11.66 63.67 \u00b10.98 70.07 \u00b10.92 75.29 \u00b12.05 - 0.00 0.24 -0.58 7.20 5.59 9.67 17.13 - 0.00 13.68 3.99 13.37 17.83 11.43 6.21 - XLM-Rbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 75.26 \u00b10.95 62.48 \u00b10.31 68.88 \u00b10.71 53.67 \u00b10.54 54.67 \u00b11.31 52.41 \u00b11.79 52.73 \u00b11.38 - 75.26 \u00b10.95 63.29 \u00b10.84 66.54 \u00b11.22 58.17 \u00b10.82 57.86 \u00b11.10 61.33 \u00b10.68 65.77 \u00b11.82 - 0.00 0.81 -2.34 4.50 3.20 8.93 13.04 - 0.00 11.97"}, {"question": " What does the study observe about the performance of Indonesian (id) in relation to zero-shot performance?", "answer": " For Indonesian (id), zero-shot performance itself is close to English performance for XLM-R.", "ref_chunk": "that there can be two contributors to the gap in performance in these seven languages as com- pared to English. First, since our fine-tuning lan- guage is English, there can be a drop in perfor- mance simply due to cross-lingual transfer. Sec- ond, there is a concept shift in these metaphors, as evidenced by our analysis in Section 4. To discern the contribution of both, we machine-translate the target test sets to en (we refer to this as translate- test). The difference between translate-test and zero-shot, can be thought of as the cross-lingual transfer gap, while the rest of the difference be- tween translate-test and en test performance can be attributed to the concept shift. Due to possible MT errors, the results here represent upper bounds for concept shift and cross-lingual shift, which is References to weather/season References to food References to friendship su The Indian Ocean is sparkling like a Peacock this Christmas season. kn That food is as sweet as Neem jv My friend\u2019s father is like a raden werkudara. kn The weather is also warm like the rainy season. hi Hotel food was like tamarind. hi He guided his friend like Krishna. su The weather looks like you can fry fish on the asphalt. sw His waist is the width of a baobab. sw His friend is abunuwasi. hi Tina and Ravi\u2019s love is like monsoon season. jv The taste of this food is like boiled tempeh. id He asks the help of his friends just like the king of Tanah Djawo Kingdom. Table 7: Translated examples with cultural references specific to regions where these languages are spoken. further discussed in Section 6.1. The concept shift gap is generally greater than the cross-lingual gap. As reported in Ta- ble 8, the concept shift gap is greater than the cross-lingual transfer gap for all languages except Swahili, across all models. This result for sw cor- roborates our findings in Section 4, where we ob- serve that en shares the greatest proportion of ob- ject concepts with sw. Given Swahili\u2019s extremely low-representation in MPLMs (Table 5), and its high concept overlap with English, we cover most of the gap by simply translating sw to en. For Indonesian (id), we observe that zero-shot perfor- mance itself is close to en performance (83.6%) for XLM-R, since id is well-represented in this model (Table 5). Hence, translating to en does not help, and the model needs to be competent in better un- derstanding the cultural references specific to id. In mBERT however, id is poorly represented, and translating to en does help improve performance. Performance increases as model and train- ing data size increase, but moreso for higher resource languages. The smallest model exam- ined, mBERT, has relatively poor performance for all languages, as all languages have < 60% ac- curacy. Hindi and Indonesian, the two highest- resource languages in our dataset, show a high gain in performance when using a larger model, increas- ing to 67.58% and 78.09% accuracy respectively. This is especially true for Indonesian, which has a relatively high amount of training data as shown in Table 5. However, lower resource languages tend to show a more modest gain in performance. 5.2 Few-shot 5.2.1 Few-shot evaluation While it is common to fine-tune MPLMs on En- glish, given its widespread use and availability, several past works have shown how this is sub- optimal (Lin et al., 2019\u037e Debnath et al., 2021) and choosing optimal transfer languages is an important research question in itself (Dhamecha et al., 2021). While the design of an ideal al- location of annotation resources is still unknown, Lauscher et al. (2020) demonstrate the effective- ness of investing in few-shot (5-10) in-language task-specific examples, which provides vast im- provements over the zero-shot setup. We include between 2-50 labelled pairs of sen- tences from each target language, in addition to the English labelled data, for fine-tuning the model. Training details for all models can be found in Ap- pendix E. 5.2.2 Few-shot results Figure 3 presents the effects of few-shot transfer for each language. Generally, the performance gain is modest. This aligns with results from Lauscher et al. (2020), who found that perfor- mance gains were quite small on XNLI. As our task is also an NLI task, we may expect simi- lar improvements. However, we find collecting some cultural examples could disproportionately help low-resource languages. Augmenting with few examples usually does not help much We observed that with a few excep- tions, the increase in accuracy on the test set gained was small (< 1%). This is likely because of the di- versity of facts needed in order to improve perfor- mance. As noted in Section 4.1 and Table 1, this dataset contains many unique cultural references that do not repeat, limiting the utility of seeing a few examples. languages benefit more greatly from augmentation However, there are a few exceptions to this trend. In particular, adding 50 paired Kannada examples to XLM-Rlarge improved performance by 3.83%. Swahili also improves by 1.10% with 50 additional examples for XLM-Rbase, and Sundanese improves by 2.33% with 50 examples for mBERTbase. Lower-resource 5.3 Evaluation of Large Language Models In addition to the three MPLMs we examine in de- tail, we also examine the zero-shot performance of large pretrained language models. We choose to Model Language Zero-shot Performance Translate-test Cross-Lingual Concept Shift Transfer Gap (to EN) Gap XLM-Rlarge en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 81.50 \u00b12.41 67.58 \u00b11.38 78.09 \u00b11.14 60.93 \u00b11.95 58.08 \u00b12.10 60.40 \u00b11.98 58.16 \u00b10.73 - 81.50 \u00b12.41 67.82 \u00b11.52 77.51 \u00b10.91 68.13 \u00b11.66 63.67 \u00b10.98 70.07 \u00b10.92 75.29 \u00b12.05 - 0.00 0.24 -0.58 7.20 5.59 9.67 17.13 - 0.00 13.68 3.99 13.37 17.83 11.43 6.21 - XLM-Rbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 75.26 \u00b10.95 62.48 \u00b10.31 68.88 \u00b10.71 53.67 \u00b10.54 54.67 \u00b11.31 52.41 \u00b11.79 52.73 \u00b11.38 - 75.26 \u00b10.95 63.29 \u00b10.84 66.54 \u00b11.22 58.17 \u00b10.82 57.86 \u00b11.10 61.33 \u00b10.68 65.77 \u00b11.82 - 0.00 0.81 -2.34 4.50 3.20 8.93 13.04 - 0.00 11.97"}, {"question": " How does the performance of Hindi and Indonesian change with the model and training data size increase?", "answer": " The performance of Hindi and Indonesian increases significantly with larger models and more training data.", "ref_chunk": "that there can be two contributors to the gap in performance in these seven languages as com- pared to English. First, since our fine-tuning lan- guage is English, there can be a drop in perfor- mance simply due to cross-lingual transfer. Sec- ond, there is a concept shift in these metaphors, as evidenced by our analysis in Section 4. To discern the contribution of both, we machine-translate the target test sets to en (we refer to this as translate- test). The difference between translate-test and zero-shot, can be thought of as the cross-lingual transfer gap, while the rest of the difference be- tween translate-test and en test performance can be attributed to the concept shift. Due to possible MT errors, the results here represent upper bounds for concept shift and cross-lingual shift, which is References to weather/season References to food References to friendship su The Indian Ocean is sparkling like a Peacock this Christmas season. kn That food is as sweet as Neem jv My friend\u2019s father is like a raden werkudara. kn The weather is also warm like the rainy season. hi Hotel food was like tamarind. hi He guided his friend like Krishna. su The weather looks like you can fry fish on the asphalt. sw His waist is the width of a baobab. sw His friend is abunuwasi. hi Tina and Ravi\u2019s love is like monsoon season. jv The taste of this food is like boiled tempeh. id He asks the help of his friends just like the king of Tanah Djawo Kingdom. Table 7: Translated examples with cultural references specific to regions where these languages are spoken. further discussed in Section 6.1. The concept shift gap is generally greater than the cross-lingual gap. As reported in Ta- ble 8, the concept shift gap is greater than the cross-lingual transfer gap for all languages except Swahili, across all models. This result for sw cor- roborates our findings in Section 4, where we ob- serve that en shares the greatest proportion of ob- ject concepts with sw. Given Swahili\u2019s extremely low-representation in MPLMs (Table 5), and its high concept overlap with English, we cover most of the gap by simply translating sw to en. For Indonesian (id), we observe that zero-shot perfor- mance itself is close to en performance (83.6%) for XLM-R, since id is well-represented in this model (Table 5). Hence, translating to en does not help, and the model needs to be competent in better un- derstanding the cultural references specific to id. In mBERT however, id is poorly represented, and translating to en does help improve performance. Performance increases as model and train- ing data size increase, but moreso for higher resource languages. The smallest model exam- ined, mBERT, has relatively poor performance for all languages, as all languages have < 60% ac- curacy. Hindi and Indonesian, the two highest- resource languages in our dataset, show a high gain in performance when using a larger model, increas- ing to 67.58% and 78.09% accuracy respectively. This is especially true for Indonesian, which has a relatively high amount of training data as shown in Table 5. However, lower resource languages tend to show a more modest gain in performance. 5.2 Few-shot 5.2.1 Few-shot evaluation While it is common to fine-tune MPLMs on En- glish, given its widespread use and availability, several past works have shown how this is sub- optimal (Lin et al., 2019\u037e Debnath et al., 2021) and choosing optimal transfer languages is an important research question in itself (Dhamecha et al., 2021). While the design of an ideal al- location of annotation resources is still unknown, Lauscher et al. (2020) demonstrate the effective- ness of investing in few-shot (5-10) in-language task-specific examples, which provides vast im- provements over the zero-shot setup. We include between 2-50 labelled pairs of sen- tences from each target language, in addition to the English labelled data, for fine-tuning the model. Training details for all models can be found in Ap- pendix E. 5.2.2 Few-shot results Figure 3 presents the effects of few-shot transfer for each language. Generally, the performance gain is modest. This aligns with results from Lauscher et al. (2020), who found that perfor- mance gains were quite small on XNLI. As our task is also an NLI task, we may expect simi- lar improvements. However, we find collecting some cultural examples could disproportionately help low-resource languages. Augmenting with few examples usually does not help much We observed that with a few excep- tions, the increase in accuracy on the test set gained was small (< 1%). This is likely because of the di- versity of facts needed in order to improve perfor- mance. As noted in Section 4.1 and Table 1, this dataset contains many unique cultural references that do not repeat, limiting the utility of seeing a few examples. languages benefit more greatly from augmentation However, there are a few exceptions to this trend. In particular, adding 50 paired Kannada examples to XLM-Rlarge improved performance by 3.83%. Swahili also improves by 1.10% with 50 additional examples for XLM-Rbase, and Sundanese improves by 2.33% with 50 examples for mBERTbase. Lower-resource 5.3 Evaluation of Large Language Models In addition to the three MPLMs we examine in de- tail, we also examine the zero-shot performance of large pretrained language models. We choose to Model Language Zero-shot Performance Translate-test Cross-Lingual Concept Shift Transfer Gap (to EN) Gap XLM-Rlarge en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 81.50 \u00b12.41 67.58 \u00b11.38 78.09 \u00b11.14 60.93 \u00b11.95 58.08 \u00b12.10 60.40 \u00b11.98 58.16 \u00b10.73 - 81.50 \u00b12.41 67.82 \u00b11.52 77.51 \u00b10.91 68.13 \u00b11.66 63.67 \u00b10.98 70.07 \u00b10.92 75.29 \u00b12.05 - 0.00 0.24 -0.58 7.20 5.59 9.67 17.13 - 0.00 13.68 3.99 13.37 17.83 11.43 6.21 - XLM-Rbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 75.26 \u00b10.95 62.48 \u00b10.31 68.88 \u00b10.71 53.67 \u00b10.54 54.67 \u00b11.31 52.41 \u00b11.79 52.73 \u00b11.38 - 75.26 \u00b10.95 63.29 \u00b10.84 66.54 \u00b11.22 58.17 \u00b10.82 57.86 \u00b11.10 61.33 \u00b10.68 65.77 \u00b11.82 - 0.00 0.81 -2.34 4.50 3.20 8.93 13.04 - 0.00 11.97"}, {"question": " What is the effectiveness of investing in few-shot in-language task-specific examples?", "answer": " Investing in few-shot in-language task-specific examples provides vast improvements over the zero-shot setup.", "ref_chunk": "that there can be two contributors to the gap in performance in these seven languages as com- pared to English. First, since our fine-tuning lan- guage is English, there can be a drop in perfor- mance simply due to cross-lingual transfer. Sec- ond, there is a concept shift in these metaphors, as evidenced by our analysis in Section 4. To discern the contribution of both, we machine-translate the target test sets to en (we refer to this as translate- test). The difference between translate-test and zero-shot, can be thought of as the cross-lingual transfer gap, while the rest of the difference be- tween translate-test and en test performance can be attributed to the concept shift. Due to possible MT errors, the results here represent upper bounds for concept shift and cross-lingual shift, which is References to weather/season References to food References to friendship su The Indian Ocean is sparkling like a Peacock this Christmas season. kn That food is as sweet as Neem jv My friend\u2019s father is like a raden werkudara. kn The weather is also warm like the rainy season. hi Hotel food was like tamarind. hi He guided his friend like Krishna. su The weather looks like you can fry fish on the asphalt. sw His waist is the width of a baobab. sw His friend is abunuwasi. hi Tina and Ravi\u2019s love is like monsoon season. jv The taste of this food is like boiled tempeh. id He asks the help of his friends just like the king of Tanah Djawo Kingdom. Table 7: Translated examples with cultural references specific to regions where these languages are spoken. further discussed in Section 6.1. The concept shift gap is generally greater than the cross-lingual gap. As reported in Ta- ble 8, the concept shift gap is greater than the cross-lingual transfer gap for all languages except Swahili, across all models. This result for sw cor- roborates our findings in Section 4, where we ob- serve that en shares the greatest proportion of ob- ject concepts with sw. Given Swahili\u2019s extremely low-representation in MPLMs (Table 5), and its high concept overlap with English, we cover most of the gap by simply translating sw to en. For Indonesian (id), we observe that zero-shot perfor- mance itself is close to en performance (83.6%) for XLM-R, since id is well-represented in this model (Table 5). Hence, translating to en does not help, and the model needs to be competent in better un- derstanding the cultural references specific to id. In mBERT however, id is poorly represented, and translating to en does help improve performance. Performance increases as model and train- ing data size increase, but moreso for higher resource languages. The smallest model exam- ined, mBERT, has relatively poor performance for all languages, as all languages have < 60% ac- curacy. Hindi and Indonesian, the two highest- resource languages in our dataset, show a high gain in performance when using a larger model, increas- ing to 67.58% and 78.09% accuracy respectively. This is especially true for Indonesian, which has a relatively high amount of training data as shown in Table 5. However, lower resource languages tend to show a more modest gain in performance. 5.2 Few-shot 5.2.1 Few-shot evaluation While it is common to fine-tune MPLMs on En- glish, given its widespread use and availability, several past works have shown how this is sub- optimal (Lin et al., 2019\u037e Debnath et al., 2021) and choosing optimal transfer languages is an important research question in itself (Dhamecha et al., 2021). While the design of an ideal al- location of annotation resources is still unknown, Lauscher et al. (2020) demonstrate the effective- ness of investing in few-shot (5-10) in-language task-specific examples, which provides vast im- provements over the zero-shot setup. We include between 2-50 labelled pairs of sen- tences from each target language, in addition to the English labelled data, for fine-tuning the model. Training details for all models can be found in Ap- pendix E. 5.2.2 Few-shot results Figure 3 presents the effects of few-shot transfer for each language. Generally, the performance gain is modest. This aligns with results from Lauscher et al. (2020), who found that perfor- mance gains were quite small on XNLI. As our task is also an NLI task, we may expect simi- lar improvements. However, we find collecting some cultural examples could disproportionately help low-resource languages. Augmenting with few examples usually does not help much We observed that with a few excep- tions, the increase in accuracy on the test set gained was small (< 1%). This is likely because of the di- versity of facts needed in order to improve perfor- mance. As noted in Section 4.1 and Table 1, this dataset contains many unique cultural references that do not repeat, limiting the utility of seeing a few examples. languages benefit more greatly from augmentation However, there are a few exceptions to this trend. In particular, adding 50 paired Kannada examples to XLM-Rlarge improved performance by 3.83%. Swahili also improves by 1.10% with 50 additional examples for XLM-Rbase, and Sundanese improves by 2.33% with 50 examples for mBERTbase. Lower-resource 5.3 Evaluation of Large Language Models In addition to the three MPLMs we examine in de- tail, we also examine the zero-shot performance of large pretrained language models. We choose to Model Language Zero-shot Performance Translate-test Cross-Lingual Concept Shift Transfer Gap (to EN) Gap XLM-Rlarge en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 81.50 \u00b12.41 67.58 \u00b11.38 78.09 \u00b11.14 60.93 \u00b11.95 58.08 \u00b12.10 60.40 \u00b11.98 58.16 \u00b10.73 - 81.50 \u00b12.41 67.82 \u00b11.52 77.51 \u00b10.91 68.13 \u00b11.66 63.67 \u00b10.98 70.07 \u00b10.92 75.29 \u00b12.05 - 0.00 0.24 -0.58 7.20 5.59 9.67 17.13 - 0.00 13.68 3.99 13.37 17.83 11.43 6.21 - XLM-Rbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 75.26 \u00b10.95 62.48 \u00b10.31 68.88 \u00b10.71 53.67 \u00b10.54 54.67 \u00b11.31 52.41 \u00b11.79 52.73 \u00b11.38 - 75.26 \u00b10.95 63.29 \u00b10.84 66.54 \u00b11.22 58.17 \u00b10.82 57.86 \u00b11.10 61.33 \u00b10.68 65.77 \u00b11.82 - 0.00 0.81 -2.34 4.50 3.20 8.93 13.04 - 0.00 11.97"}, {"question": " What was the general trend observed regarding the effects of few-shot transfer for each language?", "answer": " The performance gain from few-shot transfer was generally modest.", "ref_chunk": "that there can be two contributors to the gap in performance in these seven languages as com- pared to English. First, since our fine-tuning lan- guage is English, there can be a drop in perfor- mance simply due to cross-lingual transfer. Sec- ond, there is a concept shift in these metaphors, as evidenced by our analysis in Section 4. To discern the contribution of both, we machine-translate the target test sets to en (we refer to this as translate- test). The difference between translate-test and zero-shot, can be thought of as the cross-lingual transfer gap, while the rest of the difference be- tween translate-test and en test performance can be attributed to the concept shift. Due to possible MT errors, the results here represent upper bounds for concept shift and cross-lingual shift, which is References to weather/season References to food References to friendship su The Indian Ocean is sparkling like a Peacock this Christmas season. kn That food is as sweet as Neem jv My friend\u2019s father is like a raden werkudara. kn The weather is also warm like the rainy season. hi Hotel food was like tamarind. hi He guided his friend like Krishna. su The weather looks like you can fry fish on the asphalt. sw His waist is the width of a baobab. sw His friend is abunuwasi. hi Tina and Ravi\u2019s love is like monsoon season. jv The taste of this food is like boiled tempeh. id He asks the help of his friends just like the king of Tanah Djawo Kingdom. Table 7: Translated examples with cultural references specific to regions where these languages are spoken. further discussed in Section 6.1. The concept shift gap is generally greater than the cross-lingual gap. As reported in Ta- ble 8, the concept shift gap is greater than the cross-lingual transfer gap for all languages except Swahili, across all models. This result for sw cor- roborates our findings in Section 4, where we ob- serve that en shares the greatest proportion of ob- ject concepts with sw. Given Swahili\u2019s extremely low-representation in MPLMs (Table 5), and its high concept overlap with English, we cover most of the gap by simply translating sw to en. For Indonesian (id), we observe that zero-shot perfor- mance itself is close to en performance (83.6%) for XLM-R, since id is well-represented in this model (Table 5). Hence, translating to en does not help, and the model needs to be competent in better un- derstanding the cultural references specific to id. In mBERT however, id is poorly represented, and translating to en does help improve performance. Performance increases as model and train- ing data size increase, but moreso for higher resource languages. The smallest model exam- ined, mBERT, has relatively poor performance for all languages, as all languages have < 60% ac- curacy. Hindi and Indonesian, the two highest- resource languages in our dataset, show a high gain in performance when using a larger model, increas- ing to 67.58% and 78.09% accuracy respectively. This is especially true for Indonesian, which has a relatively high amount of training data as shown in Table 5. However, lower resource languages tend to show a more modest gain in performance. 5.2 Few-shot 5.2.1 Few-shot evaluation While it is common to fine-tune MPLMs on En- glish, given its widespread use and availability, several past works have shown how this is sub- optimal (Lin et al., 2019\u037e Debnath et al., 2021) and choosing optimal transfer languages is an important research question in itself (Dhamecha et al., 2021). While the design of an ideal al- location of annotation resources is still unknown, Lauscher et al. (2020) demonstrate the effective- ness of investing in few-shot (5-10) in-language task-specific examples, which provides vast im- provements over the zero-shot setup. We include between 2-50 labelled pairs of sen- tences from each target language, in addition to the English labelled data, for fine-tuning the model. Training details for all models can be found in Ap- pendix E. 5.2.2 Few-shot results Figure 3 presents the effects of few-shot transfer for each language. Generally, the performance gain is modest. This aligns with results from Lauscher et al. (2020), who found that perfor- mance gains were quite small on XNLI. As our task is also an NLI task, we may expect simi- lar improvements. However, we find collecting some cultural examples could disproportionately help low-resource languages. Augmenting with few examples usually does not help much We observed that with a few excep- tions, the increase in accuracy on the test set gained was small (< 1%). This is likely because of the di- versity of facts needed in order to improve perfor- mance. As noted in Section 4.1 and Table 1, this dataset contains many unique cultural references that do not repeat, limiting the utility of seeing a few examples. languages benefit more greatly from augmentation However, there are a few exceptions to this trend. In particular, adding 50 paired Kannada examples to XLM-Rlarge improved performance by 3.83%. Swahili also improves by 1.10% with 50 additional examples for XLM-Rbase, and Sundanese improves by 2.33% with 50 examples for mBERTbase. Lower-resource 5.3 Evaluation of Large Language Models In addition to the three MPLMs we examine in de- tail, we also examine the zero-shot performance of large pretrained language models. We choose to Model Language Zero-shot Performance Translate-test Cross-Lingual Concept Shift Transfer Gap (to EN) Gap XLM-Rlarge en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 81.50 \u00b12.41 67.58 \u00b11.38 78.09 \u00b11.14 60.93 \u00b11.95 58.08 \u00b12.10 60.40 \u00b11.98 58.16 \u00b10.73 - 81.50 \u00b12.41 67.82 \u00b11.52 77.51 \u00b10.91 68.13 \u00b11.66 63.67 \u00b10.98 70.07 \u00b10.92 75.29 \u00b12.05 - 0.00 0.24 -0.58 7.20 5.59 9.67 17.13 - 0.00 13.68 3.99 13.37 17.83 11.43 6.21 - XLM-Rbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 75.26 \u00b10.95 62.48 \u00b10.31 68.88 \u00b10.71 53.67 \u00b10.54 54.67 \u00b11.31 52.41 \u00b11.79 52.73 \u00b11.38 - 75.26 \u00b10.95 63.29 \u00b10.84 66.54 \u00b11.22 58.17 \u00b10.82 57.86 \u00b11.10 61.33 \u00b10.68 65.77 \u00b11.82 - 0.00 0.81 -2.34 4.50 3.20 8.93 13.04 - 0.00 11.97"}, {"question": " Which languages benefited more greatly from augmentation with additional examples?", "answer": " Kannada, Swahili, and Sundanese benefited more greatly from augmentation with additional examples.", "ref_chunk": "that there can be two contributors to the gap in performance in these seven languages as com- pared to English. First, since our fine-tuning lan- guage is English, there can be a drop in perfor- mance simply due to cross-lingual transfer. Sec- ond, there is a concept shift in these metaphors, as evidenced by our analysis in Section 4. To discern the contribution of both, we machine-translate the target test sets to en (we refer to this as translate- test). The difference between translate-test and zero-shot, can be thought of as the cross-lingual transfer gap, while the rest of the difference be- tween translate-test and en test performance can be attributed to the concept shift. Due to possible MT errors, the results here represent upper bounds for concept shift and cross-lingual shift, which is References to weather/season References to food References to friendship su The Indian Ocean is sparkling like a Peacock this Christmas season. kn That food is as sweet as Neem jv My friend\u2019s father is like a raden werkudara. kn The weather is also warm like the rainy season. hi Hotel food was like tamarind. hi He guided his friend like Krishna. su The weather looks like you can fry fish on the asphalt. sw His waist is the width of a baobab. sw His friend is abunuwasi. hi Tina and Ravi\u2019s love is like monsoon season. jv The taste of this food is like boiled tempeh. id He asks the help of his friends just like the king of Tanah Djawo Kingdom. Table 7: Translated examples with cultural references specific to regions where these languages are spoken. further discussed in Section 6.1. The concept shift gap is generally greater than the cross-lingual gap. As reported in Ta- ble 8, the concept shift gap is greater than the cross-lingual transfer gap for all languages except Swahili, across all models. This result for sw cor- roborates our findings in Section 4, where we ob- serve that en shares the greatest proportion of ob- ject concepts with sw. Given Swahili\u2019s extremely low-representation in MPLMs (Table 5), and its high concept overlap with English, we cover most of the gap by simply translating sw to en. For Indonesian (id), we observe that zero-shot perfor- mance itself is close to en performance (83.6%) for XLM-R, since id is well-represented in this model (Table 5). Hence, translating to en does not help, and the model needs to be competent in better un- derstanding the cultural references specific to id. In mBERT however, id is poorly represented, and translating to en does help improve performance. Performance increases as model and train- ing data size increase, but moreso for higher resource languages. The smallest model exam- ined, mBERT, has relatively poor performance for all languages, as all languages have < 60% ac- curacy. Hindi and Indonesian, the two highest- resource languages in our dataset, show a high gain in performance when using a larger model, increas- ing to 67.58% and 78.09% accuracy respectively. This is especially true for Indonesian, which has a relatively high amount of training data as shown in Table 5. However, lower resource languages tend to show a more modest gain in performance. 5.2 Few-shot 5.2.1 Few-shot evaluation While it is common to fine-tune MPLMs on En- glish, given its widespread use and availability, several past works have shown how this is sub- optimal (Lin et al., 2019\u037e Debnath et al., 2021) and choosing optimal transfer languages is an important research question in itself (Dhamecha et al., 2021). While the design of an ideal al- location of annotation resources is still unknown, Lauscher et al. (2020) demonstrate the effective- ness of investing in few-shot (5-10) in-language task-specific examples, which provides vast im- provements over the zero-shot setup. We include between 2-50 labelled pairs of sen- tences from each target language, in addition to the English labelled data, for fine-tuning the model. Training details for all models can be found in Ap- pendix E. 5.2.2 Few-shot results Figure 3 presents the effects of few-shot transfer for each language. Generally, the performance gain is modest. This aligns with results from Lauscher et al. (2020), who found that perfor- mance gains were quite small on XNLI. As our task is also an NLI task, we may expect simi- lar improvements. However, we find collecting some cultural examples could disproportionately help low-resource languages. Augmenting with few examples usually does not help much We observed that with a few excep- tions, the increase in accuracy on the test set gained was small (< 1%). This is likely because of the di- versity of facts needed in order to improve perfor- mance. As noted in Section 4.1 and Table 1, this dataset contains many unique cultural references that do not repeat, limiting the utility of seeing a few examples. languages benefit more greatly from augmentation However, there are a few exceptions to this trend. In particular, adding 50 paired Kannada examples to XLM-Rlarge improved performance by 3.83%. Swahili also improves by 1.10% with 50 additional examples for XLM-Rbase, and Sundanese improves by 2.33% with 50 examples for mBERTbase. Lower-resource 5.3 Evaluation of Large Language Models In addition to the three MPLMs we examine in de- tail, we also examine the zero-shot performance of large pretrained language models. We choose to Model Language Zero-shot Performance Translate-test Cross-Lingual Concept Shift Transfer Gap (to EN) Gap XLM-Rlarge en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 81.50 \u00b12.41 67.58 \u00b11.38 78.09 \u00b11.14 60.93 \u00b11.95 58.08 \u00b12.10 60.40 \u00b11.98 58.16 \u00b10.73 - 81.50 \u00b12.41 67.82 \u00b11.52 77.51 \u00b10.91 68.13 \u00b11.66 63.67 \u00b10.98 70.07 \u00b10.92 75.29 \u00b12.05 - 0.00 0.24 -0.58 7.20 5.59 9.67 17.13 - 0.00 13.68 3.99 13.37 17.83 11.43 6.21 - XLM-Rbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 75.26 \u00b10.95 62.48 \u00b10.31 68.88 \u00b10.71 53.67 \u00b10.54 54.67 \u00b11.31 52.41 \u00b11.79 52.73 \u00b11.38 - 75.26 \u00b10.95 63.29 \u00b10.84 66.54 \u00b11.22 58.17 \u00b10.82 57.86 \u00b11.10 61.33 \u00b10.68 65.77 \u00b11.82 - 0.00 0.81 -2.34 4.50 3.20 8.93 13.04 - 0.00 11.97"}], "doc_text": "that there can be two contributors to the gap in performance in these seven languages as com- pared to English. First, since our fine-tuning lan- guage is English, there can be a drop in perfor- mance simply due to cross-lingual transfer. Sec- ond, there is a concept shift in these metaphors, as evidenced by our analysis in Section 4. To discern the contribution of both, we machine-translate the target test sets to en (we refer to this as translate- test). The difference between translate-test and zero-shot, can be thought of as the cross-lingual transfer gap, while the rest of the difference be- tween translate-test and en test performance can be attributed to the concept shift. Due to possible MT errors, the results here represent upper bounds for concept shift and cross-lingual shift, which is References to weather/season References to food References to friendship su The Indian Ocean is sparkling like a Peacock this Christmas season. kn That food is as sweet as Neem jv My friend\u2019s father is like a raden werkudara. kn The weather is also warm like the rainy season. hi Hotel food was like tamarind. hi He guided his friend like Krishna. su The weather looks like you can fry fish on the asphalt. sw His waist is the width of a baobab. sw His friend is abunuwasi. hi Tina and Ravi\u2019s love is like monsoon season. jv The taste of this food is like boiled tempeh. id He asks the help of his friends just like the king of Tanah Djawo Kingdom. Table 7: Translated examples with cultural references specific to regions where these languages are spoken. further discussed in Section 6.1. The concept shift gap is generally greater than the cross-lingual gap. As reported in Ta- ble 8, the concept shift gap is greater than the cross-lingual transfer gap for all languages except Swahili, across all models. This result for sw cor- roborates our findings in Section 4, where we ob- serve that en shares the greatest proportion of ob- ject concepts with sw. Given Swahili\u2019s extremely low-representation in MPLMs (Table 5), and its high concept overlap with English, we cover most of the gap by simply translating sw to en. For Indonesian (id), we observe that zero-shot perfor- mance itself is close to en performance (83.6%) for XLM-R, since id is well-represented in this model (Table 5). Hence, translating to en does not help, and the model needs to be competent in better un- derstanding the cultural references specific to id. In mBERT however, id is poorly represented, and translating to en does help improve performance. Performance increases as model and train- ing data size increase, but moreso for higher resource languages. The smallest model exam- ined, mBERT, has relatively poor performance for all languages, as all languages have < 60% ac- curacy. Hindi and Indonesian, the two highest- resource languages in our dataset, show a high gain in performance when using a larger model, increas- ing to 67.58% and 78.09% accuracy respectively. This is especially true for Indonesian, which has a relatively high amount of training data as shown in Table 5. However, lower resource languages tend to show a more modest gain in performance. 5.2 Few-shot 5.2.1 Few-shot evaluation While it is common to fine-tune MPLMs on En- glish, given its widespread use and availability, several past works have shown how this is sub- optimal (Lin et al., 2019\u037e Debnath et al., 2021) and choosing optimal transfer languages is an important research question in itself (Dhamecha et al., 2021). While the design of an ideal al- location of annotation resources is still unknown, Lauscher et al. (2020) demonstrate the effective- ness of investing in few-shot (5-10) in-language task-specific examples, which provides vast im- provements over the zero-shot setup. We include between 2-50 labelled pairs of sen- tences from each target language, in addition to the English labelled data, for fine-tuning the model. Training details for all models can be found in Ap- pendix E. 5.2.2 Few-shot results Figure 3 presents the effects of few-shot transfer for each language. Generally, the performance gain is modest. This aligns with results from Lauscher et al. (2020), who found that perfor- mance gains were quite small on XNLI. As our task is also an NLI task, we may expect simi- lar improvements. However, we find collecting some cultural examples could disproportionately help low-resource languages. Augmenting with few examples usually does not help much We observed that with a few excep- tions, the increase in accuracy on the test set gained was small (< 1%). This is likely because of the di- versity of facts needed in order to improve perfor- mance. As noted in Section 4.1 and Table 1, this dataset contains many unique cultural references that do not repeat, limiting the utility of seeing a few examples. languages benefit more greatly from augmentation However, there are a few exceptions to this trend. In particular, adding 50 paired Kannada examples to XLM-Rlarge improved performance by 3.83%. Swahili also improves by 1.10% with 50 additional examples for XLM-Rbase, and Sundanese improves by 2.33% with 50 examples for mBERTbase. Lower-resource 5.3 Evaluation of Large Language Models In addition to the three MPLMs we examine in de- tail, we also examine the zero-shot performance of large pretrained language models. We choose to Model Language Zero-shot Performance Translate-test Cross-Lingual Concept Shift Transfer Gap (to EN) Gap XLM-Rlarge en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 81.50 \u00b12.41 67.58 \u00b11.38 78.09 \u00b11.14 60.93 \u00b11.95 58.08 \u00b12.10 60.40 \u00b11.98 58.16 \u00b10.73 - 81.50 \u00b12.41 67.82 \u00b11.52 77.51 \u00b10.91 68.13 \u00b11.66 63.67 \u00b10.98 70.07 \u00b10.92 75.29 \u00b12.05 - 0.00 0.24 -0.58 7.20 5.59 9.67 17.13 - 0.00 13.68 3.99 13.37 17.83 11.43 6.21 - XLM-Rbase en\ud835\udc51\ud835\udc52\ud835\udc63 hi id jv kn su sw yo 75.26 \u00b10.95 62.48 \u00b10.31 68.88 \u00b10.71 53.67 \u00b10.54 54.67 \u00b11.31 52.41 \u00b11.79 52.73 \u00b11.38 - 75.26 \u00b10.95 63.29 \u00b10.84 66.54 \u00b11.22 58.17 \u00b10.82 57.86 \u00b11.10 61.33 \u00b10.68 65.77 \u00b11.82 - 0.00 0.81 -2.34 4.50 3.20 8.93 13.04 - 0.00 11.97"}