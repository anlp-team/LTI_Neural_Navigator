{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Active_Retrieval_Augmented_Generation_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are some of the metrics used in the experimental setting described in the text?", "answer": " ROUGE, named entity-based F1, and UniEval", "ref_chunk": "following aspects: academics, history.\u201d Experimen- tal setting details are included in Appendix B. Metrics include ROUGE, named entity-based F1, and UniEval (Zhong et al., 2022) which measures factual consistency. 6 Experimental Results Methods EM F1 No retrieval Single-time retrieval 28.2 39.4 36.8 48.8 Multi-time retrieval Previous-window Previous-sentence Question decomposition FLAREinstruct (ours) FLAREdirect (ours) 43.2 39.0 47.8 42.4 51.0 52.3 49.2 56.4 49.8 59.7 Prec. Rec. 36.5 48.6 38.6 51.5 51.7 48.9 56.1 49.1 59.1 54.5 51.8 58.6 52.5 62.6 We first report overall results across 4 tasks/datasets and compare the performance of FLARE with all the baselines introduced in section 4. We then run ablation experiments to study the efficacy of various design choices of our method. 6.1 Comparison with Baselines Overall results. The overall performance of FLARE and baseline across all tasks/datasets are reported in Figure 4. FLARE outperforms all base- line on all tasks/datasets, indicating that FLARE is a generic method that can effectively retrieve additional information throughout the generation. Among various tasks, multihop QA shows the most significant improvement. This is largely due to the task\u2019s clear definition and specific objective of producing the final answer through a 2-hop rea- soning process, which makes it easier for LMs to generate on-topic output. In contrast, ASQA and WikiAsp are more open-ended, which increases the difficulty of both generation and evaluation. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs to stay on topic. Thorough comparisons with baselines. The per- formance of all baselines on 2WikiMultihopQA are reported in Table 1. FLARE outperforms all baselines by a large margin, which confirms that forward-looking active retrieval is highly effective. Most multi-time retrieval augmented approaches outperform single-time retrieval but with different Table 1: FLARE and baselines on 2WikiMultihopQA. Previous-window (Borgeaud et al., 2022; Ram et al., 2023), previous-sentence (Trivedi et al., 2022), and ques- tion decomposition (Press et al., 2022; Yao et al., 2022) methods are reimplemented for fair comparisons. margins. The improvement of retrieving using the previous sentence is relatively small which we hy- pothesize is mainly because the previous sentence often describes entities or relations different from those in the next sentence in 2WikiMultihopQA. While the previous-window approach might use the first half of a sentence to retrieve information potentially helpful for generating the second half. Among all baselines, the question decomposition approach (Press et al., 2022) achieves the best per- formance. which is not surprising since the in- context exemplars manually annotated with decom- posed sub-questions (Prompt D.2) guide LMs to generate sub-questions that align with the topic/in- tent of future generations. FLARE outperforms this baseline, indicating that manual exemplar an- notation is not necessary for effective future-aware retrieval. The gap between FLAREinstruct and ques- tion decomposition is large, indicating that teaching LMs to generate search queries using task-generic retrieval instructions and exemplars is challenging. We report all metrics for the other datasets in Table 2. FLARE outperforms baselines with re- spect to all metrics. Retrieval using the previ- Datasets Metrics StrategyQA EM ASQA-hint EM D-F1 R-L DR EM D-F1 R-L DR UniEval E-F1 R-L ASQA WikiAsp No retrieval Single-time retrieval 72.9 68.6 33.8 24.2 33.3 28.4 40.0 27.1 34.0 30.4 40.1 32.5 36.4 34.4 43.2 34.8 37.4 36.0 47.1 52.4 14.1 26.4 17.4 26.9 Multi-time retrieval Previous-window Previous-sentence FLARE (ours) 71.2 71.0 77.3 39.9 27.0 34.3 30.4 39.9 27.9 34.3 30.9 41.3 28.2 34.3 31.1 43.7 35.7 37.5 36.6 44.7 35.9 37.5 36.7 46.2 36.7 37.7 37.2 51.8 52.6 53.4 18.1 27.3 17.8 27.2 18.9 27.6 Table 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1. 2WikiMultihopQA EM F1 Prec. Rec. ASQA-hint EM D-F1 R-L DR Previous 39.0 49.2 48.9 51.8 48.8 57.6 57.1 60.5 Next 42.5 34.1 36.9 35.5 45.9 35.7 37.5 36.6 Table 3: A head-to-head comparison between using the previous sentence and the next sentence for retrieval. %steps/sentences with retrieval0.020.040.060.080.00.025.050.075.0100.0 StrategyQA 2WikiMultihopQA #Tokens 16 32 48 All EM F1 43.2 43.6 40.0 39.0 52.3 52.4 49.3 48.5 Prec. Rec. 51.7 52.0 49.0 48.2 54.5 55.0 52.0 51.1 Figure 5: Performance (EM) of FLARE with respect to the percentage of steps/sentences with retrieval on 2WikiMultihopQA and StrategyQA. Table 4: Previous-window approaches using different numbers of tokens as queries. the past hurts the performance, further confirming our hypothesis that previous context might not be relevant to intent of future generations. ous window underperforms single-time retrieval on ASQA, which we hypothesize is because the previous window does not accurately reflect future intent. Since we focus on evaluating factuality, met- rics with an emphasis on factual content (such as EM, Disambig-F1, UniEval) are more reliable than metrics computed over all tokens (ROUGE-L). 6.2 Ablation Study Importance of forward-looking retrieval. We first validate that forward-looking retrieval is more effective than past-context-based retrieval. We run ablation experiments on 2WikiMultihopQA and ASQA-hint comparing retrieval using the previ- ous versus the next sentence. Specifically, both methods retrieve every sentence and directly use the complete previous/next sentence as queries. As shown in Table 3, using the next sentence to retrieve is clearly better than using the previous sentence, confirming our hypothesis. We also run previous-window approaches using different numbers of past tokens as queries. As shown in Table 4, using too many tokens (> 32) in Importance of active retrieval. Next, we inves- tigate how active retrieval threshold \u03b8 affects per- formance. To alter our method from not retrieving to retrieving every sentence, we adjust the confi- dence threshold \u03b8 that determines when to trigger retrieval from 0 to 1. We then calculate the pro- portion of steps/sentences where retrieval is acti- vated, and present the performance based on it. As shown in Figure 5, on 2WikiMultihopQA, the per- formance plateaus when the retrieval percentage exceeds 60%, indicating that retrieval when LMs are confident is not necessary. On StrategyQA, the performance drops when the retrieval percentage exceeds 50%,"}, {"question": " What are the results of the multi-time retrieval approach using FLAREdirect (ours) for question decomposition?", "answer": " 43.2 EM F1, 39.0 Prec. Rec.", "ref_chunk": "following aspects: academics, history.\u201d Experimen- tal setting details are included in Appendix B. Metrics include ROUGE, named entity-based F1, and UniEval (Zhong et al., 2022) which measures factual consistency. 6 Experimental Results Methods EM F1 No retrieval Single-time retrieval 28.2 39.4 36.8 48.8 Multi-time retrieval Previous-window Previous-sentence Question decomposition FLAREinstruct (ours) FLAREdirect (ours) 43.2 39.0 47.8 42.4 51.0 52.3 49.2 56.4 49.8 59.7 Prec. Rec. 36.5 48.6 38.6 51.5 51.7 48.9 56.1 49.1 59.1 54.5 51.8 58.6 52.5 62.6 We first report overall results across 4 tasks/datasets and compare the performance of FLARE with all the baselines introduced in section 4. We then run ablation experiments to study the efficacy of various design choices of our method. 6.1 Comparison with Baselines Overall results. The overall performance of FLARE and baseline across all tasks/datasets are reported in Figure 4. FLARE outperforms all base- line on all tasks/datasets, indicating that FLARE is a generic method that can effectively retrieve additional information throughout the generation. Among various tasks, multihop QA shows the most significant improvement. This is largely due to the task\u2019s clear definition and specific objective of producing the final answer through a 2-hop rea- soning process, which makes it easier for LMs to generate on-topic output. In contrast, ASQA and WikiAsp are more open-ended, which increases the difficulty of both generation and evaluation. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs to stay on topic. Thorough comparisons with baselines. The per- formance of all baselines on 2WikiMultihopQA are reported in Table 1. FLARE outperforms all baselines by a large margin, which confirms that forward-looking active retrieval is highly effective. Most multi-time retrieval augmented approaches outperform single-time retrieval but with different Table 1: FLARE and baselines on 2WikiMultihopQA. Previous-window (Borgeaud et al., 2022; Ram et al., 2023), previous-sentence (Trivedi et al., 2022), and ques- tion decomposition (Press et al., 2022; Yao et al., 2022) methods are reimplemented for fair comparisons. margins. The improvement of retrieving using the previous sentence is relatively small which we hy- pothesize is mainly because the previous sentence often describes entities or relations different from those in the next sentence in 2WikiMultihopQA. While the previous-window approach might use the first half of a sentence to retrieve information potentially helpful for generating the second half. Among all baselines, the question decomposition approach (Press et al., 2022) achieves the best per- formance. which is not surprising since the in- context exemplars manually annotated with decom- posed sub-questions (Prompt D.2) guide LMs to generate sub-questions that align with the topic/in- tent of future generations. FLARE outperforms this baseline, indicating that manual exemplar an- notation is not necessary for effective future-aware retrieval. The gap between FLAREinstruct and ques- tion decomposition is large, indicating that teaching LMs to generate search queries using task-generic retrieval instructions and exemplars is challenging. We report all metrics for the other datasets in Table 2. FLARE outperforms baselines with re- spect to all metrics. Retrieval using the previ- Datasets Metrics StrategyQA EM ASQA-hint EM D-F1 R-L DR EM D-F1 R-L DR UniEval E-F1 R-L ASQA WikiAsp No retrieval Single-time retrieval 72.9 68.6 33.8 24.2 33.3 28.4 40.0 27.1 34.0 30.4 40.1 32.5 36.4 34.4 43.2 34.8 37.4 36.0 47.1 52.4 14.1 26.4 17.4 26.9 Multi-time retrieval Previous-window Previous-sentence FLARE (ours) 71.2 71.0 77.3 39.9 27.0 34.3 30.4 39.9 27.9 34.3 30.9 41.3 28.2 34.3 31.1 43.7 35.7 37.5 36.6 44.7 35.9 37.5 36.7 46.2 36.7 37.7 37.2 51.8 52.6 53.4 18.1 27.3 17.8 27.2 18.9 27.6 Table 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1. 2WikiMultihopQA EM F1 Prec. Rec. ASQA-hint EM D-F1 R-L DR Previous 39.0 49.2 48.9 51.8 48.8 57.6 57.1 60.5 Next 42.5 34.1 36.9 35.5 45.9 35.7 37.5 36.6 Table 3: A head-to-head comparison between using the previous sentence and the next sentence for retrieval. %steps/sentences with retrieval0.020.040.060.080.00.025.050.075.0100.0 StrategyQA 2WikiMultihopQA #Tokens 16 32 48 All EM F1 43.2 43.6 40.0 39.0 52.3 52.4 49.3 48.5 Prec. Rec. 51.7 52.0 49.0 48.2 54.5 55.0 52.0 51.1 Figure 5: Performance (EM) of FLARE with respect to the percentage of steps/sentences with retrieval on 2WikiMultihopQA and StrategyQA. Table 4: Previous-window approaches using different numbers of tokens as queries. the past hurts the performance, further confirming our hypothesis that previous context might not be relevant to intent of future generations. ous window underperforms single-time retrieval on ASQA, which we hypothesize is because the previous window does not accurately reflect future intent. Since we focus on evaluating factuality, met- rics with an emphasis on factual content (such as EM, Disambig-F1, UniEval) are more reliable than metrics computed over all tokens (ROUGE-L). 6.2 Ablation Study Importance of forward-looking retrieval. We first validate that forward-looking retrieval is more effective than past-context-based retrieval. We run ablation experiments on 2WikiMultihopQA and ASQA-hint comparing retrieval using the previ- ous versus the next sentence. Specifically, both methods retrieve every sentence and directly use the complete previous/next sentence as queries. As shown in Table 3, using the next sentence to retrieve is clearly better than using the previous sentence, confirming our hypothesis. We also run previous-window approaches using different numbers of past tokens as queries. As shown in Table 4, using too many tokens (> 32) in Importance of active retrieval. Next, we inves- tigate how active retrieval threshold \u03b8 affects per- formance. To alter our method from not retrieving to retrieving every sentence, we adjust the confi- dence threshold \u03b8 that determines when to trigger retrieval from 0 to 1. We then calculate the pro- portion of steps/sentences where retrieval is acti- vated, and present the performance based on it. As shown in Figure 5, on 2WikiMultihopQA, the per- formance plateaus when the retrieval percentage exceeds 60%, indicating that retrieval when LMs are confident is not necessary. On StrategyQA, the performance drops when the retrieval percentage exceeds 50%,"}, {"question": " How does FLARE perform compared to all baselines across all tasks/datasets?", "answer": " FLARE outperforms all baseline on all tasks/datasets", "ref_chunk": "following aspects: academics, history.\u201d Experimen- tal setting details are included in Appendix B. Metrics include ROUGE, named entity-based F1, and UniEval (Zhong et al., 2022) which measures factual consistency. 6 Experimental Results Methods EM F1 No retrieval Single-time retrieval 28.2 39.4 36.8 48.8 Multi-time retrieval Previous-window Previous-sentence Question decomposition FLAREinstruct (ours) FLAREdirect (ours) 43.2 39.0 47.8 42.4 51.0 52.3 49.2 56.4 49.8 59.7 Prec. Rec. 36.5 48.6 38.6 51.5 51.7 48.9 56.1 49.1 59.1 54.5 51.8 58.6 52.5 62.6 We first report overall results across 4 tasks/datasets and compare the performance of FLARE with all the baselines introduced in section 4. We then run ablation experiments to study the efficacy of various design choices of our method. 6.1 Comparison with Baselines Overall results. The overall performance of FLARE and baseline across all tasks/datasets are reported in Figure 4. FLARE outperforms all base- line on all tasks/datasets, indicating that FLARE is a generic method that can effectively retrieve additional information throughout the generation. Among various tasks, multihop QA shows the most significant improvement. This is largely due to the task\u2019s clear definition and specific objective of producing the final answer through a 2-hop rea- soning process, which makes it easier for LMs to generate on-topic output. In contrast, ASQA and WikiAsp are more open-ended, which increases the difficulty of both generation and evaluation. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs to stay on topic. Thorough comparisons with baselines. The per- formance of all baselines on 2WikiMultihopQA are reported in Table 1. FLARE outperforms all baselines by a large margin, which confirms that forward-looking active retrieval is highly effective. Most multi-time retrieval augmented approaches outperform single-time retrieval but with different Table 1: FLARE and baselines on 2WikiMultihopQA. Previous-window (Borgeaud et al., 2022; Ram et al., 2023), previous-sentence (Trivedi et al., 2022), and ques- tion decomposition (Press et al., 2022; Yao et al., 2022) methods are reimplemented for fair comparisons. margins. The improvement of retrieving using the previous sentence is relatively small which we hy- pothesize is mainly because the previous sentence often describes entities or relations different from those in the next sentence in 2WikiMultihopQA. While the previous-window approach might use the first half of a sentence to retrieve information potentially helpful for generating the second half. Among all baselines, the question decomposition approach (Press et al., 2022) achieves the best per- formance. which is not surprising since the in- context exemplars manually annotated with decom- posed sub-questions (Prompt D.2) guide LMs to generate sub-questions that align with the topic/in- tent of future generations. FLARE outperforms this baseline, indicating that manual exemplar an- notation is not necessary for effective future-aware retrieval. The gap between FLAREinstruct and ques- tion decomposition is large, indicating that teaching LMs to generate search queries using task-generic retrieval instructions and exemplars is challenging. We report all metrics for the other datasets in Table 2. FLARE outperforms baselines with re- spect to all metrics. Retrieval using the previ- Datasets Metrics StrategyQA EM ASQA-hint EM D-F1 R-L DR EM D-F1 R-L DR UniEval E-F1 R-L ASQA WikiAsp No retrieval Single-time retrieval 72.9 68.6 33.8 24.2 33.3 28.4 40.0 27.1 34.0 30.4 40.1 32.5 36.4 34.4 43.2 34.8 37.4 36.0 47.1 52.4 14.1 26.4 17.4 26.9 Multi-time retrieval Previous-window Previous-sentence FLARE (ours) 71.2 71.0 77.3 39.9 27.0 34.3 30.4 39.9 27.9 34.3 30.9 41.3 28.2 34.3 31.1 43.7 35.7 37.5 36.6 44.7 35.9 37.5 36.7 46.2 36.7 37.7 37.2 51.8 52.6 53.4 18.1 27.3 17.8 27.2 18.9 27.6 Table 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1. 2WikiMultihopQA EM F1 Prec. Rec. ASQA-hint EM D-F1 R-L DR Previous 39.0 49.2 48.9 51.8 48.8 57.6 57.1 60.5 Next 42.5 34.1 36.9 35.5 45.9 35.7 37.5 36.6 Table 3: A head-to-head comparison between using the previous sentence and the next sentence for retrieval. %steps/sentences with retrieval0.020.040.060.080.00.025.050.075.0100.0 StrategyQA 2WikiMultihopQA #Tokens 16 32 48 All EM F1 43.2 43.6 40.0 39.0 52.3 52.4 49.3 48.5 Prec. Rec. 51.7 52.0 49.0 48.2 54.5 55.0 52.0 51.1 Figure 5: Performance (EM) of FLARE with respect to the percentage of steps/sentences with retrieval on 2WikiMultihopQA and StrategyQA. Table 4: Previous-window approaches using different numbers of tokens as queries. the past hurts the performance, further confirming our hypothesis that previous context might not be relevant to intent of future generations. ous window underperforms single-time retrieval on ASQA, which we hypothesize is because the previous window does not accurately reflect future intent. Since we focus on evaluating factuality, met- rics with an emphasis on factual content (such as EM, Disambig-F1, UniEval) are more reliable than metrics computed over all tokens (ROUGE-L). 6.2 Ablation Study Importance of forward-looking retrieval. We first validate that forward-looking retrieval is more effective than past-context-based retrieval. We run ablation experiments on 2WikiMultihopQA and ASQA-hint comparing retrieval using the previ- ous versus the next sentence. Specifically, both methods retrieve every sentence and directly use the complete previous/next sentence as queries. As shown in Table 3, using the next sentence to retrieve is clearly better than using the previous sentence, confirming our hypothesis. We also run previous-window approaches using different numbers of past tokens as queries. As shown in Table 4, using too many tokens (> 32) in Importance of active retrieval. Next, we inves- tigate how active retrieval threshold \u03b8 affects per- formance. To alter our method from not retrieving to retrieving every sentence, we adjust the confi- dence threshold \u03b8 that determines when to trigger retrieval from 0 to 1. We then calculate the pro- portion of steps/sentences where retrieval is acti- vated, and present the performance based on it. As shown in Figure 5, on 2WikiMultihopQA, the per- formance plateaus when the retrieval percentage exceeds 60%, indicating that retrieval when LMs are confident is not necessary. On StrategyQA, the performance drops when the retrieval percentage exceeds 50%,"}, {"question": " What is the main reason for the significant improvement shown in the multihop QA task with FLARE?", "answer": " The task has a clear definition and specific objective of producing the final answer through a 2-hop reasoning process", "ref_chunk": "following aspects: academics, history.\u201d Experimen- tal setting details are included in Appendix B. Metrics include ROUGE, named entity-based F1, and UniEval (Zhong et al., 2022) which measures factual consistency. 6 Experimental Results Methods EM F1 No retrieval Single-time retrieval 28.2 39.4 36.8 48.8 Multi-time retrieval Previous-window Previous-sentence Question decomposition FLAREinstruct (ours) FLAREdirect (ours) 43.2 39.0 47.8 42.4 51.0 52.3 49.2 56.4 49.8 59.7 Prec. Rec. 36.5 48.6 38.6 51.5 51.7 48.9 56.1 49.1 59.1 54.5 51.8 58.6 52.5 62.6 We first report overall results across 4 tasks/datasets and compare the performance of FLARE with all the baselines introduced in section 4. We then run ablation experiments to study the efficacy of various design choices of our method. 6.1 Comparison with Baselines Overall results. The overall performance of FLARE and baseline across all tasks/datasets are reported in Figure 4. FLARE outperforms all base- line on all tasks/datasets, indicating that FLARE is a generic method that can effectively retrieve additional information throughout the generation. Among various tasks, multihop QA shows the most significant improvement. This is largely due to the task\u2019s clear definition and specific objective of producing the final answer through a 2-hop rea- soning process, which makes it easier for LMs to generate on-topic output. In contrast, ASQA and WikiAsp are more open-ended, which increases the difficulty of both generation and evaluation. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs to stay on topic. Thorough comparisons with baselines. The per- formance of all baselines on 2WikiMultihopQA are reported in Table 1. FLARE outperforms all baselines by a large margin, which confirms that forward-looking active retrieval is highly effective. Most multi-time retrieval augmented approaches outperform single-time retrieval but with different Table 1: FLARE and baselines on 2WikiMultihopQA. Previous-window (Borgeaud et al., 2022; Ram et al., 2023), previous-sentence (Trivedi et al., 2022), and ques- tion decomposition (Press et al., 2022; Yao et al., 2022) methods are reimplemented for fair comparisons. margins. The improvement of retrieving using the previous sentence is relatively small which we hy- pothesize is mainly because the previous sentence often describes entities or relations different from those in the next sentence in 2WikiMultihopQA. While the previous-window approach might use the first half of a sentence to retrieve information potentially helpful for generating the second half. Among all baselines, the question decomposition approach (Press et al., 2022) achieves the best per- formance. which is not surprising since the in- context exemplars manually annotated with decom- posed sub-questions (Prompt D.2) guide LMs to generate sub-questions that align with the topic/in- tent of future generations. FLARE outperforms this baseline, indicating that manual exemplar an- notation is not necessary for effective future-aware retrieval. The gap between FLAREinstruct and ques- tion decomposition is large, indicating that teaching LMs to generate search queries using task-generic retrieval instructions and exemplars is challenging. We report all metrics for the other datasets in Table 2. FLARE outperforms baselines with re- spect to all metrics. Retrieval using the previ- Datasets Metrics StrategyQA EM ASQA-hint EM D-F1 R-L DR EM D-F1 R-L DR UniEval E-F1 R-L ASQA WikiAsp No retrieval Single-time retrieval 72.9 68.6 33.8 24.2 33.3 28.4 40.0 27.1 34.0 30.4 40.1 32.5 36.4 34.4 43.2 34.8 37.4 36.0 47.1 52.4 14.1 26.4 17.4 26.9 Multi-time retrieval Previous-window Previous-sentence FLARE (ours) 71.2 71.0 77.3 39.9 27.0 34.3 30.4 39.9 27.9 34.3 30.9 41.3 28.2 34.3 31.1 43.7 35.7 37.5 36.6 44.7 35.9 37.5 36.7 46.2 36.7 37.7 37.2 51.8 52.6 53.4 18.1 27.3 17.8 27.2 18.9 27.6 Table 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1. 2WikiMultihopQA EM F1 Prec. Rec. ASQA-hint EM D-F1 R-L DR Previous 39.0 49.2 48.9 51.8 48.8 57.6 57.1 60.5 Next 42.5 34.1 36.9 35.5 45.9 35.7 37.5 36.6 Table 3: A head-to-head comparison between using the previous sentence and the next sentence for retrieval. %steps/sentences with retrieval0.020.040.060.080.00.025.050.075.0100.0 StrategyQA 2WikiMultihopQA #Tokens 16 32 48 All EM F1 43.2 43.6 40.0 39.0 52.3 52.4 49.3 48.5 Prec. Rec. 51.7 52.0 49.0 48.2 54.5 55.0 52.0 51.1 Figure 5: Performance (EM) of FLARE with respect to the percentage of steps/sentences with retrieval on 2WikiMultihopQA and StrategyQA. Table 4: Previous-window approaches using different numbers of tokens as queries. the past hurts the performance, further confirming our hypothesis that previous context might not be relevant to intent of future generations. ous window underperforms single-time retrieval on ASQA, which we hypothesize is because the previous window does not accurately reflect future intent. Since we focus on evaluating factuality, met- rics with an emphasis on factual content (such as EM, Disambig-F1, UniEval) are more reliable than metrics computed over all tokens (ROUGE-L). 6.2 Ablation Study Importance of forward-looking retrieval. We first validate that forward-looking retrieval is more effective than past-context-based retrieval. We run ablation experiments on 2WikiMultihopQA and ASQA-hint comparing retrieval using the previ- ous versus the next sentence. Specifically, both methods retrieve every sentence and directly use the complete previous/next sentence as queries. As shown in Table 3, using the next sentence to retrieve is clearly better than using the previous sentence, confirming our hypothesis. We also run previous-window approaches using different numbers of past tokens as queries. As shown in Table 4, using too many tokens (> 32) in Importance of active retrieval. Next, we inves- tigate how active retrieval threshold \u03b8 affects per- formance. To alter our method from not retrieving to retrieving every sentence, we adjust the confi- dence threshold \u03b8 that determines when to trigger retrieval from 0 to 1. We then calculate the pro- portion of steps/sentences where retrieval is acti- vated, and present the performance based on it. As shown in Figure 5, on 2WikiMultihopQA, the per- formance plateaus when the retrieval percentage exceeds 60%, indicating that retrieval when LMs are confident is not necessary. On StrategyQA, the performance drops when the retrieval percentage exceeds 50%,"}, {"question": " Why does FLARE show a larger improvement on ASQA-hint compared to ASQA?", "answer": " Identifying ambiguous aspects is challenging even for humans, providing a generic hint helps LMs to stay on topic", "ref_chunk": "following aspects: academics, history.\u201d Experimen- tal setting details are included in Appendix B. Metrics include ROUGE, named entity-based F1, and UniEval (Zhong et al., 2022) which measures factual consistency. 6 Experimental Results Methods EM F1 No retrieval Single-time retrieval 28.2 39.4 36.8 48.8 Multi-time retrieval Previous-window Previous-sentence Question decomposition FLAREinstruct (ours) FLAREdirect (ours) 43.2 39.0 47.8 42.4 51.0 52.3 49.2 56.4 49.8 59.7 Prec. Rec. 36.5 48.6 38.6 51.5 51.7 48.9 56.1 49.1 59.1 54.5 51.8 58.6 52.5 62.6 We first report overall results across 4 tasks/datasets and compare the performance of FLARE with all the baselines introduced in section 4. We then run ablation experiments to study the efficacy of various design choices of our method. 6.1 Comparison with Baselines Overall results. The overall performance of FLARE and baseline across all tasks/datasets are reported in Figure 4. FLARE outperforms all base- line on all tasks/datasets, indicating that FLARE is a generic method that can effectively retrieve additional information throughout the generation. Among various tasks, multihop QA shows the most significant improvement. This is largely due to the task\u2019s clear definition and specific objective of producing the final answer through a 2-hop rea- soning process, which makes it easier for LMs to generate on-topic output. In contrast, ASQA and WikiAsp are more open-ended, which increases the difficulty of both generation and evaluation. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs to stay on topic. Thorough comparisons with baselines. The per- formance of all baselines on 2WikiMultihopQA are reported in Table 1. FLARE outperforms all baselines by a large margin, which confirms that forward-looking active retrieval is highly effective. Most multi-time retrieval augmented approaches outperform single-time retrieval but with different Table 1: FLARE and baselines on 2WikiMultihopQA. Previous-window (Borgeaud et al., 2022; Ram et al., 2023), previous-sentence (Trivedi et al., 2022), and ques- tion decomposition (Press et al., 2022; Yao et al., 2022) methods are reimplemented for fair comparisons. margins. The improvement of retrieving using the previous sentence is relatively small which we hy- pothesize is mainly because the previous sentence often describes entities or relations different from those in the next sentence in 2WikiMultihopQA. While the previous-window approach might use the first half of a sentence to retrieve information potentially helpful for generating the second half. Among all baselines, the question decomposition approach (Press et al., 2022) achieves the best per- formance. which is not surprising since the in- context exemplars manually annotated with decom- posed sub-questions (Prompt D.2) guide LMs to generate sub-questions that align with the topic/in- tent of future generations. FLARE outperforms this baseline, indicating that manual exemplar an- notation is not necessary for effective future-aware retrieval. The gap between FLAREinstruct and ques- tion decomposition is large, indicating that teaching LMs to generate search queries using task-generic retrieval instructions and exemplars is challenging. We report all metrics for the other datasets in Table 2. FLARE outperforms baselines with re- spect to all metrics. Retrieval using the previ- Datasets Metrics StrategyQA EM ASQA-hint EM D-F1 R-L DR EM D-F1 R-L DR UniEval E-F1 R-L ASQA WikiAsp No retrieval Single-time retrieval 72.9 68.6 33.8 24.2 33.3 28.4 40.0 27.1 34.0 30.4 40.1 32.5 36.4 34.4 43.2 34.8 37.4 36.0 47.1 52.4 14.1 26.4 17.4 26.9 Multi-time retrieval Previous-window Previous-sentence FLARE (ours) 71.2 71.0 77.3 39.9 27.0 34.3 30.4 39.9 27.9 34.3 30.9 41.3 28.2 34.3 31.1 43.7 35.7 37.5 36.6 44.7 35.9 37.5 36.7 46.2 36.7 37.7 37.2 51.8 52.6 53.4 18.1 27.3 17.8 27.2 18.9 27.6 Table 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1. 2WikiMultihopQA EM F1 Prec. Rec. ASQA-hint EM D-F1 R-L DR Previous 39.0 49.2 48.9 51.8 48.8 57.6 57.1 60.5 Next 42.5 34.1 36.9 35.5 45.9 35.7 37.5 36.6 Table 3: A head-to-head comparison between using the previous sentence and the next sentence for retrieval. %steps/sentences with retrieval0.020.040.060.080.00.025.050.075.0100.0 StrategyQA 2WikiMultihopQA #Tokens 16 32 48 All EM F1 43.2 43.6 40.0 39.0 52.3 52.4 49.3 48.5 Prec. Rec. 51.7 52.0 49.0 48.2 54.5 55.0 52.0 51.1 Figure 5: Performance (EM) of FLARE with respect to the percentage of steps/sentences with retrieval on 2WikiMultihopQA and StrategyQA. Table 4: Previous-window approaches using different numbers of tokens as queries. the past hurts the performance, further confirming our hypothesis that previous context might not be relevant to intent of future generations. ous window underperforms single-time retrieval on ASQA, which we hypothesize is because the previous window does not accurately reflect future intent. Since we focus on evaluating factuality, met- rics with an emphasis on factual content (such as EM, Disambig-F1, UniEval) are more reliable than metrics computed over all tokens (ROUGE-L). 6.2 Ablation Study Importance of forward-looking retrieval. We first validate that forward-looking retrieval is more effective than past-context-based retrieval. We run ablation experiments on 2WikiMultihopQA and ASQA-hint comparing retrieval using the previ- ous versus the next sentence. Specifically, both methods retrieve every sentence and directly use the complete previous/next sentence as queries. As shown in Table 3, using the next sentence to retrieve is clearly better than using the previous sentence, confirming our hypothesis. We also run previous-window approaches using different numbers of past tokens as queries. As shown in Table 4, using too many tokens (> 32) in Importance of active retrieval. Next, we inves- tigate how active retrieval threshold \u03b8 affects per- formance. To alter our method from not retrieving to retrieving every sentence, we adjust the confi- dence threshold \u03b8 that determines when to trigger retrieval from 0 to 1. We then calculate the pro- portion of steps/sentences where retrieval is acti- vated, and present the performance based on it. As shown in Figure 5, on 2WikiMultihopQA, the per- formance plateaus when the retrieval percentage exceeds 60%, indicating that retrieval when LMs are confident is not necessary. On StrategyQA, the performance drops when the retrieval percentage exceeds 50%,"}, {"question": " Which of the baselines achieves the best performance on 2WikiMultihopQA?", "answer": " Question decomposition", "ref_chunk": "following aspects: academics, history.\u201d Experimen- tal setting details are included in Appendix B. Metrics include ROUGE, named entity-based F1, and UniEval (Zhong et al., 2022) which measures factual consistency. 6 Experimental Results Methods EM F1 No retrieval Single-time retrieval 28.2 39.4 36.8 48.8 Multi-time retrieval Previous-window Previous-sentence Question decomposition FLAREinstruct (ours) FLAREdirect (ours) 43.2 39.0 47.8 42.4 51.0 52.3 49.2 56.4 49.8 59.7 Prec. Rec. 36.5 48.6 38.6 51.5 51.7 48.9 56.1 49.1 59.1 54.5 51.8 58.6 52.5 62.6 We first report overall results across 4 tasks/datasets and compare the performance of FLARE with all the baselines introduced in section 4. We then run ablation experiments to study the efficacy of various design choices of our method. 6.1 Comparison with Baselines Overall results. The overall performance of FLARE and baseline across all tasks/datasets are reported in Figure 4. FLARE outperforms all base- line on all tasks/datasets, indicating that FLARE is a generic method that can effectively retrieve additional information throughout the generation. Among various tasks, multihop QA shows the most significant improvement. This is largely due to the task\u2019s clear definition and specific objective of producing the final answer through a 2-hop rea- soning process, which makes it easier for LMs to generate on-topic output. In contrast, ASQA and WikiAsp are more open-ended, which increases the difficulty of both generation and evaluation. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs to stay on topic. Thorough comparisons with baselines. The per- formance of all baselines on 2WikiMultihopQA are reported in Table 1. FLARE outperforms all baselines by a large margin, which confirms that forward-looking active retrieval is highly effective. Most multi-time retrieval augmented approaches outperform single-time retrieval but with different Table 1: FLARE and baselines on 2WikiMultihopQA. Previous-window (Borgeaud et al., 2022; Ram et al., 2023), previous-sentence (Trivedi et al., 2022), and ques- tion decomposition (Press et al., 2022; Yao et al., 2022) methods are reimplemented for fair comparisons. margins. The improvement of retrieving using the previous sentence is relatively small which we hy- pothesize is mainly because the previous sentence often describes entities or relations different from those in the next sentence in 2WikiMultihopQA. While the previous-window approach might use the first half of a sentence to retrieve information potentially helpful for generating the second half. Among all baselines, the question decomposition approach (Press et al., 2022) achieves the best per- formance. which is not surprising since the in- context exemplars manually annotated with decom- posed sub-questions (Prompt D.2) guide LMs to generate sub-questions that align with the topic/in- tent of future generations. FLARE outperforms this baseline, indicating that manual exemplar an- notation is not necessary for effective future-aware retrieval. The gap between FLAREinstruct and ques- tion decomposition is large, indicating that teaching LMs to generate search queries using task-generic retrieval instructions and exemplars is challenging. We report all metrics for the other datasets in Table 2. FLARE outperforms baselines with re- spect to all metrics. Retrieval using the previ- Datasets Metrics StrategyQA EM ASQA-hint EM D-F1 R-L DR EM D-F1 R-L DR UniEval E-F1 R-L ASQA WikiAsp No retrieval Single-time retrieval 72.9 68.6 33.8 24.2 33.3 28.4 40.0 27.1 34.0 30.4 40.1 32.5 36.4 34.4 43.2 34.8 37.4 36.0 47.1 52.4 14.1 26.4 17.4 26.9 Multi-time retrieval Previous-window Previous-sentence FLARE (ours) 71.2 71.0 77.3 39.9 27.0 34.3 30.4 39.9 27.9 34.3 30.9 41.3 28.2 34.3 31.1 43.7 35.7 37.5 36.6 44.7 35.9 37.5 36.7 46.2 36.7 37.7 37.2 51.8 52.6 53.4 18.1 27.3 17.8 27.2 18.9 27.6 Table 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1. 2WikiMultihopQA EM F1 Prec. Rec. ASQA-hint EM D-F1 R-L DR Previous 39.0 49.2 48.9 51.8 48.8 57.6 57.1 60.5 Next 42.5 34.1 36.9 35.5 45.9 35.7 37.5 36.6 Table 3: A head-to-head comparison between using the previous sentence and the next sentence for retrieval. %steps/sentences with retrieval0.020.040.060.080.00.025.050.075.0100.0 StrategyQA 2WikiMultihopQA #Tokens 16 32 48 All EM F1 43.2 43.6 40.0 39.0 52.3 52.4 49.3 48.5 Prec. Rec. 51.7 52.0 49.0 48.2 54.5 55.0 52.0 51.1 Figure 5: Performance (EM) of FLARE with respect to the percentage of steps/sentences with retrieval on 2WikiMultihopQA and StrategyQA. Table 4: Previous-window approaches using different numbers of tokens as queries. the past hurts the performance, further confirming our hypothesis that previous context might not be relevant to intent of future generations. ous window underperforms single-time retrieval on ASQA, which we hypothesize is because the previous window does not accurately reflect future intent. Since we focus on evaluating factuality, met- rics with an emphasis on factual content (such as EM, Disambig-F1, UniEval) are more reliable than metrics computed over all tokens (ROUGE-L). 6.2 Ablation Study Importance of forward-looking retrieval. We first validate that forward-looking retrieval is more effective than past-context-based retrieval. We run ablation experiments on 2WikiMultihopQA and ASQA-hint comparing retrieval using the previ- ous versus the next sentence. Specifically, both methods retrieve every sentence and directly use the complete previous/next sentence as queries. As shown in Table 3, using the next sentence to retrieve is clearly better than using the previous sentence, confirming our hypothesis. We also run previous-window approaches using different numbers of past tokens as queries. As shown in Table 4, using too many tokens (> 32) in Importance of active retrieval. Next, we inves- tigate how active retrieval threshold \u03b8 affects per- formance. To alter our method from not retrieving to retrieving every sentence, we adjust the confi- dence threshold \u03b8 that determines when to trigger retrieval from 0 to 1. We then calculate the pro- portion of steps/sentences where retrieval is acti- vated, and present the performance based on it. As shown in Figure 5, on 2WikiMultihopQA, the per- formance plateaus when the retrieval percentage exceeds 60%, indicating that retrieval when LMs are confident is not necessary. On StrategyQA, the performance drops when the retrieval percentage exceeds 50%,"}, {"question": " Why does FLARE outperform the question decomposition baseline?", "answer": " Manual exemplar annotation is not necessary for effective future-aware retrieval", "ref_chunk": "following aspects: academics, history.\u201d Experimen- tal setting details are included in Appendix B. Metrics include ROUGE, named entity-based F1, and UniEval (Zhong et al., 2022) which measures factual consistency. 6 Experimental Results Methods EM F1 No retrieval Single-time retrieval 28.2 39.4 36.8 48.8 Multi-time retrieval Previous-window Previous-sentence Question decomposition FLAREinstruct (ours) FLAREdirect (ours) 43.2 39.0 47.8 42.4 51.0 52.3 49.2 56.4 49.8 59.7 Prec. Rec. 36.5 48.6 38.6 51.5 51.7 48.9 56.1 49.1 59.1 54.5 51.8 58.6 52.5 62.6 We first report overall results across 4 tasks/datasets and compare the performance of FLARE with all the baselines introduced in section 4. We then run ablation experiments to study the efficacy of various design choices of our method. 6.1 Comparison with Baselines Overall results. The overall performance of FLARE and baseline across all tasks/datasets are reported in Figure 4. FLARE outperforms all base- line on all tasks/datasets, indicating that FLARE is a generic method that can effectively retrieve additional information throughout the generation. Among various tasks, multihop QA shows the most significant improvement. This is largely due to the task\u2019s clear definition and specific objective of producing the final answer through a 2-hop rea- soning process, which makes it easier for LMs to generate on-topic output. In contrast, ASQA and WikiAsp are more open-ended, which increases the difficulty of both generation and evaluation. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs to stay on topic. Thorough comparisons with baselines. The per- formance of all baselines on 2WikiMultihopQA are reported in Table 1. FLARE outperforms all baselines by a large margin, which confirms that forward-looking active retrieval is highly effective. Most multi-time retrieval augmented approaches outperform single-time retrieval but with different Table 1: FLARE and baselines on 2WikiMultihopQA. Previous-window (Borgeaud et al., 2022; Ram et al., 2023), previous-sentence (Trivedi et al., 2022), and ques- tion decomposition (Press et al., 2022; Yao et al., 2022) methods are reimplemented for fair comparisons. margins. The improvement of retrieving using the previous sentence is relatively small which we hy- pothesize is mainly because the previous sentence often describes entities or relations different from those in the next sentence in 2WikiMultihopQA. While the previous-window approach might use the first half of a sentence to retrieve information potentially helpful for generating the second half. Among all baselines, the question decomposition approach (Press et al., 2022) achieves the best per- formance. which is not surprising since the in- context exemplars manually annotated with decom- posed sub-questions (Prompt D.2) guide LMs to generate sub-questions that align with the topic/in- tent of future generations. FLARE outperforms this baseline, indicating that manual exemplar an- notation is not necessary for effective future-aware retrieval. The gap between FLAREinstruct and ques- tion decomposition is large, indicating that teaching LMs to generate search queries using task-generic retrieval instructions and exemplars is challenging. We report all metrics for the other datasets in Table 2. FLARE outperforms baselines with re- spect to all metrics. Retrieval using the previ- Datasets Metrics StrategyQA EM ASQA-hint EM D-F1 R-L DR EM D-F1 R-L DR UniEval E-F1 R-L ASQA WikiAsp No retrieval Single-time retrieval 72.9 68.6 33.8 24.2 33.3 28.4 40.0 27.1 34.0 30.4 40.1 32.5 36.4 34.4 43.2 34.8 37.4 36.0 47.1 52.4 14.1 26.4 17.4 26.9 Multi-time retrieval Previous-window Previous-sentence FLARE (ours) 71.2 71.0 77.3 39.9 27.0 34.3 30.4 39.9 27.9 34.3 30.9 41.3 28.2 34.3 31.1 43.7 35.7 37.5 36.6 44.7 35.9 37.5 36.7 46.2 36.7 37.7 37.2 51.8 52.6 53.4 18.1 27.3 17.8 27.2 18.9 27.6 Table 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1. 2WikiMultihopQA EM F1 Prec. Rec. ASQA-hint EM D-F1 R-L DR Previous 39.0 49.2 48.9 51.8 48.8 57.6 57.1 60.5 Next 42.5 34.1 36.9 35.5 45.9 35.7 37.5 36.6 Table 3: A head-to-head comparison between using the previous sentence and the next sentence for retrieval. %steps/sentences with retrieval0.020.040.060.080.00.025.050.075.0100.0 StrategyQA 2WikiMultihopQA #Tokens 16 32 48 All EM F1 43.2 43.6 40.0 39.0 52.3 52.4 49.3 48.5 Prec. Rec. 51.7 52.0 49.0 48.2 54.5 55.0 52.0 51.1 Figure 5: Performance (EM) of FLARE with respect to the percentage of steps/sentences with retrieval on 2WikiMultihopQA and StrategyQA. Table 4: Previous-window approaches using different numbers of tokens as queries. the past hurts the performance, further confirming our hypothesis that previous context might not be relevant to intent of future generations. ous window underperforms single-time retrieval on ASQA, which we hypothesize is because the previous window does not accurately reflect future intent. Since we focus on evaluating factuality, met- rics with an emphasis on factual content (such as EM, Disambig-F1, UniEval) are more reliable than metrics computed over all tokens (ROUGE-L). 6.2 Ablation Study Importance of forward-looking retrieval. We first validate that forward-looking retrieval is more effective than past-context-based retrieval. We run ablation experiments on 2WikiMultihopQA and ASQA-hint comparing retrieval using the previ- ous versus the next sentence. Specifically, both methods retrieve every sentence and directly use the complete previous/next sentence as queries. As shown in Table 3, using the next sentence to retrieve is clearly better than using the previous sentence, confirming our hypothesis. We also run previous-window approaches using different numbers of past tokens as queries. As shown in Table 4, using too many tokens (> 32) in Importance of active retrieval. Next, we inves- tigate how active retrieval threshold \u03b8 affects per- formance. To alter our method from not retrieving to retrieving every sentence, we adjust the confi- dence threshold \u03b8 that determines when to trigger retrieval from 0 to 1. We then calculate the pro- portion of steps/sentences where retrieval is acti- vated, and present the performance based on it. As shown in Figure 5, on 2WikiMultihopQA, the per- formance plateaus when the retrieval percentage exceeds 60%, indicating that retrieval when LMs are confident is not necessary. On StrategyQA, the performance drops when the retrieval percentage exceeds 50%,"}, {"question": " How does performance of the previous-sentence retrieval method compare to using the previous window method in 2WikiMultihopQA?", "answer": " The improvement of retrieving using the previous sentence is relatively small", "ref_chunk": "following aspects: academics, history.\u201d Experimen- tal setting details are included in Appendix B. Metrics include ROUGE, named entity-based F1, and UniEval (Zhong et al., 2022) which measures factual consistency. 6 Experimental Results Methods EM F1 No retrieval Single-time retrieval 28.2 39.4 36.8 48.8 Multi-time retrieval Previous-window Previous-sentence Question decomposition FLAREinstruct (ours) FLAREdirect (ours) 43.2 39.0 47.8 42.4 51.0 52.3 49.2 56.4 49.8 59.7 Prec. Rec. 36.5 48.6 38.6 51.5 51.7 48.9 56.1 49.1 59.1 54.5 51.8 58.6 52.5 62.6 We first report overall results across 4 tasks/datasets and compare the performance of FLARE with all the baselines introduced in section 4. We then run ablation experiments to study the efficacy of various design choices of our method. 6.1 Comparison with Baselines Overall results. The overall performance of FLARE and baseline across all tasks/datasets are reported in Figure 4. FLARE outperforms all base- line on all tasks/datasets, indicating that FLARE is a generic method that can effectively retrieve additional information throughout the generation. Among various tasks, multihop QA shows the most significant improvement. This is largely due to the task\u2019s clear definition and specific objective of producing the final answer through a 2-hop rea- soning process, which makes it easier for LMs to generate on-topic output. In contrast, ASQA and WikiAsp are more open-ended, which increases the difficulty of both generation and evaluation. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs to stay on topic. Thorough comparisons with baselines. The per- formance of all baselines on 2WikiMultihopQA are reported in Table 1. FLARE outperforms all baselines by a large margin, which confirms that forward-looking active retrieval is highly effective. Most multi-time retrieval augmented approaches outperform single-time retrieval but with different Table 1: FLARE and baselines on 2WikiMultihopQA. Previous-window (Borgeaud et al., 2022; Ram et al., 2023), previous-sentence (Trivedi et al., 2022), and ques- tion decomposition (Press et al., 2022; Yao et al., 2022) methods are reimplemented for fair comparisons. margins. The improvement of retrieving using the previous sentence is relatively small which we hy- pothesize is mainly because the previous sentence often describes entities or relations different from those in the next sentence in 2WikiMultihopQA. While the previous-window approach might use the first half of a sentence to retrieve information potentially helpful for generating the second half. Among all baselines, the question decomposition approach (Press et al., 2022) achieves the best per- formance. which is not surprising since the in- context exemplars manually annotated with decom- posed sub-questions (Prompt D.2) guide LMs to generate sub-questions that align with the topic/in- tent of future generations. FLARE outperforms this baseline, indicating that manual exemplar an- notation is not necessary for effective future-aware retrieval. The gap between FLAREinstruct and ques- tion decomposition is large, indicating that teaching LMs to generate search queries using task-generic retrieval instructions and exemplars is challenging. We report all metrics for the other datasets in Table 2. FLARE outperforms baselines with re- spect to all metrics. Retrieval using the previ- Datasets Metrics StrategyQA EM ASQA-hint EM D-F1 R-L DR EM D-F1 R-L DR UniEval E-F1 R-L ASQA WikiAsp No retrieval Single-time retrieval 72.9 68.6 33.8 24.2 33.3 28.4 40.0 27.1 34.0 30.4 40.1 32.5 36.4 34.4 43.2 34.8 37.4 36.0 47.1 52.4 14.1 26.4 17.4 26.9 Multi-time retrieval Previous-window Previous-sentence FLARE (ours) 71.2 71.0 77.3 39.9 27.0 34.3 30.4 39.9 27.9 34.3 30.9 41.3 28.2 34.3 31.1 43.7 35.7 37.5 36.6 44.7 35.9 37.5 36.7 46.2 36.7 37.7 37.2 51.8 52.6 53.4 18.1 27.3 17.8 27.2 18.9 27.6 Table 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1. 2WikiMultihopQA EM F1 Prec. Rec. ASQA-hint EM D-F1 R-L DR Previous 39.0 49.2 48.9 51.8 48.8 57.6 57.1 60.5 Next 42.5 34.1 36.9 35.5 45.9 35.7 37.5 36.6 Table 3: A head-to-head comparison between using the previous sentence and the next sentence for retrieval. %steps/sentences with retrieval0.020.040.060.080.00.025.050.075.0100.0 StrategyQA 2WikiMultihopQA #Tokens 16 32 48 All EM F1 43.2 43.6 40.0 39.0 52.3 52.4 49.3 48.5 Prec. Rec. 51.7 52.0 49.0 48.2 54.5 55.0 52.0 51.1 Figure 5: Performance (EM) of FLARE with respect to the percentage of steps/sentences with retrieval on 2WikiMultihopQA and StrategyQA. Table 4: Previous-window approaches using different numbers of tokens as queries. the past hurts the performance, further confirming our hypothesis that previous context might not be relevant to intent of future generations. ous window underperforms single-time retrieval on ASQA, which we hypothesize is because the previous window does not accurately reflect future intent. Since we focus on evaluating factuality, met- rics with an emphasis on factual content (such as EM, Disambig-F1, UniEval) are more reliable than metrics computed over all tokens (ROUGE-L). 6.2 Ablation Study Importance of forward-looking retrieval. We first validate that forward-looking retrieval is more effective than past-context-based retrieval. We run ablation experiments on 2WikiMultihopQA and ASQA-hint comparing retrieval using the previ- ous versus the next sentence. Specifically, both methods retrieve every sentence and directly use the complete previous/next sentence as queries. As shown in Table 3, using the next sentence to retrieve is clearly better than using the previous sentence, confirming our hypothesis. We also run previous-window approaches using different numbers of past tokens as queries. As shown in Table 4, using too many tokens (> 32) in Importance of active retrieval. Next, we inves- tigate how active retrieval threshold \u03b8 affects per- formance. To alter our method from not retrieving to retrieving every sentence, we adjust the confi- dence threshold \u03b8 that determines when to trigger retrieval from 0 to 1. We then calculate the pro- portion of steps/sentences where retrieval is acti- vated, and present the performance based on it. As shown in Figure 5, on 2WikiMultihopQA, the per- formance plateaus when the retrieval percentage exceeds 60%, indicating that retrieval when LMs are confident is not necessary. On StrategyQA, the performance drops when the retrieval percentage exceeds 50%,"}, {"question": " What does the text suggest about the importance of metrics with an emphasis on factual content?", "answer": " They are more reliable than metrics computed over all tokens", "ref_chunk": "following aspects: academics, history.\u201d Experimen- tal setting details are included in Appendix B. Metrics include ROUGE, named entity-based F1, and UniEval (Zhong et al., 2022) which measures factual consistency. 6 Experimental Results Methods EM F1 No retrieval Single-time retrieval 28.2 39.4 36.8 48.8 Multi-time retrieval Previous-window Previous-sentence Question decomposition FLAREinstruct (ours) FLAREdirect (ours) 43.2 39.0 47.8 42.4 51.0 52.3 49.2 56.4 49.8 59.7 Prec. Rec. 36.5 48.6 38.6 51.5 51.7 48.9 56.1 49.1 59.1 54.5 51.8 58.6 52.5 62.6 We first report overall results across 4 tasks/datasets and compare the performance of FLARE with all the baselines introduced in section 4. We then run ablation experiments to study the efficacy of various design choices of our method. 6.1 Comparison with Baselines Overall results. The overall performance of FLARE and baseline across all tasks/datasets are reported in Figure 4. FLARE outperforms all base- line on all tasks/datasets, indicating that FLARE is a generic method that can effectively retrieve additional information throughout the generation. Among various tasks, multihop QA shows the most significant improvement. This is largely due to the task\u2019s clear definition and specific objective of producing the final answer through a 2-hop rea- soning process, which makes it easier for LMs to generate on-topic output. In contrast, ASQA and WikiAsp are more open-ended, which increases the difficulty of both generation and evaluation. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs to stay on topic. Thorough comparisons with baselines. The per- formance of all baselines on 2WikiMultihopQA are reported in Table 1. FLARE outperforms all baselines by a large margin, which confirms that forward-looking active retrieval is highly effective. Most multi-time retrieval augmented approaches outperform single-time retrieval but with different Table 1: FLARE and baselines on 2WikiMultihopQA. Previous-window (Borgeaud et al., 2022; Ram et al., 2023), previous-sentence (Trivedi et al., 2022), and ques- tion decomposition (Press et al., 2022; Yao et al., 2022) methods are reimplemented for fair comparisons. margins. The improvement of retrieving using the previous sentence is relatively small which we hy- pothesize is mainly because the previous sentence often describes entities or relations different from those in the next sentence in 2WikiMultihopQA. While the previous-window approach might use the first half of a sentence to retrieve information potentially helpful for generating the second half. Among all baselines, the question decomposition approach (Press et al., 2022) achieves the best per- formance. which is not surprising since the in- context exemplars manually annotated with decom- posed sub-questions (Prompt D.2) guide LMs to generate sub-questions that align with the topic/in- tent of future generations. FLARE outperforms this baseline, indicating that manual exemplar an- notation is not necessary for effective future-aware retrieval. The gap between FLAREinstruct and ques- tion decomposition is large, indicating that teaching LMs to generate search queries using task-generic retrieval instructions and exemplars is challenging. We report all metrics for the other datasets in Table 2. FLARE outperforms baselines with re- spect to all metrics. Retrieval using the previ- Datasets Metrics StrategyQA EM ASQA-hint EM D-F1 R-L DR EM D-F1 R-L DR UniEval E-F1 R-L ASQA WikiAsp No retrieval Single-time retrieval 72.9 68.6 33.8 24.2 33.3 28.4 40.0 27.1 34.0 30.4 40.1 32.5 36.4 34.4 43.2 34.8 37.4 36.0 47.1 52.4 14.1 26.4 17.4 26.9 Multi-time retrieval Previous-window Previous-sentence FLARE (ours) 71.2 71.0 77.3 39.9 27.0 34.3 30.4 39.9 27.9 34.3 30.9 41.3 28.2 34.3 31.1 43.7 35.7 37.5 36.6 44.7 35.9 37.5 36.7 46.2 36.7 37.7 37.2 51.8 52.6 53.4 18.1 27.3 17.8 27.2 18.9 27.6 Table 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1. 2WikiMultihopQA EM F1 Prec. Rec. ASQA-hint EM D-F1 R-L DR Previous 39.0 49.2 48.9 51.8 48.8 57.6 57.1 60.5 Next 42.5 34.1 36.9 35.5 45.9 35.7 37.5 36.6 Table 3: A head-to-head comparison between using the previous sentence and the next sentence for retrieval. %steps/sentences with retrieval0.020.040.060.080.00.025.050.075.0100.0 StrategyQA 2WikiMultihopQA #Tokens 16 32 48 All EM F1 43.2 43.6 40.0 39.0 52.3 52.4 49.3 48.5 Prec. Rec. 51.7 52.0 49.0 48.2 54.5 55.0 52.0 51.1 Figure 5: Performance (EM) of FLARE with respect to the percentage of steps/sentences with retrieval on 2WikiMultihopQA and StrategyQA. Table 4: Previous-window approaches using different numbers of tokens as queries. the past hurts the performance, further confirming our hypothesis that previous context might not be relevant to intent of future generations. ous window underperforms single-time retrieval on ASQA, which we hypothesize is because the previous window does not accurately reflect future intent. Since we focus on evaluating factuality, met- rics with an emphasis on factual content (such as EM, Disambig-F1, UniEval) are more reliable than metrics computed over all tokens (ROUGE-L). 6.2 Ablation Study Importance of forward-looking retrieval. We first validate that forward-looking retrieval is more effective than past-context-based retrieval. We run ablation experiments on 2WikiMultihopQA and ASQA-hint comparing retrieval using the previ- ous versus the next sentence. Specifically, both methods retrieve every sentence and directly use the complete previous/next sentence as queries. As shown in Table 3, using the next sentence to retrieve is clearly better than using the previous sentence, confirming our hypothesis. We also run previous-window approaches using different numbers of past tokens as queries. As shown in Table 4, using too many tokens (> 32) in Importance of active retrieval. Next, we inves- tigate how active retrieval threshold \u03b8 affects per- formance. To alter our method from not retrieving to retrieving every sentence, we adjust the confi- dence threshold \u03b8 that determines when to trigger retrieval from 0 to 1. We then calculate the pro- portion of steps/sentences where retrieval is acti- vated, and present the performance based on it. As shown in Figure 5, on 2WikiMultihopQA, the per- formance plateaus when the retrieval percentage exceeds 60%, indicating that retrieval when LMs are confident is not necessary. On StrategyQA, the performance drops when the retrieval percentage exceeds 50%,"}, {"question": " What does the ablation study validate about the effectiveness of forward-looking retrieval?", "answer": " Using the next sentence for retrieval is clearly better than using the previous sentence", "ref_chunk": "following aspects: academics, history.\u201d Experimen- tal setting details are included in Appendix B. Metrics include ROUGE, named entity-based F1, and UniEval (Zhong et al., 2022) which measures factual consistency. 6 Experimental Results Methods EM F1 No retrieval Single-time retrieval 28.2 39.4 36.8 48.8 Multi-time retrieval Previous-window Previous-sentence Question decomposition FLAREinstruct (ours) FLAREdirect (ours) 43.2 39.0 47.8 42.4 51.0 52.3 49.2 56.4 49.8 59.7 Prec. Rec. 36.5 48.6 38.6 51.5 51.7 48.9 56.1 49.1 59.1 54.5 51.8 58.6 52.5 62.6 We first report overall results across 4 tasks/datasets and compare the performance of FLARE with all the baselines introduced in section 4. We then run ablation experiments to study the efficacy of various design choices of our method. 6.1 Comparison with Baselines Overall results. The overall performance of FLARE and baseline across all tasks/datasets are reported in Figure 4. FLARE outperforms all base- line on all tasks/datasets, indicating that FLARE is a generic method that can effectively retrieve additional information throughout the generation. Among various tasks, multihop QA shows the most significant improvement. This is largely due to the task\u2019s clear definition and specific objective of producing the final answer through a 2-hop rea- soning process, which makes it easier for LMs to generate on-topic output. In contrast, ASQA and WikiAsp are more open-ended, which increases the difficulty of both generation and evaluation. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs to stay on topic. Thorough comparisons with baselines. The per- formance of all baselines on 2WikiMultihopQA are reported in Table 1. FLARE outperforms all baselines by a large margin, which confirms that forward-looking active retrieval is highly effective. Most multi-time retrieval augmented approaches outperform single-time retrieval but with different Table 1: FLARE and baselines on 2WikiMultihopQA. Previous-window (Borgeaud et al., 2022; Ram et al., 2023), previous-sentence (Trivedi et al., 2022), and ques- tion decomposition (Press et al., 2022; Yao et al., 2022) methods are reimplemented for fair comparisons. margins. The improvement of retrieving using the previous sentence is relatively small which we hy- pothesize is mainly because the previous sentence often describes entities or relations different from those in the next sentence in 2WikiMultihopQA. While the previous-window approach might use the first half of a sentence to retrieve information potentially helpful for generating the second half. Among all baselines, the question decomposition approach (Press et al., 2022) achieves the best per- formance. which is not surprising since the in- context exemplars manually annotated with decom- posed sub-questions (Prompt D.2) guide LMs to generate sub-questions that align with the topic/in- tent of future generations. FLARE outperforms this baseline, indicating that manual exemplar an- notation is not necessary for effective future-aware retrieval. The gap between FLAREinstruct and ques- tion decomposition is large, indicating that teaching LMs to generate search queries using task-generic retrieval instructions and exemplars is challenging. We report all metrics for the other datasets in Table 2. FLARE outperforms baselines with re- spect to all metrics. Retrieval using the previ- Datasets Metrics StrategyQA EM ASQA-hint EM D-F1 R-L DR EM D-F1 R-L DR UniEval E-F1 R-L ASQA WikiAsp No retrieval Single-time retrieval 72.9 68.6 33.8 24.2 33.3 28.4 40.0 27.1 34.0 30.4 40.1 32.5 36.4 34.4 43.2 34.8 37.4 36.0 47.1 52.4 14.1 26.4 17.4 26.9 Multi-time retrieval Previous-window Previous-sentence FLARE (ours) 71.2 71.0 77.3 39.9 27.0 34.3 30.4 39.9 27.9 34.3 30.9 41.3 28.2 34.3 31.1 43.7 35.7 37.5 36.6 44.7 35.9 37.5 36.7 46.2 36.7 37.7 37.2 51.8 52.6 53.4 18.1 27.3 17.8 27.2 18.9 27.6 Table 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1. 2WikiMultihopQA EM F1 Prec. Rec. ASQA-hint EM D-F1 R-L DR Previous 39.0 49.2 48.9 51.8 48.8 57.6 57.1 60.5 Next 42.5 34.1 36.9 35.5 45.9 35.7 37.5 36.6 Table 3: A head-to-head comparison between using the previous sentence and the next sentence for retrieval. %steps/sentences with retrieval0.020.040.060.080.00.025.050.075.0100.0 StrategyQA 2WikiMultihopQA #Tokens 16 32 48 All EM F1 43.2 43.6 40.0 39.0 52.3 52.4 49.3 48.5 Prec. Rec. 51.7 52.0 49.0 48.2 54.5 55.0 52.0 51.1 Figure 5: Performance (EM) of FLARE with respect to the percentage of steps/sentences with retrieval on 2WikiMultihopQA and StrategyQA. Table 4: Previous-window approaches using different numbers of tokens as queries. the past hurts the performance, further confirming our hypothesis that previous context might not be relevant to intent of future generations. ous window underperforms single-time retrieval on ASQA, which we hypothesize is because the previous window does not accurately reflect future intent. Since we focus on evaluating factuality, met- rics with an emphasis on factual content (such as EM, Disambig-F1, UniEval) are more reliable than metrics computed over all tokens (ROUGE-L). 6.2 Ablation Study Importance of forward-looking retrieval. We first validate that forward-looking retrieval is more effective than past-context-based retrieval. We run ablation experiments on 2WikiMultihopQA and ASQA-hint comparing retrieval using the previ- ous versus the next sentence. Specifically, both methods retrieve every sentence and directly use the complete previous/next sentence as queries. As shown in Table 3, using the next sentence to retrieve is clearly better than using the previous sentence, confirming our hypothesis. We also run previous-window approaches using different numbers of past tokens as queries. As shown in Table 4, using too many tokens (> 32) in Importance of active retrieval. Next, we inves- tigate how active retrieval threshold \u03b8 affects per- formance. To alter our method from not retrieving to retrieving every sentence, we adjust the confi- dence threshold \u03b8 that determines when to trigger retrieval from 0 to 1. We then calculate the pro- portion of steps/sentences where retrieval is acti- vated, and present the performance based on it. As shown in Figure 5, on 2WikiMultihopQA, the per- formance plateaus when the retrieval percentage exceeds 60%, indicating that retrieval when LMs are confident is not necessary. On StrategyQA, the performance drops when the retrieval percentage exceeds 50%,"}], "doc_text": "following aspects: academics, history.\u201d Experimen- tal setting details are included in Appendix B. Metrics include ROUGE, named entity-based F1, and UniEval (Zhong et al., 2022) which measures factual consistency. 6 Experimental Results Methods EM F1 No retrieval Single-time retrieval 28.2 39.4 36.8 48.8 Multi-time retrieval Previous-window Previous-sentence Question decomposition FLAREinstruct (ours) FLAREdirect (ours) 43.2 39.0 47.8 42.4 51.0 52.3 49.2 56.4 49.8 59.7 Prec. Rec. 36.5 48.6 38.6 51.5 51.7 48.9 56.1 49.1 59.1 54.5 51.8 58.6 52.5 62.6 We first report overall results across 4 tasks/datasets and compare the performance of FLARE with all the baselines introduced in section 4. We then run ablation experiments to study the efficacy of various design choices of our method. 6.1 Comparison with Baselines Overall results. The overall performance of FLARE and baseline across all tasks/datasets are reported in Figure 4. FLARE outperforms all base- line on all tasks/datasets, indicating that FLARE is a generic method that can effectively retrieve additional information throughout the generation. Among various tasks, multihop QA shows the most significant improvement. This is largely due to the task\u2019s clear definition and specific objective of producing the final answer through a 2-hop rea- soning process, which makes it easier for LMs to generate on-topic output. In contrast, ASQA and WikiAsp are more open-ended, which increases the difficulty of both generation and evaluation. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs to stay on topic. Thorough comparisons with baselines. The per- formance of all baselines on 2WikiMultihopQA are reported in Table 1. FLARE outperforms all baselines by a large margin, which confirms that forward-looking active retrieval is highly effective. Most multi-time retrieval augmented approaches outperform single-time retrieval but with different Table 1: FLARE and baselines on 2WikiMultihopQA. Previous-window (Borgeaud et al., 2022; Ram et al., 2023), previous-sentence (Trivedi et al., 2022), and ques- tion decomposition (Press et al., 2022; Yao et al., 2022) methods are reimplemented for fair comparisons. margins. The improvement of retrieving using the previous sentence is relatively small which we hy- pothesize is mainly because the previous sentence often describes entities or relations different from those in the next sentence in 2WikiMultihopQA. While the previous-window approach might use the first half of a sentence to retrieve information potentially helpful for generating the second half. Among all baselines, the question decomposition approach (Press et al., 2022) achieves the best per- formance. which is not surprising since the in- context exemplars manually annotated with decom- posed sub-questions (Prompt D.2) guide LMs to generate sub-questions that align with the topic/in- tent of future generations. FLARE outperforms this baseline, indicating that manual exemplar an- notation is not necessary for effective future-aware retrieval. The gap between FLAREinstruct and ques- tion decomposition is large, indicating that teaching LMs to generate search queries using task-generic retrieval instructions and exemplars is challenging. We report all metrics for the other datasets in Table 2. FLARE outperforms baselines with re- spect to all metrics. Retrieval using the previ- Datasets Metrics StrategyQA EM ASQA-hint EM D-F1 R-L DR EM D-F1 R-L DR UniEval E-F1 R-L ASQA WikiAsp No retrieval Single-time retrieval 72.9 68.6 33.8 24.2 33.3 28.4 40.0 27.1 34.0 30.4 40.1 32.5 36.4 34.4 43.2 34.8 37.4 36.0 47.1 52.4 14.1 26.4 17.4 26.9 Multi-time retrieval Previous-window Previous-sentence FLARE (ours) 71.2 71.0 77.3 39.9 27.0 34.3 30.4 39.9 27.9 34.3 30.9 41.3 28.2 34.3 31.1 43.7 35.7 37.5 36.6 44.7 35.9 37.5 36.7 46.2 36.7 37.7 37.2 51.8 52.6 53.4 18.1 27.3 17.8 27.2 18.9 27.6 Table 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1. 2WikiMultihopQA EM F1 Prec. Rec. ASQA-hint EM D-F1 R-L DR Previous 39.0 49.2 48.9 51.8 48.8 57.6 57.1 60.5 Next 42.5 34.1 36.9 35.5 45.9 35.7 37.5 36.6 Table 3: A head-to-head comparison between using the previous sentence and the next sentence for retrieval. %steps/sentences with retrieval0.020.040.060.080.00.025.050.075.0100.0 StrategyQA 2WikiMultihopQA #Tokens 16 32 48 All EM F1 43.2 43.6 40.0 39.0 52.3 52.4 49.3 48.5 Prec. Rec. 51.7 52.0 49.0 48.2 54.5 55.0 52.0 51.1 Figure 5: Performance (EM) of FLARE with respect to the percentage of steps/sentences with retrieval on 2WikiMultihopQA and StrategyQA. Table 4: Previous-window approaches using different numbers of tokens as queries. the past hurts the performance, further confirming our hypothesis that previous context might not be relevant to intent of future generations. ous window underperforms single-time retrieval on ASQA, which we hypothesize is because the previous window does not accurately reflect future intent. Since we focus on evaluating factuality, met- rics with an emphasis on factual content (such as EM, Disambig-F1, UniEval) are more reliable than metrics computed over all tokens (ROUGE-L). 6.2 Ablation Study Importance of forward-looking retrieval. We first validate that forward-looking retrieval is more effective than past-context-based retrieval. We run ablation experiments on 2WikiMultihopQA and ASQA-hint comparing retrieval using the previ- ous versus the next sentence. Specifically, both methods retrieve every sentence and directly use the complete previous/next sentence as queries. As shown in Table 3, using the next sentence to retrieve is clearly better than using the previous sentence, confirming our hypothesis. We also run previous-window approaches using different numbers of past tokens as queries. As shown in Table 4, using too many tokens (> 32) in Importance of active retrieval. Next, we inves- tigate how active retrieval threshold \u03b8 affects per- formance. To alter our method from not retrieving to retrieving every sentence, we adjust the confi- dence threshold \u03b8 that determines when to trigger retrieval from 0 to 1. We then calculate the pro- portion of steps/sentences where retrieval is acti- vated, and present the performance based on it. As shown in Figure 5, on 2WikiMultihopQA, the per- formance plateaus when the retrieval percentage exceeds 60%, indicating that retrieval when LMs are confident is not necessary. On StrategyQA, the performance drops when the retrieval percentage exceeds 50%,"}