{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Segment-Level_Vectorized_Beam_Search_Based_on_Partially_Autoregressive_Inference_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the work described in the text?", "answer": " Improving the inference speed of an automatic speech recognition model based on a partially autoregressive framework.", "ref_chunk": "3 2 0 2 t c O 1 ] S A . s s e e [ 2 v 2 2 9 4 1 . 9 0 3 2 : v i X r a SEGMENT-LEVEL VECTORIZED BEAM SEARCH BASED ON PARTIALLY AUTOREGRESSIVE INFERENCE Masao Someki1, Nicholas Eng2, Yosuke Higuchi3, Shinji Watanabe4 1IBM Japan Ltd., Japan 2The University of Auckland, New Zealand 3Waseda University, Japan 4Carnegie Mellon University, USA ABSTRACT Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothe- sis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Ex- perimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy. Index Terms\u2014 Decoding algorithm, autoregressive, semi- autoregressive, hybrid CTC/attention, beam search 1. INTRODUCTION Due to recent advances in deep learning, automatic speech recog- nition (ASR) has witnessed remarkable achievements [1\u20133]. ASR plays an essential role in facilitating human-computer interaction by providing an interface for converting audio to text and demonstrat- ing substantial applicability in real-world scenarios. In particular, the RNN-Transducer model [4], which operates relatively fast and can be extended to streaming speech recognition, is widely used in real-world applications. Recent research in ASR has also made significant progress in achieving higher accuracy through Attention- based Encoder-Decoder (AED) models [5, 6]. AED models have also been utilized in models such as Whisper [7] and speech transla- tion [8], and their usefulness has been reevaluated. However, there are various trade-offs that can potentially limit its application in cer- tain scenarios. For example, one can construct an ASR system with high accuracy through large and complex models, but this comes at the expense of increased computational cost and inference time. time compared to AR models [17]. However, it remains a challenge to achieve the same level of recognition accuracy as AR models. In addition, NAR models require a complex model structure or a unique training strategy for the successful implementation of parallel generation during inference [17]. Regarding AR decoding, the decoder is trained to learn linguis- tic information. This allows us to utilize the relationship between the current token and the previous token. Furthermore, it is common to enhance accuracy by implementing the beam search algorithm, which is a heuristic search for the best hypothesis. However, be- cause AR requires the previous tokens to estimate the next token, it is not possible to parallelize the inference of a single audio. As a consequence, achieving NAR-level speed through inference paral- lelization is challenging. In this paper, we focus on the difference in trade-off balance be- tween AR and NAR, and propose a new decoding method, the par- tially autoregressive (PAR) method as a fast and accurate decoding method. By utilizing the NAR approach and segment-level vector- ized beam search, we can compensate for the weaknesses of both AR and NAR. This results in a better trade-off between accuracy and latency. In particular, we show that by optimizing the inference operations of the pre-trained hybrid connectionist temporal classifi- cation (CTC) and attention model, we are able to achieve NAR-level inference speed while maintaining its high accuracy, as well as not needing additional training of the model. In this paper, we make the following contributions: \u2022 We propose a new decoding framework that combines AR and NAR. We demonstrate a better balance of accuracy-latency trade-off without additional training. 2. RELATED WORKS Numerous studies have been conducted to balance the accuracy- latency trade-off. In general, there are two main approaches; reduc- ing latency while preserving good accuracy or improving accuracy while preserving fast inference speed. There has been extensive research devoted to alleviating the In- trade-off between recognition accuracy and inference speed. spired by the great success in neural machine translation [9, 10], non-autoregressive (NAR) models have been actively studied in the context of ASR, with the aim of achieving fast inference [11\u201314]. Compared to the conventional autoregressive (AR) models [15, 16], which generates output at each step conditioned on the previously generated outputs, NAR models can produce multiple outputs si- multaneously. This parallel computation accelerates the inference process of ASR, resulting in a significant reduction in inference To reduce latency, there are several methods to lower the com- putational cost during inference. One such method is the pruning technique [18\u201320], which reduces the number of parameters in a trained model by identifying subnetworks with better performance and fewer parameters. Another method is knowledge distillation [21, 22], which trains a smaller model to reduce the number of param- eters required during inference. To further reduce the number of search iterations, hypotheses can be predicted in a batch [23], or the search process can be stopped prematurely [24]. In addition, other approaches focus on utilizing machine resources more effi- ciently [25\u201327], rather than relying solely on the model architecture, 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE (a) Autoregressive (AR) (b) Non-autoregressive (NAR) (c) Partially autoregressive (PAR) Fig. 1: Overview and comparison of AR, NAR, and PAR decoding. <sos> denotes the \u201cstart-of-sequence\u201d symbol,and the mask token is denoted by # or red characters. PAR is a hybrid of AR and NAR methods, in which the masking process is applied first, followed by segment-level vectorized beam search. to achieve faster processing. In this study, we parallelized the AR decoding of single audio to achieve high-speed processing. The pro- cess is similar to batch processing of hypotheses, but in this case, the audio is segmented and parallelized in addition to the hypotheses. On the"}, {"question": " What approach has been traditionally dominant for automatic speech recognition?", "answer": " Attention-based encoder-decoder models with autoregressive decoding.", "ref_chunk": "3 2 0 2 t c O 1 ] S A . s s e e [ 2 v 2 2 9 4 1 . 9 0 3 2 : v i X r a SEGMENT-LEVEL VECTORIZED BEAM SEARCH BASED ON PARTIALLY AUTOREGRESSIVE INFERENCE Masao Someki1, Nicholas Eng2, Yosuke Higuchi3, Shinji Watanabe4 1IBM Japan Ltd., Japan 2The University of Auckland, New Zealand 3Waseda University, Japan 4Carnegie Mellon University, USA ABSTRACT Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothe- sis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Ex- perimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy. Index Terms\u2014 Decoding algorithm, autoregressive, semi- autoregressive, hybrid CTC/attention, beam search 1. INTRODUCTION Due to recent advances in deep learning, automatic speech recog- nition (ASR) has witnessed remarkable achievements [1\u20133]. ASR plays an essential role in facilitating human-computer interaction by providing an interface for converting audio to text and demonstrat- ing substantial applicability in real-world scenarios. In particular, the RNN-Transducer model [4], which operates relatively fast and can be extended to streaming speech recognition, is widely used in real-world applications. Recent research in ASR has also made significant progress in achieving higher accuracy through Attention- based Encoder-Decoder (AED) models [5, 6]. AED models have also been utilized in models such as Whisper [7] and speech transla- tion [8], and their usefulness has been reevaluated. However, there are various trade-offs that can potentially limit its application in cer- tain scenarios. For example, one can construct an ASR system with high accuracy through large and complex models, but this comes at the expense of increased computational cost and inference time. time compared to AR models [17]. However, it remains a challenge to achieve the same level of recognition accuracy as AR models. In addition, NAR models require a complex model structure or a unique training strategy for the successful implementation of parallel generation during inference [17]. Regarding AR decoding, the decoder is trained to learn linguis- tic information. This allows us to utilize the relationship between the current token and the previous token. Furthermore, it is common to enhance accuracy by implementing the beam search algorithm, which is a heuristic search for the best hypothesis. However, be- cause AR requires the previous tokens to estimate the next token, it is not possible to parallelize the inference of a single audio. As a consequence, achieving NAR-level speed through inference paral- lelization is challenging. In this paper, we focus on the difference in trade-off balance be- tween AR and NAR, and propose a new decoding method, the par- tially autoregressive (PAR) method as a fast and accurate decoding method. By utilizing the NAR approach and segment-level vector- ized beam search, we can compensate for the weaknesses of both AR and NAR. This results in a better trade-off between accuracy and latency. In particular, we show that by optimizing the inference operations of the pre-trained hybrid connectionist temporal classifi- cation (CTC) and attention model, we are able to achieve NAR-level inference speed while maintaining its high accuracy, as well as not needing additional training of the model. In this paper, we make the following contributions: \u2022 We propose a new decoding framework that combines AR and NAR. We demonstrate a better balance of accuracy-latency trade-off without additional training. 2. RELATED WORKS Numerous studies have been conducted to balance the accuracy- latency trade-off. In general, there are two main approaches; reduc- ing latency while preserving good accuracy or improving accuracy while preserving fast inference speed. There has been extensive research devoted to alleviating the In- trade-off between recognition accuracy and inference speed. spired by the great success in neural machine translation [9, 10], non-autoregressive (NAR) models have been actively studied in the context of ASR, with the aim of achieving fast inference [11\u201314]. Compared to the conventional autoregressive (AR) models [15, 16], which generates output at each step conditioned on the previously generated outputs, NAR models can produce multiple outputs si- multaneously. This parallel computation accelerates the inference process of ASR, resulting in a significant reduction in inference To reduce latency, there are several methods to lower the com- putational cost during inference. One such method is the pruning technique [18\u201320], which reduces the number of parameters in a trained model by identifying subnetworks with better performance and fewer parameters. Another method is knowledge distillation [21, 22], which trains a smaller model to reduce the number of param- eters required during inference. To further reduce the number of search iterations, hypotheses can be predicted in a batch [23], or the search process can be stopped prematurely [24]. In addition, other approaches focus on utilizing machine resources more effi- ciently [25\u201327], rather than relying solely on the model architecture, 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE (a) Autoregressive (AR) (b) Non-autoregressive (NAR) (c) Partially autoregressive (PAR) Fig. 1: Overview and comparison of AR, NAR, and PAR decoding. <sos> denotes the \u201cstart-of-sequence\u201d symbol,and the mask token is denoted by # or red characters. PAR is a hybrid of AR and NAR methods, in which the masking process is applied first, followed by segment-level vectorized beam search. to achieve faster processing. In this study, we parallelized the AR decoding of single audio to achieve high-speed processing. The pro- cess is similar to batch processing of hypotheses, but in this case, the audio is segmented and parallelized in addition to the hypotheses. On the"}, {"question": " What is the primary reason for the slow inference in traditional autoregressive decoding?", "answer": " Incremental calculation of the decoder.", "ref_chunk": "3 2 0 2 t c O 1 ] S A . s s e e [ 2 v 2 2 9 4 1 . 9 0 3 2 : v i X r a SEGMENT-LEVEL VECTORIZED BEAM SEARCH BASED ON PARTIALLY AUTOREGRESSIVE INFERENCE Masao Someki1, Nicholas Eng2, Yosuke Higuchi3, Shinji Watanabe4 1IBM Japan Ltd., Japan 2The University of Auckland, New Zealand 3Waseda University, Japan 4Carnegie Mellon University, USA ABSTRACT Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothe- sis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Ex- perimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy. Index Terms\u2014 Decoding algorithm, autoregressive, semi- autoregressive, hybrid CTC/attention, beam search 1. INTRODUCTION Due to recent advances in deep learning, automatic speech recog- nition (ASR) has witnessed remarkable achievements [1\u20133]. ASR plays an essential role in facilitating human-computer interaction by providing an interface for converting audio to text and demonstrat- ing substantial applicability in real-world scenarios. In particular, the RNN-Transducer model [4], which operates relatively fast and can be extended to streaming speech recognition, is widely used in real-world applications. Recent research in ASR has also made significant progress in achieving higher accuracy through Attention- based Encoder-Decoder (AED) models [5, 6]. AED models have also been utilized in models such as Whisper [7] and speech transla- tion [8], and their usefulness has been reevaluated. However, there are various trade-offs that can potentially limit its application in cer- tain scenarios. For example, one can construct an ASR system with high accuracy through large and complex models, but this comes at the expense of increased computational cost and inference time. time compared to AR models [17]. However, it remains a challenge to achieve the same level of recognition accuracy as AR models. In addition, NAR models require a complex model structure or a unique training strategy for the successful implementation of parallel generation during inference [17]. Regarding AR decoding, the decoder is trained to learn linguis- tic information. This allows us to utilize the relationship between the current token and the previous token. Furthermore, it is common to enhance accuracy by implementing the beam search algorithm, which is a heuristic search for the best hypothesis. However, be- cause AR requires the previous tokens to estimate the next token, it is not possible to parallelize the inference of a single audio. As a consequence, achieving NAR-level speed through inference paral- lelization is challenging. In this paper, we focus on the difference in trade-off balance be- tween AR and NAR, and propose a new decoding method, the par- tially autoregressive (PAR) method as a fast and accurate decoding method. By utilizing the NAR approach and segment-level vector- ized beam search, we can compensate for the weaknesses of both AR and NAR. This results in a better trade-off between accuracy and latency. In particular, we show that by optimizing the inference operations of the pre-trained hybrid connectionist temporal classifi- cation (CTC) and attention model, we are able to achieve NAR-level inference speed while maintaining its high accuracy, as well as not needing additional training of the model. In this paper, we make the following contributions: \u2022 We propose a new decoding framework that combines AR and NAR. We demonstrate a better balance of accuracy-latency trade-off without additional training. 2. RELATED WORKS Numerous studies have been conducted to balance the accuracy- latency trade-off. In general, there are two main approaches; reduc- ing latency while preserving good accuracy or improving accuracy while preserving fast inference speed. There has been extensive research devoted to alleviating the In- trade-off between recognition accuracy and inference speed. spired by the great success in neural machine translation [9, 10], non-autoregressive (NAR) models have been actively studied in the context of ASR, with the aim of achieving fast inference [11\u201314]. Compared to the conventional autoregressive (AR) models [15, 16], which generates output at each step conditioned on the previously generated outputs, NAR models can produce multiple outputs si- multaneously. This parallel computation accelerates the inference process of ASR, resulting in a significant reduction in inference To reduce latency, there are several methods to lower the com- putational cost during inference. One such method is the pruning technique [18\u201320], which reduces the number of parameters in a trained model by identifying subnetworks with better performance and fewer parameters. Another method is knowledge distillation [21, 22], which trains a smaller model to reduce the number of param- eters required during inference. To further reduce the number of search iterations, hypotheses can be predicted in a batch [23], or the search process can be stopped prematurely [24]. In addition, other approaches focus on utilizing machine resources more effi- ciently [25\u201327], rather than relying solely on the model architecture, 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE (a) Autoregressive (AR) (b) Non-autoregressive (NAR) (c) Partially autoregressive (PAR) Fig. 1: Overview and comparison of AR, NAR, and PAR decoding. <sos> denotes the \u201cstart-of-sequence\u201d symbol,and the mask token is denoted by # or red characters. PAR is a hybrid of AR and NAR methods, in which the masking process is applied first, followed by segment-level vectorized beam search. to achieve faster processing. In this study, we parallelized the AR decoding of single audio to achieve high-speed processing. The pro- cess is similar to batch processing of hypotheses, but in this case, the audio is segmented and parallelized in addition to the hypotheses. On the"}, {"question": " How does the proposed method aim to improve the speed of inference in automatic speech recognition models?", "answer": " By employing segment-level vectorized beam search and a partially autoregressive framework.", "ref_chunk": "3 2 0 2 t c O 1 ] S A . s s e e [ 2 v 2 2 9 4 1 . 9 0 3 2 : v i X r a SEGMENT-LEVEL VECTORIZED BEAM SEARCH BASED ON PARTIALLY AUTOREGRESSIVE INFERENCE Masao Someki1, Nicholas Eng2, Yosuke Higuchi3, Shinji Watanabe4 1IBM Japan Ltd., Japan 2The University of Auckland, New Zealand 3Waseda University, Japan 4Carnegie Mellon University, USA ABSTRACT Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothe- sis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Ex- perimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy. Index Terms\u2014 Decoding algorithm, autoregressive, semi- autoregressive, hybrid CTC/attention, beam search 1. INTRODUCTION Due to recent advances in deep learning, automatic speech recog- nition (ASR) has witnessed remarkable achievements [1\u20133]. ASR plays an essential role in facilitating human-computer interaction by providing an interface for converting audio to text and demonstrat- ing substantial applicability in real-world scenarios. In particular, the RNN-Transducer model [4], which operates relatively fast and can be extended to streaming speech recognition, is widely used in real-world applications. Recent research in ASR has also made significant progress in achieving higher accuracy through Attention- based Encoder-Decoder (AED) models [5, 6]. AED models have also been utilized in models such as Whisper [7] and speech transla- tion [8], and their usefulness has been reevaluated. However, there are various trade-offs that can potentially limit its application in cer- tain scenarios. For example, one can construct an ASR system with high accuracy through large and complex models, but this comes at the expense of increased computational cost and inference time. time compared to AR models [17]. However, it remains a challenge to achieve the same level of recognition accuracy as AR models. In addition, NAR models require a complex model structure or a unique training strategy for the successful implementation of parallel generation during inference [17]. Regarding AR decoding, the decoder is trained to learn linguis- tic information. This allows us to utilize the relationship between the current token and the previous token. Furthermore, it is common to enhance accuracy by implementing the beam search algorithm, which is a heuristic search for the best hypothesis. However, be- cause AR requires the previous tokens to estimate the next token, it is not possible to parallelize the inference of a single audio. As a consequence, achieving NAR-level speed through inference paral- lelization is challenging. In this paper, we focus on the difference in trade-off balance be- tween AR and NAR, and propose a new decoding method, the par- tially autoregressive (PAR) method as a fast and accurate decoding method. By utilizing the NAR approach and segment-level vector- ized beam search, we can compensate for the weaknesses of both AR and NAR. This results in a better trade-off between accuracy and latency. In particular, we show that by optimizing the inference operations of the pre-trained hybrid connectionist temporal classifi- cation (CTC) and attention model, we are able to achieve NAR-level inference speed while maintaining its high accuracy, as well as not needing additional training of the model. In this paper, we make the following contributions: \u2022 We propose a new decoding framework that combines AR and NAR. We demonstrate a better balance of accuracy-latency trade-off without additional training. 2. RELATED WORKS Numerous studies have been conducted to balance the accuracy- latency trade-off. In general, there are two main approaches; reduc- ing latency while preserving good accuracy or improving accuracy while preserving fast inference speed. There has been extensive research devoted to alleviating the In- trade-off between recognition accuracy and inference speed. spired by the great success in neural machine translation [9, 10], non-autoregressive (NAR) models have been actively studied in the context of ASR, with the aim of achieving fast inference [11\u201314]. Compared to the conventional autoregressive (AR) models [15, 16], which generates output at each step conditioned on the previously generated outputs, NAR models can produce multiple outputs si- multaneously. This parallel computation accelerates the inference process of ASR, resulting in a significant reduction in inference To reduce latency, there are several methods to lower the com- putational cost during inference. One such method is the pruning technique [18\u201320], which reduces the number of parameters in a trained model by identifying subnetworks with better performance and fewer parameters. Another method is knowledge distillation [21, 22], which trains a smaller model to reduce the number of param- eters required during inference. To further reduce the number of search iterations, hypotheses can be predicted in a batch [23], or the search process can be stopped prematurely [24]. In addition, other approaches focus on utilizing machine resources more effi- ciently [25\u201327], rather than relying solely on the model architecture, 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE (a) Autoregressive (AR) (b) Non-autoregressive (NAR) (c) Partially autoregressive (PAR) Fig. 1: Overview and comparison of AR, NAR, and PAR decoding. <sos> denotes the \u201cstart-of-sequence\u201d symbol,and the mask token is denoted by # or red characters. PAR is a hybrid of AR and NAR methods, in which the masking process is applied first, followed by segment-level vectorized beam search. to achieve faster processing. In this study, we parallelized the AR decoding of single audio to achieve high-speed processing. The pro- cess is similar to batch processing of hypotheses, but in this case, the audio is segmented and parallelized in addition to the hypotheses. On the"}, {"question": " What is the initial step taken by the proposed method for improving inference speed?", "answer": " Generating an initial hypothesis using greedy CTC decoding.", "ref_chunk": "3 2 0 2 t c O 1 ] S A . s s e e [ 2 v 2 2 9 4 1 . 9 0 3 2 : v i X r a SEGMENT-LEVEL VECTORIZED BEAM SEARCH BASED ON PARTIALLY AUTOREGRESSIVE INFERENCE Masao Someki1, Nicholas Eng2, Yosuke Higuchi3, Shinji Watanabe4 1IBM Japan Ltd., Japan 2The University of Auckland, New Zealand 3Waseda University, Japan 4Carnegie Mellon University, USA ABSTRACT Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothe- sis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Ex- perimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy. Index Terms\u2014 Decoding algorithm, autoregressive, semi- autoregressive, hybrid CTC/attention, beam search 1. INTRODUCTION Due to recent advances in deep learning, automatic speech recog- nition (ASR) has witnessed remarkable achievements [1\u20133]. ASR plays an essential role in facilitating human-computer interaction by providing an interface for converting audio to text and demonstrat- ing substantial applicability in real-world scenarios. In particular, the RNN-Transducer model [4], which operates relatively fast and can be extended to streaming speech recognition, is widely used in real-world applications. Recent research in ASR has also made significant progress in achieving higher accuracy through Attention- based Encoder-Decoder (AED) models [5, 6]. AED models have also been utilized in models such as Whisper [7] and speech transla- tion [8], and their usefulness has been reevaluated. However, there are various trade-offs that can potentially limit its application in cer- tain scenarios. For example, one can construct an ASR system with high accuracy through large and complex models, but this comes at the expense of increased computational cost and inference time. time compared to AR models [17]. However, it remains a challenge to achieve the same level of recognition accuracy as AR models. In addition, NAR models require a complex model structure or a unique training strategy for the successful implementation of parallel generation during inference [17]. Regarding AR decoding, the decoder is trained to learn linguis- tic information. This allows us to utilize the relationship between the current token and the previous token. Furthermore, it is common to enhance accuracy by implementing the beam search algorithm, which is a heuristic search for the best hypothesis. However, be- cause AR requires the previous tokens to estimate the next token, it is not possible to parallelize the inference of a single audio. As a consequence, achieving NAR-level speed through inference paral- lelization is challenging. In this paper, we focus on the difference in trade-off balance be- tween AR and NAR, and propose a new decoding method, the par- tially autoregressive (PAR) method as a fast and accurate decoding method. By utilizing the NAR approach and segment-level vector- ized beam search, we can compensate for the weaknesses of both AR and NAR. This results in a better trade-off between accuracy and latency. In particular, we show that by optimizing the inference operations of the pre-trained hybrid connectionist temporal classifi- cation (CTC) and attention model, we are able to achieve NAR-level inference speed while maintaining its high accuracy, as well as not needing additional training of the model. In this paper, we make the following contributions: \u2022 We propose a new decoding framework that combines AR and NAR. We demonstrate a better balance of accuracy-latency trade-off without additional training. 2. RELATED WORKS Numerous studies have been conducted to balance the accuracy- latency trade-off. In general, there are two main approaches; reduc- ing latency while preserving good accuracy or improving accuracy while preserving fast inference speed. There has been extensive research devoted to alleviating the In- trade-off between recognition accuracy and inference speed. spired by the great success in neural machine translation [9, 10], non-autoregressive (NAR) models have been actively studied in the context of ASR, with the aim of achieving fast inference [11\u201314]. Compared to the conventional autoregressive (AR) models [15, 16], which generates output at each step conditioned on the previously generated outputs, NAR models can produce multiple outputs si- multaneously. This parallel computation accelerates the inference process of ASR, resulting in a significant reduction in inference To reduce latency, there are several methods to lower the com- putational cost during inference. One such method is the pruning technique [18\u201320], which reduces the number of parameters in a trained model by identifying subnetworks with better performance and fewer parameters. Another method is knowledge distillation [21, 22], which trains a smaller model to reduce the number of param- eters required during inference. To further reduce the number of search iterations, hypotheses can be predicted in a batch [23], or the search process can be stopped prematurely [24]. In addition, other approaches focus on utilizing machine resources more effi- ciently [25\u201327], rather than relying solely on the model architecture, 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE (a) Autoregressive (AR) (b) Non-autoregressive (NAR) (c) Partially autoregressive (PAR) Fig. 1: Overview and comparison of AR, NAR, and PAR decoding. <sos> denotes the \u201cstart-of-sequence\u201d symbol,and the mask token is denoted by # or red characters. PAR is a hybrid of AR and NAR methods, in which the masking process is applied first, followed by segment-level vectorized beam search. to achieve faster processing. In this study, we parallelized the AR decoding of single audio to achieve high-speed processing. The pro- cess is similar to batch processing of hypotheses, but in this case, the audio is segmented and parallelized in addition to the hypotheses. On the"}, {"question": " What are the trade-offs mentioned in the text related to the application of large and complex ASR models?", "answer": " Increased computational cost and inference time.", "ref_chunk": "3 2 0 2 t c O 1 ] S A . s s e e [ 2 v 2 2 9 4 1 . 9 0 3 2 : v i X r a SEGMENT-LEVEL VECTORIZED BEAM SEARCH BASED ON PARTIALLY AUTOREGRESSIVE INFERENCE Masao Someki1, Nicholas Eng2, Yosuke Higuchi3, Shinji Watanabe4 1IBM Japan Ltd., Japan 2The University of Auckland, New Zealand 3Waseda University, Japan 4Carnegie Mellon University, USA ABSTRACT Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothe- sis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Ex- perimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy. Index Terms\u2014 Decoding algorithm, autoregressive, semi- autoregressive, hybrid CTC/attention, beam search 1. INTRODUCTION Due to recent advances in deep learning, automatic speech recog- nition (ASR) has witnessed remarkable achievements [1\u20133]. ASR plays an essential role in facilitating human-computer interaction by providing an interface for converting audio to text and demonstrat- ing substantial applicability in real-world scenarios. In particular, the RNN-Transducer model [4], which operates relatively fast and can be extended to streaming speech recognition, is widely used in real-world applications. Recent research in ASR has also made significant progress in achieving higher accuracy through Attention- based Encoder-Decoder (AED) models [5, 6]. AED models have also been utilized in models such as Whisper [7] and speech transla- tion [8], and their usefulness has been reevaluated. However, there are various trade-offs that can potentially limit its application in cer- tain scenarios. For example, one can construct an ASR system with high accuracy through large and complex models, but this comes at the expense of increased computational cost and inference time. time compared to AR models [17]. However, it remains a challenge to achieve the same level of recognition accuracy as AR models. In addition, NAR models require a complex model structure or a unique training strategy for the successful implementation of parallel generation during inference [17]. Regarding AR decoding, the decoder is trained to learn linguis- tic information. This allows us to utilize the relationship between the current token and the previous token. Furthermore, it is common to enhance accuracy by implementing the beam search algorithm, which is a heuristic search for the best hypothesis. However, be- cause AR requires the previous tokens to estimate the next token, it is not possible to parallelize the inference of a single audio. As a consequence, achieving NAR-level speed through inference paral- lelization is challenging. In this paper, we focus on the difference in trade-off balance be- tween AR and NAR, and propose a new decoding method, the par- tially autoregressive (PAR) method as a fast and accurate decoding method. By utilizing the NAR approach and segment-level vector- ized beam search, we can compensate for the weaknesses of both AR and NAR. This results in a better trade-off between accuracy and latency. In particular, we show that by optimizing the inference operations of the pre-trained hybrid connectionist temporal classifi- cation (CTC) and attention model, we are able to achieve NAR-level inference speed while maintaining its high accuracy, as well as not needing additional training of the model. In this paper, we make the following contributions: \u2022 We propose a new decoding framework that combines AR and NAR. We demonstrate a better balance of accuracy-latency trade-off without additional training. 2. RELATED WORKS Numerous studies have been conducted to balance the accuracy- latency trade-off. In general, there are two main approaches; reduc- ing latency while preserving good accuracy or improving accuracy while preserving fast inference speed. There has been extensive research devoted to alleviating the In- trade-off between recognition accuracy and inference speed. spired by the great success in neural machine translation [9, 10], non-autoregressive (NAR) models have been actively studied in the context of ASR, with the aim of achieving fast inference [11\u201314]. Compared to the conventional autoregressive (AR) models [15, 16], which generates output at each step conditioned on the previously generated outputs, NAR models can produce multiple outputs si- multaneously. This parallel computation accelerates the inference process of ASR, resulting in a significant reduction in inference To reduce latency, there are several methods to lower the com- putational cost during inference. One such method is the pruning technique [18\u201320], which reduces the number of parameters in a trained model by identifying subnetworks with better performance and fewer parameters. Another method is knowledge distillation [21, 22], which trains a smaller model to reduce the number of param- eters required during inference. To further reduce the number of search iterations, hypotheses can be predicted in a batch [23], or the search process can be stopped prematurely [24]. In addition, other approaches focus on utilizing machine resources more effi- ciently [25\u201327], rather than relying solely on the model architecture, 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE (a) Autoregressive (AR) (b) Non-autoregressive (NAR) (c) Partially autoregressive (PAR) Fig. 1: Overview and comparison of AR, NAR, and PAR decoding. <sos> denotes the \u201cstart-of-sequence\u201d symbol,and the mask token is denoted by # or red characters. PAR is a hybrid of AR and NAR methods, in which the masking process is applied first, followed by segment-level vectorized beam search. to achieve faster processing. In this study, we parallelized the AR decoding of single audio to achieve high-speed processing. The pro- cess is similar to batch processing of hypotheses, but in this case, the audio is segmented and parallelized in addition to the hypotheses. On the"}, {"question": " What is the difference between autoregressive (AR) and non-autoregressive (NAR) models in terms of inference speed?", "answer": " NAR models can produce multiple outputs simultaneously, accelerating the inference process.", "ref_chunk": "3 2 0 2 t c O 1 ] S A . s s e e [ 2 v 2 2 9 4 1 . 9 0 3 2 : v i X r a SEGMENT-LEVEL VECTORIZED BEAM SEARCH BASED ON PARTIALLY AUTOREGRESSIVE INFERENCE Masao Someki1, Nicholas Eng2, Yosuke Higuchi3, Shinji Watanabe4 1IBM Japan Ltd., Japan 2The University of Auckland, New Zealand 3Waseda University, Japan 4Carnegie Mellon University, USA ABSTRACT Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothe- sis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Ex- perimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy. Index Terms\u2014 Decoding algorithm, autoregressive, semi- autoregressive, hybrid CTC/attention, beam search 1. INTRODUCTION Due to recent advances in deep learning, automatic speech recog- nition (ASR) has witnessed remarkable achievements [1\u20133]. ASR plays an essential role in facilitating human-computer interaction by providing an interface for converting audio to text and demonstrat- ing substantial applicability in real-world scenarios. In particular, the RNN-Transducer model [4], which operates relatively fast and can be extended to streaming speech recognition, is widely used in real-world applications. Recent research in ASR has also made significant progress in achieving higher accuracy through Attention- based Encoder-Decoder (AED) models [5, 6]. AED models have also been utilized in models such as Whisper [7] and speech transla- tion [8], and their usefulness has been reevaluated. However, there are various trade-offs that can potentially limit its application in cer- tain scenarios. For example, one can construct an ASR system with high accuracy through large and complex models, but this comes at the expense of increased computational cost and inference time. time compared to AR models [17]. However, it remains a challenge to achieve the same level of recognition accuracy as AR models. In addition, NAR models require a complex model structure or a unique training strategy for the successful implementation of parallel generation during inference [17]. Regarding AR decoding, the decoder is trained to learn linguis- tic information. This allows us to utilize the relationship between the current token and the previous token. Furthermore, it is common to enhance accuracy by implementing the beam search algorithm, which is a heuristic search for the best hypothesis. However, be- cause AR requires the previous tokens to estimate the next token, it is not possible to parallelize the inference of a single audio. As a consequence, achieving NAR-level speed through inference paral- lelization is challenging. In this paper, we focus on the difference in trade-off balance be- tween AR and NAR, and propose a new decoding method, the par- tially autoregressive (PAR) method as a fast and accurate decoding method. By utilizing the NAR approach and segment-level vector- ized beam search, we can compensate for the weaknesses of both AR and NAR. This results in a better trade-off between accuracy and latency. In particular, we show that by optimizing the inference operations of the pre-trained hybrid connectionist temporal classifi- cation (CTC) and attention model, we are able to achieve NAR-level inference speed while maintaining its high accuracy, as well as not needing additional training of the model. In this paper, we make the following contributions: \u2022 We propose a new decoding framework that combines AR and NAR. We demonstrate a better balance of accuracy-latency trade-off without additional training. 2. RELATED WORKS Numerous studies have been conducted to balance the accuracy- latency trade-off. In general, there are two main approaches; reduc- ing latency while preserving good accuracy or improving accuracy while preserving fast inference speed. There has been extensive research devoted to alleviating the In- trade-off between recognition accuracy and inference speed. spired by the great success in neural machine translation [9, 10], non-autoregressive (NAR) models have been actively studied in the context of ASR, with the aim of achieving fast inference [11\u201314]. Compared to the conventional autoregressive (AR) models [15, 16], which generates output at each step conditioned on the previously generated outputs, NAR models can produce multiple outputs si- multaneously. This parallel computation accelerates the inference process of ASR, resulting in a significant reduction in inference To reduce latency, there are several methods to lower the com- putational cost during inference. One such method is the pruning technique [18\u201320], which reduces the number of parameters in a trained model by identifying subnetworks with better performance and fewer parameters. Another method is knowledge distillation [21, 22], which trains a smaller model to reduce the number of param- eters required during inference. To further reduce the number of search iterations, hypotheses can be predicted in a batch [23], or the search process can be stopped prematurely [24]. In addition, other approaches focus on utilizing machine resources more effi- ciently [25\u201327], rather than relying solely on the model architecture, 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE (a) Autoregressive (AR) (b) Non-autoregressive (NAR) (c) Partially autoregressive (PAR) Fig. 1: Overview and comparison of AR, NAR, and PAR decoding. <sos> denotes the \u201cstart-of-sequence\u201d symbol,and the mask token is denoted by # or red characters. PAR is a hybrid of AR and NAR methods, in which the masking process is applied first, followed by segment-level vectorized beam search. to achieve faster processing. In this study, we parallelized the AR decoding of single audio to achieve high-speed processing. The pro- cess is similar to batch processing of hypotheses, but in this case, the audio is segmented and parallelized in addition to the hypotheses. On the"}, {"question": " What is the main challenge in achieving non-autoregressive (NAR)-level speed through inference parallelization?", "answer": " AR models require previous tokens to estimate the next token, hindering parallelization.", "ref_chunk": "3 2 0 2 t c O 1 ] S A . s s e e [ 2 v 2 2 9 4 1 . 9 0 3 2 : v i X r a SEGMENT-LEVEL VECTORIZED BEAM SEARCH BASED ON PARTIALLY AUTOREGRESSIVE INFERENCE Masao Someki1, Nicholas Eng2, Yosuke Higuchi3, Shinji Watanabe4 1IBM Japan Ltd., Japan 2The University of Auckland, New Zealand 3Waseda University, Japan 4Carnegie Mellon University, USA ABSTRACT Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothe- sis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Ex- perimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy. Index Terms\u2014 Decoding algorithm, autoregressive, semi- autoregressive, hybrid CTC/attention, beam search 1. INTRODUCTION Due to recent advances in deep learning, automatic speech recog- nition (ASR) has witnessed remarkable achievements [1\u20133]. ASR plays an essential role in facilitating human-computer interaction by providing an interface for converting audio to text and demonstrat- ing substantial applicability in real-world scenarios. In particular, the RNN-Transducer model [4], which operates relatively fast and can be extended to streaming speech recognition, is widely used in real-world applications. Recent research in ASR has also made significant progress in achieving higher accuracy through Attention- based Encoder-Decoder (AED) models [5, 6]. AED models have also been utilized in models such as Whisper [7] and speech transla- tion [8], and their usefulness has been reevaluated. However, there are various trade-offs that can potentially limit its application in cer- tain scenarios. For example, one can construct an ASR system with high accuracy through large and complex models, but this comes at the expense of increased computational cost and inference time. time compared to AR models [17]. However, it remains a challenge to achieve the same level of recognition accuracy as AR models. In addition, NAR models require a complex model structure or a unique training strategy for the successful implementation of parallel generation during inference [17]. Regarding AR decoding, the decoder is trained to learn linguis- tic information. This allows us to utilize the relationship between the current token and the previous token. Furthermore, it is common to enhance accuracy by implementing the beam search algorithm, which is a heuristic search for the best hypothesis. However, be- cause AR requires the previous tokens to estimate the next token, it is not possible to parallelize the inference of a single audio. As a consequence, achieving NAR-level speed through inference paral- lelization is challenging. In this paper, we focus on the difference in trade-off balance be- tween AR and NAR, and propose a new decoding method, the par- tially autoregressive (PAR) method as a fast and accurate decoding method. By utilizing the NAR approach and segment-level vector- ized beam search, we can compensate for the weaknesses of both AR and NAR. This results in a better trade-off between accuracy and latency. In particular, we show that by optimizing the inference operations of the pre-trained hybrid connectionist temporal classifi- cation (CTC) and attention model, we are able to achieve NAR-level inference speed while maintaining its high accuracy, as well as not needing additional training of the model. In this paper, we make the following contributions: \u2022 We propose a new decoding framework that combines AR and NAR. We demonstrate a better balance of accuracy-latency trade-off without additional training. 2. RELATED WORKS Numerous studies have been conducted to balance the accuracy- latency trade-off. In general, there are two main approaches; reduc- ing latency while preserving good accuracy or improving accuracy while preserving fast inference speed. There has been extensive research devoted to alleviating the In- trade-off between recognition accuracy and inference speed. spired by the great success in neural machine translation [9, 10], non-autoregressive (NAR) models have been actively studied in the context of ASR, with the aim of achieving fast inference [11\u201314]. Compared to the conventional autoregressive (AR) models [15, 16], which generates output at each step conditioned on the previously generated outputs, NAR models can produce multiple outputs si- multaneously. This parallel computation accelerates the inference process of ASR, resulting in a significant reduction in inference To reduce latency, there are several methods to lower the com- putational cost during inference. One such method is the pruning technique [18\u201320], which reduces the number of parameters in a trained model by identifying subnetworks with better performance and fewer parameters. Another method is knowledge distillation [21, 22], which trains a smaller model to reduce the number of param- eters required during inference. To further reduce the number of search iterations, hypotheses can be predicted in a batch [23], or the search process can be stopped prematurely [24]. In addition, other approaches focus on utilizing machine resources more effi- ciently [25\u201327], rather than relying solely on the model architecture, 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE (a) Autoregressive (AR) (b) Non-autoregressive (NAR) (c) Partially autoregressive (PAR) Fig. 1: Overview and comparison of AR, NAR, and PAR decoding. <sos> denotes the \u201cstart-of-sequence\u201d symbol,and the mask token is denoted by # or red characters. PAR is a hybrid of AR and NAR methods, in which the masking process is applied first, followed by segment-level vectorized beam search. to achieve faster processing. In this study, we parallelized the AR decoding of single audio to achieve high-speed processing. The pro- cess is similar to batch processing of hypotheses, but in this case, the audio is segmented and parallelized in addition to the hypotheses. On the"}, {"question": " What decoding method is proposed in the text as a fast and accurate decoding method?", "answer": " Partially autoregressive (PAR) method.", "ref_chunk": "3 2 0 2 t c O 1 ] S A . s s e e [ 2 v 2 2 9 4 1 . 9 0 3 2 : v i X r a SEGMENT-LEVEL VECTORIZED BEAM SEARCH BASED ON PARTIALLY AUTOREGRESSIVE INFERENCE Masao Someki1, Nicholas Eng2, Yosuke Higuchi3, Shinji Watanabe4 1IBM Japan Ltd., Japan 2The University of Auckland, New Zealand 3Waseda University, Japan 4Carnegie Mellon University, USA ABSTRACT Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothe- sis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Ex- perimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy. Index Terms\u2014 Decoding algorithm, autoregressive, semi- autoregressive, hybrid CTC/attention, beam search 1. INTRODUCTION Due to recent advances in deep learning, automatic speech recog- nition (ASR) has witnessed remarkable achievements [1\u20133]. ASR plays an essential role in facilitating human-computer interaction by providing an interface for converting audio to text and demonstrat- ing substantial applicability in real-world scenarios. In particular, the RNN-Transducer model [4], which operates relatively fast and can be extended to streaming speech recognition, is widely used in real-world applications. Recent research in ASR has also made significant progress in achieving higher accuracy through Attention- based Encoder-Decoder (AED) models [5, 6]. AED models have also been utilized in models such as Whisper [7] and speech transla- tion [8], and their usefulness has been reevaluated. However, there are various trade-offs that can potentially limit its application in cer- tain scenarios. For example, one can construct an ASR system with high accuracy through large and complex models, but this comes at the expense of increased computational cost and inference time. time compared to AR models [17]. However, it remains a challenge to achieve the same level of recognition accuracy as AR models. In addition, NAR models require a complex model structure or a unique training strategy for the successful implementation of parallel generation during inference [17]. Regarding AR decoding, the decoder is trained to learn linguis- tic information. This allows us to utilize the relationship between the current token and the previous token. Furthermore, it is common to enhance accuracy by implementing the beam search algorithm, which is a heuristic search for the best hypothesis. However, be- cause AR requires the previous tokens to estimate the next token, it is not possible to parallelize the inference of a single audio. As a consequence, achieving NAR-level speed through inference paral- lelization is challenging. In this paper, we focus on the difference in trade-off balance be- tween AR and NAR, and propose a new decoding method, the par- tially autoregressive (PAR) method as a fast and accurate decoding method. By utilizing the NAR approach and segment-level vector- ized beam search, we can compensate for the weaknesses of both AR and NAR. This results in a better trade-off between accuracy and latency. In particular, we show that by optimizing the inference operations of the pre-trained hybrid connectionist temporal classifi- cation (CTC) and attention model, we are able to achieve NAR-level inference speed while maintaining its high accuracy, as well as not needing additional training of the model. In this paper, we make the following contributions: \u2022 We propose a new decoding framework that combines AR and NAR. We demonstrate a better balance of accuracy-latency trade-off without additional training. 2. RELATED WORKS Numerous studies have been conducted to balance the accuracy- latency trade-off. In general, there are two main approaches; reduc- ing latency while preserving good accuracy or improving accuracy while preserving fast inference speed. There has been extensive research devoted to alleviating the In- trade-off between recognition accuracy and inference speed. spired by the great success in neural machine translation [9, 10], non-autoregressive (NAR) models have been actively studied in the context of ASR, with the aim of achieving fast inference [11\u201314]. Compared to the conventional autoregressive (AR) models [15, 16], which generates output at each step conditioned on the previously generated outputs, NAR models can produce multiple outputs si- multaneously. This parallel computation accelerates the inference process of ASR, resulting in a significant reduction in inference To reduce latency, there are several methods to lower the com- putational cost during inference. One such method is the pruning technique [18\u201320], which reduces the number of parameters in a trained model by identifying subnetworks with better performance and fewer parameters. Another method is knowledge distillation [21, 22], which trains a smaller model to reduce the number of param- eters required during inference. To further reduce the number of search iterations, hypotheses can be predicted in a batch [23], or the search process can be stopped prematurely [24]. In addition, other approaches focus on utilizing machine resources more effi- ciently [25\u201327], rather than relying solely on the model architecture, 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE (a) Autoregressive (AR) (b) Non-autoregressive (NAR) (c) Partially autoregressive (PAR) Fig. 1: Overview and comparison of AR, NAR, and PAR decoding. <sos> denotes the \u201cstart-of-sequence\u201d symbol,and the mask token is denoted by # or red characters. PAR is a hybrid of AR and NAR methods, in which the masking process is applied first, followed by segment-level vectorized beam search. to achieve faster processing. In this study, we parallelized the AR decoding of single audio to achieve high-speed processing. The pro- cess is similar to batch processing of hypotheses, but in this case, the audio is segmented and parallelized in addition to the hypotheses. On the"}, {"question": " What is the key contribution of the proposed framework highlighted in the text?", "answer": " Combining autoregressive and non-autoregressive methods to achieve a better balance of accuracy-latency trade-off.", "ref_chunk": "3 2 0 2 t c O 1 ] S A . s s e e [ 2 v 2 2 9 4 1 . 9 0 3 2 : v i X r a SEGMENT-LEVEL VECTORIZED BEAM SEARCH BASED ON PARTIALLY AUTOREGRESSIVE INFERENCE Masao Someki1, Nicholas Eng2, Yosuke Higuchi3, Shinji Watanabe4 1IBM Japan Ltd., Japan 2The University of Auckland, New Zealand 3Waseda University, Japan 4Carnegie Mellon University, USA ABSTRACT Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothe- sis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Ex- perimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy. Index Terms\u2014 Decoding algorithm, autoregressive, semi- autoregressive, hybrid CTC/attention, beam search 1. INTRODUCTION Due to recent advances in deep learning, automatic speech recog- nition (ASR) has witnessed remarkable achievements [1\u20133]. ASR plays an essential role in facilitating human-computer interaction by providing an interface for converting audio to text and demonstrat- ing substantial applicability in real-world scenarios. In particular, the RNN-Transducer model [4], which operates relatively fast and can be extended to streaming speech recognition, is widely used in real-world applications. Recent research in ASR has also made significant progress in achieving higher accuracy through Attention- based Encoder-Decoder (AED) models [5, 6]. AED models have also been utilized in models such as Whisper [7] and speech transla- tion [8], and their usefulness has been reevaluated. However, there are various trade-offs that can potentially limit its application in cer- tain scenarios. For example, one can construct an ASR system with high accuracy through large and complex models, but this comes at the expense of increased computational cost and inference time. time compared to AR models [17]. However, it remains a challenge to achieve the same level of recognition accuracy as AR models. In addition, NAR models require a complex model structure or a unique training strategy for the successful implementation of parallel generation during inference [17]. Regarding AR decoding, the decoder is trained to learn linguis- tic information. This allows us to utilize the relationship between the current token and the previous token. Furthermore, it is common to enhance accuracy by implementing the beam search algorithm, which is a heuristic search for the best hypothesis. However, be- cause AR requires the previous tokens to estimate the next token, it is not possible to parallelize the inference of a single audio. As a consequence, achieving NAR-level speed through inference paral- lelization is challenging. In this paper, we focus on the difference in trade-off balance be- tween AR and NAR, and propose a new decoding method, the par- tially autoregressive (PAR) method as a fast and accurate decoding method. By utilizing the NAR approach and segment-level vector- ized beam search, we can compensate for the weaknesses of both AR and NAR. This results in a better trade-off between accuracy and latency. In particular, we show that by optimizing the inference operations of the pre-trained hybrid connectionist temporal classifi- cation (CTC) and attention model, we are able to achieve NAR-level inference speed while maintaining its high accuracy, as well as not needing additional training of the model. In this paper, we make the following contributions: \u2022 We propose a new decoding framework that combines AR and NAR. We demonstrate a better balance of accuracy-latency trade-off without additional training. 2. RELATED WORKS Numerous studies have been conducted to balance the accuracy- latency trade-off. In general, there are two main approaches; reduc- ing latency while preserving good accuracy or improving accuracy while preserving fast inference speed. There has been extensive research devoted to alleviating the In- trade-off between recognition accuracy and inference speed. spired by the great success in neural machine translation [9, 10], non-autoregressive (NAR) models have been actively studied in the context of ASR, with the aim of achieving fast inference [11\u201314]. Compared to the conventional autoregressive (AR) models [15, 16], which generates output at each step conditioned on the previously generated outputs, NAR models can produce multiple outputs si- multaneously. This parallel computation accelerates the inference process of ASR, resulting in a significant reduction in inference To reduce latency, there are several methods to lower the com- putational cost during inference. One such method is the pruning technique [18\u201320], which reduces the number of parameters in a trained model by identifying subnetworks with better performance and fewer parameters. Another method is knowledge distillation [21, 22], which trains a smaller model to reduce the number of param- eters required during inference. To further reduce the number of search iterations, hypotheses can be predicted in a batch [23], or the search process can be stopped prematurely [24]. In addition, other approaches focus on utilizing machine resources more effi- ciently [25\u201327], rather than relying solely on the model architecture, 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE (a) Autoregressive (AR) (b) Non-autoregressive (NAR) (c) Partially autoregressive (PAR) Fig. 1: Overview and comparison of AR, NAR, and PAR decoding. <sos> denotes the \u201cstart-of-sequence\u201d symbol,and the mask token is denoted by # or red characters. PAR is a hybrid of AR and NAR methods, in which the masking process is applied first, followed by segment-level vectorized beam search. to achieve faster processing. In this study, we parallelized the AR decoding of single audio to achieve high-speed processing. The pro- cess is similar to batch processing of hypotheses, but in this case, the audio is segmented and parallelized in addition to the hypotheses. On the"}], "doc_text": "3 2 0 2 t c O 1 ] S A . s s e e [ 2 v 2 2 9 4 1 . 9 0 3 2 : v i X r a SEGMENT-LEVEL VECTORIZED BEAM SEARCH BASED ON PARTIALLY AUTOREGRESSIVE INFERENCE Masao Someki1, Nicholas Eng2, Yosuke Higuchi3, Shinji Watanabe4 1IBM Japan Ltd., Japan 2The University of Auckland, New Zealand 3Waseda University, Japan 4Carnegie Mellon University, USA ABSTRACT Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothe- sis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Ex- perimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy. Index Terms\u2014 Decoding algorithm, autoregressive, semi- autoregressive, hybrid CTC/attention, beam search 1. INTRODUCTION Due to recent advances in deep learning, automatic speech recog- nition (ASR) has witnessed remarkable achievements [1\u20133]. ASR plays an essential role in facilitating human-computer interaction by providing an interface for converting audio to text and demonstrat- ing substantial applicability in real-world scenarios. In particular, the RNN-Transducer model [4], which operates relatively fast and can be extended to streaming speech recognition, is widely used in real-world applications. Recent research in ASR has also made significant progress in achieving higher accuracy through Attention- based Encoder-Decoder (AED) models [5, 6]. AED models have also been utilized in models such as Whisper [7] and speech transla- tion [8], and their usefulness has been reevaluated. However, there are various trade-offs that can potentially limit its application in cer- tain scenarios. For example, one can construct an ASR system with high accuracy through large and complex models, but this comes at the expense of increased computational cost and inference time. time compared to AR models [17]. However, it remains a challenge to achieve the same level of recognition accuracy as AR models. In addition, NAR models require a complex model structure or a unique training strategy for the successful implementation of parallel generation during inference [17]. Regarding AR decoding, the decoder is trained to learn linguis- tic information. This allows us to utilize the relationship between the current token and the previous token. Furthermore, it is common to enhance accuracy by implementing the beam search algorithm, which is a heuristic search for the best hypothesis. However, be- cause AR requires the previous tokens to estimate the next token, it is not possible to parallelize the inference of a single audio. As a consequence, achieving NAR-level speed through inference paral- lelization is challenging. In this paper, we focus on the difference in trade-off balance be- tween AR and NAR, and propose a new decoding method, the par- tially autoregressive (PAR) method as a fast and accurate decoding method. By utilizing the NAR approach and segment-level vector- ized beam search, we can compensate for the weaknesses of both AR and NAR. This results in a better trade-off between accuracy and latency. In particular, we show that by optimizing the inference operations of the pre-trained hybrid connectionist temporal classifi- cation (CTC) and attention model, we are able to achieve NAR-level inference speed while maintaining its high accuracy, as well as not needing additional training of the model. In this paper, we make the following contributions: \u2022 We propose a new decoding framework that combines AR and NAR. We demonstrate a better balance of accuracy-latency trade-off without additional training. 2. RELATED WORKS Numerous studies have been conducted to balance the accuracy- latency trade-off. In general, there are two main approaches; reduc- ing latency while preserving good accuracy or improving accuracy while preserving fast inference speed. There has been extensive research devoted to alleviating the In- trade-off between recognition accuracy and inference speed. spired by the great success in neural machine translation [9, 10], non-autoregressive (NAR) models have been actively studied in the context of ASR, with the aim of achieving fast inference [11\u201314]. Compared to the conventional autoregressive (AR) models [15, 16], which generates output at each step conditioned on the previously generated outputs, NAR models can produce multiple outputs si- multaneously. This parallel computation accelerates the inference process of ASR, resulting in a significant reduction in inference To reduce latency, there are several methods to lower the com- putational cost during inference. One such method is the pruning technique [18\u201320], which reduces the number of parameters in a trained model by identifying subnetworks with better performance and fewer parameters. Another method is knowledge distillation [21, 22], which trains a smaller model to reduce the number of param- eters required during inference. To further reduce the number of search iterations, hypotheses can be predicted in a batch [23], or the search process can be stopped prematurely [24]. In addition, other approaches focus on utilizing machine resources more effi- ciently [25\u201327], rather than relying solely on the model architecture, 979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE (a) Autoregressive (AR) (b) Non-autoregressive (NAR) (c) Partially autoregressive (PAR) Fig. 1: Overview and comparison of AR, NAR, and PAR decoding. <sos> denotes the \u201cstart-of-sequence\u201d symbol,and the mask token is denoted by # or red characters. PAR is a hybrid of AR and NAR methods, in which the masking process is applied first, followed by segment-level vectorized beam search. to achieve faster processing. In this study, we parallelized the AR decoding of single audio to achieve high-speed processing. The pro- cess is similar to batch processing of hypotheses, but in this case, the audio is segmented and parallelized in addition to the hypotheses. On the"}