{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Cross-Modal_Fine-Tuning:_Align_then_Refine_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " According to the text, what are some potential disadvantages of optimizing directly for the task loss during fine-tuning?,answer: Optimizing directly for the task loss may distort the pretrained weights and lead to suboptimal solutions.", "ref_chunk": "quality. Right: Accuracy (\u2191) of ORCA vs. naive \ufb01ne-tuning with varying dataset size on task Satellite. ORCA has higher performance gains in low-data regime. transfer. Indeed, recent work has shown that optimizing directly for the task loss may distort the pretrained weights and lead to suboptimal solutions (Kumar et al., 2022; Lee et al., 2022). By manipulating the target distribution to look like the source distribution, we lower the risk of weight distortion, thus obtaining better downstream performance. We also quantify the effect of data alignment by training the embedder for different number of epochs and see whether optimizing distribution distance to various levels of conver- gence affects downstream performance. Figure 4 (left) plots the \ufb01ne-tuning accuracy and the \ufb01nal distribution distance for different embedder learning levels. We see that as the dataset distance decreases, the \ufb01ne-tuning accuracy in- creases. In addition, learning the embedder separately from \ufb01ne-tuning stabilizes training, as the performance variance of ORCA is constantly lower than that of naive \ufb01ne-tuning. These results con\ufb01rm that data alignment is the key to effec- tive cross-modal \ufb01ne-tuning. KEY 3: ADAPTING FROM THE RIGHT MODALITY Finally, we study how the pretraining modality affects \ufb01ne- tuning. In the results reported so far, we choose pretrained models for each task based on the input dimension, i.e., we use RoBERTa for all 1D tasks and Swin for all 2D tasks. Now, we evaluate the opposite approach, focusing on two tasks: DeepSEA (1D) and Spherical (2D). This evaluation is straightforward to perform by switching the model bodies, since the embedder architecture of ORCA handles all input transformations needed to obtain the sequence features. The results are shown in Table 13 in the Appendix. We see that \ufb01ne-tuned RoBERTa outperforms Swin on the 1D task, possibly because the DeepSEA data (genomics sequences) are structured more like language than images with discrete units of information and general grammatical rules. More crucially, for both tasks, models with smaller \ufb01nal OTDDs have better \ufb01ne-tuning accuracy. This suggests a way of selecting pretrained models by comparing the optimized OTDDs and picking the one with the smallest value. KEY 2: FINE-TUNING ALL MODEL PARAMETERS As discussed in Section 2, Frozen Pretrained Transformers (FPT) (Lu et al., 2022) is a related work that showed pre- trained language models contain knowledge relevant to out- of-modality tasks. While FPT presented a general pipeline for adapting GPT-2 to tasks like CIFAR-10, the resulting models were not as good as those trained from scratch. FPT differs from ORCA in that (1) it does not perform data alignment, and (2) it only \ufb01ne-tunes the layer norms. We have veri\ufb01ed the importance of (1). Now, we isolate the impact of (2) by \ufb01ne-tuning only the layer norms for ORCA. The bottom rows of Table 3 show that ORCA with \ufb01ne- tuning the layer norms outperforms FPT, so pretraining the embedder can boost the performance of FPT. However, this performance gain is smaller than that in the full \ufb01ne-tuning setting, which implies that full \ufb01ne-tuning can take better advantage of the learned embeddings. In terms of run- time, FPT yields less than a 2\u00d7 speedup compared with full \ufb01ne-tuning (Appendix A.4.6), despite the fact that we are updating many fewer parameters. This is unsurprising since gradients are still back-propagated through the entire net- work. Therefore, when computation allows, we recommend using ORCA with full \ufb01ne-tuning for better performance. Apart from these three key insights, recall that one of our motivations for cross-modal \ufb01ne-tuning is to help tasks with limited data, where training models from scratch is dif\ufb01cult. Indeed, for vanilla \ufb01ne-tuning, a small amount of data may not give enough signal to update the pretrained weights, but it is possible to learn a good embedder \ufb01rst with ORCA, which can then make \ufb01ne-tuning easier. In Figure 4 (right), we vary the dataset size and \ufb01nd that the performance gain of ORCA increases as the dataset size decreases. Mean- while, using ORCA allows us to match the performance of naive \ufb01ne-tuning on 3\u00d7 amount of data. Thus, it can bene\ufb01t model development in domains where data collection is costly. Beyond the cross-modal setting, we also verify ORCA\u2019s ef\ufb01cacy for in-modality transfer in Appendix A.8.1. 4.2. A Depth Perspective: Cross-Modal Fine-Tuning for PDE and Tabular Tasks After validating ORCA on a broad set of tasks, we dive into two speci\ufb01c modalities, PDE solving and tabular classi\ufb01ca- tion, to show that cross-modal \ufb01ne-tuning is promising for model development in highly specialized areas. ORCA can not only achieve high prediction accuracy in both domains, Cross-Modal Fine-Tuning: Align then Re\ufb01ne Diff-React 101 # wins vs. U-Net: 6/6 PINN 101 NaiverStokes 101 Diff-React 2DPDEs 102 102 # wins vs. FNO: 4/8 # wins vs. PINN: 8/8 Advection ShallowWater 102 Diff-Sorp DarcyFlow 100 100 100 Burgers FNO nRMSE ORCA U-Net Figure 5: Left: Normalized Root Mean Squared Errors (nRMSEs, \u2193) for ORCA vs. baselines on 8 PDEBench tasks with varying dimensions (1D/2D). We only evaluate datasets that can \ufb01t into a single V100 GPU. Overall, ORCA is much better than U-Net and PINN and on par with FNO. For detailed numerical results, see Table 14 in the Appendix. Right: ORCA is trained on resolution 256 and directly evaluated on resolution 512. The prediction still matches the ground truth. but also recover an important property of Neural Operators\u2014 modeling PDEs with zero-shot super-resolution. PDEBENCH FOR SCIENTIFIC ML Table 4: Tabular results with baselines from Hollmann et al. (2022) and Dinh et al. (2022). \u201cDiff. from XGBoost\u201d is the across- task average of per-task difference from XGBoost. ORCA beats classical approaches and advanced transformer methods on 19 tasks. For per-task results, see Appendix A.6. ML models for physical systems have gained increasing interest in recent years. To study how cross-modal \ufb01ne- tuning can help in the scienti\ufb01c ML context, we evaluate ORCA on 8 datasets from PDEBench (Takamoto et al., 2022) and compare against state-of-the-art task-speci\ufb01c models: the physics-informed neural network PINN (Raissi et al., 2019), Fourier neural operator (FNO) (Li et al., 2021), and the generic"}, {"question": " How does manipulating the target distribution to look like the source distribution help in fine-tuning?,answer: Manipulating the target distribution to look like the source distribution lowers the risk of weight distortion, thus obtaining better downstream performance.", "ref_chunk": "quality. Right: Accuracy (\u2191) of ORCA vs. naive \ufb01ne-tuning with varying dataset size on task Satellite. ORCA has higher performance gains in low-data regime. transfer. Indeed, recent work has shown that optimizing directly for the task loss may distort the pretrained weights and lead to suboptimal solutions (Kumar et al., 2022; Lee et al., 2022). By manipulating the target distribution to look like the source distribution, we lower the risk of weight distortion, thus obtaining better downstream performance. We also quantify the effect of data alignment by training the embedder for different number of epochs and see whether optimizing distribution distance to various levels of conver- gence affects downstream performance. Figure 4 (left) plots the \ufb01ne-tuning accuracy and the \ufb01nal distribution distance for different embedder learning levels. We see that as the dataset distance decreases, the \ufb01ne-tuning accuracy in- creases. In addition, learning the embedder separately from \ufb01ne-tuning stabilizes training, as the performance variance of ORCA is constantly lower than that of naive \ufb01ne-tuning. These results con\ufb01rm that data alignment is the key to effec- tive cross-modal \ufb01ne-tuning. KEY 3: ADAPTING FROM THE RIGHT MODALITY Finally, we study how the pretraining modality affects \ufb01ne- tuning. In the results reported so far, we choose pretrained models for each task based on the input dimension, i.e., we use RoBERTa for all 1D tasks and Swin for all 2D tasks. Now, we evaluate the opposite approach, focusing on two tasks: DeepSEA (1D) and Spherical (2D). This evaluation is straightforward to perform by switching the model bodies, since the embedder architecture of ORCA handles all input transformations needed to obtain the sequence features. The results are shown in Table 13 in the Appendix. We see that \ufb01ne-tuned RoBERTa outperforms Swin on the 1D task, possibly because the DeepSEA data (genomics sequences) are structured more like language than images with discrete units of information and general grammatical rules. More crucially, for both tasks, models with smaller \ufb01nal OTDDs have better \ufb01ne-tuning accuracy. This suggests a way of selecting pretrained models by comparing the optimized OTDDs and picking the one with the smallest value. KEY 2: FINE-TUNING ALL MODEL PARAMETERS As discussed in Section 2, Frozen Pretrained Transformers (FPT) (Lu et al., 2022) is a related work that showed pre- trained language models contain knowledge relevant to out- of-modality tasks. While FPT presented a general pipeline for adapting GPT-2 to tasks like CIFAR-10, the resulting models were not as good as those trained from scratch. FPT differs from ORCA in that (1) it does not perform data alignment, and (2) it only \ufb01ne-tunes the layer norms. We have veri\ufb01ed the importance of (1). Now, we isolate the impact of (2) by \ufb01ne-tuning only the layer norms for ORCA. The bottom rows of Table 3 show that ORCA with \ufb01ne- tuning the layer norms outperforms FPT, so pretraining the embedder can boost the performance of FPT. However, this performance gain is smaller than that in the full \ufb01ne-tuning setting, which implies that full \ufb01ne-tuning can take better advantage of the learned embeddings. In terms of run- time, FPT yields less than a 2\u00d7 speedup compared with full \ufb01ne-tuning (Appendix A.4.6), despite the fact that we are updating many fewer parameters. This is unsurprising since gradients are still back-propagated through the entire net- work. Therefore, when computation allows, we recommend using ORCA with full \ufb01ne-tuning for better performance. Apart from these three key insights, recall that one of our motivations for cross-modal \ufb01ne-tuning is to help tasks with limited data, where training models from scratch is dif\ufb01cult. Indeed, for vanilla \ufb01ne-tuning, a small amount of data may not give enough signal to update the pretrained weights, but it is possible to learn a good embedder \ufb01rst with ORCA, which can then make \ufb01ne-tuning easier. In Figure 4 (right), we vary the dataset size and \ufb01nd that the performance gain of ORCA increases as the dataset size decreases. Mean- while, using ORCA allows us to match the performance of naive \ufb01ne-tuning on 3\u00d7 amount of data. Thus, it can bene\ufb01t model development in domains where data collection is costly. Beyond the cross-modal setting, we also verify ORCA\u2019s ef\ufb01cacy for in-modality transfer in Appendix A.8.1. 4.2. A Depth Perspective: Cross-Modal Fine-Tuning for PDE and Tabular Tasks After validating ORCA on a broad set of tasks, we dive into two speci\ufb01c modalities, PDE solving and tabular classi\ufb01ca- tion, to show that cross-modal \ufb01ne-tuning is promising for model development in highly specialized areas. ORCA can not only achieve high prediction accuracy in both domains, Cross-Modal Fine-Tuning: Align then Re\ufb01ne Diff-React 101 # wins vs. U-Net: 6/6 PINN 101 NaiverStokes 101 Diff-React 2DPDEs 102 102 # wins vs. FNO: 4/8 # wins vs. PINN: 8/8 Advection ShallowWater 102 Diff-Sorp DarcyFlow 100 100 100 Burgers FNO nRMSE ORCA U-Net Figure 5: Left: Normalized Root Mean Squared Errors (nRMSEs, \u2193) for ORCA vs. baselines on 8 PDEBench tasks with varying dimensions (1D/2D). We only evaluate datasets that can \ufb01t into a single V100 GPU. Overall, ORCA is much better than U-Net and PINN and on par with FNO. For detailed numerical results, see Table 14 in the Appendix. Right: ORCA is trained on resolution 256 and directly evaluated on resolution 512. The prediction still matches the ground truth. but also recover an important property of Neural Operators\u2014 modeling PDEs with zero-shot super-resolution. PDEBENCH FOR SCIENTIFIC ML Table 4: Tabular results with baselines from Hollmann et al. (2022) and Dinh et al. (2022). \u201cDiff. from XGBoost\u201d is the across- task average of per-task difference from XGBoost. ORCA beats classical approaches and advanced transformer methods on 19 tasks. For per-task results, see Appendix A.6. ML models for physical systems have gained increasing interest in recent years. To study how cross-modal \ufb01ne- tuning can help in the scienti\ufb01c ML context, we evaluate ORCA on 8 datasets from PDEBench (Takamoto et al., 2022) and compare against state-of-the-art task-speci\ufb01c models: the physics-informed neural network PINN (Raissi et al., 2019), Fourier neural operator (FNO) (Li et al., 2021), and the generic"}, {"question": " What effect does training the embedder for different number of epochs have on downstream performance?,answer: Training the embedder for different number of epochs can impact downstream performance by affecting distribution distance and fine-tuning accuracy.", "ref_chunk": "quality. Right: Accuracy (\u2191) of ORCA vs. naive \ufb01ne-tuning with varying dataset size on task Satellite. ORCA has higher performance gains in low-data regime. transfer. Indeed, recent work has shown that optimizing directly for the task loss may distort the pretrained weights and lead to suboptimal solutions (Kumar et al., 2022; Lee et al., 2022). By manipulating the target distribution to look like the source distribution, we lower the risk of weight distortion, thus obtaining better downstream performance. We also quantify the effect of data alignment by training the embedder for different number of epochs and see whether optimizing distribution distance to various levels of conver- gence affects downstream performance. Figure 4 (left) plots the \ufb01ne-tuning accuracy and the \ufb01nal distribution distance for different embedder learning levels. We see that as the dataset distance decreases, the \ufb01ne-tuning accuracy in- creases. In addition, learning the embedder separately from \ufb01ne-tuning stabilizes training, as the performance variance of ORCA is constantly lower than that of naive \ufb01ne-tuning. These results con\ufb01rm that data alignment is the key to effec- tive cross-modal \ufb01ne-tuning. KEY 3: ADAPTING FROM THE RIGHT MODALITY Finally, we study how the pretraining modality affects \ufb01ne- tuning. In the results reported so far, we choose pretrained models for each task based on the input dimension, i.e., we use RoBERTa for all 1D tasks and Swin for all 2D tasks. Now, we evaluate the opposite approach, focusing on two tasks: DeepSEA (1D) and Spherical (2D). This evaluation is straightforward to perform by switching the model bodies, since the embedder architecture of ORCA handles all input transformations needed to obtain the sequence features. The results are shown in Table 13 in the Appendix. We see that \ufb01ne-tuned RoBERTa outperforms Swin on the 1D task, possibly because the DeepSEA data (genomics sequences) are structured more like language than images with discrete units of information and general grammatical rules. More crucially, for both tasks, models with smaller \ufb01nal OTDDs have better \ufb01ne-tuning accuracy. This suggests a way of selecting pretrained models by comparing the optimized OTDDs and picking the one with the smallest value. KEY 2: FINE-TUNING ALL MODEL PARAMETERS As discussed in Section 2, Frozen Pretrained Transformers (FPT) (Lu et al., 2022) is a related work that showed pre- trained language models contain knowledge relevant to out- of-modality tasks. While FPT presented a general pipeline for adapting GPT-2 to tasks like CIFAR-10, the resulting models were not as good as those trained from scratch. FPT differs from ORCA in that (1) it does not perform data alignment, and (2) it only \ufb01ne-tunes the layer norms. We have veri\ufb01ed the importance of (1). Now, we isolate the impact of (2) by \ufb01ne-tuning only the layer norms for ORCA. The bottom rows of Table 3 show that ORCA with \ufb01ne- tuning the layer norms outperforms FPT, so pretraining the embedder can boost the performance of FPT. However, this performance gain is smaller than that in the full \ufb01ne-tuning setting, which implies that full \ufb01ne-tuning can take better advantage of the learned embeddings. In terms of run- time, FPT yields less than a 2\u00d7 speedup compared with full \ufb01ne-tuning (Appendix A.4.6), despite the fact that we are updating many fewer parameters. This is unsurprising since gradients are still back-propagated through the entire net- work. Therefore, when computation allows, we recommend using ORCA with full \ufb01ne-tuning for better performance. Apart from these three key insights, recall that one of our motivations for cross-modal \ufb01ne-tuning is to help tasks with limited data, where training models from scratch is dif\ufb01cult. Indeed, for vanilla \ufb01ne-tuning, a small amount of data may not give enough signal to update the pretrained weights, but it is possible to learn a good embedder \ufb01rst with ORCA, which can then make \ufb01ne-tuning easier. In Figure 4 (right), we vary the dataset size and \ufb01nd that the performance gain of ORCA increases as the dataset size decreases. Mean- while, using ORCA allows us to match the performance of naive \ufb01ne-tuning on 3\u00d7 amount of data. Thus, it can bene\ufb01t model development in domains where data collection is costly. Beyond the cross-modal setting, we also verify ORCA\u2019s ef\ufb01cacy for in-modality transfer in Appendix A.8.1. 4.2. A Depth Perspective: Cross-Modal Fine-Tuning for PDE and Tabular Tasks After validating ORCA on a broad set of tasks, we dive into two speci\ufb01c modalities, PDE solving and tabular classi\ufb01ca- tion, to show that cross-modal \ufb01ne-tuning is promising for model development in highly specialized areas. ORCA can not only achieve high prediction accuracy in both domains, Cross-Modal Fine-Tuning: Align then Re\ufb01ne Diff-React 101 # wins vs. U-Net: 6/6 PINN 101 NaiverStokes 101 Diff-React 2DPDEs 102 102 # wins vs. FNO: 4/8 # wins vs. PINN: 8/8 Advection ShallowWater 102 Diff-Sorp DarcyFlow 100 100 100 Burgers FNO nRMSE ORCA U-Net Figure 5: Left: Normalized Root Mean Squared Errors (nRMSEs, \u2193) for ORCA vs. baselines on 8 PDEBench tasks with varying dimensions (1D/2D). We only evaluate datasets that can \ufb01t into a single V100 GPU. Overall, ORCA is much better than U-Net and PINN and on par with FNO. For detailed numerical results, see Table 14 in the Appendix. Right: ORCA is trained on resolution 256 and directly evaluated on resolution 512. The prediction still matches the ground truth. but also recover an important property of Neural Operators\u2014 modeling PDEs with zero-shot super-resolution. PDEBENCH FOR SCIENTIFIC ML Table 4: Tabular results with baselines from Hollmann et al. (2022) and Dinh et al. (2022). \u201cDiff. from XGBoost\u201d is the across- task average of per-task difference from XGBoost. ORCA beats classical approaches and advanced transformer methods on 19 tasks. For per-task results, see Appendix A.6. ML models for physical systems have gained increasing interest in recent years. To study how cross-modal \ufb01ne- tuning can help in the scienti\ufb01c ML context, we evaluate ORCA on 8 datasets from PDEBench (Takamoto et al., 2022) and compare against state-of-the-art task-speci\ufb01c models: the physics-informed neural network PINN (Raissi et al., 2019), Fourier neural operator (FNO) (Li et al., 2021), and the generic"}, {"question": " How does learning the embedder separately from fine-tuning contribute to the training process?,answer: Learning the embedder separately from fine-tuning stabilizes training, resulting in lower performance variance compared to naive fine-tuning.", "ref_chunk": "quality. Right: Accuracy (\u2191) of ORCA vs. naive \ufb01ne-tuning with varying dataset size on task Satellite. ORCA has higher performance gains in low-data regime. transfer. Indeed, recent work has shown that optimizing directly for the task loss may distort the pretrained weights and lead to suboptimal solutions (Kumar et al., 2022; Lee et al., 2022). By manipulating the target distribution to look like the source distribution, we lower the risk of weight distortion, thus obtaining better downstream performance. We also quantify the effect of data alignment by training the embedder for different number of epochs and see whether optimizing distribution distance to various levels of conver- gence affects downstream performance. Figure 4 (left) plots the \ufb01ne-tuning accuracy and the \ufb01nal distribution distance for different embedder learning levels. We see that as the dataset distance decreases, the \ufb01ne-tuning accuracy in- creases. In addition, learning the embedder separately from \ufb01ne-tuning stabilizes training, as the performance variance of ORCA is constantly lower than that of naive \ufb01ne-tuning. These results con\ufb01rm that data alignment is the key to effec- tive cross-modal \ufb01ne-tuning. KEY 3: ADAPTING FROM THE RIGHT MODALITY Finally, we study how the pretraining modality affects \ufb01ne- tuning. In the results reported so far, we choose pretrained models for each task based on the input dimension, i.e., we use RoBERTa for all 1D tasks and Swin for all 2D tasks. Now, we evaluate the opposite approach, focusing on two tasks: DeepSEA (1D) and Spherical (2D). This evaluation is straightforward to perform by switching the model bodies, since the embedder architecture of ORCA handles all input transformations needed to obtain the sequence features. The results are shown in Table 13 in the Appendix. We see that \ufb01ne-tuned RoBERTa outperforms Swin on the 1D task, possibly because the DeepSEA data (genomics sequences) are structured more like language than images with discrete units of information and general grammatical rules. More crucially, for both tasks, models with smaller \ufb01nal OTDDs have better \ufb01ne-tuning accuracy. This suggests a way of selecting pretrained models by comparing the optimized OTDDs and picking the one with the smallest value. KEY 2: FINE-TUNING ALL MODEL PARAMETERS As discussed in Section 2, Frozen Pretrained Transformers (FPT) (Lu et al., 2022) is a related work that showed pre- trained language models contain knowledge relevant to out- of-modality tasks. While FPT presented a general pipeline for adapting GPT-2 to tasks like CIFAR-10, the resulting models were not as good as those trained from scratch. FPT differs from ORCA in that (1) it does not perform data alignment, and (2) it only \ufb01ne-tunes the layer norms. We have veri\ufb01ed the importance of (1). Now, we isolate the impact of (2) by \ufb01ne-tuning only the layer norms for ORCA. The bottom rows of Table 3 show that ORCA with \ufb01ne- tuning the layer norms outperforms FPT, so pretraining the embedder can boost the performance of FPT. However, this performance gain is smaller than that in the full \ufb01ne-tuning setting, which implies that full \ufb01ne-tuning can take better advantage of the learned embeddings. In terms of run- time, FPT yields less than a 2\u00d7 speedup compared with full \ufb01ne-tuning (Appendix A.4.6), despite the fact that we are updating many fewer parameters. This is unsurprising since gradients are still back-propagated through the entire net- work. Therefore, when computation allows, we recommend using ORCA with full \ufb01ne-tuning for better performance. Apart from these three key insights, recall that one of our motivations for cross-modal \ufb01ne-tuning is to help tasks with limited data, where training models from scratch is dif\ufb01cult. Indeed, for vanilla \ufb01ne-tuning, a small amount of data may not give enough signal to update the pretrained weights, but it is possible to learn a good embedder \ufb01rst with ORCA, which can then make \ufb01ne-tuning easier. In Figure 4 (right), we vary the dataset size and \ufb01nd that the performance gain of ORCA increases as the dataset size decreases. Mean- while, using ORCA allows us to match the performance of naive \ufb01ne-tuning on 3\u00d7 amount of data. Thus, it can bene\ufb01t model development in domains where data collection is costly. Beyond the cross-modal setting, we also verify ORCA\u2019s ef\ufb01cacy for in-modality transfer in Appendix A.8.1. 4.2. A Depth Perspective: Cross-Modal Fine-Tuning for PDE and Tabular Tasks After validating ORCA on a broad set of tasks, we dive into two speci\ufb01c modalities, PDE solving and tabular classi\ufb01ca- tion, to show that cross-modal \ufb01ne-tuning is promising for model development in highly specialized areas. ORCA can not only achieve high prediction accuracy in both domains, Cross-Modal Fine-Tuning: Align then Re\ufb01ne Diff-React 101 # wins vs. U-Net: 6/6 PINN 101 NaiverStokes 101 Diff-React 2DPDEs 102 102 # wins vs. FNO: 4/8 # wins vs. PINN: 8/8 Advection ShallowWater 102 Diff-Sorp DarcyFlow 100 100 100 Burgers FNO nRMSE ORCA U-Net Figure 5: Left: Normalized Root Mean Squared Errors (nRMSEs, \u2193) for ORCA vs. baselines on 8 PDEBench tasks with varying dimensions (1D/2D). We only evaluate datasets that can \ufb01t into a single V100 GPU. Overall, ORCA is much better than U-Net and PINN and on par with FNO. For detailed numerical results, see Table 14 in the Appendix. Right: ORCA is trained on resolution 256 and directly evaluated on resolution 512. The prediction still matches the ground truth. but also recover an important property of Neural Operators\u2014 modeling PDEs with zero-shot super-resolution. PDEBENCH FOR SCIENTIFIC ML Table 4: Tabular results with baselines from Hollmann et al. (2022) and Dinh et al. (2022). \u201cDiff. from XGBoost\u201d is the across- task average of per-task difference from XGBoost. ORCA beats classical approaches and advanced transformer methods on 19 tasks. For per-task results, see Appendix A.6. ML models for physical systems have gained increasing interest in recent years. To study how cross-modal \ufb01ne- tuning can help in the scienti\ufb01c ML context, we evaluate ORCA on 8 datasets from PDEBench (Takamoto et al., 2022) and compare against state-of-the-art task-speci\ufb01c models: the physics-informed neural network PINN (Raissi et al., 2019), Fourier neural operator (FNO) (Li et al., 2021), and the generic"}, {"question": " What is the key to effective cross-modal fine-tuning, according to the text?,answer: The key to effective cross-modal fine-tuning is data alignment.", "ref_chunk": "quality. Right: Accuracy (\u2191) of ORCA vs. naive \ufb01ne-tuning with varying dataset size on task Satellite. ORCA has higher performance gains in low-data regime. transfer. Indeed, recent work has shown that optimizing directly for the task loss may distort the pretrained weights and lead to suboptimal solutions (Kumar et al., 2022; Lee et al., 2022). By manipulating the target distribution to look like the source distribution, we lower the risk of weight distortion, thus obtaining better downstream performance. We also quantify the effect of data alignment by training the embedder for different number of epochs and see whether optimizing distribution distance to various levels of conver- gence affects downstream performance. Figure 4 (left) plots the \ufb01ne-tuning accuracy and the \ufb01nal distribution distance for different embedder learning levels. We see that as the dataset distance decreases, the \ufb01ne-tuning accuracy in- creases. In addition, learning the embedder separately from \ufb01ne-tuning stabilizes training, as the performance variance of ORCA is constantly lower than that of naive \ufb01ne-tuning. These results con\ufb01rm that data alignment is the key to effec- tive cross-modal \ufb01ne-tuning. KEY 3: ADAPTING FROM THE RIGHT MODALITY Finally, we study how the pretraining modality affects \ufb01ne- tuning. In the results reported so far, we choose pretrained models for each task based on the input dimension, i.e., we use RoBERTa for all 1D tasks and Swin for all 2D tasks. Now, we evaluate the opposite approach, focusing on two tasks: DeepSEA (1D) and Spherical (2D). This evaluation is straightforward to perform by switching the model bodies, since the embedder architecture of ORCA handles all input transformations needed to obtain the sequence features. The results are shown in Table 13 in the Appendix. We see that \ufb01ne-tuned RoBERTa outperforms Swin on the 1D task, possibly because the DeepSEA data (genomics sequences) are structured more like language than images with discrete units of information and general grammatical rules. More crucially, for both tasks, models with smaller \ufb01nal OTDDs have better \ufb01ne-tuning accuracy. This suggests a way of selecting pretrained models by comparing the optimized OTDDs and picking the one with the smallest value. KEY 2: FINE-TUNING ALL MODEL PARAMETERS As discussed in Section 2, Frozen Pretrained Transformers (FPT) (Lu et al., 2022) is a related work that showed pre- trained language models contain knowledge relevant to out- of-modality tasks. While FPT presented a general pipeline for adapting GPT-2 to tasks like CIFAR-10, the resulting models were not as good as those trained from scratch. FPT differs from ORCA in that (1) it does not perform data alignment, and (2) it only \ufb01ne-tunes the layer norms. We have veri\ufb01ed the importance of (1). Now, we isolate the impact of (2) by \ufb01ne-tuning only the layer norms for ORCA. The bottom rows of Table 3 show that ORCA with \ufb01ne- tuning the layer norms outperforms FPT, so pretraining the embedder can boost the performance of FPT. However, this performance gain is smaller than that in the full \ufb01ne-tuning setting, which implies that full \ufb01ne-tuning can take better advantage of the learned embeddings. In terms of run- time, FPT yields less than a 2\u00d7 speedup compared with full \ufb01ne-tuning (Appendix A.4.6), despite the fact that we are updating many fewer parameters. This is unsurprising since gradients are still back-propagated through the entire net- work. Therefore, when computation allows, we recommend using ORCA with full \ufb01ne-tuning for better performance. Apart from these three key insights, recall that one of our motivations for cross-modal \ufb01ne-tuning is to help tasks with limited data, where training models from scratch is dif\ufb01cult. Indeed, for vanilla \ufb01ne-tuning, a small amount of data may not give enough signal to update the pretrained weights, but it is possible to learn a good embedder \ufb01rst with ORCA, which can then make \ufb01ne-tuning easier. In Figure 4 (right), we vary the dataset size and \ufb01nd that the performance gain of ORCA increases as the dataset size decreases. Mean- while, using ORCA allows us to match the performance of naive \ufb01ne-tuning on 3\u00d7 amount of data. Thus, it can bene\ufb01t model development in domains where data collection is costly. Beyond the cross-modal setting, we also verify ORCA\u2019s ef\ufb01cacy for in-modality transfer in Appendix A.8.1. 4.2. A Depth Perspective: Cross-Modal Fine-Tuning for PDE and Tabular Tasks After validating ORCA on a broad set of tasks, we dive into two speci\ufb01c modalities, PDE solving and tabular classi\ufb01ca- tion, to show that cross-modal \ufb01ne-tuning is promising for model development in highly specialized areas. ORCA can not only achieve high prediction accuracy in both domains, Cross-Modal Fine-Tuning: Align then Re\ufb01ne Diff-React 101 # wins vs. U-Net: 6/6 PINN 101 NaiverStokes 101 Diff-React 2DPDEs 102 102 # wins vs. FNO: 4/8 # wins vs. PINN: 8/8 Advection ShallowWater 102 Diff-Sorp DarcyFlow 100 100 100 Burgers FNO nRMSE ORCA U-Net Figure 5: Left: Normalized Root Mean Squared Errors (nRMSEs, \u2193) for ORCA vs. baselines on 8 PDEBench tasks with varying dimensions (1D/2D). We only evaluate datasets that can \ufb01t into a single V100 GPU. Overall, ORCA is much better than U-Net and PINN and on par with FNO. For detailed numerical results, see Table 14 in the Appendix. Right: ORCA is trained on resolution 256 and directly evaluated on resolution 512. The prediction still matches the ground truth. but also recover an important property of Neural Operators\u2014 modeling PDEs with zero-shot super-resolution. PDEBENCH FOR SCIENTIFIC ML Table 4: Tabular results with baselines from Hollmann et al. (2022) and Dinh et al. (2022). \u201cDiff. from XGBoost\u201d is the across- task average of per-task difference from XGBoost. ORCA beats classical approaches and advanced transformer methods on 19 tasks. For per-task results, see Appendix A.6. ML models for physical systems have gained increasing interest in recent years. To study how cross-modal \ufb01ne- tuning can help in the scienti\ufb01c ML context, we evaluate ORCA on 8 datasets from PDEBench (Takamoto et al., 2022) and compare against state-of-the-art task-speci\ufb01c models: the physics-informed neural network PINN (Raissi et al., 2019), Fourier neural operator (FNO) (Li et al., 2021), and the generic"}, {"question": " How does the choice of pretraining modality affect the fine-tuning process?,answer: The choice of pretraining modality can impact fine-tuning performance, as demonstrated by the evaluation of pretrained models on specific tasks like DeepSEA and Spherical.", "ref_chunk": "quality. Right: Accuracy (\u2191) of ORCA vs. naive \ufb01ne-tuning with varying dataset size on task Satellite. ORCA has higher performance gains in low-data regime. transfer. Indeed, recent work has shown that optimizing directly for the task loss may distort the pretrained weights and lead to suboptimal solutions (Kumar et al., 2022; Lee et al., 2022). By manipulating the target distribution to look like the source distribution, we lower the risk of weight distortion, thus obtaining better downstream performance. We also quantify the effect of data alignment by training the embedder for different number of epochs and see whether optimizing distribution distance to various levels of conver- gence affects downstream performance. Figure 4 (left) plots the \ufb01ne-tuning accuracy and the \ufb01nal distribution distance for different embedder learning levels. We see that as the dataset distance decreases, the \ufb01ne-tuning accuracy in- creases. In addition, learning the embedder separately from \ufb01ne-tuning stabilizes training, as the performance variance of ORCA is constantly lower than that of naive \ufb01ne-tuning. These results con\ufb01rm that data alignment is the key to effec- tive cross-modal \ufb01ne-tuning. KEY 3: ADAPTING FROM THE RIGHT MODALITY Finally, we study how the pretraining modality affects \ufb01ne- tuning. In the results reported so far, we choose pretrained models for each task based on the input dimension, i.e., we use RoBERTa for all 1D tasks and Swin for all 2D tasks. Now, we evaluate the opposite approach, focusing on two tasks: DeepSEA (1D) and Spherical (2D). This evaluation is straightforward to perform by switching the model bodies, since the embedder architecture of ORCA handles all input transformations needed to obtain the sequence features. The results are shown in Table 13 in the Appendix. We see that \ufb01ne-tuned RoBERTa outperforms Swin on the 1D task, possibly because the DeepSEA data (genomics sequences) are structured more like language than images with discrete units of information and general grammatical rules. More crucially, for both tasks, models with smaller \ufb01nal OTDDs have better \ufb01ne-tuning accuracy. This suggests a way of selecting pretrained models by comparing the optimized OTDDs and picking the one with the smallest value. KEY 2: FINE-TUNING ALL MODEL PARAMETERS As discussed in Section 2, Frozen Pretrained Transformers (FPT) (Lu et al., 2022) is a related work that showed pre- trained language models contain knowledge relevant to out- of-modality tasks. While FPT presented a general pipeline for adapting GPT-2 to tasks like CIFAR-10, the resulting models were not as good as those trained from scratch. FPT differs from ORCA in that (1) it does not perform data alignment, and (2) it only \ufb01ne-tunes the layer norms. We have veri\ufb01ed the importance of (1). Now, we isolate the impact of (2) by \ufb01ne-tuning only the layer norms for ORCA. The bottom rows of Table 3 show that ORCA with \ufb01ne- tuning the layer norms outperforms FPT, so pretraining the embedder can boost the performance of FPT. However, this performance gain is smaller than that in the full \ufb01ne-tuning setting, which implies that full \ufb01ne-tuning can take better advantage of the learned embeddings. In terms of run- time, FPT yields less than a 2\u00d7 speedup compared with full \ufb01ne-tuning (Appendix A.4.6), despite the fact that we are updating many fewer parameters. This is unsurprising since gradients are still back-propagated through the entire net- work. Therefore, when computation allows, we recommend using ORCA with full \ufb01ne-tuning for better performance. Apart from these three key insights, recall that one of our motivations for cross-modal \ufb01ne-tuning is to help tasks with limited data, where training models from scratch is dif\ufb01cult. Indeed, for vanilla \ufb01ne-tuning, a small amount of data may not give enough signal to update the pretrained weights, but it is possible to learn a good embedder \ufb01rst with ORCA, which can then make \ufb01ne-tuning easier. In Figure 4 (right), we vary the dataset size and \ufb01nd that the performance gain of ORCA increases as the dataset size decreases. Mean- while, using ORCA allows us to match the performance of naive \ufb01ne-tuning on 3\u00d7 amount of data. Thus, it can bene\ufb01t model development in domains where data collection is costly. Beyond the cross-modal setting, we also verify ORCA\u2019s ef\ufb01cacy for in-modality transfer in Appendix A.8.1. 4.2. A Depth Perspective: Cross-Modal Fine-Tuning for PDE and Tabular Tasks After validating ORCA on a broad set of tasks, we dive into two speci\ufb01c modalities, PDE solving and tabular classi\ufb01ca- tion, to show that cross-modal \ufb01ne-tuning is promising for model development in highly specialized areas. ORCA can not only achieve high prediction accuracy in both domains, Cross-Modal Fine-Tuning: Align then Re\ufb01ne Diff-React 101 # wins vs. U-Net: 6/6 PINN 101 NaiverStokes 101 Diff-React 2DPDEs 102 102 # wins vs. FNO: 4/8 # wins vs. PINN: 8/8 Advection ShallowWater 102 Diff-Sorp DarcyFlow 100 100 100 Burgers FNO nRMSE ORCA U-Net Figure 5: Left: Normalized Root Mean Squared Errors (nRMSEs, \u2193) for ORCA vs. baselines on 8 PDEBench tasks with varying dimensions (1D/2D). We only evaluate datasets that can \ufb01t into a single V100 GPU. Overall, ORCA is much better than U-Net and PINN and on par with FNO. For detailed numerical results, see Table 14 in the Appendix. Right: ORCA is trained on resolution 256 and directly evaluated on resolution 512. The prediction still matches the ground truth. but also recover an important property of Neural Operators\u2014 modeling PDEs with zero-shot super-resolution. PDEBENCH FOR SCIENTIFIC ML Table 4: Tabular results with baselines from Hollmann et al. (2022) and Dinh et al. (2022). \u201cDiff. from XGBoost\u201d is the across- task average of per-task difference from XGBoost. ORCA beats classical approaches and advanced transformer methods on 19 tasks. For per-task results, see Appendix A.6. ML models for physical systems have gained increasing interest in recent years. To study how cross-modal \ufb01ne- tuning can help in the scienti\ufb01c ML context, we evaluate ORCA on 8 datasets from PDEBench (Takamoto et al., 2022) and compare against state-of-the-art task-speci\ufb01c models: the physics-informed neural network PINN (Raissi et al., 2019), Fourier neural operator (FNO) (Li et al., 2021), and the generic"}, {"question": " What is the difference between ORCA and Frozen Pretrained Transformers (FPT) in terms of fine-tuning strategy?,answer: ORCA performs data alignment and fine-tunes all model parameters, while FPT only fine-tunes the layer norms.", "ref_chunk": "quality. Right: Accuracy (\u2191) of ORCA vs. naive \ufb01ne-tuning with varying dataset size on task Satellite. ORCA has higher performance gains in low-data regime. transfer. Indeed, recent work has shown that optimizing directly for the task loss may distort the pretrained weights and lead to suboptimal solutions (Kumar et al., 2022; Lee et al., 2022). By manipulating the target distribution to look like the source distribution, we lower the risk of weight distortion, thus obtaining better downstream performance. We also quantify the effect of data alignment by training the embedder for different number of epochs and see whether optimizing distribution distance to various levels of conver- gence affects downstream performance. Figure 4 (left) plots the \ufb01ne-tuning accuracy and the \ufb01nal distribution distance for different embedder learning levels. We see that as the dataset distance decreases, the \ufb01ne-tuning accuracy in- creases. In addition, learning the embedder separately from \ufb01ne-tuning stabilizes training, as the performance variance of ORCA is constantly lower than that of naive \ufb01ne-tuning. These results con\ufb01rm that data alignment is the key to effec- tive cross-modal \ufb01ne-tuning. KEY 3: ADAPTING FROM THE RIGHT MODALITY Finally, we study how the pretraining modality affects \ufb01ne- tuning. In the results reported so far, we choose pretrained models for each task based on the input dimension, i.e., we use RoBERTa for all 1D tasks and Swin for all 2D tasks. Now, we evaluate the opposite approach, focusing on two tasks: DeepSEA (1D) and Spherical (2D). This evaluation is straightforward to perform by switching the model bodies, since the embedder architecture of ORCA handles all input transformations needed to obtain the sequence features. The results are shown in Table 13 in the Appendix. We see that \ufb01ne-tuned RoBERTa outperforms Swin on the 1D task, possibly because the DeepSEA data (genomics sequences) are structured more like language than images with discrete units of information and general grammatical rules. More crucially, for both tasks, models with smaller \ufb01nal OTDDs have better \ufb01ne-tuning accuracy. This suggests a way of selecting pretrained models by comparing the optimized OTDDs and picking the one with the smallest value. KEY 2: FINE-TUNING ALL MODEL PARAMETERS As discussed in Section 2, Frozen Pretrained Transformers (FPT) (Lu et al., 2022) is a related work that showed pre- trained language models contain knowledge relevant to out- of-modality tasks. While FPT presented a general pipeline for adapting GPT-2 to tasks like CIFAR-10, the resulting models were not as good as those trained from scratch. FPT differs from ORCA in that (1) it does not perform data alignment, and (2) it only \ufb01ne-tunes the layer norms. We have veri\ufb01ed the importance of (1). Now, we isolate the impact of (2) by \ufb01ne-tuning only the layer norms for ORCA. The bottom rows of Table 3 show that ORCA with \ufb01ne- tuning the layer norms outperforms FPT, so pretraining the embedder can boost the performance of FPT. However, this performance gain is smaller than that in the full \ufb01ne-tuning setting, which implies that full \ufb01ne-tuning can take better advantage of the learned embeddings. In terms of run- time, FPT yields less than a 2\u00d7 speedup compared with full \ufb01ne-tuning (Appendix A.4.6), despite the fact that we are updating many fewer parameters. This is unsurprising since gradients are still back-propagated through the entire net- work. Therefore, when computation allows, we recommend using ORCA with full \ufb01ne-tuning for better performance. Apart from these three key insights, recall that one of our motivations for cross-modal \ufb01ne-tuning is to help tasks with limited data, where training models from scratch is dif\ufb01cult. Indeed, for vanilla \ufb01ne-tuning, a small amount of data may not give enough signal to update the pretrained weights, but it is possible to learn a good embedder \ufb01rst with ORCA, which can then make \ufb01ne-tuning easier. In Figure 4 (right), we vary the dataset size and \ufb01nd that the performance gain of ORCA increases as the dataset size decreases. Mean- while, using ORCA allows us to match the performance of naive \ufb01ne-tuning on 3\u00d7 amount of data. Thus, it can bene\ufb01t model development in domains where data collection is costly. Beyond the cross-modal setting, we also verify ORCA\u2019s ef\ufb01cacy for in-modality transfer in Appendix A.8.1. 4.2. A Depth Perspective: Cross-Modal Fine-Tuning for PDE and Tabular Tasks After validating ORCA on a broad set of tasks, we dive into two speci\ufb01c modalities, PDE solving and tabular classi\ufb01ca- tion, to show that cross-modal \ufb01ne-tuning is promising for model development in highly specialized areas. ORCA can not only achieve high prediction accuracy in both domains, Cross-Modal Fine-Tuning: Align then Re\ufb01ne Diff-React 101 # wins vs. U-Net: 6/6 PINN 101 NaiverStokes 101 Diff-React 2DPDEs 102 102 # wins vs. FNO: 4/8 # wins vs. PINN: 8/8 Advection ShallowWater 102 Diff-Sorp DarcyFlow 100 100 100 Burgers FNO nRMSE ORCA U-Net Figure 5: Left: Normalized Root Mean Squared Errors (nRMSEs, \u2193) for ORCA vs. baselines on 8 PDEBench tasks with varying dimensions (1D/2D). We only evaluate datasets that can \ufb01t into a single V100 GPU. Overall, ORCA is much better than U-Net and PINN and on par with FNO. For detailed numerical results, see Table 14 in the Appendix. Right: ORCA is trained on resolution 256 and directly evaluated on resolution 512. The prediction still matches the ground truth. but also recover an important property of Neural Operators\u2014 modeling PDEs with zero-shot super-resolution. PDEBENCH FOR SCIENTIFIC ML Table 4: Tabular results with baselines from Hollmann et al. (2022) and Dinh et al. (2022). \u201cDiff. from XGBoost\u201d is the across- task average of per-task difference from XGBoost. ORCA beats classical approaches and advanced transformer methods on 19 tasks. For per-task results, see Appendix A.6. ML models for physical systems have gained increasing interest in recent years. To study how cross-modal \ufb01ne- tuning can help in the scienti\ufb01c ML context, we evaluate ORCA on 8 datasets from PDEBench (Takamoto et al., 2022) and compare against state-of-the-art task-speci\ufb01c models: the physics-informed neural network PINN (Raissi et al., 2019), Fourier neural operator (FNO) (Li et al., 2021), and the generic"}, {"question": " What does the text recommend in terms of using FPT versus ORCA for better performance?,answer: The text recommends using ORCA with full fine-tuning for better performance, as it can take better advantage of the learned embeddings.", "ref_chunk": "quality. Right: Accuracy (\u2191) of ORCA vs. naive \ufb01ne-tuning with varying dataset size on task Satellite. ORCA has higher performance gains in low-data regime. transfer. Indeed, recent work has shown that optimizing directly for the task loss may distort the pretrained weights and lead to suboptimal solutions (Kumar et al., 2022; Lee et al., 2022). By manipulating the target distribution to look like the source distribution, we lower the risk of weight distortion, thus obtaining better downstream performance. We also quantify the effect of data alignment by training the embedder for different number of epochs and see whether optimizing distribution distance to various levels of conver- gence affects downstream performance. Figure 4 (left) plots the \ufb01ne-tuning accuracy and the \ufb01nal distribution distance for different embedder learning levels. We see that as the dataset distance decreases, the \ufb01ne-tuning accuracy in- creases. In addition, learning the embedder separately from \ufb01ne-tuning stabilizes training, as the performance variance of ORCA is constantly lower than that of naive \ufb01ne-tuning. These results con\ufb01rm that data alignment is the key to effec- tive cross-modal \ufb01ne-tuning. KEY 3: ADAPTING FROM THE RIGHT MODALITY Finally, we study how the pretraining modality affects \ufb01ne- tuning. In the results reported so far, we choose pretrained models for each task based on the input dimension, i.e., we use RoBERTa for all 1D tasks and Swin for all 2D tasks. Now, we evaluate the opposite approach, focusing on two tasks: DeepSEA (1D) and Spherical (2D). This evaluation is straightforward to perform by switching the model bodies, since the embedder architecture of ORCA handles all input transformations needed to obtain the sequence features. The results are shown in Table 13 in the Appendix. We see that \ufb01ne-tuned RoBERTa outperforms Swin on the 1D task, possibly because the DeepSEA data (genomics sequences) are structured more like language than images with discrete units of information and general grammatical rules. More crucially, for both tasks, models with smaller \ufb01nal OTDDs have better \ufb01ne-tuning accuracy. This suggests a way of selecting pretrained models by comparing the optimized OTDDs and picking the one with the smallest value. KEY 2: FINE-TUNING ALL MODEL PARAMETERS As discussed in Section 2, Frozen Pretrained Transformers (FPT) (Lu et al., 2022) is a related work that showed pre- trained language models contain knowledge relevant to out- of-modality tasks. While FPT presented a general pipeline for adapting GPT-2 to tasks like CIFAR-10, the resulting models were not as good as those trained from scratch. FPT differs from ORCA in that (1) it does not perform data alignment, and (2) it only \ufb01ne-tunes the layer norms. We have veri\ufb01ed the importance of (1). Now, we isolate the impact of (2) by \ufb01ne-tuning only the layer norms for ORCA. The bottom rows of Table 3 show that ORCA with \ufb01ne- tuning the layer norms outperforms FPT, so pretraining the embedder can boost the performance of FPT. However, this performance gain is smaller than that in the full \ufb01ne-tuning setting, which implies that full \ufb01ne-tuning can take better advantage of the learned embeddings. In terms of run- time, FPT yields less than a 2\u00d7 speedup compared with full \ufb01ne-tuning (Appendix A.4.6), despite the fact that we are updating many fewer parameters. This is unsurprising since gradients are still back-propagated through the entire net- work. Therefore, when computation allows, we recommend using ORCA with full \ufb01ne-tuning for better performance. Apart from these three key insights, recall that one of our motivations for cross-modal \ufb01ne-tuning is to help tasks with limited data, where training models from scratch is dif\ufb01cult. Indeed, for vanilla \ufb01ne-tuning, a small amount of data may not give enough signal to update the pretrained weights, but it is possible to learn a good embedder \ufb01rst with ORCA, which can then make \ufb01ne-tuning easier. In Figure 4 (right), we vary the dataset size and \ufb01nd that the performance gain of ORCA increases as the dataset size decreases. Mean- while, using ORCA allows us to match the performance of naive \ufb01ne-tuning on 3\u00d7 amount of data. Thus, it can bene\ufb01t model development in domains where data collection is costly. Beyond the cross-modal setting, we also verify ORCA\u2019s ef\ufb01cacy for in-modality transfer in Appendix A.8.1. 4.2. A Depth Perspective: Cross-Modal Fine-Tuning for PDE and Tabular Tasks After validating ORCA on a broad set of tasks, we dive into two speci\ufb01c modalities, PDE solving and tabular classi\ufb01ca- tion, to show that cross-modal \ufb01ne-tuning is promising for model development in highly specialized areas. ORCA can not only achieve high prediction accuracy in both domains, Cross-Modal Fine-Tuning: Align then Re\ufb01ne Diff-React 101 # wins vs. U-Net: 6/6 PINN 101 NaiverStokes 101 Diff-React 2DPDEs 102 102 # wins vs. FNO: 4/8 # wins vs. PINN: 8/8 Advection ShallowWater 102 Diff-Sorp DarcyFlow 100 100 100 Burgers FNO nRMSE ORCA U-Net Figure 5: Left: Normalized Root Mean Squared Errors (nRMSEs, \u2193) for ORCA vs. baselines on 8 PDEBench tasks with varying dimensions (1D/2D). We only evaluate datasets that can \ufb01t into a single V100 GPU. Overall, ORCA is much better than U-Net and PINN and on par with FNO. For detailed numerical results, see Table 14 in the Appendix. Right: ORCA is trained on resolution 256 and directly evaluated on resolution 512. The prediction still matches the ground truth. but also recover an important property of Neural Operators\u2014 modeling PDEs with zero-shot super-resolution. PDEBENCH FOR SCIENTIFIC ML Table 4: Tabular results with baselines from Hollmann et al. (2022) and Dinh et al. (2022). \u201cDiff. from XGBoost\u201d is the across- task average of per-task difference from XGBoost. ORCA beats classical approaches and advanced transformer methods on 19 tasks. For per-task results, see Appendix A.6. ML models for physical systems have gained increasing interest in recent years. To study how cross-modal \ufb01ne- tuning can help in the scienti\ufb01c ML context, we evaluate ORCA on 8 datasets from PDEBench (Takamoto et al., 2022) and compare against state-of-the-art task-speci\ufb01c models: the physics-informed neural network PINN (Raissi et al., 2019), Fourier neural operator (FNO) (Li et al., 2021), and the generic"}, {"question": " How does ORCA help in situations where there is limited data for training models?,answer: In situations with limited data, ORCA can help by learning a good embedder first, which makes fine-tuning easier and more effective.", "ref_chunk": "quality. Right: Accuracy (\u2191) of ORCA vs. naive \ufb01ne-tuning with varying dataset size on task Satellite. ORCA has higher performance gains in low-data regime. transfer. Indeed, recent work has shown that optimizing directly for the task loss may distort the pretrained weights and lead to suboptimal solutions (Kumar et al., 2022; Lee et al., 2022). By manipulating the target distribution to look like the source distribution, we lower the risk of weight distortion, thus obtaining better downstream performance. We also quantify the effect of data alignment by training the embedder for different number of epochs and see whether optimizing distribution distance to various levels of conver- gence affects downstream performance. Figure 4 (left) plots the \ufb01ne-tuning accuracy and the \ufb01nal distribution distance for different embedder learning levels. We see that as the dataset distance decreases, the \ufb01ne-tuning accuracy in- creases. In addition, learning the embedder separately from \ufb01ne-tuning stabilizes training, as the performance variance of ORCA is constantly lower than that of naive \ufb01ne-tuning. These results con\ufb01rm that data alignment is the key to effec- tive cross-modal \ufb01ne-tuning. KEY 3: ADAPTING FROM THE RIGHT MODALITY Finally, we study how the pretraining modality affects \ufb01ne- tuning. In the results reported so far, we choose pretrained models for each task based on the input dimension, i.e., we use RoBERTa for all 1D tasks and Swin for all 2D tasks. Now, we evaluate the opposite approach, focusing on two tasks: DeepSEA (1D) and Spherical (2D). This evaluation is straightforward to perform by switching the model bodies, since the embedder architecture of ORCA handles all input transformations needed to obtain the sequence features. The results are shown in Table 13 in the Appendix. We see that \ufb01ne-tuned RoBERTa outperforms Swin on the 1D task, possibly because the DeepSEA data (genomics sequences) are structured more like language than images with discrete units of information and general grammatical rules. More crucially, for both tasks, models with smaller \ufb01nal OTDDs have better \ufb01ne-tuning accuracy. This suggests a way of selecting pretrained models by comparing the optimized OTDDs and picking the one with the smallest value. KEY 2: FINE-TUNING ALL MODEL PARAMETERS As discussed in Section 2, Frozen Pretrained Transformers (FPT) (Lu et al., 2022) is a related work that showed pre- trained language models contain knowledge relevant to out- of-modality tasks. While FPT presented a general pipeline for adapting GPT-2 to tasks like CIFAR-10, the resulting models were not as good as those trained from scratch. FPT differs from ORCA in that (1) it does not perform data alignment, and (2) it only \ufb01ne-tunes the layer norms. We have veri\ufb01ed the importance of (1). Now, we isolate the impact of (2) by \ufb01ne-tuning only the layer norms for ORCA. The bottom rows of Table 3 show that ORCA with \ufb01ne- tuning the layer norms outperforms FPT, so pretraining the embedder can boost the performance of FPT. However, this performance gain is smaller than that in the full \ufb01ne-tuning setting, which implies that full \ufb01ne-tuning can take better advantage of the learned embeddings. In terms of run- time, FPT yields less than a 2\u00d7 speedup compared with full \ufb01ne-tuning (Appendix A.4.6), despite the fact that we are updating many fewer parameters. This is unsurprising since gradients are still back-propagated through the entire net- work. Therefore, when computation allows, we recommend using ORCA with full \ufb01ne-tuning for better performance. Apart from these three key insights, recall that one of our motivations for cross-modal \ufb01ne-tuning is to help tasks with limited data, where training models from scratch is dif\ufb01cult. Indeed, for vanilla \ufb01ne-tuning, a small amount of data may not give enough signal to update the pretrained weights, but it is possible to learn a good embedder \ufb01rst with ORCA, which can then make \ufb01ne-tuning easier. In Figure 4 (right), we vary the dataset size and \ufb01nd that the performance gain of ORCA increases as the dataset size decreases. Mean- while, using ORCA allows us to match the performance of naive \ufb01ne-tuning on 3\u00d7 amount of data. Thus, it can bene\ufb01t model development in domains where data collection is costly. Beyond the cross-modal setting, we also verify ORCA\u2019s ef\ufb01cacy for in-modality transfer in Appendix A.8.1. 4.2. A Depth Perspective: Cross-Modal Fine-Tuning for PDE and Tabular Tasks After validating ORCA on a broad set of tasks, we dive into two speci\ufb01c modalities, PDE solving and tabular classi\ufb01ca- tion, to show that cross-modal \ufb01ne-tuning is promising for model development in highly specialized areas. ORCA can not only achieve high prediction accuracy in both domains, Cross-Modal Fine-Tuning: Align then Re\ufb01ne Diff-React 101 # wins vs. U-Net: 6/6 PINN 101 NaiverStokes 101 Diff-React 2DPDEs 102 102 # wins vs. FNO: 4/8 # wins vs. PINN: 8/8 Advection ShallowWater 102 Diff-Sorp DarcyFlow 100 100 100 Burgers FNO nRMSE ORCA U-Net Figure 5: Left: Normalized Root Mean Squared Errors (nRMSEs, \u2193) for ORCA vs. baselines on 8 PDEBench tasks with varying dimensions (1D/2D). We only evaluate datasets that can \ufb01t into a single V100 GPU. Overall, ORCA is much better than U-Net and PINN and on par with FNO. For detailed numerical results, see Table 14 in the Appendix. Right: ORCA is trained on resolution 256 and directly evaluated on resolution 512. The prediction still matches the ground truth. but also recover an important property of Neural Operators\u2014 modeling PDEs with zero-shot super-resolution. PDEBENCH FOR SCIENTIFIC ML Table 4: Tabular results with baselines from Hollmann et al. (2022) and Dinh et al. (2022). \u201cDiff. from XGBoost\u201d is the across- task average of per-task difference from XGBoost. ORCA beats classical approaches and advanced transformer methods on 19 tasks. For per-task results, see Appendix A.6. ML models for physical systems have gained increasing interest in recent years. To study how cross-modal \ufb01ne- tuning can help in the scienti\ufb01c ML context, we evaluate ORCA on 8 datasets from PDEBench (Takamoto et al., 2022) and compare against state-of-the-art task-speci\ufb01c models: the physics-informed neural network PINN (Raissi et al., 2019), Fourier neural operator (FNO) (Li et al., 2021), and the generic"}, {"question": " What benefit does ORCA provide in terms of dataset size and performance gain?,answer: ORCA provides a performance gain that increases as the dataset size decreases, and it allows matching the performance of naive fine-tuning on a larger amount of data.", "ref_chunk": "quality. Right: Accuracy (\u2191) of ORCA vs. naive \ufb01ne-tuning with varying dataset size on task Satellite. ORCA has higher performance gains in low-data regime. transfer. Indeed, recent work has shown that optimizing directly for the task loss may distort the pretrained weights and lead to suboptimal solutions (Kumar et al., 2022; Lee et al., 2022). By manipulating the target distribution to look like the source distribution, we lower the risk of weight distortion, thus obtaining better downstream performance. We also quantify the effect of data alignment by training the embedder for different number of epochs and see whether optimizing distribution distance to various levels of conver- gence affects downstream performance. Figure 4 (left) plots the \ufb01ne-tuning accuracy and the \ufb01nal distribution distance for different embedder learning levels. We see that as the dataset distance decreases, the \ufb01ne-tuning accuracy in- creases. In addition, learning the embedder separately from \ufb01ne-tuning stabilizes training, as the performance variance of ORCA is constantly lower than that of naive \ufb01ne-tuning. These results con\ufb01rm that data alignment is the key to effec- tive cross-modal \ufb01ne-tuning. KEY 3: ADAPTING FROM THE RIGHT MODALITY Finally, we study how the pretraining modality affects \ufb01ne- tuning. In the results reported so far, we choose pretrained models for each task based on the input dimension, i.e., we use RoBERTa for all 1D tasks and Swin for all 2D tasks. Now, we evaluate the opposite approach, focusing on two tasks: DeepSEA (1D) and Spherical (2D). This evaluation is straightforward to perform by switching the model bodies, since the embedder architecture of ORCA handles all input transformations needed to obtain the sequence features. The results are shown in Table 13 in the Appendix. We see that \ufb01ne-tuned RoBERTa outperforms Swin on the 1D task, possibly because the DeepSEA data (genomics sequences) are structured more like language than images with discrete units of information and general grammatical rules. More crucially, for both tasks, models with smaller \ufb01nal OTDDs have better \ufb01ne-tuning accuracy. This suggests a way of selecting pretrained models by comparing the optimized OTDDs and picking the one with the smallest value. KEY 2: FINE-TUNING ALL MODEL PARAMETERS As discussed in Section 2, Frozen Pretrained Transformers (FPT) (Lu et al., 2022) is a related work that showed pre- trained language models contain knowledge relevant to out- of-modality tasks. While FPT presented a general pipeline for adapting GPT-2 to tasks like CIFAR-10, the resulting models were not as good as those trained from scratch. FPT differs from ORCA in that (1) it does not perform data alignment, and (2) it only \ufb01ne-tunes the layer norms. We have veri\ufb01ed the importance of (1). Now, we isolate the impact of (2) by \ufb01ne-tuning only the layer norms for ORCA. The bottom rows of Table 3 show that ORCA with \ufb01ne- tuning the layer norms outperforms FPT, so pretraining the embedder can boost the performance of FPT. However, this performance gain is smaller than that in the full \ufb01ne-tuning setting, which implies that full \ufb01ne-tuning can take better advantage of the learned embeddings. In terms of run- time, FPT yields less than a 2\u00d7 speedup compared with full \ufb01ne-tuning (Appendix A.4.6), despite the fact that we are updating many fewer parameters. This is unsurprising since gradients are still back-propagated through the entire net- work. Therefore, when computation allows, we recommend using ORCA with full \ufb01ne-tuning for better performance. Apart from these three key insights, recall that one of our motivations for cross-modal \ufb01ne-tuning is to help tasks with limited data, where training models from scratch is dif\ufb01cult. Indeed, for vanilla \ufb01ne-tuning, a small amount of data may not give enough signal to update the pretrained weights, but it is possible to learn a good embedder \ufb01rst with ORCA, which can then make \ufb01ne-tuning easier. In Figure 4 (right), we vary the dataset size and \ufb01nd that the performance gain of ORCA increases as the dataset size decreases. Mean- while, using ORCA allows us to match the performance of naive \ufb01ne-tuning on 3\u00d7 amount of data. Thus, it can bene\ufb01t model development in domains where data collection is costly. Beyond the cross-modal setting, we also verify ORCA\u2019s ef\ufb01cacy for in-modality transfer in Appendix A.8.1. 4.2. A Depth Perspective: Cross-Modal Fine-Tuning for PDE and Tabular Tasks After validating ORCA on a broad set of tasks, we dive into two speci\ufb01c modalities, PDE solving and tabular classi\ufb01ca- tion, to show that cross-modal \ufb01ne-tuning is promising for model development in highly specialized areas. ORCA can not only achieve high prediction accuracy in both domains, Cross-Modal Fine-Tuning: Align then Re\ufb01ne Diff-React 101 # wins vs. U-Net: 6/6 PINN 101 NaiverStokes 101 Diff-React 2DPDEs 102 102 # wins vs. FNO: 4/8 # wins vs. PINN: 8/8 Advection ShallowWater 102 Diff-Sorp DarcyFlow 100 100 100 Burgers FNO nRMSE ORCA U-Net Figure 5: Left: Normalized Root Mean Squared Errors (nRMSEs, \u2193) for ORCA vs. baselines on 8 PDEBench tasks with varying dimensions (1D/2D). We only evaluate datasets that can \ufb01t into a single V100 GPU. Overall, ORCA is much better than U-Net and PINN and on par with FNO. For detailed numerical results, see Table 14 in the Appendix. Right: ORCA is trained on resolution 256 and directly evaluated on resolution 512. The prediction still matches the ground truth. but also recover an important property of Neural Operators\u2014 modeling PDEs with zero-shot super-resolution. PDEBENCH FOR SCIENTIFIC ML Table 4: Tabular results with baselines from Hollmann et al. (2022) and Dinh et al. (2022). \u201cDiff. from XGBoost\u201d is the across- task average of per-task difference from XGBoost. ORCA beats classical approaches and advanced transformer methods on 19 tasks. For per-task results, see Appendix A.6. ML models for physical systems have gained increasing interest in recent years. To study how cross-modal \ufb01ne- tuning can help in the scienti\ufb01c ML context, we evaluate ORCA on 8 datasets from PDEBench (Takamoto et al., 2022) and compare against state-of-the-art task-speci\ufb01c models: the physics-informed neural network PINN (Raissi et al., 2019), Fourier neural operator (FNO) (Li et al., 2021), and the generic"}], "doc_text": "quality. Right: Accuracy (\u2191) of ORCA vs. naive \ufb01ne-tuning with varying dataset size on task Satellite. ORCA has higher performance gains in low-data regime. transfer. Indeed, recent work has shown that optimizing directly for the task loss may distort the pretrained weights and lead to suboptimal solutions (Kumar et al., 2022; Lee et al., 2022). By manipulating the target distribution to look like the source distribution, we lower the risk of weight distortion, thus obtaining better downstream performance. We also quantify the effect of data alignment by training the embedder for different number of epochs and see whether optimizing distribution distance to various levels of conver- gence affects downstream performance. Figure 4 (left) plots the \ufb01ne-tuning accuracy and the \ufb01nal distribution distance for different embedder learning levels. We see that as the dataset distance decreases, the \ufb01ne-tuning accuracy in- creases. In addition, learning the embedder separately from \ufb01ne-tuning stabilizes training, as the performance variance of ORCA is constantly lower than that of naive \ufb01ne-tuning. These results con\ufb01rm that data alignment is the key to effec- tive cross-modal \ufb01ne-tuning. KEY 3: ADAPTING FROM THE RIGHT MODALITY Finally, we study how the pretraining modality affects \ufb01ne- tuning. In the results reported so far, we choose pretrained models for each task based on the input dimension, i.e., we use RoBERTa for all 1D tasks and Swin for all 2D tasks. Now, we evaluate the opposite approach, focusing on two tasks: DeepSEA (1D) and Spherical (2D). This evaluation is straightforward to perform by switching the model bodies, since the embedder architecture of ORCA handles all input transformations needed to obtain the sequence features. The results are shown in Table 13 in the Appendix. We see that \ufb01ne-tuned RoBERTa outperforms Swin on the 1D task, possibly because the DeepSEA data (genomics sequences) are structured more like language than images with discrete units of information and general grammatical rules. More crucially, for both tasks, models with smaller \ufb01nal OTDDs have better \ufb01ne-tuning accuracy. This suggests a way of selecting pretrained models by comparing the optimized OTDDs and picking the one with the smallest value. KEY 2: FINE-TUNING ALL MODEL PARAMETERS As discussed in Section 2, Frozen Pretrained Transformers (FPT) (Lu et al., 2022) is a related work that showed pre- trained language models contain knowledge relevant to out- of-modality tasks. While FPT presented a general pipeline for adapting GPT-2 to tasks like CIFAR-10, the resulting models were not as good as those trained from scratch. FPT differs from ORCA in that (1) it does not perform data alignment, and (2) it only \ufb01ne-tunes the layer norms. We have veri\ufb01ed the importance of (1). Now, we isolate the impact of (2) by \ufb01ne-tuning only the layer norms for ORCA. The bottom rows of Table 3 show that ORCA with \ufb01ne- tuning the layer norms outperforms FPT, so pretraining the embedder can boost the performance of FPT. However, this performance gain is smaller than that in the full \ufb01ne-tuning setting, which implies that full \ufb01ne-tuning can take better advantage of the learned embeddings. In terms of run- time, FPT yields less than a 2\u00d7 speedup compared with full \ufb01ne-tuning (Appendix A.4.6), despite the fact that we are updating many fewer parameters. This is unsurprising since gradients are still back-propagated through the entire net- work. Therefore, when computation allows, we recommend using ORCA with full \ufb01ne-tuning for better performance. Apart from these three key insights, recall that one of our motivations for cross-modal \ufb01ne-tuning is to help tasks with limited data, where training models from scratch is dif\ufb01cult. Indeed, for vanilla \ufb01ne-tuning, a small amount of data may not give enough signal to update the pretrained weights, but it is possible to learn a good embedder \ufb01rst with ORCA, which can then make \ufb01ne-tuning easier. In Figure 4 (right), we vary the dataset size and \ufb01nd that the performance gain of ORCA increases as the dataset size decreases. Mean- while, using ORCA allows us to match the performance of naive \ufb01ne-tuning on 3\u00d7 amount of data. Thus, it can bene\ufb01t model development in domains where data collection is costly. Beyond the cross-modal setting, we also verify ORCA\u2019s ef\ufb01cacy for in-modality transfer in Appendix A.8.1. 4.2. A Depth Perspective: Cross-Modal Fine-Tuning for PDE and Tabular Tasks After validating ORCA on a broad set of tasks, we dive into two speci\ufb01c modalities, PDE solving and tabular classi\ufb01ca- tion, to show that cross-modal \ufb01ne-tuning is promising for model development in highly specialized areas. ORCA can not only achieve high prediction accuracy in both domains, Cross-Modal Fine-Tuning: Align then Re\ufb01ne Diff-React 101 # wins vs. U-Net: 6/6 PINN 101 NaiverStokes 101 Diff-React 2DPDEs 102 102 # wins vs. FNO: 4/8 # wins vs. PINN: 8/8 Advection ShallowWater 102 Diff-Sorp DarcyFlow 100 100 100 Burgers FNO nRMSE ORCA U-Net Figure 5: Left: Normalized Root Mean Squared Errors (nRMSEs, \u2193) for ORCA vs. baselines on 8 PDEBench tasks with varying dimensions (1D/2D). We only evaluate datasets that can \ufb01t into a single V100 GPU. Overall, ORCA is much better than U-Net and PINN and on par with FNO. For detailed numerical results, see Table 14 in the Appendix. Right: ORCA is trained on resolution 256 and directly evaluated on resolution 512. The prediction still matches the ground truth. but also recover an important property of Neural Operators\u2014 modeling PDEs with zero-shot super-resolution. PDEBENCH FOR SCIENTIFIC ML Table 4: Tabular results with baselines from Hollmann et al. (2022) and Dinh et al. (2022). \u201cDiff. from XGBoost\u201d is the across- task average of per-task difference from XGBoost. ORCA beats classical approaches and advanced transformer methods on 19 tasks. For per-task results, see Appendix A.6. ML models for physical systems have gained increasing interest in recent years. To study how cross-modal \ufb01ne- tuning can help in the scienti\ufb01c ML context, we evaluate ORCA on 8 datasets from PDEBench (Takamoto et al., 2022) and compare against state-of-the-art task-speci\ufb01c models: the physics-informed neural network PINN (Raissi et al., 2019), Fourier neural operator (FNO) (Li et al., 2021), and the generic"}