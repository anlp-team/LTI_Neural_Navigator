{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yiming_Yang_Aligning_Large_Multimodal_Models_with_Factually_Augmented_RLHF_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the two main improvements made to the reward model mentioned in the text?", "answer": " The two main improvements made to the reward model are using a better vision encoder with higher resolutions and a larger language model, and introducing a novel algorithm named Factually Augmented RLHF.", "ref_chunk": "and knowledge in larger language models. Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. 1We instructed crowdworkers to prioritize the responses that exhibit better multimodal alignment and min- imize hallucinations. That is, if two responses are free of hallucinations, the crowdworkers were asked to choose/create a more helpful one. 2 Preprint Human Preference SFT Data RM Score LMM-RLHF GPT-4 LMM-RM Captions Javier\u2019s Tacos \u2013Mexican Fast Food \u2013Open 24 hours Captions [The sign is not very clear, so perhaps]A: American Fast Food Output (A) is better with less hallucinations. A: The cat is resting on a black couchwith its front paws tucked under its chest. Q: What is inthe image? PPO Data Human Sampled Output Q: How does the sleeping environment benefit the cat?(b) Collect Human Preference (More Helpful & Less Hallucinated) Data for Reward Models (RM) This image shows the menu of a coffee chop called Roly\u2019sCaf\u00e9.HumanLMM-SFT SampledOutput (A) SampledOutput (B) A: The sleeping environment on the couch provides the cat with a comfortable and cozy space to rest. (a) Misaligned Supervised Fine-Tuning (SFT) Data contains Hallucination RM Data [The RL model\u2019s output is clearly contradictoryto the image captions]Reward Score: 0.0 Q: What is inthe image?A: Menu from Roly\u2019sCaf\u00e9, Chairs and Tables.(c) Factually Augmented Reinforcement Learning from Human Feedback (Fact-RLHF) Figure 1: Illustration of how hallucination may occur during the Supervised Fine-Tuning (SFT) phase of LMM training and how Factually Augmented RLHF alleviates the issue of limited capacity in the reward model which is initialized from the SFT model. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high- quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenar- ios, placing particular emphasis on penalizing any hallucinations. We create a set of varied bench- mark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 dif- ferent task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impres- sive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improve- ment by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at https://llava-rlhf.github.io. 3 Preprint Instruction We have developed an AI assistant adept at facilitating image-based conversations. However, it oc- casionally generates what we call hallucinations, which are inaccuracies unsupported by the image content or real-world knowledge. In this task, we request that you select the most appropriate response from the AI model based on the conversation context. When making this selection, primarily consider these two factors: Honesty: Fundamentally, the AI should provide accurate information and articulate its uncer- tainty without misleading the user. If one response includes hallucination and the other doesn\u2019t, or if both responses contain hallucinations but one does to a greater extent, you should opt for the more honest response. Helpfulness: In scenarios where both responses are free from hallucinations, you should opt for the more helpful one. The AI should attempt to accomplish the task or answer the question posed, provided it\u2019s not harmful, in the most helpful and engaging manner possible. Annotation Task Please select the better response from A and B [IMAGE] [CONVERSATION CONTEXT] [RESPONSE A] [RESPONSE B] Question 1: Which response has fewer hallucinations in terms of the given image? Question 2: If you have selected a tie between Response 1 and Response 2 from the previous question, which response would be more helpful or less incorrect? Table 2: The instruction to the crowdworkers for human preference collection. 2 METHOD 2.1 MULTIMODAL RLHF Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a) has emerged as a powerful and scalable strategy for aligning Large Language Models (LLMs) with human values. In this work, we use RLHF to align LMMs. The basic pipeline of our multimodal RLHF can be summarized into three stages: Multimodal Supervised Fine-Tuning A vision encoder and a pre-trained LLM are jointly fine- tuned on an instruction-following demonstration dataset using token-level supervision to produce a supervised fine-tuned (SFT) model \u03c0SFT. Multimodal Preference Modeling In this stage, a reward model, alternatively referred to as a preference model, is trained to give a higher score to the \u201cbetter\u201d response. The pairwise comparison training data are typically annotated by human annotators. Formally, let the aggregated preference data be represented as DRM = {(I, x, y0, y1, i)}, where I denotes the image, x denotes the prompt, y0 and y1 are two associated responses, and i indicates the index of the preferred response. The reward model employs a cross-entropy loss function: L(r\u03b8) = \u2212E(I,x,y0,y1,i)\u223cDRM [log \u03c3(r\u03b8(I, x, yi) \u2212 r\u03b8(I, x, y1\u2212i))] . Reinforcement Learning Here, a policy model, initialized through multimodal supervised fine- tuning (SFT) (Ouyang et al., 2022; Touvron et al., 2023b), is trained to generate an appropriate response for each user query by maximizing the reward signal as provided by the reward model. To address potential over-optimization challenges, notably reward hacking, a per-token KL penalty"}, {"question": " What additional information is used to calibrate the reward signals in the Factually Augmented RLHF algorithm?", "answer": " The reward signals are calibrated by augmenting them with additional information such as image captions or ground-truth multi-choice options.", "ref_chunk": "and knowledge in larger language models. Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. 1We instructed crowdworkers to prioritize the responses that exhibit better multimodal alignment and min- imize hallucinations. That is, if two responses are free of hallucinations, the crowdworkers were asked to choose/create a more helpful one. 2 Preprint Human Preference SFT Data RM Score LMM-RLHF GPT-4 LMM-RM Captions Javier\u2019s Tacos \u2013Mexican Fast Food \u2013Open 24 hours Captions [The sign is not very clear, so perhaps]A: American Fast Food Output (A) is better with less hallucinations. A: The cat is resting on a black couchwith its front paws tucked under its chest. Q: What is inthe image? PPO Data Human Sampled Output Q: How does the sleeping environment benefit the cat?(b) Collect Human Preference (More Helpful & Less Hallucinated) Data for Reward Models (RM) This image shows the menu of a coffee chop called Roly\u2019sCaf\u00e9.HumanLMM-SFT SampledOutput (A) SampledOutput (B) A: The sleeping environment on the couch provides the cat with a comfortable and cozy space to rest. (a) Misaligned Supervised Fine-Tuning (SFT) Data contains Hallucination RM Data [The RL model\u2019s output is clearly contradictoryto the image captions]Reward Score: 0.0 Q: What is inthe image?A: Menu from Roly\u2019sCaf\u00e9, Chairs and Tables.(c) Factually Augmented Reinforcement Learning from Human Feedback (Fact-RLHF) Figure 1: Illustration of how hallucination may occur during the Supervised Fine-Tuning (SFT) phase of LMM training and how Factually Augmented RLHF alleviates the issue of limited capacity in the reward model which is initialized from the SFT model. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high- quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenar- ios, placing particular emphasis on penalizing any hallucinations. We create a set of varied bench- mark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 dif- ferent task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impres- sive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improve- ment by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at https://llava-rlhf.github.io. 3 Preprint Instruction We have developed an AI assistant adept at facilitating image-based conversations. However, it oc- casionally generates what we call hallucinations, which are inaccuracies unsupported by the image content or real-world knowledge. In this task, we request that you select the most appropriate response from the AI model based on the conversation context. When making this selection, primarily consider these two factors: Honesty: Fundamentally, the AI should provide accurate information and articulate its uncer- tainty without misleading the user. If one response includes hallucination and the other doesn\u2019t, or if both responses contain hallucinations but one does to a greater extent, you should opt for the more honest response. Helpfulness: In scenarios where both responses are free from hallucinations, you should opt for the more helpful one. The AI should attempt to accomplish the task or answer the question posed, provided it\u2019s not harmful, in the most helpful and engaging manner possible. Annotation Task Please select the better response from A and B [IMAGE] [CONVERSATION CONTEXT] [RESPONSE A] [RESPONSE B] Question 1: Which response has fewer hallucinations in terms of the given image? Question 2: If you have selected a tie between Response 1 and Response 2 from the previous question, which response would be more helpful or less incorrect? Table 2: The instruction to the crowdworkers for human preference collection. 2 METHOD 2.1 MULTIMODAL RLHF Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a) has emerged as a powerful and scalable strategy for aligning Large Language Models (LLMs) with human values. In this work, we use RLHF to align LMMs. The basic pipeline of our multimodal RLHF can be summarized into three stages: Multimodal Supervised Fine-Tuning A vision encoder and a pre-trained LLM are jointly fine- tuned on an instruction-following demonstration dataset using token-level supervision to produce a supervised fine-tuned (SFT) model \u03c0SFT. Multimodal Preference Modeling In this stage, a reward model, alternatively referred to as a preference model, is trained to give a higher score to the \u201cbetter\u201d response. The pairwise comparison training data are typically annotated by human annotators. Formally, let the aggregated preference data be represented as DRM = {(I, x, y0, y1, i)}, where I denotes the image, x denotes the prompt, y0 and y1 are two associated responses, and i indicates the index of the preferred response. The reward model employs a cross-entropy loss function: L(r\u03b8) = \u2212E(I,x,y0,y1,i)\u223cDRM [log \u03c3(r\u03b8(I, x, yi) \u2212 r\u03b8(I, x, y1\u2212i))] . Reinforcement Learning Here, a policy model, initialized through multimodal supervised fine- tuning (SFT) (Ouyang et al., 2022; Touvron et al., 2023b), is trained to generate an appropriate response for each user query by maximizing the reward signal as provided by the reward model. To address potential over-optimization challenges, notably reward hacking, a per-token KL penalty"}, {"question": " What instructions were given to the crowdworkers in terms of prioritizing responses?", "answer": " The crowdworkers were instructed to prioritize responses that exhibit better multimodal alignment and minimize hallucinations. If two responses are free of hallucinations, they were asked to choose/create a more helpful one.", "ref_chunk": "and knowledge in larger language models. Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. 1We instructed crowdworkers to prioritize the responses that exhibit better multimodal alignment and min- imize hallucinations. That is, if two responses are free of hallucinations, the crowdworkers were asked to choose/create a more helpful one. 2 Preprint Human Preference SFT Data RM Score LMM-RLHF GPT-4 LMM-RM Captions Javier\u2019s Tacos \u2013Mexican Fast Food \u2013Open 24 hours Captions [The sign is not very clear, so perhaps]A: American Fast Food Output (A) is better with less hallucinations. A: The cat is resting on a black couchwith its front paws tucked under its chest. Q: What is inthe image? PPO Data Human Sampled Output Q: How does the sleeping environment benefit the cat?(b) Collect Human Preference (More Helpful & Less Hallucinated) Data for Reward Models (RM) This image shows the menu of a coffee chop called Roly\u2019sCaf\u00e9.HumanLMM-SFT SampledOutput (A) SampledOutput (B) A: The sleeping environment on the couch provides the cat with a comfortable and cozy space to rest. (a) Misaligned Supervised Fine-Tuning (SFT) Data contains Hallucination RM Data [The RL model\u2019s output is clearly contradictoryto the image captions]Reward Score: 0.0 Q: What is inthe image?A: Menu from Roly\u2019sCaf\u00e9, Chairs and Tables.(c) Factually Augmented Reinforcement Learning from Human Feedback (Fact-RLHF) Figure 1: Illustration of how hallucination may occur during the Supervised Fine-Tuning (SFT) phase of LMM training and how Factually Augmented RLHF alleviates the issue of limited capacity in the reward model which is initialized from the SFT model. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high- quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenar- ios, placing particular emphasis on penalizing any hallucinations. We create a set of varied bench- mark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 dif- ferent task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impres- sive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improve- ment by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at https://llava-rlhf.github.io. 3 Preprint Instruction We have developed an AI assistant adept at facilitating image-based conversations. However, it oc- casionally generates what we call hallucinations, which are inaccuracies unsupported by the image content or real-world knowledge. In this task, we request that you select the most appropriate response from the AI model based on the conversation context. When making this selection, primarily consider these two factors: Honesty: Fundamentally, the AI should provide accurate information and articulate its uncer- tainty without misleading the user. If one response includes hallucination and the other doesn\u2019t, or if both responses contain hallucinations but one does to a greater extent, you should opt for the more honest response. Helpfulness: In scenarios where both responses are free from hallucinations, you should opt for the more helpful one. The AI should attempt to accomplish the task or answer the question posed, provided it\u2019s not harmful, in the most helpful and engaging manner possible. Annotation Task Please select the better response from A and B [IMAGE] [CONVERSATION CONTEXT] [RESPONSE A] [RESPONSE B] Question 1: Which response has fewer hallucinations in terms of the given image? Question 2: If you have selected a tie between Response 1 and Response 2 from the previous question, which response would be more helpful or less incorrect? Table 2: The instruction to the crowdworkers for human preference collection. 2 METHOD 2.1 MULTIMODAL RLHF Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a) has emerged as a powerful and scalable strategy for aligning Large Language Models (LLMs) with human values. In this work, we use RLHF to align LMMs. The basic pipeline of our multimodal RLHF can be summarized into three stages: Multimodal Supervised Fine-Tuning A vision encoder and a pre-trained LLM are jointly fine- tuned on an instruction-following demonstration dataset using token-level supervision to produce a supervised fine-tuned (SFT) model \u03c0SFT. Multimodal Preference Modeling In this stage, a reward model, alternatively referred to as a preference model, is trained to give a higher score to the \u201cbetter\u201d response. The pairwise comparison training data are typically annotated by human annotators. Formally, let the aggregated preference data be represented as DRM = {(I, x, y0, y1, i)}, where I denotes the image, x denotes the prompt, y0 and y1 are two associated responses, and i indicates the index of the preferred response. The reward model employs a cross-entropy loss function: L(r\u03b8) = \u2212E(I,x,y0,y1,i)\u223cDRM [log \u03c3(r\u03b8(I, x, yi) \u2212 r\u03b8(I, x, y1\u2212i))] . Reinforcement Learning Here, a policy model, initialized through multimodal supervised fine- tuning (SFT) (Ouyang et al., 2022; Touvron et al., 2023b), is trained to generate an appropriate response for each user query by maximizing the reward signal as provided by the reward model. To address potential over-optimization challenges, notably reward hacking, a per-token KL penalty"}, {"question": " What is the purpose of the Factually Augmented Reinforcement Learning from Human Feedback (Fact-RLHF) algorithm?", "answer": " The purpose of the Fact-RLHF algorithm is to alleviate the issue of limited capacity in the reward model and to improve the general capabilities of Large Language Models during the Supervised Fine-Tuning stage.", "ref_chunk": "and knowledge in larger language models. Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. 1We instructed crowdworkers to prioritize the responses that exhibit better multimodal alignment and min- imize hallucinations. That is, if two responses are free of hallucinations, the crowdworkers were asked to choose/create a more helpful one. 2 Preprint Human Preference SFT Data RM Score LMM-RLHF GPT-4 LMM-RM Captions Javier\u2019s Tacos \u2013Mexican Fast Food \u2013Open 24 hours Captions [The sign is not very clear, so perhaps]A: American Fast Food Output (A) is better with less hallucinations. A: The cat is resting on a black couchwith its front paws tucked under its chest. Q: What is inthe image? PPO Data Human Sampled Output Q: How does the sleeping environment benefit the cat?(b) Collect Human Preference (More Helpful & Less Hallucinated) Data for Reward Models (RM) This image shows the menu of a coffee chop called Roly\u2019sCaf\u00e9.HumanLMM-SFT SampledOutput (A) SampledOutput (B) A: The sleeping environment on the couch provides the cat with a comfortable and cozy space to rest. (a) Misaligned Supervised Fine-Tuning (SFT) Data contains Hallucination RM Data [The RL model\u2019s output is clearly contradictoryto the image captions]Reward Score: 0.0 Q: What is inthe image?A: Menu from Roly\u2019sCaf\u00e9, Chairs and Tables.(c) Factually Augmented Reinforcement Learning from Human Feedback (Fact-RLHF) Figure 1: Illustration of how hallucination may occur during the Supervised Fine-Tuning (SFT) phase of LMM training and how Factually Augmented RLHF alleviates the issue of limited capacity in the reward model which is initialized from the SFT model. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high- quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenar- ios, placing particular emphasis on penalizing any hallucinations. We create a set of varied bench- mark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 dif- ferent task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impres- sive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improve- ment by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at https://llava-rlhf.github.io. 3 Preprint Instruction We have developed an AI assistant adept at facilitating image-based conversations. However, it oc- casionally generates what we call hallucinations, which are inaccuracies unsupported by the image content or real-world knowledge. In this task, we request that you select the most appropriate response from the AI model based on the conversation context. When making this selection, primarily consider these two factors: Honesty: Fundamentally, the AI should provide accurate information and articulate its uncer- tainty without misleading the user. If one response includes hallucination and the other doesn\u2019t, or if both responses contain hallucinations but one does to a greater extent, you should opt for the more honest response. Helpfulness: In scenarios where both responses are free from hallucinations, you should opt for the more helpful one. The AI should attempt to accomplish the task or answer the question posed, provided it\u2019s not harmful, in the most helpful and engaging manner possible. Annotation Task Please select the better response from A and B [IMAGE] [CONVERSATION CONTEXT] [RESPONSE A] [RESPONSE B] Question 1: Which response has fewer hallucinations in terms of the given image? Question 2: If you have selected a tie between Response 1 and Response 2 from the previous question, which response would be more helpful or less incorrect? Table 2: The instruction to the crowdworkers for human preference collection. 2 METHOD 2.1 MULTIMODAL RLHF Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a) has emerged as a powerful and scalable strategy for aligning Large Language Models (LLMs) with human values. In this work, we use RLHF to align LMMs. The basic pipeline of our multimodal RLHF can be summarized into three stages: Multimodal Supervised Fine-Tuning A vision encoder and a pre-trained LLM are jointly fine- tuned on an instruction-following demonstration dataset using token-level supervision to produce a supervised fine-tuned (SFT) model \u03c0SFT. Multimodal Preference Modeling In this stage, a reward model, alternatively referred to as a preference model, is trained to give a higher score to the \u201cbetter\u201d response. The pairwise comparison training data are typically annotated by human annotators. Formally, let the aggregated preference data be represented as DRM = {(I, x, y0, y1, i)}, where I denotes the image, x denotes the prompt, y0 and y1 are two associated responses, and i indicates the index of the preferred response. The reward model employs a cross-entropy loss function: L(r\u03b8) = \u2212E(I,x,y0,y1,i)\u223cDRM [log \u03c3(r\u03b8(I, x, yi) \u2212 r\u03b8(I, x, y1\u2212i))] . Reinforcement Learning Here, a policy model, initialized through multimodal supervised fine- tuning (SFT) (Ouyang et al., 2022; Touvron et al., 2023b), is trained to generate an appropriate response for each user query by maximizing the reward signal as provided by the reward model. To address potential over-optimization challenges, notably reward hacking, a per-token KL penalty"}, {"question": " What are the two factors to consider when selecting responses from the AI model based on the conversation context?", "answer": " The two factors to consider are honesty (providing accurate information and articulating uncertainty without misleading the user) and helpfulness (opting for the more helpful response when both responses are free from hallucinations).", "ref_chunk": "and knowledge in larger language models. Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. 1We instructed crowdworkers to prioritize the responses that exhibit better multimodal alignment and min- imize hallucinations. That is, if two responses are free of hallucinations, the crowdworkers were asked to choose/create a more helpful one. 2 Preprint Human Preference SFT Data RM Score LMM-RLHF GPT-4 LMM-RM Captions Javier\u2019s Tacos \u2013Mexican Fast Food \u2013Open 24 hours Captions [The sign is not very clear, so perhaps]A: American Fast Food Output (A) is better with less hallucinations. A: The cat is resting on a black couchwith its front paws tucked under its chest. Q: What is inthe image? PPO Data Human Sampled Output Q: How does the sleeping environment benefit the cat?(b) Collect Human Preference (More Helpful & Less Hallucinated) Data for Reward Models (RM) This image shows the menu of a coffee chop called Roly\u2019sCaf\u00e9.HumanLMM-SFT SampledOutput (A) SampledOutput (B) A: The sleeping environment on the couch provides the cat with a comfortable and cozy space to rest. (a) Misaligned Supervised Fine-Tuning (SFT) Data contains Hallucination RM Data [The RL model\u2019s output is clearly contradictoryto the image captions]Reward Score: 0.0 Q: What is inthe image?A: Menu from Roly\u2019sCaf\u00e9, Chairs and Tables.(c) Factually Augmented Reinforcement Learning from Human Feedback (Fact-RLHF) Figure 1: Illustration of how hallucination may occur during the Supervised Fine-Tuning (SFT) phase of LMM training and how Factually Augmented RLHF alleviates the issue of limited capacity in the reward model which is initialized from the SFT model. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high- quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenar- ios, placing particular emphasis on penalizing any hallucinations. We create a set of varied bench- mark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 dif- ferent task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impres- sive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improve- ment by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at https://llava-rlhf.github.io. 3 Preprint Instruction We have developed an AI assistant adept at facilitating image-based conversations. However, it oc- casionally generates what we call hallucinations, which are inaccuracies unsupported by the image content or real-world knowledge. In this task, we request that you select the most appropriate response from the AI model based on the conversation context. When making this selection, primarily consider these two factors: Honesty: Fundamentally, the AI should provide accurate information and articulate its uncer- tainty without misleading the user. If one response includes hallucination and the other doesn\u2019t, or if both responses contain hallucinations but one does to a greater extent, you should opt for the more honest response. Helpfulness: In scenarios where both responses are free from hallucinations, you should opt for the more helpful one. The AI should attempt to accomplish the task or answer the question posed, provided it\u2019s not harmful, in the most helpful and engaging manner possible. Annotation Task Please select the better response from A and B [IMAGE] [CONVERSATION CONTEXT] [RESPONSE A] [RESPONSE B] Question 1: Which response has fewer hallucinations in terms of the given image? Question 2: If you have selected a tie between Response 1 and Response 2 from the previous question, which response would be more helpful or less incorrect? Table 2: The instruction to the crowdworkers for human preference collection. 2 METHOD 2.1 MULTIMODAL RLHF Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a) has emerged as a powerful and scalable strategy for aligning Large Language Models (LLMs) with human values. In this work, we use RLHF to align LMMs. The basic pipeline of our multimodal RLHF can be summarized into three stages: Multimodal Supervised Fine-Tuning A vision encoder and a pre-trained LLM are jointly fine- tuned on an instruction-following demonstration dataset using token-level supervision to produce a supervised fine-tuned (SFT) model \u03c0SFT. Multimodal Preference Modeling In this stage, a reward model, alternatively referred to as a preference model, is trained to give a higher score to the \u201cbetter\u201d response. The pairwise comparison training data are typically annotated by human annotators. Formally, let the aggregated preference data be represented as DRM = {(I, x, y0, y1, i)}, where I denotes the image, x denotes the prompt, y0 and y1 are two associated responses, and i indicates the index of the preferred response. The reward model employs a cross-entropy loss function: L(r\u03b8) = \u2212E(I,x,y0,y1,i)\u223cDRM [log \u03c3(r\u03b8(I, x, yi) \u2212 r\u03b8(I, x, y1\u2212i))] . Reinforcement Learning Here, a policy model, initialized through multimodal supervised fine- tuning (SFT) (Ouyang et al., 2022; Touvron et al., 2023b), is trained to generate an appropriate response for each user query by maximizing the reward signal as provided by the reward model. To address potential over-optimization challenges, notably reward hacking, a per-token KL penalty"}, {"question": " What are the three stages involved in the Multimodal RLHF pipeline?", "answer": " The three stages involved in the Multimodal RLHF pipeline are Multimodal Supervised Fine-Tuning, Multimodal Preference Modeling, and Reinforcement Learning.", "ref_chunk": "and knowledge in larger language models. Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. 1We instructed crowdworkers to prioritize the responses that exhibit better multimodal alignment and min- imize hallucinations. That is, if two responses are free of hallucinations, the crowdworkers were asked to choose/create a more helpful one. 2 Preprint Human Preference SFT Data RM Score LMM-RLHF GPT-4 LMM-RM Captions Javier\u2019s Tacos \u2013Mexican Fast Food \u2013Open 24 hours Captions [The sign is not very clear, so perhaps]A: American Fast Food Output (A) is better with less hallucinations. A: The cat is resting on a black couchwith its front paws tucked under its chest. Q: What is inthe image? PPO Data Human Sampled Output Q: How does the sleeping environment benefit the cat?(b) Collect Human Preference (More Helpful & Less Hallucinated) Data for Reward Models (RM) This image shows the menu of a coffee chop called Roly\u2019sCaf\u00e9.HumanLMM-SFT SampledOutput (A) SampledOutput (B) A: The sleeping environment on the couch provides the cat with a comfortable and cozy space to rest. (a) Misaligned Supervised Fine-Tuning (SFT) Data contains Hallucination RM Data [The RL model\u2019s output is clearly contradictoryto the image captions]Reward Score: 0.0 Q: What is inthe image?A: Menu from Roly\u2019sCaf\u00e9, Chairs and Tables.(c) Factually Augmented Reinforcement Learning from Human Feedback (Fact-RLHF) Figure 1: Illustration of how hallucination may occur during the Supervised Fine-Tuning (SFT) phase of LMM training and how Factually Augmented RLHF alleviates the issue of limited capacity in the reward model which is initialized from the SFT model. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high- quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenar- ios, placing particular emphasis on penalizing any hallucinations. We create a set of varied bench- mark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 dif- ferent task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impres- sive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improve- ment by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at https://llava-rlhf.github.io. 3 Preprint Instruction We have developed an AI assistant adept at facilitating image-based conversations. However, it oc- casionally generates what we call hallucinations, which are inaccuracies unsupported by the image content or real-world knowledge. In this task, we request that you select the most appropriate response from the AI model based on the conversation context. When making this selection, primarily consider these two factors: Honesty: Fundamentally, the AI should provide accurate information and articulate its uncer- tainty without misleading the user. If one response includes hallucination and the other doesn\u2019t, or if both responses contain hallucinations but one does to a greater extent, you should opt for the more honest response. Helpfulness: In scenarios where both responses are free from hallucinations, you should opt for the more helpful one. The AI should attempt to accomplish the task or answer the question posed, provided it\u2019s not harmful, in the most helpful and engaging manner possible. Annotation Task Please select the better response from A and B [IMAGE] [CONVERSATION CONTEXT] [RESPONSE A] [RESPONSE B] Question 1: Which response has fewer hallucinations in terms of the given image? Question 2: If you have selected a tie between Response 1 and Response 2 from the previous question, which response would be more helpful or less incorrect? Table 2: The instruction to the crowdworkers for human preference collection. 2 METHOD 2.1 MULTIMODAL RLHF Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a) has emerged as a powerful and scalable strategy for aligning Large Language Models (LLMs) with human values. In this work, we use RLHF to align LMMs. The basic pipeline of our multimodal RLHF can be summarized into three stages: Multimodal Supervised Fine-Tuning A vision encoder and a pre-trained LLM are jointly fine- tuned on an instruction-following demonstration dataset using token-level supervision to produce a supervised fine-tuned (SFT) model \u03c0SFT. Multimodal Preference Modeling In this stage, a reward model, alternatively referred to as a preference model, is trained to give a higher score to the \u201cbetter\u201d response. The pairwise comparison training data are typically annotated by human annotators. Formally, let the aggregated preference data be represented as DRM = {(I, x, y0, y1, i)}, where I denotes the image, x denotes the prompt, y0 and y1 are two associated responses, and i indicates the index of the preferred response. The reward model employs a cross-entropy loss function: L(r\u03b8) = \u2212E(I,x,y0,y1,i)\u223cDRM [log \u03c3(r\u03b8(I, x, yi) \u2212 r\u03b8(I, x, y1\u2212i))] . Reinforcement Learning Here, a policy model, initialized through multimodal supervised fine- tuning (SFT) (Ouyang et al., 2022; Touvron et al., 2023b), is trained to generate an appropriate response for each user query by maximizing the reward signal as provided by the reward model. To address potential over-optimization challenges, notably reward hacking, a per-token KL penalty"}, {"question": " What type of loss function does the reward model employ in the training process?", "answer": " The reward model employs a cross-entropy loss function during training.", "ref_chunk": "and knowledge in larger language models. Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. 1We instructed crowdworkers to prioritize the responses that exhibit better multimodal alignment and min- imize hallucinations. That is, if two responses are free of hallucinations, the crowdworkers were asked to choose/create a more helpful one. 2 Preprint Human Preference SFT Data RM Score LMM-RLHF GPT-4 LMM-RM Captions Javier\u2019s Tacos \u2013Mexican Fast Food \u2013Open 24 hours Captions [The sign is not very clear, so perhaps]A: American Fast Food Output (A) is better with less hallucinations. A: The cat is resting on a black couchwith its front paws tucked under its chest. Q: What is inthe image? PPO Data Human Sampled Output Q: How does the sleeping environment benefit the cat?(b) Collect Human Preference (More Helpful & Less Hallucinated) Data for Reward Models (RM) This image shows the menu of a coffee chop called Roly\u2019sCaf\u00e9.HumanLMM-SFT SampledOutput (A) SampledOutput (B) A: The sleeping environment on the couch provides the cat with a comfortable and cozy space to rest. (a) Misaligned Supervised Fine-Tuning (SFT) Data contains Hallucination RM Data [The RL model\u2019s output is clearly contradictoryto the image captions]Reward Score: 0.0 Q: What is inthe image?A: Menu from Roly\u2019sCaf\u00e9, Chairs and Tables.(c) Factually Augmented Reinforcement Learning from Human Feedback (Fact-RLHF) Figure 1: Illustration of how hallucination may occur during the Supervised Fine-Tuning (SFT) phase of LMM training and how Factually Augmented RLHF alleviates the issue of limited capacity in the reward model which is initialized from the SFT model. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high- quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenar- ios, placing particular emphasis on penalizing any hallucinations. We create a set of varied bench- mark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 dif- ferent task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impres- sive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improve- ment by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at https://llava-rlhf.github.io. 3 Preprint Instruction We have developed an AI assistant adept at facilitating image-based conversations. However, it oc- casionally generates what we call hallucinations, which are inaccuracies unsupported by the image content or real-world knowledge. In this task, we request that you select the most appropriate response from the AI model based on the conversation context. When making this selection, primarily consider these two factors: Honesty: Fundamentally, the AI should provide accurate information and articulate its uncer- tainty without misleading the user. If one response includes hallucination and the other doesn\u2019t, or if both responses contain hallucinations but one does to a greater extent, you should opt for the more honest response. Helpfulness: In scenarios where both responses are free from hallucinations, you should opt for the more helpful one. The AI should attempt to accomplish the task or answer the question posed, provided it\u2019s not harmful, in the most helpful and engaging manner possible. Annotation Task Please select the better response from A and B [IMAGE] [CONVERSATION CONTEXT] [RESPONSE A] [RESPONSE B] Question 1: Which response has fewer hallucinations in terms of the given image? Question 2: If you have selected a tie between Response 1 and Response 2 from the previous question, which response would be more helpful or less incorrect? Table 2: The instruction to the crowdworkers for human preference collection. 2 METHOD 2.1 MULTIMODAL RLHF Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a) has emerged as a powerful and scalable strategy for aligning Large Language Models (LLMs) with human values. In this work, we use RLHF to align LMMs. The basic pipeline of our multimodal RLHF can be summarized into three stages: Multimodal Supervised Fine-Tuning A vision encoder and a pre-trained LLM are jointly fine- tuned on an instruction-following demonstration dataset using token-level supervision to produce a supervised fine-tuned (SFT) model \u03c0SFT. Multimodal Preference Modeling In this stage, a reward model, alternatively referred to as a preference model, is trained to give a higher score to the \u201cbetter\u201d response. The pairwise comparison training data are typically annotated by human annotators. Formally, let the aggregated preference data be represented as DRM = {(I, x, y0, y1, i)}, where I denotes the image, x denotes the prompt, y0 and y1 are two associated responses, and i indicates the index of the preferred response. The reward model employs a cross-entropy loss function: L(r\u03b8) = \u2212E(I,x,y0,y1,i)\u223cDRM [log \u03c3(r\u03b8(I, x, yi) \u2212 r\u03b8(I, x, y1\u2212i))] . Reinforcement Learning Here, a policy model, initialized through multimodal supervised fine- tuning (SFT) (Ouyang et al., 2022; Touvron et al., 2023b), is trained to generate an appropriate response for each user query by maximizing the reward signal as provided by the reward model. To address potential over-optimization challenges, notably reward hacking, a per-token KL penalty"}, {"question": " What is the purpose of the reinforcement learning stage in the Multimodal RLHF pipeline?", "answer": " The reinforcement learning stage trains a policy model to generate an appropriate response for each user query by maximizing the reward signal provided by the reward model.", "ref_chunk": "and knowledge in larger language models. Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. 1We instructed crowdworkers to prioritize the responses that exhibit better multimodal alignment and min- imize hallucinations. That is, if two responses are free of hallucinations, the crowdworkers were asked to choose/create a more helpful one. 2 Preprint Human Preference SFT Data RM Score LMM-RLHF GPT-4 LMM-RM Captions Javier\u2019s Tacos \u2013Mexican Fast Food \u2013Open 24 hours Captions [The sign is not very clear, so perhaps]A: American Fast Food Output (A) is better with less hallucinations. A: The cat is resting on a black couchwith its front paws tucked under its chest. Q: What is inthe image? PPO Data Human Sampled Output Q: How does the sleeping environment benefit the cat?(b) Collect Human Preference (More Helpful & Less Hallucinated) Data for Reward Models (RM) This image shows the menu of a coffee chop called Roly\u2019sCaf\u00e9.HumanLMM-SFT SampledOutput (A) SampledOutput (B) A: The sleeping environment on the couch provides the cat with a comfortable and cozy space to rest. (a) Misaligned Supervised Fine-Tuning (SFT) Data contains Hallucination RM Data [The RL model\u2019s output is clearly contradictoryto the image captions]Reward Score: 0.0 Q: What is inthe image?A: Menu from Roly\u2019sCaf\u00e9, Chairs and Tables.(c) Factually Augmented Reinforcement Learning from Human Feedback (Fact-RLHF) Figure 1: Illustration of how hallucination may occur during the Supervised Fine-Tuning (SFT) phase of LMM training and how Factually Augmented RLHF alleviates the issue of limited capacity in the reward model which is initialized from the SFT model. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high- quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenar- ios, placing particular emphasis on penalizing any hallucinations. We create a set of varied bench- mark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 dif- ferent task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impres- sive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improve- ment by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at https://llava-rlhf.github.io. 3 Preprint Instruction We have developed an AI assistant adept at facilitating image-based conversations. However, it oc- casionally generates what we call hallucinations, which are inaccuracies unsupported by the image content or real-world knowledge. In this task, we request that you select the most appropriate response from the AI model based on the conversation context. When making this selection, primarily consider these two factors: Honesty: Fundamentally, the AI should provide accurate information and articulate its uncer- tainty without misleading the user. If one response includes hallucination and the other doesn\u2019t, or if both responses contain hallucinations but one does to a greater extent, you should opt for the more honest response. Helpfulness: In scenarios where both responses are free from hallucinations, you should opt for the more helpful one. The AI should attempt to accomplish the task or answer the question posed, provided it\u2019s not harmful, in the most helpful and engaging manner possible. Annotation Task Please select the better response from A and B [IMAGE] [CONVERSATION CONTEXT] [RESPONSE A] [RESPONSE B] Question 1: Which response has fewer hallucinations in terms of the given image? Question 2: If you have selected a tie between Response 1 and Response 2 from the previous question, which response would be more helpful or less incorrect? Table 2: The instruction to the crowdworkers for human preference collection. 2 METHOD 2.1 MULTIMODAL RLHF Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a) has emerged as a powerful and scalable strategy for aligning Large Language Models (LLMs) with human values. In this work, we use RLHF to align LMMs. The basic pipeline of our multimodal RLHF can be summarized into three stages: Multimodal Supervised Fine-Tuning A vision encoder and a pre-trained LLM are jointly fine- tuned on an instruction-following demonstration dataset using token-level supervision to produce a supervised fine-tuned (SFT) model \u03c0SFT. Multimodal Preference Modeling In this stage, a reward model, alternatively referred to as a preference model, is trained to give a higher score to the \u201cbetter\u201d response. The pairwise comparison training data are typically annotated by human annotators. Formally, let the aggregated preference data be represented as DRM = {(I, x, y0, y1, i)}, where I denotes the image, x denotes the prompt, y0 and y1 are two associated responses, and i indicates the index of the preferred response. The reward model employs a cross-entropy loss function: L(r\u03b8) = \u2212E(I,x,y0,y1,i)\u223cDRM [log \u03c3(r\u03b8(I, x, yi) \u2212 r\u03b8(I, x, y1\u2212i))] . Reinforcement Learning Here, a policy model, initialized through multimodal supervised fine- tuning (SFT) (Ouyang et al., 2022; Touvron et al., 2023b), is trained to generate an appropriate response for each user query by maximizing the reward signal as provided by the reward model. To address potential over-optimization challenges, notably reward hacking, a per-token KL penalty"}, {"question": " What challenge does the reinforcement learning stage aim to address, particularly in terms of potential problems like reward hacking?", "answer": " The reinforcement learning stage aims to address potential challenges like reward hacking by using a per-token KL penalty to prevent over-optimization.", "ref_chunk": "and knowledge in larger language models. Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. 1We instructed crowdworkers to prioritize the responses that exhibit better multimodal alignment and min- imize hallucinations. That is, if two responses are free of hallucinations, the crowdworkers were asked to choose/create a more helpful one. 2 Preprint Human Preference SFT Data RM Score LMM-RLHF GPT-4 LMM-RM Captions Javier\u2019s Tacos \u2013Mexican Fast Food \u2013Open 24 hours Captions [The sign is not very clear, so perhaps]A: American Fast Food Output (A) is better with less hallucinations. A: The cat is resting on a black couchwith its front paws tucked under its chest. Q: What is inthe image? PPO Data Human Sampled Output Q: How does the sleeping environment benefit the cat?(b) Collect Human Preference (More Helpful & Less Hallucinated) Data for Reward Models (RM) This image shows the menu of a coffee chop called Roly\u2019sCaf\u00e9.HumanLMM-SFT SampledOutput (A) SampledOutput (B) A: The sleeping environment on the couch provides the cat with a comfortable and cozy space to rest. (a) Misaligned Supervised Fine-Tuning (SFT) Data contains Hallucination RM Data [The RL model\u2019s output is clearly contradictoryto the image captions]Reward Score: 0.0 Q: What is inthe image?A: Menu from Roly\u2019sCaf\u00e9, Chairs and Tables.(c) Factually Augmented Reinforcement Learning from Human Feedback (Fact-RLHF) Figure 1: Illustration of how hallucination may occur during the Supervised Fine-Tuning (SFT) phase of LMM training and how Factually Augmented RLHF alleviates the issue of limited capacity in the reward model which is initialized from the SFT model. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high- quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenar- ios, placing particular emphasis on penalizing any hallucinations. We create a set of varied bench- mark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 dif- ferent task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impres- sive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improve- ment by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at https://llava-rlhf.github.io. 3 Preprint Instruction We have developed an AI assistant adept at facilitating image-based conversations. However, it oc- casionally generates what we call hallucinations, which are inaccuracies unsupported by the image content or real-world knowledge. In this task, we request that you select the most appropriate response from the AI model based on the conversation context. When making this selection, primarily consider these two factors: Honesty: Fundamentally, the AI should provide accurate information and articulate its uncer- tainty without misleading the user. If one response includes hallucination and the other doesn\u2019t, or if both responses contain hallucinations but one does to a greater extent, you should opt for the more honest response. Helpfulness: In scenarios where both responses are free from hallucinations, you should opt for the more helpful one. The AI should attempt to accomplish the task or answer the question posed, provided it\u2019s not harmful, in the most helpful and engaging manner possible. Annotation Task Please select the better response from A and B [IMAGE] [CONVERSATION CONTEXT] [RESPONSE A] [RESPONSE B] Question 1: Which response has fewer hallucinations in terms of the given image? Question 2: If you have selected a tie between Response 1 and Response 2 from the previous question, which response would be more helpful or less incorrect? Table 2: The instruction to the crowdworkers for human preference collection. 2 METHOD 2.1 MULTIMODAL RLHF Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a) has emerged as a powerful and scalable strategy for aligning Large Language Models (LLMs) with human values. In this work, we use RLHF to align LMMs. The basic pipeline of our multimodal RLHF can be summarized into three stages: Multimodal Supervised Fine-Tuning A vision encoder and a pre-trained LLM are jointly fine- tuned on an instruction-following demonstration dataset using token-level supervision to produce a supervised fine-tuned (SFT) model \u03c0SFT. Multimodal Preference Modeling In this stage, a reward model, alternatively referred to as a preference model, is trained to give a higher score to the \u201cbetter\u201d response. The pairwise comparison training data are typically annotated by human annotators. Formally, let the aggregated preference data be represented as DRM = {(I, x, y0, y1, i)}, where I denotes the image, x denotes the prompt, y0 and y1 are two associated responses, and i indicates the index of the preferred response. The reward model employs a cross-entropy loss function: L(r\u03b8) = \u2212E(I,x,y0,y1,i)\u223cDRM [log \u03c3(r\u03b8(I, x, yi) \u2212 r\u03b8(I, x, y1\u2212i))] . Reinforcement Learning Here, a policy model, initialized through multimodal supervised fine- tuning (SFT) (Ouyang et al., 2022; Touvron et al., 2023b), is trained to generate an appropriate response for each user query by maximizing the reward signal as provided by the reward model. To address potential over-optimization challenges, notably reward hacking, a per-token KL penalty"}], "doc_text": "and knowledge in larger language models. Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. 1We instructed crowdworkers to prioritize the responses that exhibit better multimodal alignment and min- imize hallucinations. That is, if two responses are free of hallucinations, the crowdworkers were asked to choose/create a more helpful one. 2 Preprint Human Preference SFT Data RM Score LMM-RLHF GPT-4 LMM-RM Captions Javier\u2019s Tacos \u2013Mexican Fast Food \u2013Open 24 hours Captions [The sign is not very clear, so perhaps]A: American Fast Food Output (A) is better with less hallucinations. A: The cat is resting on a black couchwith its front paws tucked under its chest. Q: What is inthe image? PPO Data Human Sampled Output Q: How does the sleeping environment benefit the cat?(b) Collect Human Preference (More Helpful & Less Hallucinated) Data for Reward Models (RM) This image shows the menu of a coffee chop called Roly\u2019sCaf\u00e9.HumanLMM-SFT SampledOutput (A) SampledOutput (B) A: The sleeping environment on the couch provides the cat with a comfortable and cozy space to rest. (a) Misaligned Supervised Fine-Tuning (SFT) Data contains Hallucination RM Data [The RL model\u2019s output is clearly contradictoryto the image captions]Reward Score: 0.0 Q: What is inthe image?A: Menu from Roly\u2019sCaf\u00e9, Chairs and Tables.(c) Factually Augmented Reinforcement Learning from Human Feedback (Fact-RLHF) Figure 1: Illustration of how hallucination may occur during the Supervised Fine-Tuning (SFT) phase of LMM training and how Factually Augmented RLHF alleviates the issue of limited capacity in the reward model which is initialized from the SFT model. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high- quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenar- ios, placing particular emphasis on penalizing any hallucinations. We create a set of varied bench- mark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 dif- ferent task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impres- sive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improve- ment by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at https://llava-rlhf.github.io. 3 Preprint Instruction We have developed an AI assistant adept at facilitating image-based conversations. However, it oc- casionally generates what we call hallucinations, which are inaccuracies unsupported by the image content or real-world knowledge. In this task, we request that you select the most appropriate response from the AI model based on the conversation context. When making this selection, primarily consider these two factors: Honesty: Fundamentally, the AI should provide accurate information and articulate its uncer- tainty without misleading the user. If one response includes hallucination and the other doesn\u2019t, or if both responses contain hallucinations but one does to a greater extent, you should opt for the more honest response. Helpfulness: In scenarios where both responses are free from hallucinations, you should opt for the more helpful one. The AI should attempt to accomplish the task or answer the question posed, provided it\u2019s not harmful, in the most helpful and engaging manner possible. Annotation Task Please select the better response from A and B [IMAGE] [CONVERSATION CONTEXT] [RESPONSE A] [RESPONSE B] Question 1: Which response has fewer hallucinations in terms of the given image? Question 2: If you have selected a tie between Response 1 and Response 2 from the previous question, which response would be more helpful or less incorrect? Table 2: The instruction to the crowdworkers for human preference collection. 2 METHOD 2.1 MULTIMODAL RLHF Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a) has emerged as a powerful and scalable strategy for aligning Large Language Models (LLMs) with human values. In this work, we use RLHF to align LMMs. The basic pipeline of our multimodal RLHF can be summarized into three stages: Multimodal Supervised Fine-Tuning A vision encoder and a pre-trained LLM are jointly fine- tuned on an instruction-following demonstration dataset using token-level supervision to produce a supervised fine-tuned (SFT) model \u03c0SFT. Multimodal Preference Modeling In this stage, a reward model, alternatively referred to as a preference model, is trained to give a higher score to the \u201cbetter\u201d response. The pairwise comparison training data are typically annotated by human annotators. Formally, let the aggregated preference data be represented as DRM = {(I, x, y0, y1, i)}, where I denotes the image, x denotes the prompt, y0 and y1 are two associated responses, and i indicates the index of the preferred response. The reward model employs a cross-entropy loss function: L(r\u03b8) = \u2212E(I,x,y0,y1,i)\u223cDRM [log \u03c3(r\u03b8(I, x, yi) \u2212 r\u03b8(I, x, y1\u2212i))] . Reinforcement Learning Here, a policy model, initialized through multimodal supervised fine- tuning (SFT) (Ouyang et al., 2022; Touvron et al., 2023b), is trained to generate an appropriate response for each user query by maximizing the reward signal as provided by the reward model. To address potential over-optimization challenges, notably reward hacking, a per-token KL penalty"}