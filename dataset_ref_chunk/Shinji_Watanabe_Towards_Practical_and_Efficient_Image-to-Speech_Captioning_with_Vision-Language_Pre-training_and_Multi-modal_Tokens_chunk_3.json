{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Towards_Practical_and_Efficient_Image-to-Speech_Captioning_with_Vision-Language_Pre-training_and_Multi-modal_Tokens_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What method is used to maintain the content in the system described in the text?", "answer": " Vector Quantization (VQ)", "ref_chunk": "maintaining its content by applying Vector Quantization (VQ) to the continuous image features. To assess the feasibility of creating effi- cient multi-modal processing systems, we investigate the Im2Sp sys- tem working with quantized image representations, the image units. Therefore, our system now takes discrete image tokens as input and generates discrete speech tokens as output, resembling the operation of an NLP system that works with discrete text input and output [9]. To this end, we employ a pre-trained image vector quantizer of ViT-VQGAN [22, 23], as shown in Fig. 2. The quantizer tokenizes the input image x into image units i \u2208 {1, . . . , Ni}H/8\u00d7W/8 by downsampling its spatial size with a factor of 8. The token size of image units Ni is 8,192 (13 bits). Then, with the image units, we train the Im2Sp model with the aforementioned vision-language pre- training strategy. Therefore, we first train an image-to-text system and then transfer the knowledge into the Im2Sp model. To employ image units as inputs for the image encoder, we follow SEiT [35] and utilize Stem-Adpator to handle the different input sizes. As the system purely works with discrete inputs and outputs, the required data size can be greatly reduced. We compare the bit size of different input [35] and output [15] representations in Table 1. By utilizing image units, we can reduce the required bits to 0.8% compared to the raw image. Moreover, by utilizing speech units at the output side, we only require 0.2% bits compared to raw waveform (based on 16bit, 16kHz audio) or Mel-spectrogram (based on 100 FPS and 80 mel-spectral dimensions). As we remove the repetition of speech units, we can further reduce the data size, similar to that reported in [15]. As a result, we can significantly shrink the amount of data storage and GPU memory needed for training the model for both input and output parts. This makes it much easier to scale multi- modal processing systems to large-scale training. 3. EXPERIMENTS 3.1. Dataset We utilize two Im2Sp databases, Flickr8kAudio [36] and Spo- kenCOCO [28]. For both datasets, Karpathy split [37] is used. Flickr8kAudio is a spoken version of Flickr8k [25] recorded from 183 speakers. It consists of 6,000 images for training, and 1,000 images for validation and testing, respectively. Each image has 5 speech captions. SpokenCOCO is a spoken version of COCO-2014 captioning dataset [24] and is collected by recording the utterances from 2,532 speakers. It has 82,783 training images with 5,000 im- ages for validation and testing, respectively. Five speech captions are provided for each image. For training, the two datasets are utilized together following [28]. Then, the model is evaluated on each validation and test splits of COCO [24] and Flickr8k [25]. For measuring the performance, we employ an off-the-shelf ASR model [38] to transcribe the generated speech. Then, we measure BLEU-4 Table 1. Data size (bits) comparisons according to different data types for image and audio modalities. Based on the image size of 224\u00d7224, audio of 16kHz and 16bits, and Mel-spectrogram of 100 FPS and 80 filter banks. L represents the time length of the audio. Modality Image Audio Data Type Raw Image Image Unit Raw Audio Data Size (bits) 224 \u00d7 224 \u00d7 3 \u00d7 8 28 \u00d7 28 \u00d7 13 16000 \u00d7 L \u00d7 16 Mel-spectrogram 100 \u00d7 80 \u00d7 L \u00d7 32 Speech Unit (<50) \u00d7 L \u00d7 8 Reduction Rate 100% 0.8% 100% 100% <0.2% [39], METEOR [40], ROUGE [41], CIDEr [42], and SPICE [43], which are the standard metrics in image captioning [24], where all metrics indicate better performance with higher values. 3.2. Implementation Details Basically, our Im2Sp model has the similar architecture of GiT-large [21] whose image encoder is ViT-large [32] with a patch size of 14 (i.e., P =14) and decoder is composed of 6-layered transformers [45]. For the input image, we resize images to a size of 224\u00d7224. For the speech unit extraction (Quantizer in Fig. 1b), we use a pre-trained HuBERT-base model [14] and perform K-means clustering on fea- tures extracted at the 6th layer into 200 units (i.e., Nu=200), follow- ing [11]. To generate a waveform, we train a unit-based HiFi-GAN [30, 31] on LJSpeech [46]. For training the Im2Sp model, the im- age encoder and speech decoder are initialized from pre-trained GiT- large of [21]. We freeze the image encoder, and only train the speech decoder and unit embedding layers, for 100k steps with a batch size of 64, a learning rate of 5e\u22125 with a warmup for 10k steps. Mod- els are selected based on the BLEU score on the validation set. For training the image unit-based Im2Sp model, we first pre-train the image unit-based vision-language model on CC3M [47], SBU [48], COCO, and Flickr8k by initializing the image encoder with a pre- trained SEiT [35]. The same text tokenizer with GiT is utilized. Then, the pre-trained image-text model is transferred into Im2Sp. 3.3. Experimental Results Effectiveness of vision-language pre-training. To confirm the effectiveness of vision-language pre-training strategies, we train three variants of the Im2Sp model. 1) The baseline that does not utilize the strategy and just initializes the image encoder with a pre- trained image classifier [32], similar to [28]. 2) The model whose image encoder is initialized with the vision-language pre-trained model, CLIP [20]. 3) The proposed model that both image encoder and text decoder are initialized from the vision-language pre-trained model, GiT [21]. The speech decoders for the first two models are randomly initialized and trained on Im2Sp datasets. Table 2 shows the ablation results on Flickr8k and COCO. Without utilizing the vision-language pre-training, we achieve 12.9 and 17.4 BLEU scores on each database. By initializing the image encoder with pre- trained CLIP using image-text association, we can greatly improve the performances on all metrics from the baseline. Therefore, we can confirm that by utilizing a vision-language pre-trained image encoder instead of a simple image classifier, the model can better capture the language-associated semantics from input images. Next, when we additionally"}, {"question": " How are discrete image tokens processed in the Im2Sp system?", "answer": " They are used as input to generate discrete speech tokens as output.", "ref_chunk": "maintaining its content by applying Vector Quantization (VQ) to the continuous image features. To assess the feasibility of creating effi- cient multi-modal processing systems, we investigate the Im2Sp sys- tem working with quantized image representations, the image units. Therefore, our system now takes discrete image tokens as input and generates discrete speech tokens as output, resembling the operation of an NLP system that works with discrete text input and output [9]. To this end, we employ a pre-trained image vector quantizer of ViT-VQGAN [22, 23], as shown in Fig. 2. The quantizer tokenizes the input image x into image units i \u2208 {1, . . . , Ni}H/8\u00d7W/8 by downsampling its spatial size with a factor of 8. The token size of image units Ni is 8,192 (13 bits). Then, with the image units, we train the Im2Sp model with the aforementioned vision-language pre- training strategy. Therefore, we first train an image-to-text system and then transfer the knowledge into the Im2Sp model. To employ image units as inputs for the image encoder, we follow SEiT [35] and utilize Stem-Adpator to handle the different input sizes. As the system purely works with discrete inputs and outputs, the required data size can be greatly reduced. We compare the bit size of different input [35] and output [15] representations in Table 1. By utilizing image units, we can reduce the required bits to 0.8% compared to the raw image. Moreover, by utilizing speech units at the output side, we only require 0.2% bits compared to raw waveform (based on 16bit, 16kHz audio) or Mel-spectrogram (based on 100 FPS and 80 mel-spectral dimensions). As we remove the repetition of speech units, we can further reduce the data size, similar to that reported in [15]. As a result, we can significantly shrink the amount of data storage and GPU memory needed for training the model for both input and output parts. This makes it much easier to scale multi- modal processing systems to large-scale training. 3. EXPERIMENTS 3.1. Dataset We utilize two Im2Sp databases, Flickr8kAudio [36] and Spo- kenCOCO [28]. For both datasets, Karpathy split [37] is used. Flickr8kAudio is a spoken version of Flickr8k [25] recorded from 183 speakers. It consists of 6,000 images for training, and 1,000 images for validation and testing, respectively. Each image has 5 speech captions. SpokenCOCO is a spoken version of COCO-2014 captioning dataset [24] and is collected by recording the utterances from 2,532 speakers. It has 82,783 training images with 5,000 im- ages for validation and testing, respectively. Five speech captions are provided for each image. For training, the two datasets are utilized together following [28]. Then, the model is evaluated on each validation and test splits of COCO [24] and Flickr8k [25]. For measuring the performance, we employ an off-the-shelf ASR model [38] to transcribe the generated speech. Then, we measure BLEU-4 Table 1. Data size (bits) comparisons according to different data types for image and audio modalities. Based on the image size of 224\u00d7224, audio of 16kHz and 16bits, and Mel-spectrogram of 100 FPS and 80 filter banks. L represents the time length of the audio. Modality Image Audio Data Type Raw Image Image Unit Raw Audio Data Size (bits) 224 \u00d7 224 \u00d7 3 \u00d7 8 28 \u00d7 28 \u00d7 13 16000 \u00d7 L \u00d7 16 Mel-spectrogram 100 \u00d7 80 \u00d7 L \u00d7 32 Speech Unit (<50) \u00d7 L \u00d7 8 Reduction Rate 100% 0.8% 100% 100% <0.2% [39], METEOR [40], ROUGE [41], CIDEr [42], and SPICE [43], which are the standard metrics in image captioning [24], where all metrics indicate better performance with higher values. 3.2. Implementation Details Basically, our Im2Sp model has the similar architecture of GiT-large [21] whose image encoder is ViT-large [32] with a patch size of 14 (i.e., P =14) and decoder is composed of 6-layered transformers [45]. For the input image, we resize images to a size of 224\u00d7224. For the speech unit extraction (Quantizer in Fig. 1b), we use a pre-trained HuBERT-base model [14] and perform K-means clustering on fea- tures extracted at the 6th layer into 200 units (i.e., Nu=200), follow- ing [11]. To generate a waveform, we train a unit-based HiFi-GAN [30, 31] on LJSpeech [46]. For training the Im2Sp model, the im- age encoder and speech decoder are initialized from pre-trained GiT- large of [21]. We freeze the image encoder, and only train the speech decoder and unit embedding layers, for 100k steps with a batch size of 64, a learning rate of 5e\u22125 with a warmup for 10k steps. Mod- els are selected based on the BLEU score on the validation set. For training the image unit-based Im2Sp model, we first pre-train the image unit-based vision-language model on CC3M [47], SBU [48], COCO, and Flickr8k by initializing the image encoder with a pre- trained SEiT [35]. The same text tokenizer with GiT is utilized. Then, the pre-trained image-text model is transferred into Im2Sp. 3.3. Experimental Results Effectiveness of vision-language pre-training. To confirm the effectiveness of vision-language pre-training strategies, we train three variants of the Im2Sp model. 1) The baseline that does not utilize the strategy and just initializes the image encoder with a pre- trained image classifier [32], similar to [28]. 2) The model whose image encoder is initialized with the vision-language pre-trained model, CLIP [20]. 3) The proposed model that both image encoder and text decoder are initialized from the vision-language pre-trained model, GiT [21]. The speech decoders for the first two models are randomly initialized and trained on Im2Sp datasets. Table 2 shows the ablation results on Flickr8k and COCO. Without utilizing the vision-language pre-training, we achieve 12.9 and 17.4 BLEU scores on each database. By initializing the image encoder with pre- trained CLIP using image-text association, we can greatly improve the performances on all metrics from the baseline. Therefore, we can confirm that by utilizing a vision-language pre-trained image encoder instead of a simple image classifier, the model can better capture the language-associated semantics from input images. Next, when we additionally"}, {"question": " What is the function of the ViT-VQGAN in the system?", "answer": " It tokenizes the input image into image units and downsamples its spatial size.", "ref_chunk": "maintaining its content by applying Vector Quantization (VQ) to the continuous image features. To assess the feasibility of creating effi- cient multi-modal processing systems, we investigate the Im2Sp sys- tem working with quantized image representations, the image units. Therefore, our system now takes discrete image tokens as input and generates discrete speech tokens as output, resembling the operation of an NLP system that works with discrete text input and output [9]. To this end, we employ a pre-trained image vector quantizer of ViT-VQGAN [22, 23], as shown in Fig. 2. The quantizer tokenizes the input image x into image units i \u2208 {1, . . . , Ni}H/8\u00d7W/8 by downsampling its spatial size with a factor of 8. The token size of image units Ni is 8,192 (13 bits). Then, with the image units, we train the Im2Sp model with the aforementioned vision-language pre- training strategy. Therefore, we first train an image-to-text system and then transfer the knowledge into the Im2Sp model. To employ image units as inputs for the image encoder, we follow SEiT [35] and utilize Stem-Adpator to handle the different input sizes. As the system purely works with discrete inputs and outputs, the required data size can be greatly reduced. We compare the bit size of different input [35] and output [15] representations in Table 1. By utilizing image units, we can reduce the required bits to 0.8% compared to the raw image. Moreover, by utilizing speech units at the output side, we only require 0.2% bits compared to raw waveform (based on 16bit, 16kHz audio) or Mel-spectrogram (based on 100 FPS and 80 mel-spectral dimensions). As we remove the repetition of speech units, we can further reduce the data size, similar to that reported in [15]. As a result, we can significantly shrink the amount of data storage and GPU memory needed for training the model for both input and output parts. This makes it much easier to scale multi- modal processing systems to large-scale training. 3. EXPERIMENTS 3.1. Dataset We utilize two Im2Sp databases, Flickr8kAudio [36] and Spo- kenCOCO [28]. For both datasets, Karpathy split [37] is used. Flickr8kAudio is a spoken version of Flickr8k [25] recorded from 183 speakers. It consists of 6,000 images for training, and 1,000 images for validation and testing, respectively. Each image has 5 speech captions. SpokenCOCO is a spoken version of COCO-2014 captioning dataset [24] and is collected by recording the utterances from 2,532 speakers. It has 82,783 training images with 5,000 im- ages for validation and testing, respectively. Five speech captions are provided for each image. For training, the two datasets are utilized together following [28]. Then, the model is evaluated on each validation and test splits of COCO [24] and Flickr8k [25]. For measuring the performance, we employ an off-the-shelf ASR model [38] to transcribe the generated speech. Then, we measure BLEU-4 Table 1. Data size (bits) comparisons according to different data types for image and audio modalities. Based on the image size of 224\u00d7224, audio of 16kHz and 16bits, and Mel-spectrogram of 100 FPS and 80 filter banks. L represents the time length of the audio. Modality Image Audio Data Type Raw Image Image Unit Raw Audio Data Size (bits) 224 \u00d7 224 \u00d7 3 \u00d7 8 28 \u00d7 28 \u00d7 13 16000 \u00d7 L \u00d7 16 Mel-spectrogram 100 \u00d7 80 \u00d7 L \u00d7 32 Speech Unit (<50) \u00d7 L \u00d7 8 Reduction Rate 100% 0.8% 100% 100% <0.2% [39], METEOR [40], ROUGE [41], CIDEr [42], and SPICE [43], which are the standard metrics in image captioning [24], where all metrics indicate better performance with higher values. 3.2. Implementation Details Basically, our Im2Sp model has the similar architecture of GiT-large [21] whose image encoder is ViT-large [32] with a patch size of 14 (i.e., P =14) and decoder is composed of 6-layered transformers [45]. For the input image, we resize images to a size of 224\u00d7224. For the speech unit extraction (Quantizer in Fig. 1b), we use a pre-trained HuBERT-base model [14] and perform K-means clustering on fea- tures extracted at the 6th layer into 200 units (i.e., Nu=200), follow- ing [11]. To generate a waveform, we train a unit-based HiFi-GAN [30, 31] on LJSpeech [46]. For training the Im2Sp model, the im- age encoder and speech decoder are initialized from pre-trained GiT- large of [21]. We freeze the image encoder, and only train the speech decoder and unit embedding layers, for 100k steps with a batch size of 64, a learning rate of 5e\u22125 with a warmup for 10k steps. Mod- els are selected based on the BLEU score on the validation set. For training the image unit-based Im2Sp model, we first pre-train the image unit-based vision-language model on CC3M [47], SBU [48], COCO, and Flickr8k by initializing the image encoder with a pre- trained SEiT [35]. The same text tokenizer with GiT is utilized. Then, the pre-trained image-text model is transferred into Im2Sp. 3.3. Experimental Results Effectiveness of vision-language pre-training. To confirm the effectiveness of vision-language pre-training strategies, we train three variants of the Im2Sp model. 1) The baseline that does not utilize the strategy and just initializes the image encoder with a pre- trained image classifier [32], similar to [28]. 2) The model whose image encoder is initialized with the vision-language pre-trained model, CLIP [20]. 3) The proposed model that both image encoder and text decoder are initialized from the vision-language pre-trained model, GiT [21]. The speech decoders for the first two models are randomly initialized and trained on Im2Sp datasets. Table 2 shows the ablation results on Flickr8k and COCO. Without utilizing the vision-language pre-training, we achieve 12.9 and 17.4 BLEU scores on each database. By initializing the image encoder with pre- trained CLIP using image-text association, we can greatly improve the performances on all metrics from the baseline. Therefore, we can confirm that by utilizing a vision-language pre-trained image encoder instead of a simple image classifier, the model can better capture the language-associated semantics from input images. Next, when we additionally"}, {"question": " How are discrete inputs and outputs beneficial in reducing data size according to the text?", "answer": " The required data size can be greatly reduced by working with discrete inputs and outputs.", "ref_chunk": "maintaining its content by applying Vector Quantization (VQ) to the continuous image features. To assess the feasibility of creating effi- cient multi-modal processing systems, we investigate the Im2Sp sys- tem working with quantized image representations, the image units. Therefore, our system now takes discrete image tokens as input and generates discrete speech tokens as output, resembling the operation of an NLP system that works with discrete text input and output [9]. To this end, we employ a pre-trained image vector quantizer of ViT-VQGAN [22, 23], as shown in Fig. 2. The quantizer tokenizes the input image x into image units i \u2208 {1, . . . , Ni}H/8\u00d7W/8 by downsampling its spatial size with a factor of 8. The token size of image units Ni is 8,192 (13 bits). Then, with the image units, we train the Im2Sp model with the aforementioned vision-language pre- training strategy. Therefore, we first train an image-to-text system and then transfer the knowledge into the Im2Sp model. To employ image units as inputs for the image encoder, we follow SEiT [35] and utilize Stem-Adpator to handle the different input sizes. As the system purely works with discrete inputs and outputs, the required data size can be greatly reduced. We compare the bit size of different input [35] and output [15] representations in Table 1. By utilizing image units, we can reduce the required bits to 0.8% compared to the raw image. Moreover, by utilizing speech units at the output side, we only require 0.2% bits compared to raw waveform (based on 16bit, 16kHz audio) or Mel-spectrogram (based on 100 FPS and 80 mel-spectral dimensions). As we remove the repetition of speech units, we can further reduce the data size, similar to that reported in [15]. As a result, we can significantly shrink the amount of data storage and GPU memory needed for training the model for both input and output parts. This makes it much easier to scale multi- modal processing systems to large-scale training. 3. EXPERIMENTS 3.1. Dataset We utilize two Im2Sp databases, Flickr8kAudio [36] and Spo- kenCOCO [28]. For both datasets, Karpathy split [37] is used. Flickr8kAudio is a spoken version of Flickr8k [25] recorded from 183 speakers. It consists of 6,000 images for training, and 1,000 images for validation and testing, respectively. Each image has 5 speech captions. SpokenCOCO is a spoken version of COCO-2014 captioning dataset [24] and is collected by recording the utterances from 2,532 speakers. It has 82,783 training images with 5,000 im- ages for validation and testing, respectively. Five speech captions are provided for each image. For training, the two datasets are utilized together following [28]. Then, the model is evaluated on each validation and test splits of COCO [24] and Flickr8k [25]. For measuring the performance, we employ an off-the-shelf ASR model [38] to transcribe the generated speech. Then, we measure BLEU-4 Table 1. Data size (bits) comparisons according to different data types for image and audio modalities. Based on the image size of 224\u00d7224, audio of 16kHz and 16bits, and Mel-spectrogram of 100 FPS and 80 filter banks. L represents the time length of the audio. Modality Image Audio Data Type Raw Image Image Unit Raw Audio Data Size (bits) 224 \u00d7 224 \u00d7 3 \u00d7 8 28 \u00d7 28 \u00d7 13 16000 \u00d7 L \u00d7 16 Mel-spectrogram 100 \u00d7 80 \u00d7 L \u00d7 32 Speech Unit (<50) \u00d7 L \u00d7 8 Reduction Rate 100% 0.8% 100% 100% <0.2% [39], METEOR [40], ROUGE [41], CIDEr [42], and SPICE [43], which are the standard metrics in image captioning [24], where all metrics indicate better performance with higher values. 3.2. Implementation Details Basically, our Im2Sp model has the similar architecture of GiT-large [21] whose image encoder is ViT-large [32] with a patch size of 14 (i.e., P =14) and decoder is composed of 6-layered transformers [45]. For the input image, we resize images to a size of 224\u00d7224. For the speech unit extraction (Quantizer in Fig. 1b), we use a pre-trained HuBERT-base model [14] and perform K-means clustering on fea- tures extracted at the 6th layer into 200 units (i.e., Nu=200), follow- ing [11]. To generate a waveform, we train a unit-based HiFi-GAN [30, 31] on LJSpeech [46]. For training the Im2Sp model, the im- age encoder and speech decoder are initialized from pre-trained GiT- large of [21]. We freeze the image encoder, and only train the speech decoder and unit embedding layers, for 100k steps with a batch size of 64, a learning rate of 5e\u22125 with a warmup for 10k steps. Mod- els are selected based on the BLEU score on the validation set. For training the image unit-based Im2Sp model, we first pre-train the image unit-based vision-language model on CC3M [47], SBU [48], COCO, and Flickr8k by initializing the image encoder with a pre- trained SEiT [35]. The same text tokenizer with GiT is utilized. Then, the pre-trained image-text model is transferred into Im2Sp. 3.3. Experimental Results Effectiveness of vision-language pre-training. To confirm the effectiveness of vision-language pre-training strategies, we train three variants of the Im2Sp model. 1) The baseline that does not utilize the strategy and just initializes the image encoder with a pre- trained image classifier [32], similar to [28]. 2) The model whose image encoder is initialized with the vision-language pre-trained model, CLIP [20]. 3) The proposed model that both image encoder and text decoder are initialized from the vision-language pre-trained model, GiT [21]. The speech decoders for the first two models are randomly initialized and trained on Im2Sp datasets. Table 2 shows the ablation results on Flickr8k and COCO. Without utilizing the vision-language pre-training, we achieve 12.9 and 17.4 BLEU scores on each database. By initializing the image encoder with pre- trained CLIP using image-text association, we can greatly improve the performances on all metrics from the baseline. Therefore, we can confirm that by utilizing a vision-language pre-trained image encoder instead of a simple image classifier, the model can better capture the language-associated semantics from input images. Next, when we additionally"}, {"question": " What datasets are utilized in the experiments mentioned in the text?", "answer": " The Im2Sp databases, Flickr8kAudio and SpokenCOCO.", "ref_chunk": "maintaining its content by applying Vector Quantization (VQ) to the continuous image features. To assess the feasibility of creating effi- cient multi-modal processing systems, we investigate the Im2Sp sys- tem working with quantized image representations, the image units. Therefore, our system now takes discrete image tokens as input and generates discrete speech tokens as output, resembling the operation of an NLP system that works with discrete text input and output [9]. To this end, we employ a pre-trained image vector quantizer of ViT-VQGAN [22, 23], as shown in Fig. 2. The quantizer tokenizes the input image x into image units i \u2208 {1, . . . , Ni}H/8\u00d7W/8 by downsampling its spatial size with a factor of 8. The token size of image units Ni is 8,192 (13 bits). Then, with the image units, we train the Im2Sp model with the aforementioned vision-language pre- training strategy. Therefore, we first train an image-to-text system and then transfer the knowledge into the Im2Sp model. To employ image units as inputs for the image encoder, we follow SEiT [35] and utilize Stem-Adpator to handle the different input sizes. As the system purely works with discrete inputs and outputs, the required data size can be greatly reduced. We compare the bit size of different input [35] and output [15] representations in Table 1. By utilizing image units, we can reduce the required bits to 0.8% compared to the raw image. Moreover, by utilizing speech units at the output side, we only require 0.2% bits compared to raw waveform (based on 16bit, 16kHz audio) or Mel-spectrogram (based on 100 FPS and 80 mel-spectral dimensions). As we remove the repetition of speech units, we can further reduce the data size, similar to that reported in [15]. As a result, we can significantly shrink the amount of data storage and GPU memory needed for training the model for both input and output parts. This makes it much easier to scale multi- modal processing systems to large-scale training. 3. EXPERIMENTS 3.1. Dataset We utilize two Im2Sp databases, Flickr8kAudio [36] and Spo- kenCOCO [28]. For both datasets, Karpathy split [37] is used. Flickr8kAudio is a spoken version of Flickr8k [25] recorded from 183 speakers. It consists of 6,000 images for training, and 1,000 images for validation and testing, respectively. Each image has 5 speech captions. SpokenCOCO is a spoken version of COCO-2014 captioning dataset [24] and is collected by recording the utterances from 2,532 speakers. It has 82,783 training images with 5,000 im- ages for validation and testing, respectively. Five speech captions are provided for each image. For training, the two datasets are utilized together following [28]. Then, the model is evaluated on each validation and test splits of COCO [24] and Flickr8k [25]. For measuring the performance, we employ an off-the-shelf ASR model [38] to transcribe the generated speech. Then, we measure BLEU-4 Table 1. Data size (bits) comparisons according to different data types for image and audio modalities. Based on the image size of 224\u00d7224, audio of 16kHz and 16bits, and Mel-spectrogram of 100 FPS and 80 filter banks. L represents the time length of the audio. Modality Image Audio Data Type Raw Image Image Unit Raw Audio Data Size (bits) 224 \u00d7 224 \u00d7 3 \u00d7 8 28 \u00d7 28 \u00d7 13 16000 \u00d7 L \u00d7 16 Mel-spectrogram 100 \u00d7 80 \u00d7 L \u00d7 32 Speech Unit (<50) \u00d7 L \u00d7 8 Reduction Rate 100% 0.8% 100% 100% <0.2% [39], METEOR [40], ROUGE [41], CIDEr [42], and SPICE [43], which are the standard metrics in image captioning [24], where all metrics indicate better performance with higher values. 3.2. Implementation Details Basically, our Im2Sp model has the similar architecture of GiT-large [21] whose image encoder is ViT-large [32] with a patch size of 14 (i.e., P =14) and decoder is composed of 6-layered transformers [45]. For the input image, we resize images to a size of 224\u00d7224. For the speech unit extraction (Quantizer in Fig. 1b), we use a pre-trained HuBERT-base model [14] and perform K-means clustering on fea- tures extracted at the 6th layer into 200 units (i.e., Nu=200), follow- ing [11]. To generate a waveform, we train a unit-based HiFi-GAN [30, 31] on LJSpeech [46]. For training the Im2Sp model, the im- age encoder and speech decoder are initialized from pre-trained GiT- large of [21]. We freeze the image encoder, and only train the speech decoder and unit embedding layers, for 100k steps with a batch size of 64, a learning rate of 5e\u22125 with a warmup for 10k steps. Mod- els are selected based on the BLEU score on the validation set. For training the image unit-based Im2Sp model, we first pre-train the image unit-based vision-language model on CC3M [47], SBU [48], COCO, and Flickr8k by initializing the image encoder with a pre- trained SEiT [35]. The same text tokenizer with GiT is utilized. Then, the pre-trained image-text model is transferred into Im2Sp. 3.3. Experimental Results Effectiveness of vision-language pre-training. To confirm the effectiveness of vision-language pre-training strategies, we train three variants of the Im2Sp model. 1) The baseline that does not utilize the strategy and just initializes the image encoder with a pre- trained image classifier [32], similar to [28]. 2) The model whose image encoder is initialized with the vision-language pre-trained model, CLIP [20]. 3) The proposed model that both image encoder and text decoder are initialized from the vision-language pre-trained model, GiT [21]. The speech decoders for the first two models are randomly initialized and trained on Im2Sp datasets. Table 2 shows the ablation results on Flickr8k and COCO. Without utilizing the vision-language pre-training, we achieve 12.9 and 17.4 BLEU scores on each database. By initializing the image encoder with pre- trained CLIP using image-text association, we can greatly improve the performances on all metrics from the baseline. Therefore, we can confirm that by utilizing a vision-language pre-trained image encoder instead of a simple image classifier, the model can better capture the language-associated semantics from input images. Next, when we additionally"}, {"question": " How many images are used for training in the Flickr8kAudio dataset?", "answer": " 6,000 images.", "ref_chunk": "maintaining its content by applying Vector Quantization (VQ) to the continuous image features. To assess the feasibility of creating effi- cient multi-modal processing systems, we investigate the Im2Sp sys- tem working with quantized image representations, the image units. Therefore, our system now takes discrete image tokens as input and generates discrete speech tokens as output, resembling the operation of an NLP system that works with discrete text input and output [9]. To this end, we employ a pre-trained image vector quantizer of ViT-VQGAN [22, 23], as shown in Fig. 2. The quantizer tokenizes the input image x into image units i \u2208 {1, . . . , Ni}H/8\u00d7W/8 by downsampling its spatial size with a factor of 8. The token size of image units Ni is 8,192 (13 bits). Then, with the image units, we train the Im2Sp model with the aforementioned vision-language pre- training strategy. Therefore, we first train an image-to-text system and then transfer the knowledge into the Im2Sp model. To employ image units as inputs for the image encoder, we follow SEiT [35] and utilize Stem-Adpator to handle the different input sizes. As the system purely works with discrete inputs and outputs, the required data size can be greatly reduced. We compare the bit size of different input [35] and output [15] representations in Table 1. By utilizing image units, we can reduce the required bits to 0.8% compared to the raw image. Moreover, by utilizing speech units at the output side, we only require 0.2% bits compared to raw waveform (based on 16bit, 16kHz audio) or Mel-spectrogram (based on 100 FPS and 80 mel-spectral dimensions). As we remove the repetition of speech units, we can further reduce the data size, similar to that reported in [15]. As a result, we can significantly shrink the amount of data storage and GPU memory needed for training the model for both input and output parts. This makes it much easier to scale multi- modal processing systems to large-scale training. 3. EXPERIMENTS 3.1. Dataset We utilize two Im2Sp databases, Flickr8kAudio [36] and Spo- kenCOCO [28]. For both datasets, Karpathy split [37] is used. Flickr8kAudio is a spoken version of Flickr8k [25] recorded from 183 speakers. It consists of 6,000 images for training, and 1,000 images for validation and testing, respectively. Each image has 5 speech captions. SpokenCOCO is a spoken version of COCO-2014 captioning dataset [24] and is collected by recording the utterances from 2,532 speakers. It has 82,783 training images with 5,000 im- ages for validation and testing, respectively. Five speech captions are provided for each image. For training, the two datasets are utilized together following [28]. Then, the model is evaluated on each validation and test splits of COCO [24] and Flickr8k [25]. For measuring the performance, we employ an off-the-shelf ASR model [38] to transcribe the generated speech. Then, we measure BLEU-4 Table 1. Data size (bits) comparisons according to different data types for image and audio modalities. Based on the image size of 224\u00d7224, audio of 16kHz and 16bits, and Mel-spectrogram of 100 FPS and 80 filter banks. L represents the time length of the audio. Modality Image Audio Data Type Raw Image Image Unit Raw Audio Data Size (bits) 224 \u00d7 224 \u00d7 3 \u00d7 8 28 \u00d7 28 \u00d7 13 16000 \u00d7 L \u00d7 16 Mel-spectrogram 100 \u00d7 80 \u00d7 L \u00d7 32 Speech Unit (<50) \u00d7 L \u00d7 8 Reduction Rate 100% 0.8% 100% 100% <0.2% [39], METEOR [40], ROUGE [41], CIDEr [42], and SPICE [43], which are the standard metrics in image captioning [24], where all metrics indicate better performance with higher values. 3.2. Implementation Details Basically, our Im2Sp model has the similar architecture of GiT-large [21] whose image encoder is ViT-large [32] with a patch size of 14 (i.e., P =14) and decoder is composed of 6-layered transformers [45]. For the input image, we resize images to a size of 224\u00d7224. For the speech unit extraction (Quantizer in Fig. 1b), we use a pre-trained HuBERT-base model [14] and perform K-means clustering on fea- tures extracted at the 6th layer into 200 units (i.e., Nu=200), follow- ing [11]. To generate a waveform, we train a unit-based HiFi-GAN [30, 31] on LJSpeech [46]. For training the Im2Sp model, the im- age encoder and speech decoder are initialized from pre-trained GiT- large of [21]. We freeze the image encoder, and only train the speech decoder and unit embedding layers, for 100k steps with a batch size of 64, a learning rate of 5e\u22125 with a warmup for 10k steps. Mod- els are selected based on the BLEU score on the validation set. For training the image unit-based Im2Sp model, we first pre-train the image unit-based vision-language model on CC3M [47], SBU [48], COCO, and Flickr8k by initializing the image encoder with a pre- trained SEiT [35]. The same text tokenizer with GiT is utilized. Then, the pre-trained image-text model is transferred into Im2Sp. 3.3. Experimental Results Effectiveness of vision-language pre-training. To confirm the effectiveness of vision-language pre-training strategies, we train three variants of the Im2Sp model. 1) The baseline that does not utilize the strategy and just initializes the image encoder with a pre- trained image classifier [32], similar to [28]. 2) The model whose image encoder is initialized with the vision-language pre-trained model, CLIP [20]. 3) The proposed model that both image encoder and text decoder are initialized from the vision-language pre-trained model, GiT [21]. The speech decoders for the first two models are randomly initialized and trained on Im2Sp datasets. Table 2 shows the ablation results on Flickr8k and COCO. Without utilizing the vision-language pre-training, we achieve 12.9 and 17.4 BLEU scores on each database. By initializing the image encoder with pre- trained CLIP using image-text association, we can greatly improve the performances on all metrics from the baseline. Therefore, we can confirm that by utilizing a vision-language pre-trained image encoder instead of a simple image classifier, the model can better capture the language-associated semantics from input images. Next, when we additionally"}, {"question": " What is the purpose of utilizing an off-the-shelf ASR model in the evaluation process?", "answer": " To transcribe the generated speech and measure performance using standard metrics.", "ref_chunk": "maintaining its content by applying Vector Quantization (VQ) to the continuous image features. To assess the feasibility of creating effi- cient multi-modal processing systems, we investigate the Im2Sp sys- tem working with quantized image representations, the image units. Therefore, our system now takes discrete image tokens as input and generates discrete speech tokens as output, resembling the operation of an NLP system that works with discrete text input and output [9]. To this end, we employ a pre-trained image vector quantizer of ViT-VQGAN [22, 23], as shown in Fig. 2. The quantizer tokenizes the input image x into image units i \u2208 {1, . . . , Ni}H/8\u00d7W/8 by downsampling its spatial size with a factor of 8. The token size of image units Ni is 8,192 (13 bits). Then, with the image units, we train the Im2Sp model with the aforementioned vision-language pre- training strategy. Therefore, we first train an image-to-text system and then transfer the knowledge into the Im2Sp model. To employ image units as inputs for the image encoder, we follow SEiT [35] and utilize Stem-Adpator to handle the different input sizes. As the system purely works with discrete inputs and outputs, the required data size can be greatly reduced. We compare the bit size of different input [35] and output [15] representations in Table 1. By utilizing image units, we can reduce the required bits to 0.8% compared to the raw image. Moreover, by utilizing speech units at the output side, we only require 0.2% bits compared to raw waveform (based on 16bit, 16kHz audio) or Mel-spectrogram (based on 100 FPS and 80 mel-spectral dimensions). As we remove the repetition of speech units, we can further reduce the data size, similar to that reported in [15]. As a result, we can significantly shrink the amount of data storage and GPU memory needed for training the model for both input and output parts. This makes it much easier to scale multi- modal processing systems to large-scale training. 3. EXPERIMENTS 3.1. Dataset We utilize two Im2Sp databases, Flickr8kAudio [36] and Spo- kenCOCO [28]. For both datasets, Karpathy split [37] is used. Flickr8kAudio is a spoken version of Flickr8k [25] recorded from 183 speakers. It consists of 6,000 images for training, and 1,000 images for validation and testing, respectively. Each image has 5 speech captions. SpokenCOCO is a spoken version of COCO-2014 captioning dataset [24] and is collected by recording the utterances from 2,532 speakers. It has 82,783 training images with 5,000 im- ages for validation and testing, respectively. Five speech captions are provided for each image. For training, the two datasets are utilized together following [28]. Then, the model is evaluated on each validation and test splits of COCO [24] and Flickr8k [25]. For measuring the performance, we employ an off-the-shelf ASR model [38] to transcribe the generated speech. Then, we measure BLEU-4 Table 1. Data size (bits) comparisons according to different data types for image and audio modalities. Based on the image size of 224\u00d7224, audio of 16kHz and 16bits, and Mel-spectrogram of 100 FPS and 80 filter banks. L represents the time length of the audio. Modality Image Audio Data Type Raw Image Image Unit Raw Audio Data Size (bits) 224 \u00d7 224 \u00d7 3 \u00d7 8 28 \u00d7 28 \u00d7 13 16000 \u00d7 L \u00d7 16 Mel-spectrogram 100 \u00d7 80 \u00d7 L \u00d7 32 Speech Unit (<50) \u00d7 L \u00d7 8 Reduction Rate 100% 0.8% 100% 100% <0.2% [39], METEOR [40], ROUGE [41], CIDEr [42], and SPICE [43], which are the standard metrics in image captioning [24], where all metrics indicate better performance with higher values. 3.2. Implementation Details Basically, our Im2Sp model has the similar architecture of GiT-large [21] whose image encoder is ViT-large [32] with a patch size of 14 (i.e., P =14) and decoder is composed of 6-layered transformers [45]. For the input image, we resize images to a size of 224\u00d7224. For the speech unit extraction (Quantizer in Fig. 1b), we use a pre-trained HuBERT-base model [14] and perform K-means clustering on fea- tures extracted at the 6th layer into 200 units (i.e., Nu=200), follow- ing [11]. To generate a waveform, we train a unit-based HiFi-GAN [30, 31] on LJSpeech [46]. For training the Im2Sp model, the im- age encoder and speech decoder are initialized from pre-trained GiT- large of [21]. We freeze the image encoder, and only train the speech decoder and unit embedding layers, for 100k steps with a batch size of 64, a learning rate of 5e\u22125 with a warmup for 10k steps. Mod- els are selected based on the BLEU score on the validation set. For training the image unit-based Im2Sp model, we first pre-train the image unit-based vision-language model on CC3M [47], SBU [48], COCO, and Flickr8k by initializing the image encoder with a pre- trained SEiT [35]. The same text tokenizer with GiT is utilized. Then, the pre-trained image-text model is transferred into Im2Sp. 3.3. Experimental Results Effectiveness of vision-language pre-training. To confirm the effectiveness of vision-language pre-training strategies, we train three variants of the Im2Sp model. 1) The baseline that does not utilize the strategy and just initializes the image encoder with a pre- trained image classifier [32], similar to [28]. 2) The model whose image encoder is initialized with the vision-language pre-trained model, CLIP [20]. 3) The proposed model that both image encoder and text decoder are initialized from the vision-language pre-trained model, GiT [21]. The speech decoders for the first two models are randomly initialized and trained on Im2Sp datasets. Table 2 shows the ablation results on Flickr8k and COCO. Without utilizing the vision-language pre-training, we achieve 12.9 and 17.4 BLEU scores on each database. By initializing the image encoder with pre- trained CLIP using image-text association, we can greatly improve the performances on all metrics from the baseline. Therefore, we can confirm that by utilizing a vision-language pre-trained image encoder instead of a simple image classifier, the model can better capture the language-associated semantics from input images. Next, when we additionally"}, {"question": " What is the architecture of the Im2Sp model described in the text based on?", "answer": " It is based on GiT-large, with ViT-large as the image encoder and a decoder composed of 6-layered transformers.", "ref_chunk": "maintaining its content by applying Vector Quantization (VQ) to the continuous image features. To assess the feasibility of creating effi- cient multi-modal processing systems, we investigate the Im2Sp sys- tem working with quantized image representations, the image units. Therefore, our system now takes discrete image tokens as input and generates discrete speech tokens as output, resembling the operation of an NLP system that works with discrete text input and output [9]. To this end, we employ a pre-trained image vector quantizer of ViT-VQGAN [22, 23], as shown in Fig. 2. The quantizer tokenizes the input image x into image units i \u2208 {1, . . . , Ni}H/8\u00d7W/8 by downsampling its spatial size with a factor of 8. The token size of image units Ni is 8,192 (13 bits). Then, with the image units, we train the Im2Sp model with the aforementioned vision-language pre- training strategy. Therefore, we first train an image-to-text system and then transfer the knowledge into the Im2Sp model. To employ image units as inputs for the image encoder, we follow SEiT [35] and utilize Stem-Adpator to handle the different input sizes. As the system purely works with discrete inputs and outputs, the required data size can be greatly reduced. We compare the bit size of different input [35] and output [15] representations in Table 1. By utilizing image units, we can reduce the required bits to 0.8% compared to the raw image. Moreover, by utilizing speech units at the output side, we only require 0.2% bits compared to raw waveform (based on 16bit, 16kHz audio) or Mel-spectrogram (based on 100 FPS and 80 mel-spectral dimensions). As we remove the repetition of speech units, we can further reduce the data size, similar to that reported in [15]. As a result, we can significantly shrink the amount of data storage and GPU memory needed for training the model for both input and output parts. This makes it much easier to scale multi- modal processing systems to large-scale training. 3. EXPERIMENTS 3.1. Dataset We utilize two Im2Sp databases, Flickr8kAudio [36] and Spo- kenCOCO [28]. For both datasets, Karpathy split [37] is used. Flickr8kAudio is a spoken version of Flickr8k [25] recorded from 183 speakers. It consists of 6,000 images for training, and 1,000 images for validation and testing, respectively. Each image has 5 speech captions. SpokenCOCO is a spoken version of COCO-2014 captioning dataset [24] and is collected by recording the utterances from 2,532 speakers. It has 82,783 training images with 5,000 im- ages for validation and testing, respectively. Five speech captions are provided for each image. For training, the two datasets are utilized together following [28]. Then, the model is evaluated on each validation and test splits of COCO [24] and Flickr8k [25]. For measuring the performance, we employ an off-the-shelf ASR model [38] to transcribe the generated speech. Then, we measure BLEU-4 Table 1. Data size (bits) comparisons according to different data types for image and audio modalities. Based on the image size of 224\u00d7224, audio of 16kHz and 16bits, and Mel-spectrogram of 100 FPS and 80 filter banks. L represents the time length of the audio. Modality Image Audio Data Type Raw Image Image Unit Raw Audio Data Size (bits) 224 \u00d7 224 \u00d7 3 \u00d7 8 28 \u00d7 28 \u00d7 13 16000 \u00d7 L \u00d7 16 Mel-spectrogram 100 \u00d7 80 \u00d7 L \u00d7 32 Speech Unit (<50) \u00d7 L \u00d7 8 Reduction Rate 100% 0.8% 100% 100% <0.2% [39], METEOR [40], ROUGE [41], CIDEr [42], and SPICE [43], which are the standard metrics in image captioning [24], where all metrics indicate better performance with higher values. 3.2. Implementation Details Basically, our Im2Sp model has the similar architecture of GiT-large [21] whose image encoder is ViT-large [32] with a patch size of 14 (i.e., P =14) and decoder is composed of 6-layered transformers [45]. For the input image, we resize images to a size of 224\u00d7224. For the speech unit extraction (Quantizer in Fig. 1b), we use a pre-trained HuBERT-base model [14] and perform K-means clustering on fea- tures extracted at the 6th layer into 200 units (i.e., Nu=200), follow- ing [11]. To generate a waveform, we train a unit-based HiFi-GAN [30, 31] on LJSpeech [46]. For training the Im2Sp model, the im- age encoder and speech decoder are initialized from pre-trained GiT- large of [21]. We freeze the image encoder, and only train the speech decoder and unit embedding layers, for 100k steps with a batch size of 64, a learning rate of 5e\u22125 with a warmup for 10k steps. Mod- els are selected based on the BLEU score on the validation set. For training the image unit-based Im2Sp model, we first pre-train the image unit-based vision-language model on CC3M [47], SBU [48], COCO, and Flickr8k by initializing the image encoder with a pre- trained SEiT [35]. The same text tokenizer with GiT is utilized. Then, the pre-trained image-text model is transferred into Im2Sp. 3.3. Experimental Results Effectiveness of vision-language pre-training. To confirm the effectiveness of vision-language pre-training strategies, we train three variants of the Im2Sp model. 1) The baseline that does not utilize the strategy and just initializes the image encoder with a pre- trained image classifier [32], similar to [28]. 2) The model whose image encoder is initialized with the vision-language pre-trained model, CLIP [20]. 3) The proposed model that both image encoder and text decoder are initialized from the vision-language pre-trained model, GiT [21]. The speech decoders for the first two models are randomly initialized and trained on Im2Sp datasets. Table 2 shows the ablation results on Flickr8k and COCO. Without utilizing the vision-language pre-training, we achieve 12.9 and 17.4 BLEU scores on each database. By initializing the image encoder with pre- trained CLIP using image-text association, we can greatly improve the performances on all metrics from the baseline. Therefore, we can confirm that by utilizing a vision-language pre-trained image encoder instead of a simple image classifier, the model can better capture the language-associated semantics from input images. Next, when we additionally"}, {"question": " How many units are generated through K-means clustering for speech unit extraction?", "answer": " 200 units.", "ref_chunk": "maintaining its content by applying Vector Quantization (VQ) to the continuous image features. To assess the feasibility of creating effi- cient multi-modal processing systems, we investigate the Im2Sp sys- tem working with quantized image representations, the image units. Therefore, our system now takes discrete image tokens as input and generates discrete speech tokens as output, resembling the operation of an NLP system that works with discrete text input and output [9]. To this end, we employ a pre-trained image vector quantizer of ViT-VQGAN [22, 23], as shown in Fig. 2. The quantizer tokenizes the input image x into image units i \u2208 {1, . . . , Ni}H/8\u00d7W/8 by downsampling its spatial size with a factor of 8. The token size of image units Ni is 8,192 (13 bits). Then, with the image units, we train the Im2Sp model with the aforementioned vision-language pre- training strategy. Therefore, we first train an image-to-text system and then transfer the knowledge into the Im2Sp model. To employ image units as inputs for the image encoder, we follow SEiT [35] and utilize Stem-Adpator to handle the different input sizes. As the system purely works with discrete inputs and outputs, the required data size can be greatly reduced. We compare the bit size of different input [35] and output [15] representations in Table 1. By utilizing image units, we can reduce the required bits to 0.8% compared to the raw image. Moreover, by utilizing speech units at the output side, we only require 0.2% bits compared to raw waveform (based on 16bit, 16kHz audio) or Mel-spectrogram (based on 100 FPS and 80 mel-spectral dimensions). As we remove the repetition of speech units, we can further reduce the data size, similar to that reported in [15]. As a result, we can significantly shrink the amount of data storage and GPU memory needed for training the model for both input and output parts. This makes it much easier to scale multi- modal processing systems to large-scale training. 3. EXPERIMENTS 3.1. Dataset We utilize two Im2Sp databases, Flickr8kAudio [36] and Spo- kenCOCO [28]. For both datasets, Karpathy split [37] is used. Flickr8kAudio is a spoken version of Flickr8k [25] recorded from 183 speakers. It consists of 6,000 images for training, and 1,000 images for validation and testing, respectively. Each image has 5 speech captions. SpokenCOCO is a spoken version of COCO-2014 captioning dataset [24] and is collected by recording the utterances from 2,532 speakers. It has 82,783 training images with 5,000 im- ages for validation and testing, respectively. Five speech captions are provided for each image. For training, the two datasets are utilized together following [28]. Then, the model is evaluated on each validation and test splits of COCO [24] and Flickr8k [25]. For measuring the performance, we employ an off-the-shelf ASR model [38] to transcribe the generated speech. Then, we measure BLEU-4 Table 1. Data size (bits) comparisons according to different data types for image and audio modalities. Based on the image size of 224\u00d7224, audio of 16kHz and 16bits, and Mel-spectrogram of 100 FPS and 80 filter banks. L represents the time length of the audio. Modality Image Audio Data Type Raw Image Image Unit Raw Audio Data Size (bits) 224 \u00d7 224 \u00d7 3 \u00d7 8 28 \u00d7 28 \u00d7 13 16000 \u00d7 L \u00d7 16 Mel-spectrogram 100 \u00d7 80 \u00d7 L \u00d7 32 Speech Unit (<50) \u00d7 L \u00d7 8 Reduction Rate 100% 0.8% 100% 100% <0.2% [39], METEOR [40], ROUGE [41], CIDEr [42], and SPICE [43], which are the standard metrics in image captioning [24], where all metrics indicate better performance with higher values. 3.2. Implementation Details Basically, our Im2Sp model has the similar architecture of GiT-large [21] whose image encoder is ViT-large [32] with a patch size of 14 (i.e., P =14) and decoder is composed of 6-layered transformers [45]. For the input image, we resize images to a size of 224\u00d7224. For the speech unit extraction (Quantizer in Fig. 1b), we use a pre-trained HuBERT-base model [14] and perform K-means clustering on fea- tures extracted at the 6th layer into 200 units (i.e., Nu=200), follow- ing [11]. To generate a waveform, we train a unit-based HiFi-GAN [30, 31] on LJSpeech [46]. For training the Im2Sp model, the im- age encoder and speech decoder are initialized from pre-trained GiT- large of [21]. We freeze the image encoder, and only train the speech decoder and unit embedding layers, for 100k steps with a batch size of 64, a learning rate of 5e\u22125 with a warmup for 10k steps. Mod- els are selected based on the BLEU score on the validation set. For training the image unit-based Im2Sp model, we first pre-train the image unit-based vision-language model on CC3M [47], SBU [48], COCO, and Flickr8k by initializing the image encoder with a pre- trained SEiT [35]. The same text tokenizer with GiT is utilized. Then, the pre-trained image-text model is transferred into Im2Sp. 3.3. Experimental Results Effectiveness of vision-language pre-training. To confirm the effectiveness of vision-language pre-training strategies, we train three variants of the Im2Sp model. 1) The baseline that does not utilize the strategy and just initializes the image encoder with a pre- trained image classifier [32], similar to [28]. 2) The model whose image encoder is initialized with the vision-language pre-trained model, CLIP [20]. 3) The proposed model that both image encoder and text decoder are initialized from the vision-language pre-trained model, GiT [21]. The speech decoders for the first two models are randomly initialized and trained on Im2Sp datasets. Table 2 shows the ablation results on Flickr8k and COCO. Without utilizing the vision-language pre-training, we achieve 12.9 and 17.4 BLEU scores on each database. By initializing the image encoder with pre- trained CLIP using image-text association, we can greatly improve the performances on all metrics from the baseline. Therefore, we can confirm that by utilizing a vision-language pre-trained image encoder instead of a simple image classifier, the model can better capture the language-associated semantics from input images. Next, when we additionally"}, {"question": " In the experimental results, what is the proposed model initialized from?", "answer": " The vision-language pre-trained model, GiT.", "ref_chunk": "maintaining its content by applying Vector Quantization (VQ) to the continuous image features. To assess the feasibility of creating effi- cient multi-modal processing systems, we investigate the Im2Sp sys- tem working with quantized image representations, the image units. Therefore, our system now takes discrete image tokens as input and generates discrete speech tokens as output, resembling the operation of an NLP system that works with discrete text input and output [9]. To this end, we employ a pre-trained image vector quantizer of ViT-VQGAN [22, 23], as shown in Fig. 2. The quantizer tokenizes the input image x into image units i \u2208 {1, . . . , Ni}H/8\u00d7W/8 by downsampling its spatial size with a factor of 8. The token size of image units Ni is 8,192 (13 bits). Then, with the image units, we train the Im2Sp model with the aforementioned vision-language pre- training strategy. Therefore, we first train an image-to-text system and then transfer the knowledge into the Im2Sp model. To employ image units as inputs for the image encoder, we follow SEiT [35] and utilize Stem-Adpator to handle the different input sizes. As the system purely works with discrete inputs and outputs, the required data size can be greatly reduced. We compare the bit size of different input [35] and output [15] representations in Table 1. By utilizing image units, we can reduce the required bits to 0.8% compared to the raw image. Moreover, by utilizing speech units at the output side, we only require 0.2% bits compared to raw waveform (based on 16bit, 16kHz audio) or Mel-spectrogram (based on 100 FPS and 80 mel-spectral dimensions). As we remove the repetition of speech units, we can further reduce the data size, similar to that reported in [15]. As a result, we can significantly shrink the amount of data storage and GPU memory needed for training the model for both input and output parts. This makes it much easier to scale multi- modal processing systems to large-scale training. 3. EXPERIMENTS 3.1. Dataset We utilize two Im2Sp databases, Flickr8kAudio [36] and Spo- kenCOCO [28]. For both datasets, Karpathy split [37] is used. Flickr8kAudio is a spoken version of Flickr8k [25] recorded from 183 speakers. It consists of 6,000 images for training, and 1,000 images for validation and testing, respectively. Each image has 5 speech captions. SpokenCOCO is a spoken version of COCO-2014 captioning dataset [24] and is collected by recording the utterances from 2,532 speakers. It has 82,783 training images with 5,000 im- ages for validation and testing, respectively. Five speech captions are provided for each image. For training, the two datasets are utilized together following [28]. Then, the model is evaluated on each validation and test splits of COCO [24] and Flickr8k [25]. For measuring the performance, we employ an off-the-shelf ASR model [38] to transcribe the generated speech. Then, we measure BLEU-4 Table 1. Data size (bits) comparisons according to different data types for image and audio modalities. Based on the image size of 224\u00d7224, audio of 16kHz and 16bits, and Mel-spectrogram of 100 FPS and 80 filter banks. L represents the time length of the audio. Modality Image Audio Data Type Raw Image Image Unit Raw Audio Data Size (bits) 224 \u00d7 224 \u00d7 3 \u00d7 8 28 \u00d7 28 \u00d7 13 16000 \u00d7 L \u00d7 16 Mel-spectrogram 100 \u00d7 80 \u00d7 L \u00d7 32 Speech Unit (<50) \u00d7 L \u00d7 8 Reduction Rate 100% 0.8% 100% 100% <0.2% [39], METEOR [40], ROUGE [41], CIDEr [42], and SPICE [43], which are the standard metrics in image captioning [24], where all metrics indicate better performance with higher values. 3.2. Implementation Details Basically, our Im2Sp model has the similar architecture of GiT-large [21] whose image encoder is ViT-large [32] with a patch size of 14 (i.e., P =14) and decoder is composed of 6-layered transformers [45]. For the input image, we resize images to a size of 224\u00d7224. For the speech unit extraction (Quantizer in Fig. 1b), we use a pre-trained HuBERT-base model [14] and perform K-means clustering on fea- tures extracted at the 6th layer into 200 units (i.e., Nu=200), follow- ing [11]. To generate a waveform, we train a unit-based HiFi-GAN [30, 31] on LJSpeech [46]. For training the Im2Sp model, the im- age encoder and speech decoder are initialized from pre-trained GiT- large of [21]. We freeze the image encoder, and only train the speech decoder and unit embedding layers, for 100k steps with a batch size of 64, a learning rate of 5e\u22125 with a warmup for 10k steps. Mod- els are selected based on the BLEU score on the validation set. For training the image unit-based Im2Sp model, we first pre-train the image unit-based vision-language model on CC3M [47], SBU [48], COCO, and Flickr8k by initializing the image encoder with a pre- trained SEiT [35]. The same text tokenizer with GiT is utilized. Then, the pre-trained image-text model is transferred into Im2Sp. 3.3. Experimental Results Effectiveness of vision-language pre-training. To confirm the effectiveness of vision-language pre-training strategies, we train three variants of the Im2Sp model. 1) The baseline that does not utilize the strategy and just initializes the image encoder with a pre- trained image classifier [32], similar to [28]. 2) The model whose image encoder is initialized with the vision-language pre-trained model, CLIP [20]. 3) The proposed model that both image encoder and text decoder are initialized from the vision-language pre-trained model, GiT [21]. The speech decoders for the first two models are randomly initialized and trained on Im2Sp datasets. Table 2 shows the ablation results on Flickr8k and COCO. Without utilizing the vision-language pre-training, we achieve 12.9 and 17.4 BLEU scores on each database. By initializing the image encoder with pre- trained CLIP using image-text association, we can greatly improve the performances on all metrics from the baseline. Therefore, we can confirm that by utilizing a vision-language pre-trained image encoder instead of a simple image classifier, the model can better capture the language-associated semantics from input images. Next, when we additionally"}], "doc_text": "maintaining its content by applying Vector Quantization (VQ) to the continuous image features. To assess the feasibility of creating effi- cient multi-modal processing systems, we investigate the Im2Sp sys- tem working with quantized image representations, the image units. Therefore, our system now takes discrete image tokens as input and generates discrete speech tokens as output, resembling the operation of an NLP system that works with discrete text input and output [9]. To this end, we employ a pre-trained image vector quantizer of ViT-VQGAN [22, 23], as shown in Fig. 2. The quantizer tokenizes the input image x into image units i \u2208 {1, . . . , Ni}H/8\u00d7W/8 by downsampling its spatial size with a factor of 8. The token size of image units Ni is 8,192 (13 bits). Then, with the image units, we train the Im2Sp model with the aforementioned vision-language pre- training strategy. Therefore, we first train an image-to-text system and then transfer the knowledge into the Im2Sp model. To employ image units as inputs for the image encoder, we follow SEiT [35] and utilize Stem-Adpator to handle the different input sizes. As the system purely works with discrete inputs and outputs, the required data size can be greatly reduced. We compare the bit size of different input [35] and output [15] representations in Table 1. By utilizing image units, we can reduce the required bits to 0.8% compared to the raw image. Moreover, by utilizing speech units at the output side, we only require 0.2% bits compared to raw waveform (based on 16bit, 16kHz audio) or Mel-spectrogram (based on 100 FPS and 80 mel-spectral dimensions). As we remove the repetition of speech units, we can further reduce the data size, similar to that reported in [15]. As a result, we can significantly shrink the amount of data storage and GPU memory needed for training the model for both input and output parts. This makes it much easier to scale multi- modal processing systems to large-scale training. 3. EXPERIMENTS 3.1. Dataset We utilize two Im2Sp databases, Flickr8kAudio [36] and Spo- kenCOCO [28]. For both datasets, Karpathy split [37] is used. Flickr8kAudio is a spoken version of Flickr8k [25] recorded from 183 speakers. It consists of 6,000 images for training, and 1,000 images for validation and testing, respectively. Each image has 5 speech captions. SpokenCOCO is a spoken version of COCO-2014 captioning dataset [24] and is collected by recording the utterances from 2,532 speakers. It has 82,783 training images with 5,000 im- ages for validation and testing, respectively. Five speech captions are provided for each image. For training, the two datasets are utilized together following [28]. Then, the model is evaluated on each validation and test splits of COCO [24] and Flickr8k [25]. For measuring the performance, we employ an off-the-shelf ASR model [38] to transcribe the generated speech. Then, we measure BLEU-4 Table 1. Data size (bits) comparisons according to different data types for image and audio modalities. Based on the image size of 224\u00d7224, audio of 16kHz and 16bits, and Mel-spectrogram of 100 FPS and 80 filter banks. L represents the time length of the audio. Modality Image Audio Data Type Raw Image Image Unit Raw Audio Data Size (bits) 224 \u00d7 224 \u00d7 3 \u00d7 8 28 \u00d7 28 \u00d7 13 16000 \u00d7 L \u00d7 16 Mel-spectrogram 100 \u00d7 80 \u00d7 L \u00d7 32 Speech Unit (<50) \u00d7 L \u00d7 8 Reduction Rate 100% 0.8% 100% 100% <0.2% [39], METEOR [40], ROUGE [41], CIDEr [42], and SPICE [43], which are the standard metrics in image captioning [24], where all metrics indicate better performance with higher values. 3.2. Implementation Details Basically, our Im2Sp model has the similar architecture of GiT-large [21] whose image encoder is ViT-large [32] with a patch size of 14 (i.e., P =14) and decoder is composed of 6-layered transformers [45]. For the input image, we resize images to a size of 224\u00d7224. For the speech unit extraction (Quantizer in Fig. 1b), we use a pre-trained HuBERT-base model [14] and perform K-means clustering on fea- tures extracted at the 6th layer into 200 units (i.e., Nu=200), follow- ing [11]. To generate a waveform, we train a unit-based HiFi-GAN [30, 31] on LJSpeech [46]. For training the Im2Sp model, the im- age encoder and speech decoder are initialized from pre-trained GiT- large of [21]. We freeze the image encoder, and only train the speech decoder and unit embedding layers, for 100k steps with a batch size of 64, a learning rate of 5e\u22125 with a warmup for 10k steps. Mod- els are selected based on the BLEU score on the validation set. For training the image unit-based Im2Sp model, we first pre-train the image unit-based vision-language model on CC3M [47], SBU [48], COCO, and Flickr8k by initializing the image encoder with a pre- trained SEiT [35]. The same text tokenizer with GiT is utilized. Then, the pre-trained image-text model is transferred into Im2Sp. 3.3. Experimental Results Effectiveness of vision-language pre-training. To confirm the effectiveness of vision-language pre-training strategies, we train three variants of the Im2Sp model. 1) The baseline that does not utilize the strategy and just initializes the image encoder with a pre- trained image classifier [32], similar to [28]. 2) The model whose image encoder is initialized with the vision-language pre-trained model, CLIP [20]. 3) The proposed model that both image encoder and text decoder are initialized from the vision-language pre-trained model, GiT [21]. The speech decoders for the first two models are randomly initialized and trained on Im2Sp datasets. Table 2 shows the ablation results on Flickr8k and COCO. Without utilizing the vision-language pre-training, we achieve 12.9 and 17.4 BLEU scores on each database. By initializing the image encoder with pre- trained CLIP using image-text association, we can greatly improve the performances on all metrics from the baseline. Therefore, we can confirm that by utilizing a vision-language pre-trained image encoder instead of a simple image classifier, the model can better capture the language-associated semantics from input images. Next, when we additionally"}