{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Lei_Li_PlayGround_Low_Resource_Machine_Translation_System_for_the_2023_AmericasNLP_Shared_Task.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the goal of the shared task in the AmericasNLP 2023 mentioned in the text?,answer: The goal of the shared task is to translate Spanish into 10 indigenous languages.", "ref_chunk": "2https://github.com/KaieChen/ameircasnlp2023\n1https://scholarworks.iu.edu/dspace/handle/2022/21028qualityforlow-resourcelanguages,althoughitissensitivetothequalityofsyntheticdata.Toaddressthis,weintroducedadatafilteringtechniquetoim-provethequalityofsyntheticdata.Withfilteredbacktranslation,oursystemachievedanaverageimprovementof0.008intheChrF++score.Fur-thermore,ourstudyrevealsthatmultilingualfine-tuningachievescomparabletranslationqualitytobilingualfine-tuningforlow-resourcelanguages.Weselectedthebilingualmodelwithweightaveragingandbacktranslationasourfinalsubmis-sion.TheimplementationofthisstudyisavailableinourGitrepository2.2Methods2.1DataWeadoptedthedatapreparationmethoddescribedbytheUniversityofHelsinki\u2019ssubmissiontoAmer-icasNLP2021(V\u00e1zquezetal.,2021)foroursys-tem.ThedetailsofthedatasetcanbefoundinTable1.Ourmodeltrainingutilizedthefilteredparalleldata(referredtoasparalleldata),whichconsistedofthetrainingdataprovidedbytheorga-nizersaswellasadditionaldatacollectedbytheUniversityofHelsinki(V\u00e1zquezetal.,2021).Inordertogeneratesyntheticparalleldata(referredtoassyntheticdata),weemployedmonolingualdataandappliedbacktranslationtechniques(refertoSection2.3).Thedevelopmentdatawasusedformodelselectionpurposes.2.2Pre-trainedModelOurmodelsarebasedontheNLLB-600MSeq2Seqpre-trainingschemeintroducedbytheNLLBteam(NLLBTeametal.,2022).Fortokenization,weutilizetheSentencePiecetokenizer(KudoandRichardson,2018),followingtheNLLBconfig-uration.TheNLLBmodelwasinitiallytrainedon\nPlayGroundLowResourceMachineTranslationSystemforthe2023AmericasNLPSharedTaskTianruiGu,KaieChen,SiqiOuyang,LeiLiUniversityofCalifornia,SantaBarbara{tianruigu,kaiechen,siqiouyang,leili}@ucsb.eduAbstractThispaperpresentsPlayGround\u2019ssubmissiontotheAmericasNLP2023sharedtaskonma-chinetranslation(MT)intoindigenouslan-guages.WefinetunedNLLB-600M,amultilin-gualMTmodelpre-trainedonFlores-200,on10low-resourcelanguagedirectionsandexam-inedtheeffectivenessofweightaveragingandbacktranslation.Ourexperimentsshowedthatweightaveraging,onaverage,ledtoa0.0169improvementintheChrF++score.Addition-ally,wefoundthatbacktranslationresultedina0.008improvementintheChrF++score.1IntroductionWeparticipatedintheAmericasNLP2023(Ebrahimietal.,2023)sharedtaskwiththegoalofadvancingpreviousstudies(Mageretal.,2021)onindigenousAmericanlanguages.ThetaskistotranslateSpanishinto10indigenouslanguages,includingAshaninka,Aymara,Bribri,Guarani,H\u00f1\u00e4h\u00f1u,Nahuatl,Quechua,Raramuri,Shipibo-Konibo,andWixarika.Additionally,therewasanotherlanguage,Chatino1,forwhichwedidnotparticipatein.WestartedwiththemonolingualandbilingualdatafromMageretal.(2021)andfinetunedNLLB-600M,amultilingualpre-trainedMTmodelfromMeta\u2019sNoLanguageLeftBehind(NLLB)project(NLLBTeametal.,2022)bothbilinguallyandmul-tilingually.Ontopofthat,weemployedweightaveragingandbacktranslation.Forbacktransla-tion,weadditionallyfilteredthebacktranslatedsentencepairstoimprovethedataquality.Wedemonstratethattrainingonmodelweightsaveragedfrommultiplecheckpointsimprovestrans-lationquality,asindicatedbya0.0169increaseintheChrF++scoreonaverage,withoutrequiringad-ditionalcomputationresources.Additionally,wefoundthatbacktranslationcanenhancetranslation\n173 Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP), pages 173\u2013176 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\nKKXk=1\u0398t\u2212k)(2)where\u0398trepresentsthemodelparametersatepocht.Thistechniquesharessimilaritieswithtrainingdifferentmodelsusingvarioushyperparameters(Wortsmanetal.,2022;Xuetal.,2020).How-ever,asweonlyneedtotrainasinglemodel,thistechniquecanbeparticularlyefficientforlargelan-guagemodels.TheeffectivenessofthisapproachisfurtherdiscussedinSection3.2.3.3HyperparametersInthefine-tuningprocess,wefrozetheencoderlayersoftheNLLBmodel,consideringitspriortrainingonavastamountofSpanishsentences.WeoptimizedthemodelusingAdamW(LoshchilovandHutter,2017)withhyperparameters\u03b2=(0.9,0.999),\u03f5=10\u22126.Weemployedalearningrateof3\u00d710\u22124foratotalof10,000iterations.Forregularization,weutilizedthesamedropoutrateastheoriginalNLLBmodelandaweightdecayof0.01.Furthermore,forweightaveraging,wesetthevalueofKtobe5.2.4EvaluationWereporttheresultsusingChrF++(Popovi\u00b4c,2017),followingtheevaluationscript3providedbytheAmericasNLP2023sharedtask.ChrF++\nLangFilteredMonolingDev\n3https://github.com/AmericasNLP/americasnlp2023\nTable1:Numberofsegmentsindataset.FiltereddataandmonolingualdataarecollectedandfilteredbyUni-versityofHelsinkiteam(V\u00e1zquezetal.,2021)fromAmericasNLP2021.theFlores-200dataset,whichconsistsofAymara,Guarani,Quechua,andSpanish.2.3Fine-tunedModelsWefine-tuneNLLB-600Musingthedatamen-tionedinTable1.ForbothX-to-SpanishandSpanish-to-Xdirections,wefine-tuneNLLB-600Musingfilteredparalleldatainbothbilingualandmultilingualway.Thisproduces20bilingualmod-elsand2multilingualmodels.WeleveragetheaboveX-to-Spanishmodelstogeneratebacktranslateddatatoenrichthetrainingcorpus.Thenwefurtherfine-tunetheSpanish-to-Xmodelswithparalleldatasetextendedwithbacktranslatedsentencepairs.Thefinalmodelsareobtainedwithweightaver-agingsincethetrainingcanbeunstablewithinsuf-ficientdata.2.3.1BackTranslationInordertomakeuseofmonolingualdatainin-digenouslanguages,weemployedbacktransla-tion.Specifically,wefrozethedecoderlayersofNLLBmodelandperformedfine-tuningofanX-to-Spanishmodelusingparalleldata.Then,weutilizedthismodeltogeneratesyntheticsentences.Datafiltering:Syntheticsentencesmaycon-tainnoise.Toaddressthisissue,weimplementadatafiltertoselectasubsetofsyntheticsen-tencesthatwillexpandtheoriginalparalleldataset(Ranathungaetal.,2023).Inourtask,weinitiallyfine-tunedaSpanish-to-Xmodelusingtheparalleldata.Subsequently,weevaluatedthismodelonthesyntheticsentencesandselectedthetopNsampleswiththelowestcross-entropyloss.ThevalueofNisdeterminedbythefollowing:N=min(|Ypar|,|Ysyn|)(1)where|Ypar|representsthenumberofsegmentsintheparalleldataset,and|Ysyn|representsthenumberofsegmentsinthesyntheticdataset.Finally,wecombinedtheselectedsyntheticdatawiththeparalleldataandproceededtoperformadditionalfine-tuningoftheNLLBmodel.2.3.2WeightAveragingStudieshaveshownthataveragingtheweightsofmultiplefinetunedmodelscanenhanceaccuracy(Wortsmanetal.,2022).Inourtrainingapproach,theweightsofthenextepocharetrainedbasedontheaverageofthemodelweightsfromthepreviousKepochs.Forinference,wecomputethefinalmodelbyaveragingthemodelweightsfromthelastKepochs.Themodelcanbedefinedasfollows:NLLB(x;\u0398t)=NLLB(x;1\nAshaninka385813195883Aymara835216750996Bribri73030996Guarani1448340516995H\u00f1\u00e4h\u00f1u7049537599Nahuatl174319222672Quechua22862460399996Raramuri165290995Shipibo-Konibo2885423595996Wixarika11525511994\n174\n\n\nTable2:ResultinChrF++ondevelopdataset,exceptforbaselineandBi++(test).BaselinemodelisthebestsubmissionforAmericasNLP2021.Theeffectivenessofweightaveraging(Multi+andBi+)andbacktranslationiscompared(Multi++andBi++).Wealsocomparedtheperformanceofbilingual(Bi)andmultilingual(Multi).capturesthecharacter-levelperformance,makingitparticularlysuitableforevaluatingthepolysyn-theticpropertiesobservedinmanyindigenouslan-guages(Zhengetal.,2021).3ResultsTheresultsarepresentedinTable2forboththedevelopmentandtestdatasets.OurBi++modeldemonstratesimprovementsinfourlanguages:H\u00f1\u00e4h\u00f1u,Aymara,Ash\u00e1ninka,andQuechua,com-paredtotheBaselinemodelprovidedbytheorga-nizer.Ingeneral,thetrendsinresultsforthede-velopmentandtrainingdatasetsaresimilar,exceptforRar\u00e1muriandBribri.Thisdiscrepancymaybeattributedtothetestdatasetcontainingmoreunknowntokens,towhichourmodelissensitive.Previousstudy(Mageretal.,2021)haspri-marilyfocusedonfine-tuningbilingualmachinetranslationmodels.However,theresultsfromourMulti++andBi++modelsdemonstratethepromis-ingpotentialofmultilingualfine-tuning(Tangetal.,2020).Onaverage,theChrF++scoreforMulti++isonly0.0012lowerthanthatofBi++.Wealsocomparedtheeffectivenessofweightaveragingandbacktranslation.Weightaveragingimprovedtranslationsforalltargetlanguages.Onaverage,Multi+achievedaChrF++scorethatwas0.0169higherthanMulti.Theseresultsindicatethatoursimpletechniquecanenhancelow-resourcemachinetranslationwithoutrequiringadditionalcomputationalresources.However,theimpactofbacktranslationvariedacrosslanguages,asobservedintheresultsforMulti+andMulti++.Onaverage,theimplemen-tationofbacktranslationresultedina0.008im-provementintheChrF++metric.ForWixarikaandAymara,therewasaslightdropintheChrF++scoresafterbacktranslation.Despiteperformingdatafiltering,thequalityofsyntheticdatalargelydependsontheperformanceoftheX-to-Spanishmodel.Insummary,ourfine-tuningtechniquehasshownimprovementsinperformance.However,withfurtherrefinementsanddesignenhancements,thereispotentialforourmodeltoachievehigherlevelsofperformance.4ConclusionInthispaper,wepresentedoursubmissiontotheAmericasNLP2023sharedtask.Oursystemuti-lizedtheNLLB-600Mpre-trainedmodeltotrans-lateSpanishinto10indigenouslanguages.Wealsoinvestigatedthepotentialofmultilingualtranslationmodels,whichshowedpromisingresults.Addition-ally,wefoundthataveragingmodelweightsfrompreviousepochsprovedtobeanefficientandeffec-tiveapproach.Whilebacktranslationdemonstratedperformanceimprovements,furthermethodsarenecessarytoaddressnoisydata.Thesefindingshighlightthepositiveoutcomesofourstudyandprovidevaluableinsightsforfutureadvancementsinlow-resourcemachinetranslationtechniques.ReferencesAbteenEbrahimi,ManuelMager,ArturoOncevay,EnoraRice,CynthiaMonta\u00f1o,JohnOrtega,ShrutiRijhwani,AlexisPalmer,RolandoCoto-Solano,Hi-lariaCruz,andKatharinaKann.2023.FindingsoftheAmericasNLP2023sharedtaskonmachinetrans-lationintoindigenouslanguages.InProceedingsof\nTargetlanguageBaseline(Test)MultiMulti+Multi++BiBi++Bi++(Test)\nWixarika0.3040.2770.2940.2940.2660.2790.288H\u00f1\u00e4h\u00f1u0.1470.1290.1330.1380.1440.1410.148Aymara0.2830.2910.3280.3260.3360.3260.300Shipibo-Konibo0.3290.2240.2380.2530.2610.2830.277Nahuatl0.2660.2410.2520.2750.2820.2830.237Guarani0.3360.3040.3160.3210.3150.3030.331Ash\u00e1ninka0.2580.2220.2380.2720.2690.2860.280Quechua0.3430.3240.341-0.337-0.344Rar\u00e1muri0.1840.1610.175-0.184-0.145Bribri0.1650.2100.237-0.231-0.148\n175\n\n\ntheThirdWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas.Associa-tionforComputationalLinguistics.TakuKudoandJohnRichardson.2018.SentencePiece:Asimpleandlanguageindependentsubwordtok-enizeranddetokenizerforneuraltextprocessing.InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations,pages66\u201371,Brussels,Belgium.AssociationforComputationalLinguistics.IlyaLoshchilovandFrankHutter.2017.Fixingweightdecayregularizationinadam.CoRR,abs/1711.05101.ManuelMager,ArturoOncevay,AbteenEbrahimi,JohnOrtega,AnnetteRios,AngelaFan,XimenaGutierrez-Vasques,LuisChiruzzo,GustavoGim\u00e9nez-Lugo,Ri-cardoRamos,IvanVladimirMezaRuiz,RolandoCoto-Solano,AlexisPalmer,ElisabethMager-Hois,VishravChaudhary,GrahamNeubig,NgocThangVu,andKatharinaKann.2021.FindingsoftheAmeri-casNLP2021sharedtaskonopenmachinetransla-tionforindigenouslanguagesoftheAmericas.InProceedingsoftheFirstWorkshoponNaturalLan-guageProcessingforIndigenousLanguagesoftheAmericas,pages202\u2013217,Online.AssociationforComputationalLinguistics.NLLBTeam,MartaR.Costa-juss\u00e0,JamesCross,Onur\u00c7elebi,MahaElbayad,KennethHeafield,KevinHef-fernan,ElaheKalbassi,JaniceLam,DanielLicht,JeanMaillard,AnnaSun,SkylerWang,GuillaumeWenzek,AlYoungblood,BapiAkula,LoicBar-rault,GabrielMejiaGonzalez,PrangthipHansanti,JohnHoffman,SemarleyJarrett,KaushikRamSadagopan,DirkRowe,ShannonSpruit,ChauTran,PierreAndrews,NecipFazilAyan,ShrutiBhosale,SergeyEdunov,AngelaFan,CynthiaGao,VedanujGoswami,FranciscoGuzm\u00e1n,PhilippKoehn,AlexandreMourachko,ChristopheRopers,SafiyyahSaleem,HolgerSchwenk,andJeffWang.2022.Nolanguageleftbehind:Scalinghuman-centeredmachinetranslation.MajaPopovi\u00b4c.2017.chrF++:wordshelpingcharac-tern-grams.InProceedingsoftheSecondConfer-enceonMachineTranslation,pages612\u2013618,Copen-hagen,Denmark.AssociationforComputationalLin-guistics.SurangikaRanathunga,En-ShiunAnnieLee,MarjanaPriftiSkenduli,RaviShekhar,MehreenAlam,andRishemjitKaur.2023.Neuralmachinetranslationforlow-resourcelanguages:Asurvey.ACMComput.Surv.,55(11).YuqingTang,ChauTran,XianLi,Peng-JenChen,Na-manGoyal,VishravChaudhary,JiataoGu,andAn-gelaFan.2020.Multilingualtranslationwithexten-siblemultilingualpretrainingandfinetuning.CoRR,abs/2008.00401.Ra\u00falV\u00e1zquez,YvesScherrer,SamiVirpioja,andJ\u00f6rgTiedemann.2021.TheHelsinkisubmissiontotheAmericasNLPsharedtask.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages255\u2013264,Online.AssociationforComputationalLinguis-tics.MitchellWortsman,GabrielIlharco,SamirYaGadre,RebeccaRoelofs,RaphaelGontijo-Lopes,AriSMor-cos,HongseokNamkoong,AliFarhadi,YairCar-mon,SimonKornblith,andLudwigSchmidt.2022.Modelsoups:averagingweightsofmultiplefine-tunedmodelsimprovesaccuracywithoutincreasinginferencetime.InProceedingsofthe39thInterna-tionalConferenceonMachineLearning,volume162ofProceedingsofMachineLearningResearch,pages23965\u201323998.PMLR.YigeXu,XipengQiu,LigaoZhou,andXuan-jingHuang.2020.ImprovingBERTfine-tuningviaself-ensembleandself-distillation.CoRR,abs/2002.10345.FrancisZheng,MachelReid,EdisonMarrese-Taylor,andYutakaMatsuo.2021.Low-resourcemachinetranslationusingcross-linguallanguagemodelpre-training.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLan-guagesoftheAmericas,pages234\u2013240,Online.As-sociationforComputationalLinguistics.\n176"}, {"question": " What technique was introduced to improve the quality of synthetic data?,answer: A data filtering technique was introduced.", "ref_chunk": "2https://github.com/KaieChen/ameircasnlp2023\n1https://scholarworks.iu.edu/dspace/handle/2022/21028qualityforlow-resourcelanguages,althoughitissensitivetothequalityofsyntheticdata.Toaddressthis,weintroducedadatafilteringtechniquetoim-provethequalityofsyntheticdata.Withfilteredbacktranslation,oursystemachievedanaverageimprovementof0.008intheChrF++score.Fur-thermore,ourstudyrevealsthatmultilingualfine-tuningachievescomparabletranslationqualitytobilingualfine-tuningforlow-resourcelanguages.Weselectedthebilingualmodelwithweightaveragingandbacktranslationasourfinalsubmis-sion.TheimplementationofthisstudyisavailableinourGitrepository2.2Methods2.1DataWeadoptedthedatapreparationmethoddescribedbytheUniversityofHelsinki\u2019ssubmissiontoAmer-icasNLP2021(V\u00e1zquezetal.,2021)foroursys-tem.ThedetailsofthedatasetcanbefoundinTable1.Ourmodeltrainingutilizedthefilteredparalleldata(referredtoasparalleldata),whichconsistedofthetrainingdataprovidedbytheorga-nizersaswellasadditionaldatacollectedbytheUniversityofHelsinki(V\u00e1zquezetal.,2021).Inordertogeneratesyntheticparalleldata(referredtoassyntheticdata),weemployedmonolingualdataandappliedbacktranslationtechniques(refertoSection2.3).Thedevelopmentdatawasusedformodelselectionpurposes.2.2Pre-trainedModelOurmodelsarebasedontheNLLB-600MSeq2Seqpre-trainingschemeintroducedbytheNLLBteam(NLLBTeametal.,2022).Fortokenization,weutilizetheSentencePiecetokenizer(KudoandRichardson,2018),followingtheNLLBconfig-uration.TheNLLBmodelwasinitiallytrainedon\nPlayGroundLowResourceMachineTranslationSystemforthe2023AmericasNLPSharedTaskTianruiGu,KaieChen,SiqiOuyang,LeiLiUniversityofCalifornia,SantaBarbara{tianruigu,kaiechen,siqiouyang,leili}@ucsb.eduAbstractThispaperpresentsPlayGround\u2019ssubmissiontotheAmericasNLP2023sharedtaskonma-chinetranslation(MT)intoindigenouslan-guages.WefinetunedNLLB-600M,amultilin-gualMTmodelpre-trainedonFlores-200,on10low-resourcelanguagedirectionsandexam-inedtheeffectivenessofweightaveragingandbacktranslation.Ourexperimentsshowedthatweightaveraging,onaverage,ledtoa0.0169improvementintheChrF++score.Addition-ally,wefoundthatbacktranslationresultedina0.008improvementintheChrF++score.1IntroductionWeparticipatedintheAmericasNLP2023(Ebrahimietal.,2023)sharedtaskwiththegoalofadvancingpreviousstudies(Mageretal.,2021)onindigenousAmericanlanguages.ThetaskistotranslateSpanishinto10indigenouslanguages,includingAshaninka,Aymara,Bribri,Guarani,H\u00f1\u00e4h\u00f1u,Nahuatl,Quechua,Raramuri,Shipibo-Konibo,andWixarika.Additionally,therewasanotherlanguage,Chatino1,forwhichwedidnotparticipatein.WestartedwiththemonolingualandbilingualdatafromMageretal.(2021)andfinetunedNLLB-600M,amultilingualpre-trainedMTmodelfromMeta\u2019sNoLanguageLeftBehind(NLLB)project(NLLBTeametal.,2022)bothbilinguallyandmul-tilingually.Ontopofthat,weemployedweightaveragingandbacktranslation.Forbacktransla-tion,weadditionallyfilteredthebacktranslatedsentencepairstoimprovethedataquality.Wedemonstratethattrainingonmodelweightsaveragedfrommultiplecheckpointsimprovestrans-lationquality,asindicatedbya0.0169increaseintheChrF++scoreonaverage,withoutrequiringad-ditionalcomputationresources.Additionally,wefoundthatbacktranslationcanenhancetranslation\n173 Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP), pages 173\u2013176 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\nKKXk=1\u0398t\u2212k)(2)where\u0398trepresentsthemodelparametersatepocht.Thistechniquesharessimilaritieswithtrainingdifferentmodelsusingvarioushyperparameters(Wortsmanetal.,2022;Xuetal.,2020).How-ever,asweonlyneedtotrainasinglemodel,thistechniquecanbeparticularlyefficientforlargelan-guagemodels.TheeffectivenessofthisapproachisfurtherdiscussedinSection3.2.3.3HyperparametersInthefine-tuningprocess,wefrozetheencoderlayersoftheNLLBmodel,consideringitspriortrainingonavastamountofSpanishsentences.WeoptimizedthemodelusingAdamW(LoshchilovandHutter,2017)withhyperparameters\u03b2=(0.9,0.999),\u03f5=10\u22126.Weemployedalearningrateof3\u00d710\u22124foratotalof10,000iterations.Forregularization,weutilizedthesamedropoutrateastheoriginalNLLBmodelandaweightdecayof0.01.Furthermore,forweightaveraging,wesetthevalueofKtobe5.2.4EvaluationWereporttheresultsusingChrF++(Popovi\u00b4c,2017),followingtheevaluationscript3providedbytheAmericasNLP2023sharedtask.ChrF++\nLangFilteredMonolingDev\n3https://github.com/AmericasNLP/americasnlp2023\nTable1:Numberofsegmentsindataset.FiltereddataandmonolingualdataarecollectedandfilteredbyUni-versityofHelsinkiteam(V\u00e1zquezetal.,2021)fromAmericasNLP2021.theFlores-200dataset,whichconsistsofAymara,Guarani,Quechua,andSpanish.2.3Fine-tunedModelsWefine-tuneNLLB-600Musingthedatamen-tionedinTable1.ForbothX-to-SpanishandSpanish-to-Xdirections,wefine-tuneNLLB-600Musingfilteredparalleldatainbothbilingualandmultilingualway.Thisproduces20bilingualmod-elsand2multilingualmodels.WeleveragetheaboveX-to-Spanishmodelstogeneratebacktranslateddatatoenrichthetrainingcorpus.Thenwefurtherfine-tunetheSpanish-to-Xmodelswithparalleldatasetextendedwithbacktranslatedsentencepairs.Thefinalmodelsareobtainedwithweightaver-agingsincethetrainingcanbeunstablewithinsuf-ficientdata.2.3.1BackTranslationInordertomakeuseofmonolingualdatainin-digenouslanguages,weemployedbacktransla-tion.Specifically,wefrozethedecoderlayersofNLLBmodelandperformedfine-tuningofanX-to-Spanishmodelusingparalleldata.Then,weutilizedthismodeltogeneratesyntheticsentences.Datafiltering:Syntheticsentencesmaycon-tainnoise.Toaddressthisissue,weimplementadatafiltertoselectasubsetofsyntheticsen-tencesthatwillexpandtheoriginalparalleldataset(Ranathungaetal.,2023).Inourtask,weinitiallyfine-tunedaSpanish-to-Xmodelusingtheparalleldata.Subsequently,weevaluatedthismodelonthesyntheticsentencesandselectedthetopNsampleswiththelowestcross-entropyloss.ThevalueofNisdeterminedbythefollowing:N=min(|Ypar|,|Ysyn|)(1)where|Ypar|representsthenumberofsegmentsintheparalleldataset,and|Ysyn|representsthenumberofsegmentsinthesyntheticdataset.Finally,wecombinedtheselectedsyntheticdatawiththeparalleldataandproceededtoperformadditionalfine-tuningoftheNLLBmodel.2.3.2WeightAveragingStudieshaveshownthataveragingtheweightsofmultiplefinetunedmodelscanenhanceaccuracy(Wortsmanetal.,2022).Inourtrainingapproach,theweightsofthenextepocharetrainedbasedontheaverageofthemodelweightsfromthepreviousKepochs.Forinference,wecomputethefinalmodelbyaveragingthemodelweightsfromthelastKepochs.Themodelcanbedefinedasfollows:NLLB(x;\u0398t)=NLLB(x;1\nAshaninka385813195883Aymara835216750996Bribri73030996Guarani1448340516995H\u00f1\u00e4h\u00f1u7049537599Nahuatl174319222672Quechua22862460399996Raramuri165290995Shipibo-Konibo2885423595996Wixarika11525511994\n174\n\n\nTable2:ResultinChrF++ondevelopdataset,exceptforbaselineandBi++(test).BaselinemodelisthebestsubmissionforAmericasNLP2021.Theeffectivenessofweightaveraging(Multi+andBi+)andbacktranslationiscompared(Multi++andBi++).Wealsocomparedtheperformanceofbilingual(Bi)andmultilingual(Multi).capturesthecharacter-levelperformance,makingitparticularlysuitableforevaluatingthepolysyn-theticpropertiesobservedinmanyindigenouslan-guages(Zhengetal.,2021).3ResultsTheresultsarepresentedinTable2forboththedevelopmentandtestdatasets.OurBi++modeldemonstratesimprovementsinfourlanguages:H\u00f1\u00e4h\u00f1u,Aymara,Ash\u00e1ninka,andQuechua,com-paredtotheBaselinemodelprovidedbytheorga-nizer.Ingeneral,thetrendsinresultsforthede-velopmentandtrainingdatasetsaresimilar,exceptforRar\u00e1muriandBribri.Thisdiscrepancymaybeattributedtothetestdatasetcontainingmoreunknowntokens,towhichourmodelissensitive.Previousstudy(Mageretal.,2021)haspri-marilyfocusedonfine-tuningbilingualmachinetranslationmodels.However,theresultsfromourMulti++andBi++modelsdemonstratethepromis-ingpotentialofmultilingualfine-tuning(Tangetal.,2020).Onaverage,theChrF++scoreforMulti++isonly0.0012lowerthanthatofBi++.Wealsocomparedtheeffectivenessofweightaveragingandbacktranslation.Weightaveragingimprovedtranslationsforalltargetlanguages.Onaverage,Multi+achievedaChrF++scorethatwas0.0169higherthanMulti.Theseresultsindicatethatoursimpletechniquecanenhancelow-resourcemachinetranslationwithoutrequiringadditionalcomputationalresources.However,theimpactofbacktranslationvariedacrosslanguages,asobservedintheresultsforMulti+andMulti++.Onaverage,theimplemen-tationofbacktranslationresultedina0.008im-provementintheChrF++metric.ForWixarikaandAymara,therewasaslightdropintheChrF++scoresafterbacktranslation.Despiteperformingdatafiltering,thequalityofsyntheticdatalargelydependsontheperformanceoftheX-to-Spanishmodel.Insummary,ourfine-tuningtechniquehasshownimprovementsinperformance.However,withfurtherrefinementsanddesignenhancements,thereispotentialforourmodeltoachievehigherlevelsofperformance.4ConclusionInthispaper,wepresentedoursubmissiontotheAmericasNLP2023sharedtask.Oursystemuti-lizedtheNLLB-600Mpre-trainedmodeltotrans-lateSpanishinto10indigenouslanguages.Wealsoinvestigatedthepotentialofmultilingualtranslationmodels,whichshowedpromisingresults.Addition-ally,wefoundthataveragingmodelweightsfrompreviousepochsprovedtobeanefficientandeffec-tiveapproach.Whilebacktranslationdemonstratedperformanceimprovements,furthermethodsarenecessarytoaddressnoisydata.Thesefindingshighlightthepositiveoutcomesofourstudyandprovidevaluableinsightsforfutureadvancementsinlow-resourcemachinetranslationtechniques.ReferencesAbteenEbrahimi,ManuelMager,ArturoOncevay,EnoraRice,CynthiaMonta\u00f1o,JohnOrtega,ShrutiRijhwani,AlexisPalmer,RolandoCoto-Solano,Hi-lariaCruz,andKatharinaKann.2023.FindingsoftheAmericasNLP2023sharedtaskonmachinetrans-lationintoindigenouslanguages.InProceedingsof\nTargetlanguageBaseline(Test)MultiMulti+Multi++BiBi++Bi++(Test)\nWixarika0.3040.2770.2940.2940.2660.2790.288H\u00f1\u00e4h\u00f1u0.1470.1290.1330.1380.1440.1410.148Aymara0.2830.2910.3280.3260.3360.3260.300Shipibo-Konibo0.3290.2240.2380.2530.2610.2830.277Nahuatl0.2660.2410.2520.2750.2820.2830.237Guarani0.3360.3040.3160.3210.3150.3030.331Ash\u00e1ninka0.2580.2220.2380.2720.2690.2860.280Quechua0.3430.3240.341-0.337-0.344Rar\u00e1muri0.1840.1610.175-0.184-0.145Bribri0.1650.2100.237-0.231-0.148\n175\n\n\ntheThirdWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas.Associa-tionforComputationalLinguistics.TakuKudoandJohnRichardson.2018.SentencePiece:Asimpleandlanguageindependentsubwordtok-enizeranddetokenizerforneuraltextprocessing.InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations,pages66\u201371,Brussels,Belgium.AssociationforComputationalLinguistics.IlyaLoshchilovandFrankHutter.2017.Fixingweightdecayregularizationinadam.CoRR,abs/1711.05101.ManuelMager,ArturoOncevay,AbteenEbrahimi,JohnOrtega,AnnetteRios,AngelaFan,XimenaGutierrez-Vasques,LuisChiruzzo,GustavoGim\u00e9nez-Lugo,Ri-cardoRamos,IvanVladimirMezaRuiz,RolandoCoto-Solano,AlexisPalmer,ElisabethMager-Hois,VishravChaudhary,GrahamNeubig,NgocThangVu,andKatharinaKann.2021.FindingsoftheAmeri-casNLP2021sharedtaskonopenmachinetransla-tionforindigenouslanguagesoftheAmericas.InProceedingsoftheFirstWorkshoponNaturalLan-guageProcessingforIndigenousLanguagesoftheAmericas,pages202\u2013217,Online.AssociationforComputationalLinguistics.NLLBTeam,MartaR.Costa-juss\u00e0,JamesCross,Onur\u00c7elebi,MahaElbayad,KennethHeafield,KevinHef-fernan,ElaheKalbassi,JaniceLam,DanielLicht,JeanMaillard,AnnaSun,SkylerWang,GuillaumeWenzek,AlYoungblood,BapiAkula,LoicBar-rault,GabrielMejiaGonzalez,PrangthipHansanti,JohnHoffman,SemarleyJarrett,KaushikRamSadagopan,DirkRowe,ShannonSpruit,ChauTran,PierreAndrews,NecipFazilAyan,ShrutiBhosale,SergeyEdunov,AngelaFan,CynthiaGao,VedanujGoswami,FranciscoGuzm\u00e1n,PhilippKoehn,AlexandreMourachko,ChristopheRopers,SafiyyahSaleem,HolgerSchwenk,andJeffWang.2022.Nolanguageleftbehind:Scalinghuman-centeredmachinetranslation.MajaPopovi\u00b4c.2017.chrF++:wordshelpingcharac-tern-grams.InProceedingsoftheSecondConfer-enceonMachineTranslation,pages612\u2013618,Copen-hagen,Denmark.AssociationforComputationalLin-guistics.SurangikaRanathunga,En-ShiunAnnieLee,MarjanaPriftiSkenduli,RaviShekhar,MehreenAlam,andRishemjitKaur.2023.Neuralmachinetranslationforlow-resourcelanguages:Asurvey.ACMComput.Surv.,55(11).YuqingTang,ChauTran,XianLi,Peng-JenChen,Na-manGoyal,VishravChaudhary,JiataoGu,andAn-gelaFan.2020.Multilingualtranslationwithexten-siblemultilingualpretrainingandfinetuning.CoRR,abs/2008.00401.Ra\u00falV\u00e1zquez,YvesScherrer,SamiVirpioja,andJ\u00f6rgTiedemann.2021.TheHelsinkisubmissiontotheAmericasNLPsharedtask.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages255\u2013264,Online.AssociationforComputationalLinguis-tics.MitchellWortsman,GabrielIlharco,SamirYaGadre,RebeccaRoelofs,RaphaelGontijo-Lopes,AriSMor-cos,HongseokNamkoong,AliFarhadi,YairCar-mon,SimonKornblith,andLudwigSchmidt.2022.Modelsoups:averagingweightsofmultiplefine-tunedmodelsimprovesaccuracywithoutincreasinginferencetime.InProceedingsofthe39thInterna-tionalConferenceonMachineLearning,volume162ofProceedingsofMachineLearningResearch,pages23965\u201323998.PMLR.YigeXu,XipengQiu,LigaoZhou,andXuan-jingHuang.2020.ImprovingBERTfine-tuningviaself-ensembleandself-distillation.CoRR,abs/2002.10345.FrancisZheng,MachelReid,EdisonMarrese-Taylor,andYutakaMatsuo.2021.Low-resourcemachinetranslationusingcross-linguallanguagemodelpre-training.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLan-guagesoftheAmericas,pages234\u2013240,Online.As-sociationforComputationalLinguistics.\n176"}, {"question": " How much improvement was achieved in the ChrF++ score through filtered back translation?,answer: An average improvement of 0.008 in the ChrF++ score was achieved.", "ref_chunk": "2https://github.com/KaieChen/ameircasnlp2023\n1https://scholarworks.iu.edu/dspace/handle/2022/21028qualityforlow-resourcelanguages,althoughitissensitivetothequalityofsyntheticdata.Toaddressthis,weintroducedadatafilteringtechniquetoim-provethequalityofsyntheticdata.Withfilteredbacktranslation,oursystemachievedanaverageimprovementof0.008intheChrF++score.Fur-thermore,ourstudyrevealsthatmultilingualfine-tuningachievescomparabletranslationqualitytobilingualfine-tuningforlow-resourcelanguages.Weselectedthebilingualmodelwithweightaveragingandbacktranslationasourfinalsubmis-sion.TheimplementationofthisstudyisavailableinourGitrepository2.2Methods2.1DataWeadoptedthedatapreparationmethoddescribedbytheUniversityofHelsinki\u2019ssubmissiontoAmer-icasNLP2021(V\u00e1zquezetal.,2021)foroursys-tem.ThedetailsofthedatasetcanbefoundinTable1.Ourmodeltrainingutilizedthefilteredparalleldata(referredtoasparalleldata),whichconsistedofthetrainingdataprovidedbytheorga-nizersaswellasadditionaldatacollectedbytheUniversityofHelsinki(V\u00e1zquezetal.,2021).Inordertogeneratesyntheticparalleldata(referredtoassyntheticdata),weemployedmonolingualdataandappliedbacktranslationtechniques(refertoSection2.3).Thedevelopmentdatawasusedformodelselectionpurposes.2.2Pre-trainedModelOurmodelsarebasedontheNLLB-600MSeq2Seqpre-trainingschemeintroducedbytheNLLBteam(NLLBTeametal.,2022).Fortokenization,weutilizetheSentencePiecetokenizer(KudoandRichardson,2018),followingtheNLLBconfig-uration.TheNLLBmodelwasinitiallytrainedon\nPlayGroundLowResourceMachineTranslationSystemforthe2023AmericasNLPSharedTaskTianruiGu,KaieChen,SiqiOuyang,LeiLiUniversityofCalifornia,SantaBarbara{tianruigu,kaiechen,siqiouyang,leili}@ucsb.eduAbstractThispaperpresentsPlayGround\u2019ssubmissiontotheAmericasNLP2023sharedtaskonma-chinetranslation(MT)intoindigenouslan-guages.WefinetunedNLLB-600M,amultilin-gualMTmodelpre-trainedonFlores-200,on10low-resourcelanguagedirectionsandexam-inedtheeffectivenessofweightaveragingandbacktranslation.Ourexperimentsshowedthatweightaveraging,onaverage,ledtoa0.0169improvementintheChrF++score.Addition-ally,wefoundthatbacktranslationresultedina0.008improvementintheChrF++score.1IntroductionWeparticipatedintheAmericasNLP2023(Ebrahimietal.,2023)sharedtaskwiththegoalofadvancingpreviousstudies(Mageretal.,2021)onindigenousAmericanlanguages.ThetaskistotranslateSpanishinto10indigenouslanguages,includingAshaninka,Aymara,Bribri,Guarani,H\u00f1\u00e4h\u00f1u,Nahuatl,Quechua,Raramuri,Shipibo-Konibo,andWixarika.Additionally,therewasanotherlanguage,Chatino1,forwhichwedidnotparticipatein.WestartedwiththemonolingualandbilingualdatafromMageretal.(2021)andfinetunedNLLB-600M,amultilingualpre-trainedMTmodelfromMeta\u2019sNoLanguageLeftBehind(NLLB)project(NLLBTeametal.,2022)bothbilinguallyandmul-tilingually.Ontopofthat,weemployedweightaveragingandbacktranslation.Forbacktransla-tion,weadditionallyfilteredthebacktranslatedsentencepairstoimprovethedataquality.Wedemonstratethattrainingonmodelweightsaveragedfrommultiplecheckpointsimprovestrans-lationquality,asindicatedbya0.0169increaseintheChrF++scoreonaverage,withoutrequiringad-ditionalcomputationresources.Additionally,wefoundthatbacktranslationcanenhancetranslation\n173 Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP), pages 173\u2013176 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\nKKXk=1\u0398t\u2212k)(2)where\u0398trepresentsthemodelparametersatepocht.Thistechniquesharessimilaritieswithtrainingdifferentmodelsusingvarioushyperparameters(Wortsmanetal.,2022;Xuetal.,2020).How-ever,asweonlyneedtotrainasinglemodel,thistechniquecanbeparticularlyefficientforlargelan-guagemodels.TheeffectivenessofthisapproachisfurtherdiscussedinSection3.2.3.3HyperparametersInthefine-tuningprocess,wefrozetheencoderlayersoftheNLLBmodel,consideringitspriortrainingonavastamountofSpanishsentences.WeoptimizedthemodelusingAdamW(LoshchilovandHutter,2017)withhyperparameters\u03b2=(0.9,0.999),\u03f5=10\u22126.Weemployedalearningrateof3\u00d710\u22124foratotalof10,000iterations.Forregularization,weutilizedthesamedropoutrateastheoriginalNLLBmodelandaweightdecayof0.01.Furthermore,forweightaveraging,wesetthevalueofKtobe5.2.4EvaluationWereporttheresultsusingChrF++(Popovi\u00b4c,2017),followingtheevaluationscript3providedbytheAmericasNLP2023sharedtask.ChrF++\nLangFilteredMonolingDev\n3https://github.com/AmericasNLP/americasnlp2023\nTable1:Numberofsegmentsindataset.FiltereddataandmonolingualdataarecollectedandfilteredbyUni-versityofHelsinkiteam(V\u00e1zquezetal.,2021)fromAmericasNLP2021.theFlores-200dataset,whichconsistsofAymara,Guarani,Quechua,andSpanish.2.3Fine-tunedModelsWefine-tuneNLLB-600Musingthedatamen-tionedinTable1.ForbothX-to-SpanishandSpanish-to-Xdirections,wefine-tuneNLLB-600Musingfilteredparalleldatainbothbilingualandmultilingualway.Thisproduces20bilingualmod-elsand2multilingualmodels.WeleveragetheaboveX-to-Spanishmodelstogeneratebacktranslateddatatoenrichthetrainingcorpus.Thenwefurtherfine-tunetheSpanish-to-Xmodelswithparalleldatasetextendedwithbacktranslatedsentencepairs.Thefinalmodelsareobtainedwithweightaver-agingsincethetrainingcanbeunstablewithinsuf-ficientdata.2.3.1BackTranslationInordertomakeuseofmonolingualdatainin-digenouslanguages,weemployedbacktransla-tion.Specifically,wefrozethedecoderlayersofNLLBmodelandperformedfine-tuningofanX-to-Spanishmodelusingparalleldata.Then,weutilizedthismodeltogeneratesyntheticsentences.Datafiltering:Syntheticsentencesmaycon-tainnoise.Toaddressthisissue,weimplementadatafiltertoselectasubsetofsyntheticsen-tencesthatwillexpandtheoriginalparalleldataset(Ranathungaetal.,2023).Inourtask,weinitiallyfine-tunedaSpanish-to-Xmodelusingtheparalleldata.Subsequently,weevaluatedthismodelonthesyntheticsentencesandselectedthetopNsampleswiththelowestcross-entropyloss.ThevalueofNisdeterminedbythefollowing:N=min(|Ypar|,|Ysyn|)(1)where|Ypar|representsthenumberofsegmentsintheparalleldataset,and|Ysyn|representsthenumberofsegmentsinthesyntheticdataset.Finally,wecombinedtheselectedsyntheticdatawiththeparalleldataandproceededtoperformadditionalfine-tuningoftheNLLBmodel.2.3.2WeightAveragingStudieshaveshownthataveragingtheweightsofmultiplefinetunedmodelscanenhanceaccuracy(Wortsmanetal.,2022).Inourtrainingapproach,theweightsofthenextepocharetrainedbasedontheaverageofthemodelweightsfromthepreviousKepochs.Forinference,wecomputethefinalmodelbyaveragingthemodelweightsfromthelastKepochs.Themodelcanbedefinedasfollows:NLLB(x;\u0398t)=NLLB(x;1\nAshaninka385813195883Aymara835216750996Bribri73030996Guarani1448340516995H\u00f1\u00e4h\u00f1u7049537599Nahuatl174319222672Quechua22862460399996Raramuri165290995Shipibo-Konibo2885423595996Wixarika11525511994\n174\n\n\nTable2:ResultinChrF++ondevelopdataset,exceptforbaselineandBi++(test).BaselinemodelisthebestsubmissionforAmericasNLP2021.Theeffectivenessofweightaveraging(Multi+andBi+)andbacktranslationiscompared(Multi++andBi++).Wealsocomparedtheperformanceofbilingual(Bi)andmultilingual(Multi).capturesthecharacter-levelperformance,makingitparticularlysuitableforevaluatingthepolysyn-theticpropertiesobservedinmanyindigenouslan-guages(Zhengetal.,2021).3ResultsTheresultsarepresentedinTable2forboththedevelopmentandtestdatasets.OurBi++modeldemonstratesimprovementsinfourlanguages:H\u00f1\u00e4h\u00f1u,Aymara,Ash\u00e1ninka,andQuechua,com-paredtotheBaselinemodelprovidedbytheorga-nizer.Ingeneral,thetrendsinresultsforthede-velopmentandtrainingdatasetsaresimilar,exceptforRar\u00e1muriandBribri.Thisdiscrepancymaybeattributedtothetestdatasetcontainingmoreunknowntokens,towhichourmodelissensitive.Previousstudy(Mageretal.,2021)haspri-marilyfocusedonfine-tuningbilingualmachinetranslationmodels.However,theresultsfromourMulti++andBi++modelsdemonstratethepromis-ingpotentialofmultilingualfine-tuning(Tangetal.,2020).Onaverage,theChrF++scoreforMulti++isonly0.0012lowerthanthatofBi++.Wealsocomparedtheeffectivenessofweightaveragingandbacktranslation.Weightaveragingimprovedtranslationsforalltargetlanguages.Onaverage,Multi+achievedaChrF++scorethatwas0.0169higherthanMulti.Theseresultsindicatethatoursimpletechniquecanenhancelow-resourcemachinetranslationwithoutrequiringadditionalcomputationalresources.However,theimpactofbacktranslationvariedacrosslanguages,asobservedintheresultsforMulti+andMulti++.Onaverage,theimplemen-tationofbacktranslationresultedina0.008im-provementintheChrF++metric.ForWixarikaandAymara,therewasaslightdropintheChrF++scoresafterbacktranslation.Despiteperformingdatafiltering,thequalityofsyntheticdatalargelydependsontheperformanceoftheX-to-Spanishmodel.Insummary,ourfine-tuningtechniquehasshownimprovementsinperformance.However,withfurtherrefinementsanddesignenhancements,thereispotentialforourmodeltoachievehigherlevelsofperformance.4ConclusionInthispaper,wepresentedoursubmissiontotheAmericasNLP2023sharedtask.Oursystemuti-lizedtheNLLB-600Mpre-trainedmodeltotrans-lateSpanishinto10indigenouslanguages.Wealsoinvestigatedthepotentialofmultilingualtranslationmodels,whichshowedpromisingresults.Addition-ally,wefoundthataveragingmodelweightsfrompreviousepochsprovedtobeanefficientandeffec-tiveapproach.Whilebacktranslationdemonstratedperformanceimprovements,furthermethodsarenecessarytoaddressnoisydata.Thesefindingshighlightthepositiveoutcomesofourstudyandprovidevaluableinsightsforfutureadvancementsinlow-resourcemachinetranslationtechniques.ReferencesAbteenEbrahimi,ManuelMager,ArturoOncevay,EnoraRice,CynthiaMonta\u00f1o,JohnOrtega,ShrutiRijhwani,AlexisPalmer,RolandoCoto-Solano,Hi-lariaCruz,andKatharinaKann.2023.FindingsoftheAmericasNLP2023sharedtaskonmachinetrans-lationintoindigenouslanguages.InProceedingsof\nTargetlanguageBaseline(Test)MultiMulti+Multi++BiBi++Bi++(Test)\nWixarika0.3040.2770.2940.2940.2660.2790.288H\u00f1\u00e4h\u00f1u0.1470.1290.1330.1380.1440.1410.148Aymara0.2830.2910.3280.3260.3360.3260.300Shipibo-Konibo0.3290.2240.2380.2530.2610.2830.277Nahuatl0.2660.2410.2520.2750.2820.2830.237Guarani0.3360.3040.3160.3210.3150.3030.331Ash\u00e1ninka0.2580.2220.2380.2720.2690.2860.280Quechua0.3430.3240.341-0.337-0.344Rar\u00e1muri0.1840.1610.175-0.184-0.145Bribri0.1650.2100.237-0.231-0.148\n175\n\n\ntheThirdWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas.Associa-tionforComputationalLinguistics.TakuKudoandJohnRichardson.2018.SentencePiece:Asimpleandlanguageindependentsubwordtok-enizeranddetokenizerforneuraltextprocessing.InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations,pages66\u201371,Brussels,Belgium.AssociationforComputationalLinguistics.IlyaLoshchilovandFrankHutter.2017.Fixingweightdecayregularizationinadam.CoRR,abs/1711.05101.ManuelMager,ArturoOncevay,AbteenEbrahimi,JohnOrtega,AnnetteRios,AngelaFan,XimenaGutierrez-Vasques,LuisChiruzzo,GustavoGim\u00e9nez-Lugo,Ri-cardoRamos,IvanVladimirMezaRuiz,RolandoCoto-Solano,AlexisPalmer,ElisabethMager-Hois,VishravChaudhary,GrahamNeubig,NgocThangVu,andKatharinaKann.2021.FindingsoftheAmeri-casNLP2021sharedtaskonopenmachinetransla-tionforindigenouslanguagesoftheAmericas.InProceedingsoftheFirstWorkshoponNaturalLan-guageProcessingforIndigenousLanguagesoftheAmericas,pages202\u2013217,Online.AssociationforComputationalLinguistics.NLLBTeam,MartaR.Costa-juss\u00e0,JamesCross,Onur\u00c7elebi,MahaElbayad,KennethHeafield,KevinHef-fernan,ElaheKalbassi,JaniceLam,DanielLicht,JeanMaillard,AnnaSun,SkylerWang,GuillaumeWenzek,AlYoungblood,BapiAkula,LoicBar-rault,GabrielMejiaGonzalez,PrangthipHansanti,JohnHoffman,SemarleyJarrett,KaushikRamSadagopan,DirkRowe,ShannonSpruit,ChauTran,PierreAndrews,NecipFazilAyan,ShrutiBhosale,SergeyEdunov,AngelaFan,CynthiaGao,VedanujGoswami,FranciscoGuzm\u00e1n,PhilippKoehn,AlexandreMourachko,ChristopheRopers,SafiyyahSaleem,HolgerSchwenk,andJeffWang.2022.Nolanguageleftbehind:Scalinghuman-centeredmachinetranslation.MajaPopovi\u00b4c.2017.chrF++:wordshelpingcharac-tern-grams.InProceedingsoftheSecondConfer-enceonMachineTranslation,pages612\u2013618,Copen-hagen,Denmark.AssociationforComputationalLin-guistics.SurangikaRanathunga,En-ShiunAnnieLee,MarjanaPriftiSkenduli,RaviShekhar,MehreenAlam,andRishemjitKaur.2023.Neuralmachinetranslationforlow-resourcelanguages:Asurvey.ACMComput.Surv.,55(11).YuqingTang,ChauTran,XianLi,Peng-JenChen,Na-manGoyal,VishravChaudhary,JiataoGu,andAn-gelaFan.2020.Multilingualtranslationwithexten-siblemultilingualpretrainingandfinetuning.CoRR,abs/2008.00401.Ra\u00falV\u00e1zquez,YvesScherrer,SamiVirpioja,andJ\u00f6rgTiedemann.2021.TheHelsinkisubmissiontotheAmericasNLPsharedtask.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages255\u2013264,Online.AssociationforComputationalLinguis-tics.MitchellWortsman,GabrielIlharco,SamirYaGadre,RebeccaRoelofs,RaphaelGontijo-Lopes,AriSMor-cos,HongseokNamkoong,AliFarhadi,YairCar-mon,SimonKornblith,andLudwigSchmidt.2022.Modelsoups:averagingweightsofmultiplefine-tunedmodelsimprovesaccuracywithoutincreasinginferencetime.InProceedingsofthe39thInterna-tionalConferenceonMachineLearning,volume162ofProceedingsofMachineLearningResearch,pages23965\u201323998.PMLR.YigeXu,XipengQiu,LigaoZhou,andXuan-jingHuang.2020.ImprovingBERTfine-tuningviaself-ensembleandself-distillation.CoRR,abs/2002.10345.FrancisZheng,MachelReid,EdisonMarrese-Taylor,andYutakaMatsuo.2021.Low-resourcemachinetranslationusingcross-linguallanguagemodelpre-training.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLan-guagesoftheAmericas,pages234\u2013240,Online.As-sociationforComputationalLinguistics.\n176"}, {"question": " What model was selected as the final submission?,answer: The bilingual model with weight averaging and back translation was selected.", "ref_chunk": "2https://github.com/KaieChen/ameircasnlp2023\n1https://scholarworks.iu.edu/dspace/handle/2022/21028qualityforlow-resourcelanguages,althoughitissensitivetothequalityofsyntheticdata.Toaddressthis,weintroducedadatafilteringtechniquetoim-provethequalityofsyntheticdata.Withfilteredbacktranslation,oursystemachievedanaverageimprovementof0.008intheChrF++score.Fur-thermore,ourstudyrevealsthatmultilingualfine-tuningachievescomparabletranslationqualitytobilingualfine-tuningforlow-resourcelanguages.Weselectedthebilingualmodelwithweightaveragingandbacktranslationasourfinalsubmis-sion.TheimplementationofthisstudyisavailableinourGitrepository2.2Methods2.1DataWeadoptedthedatapreparationmethoddescribedbytheUniversityofHelsinki\u2019ssubmissiontoAmer-icasNLP2021(V\u00e1zquezetal.,2021)foroursys-tem.ThedetailsofthedatasetcanbefoundinTable1.Ourmodeltrainingutilizedthefilteredparalleldata(referredtoasparalleldata),whichconsistedofthetrainingdataprovidedbytheorga-nizersaswellasadditionaldatacollectedbytheUniversityofHelsinki(V\u00e1zquezetal.,2021).Inordertogeneratesyntheticparalleldata(referredtoassyntheticdata),weemployedmonolingualdataandappliedbacktranslationtechniques(refertoSection2.3).Thedevelopmentdatawasusedformodelselectionpurposes.2.2Pre-trainedModelOurmodelsarebasedontheNLLB-600MSeq2Seqpre-trainingschemeintroducedbytheNLLBteam(NLLBTeametal.,2022).Fortokenization,weutilizetheSentencePiecetokenizer(KudoandRichardson,2018),followingtheNLLBconfig-uration.TheNLLBmodelwasinitiallytrainedon\nPlayGroundLowResourceMachineTranslationSystemforthe2023AmericasNLPSharedTaskTianruiGu,KaieChen,SiqiOuyang,LeiLiUniversityofCalifornia,SantaBarbara{tianruigu,kaiechen,siqiouyang,leili}@ucsb.eduAbstractThispaperpresentsPlayGround\u2019ssubmissiontotheAmericasNLP2023sharedtaskonma-chinetranslation(MT)intoindigenouslan-guages.WefinetunedNLLB-600M,amultilin-gualMTmodelpre-trainedonFlores-200,on10low-resourcelanguagedirectionsandexam-inedtheeffectivenessofweightaveragingandbacktranslation.Ourexperimentsshowedthatweightaveraging,onaverage,ledtoa0.0169improvementintheChrF++score.Addition-ally,wefoundthatbacktranslationresultedina0.008improvementintheChrF++score.1IntroductionWeparticipatedintheAmericasNLP2023(Ebrahimietal.,2023)sharedtaskwiththegoalofadvancingpreviousstudies(Mageretal.,2021)onindigenousAmericanlanguages.ThetaskistotranslateSpanishinto10indigenouslanguages,includingAshaninka,Aymara,Bribri,Guarani,H\u00f1\u00e4h\u00f1u,Nahuatl,Quechua,Raramuri,Shipibo-Konibo,andWixarika.Additionally,therewasanotherlanguage,Chatino1,forwhichwedidnotparticipatein.WestartedwiththemonolingualandbilingualdatafromMageretal.(2021)andfinetunedNLLB-600M,amultilingualpre-trainedMTmodelfromMeta\u2019sNoLanguageLeftBehind(NLLB)project(NLLBTeametal.,2022)bothbilinguallyandmul-tilingually.Ontopofthat,weemployedweightaveragingandbacktranslation.Forbacktransla-tion,weadditionallyfilteredthebacktranslatedsentencepairstoimprovethedataquality.Wedemonstratethattrainingonmodelweightsaveragedfrommultiplecheckpointsimprovestrans-lationquality,asindicatedbya0.0169increaseintheChrF++scoreonaverage,withoutrequiringad-ditionalcomputationresources.Additionally,wefoundthatbacktranslationcanenhancetranslation\n173 Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP), pages 173\u2013176 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\nKKXk=1\u0398t\u2212k)(2)where\u0398trepresentsthemodelparametersatepocht.Thistechniquesharessimilaritieswithtrainingdifferentmodelsusingvarioushyperparameters(Wortsmanetal.,2022;Xuetal.,2020).How-ever,asweonlyneedtotrainasinglemodel,thistechniquecanbeparticularlyefficientforlargelan-guagemodels.TheeffectivenessofthisapproachisfurtherdiscussedinSection3.2.3.3HyperparametersInthefine-tuningprocess,wefrozetheencoderlayersoftheNLLBmodel,consideringitspriortrainingonavastamountofSpanishsentences.WeoptimizedthemodelusingAdamW(LoshchilovandHutter,2017)withhyperparameters\u03b2=(0.9,0.999),\u03f5=10\u22126.Weemployedalearningrateof3\u00d710\u22124foratotalof10,000iterations.Forregularization,weutilizedthesamedropoutrateastheoriginalNLLBmodelandaweightdecayof0.01.Furthermore,forweightaveraging,wesetthevalueofKtobe5.2.4EvaluationWereporttheresultsusingChrF++(Popovi\u00b4c,2017),followingtheevaluationscript3providedbytheAmericasNLP2023sharedtask.ChrF++\nLangFilteredMonolingDev\n3https://github.com/AmericasNLP/americasnlp2023\nTable1:Numberofsegmentsindataset.FiltereddataandmonolingualdataarecollectedandfilteredbyUni-versityofHelsinkiteam(V\u00e1zquezetal.,2021)fromAmericasNLP2021.theFlores-200dataset,whichconsistsofAymara,Guarani,Quechua,andSpanish.2.3Fine-tunedModelsWefine-tuneNLLB-600Musingthedatamen-tionedinTable1.ForbothX-to-SpanishandSpanish-to-Xdirections,wefine-tuneNLLB-600Musingfilteredparalleldatainbothbilingualandmultilingualway.Thisproduces20bilingualmod-elsand2multilingualmodels.WeleveragetheaboveX-to-Spanishmodelstogeneratebacktranslateddatatoenrichthetrainingcorpus.Thenwefurtherfine-tunetheSpanish-to-Xmodelswithparalleldatasetextendedwithbacktranslatedsentencepairs.Thefinalmodelsareobtainedwithweightaver-agingsincethetrainingcanbeunstablewithinsuf-ficientdata.2.3.1BackTranslationInordertomakeuseofmonolingualdatainin-digenouslanguages,weemployedbacktransla-tion.Specifically,wefrozethedecoderlayersofNLLBmodelandperformedfine-tuningofanX-to-Spanishmodelusingparalleldata.Then,weutilizedthismodeltogeneratesyntheticsentences.Datafiltering:Syntheticsentencesmaycon-tainnoise.Toaddressthisissue,weimplementadatafiltertoselectasubsetofsyntheticsen-tencesthatwillexpandtheoriginalparalleldataset(Ranathungaetal.,2023).Inourtask,weinitiallyfine-tunedaSpanish-to-Xmodelusingtheparalleldata.Subsequently,weevaluatedthismodelonthesyntheticsentencesandselectedthetopNsampleswiththelowestcross-entropyloss.ThevalueofNisdeterminedbythefollowing:N=min(|Ypar|,|Ysyn|)(1)where|Ypar|representsthenumberofsegmentsintheparalleldataset,and|Ysyn|representsthenumberofsegmentsinthesyntheticdataset.Finally,wecombinedtheselectedsyntheticdatawiththeparalleldataandproceededtoperformadditionalfine-tuningoftheNLLBmodel.2.3.2WeightAveragingStudieshaveshownthataveragingtheweightsofmultiplefinetunedmodelscanenhanceaccuracy(Wortsmanetal.,2022).Inourtrainingapproach,theweightsofthenextepocharetrainedbasedontheaverageofthemodelweightsfromthepreviousKepochs.Forinference,wecomputethefinalmodelbyaveragingthemodelweightsfromthelastKepochs.Themodelcanbedefinedasfollows:NLLB(x;\u0398t)=NLLB(x;1\nAshaninka385813195883Aymara835216750996Bribri73030996Guarani1448340516995H\u00f1\u00e4h\u00f1u7049537599Nahuatl174319222672Quechua22862460399996Raramuri165290995Shipibo-Konibo2885423595996Wixarika11525511994\n174\n\n\nTable2:ResultinChrF++ondevelopdataset,exceptforbaselineandBi++(test).BaselinemodelisthebestsubmissionforAmericasNLP2021.Theeffectivenessofweightaveraging(Multi+andBi+)andbacktranslationiscompared(Multi++andBi++).Wealsocomparedtheperformanceofbilingual(Bi)andmultilingual(Multi).capturesthecharacter-levelperformance,makingitparticularlysuitableforevaluatingthepolysyn-theticpropertiesobservedinmanyindigenouslan-guages(Zhengetal.,2021).3ResultsTheresultsarepresentedinTable2forboththedevelopmentandtestdatasets.OurBi++modeldemonstratesimprovementsinfourlanguages:H\u00f1\u00e4h\u00f1u,Aymara,Ash\u00e1ninka,andQuechua,com-paredtotheBaselinemodelprovidedbytheorga-nizer.Ingeneral,thetrendsinresultsforthede-velopmentandtrainingdatasetsaresimilar,exceptforRar\u00e1muriandBribri.Thisdiscrepancymaybeattributedtothetestdatasetcontainingmoreunknowntokens,towhichourmodelissensitive.Previousstudy(Mageretal.,2021)haspri-marilyfocusedonfine-tuningbilingualmachinetranslationmodels.However,theresultsfromourMulti++andBi++modelsdemonstratethepromis-ingpotentialofmultilingualfine-tuning(Tangetal.,2020).Onaverage,theChrF++scoreforMulti++isonly0.0012lowerthanthatofBi++.Wealsocomparedtheeffectivenessofweightaveragingandbacktranslation.Weightaveragingimprovedtranslationsforalltargetlanguages.Onaverage,Multi+achievedaChrF++scorethatwas0.0169higherthanMulti.Theseresultsindicatethatoursimpletechniquecanenhancelow-resourcemachinetranslationwithoutrequiringadditionalcomputationalresources.However,theimpactofbacktranslationvariedacrosslanguages,asobservedintheresultsforMulti+andMulti++.Onaverage,theimplemen-tationofbacktranslationresultedina0.008im-provementintheChrF++metric.ForWixarikaandAymara,therewasaslightdropintheChrF++scoresafterbacktranslation.Despiteperformingdatafiltering,thequalityofsyntheticdatalargelydependsontheperformanceoftheX-to-Spanishmodel.Insummary,ourfine-tuningtechniquehasshownimprovementsinperformance.However,withfurtherrefinementsanddesignenhancements,thereispotentialforourmodeltoachievehigherlevelsofperformance.4ConclusionInthispaper,wepresentedoursubmissiontotheAmericasNLP2023sharedtask.Oursystemuti-lizedtheNLLB-600Mpre-trainedmodeltotrans-lateSpanishinto10indigenouslanguages.Wealsoinvestigatedthepotentialofmultilingualtranslationmodels,whichshowedpromisingresults.Addition-ally,wefoundthataveragingmodelweightsfrompreviousepochsprovedtobeanefficientandeffec-tiveapproach.Whilebacktranslationdemonstratedperformanceimprovements,furthermethodsarenecessarytoaddressnoisydata.Thesefindingshighlightthepositiveoutcomesofourstudyandprovidevaluableinsightsforfutureadvancementsinlow-resourcemachinetranslationtechniques.ReferencesAbteenEbrahimi,ManuelMager,ArturoOncevay,EnoraRice,CynthiaMonta\u00f1o,JohnOrtega,ShrutiRijhwani,AlexisPalmer,RolandoCoto-Solano,Hi-lariaCruz,andKatharinaKann.2023.FindingsoftheAmericasNLP2023sharedtaskonmachinetrans-lationintoindigenouslanguages.InProceedingsof\nTargetlanguageBaseline(Test)MultiMulti+Multi++BiBi++Bi++(Test)\nWixarika0.3040.2770.2940.2940.2660.2790.288H\u00f1\u00e4h\u00f1u0.1470.1290.1330.1380.1440.1410.148Aymara0.2830.2910.3280.3260.3360.3260.300Shipibo-Konibo0.3290.2240.2380.2530.2610.2830.277Nahuatl0.2660.2410.2520.2750.2820.2830.237Guarani0.3360.3040.3160.3210.3150.3030.331Ash\u00e1ninka0.2580.2220.2380.2720.2690.2860.280Quechua0.3430.3240.341-0.337-0.344Rar\u00e1muri0.1840.1610.175-0.184-0.145Bribri0.1650.2100.237-0.231-0.148\n175\n\n\ntheThirdWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas.Associa-tionforComputationalLinguistics.TakuKudoandJohnRichardson.2018.SentencePiece:Asimpleandlanguageindependentsubwordtok-enizeranddetokenizerforneuraltextprocessing.InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations,pages66\u201371,Brussels,Belgium.AssociationforComputationalLinguistics.IlyaLoshchilovandFrankHutter.2017.Fixingweightdecayregularizationinadam.CoRR,abs/1711.05101.ManuelMager,ArturoOncevay,AbteenEbrahimi,JohnOrtega,AnnetteRios,AngelaFan,XimenaGutierrez-Vasques,LuisChiruzzo,GustavoGim\u00e9nez-Lugo,Ri-cardoRamos,IvanVladimirMezaRuiz,RolandoCoto-Solano,AlexisPalmer,ElisabethMager-Hois,VishravChaudhary,GrahamNeubig,NgocThangVu,andKatharinaKann.2021.FindingsoftheAmeri-casNLP2021sharedtaskonopenmachinetransla-tionforindigenouslanguagesoftheAmericas.InProceedingsoftheFirstWorkshoponNaturalLan-guageProcessingforIndigenousLanguagesoftheAmericas,pages202\u2013217,Online.AssociationforComputationalLinguistics.NLLBTeam,MartaR.Costa-juss\u00e0,JamesCross,Onur\u00c7elebi,MahaElbayad,KennethHeafield,KevinHef-fernan,ElaheKalbassi,JaniceLam,DanielLicht,JeanMaillard,AnnaSun,SkylerWang,GuillaumeWenzek,AlYoungblood,BapiAkula,LoicBar-rault,GabrielMejiaGonzalez,PrangthipHansanti,JohnHoffman,SemarleyJarrett,KaushikRamSadagopan,DirkRowe,ShannonSpruit,ChauTran,PierreAndrews,NecipFazilAyan,ShrutiBhosale,SergeyEdunov,AngelaFan,CynthiaGao,VedanujGoswami,FranciscoGuzm\u00e1n,PhilippKoehn,AlexandreMourachko,ChristopheRopers,SafiyyahSaleem,HolgerSchwenk,andJeffWang.2022.Nolanguageleftbehind:Scalinghuman-centeredmachinetranslation.MajaPopovi\u00b4c.2017.chrF++:wordshelpingcharac-tern-grams.InProceedingsoftheSecondConfer-enceonMachineTranslation,pages612\u2013618,Copen-hagen,Denmark.AssociationforComputationalLin-guistics.SurangikaRanathunga,En-ShiunAnnieLee,MarjanaPriftiSkenduli,RaviShekhar,MehreenAlam,andRishemjitKaur.2023.Neuralmachinetranslationforlow-resourcelanguages:Asurvey.ACMComput.Surv.,55(11).YuqingTang,ChauTran,XianLi,Peng-JenChen,Na-manGoyal,VishravChaudhary,JiataoGu,andAn-gelaFan.2020.Multilingualtranslationwithexten-siblemultilingualpretrainingandfinetuning.CoRR,abs/2008.00401.Ra\u00falV\u00e1zquez,YvesScherrer,SamiVirpioja,andJ\u00f6rgTiedemann.2021.TheHelsinkisubmissiontotheAmericasNLPsharedtask.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages255\u2013264,Online.AssociationforComputationalLinguis-tics.MitchellWortsman,GabrielIlharco,SamirYaGadre,RebeccaRoelofs,RaphaelGontijo-Lopes,AriSMor-cos,HongseokNamkoong,AliFarhadi,YairCar-mon,SimonKornblith,andLudwigSchmidt.2022.Modelsoups:averagingweightsofmultiplefine-tunedmodelsimprovesaccuracywithoutincreasinginferencetime.InProceedingsofthe39thInterna-tionalConferenceonMachineLearning,volume162ofProceedingsofMachineLearningResearch,pages23965\u201323998.PMLR.YigeXu,XipengQiu,LigaoZhou,andXuan-jingHuang.2020.ImprovingBERTfine-tuningviaself-ensembleandself-distillation.CoRR,abs/2002.10345.FrancisZheng,MachelReid,EdisonMarrese-Taylor,andYutakaMatsuo.2021.Low-resourcemachinetranslationusingcross-linguallanguagemodelpre-training.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLan-guagesoftheAmericas,pages234\u2013240,Online.As-sociationforComputationalLinguistics.\n176"}, {"question": " What tokenization technique was utilized for the models?,answer: The SentencePiece tokenizer was utilized.", "ref_chunk": "2https://github.com/KaieChen/ameircasnlp2023\n1https://scholarworks.iu.edu/dspace/handle/2022/21028qualityforlow-resourcelanguages,althoughitissensitivetothequalityofsyntheticdata.Toaddressthis,weintroducedadatafilteringtechniquetoim-provethequalityofsyntheticdata.Withfilteredbacktranslation,oursystemachievedanaverageimprovementof0.008intheChrF++score.Fur-thermore,ourstudyrevealsthatmultilingualfine-tuningachievescomparabletranslationqualitytobilingualfine-tuningforlow-resourcelanguages.Weselectedthebilingualmodelwithweightaveragingandbacktranslationasourfinalsubmis-sion.TheimplementationofthisstudyisavailableinourGitrepository2.2Methods2.1DataWeadoptedthedatapreparationmethoddescribedbytheUniversityofHelsinki\u2019ssubmissiontoAmer-icasNLP2021(V\u00e1zquezetal.,2021)foroursys-tem.ThedetailsofthedatasetcanbefoundinTable1.Ourmodeltrainingutilizedthefilteredparalleldata(referredtoasparalleldata),whichconsistedofthetrainingdataprovidedbytheorga-nizersaswellasadditionaldatacollectedbytheUniversityofHelsinki(V\u00e1zquezetal.,2021).Inordertogeneratesyntheticparalleldata(referredtoassyntheticdata),weemployedmonolingualdataandappliedbacktranslationtechniques(refertoSection2.3).Thedevelopmentdatawasusedformodelselectionpurposes.2.2Pre-trainedModelOurmodelsarebasedontheNLLB-600MSeq2Seqpre-trainingschemeintroducedbytheNLLBteam(NLLBTeametal.,2022).Fortokenization,weutilizetheSentencePiecetokenizer(KudoandRichardson,2018),followingtheNLLBconfig-uration.TheNLLBmodelwasinitiallytrainedon\nPlayGroundLowResourceMachineTranslationSystemforthe2023AmericasNLPSharedTaskTianruiGu,KaieChen,SiqiOuyang,LeiLiUniversityofCalifornia,SantaBarbara{tianruigu,kaiechen,siqiouyang,leili}@ucsb.eduAbstractThispaperpresentsPlayGround\u2019ssubmissiontotheAmericasNLP2023sharedtaskonma-chinetranslation(MT)intoindigenouslan-guages.WefinetunedNLLB-600M,amultilin-gualMTmodelpre-trainedonFlores-200,on10low-resourcelanguagedirectionsandexam-inedtheeffectivenessofweightaveragingandbacktranslation.Ourexperimentsshowedthatweightaveraging,onaverage,ledtoa0.0169improvementintheChrF++score.Addition-ally,wefoundthatbacktranslationresultedina0.008improvementintheChrF++score.1IntroductionWeparticipatedintheAmericasNLP2023(Ebrahimietal.,2023)sharedtaskwiththegoalofadvancingpreviousstudies(Mageretal.,2021)onindigenousAmericanlanguages.ThetaskistotranslateSpanishinto10indigenouslanguages,includingAshaninka,Aymara,Bribri,Guarani,H\u00f1\u00e4h\u00f1u,Nahuatl,Quechua,Raramuri,Shipibo-Konibo,andWixarika.Additionally,therewasanotherlanguage,Chatino1,forwhichwedidnotparticipatein.WestartedwiththemonolingualandbilingualdatafromMageretal.(2021)andfinetunedNLLB-600M,amultilingualpre-trainedMTmodelfromMeta\u2019sNoLanguageLeftBehind(NLLB)project(NLLBTeametal.,2022)bothbilinguallyandmul-tilingually.Ontopofthat,weemployedweightaveragingandbacktranslation.Forbacktransla-tion,weadditionallyfilteredthebacktranslatedsentencepairstoimprovethedataquality.Wedemonstratethattrainingonmodelweightsaveragedfrommultiplecheckpointsimprovestrans-lationquality,asindicatedbya0.0169increaseintheChrF++scoreonaverage,withoutrequiringad-ditionalcomputationresources.Additionally,wefoundthatbacktranslationcanenhancetranslation\n173 Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP), pages 173\u2013176 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\nKKXk=1\u0398t\u2212k)(2)where\u0398trepresentsthemodelparametersatepocht.Thistechniquesharessimilaritieswithtrainingdifferentmodelsusingvarioushyperparameters(Wortsmanetal.,2022;Xuetal.,2020).How-ever,asweonlyneedtotrainasinglemodel,thistechniquecanbeparticularlyefficientforlargelan-guagemodels.TheeffectivenessofthisapproachisfurtherdiscussedinSection3.2.3.3HyperparametersInthefine-tuningprocess,wefrozetheencoderlayersoftheNLLBmodel,consideringitspriortrainingonavastamountofSpanishsentences.WeoptimizedthemodelusingAdamW(LoshchilovandHutter,2017)withhyperparameters\u03b2=(0.9,0.999),\u03f5=10\u22126.Weemployedalearningrateof3\u00d710\u22124foratotalof10,000iterations.Forregularization,weutilizedthesamedropoutrateastheoriginalNLLBmodelandaweightdecayof0.01.Furthermore,forweightaveraging,wesetthevalueofKtobe5.2.4EvaluationWereporttheresultsusingChrF++(Popovi\u00b4c,2017),followingtheevaluationscript3providedbytheAmericasNLP2023sharedtask.ChrF++\nLangFilteredMonolingDev\n3https://github.com/AmericasNLP/americasnlp2023\nTable1:Numberofsegmentsindataset.FiltereddataandmonolingualdataarecollectedandfilteredbyUni-versityofHelsinkiteam(V\u00e1zquezetal.,2021)fromAmericasNLP2021.theFlores-200dataset,whichconsistsofAymara,Guarani,Quechua,andSpanish.2.3Fine-tunedModelsWefine-tuneNLLB-600Musingthedatamen-tionedinTable1.ForbothX-to-SpanishandSpanish-to-Xdirections,wefine-tuneNLLB-600Musingfilteredparalleldatainbothbilingualandmultilingualway.Thisproduces20bilingualmod-elsand2multilingualmodels.WeleveragetheaboveX-to-Spanishmodelstogeneratebacktranslateddatatoenrichthetrainingcorpus.Thenwefurtherfine-tunetheSpanish-to-Xmodelswithparalleldatasetextendedwithbacktranslatedsentencepairs.Thefinalmodelsareobtainedwithweightaver-agingsincethetrainingcanbeunstablewithinsuf-ficientdata.2.3.1BackTranslationInordertomakeuseofmonolingualdatainin-digenouslanguages,weemployedbacktransla-tion.Specifically,wefrozethedecoderlayersofNLLBmodelandperformedfine-tuningofanX-to-Spanishmodelusingparalleldata.Then,weutilizedthismodeltogeneratesyntheticsentences.Datafiltering:Syntheticsentencesmaycon-tainnoise.Toaddressthisissue,weimplementadatafiltertoselectasubsetofsyntheticsen-tencesthatwillexpandtheoriginalparalleldataset(Ranathungaetal.,2023).Inourtask,weinitiallyfine-tunedaSpanish-to-Xmodelusingtheparalleldata.Subsequently,weevaluatedthismodelonthesyntheticsentencesandselectedthetopNsampleswiththelowestcross-entropyloss.ThevalueofNisdeterminedbythefollowing:N=min(|Ypar|,|Ysyn|)(1)where|Ypar|representsthenumberofsegmentsintheparalleldataset,and|Ysyn|representsthenumberofsegmentsinthesyntheticdataset.Finally,wecombinedtheselectedsyntheticdatawiththeparalleldataandproceededtoperformadditionalfine-tuningoftheNLLBmodel.2.3.2WeightAveragingStudieshaveshownthataveragingtheweightsofmultiplefinetunedmodelscanenhanceaccuracy(Wortsmanetal.,2022).Inourtrainingapproach,theweightsofthenextepocharetrainedbasedontheaverageofthemodelweightsfromthepreviousKepochs.Forinference,wecomputethefinalmodelbyaveragingthemodelweightsfromthelastKepochs.Themodelcanbedefinedasfollows:NLLB(x;\u0398t)=NLLB(x;1\nAshaninka385813195883Aymara835216750996Bribri73030996Guarani1448340516995H\u00f1\u00e4h\u00f1u7049537599Nahuatl174319222672Quechua22862460399996Raramuri165290995Shipibo-Konibo2885423595996Wixarika11525511994\n174\n\n\nTable2:ResultinChrF++ondevelopdataset,exceptforbaselineandBi++(test).BaselinemodelisthebestsubmissionforAmericasNLP2021.Theeffectivenessofweightaveraging(Multi+andBi+)andbacktranslationiscompared(Multi++andBi++).Wealsocomparedtheperformanceofbilingual(Bi)andmultilingual(Multi).capturesthecharacter-levelperformance,makingitparticularlysuitableforevaluatingthepolysyn-theticpropertiesobservedinmanyindigenouslan-guages(Zhengetal.,2021).3ResultsTheresultsarepresentedinTable2forboththedevelopmentandtestdatasets.OurBi++modeldemonstratesimprovementsinfourlanguages:H\u00f1\u00e4h\u00f1u,Aymara,Ash\u00e1ninka,andQuechua,com-paredtotheBaselinemodelprovidedbytheorga-nizer.Ingeneral,thetrendsinresultsforthede-velopmentandtrainingdatasetsaresimilar,exceptforRar\u00e1muriandBribri.Thisdiscrepancymaybeattributedtothetestdatasetcontainingmoreunknowntokens,towhichourmodelissensitive.Previousstudy(Mageretal.,2021)haspri-marilyfocusedonfine-tuningbilingualmachinetranslationmodels.However,theresultsfromourMulti++andBi++modelsdemonstratethepromis-ingpotentialofmultilingualfine-tuning(Tangetal.,2020).Onaverage,theChrF++scoreforMulti++isonly0.0012lowerthanthatofBi++.Wealsocomparedtheeffectivenessofweightaveragingandbacktranslation.Weightaveragingimprovedtranslationsforalltargetlanguages.Onaverage,Multi+achievedaChrF++scorethatwas0.0169higherthanMulti.Theseresultsindicatethatoursimpletechniquecanenhancelow-resourcemachinetranslationwithoutrequiringadditionalcomputationalresources.However,theimpactofbacktranslationvariedacrosslanguages,asobservedintheresultsforMulti+andMulti++.Onaverage,theimplemen-tationofbacktranslationresultedina0.008im-provementintheChrF++metric.ForWixarikaandAymara,therewasaslightdropintheChrF++scoresafterbacktranslation.Despiteperformingdatafiltering,thequalityofsyntheticdatalargelydependsontheperformanceoftheX-to-Spanishmodel.Insummary,ourfine-tuningtechniquehasshownimprovementsinperformance.However,withfurtherrefinementsanddesignenhancements,thereispotentialforourmodeltoachievehigherlevelsofperformance.4ConclusionInthispaper,wepresentedoursubmissiontotheAmericasNLP2023sharedtask.Oursystemuti-lizedtheNLLB-600Mpre-trainedmodeltotrans-lateSpanishinto10indigenouslanguages.Wealsoinvestigatedthepotentialofmultilingualtranslationmodels,whichshowedpromisingresults.Addition-ally,wefoundthataveragingmodelweightsfrompreviousepochsprovedtobeanefficientandeffec-tiveapproach.Whilebacktranslationdemonstratedperformanceimprovements,furthermethodsarenecessarytoaddressnoisydata.Thesefindingshighlightthepositiveoutcomesofourstudyandprovidevaluableinsightsforfutureadvancementsinlow-resourcemachinetranslationtechniques.ReferencesAbteenEbrahimi,ManuelMager,ArturoOncevay,EnoraRice,CynthiaMonta\u00f1o,JohnOrtega,ShrutiRijhwani,AlexisPalmer,RolandoCoto-Solano,Hi-lariaCruz,andKatharinaKann.2023.FindingsoftheAmericasNLP2023sharedtaskonmachinetrans-lationintoindigenouslanguages.InProceedingsof\nTargetlanguageBaseline(Test)MultiMulti+Multi++BiBi++Bi++(Test)\nWixarika0.3040.2770.2940.2940.2660.2790.288H\u00f1\u00e4h\u00f1u0.1470.1290.1330.1380.1440.1410.148Aymara0.2830.2910.3280.3260.3360.3260.300Shipibo-Konibo0.3290.2240.2380.2530.2610.2830.277Nahuatl0.2660.2410.2520.2750.2820.2830.237Guarani0.3360.3040.3160.3210.3150.3030.331Ash\u00e1ninka0.2580.2220.2380.2720.2690.2860.280Quechua0.3430.3240.341-0.337-0.344Rar\u00e1muri0.1840.1610.175-0.184-0.145Bribri0.1650.2100.237-0.231-0.148\n175\n\n\ntheThirdWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas.Associa-tionforComputationalLinguistics.TakuKudoandJohnRichardson.2018.SentencePiece:Asimpleandlanguageindependentsubwordtok-enizeranddetokenizerforneuraltextprocessing.InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations,pages66\u201371,Brussels,Belgium.AssociationforComputationalLinguistics.IlyaLoshchilovandFrankHutter.2017.Fixingweightdecayregularizationinadam.CoRR,abs/1711.05101.ManuelMager,ArturoOncevay,AbteenEbrahimi,JohnOrtega,AnnetteRios,AngelaFan,XimenaGutierrez-Vasques,LuisChiruzzo,GustavoGim\u00e9nez-Lugo,Ri-cardoRamos,IvanVladimirMezaRuiz,RolandoCoto-Solano,AlexisPalmer,ElisabethMager-Hois,VishravChaudhary,GrahamNeubig,NgocThangVu,andKatharinaKann.2021.FindingsoftheAmeri-casNLP2021sharedtaskonopenmachinetransla-tionforindigenouslanguagesoftheAmericas.InProceedingsoftheFirstWorkshoponNaturalLan-guageProcessingforIndigenousLanguagesoftheAmericas,pages202\u2013217,Online.AssociationforComputationalLinguistics.NLLBTeam,MartaR.Costa-juss\u00e0,JamesCross,Onur\u00c7elebi,MahaElbayad,KennethHeafield,KevinHef-fernan,ElaheKalbassi,JaniceLam,DanielLicht,JeanMaillard,AnnaSun,SkylerWang,GuillaumeWenzek,AlYoungblood,BapiAkula,LoicBar-rault,GabrielMejiaGonzalez,PrangthipHansanti,JohnHoffman,SemarleyJarrett,KaushikRamSadagopan,DirkRowe,ShannonSpruit,ChauTran,PierreAndrews,NecipFazilAyan,ShrutiBhosale,SergeyEdunov,AngelaFan,CynthiaGao,VedanujGoswami,FranciscoGuzm\u00e1n,PhilippKoehn,AlexandreMourachko,ChristopheRopers,SafiyyahSaleem,HolgerSchwenk,andJeffWang.2022.Nolanguageleftbehind:Scalinghuman-centeredmachinetranslation.MajaPopovi\u00b4c.2017.chrF++:wordshelpingcharac-tern-grams.InProceedingsoftheSecondConfer-enceonMachineTranslation,pages612\u2013618,Copen-hagen,Denmark.AssociationforComputationalLin-guistics.SurangikaRanathunga,En-ShiunAnnieLee,MarjanaPriftiSkenduli,RaviShekhar,MehreenAlam,andRishemjitKaur.2023.Neuralmachinetranslationforlow-resourcelanguages:Asurvey.ACMComput.Surv.,55(11).YuqingTang,ChauTran,XianLi,Peng-JenChen,Na-manGoyal,VishravChaudhary,JiataoGu,andAn-gelaFan.2020.Multilingualtranslationwithexten-siblemultilingualpretrainingandfinetuning.CoRR,abs/2008.00401.Ra\u00falV\u00e1zquez,YvesScherrer,SamiVirpioja,andJ\u00f6rgTiedemann.2021.TheHelsinkisubmissiontotheAmericasNLPsharedtask.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages255\u2013264,Online.AssociationforComputationalLinguis-tics.MitchellWortsman,GabrielIlharco,SamirYaGadre,RebeccaRoelofs,RaphaelGontijo-Lopes,AriSMor-cos,HongseokNamkoong,AliFarhadi,YairCar-mon,SimonKornblith,andLudwigSchmidt.2022.Modelsoups:averagingweightsofmultiplefine-tunedmodelsimprovesaccuracywithoutincreasinginferencetime.InProceedingsofthe39thInterna-tionalConferenceonMachineLearning,volume162ofProceedingsofMachineLearningResearch,pages23965\u201323998.PMLR.YigeXu,XipengQiu,LigaoZhou,andXuan-jingHuang.2020.ImprovingBERTfine-tuningviaself-ensembleandself-distillation.CoRR,abs/2002.10345.FrancisZheng,MachelReid,EdisonMarrese-Taylor,andYutakaMatsuo.2021.Low-resourcemachinetranslationusingcross-linguallanguagemodelpre-training.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLan-guagesoftheAmericas,pages234\u2013240,Online.As-sociationforComputationalLinguistics.\n176"}, {"question": " What dataset was adopted for model training?,answer: The University of Helsinki\u2019s submission to AmericasNLP 2021 dataset was adopted.", "ref_chunk": "2https://github.com/KaieChen/ameircasnlp2023\n1https://scholarworks.iu.edu/dspace/handle/2022/21028qualityforlow-resourcelanguages,althoughitissensitivetothequalityofsyntheticdata.Toaddressthis,weintroducedadatafilteringtechniquetoim-provethequalityofsyntheticdata.Withfilteredbacktranslation,oursystemachievedanaverageimprovementof0.008intheChrF++score.Fur-thermore,ourstudyrevealsthatmultilingualfine-tuningachievescomparabletranslationqualitytobilingualfine-tuningforlow-resourcelanguages.Weselectedthebilingualmodelwithweightaveragingandbacktranslationasourfinalsubmis-sion.TheimplementationofthisstudyisavailableinourGitrepository2.2Methods2.1DataWeadoptedthedatapreparationmethoddescribedbytheUniversityofHelsinki\u2019ssubmissiontoAmer-icasNLP2021(V\u00e1zquezetal.,2021)foroursys-tem.ThedetailsofthedatasetcanbefoundinTable1.Ourmodeltrainingutilizedthefilteredparalleldata(referredtoasparalleldata),whichconsistedofthetrainingdataprovidedbytheorga-nizersaswellasadditionaldatacollectedbytheUniversityofHelsinki(V\u00e1zquezetal.,2021).Inordertogeneratesyntheticparalleldata(referredtoassyntheticdata),weemployedmonolingualdataandappliedbacktranslationtechniques(refertoSection2.3).Thedevelopmentdatawasusedformodelselectionpurposes.2.2Pre-trainedModelOurmodelsarebasedontheNLLB-600MSeq2Seqpre-trainingschemeintroducedbytheNLLBteam(NLLBTeametal.,2022).Fortokenization,weutilizetheSentencePiecetokenizer(KudoandRichardson,2018),followingtheNLLBconfig-uration.TheNLLBmodelwasinitiallytrainedon\nPlayGroundLowResourceMachineTranslationSystemforthe2023AmericasNLPSharedTaskTianruiGu,KaieChen,SiqiOuyang,LeiLiUniversityofCalifornia,SantaBarbara{tianruigu,kaiechen,siqiouyang,leili}@ucsb.eduAbstractThispaperpresentsPlayGround\u2019ssubmissiontotheAmericasNLP2023sharedtaskonma-chinetranslation(MT)intoindigenouslan-guages.WefinetunedNLLB-600M,amultilin-gualMTmodelpre-trainedonFlores-200,on10low-resourcelanguagedirectionsandexam-inedtheeffectivenessofweightaveragingandbacktranslation.Ourexperimentsshowedthatweightaveraging,onaverage,ledtoa0.0169improvementintheChrF++score.Addition-ally,wefoundthatbacktranslationresultedina0.008improvementintheChrF++score.1IntroductionWeparticipatedintheAmericasNLP2023(Ebrahimietal.,2023)sharedtaskwiththegoalofadvancingpreviousstudies(Mageretal.,2021)onindigenousAmericanlanguages.ThetaskistotranslateSpanishinto10indigenouslanguages,includingAshaninka,Aymara,Bribri,Guarani,H\u00f1\u00e4h\u00f1u,Nahuatl,Quechua,Raramuri,Shipibo-Konibo,andWixarika.Additionally,therewasanotherlanguage,Chatino1,forwhichwedidnotparticipatein.WestartedwiththemonolingualandbilingualdatafromMageretal.(2021)andfinetunedNLLB-600M,amultilingualpre-trainedMTmodelfromMeta\u2019sNoLanguageLeftBehind(NLLB)project(NLLBTeametal.,2022)bothbilinguallyandmul-tilingually.Ontopofthat,weemployedweightaveragingandbacktranslation.Forbacktransla-tion,weadditionallyfilteredthebacktranslatedsentencepairstoimprovethedataquality.Wedemonstratethattrainingonmodelweightsaveragedfrommultiplecheckpointsimprovestrans-lationquality,asindicatedbya0.0169increaseintheChrF++scoreonaverage,withoutrequiringad-ditionalcomputationresources.Additionally,wefoundthatbacktranslationcanenhancetranslation\n173 Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP), pages 173\u2013176 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\nKKXk=1\u0398t\u2212k)(2)where\u0398trepresentsthemodelparametersatepocht.Thistechniquesharessimilaritieswithtrainingdifferentmodelsusingvarioushyperparameters(Wortsmanetal.,2022;Xuetal.,2020).How-ever,asweonlyneedtotrainasinglemodel,thistechniquecanbeparticularlyefficientforlargelan-guagemodels.TheeffectivenessofthisapproachisfurtherdiscussedinSection3.2.3.3HyperparametersInthefine-tuningprocess,wefrozetheencoderlayersoftheNLLBmodel,consideringitspriortrainingonavastamountofSpanishsentences.WeoptimizedthemodelusingAdamW(LoshchilovandHutter,2017)withhyperparameters\u03b2=(0.9,0.999),\u03f5=10\u22126.Weemployedalearningrateof3\u00d710\u22124foratotalof10,000iterations.Forregularization,weutilizedthesamedropoutrateastheoriginalNLLBmodelandaweightdecayof0.01.Furthermore,forweightaveraging,wesetthevalueofKtobe5.2.4EvaluationWereporttheresultsusingChrF++(Popovi\u00b4c,2017),followingtheevaluationscript3providedbytheAmericasNLP2023sharedtask.ChrF++\nLangFilteredMonolingDev\n3https://github.com/AmericasNLP/americasnlp2023\nTable1:Numberofsegmentsindataset.FiltereddataandmonolingualdataarecollectedandfilteredbyUni-versityofHelsinkiteam(V\u00e1zquezetal.,2021)fromAmericasNLP2021.theFlores-200dataset,whichconsistsofAymara,Guarani,Quechua,andSpanish.2.3Fine-tunedModelsWefine-tuneNLLB-600Musingthedatamen-tionedinTable1.ForbothX-to-SpanishandSpanish-to-Xdirections,wefine-tuneNLLB-600Musingfilteredparalleldatainbothbilingualandmultilingualway.Thisproduces20bilingualmod-elsand2multilingualmodels.WeleveragetheaboveX-to-Spanishmodelstogeneratebacktranslateddatatoenrichthetrainingcorpus.Thenwefurtherfine-tunetheSpanish-to-Xmodelswithparalleldatasetextendedwithbacktranslatedsentencepairs.Thefinalmodelsareobtainedwithweightaver-agingsincethetrainingcanbeunstablewithinsuf-ficientdata.2.3.1BackTranslationInordertomakeuseofmonolingualdatainin-digenouslanguages,weemployedbacktransla-tion.Specifically,wefrozethedecoderlayersofNLLBmodelandperformedfine-tuningofanX-to-Spanishmodelusingparalleldata.Then,weutilizedthismodeltogeneratesyntheticsentences.Datafiltering:Syntheticsentencesmaycon-tainnoise.Toaddressthisissue,weimplementadatafiltertoselectasubsetofsyntheticsen-tencesthatwillexpandtheoriginalparalleldataset(Ranathungaetal.,2023).Inourtask,weinitiallyfine-tunedaSpanish-to-Xmodelusingtheparalleldata.Subsequently,weevaluatedthismodelonthesyntheticsentencesandselectedthetopNsampleswiththelowestcross-entropyloss.ThevalueofNisdeterminedbythefollowing:N=min(|Ypar|,|Ysyn|)(1)where|Ypar|representsthenumberofsegmentsintheparalleldataset,and|Ysyn|representsthenumberofsegmentsinthesyntheticdataset.Finally,wecombinedtheselectedsyntheticdatawiththeparalleldataandproceededtoperformadditionalfine-tuningoftheNLLBmodel.2.3.2WeightAveragingStudieshaveshownthataveragingtheweightsofmultiplefinetunedmodelscanenhanceaccuracy(Wortsmanetal.,2022).Inourtrainingapproach,theweightsofthenextepocharetrainedbasedontheaverageofthemodelweightsfromthepreviousKepochs.Forinference,wecomputethefinalmodelbyaveragingthemodelweightsfromthelastKepochs.Themodelcanbedefinedasfollows:NLLB(x;\u0398t)=NLLB(x;1\nAshaninka385813195883Aymara835216750996Bribri73030996Guarani1448340516995H\u00f1\u00e4h\u00f1u7049537599Nahuatl174319222672Quechua22862460399996Raramuri165290995Shipibo-Konibo2885423595996Wixarika11525511994\n174\n\n\nTable2:ResultinChrF++ondevelopdataset,exceptforbaselineandBi++(test).BaselinemodelisthebestsubmissionforAmericasNLP2021.Theeffectivenessofweightaveraging(Multi+andBi+)andbacktranslationiscompared(Multi++andBi++).Wealsocomparedtheperformanceofbilingual(Bi)andmultilingual(Multi).capturesthecharacter-levelperformance,makingitparticularlysuitableforevaluatingthepolysyn-theticpropertiesobservedinmanyindigenouslan-guages(Zhengetal.,2021).3ResultsTheresultsarepresentedinTable2forboththedevelopmentandtestdatasets.OurBi++modeldemonstratesimprovementsinfourlanguages:H\u00f1\u00e4h\u00f1u,Aymara,Ash\u00e1ninka,andQuechua,com-paredtotheBaselinemodelprovidedbytheorga-nizer.Ingeneral,thetrendsinresultsforthede-velopmentandtrainingdatasetsaresimilar,exceptforRar\u00e1muriandBribri.Thisdiscrepancymaybeattributedtothetestdatasetcontainingmoreunknowntokens,towhichourmodelissensitive.Previousstudy(Mageretal.,2021)haspri-marilyfocusedonfine-tuningbilingualmachinetranslationmodels.However,theresultsfromourMulti++andBi++modelsdemonstratethepromis-ingpotentialofmultilingualfine-tuning(Tangetal.,2020).Onaverage,theChrF++scoreforMulti++isonly0.0012lowerthanthatofBi++.Wealsocomparedtheeffectivenessofweightaveragingandbacktranslation.Weightaveragingimprovedtranslationsforalltargetlanguages.Onaverage,Multi+achievedaChrF++scorethatwas0.0169higherthanMulti.Theseresultsindicatethatoursimpletechniquecanenhancelow-resourcemachinetranslationwithoutrequiringadditionalcomputationalresources.However,theimpactofbacktranslationvariedacrosslanguages,asobservedintheresultsforMulti+andMulti++.Onaverage,theimplemen-tationofbacktranslationresultedina0.008im-provementintheChrF++metric.ForWixarikaandAymara,therewasaslightdropintheChrF++scoresafterbacktranslation.Despiteperformingdatafiltering,thequalityofsyntheticdatalargelydependsontheperformanceoftheX-to-Spanishmodel.Insummary,ourfine-tuningtechniquehasshownimprovementsinperformance.However,withfurtherrefinementsanddesignenhancements,thereispotentialforourmodeltoachievehigherlevelsofperformance.4ConclusionInthispaper,wepresentedoursubmissiontotheAmericasNLP2023sharedtask.Oursystemuti-lizedtheNLLB-600Mpre-trainedmodeltotrans-lateSpanishinto10indigenouslanguages.Wealsoinvestigatedthepotentialofmultilingualtranslationmodels,whichshowedpromisingresults.Addition-ally,wefoundthataveragingmodelweightsfrompreviousepochsprovedtobeanefficientandeffec-tiveapproach.Whilebacktranslationdemonstratedperformanceimprovements,furthermethodsarenecessarytoaddressnoisydata.Thesefindingshighlightthepositiveoutcomesofourstudyandprovidevaluableinsightsforfutureadvancementsinlow-resourcemachinetranslationtechniques.ReferencesAbteenEbrahimi,ManuelMager,ArturoOncevay,EnoraRice,CynthiaMonta\u00f1o,JohnOrtega,ShrutiRijhwani,AlexisPalmer,RolandoCoto-Solano,Hi-lariaCruz,andKatharinaKann.2023.FindingsoftheAmericasNLP2023sharedtaskonmachinetrans-lationintoindigenouslanguages.InProceedingsof\nTargetlanguageBaseline(Test)MultiMulti+Multi++BiBi++Bi++(Test)\nWixarika0.3040.2770.2940.2940.2660.2790.288H\u00f1\u00e4h\u00f1u0.1470.1290.1330.1380.1440.1410.148Aymara0.2830.2910.3280.3260.3360.3260.300Shipibo-Konibo0.3290.2240.2380.2530.2610.2830.277Nahuatl0.2660.2410.2520.2750.2820.2830.237Guarani0.3360.3040.3160.3210.3150.3030.331Ash\u00e1ninka0.2580.2220.2380.2720.2690.2860.280Quechua0.3430.3240.341-0.337-0.344Rar\u00e1muri0.1840.1610.175-0.184-0.145Bribri0.1650.2100.237-0.231-0.148\n175\n\n\ntheThirdWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas.Associa-tionforComputationalLinguistics.TakuKudoandJohnRichardson.2018.SentencePiece:Asimpleandlanguageindependentsubwordtok-enizeranddetokenizerforneuraltextprocessing.InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations,pages66\u201371,Brussels,Belgium.AssociationforComputationalLinguistics.IlyaLoshchilovandFrankHutter.2017.Fixingweightdecayregularizationinadam.CoRR,abs/1711.05101.ManuelMager,ArturoOncevay,AbteenEbrahimi,JohnOrtega,AnnetteRios,AngelaFan,XimenaGutierrez-Vasques,LuisChiruzzo,GustavoGim\u00e9nez-Lugo,Ri-cardoRamos,IvanVladimirMezaRuiz,RolandoCoto-Solano,AlexisPalmer,ElisabethMager-Hois,VishravChaudhary,GrahamNeubig,NgocThangVu,andKatharinaKann.2021.FindingsoftheAmeri-casNLP2021sharedtaskonopenmachinetransla-tionforindigenouslanguagesoftheAmericas.InProceedingsoftheFirstWorkshoponNaturalLan-guageProcessingforIndigenousLanguagesoftheAmericas,pages202\u2013217,Online.AssociationforComputationalLinguistics.NLLBTeam,MartaR.Costa-juss\u00e0,JamesCross,Onur\u00c7elebi,MahaElbayad,KennethHeafield,KevinHef-fernan,ElaheKalbassi,JaniceLam,DanielLicht,JeanMaillard,AnnaSun,SkylerWang,GuillaumeWenzek,AlYoungblood,BapiAkula,LoicBar-rault,GabrielMejiaGonzalez,PrangthipHansanti,JohnHoffman,SemarleyJarrett,KaushikRamSadagopan,DirkRowe,ShannonSpruit,ChauTran,PierreAndrews,NecipFazilAyan,ShrutiBhosale,SergeyEdunov,AngelaFan,CynthiaGao,VedanujGoswami,FranciscoGuzm\u00e1n,PhilippKoehn,AlexandreMourachko,ChristopheRopers,SafiyyahSaleem,HolgerSchwenk,andJeffWang.2022.Nolanguageleftbehind:Scalinghuman-centeredmachinetranslation.MajaPopovi\u00b4c.2017.chrF++:wordshelpingcharac-tern-grams.InProceedingsoftheSecondConfer-enceonMachineTranslation,pages612\u2013618,Copen-hagen,Denmark.AssociationforComputationalLin-guistics.SurangikaRanathunga,En-ShiunAnnieLee,MarjanaPriftiSkenduli,RaviShekhar,MehreenAlam,andRishemjitKaur.2023.Neuralmachinetranslationforlow-resourcelanguages:Asurvey.ACMComput.Surv.,55(11).YuqingTang,ChauTran,XianLi,Peng-JenChen,Na-manGoyal,VishravChaudhary,JiataoGu,andAn-gelaFan.2020.Multilingualtranslationwithexten-siblemultilingualpretrainingandfinetuning.CoRR,abs/2008.00401.Ra\u00falV\u00e1zquez,YvesScherrer,SamiVirpioja,andJ\u00f6rgTiedemann.2021.TheHelsinkisubmissiontotheAmericasNLPsharedtask.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages255\u2013264,Online.AssociationforComputationalLinguis-tics.MitchellWortsman,GabrielIlharco,SamirYaGadre,RebeccaRoelofs,RaphaelGontijo-Lopes,AriSMor-cos,HongseokNamkoong,AliFarhadi,YairCar-mon,SimonKornblith,andLudwigSchmidt.2022.Modelsoups:averagingweightsofmultiplefine-tunedmodelsimprovesaccuracywithoutincreasinginferencetime.InProceedingsofthe39thInterna-tionalConferenceonMachineLearning,volume162ofProceedingsofMachineLearningResearch,pages23965\u201323998.PMLR.YigeXu,XipengQiu,LigaoZhou,andXuan-jingHuang.2020.ImprovingBERTfine-tuningviaself-ensembleandself-distillation.CoRR,abs/2002.10345.FrancisZheng,MachelReid,EdisonMarrese-Taylor,andYutakaMatsuo.2021.Low-resourcemachinetranslationusingcross-linguallanguagemodelpre-training.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLan-guagesoftheAmericas,pages234\u2013240,Online.As-sociationforComputationalLinguistics.\n176"}, {"question": " What technique was employed to generate synthetic parallel data?,answer: Backtranslation techniques were employed.", "ref_chunk": "2https://github.com/KaieChen/ameircasnlp2023\n1https://scholarworks.iu.edu/dspace/handle/2022/21028qualityforlow-resourcelanguages,althoughitissensitivetothequalityofsyntheticdata.Toaddressthis,weintroducedadatafilteringtechniquetoim-provethequalityofsyntheticdata.Withfilteredbacktranslation,oursystemachievedanaverageimprovementof0.008intheChrF++score.Fur-thermore,ourstudyrevealsthatmultilingualfine-tuningachievescomparabletranslationqualitytobilingualfine-tuningforlow-resourcelanguages.Weselectedthebilingualmodelwithweightaveragingandbacktranslationasourfinalsubmis-sion.TheimplementationofthisstudyisavailableinourGitrepository2.2Methods2.1DataWeadoptedthedatapreparationmethoddescribedbytheUniversityofHelsinki\u2019ssubmissiontoAmer-icasNLP2021(V\u00e1zquezetal.,2021)foroursys-tem.ThedetailsofthedatasetcanbefoundinTable1.Ourmodeltrainingutilizedthefilteredparalleldata(referredtoasparalleldata),whichconsistedofthetrainingdataprovidedbytheorga-nizersaswellasadditionaldatacollectedbytheUniversityofHelsinki(V\u00e1zquezetal.,2021).Inordertogeneratesyntheticparalleldata(referredtoassyntheticdata),weemployedmonolingualdataandappliedbacktranslationtechniques(refertoSection2.3).Thedevelopmentdatawasusedformodelselectionpurposes.2.2Pre-trainedModelOurmodelsarebasedontheNLLB-600MSeq2Seqpre-trainingschemeintroducedbytheNLLBteam(NLLBTeametal.,2022).Fortokenization,weutilizetheSentencePiecetokenizer(KudoandRichardson,2018),followingtheNLLBconfig-uration.TheNLLBmodelwasinitiallytrainedon\nPlayGroundLowResourceMachineTranslationSystemforthe2023AmericasNLPSharedTaskTianruiGu,KaieChen,SiqiOuyang,LeiLiUniversityofCalifornia,SantaBarbara{tianruigu,kaiechen,siqiouyang,leili}@ucsb.eduAbstractThispaperpresentsPlayGround\u2019ssubmissiontotheAmericasNLP2023sharedtaskonma-chinetranslation(MT)intoindigenouslan-guages.WefinetunedNLLB-600M,amultilin-gualMTmodelpre-trainedonFlores-200,on10low-resourcelanguagedirectionsandexam-inedtheeffectivenessofweightaveragingandbacktranslation.Ourexperimentsshowedthatweightaveraging,onaverage,ledtoa0.0169improvementintheChrF++score.Addition-ally,wefoundthatbacktranslationresultedina0.008improvementintheChrF++score.1IntroductionWeparticipatedintheAmericasNLP2023(Ebrahimietal.,2023)sharedtaskwiththegoalofadvancingpreviousstudies(Mageretal.,2021)onindigenousAmericanlanguages.ThetaskistotranslateSpanishinto10indigenouslanguages,includingAshaninka,Aymara,Bribri,Guarani,H\u00f1\u00e4h\u00f1u,Nahuatl,Quechua,Raramuri,Shipibo-Konibo,andWixarika.Additionally,therewasanotherlanguage,Chatino1,forwhichwedidnotparticipatein.WestartedwiththemonolingualandbilingualdatafromMageretal.(2021)andfinetunedNLLB-600M,amultilingualpre-trainedMTmodelfromMeta\u2019sNoLanguageLeftBehind(NLLB)project(NLLBTeametal.,2022)bothbilinguallyandmul-tilingually.Ontopofthat,weemployedweightaveragingandbacktranslation.Forbacktransla-tion,weadditionallyfilteredthebacktranslatedsentencepairstoimprovethedataquality.Wedemonstratethattrainingonmodelweightsaveragedfrommultiplecheckpointsimprovestrans-lationquality,asindicatedbya0.0169increaseintheChrF++scoreonaverage,withoutrequiringad-ditionalcomputationresources.Additionally,wefoundthatbacktranslationcanenhancetranslation\n173 Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP), pages 173\u2013176 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\nKKXk=1\u0398t\u2212k)(2)where\u0398trepresentsthemodelparametersatepocht.Thistechniquesharessimilaritieswithtrainingdifferentmodelsusingvarioushyperparameters(Wortsmanetal.,2022;Xuetal.,2020).How-ever,asweonlyneedtotrainasinglemodel,thistechniquecanbeparticularlyefficientforlargelan-guagemodels.TheeffectivenessofthisapproachisfurtherdiscussedinSection3.2.3.3HyperparametersInthefine-tuningprocess,wefrozetheencoderlayersoftheNLLBmodel,consideringitspriortrainingonavastamountofSpanishsentences.WeoptimizedthemodelusingAdamW(LoshchilovandHutter,2017)withhyperparameters\u03b2=(0.9,0.999),\u03f5=10\u22126.Weemployedalearningrateof3\u00d710\u22124foratotalof10,000iterations.Forregularization,weutilizedthesamedropoutrateastheoriginalNLLBmodelandaweightdecayof0.01.Furthermore,forweightaveraging,wesetthevalueofKtobe5.2.4EvaluationWereporttheresultsusingChrF++(Popovi\u00b4c,2017),followingtheevaluationscript3providedbytheAmericasNLP2023sharedtask.ChrF++\nLangFilteredMonolingDev\n3https://github.com/AmericasNLP/americasnlp2023\nTable1:Numberofsegmentsindataset.FiltereddataandmonolingualdataarecollectedandfilteredbyUni-versityofHelsinkiteam(V\u00e1zquezetal.,2021)fromAmericasNLP2021.theFlores-200dataset,whichconsistsofAymara,Guarani,Quechua,andSpanish.2.3Fine-tunedModelsWefine-tuneNLLB-600Musingthedatamen-tionedinTable1.ForbothX-to-SpanishandSpanish-to-Xdirections,wefine-tuneNLLB-600Musingfilteredparalleldatainbothbilingualandmultilingualway.Thisproduces20bilingualmod-elsand2multilingualmodels.WeleveragetheaboveX-to-Spanishmodelstogeneratebacktranslateddatatoenrichthetrainingcorpus.Thenwefurtherfine-tunetheSpanish-to-Xmodelswithparalleldatasetextendedwithbacktranslatedsentencepairs.Thefinalmodelsareobtainedwithweightaver-agingsincethetrainingcanbeunstablewithinsuf-ficientdata.2.3.1BackTranslationInordertomakeuseofmonolingualdatainin-digenouslanguages,weemployedbacktransla-tion.Specifically,wefrozethedecoderlayersofNLLBmodelandperformedfine-tuningofanX-to-Spanishmodelusingparalleldata.Then,weutilizedthismodeltogeneratesyntheticsentences.Datafiltering:Syntheticsentencesmaycon-tainnoise.Toaddressthisissue,weimplementadatafiltertoselectasubsetofsyntheticsen-tencesthatwillexpandtheoriginalparalleldataset(Ranathungaetal.,2023).Inourtask,weinitiallyfine-tunedaSpanish-to-Xmodelusingtheparalleldata.Subsequently,weevaluatedthismodelonthesyntheticsentencesandselectedthetopNsampleswiththelowestcross-entropyloss.ThevalueofNisdeterminedbythefollowing:N=min(|Ypar|,|Ysyn|)(1)where|Ypar|representsthenumberofsegmentsintheparalleldataset,and|Ysyn|representsthenumberofsegmentsinthesyntheticdataset.Finally,wecombinedtheselectedsyntheticdatawiththeparalleldataandproceededtoperformadditionalfine-tuningoftheNLLBmodel.2.3.2WeightAveragingStudieshaveshownthataveragingtheweightsofmultiplefinetunedmodelscanenhanceaccuracy(Wortsmanetal.,2022).Inourtrainingapproach,theweightsofthenextepocharetrainedbasedontheaverageofthemodelweightsfromthepreviousKepochs.Forinference,wecomputethefinalmodelbyaveragingthemodelweightsfromthelastKepochs.Themodelcanbedefinedasfollows:NLLB(x;\u0398t)=NLLB(x;1\nAshaninka385813195883Aymara835216750996Bribri73030996Guarani1448340516995H\u00f1\u00e4h\u00f1u7049537599Nahuatl174319222672Quechua22862460399996Raramuri165290995Shipibo-Konibo2885423595996Wixarika11525511994\n174\n\n\nTable2:ResultinChrF++ondevelopdataset,exceptforbaselineandBi++(test).BaselinemodelisthebestsubmissionforAmericasNLP2021.Theeffectivenessofweightaveraging(Multi+andBi+)andbacktranslationiscompared(Multi++andBi++).Wealsocomparedtheperformanceofbilingual(Bi)andmultilingual(Multi).capturesthecharacter-levelperformance,makingitparticularlysuitableforevaluatingthepolysyn-theticpropertiesobservedinmanyindigenouslan-guages(Zhengetal.,2021).3ResultsTheresultsarepresentedinTable2forboththedevelopmentandtestdatasets.OurBi++modeldemonstratesimprovementsinfourlanguages:H\u00f1\u00e4h\u00f1u,Aymara,Ash\u00e1ninka,andQuechua,com-paredtotheBaselinemodelprovidedbytheorga-nizer.Ingeneral,thetrendsinresultsforthede-velopmentandtrainingdatasetsaresimilar,exceptforRar\u00e1muriandBribri.Thisdiscrepancymaybeattributedtothetestdatasetcontainingmoreunknowntokens,towhichourmodelissensitive.Previousstudy(Mageretal.,2021)haspri-marilyfocusedonfine-tuningbilingualmachinetranslationmodels.However,theresultsfromourMulti++andBi++modelsdemonstratethepromis-ingpotentialofmultilingualfine-tuning(Tangetal.,2020).Onaverage,theChrF++scoreforMulti++isonly0.0012lowerthanthatofBi++.Wealsocomparedtheeffectivenessofweightaveragingandbacktranslation.Weightaveragingimprovedtranslationsforalltargetlanguages.Onaverage,Multi+achievedaChrF++scorethatwas0.0169higherthanMulti.Theseresultsindicatethatoursimpletechniquecanenhancelow-resourcemachinetranslationwithoutrequiringadditionalcomputationalresources.However,theimpactofbacktranslationvariedacrosslanguages,asobservedintheresultsforMulti+andMulti++.Onaverage,theimplemen-tationofbacktranslationresultedina0.008im-provementintheChrF++metric.ForWixarikaandAymara,therewasaslightdropintheChrF++scoresafterbacktranslation.Despiteperformingdatafiltering,thequalityofsyntheticdatalargelydependsontheperformanceoftheX-to-Spanishmodel.Insummary,ourfine-tuningtechniquehasshownimprovementsinperformance.However,withfurtherrefinementsanddesignenhancements,thereispotentialforourmodeltoachievehigherlevelsofperformance.4ConclusionInthispaper,wepresentedoursubmissiontotheAmericasNLP2023sharedtask.Oursystemuti-lizedtheNLLB-600Mpre-trainedmodeltotrans-lateSpanishinto10indigenouslanguages.Wealsoinvestigatedthepotentialofmultilingualtranslationmodels,whichshowedpromisingresults.Addition-ally,wefoundthataveragingmodelweightsfrompreviousepochsprovedtobeanefficientandeffec-tiveapproach.Whilebacktranslationdemonstratedperformanceimprovements,furthermethodsarenecessarytoaddressnoisydata.Thesefindingshighlightthepositiveoutcomesofourstudyandprovidevaluableinsightsforfutureadvancementsinlow-resourcemachinetranslationtechniques.ReferencesAbteenEbrahimi,ManuelMager,ArturoOncevay,EnoraRice,CynthiaMonta\u00f1o,JohnOrtega,ShrutiRijhwani,AlexisPalmer,RolandoCoto-Solano,Hi-lariaCruz,andKatharinaKann.2023.FindingsoftheAmericasNLP2023sharedtaskonmachinetrans-lationintoindigenouslanguages.InProceedingsof\nTargetlanguageBaseline(Test)MultiMulti+Multi++BiBi++Bi++(Test)\nWixarika0.3040.2770.2940.2940.2660.2790.288H\u00f1\u00e4h\u00f1u0.1470.1290.1330.1380.1440.1410.148Aymara0.2830.2910.3280.3260.3360.3260.300Shipibo-Konibo0.3290.2240.2380.2530.2610.2830.277Nahuatl0.2660.2410.2520.2750.2820.2830.237Guarani0.3360.3040.3160.3210.3150.3030.331Ash\u00e1ninka0.2580.2220.2380.2720.2690.2860.280Quechua0.3430.3240.341-0.337-0.344Rar\u00e1muri0.1840.1610.175-0.184-0.145Bribri0.1650.2100.237-0.231-0.148\n175\n\n\ntheThirdWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas.Associa-tionforComputationalLinguistics.TakuKudoandJohnRichardson.2018.SentencePiece:Asimpleandlanguageindependentsubwordtok-enizeranddetokenizerforneuraltextprocessing.InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations,pages66\u201371,Brussels,Belgium.AssociationforComputationalLinguistics.IlyaLoshchilovandFrankHutter.2017.Fixingweightdecayregularizationinadam.CoRR,abs/1711.05101.ManuelMager,ArturoOncevay,AbteenEbrahimi,JohnOrtega,AnnetteRios,AngelaFan,XimenaGutierrez-Vasques,LuisChiruzzo,GustavoGim\u00e9nez-Lugo,Ri-cardoRamos,IvanVladimirMezaRuiz,RolandoCoto-Solano,AlexisPalmer,ElisabethMager-Hois,VishravChaudhary,GrahamNeubig,NgocThangVu,andKatharinaKann.2021.FindingsoftheAmeri-casNLP2021sharedtaskonopenmachinetransla-tionforindigenouslanguagesoftheAmericas.InProceedingsoftheFirstWorkshoponNaturalLan-guageProcessingforIndigenousLanguagesoftheAmericas,pages202\u2013217,Online.AssociationforComputationalLinguistics.NLLBTeam,MartaR.Costa-juss\u00e0,JamesCross,Onur\u00c7elebi,MahaElbayad,KennethHeafield,KevinHef-fernan,ElaheKalbassi,JaniceLam,DanielLicht,JeanMaillard,AnnaSun,SkylerWang,GuillaumeWenzek,AlYoungblood,BapiAkula,LoicBar-rault,GabrielMejiaGonzalez,PrangthipHansanti,JohnHoffman,SemarleyJarrett,KaushikRamSadagopan,DirkRowe,ShannonSpruit,ChauTran,PierreAndrews,NecipFazilAyan,ShrutiBhosale,SergeyEdunov,AngelaFan,CynthiaGao,VedanujGoswami,FranciscoGuzm\u00e1n,PhilippKoehn,AlexandreMourachko,ChristopheRopers,SafiyyahSaleem,HolgerSchwenk,andJeffWang.2022.Nolanguageleftbehind:Scalinghuman-centeredmachinetranslation.MajaPopovi\u00b4c.2017.chrF++:wordshelpingcharac-tern-grams.InProceedingsoftheSecondConfer-enceonMachineTranslation,pages612\u2013618,Copen-hagen,Denmark.AssociationforComputationalLin-guistics.SurangikaRanathunga,En-ShiunAnnieLee,MarjanaPriftiSkenduli,RaviShekhar,MehreenAlam,andRishemjitKaur.2023.Neuralmachinetranslationforlow-resourcelanguages:Asurvey.ACMComput.Surv.,55(11).YuqingTang,ChauTran,XianLi,Peng-JenChen,Na-manGoyal,VishravChaudhary,JiataoGu,andAn-gelaFan.2020.Multilingualtranslationwithexten-siblemultilingualpretrainingandfinetuning.CoRR,abs/2008.00401.Ra\u00falV\u00e1zquez,YvesScherrer,SamiVirpioja,andJ\u00f6rgTiedemann.2021.TheHelsinkisubmissiontotheAmericasNLPsharedtask.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages255\u2013264,Online.AssociationforComputationalLinguis-tics.MitchellWortsman,GabrielIlharco,SamirYaGadre,RebeccaRoelofs,RaphaelGontijo-Lopes,AriSMor-cos,HongseokNamkoong,AliFarhadi,YairCar-mon,SimonKornblith,andLudwigSchmidt.2022.Modelsoups:averagingweightsofmultiplefine-tunedmodelsimprovesaccuracywithoutincreasinginferencetime.InProceedingsofthe39thInterna-tionalConferenceonMachineLearning,volume162ofProceedingsofMachineLearningResearch,pages23965\u201323998.PMLR.YigeXu,XipengQiu,LigaoZhou,andXuan-jingHuang.2020.ImprovingBERTfine-tuningviaself-ensembleandself-distillation.CoRR,abs/2002.10345.FrancisZheng,MachelReid,EdisonMarrese-Taylor,andYutakaMatsuo.2021.Low-resourcemachinetranslationusingcross-linguallanguagemodelpre-training.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLan-guagesoftheAmericas,pages234\u2013240,Online.As-sociationforComputationalLinguistics.\n176"}, {"question": " What optimization algorithm was used during the fine-tuning process?,answer: The model was optimized using AdamW.", "ref_chunk": "2https://github.com/KaieChen/ameircasnlp2023\n1https://scholarworks.iu.edu/dspace/handle/2022/21028qualityforlow-resourcelanguages,althoughitissensitivetothequalityofsyntheticdata.Toaddressthis,weintroducedadatafilteringtechniquetoim-provethequalityofsyntheticdata.Withfilteredbacktranslation,oursystemachievedanaverageimprovementof0.008intheChrF++score.Fur-thermore,ourstudyrevealsthatmultilingualfine-tuningachievescomparabletranslationqualitytobilingualfine-tuningforlow-resourcelanguages.Weselectedthebilingualmodelwithweightaveragingandbacktranslationasourfinalsubmis-sion.TheimplementationofthisstudyisavailableinourGitrepository2.2Methods2.1DataWeadoptedthedatapreparationmethoddescribedbytheUniversityofHelsinki\u2019ssubmissiontoAmer-icasNLP2021(V\u00e1zquezetal.,2021)foroursys-tem.ThedetailsofthedatasetcanbefoundinTable1.Ourmodeltrainingutilizedthefilteredparalleldata(referredtoasparalleldata),whichconsistedofthetrainingdataprovidedbytheorga-nizersaswellasadditionaldatacollectedbytheUniversityofHelsinki(V\u00e1zquezetal.,2021).Inordertogeneratesyntheticparalleldata(referredtoassyntheticdata),weemployedmonolingualdataandappliedbacktranslationtechniques(refertoSection2.3).Thedevelopmentdatawasusedformodelselectionpurposes.2.2Pre-trainedModelOurmodelsarebasedontheNLLB-600MSeq2Seqpre-trainingschemeintroducedbytheNLLBteam(NLLBTeametal.,2022).Fortokenization,weutilizetheSentencePiecetokenizer(KudoandRichardson,2018),followingtheNLLBconfig-uration.TheNLLBmodelwasinitiallytrainedon\nPlayGroundLowResourceMachineTranslationSystemforthe2023AmericasNLPSharedTaskTianruiGu,KaieChen,SiqiOuyang,LeiLiUniversityofCalifornia,SantaBarbara{tianruigu,kaiechen,siqiouyang,leili}@ucsb.eduAbstractThispaperpresentsPlayGround\u2019ssubmissiontotheAmericasNLP2023sharedtaskonma-chinetranslation(MT)intoindigenouslan-guages.WefinetunedNLLB-600M,amultilin-gualMTmodelpre-trainedonFlores-200,on10low-resourcelanguagedirectionsandexam-inedtheeffectivenessofweightaveragingandbacktranslation.Ourexperimentsshowedthatweightaveraging,onaverage,ledtoa0.0169improvementintheChrF++score.Addition-ally,wefoundthatbacktranslationresultedina0.008improvementintheChrF++score.1IntroductionWeparticipatedintheAmericasNLP2023(Ebrahimietal.,2023)sharedtaskwiththegoalofadvancingpreviousstudies(Mageretal.,2021)onindigenousAmericanlanguages.ThetaskistotranslateSpanishinto10indigenouslanguages,includingAshaninka,Aymara,Bribri,Guarani,H\u00f1\u00e4h\u00f1u,Nahuatl,Quechua,Raramuri,Shipibo-Konibo,andWixarika.Additionally,therewasanotherlanguage,Chatino1,forwhichwedidnotparticipatein.WestartedwiththemonolingualandbilingualdatafromMageretal.(2021)andfinetunedNLLB-600M,amultilingualpre-trainedMTmodelfromMeta\u2019sNoLanguageLeftBehind(NLLB)project(NLLBTeametal.,2022)bothbilinguallyandmul-tilingually.Ontopofthat,weemployedweightaveragingandbacktranslation.Forbacktransla-tion,weadditionallyfilteredthebacktranslatedsentencepairstoimprovethedataquality.Wedemonstratethattrainingonmodelweightsaveragedfrommultiplecheckpointsimprovestrans-lationquality,asindicatedbya0.0169increaseintheChrF++scoreonaverage,withoutrequiringad-ditionalcomputationresources.Additionally,wefoundthatbacktranslationcanenhancetranslation\n173 Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP), pages 173\u2013176 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\nKKXk=1\u0398t\u2212k)(2)where\u0398trepresentsthemodelparametersatepocht.Thistechniquesharessimilaritieswithtrainingdifferentmodelsusingvarioushyperparameters(Wortsmanetal.,2022;Xuetal.,2020).How-ever,asweonlyneedtotrainasinglemodel,thistechniquecanbeparticularlyefficientforlargelan-guagemodels.TheeffectivenessofthisapproachisfurtherdiscussedinSection3.2.3.3HyperparametersInthefine-tuningprocess,wefrozetheencoderlayersoftheNLLBmodel,consideringitspriortrainingonavastamountofSpanishsentences.WeoptimizedthemodelusingAdamW(LoshchilovandHutter,2017)withhyperparameters\u03b2=(0.9,0.999),\u03f5=10\u22126.Weemployedalearningrateof3\u00d710\u22124foratotalof10,000iterations.Forregularization,weutilizedthesamedropoutrateastheoriginalNLLBmodelandaweightdecayof0.01.Furthermore,forweightaveraging,wesetthevalueofKtobe5.2.4EvaluationWereporttheresultsusingChrF++(Popovi\u00b4c,2017),followingtheevaluationscript3providedbytheAmericasNLP2023sharedtask.ChrF++\nLangFilteredMonolingDev\n3https://github.com/AmericasNLP/americasnlp2023\nTable1:Numberofsegmentsindataset.FiltereddataandmonolingualdataarecollectedandfilteredbyUni-versityofHelsinkiteam(V\u00e1zquezetal.,2021)fromAmericasNLP2021.theFlores-200dataset,whichconsistsofAymara,Guarani,Quechua,andSpanish.2.3Fine-tunedModelsWefine-tuneNLLB-600Musingthedatamen-tionedinTable1.ForbothX-to-SpanishandSpanish-to-Xdirections,wefine-tuneNLLB-600Musingfilteredparalleldatainbothbilingualandmultilingualway.Thisproduces20bilingualmod-elsand2multilingualmodels.WeleveragetheaboveX-to-Spanishmodelstogeneratebacktranslateddatatoenrichthetrainingcorpus.Thenwefurtherfine-tunetheSpanish-to-Xmodelswithparalleldatasetextendedwithbacktranslatedsentencepairs.Thefinalmodelsareobtainedwithweightaver-agingsincethetrainingcanbeunstablewithinsuf-ficientdata.2.3.1BackTranslationInordertomakeuseofmonolingualdatainin-digenouslanguages,weemployedbacktransla-tion.Specifically,wefrozethedecoderlayersofNLLBmodelandperformedfine-tuningofanX-to-Spanishmodelusingparalleldata.Then,weutilizedthismodeltogeneratesyntheticsentences.Datafiltering:Syntheticsentencesmaycon-tainnoise.Toaddressthisissue,weimplementadatafiltertoselectasubsetofsyntheticsen-tencesthatwillexpandtheoriginalparalleldataset(Ranathungaetal.,2023).Inourtask,weinitiallyfine-tunedaSpanish-to-Xmodelusingtheparalleldata.Subsequently,weevaluatedthismodelonthesyntheticsentencesandselectedthetopNsampleswiththelowestcross-entropyloss.ThevalueofNisdeterminedbythefollowing:N=min(|Ypar|,|Ysyn|)(1)where|Ypar|representsthenumberofsegmentsintheparalleldataset,and|Ysyn|representsthenumberofsegmentsinthesyntheticdataset.Finally,wecombinedtheselectedsyntheticdatawiththeparalleldataandproceededtoperformadditionalfine-tuningoftheNLLBmodel.2.3.2WeightAveragingStudieshaveshownthataveragingtheweightsofmultiplefinetunedmodelscanenhanceaccuracy(Wortsmanetal.,2022).Inourtrainingapproach,theweightsofthenextepocharetrainedbasedontheaverageofthemodelweightsfromthepreviousKepochs.Forinference,wecomputethefinalmodelbyaveragingthemodelweightsfromthelastKepochs.Themodelcanbedefinedasfollows:NLLB(x;\u0398t)=NLLB(x;1\nAshaninka385813195883Aymara835216750996Bribri73030996Guarani1448340516995H\u00f1\u00e4h\u00f1u7049537599Nahuatl174319222672Quechua22862460399996Raramuri165290995Shipibo-Konibo2885423595996Wixarika11525511994\n174\n\n\nTable2:ResultinChrF++ondevelopdataset,exceptforbaselineandBi++(test).BaselinemodelisthebestsubmissionforAmericasNLP2021.Theeffectivenessofweightaveraging(Multi+andBi+)andbacktranslationiscompared(Multi++andBi++).Wealsocomparedtheperformanceofbilingual(Bi)andmultilingual(Multi).capturesthecharacter-levelperformance,makingitparticularlysuitableforevaluatingthepolysyn-theticpropertiesobservedinmanyindigenouslan-guages(Zhengetal.,2021).3ResultsTheresultsarepresentedinTable2forboththedevelopmentandtestdatasets.OurBi++modeldemonstratesimprovementsinfourlanguages:H\u00f1\u00e4h\u00f1u,Aymara,Ash\u00e1ninka,andQuechua,com-paredtotheBaselinemodelprovidedbytheorga-nizer.Ingeneral,thetrendsinresultsforthede-velopmentandtrainingdatasetsaresimilar,exceptforRar\u00e1muriandBribri.Thisdiscrepancymaybeattributedtothetestdatasetcontainingmoreunknowntokens,towhichourmodelissensitive.Previousstudy(Mageretal.,2021)haspri-marilyfocusedonfine-tuningbilingualmachinetranslationmodels.However,theresultsfromourMulti++andBi++modelsdemonstratethepromis-ingpotentialofmultilingualfine-tuning(Tangetal.,2020).Onaverage,theChrF++scoreforMulti++isonly0.0012lowerthanthatofBi++.Wealsocomparedtheeffectivenessofweightaveragingandbacktranslation.Weightaveragingimprovedtranslationsforalltargetlanguages.Onaverage,Multi+achievedaChrF++scorethatwas0.0169higherthanMulti.Theseresultsindicatethatoursimpletechniquecanenhancelow-resourcemachinetranslationwithoutrequiringadditionalcomputationalresources.However,theimpactofbacktranslationvariedacrosslanguages,asobservedintheresultsforMulti+andMulti++.Onaverage,theimplemen-tationofbacktranslationresultedina0.008im-provementintheChrF++metric.ForWixarikaandAymara,therewasaslightdropintheChrF++scoresafterbacktranslation.Despiteperformingdatafiltering,thequalityofsyntheticdatalargelydependsontheperformanceoftheX-to-Spanishmodel.Insummary,ourfine-tuningtechniquehasshownimprovementsinperformance.However,withfurtherrefinementsanddesignenhancements,thereispotentialforourmodeltoachievehigherlevelsofperformance.4ConclusionInthispaper,wepresentedoursubmissiontotheAmericasNLP2023sharedtask.Oursystemuti-lizedtheNLLB-600Mpre-trainedmodeltotrans-lateSpanishinto10indigenouslanguages.Wealsoinvestigatedthepotentialofmultilingualtranslationmodels,whichshowedpromisingresults.Addition-ally,wefoundthataveragingmodelweightsfrompreviousepochsprovedtobeanefficientandeffec-tiveapproach.Whilebacktranslationdemonstratedperformanceimprovements,furthermethodsarenecessarytoaddressnoisydata.Thesefindingshighlightthepositiveoutcomesofourstudyandprovidevaluableinsightsforfutureadvancementsinlow-resourcemachinetranslationtechniques.ReferencesAbteenEbrahimi,ManuelMager,ArturoOncevay,EnoraRice,CynthiaMonta\u00f1o,JohnOrtega,ShrutiRijhwani,AlexisPalmer,RolandoCoto-Solano,Hi-lariaCruz,andKatharinaKann.2023.FindingsoftheAmericasNLP2023sharedtaskonmachinetrans-lationintoindigenouslanguages.InProceedingsof\nTargetlanguageBaseline(Test)MultiMulti+Multi++BiBi++Bi++(Test)\nWixarika0.3040.2770.2940.2940.2660.2790.288H\u00f1\u00e4h\u00f1u0.1470.1290.1330.1380.1440.1410.148Aymara0.2830.2910.3280.3260.3360.3260.300Shipibo-Konibo0.3290.2240.2380.2530.2610.2830.277Nahuatl0.2660.2410.2520.2750.2820.2830.237Guarani0.3360.3040.3160.3210.3150.3030.331Ash\u00e1ninka0.2580.2220.2380.2720.2690.2860.280Quechua0.3430.3240.341-0.337-0.344Rar\u00e1muri0.1840.1610.175-0.184-0.145Bribri0.1650.2100.237-0.231-0.148\n175\n\n\ntheThirdWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas.Associa-tionforComputationalLinguistics.TakuKudoandJohnRichardson.2018.SentencePiece:Asimpleandlanguageindependentsubwordtok-enizeranddetokenizerforneuraltextprocessing.InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations,pages66\u201371,Brussels,Belgium.AssociationforComputationalLinguistics.IlyaLoshchilovandFrankHutter.2017.Fixingweightdecayregularizationinadam.CoRR,abs/1711.05101.ManuelMager,ArturoOncevay,AbteenEbrahimi,JohnOrtega,AnnetteRios,AngelaFan,XimenaGutierrez-Vasques,LuisChiruzzo,GustavoGim\u00e9nez-Lugo,Ri-cardoRamos,IvanVladimirMezaRuiz,RolandoCoto-Solano,AlexisPalmer,ElisabethMager-Hois,VishravChaudhary,GrahamNeubig,NgocThangVu,andKatharinaKann.2021.FindingsoftheAmeri-casNLP2021sharedtaskonopenmachinetransla-tionforindigenouslanguagesoftheAmericas.InProceedingsoftheFirstWorkshoponNaturalLan-guageProcessingforIndigenousLanguagesoftheAmericas,pages202\u2013217,Online.AssociationforComputationalLinguistics.NLLBTeam,MartaR.Costa-juss\u00e0,JamesCross,Onur\u00c7elebi,MahaElbayad,KennethHeafield,KevinHef-fernan,ElaheKalbassi,JaniceLam,DanielLicht,JeanMaillard,AnnaSun,SkylerWang,GuillaumeWenzek,AlYoungblood,BapiAkula,LoicBar-rault,GabrielMejiaGonzalez,PrangthipHansanti,JohnHoffman,SemarleyJarrett,KaushikRamSadagopan,DirkRowe,ShannonSpruit,ChauTran,PierreAndrews,NecipFazilAyan,ShrutiBhosale,SergeyEdunov,AngelaFan,CynthiaGao,VedanujGoswami,FranciscoGuzm\u00e1n,PhilippKoehn,AlexandreMourachko,ChristopheRopers,SafiyyahSaleem,HolgerSchwenk,andJeffWang.2022.Nolanguageleftbehind:Scalinghuman-centeredmachinetranslation.MajaPopovi\u00b4c.2017.chrF++:wordshelpingcharac-tern-grams.InProceedingsoftheSecondConfer-enceonMachineTranslation,pages612\u2013618,Copen-hagen,Denmark.AssociationforComputationalLin-guistics.SurangikaRanathunga,En-ShiunAnnieLee,MarjanaPriftiSkenduli,RaviShekhar,MehreenAlam,andRishemjitKaur.2023.Neuralmachinetranslationforlow-resourcelanguages:Asurvey.ACMComput.Surv.,55(11).YuqingTang,ChauTran,XianLi,Peng-JenChen,Na-manGoyal,VishravChaudhary,JiataoGu,andAn-gelaFan.2020.Multilingualtranslationwithexten-siblemultilingualpretrainingandfinetuning.CoRR,abs/2008.00401.Ra\u00falV\u00e1zquez,YvesScherrer,SamiVirpioja,andJ\u00f6rgTiedemann.2021.TheHelsinkisubmissiontotheAmericasNLPsharedtask.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages255\u2013264,Online.AssociationforComputationalLinguis-tics.MitchellWortsman,GabrielIlharco,SamirYaGadre,RebeccaRoelofs,RaphaelGontijo-Lopes,AriSMor-cos,HongseokNamkoong,AliFarhadi,YairCar-mon,SimonKornblith,andLudwigSchmidt.2022.Modelsoups:averagingweightsofmultiplefine-tunedmodelsimprovesaccuracywithoutincreasinginferencetime.InProceedingsofthe39thInterna-tionalConferenceonMachineLearning,volume162ofProceedingsofMachineLearningResearch,pages23965\u201323998.PMLR.YigeXu,XipengQiu,LigaoZhou,andXuan-jingHuang.2020.ImprovingBERTfine-tuningviaself-ensembleandself-distillation.CoRR,abs/2002.10345.FrancisZheng,MachelReid,EdisonMarrese-Taylor,andYutakaMatsuo.2021.Low-resourcemachinetranslationusingcross-linguallanguagemodelpre-training.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLan-guagesoftheAmericas,pages234\u2013240,Online.As-sociationforComputationalLinguistics.\n176"}, {"question": " What evaluation metric was used to report the results in the text?,answer: The ChrF++ metric was used to report the results.", "ref_chunk": "2https://github.com/KaieChen/ameircasnlp2023\n1https://scholarworks.iu.edu/dspace/handle/2022/21028qualityforlow-resourcelanguages,althoughitissensitivetothequalityofsyntheticdata.Toaddressthis,weintroducedadatafilteringtechniquetoim-provethequalityofsyntheticdata.Withfilteredbacktranslation,oursystemachievedanaverageimprovementof0.008intheChrF++score.Fur-thermore,ourstudyrevealsthatmultilingualfine-tuningachievescomparabletranslationqualitytobilingualfine-tuningforlow-resourcelanguages.Weselectedthebilingualmodelwithweightaveragingandbacktranslationasourfinalsubmis-sion.TheimplementationofthisstudyisavailableinourGitrepository2.2Methods2.1DataWeadoptedthedatapreparationmethoddescribedbytheUniversityofHelsinki\u2019ssubmissiontoAmer-icasNLP2021(V\u00e1zquezetal.,2021)foroursys-tem.ThedetailsofthedatasetcanbefoundinTable1.Ourmodeltrainingutilizedthefilteredparalleldata(referredtoasparalleldata),whichconsistedofthetrainingdataprovidedbytheorga-nizersaswellasadditionaldatacollectedbytheUniversityofHelsinki(V\u00e1zquezetal.,2021).Inordertogeneratesyntheticparalleldata(referredtoassyntheticdata),weemployedmonolingualdataandappliedbacktranslationtechniques(refertoSection2.3).Thedevelopmentdatawasusedformodelselectionpurposes.2.2Pre-trainedModelOurmodelsarebasedontheNLLB-600MSeq2Seqpre-trainingschemeintroducedbytheNLLBteam(NLLBTeametal.,2022).Fortokenization,weutilizetheSentencePiecetokenizer(KudoandRichardson,2018),followingtheNLLBconfig-uration.TheNLLBmodelwasinitiallytrainedon\nPlayGroundLowResourceMachineTranslationSystemforthe2023AmericasNLPSharedTaskTianruiGu,KaieChen,SiqiOuyang,LeiLiUniversityofCalifornia,SantaBarbara{tianruigu,kaiechen,siqiouyang,leili}@ucsb.eduAbstractThispaperpresentsPlayGround\u2019ssubmissiontotheAmericasNLP2023sharedtaskonma-chinetranslation(MT)intoindigenouslan-guages.WefinetunedNLLB-600M,amultilin-gualMTmodelpre-trainedonFlores-200,on10low-resourcelanguagedirectionsandexam-inedtheeffectivenessofweightaveragingandbacktranslation.Ourexperimentsshowedthatweightaveraging,onaverage,ledtoa0.0169improvementintheChrF++score.Addition-ally,wefoundthatbacktranslationresultedina0.008improvementintheChrF++score.1IntroductionWeparticipatedintheAmericasNLP2023(Ebrahimietal.,2023)sharedtaskwiththegoalofadvancingpreviousstudies(Mageretal.,2021)onindigenousAmericanlanguages.ThetaskistotranslateSpanishinto10indigenouslanguages,includingAshaninka,Aymara,Bribri,Guarani,H\u00f1\u00e4h\u00f1u,Nahuatl,Quechua,Raramuri,Shipibo-Konibo,andWixarika.Additionally,therewasanotherlanguage,Chatino1,forwhichwedidnotparticipatein.WestartedwiththemonolingualandbilingualdatafromMageretal.(2021)andfinetunedNLLB-600M,amultilingualpre-trainedMTmodelfromMeta\u2019sNoLanguageLeftBehind(NLLB)project(NLLBTeametal.,2022)bothbilinguallyandmul-tilingually.Ontopofthat,weemployedweightaveragingandbacktranslation.Forbacktransla-tion,weadditionallyfilteredthebacktranslatedsentencepairstoimprovethedataquality.Wedemonstratethattrainingonmodelweightsaveragedfrommultiplecheckpointsimprovestrans-lationquality,asindicatedbya0.0169increaseintheChrF++scoreonaverage,withoutrequiringad-ditionalcomputationresources.Additionally,wefoundthatbacktranslationcanenhancetranslation\n173 Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP), pages 173\u2013176 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\nKKXk=1\u0398t\u2212k)(2)where\u0398trepresentsthemodelparametersatepocht.Thistechniquesharessimilaritieswithtrainingdifferentmodelsusingvarioushyperparameters(Wortsmanetal.,2022;Xuetal.,2020).How-ever,asweonlyneedtotrainasinglemodel,thistechniquecanbeparticularlyefficientforlargelan-guagemodels.TheeffectivenessofthisapproachisfurtherdiscussedinSection3.2.3.3HyperparametersInthefine-tuningprocess,wefrozetheencoderlayersoftheNLLBmodel,consideringitspriortrainingonavastamountofSpanishsentences.WeoptimizedthemodelusingAdamW(LoshchilovandHutter,2017)withhyperparameters\u03b2=(0.9,0.999),\u03f5=10\u22126.Weemployedalearningrateof3\u00d710\u22124foratotalof10,000iterations.Forregularization,weutilizedthesamedropoutrateastheoriginalNLLBmodelandaweightdecayof0.01.Furthermore,forweightaveraging,wesetthevalueofKtobe5.2.4EvaluationWereporttheresultsusingChrF++(Popovi\u00b4c,2017),followingtheevaluationscript3providedbytheAmericasNLP2023sharedtask.ChrF++\nLangFilteredMonolingDev\n3https://github.com/AmericasNLP/americasnlp2023\nTable1:Numberofsegmentsindataset.FiltereddataandmonolingualdataarecollectedandfilteredbyUni-versityofHelsinkiteam(V\u00e1zquezetal.,2021)fromAmericasNLP2021.theFlores-200dataset,whichconsistsofAymara,Guarani,Quechua,andSpanish.2.3Fine-tunedModelsWefine-tuneNLLB-600Musingthedatamen-tionedinTable1.ForbothX-to-SpanishandSpanish-to-Xdirections,wefine-tuneNLLB-600Musingfilteredparalleldatainbothbilingualandmultilingualway.Thisproduces20bilingualmod-elsand2multilingualmodels.WeleveragetheaboveX-to-Spanishmodelstogeneratebacktranslateddatatoenrichthetrainingcorpus.Thenwefurtherfine-tunetheSpanish-to-Xmodelswithparalleldatasetextendedwithbacktranslatedsentencepairs.Thefinalmodelsareobtainedwithweightaver-agingsincethetrainingcanbeunstablewithinsuf-ficientdata.2.3.1BackTranslationInordertomakeuseofmonolingualdatainin-digenouslanguages,weemployedbacktransla-tion.Specifically,wefrozethedecoderlayersofNLLBmodelandperformedfine-tuningofanX-to-Spanishmodelusingparalleldata.Then,weutilizedthismodeltogeneratesyntheticsentences.Datafiltering:Syntheticsentencesmaycon-tainnoise.Toaddressthisissue,weimplementadatafiltertoselectasubsetofsyntheticsen-tencesthatwillexpandtheoriginalparalleldataset(Ranathungaetal.,2023).Inourtask,weinitiallyfine-tunedaSpanish-to-Xmodelusingtheparalleldata.Subsequently,weevaluatedthismodelonthesyntheticsentencesandselectedthetopNsampleswiththelowestcross-entropyloss.ThevalueofNisdeterminedbythefollowing:N=min(|Ypar|,|Ysyn|)(1)where|Ypar|representsthenumberofsegmentsintheparalleldataset,and|Ysyn|representsthenumberofsegmentsinthesyntheticdataset.Finally,wecombinedtheselectedsyntheticdatawiththeparalleldataandproceededtoperformadditionalfine-tuningoftheNLLBmodel.2.3.2WeightAveragingStudieshaveshownthataveragingtheweightsofmultiplefinetunedmodelscanenhanceaccuracy(Wortsmanetal.,2022).Inourtrainingapproach,theweightsofthenextepocharetrainedbasedontheaverageofthemodelweightsfromthepreviousKepochs.Forinference,wecomputethefinalmodelbyaveragingthemodelweightsfromthelastKepochs.Themodelcanbedefinedasfollows:NLLB(x;\u0398t)=NLLB(x;1\nAshaninka385813195883Aymara835216750996Bribri73030996Guarani1448340516995H\u00f1\u00e4h\u00f1u7049537599Nahuatl174319222672Quechua22862460399996Raramuri165290995Shipibo-Konibo2885423595996Wixarika11525511994\n174\n\n\nTable2:ResultinChrF++ondevelopdataset,exceptforbaselineandBi++(test).BaselinemodelisthebestsubmissionforAmericasNLP2021.Theeffectivenessofweightaveraging(Multi+andBi+)andbacktranslationiscompared(Multi++andBi++).Wealsocomparedtheperformanceofbilingual(Bi)andmultilingual(Multi).capturesthecharacter-levelperformance,makingitparticularlysuitableforevaluatingthepolysyn-theticpropertiesobservedinmanyindigenouslan-guages(Zhengetal.,2021).3ResultsTheresultsarepresentedinTable2forboththedevelopmentandtestdatasets.OurBi++modeldemonstratesimprovementsinfourlanguages:H\u00f1\u00e4h\u00f1u,Aymara,Ash\u00e1ninka,andQuechua,com-paredtotheBaselinemodelprovidedbytheorga-nizer.Ingeneral,thetrendsinresultsforthede-velopmentandtrainingdatasetsaresimilar,exceptforRar\u00e1muriandBribri.Thisdiscrepancymaybeattributedtothetestdatasetcontainingmoreunknowntokens,towhichourmodelissensitive.Previousstudy(Mageretal.,2021)haspri-marilyfocusedonfine-tuningbilingualmachinetranslationmodels.However,theresultsfromourMulti++andBi++modelsdemonstratethepromis-ingpotentialofmultilingualfine-tuning(Tangetal.,2020).Onaverage,theChrF++scoreforMulti++isonly0.0012lowerthanthatofBi++.Wealsocomparedtheeffectivenessofweightaveragingandbacktranslation.Weightaveragingimprovedtranslationsforalltargetlanguages.Onaverage,Multi+achievedaChrF++scorethatwas0.0169higherthanMulti.Theseresultsindicatethatoursimpletechniquecanenhancelow-resourcemachinetranslationwithoutrequiringadditionalcomputationalresources.However,theimpactofbacktranslationvariedacrosslanguages,asobservedintheresultsforMulti+andMulti++.Onaverage,theimplemen-tationofbacktranslationresultedina0.008im-provementintheChrF++metric.ForWixarikaandAymara,therewasaslightdropintheChrF++scoresafterbacktranslation.Despiteperformingdatafiltering,thequalityofsyntheticdatalargelydependsontheperformanceoftheX-to-Spanishmodel.Insummary,ourfine-tuningtechniquehasshownimprovementsinperformance.However,withfurtherrefinementsanddesignenhancements,thereispotentialforourmodeltoachievehigherlevelsofperformance.4ConclusionInthispaper,wepresentedoursubmissiontotheAmericasNLP2023sharedtask.Oursystemuti-lizedtheNLLB-600Mpre-trainedmodeltotrans-lateSpanishinto10indigenouslanguages.Wealsoinvestigatedthepotentialofmultilingualtranslationmodels,whichshowedpromisingresults.Addition-ally,wefoundthataveragingmodelweightsfrompreviousepochsprovedtobeanefficientandeffec-tiveapproach.Whilebacktranslationdemonstratedperformanceimprovements,furthermethodsarenecessarytoaddressnoisydata.Thesefindingshighlightthepositiveoutcomesofourstudyandprovidevaluableinsightsforfutureadvancementsinlow-resourcemachinetranslationtechniques.ReferencesAbteenEbrahimi,ManuelMager,ArturoOncevay,EnoraRice,CynthiaMonta\u00f1o,JohnOrtega,ShrutiRijhwani,AlexisPalmer,RolandoCoto-Solano,Hi-lariaCruz,andKatharinaKann.2023.FindingsoftheAmericasNLP2023sharedtaskonmachinetrans-lationintoindigenouslanguages.InProceedingsof\nTargetlanguageBaseline(Test)MultiMulti+Multi++BiBi++Bi++(Test)\nWixarika0.3040.2770.2940.2940.2660.2790.288H\u00f1\u00e4h\u00f1u0.1470.1290.1330.1380.1440.1410.148Aymara0.2830.2910.3280.3260.3360.3260.300Shipibo-Konibo0.3290.2240.2380.2530.2610.2830.277Nahuatl0.2660.2410.2520.2750.2820.2830.237Guarani0.3360.3040.3160.3210.3150.3030.331Ash\u00e1ninka0.2580.2220.2380.2720.2690.2860.280Quechua0.3430.3240.341-0.337-0.344Rar\u00e1muri0.1840.1610.175-0.184-0.145Bribri0.1650.2100.237-0.231-0.148\n175\n\n\ntheThirdWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas.Associa-tionforComputationalLinguistics.TakuKudoandJohnRichardson.2018.SentencePiece:Asimpleandlanguageindependentsubwordtok-enizeranddetokenizerforneuraltextprocessing.InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations,pages66\u201371,Brussels,Belgium.AssociationforComputationalLinguistics.IlyaLoshchilovandFrankHutter.2017.Fixingweightdecayregularizationinadam.CoRR,abs/1711.05101.ManuelMager,ArturoOncevay,AbteenEbrahimi,JohnOrtega,AnnetteRios,AngelaFan,XimenaGutierrez-Vasques,LuisChiruzzo,GustavoGim\u00e9nez-Lugo,Ri-cardoRamos,IvanVladimirMezaRuiz,RolandoCoto-Solano,AlexisPalmer,ElisabethMager-Hois,VishravChaudhary,GrahamNeubig,NgocThangVu,andKatharinaKann.2021.FindingsoftheAmeri-casNLP2021sharedtaskonopenmachinetransla-tionforindigenouslanguagesoftheAmericas.InProceedingsoftheFirstWorkshoponNaturalLan-guageProcessingforIndigenousLanguagesoftheAmericas,pages202\u2013217,Online.AssociationforComputationalLinguistics.NLLBTeam,MartaR.Costa-juss\u00e0,JamesCross,Onur\u00c7elebi,MahaElbayad,KennethHeafield,KevinHef-fernan,ElaheKalbassi,JaniceLam,DanielLicht,JeanMaillard,AnnaSun,SkylerWang,GuillaumeWenzek,AlYoungblood,BapiAkula,LoicBar-rault,GabrielMejiaGonzalez,PrangthipHansanti,JohnHoffman,SemarleyJarrett,KaushikRamSadagopan,DirkRowe,ShannonSpruit,ChauTran,PierreAndrews,NecipFazilAyan,ShrutiBhosale,SergeyEdunov,AngelaFan,CynthiaGao,VedanujGoswami,FranciscoGuzm\u00e1n,PhilippKoehn,AlexandreMourachko,ChristopheRopers,SafiyyahSaleem,HolgerSchwenk,andJeffWang.2022.Nolanguageleftbehind:Scalinghuman-centeredmachinetranslation.MajaPopovi\u00b4c.2017.chrF++:wordshelpingcharac-tern-grams.InProceedingsoftheSecondConfer-enceonMachineTranslation,pages612\u2013618,Copen-hagen,Denmark.AssociationforComputationalLin-guistics.SurangikaRanathunga,En-ShiunAnnieLee,MarjanaPriftiSkenduli,RaviShekhar,MehreenAlam,andRishemjitKaur.2023.Neuralmachinetranslationforlow-resourcelanguages:Asurvey.ACMComput.Surv.,55(11).YuqingTang,ChauTran,XianLi,Peng-JenChen,Na-manGoyal,VishravChaudhary,JiataoGu,andAn-gelaFan.2020.Multilingualtranslationwithexten-siblemultilingualpretrainingandfinetuning.CoRR,abs/2008.00401.Ra\u00falV\u00e1zquez,YvesScherrer,SamiVirpioja,andJ\u00f6rgTiedemann.2021.TheHelsinkisubmissiontotheAmericasNLPsharedtask.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages255\u2013264,Online.AssociationforComputationalLinguis-tics.MitchellWortsman,GabrielIlharco,SamirYaGadre,RebeccaRoelofs,RaphaelGontijo-Lopes,AriSMor-cos,HongseokNamkoong,AliFarhadi,YairCar-mon,SimonKornblith,andLudwigSchmidt.2022.Modelsoups:averagingweightsofmultiplefine-tunedmodelsimprovesaccuracywithoutincreasinginferencetime.InProceedingsofthe39thInterna-tionalConferenceonMachineLearning,volume162ofProceedingsofMachineLearningResearch,pages23965\u201323998.PMLR.YigeXu,XipengQiu,LigaoZhou,andXuan-jingHuang.2020.ImprovingBERTfine-tuningviaself-ensembleandself-distillation.CoRR,abs/2002.10345.FrancisZheng,MachelReid,EdisonMarrese-Taylor,andYutakaMatsuo.2021.Low-resourcemachinetranslationusingcross-linguallanguagemodelpre-training.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLan-guagesoftheAmericas,pages234\u2013240,Online.As-sociationforComputationalLinguistics.\n176"}, {"question": " What is the average improvement in the ChrF++ score achieved by Multi+ compared to Multi?,answer: On average, Multi+ achieved a 0.0169 higher ChrF++ score than Multi.", "ref_chunk": "2https://github.com/KaieChen/ameircasnlp2023\n1https://scholarworks.iu.edu/dspace/handle/2022/21028qualityforlow-resourcelanguages,althoughitissensitivetothequalityofsyntheticdata.Toaddressthis,weintroducedadatafilteringtechniquetoim-provethequalityofsyntheticdata.Withfilteredbacktranslation,oursystemachievedanaverageimprovementof0.008intheChrF++score.Fur-thermore,ourstudyrevealsthatmultilingualfine-tuningachievescomparabletranslationqualitytobilingualfine-tuningforlow-resourcelanguages.Weselectedthebilingualmodelwithweightaveragingandbacktranslationasourfinalsubmis-sion.TheimplementationofthisstudyisavailableinourGitrepository2.2Methods2.1DataWeadoptedthedatapreparationmethoddescribedbytheUniversityofHelsinki\u2019ssubmissiontoAmer-icasNLP2021(V\u00e1zquezetal.,2021)foroursys-tem.ThedetailsofthedatasetcanbefoundinTable1.Ourmodeltrainingutilizedthefilteredparalleldata(referredtoasparalleldata),whichconsistedofthetrainingdataprovidedbytheorga-nizersaswellasadditionaldatacollectedbytheUniversityofHelsinki(V\u00e1zquezetal.,2021).Inordertogeneratesyntheticparalleldata(referredtoassyntheticdata),weemployedmonolingualdataandappliedbacktranslationtechniques(refertoSection2.3).Thedevelopmentdatawasusedformodelselectionpurposes.2.2Pre-trainedModelOurmodelsarebasedontheNLLB-600MSeq2Seqpre-trainingschemeintroducedbytheNLLBteam(NLLBTeametal.,2022).Fortokenization,weutilizetheSentencePiecetokenizer(KudoandRichardson,2018),followingtheNLLBconfig-uration.TheNLLBmodelwasinitiallytrainedon\nPlayGroundLowResourceMachineTranslationSystemforthe2023AmericasNLPSharedTaskTianruiGu,KaieChen,SiqiOuyang,LeiLiUniversityofCalifornia,SantaBarbara{tianruigu,kaiechen,siqiouyang,leili}@ucsb.eduAbstractThispaperpresentsPlayGround\u2019ssubmissiontotheAmericasNLP2023sharedtaskonma-chinetranslation(MT)intoindigenouslan-guages.WefinetunedNLLB-600M,amultilin-gualMTmodelpre-trainedonFlores-200,on10low-resourcelanguagedirectionsandexam-inedtheeffectivenessofweightaveragingandbacktranslation.Ourexperimentsshowedthatweightaveraging,onaverage,ledtoa0.0169improvementintheChrF++score.Addition-ally,wefoundthatbacktranslationresultedina0.008improvementintheChrF++score.1IntroductionWeparticipatedintheAmericasNLP2023(Ebrahimietal.,2023)sharedtaskwiththegoalofadvancingpreviousstudies(Mageretal.,2021)onindigenousAmericanlanguages.ThetaskistotranslateSpanishinto10indigenouslanguages,includingAshaninka,Aymara,Bribri,Guarani,H\u00f1\u00e4h\u00f1u,Nahuatl,Quechua,Raramuri,Shipibo-Konibo,andWixarika.Additionally,therewasanotherlanguage,Chatino1,forwhichwedidnotparticipatein.WestartedwiththemonolingualandbilingualdatafromMageretal.(2021)andfinetunedNLLB-600M,amultilingualpre-trainedMTmodelfromMeta\u2019sNoLanguageLeftBehind(NLLB)project(NLLBTeametal.,2022)bothbilinguallyandmul-tilingually.Ontopofthat,weemployedweightaveragingandbacktranslation.Forbacktransla-tion,weadditionallyfilteredthebacktranslatedsentencepairstoimprovethedataquality.Wedemonstratethattrainingonmodelweightsaveragedfrommultiplecheckpointsimprovestrans-lationquality,asindicatedbya0.0169increaseintheChrF++scoreonaverage,withoutrequiringad-ditionalcomputationresources.Additionally,wefoundthatbacktranslationcanenhancetranslation\n173 Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP), pages 173\u2013176 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\nKKXk=1\u0398t\u2212k)(2)where\u0398trepresentsthemodelparametersatepocht.Thistechniquesharessimilaritieswithtrainingdifferentmodelsusingvarioushyperparameters(Wortsmanetal.,2022;Xuetal.,2020).How-ever,asweonlyneedtotrainasinglemodel,thistechniquecanbeparticularlyefficientforlargelan-guagemodels.TheeffectivenessofthisapproachisfurtherdiscussedinSection3.2.3.3HyperparametersInthefine-tuningprocess,wefrozetheencoderlayersoftheNLLBmodel,consideringitspriortrainingonavastamountofSpanishsentences.WeoptimizedthemodelusingAdamW(LoshchilovandHutter,2017)withhyperparameters\u03b2=(0.9,0.999),\u03f5=10\u22126.Weemployedalearningrateof3\u00d710\u22124foratotalof10,000iterations.Forregularization,weutilizedthesamedropoutrateastheoriginalNLLBmodelandaweightdecayof0.01.Furthermore,forweightaveraging,wesetthevalueofKtobe5.2.4EvaluationWereporttheresultsusingChrF++(Popovi\u00b4c,2017),followingtheevaluationscript3providedbytheAmericasNLP2023sharedtask.ChrF++\nLangFilteredMonolingDev\n3https://github.com/AmericasNLP/americasnlp2023\nTable1:Numberofsegmentsindataset.FiltereddataandmonolingualdataarecollectedandfilteredbyUni-versityofHelsinkiteam(V\u00e1zquezetal.,2021)fromAmericasNLP2021.theFlores-200dataset,whichconsistsofAymara,Guarani,Quechua,andSpanish.2.3Fine-tunedModelsWefine-tuneNLLB-600Musingthedatamen-tionedinTable1.ForbothX-to-SpanishandSpanish-to-Xdirections,wefine-tuneNLLB-600Musingfilteredparalleldatainbothbilingualandmultilingualway.Thisproduces20bilingualmod-elsand2multilingualmodels.WeleveragetheaboveX-to-Spanishmodelstogeneratebacktranslateddatatoenrichthetrainingcorpus.Thenwefurtherfine-tunetheSpanish-to-Xmodelswithparalleldatasetextendedwithbacktranslatedsentencepairs.Thefinalmodelsareobtainedwithweightaver-agingsincethetrainingcanbeunstablewithinsuf-ficientdata.2.3.1BackTranslationInordertomakeuseofmonolingualdatainin-digenouslanguages,weemployedbacktransla-tion.Specifically,wefrozethedecoderlayersofNLLBmodelandperformedfine-tuningofanX-to-Spanishmodelusingparalleldata.Then,weutilizedthismodeltogeneratesyntheticsentences.Datafiltering:Syntheticsentencesmaycon-tainnoise.Toaddressthisissue,weimplementadatafiltertoselectasubsetofsyntheticsen-tencesthatwillexpandtheoriginalparalleldataset(Ranathungaetal.,2023).Inourtask,weinitiallyfine-tunedaSpanish-to-Xmodelusingtheparalleldata.Subsequently,weevaluatedthismodelonthesyntheticsentencesandselectedthetopNsampleswiththelowestcross-entropyloss.ThevalueofNisdeterminedbythefollowing:N=min(|Ypar|,|Ysyn|)(1)where|Ypar|representsthenumberofsegmentsintheparalleldataset,and|Ysyn|representsthenumberofsegmentsinthesyntheticdataset.Finally,wecombinedtheselectedsyntheticdatawiththeparalleldataandproceededtoperformadditionalfine-tuningoftheNLLBmodel.2.3.2WeightAveragingStudieshaveshownthataveragingtheweightsofmultiplefinetunedmodelscanenhanceaccuracy(Wortsmanetal.,2022).Inourtrainingapproach,theweightsofthenextepocharetrainedbasedontheaverageofthemodelweightsfromthepreviousKepochs.Forinference,wecomputethefinalmodelbyaveragingthemodelweightsfromthelastKepochs.Themodelcanbedefinedasfollows:NLLB(x;\u0398t)=NLLB(x;1\nAshaninka385813195883Aymara835216750996Bribri73030996Guarani1448340516995H\u00f1\u00e4h\u00f1u7049537599Nahuatl174319222672Quechua22862460399996Raramuri165290995Shipibo-Konibo2885423595996Wixarika11525511994\n174\n\n\nTable2:ResultinChrF++ondevelopdataset,exceptforbaselineandBi++(test).BaselinemodelisthebestsubmissionforAmericasNLP2021.Theeffectivenessofweightaveraging(Multi+andBi+)andbacktranslationiscompared(Multi++andBi++).Wealsocomparedtheperformanceofbilingual(Bi)andmultilingual(Multi).capturesthecharacter-levelperformance,makingitparticularlysuitableforevaluatingthepolysyn-theticpropertiesobservedinmanyindigenouslan-guages(Zhengetal.,2021).3ResultsTheresultsarepresentedinTable2forboththedevelopmentandtestdatasets.OurBi++modeldemonstratesimprovementsinfourlanguages:H\u00f1\u00e4h\u00f1u,Aymara,Ash\u00e1ninka,andQuechua,com-paredtotheBaselinemodelprovidedbytheorga-nizer.Ingeneral,thetrendsinresultsforthede-velopmentandtrainingdatasetsaresimilar,exceptforRar\u00e1muriandBribri.Thisdiscrepancymaybeattributedtothetestdatasetcontainingmoreunknowntokens,towhichourmodelissensitive.Previousstudy(Mageretal.,2021)haspri-marilyfocusedonfine-tuningbilingualmachinetranslationmodels.However,theresultsfromourMulti++andBi++modelsdemonstratethepromis-ingpotentialofmultilingualfine-tuning(Tangetal.,2020).Onaverage,theChrF++scoreforMulti++isonly0.0012lowerthanthatofBi++.Wealsocomparedtheeffectivenessofweightaveragingandbacktranslation.Weightaveragingimprovedtranslationsforalltargetlanguages.Onaverage,Multi+achievedaChrF++scorethatwas0.0169higherthanMulti.Theseresultsindicatethatoursimpletechniquecanenhancelow-resourcemachinetranslationwithoutrequiringadditionalcomputationalresources.However,theimpactofbacktranslationvariedacrosslanguages,asobservedintheresultsforMulti+andMulti++.Onaverage,theimplemen-tationofbacktranslationresultedina0.008im-provementintheChrF++metric.ForWixarikaandAymara,therewasaslightdropintheChrF++scoresafterbacktranslation.Despiteperformingdatafiltering,thequalityofsyntheticdatalargelydependsontheperformanceoftheX-to-Spanishmodel.Insummary,ourfine-tuningtechniquehasshownimprovementsinperformance.However,withfurtherrefinementsanddesignenhancements,thereispotentialforourmodeltoachievehigherlevelsofperformance.4ConclusionInthispaper,wepresentedoursubmissiontotheAmericasNLP2023sharedtask.Oursystemuti-lizedtheNLLB-600Mpre-trainedmodeltotrans-lateSpanishinto10indigenouslanguages.Wealsoinvestigatedthepotentialofmultilingualtranslationmodels,whichshowedpromisingresults.Addition-ally,wefoundthataveragingmodelweightsfrompreviousepochsprovedtobeanefficientandeffec-tiveapproach.Whilebacktranslationdemonstratedperformanceimprovements,furthermethodsarenecessarytoaddressnoisydata.Thesefindingshighlightthepositiveoutcomesofourstudyandprovidevaluableinsightsforfutureadvancementsinlow-resourcemachinetranslationtechniques.ReferencesAbteenEbrahimi,ManuelMager,ArturoOncevay,EnoraRice,CynthiaMonta\u00f1o,JohnOrtega,ShrutiRijhwani,AlexisPalmer,RolandoCoto-Solano,Hi-lariaCruz,andKatharinaKann.2023.FindingsoftheAmericasNLP2023sharedtaskonmachinetrans-lationintoindigenouslanguages.InProceedingsof\nTargetlanguageBaseline(Test)MultiMulti+Multi++BiBi++Bi++(Test)\nWixarika0.3040.2770.2940.2940.2660.2790.288H\u00f1\u00e4h\u00f1u0.1470.1290.1330.1380.1440.1410.148Aymara0.2830.2910.3280.3260.3360.3260.300Shipibo-Konibo0.3290.2240.2380.2530.2610.2830.277Nahuatl0.2660.2410.2520.2750.2820.2830.237Guarani0.3360.3040.3160.3210.3150.3030.331Ash\u00e1ninka0.2580.2220.2380.2720.2690.2860.280Quechua0.3430.3240.341-0.337-0.344Rar\u00e1muri0.1840.1610.175-0.184-0.145Bribri0.1650.2100.237-0.231-0.148\n175\n\n\ntheThirdWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas.Associa-tionforComputationalLinguistics.TakuKudoandJohnRichardson.2018.SentencePiece:Asimpleandlanguageindependentsubwordtok-enizeranddetokenizerforneuraltextprocessing.InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations,pages66\u201371,Brussels,Belgium.AssociationforComputationalLinguistics.IlyaLoshchilovandFrankHutter.2017.Fixingweightdecayregularizationinadam.CoRR,abs/1711.05101.ManuelMager,ArturoOncevay,AbteenEbrahimi,JohnOrtega,AnnetteRios,AngelaFan,XimenaGutierrez-Vasques,LuisChiruzzo,GustavoGim\u00e9nez-Lugo,Ri-cardoRamos,IvanVladimirMezaRuiz,RolandoCoto-Solano,AlexisPalmer,ElisabethMager-Hois,VishravChaudhary,GrahamNeubig,NgocThangVu,andKatharinaKann.2021.FindingsoftheAmeri-casNLP2021sharedtaskonopenmachinetransla-tionforindigenouslanguagesoftheAmericas.InProceedingsoftheFirstWorkshoponNaturalLan-guageProcessingforIndigenousLanguagesoftheAmericas,pages202\u2013217,Online.AssociationforComputationalLinguistics.NLLBTeam,MartaR.Costa-juss\u00e0,JamesCross,Onur\u00c7elebi,MahaElbayad,KennethHeafield,KevinHef-fernan,ElaheKalbassi,JaniceLam,DanielLicht,JeanMaillard,AnnaSun,SkylerWang,GuillaumeWenzek,AlYoungblood,BapiAkula,LoicBar-rault,GabrielMejiaGonzalez,PrangthipHansanti,JohnHoffman,SemarleyJarrett,KaushikRamSadagopan,DirkRowe,ShannonSpruit,ChauTran,PierreAndrews,NecipFazilAyan,ShrutiBhosale,SergeyEdunov,AngelaFan,CynthiaGao,VedanujGoswami,FranciscoGuzm\u00e1n,PhilippKoehn,AlexandreMourachko,ChristopheRopers,SafiyyahSaleem,HolgerSchwenk,andJeffWang.2022.Nolanguageleftbehind:Scalinghuman-centeredmachinetranslation.MajaPopovi\u00b4c.2017.chrF++:wordshelpingcharac-tern-grams.InProceedingsoftheSecondConfer-enceonMachineTranslation,pages612\u2013618,Copen-hagen,Denmark.AssociationforComputationalLin-guistics.SurangikaRanathunga,En-ShiunAnnieLee,MarjanaPriftiSkenduli,RaviShekhar,MehreenAlam,andRishemjitKaur.2023.Neuralmachinetranslationforlow-resourcelanguages:Asurvey.ACMComput.Surv.,55(11).YuqingTang,ChauTran,XianLi,Peng-JenChen,Na-manGoyal,VishravChaudhary,JiataoGu,andAn-gelaFan.2020.Multilingualtranslationwithexten-siblemultilingualpretrainingandfinetuning.CoRR,abs/2008.00401.Ra\u00falV\u00e1zquez,YvesScherrer,SamiVirpioja,andJ\u00f6rgTiedemann.2021.TheHelsinkisubmissiontotheAmericasNLPsharedtask.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages255\u2013264,Online.AssociationforComputationalLinguis-tics.MitchellWortsman,GabrielIlharco,SamirYaGadre,RebeccaRoelofs,RaphaelGontijo-Lopes,AriSMor-cos,HongseokNamkoong,AliFarhadi,YairCar-mon,SimonKornblith,andLudwigSchmidt.2022.Modelsoups:averagingweightsofmultiplefine-tunedmodelsimprovesaccuracywithoutincreasinginferencetime.InProceedingsofthe39thInterna-tionalConferenceonMachineLearning,volume162ofProceedingsofMachineLearningResearch,pages23965\u201323998.PMLR.YigeXu,XipengQiu,LigaoZhou,andXuan-jingHuang.2020.ImprovingBERTfine-tuningviaself-ensembleandself-distillation.CoRR,abs/2002.10345.FrancisZheng,MachelReid,EdisonMarrese-Taylor,andYutakaMatsuo.2021.Low-resourcemachinetranslationusingcross-linguallanguagemodelpre-training.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLan-guagesoftheAmericas,pages234\u2013240,Online.As-sociationforComputationalLinguistics.\n176"}], "doc_text": "2https://github.com/KaieChen/ameircasnlp2023\n1https://scholarworks.iu.edu/dspace/handle/2022/21028qualityforlow-resourcelanguages,althoughitissensitivetothequalityofsyntheticdata.Toaddressthis,weintroducedadatafilteringtechniquetoim-provethequalityofsyntheticdata.Withfilteredbacktranslation,oursystemachievedanaverageimprovementof0.008intheChrF++score.Fur-thermore,ourstudyrevealsthatmultilingualfine-tuningachievescomparabletranslationqualitytobilingualfine-tuningforlow-resourcelanguages.Weselectedthebilingualmodelwithweightaveragingandbacktranslationasourfinalsubmis-sion.TheimplementationofthisstudyisavailableinourGitrepository2.2Methods2.1DataWeadoptedthedatapreparationmethoddescribedbytheUniversityofHelsinki\u2019ssubmissiontoAmer-icasNLP2021(V\u00e1zquezetal.,2021)foroursys-tem.ThedetailsofthedatasetcanbefoundinTable1.Ourmodeltrainingutilizedthefilteredparalleldata(referredtoasparalleldata),whichconsistedofthetrainingdataprovidedbytheorga-nizersaswellasadditionaldatacollectedbytheUniversityofHelsinki(V\u00e1zquezetal.,2021).Inordertogeneratesyntheticparalleldata(referredtoassyntheticdata),weemployedmonolingualdataandappliedbacktranslationtechniques(refertoSection2.3).Thedevelopmentdatawasusedformodelselectionpurposes.2.2Pre-trainedModelOurmodelsarebasedontheNLLB-600MSeq2Seqpre-trainingschemeintroducedbytheNLLBteam(NLLBTeametal.,2022).Fortokenization,weutilizetheSentencePiecetokenizer(KudoandRichardson,2018),followingtheNLLBconfig-uration.TheNLLBmodelwasinitiallytrainedon\nPlayGroundLowResourceMachineTranslationSystemforthe2023AmericasNLPSharedTaskTianruiGu,KaieChen,SiqiOuyang,LeiLiUniversityofCalifornia,SantaBarbara{tianruigu,kaiechen,siqiouyang,leili}@ucsb.eduAbstractThispaperpresentsPlayGround\u2019ssubmissiontotheAmericasNLP2023sharedtaskonma-chinetranslation(MT)intoindigenouslan-guages.WefinetunedNLLB-600M,amultilin-gualMTmodelpre-trainedonFlores-200,on10low-resourcelanguagedirectionsandexam-inedtheeffectivenessofweightaveragingandbacktranslation.Ourexperimentsshowedthatweightaveraging,onaverage,ledtoa0.0169improvementintheChrF++score.Addition-ally,wefoundthatbacktranslationresultedina0.008improvementintheChrF++score.1IntroductionWeparticipatedintheAmericasNLP2023(Ebrahimietal.,2023)sharedtaskwiththegoalofadvancingpreviousstudies(Mageretal.,2021)onindigenousAmericanlanguages.ThetaskistotranslateSpanishinto10indigenouslanguages,includingAshaninka,Aymara,Bribri,Guarani,H\u00f1\u00e4h\u00f1u,Nahuatl,Quechua,Raramuri,Shipibo-Konibo,andWixarika.Additionally,therewasanotherlanguage,Chatino1,forwhichwedidnotparticipatein.WestartedwiththemonolingualandbilingualdatafromMageretal.(2021)andfinetunedNLLB-600M,amultilingualpre-trainedMTmodelfromMeta\u2019sNoLanguageLeftBehind(NLLB)project(NLLBTeametal.,2022)bothbilinguallyandmul-tilingually.Ontopofthat,weemployedweightaveragingandbacktranslation.Forbacktransla-tion,weadditionallyfilteredthebacktranslatedsentencepairstoimprovethedataquality.Wedemonstratethattrainingonmodelweightsaveragedfrommultiplecheckpointsimprovestrans-lationquality,asindicatedbya0.0169increaseintheChrF++scoreonaverage,withoutrequiringad-ditionalcomputationresources.Additionally,wefoundthatbacktranslationcanenhancetranslation\n173 Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP), pages 173\u2013176 July 14, 2023 \u00a92023 Association for Computational Linguistics\n\n\nKKXk=1\u0398t\u2212k)(2)where\u0398trepresentsthemodelparametersatepocht.Thistechniquesharessimilaritieswithtrainingdifferentmodelsusingvarioushyperparameters(Wortsmanetal.,2022;Xuetal.,2020).How-ever,asweonlyneedtotrainasinglemodel,thistechniquecanbeparticularlyefficientforlargelan-guagemodels.TheeffectivenessofthisapproachisfurtherdiscussedinSection3.2.3.3HyperparametersInthefine-tuningprocess,wefrozetheencoderlayersoftheNLLBmodel,consideringitspriortrainingonavastamountofSpanishsentences.WeoptimizedthemodelusingAdamW(LoshchilovandHutter,2017)withhyperparameters\u03b2=(0.9,0.999),\u03f5=10\u22126.Weemployedalearningrateof3\u00d710\u22124foratotalof10,000iterations.Forregularization,weutilizedthesamedropoutrateastheoriginalNLLBmodelandaweightdecayof0.01.Furthermore,forweightaveraging,wesetthevalueofKtobe5.2.4EvaluationWereporttheresultsusingChrF++(Popovi\u00b4c,2017),followingtheevaluationscript3providedbytheAmericasNLP2023sharedtask.ChrF++\nLangFilteredMonolingDev\n3https://github.com/AmericasNLP/americasnlp2023\nTable1:Numberofsegmentsindataset.FiltereddataandmonolingualdataarecollectedandfilteredbyUni-versityofHelsinkiteam(V\u00e1zquezetal.,2021)fromAmericasNLP2021.theFlores-200dataset,whichconsistsofAymara,Guarani,Quechua,andSpanish.2.3Fine-tunedModelsWefine-tuneNLLB-600Musingthedatamen-tionedinTable1.ForbothX-to-SpanishandSpanish-to-Xdirections,wefine-tuneNLLB-600Musingfilteredparalleldatainbothbilingualandmultilingualway.Thisproduces20bilingualmod-elsand2multilingualmodels.WeleveragetheaboveX-to-Spanishmodelstogeneratebacktranslateddatatoenrichthetrainingcorpus.Thenwefurtherfine-tunetheSpanish-to-Xmodelswithparalleldatasetextendedwithbacktranslatedsentencepairs.Thefinalmodelsareobtainedwithweightaver-agingsincethetrainingcanbeunstablewithinsuf-ficientdata.2.3.1BackTranslationInordertomakeuseofmonolingualdatainin-digenouslanguages,weemployedbacktransla-tion.Specifically,wefrozethedecoderlayersofNLLBmodelandperformedfine-tuningofanX-to-Spanishmodelusingparalleldata.Then,weutilizedthismodeltogeneratesyntheticsentences.Datafiltering:Syntheticsentencesmaycon-tainnoise.Toaddressthisissue,weimplementadatafiltertoselectasubsetofsyntheticsen-tencesthatwillexpandtheoriginalparalleldataset(Ranathungaetal.,2023).Inourtask,weinitiallyfine-tunedaSpanish-to-Xmodelusingtheparalleldata.Subsequently,weevaluatedthismodelonthesyntheticsentencesandselectedthetopNsampleswiththelowestcross-entropyloss.ThevalueofNisdeterminedbythefollowing:N=min(|Ypar|,|Ysyn|)(1)where|Ypar|representsthenumberofsegmentsintheparalleldataset,and|Ysyn|representsthenumberofsegmentsinthesyntheticdataset.Finally,wecombinedtheselectedsyntheticdatawiththeparalleldataandproceededtoperformadditionalfine-tuningoftheNLLBmodel.2.3.2WeightAveragingStudieshaveshownthataveragingtheweightsofmultiplefinetunedmodelscanenhanceaccuracy(Wortsmanetal.,2022).Inourtrainingapproach,theweightsofthenextepocharetrainedbasedontheaverageofthemodelweightsfromthepreviousKepochs.Forinference,wecomputethefinalmodelbyaveragingthemodelweightsfromthelastKepochs.Themodelcanbedefinedasfollows:NLLB(x;\u0398t)=NLLB(x;1\nAshaninka385813195883Aymara835216750996Bribri73030996Guarani1448340516995H\u00f1\u00e4h\u00f1u7049537599Nahuatl174319222672Quechua22862460399996Raramuri165290995Shipibo-Konibo2885423595996Wixarika11525511994\n174\n\n\nTable2:ResultinChrF++ondevelopdataset,exceptforbaselineandBi++(test).BaselinemodelisthebestsubmissionforAmericasNLP2021.Theeffectivenessofweightaveraging(Multi+andBi+)andbacktranslationiscompared(Multi++andBi++).Wealsocomparedtheperformanceofbilingual(Bi)andmultilingual(Multi).capturesthecharacter-levelperformance,makingitparticularlysuitableforevaluatingthepolysyn-theticpropertiesobservedinmanyindigenouslan-guages(Zhengetal.,2021).3ResultsTheresultsarepresentedinTable2forboththedevelopmentandtestdatasets.OurBi++modeldemonstratesimprovementsinfourlanguages:H\u00f1\u00e4h\u00f1u,Aymara,Ash\u00e1ninka,andQuechua,com-paredtotheBaselinemodelprovidedbytheorga-nizer.Ingeneral,thetrendsinresultsforthede-velopmentandtrainingdatasetsaresimilar,exceptforRar\u00e1muriandBribri.Thisdiscrepancymaybeattributedtothetestdatasetcontainingmoreunknowntokens,towhichourmodelissensitive.Previousstudy(Mageretal.,2021)haspri-marilyfocusedonfine-tuningbilingualmachinetranslationmodels.However,theresultsfromourMulti++andBi++modelsdemonstratethepromis-ingpotentialofmultilingualfine-tuning(Tangetal.,2020).Onaverage,theChrF++scoreforMulti++isonly0.0012lowerthanthatofBi++.Wealsocomparedtheeffectivenessofweightaveragingandbacktranslation.Weightaveragingimprovedtranslationsforalltargetlanguages.Onaverage,Multi+achievedaChrF++scorethatwas0.0169higherthanMulti.Theseresultsindicatethatoursimpletechniquecanenhancelow-resourcemachinetranslationwithoutrequiringadditionalcomputationalresources.However,theimpactofbacktranslationvariedacrosslanguages,asobservedintheresultsforMulti+andMulti++.Onaverage,theimplemen-tationofbacktranslationresultedina0.008im-provementintheChrF++metric.ForWixarikaandAymara,therewasaslightdropintheChrF++scoresafterbacktranslation.Despiteperformingdatafiltering,thequalityofsyntheticdatalargelydependsontheperformanceoftheX-to-Spanishmodel.Insummary,ourfine-tuningtechniquehasshownimprovementsinperformance.However,withfurtherrefinementsanddesignenhancements,thereispotentialforourmodeltoachievehigherlevelsofperformance.4ConclusionInthispaper,wepresentedoursubmissiontotheAmericasNLP2023sharedtask.Oursystemuti-lizedtheNLLB-600Mpre-trainedmodeltotrans-lateSpanishinto10indigenouslanguages.Wealsoinvestigatedthepotentialofmultilingualtranslationmodels,whichshowedpromisingresults.Addition-ally,wefoundthataveragingmodelweightsfrompreviousepochsprovedtobeanefficientandeffec-tiveapproach.Whilebacktranslationdemonstratedperformanceimprovements,furthermethodsarenecessarytoaddressnoisydata.Thesefindingshighlightthepositiveoutcomesofourstudyandprovidevaluableinsightsforfutureadvancementsinlow-resourcemachinetranslationtechniques.ReferencesAbteenEbrahimi,ManuelMager,ArturoOncevay,EnoraRice,CynthiaMonta\u00f1o,JohnOrtega,ShrutiRijhwani,AlexisPalmer,RolandoCoto-Solano,Hi-lariaCruz,andKatharinaKann.2023.FindingsoftheAmericasNLP2023sharedtaskonmachinetrans-lationintoindigenouslanguages.InProceedingsof\nTargetlanguageBaseline(Test)MultiMulti+Multi++BiBi++Bi++(Test)\nWixarika0.3040.2770.2940.2940.2660.2790.288H\u00f1\u00e4h\u00f1u0.1470.1290.1330.1380.1440.1410.148Aymara0.2830.2910.3280.3260.3360.3260.300Shipibo-Konibo0.3290.2240.2380.2530.2610.2830.277Nahuatl0.2660.2410.2520.2750.2820.2830.237Guarani0.3360.3040.3160.3210.3150.3030.331Ash\u00e1ninka0.2580.2220.2380.2720.2690.2860.280Quechua0.3430.3240.341-0.337-0.344Rar\u00e1muri0.1840.1610.175-0.184-0.145Bribri0.1650.2100.237-0.231-0.148\n175\n\n\ntheThirdWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas.Associa-tionforComputationalLinguistics.TakuKudoandJohnRichardson.2018.SentencePiece:Asimpleandlanguageindependentsubwordtok-enizeranddetokenizerforneuraltextprocessing.InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations,pages66\u201371,Brussels,Belgium.AssociationforComputationalLinguistics.IlyaLoshchilovandFrankHutter.2017.Fixingweightdecayregularizationinadam.CoRR,abs/1711.05101.ManuelMager,ArturoOncevay,AbteenEbrahimi,JohnOrtega,AnnetteRios,AngelaFan,XimenaGutierrez-Vasques,LuisChiruzzo,GustavoGim\u00e9nez-Lugo,Ri-cardoRamos,IvanVladimirMezaRuiz,RolandoCoto-Solano,AlexisPalmer,ElisabethMager-Hois,VishravChaudhary,GrahamNeubig,NgocThangVu,andKatharinaKann.2021.FindingsoftheAmeri-casNLP2021sharedtaskonopenmachinetransla-tionforindigenouslanguagesoftheAmericas.InProceedingsoftheFirstWorkshoponNaturalLan-guageProcessingforIndigenousLanguagesoftheAmericas,pages202\u2013217,Online.AssociationforComputationalLinguistics.NLLBTeam,MartaR.Costa-juss\u00e0,JamesCross,Onur\u00c7elebi,MahaElbayad,KennethHeafield,KevinHef-fernan,ElaheKalbassi,JaniceLam,DanielLicht,JeanMaillard,AnnaSun,SkylerWang,GuillaumeWenzek,AlYoungblood,BapiAkula,LoicBar-rault,GabrielMejiaGonzalez,PrangthipHansanti,JohnHoffman,SemarleyJarrett,KaushikRamSadagopan,DirkRowe,ShannonSpruit,ChauTran,PierreAndrews,NecipFazilAyan,ShrutiBhosale,SergeyEdunov,AngelaFan,CynthiaGao,VedanujGoswami,FranciscoGuzm\u00e1n,PhilippKoehn,AlexandreMourachko,ChristopheRopers,SafiyyahSaleem,HolgerSchwenk,andJeffWang.2022.Nolanguageleftbehind:Scalinghuman-centeredmachinetranslation.MajaPopovi\u00b4c.2017.chrF++:wordshelpingcharac-tern-grams.InProceedingsoftheSecondConfer-enceonMachineTranslation,pages612\u2013618,Copen-hagen,Denmark.AssociationforComputationalLin-guistics.SurangikaRanathunga,En-ShiunAnnieLee,MarjanaPriftiSkenduli,RaviShekhar,MehreenAlam,andRishemjitKaur.2023.Neuralmachinetranslationforlow-resourcelanguages:Asurvey.ACMComput.Surv.,55(11).YuqingTang,ChauTran,XianLi,Peng-JenChen,Na-manGoyal,VishravChaudhary,JiataoGu,andAn-gelaFan.2020.Multilingualtranslationwithexten-siblemultilingualpretrainingandfinetuning.CoRR,abs/2008.00401.Ra\u00falV\u00e1zquez,YvesScherrer,SamiVirpioja,andJ\u00f6rgTiedemann.2021.TheHelsinkisubmissiontotheAmericasNLPsharedtask.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages255\u2013264,Online.AssociationforComputationalLinguis-tics.MitchellWortsman,GabrielIlharco,SamirYaGadre,RebeccaRoelofs,RaphaelGontijo-Lopes,AriSMor-cos,HongseokNamkoong,AliFarhadi,YairCar-mon,SimonKornblith,andLudwigSchmidt.2022.Modelsoups:averagingweightsofmultiplefine-tunedmodelsimprovesaccuracywithoutincreasinginferencetime.InProceedingsofthe39thInterna-tionalConferenceonMachineLearning,volume162ofProceedingsofMachineLearningResearch,pages23965\u201323998.PMLR.YigeXu,XipengQiu,LigaoZhou,andXuan-jingHuang.2020.ImprovingBERTfine-tuningviaself-ensembleandself-distillation.CoRR,abs/2002.10345.FrancisZheng,MachelReid,EdisonMarrese-Taylor,andYutakaMatsuo.2021.Low-resourcemachinetranslationusingcross-linguallanguagemodelpre-training.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLan-guagesoftheAmericas,pages234\u2013240,Online.As-sociationforComputationalLinguistics.\n176"}