{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_COBRA_Frames:_Contextual_Reasoning_about_Effects_and_Harms_of_Offensive_Statements_chunk_7.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the focus of the investigation mentioned in the text?", "answer": " The focus is on a US-centric perspective.", "ref_chunk": "essentializing groups. English Only We only look at a US-centric per- spective in our investigation. Obviously, online hate and abuse is manifested in many languages (Arango Monnar et al., 2022), so we hope future work will adapt our frames to different languages and different cultures. Subjectivity in Offensiveness Not everyone agrees that things are offensive, or has the same interpretation of offensiveness (depending on their own background and beliefs; Sap et al., 2022). Our in-context prompts and qualification likely make both our machine-generated explanations and hu- man annotations prescriptive (R\u00f6ttger et al., 2021), in contrast to a more descriptive approach where we would examine different interpretations. We leave that up for future work. Dual Use We aim to combat the negative effects and harms of discriminatory language on already marginalized people (Sap et al., 2019b; Davidson et al., 2019). It is possible however that our frames, dataset, and models could be used to perpetuate harm against those very people. We do not endorse the use of our data for those purposes. Risk of Suppressing Speech Our frames, dataset, and models are built with content moderation in mind, as online spaces are increasingly riddled with hate and abuse and content moderators are strug- gling to sift through all of the content. We hope future work will examine frameworks for using our frames to help content moderators. We do not endorse the use of our system to suppress speech without human oversight and encourage practition- ers to take non-censorship-oriented approaches to content moderation (e.g., counterspeech (Tekiro\u02d8glu et al., 2022)). Harms of Exposing Workers to Toxic Content The verification process of COBRACORPUS and COBRACORPUS-CF is performed by human anno- tators. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We mitigated these by designing minimum annotation workload, paying workers above minimum wage ($7-12), and providing them with crisis manage- ment resources. Our annotation work is also super- vised by an Institutional Review Board (IRB). Acknowledgements First of all, we thank our workers on MTurk for their hard work and thoughtful responses. We thank the anonymous reviewers for their helpful com- ments. We also thank Shengyu Feng and mem- bers of the CMU LTI COMEDY group for their feedback, and OpenAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fundamental AI Research Laboratories (FAIR) \u201cDynabench Data Collection and Benchmarking Platform\u201d award \u201cContExTox: Context-Aware and Explainable Toxicity Detection,\u201d and CISCO Ethics in AI award \u201cExpHarm: So- cially Aware, Ethically Informed, and Explanation- Centric AI Systems.\u201d References Herman Aguinis, Isabel Villamor, and Ravi S. Ramani. 2021. Mturk research: Review and recommendations. Journal of Management, 47(4):823\u2013837. Ayme Arango Monnar, Jorge Perez, Barbara Poblete, Magdalena Salda\u00f1a, and Valentina Proust. 2022. Re- sources for multilingual hate speech detection. In Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH). Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics. Emily M. Bender, Timnit Gebru, Angelina McMillan- Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Confer- ence on Fairness, Accountability, and Transparency, FAccT \u201921. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. Enrico Bunde. 2021. AI-Assisted and explainable hate speech detection for social media Moderators\u2013A de- sign science approach. In Proceedings of the 54th Hawaii International Conference on System Sciences. Enrico Bunde. 2023. AI-Assisted and Ex- for Detection https:// Hate plainable Social Media Moderators. scholarspace.manoa.hawaii.edu/items/ f21c8b34-5d62-40d0-919f-a4e07cfbbc32. [Accessed 13-May-2023]. Speech Kai-Wei Chang, He He, Robin Jia, and Sameer Singh. 2021. Robustness and adversarial examples in natu- ral language processing. In Proc. of EMNLP. Jacqueline M Chen and Kate A Ratliff. 2018. Psycho- logical essentialism predicts intergroup bias. Social Cognition, 36(3):301\u2013323. Yejin Choi. 2022. The curious case of commonsense intelligence. Daedalus. Hyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. ArXiv preprint. Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that\u2019s \u2018human\u2019 is not gold: Evaluating human evaluation of generated text. In Proc. of ACL. Gloria Cowan and Cyndi Hodge. 1996. Judgments of Hate Speech: The Effects of Target Group, Public- ness, and Behavioral Responses of the Target. Jour- nal of Applied Social Psychology. Gloria Cowan and Jon Mettrick. 2002. The effects of Target Variables and Settting on Perceptions of Hate Speech1. Journal of Applied Social Psychology. Thomas Davidson, Debasmita Bhattacharya, and Ing- mar Weber. 2019. Racial bias in hate speech and abusive language detection datasets. In Proceedings of the Third Workshop on Abusive Language Online. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated Hate Speech De- tection and the Problem of Offensive Language. In Proceedings of the 11th International Conference on Web and Social Media (ICWSM). Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency. Marta Dynel. 2015. The landscape of impoliteness research. Journal of Politeness Research. Nina Eliasoph and Paul Lichterman. 2003. Culture in Interaction. American Journal of Sociology. Mai ElSherief, Caleb Ziems, David Muchlinski, Vaish- navi Anupindi, Jordyn Seybolt, Munmun De Choud- hury, and Diyi Yang. 2021. Latent hatred: A bench- mark for understanding implicit hate speech. In Proc. of EMNLP. Charles J. Fillmore. 1976. Frame semantics and the na- ture of language*. Annals of the New York Academy of Sciences. Gary Alan Fine. 1979. Small Groups and Culture Cre- ation: The Idioculture of Little League Baseball Teams. American Sociological Review. Susan T Fiske. 2017. Prejudices in cultural contexts:"}, {"question": " Why is it important to adapt the frames to different languages and cultures?", "answer": " Because online hate and abuse is manifested in many languages.", "ref_chunk": "essentializing groups. English Only We only look at a US-centric per- spective in our investigation. Obviously, online hate and abuse is manifested in many languages (Arango Monnar et al., 2022), so we hope future work will adapt our frames to different languages and different cultures. Subjectivity in Offensiveness Not everyone agrees that things are offensive, or has the same interpretation of offensiveness (depending on their own background and beliefs; Sap et al., 2022). Our in-context prompts and qualification likely make both our machine-generated explanations and hu- man annotations prescriptive (R\u00f6ttger et al., 2021), in contrast to a more descriptive approach where we would examine different interpretations. We leave that up for future work. Dual Use We aim to combat the negative effects and harms of discriminatory language on already marginalized people (Sap et al., 2019b; Davidson et al., 2019). It is possible however that our frames, dataset, and models could be used to perpetuate harm against those very people. We do not endorse the use of our data for those purposes. Risk of Suppressing Speech Our frames, dataset, and models are built with content moderation in mind, as online spaces are increasingly riddled with hate and abuse and content moderators are strug- gling to sift through all of the content. We hope future work will examine frameworks for using our frames to help content moderators. We do not endorse the use of our system to suppress speech without human oversight and encourage practition- ers to take non-censorship-oriented approaches to content moderation (e.g., counterspeech (Tekiro\u02d8glu et al., 2022)). Harms of Exposing Workers to Toxic Content The verification process of COBRACORPUS and COBRACORPUS-CF is performed by human anno- tators. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We mitigated these by designing minimum annotation workload, paying workers above minimum wage ($7-12), and providing them with crisis manage- ment resources. Our annotation work is also super- vised by an Institutional Review Board (IRB). Acknowledgements First of all, we thank our workers on MTurk for their hard work and thoughtful responses. We thank the anonymous reviewers for their helpful com- ments. We also thank Shengyu Feng and mem- bers of the CMU LTI COMEDY group for their feedback, and OpenAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fundamental AI Research Laboratories (FAIR) \u201cDynabench Data Collection and Benchmarking Platform\u201d award \u201cContExTox: Context-Aware and Explainable Toxicity Detection,\u201d and CISCO Ethics in AI award \u201cExpHarm: So- cially Aware, Ethically Informed, and Explanation- Centric AI Systems.\u201d References Herman Aguinis, Isabel Villamor, and Ravi S. Ramani. 2021. Mturk research: Review and recommendations. Journal of Management, 47(4):823\u2013837. Ayme Arango Monnar, Jorge Perez, Barbara Poblete, Magdalena Salda\u00f1a, and Valentina Proust. 2022. Re- sources for multilingual hate speech detection. In Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH). Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics. Emily M. Bender, Timnit Gebru, Angelina McMillan- Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Confer- ence on Fairness, Accountability, and Transparency, FAccT \u201921. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. Enrico Bunde. 2021. AI-Assisted and explainable hate speech detection for social media Moderators\u2013A de- sign science approach. In Proceedings of the 54th Hawaii International Conference on System Sciences. Enrico Bunde. 2023. AI-Assisted and Ex- for Detection https:// Hate plainable Social Media Moderators. scholarspace.manoa.hawaii.edu/items/ f21c8b34-5d62-40d0-919f-a4e07cfbbc32. [Accessed 13-May-2023]. Speech Kai-Wei Chang, He He, Robin Jia, and Sameer Singh. 2021. Robustness and adversarial examples in natu- ral language processing. In Proc. of EMNLP. Jacqueline M Chen and Kate A Ratliff. 2018. Psycho- logical essentialism predicts intergroup bias. Social Cognition, 36(3):301\u2013323. Yejin Choi. 2022. The curious case of commonsense intelligence. Daedalus. Hyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. ArXiv preprint. Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that\u2019s \u2018human\u2019 is not gold: Evaluating human evaluation of generated text. In Proc. of ACL. Gloria Cowan and Cyndi Hodge. 1996. Judgments of Hate Speech: The Effects of Target Group, Public- ness, and Behavioral Responses of the Target. Jour- nal of Applied Social Psychology. Gloria Cowan and Jon Mettrick. 2002. The effects of Target Variables and Settting on Perceptions of Hate Speech1. Journal of Applied Social Psychology. Thomas Davidson, Debasmita Bhattacharya, and Ing- mar Weber. 2019. Racial bias in hate speech and abusive language detection datasets. In Proceedings of the Third Workshop on Abusive Language Online. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated Hate Speech De- tection and the Problem of Offensive Language. In Proceedings of the 11th International Conference on Web and Social Media (ICWSM). Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency. Marta Dynel. 2015. The landscape of impoliteness research. Journal of Politeness Research. Nina Eliasoph and Paul Lichterman. 2003. Culture in Interaction. American Journal of Sociology. Mai ElSherief, Caleb Ziems, David Muchlinski, Vaish- navi Anupindi, Jordyn Seybolt, Munmun De Choud- hury, and Diyi Yang. 2021. Latent hatred: A bench- mark for understanding implicit hate speech. In Proc. of EMNLP. Charles J. Fillmore. 1976. Frame semantics and the na- ture of language*. Annals of the New York Academy of Sciences. Gary Alan Fine. 1979. Small Groups and Culture Cre- ation: The Idioculture of Little League Baseball Teams. American Sociological Review. Susan T Fiske. 2017. Prejudices in cultural contexts:"}, {"question": " According to Sap et al., why do not all individuals agree on what is considered offensive?", "answer": " Different individuals have different interpretations based on their background and beliefs.", "ref_chunk": "essentializing groups. English Only We only look at a US-centric per- spective in our investigation. Obviously, online hate and abuse is manifested in many languages (Arango Monnar et al., 2022), so we hope future work will adapt our frames to different languages and different cultures. Subjectivity in Offensiveness Not everyone agrees that things are offensive, or has the same interpretation of offensiveness (depending on their own background and beliefs; Sap et al., 2022). Our in-context prompts and qualification likely make both our machine-generated explanations and hu- man annotations prescriptive (R\u00f6ttger et al., 2021), in contrast to a more descriptive approach where we would examine different interpretations. We leave that up for future work. Dual Use We aim to combat the negative effects and harms of discriminatory language on already marginalized people (Sap et al., 2019b; Davidson et al., 2019). It is possible however that our frames, dataset, and models could be used to perpetuate harm against those very people. We do not endorse the use of our data for those purposes. Risk of Suppressing Speech Our frames, dataset, and models are built with content moderation in mind, as online spaces are increasingly riddled with hate and abuse and content moderators are strug- gling to sift through all of the content. We hope future work will examine frameworks for using our frames to help content moderators. We do not endorse the use of our system to suppress speech without human oversight and encourage practition- ers to take non-censorship-oriented approaches to content moderation (e.g., counterspeech (Tekiro\u02d8glu et al., 2022)). Harms of Exposing Workers to Toxic Content The verification process of COBRACORPUS and COBRACORPUS-CF is performed by human anno- tators. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We mitigated these by designing minimum annotation workload, paying workers above minimum wage ($7-12), and providing them with crisis manage- ment resources. Our annotation work is also super- vised by an Institutional Review Board (IRB). Acknowledgements First of all, we thank our workers on MTurk for their hard work and thoughtful responses. We thank the anonymous reviewers for their helpful com- ments. We also thank Shengyu Feng and mem- bers of the CMU LTI COMEDY group for their feedback, and OpenAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fundamental AI Research Laboratories (FAIR) \u201cDynabench Data Collection and Benchmarking Platform\u201d award \u201cContExTox: Context-Aware and Explainable Toxicity Detection,\u201d and CISCO Ethics in AI award \u201cExpHarm: So- cially Aware, Ethically Informed, and Explanation- Centric AI Systems.\u201d References Herman Aguinis, Isabel Villamor, and Ravi S. Ramani. 2021. Mturk research: Review and recommendations. Journal of Management, 47(4):823\u2013837. Ayme Arango Monnar, Jorge Perez, Barbara Poblete, Magdalena Salda\u00f1a, and Valentina Proust. 2022. Re- sources for multilingual hate speech detection. In Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH). Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics. Emily M. Bender, Timnit Gebru, Angelina McMillan- Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Confer- ence on Fairness, Accountability, and Transparency, FAccT \u201921. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. Enrico Bunde. 2021. AI-Assisted and explainable hate speech detection for social media Moderators\u2013A de- sign science approach. In Proceedings of the 54th Hawaii International Conference on System Sciences. Enrico Bunde. 2023. AI-Assisted and Ex- for Detection https:// Hate plainable Social Media Moderators. scholarspace.manoa.hawaii.edu/items/ f21c8b34-5d62-40d0-919f-a4e07cfbbc32. [Accessed 13-May-2023]. Speech Kai-Wei Chang, He He, Robin Jia, and Sameer Singh. 2021. Robustness and adversarial examples in natu- ral language processing. In Proc. of EMNLP. Jacqueline M Chen and Kate A Ratliff. 2018. Psycho- logical essentialism predicts intergroup bias. Social Cognition, 36(3):301\u2013323. Yejin Choi. 2022. The curious case of commonsense intelligence. Daedalus. Hyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. ArXiv preprint. Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that\u2019s \u2018human\u2019 is not gold: Evaluating human evaluation of generated text. In Proc. of ACL. Gloria Cowan and Cyndi Hodge. 1996. Judgments of Hate Speech: The Effects of Target Group, Public- ness, and Behavioral Responses of the Target. Jour- nal of Applied Social Psychology. Gloria Cowan and Jon Mettrick. 2002. The effects of Target Variables and Settting on Perceptions of Hate Speech1. Journal of Applied Social Psychology. Thomas Davidson, Debasmita Bhattacharya, and Ing- mar Weber. 2019. Racial bias in hate speech and abusive language detection datasets. In Proceedings of the Third Workshop on Abusive Language Online. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated Hate Speech De- tection and the Problem of Offensive Language. In Proceedings of the 11th International Conference on Web and Social Media (ICWSM). Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency. Marta Dynel. 2015. The landscape of impoliteness research. Journal of Politeness Research. Nina Eliasoph and Paul Lichterman. 2003. Culture in Interaction. American Journal of Sociology. Mai ElSherief, Caleb Ziems, David Muchlinski, Vaish- navi Anupindi, Jordyn Seybolt, Munmun De Choud- hury, and Diyi Yang. 2021. Latent hatred: A bench- mark for understanding implicit hate speech. In Proc. of EMNLP. Charles J. Fillmore. 1976. Frame semantics and the na- ture of language*. Annals of the New York Academy of Sciences. Gary Alan Fine. 1979. Small Groups and Culture Cre- ation: The Idioculture of Little League Baseball Teams. American Sociological Review. Susan T Fiske. 2017. Prejudices in cultural contexts:"}, {"question": " How are the machine-generated explanations and human annotations described in the text?", "answer": " They are described as prescriptive.", "ref_chunk": "essentializing groups. English Only We only look at a US-centric per- spective in our investigation. Obviously, online hate and abuse is manifested in many languages (Arango Monnar et al., 2022), so we hope future work will adapt our frames to different languages and different cultures. Subjectivity in Offensiveness Not everyone agrees that things are offensive, or has the same interpretation of offensiveness (depending on their own background and beliefs; Sap et al., 2022). Our in-context prompts and qualification likely make both our machine-generated explanations and hu- man annotations prescriptive (R\u00f6ttger et al., 2021), in contrast to a more descriptive approach where we would examine different interpretations. We leave that up for future work. Dual Use We aim to combat the negative effects and harms of discriminatory language on already marginalized people (Sap et al., 2019b; Davidson et al., 2019). It is possible however that our frames, dataset, and models could be used to perpetuate harm against those very people. We do not endorse the use of our data for those purposes. Risk of Suppressing Speech Our frames, dataset, and models are built with content moderation in mind, as online spaces are increasingly riddled with hate and abuse and content moderators are strug- gling to sift through all of the content. We hope future work will examine frameworks for using our frames to help content moderators. We do not endorse the use of our system to suppress speech without human oversight and encourage practition- ers to take non-censorship-oriented approaches to content moderation (e.g., counterspeech (Tekiro\u02d8glu et al., 2022)). Harms of Exposing Workers to Toxic Content The verification process of COBRACORPUS and COBRACORPUS-CF is performed by human anno- tators. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We mitigated these by designing minimum annotation workload, paying workers above minimum wage ($7-12), and providing them with crisis manage- ment resources. Our annotation work is also super- vised by an Institutional Review Board (IRB). Acknowledgements First of all, we thank our workers on MTurk for their hard work and thoughtful responses. We thank the anonymous reviewers for their helpful com- ments. We also thank Shengyu Feng and mem- bers of the CMU LTI COMEDY group for their feedback, and OpenAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fundamental AI Research Laboratories (FAIR) \u201cDynabench Data Collection and Benchmarking Platform\u201d award \u201cContExTox: Context-Aware and Explainable Toxicity Detection,\u201d and CISCO Ethics in AI award \u201cExpHarm: So- cially Aware, Ethically Informed, and Explanation- Centric AI Systems.\u201d References Herman Aguinis, Isabel Villamor, and Ravi S. Ramani. 2021. Mturk research: Review and recommendations. Journal of Management, 47(4):823\u2013837. Ayme Arango Monnar, Jorge Perez, Barbara Poblete, Magdalena Salda\u00f1a, and Valentina Proust. 2022. Re- sources for multilingual hate speech detection. In Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH). Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics. Emily M. Bender, Timnit Gebru, Angelina McMillan- Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Confer- ence on Fairness, Accountability, and Transparency, FAccT \u201921. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. Enrico Bunde. 2021. AI-Assisted and explainable hate speech detection for social media Moderators\u2013A de- sign science approach. In Proceedings of the 54th Hawaii International Conference on System Sciences. Enrico Bunde. 2023. AI-Assisted and Ex- for Detection https:// Hate plainable Social Media Moderators. scholarspace.manoa.hawaii.edu/items/ f21c8b34-5d62-40d0-919f-a4e07cfbbc32. [Accessed 13-May-2023]. Speech Kai-Wei Chang, He He, Robin Jia, and Sameer Singh. 2021. Robustness and adversarial examples in natu- ral language processing. In Proc. of EMNLP. Jacqueline M Chen and Kate A Ratliff. 2018. Psycho- logical essentialism predicts intergroup bias. Social Cognition, 36(3):301\u2013323. Yejin Choi. 2022. The curious case of commonsense intelligence. Daedalus. Hyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. ArXiv preprint. Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that\u2019s \u2018human\u2019 is not gold: Evaluating human evaluation of generated text. In Proc. of ACL. Gloria Cowan and Cyndi Hodge. 1996. Judgments of Hate Speech: The Effects of Target Group, Public- ness, and Behavioral Responses of the Target. Jour- nal of Applied Social Psychology. Gloria Cowan and Jon Mettrick. 2002. The effects of Target Variables and Settting on Perceptions of Hate Speech1. Journal of Applied Social Psychology. Thomas Davidson, Debasmita Bhattacharya, and Ing- mar Weber. 2019. Racial bias in hate speech and abusive language detection datasets. In Proceedings of the Third Workshop on Abusive Language Online. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated Hate Speech De- tection and the Problem of Offensive Language. In Proceedings of the 11th International Conference on Web and Social Media (ICWSM). Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency. Marta Dynel. 2015. The landscape of impoliteness research. Journal of Politeness Research. Nina Eliasoph and Paul Lichterman. 2003. Culture in Interaction. American Journal of Sociology. Mai ElSherief, Caleb Ziems, David Muchlinski, Vaish- navi Anupindi, Jordyn Seybolt, Munmun De Choud- hury, and Diyi Yang. 2021. Latent hatred: A bench- mark for understanding implicit hate speech. In Proc. of EMNLP. Charles J. Fillmore. 1976. Frame semantics and the na- ture of language*. Annals of the New York Academy of Sciences. Gary Alan Fine. 1979. Small Groups and Culture Cre- ation: The Idioculture of Little League Baseball Teams. American Sociological Review. Susan T Fiske. 2017. Prejudices in cultural contexts:"}, {"question": " What is the aim of combating discriminatory language mentioned in the text?", "answer": " To combat the negative effects and harms on marginalized people.", "ref_chunk": "essentializing groups. English Only We only look at a US-centric per- spective in our investigation. Obviously, online hate and abuse is manifested in many languages (Arango Monnar et al., 2022), so we hope future work will adapt our frames to different languages and different cultures. Subjectivity in Offensiveness Not everyone agrees that things are offensive, or has the same interpretation of offensiveness (depending on their own background and beliefs; Sap et al., 2022). Our in-context prompts and qualification likely make both our machine-generated explanations and hu- man annotations prescriptive (R\u00f6ttger et al., 2021), in contrast to a more descriptive approach where we would examine different interpretations. We leave that up for future work. Dual Use We aim to combat the negative effects and harms of discriminatory language on already marginalized people (Sap et al., 2019b; Davidson et al., 2019). It is possible however that our frames, dataset, and models could be used to perpetuate harm against those very people. We do not endorse the use of our data for those purposes. Risk of Suppressing Speech Our frames, dataset, and models are built with content moderation in mind, as online spaces are increasingly riddled with hate and abuse and content moderators are strug- gling to sift through all of the content. We hope future work will examine frameworks for using our frames to help content moderators. We do not endorse the use of our system to suppress speech without human oversight and encourage practition- ers to take non-censorship-oriented approaches to content moderation (e.g., counterspeech (Tekiro\u02d8glu et al., 2022)). Harms of Exposing Workers to Toxic Content The verification process of COBRACORPUS and COBRACORPUS-CF is performed by human anno- tators. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We mitigated these by designing minimum annotation workload, paying workers above minimum wage ($7-12), and providing them with crisis manage- ment resources. Our annotation work is also super- vised by an Institutional Review Board (IRB). Acknowledgements First of all, we thank our workers on MTurk for their hard work and thoughtful responses. We thank the anonymous reviewers for their helpful com- ments. We also thank Shengyu Feng and mem- bers of the CMU LTI COMEDY group for their feedback, and OpenAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fundamental AI Research Laboratories (FAIR) \u201cDynabench Data Collection and Benchmarking Platform\u201d award \u201cContExTox: Context-Aware and Explainable Toxicity Detection,\u201d and CISCO Ethics in AI award \u201cExpHarm: So- cially Aware, Ethically Informed, and Explanation- Centric AI Systems.\u201d References Herman Aguinis, Isabel Villamor, and Ravi S. Ramani. 2021. Mturk research: Review and recommendations. Journal of Management, 47(4):823\u2013837. Ayme Arango Monnar, Jorge Perez, Barbara Poblete, Magdalena Salda\u00f1a, and Valentina Proust. 2022. Re- sources for multilingual hate speech detection. In Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH). Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics. Emily M. Bender, Timnit Gebru, Angelina McMillan- Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Confer- ence on Fairness, Accountability, and Transparency, FAccT \u201921. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. Enrico Bunde. 2021. AI-Assisted and explainable hate speech detection for social media Moderators\u2013A de- sign science approach. In Proceedings of the 54th Hawaii International Conference on System Sciences. Enrico Bunde. 2023. AI-Assisted and Ex- for Detection https:// Hate plainable Social Media Moderators. scholarspace.manoa.hawaii.edu/items/ f21c8b34-5d62-40d0-919f-a4e07cfbbc32. [Accessed 13-May-2023]. Speech Kai-Wei Chang, He He, Robin Jia, and Sameer Singh. 2021. Robustness and adversarial examples in natu- ral language processing. In Proc. of EMNLP. Jacqueline M Chen and Kate A Ratliff. 2018. Psycho- logical essentialism predicts intergroup bias. Social Cognition, 36(3):301\u2013323. Yejin Choi. 2022. The curious case of commonsense intelligence. Daedalus. Hyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. ArXiv preprint. Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that\u2019s \u2018human\u2019 is not gold: Evaluating human evaluation of generated text. In Proc. of ACL. Gloria Cowan and Cyndi Hodge. 1996. Judgments of Hate Speech: The Effects of Target Group, Public- ness, and Behavioral Responses of the Target. Jour- nal of Applied Social Psychology. Gloria Cowan and Jon Mettrick. 2002. The effects of Target Variables and Settting on Perceptions of Hate Speech1. Journal of Applied Social Psychology. Thomas Davidson, Debasmita Bhattacharya, and Ing- mar Weber. 2019. Racial bias in hate speech and abusive language detection datasets. In Proceedings of the Third Workshop on Abusive Language Online. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated Hate Speech De- tection and the Problem of Offensive Language. In Proceedings of the 11th International Conference on Web and Social Media (ICWSM). Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency. Marta Dynel. 2015. The landscape of impoliteness research. Journal of Politeness Research. Nina Eliasoph and Paul Lichterman. 2003. Culture in Interaction. American Journal of Sociology. Mai ElSherief, Caleb Ziems, David Muchlinski, Vaish- navi Anupindi, Jordyn Seybolt, Munmun De Choud- hury, and Diyi Yang. 2021. Latent hatred: A bench- mark for understanding implicit hate speech. In Proc. of EMNLP. Charles J. Fillmore. 1976. Frame semantics and the na- ture of language*. Annals of the New York Academy of Sciences. Gary Alan Fine. 1979. Small Groups and Culture Cre- ation: The Idioculture of Little League Baseball Teams. American Sociological Review. Susan T Fiske. 2017. Prejudices in cultural contexts:"}, {"question": " How is the potential harm of perpetuating harm against marginalized people addressed?", "answer": " By not endorsing the use of data for such purposes.", "ref_chunk": "essentializing groups. English Only We only look at a US-centric per- spective in our investigation. Obviously, online hate and abuse is manifested in many languages (Arango Monnar et al., 2022), so we hope future work will adapt our frames to different languages and different cultures. Subjectivity in Offensiveness Not everyone agrees that things are offensive, or has the same interpretation of offensiveness (depending on their own background and beliefs; Sap et al., 2022). Our in-context prompts and qualification likely make both our machine-generated explanations and hu- man annotations prescriptive (R\u00f6ttger et al., 2021), in contrast to a more descriptive approach where we would examine different interpretations. We leave that up for future work. Dual Use We aim to combat the negative effects and harms of discriminatory language on already marginalized people (Sap et al., 2019b; Davidson et al., 2019). It is possible however that our frames, dataset, and models could be used to perpetuate harm against those very people. We do not endorse the use of our data for those purposes. Risk of Suppressing Speech Our frames, dataset, and models are built with content moderation in mind, as online spaces are increasingly riddled with hate and abuse and content moderators are strug- gling to sift through all of the content. We hope future work will examine frameworks for using our frames to help content moderators. We do not endorse the use of our system to suppress speech without human oversight and encourage practition- ers to take non-censorship-oriented approaches to content moderation (e.g., counterspeech (Tekiro\u02d8glu et al., 2022)). Harms of Exposing Workers to Toxic Content The verification process of COBRACORPUS and COBRACORPUS-CF is performed by human anno- tators. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We mitigated these by designing minimum annotation workload, paying workers above minimum wage ($7-12), and providing them with crisis manage- ment resources. Our annotation work is also super- vised by an Institutional Review Board (IRB). Acknowledgements First of all, we thank our workers on MTurk for their hard work and thoughtful responses. We thank the anonymous reviewers for their helpful com- ments. We also thank Shengyu Feng and mem- bers of the CMU LTI COMEDY group for their feedback, and OpenAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fundamental AI Research Laboratories (FAIR) \u201cDynabench Data Collection and Benchmarking Platform\u201d award \u201cContExTox: Context-Aware and Explainable Toxicity Detection,\u201d and CISCO Ethics in AI award \u201cExpHarm: So- cially Aware, Ethically Informed, and Explanation- Centric AI Systems.\u201d References Herman Aguinis, Isabel Villamor, and Ravi S. Ramani. 2021. Mturk research: Review and recommendations. Journal of Management, 47(4):823\u2013837. Ayme Arango Monnar, Jorge Perez, Barbara Poblete, Magdalena Salda\u00f1a, and Valentina Proust. 2022. Re- sources for multilingual hate speech detection. In Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH). Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics. Emily M. Bender, Timnit Gebru, Angelina McMillan- Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Confer- ence on Fairness, Accountability, and Transparency, FAccT \u201921. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. Enrico Bunde. 2021. AI-Assisted and explainable hate speech detection for social media Moderators\u2013A de- sign science approach. In Proceedings of the 54th Hawaii International Conference on System Sciences. Enrico Bunde. 2023. AI-Assisted and Ex- for Detection https:// Hate plainable Social Media Moderators. scholarspace.manoa.hawaii.edu/items/ f21c8b34-5d62-40d0-919f-a4e07cfbbc32. [Accessed 13-May-2023]. Speech Kai-Wei Chang, He He, Robin Jia, and Sameer Singh. 2021. Robustness and adversarial examples in natu- ral language processing. In Proc. of EMNLP. Jacqueline M Chen and Kate A Ratliff. 2018. Psycho- logical essentialism predicts intergroup bias. Social Cognition, 36(3):301\u2013323. Yejin Choi. 2022. The curious case of commonsense intelligence. Daedalus. Hyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. ArXiv preprint. Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that\u2019s \u2018human\u2019 is not gold: Evaluating human evaluation of generated text. In Proc. of ACL. Gloria Cowan and Cyndi Hodge. 1996. Judgments of Hate Speech: The Effects of Target Group, Public- ness, and Behavioral Responses of the Target. Jour- nal of Applied Social Psychology. Gloria Cowan and Jon Mettrick. 2002. The effects of Target Variables and Settting on Perceptions of Hate Speech1. Journal of Applied Social Psychology. Thomas Davidson, Debasmita Bhattacharya, and Ing- mar Weber. 2019. Racial bias in hate speech and abusive language detection datasets. In Proceedings of the Third Workshop on Abusive Language Online. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated Hate Speech De- tection and the Problem of Offensive Language. In Proceedings of the 11th International Conference on Web and Social Media (ICWSM). Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency. Marta Dynel. 2015. The landscape of impoliteness research. Journal of Politeness Research. Nina Eliasoph and Paul Lichterman. 2003. Culture in Interaction. American Journal of Sociology. Mai ElSherief, Caleb Ziems, David Muchlinski, Vaish- navi Anupindi, Jordyn Seybolt, Munmun De Choud- hury, and Diyi Yang. 2021. Latent hatred: A bench- mark for understanding implicit hate speech. In Proc. of EMNLP. Charles J. Fillmore. 1976. Frame semantics and the na- ture of language*. Annals of the New York Academy of Sciences. Gary Alan Fine. 1979. Small Groups and Culture Cre- ation: The Idioculture of Little League Baseball Teams. American Sociological Review. Susan T Fiske. 2017. Prejudices in cultural contexts:"}, {"question": " What is the intended purpose of the frames, dataset, and models mentioned in the text?", "answer": " They are built with content moderation in mind.", "ref_chunk": "essentializing groups. English Only We only look at a US-centric per- spective in our investigation. Obviously, online hate and abuse is manifested in many languages (Arango Monnar et al., 2022), so we hope future work will adapt our frames to different languages and different cultures. Subjectivity in Offensiveness Not everyone agrees that things are offensive, or has the same interpretation of offensiveness (depending on their own background and beliefs; Sap et al., 2022). Our in-context prompts and qualification likely make both our machine-generated explanations and hu- man annotations prescriptive (R\u00f6ttger et al., 2021), in contrast to a more descriptive approach where we would examine different interpretations. We leave that up for future work. Dual Use We aim to combat the negative effects and harms of discriminatory language on already marginalized people (Sap et al., 2019b; Davidson et al., 2019). It is possible however that our frames, dataset, and models could be used to perpetuate harm against those very people. We do not endorse the use of our data for those purposes. Risk of Suppressing Speech Our frames, dataset, and models are built with content moderation in mind, as online spaces are increasingly riddled with hate and abuse and content moderators are strug- gling to sift through all of the content. We hope future work will examine frameworks for using our frames to help content moderators. We do not endorse the use of our system to suppress speech without human oversight and encourage practition- ers to take non-censorship-oriented approaches to content moderation (e.g., counterspeech (Tekiro\u02d8glu et al., 2022)). Harms of Exposing Workers to Toxic Content The verification process of COBRACORPUS and COBRACORPUS-CF is performed by human anno- tators. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We mitigated these by designing minimum annotation workload, paying workers above minimum wage ($7-12), and providing them with crisis manage- ment resources. Our annotation work is also super- vised by an Institutional Review Board (IRB). Acknowledgements First of all, we thank our workers on MTurk for their hard work and thoughtful responses. We thank the anonymous reviewers for their helpful com- ments. We also thank Shengyu Feng and mem- bers of the CMU LTI COMEDY group for their feedback, and OpenAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fundamental AI Research Laboratories (FAIR) \u201cDynabench Data Collection and Benchmarking Platform\u201d award \u201cContExTox: Context-Aware and Explainable Toxicity Detection,\u201d and CISCO Ethics in AI award \u201cExpHarm: So- cially Aware, Ethically Informed, and Explanation- Centric AI Systems.\u201d References Herman Aguinis, Isabel Villamor, and Ravi S. Ramani. 2021. Mturk research: Review and recommendations. Journal of Management, 47(4):823\u2013837. Ayme Arango Monnar, Jorge Perez, Barbara Poblete, Magdalena Salda\u00f1a, and Valentina Proust. 2022. Re- sources for multilingual hate speech detection. In Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH). Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics. Emily M. Bender, Timnit Gebru, Angelina McMillan- Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Confer- ence on Fairness, Accountability, and Transparency, FAccT \u201921. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. Enrico Bunde. 2021. AI-Assisted and explainable hate speech detection for social media Moderators\u2013A de- sign science approach. In Proceedings of the 54th Hawaii International Conference on System Sciences. Enrico Bunde. 2023. AI-Assisted and Ex- for Detection https:// Hate plainable Social Media Moderators. scholarspace.manoa.hawaii.edu/items/ f21c8b34-5d62-40d0-919f-a4e07cfbbc32. [Accessed 13-May-2023]. Speech Kai-Wei Chang, He He, Robin Jia, and Sameer Singh. 2021. Robustness and adversarial examples in natu- ral language processing. In Proc. of EMNLP. Jacqueline M Chen and Kate A Ratliff. 2018. Psycho- logical essentialism predicts intergroup bias. Social Cognition, 36(3):301\u2013323. Yejin Choi. 2022. The curious case of commonsense intelligence. Daedalus. Hyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. ArXiv preprint. Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that\u2019s \u2018human\u2019 is not gold: Evaluating human evaluation of generated text. In Proc. of ACL. Gloria Cowan and Cyndi Hodge. 1996. Judgments of Hate Speech: The Effects of Target Group, Public- ness, and Behavioral Responses of the Target. Jour- nal of Applied Social Psychology. Gloria Cowan and Jon Mettrick. 2002. The effects of Target Variables and Settting on Perceptions of Hate Speech1. Journal of Applied Social Psychology. Thomas Davidson, Debasmita Bhattacharya, and Ing- mar Weber. 2019. Racial bias in hate speech and abusive language detection datasets. In Proceedings of the Third Workshop on Abusive Language Online. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated Hate Speech De- tection and the Problem of Offensive Language. In Proceedings of the 11th International Conference on Web and Social Media (ICWSM). Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency. Marta Dynel. 2015. The landscape of impoliteness research. Journal of Politeness Research. Nina Eliasoph and Paul Lichterman. 2003. Culture in Interaction. American Journal of Sociology. Mai ElSherief, Caleb Ziems, David Muchlinski, Vaish- navi Anupindi, Jordyn Seybolt, Munmun De Choud- hury, and Diyi Yang. 2021. Latent hatred: A bench- mark for understanding implicit hate speech. In Proc. of EMNLP. Charles J. Fillmore. 1976. Frame semantics and the na- ture of language*. Annals of the New York Academy of Sciences. Gary Alan Fine. 1979. Small Groups and Culture Cre- ation: The Idioculture of Little League Baseball Teams. American Sociological Review. Susan T Fiske. 2017. Prejudices in cultural contexts:"}, {"question": " What approach is encouraged for content moderation in the text?", "answer": " Encouraging non-censorship-oriented approaches like counterspeech.", "ref_chunk": "essentializing groups. English Only We only look at a US-centric per- spective in our investigation. Obviously, online hate and abuse is manifested in many languages (Arango Monnar et al., 2022), so we hope future work will adapt our frames to different languages and different cultures. Subjectivity in Offensiveness Not everyone agrees that things are offensive, or has the same interpretation of offensiveness (depending on their own background and beliefs; Sap et al., 2022). Our in-context prompts and qualification likely make both our machine-generated explanations and hu- man annotations prescriptive (R\u00f6ttger et al., 2021), in contrast to a more descriptive approach where we would examine different interpretations. We leave that up for future work. Dual Use We aim to combat the negative effects and harms of discriminatory language on already marginalized people (Sap et al., 2019b; Davidson et al., 2019). It is possible however that our frames, dataset, and models could be used to perpetuate harm against those very people. We do not endorse the use of our data for those purposes. Risk of Suppressing Speech Our frames, dataset, and models are built with content moderation in mind, as online spaces are increasingly riddled with hate and abuse and content moderators are strug- gling to sift through all of the content. We hope future work will examine frameworks for using our frames to help content moderators. We do not endorse the use of our system to suppress speech without human oversight and encourage practition- ers to take non-censorship-oriented approaches to content moderation (e.g., counterspeech (Tekiro\u02d8glu et al., 2022)). Harms of Exposing Workers to Toxic Content The verification process of COBRACORPUS and COBRACORPUS-CF is performed by human anno- tators. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We mitigated these by designing minimum annotation workload, paying workers above minimum wage ($7-12), and providing them with crisis manage- ment resources. Our annotation work is also super- vised by an Institutional Review Board (IRB). Acknowledgements First of all, we thank our workers on MTurk for their hard work and thoughtful responses. We thank the anonymous reviewers for their helpful com- ments. We also thank Shengyu Feng and mem- bers of the CMU LTI COMEDY group for their feedback, and OpenAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fundamental AI Research Laboratories (FAIR) \u201cDynabench Data Collection and Benchmarking Platform\u201d award \u201cContExTox: Context-Aware and Explainable Toxicity Detection,\u201d and CISCO Ethics in AI award \u201cExpHarm: So- cially Aware, Ethically Informed, and Explanation- Centric AI Systems.\u201d References Herman Aguinis, Isabel Villamor, and Ravi S. Ramani. 2021. Mturk research: Review and recommendations. Journal of Management, 47(4):823\u2013837. Ayme Arango Monnar, Jorge Perez, Barbara Poblete, Magdalena Salda\u00f1a, and Valentina Proust. 2022. Re- sources for multilingual hate speech detection. In Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH). Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics. Emily M. Bender, Timnit Gebru, Angelina McMillan- Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Confer- ence on Fairness, Accountability, and Transparency, FAccT \u201921. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. Enrico Bunde. 2021. AI-Assisted and explainable hate speech detection for social media Moderators\u2013A de- sign science approach. In Proceedings of the 54th Hawaii International Conference on System Sciences. Enrico Bunde. 2023. AI-Assisted and Ex- for Detection https:// Hate plainable Social Media Moderators. scholarspace.manoa.hawaii.edu/items/ f21c8b34-5d62-40d0-919f-a4e07cfbbc32. [Accessed 13-May-2023]. Speech Kai-Wei Chang, He He, Robin Jia, and Sameer Singh. 2021. Robustness and adversarial examples in natu- ral language processing. In Proc. of EMNLP. Jacqueline M Chen and Kate A Ratliff. 2018. Psycho- logical essentialism predicts intergroup bias. Social Cognition, 36(3):301\u2013323. Yejin Choi. 2022. The curious case of commonsense intelligence. Daedalus. Hyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. ArXiv preprint. Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that\u2019s \u2018human\u2019 is not gold: Evaluating human evaluation of generated text. In Proc. of ACL. Gloria Cowan and Cyndi Hodge. 1996. Judgments of Hate Speech: The Effects of Target Group, Public- ness, and Behavioral Responses of the Target. Jour- nal of Applied Social Psychology. Gloria Cowan and Jon Mettrick. 2002. The effects of Target Variables and Settting on Perceptions of Hate Speech1. Journal of Applied Social Psychology. Thomas Davidson, Debasmita Bhattacharya, and Ing- mar Weber. 2019. Racial bias in hate speech and abusive language detection datasets. In Proceedings of the Third Workshop on Abusive Language Online. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated Hate Speech De- tection and the Problem of Offensive Language. In Proceedings of the 11th International Conference on Web and Social Media (ICWSM). Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency. Marta Dynel. 2015. The landscape of impoliteness research. Journal of Politeness Research. Nina Eliasoph and Paul Lichterman. 2003. Culture in Interaction. American Journal of Sociology. Mai ElSherief, Caleb Ziems, David Muchlinski, Vaish- navi Anupindi, Jordyn Seybolt, Munmun De Choud- hury, and Diyi Yang. 2021. Latent hatred: A bench- mark for understanding implicit hate speech. In Proc. of EMNLP. Charles J. Fillmore. 1976. Frame semantics and the na- ture of language*. Annals of the New York Academy of Sciences. Gary Alan Fine. 1979. Small Groups and Culture Cre- ation: The Idioculture of Little League Baseball Teams. American Sociological Review. Susan T Fiske. 2017. Prejudices in cultural contexts:"}, {"question": " How is the verification process of COBRACORPUS and COBRACORPUS-CF described?", "answer": " It is performed by human annotators.", "ref_chunk": "essentializing groups. English Only We only look at a US-centric per- spective in our investigation. Obviously, online hate and abuse is manifested in many languages (Arango Monnar et al., 2022), so we hope future work will adapt our frames to different languages and different cultures. Subjectivity in Offensiveness Not everyone agrees that things are offensive, or has the same interpretation of offensiveness (depending on their own background and beliefs; Sap et al., 2022). Our in-context prompts and qualification likely make both our machine-generated explanations and hu- man annotations prescriptive (R\u00f6ttger et al., 2021), in contrast to a more descriptive approach where we would examine different interpretations. We leave that up for future work. Dual Use We aim to combat the negative effects and harms of discriminatory language on already marginalized people (Sap et al., 2019b; Davidson et al., 2019). It is possible however that our frames, dataset, and models could be used to perpetuate harm against those very people. We do not endorse the use of our data for those purposes. Risk of Suppressing Speech Our frames, dataset, and models are built with content moderation in mind, as online spaces are increasingly riddled with hate and abuse and content moderators are strug- gling to sift through all of the content. We hope future work will examine frameworks for using our frames to help content moderators. We do not endorse the use of our system to suppress speech without human oversight and encourage practition- ers to take non-censorship-oriented approaches to content moderation (e.g., counterspeech (Tekiro\u02d8glu et al., 2022)). Harms of Exposing Workers to Toxic Content The verification process of COBRACORPUS and COBRACORPUS-CF is performed by human anno- tators. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We mitigated these by designing minimum annotation workload, paying workers above minimum wage ($7-12), and providing them with crisis manage- ment resources. Our annotation work is also super- vised by an Institutional Review Board (IRB). Acknowledgements First of all, we thank our workers on MTurk for their hard work and thoughtful responses. We thank the anonymous reviewers for their helpful com- ments. We also thank Shengyu Feng and mem- bers of the CMU LTI COMEDY group for their feedback, and OpenAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fundamental AI Research Laboratories (FAIR) \u201cDynabench Data Collection and Benchmarking Platform\u201d award \u201cContExTox: Context-Aware and Explainable Toxicity Detection,\u201d and CISCO Ethics in AI award \u201cExpHarm: So- cially Aware, Ethically Informed, and Explanation- Centric AI Systems.\u201d References Herman Aguinis, Isabel Villamor, and Ravi S. Ramani. 2021. Mturk research: Review and recommendations. Journal of Management, 47(4):823\u2013837. Ayme Arango Monnar, Jorge Perez, Barbara Poblete, Magdalena Salda\u00f1a, and Valentina Proust. 2022. Re- sources for multilingual hate speech detection. In Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH). Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics. Emily M. Bender, Timnit Gebru, Angelina McMillan- Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Confer- ence on Fairness, Accountability, and Transparency, FAccT \u201921. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. Enrico Bunde. 2021. AI-Assisted and explainable hate speech detection for social media Moderators\u2013A de- sign science approach. In Proceedings of the 54th Hawaii International Conference on System Sciences. Enrico Bunde. 2023. AI-Assisted and Ex- for Detection https:// Hate plainable Social Media Moderators. scholarspace.manoa.hawaii.edu/items/ f21c8b34-5d62-40d0-919f-a4e07cfbbc32. [Accessed 13-May-2023]. Speech Kai-Wei Chang, He He, Robin Jia, and Sameer Singh. 2021. Robustness and adversarial examples in natu- ral language processing. In Proc. of EMNLP. Jacqueline M Chen and Kate A Ratliff. 2018. Psycho- logical essentialism predicts intergroup bias. Social Cognition, 36(3):301\u2013323. Yejin Choi. 2022. The curious case of commonsense intelligence. Daedalus. Hyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. ArXiv preprint. Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that\u2019s \u2018human\u2019 is not gold: Evaluating human evaluation of generated text. In Proc. of ACL. Gloria Cowan and Cyndi Hodge. 1996. Judgments of Hate Speech: The Effects of Target Group, Public- ness, and Behavioral Responses of the Target. Jour- nal of Applied Social Psychology. Gloria Cowan and Jon Mettrick. 2002. The effects of Target Variables and Settting on Perceptions of Hate Speech1. Journal of Applied Social Psychology. Thomas Davidson, Debasmita Bhattacharya, and Ing- mar Weber. 2019. Racial bias in hate speech and abusive language detection datasets. In Proceedings of the Third Workshop on Abusive Language Online. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated Hate Speech De- tection and the Problem of Offensive Language. In Proceedings of the 11th International Conference on Web and Social Media (ICWSM). Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency. Marta Dynel. 2015. The landscape of impoliteness research. Journal of Politeness Research. Nina Eliasoph and Paul Lichterman. 2003. Culture in Interaction. American Journal of Sociology. Mai ElSherief, Caleb Ziems, David Muchlinski, Vaish- navi Anupindi, Jordyn Seybolt, Munmun De Choud- hury, and Diyi Yang. 2021. Latent hatred: A bench- mark for understanding implicit hate speech. In Proc. of EMNLP. Charles J. Fillmore. 1976. Frame semantics and the na- ture of language*. Annals of the New York Academy of Sciences. Gary Alan Fine. 1979. Small Groups and Culture Cre- ation: The Idioculture of Little League Baseball Teams. American Sociological Review. Susan T Fiske. 2017. Prejudices in cultural contexts:"}, {"question": " How are the annotators exposed to offensive content protected?", "answer": " By providing crisis management resources, supervising by an Institutional Review Board, and paying a fair wage.", "ref_chunk": "essentializing groups. English Only We only look at a US-centric per- spective in our investigation. Obviously, online hate and abuse is manifested in many languages (Arango Monnar et al., 2022), so we hope future work will adapt our frames to different languages and different cultures. Subjectivity in Offensiveness Not everyone agrees that things are offensive, or has the same interpretation of offensiveness (depending on their own background and beliefs; Sap et al., 2022). Our in-context prompts and qualification likely make both our machine-generated explanations and hu- man annotations prescriptive (R\u00f6ttger et al., 2021), in contrast to a more descriptive approach where we would examine different interpretations. We leave that up for future work. Dual Use We aim to combat the negative effects and harms of discriminatory language on already marginalized people (Sap et al., 2019b; Davidson et al., 2019). It is possible however that our frames, dataset, and models could be used to perpetuate harm against those very people. We do not endorse the use of our data for those purposes. Risk of Suppressing Speech Our frames, dataset, and models are built with content moderation in mind, as online spaces are increasingly riddled with hate and abuse and content moderators are strug- gling to sift through all of the content. We hope future work will examine frameworks for using our frames to help content moderators. We do not endorse the use of our system to suppress speech without human oversight and encourage practition- ers to take non-censorship-oriented approaches to content moderation (e.g., counterspeech (Tekiro\u02d8glu et al., 2022)). Harms of Exposing Workers to Toxic Content The verification process of COBRACORPUS and COBRACORPUS-CF is performed by human anno- tators. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We mitigated these by designing minimum annotation workload, paying workers above minimum wage ($7-12), and providing them with crisis manage- ment resources. Our annotation work is also super- vised by an Institutional Review Board (IRB). Acknowledgements First of all, we thank our workers on MTurk for their hard work and thoughtful responses. We thank the anonymous reviewers for their helpful com- ments. We also thank Shengyu Feng and mem- bers of the CMU LTI COMEDY group for their feedback, and OpenAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fundamental AI Research Laboratories (FAIR) \u201cDynabench Data Collection and Benchmarking Platform\u201d award \u201cContExTox: Context-Aware and Explainable Toxicity Detection,\u201d and CISCO Ethics in AI award \u201cExpHarm: So- cially Aware, Ethically Informed, and Explanation- Centric AI Systems.\u201d References Herman Aguinis, Isabel Villamor, and Ravi S. Ramani. 2021. Mturk research: Review and recommendations. Journal of Management, 47(4):823\u2013837. Ayme Arango Monnar, Jorge Perez, Barbara Poblete, Magdalena Salda\u00f1a, and Valentina Proust. 2022. Re- sources for multilingual hate speech detection. In Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH). Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics. Emily M. Bender, Timnit Gebru, Angelina McMillan- Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Confer- ence on Fairness, Accountability, and Transparency, FAccT \u201921. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. Enrico Bunde. 2021. AI-Assisted and explainable hate speech detection for social media Moderators\u2013A de- sign science approach. In Proceedings of the 54th Hawaii International Conference on System Sciences. Enrico Bunde. 2023. AI-Assisted and Ex- for Detection https:// Hate plainable Social Media Moderators. scholarspace.manoa.hawaii.edu/items/ f21c8b34-5d62-40d0-919f-a4e07cfbbc32. [Accessed 13-May-2023]. Speech Kai-Wei Chang, He He, Robin Jia, and Sameer Singh. 2021. Robustness and adversarial examples in natu- ral language processing. In Proc. of EMNLP. Jacqueline M Chen and Kate A Ratliff. 2018. Psycho- logical essentialism predicts intergroup bias. Social Cognition, 36(3):301\u2013323. Yejin Choi. 2022. The curious case of commonsense intelligence. Daedalus. Hyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. ArXiv preprint. Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that\u2019s \u2018human\u2019 is not gold: Evaluating human evaluation of generated text. In Proc. of ACL. Gloria Cowan and Cyndi Hodge. 1996. Judgments of Hate Speech: The Effects of Target Group, Public- ness, and Behavioral Responses of the Target. Jour- nal of Applied Social Psychology. Gloria Cowan and Jon Mettrick. 2002. The effects of Target Variables and Settting on Perceptions of Hate Speech1. Journal of Applied Social Psychology. Thomas Davidson, Debasmita Bhattacharya, and Ing- mar Weber. 2019. Racial bias in hate speech and abusive language detection datasets. In Proceedings of the Third Workshop on Abusive Language Online. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated Hate Speech De- tection and the Problem of Offensive Language. In Proceedings of the 11th International Conference on Web and Social Media (ICWSM). Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency. Marta Dynel. 2015. The landscape of impoliteness research. Journal of Politeness Research. Nina Eliasoph and Paul Lichterman. 2003. Culture in Interaction. American Journal of Sociology. Mai ElSherief, Caleb Ziems, David Muchlinski, Vaish- navi Anupindi, Jordyn Seybolt, Munmun De Choud- hury, and Diyi Yang. 2021. Latent hatred: A bench- mark for understanding implicit hate speech. In Proc. of EMNLP. Charles J. Fillmore. 1976. Frame semantics and the na- ture of language*. Annals of the New York Academy of Sciences. Gary Alan Fine. 1979. Small Groups and Culture Cre- ation: The Idioculture of Little League Baseball Teams. American Sociological Review. Susan T Fiske. 2017. Prejudices in cultural contexts:"}], "doc_text": "essentializing groups. English Only We only look at a US-centric per- spective in our investigation. Obviously, online hate and abuse is manifested in many languages (Arango Monnar et al., 2022), so we hope future work will adapt our frames to different languages and different cultures. Subjectivity in Offensiveness Not everyone agrees that things are offensive, or has the same interpretation of offensiveness (depending on their own background and beliefs; Sap et al., 2022). Our in-context prompts and qualification likely make both our machine-generated explanations and hu- man annotations prescriptive (R\u00f6ttger et al., 2021), in contrast to a more descriptive approach where we would examine different interpretations. We leave that up for future work. Dual Use We aim to combat the negative effects and harms of discriminatory language on already marginalized people (Sap et al., 2019b; Davidson et al., 2019). It is possible however that our frames, dataset, and models could be used to perpetuate harm against those very people. We do not endorse the use of our data for those purposes. Risk of Suppressing Speech Our frames, dataset, and models are built with content moderation in mind, as online spaces are increasingly riddled with hate and abuse and content moderators are strug- gling to sift through all of the content. We hope future work will examine frameworks for using our frames to help content moderators. We do not endorse the use of our system to suppress speech without human oversight and encourage practition- ers to take non-censorship-oriented approaches to content moderation (e.g., counterspeech (Tekiro\u02d8glu et al., 2022)). Harms of Exposing Workers to Toxic Content The verification process of COBRACORPUS and COBRACORPUS-CF is performed by human anno- tators. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We mitigated these by designing minimum annotation workload, paying workers above minimum wage ($7-12), and providing them with crisis manage- ment resources. Our annotation work is also super- vised by an Institutional Review Board (IRB). Acknowledgements First of all, we thank our workers on MTurk for their hard work and thoughtful responses. We thank the anonymous reviewers for their helpful com- ments. We also thank Shengyu Feng and mem- bers of the CMU LTI COMEDY group for their feedback, and OpenAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fundamental AI Research Laboratories (FAIR) \u201cDynabench Data Collection and Benchmarking Platform\u201d award \u201cContExTox: Context-Aware and Explainable Toxicity Detection,\u201d and CISCO Ethics in AI award \u201cExpHarm: So- cially Aware, Ethically Informed, and Explanation- Centric AI Systems.\u201d References Herman Aguinis, Isabel Villamor, and Ravi S. Ramani. 2021. Mturk research: Review and recommendations. Journal of Management, 47(4):823\u2013837. Ayme Arango Monnar, Jorge Perez, Barbara Poblete, Magdalena Salda\u00f1a, and Valentina Proust. 2022. Re- sources for multilingual hate speech detection. In Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH). Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics. Emily M. Bender, Timnit Gebru, Angelina McMillan- Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Confer- ence on Fairness, Accountability, and Transparency, FAccT \u201921. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. Enrico Bunde. 2021. AI-Assisted and explainable hate speech detection for social media Moderators\u2013A de- sign science approach. In Proceedings of the 54th Hawaii International Conference on System Sciences. Enrico Bunde. 2023. AI-Assisted and Ex- for Detection https:// Hate plainable Social Media Moderators. scholarspace.manoa.hawaii.edu/items/ f21c8b34-5d62-40d0-919f-a4e07cfbbc32. [Accessed 13-May-2023]. Speech Kai-Wei Chang, He He, Robin Jia, and Sameer Singh. 2021. Robustness and adversarial examples in natu- ral language processing. In Proc. of EMNLP. Jacqueline M Chen and Kate A Ratliff. 2018. Psycho- logical essentialism predicts intergroup bias. Social Cognition, 36(3):301\u2013323. Yejin Choi. 2022. The curious case of commonsense intelligence. Daedalus. Hyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. ArXiv preprint. Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that\u2019s \u2018human\u2019 is not gold: Evaluating human evaluation of generated text. In Proc. of ACL. Gloria Cowan and Cyndi Hodge. 1996. Judgments of Hate Speech: The Effects of Target Group, Public- ness, and Behavioral Responses of the Target. Jour- nal of Applied Social Psychology. Gloria Cowan and Jon Mettrick. 2002. The effects of Target Variables and Settting on Perceptions of Hate Speech1. Journal of Applied Social Psychology. Thomas Davidson, Debasmita Bhattacharya, and Ing- mar Weber. 2019. Racial bias in hate speech and abusive language detection datasets. In Proceedings of the Third Workshop on Abusive Language Online. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated Hate Speech De- tection and the Problem of Offensive Language. In Proceedings of the 11th International Conference on Web and Social Media (ICWSM). Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency. Marta Dynel. 2015. The landscape of impoliteness research. Journal of Politeness Research. Nina Eliasoph and Paul Lichterman. 2003. Culture in Interaction. American Journal of Sociology. Mai ElSherief, Caleb Ziems, David Muchlinski, Vaish- navi Anupindi, Jordyn Seybolt, Munmun De Choud- hury, and Diyi Yang. 2021. Latent hatred: A bench- mark for understanding implicit hate speech. In Proc. of EMNLP. Charles J. Fillmore. 1976. Frame semantics and the na- ture of language*. Annals of the New York Academy of Sciences. Gary Alan Fine. 1979. Small Groups and Culture Cre- ation: The Idioculture of Little League Baseball Teams. American Sociological Review. Susan T Fiske. 2017. Prejudices in cultural contexts:"}