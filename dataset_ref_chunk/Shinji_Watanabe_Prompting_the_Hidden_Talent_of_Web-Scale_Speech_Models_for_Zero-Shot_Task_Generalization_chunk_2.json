{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Prompting_the_Hidden_Talent_of_Web-Scale_Speech_Models_for_Zero-Shot_Task_Generalization_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main task of using an ASR system in the context of this text?", "answer": " Producing transcription for a video where the visual content aids in recognition", "ref_chunk": "3. Audio-visual speech recognition The first task is using an ASR system to produce transcription for a video, where the on-screen visual content is semantically related to the speech audio and can therefore aid in recogni- tion [18, 19]. This task is related to, but more general than, performing audio-visual speech recognition (AVSR) on speech audio accompanied by a video of the speaker\u2019s facial or lip movements [20]. Approach. Our approach is shown in figure 1. To provide Whisper with a visually-conditioned prompt, we utilize the pop- ular vision-and-language CLIP [5] model along with an external vocabulary of common object words to first \u2018convert\u2019 the visual stream into a sequence of word tokens. To do so, we take every word/phrase in the external vocabulary, construct a sentence with template \u201cThis is a photo of a { }\u201d. Then we use the CLIP text en- coder to pre-compute an embedding vector for each sentence in an offline fashion. At inference time, for each video we sample 3 equally-spaced RGB image frames, use the CLIP image encoder to embed them, and calculate the similarity between the image embeddings and the pre-computed text embeddings. We select the top K objects whose embeddings have the highest similarity scores with the image embeddings for the prompt. Next, we con- catenate the K selected object names into a comma-separated list of words, and insert this token sequence into the previous text slot of the prompt. This method draws inspiration from the idea of Socratic Models [21], where an engineered interface enables large pretrained models to \u2018talk\u2019 to one other to solve a complex task. <|sop|>CLIP retrie.<default> <|sot|><|zh|><|en|><|asr|> <|sot|><|ru|><|asr|> 9% 19% 45% ObjectVocabcraftsman, powder, concrete ... I've never had a color coat separate from a house As prompt WhisperEncoder CLIP Whisper Decoder Figure 1: Framework for visually prompting Whisper. The exter- nal object vocab is dataset agnostic. in our How2 dataset have a ground truth transcription less than 30 words. This shows that Whisper is very robust to the noise and length of the prompt. For each model, the top 3 best performing number of object choices selected from How2 are used for the experiments on VisSpeech3, and the average WER is shown in figure 2. We see that the visually-informed prompt improves the performance for all four English models and three smaller multilingual models, but hurts the performance of the multilingual models Medium, Large, and LargeV2. In table 2, we compare the previous SotA AVSR results on VisSpeech with the audio-only Whisper perfor- mance, and Whisper Medium.en with 50 objects as the visual prompt. We highlight that visual prompt improve Medium.en by 9%, and even outperforms Large. Datasets and implementation details. Our main dataset for the AVSR task is the recently proposed VisSpeech [19], which is a subset of the instructional video dataset HowTo100M [22]. VisSpeech consists of those videos where an audio-only base- line ASR system performs badly, and whose visual stream and speech audio are semantically related. Since VisSpeech is pro- posed as a test set and it only contains 508 examples, we use another instructional video dataset, How2 [18], for hyperparam- eter tuning. We use a randomly selected 2000 example subset of How2, and add pub noise to the audio to increase the ASR difficulty similar to [19], since the dataset has been shown to be biased towards clean audio [19] preventing the visual modality from offering significant benefit to its ASR task. For the external object vocabulary, we follow [21] and used the label set of Ten- cent ML-Images [23], which contains around 10,000 common objects. The number of object K used in the prompt is tuned for each Whisper model separately on our version of the How2 dataset with three different noise levels (SNR=5,0,\u22125dB). Results. We found that on our How2 tuning set, using very large number of objects (as many as 90 objects) does not hurt performance. Our manual inspection shows that even when using 30 objects, there are already many irrelevant ones that got mis-retrieved by CLIP. For example, in the example shown in figure 1, we found \u2018yogurt\u2019, \u2018heavy cream\u2019, and \u2018mayonnaise\u2019 in the visual prompt. In addition, more than 90% of the utterances Remarks. We propose a prompting approach that that adapts the audio-only Whisper for audio-visual speech recog- nition. Based on figure 2, visual prompting helps most of the models with the exception of three larger multilingual models. However, because Large.en and LargeV2.en are not available, it is difficult to draw conclusions on whether it is the model size or multilinguality that hinders the model from benefiting from visual prompting. The fact that visual prompting improves the performance of Medium.en while degrading the performance of Medium suggests that the cause could be multilinguality. If this is the case, multilingual models may benefit from being fine-tuned on monolingual data. 4. Code-switched speech recognition Code-switched speech refers to the scenario where more than one language is used in the same utterance. With the raise of globalization and democratization of speech recognition tech- nologies, Code-switched ASR (CS-ASR) has become a popular research area [24]. While we cannot know for certain whether Whisper was trained on code-switched data, it is clear that the model\u2019s language and task tokens can not explicitly direct the model to do CS-ASR - each language token only represents one 3We use 3 number of objects choices to reduce the tuning noise introduced by the mismatch between How2 and VisSpeech. 16 Medium.en Audio + Visual Prompt 6 Large LargeV2 Base.en 8 Tiny.en English ModelsMultilingual Models Tiny Small.en Audio Only 10 18WER Small 12 Medium 14 Base Table 2: Comparison of model per- formance on VisSpeech. With vi- sual prompt, Medium.en outper- forms Large. Model Modality WER SotA [19] A+V 11.28 Whisper Medium.en Whisper Medium.en A A+V 8.35 7.60 Whisper Large Whisper LargeV2 A A 8.02 7.16 Figure 2: The effectiveness of visual prompt on VisSpeech across different models. of the 99 training languages, and the task tokens do not convey any information on whether the model"}, {"question": " How is the visual content used to aid in recognition in the proposed approach?", "answer": " The visual stream is converted into a sequence of word tokens related to common object names, which are inserted into the prompt", "ref_chunk": "3. Audio-visual speech recognition The first task is using an ASR system to produce transcription for a video, where the on-screen visual content is semantically related to the speech audio and can therefore aid in recogni- tion [18, 19]. This task is related to, but more general than, performing audio-visual speech recognition (AVSR) on speech audio accompanied by a video of the speaker\u2019s facial or lip movements [20]. Approach. Our approach is shown in figure 1. To provide Whisper with a visually-conditioned prompt, we utilize the pop- ular vision-and-language CLIP [5] model along with an external vocabulary of common object words to first \u2018convert\u2019 the visual stream into a sequence of word tokens. To do so, we take every word/phrase in the external vocabulary, construct a sentence with template \u201cThis is a photo of a { }\u201d. Then we use the CLIP text en- coder to pre-compute an embedding vector for each sentence in an offline fashion. At inference time, for each video we sample 3 equally-spaced RGB image frames, use the CLIP image encoder to embed them, and calculate the similarity between the image embeddings and the pre-computed text embeddings. We select the top K objects whose embeddings have the highest similarity scores with the image embeddings for the prompt. Next, we con- catenate the K selected object names into a comma-separated list of words, and insert this token sequence into the previous text slot of the prompt. This method draws inspiration from the idea of Socratic Models [21], where an engineered interface enables large pretrained models to \u2018talk\u2019 to one other to solve a complex task. <|sop|>CLIP retrie.<default> <|sot|><|zh|><|en|><|asr|> <|sot|><|ru|><|asr|> 9% 19% 45% ObjectVocabcraftsman, powder, concrete ... I've never had a color coat separate from a house As prompt WhisperEncoder CLIP Whisper Decoder Figure 1: Framework for visually prompting Whisper. The exter- nal object vocab is dataset agnostic. in our How2 dataset have a ground truth transcription less than 30 words. This shows that Whisper is very robust to the noise and length of the prompt. For each model, the top 3 best performing number of object choices selected from How2 are used for the experiments on VisSpeech3, and the average WER is shown in figure 2. We see that the visually-informed prompt improves the performance for all four English models and three smaller multilingual models, but hurts the performance of the multilingual models Medium, Large, and LargeV2. In table 2, we compare the previous SotA AVSR results on VisSpeech with the audio-only Whisper perfor- mance, and Whisper Medium.en with 50 objects as the visual prompt. We highlight that visual prompt improve Medium.en by 9%, and even outperforms Large. Datasets and implementation details. Our main dataset for the AVSR task is the recently proposed VisSpeech [19], which is a subset of the instructional video dataset HowTo100M [22]. VisSpeech consists of those videos where an audio-only base- line ASR system performs badly, and whose visual stream and speech audio are semantically related. Since VisSpeech is pro- posed as a test set and it only contains 508 examples, we use another instructional video dataset, How2 [18], for hyperparam- eter tuning. We use a randomly selected 2000 example subset of How2, and add pub noise to the audio to increase the ASR difficulty similar to [19], since the dataset has been shown to be biased towards clean audio [19] preventing the visual modality from offering significant benefit to its ASR task. For the external object vocabulary, we follow [21] and used the label set of Ten- cent ML-Images [23], which contains around 10,000 common objects. The number of object K used in the prompt is tuned for each Whisper model separately on our version of the How2 dataset with three different noise levels (SNR=5,0,\u22125dB). Results. We found that on our How2 tuning set, using very large number of objects (as many as 90 objects) does not hurt performance. Our manual inspection shows that even when using 30 objects, there are already many irrelevant ones that got mis-retrieved by CLIP. For example, in the example shown in figure 1, we found \u2018yogurt\u2019, \u2018heavy cream\u2019, and \u2018mayonnaise\u2019 in the visual prompt. In addition, more than 90% of the utterances Remarks. We propose a prompting approach that that adapts the audio-only Whisper for audio-visual speech recog- nition. Based on figure 2, visual prompting helps most of the models with the exception of three larger multilingual models. However, because Large.en and LargeV2.en are not available, it is difficult to draw conclusions on whether it is the model size or multilinguality that hinders the model from benefiting from visual prompting. The fact that visual prompting improves the performance of Medium.en while degrading the performance of Medium suggests that the cause could be multilinguality. If this is the case, multilingual models may benefit from being fine-tuned on monolingual data. 4. Code-switched speech recognition Code-switched speech refers to the scenario where more than one language is used in the same utterance. With the raise of globalization and democratization of speech recognition tech- nologies, Code-switched ASR (CS-ASR) has become a popular research area [24]. While we cannot know for certain whether Whisper was trained on code-switched data, it is clear that the model\u2019s language and task tokens can not explicitly direct the model to do CS-ASR - each language token only represents one 3We use 3 number of objects choices to reduce the tuning noise introduced by the mismatch between How2 and VisSpeech. 16 Medium.en Audio + Visual Prompt 6 Large LargeV2 Base.en 8 Tiny.en English ModelsMultilingual Models Tiny Small.en Audio Only 10 18WER Small 12 Medium 14 Base Table 2: Comparison of model per- formance on VisSpeech. With vi- sual prompt, Medium.en outper- forms Large. Model Modality WER SotA [19] A+V 11.28 Whisper Medium.en Whisper Medium.en A A+V 8.35 7.60 Whisper Large Whisper LargeV2 A A 8.02 7.16 Figure 2: The effectiveness of visual prompt on VisSpeech across different models. of the 99 training languages, and the task tokens do not convey any information on whether the model"}, {"question": " Which model is utilized to provide a visually-conditioned prompt in the approach described in the text?", "answer": " The vision-and-language CLIP model", "ref_chunk": "3. Audio-visual speech recognition The first task is using an ASR system to produce transcription for a video, where the on-screen visual content is semantically related to the speech audio and can therefore aid in recogni- tion [18, 19]. This task is related to, but more general than, performing audio-visual speech recognition (AVSR) on speech audio accompanied by a video of the speaker\u2019s facial or lip movements [20]. Approach. Our approach is shown in figure 1. To provide Whisper with a visually-conditioned prompt, we utilize the pop- ular vision-and-language CLIP [5] model along with an external vocabulary of common object words to first \u2018convert\u2019 the visual stream into a sequence of word tokens. To do so, we take every word/phrase in the external vocabulary, construct a sentence with template \u201cThis is a photo of a { }\u201d. Then we use the CLIP text en- coder to pre-compute an embedding vector for each sentence in an offline fashion. At inference time, for each video we sample 3 equally-spaced RGB image frames, use the CLIP image encoder to embed them, and calculate the similarity between the image embeddings and the pre-computed text embeddings. We select the top K objects whose embeddings have the highest similarity scores with the image embeddings for the prompt. Next, we con- catenate the K selected object names into a comma-separated list of words, and insert this token sequence into the previous text slot of the prompt. This method draws inspiration from the idea of Socratic Models [21], where an engineered interface enables large pretrained models to \u2018talk\u2019 to one other to solve a complex task. <|sop|>CLIP retrie.<default> <|sot|><|zh|><|en|><|asr|> <|sot|><|ru|><|asr|> 9% 19% 45% ObjectVocabcraftsman, powder, concrete ... I've never had a color coat separate from a house As prompt WhisperEncoder CLIP Whisper Decoder Figure 1: Framework for visually prompting Whisper. The exter- nal object vocab is dataset agnostic. in our How2 dataset have a ground truth transcription less than 30 words. This shows that Whisper is very robust to the noise and length of the prompt. For each model, the top 3 best performing number of object choices selected from How2 are used for the experiments on VisSpeech3, and the average WER is shown in figure 2. We see that the visually-informed prompt improves the performance for all four English models and three smaller multilingual models, but hurts the performance of the multilingual models Medium, Large, and LargeV2. In table 2, we compare the previous SotA AVSR results on VisSpeech with the audio-only Whisper perfor- mance, and Whisper Medium.en with 50 objects as the visual prompt. We highlight that visual prompt improve Medium.en by 9%, and even outperforms Large. Datasets and implementation details. Our main dataset for the AVSR task is the recently proposed VisSpeech [19], which is a subset of the instructional video dataset HowTo100M [22]. VisSpeech consists of those videos where an audio-only base- line ASR system performs badly, and whose visual stream and speech audio are semantically related. Since VisSpeech is pro- posed as a test set and it only contains 508 examples, we use another instructional video dataset, How2 [18], for hyperparam- eter tuning. We use a randomly selected 2000 example subset of How2, and add pub noise to the audio to increase the ASR difficulty similar to [19], since the dataset has been shown to be biased towards clean audio [19] preventing the visual modality from offering significant benefit to its ASR task. For the external object vocabulary, we follow [21] and used the label set of Ten- cent ML-Images [23], which contains around 10,000 common objects. The number of object K used in the prompt is tuned for each Whisper model separately on our version of the How2 dataset with three different noise levels (SNR=5,0,\u22125dB). Results. We found that on our How2 tuning set, using very large number of objects (as many as 90 objects) does not hurt performance. Our manual inspection shows that even when using 30 objects, there are already many irrelevant ones that got mis-retrieved by CLIP. For example, in the example shown in figure 1, we found \u2018yogurt\u2019, \u2018heavy cream\u2019, and \u2018mayonnaise\u2019 in the visual prompt. In addition, more than 90% of the utterances Remarks. We propose a prompting approach that that adapts the audio-only Whisper for audio-visual speech recog- nition. Based on figure 2, visual prompting helps most of the models with the exception of three larger multilingual models. However, because Large.en and LargeV2.en are not available, it is difficult to draw conclusions on whether it is the model size or multilinguality that hinders the model from benefiting from visual prompting. The fact that visual prompting improves the performance of Medium.en while degrading the performance of Medium suggests that the cause could be multilinguality. If this is the case, multilingual models may benefit from being fine-tuned on monolingual data. 4. Code-switched speech recognition Code-switched speech refers to the scenario where more than one language is used in the same utterance. With the raise of globalization and democratization of speech recognition tech- nologies, Code-switched ASR (CS-ASR) has become a popular research area [24]. While we cannot know for certain whether Whisper was trained on code-switched data, it is clear that the model\u2019s language and task tokens can not explicitly direct the model to do CS-ASR - each language token only represents one 3We use 3 number of objects choices to reduce the tuning noise introduced by the mismatch between How2 and VisSpeech. 16 Medium.en Audio + Visual Prompt 6 Large LargeV2 Base.en 8 Tiny.en English ModelsMultilingual Models Tiny Small.en Audio Only 10 18WER Small 12 Medium 14 Base Table 2: Comparison of model per- formance on VisSpeech. With vi- sual prompt, Medium.en outper- forms Large. Model Modality WER SotA [19] A+V 11.28 Whisper Medium.en Whisper Medium.en A A+V 8.35 7.60 Whisper Large Whisper LargeV2 A A 8.02 7.16 Figure 2: The effectiveness of visual prompt on VisSpeech across different models. of the 99 training languages, and the task tokens do not convey any information on whether the model"}, {"question": " What is the purpose of using the CLIP text encoder in the approach?", "answer": " To pre-compute an embedding vector for each sentence related to common object names", "ref_chunk": "3. Audio-visual speech recognition The first task is using an ASR system to produce transcription for a video, where the on-screen visual content is semantically related to the speech audio and can therefore aid in recogni- tion [18, 19]. This task is related to, but more general than, performing audio-visual speech recognition (AVSR) on speech audio accompanied by a video of the speaker\u2019s facial or lip movements [20]. Approach. Our approach is shown in figure 1. To provide Whisper with a visually-conditioned prompt, we utilize the pop- ular vision-and-language CLIP [5] model along with an external vocabulary of common object words to first \u2018convert\u2019 the visual stream into a sequence of word tokens. To do so, we take every word/phrase in the external vocabulary, construct a sentence with template \u201cThis is a photo of a { }\u201d. Then we use the CLIP text en- coder to pre-compute an embedding vector for each sentence in an offline fashion. At inference time, for each video we sample 3 equally-spaced RGB image frames, use the CLIP image encoder to embed them, and calculate the similarity between the image embeddings and the pre-computed text embeddings. We select the top K objects whose embeddings have the highest similarity scores with the image embeddings for the prompt. Next, we con- catenate the K selected object names into a comma-separated list of words, and insert this token sequence into the previous text slot of the prompt. This method draws inspiration from the idea of Socratic Models [21], where an engineered interface enables large pretrained models to \u2018talk\u2019 to one other to solve a complex task. <|sop|>CLIP retrie.<default> <|sot|><|zh|><|en|><|asr|> <|sot|><|ru|><|asr|> 9% 19% 45% ObjectVocabcraftsman, powder, concrete ... I've never had a color coat separate from a house As prompt WhisperEncoder CLIP Whisper Decoder Figure 1: Framework for visually prompting Whisper. The exter- nal object vocab is dataset agnostic. in our How2 dataset have a ground truth transcription less than 30 words. This shows that Whisper is very robust to the noise and length of the prompt. For each model, the top 3 best performing number of object choices selected from How2 are used for the experiments on VisSpeech3, and the average WER is shown in figure 2. We see that the visually-informed prompt improves the performance for all four English models and three smaller multilingual models, but hurts the performance of the multilingual models Medium, Large, and LargeV2. In table 2, we compare the previous SotA AVSR results on VisSpeech with the audio-only Whisper perfor- mance, and Whisper Medium.en with 50 objects as the visual prompt. We highlight that visual prompt improve Medium.en by 9%, and even outperforms Large. Datasets and implementation details. Our main dataset for the AVSR task is the recently proposed VisSpeech [19], which is a subset of the instructional video dataset HowTo100M [22]. VisSpeech consists of those videos where an audio-only base- line ASR system performs badly, and whose visual stream and speech audio are semantically related. Since VisSpeech is pro- posed as a test set and it only contains 508 examples, we use another instructional video dataset, How2 [18], for hyperparam- eter tuning. We use a randomly selected 2000 example subset of How2, and add pub noise to the audio to increase the ASR difficulty similar to [19], since the dataset has been shown to be biased towards clean audio [19] preventing the visual modality from offering significant benefit to its ASR task. For the external object vocabulary, we follow [21] and used the label set of Ten- cent ML-Images [23], which contains around 10,000 common objects. The number of object K used in the prompt is tuned for each Whisper model separately on our version of the How2 dataset with three different noise levels (SNR=5,0,\u22125dB). Results. We found that on our How2 tuning set, using very large number of objects (as many as 90 objects) does not hurt performance. Our manual inspection shows that even when using 30 objects, there are already many irrelevant ones that got mis-retrieved by CLIP. For example, in the example shown in figure 1, we found \u2018yogurt\u2019, \u2018heavy cream\u2019, and \u2018mayonnaise\u2019 in the visual prompt. In addition, more than 90% of the utterances Remarks. We propose a prompting approach that that adapts the audio-only Whisper for audio-visual speech recog- nition. Based on figure 2, visual prompting helps most of the models with the exception of three larger multilingual models. However, because Large.en and LargeV2.en are not available, it is difficult to draw conclusions on whether it is the model size or multilinguality that hinders the model from benefiting from visual prompting. The fact that visual prompting improves the performance of Medium.en while degrading the performance of Medium suggests that the cause could be multilinguality. If this is the case, multilingual models may benefit from being fine-tuned on monolingual data. 4. Code-switched speech recognition Code-switched speech refers to the scenario where more than one language is used in the same utterance. With the raise of globalization and democratization of speech recognition tech- nologies, Code-switched ASR (CS-ASR) has become a popular research area [24]. While we cannot know for certain whether Whisper was trained on code-switched data, it is clear that the model\u2019s language and task tokens can not explicitly direct the model to do CS-ASR - each language token only represents one 3We use 3 number of objects choices to reduce the tuning noise introduced by the mismatch between How2 and VisSpeech. 16 Medium.en Audio + Visual Prompt 6 Large LargeV2 Base.en 8 Tiny.en English ModelsMultilingual Models Tiny Small.en Audio Only 10 18WER Small 12 Medium 14 Base Table 2: Comparison of model per- formance on VisSpeech. With vi- sual prompt, Medium.en outper- forms Large. Model Modality WER SotA [19] A+V 11.28 Whisper Medium.en Whisper Medium.en A A+V 8.35 7.60 Whisper Large Whisper LargeV2 A A 8.02 7.16 Figure 2: The effectiveness of visual prompt on VisSpeech across different models. of the 99 training languages, and the task tokens do not convey any information on whether the model"}, {"question": " What is the inspiration behind the method used in this approach?", "answer": " Socratic Models, where large pretrained models communicate to solve complex tasks", "ref_chunk": "3. Audio-visual speech recognition The first task is using an ASR system to produce transcription for a video, where the on-screen visual content is semantically related to the speech audio and can therefore aid in recogni- tion [18, 19]. This task is related to, but more general than, performing audio-visual speech recognition (AVSR) on speech audio accompanied by a video of the speaker\u2019s facial or lip movements [20]. Approach. Our approach is shown in figure 1. To provide Whisper with a visually-conditioned prompt, we utilize the pop- ular vision-and-language CLIP [5] model along with an external vocabulary of common object words to first \u2018convert\u2019 the visual stream into a sequence of word tokens. To do so, we take every word/phrase in the external vocabulary, construct a sentence with template \u201cThis is a photo of a { }\u201d. Then we use the CLIP text en- coder to pre-compute an embedding vector for each sentence in an offline fashion. At inference time, for each video we sample 3 equally-spaced RGB image frames, use the CLIP image encoder to embed them, and calculate the similarity between the image embeddings and the pre-computed text embeddings. We select the top K objects whose embeddings have the highest similarity scores with the image embeddings for the prompt. Next, we con- catenate the K selected object names into a comma-separated list of words, and insert this token sequence into the previous text slot of the prompt. This method draws inspiration from the idea of Socratic Models [21], where an engineered interface enables large pretrained models to \u2018talk\u2019 to one other to solve a complex task. <|sop|>CLIP retrie.<default> <|sot|><|zh|><|en|><|asr|> <|sot|><|ru|><|asr|> 9% 19% 45% ObjectVocabcraftsman, powder, concrete ... I've never had a color coat separate from a house As prompt WhisperEncoder CLIP Whisper Decoder Figure 1: Framework for visually prompting Whisper. The exter- nal object vocab is dataset agnostic. in our How2 dataset have a ground truth transcription less than 30 words. This shows that Whisper is very robust to the noise and length of the prompt. For each model, the top 3 best performing number of object choices selected from How2 are used for the experiments on VisSpeech3, and the average WER is shown in figure 2. We see that the visually-informed prompt improves the performance for all four English models and three smaller multilingual models, but hurts the performance of the multilingual models Medium, Large, and LargeV2. In table 2, we compare the previous SotA AVSR results on VisSpeech with the audio-only Whisper perfor- mance, and Whisper Medium.en with 50 objects as the visual prompt. We highlight that visual prompt improve Medium.en by 9%, and even outperforms Large. Datasets and implementation details. Our main dataset for the AVSR task is the recently proposed VisSpeech [19], which is a subset of the instructional video dataset HowTo100M [22]. VisSpeech consists of those videos where an audio-only base- line ASR system performs badly, and whose visual stream and speech audio are semantically related. Since VisSpeech is pro- posed as a test set and it only contains 508 examples, we use another instructional video dataset, How2 [18], for hyperparam- eter tuning. We use a randomly selected 2000 example subset of How2, and add pub noise to the audio to increase the ASR difficulty similar to [19], since the dataset has been shown to be biased towards clean audio [19] preventing the visual modality from offering significant benefit to its ASR task. For the external object vocabulary, we follow [21] and used the label set of Ten- cent ML-Images [23], which contains around 10,000 common objects. The number of object K used in the prompt is tuned for each Whisper model separately on our version of the How2 dataset with three different noise levels (SNR=5,0,\u22125dB). Results. We found that on our How2 tuning set, using very large number of objects (as many as 90 objects) does not hurt performance. Our manual inspection shows that even when using 30 objects, there are already many irrelevant ones that got mis-retrieved by CLIP. For example, in the example shown in figure 1, we found \u2018yogurt\u2019, \u2018heavy cream\u2019, and \u2018mayonnaise\u2019 in the visual prompt. In addition, more than 90% of the utterances Remarks. We propose a prompting approach that that adapts the audio-only Whisper for audio-visual speech recog- nition. Based on figure 2, visual prompting helps most of the models with the exception of three larger multilingual models. However, because Large.en and LargeV2.en are not available, it is difficult to draw conclusions on whether it is the model size or multilinguality that hinders the model from benefiting from visual prompting. The fact that visual prompting improves the performance of Medium.en while degrading the performance of Medium suggests that the cause could be multilinguality. If this is the case, multilingual models may benefit from being fine-tuned on monolingual data. 4. Code-switched speech recognition Code-switched speech refers to the scenario where more than one language is used in the same utterance. With the raise of globalization and democratization of speech recognition tech- nologies, Code-switched ASR (CS-ASR) has become a popular research area [24]. While we cannot know for certain whether Whisper was trained on code-switched data, it is clear that the model\u2019s language and task tokens can not explicitly direct the model to do CS-ASR - each language token only represents one 3We use 3 number of objects choices to reduce the tuning noise introduced by the mismatch between How2 and VisSpeech. 16 Medium.en Audio + Visual Prompt 6 Large LargeV2 Base.en 8 Tiny.en English ModelsMultilingual Models Tiny Small.en Audio Only 10 18WER Small 12 Medium 14 Base Table 2: Comparison of model per- formance on VisSpeech. With vi- sual prompt, Medium.en outper- forms Large. Model Modality WER SotA [19] A+V 11.28 Whisper Medium.en Whisper Medium.en A A+V 8.35 7.60 Whisper Large Whisper LargeV2 A A 8.02 7.16 Figure 2: The effectiveness of visual prompt on VisSpeech across different models. of the 99 training languages, and the task tokens do not convey any information on whether the model"}, {"question": " Which dataset is used as the main dataset for the AVSR task?", "answer": " VisSpeech", "ref_chunk": "3. Audio-visual speech recognition The first task is using an ASR system to produce transcription for a video, where the on-screen visual content is semantically related to the speech audio and can therefore aid in recogni- tion [18, 19]. This task is related to, but more general than, performing audio-visual speech recognition (AVSR) on speech audio accompanied by a video of the speaker\u2019s facial or lip movements [20]. Approach. Our approach is shown in figure 1. To provide Whisper with a visually-conditioned prompt, we utilize the pop- ular vision-and-language CLIP [5] model along with an external vocabulary of common object words to first \u2018convert\u2019 the visual stream into a sequence of word tokens. To do so, we take every word/phrase in the external vocabulary, construct a sentence with template \u201cThis is a photo of a { }\u201d. Then we use the CLIP text en- coder to pre-compute an embedding vector for each sentence in an offline fashion. At inference time, for each video we sample 3 equally-spaced RGB image frames, use the CLIP image encoder to embed them, and calculate the similarity between the image embeddings and the pre-computed text embeddings. We select the top K objects whose embeddings have the highest similarity scores with the image embeddings for the prompt. Next, we con- catenate the K selected object names into a comma-separated list of words, and insert this token sequence into the previous text slot of the prompt. This method draws inspiration from the idea of Socratic Models [21], where an engineered interface enables large pretrained models to \u2018talk\u2019 to one other to solve a complex task. <|sop|>CLIP retrie.<default> <|sot|><|zh|><|en|><|asr|> <|sot|><|ru|><|asr|> 9% 19% 45% ObjectVocabcraftsman, powder, concrete ... I've never had a color coat separate from a house As prompt WhisperEncoder CLIP Whisper Decoder Figure 1: Framework for visually prompting Whisper. The exter- nal object vocab is dataset agnostic. in our How2 dataset have a ground truth transcription less than 30 words. This shows that Whisper is very robust to the noise and length of the prompt. For each model, the top 3 best performing number of object choices selected from How2 are used for the experiments on VisSpeech3, and the average WER is shown in figure 2. We see that the visually-informed prompt improves the performance for all four English models and three smaller multilingual models, but hurts the performance of the multilingual models Medium, Large, and LargeV2. In table 2, we compare the previous SotA AVSR results on VisSpeech with the audio-only Whisper perfor- mance, and Whisper Medium.en with 50 objects as the visual prompt. We highlight that visual prompt improve Medium.en by 9%, and even outperforms Large. Datasets and implementation details. Our main dataset for the AVSR task is the recently proposed VisSpeech [19], which is a subset of the instructional video dataset HowTo100M [22]. VisSpeech consists of those videos where an audio-only base- line ASR system performs badly, and whose visual stream and speech audio are semantically related. Since VisSpeech is pro- posed as a test set and it only contains 508 examples, we use another instructional video dataset, How2 [18], for hyperparam- eter tuning. We use a randomly selected 2000 example subset of How2, and add pub noise to the audio to increase the ASR difficulty similar to [19], since the dataset has been shown to be biased towards clean audio [19] preventing the visual modality from offering significant benefit to its ASR task. For the external object vocabulary, we follow [21] and used the label set of Ten- cent ML-Images [23], which contains around 10,000 common objects. The number of object K used in the prompt is tuned for each Whisper model separately on our version of the How2 dataset with three different noise levels (SNR=5,0,\u22125dB). Results. We found that on our How2 tuning set, using very large number of objects (as many as 90 objects) does not hurt performance. Our manual inspection shows that even when using 30 objects, there are already many irrelevant ones that got mis-retrieved by CLIP. For example, in the example shown in figure 1, we found \u2018yogurt\u2019, \u2018heavy cream\u2019, and \u2018mayonnaise\u2019 in the visual prompt. In addition, more than 90% of the utterances Remarks. We propose a prompting approach that that adapts the audio-only Whisper for audio-visual speech recog- nition. Based on figure 2, visual prompting helps most of the models with the exception of three larger multilingual models. However, because Large.en and LargeV2.en are not available, it is difficult to draw conclusions on whether it is the model size or multilinguality that hinders the model from benefiting from visual prompting. The fact that visual prompting improves the performance of Medium.en while degrading the performance of Medium suggests that the cause could be multilinguality. If this is the case, multilingual models may benefit from being fine-tuned on monolingual data. 4. Code-switched speech recognition Code-switched speech refers to the scenario where more than one language is used in the same utterance. With the raise of globalization and democratization of speech recognition tech- nologies, Code-switched ASR (CS-ASR) has become a popular research area [24]. While we cannot know for certain whether Whisper was trained on code-switched data, it is clear that the model\u2019s language and task tokens can not explicitly direct the model to do CS-ASR - each language token only represents one 3We use 3 number of objects choices to reduce the tuning noise introduced by the mismatch between How2 and VisSpeech. 16 Medium.en Audio + Visual Prompt 6 Large LargeV2 Base.en 8 Tiny.en English ModelsMultilingual Models Tiny Small.en Audio Only 10 18WER Small 12 Medium 14 Base Table 2: Comparison of model per- formance on VisSpeech. With vi- sual prompt, Medium.en outper- forms Large. Model Modality WER SotA [19] A+V 11.28 Whisper Medium.en Whisper Medium.en A A+V 8.35 7.60 Whisper Large Whisper LargeV2 A A 8.02 7.16 Figure 2: The effectiveness of visual prompt on VisSpeech across different models. of the 99 training languages, and the task tokens do not convey any information on whether the model"}, {"question": " How many examples does the VisSpeech dataset contain?", "answer": " 508 examples", "ref_chunk": "3. Audio-visual speech recognition The first task is using an ASR system to produce transcription for a video, where the on-screen visual content is semantically related to the speech audio and can therefore aid in recogni- tion [18, 19]. This task is related to, but more general than, performing audio-visual speech recognition (AVSR) on speech audio accompanied by a video of the speaker\u2019s facial or lip movements [20]. Approach. Our approach is shown in figure 1. To provide Whisper with a visually-conditioned prompt, we utilize the pop- ular vision-and-language CLIP [5] model along with an external vocabulary of common object words to first \u2018convert\u2019 the visual stream into a sequence of word tokens. To do so, we take every word/phrase in the external vocabulary, construct a sentence with template \u201cThis is a photo of a { }\u201d. Then we use the CLIP text en- coder to pre-compute an embedding vector for each sentence in an offline fashion. At inference time, for each video we sample 3 equally-spaced RGB image frames, use the CLIP image encoder to embed them, and calculate the similarity between the image embeddings and the pre-computed text embeddings. We select the top K objects whose embeddings have the highest similarity scores with the image embeddings for the prompt. Next, we con- catenate the K selected object names into a comma-separated list of words, and insert this token sequence into the previous text slot of the prompt. This method draws inspiration from the idea of Socratic Models [21], where an engineered interface enables large pretrained models to \u2018talk\u2019 to one other to solve a complex task. <|sop|>CLIP retrie.<default> <|sot|><|zh|><|en|><|asr|> <|sot|><|ru|><|asr|> 9% 19% 45% ObjectVocabcraftsman, powder, concrete ... I've never had a color coat separate from a house As prompt WhisperEncoder CLIP Whisper Decoder Figure 1: Framework for visually prompting Whisper. The exter- nal object vocab is dataset agnostic. in our How2 dataset have a ground truth transcription less than 30 words. This shows that Whisper is very robust to the noise and length of the prompt. For each model, the top 3 best performing number of object choices selected from How2 are used for the experiments on VisSpeech3, and the average WER is shown in figure 2. We see that the visually-informed prompt improves the performance for all four English models and three smaller multilingual models, but hurts the performance of the multilingual models Medium, Large, and LargeV2. In table 2, we compare the previous SotA AVSR results on VisSpeech with the audio-only Whisper perfor- mance, and Whisper Medium.en with 50 objects as the visual prompt. We highlight that visual prompt improve Medium.en by 9%, and even outperforms Large. Datasets and implementation details. Our main dataset for the AVSR task is the recently proposed VisSpeech [19], which is a subset of the instructional video dataset HowTo100M [22]. VisSpeech consists of those videos where an audio-only base- line ASR system performs badly, and whose visual stream and speech audio are semantically related. Since VisSpeech is pro- posed as a test set and it only contains 508 examples, we use another instructional video dataset, How2 [18], for hyperparam- eter tuning. We use a randomly selected 2000 example subset of How2, and add pub noise to the audio to increase the ASR difficulty similar to [19], since the dataset has been shown to be biased towards clean audio [19] preventing the visual modality from offering significant benefit to its ASR task. For the external object vocabulary, we follow [21] and used the label set of Ten- cent ML-Images [23], which contains around 10,000 common objects. The number of object K used in the prompt is tuned for each Whisper model separately on our version of the How2 dataset with three different noise levels (SNR=5,0,\u22125dB). Results. We found that on our How2 tuning set, using very large number of objects (as many as 90 objects) does not hurt performance. Our manual inspection shows that even when using 30 objects, there are already many irrelevant ones that got mis-retrieved by CLIP. For example, in the example shown in figure 1, we found \u2018yogurt\u2019, \u2018heavy cream\u2019, and \u2018mayonnaise\u2019 in the visual prompt. In addition, more than 90% of the utterances Remarks. We propose a prompting approach that that adapts the audio-only Whisper for audio-visual speech recog- nition. Based on figure 2, visual prompting helps most of the models with the exception of three larger multilingual models. However, because Large.en and LargeV2.en are not available, it is difficult to draw conclusions on whether it is the model size or multilinguality that hinders the model from benefiting from visual prompting. The fact that visual prompting improves the performance of Medium.en while degrading the performance of Medium suggests that the cause could be multilinguality. If this is the case, multilingual models may benefit from being fine-tuned on monolingual data. 4. Code-switched speech recognition Code-switched speech refers to the scenario where more than one language is used in the same utterance. With the raise of globalization and democratization of speech recognition tech- nologies, Code-switched ASR (CS-ASR) has become a popular research area [24]. While we cannot know for certain whether Whisper was trained on code-switched data, it is clear that the model\u2019s language and task tokens can not explicitly direct the model to do CS-ASR - each language token only represents one 3We use 3 number of objects choices to reduce the tuning noise introduced by the mismatch between How2 and VisSpeech. 16 Medium.en Audio + Visual Prompt 6 Large LargeV2 Base.en 8 Tiny.en English ModelsMultilingual Models Tiny Small.en Audio Only 10 18WER Small 12 Medium 14 Base Table 2: Comparison of model per- formance on VisSpeech. With vi- sual prompt, Medium.en outper- forms Large. Model Modality WER SotA [19] A+V 11.28 Whisper Medium.en Whisper Medium.en A A+V 8.35 7.60 Whisper Large Whisper LargeV2 A A 8.02 7.16 Figure 2: The effectiveness of visual prompt on VisSpeech across different models. of the 99 training languages, and the task tokens do not convey any information on whether the model"}, {"question": " What is the main dataset used for hyperparameter tuning?", "answer": " How2", "ref_chunk": "3. Audio-visual speech recognition The first task is using an ASR system to produce transcription for a video, where the on-screen visual content is semantically related to the speech audio and can therefore aid in recogni- tion [18, 19]. This task is related to, but more general than, performing audio-visual speech recognition (AVSR) on speech audio accompanied by a video of the speaker\u2019s facial or lip movements [20]. Approach. Our approach is shown in figure 1. To provide Whisper with a visually-conditioned prompt, we utilize the pop- ular vision-and-language CLIP [5] model along with an external vocabulary of common object words to first \u2018convert\u2019 the visual stream into a sequence of word tokens. To do so, we take every word/phrase in the external vocabulary, construct a sentence with template \u201cThis is a photo of a { }\u201d. Then we use the CLIP text en- coder to pre-compute an embedding vector for each sentence in an offline fashion. At inference time, for each video we sample 3 equally-spaced RGB image frames, use the CLIP image encoder to embed them, and calculate the similarity between the image embeddings and the pre-computed text embeddings. We select the top K objects whose embeddings have the highest similarity scores with the image embeddings for the prompt. Next, we con- catenate the K selected object names into a comma-separated list of words, and insert this token sequence into the previous text slot of the prompt. This method draws inspiration from the idea of Socratic Models [21], where an engineered interface enables large pretrained models to \u2018talk\u2019 to one other to solve a complex task. <|sop|>CLIP retrie.<default> <|sot|><|zh|><|en|><|asr|> <|sot|><|ru|><|asr|> 9% 19% 45% ObjectVocabcraftsman, powder, concrete ... I've never had a color coat separate from a house As prompt WhisperEncoder CLIP Whisper Decoder Figure 1: Framework for visually prompting Whisper. The exter- nal object vocab is dataset agnostic. in our How2 dataset have a ground truth transcription less than 30 words. This shows that Whisper is very robust to the noise and length of the prompt. For each model, the top 3 best performing number of object choices selected from How2 are used for the experiments on VisSpeech3, and the average WER is shown in figure 2. We see that the visually-informed prompt improves the performance for all four English models and three smaller multilingual models, but hurts the performance of the multilingual models Medium, Large, and LargeV2. In table 2, we compare the previous SotA AVSR results on VisSpeech with the audio-only Whisper perfor- mance, and Whisper Medium.en with 50 objects as the visual prompt. We highlight that visual prompt improve Medium.en by 9%, and even outperforms Large. Datasets and implementation details. Our main dataset for the AVSR task is the recently proposed VisSpeech [19], which is a subset of the instructional video dataset HowTo100M [22]. VisSpeech consists of those videos where an audio-only base- line ASR system performs badly, and whose visual stream and speech audio are semantically related. Since VisSpeech is pro- posed as a test set and it only contains 508 examples, we use another instructional video dataset, How2 [18], for hyperparam- eter tuning. We use a randomly selected 2000 example subset of How2, and add pub noise to the audio to increase the ASR difficulty similar to [19], since the dataset has been shown to be biased towards clean audio [19] preventing the visual modality from offering significant benefit to its ASR task. For the external object vocabulary, we follow [21] and used the label set of Ten- cent ML-Images [23], which contains around 10,000 common objects. The number of object K used in the prompt is tuned for each Whisper model separately on our version of the How2 dataset with three different noise levels (SNR=5,0,\u22125dB). Results. We found that on our How2 tuning set, using very large number of objects (as many as 90 objects) does not hurt performance. Our manual inspection shows that even when using 30 objects, there are already many irrelevant ones that got mis-retrieved by CLIP. For example, in the example shown in figure 1, we found \u2018yogurt\u2019, \u2018heavy cream\u2019, and \u2018mayonnaise\u2019 in the visual prompt. In addition, more than 90% of the utterances Remarks. We propose a prompting approach that that adapts the audio-only Whisper for audio-visual speech recog- nition. Based on figure 2, visual prompting helps most of the models with the exception of three larger multilingual models. However, because Large.en and LargeV2.en are not available, it is difficult to draw conclusions on whether it is the model size or multilinguality that hinders the model from benefiting from visual prompting. The fact that visual prompting improves the performance of Medium.en while degrading the performance of Medium suggests that the cause could be multilinguality. If this is the case, multilingual models may benefit from being fine-tuned on monolingual data. 4. Code-switched speech recognition Code-switched speech refers to the scenario where more than one language is used in the same utterance. With the raise of globalization and democratization of speech recognition tech- nologies, Code-switched ASR (CS-ASR) has become a popular research area [24]. While we cannot know for certain whether Whisper was trained on code-switched data, it is clear that the model\u2019s language and task tokens can not explicitly direct the model to do CS-ASR - each language token only represents one 3We use 3 number of objects choices to reduce the tuning noise introduced by the mismatch between How2 and VisSpeech. 16 Medium.en Audio + Visual Prompt 6 Large LargeV2 Base.en 8 Tiny.en English ModelsMultilingual Models Tiny Small.en Audio Only 10 18WER Small 12 Medium 14 Base Table 2: Comparison of model per- formance on VisSpeech. With vi- sual prompt, Medium.en outper- forms Large. Model Modality WER SotA [19] A+V 11.28 Whisper Medium.en Whisper Medium.en A A+V 8.35 7.60 Whisper Large Whisper LargeV2 A A 8.02 7.16 Figure 2: The effectiveness of visual prompt on VisSpeech across different models. of the 99 training languages, and the task tokens do not convey any information on whether the model"}, {"question": " What external object vocabulary is used in the proposed approach?", "answer": " Label set of Tencent ML-Images", "ref_chunk": "3. Audio-visual speech recognition The first task is using an ASR system to produce transcription for a video, where the on-screen visual content is semantically related to the speech audio and can therefore aid in recogni- tion [18, 19]. This task is related to, but more general than, performing audio-visual speech recognition (AVSR) on speech audio accompanied by a video of the speaker\u2019s facial or lip movements [20]. Approach. Our approach is shown in figure 1. To provide Whisper with a visually-conditioned prompt, we utilize the pop- ular vision-and-language CLIP [5] model along with an external vocabulary of common object words to first \u2018convert\u2019 the visual stream into a sequence of word tokens. To do so, we take every word/phrase in the external vocabulary, construct a sentence with template \u201cThis is a photo of a { }\u201d. Then we use the CLIP text en- coder to pre-compute an embedding vector for each sentence in an offline fashion. At inference time, for each video we sample 3 equally-spaced RGB image frames, use the CLIP image encoder to embed them, and calculate the similarity between the image embeddings and the pre-computed text embeddings. We select the top K objects whose embeddings have the highest similarity scores with the image embeddings for the prompt. Next, we con- catenate the K selected object names into a comma-separated list of words, and insert this token sequence into the previous text slot of the prompt. This method draws inspiration from the idea of Socratic Models [21], where an engineered interface enables large pretrained models to \u2018talk\u2019 to one other to solve a complex task. <|sop|>CLIP retrie.<default> <|sot|><|zh|><|en|><|asr|> <|sot|><|ru|><|asr|> 9% 19% 45% ObjectVocabcraftsman, powder, concrete ... I've never had a color coat separate from a house As prompt WhisperEncoder CLIP Whisper Decoder Figure 1: Framework for visually prompting Whisper. The exter- nal object vocab is dataset agnostic. in our How2 dataset have a ground truth transcription less than 30 words. This shows that Whisper is very robust to the noise and length of the prompt. For each model, the top 3 best performing number of object choices selected from How2 are used for the experiments on VisSpeech3, and the average WER is shown in figure 2. We see that the visually-informed prompt improves the performance for all four English models and three smaller multilingual models, but hurts the performance of the multilingual models Medium, Large, and LargeV2. In table 2, we compare the previous SotA AVSR results on VisSpeech with the audio-only Whisper perfor- mance, and Whisper Medium.en with 50 objects as the visual prompt. We highlight that visual prompt improve Medium.en by 9%, and even outperforms Large. Datasets and implementation details. Our main dataset for the AVSR task is the recently proposed VisSpeech [19], which is a subset of the instructional video dataset HowTo100M [22]. VisSpeech consists of those videos where an audio-only base- line ASR system performs badly, and whose visual stream and speech audio are semantically related. Since VisSpeech is pro- posed as a test set and it only contains 508 examples, we use another instructional video dataset, How2 [18], for hyperparam- eter tuning. We use a randomly selected 2000 example subset of How2, and add pub noise to the audio to increase the ASR difficulty similar to [19], since the dataset has been shown to be biased towards clean audio [19] preventing the visual modality from offering significant benefit to its ASR task. For the external object vocabulary, we follow [21] and used the label set of Ten- cent ML-Images [23], which contains around 10,000 common objects. The number of object K used in the prompt is tuned for each Whisper model separately on our version of the How2 dataset with three different noise levels (SNR=5,0,\u22125dB). Results. We found that on our How2 tuning set, using very large number of objects (as many as 90 objects) does not hurt performance. Our manual inspection shows that even when using 30 objects, there are already many irrelevant ones that got mis-retrieved by CLIP. For example, in the example shown in figure 1, we found \u2018yogurt\u2019, \u2018heavy cream\u2019, and \u2018mayonnaise\u2019 in the visual prompt. In addition, more than 90% of the utterances Remarks. We propose a prompting approach that that adapts the audio-only Whisper for audio-visual speech recog- nition. Based on figure 2, visual prompting helps most of the models with the exception of three larger multilingual models. However, because Large.en and LargeV2.en are not available, it is difficult to draw conclusions on whether it is the model size or multilinguality that hinders the model from benefiting from visual prompting. The fact that visual prompting improves the performance of Medium.en while degrading the performance of Medium suggests that the cause could be multilinguality. If this is the case, multilingual models may benefit from being fine-tuned on monolingual data. 4. Code-switched speech recognition Code-switched speech refers to the scenario where more than one language is used in the same utterance. With the raise of globalization and democratization of speech recognition tech- nologies, Code-switched ASR (CS-ASR) has become a popular research area [24]. While we cannot know for certain whether Whisper was trained on code-switched data, it is clear that the model\u2019s language and task tokens can not explicitly direct the model to do CS-ASR - each language token only represents one 3We use 3 number of objects choices to reduce the tuning noise introduced by the mismatch between How2 and VisSpeech. 16 Medium.en Audio + Visual Prompt 6 Large LargeV2 Base.en 8 Tiny.en English ModelsMultilingual Models Tiny Small.en Audio Only 10 18WER Small 12 Medium 14 Base Table 2: Comparison of model per- formance on VisSpeech. With vi- sual prompt, Medium.en outper- forms Large. Model Modality WER SotA [19] A+V 11.28 Whisper Medium.en Whisper Medium.en A A+V 8.35 7.60 Whisper Large Whisper LargeV2 A A 8.02 7.16 Figure 2: The effectiveness of visual prompt on VisSpeech across different models. of the 99 training languages, and the task tokens do not convey any information on whether the model"}, {"question": " What does the text state about the performance of using a large number of objects in the prompt?", "answer": " Using many objects does not hurt performance, but there are irrelevant objects that can be mis-retrieved", "ref_chunk": "3. Audio-visual speech recognition The first task is using an ASR system to produce transcription for a video, where the on-screen visual content is semantically related to the speech audio and can therefore aid in recogni- tion [18, 19]. This task is related to, but more general than, performing audio-visual speech recognition (AVSR) on speech audio accompanied by a video of the speaker\u2019s facial or lip movements [20]. Approach. Our approach is shown in figure 1. To provide Whisper with a visually-conditioned prompt, we utilize the pop- ular vision-and-language CLIP [5] model along with an external vocabulary of common object words to first \u2018convert\u2019 the visual stream into a sequence of word tokens. To do so, we take every word/phrase in the external vocabulary, construct a sentence with template \u201cThis is a photo of a { }\u201d. Then we use the CLIP text en- coder to pre-compute an embedding vector for each sentence in an offline fashion. At inference time, for each video we sample 3 equally-spaced RGB image frames, use the CLIP image encoder to embed them, and calculate the similarity between the image embeddings and the pre-computed text embeddings. We select the top K objects whose embeddings have the highest similarity scores with the image embeddings for the prompt. Next, we con- catenate the K selected object names into a comma-separated list of words, and insert this token sequence into the previous text slot of the prompt. This method draws inspiration from the idea of Socratic Models [21], where an engineered interface enables large pretrained models to \u2018talk\u2019 to one other to solve a complex task. <|sop|>CLIP retrie.<default> <|sot|><|zh|><|en|><|asr|> <|sot|><|ru|><|asr|> 9% 19% 45% ObjectVocabcraftsman, powder, concrete ... I've never had a color coat separate from a house As prompt WhisperEncoder CLIP Whisper Decoder Figure 1: Framework for visually prompting Whisper. The exter- nal object vocab is dataset agnostic. in our How2 dataset have a ground truth transcription less than 30 words. This shows that Whisper is very robust to the noise and length of the prompt. For each model, the top 3 best performing number of object choices selected from How2 are used for the experiments on VisSpeech3, and the average WER is shown in figure 2. We see that the visually-informed prompt improves the performance for all four English models and three smaller multilingual models, but hurts the performance of the multilingual models Medium, Large, and LargeV2. In table 2, we compare the previous SotA AVSR results on VisSpeech with the audio-only Whisper perfor- mance, and Whisper Medium.en with 50 objects as the visual prompt. We highlight that visual prompt improve Medium.en by 9%, and even outperforms Large. Datasets and implementation details. Our main dataset for the AVSR task is the recently proposed VisSpeech [19], which is a subset of the instructional video dataset HowTo100M [22]. VisSpeech consists of those videos where an audio-only base- line ASR system performs badly, and whose visual stream and speech audio are semantically related. Since VisSpeech is pro- posed as a test set and it only contains 508 examples, we use another instructional video dataset, How2 [18], for hyperparam- eter tuning. We use a randomly selected 2000 example subset of How2, and add pub noise to the audio to increase the ASR difficulty similar to [19], since the dataset has been shown to be biased towards clean audio [19] preventing the visual modality from offering significant benefit to its ASR task. For the external object vocabulary, we follow [21] and used the label set of Ten- cent ML-Images [23], which contains around 10,000 common objects. The number of object K used in the prompt is tuned for each Whisper model separately on our version of the How2 dataset with three different noise levels (SNR=5,0,\u22125dB). Results. We found that on our How2 tuning set, using very large number of objects (as many as 90 objects) does not hurt performance. Our manual inspection shows that even when using 30 objects, there are already many irrelevant ones that got mis-retrieved by CLIP. For example, in the example shown in figure 1, we found \u2018yogurt\u2019, \u2018heavy cream\u2019, and \u2018mayonnaise\u2019 in the visual prompt. In addition, more than 90% of the utterances Remarks. We propose a prompting approach that that adapts the audio-only Whisper for audio-visual speech recog- nition. Based on figure 2, visual prompting helps most of the models with the exception of three larger multilingual models. However, because Large.en and LargeV2.en are not available, it is difficult to draw conclusions on whether it is the model size or multilinguality that hinders the model from benefiting from visual prompting. The fact that visual prompting improves the performance of Medium.en while degrading the performance of Medium suggests that the cause could be multilinguality. If this is the case, multilingual models may benefit from being fine-tuned on monolingual data. 4. Code-switched speech recognition Code-switched speech refers to the scenario where more than one language is used in the same utterance. With the raise of globalization and democratization of speech recognition tech- nologies, Code-switched ASR (CS-ASR) has become a popular research area [24]. While we cannot know for certain whether Whisper was trained on code-switched data, it is clear that the model\u2019s language and task tokens can not explicitly direct the model to do CS-ASR - each language token only represents one 3We use 3 number of objects choices to reduce the tuning noise introduced by the mismatch between How2 and VisSpeech. 16 Medium.en Audio + Visual Prompt 6 Large LargeV2 Base.en 8 Tiny.en English ModelsMultilingual Models Tiny Small.en Audio Only 10 18WER Small 12 Medium 14 Base Table 2: Comparison of model per- formance on VisSpeech. With vi- sual prompt, Medium.en outper- forms Large. Model Modality WER SotA [19] A+V 11.28 Whisper Medium.en Whisper Medium.en A A+V 8.35 7.60 Whisper Large Whisper LargeV2 A A 8.02 7.16 Figure 2: The effectiveness of visual prompt on VisSpeech across different models. of the 99 training languages, and the task tokens do not convey any information on whether the model"}], "doc_text": "3. Audio-visual speech recognition The first task is using an ASR system to produce transcription for a video, where the on-screen visual content is semantically related to the speech audio and can therefore aid in recogni- tion [18, 19]. This task is related to, but more general than, performing audio-visual speech recognition (AVSR) on speech audio accompanied by a video of the speaker\u2019s facial or lip movements [20]. Approach. Our approach is shown in figure 1. To provide Whisper with a visually-conditioned prompt, we utilize the pop- ular vision-and-language CLIP [5] model along with an external vocabulary of common object words to first \u2018convert\u2019 the visual stream into a sequence of word tokens. To do so, we take every word/phrase in the external vocabulary, construct a sentence with template \u201cThis is a photo of a { }\u201d. Then we use the CLIP text en- coder to pre-compute an embedding vector for each sentence in an offline fashion. At inference time, for each video we sample 3 equally-spaced RGB image frames, use the CLIP image encoder to embed them, and calculate the similarity between the image embeddings and the pre-computed text embeddings. We select the top K objects whose embeddings have the highest similarity scores with the image embeddings for the prompt. Next, we con- catenate the K selected object names into a comma-separated list of words, and insert this token sequence into the previous text slot of the prompt. This method draws inspiration from the idea of Socratic Models [21], where an engineered interface enables large pretrained models to \u2018talk\u2019 to one other to solve a complex task. <|sop|>CLIP retrie.<default> <|sot|><|zh|><|en|><|asr|> <|sot|><|ru|><|asr|> 9% 19% 45% ObjectVocabcraftsman, powder, concrete ... I've never had a color coat separate from a house As prompt WhisperEncoder CLIP Whisper Decoder Figure 1: Framework for visually prompting Whisper. The exter- nal object vocab is dataset agnostic. in our How2 dataset have a ground truth transcription less than 30 words. This shows that Whisper is very robust to the noise and length of the prompt. For each model, the top 3 best performing number of object choices selected from How2 are used for the experiments on VisSpeech3, and the average WER is shown in figure 2. We see that the visually-informed prompt improves the performance for all four English models and three smaller multilingual models, but hurts the performance of the multilingual models Medium, Large, and LargeV2. In table 2, we compare the previous SotA AVSR results on VisSpeech with the audio-only Whisper perfor- mance, and Whisper Medium.en with 50 objects as the visual prompt. We highlight that visual prompt improve Medium.en by 9%, and even outperforms Large. Datasets and implementation details. Our main dataset for the AVSR task is the recently proposed VisSpeech [19], which is a subset of the instructional video dataset HowTo100M [22]. VisSpeech consists of those videos where an audio-only base- line ASR system performs badly, and whose visual stream and speech audio are semantically related. Since VisSpeech is pro- posed as a test set and it only contains 508 examples, we use another instructional video dataset, How2 [18], for hyperparam- eter tuning. We use a randomly selected 2000 example subset of How2, and add pub noise to the audio to increase the ASR difficulty similar to [19], since the dataset has been shown to be biased towards clean audio [19] preventing the visual modality from offering significant benefit to its ASR task. For the external object vocabulary, we follow [21] and used the label set of Ten- cent ML-Images [23], which contains around 10,000 common objects. The number of object K used in the prompt is tuned for each Whisper model separately on our version of the How2 dataset with three different noise levels (SNR=5,0,\u22125dB). Results. We found that on our How2 tuning set, using very large number of objects (as many as 90 objects) does not hurt performance. Our manual inspection shows that even when using 30 objects, there are already many irrelevant ones that got mis-retrieved by CLIP. For example, in the example shown in figure 1, we found \u2018yogurt\u2019, \u2018heavy cream\u2019, and \u2018mayonnaise\u2019 in the visual prompt. In addition, more than 90% of the utterances Remarks. We propose a prompting approach that that adapts the audio-only Whisper for audio-visual speech recog- nition. Based on figure 2, visual prompting helps most of the models with the exception of three larger multilingual models. However, because Large.en and LargeV2.en are not available, it is difficult to draw conclusions on whether it is the model size or multilinguality that hinders the model from benefiting from visual prompting. The fact that visual prompting improves the performance of Medium.en while degrading the performance of Medium suggests that the cause could be multilinguality. If this is the case, multilingual models may benefit from being fine-tuned on monolingual data. 4. Code-switched speech recognition Code-switched speech refers to the scenario where more than one language is used in the same utterance. With the raise of globalization and democratization of speech recognition tech- nologies, Code-switched ASR (CS-ASR) has become a popular research area [24]. While we cannot know for certain whether Whisper was trained on code-switched data, it is clear that the model\u2019s language and task tokens can not explicitly direct the model to do CS-ASR - each language token only represents one 3We use 3 number of objects choices to reduce the tuning noise introduced by the mismatch between How2 and VisSpeech. 16 Medium.en Audio + Visual Prompt 6 Large LargeV2 Base.en 8 Tiny.en English ModelsMultilingual Models Tiny Small.en Audio Only 10 18WER Small 12 Medium 14 Base Table 2: Comparison of model per- formance on VisSpeech. With vi- sual prompt, Medium.en outper- forms Large. Model Modality WER SotA [19] A+V 11.28 Whisper Medium.en Whisper Medium.en A A+V 8.35 7.60 Whisper Large Whisper LargeV2 A A 8.02 7.16 Figure 2: The effectiveness of visual prompt on VisSpeech across different models. of the 99 training languages, and the task tokens do not convey any information on whether the model"}