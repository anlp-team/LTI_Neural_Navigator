{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_The_CHiME-7_DASR_Challenge:_Distant_Meeting_Transcription_with_Multiple_Devices_in_Diverse_Scenarios_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What makes the CHiME-7 DASR task more accessible to participants with limited resources?", "answer": " Allowing the use of pre-trained models, which makes training more efficient and quicker.", "ref_chunk": "allowing the use of pre-trained models, the CHiME-7 DASR task becomes more accessible to participants with limited re- sources, since training is often more efficient and quicker with such models. While SSLR models can be quite large, the fea- tures could be pre-extracted, thus enabling significant computa- tional savings in the training phase. From a research perspec- tive, it may be interesting to investigate how such models, which are trained in monoaural conditions, can make effective use of multi-channel audio and/or can be distilled into smaller models. 2.3. Open-Source Datasets The allowable external data sources are expanded to in- clude many commonly used open-source datasets (e.g. Lib- riSpeech [22] and FSD50k [33]). They facilitate the use of models for which, in prior challenges, necessary training data were lacking, but also enables the study of new research di- rections. For example, participants can create large synthetic datasets to train powerful deep neural network (DNN)-based speech separation and enhancement (SSE) [34, 35]. The struc- ture of the challenge then encourages research into whether such models can effectively help improving real-world meeting transcription. This latter research direction also intersects with this year\u2019s \u201csister\u201d CHiME-7 UDASE challenge, whose focus is on unsupervised domain adaptation for speech enhancement. 3. Challenge Tracks & Rules The CHiME-7 DASR challenge features two tracks \u2014 a main track, and an optional sub-track, as described below. 3.1. Main Track For the main track, we provide multi-channel recordings for the whole session without segmentation or speaker labels, and participants are required to generate time-marked, speaker- attributed transcripts, as shown in Fig. 1. For any session, let ri = (\u2206, s, v) denote a time-marked, speaker-attributed tran- script of a segment, where \u2206 = (tst, ten) is a tuple containing start and end times, s \u2208 Z+ is the speaker label2, and v \u2208 \u03a3\u2217 represents the transcript for the segment, for some vocabulary \u03a3. A session is then completely defined by R = {r1, . . . , rN }, which we call the reference. Challenge participants are required to estimate the reference through generated hypothesis, denoted by H = {h1, . . . , h \u02c6N }. 2Actual label names may be arbitrary, but can be mapped to the set of positive integers without loss of generality. DA-WER = 20%Used forranking {\"end_time\": \"11.370\",\"start_time\": \"11.000\", \"words\": \"so ummm\",\"speaker\": \"P03\",{ \"session_id\": \"S05\"},\"end_time\": \"14.110\", \"start_time\": \"12.100\", \"words\": \"where is he?\" \"speaker\": \"P01\",} \"session_id\": \"S05\" DER =7.14% {\"end_time\": \"11.350\",\"start_time\": \"11.010\", \"words\": \"so\",\"speaker\": \"spk1\",{ \"session_id\": \"S05\" },\"end_time\": \"14.150\", \"start_time\": \"12.000\", \"words\": \"Where is\", \"speaker\": \"spk2\",} \"session_id\": \"S05\"ReferenceHypothesis Speaker assignment DER-basedevaluation WER-basedevaluation \"P03\": \"spk1\" \"P01\": \"spk2\" Figure 1: Evaluation scheme for CHiME-7 DASR main track. Optimal speaker assignments are first determined using a DER- based evaluation, and then used to compute SA-WER, which is used as the final ranking metric. In this challenge, estimated speaker labels \u02c6s are not required to be identical to the reference labels. This flexibility poses a challenge in computing speaker-attributed word error rates (SA- WER) since, unlike earlier studies [36], the mapping between reference and estimated labels is not known. We solve this problem using mechanisms employed in the evaluation of diarization systems, as shown in Fig. 1. Let \u03a8 : Z+ \u2192 Z+ \u222a {\u03d5} denote a mapping from the reference la- bels to the estimated labels. We use the Hungarian method [37] to obtain the optimal mapping \u02c6\u03a8 that minimizes the diarization error rate (DER) between the reference segments and hypothe- sized segments, i.e., \u02c6\u03a8 = arg min \u03a8 DER(R, H), where the DER is calculated with a 250 ms collar, following the setup in CHiME-6. Given \u02c6\u03a8, we compute this newly proposed diarization attributed WER (DA-WER) metric as DA-WER(R, H) = (cid:80) s L(\u2322 Rs, \u2322 H \u02c6\u03a8(s)) s | \u2322 Rs| (cid:80) , where Rs denotes segments in R with speaker s, L is the Levenshtein distance, and the \u2322 operator concatenates seg- ment transcripts of its operand. Final ranking of systems will be based on the DA-WER metric macro-averaged across all scenarios. The macro-average operation is to encourage par- ticipants to develop a model whose performance is consistent across all three scenarios. Note that this evaluation is different from the concatenated minimum-permutation WER (cpWER) based ranking in the CHiME-6 challenge, since it requires par- ticipants to also produce estimates of the utterances boundaries. Utterance boundaries estimation are important in actual appli- cations, e.g. for double checking the transcripts. It also differs from the Asclite multi-dimensional Levenshtein metric [38], which can account for segmentation errors when the time-based cost is used. Contrary to this latter, DA-WER does not re- quire word-level alignment, which is quite time-consuming to double-check for ground-truth purposes. DA-WER instead of- fers a simple and \u201cloose\u201d way to encourage participants to pro- duce reasonable segmentation at the utterance level, which is sufficient in most applications. Future work is needed to devise more principled metrics for joint ASR and diarization. 3.2. Optional Sub-Track Similar to the CHiME-6 challenge, we include an optional sub- track where participants can use oracle diarization. The goal of this sub-track is to assess the acoustic front-end and ASR performance and disentangle it from the impact of diarization. The segmentation and speaker labels are known and are freely usable by participants, here the problem reduces to a standard (1) (2) ASR task. 3.3. Rules Detailed description of the task rules are available in the CH- iME-7 DASR website3. Hereafter we briefly summarize them. \u2022 Only some external data and pre-trained models are allowed. Participants are encouraged to propose additional ones in the first month. Close-talk microphones cannot be used for inference. \u2022 Automatic domain identification is prohibited. This includes the use of a-priori information such as array topology (includ- ing number of channels). A single system must be submitted for all three scenarios. Automatic channel selection is allowed and encouraged. \u2022 Self-supervised adaptation/training is allowed, as long as it is performed for each evaluation session independently i.e. as it would happen in a real-world deployment. Participants may use all available data"}, {"question": " How can significant computational savings be achieved in the training phase of the CHiME-7 DASR task?", "answer": " By pre-extracting features in SSLR models.", "ref_chunk": "allowing the use of pre-trained models, the CHiME-7 DASR task becomes more accessible to participants with limited re- sources, since training is often more efficient and quicker with such models. While SSLR models can be quite large, the fea- tures could be pre-extracted, thus enabling significant computa- tional savings in the training phase. From a research perspec- tive, it may be interesting to investigate how such models, which are trained in monoaural conditions, can make effective use of multi-channel audio and/or can be distilled into smaller models. 2.3. Open-Source Datasets The allowable external data sources are expanded to in- clude many commonly used open-source datasets (e.g. Lib- riSpeech [22] and FSD50k [33]). They facilitate the use of models for which, in prior challenges, necessary training data were lacking, but also enables the study of new research di- rections. For example, participants can create large synthetic datasets to train powerful deep neural network (DNN)-based speech separation and enhancement (SSE) [34, 35]. The struc- ture of the challenge then encourages research into whether such models can effectively help improving real-world meeting transcription. This latter research direction also intersects with this year\u2019s \u201csister\u201d CHiME-7 UDASE challenge, whose focus is on unsupervised domain adaptation for speech enhancement. 3. Challenge Tracks & Rules The CHiME-7 DASR challenge features two tracks \u2014 a main track, and an optional sub-track, as described below. 3.1. Main Track For the main track, we provide multi-channel recordings for the whole session without segmentation or speaker labels, and participants are required to generate time-marked, speaker- attributed transcripts, as shown in Fig. 1. For any session, let ri = (\u2206, s, v) denote a time-marked, speaker-attributed tran- script of a segment, where \u2206 = (tst, ten) is a tuple containing start and end times, s \u2208 Z+ is the speaker label2, and v \u2208 \u03a3\u2217 represents the transcript for the segment, for some vocabulary \u03a3. A session is then completely defined by R = {r1, . . . , rN }, which we call the reference. Challenge participants are required to estimate the reference through generated hypothesis, denoted by H = {h1, . . . , h \u02c6N }. 2Actual label names may be arbitrary, but can be mapped to the set of positive integers without loss of generality. DA-WER = 20%Used forranking {\"end_time\": \"11.370\",\"start_time\": \"11.000\", \"words\": \"so ummm\",\"speaker\": \"P03\",{ \"session_id\": \"S05\"},\"end_time\": \"14.110\", \"start_time\": \"12.100\", \"words\": \"where is he?\" \"speaker\": \"P01\",} \"session_id\": \"S05\" DER =7.14% {\"end_time\": \"11.350\",\"start_time\": \"11.010\", \"words\": \"so\",\"speaker\": \"spk1\",{ \"session_id\": \"S05\" },\"end_time\": \"14.150\", \"start_time\": \"12.000\", \"words\": \"Where is\", \"speaker\": \"spk2\",} \"session_id\": \"S05\"ReferenceHypothesis Speaker assignment DER-basedevaluation WER-basedevaluation \"P03\": \"spk1\" \"P01\": \"spk2\" Figure 1: Evaluation scheme for CHiME-7 DASR main track. Optimal speaker assignments are first determined using a DER- based evaluation, and then used to compute SA-WER, which is used as the final ranking metric. In this challenge, estimated speaker labels \u02c6s are not required to be identical to the reference labels. This flexibility poses a challenge in computing speaker-attributed word error rates (SA- WER) since, unlike earlier studies [36], the mapping between reference and estimated labels is not known. We solve this problem using mechanisms employed in the evaluation of diarization systems, as shown in Fig. 1. Let \u03a8 : Z+ \u2192 Z+ \u222a {\u03d5} denote a mapping from the reference la- bels to the estimated labels. We use the Hungarian method [37] to obtain the optimal mapping \u02c6\u03a8 that minimizes the diarization error rate (DER) between the reference segments and hypothe- sized segments, i.e., \u02c6\u03a8 = arg min \u03a8 DER(R, H), where the DER is calculated with a 250 ms collar, following the setup in CHiME-6. Given \u02c6\u03a8, we compute this newly proposed diarization attributed WER (DA-WER) metric as DA-WER(R, H) = (cid:80) s L(\u2322 Rs, \u2322 H \u02c6\u03a8(s)) s | \u2322 Rs| (cid:80) , where Rs denotes segments in R with speaker s, L is the Levenshtein distance, and the \u2322 operator concatenates seg- ment transcripts of its operand. Final ranking of systems will be based on the DA-WER metric macro-averaged across all scenarios. The macro-average operation is to encourage par- ticipants to develop a model whose performance is consistent across all three scenarios. Note that this evaluation is different from the concatenated minimum-permutation WER (cpWER) based ranking in the CHiME-6 challenge, since it requires par- ticipants to also produce estimates of the utterances boundaries. Utterance boundaries estimation are important in actual appli- cations, e.g. for double checking the transcripts. It also differs from the Asclite multi-dimensional Levenshtein metric [38], which can account for segmentation errors when the time-based cost is used. Contrary to this latter, DA-WER does not re- quire word-level alignment, which is quite time-consuming to double-check for ground-truth purposes. DA-WER instead of- fers a simple and \u201cloose\u201d way to encourage participants to pro- duce reasonable segmentation at the utterance level, which is sufficient in most applications. Future work is needed to devise more principled metrics for joint ASR and diarization. 3.2. Optional Sub-Track Similar to the CHiME-6 challenge, we include an optional sub- track where participants can use oracle diarization. The goal of this sub-track is to assess the acoustic front-end and ASR performance and disentangle it from the impact of diarization. The segmentation and speaker labels are known and are freely usable by participants, here the problem reduces to a standard (1) (2) ASR task. 3.3. Rules Detailed description of the task rules are available in the CH- iME-7 DASR website3. Hereafter we briefly summarize them. \u2022 Only some external data and pre-trained models are allowed. Participants are encouraged to propose additional ones in the first month. Close-talk microphones cannot be used for inference. \u2022 Automatic domain identification is prohibited. This includes the use of a-priori information such as array topology (includ- ing number of channels). A single system must be submitted for all three scenarios. Automatic channel selection is allowed and encouraged. \u2022 Self-supervised adaptation/training is allowed, as long as it is performed for each evaluation session independently i.e. as it would happen in a real-world deployment. Participants may use all available data"}, {"question": " What are some examples of open-source datasets expanded for the CHiME-7 DASR challenge?", "answer": " LibriSpeech and FSD50k.", "ref_chunk": "allowing the use of pre-trained models, the CHiME-7 DASR task becomes more accessible to participants with limited re- sources, since training is often more efficient and quicker with such models. While SSLR models can be quite large, the fea- tures could be pre-extracted, thus enabling significant computa- tional savings in the training phase. From a research perspec- tive, it may be interesting to investigate how such models, which are trained in monoaural conditions, can make effective use of multi-channel audio and/or can be distilled into smaller models. 2.3. Open-Source Datasets The allowable external data sources are expanded to in- clude many commonly used open-source datasets (e.g. Lib- riSpeech [22] and FSD50k [33]). They facilitate the use of models for which, in prior challenges, necessary training data were lacking, but also enables the study of new research di- rections. For example, participants can create large synthetic datasets to train powerful deep neural network (DNN)-based speech separation and enhancement (SSE) [34, 35]. The struc- ture of the challenge then encourages research into whether such models can effectively help improving real-world meeting transcription. This latter research direction also intersects with this year\u2019s \u201csister\u201d CHiME-7 UDASE challenge, whose focus is on unsupervised domain adaptation for speech enhancement. 3. Challenge Tracks & Rules The CHiME-7 DASR challenge features two tracks \u2014 a main track, and an optional sub-track, as described below. 3.1. Main Track For the main track, we provide multi-channel recordings for the whole session without segmentation or speaker labels, and participants are required to generate time-marked, speaker- attributed transcripts, as shown in Fig. 1. For any session, let ri = (\u2206, s, v) denote a time-marked, speaker-attributed tran- script of a segment, where \u2206 = (tst, ten) is a tuple containing start and end times, s \u2208 Z+ is the speaker label2, and v \u2208 \u03a3\u2217 represents the transcript for the segment, for some vocabulary \u03a3. A session is then completely defined by R = {r1, . . . , rN }, which we call the reference. Challenge participants are required to estimate the reference through generated hypothesis, denoted by H = {h1, . . . , h \u02c6N }. 2Actual label names may be arbitrary, but can be mapped to the set of positive integers without loss of generality. DA-WER = 20%Used forranking {\"end_time\": \"11.370\",\"start_time\": \"11.000\", \"words\": \"so ummm\",\"speaker\": \"P03\",{ \"session_id\": \"S05\"},\"end_time\": \"14.110\", \"start_time\": \"12.100\", \"words\": \"where is he?\" \"speaker\": \"P01\",} \"session_id\": \"S05\" DER =7.14% {\"end_time\": \"11.350\",\"start_time\": \"11.010\", \"words\": \"so\",\"speaker\": \"spk1\",{ \"session_id\": \"S05\" },\"end_time\": \"14.150\", \"start_time\": \"12.000\", \"words\": \"Where is\", \"speaker\": \"spk2\",} \"session_id\": \"S05\"ReferenceHypothesis Speaker assignment DER-basedevaluation WER-basedevaluation \"P03\": \"spk1\" \"P01\": \"spk2\" Figure 1: Evaluation scheme for CHiME-7 DASR main track. Optimal speaker assignments are first determined using a DER- based evaluation, and then used to compute SA-WER, which is used as the final ranking metric. In this challenge, estimated speaker labels \u02c6s are not required to be identical to the reference labels. This flexibility poses a challenge in computing speaker-attributed word error rates (SA- WER) since, unlike earlier studies [36], the mapping between reference and estimated labels is not known. We solve this problem using mechanisms employed in the evaluation of diarization systems, as shown in Fig. 1. Let \u03a8 : Z+ \u2192 Z+ \u222a {\u03d5} denote a mapping from the reference la- bels to the estimated labels. We use the Hungarian method [37] to obtain the optimal mapping \u02c6\u03a8 that minimizes the diarization error rate (DER) between the reference segments and hypothe- sized segments, i.e., \u02c6\u03a8 = arg min \u03a8 DER(R, H), where the DER is calculated with a 250 ms collar, following the setup in CHiME-6. Given \u02c6\u03a8, we compute this newly proposed diarization attributed WER (DA-WER) metric as DA-WER(R, H) = (cid:80) s L(\u2322 Rs, \u2322 H \u02c6\u03a8(s)) s | \u2322 Rs| (cid:80) , where Rs denotes segments in R with speaker s, L is the Levenshtein distance, and the \u2322 operator concatenates seg- ment transcripts of its operand. Final ranking of systems will be based on the DA-WER metric macro-averaged across all scenarios. The macro-average operation is to encourage par- ticipants to develop a model whose performance is consistent across all three scenarios. Note that this evaluation is different from the concatenated minimum-permutation WER (cpWER) based ranking in the CHiME-6 challenge, since it requires par- ticipants to also produce estimates of the utterances boundaries. Utterance boundaries estimation are important in actual appli- cations, e.g. for double checking the transcripts. It also differs from the Asclite multi-dimensional Levenshtein metric [38], which can account for segmentation errors when the time-based cost is used. Contrary to this latter, DA-WER does not re- quire word-level alignment, which is quite time-consuming to double-check for ground-truth purposes. DA-WER instead of- fers a simple and \u201cloose\u201d way to encourage participants to pro- duce reasonable segmentation at the utterance level, which is sufficient in most applications. Future work is needed to devise more principled metrics for joint ASR and diarization. 3.2. Optional Sub-Track Similar to the CHiME-6 challenge, we include an optional sub- track where participants can use oracle diarization. The goal of this sub-track is to assess the acoustic front-end and ASR performance and disentangle it from the impact of diarization. The segmentation and speaker labels are known and are freely usable by participants, here the problem reduces to a standard (1) (2) ASR task. 3.3. Rules Detailed description of the task rules are available in the CH- iME-7 DASR website3. Hereafter we briefly summarize them. \u2022 Only some external data and pre-trained models are allowed. Participants are encouraged to propose additional ones in the first month. Close-talk microphones cannot be used for inference. \u2022 Automatic domain identification is prohibited. This includes the use of a-priori information such as array topology (includ- ing number of channels). A single system must be submitted for all three scenarios. Automatic channel selection is allowed and encouraged. \u2022 Self-supervised adaptation/training is allowed, as long as it is performed for each evaluation session independently i.e. as it would happen in a real-world deployment. Participants may use all available data"}, {"question": " What type of research direction does the use of open-source datasets facilitate in the CHiME-7 DASR challenge?", "answer": " The study of new research directions and the creation of large synthetic datasets for training deep neural network-based speech separation and enhancement.", "ref_chunk": "allowing the use of pre-trained models, the CHiME-7 DASR task becomes more accessible to participants with limited re- sources, since training is often more efficient and quicker with such models. While SSLR models can be quite large, the fea- tures could be pre-extracted, thus enabling significant computa- tional savings in the training phase. From a research perspec- tive, it may be interesting to investigate how such models, which are trained in monoaural conditions, can make effective use of multi-channel audio and/or can be distilled into smaller models. 2.3. Open-Source Datasets The allowable external data sources are expanded to in- clude many commonly used open-source datasets (e.g. Lib- riSpeech [22] and FSD50k [33]). They facilitate the use of models for which, in prior challenges, necessary training data were lacking, but also enables the study of new research di- rections. For example, participants can create large synthetic datasets to train powerful deep neural network (DNN)-based speech separation and enhancement (SSE) [34, 35]. The struc- ture of the challenge then encourages research into whether such models can effectively help improving real-world meeting transcription. This latter research direction also intersects with this year\u2019s \u201csister\u201d CHiME-7 UDASE challenge, whose focus is on unsupervised domain adaptation for speech enhancement. 3. Challenge Tracks & Rules The CHiME-7 DASR challenge features two tracks \u2014 a main track, and an optional sub-track, as described below. 3.1. Main Track For the main track, we provide multi-channel recordings for the whole session without segmentation or speaker labels, and participants are required to generate time-marked, speaker- attributed transcripts, as shown in Fig. 1. For any session, let ri = (\u2206, s, v) denote a time-marked, speaker-attributed tran- script of a segment, where \u2206 = (tst, ten) is a tuple containing start and end times, s \u2208 Z+ is the speaker label2, and v \u2208 \u03a3\u2217 represents the transcript for the segment, for some vocabulary \u03a3. A session is then completely defined by R = {r1, . . . , rN }, which we call the reference. Challenge participants are required to estimate the reference through generated hypothesis, denoted by H = {h1, . . . , h \u02c6N }. 2Actual label names may be arbitrary, but can be mapped to the set of positive integers without loss of generality. DA-WER = 20%Used forranking {\"end_time\": \"11.370\",\"start_time\": \"11.000\", \"words\": \"so ummm\",\"speaker\": \"P03\",{ \"session_id\": \"S05\"},\"end_time\": \"14.110\", \"start_time\": \"12.100\", \"words\": \"where is he?\" \"speaker\": \"P01\",} \"session_id\": \"S05\" DER =7.14% {\"end_time\": \"11.350\",\"start_time\": \"11.010\", \"words\": \"so\",\"speaker\": \"spk1\",{ \"session_id\": \"S05\" },\"end_time\": \"14.150\", \"start_time\": \"12.000\", \"words\": \"Where is\", \"speaker\": \"spk2\",} \"session_id\": \"S05\"ReferenceHypothesis Speaker assignment DER-basedevaluation WER-basedevaluation \"P03\": \"spk1\" \"P01\": \"spk2\" Figure 1: Evaluation scheme for CHiME-7 DASR main track. Optimal speaker assignments are first determined using a DER- based evaluation, and then used to compute SA-WER, which is used as the final ranking metric. In this challenge, estimated speaker labels \u02c6s are not required to be identical to the reference labels. This flexibility poses a challenge in computing speaker-attributed word error rates (SA- WER) since, unlike earlier studies [36], the mapping between reference and estimated labels is not known. We solve this problem using mechanisms employed in the evaluation of diarization systems, as shown in Fig. 1. Let \u03a8 : Z+ \u2192 Z+ \u222a {\u03d5} denote a mapping from the reference la- bels to the estimated labels. We use the Hungarian method [37] to obtain the optimal mapping \u02c6\u03a8 that minimizes the diarization error rate (DER) between the reference segments and hypothe- sized segments, i.e., \u02c6\u03a8 = arg min \u03a8 DER(R, H), where the DER is calculated with a 250 ms collar, following the setup in CHiME-6. Given \u02c6\u03a8, we compute this newly proposed diarization attributed WER (DA-WER) metric as DA-WER(R, H) = (cid:80) s L(\u2322 Rs, \u2322 H \u02c6\u03a8(s)) s | \u2322 Rs| (cid:80) , where Rs denotes segments in R with speaker s, L is the Levenshtein distance, and the \u2322 operator concatenates seg- ment transcripts of its operand. Final ranking of systems will be based on the DA-WER metric macro-averaged across all scenarios. The macro-average operation is to encourage par- ticipants to develop a model whose performance is consistent across all three scenarios. Note that this evaluation is different from the concatenated minimum-permutation WER (cpWER) based ranking in the CHiME-6 challenge, since it requires par- ticipants to also produce estimates of the utterances boundaries. Utterance boundaries estimation are important in actual appli- cations, e.g. for double checking the transcripts. It also differs from the Asclite multi-dimensional Levenshtein metric [38], which can account for segmentation errors when the time-based cost is used. Contrary to this latter, DA-WER does not re- quire word-level alignment, which is quite time-consuming to double-check for ground-truth purposes. DA-WER instead of- fers a simple and \u201cloose\u201d way to encourage participants to pro- duce reasonable segmentation at the utterance level, which is sufficient in most applications. Future work is needed to devise more principled metrics for joint ASR and diarization. 3.2. Optional Sub-Track Similar to the CHiME-6 challenge, we include an optional sub- track where participants can use oracle diarization. The goal of this sub-track is to assess the acoustic front-end and ASR performance and disentangle it from the impact of diarization. The segmentation and speaker labels are known and are freely usable by participants, here the problem reduces to a standard (1) (2) ASR task. 3.3. Rules Detailed description of the task rules are available in the CH- iME-7 DASR website3. Hereafter we briefly summarize them. \u2022 Only some external data and pre-trained models are allowed. Participants are encouraged to propose additional ones in the first month. Close-talk microphones cannot be used for inference. \u2022 Automatic domain identification is prohibited. This includes the use of a-priori information such as array topology (includ- ing number of channels). A single system must be submitted for all three scenarios. Automatic channel selection is allowed and encouraged. \u2022 Self-supervised adaptation/training is allowed, as long as it is performed for each evaluation session independently i.e. as it would happen in a real-world deployment. Participants may use all available data"}, {"question": " What is the structure of the CHiME-7 DASR challenge with regards to tracks?", "answer": " It features two tracks - a main track and an optional sub-track.", "ref_chunk": "allowing the use of pre-trained models, the CHiME-7 DASR task becomes more accessible to participants with limited re- sources, since training is often more efficient and quicker with such models. While SSLR models can be quite large, the fea- tures could be pre-extracted, thus enabling significant computa- tional savings in the training phase. From a research perspec- tive, it may be interesting to investigate how such models, which are trained in monoaural conditions, can make effective use of multi-channel audio and/or can be distilled into smaller models. 2.3. Open-Source Datasets The allowable external data sources are expanded to in- clude many commonly used open-source datasets (e.g. Lib- riSpeech [22] and FSD50k [33]). They facilitate the use of models for which, in prior challenges, necessary training data were lacking, but also enables the study of new research di- rections. For example, participants can create large synthetic datasets to train powerful deep neural network (DNN)-based speech separation and enhancement (SSE) [34, 35]. The struc- ture of the challenge then encourages research into whether such models can effectively help improving real-world meeting transcription. This latter research direction also intersects with this year\u2019s \u201csister\u201d CHiME-7 UDASE challenge, whose focus is on unsupervised domain adaptation for speech enhancement. 3. Challenge Tracks & Rules The CHiME-7 DASR challenge features two tracks \u2014 a main track, and an optional sub-track, as described below. 3.1. Main Track For the main track, we provide multi-channel recordings for the whole session without segmentation or speaker labels, and participants are required to generate time-marked, speaker- attributed transcripts, as shown in Fig. 1. For any session, let ri = (\u2206, s, v) denote a time-marked, speaker-attributed tran- script of a segment, where \u2206 = (tst, ten) is a tuple containing start and end times, s \u2208 Z+ is the speaker label2, and v \u2208 \u03a3\u2217 represents the transcript for the segment, for some vocabulary \u03a3. A session is then completely defined by R = {r1, . . . , rN }, which we call the reference. Challenge participants are required to estimate the reference through generated hypothesis, denoted by H = {h1, . . . , h \u02c6N }. 2Actual label names may be arbitrary, but can be mapped to the set of positive integers without loss of generality. DA-WER = 20%Used forranking {\"end_time\": \"11.370\",\"start_time\": \"11.000\", \"words\": \"so ummm\",\"speaker\": \"P03\",{ \"session_id\": \"S05\"},\"end_time\": \"14.110\", \"start_time\": \"12.100\", \"words\": \"where is he?\" \"speaker\": \"P01\",} \"session_id\": \"S05\" DER =7.14% {\"end_time\": \"11.350\",\"start_time\": \"11.010\", \"words\": \"so\",\"speaker\": \"spk1\",{ \"session_id\": \"S05\" },\"end_time\": \"14.150\", \"start_time\": \"12.000\", \"words\": \"Where is\", \"speaker\": \"spk2\",} \"session_id\": \"S05\"ReferenceHypothesis Speaker assignment DER-basedevaluation WER-basedevaluation \"P03\": \"spk1\" \"P01\": \"spk2\" Figure 1: Evaluation scheme for CHiME-7 DASR main track. Optimal speaker assignments are first determined using a DER- based evaluation, and then used to compute SA-WER, which is used as the final ranking metric. In this challenge, estimated speaker labels \u02c6s are not required to be identical to the reference labels. This flexibility poses a challenge in computing speaker-attributed word error rates (SA- WER) since, unlike earlier studies [36], the mapping between reference and estimated labels is not known. We solve this problem using mechanisms employed in the evaluation of diarization systems, as shown in Fig. 1. Let \u03a8 : Z+ \u2192 Z+ \u222a {\u03d5} denote a mapping from the reference la- bels to the estimated labels. We use the Hungarian method [37] to obtain the optimal mapping \u02c6\u03a8 that minimizes the diarization error rate (DER) between the reference segments and hypothe- sized segments, i.e., \u02c6\u03a8 = arg min \u03a8 DER(R, H), where the DER is calculated with a 250 ms collar, following the setup in CHiME-6. Given \u02c6\u03a8, we compute this newly proposed diarization attributed WER (DA-WER) metric as DA-WER(R, H) = (cid:80) s L(\u2322 Rs, \u2322 H \u02c6\u03a8(s)) s | \u2322 Rs| (cid:80) , where Rs denotes segments in R with speaker s, L is the Levenshtein distance, and the \u2322 operator concatenates seg- ment transcripts of its operand. Final ranking of systems will be based on the DA-WER metric macro-averaged across all scenarios. The macro-average operation is to encourage par- ticipants to develop a model whose performance is consistent across all three scenarios. Note that this evaluation is different from the concatenated minimum-permutation WER (cpWER) based ranking in the CHiME-6 challenge, since it requires par- ticipants to also produce estimates of the utterances boundaries. Utterance boundaries estimation are important in actual appli- cations, e.g. for double checking the transcripts. It also differs from the Asclite multi-dimensional Levenshtein metric [38], which can account for segmentation errors when the time-based cost is used. Contrary to this latter, DA-WER does not re- quire word-level alignment, which is quite time-consuming to double-check for ground-truth purposes. DA-WER instead of- fers a simple and \u201cloose\u201d way to encourage participants to pro- duce reasonable segmentation at the utterance level, which is sufficient in most applications. Future work is needed to devise more principled metrics for joint ASR and diarization. 3.2. Optional Sub-Track Similar to the CHiME-6 challenge, we include an optional sub- track where participants can use oracle diarization. The goal of this sub-track is to assess the acoustic front-end and ASR performance and disentangle it from the impact of diarization. The segmentation and speaker labels are known and are freely usable by participants, here the problem reduces to a standard (1) (2) ASR task. 3.3. Rules Detailed description of the task rules are available in the CH- iME-7 DASR website3. Hereafter we briefly summarize them. \u2022 Only some external data and pre-trained models are allowed. Participants are encouraged to propose additional ones in the first month. Close-talk microphones cannot be used for inference. \u2022 Automatic domain identification is prohibited. This includes the use of a-priori information such as array topology (includ- ing number of channels). A single system must be submitted for all three scenarios. Automatic channel selection is allowed and encouraged. \u2022 Self-supervised adaptation/training is allowed, as long as it is performed for each evaluation session independently i.e. as it would happen in a real-world deployment. Participants may use all available data"}, {"question": " What is the main requirement for participants in the main track of the CHiME-7 DASR challenge?", "answer": " To generate time-marked, speaker-attributed transcripts for multi-channel recordings without segmentation or speaker labels.", "ref_chunk": "allowing the use of pre-trained models, the CHiME-7 DASR task becomes more accessible to participants with limited re- sources, since training is often more efficient and quicker with such models. While SSLR models can be quite large, the fea- tures could be pre-extracted, thus enabling significant computa- tional savings in the training phase. From a research perspec- tive, it may be interesting to investigate how such models, which are trained in monoaural conditions, can make effective use of multi-channel audio and/or can be distilled into smaller models. 2.3. Open-Source Datasets The allowable external data sources are expanded to in- clude many commonly used open-source datasets (e.g. Lib- riSpeech [22] and FSD50k [33]). They facilitate the use of models for which, in prior challenges, necessary training data were lacking, but also enables the study of new research di- rections. For example, participants can create large synthetic datasets to train powerful deep neural network (DNN)-based speech separation and enhancement (SSE) [34, 35]. The struc- ture of the challenge then encourages research into whether such models can effectively help improving real-world meeting transcription. This latter research direction also intersects with this year\u2019s \u201csister\u201d CHiME-7 UDASE challenge, whose focus is on unsupervised domain adaptation for speech enhancement. 3. Challenge Tracks & Rules The CHiME-7 DASR challenge features two tracks \u2014 a main track, and an optional sub-track, as described below. 3.1. Main Track For the main track, we provide multi-channel recordings for the whole session without segmentation or speaker labels, and participants are required to generate time-marked, speaker- attributed transcripts, as shown in Fig. 1. For any session, let ri = (\u2206, s, v) denote a time-marked, speaker-attributed tran- script of a segment, where \u2206 = (tst, ten) is a tuple containing start and end times, s \u2208 Z+ is the speaker label2, and v \u2208 \u03a3\u2217 represents the transcript for the segment, for some vocabulary \u03a3. A session is then completely defined by R = {r1, . . . , rN }, which we call the reference. Challenge participants are required to estimate the reference through generated hypothesis, denoted by H = {h1, . . . , h \u02c6N }. 2Actual label names may be arbitrary, but can be mapped to the set of positive integers without loss of generality. DA-WER = 20%Used forranking {\"end_time\": \"11.370\",\"start_time\": \"11.000\", \"words\": \"so ummm\",\"speaker\": \"P03\",{ \"session_id\": \"S05\"},\"end_time\": \"14.110\", \"start_time\": \"12.100\", \"words\": \"where is he?\" \"speaker\": \"P01\",} \"session_id\": \"S05\" DER =7.14% {\"end_time\": \"11.350\",\"start_time\": \"11.010\", \"words\": \"so\",\"speaker\": \"spk1\",{ \"session_id\": \"S05\" },\"end_time\": \"14.150\", \"start_time\": \"12.000\", \"words\": \"Where is\", \"speaker\": \"spk2\",} \"session_id\": \"S05\"ReferenceHypothesis Speaker assignment DER-basedevaluation WER-basedevaluation \"P03\": \"spk1\" \"P01\": \"spk2\" Figure 1: Evaluation scheme for CHiME-7 DASR main track. Optimal speaker assignments are first determined using a DER- based evaluation, and then used to compute SA-WER, which is used as the final ranking metric. In this challenge, estimated speaker labels \u02c6s are not required to be identical to the reference labels. This flexibility poses a challenge in computing speaker-attributed word error rates (SA- WER) since, unlike earlier studies [36], the mapping between reference and estimated labels is not known. We solve this problem using mechanisms employed in the evaluation of diarization systems, as shown in Fig. 1. Let \u03a8 : Z+ \u2192 Z+ \u222a {\u03d5} denote a mapping from the reference la- bels to the estimated labels. We use the Hungarian method [37] to obtain the optimal mapping \u02c6\u03a8 that minimizes the diarization error rate (DER) between the reference segments and hypothe- sized segments, i.e., \u02c6\u03a8 = arg min \u03a8 DER(R, H), where the DER is calculated with a 250 ms collar, following the setup in CHiME-6. Given \u02c6\u03a8, we compute this newly proposed diarization attributed WER (DA-WER) metric as DA-WER(R, H) = (cid:80) s L(\u2322 Rs, \u2322 H \u02c6\u03a8(s)) s | \u2322 Rs| (cid:80) , where Rs denotes segments in R with speaker s, L is the Levenshtein distance, and the \u2322 operator concatenates seg- ment transcripts of its operand. Final ranking of systems will be based on the DA-WER metric macro-averaged across all scenarios. The macro-average operation is to encourage par- ticipants to develop a model whose performance is consistent across all three scenarios. Note that this evaluation is different from the concatenated minimum-permutation WER (cpWER) based ranking in the CHiME-6 challenge, since it requires par- ticipants to also produce estimates of the utterances boundaries. Utterance boundaries estimation are important in actual appli- cations, e.g. for double checking the transcripts. It also differs from the Asclite multi-dimensional Levenshtein metric [38], which can account for segmentation errors when the time-based cost is used. Contrary to this latter, DA-WER does not re- quire word-level alignment, which is quite time-consuming to double-check for ground-truth purposes. DA-WER instead of- fers a simple and \u201cloose\u201d way to encourage participants to pro- duce reasonable segmentation at the utterance level, which is sufficient in most applications. Future work is needed to devise more principled metrics for joint ASR and diarization. 3.2. Optional Sub-Track Similar to the CHiME-6 challenge, we include an optional sub- track where participants can use oracle diarization. The goal of this sub-track is to assess the acoustic front-end and ASR performance and disentangle it from the impact of diarization. The segmentation and speaker labels are known and are freely usable by participants, here the problem reduces to a standard (1) (2) ASR task. 3.3. Rules Detailed description of the task rules are available in the CH- iME-7 DASR website3. Hereafter we briefly summarize them. \u2022 Only some external data and pre-trained models are allowed. Participants are encouraged to propose additional ones in the first month. Close-talk microphones cannot be used for inference. \u2022 Automatic domain identification is prohibited. This includes the use of a-priori information such as array topology (includ- ing number of channels). A single system must be submitted for all three scenarios. Automatic channel selection is allowed and encouraged. \u2022 Self-supervised adaptation/training is allowed, as long as it is performed for each evaluation session independently i.e. as it would happen in a real-world deployment. Participants may use all available data"}, {"question": " How is the speaker-attributed word error rate (SA-WER) calculated in the CHiME-7 DASR challenge?", "answer": " Using mechanisms employed in the evaluation of diarization systems, with the optimal mapping determined by the Hungarian method.", "ref_chunk": "allowing the use of pre-trained models, the CHiME-7 DASR task becomes more accessible to participants with limited re- sources, since training is often more efficient and quicker with such models. While SSLR models can be quite large, the fea- tures could be pre-extracted, thus enabling significant computa- tional savings in the training phase. From a research perspec- tive, it may be interesting to investigate how such models, which are trained in monoaural conditions, can make effective use of multi-channel audio and/or can be distilled into smaller models. 2.3. Open-Source Datasets The allowable external data sources are expanded to in- clude many commonly used open-source datasets (e.g. Lib- riSpeech [22] and FSD50k [33]). They facilitate the use of models for which, in prior challenges, necessary training data were lacking, but also enables the study of new research di- rections. For example, participants can create large synthetic datasets to train powerful deep neural network (DNN)-based speech separation and enhancement (SSE) [34, 35]. The struc- ture of the challenge then encourages research into whether such models can effectively help improving real-world meeting transcription. This latter research direction also intersects with this year\u2019s \u201csister\u201d CHiME-7 UDASE challenge, whose focus is on unsupervised domain adaptation for speech enhancement. 3. Challenge Tracks & Rules The CHiME-7 DASR challenge features two tracks \u2014 a main track, and an optional sub-track, as described below. 3.1. Main Track For the main track, we provide multi-channel recordings for the whole session without segmentation or speaker labels, and participants are required to generate time-marked, speaker- attributed transcripts, as shown in Fig. 1. For any session, let ri = (\u2206, s, v) denote a time-marked, speaker-attributed tran- script of a segment, where \u2206 = (tst, ten) is a tuple containing start and end times, s \u2208 Z+ is the speaker label2, and v \u2208 \u03a3\u2217 represents the transcript for the segment, for some vocabulary \u03a3. A session is then completely defined by R = {r1, . . . , rN }, which we call the reference. Challenge participants are required to estimate the reference through generated hypothesis, denoted by H = {h1, . . . , h \u02c6N }. 2Actual label names may be arbitrary, but can be mapped to the set of positive integers without loss of generality. DA-WER = 20%Used forranking {\"end_time\": \"11.370\",\"start_time\": \"11.000\", \"words\": \"so ummm\",\"speaker\": \"P03\",{ \"session_id\": \"S05\"},\"end_time\": \"14.110\", \"start_time\": \"12.100\", \"words\": \"where is he?\" \"speaker\": \"P01\",} \"session_id\": \"S05\" DER =7.14% {\"end_time\": \"11.350\",\"start_time\": \"11.010\", \"words\": \"so\",\"speaker\": \"spk1\",{ \"session_id\": \"S05\" },\"end_time\": \"14.150\", \"start_time\": \"12.000\", \"words\": \"Where is\", \"speaker\": \"spk2\",} \"session_id\": \"S05\"ReferenceHypothesis Speaker assignment DER-basedevaluation WER-basedevaluation \"P03\": \"spk1\" \"P01\": \"spk2\" Figure 1: Evaluation scheme for CHiME-7 DASR main track. Optimal speaker assignments are first determined using a DER- based evaluation, and then used to compute SA-WER, which is used as the final ranking metric. In this challenge, estimated speaker labels \u02c6s are not required to be identical to the reference labels. This flexibility poses a challenge in computing speaker-attributed word error rates (SA- WER) since, unlike earlier studies [36], the mapping between reference and estimated labels is not known. We solve this problem using mechanisms employed in the evaluation of diarization systems, as shown in Fig. 1. Let \u03a8 : Z+ \u2192 Z+ \u222a {\u03d5} denote a mapping from the reference la- bels to the estimated labels. We use the Hungarian method [37] to obtain the optimal mapping \u02c6\u03a8 that minimizes the diarization error rate (DER) between the reference segments and hypothe- sized segments, i.e., \u02c6\u03a8 = arg min \u03a8 DER(R, H), where the DER is calculated with a 250 ms collar, following the setup in CHiME-6. Given \u02c6\u03a8, we compute this newly proposed diarization attributed WER (DA-WER) metric as DA-WER(R, H) = (cid:80) s L(\u2322 Rs, \u2322 H \u02c6\u03a8(s)) s | \u2322 Rs| (cid:80) , where Rs denotes segments in R with speaker s, L is the Levenshtein distance, and the \u2322 operator concatenates seg- ment transcripts of its operand. Final ranking of systems will be based on the DA-WER metric macro-averaged across all scenarios. The macro-average operation is to encourage par- ticipants to develop a model whose performance is consistent across all three scenarios. Note that this evaluation is different from the concatenated minimum-permutation WER (cpWER) based ranking in the CHiME-6 challenge, since it requires par- ticipants to also produce estimates of the utterances boundaries. Utterance boundaries estimation are important in actual appli- cations, e.g. for double checking the transcripts. It also differs from the Asclite multi-dimensional Levenshtein metric [38], which can account for segmentation errors when the time-based cost is used. Contrary to this latter, DA-WER does not re- quire word-level alignment, which is quite time-consuming to double-check for ground-truth purposes. DA-WER instead of- fers a simple and \u201cloose\u201d way to encourage participants to pro- duce reasonable segmentation at the utterance level, which is sufficient in most applications. Future work is needed to devise more principled metrics for joint ASR and diarization. 3.2. Optional Sub-Track Similar to the CHiME-6 challenge, we include an optional sub- track where participants can use oracle diarization. The goal of this sub-track is to assess the acoustic front-end and ASR performance and disentangle it from the impact of diarization. The segmentation and speaker labels are known and are freely usable by participants, here the problem reduces to a standard (1) (2) ASR task. 3.3. Rules Detailed description of the task rules are available in the CH- iME-7 DASR website3. Hereafter we briefly summarize them. \u2022 Only some external data and pre-trained models are allowed. Participants are encouraged to propose additional ones in the first month. Close-talk microphones cannot be used for inference. \u2022 Automatic domain identification is prohibited. This includes the use of a-priori information such as array topology (includ- ing number of channels). A single system must be submitted for all three scenarios. Automatic channel selection is allowed and encouraged. \u2022 Self-supervised adaptation/training is allowed, as long as it is performed for each evaluation session independently i.e. as it would happen in a real-world deployment. Participants may use all available data"}, {"question": " What metric is used for the final ranking of systems in the CHiME-7 DASR challenge?", "answer": " DA-WER (Diarization-Attributed Word Error Rate) macro-averaged across all scenarios.", "ref_chunk": "allowing the use of pre-trained models, the CHiME-7 DASR task becomes more accessible to participants with limited re- sources, since training is often more efficient and quicker with such models. While SSLR models can be quite large, the fea- tures could be pre-extracted, thus enabling significant computa- tional savings in the training phase. From a research perspec- tive, it may be interesting to investigate how such models, which are trained in monoaural conditions, can make effective use of multi-channel audio and/or can be distilled into smaller models. 2.3. Open-Source Datasets The allowable external data sources are expanded to in- clude many commonly used open-source datasets (e.g. Lib- riSpeech [22] and FSD50k [33]). They facilitate the use of models for which, in prior challenges, necessary training data were lacking, but also enables the study of new research di- rections. For example, participants can create large synthetic datasets to train powerful deep neural network (DNN)-based speech separation and enhancement (SSE) [34, 35]. The struc- ture of the challenge then encourages research into whether such models can effectively help improving real-world meeting transcription. This latter research direction also intersects with this year\u2019s \u201csister\u201d CHiME-7 UDASE challenge, whose focus is on unsupervised domain adaptation for speech enhancement. 3. Challenge Tracks & Rules The CHiME-7 DASR challenge features two tracks \u2014 a main track, and an optional sub-track, as described below. 3.1. Main Track For the main track, we provide multi-channel recordings for the whole session without segmentation or speaker labels, and participants are required to generate time-marked, speaker- attributed transcripts, as shown in Fig. 1. For any session, let ri = (\u2206, s, v) denote a time-marked, speaker-attributed tran- script of a segment, where \u2206 = (tst, ten) is a tuple containing start and end times, s \u2208 Z+ is the speaker label2, and v \u2208 \u03a3\u2217 represents the transcript for the segment, for some vocabulary \u03a3. A session is then completely defined by R = {r1, . . . , rN }, which we call the reference. Challenge participants are required to estimate the reference through generated hypothesis, denoted by H = {h1, . . . , h \u02c6N }. 2Actual label names may be arbitrary, but can be mapped to the set of positive integers without loss of generality. DA-WER = 20%Used forranking {\"end_time\": \"11.370\",\"start_time\": \"11.000\", \"words\": \"so ummm\",\"speaker\": \"P03\",{ \"session_id\": \"S05\"},\"end_time\": \"14.110\", \"start_time\": \"12.100\", \"words\": \"where is he?\" \"speaker\": \"P01\",} \"session_id\": \"S05\" DER =7.14% {\"end_time\": \"11.350\",\"start_time\": \"11.010\", \"words\": \"so\",\"speaker\": \"spk1\",{ \"session_id\": \"S05\" },\"end_time\": \"14.150\", \"start_time\": \"12.000\", \"words\": \"Where is\", \"speaker\": \"spk2\",} \"session_id\": \"S05\"ReferenceHypothesis Speaker assignment DER-basedevaluation WER-basedevaluation \"P03\": \"spk1\" \"P01\": \"spk2\" Figure 1: Evaluation scheme for CHiME-7 DASR main track. Optimal speaker assignments are first determined using a DER- based evaluation, and then used to compute SA-WER, which is used as the final ranking metric. In this challenge, estimated speaker labels \u02c6s are not required to be identical to the reference labels. This flexibility poses a challenge in computing speaker-attributed word error rates (SA- WER) since, unlike earlier studies [36], the mapping between reference and estimated labels is not known. We solve this problem using mechanisms employed in the evaluation of diarization systems, as shown in Fig. 1. Let \u03a8 : Z+ \u2192 Z+ \u222a {\u03d5} denote a mapping from the reference la- bels to the estimated labels. We use the Hungarian method [37] to obtain the optimal mapping \u02c6\u03a8 that minimizes the diarization error rate (DER) between the reference segments and hypothe- sized segments, i.e., \u02c6\u03a8 = arg min \u03a8 DER(R, H), where the DER is calculated with a 250 ms collar, following the setup in CHiME-6. Given \u02c6\u03a8, we compute this newly proposed diarization attributed WER (DA-WER) metric as DA-WER(R, H) = (cid:80) s L(\u2322 Rs, \u2322 H \u02c6\u03a8(s)) s | \u2322 Rs| (cid:80) , where Rs denotes segments in R with speaker s, L is the Levenshtein distance, and the \u2322 operator concatenates seg- ment transcripts of its operand. Final ranking of systems will be based on the DA-WER metric macro-averaged across all scenarios. The macro-average operation is to encourage par- ticipants to develop a model whose performance is consistent across all three scenarios. Note that this evaluation is different from the concatenated minimum-permutation WER (cpWER) based ranking in the CHiME-6 challenge, since it requires par- ticipants to also produce estimates of the utterances boundaries. Utterance boundaries estimation are important in actual appli- cations, e.g. for double checking the transcripts. It also differs from the Asclite multi-dimensional Levenshtein metric [38], which can account for segmentation errors when the time-based cost is used. Contrary to this latter, DA-WER does not re- quire word-level alignment, which is quite time-consuming to double-check for ground-truth purposes. DA-WER instead of- fers a simple and \u201cloose\u201d way to encourage participants to pro- duce reasonable segmentation at the utterance level, which is sufficient in most applications. Future work is needed to devise more principled metrics for joint ASR and diarization. 3.2. Optional Sub-Track Similar to the CHiME-6 challenge, we include an optional sub- track where participants can use oracle diarization. The goal of this sub-track is to assess the acoustic front-end and ASR performance and disentangle it from the impact of diarization. The segmentation and speaker labels are known and are freely usable by participants, here the problem reduces to a standard (1) (2) ASR task. 3.3. Rules Detailed description of the task rules are available in the CH- iME-7 DASR website3. Hereafter we briefly summarize them. \u2022 Only some external data and pre-trained models are allowed. Participants are encouraged to propose additional ones in the first month. Close-talk microphones cannot be used for inference. \u2022 Automatic domain identification is prohibited. This includes the use of a-priori information such as array topology (includ- ing number of channels). A single system must be submitted for all three scenarios. Automatic channel selection is allowed and encouraged. \u2022 Self-supervised adaptation/training is allowed, as long as it is performed for each evaluation session independently i.e. as it would happen in a real-world deployment. Participants may use all available data"}, {"question": " What is the goal of the optional sub-track in the CHiME-7 DASR challenge?", "answer": " To assess the acoustic front-end and ASR performance separately from the impact of diarization using oracle diarization.", "ref_chunk": "allowing the use of pre-trained models, the CHiME-7 DASR task becomes more accessible to participants with limited re- sources, since training is often more efficient and quicker with such models. While SSLR models can be quite large, the fea- tures could be pre-extracted, thus enabling significant computa- tional savings in the training phase. From a research perspec- tive, it may be interesting to investigate how such models, which are trained in monoaural conditions, can make effective use of multi-channel audio and/or can be distilled into smaller models. 2.3. Open-Source Datasets The allowable external data sources are expanded to in- clude many commonly used open-source datasets (e.g. Lib- riSpeech [22] and FSD50k [33]). They facilitate the use of models for which, in prior challenges, necessary training data were lacking, but also enables the study of new research di- rections. For example, participants can create large synthetic datasets to train powerful deep neural network (DNN)-based speech separation and enhancement (SSE) [34, 35]. The struc- ture of the challenge then encourages research into whether such models can effectively help improving real-world meeting transcription. This latter research direction also intersects with this year\u2019s \u201csister\u201d CHiME-7 UDASE challenge, whose focus is on unsupervised domain adaptation for speech enhancement. 3. Challenge Tracks & Rules The CHiME-7 DASR challenge features two tracks \u2014 a main track, and an optional sub-track, as described below. 3.1. Main Track For the main track, we provide multi-channel recordings for the whole session without segmentation or speaker labels, and participants are required to generate time-marked, speaker- attributed transcripts, as shown in Fig. 1. For any session, let ri = (\u2206, s, v) denote a time-marked, speaker-attributed tran- script of a segment, where \u2206 = (tst, ten) is a tuple containing start and end times, s \u2208 Z+ is the speaker label2, and v \u2208 \u03a3\u2217 represents the transcript for the segment, for some vocabulary \u03a3. A session is then completely defined by R = {r1, . . . , rN }, which we call the reference. Challenge participants are required to estimate the reference through generated hypothesis, denoted by H = {h1, . . . , h \u02c6N }. 2Actual label names may be arbitrary, but can be mapped to the set of positive integers without loss of generality. DA-WER = 20%Used forranking {\"end_time\": \"11.370\",\"start_time\": \"11.000\", \"words\": \"so ummm\",\"speaker\": \"P03\",{ \"session_id\": \"S05\"},\"end_time\": \"14.110\", \"start_time\": \"12.100\", \"words\": \"where is he?\" \"speaker\": \"P01\",} \"session_id\": \"S05\" DER =7.14% {\"end_time\": \"11.350\",\"start_time\": \"11.010\", \"words\": \"so\",\"speaker\": \"spk1\",{ \"session_id\": \"S05\" },\"end_time\": \"14.150\", \"start_time\": \"12.000\", \"words\": \"Where is\", \"speaker\": \"spk2\",} \"session_id\": \"S05\"ReferenceHypothesis Speaker assignment DER-basedevaluation WER-basedevaluation \"P03\": \"spk1\" \"P01\": \"spk2\" Figure 1: Evaluation scheme for CHiME-7 DASR main track. Optimal speaker assignments are first determined using a DER- based evaluation, and then used to compute SA-WER, which is used as the final ranking metric. In this challenge, estimated speaker labels \u02c6s are not required to be identical to the reference labels. This flexibility poses a challenge in computing speaker-attributed word error rates (SA- WER) since, unlike earlier studies [36], the mapping between reference and estimated labels is not known. We solve this problem using mechanisms employed in the evaluation of diarization systems, as shown in Fig. 1. Let \u03a8 : Z+ \u2192 Z+ \u222a {\u03d5} denote a mapping from the reference la- bels to the estimated labels. We use the Hungarian method [37] to obtain the optimal mapping \u02c6\u03a8 that minimizes the diarization error rate (DER) between the reference segments and hypothe- sized segments, i.e., \u02c6\u03a8 = arg min \u03a8 DER(R, H), where the DER is calculated with a 250 ms collar, following the setup in CHiME-6. Given \u02c6\u03a8, we compute this newly proposed diarization attributed WER (DA-WER) metric as DA-WER(R, H) = (cid:80) s L(\u2322 Rs, \u2322 H \u02c6\u03a8(s)) s | \u2322 Rs| (cid:80) , where Rs denotes segments in R with speaker s, L is the Levenshtein distance, and the \u2322 operator concatenates seg- ment transcripts of its operand. Final ranking of systems will be based on the DA-WER metric macro-averaged across all scenarios. The macro-average operation is to encourage par- ticipants to develop a model whose performance is consistent across all three scenarios. Note that this evaluation is different from the concatenated minimum-permutation WER (cpWER) based ranking in the CHiME-6 challenge, since it requires par- ticipants to also produce estimates of the utterances boundaries. Utterance boundaries estimation are important in actual appli- cations, e.g. for double checking the transcripts. It also differs from the Asclite multi-dimensional Levenshtein metric [38], which can account for segmentation errors when the time-based cost is used. Contrary to this latter, DA-WER does not re- quire word-level alignment, which is quite time-consuming to double-check for ground-truth purposes. DA-WER instead of- fers a simple and \u201cloose\u201d way to encourage participants to pro- duce reasonable segmentation at the utterance level, which is sufficient in most applications. Future work is needed to devise more principled metrics for joint ASR and diarization. 3.2. Optional Sub-Track Similar to the CHiME-6 challenge, we include an optional sub- track where participants can use oracle diarization. The goal of this sub-track is to assess the acoustic front-end and ASR performance and disentangle it from the impact of diarization. The segmentation and speaker labels are known and are freely usable by participants, here the problem reduces to a standard (1) (2) ASR task. 3.3. Rules Detailed description of the task rules are available in the CH- iME-7 DASR website3. Hereafter we briefly summarize them. \u2022 Only some external data and pre-trained models are allowed. Participants are encouraged to propose additional ones in the first month. Close-talk microphones cannot be used for inference. \u2022 Automatic domain identification is prohibited. This includes the use of a-priori information such as array topology (includ- ing number of channels). A single system must be submitted for all three scenarios. Automatic channel selection is allowed and encouraged. \u2022 Self-supervised adaptation/training is allowed, as long as it is performed for each evaluation session independently i.e. as it would happen in a real-world deployment. Participants may use all available data"}, {"question": " What are some of the rules participants must follow in the CHiME-7 DASR challenge?", "answer": " Only some external data and pre-trained models are allowed, automatic domain identification is prohibited, and a single system must be submitted for all three scenarios.", "ref_chunk": "allowing the use of pre-trained models, the CHiME-7 DASR task becomes more accessible to participants with limited re- sources, since training is often more efficient and quicker with such models. While SSLR models can be quite large, the fea- tures could be pre-extracted, thus enabling significant computa- tional savings in the training phase. From a research perspec- tive, it may be interesting to investigate how such models, which are trained in monoaural conditions, can make effective use of multi-channel audio and/or can be distilled into smaller models. 2.3. Open-Source Datasets The allowable external data sources are expanded to in- clude many commonly used open-source datasets (e.g. Lib- riSpeech [22] and FSD50k [33]). They facilitate the use of models for which, in prior challenges, necessary training data were lacking, but also enables the study of new research di- rections. For example, participants can create large synthetic datasets to train powerful deep neural network (DNN)-based speech separation and enhancement (SSE) [34, 35]. The struc- ture of the challenge then encourages research into whether such models can effectively help improving real-world meeting transcription. This latter research direction also intersects with this year\u2019s \u201csister\u201d CHiME-7 UDASE challenge, whose focus is on unsupervised domain adaptation for speech enhancement. 3. Challenge Tracks & Rules The CHiME-7 DASR challenge features two tracks \u2014 a main track, and an optional sub-track, as described below. 3.1. Main Track For the main track, we provide multi-channel recordings for the whole session without segmentation or speaker labels, and participants are required to generate time-marked, speaker- attributed transcripts, as shown in Fig. 1. For any session, let ri = (\u2206, s, v) denote a time-marked, speaker-attributed tran- script of a segment, where \u2206 = (tst, ten) is a tuple containing start and end times, s \u2208 Z+ is the speaker label2, and v \u2208 \u03a3\u2217 represents the transcript for the segment, for some vocabulary \u03a3. A session is then completely defined by R = {r1, . . . , rN }, which we call the reference. Challenge participants are required to estimate the reference through generated hypothesis, denoted by H = {h1, . . . , h \u02c6N }. 2Actual label names may be arbitrary, but can be mapped to the set of positive integers without loss of generality. DA-WER = 20%Used forranking {\"end_time\": \"11.370\",\"start_time\": \"11.000\", \"words\": \"so ummm\",\"speaker\": \"P03\",{ \"session_id\": \"S05\"},\"end_time\": \"14.110\", \"start_time\": \"12.100\", \"words\": \"where is he?\" \"speaker\": \"P01\",} \"session_id\": \"S05\" DER =7.14% {\"end_time\": \"11.350\",\"start_time\": \"11.010\", \"words\": \"so\",\"speaker\": \"spk1\",{ \"session_id\": \"S05\" },\"end_time\": \"14.150\", \"start_time\": \"12.000\", \"words\": \"Where is\", \"speaker\": \"spk2\",} \"session_id\": \"S05\"ReferenceHypothesis Speaker assignment DER-basedevaluation WER-basedevaluation \"P03\": \"spk1\" \"P01\": \"spk2\" Figure 1: Evaluation scheme for CHiME-7 DASR main track. Optimal speaker assignments are first determined using a DER- based evaluation, and then used to compute SA-WER, which is used as the final ranking metric. In this challenge, estimated speaker labels \u02c6s are not required to be identical to the reference labels. This flexibility poses a challenge in computing speaker-attributed word error rates (SA- WER) since, unlike earlier studies [36], the mapping between reference and estimated labels is not known. We solve this problem using mechanisms employed in the evaluation of diarization systems, as shown in Fig. 1. Let \u03a8 : Z+ \u2192 Z+ \u222a {\u03d5} denote a mapping from the reference la- bels to the estimated labels. We use the Hungarian method [37] to obtain the optimal mapping \u02c6\u03a8 that minimizes the diarization error rate (DER) between the reference segments and hypothe- sized segments, i.e., \u02c6\u03a8 = arg min \u03a8 DER(R, H), where the DER is calculated with a 250 ms collar, following the setup in CHiME-6. Given \u02c6\u03a8, we compute this newly proposed diarization attributed WER (DA-WER) metric as DA-WER(R, H) = (cid:80) s L(\u2322 Rs, \u2322 H \u02c6\u03a8(s)) s | \u2322 Rs| (cid:80) , where Rs denotes segments in R with speaker s, L is the Levenshtein distance, and the \u2322 operator concatenates seg- ment transcripts of its operand. Final ranking of systems will be based on the DA-WER metric macro-averaged across all scenarios. The macro-average operation is to encourage par- ticipants to develop a model whose performance is consistent across all three scenarios. Note that this evaluation is different from the concatenated minimum-permutation WER (cpWER) based ranking in the CHiME-6 challenge, since it requires par- ticipants to also produce estimates of the utterances boundaries. Utterance boundaries estimation are important in actual appli- cations, e.g. for double checking the transcripts. It also differs from the Asclite multi-dimensional Levenshtein metric [38], which can account for segmentation errors when the time-based cost is used. Contrary to this latter, DA-WER does not re- quire word-level alignment, which is quite time-consuming to double-check for ground-truth purposes. DA-WER instead of- fers a simple and \u201cloose\u201d way to encourage participants to pro- duce reasonable segmentation at the utterance level, which is sufficient in most applications. Future work is needed to devise more principled metrics for joint ASR and diarization. 3.2. Optional Sub-Track Similar to the CHiME-6 challenge, we include an optional sub- track where participants can use oracle diarization. The goal of this sub-track is to assess the acoustic front-end and ASR performance and disentangle it from the impact of diarization. The segmentation and speaker labels are known and are freely usable by participants, here the problem reduces to a standard (1) (2) ASR task. 3.3. Rules Detailed description of the task rules are available in the CH- iME-7 DASR website3. Hereafter we briefly summarize them. \u2022 Only some external data and pre-trained models are allowed. Participants are encouraged to propose additional ones in the first month. Close-talk microphones cannot be used for inference. \u2022 Automatic domain identification is prohibited. This includes the use of a-priori information such as array topology (includ- ing number of channels). A single system must be submitted for all three scenarios. Automatic channel selection is allowed and encouraged. \u2022 Self-supervised adaptation/training is allowed, as long as it is performed for each evaluation session independently i.e. as it would happen in a real-world deployment. Participants may use all available data"}], "doc_text": "allowing the use of pre-trained models, the CHiME-7 DASR task becomes more accessible to participants with limited re- sources, since training is often more efficient and quicker with such models. While SSLR models can be quite large, the fea- tures could be pre-extracted, thus enabling significant computa- tional savings in the training phase. From a research perspec- tive, it may be interesting to investigate how such models, which are trained in monoaural conditions, can make effective use of multi-channel audio and/or can be distilled into smaller models. 2.3. Open-Source Datasets The allowable external data sources are expanded to in- clude many commonly used open-source datasets (e.g. Lib- riSpeech [22] and FSD50k [33]). They facilitate the use of models for which, in prior challenges, necessary training data were lacking, but also enables the study of new research di- rections. For example, participants can create large synthetic datasets to train powerful deep neural network (DNN)-based speech separation and enhancement (SSE) [34, 35]. The struc- ture of the challenge then encourages research into whether such models can effectively help improving real-world meeting transcription. This latter research direction also intersects with this year\u2019s \u201csister\u201d CHiME-7 UDASE challenge, whose focus is on unsupervised domain adaptation for speech enhancement. 3. Challenge Tracks & Rules The CHiME-7 DASR challenge features two tracks \u2014 a main track, and an optional sub-track, as described below. 3.1. Main Track For the main track, we provide multi-channel recordings for the whole session without segmentation or speaker labels, and participants are required to generate time-marked, speaker- attributed transcripts, as shown in Fig. 1. For any session, let ri = (\u2206, s, v) denote a time-marked, speaker-attributed tran- script of a segment, where \u2206 = (tst, ten) is a tuple containing start and end times, s \u2208 Z+ is the speaker label2, and v \u2208 \u03a3\u2217 represents the transcript for the segment, for some vocabulary \u03a3. A session is then completely defined by R = {r1, . . . , rN }, which we call the reference. Challenge participants are required to estimate the reference through generated hypothesis, denoted by H = {h1, . . . , h \u02c6N }. 2Actual label names may be arbitrary, but can be mapped to the set of positive integers without loss of generality. DA-WER = 20%Used forranking {\"end_time\": \"11.370\",\"start_time\": \"11.000\", \"words\": \"so ummm\",\"speaker\": \"P03\",{ \"session_id\": \"S05\"},\"end_time\": \"14.110\", \"start_time\": \"12.100\", \"words\": \"where is he?\" \"speaker\": \"P01\",} \"session_id\": \"S05\" DER =7.14% {\"end_time\": \"11.350\",\"start_time\": \"11.010\", \"words\": \"so\",\"speaker\": \"spk1\",{ \"session_id\": \"S05\" },\"end_time\": \"14.150\", \"start_time\": \"12.000\", \"words\": \"Where is\", \"speaker\": \"spk2\",} \"session_id\": \"S05\"ReferenceHypothesis Speaker assignment DER-basedevaluation WER-basedevaluation \"P03\": \"spk1\" \"P01\": \"spk2\" Figure 1: Evaluation scheme for CHiME-7 DASR main track. Optimal speaker assignments are first determined using a DER- based evaluation, and then used to compute SA-WER, which is used as the final ranking metric. In this challenge, estimated speaker labels \u02c6s are not required to be identical to the reference labels. This flexibility poses a challenge in computing speaker-attributed word error rates (SA- WER) since, unlike earlier studies [36], the mapping between reference and estimated labels is not known. We solve this problem using mechanisms employed in the evaluation of diarization systems, as shown in Fig. 1. Let \u03a8 : Z+ \u2192 Z+ \u222a {\u03d5} denote a mapping from the reference la- bels to the estimated labels. We use the Hungarian method [37] to obtain the optimal mapping \u02c6\u03a8 that minimizes the diarization error rate (DER) between the reference segments and hypothe- sized segments, i.e., \u02c6\u03a8 = arg min \u03a8 DER(R, H), where the DER is calculated with a 250 ms collar, following the setup in CHiME-6. Given \u02c6\u03a8, we compute this newly proposed diarization attributed WER (DA-WER) metric as DA-WER(R, H) = (cid:80) s L(\u2322 Rs, \u2322 H \u02c6\u03a8(s)) s | \u2322 Rs| (cid:80) , where Rs denotes segments in R with speaker s, L is the Levenshtein distance, and the \u2322 operator concatenates seg- ment transcripts of its operand. Final ranking of systems will be based on the DA-WER metric macro-averaged across all scenarios. The macro-average operation is to encourage par- ticipants to develop a model whose performance is consistent across all three scenarios. Note that this evaluation is different from the concatenated minimum-permutation WER (cpWER) based ranking in the CHiME-6 challenge, since it requires par- ticipants to also produce estimates of the utterances boundaries. Utterance boundaries estimation are important in actual appli- cations, e.g. for double checking the transcripts. It also differs from the Asclite multi-dimensional Levenshtein metric [38], which can account for segmentation errors when the time-based cost is used. Contrary to this latter, DA-WER does not re- quire word-level alignment, which is quite time-consuming to double-check for ground-truth purposes. DA-WER instead of- fers a simple and \u201cloose\u201d way to encourage participants to pro- duce reasonable segmentation at the utterance level, which is sufficient in most applications. Future work is needed to devise more principled metrics for joint ASR and diarization. 3.2. Optional Sub-Track Similar to the CHiME-6 challenge, we include an optional sub- track where participants can use oracle diarization. The goal of this sub-track is to assess the acoustic front-end and ASR performance and disentangle it from the impact of diarization. The segmentation and speaker labels are known and are freely usable by participants, here the problem reduces to a standard (1) (2) ASR task. 3.3. Rules Detailed description of the task rules are available in the CH- iME-7 DASR website3. Hereafter we briefly summarize them. \u2022 Only some external data and pre-trained models are allowed. Participants are encouraged to propose additional ones in the first month. Close-talk microphones cannot be used for inference. \u2022 Automatic domain identification is prohibited. This includes the use of a-priori information such as array topology (includ- ing number of channels). A single system must be submitted for all three scenarios. Automatic channel selection is allowed and encouraged. \u2022 Self-supervised adaptation/training is allowed, as long as it is performed for each evaluation session independently i.e. as it would happen in a real-world deployment. Participants may use all available data"}