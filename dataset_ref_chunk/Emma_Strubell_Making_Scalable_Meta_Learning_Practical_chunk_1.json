{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Making_Scalable_Meta_Learning_Practical_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the focus of the work described in the text?", "answer": " The focus is on making scalable meta learning practical.", "ref_chunk": "3 2 0 2 t c O 3 2 ] G L . s c [ 2 v 4 7 6 5 0 . 0 1 3 2 : v i X r a Making Scalable Meta Learning Practical Sang Keun Choe1\u2217 Sanket Vaibhav Mehta1 Hwijeen Ahn1 Willie Neiswanger2 Pengtao Xie3,5 Emma Strubell1,4 Eric Xing1,5 1Carnegie Mellon University 2Stanford University 3UCSD 4Allen Institute for AI 5MBZUAI Abstract Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains. 1 Introduction Meta learning aims to learn the inductive biases (e.g. training data, neural architecture) of a machine learning program in such a way that a model trained with these inductive biases achieves optimal performance on user-specified objectives (e.g. fairness, quick generalization). This concept of meta learning can naturally be formulated as bilevel optimization, where the upper (meta) level problem encodes inductive biases and objectives, and the lower (base) level optimization problem represents the main machine learning program of interest, such as image classification or language modeling. Depending on the design of inductive biases and objectives, meta learning has found many applications in machine learning, including hyperparameter optimization [16], data optimization [21, 58], neural architecture search [38, 69], learned optimizers [43, 44], and few-shot learning [14, 51]. Following its versatility, numerous algorithms have been proposed to solve meta learning. Among them, gradient-based meta learning (GBML) has in particular gained considerable attention, due to its capability to optimize a wide range of high-dimensional inductive biases in an efficient manner. For example, MAML [14] finds optimal initialization weights (inductive bias) that achieve quick generalization to new tasks (objective), and L2RW [54] optimizes training sample weights (inductive bias) to achieve robustness against label noise (objective). However, the above benefits of GBML oftentimes get overshadowed by its poor scalability in practice, especially under the recent trend of large models [5, 33, 50], which arises due to several factors. First, many GBML algorithms [14, 40, 51] require inversion of a large Jacobian matrix, which suffers from both algorithmic instability as well \u2217Correspondence: sangkeuc@andrew.cmu.edu 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Constant Memory Jacobian Inverse Free Adaptive Optimizer Support (Efficient) Distributed Training Support Overall Scalability Iterative Differentiation [14, 15, 42] Recurrent Backpropagation [36] T1 \u2212 T2 [38, 41] Neumann Series [40] Conjugate Gradient [51] \u2717 \u2714 \u2714 \u2714 \u2717 \u2717 \u2714 \u2717 \u2717 \u2714 \u2714 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 SAMA (ours) \u2714 \u2714 \u2714 \u2714 \u2714 SAMA-NA 20 15 100 150 SAMA (2 GPUs) 250 SAMA CG 300 SAMA (4 GPUs) 10 200 Neumann betterNoisy Finetuning w/ BERT-base 400Throughput (samples/s) 350 25GPU Memory Usage (GB) 200 50Memory Usage (GB) 10 50 250 400Parameter Count (M) Distill-RoBERTa 30 0 300 RoBERTa-base 0 150 20 350 100 RoBERTa-largeContinued Pretraining w/ RoBERTa Variants CG A6000 Max Memory (48GB) 40 SAMA Neumann Figure 1: Top: Table showing a scalability comparison. Bottom left: Plot of throughput vs memory of different GBML algorithms on the noisy finetuning of BERT-base experiment. SAMA achieves better memory/compute efficiency overall given a fixed model, and the gap further widens by distributing compute across multiple GPUs with our efficient distributed training strategy. Bottom right: Plot of memory vs model size (i.e., # of parameters) of different GBML algorithms on the continued pretraining of RoBERTa experiment. SAMA demonstrates the least significant increase in GPU memory usage with the increasing model size compared to baseline methods. as exorbitant compute/memory costs. Second, most GBML research assumes that lower (base) level optimization is performed with SGD, whereas most large models, exemplified by Transformers [62], are by default optimized with adaptive optimizers like Adam [32]; consequently, the applicability of SGD-based GBML methods to large models trained with adaptive optimizers remains unclear. Finally, most GBML research to date has been limited to the single-GPU setup due to the lack of distributed training support [9, 20], which is essential in large-scale learning. In this work, we endeavor to resolve the aforementioned scalability issues in GBML by co-developing algorithms and systems, and explore the initial potential of scalable meta learning in diverse applica- tions. Our main contributions can be summarized as follows: 1. We investigate the root causes of substantial memory/compute costs, algorithmic instability, and a lack of distributed training support in GBML, all of which significantly limit its scalability, through a technical analysis of implicit differentiation. In doing so, we identify three major factors, namely (i) base Jacobian inversion, (ii) a lack of algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of the backward pass of meta gradient computation, and discuss how each of them contributes negatively to the above limitations in depth. 2. Taking one step further, we propose an initial solution to each of the aforementioned issues by respectively (i) approximating the base Jacobian with an identity matrix, (ii) additionally expanding the meta Jacobian via the chain rule, and (iii) devising a novel communication strategy that efficiently uses the communication-computation overlap trick"}, {"question": " What is SAMA and how does it contribute to scalable meta learning?", "answer": " SAMA combines advances in implicit differentiation algorithms and systems to support a broad range of adaptive optimizers, reduce computational burden, and exploit efficient distributed training techniques.", "ref_chunk": "3 2 0 2 t c O 3 2 ] G L . s c [ 2 v 4 7 6 5 0 . 0 1 3 2 : v i X r a Making Scalable Meta Learning Practical Sang Keun Choe1\u2217 Sanket Vaibhav Mehta1 Hwijeen Ahn1 Willie Neiswanger2 Pengtao Xie3,5 Emma Strubell1,4 Eric Xing1,5 1Carnegie Mellon University 2Stanford University 3UCSD 4Allen Institute for AI 5MBZUAI Abstract Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains. 1 Introduction Meta learning aims to learn the inductive biases (e.g. training data, neural architecture) of a machine learning program in such a way that a model trained with these inductive biases achieves optimal performance on user-specified objectives (e.g. fairness, quick generalization). This concept of meta learning can naturally be formulated as bilevel optimization, where the upper (meta) level problem encodes inductive biases and objectives, and the lower (base) level optimization problem represents the main machine learning program of interest, such as image classification or language modeling. Depending on the design of inductive biases and objectives, meta learning has found many applications in machine learning, including hyperparameter optimization [16], data optimization [21, 58], neural architecture search [38, 69], learned optimizers [43, 44], and few-shot learning [14, 51]. Following its versatility, numerous algorithms have been proposed to solve meta learning. Among them, gradient-based meta learning (GBML) has in particular gained considerable attention, due to its capability to optimize a wide range of high-dimensional inductive biases in an efficient manner. For example, MAML [14] finds optimal initialization weights (inductive bias) that achieve quick generalization to new tasks (objective), and L2RW [54] optimizes training sample weights (inductive bias) to achieve robustness against label noise (objective). However, the above benefits of GBML oftentimes get overshadowed by its poor scalability in practice, especially under the recent trend of large models [5, 33, 50], which arises due to several factors. First, many GBML algorithms [14, 40, 51] require inversion of a large Jacobian matrix, which suffers from both algorithmic instability as well \u2217Correspondence: sangkeuc@andrew.cmu.edu 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Constant Memory Jacobian Inverse Free Adaptive Optimizer Support (Efficient) Distributed Training Support Overall Scalability Iterative Differentiation [14, 15, 42] Recurrent Backpropagation [36] T1 \u2212 T2 [38, 41] Neumann Series [40] Conjugate Gradient [51] \u2717 \u2714 \u2714 \u2714 \u2717 \u2717 \u2714 \u2717 \u2717 \u2714 \u2714 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 SAMA (ours) \u2714 \u2714 \u2714 \u2714 \u2714 SAMA-NA 20 15 100 150 SAMA (2 GPUs) 250 SAMA CG 300 SAMA (4 GPUs) 10 200 Neumann betterNoisy Finetuning w/ BERT-base 400Throughput (samples/s) 350 25GPU Memory Usage (GB) 200 50Memory Usage (GB) 10 50 250 400Parameter Count (M) Distill-RoBERTa 30 0 300 RoBERTa-base 0 150 20 350 100 RoBERTa-largeContinued Pretraining w/ RoBERTa Variants CG A6000 Max Memory (48GB) 40 SAMA Neumann Figure 1: Top: Table showing a scalability comparison. Bottom left: Plot of throughput vs memory of different GBML algorithms on the noisy finetuning of BERT-base experiment. SAMA achieves better memory/compute efficiency overall given a fixed model, and the gap further widens by distributing compute across multiple GPUs with our efficient distributed training strategy. Bottom right: Plot of memory vs model size (i.e., # of parameters) of different GBML algorithms on the continued pretraining of RoBERTa experiment. SAMA demonstrates the least significant increase in GPU memory usage with the increasing model size compared to baseline methods. as exorbitant compute/memory costs. Second, most GBML research assumes that lower (base) level optimization is performed with SGD, whereas most large models, exemplified by Transformers [62], are by default optimized with adaptive optimizers like Adam [32]; consequently, the applicability of SGD-based GBML methods to large models trained with adaptive optimizers remains unclear. Finally, most GBML research to date has been limited to the single-GPU setup due to the lack of distributed training support [9, 20], which is essential in large-scale learning. In this work, we endeavor to resolve the aforementioned scalability issues in GBML by co-developing algorithms and systems, and explore the initial potential of scalable meta learning in diverse applica- tions. Our main contributions can be summarized as follows: 1. We investigate the root causes of substantial memory/compute costs, algorithmic instability, and a lack of distributed training support in GBML, all of which significantly limit its scalability, through a technical analysis of implicit differentiation. In doing so, we identify three major factors, namely (i) base Jacobian inversion, (ii) a lack of algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of the backward pass of meta gradient computation, and discuss how each of them contributes negatively to the above limitations in depth. 2. Taking one step further, we propose an initial solution to each of the aforementioned issues by respectively (i) approximating the base Jacobian with an identity matrix, (ii) additionally expanding the meta Jacobian via the chain rule, and (iii) devising a novel communication strategy that efficiently uses the communication-computation overlap trick"}, {"question": " How does SAMA compare to other baseline meta learning algorithms in terms of throughput and memory consumption?", "answer": " SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms.", "ref_chunk": "3 2 0 2 t c O 3 2 ] G L . s c [ 2 v 4 7 6 5 0 . 0 1 3 2 : v i X r a Making Scalable Meta Learning Practical Sang Keun Choe1\u2217 Sanket Vaibhav Mehta1 Hwijeen Ahn1 Willie Neiswanger2 Pengtao Xie3,5 Emma Strubell1,4 Eric Xing1,5 1Carnegie Mellon University 2Stanford University 3UCSD 4Allen Institute for AI 5MBZUAI Abstract Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains. 1 Introduction Meta learning aims to learn the inductive biases (e.g. training data, neural architecture) of a machine learning program in such a way that a model trained with these inductive biases achieves optimal performance on user-specified objectives (e.g. fairness, quick generalization). This concept of meta learning can naturally be formulated as bilevel optimization, where the upper (meta) level problem encodes inductive biases and objectives, and the lower (base) level optimization problem represents the main machine learning program of interest, such as image classification or language modeling. Depending on the design of inductive biases and objectives, meta learning has found many applications in machine learning, including hyperparameter optimization [16], data optimization [21, 58], neural architecture search [38, 69], learned optimizers [43, 44], and few-shot learning [14, 51]. Following its versatility, numerous algorithms have been proposed to solve meta learning. Among them, gradient-based meta learning (GBML) has in particular gained considerable attention, due to its capability to optimize a wide range of high-dimensional inductive biases in an efficient manner. For example, MAML [14] finds optimal initialization weights (inductive bias) that achieve quick generalization to new tasks (objective), and L2RW [54] optimizes training sample weights (inductive bias) to achieve robustness against label noise (objective). However, the above benefits of GBML oftentimes get overshadowed by its poor scalability in practice, especially under the recent trend of large models [5, 33, 50], which arises due to several factors. First, many GBML algorithms [14, 40, 51] require inversion of a large Jacobian matrix, which suffers from both algorithmic instability as well \u2217Correspondence: sangkeuc@andrew.cmu.edu 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Constant Memory Jacobian Inverse Free Adaptive Optimizer Support (Efficient) Distributed Training Support Overall Scalability Iterative Differentiation [14, 15, 42] Recurrent Backpropagation [36] T1 \u2212 T2 [38, 41] Neumann Series [40] Conjugate Gradient [51] \u2717 \u2714 \u2714 \u2714 \u2717 \u2717 \u2714 \u2717 \u2717 \u2714 \u2714 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 SAMA (ours) \u2714 \u2714 \u2714 \u2714 \u2714 SAMA-NA 20 15 100 150 SAMA (2 GPUs) 250 SAMA CG 300 SAMA (4 GPUs) 10 200 Neumann betterNoisy Finetuning w/ BERT-base 400Throughput (samples/s) 350 25GPU Memory Usage (GB) 200 50Memory Usage (GB) 10 50 250 400Parameter Count (M) Distill-RoBERTa 30 0 300 RoBERTa-base 0 150 20 350 100 RoBERTa-largeContinued Pretraining w/ RoBERTa Variants CG A6000 Max Memory (48GB) 40 SAMA Neumann Figure 1: Top: Table showing a scalability comparison. Bottom left: Plot of throughput vs memory of different GBML algorithms on the noisy finetuning of BERT-base experiment. SAMA achieves better memory/compute efficiency overall given a fixed model, and the gap further widens by distributing compute across multiple GPUs with our efficient distributed training strategy. Bottom right: Plot of memory vs model size (i.e., # of parameters) of different GBML algorithms on the continued pretraining of RoBERTa experiment. SAMA demonstrates the least significant increase in GPU memory usage with the increasing model size compared to baseline methods. as exorbitant compute/memory costs. Second, most GBML research assumes that lower (base) level optimization is performed with SGD, whereas most large models, exemplified by Transformers [62], are by default optimized with adaptive optimizers like Adam [32]; consequently, the applicability of SGD-based GBML methods to large models trained with adaptive optimizers remains unclear. Finally, most GBML research to date has been limited to the single-GPU setup due to the lack of distributed training support [9, 20], which is essential in large-scale learning. In this work, we endeavor to resolve the aforementioned scalability issues in GBML by co-developing algorithms and systems, and explore the initial potential of scalable meta learning in diverse applica- tions. Our main contributions can be summarized as follows: 1. We investigate the root causes of substantial memory/compute costs, algorithmic instability, and a lack of distributed training support in GBML, all of which significantly limit its scalability, through a technical analysis of implicit differentiation. In doing so, we identify three major factors, namely (i) base Jacobian inversion, (ii) a lack of algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of the backward pass of meta gradient computation, and discuss how each of them contributes negatively to the above limitations in depth. 2. Taking one step further, we propose an initial solution to each of the aforementioned issues by respectively (i) approximating the base Jacobian with an identity matrix, (ii) additionally expanding the meta Jacobian via the chain rule, and (iii) devising a novel communication strategy that efficiently uses the communication-computation overlap trick"}, {"question": " What are some applications of meta learning mentioned in the text?", "answer": " Applications of meta learning mentioned include hyperparameter optimization, data optimization, neural architecture search, learned optimizers, and few-shot learning.", "ref_chunk": "3 2 0 2 t c O 3 2 ] G L . s c [ 2 v 4 7 6 5 0 . 0 1 3 2 : v i X r a Making Scalable Meta Learning Practical Sang Keun Choe1\u2217 Sanket Vaibhav Mehta1 Hwijeen Ahn1 Willie Neiswanger2 Pengtao Xie3,5 Emma Strubell1,4 Eric Xing1,5 1Carnegie Mellon University 2Stanford University 3UCSD 4Allen Institute for AI 5MBZUAI Abstract Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains. 1 Introduction Meta learning aims to learn the inductive biases (e.g. training data, neural architecture) of a machine learning program in such a way that a model trained with these inductive biases achieves optimal performance on user-specified objectives (e.g. fairness, quick generalization). This concept of meta learning can naturally be formulated as bilevel optimization, where the upper (meta) level problem encodes inductive biases and objectives, and the lower (base) level optimization problem represents the main machine learning program of interest, such as image classification or language modeling. Depending on the design of inductive biases and objectives, meta learning has found many applications in machine learning, including hyperparameter optimization [16], data optimization [21, 58], neural architecture search [38, 69], learned optimizers [43, 44], and few-shot learning [14, 51]. Following its versatility, numerous algorithms have been proposed to solve meta learning. Among them, gradient-based meta learning (GBML) has in particular gained considerable attention, due to its capability to optimize a wide range of high-dimensional inductive biases in an efficient manner. For example, MAML [14] finds optimal initialization weights (inductive bias) that achieve quick generalization to new tasks (objective), and L2RW [54] optimizes training sample weights (inductive bias) to achieve robustness against label noise (objective). However, the above benefits of GBML oftentimes get overshadowed by its poor scalability in practice, especially under the recent trend of large models [5, 33, 50], which arises due to several factors. First, many GBML algorithms [14, 40, 51] require inversion of a large Jacobian matrix, which suffers from both algorithmic instability as well \u2217Correspondence: sangkeuc@andrew.cmu.edu 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Constant Memory Jacobian Inverse Free Adaptive Optimizer Support (Efficient) Distributed Training Support Overall Scalability Iterative Differentiation [14, 15, 42] Recurrent Backpropagation [36] T1 \u2212 T2 [38, 41] Neumann Series [40] Conjugate Gradient [51] \u2717 \u2714 \u2714 \u2714 \u2717 \u2717 \u2714 \u2717 \u2717 \u2714 \u2714 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 SAMA (ours) \u2714 \u2714 \u2714 \u2714 \u2714 SAMA-NA 20 15 100 150 SAMA (2 GPUs) 250 SAMA CG 300 SAMA (4 GPUs) 10 200 Neumann betterNoisy Finetuning w/ BERT-base 400Throughput (samples/s) 350 25GPU Memory Usage (GB) 200 50Memory Usage (GB) 10 50 250 400Parameter Count (M) Distill-RoBERTa 30 0 300 RoBERTa-base 0 150 20 350 100 RoBERTa-largeContinued Pretraining w/ RoBERTa Variants CG A6000 Max Memory (48GB) 40 SAMA Neumann Figure 1: Top: Table showing a scalability comparison. Bottom left: Plot of throughput vs memory of different GBML algorithms on the noisy finetuning of BERT-base experiment. SAMA achieves better memory/compute efficiency overall given a fixed model, and the gap further widens by distributing compute across multiple GPUs with our efficient distributed training strategy. Bottom right: Plot of memory vs model size (i.e., # of parameters) of different GBML algorithms on the continued pretraining of RoBERTa experiment. SAMA demonstrates the least significant increase in GPU memory usage with the increasing model size compared to baseline methods. as exorbitant compute/memory costs. Second, most GBML research assumes that lower (base) level optimization is performed with SGD, whereas most large models, exemplified by Transformers [62], are by default optimized with adaptive optimizers like Adam [32]; consequently, the applicability of SGD-based GBML methods to large models trained with adaptive optimizers remains unclear. Finally, most GBML research to date has been limited to the single-GPU setup due to the lack of distributed training support [9, 20], which is essential in large-scale learning. In this work, we endeavor to resolve the aforementioned scalability issues in GBML by co-developing algorithms and systems, and explore the initial potential of scalable meta learning in diverse applica- tions. Our main contributions can be summarized as follows: 1. We investigate the root causes of substantial memory/compute costs, algorithmic instability, and a lack of distributed training support in GBML, all of which significantly limit its scalability, through a technical analysis of implicit differentiation. In doing so, we identify three major factors, namely (i) base Jacobian inversion, (ii) a lack of algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of the backward pass of meta gradient computation, and discuss how each of them contributes negatively to the above limitations in depth. 2. Taking one step further, we propose an initial solution to each of the aforementioned issues by respectively (i) approximating the base Jacobian with an identity matrix, (ii) additionally expanding the meta Jacobian via the chain rule, and (iii) devising a novel communication strategy that efficiently uses the communication-computation overlap trick"}, {"question": " What are some factors contributing to the poor scalability of gradient-based meta learning?", "answer": " Factors contributing to poor scalability of gradient-based meta learning include the requirement to invert a large Jacobian matrix, training instability, and lack of efficient distributed training support.", "ref_chunk": "3 2 0 2 t c O 3 2 ] G L . s c [ 2 v 4 7 6 5 0 . 0 1 3 2 : v i X r a Making Scalable Meta Learning Practical Sang Keun Choe1\u2217 Sanket Vaibhav Mehta1 Hwijeen Ahn1 Willie Neiswanger2 Pengtao Xie3,5 Emma Strubell1,4 Eric Xing1,5 1Carnegie Mellon University 2Stanford University 3UCSD 4Allen Institute for AI 5MBZUAI Abstract Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains. 1 Introduction Meta learning aims to learn the inductive biases (e.g. training data, neural architecture) of a machine learning program in such a way that a model trained with these inductive biases achieves optimal performance on user-specified objectives (e.g. fairness, quick generalization). This concept of meta learning can naturally be formulated as bilevel optimization, where the upper (meta) level problem encodes inductive biases and objectives, and the lower (base) level optimization problem represents the main machine learning program of interest, such as image classification or language modeling. Depending on the design of inductive biases and objectives, meta learning has found many applications in machine learning, including hyperparameter optimization [16], data optimization [21, 58], neural architecture search [38, 69], learned optimizers [43, 44], and few-shot learning [14, 51]. Following its versatility, numerous algorithms have been proposed to solve meta learning. Among them, gradient-based meta learning (GBML) has in particular gained considerable attention, due to its capability to optimize a wide range of high-dimensional inductive biases in an efficient manner. For example, MAML [14] finds optimal initialization weights (inductive bias) that achieve quick generalization to new tasks (objective), and L2RW [54] optimizes training sample weights (inductive bias) to achieve robustness against label noise (objective). However, the above benefits of GBML oftentimes get overshadowed by its poor scalability in practice, especially under the recent trend of large models [5, 33, 50], which arises due to several factors. First, many GBML algorithms [14, 40, 51] require inversion of a large Jacobian matrix, which suffers from both algorithmic instability as well \u2217Correspondence: sangkeuc@andrew.cmu.edu 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Constant Memory Jacobian Inverse Free Adaptive Optimizer Support (Efficient) Distributed Training Support Overall Scalability Iterative Differentiation [14, 15, 42] Recurrent Backpropagation [36] T1 \u2212 T2 [38, 41] Neumann Series [40] Conjugate Gradient [51] \u2717 \u2714 \u2714 \u2714 \u2717 \u2717 \u2714 \u2717 \u2717 \u2714 \u2714 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 SAMA (ours) \u2714 \u2714 \u2714 \u2714 \u2714 SAMA-NA 20 15 100 150 SAMA (2 GPUs) 250 SAMA CG 300 SAMA (4 GPUs) 10 200 Neumann betterNoisy Finetuning w/ BERT-base 400Throughput (samples/s) 350 25GPU Memory Usage (GB) 200 50Memory Usage (GB) 10 50 250 400Parameter Count (M) Distill-RoBERTa 30 0 300 RoBERTa-base 0 150 20 350 100 RoBERTa-largeContinued Pretraining w/ RoBERTa Variants CG A6000 Max Memory (48GB) 40 SAMA Neumann Figure 1: Top: Table showing a scalability comparison. Bottom left: Plot of throughput vs memory of different GBML algorithms on the noisy finetuning of BERT-base experiment. SAMA achieves better memory/compute efficiency overall given a fixed model, and the gap further widens by distributing compute across multiple GPUs with our efficient distributed training strategy. Bottom right: Plot of memory vs model size (i.e., # of parameters) of different GBML algorithms on the continued pretraining of RoBERTa experiment. SAMA demonstrates the least significant increase in GPU memory usage with the increasing model size compared to baseline methods. as exorbitant compute/memory costs. Second, most GBML research assumes that lower (base) level optimization is performed with SGD, whereas most large models, exemplified by Transformers [62], are by default optimized with adaptive optimizers like Adam [32]; consequently, the applicability of SGD-based GBML methods to large models trained with adaptive optimizers remains unclear. Finally, most GBML research to date has been limited to the single-GPU setup due to the lack of distributed training support [9, 20], which is essential in large-scale learning. In this work, we endeavor to resolve the aforementioned scalability issues in GBML by co-developing algorithms and systems, and explore the initial potential of scalable meta learning in diverse applica- tions. Our main contributions can be summarized as follows: 1. We investigate the root causes of substantial memory/compute costs, algorithmic instability, and a lack of distributed training support in GBML, all of which significantly limit its scalability, through a technical analysis of implicit differentiation. In doing so, we identify three major factors, namely (i) base Jacobian inversion, (ii) a lack of algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of the backward pass of meta gradient computation, and discuss how each of them contributes negatively to the above limitations in depth. 2. Taking one step further, we propose an initial solution to each of the aforementioned issues by respectively (i) approximating the base Jacobian with an identity matrix, (ii) additionally expanding the meta Jacobian via the chain rule, and (iii) devising a novel communication strategy that efficiently uses the communication-computation overlap trick"}, {"question": " Why do many gradient-based meta learning algorithms struggle with scalability in practice?", "answer": " Many gradient-based meta learning algorithms struggle with scalability in practice due to their requirement to invert a large Jacobian matrix, which leads to algorithmic instability and high compute/memory costs.", "ref_chunk": "3 2 0 2 t c O 3 2 ] G L . s c [ 2 v 4 7 6 5 0 . 0 1 3 2 : v i X r a Making Scalable Meta Learning Practical Sang Keun Choe1\u2217 Sanket Vaibhav Mehta1 Hwijeen Ahn1 Willie Neiswanger2 Pengtao Xie3,5 Emma Strubell1,4 Eric Xing1,5 1Carnegie Mellon University 2Stanford University 3UCSD 4Allen Institute for AI 5MBZUAI Abstract Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains. 1 Introduction Meta learning aims to learn the inductive biases (e.g. training data, neural architecture) of a machine learning program in such a way that a model trained with these inductive biases achieves optimal performance on user-specified objectives (e.g. fairness, quick generalization). This concept of meta learning can naturally be formulated as bilevel optimization, where the upper (meta) level problem encodes inductive biases and objectives, and the lower (base) level optimization problem represents the main machine learning program of interest, such as image classification or language modeling. Depending on the design of inductive biases and objectives, meta learning has found many applications in machine learning, including hyperparameter optimization [16], data optimization [21, 58], neural architecture search [38, 69], learned optimizers [43, 44], and few-shot learning [14, 51]. Following its versatility, numerous algorithms have been proposed to solve meta learning. Among them, gradient-based meta learning (GBML) has in particular gained considerable attention, due to its capability to optimize a wide range of high-dimensional inductive biases in an efficient manner. For example, MAML [14] finds optimal initialization weights (inductive bias) that achieve quick generalization to new tasks (objective), and L2RW [54] optimizes training sample weights (inductive bias) to achieve robustness against label noise (objective). However, the above benefits of GBML oftentimes get overshadowed by its poor scalability in practice, especially under the recent trend of large models [5, 33, 50], which arises due to several factors. First, many GBML algorithms [14, 40, 51] require inversion of a large Jacobian matrix, which suffers from both algorithmic instability as well \u2217Correspondence: sangkeuc@andrew.cmu.edu 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Constant Memory Jacobian Inverse Free Adaptive Optimizer Support (Efficient) Distributed Training Support Overall Scalability Iterative Differentiation [14, 15, 42] Recurrent Backpropagation [36] T1 \u2212 T2 [38, 41] Neumann Series [40] Conjugate Gradient [51] \u2717 \u2714 \u2714 \u2714 \u2717 \u2717 \u2714 \u2717 \u2717 \u2714 \u2714 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 SAMA (ours) \u2714 \u2714 \u2714 \u2714 \u2714 SAMA-NA 20 15 100 150 SAMA (2 GPUs) 250 SAMA CG 300 SAMA (4 GPUs) 10 200 Neumann betterNoisy Finetuning w/ BERT-base 400Throughput (samples/s) 350 25GPU Memory Usage (GB) 200 50Memory Usage (GB) 10 50 250 400Parameter Count (M) Distill-RoBERTa 30 0 300 RoBERTa-base 0 150 20 350 100 RoBERTa-largeContinued Pretraining w/ RoBERTa Variants CG A6000 Max Memory (48GB) 40 SAMA Neumann Figure 1: Top: Table showing a scalability comparison. Bottom left: Plot of throughput vs memory of different GBML algorithms on the noisy finetuning of BERT-base experiment. SAMA achieves better memory/compute efficiency overall given a fixed model, and the gap further widens by distributing compute across multiple GPUs with our efficient distributed training strategy. Bottom right: Plot of memory vs model size (i.e., # of parameters) of different GBML algorithms on the continued pretraining of RoBERTa experiment. SAMA demonstrates the least significant increase in GPU memory usage with the increasing model size compared to baseline methods. as exorbitant compute/memory costs. Second, most GBML research assumes that lower (base) level optimization is performed with SGD, whereas most large models, exemplified by Transformers [62], are by default optimized with adaptive optimizers like Adam [32]; consequently, the applicability of SGD-based GBML methods to large models trained with adaptive optimizers remains unclear. Finally, most GBML research to date has been limited to the single-GPU setup due to the lack of distributed training support [9, 20], which is essential in large-scale learning. In this work, we endeavor to resolve the aforementioned scalability issues in GBML by co-developing algorithms and systems, and explore the initial potential of scalable meta learning in diverse applica- tions. Our main contributions can be summarized as follows: 1. We investigate the root causes of substantial memory/compute costs, algorithmic instability, and a lack of distributed training support in GBML, all of which significantly limit its scalability, through a technical analysis of implicit differentiation. In doing so, we identify three major factors, namely (i) base Jacobian inversion, (ii) a lack of algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of the backward pass of meta gradient computation, and discuss how each of them contributes negatively to the above limitations in depth. 2. Taking one step further, we propose an initial solution to each of the aforementioned issues by respectively (i) approximating the base Jacobian with an identity matrix, (ii) additionally expanding the meta Jacobian via the chain rule, and (iii) devising a novel communication strategy that efficiently uses the communication-computation overlap trick"}, {"question": " What are the root causes of substantial memory/compute costs, algorithmic instability, and a lack of distributed training support in gradient-based meta learning?", "answer": " The root causes are identified as base Jacobian inversion, a lack of algorithmic adaptation for adaptive optimizers, and the need for custom implementation of the backward pass of meta gradient computation.", "ref_chunk": "3 2 0 2 t c O 3 2 ] G L . s c [ 2 v 4 7 6 5 0 . 0 1 3 2 : v i X r a Making Scalable Meta Learning Practical Sang Keun Choe1\u2217 Sanket Vaibhav Mehta1 Hwijeen Ahn1 Willie Neiswanger2 Pengtao Xie3,5 Emma Strubell1,4 Eric Xing1,5 1Carnegie Mellon University 2Stanford University 3UCSD 4Allen Institute for AI 5MBZUAI Abstract Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains. 1 Introduction Meta learning aims to learn the inductive biases (e.g. training data, neural architecture) of a machine learning program in such a way that a model trained with these inductive biases achieves optimal performance on user-specified objectives (e.g. fairness, quick generalization). This concept of meta learning can naturally be formulated as bilevel optimization, where the upper (meta) level problem encodes inductive biases and objectives, and the lower (base) level optimization problem represents the main machine learning program of interest, such as image classification or language modeling. Depending on the design of inductive biases and objectives, meta learning has found many applications in machine learning, including hyperparameter optimization [16], data optimization [21, 58], neural architecture search [38, 69], learned optimizers [43, 44], and few-shot learning [14, 51]. Following its versatility, numerous algorithms have been proposed to solve meta learning. Among them, gradient-based meta learning (GBML) has in particular gained considerable attention, due to its capability to optimize a wide range of high-dimensional inductive biases in an efficient manner. For example, MAML [14] finds optimal initialization weights (inductive bias) that achieve quick generalization to new tasks (objective), and L2RW [54] optimizes training sample weights (inductive bias) to achieve robustness against label noise (objective). However, the above benefits of GBML oftentimes get overshadowed by its poor scalability in practice, especially under the recent trend of large models [5, 33, 50], which arises due to several factors. First, many GBML algorithms [14, 40, 51] require inversion of a large Jacobian matrix, which suffers from both algorithmic instability as well \u2217Correspondence: sangkeuc@andrew.cmu.edu 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Constant Memory Jacobian Inverse Free Adaptive Optimizer Support (Efficient) Distributed Training Support Overall Scalability Iterative Differentiation [14, 15, 42] Recurrent Backpropagation [36] T1 \u2212 T2 [38, 41] Neumann Series [40] Conjugate Gradient [51] \u2717 \u2714 \u2714 \u2714 \u2717 \u2717 \u2714 \u2717 \u2717 \u2714 \u2714 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 SAMA (ours) \u2714 \u2714 \u2714 \u2714 \u2714 SAMA-NA 20 15 100 150 SAMA (2 GPUs) 250 SAMA CG 300 SAMA (4 GPUs) 10 200 Neumann betterNoisy Finetuning w/ BERT-base 400Throughput (samples/s) 350 25GPU Memory Usage (GB) 200 50Memory Usage (GB) 10 50 250 400Parameter Count (M) Distill-RoBERTa 30 0 300 RoBERTa-base 0 150 20 350 100 RoBERTa-largeContinued Pretraining w/ RoBERTa Variants CG A6000 Max Memory (48GB) 40 SAMA Neumann Figure 1: Top: Table showing a scalability comparison. Bottom left: Plot of throughput vs memory of different GBML algorithms on the noisy finetuning of BERT-base experiment. SAMA achieves better memory/compute efficiency overall given a fixed model, and the gap further widens by distributing compute across multiple GPUs with our efficient distributed training strategy. Bottom right: Plot of memory vs model size (i.e., # of parameters) of different GBML algorithms on the continued pretraining of RoBERTa experiment. SAMA demonstrates the least significant increase in GPU memory usage with the increasing model size compared to baseline methods. as exorbitant compute/memory costs. Second, most GBML research assumes that lower (base) level optimization is performed with SGD, whereas most large models, exemplified by Transformers [62], are by default optimized with adaptive optimizers like Adam [32]; consequently, the applicability of SGD-based GBML methods to large models trained with adaptive optimizers remains unclear. Finally, most GBML research to date has been limited to the single-GPU setup due to the lack of distributed training support [9, 20], which is essential in large-scale learning. In this work, we endeavor to resolve the aforementioned scalability issues in GBML by co-developing algorithms and systems, and explore the initial potential of scalable meta learning in diverse applica- tions. Our main contributions can be summarized as follows: 1. We investigate the root causes of substantial memory/compute costs, algorithmic instability, and a lack of distributed training support in GBML, all of which significantly limit its scalability, through a technical analysis of implicit differentiation. In doing so, we identify three major factors, namely (i) base Jacobian inversion, (ii) a lack of algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of the backward pass of meta gradient computation, and discuss how each of them contributes negatively to the above limitations in depth. 2. Taking one step further, we propose an initial solution to each of the aforementioned issues by respectively (i) approximating the base Jacobian with an identity matrix, (ii) additionally expanding the meta Jacobian via the chain rule, and (iii) devising a novel communication strategy that efficiently uses the communication-computation overlap trick"}, {"question": " What are the main contributions of the work described in the text?", "answer": " The main contributions include investigating the root causes of scalability limitations in gradient-based meta learning, proposing solutions to address these issues, and exploring the potential of scalable meta learning in diverse applications.", "ref_chunk": "3 2 0 2 t c O 3 2 ] G L . s c [ 2 v 4 7 6 5 0 . 0 1 3 2 : v i X r a Making Scalable Meta Learning Practical Sang Keun Choe1\u2217 Sanket Vaibhav Mehta1 Hwijeen Ahn1 Willie Neiswanger2 Pengtao Xie3,5 Emma Strubell1,4 Eric Xing1,5 1Carnegie Mellon University 2Stanford University 3UCSD 4Allen Institute for AI 5MBZUAI Abstract Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains. 1 Introduction Meta learning aims to learn the inductive biases (e.g. training data, neural architecture) of a machine learning program in such a way that a model trained with these inductive biases achieves optimal performance on user-specified objectives (e.g. fairness, quick generalization). This concept of meta learning can naturally be formulated as bilevel optimization, where the upper (meta) level problem encodes inductive biases and objectives, and the lower (base) level optimization problem represents the main machine learning program of interest, such as image classification or language modeling. Depending on the design of inductive biases and objectives, meta learning has found many applications in machine learning, including hyperparameter optimization [16], data optimization [21, 58], neural architecture search [38, 69], learned optimizers [43, 44], and few-shot learning [14, 51]. Following its versatility, numerous algorithms have been proposed to solve meta learning. Among them, gradient-based meta learning (GBML) has in particular gained considerable attention, due to its capability to optimize a wide range of high-dimensional inductive biases in an efficient manner. For example, MAML [14] finds optimal initialization weights (inductive bias) that achieve quick generalization to new tasks (objective), and L2RW [54] optimizes training sample weights (inductive bias) to achieve robustness against label noise (objective). However, the above benefits of GBML oftentimes get overshadowed by its poor scalability in practice, especially under the recent trend of large models [5, 33, 50], which arises due to several factors. First, many GBML algorithms [14, 40, 51] require inversion of a large Jacobian matrix, which suffers from both algorithmic instability as well \u2217Correspondence: sangkeuc@andrew.cmu.edu 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Constant Memory Jacobian Inverse Free Adaptive Optimizer Support (Efficient) Distributed Training Support Overall Scalability Iterative Differentiation [14, 15, 42] Recurrent Backpropagation [36] T1 \u2212 T2 [38, 41] Neumann Series [40] Conjugate Gradient [51] \u2717 \u2714 \u2714 \u2714 \u2717 \u2717 \u2714 \u2717 \u2717 \u2714 \u2714 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 SAMA (ours) \u2714 \u2714 \u2714 \u2714 \u2714 SAMA-NA 20 15 100 150 SAMA (2 GPUs) 250 SAMA CG 300 SAMA (4 GPUs) 10 200 Neumann betterNoisy Finetuning w/ BERT-base 400Throughput (samples/s) 350 25GPU Memory Usage (GB) 200 50Memory Usage (GB) 10 50 250 400Parameter Count (M) Distill-RoBERTa 30 0 300 RoBERTa-base 0 150 20 350 100 RoBERTa-largeContinued Pretraining w/ RoBERTa Variants CG A6000 Max Memory (48GB) 40 SAMA Neumann Figure 1: Top: Table showing a scalability comparison. Bottom left: Plot of throughput vs memory of different GBML algorithms on the noisy finetuning of BERT-base experiment. SAMA achieves better memory/compute efficiency overall given a fixed model, and the gap further widens by distributing compute across multiple GPUs with our efficient distributed training strategy. Bottom right: Plot of memory vs model size (i.e., # of parameters) of different GBML algorithms on the continued pretraining of RoBERTa experiment. SAMA demonstrates the least significant increase in GPU memory usage with the increasing model size compared to baseline methods. as exorbitant compute/memory costs. Second, most GBML research assumes that lower (base) level optimization is performed with SGD, whereas most large models, exemplified by Transformers [62], are by default optimized with adaptive optimizers like Adam [32]; consequently, the applicability of SGD-based GBML methods to large models trained with adaptive optimizers remains unclear. Finally, most GBML research to date has been limited to the single-GPU setup due to the lack of distributed training support [9, 20], which is essential in large-scale learning. In this work, we endeavor to resolve the aforementioned scalability issues in GBML by co-developing algorithms and systems, and explore the initial potential of scalable meta learning in diverse applica- tions. Our main contributions can be summarized as follows: 1. We investigate the root causes of substantial memory/compute costs, algorithmic instability, and a lack of distributed training support in GBML, all of which significantly limit its scalability, through a technical analysis of implicit differentiation. In doing so, we identify three major factors, namely (i) base Jacobian inversion, (ii) a lack of algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of the backward pass of meta gradient computation, and discuss how each of them contributes negatively to the above limitations in depth. 2. Taking one step further, we propose an initial solution to each of the aforementioned issues by respectively (i) approximating the base Jacobian with an identity matrix, (ii) additionally expanding the meta Jacobian via the chain rule, and (iii) devising a novel communication strategy that efficiently uses the communication-computation overlap trick"}, {"question": " How does the text propose to address the issue of base Jacobian inversion in gradient-based meta learning?", "answer": " The text proposes to approximate the base Jacobian with an identity matrix to address the issue of base Jacobian inversion.", "ref_chunk": "3 2 0 2 t c O 3 2 ] G L . s c [ 2 v 4 7 6 5 0 . 0 1 3 2 : v i X r a Making Scalable Meta Learning Practical Sang Keun Choe1\u2217 Sanket Vaibhav Mehta1 Hwijeen Ahn1 Willie Neiswanger2 Pengtao Xie3,5 Emma Strubell1,4 Eric Xing1,5 1Carnegie Mellon University 2Stanford University 3UCSD 4Allen Institute for AI 5MBZUAI Abstract Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains. 1 Introduction Meta learning aims to learn the inductive biases (e.g. training data, neural architecture) of a machine learning program in such a way that a model trained with these inductive biases achieves optimal performance on user-specified objectives (e.g. fairness, quick generalization). This concept of meta learning can naturally be formulated as bilevel optimization, where the upper (meta) level problem encodes inductive biases and objectives, and the lower (base) level optimization problem represents the main machine learning program of interest, such as image classification or language modeling. Depending on the design of inductive biases and objectives, meta learning has found many applications in machine learning, including hyperparameter optimization [16], data optimization [21, 58], neural architecture search [38, 69], learned optimizers [43, 44], and few-shot learning [14, 51]. Following its versatility, numerous algorithms have been proposed to solve meta learning. Among them, gradient-based meta learning (GBML) has in particular gained considerable attention, due to its capability to optimize a wide range of high-dimensional inductive biases in an efficient manner. For example, MAML [14] finds optimal initialization weights (inductive bias) that achieve quick generalization to new tasks (objective), and L2RW [54] optimizes training sample weights (inductive bias) to achieve robustness against label noise (objective). However, the above benefits of GBML oftentimes get overshadowed by its poor scalability in practice, especially under the recent trend of large models [5, 33, 50], which arises due to several factors. First, many GBML algorithms [14, 40, 51] require inversion of a large Jacobian matrix, which suffers from both algorithmic instability as well \u2217Correspondence: sangkeuc@andrew.cmu.edu 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Constant Memory Jacobian Inverse Free Adaptive Optimizer Support (Efficient) Distributed Training Support Overall Scalability Iterative Differentiation [14, 15, 42] Recurrent Backpropagation [36] T1 \u2212 T2 [38, 41] Neumann Series [40] Conjugate Gradient [51] \u2717 \u2714 \u2714 \u2714 \u2717 \u2717 \u2714 \u2717 \u2717 \u2714 \u2714 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 SAMA (ours) \u2714 \u2714 \u2714 \u2714 \u2714 SAMA-NA 20 15 100 150 SAMA (2 GPUs) 250 SAMA CG 300 SAMA (4 GPUs) 10 200 Neumann betterNoisy Finetuning w/ BERT-base 400Throughput (samples/s) 350 25GPU Memory Usage (GB) 200 50Memory Usage (GB) 10 50 250 400Parameter Count (M) Distill-RoBERTa 30 0 300 RoBERTa-base 0 150 20 350 100 RoBERTa-largeContinued Pretraining w/ RoBERTa Variants CG A6000 Max Memory (48GB) 40 SAMA Neumann Figure 1: Top: Table showing a scalability comparison. Bottom left: Plot of throughput vs memory of different GBML algorithms on the noisy finetuning of BERT-base experiment. SAMA achieves better memory/compute efficiency overall given a fixed model, and the gap further widens by distributing compute across multiple GPUs with our efficient distributed training strategy. Bottom right: Plot of memory vs model size (i.e., # of parameters) of different GBML algorithms on the continued pretraining of RoBERTa experiment. SAMA demonstrates the least significant increase in GPU memory usage with the increasing model size compared to baseline methods. as exorbitant compute/memory costs. Second, most GBML research assumes that lower (base) level optimization is performed with SGD, whereas most large models, exemplified by Transformers [62], are by default optimized with adaptive optimizers like Adam [32]; consequently, the applicability of SGD-based GBML methods to large models trained with adaptive optimizers remains unclear. Finally, most GBML research to date has been limited to the single-GPU setup due to the lack of distributed training support [9, 20], which is essential in large-scale learning. In this work, we endeavor to resolve the aforementioned scalability issues in GBML by co-developing algorithms and systems, and explore the initial potential of scalable meta learning in diverse applica- tions. Our main contributions can be summarized as follows: 1. We investigate the root causes of substantial memory/compute costs, algorithmic instability, and a lack of distributed training support in GBML, all of which significantly limit its scalability, through a technical analysis of implicit differentiation. In doing so, we identify three major factors, namely (i) base Jacobian inversion, (ii) a lack of algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of the backward pass of meta gradient computation, and discuss how each of them contributes negatively to the above limitations in depth. 2. Taking one step further, we propose an initial solution to each of the aforementioned issues by respectively (i) approximating the base Jacobian with an identity matrix, (ii) additionally expanding the meta Jacobian via the chain rule, and (iii) devising a novel communication strategy that efficiently uses the communication-computation overlap trick"}, {"question": " What is one of the initial solutions proposed in the text to improve the scalability of gradient-based meta learning?", "answer": " One of the initial solutions proposed is to expand the meta Jacobian via the chain rule to improve the scalability of gradient-based meta learning.", "ref_chunk": "3 2 0 2 t c O 3 2 ] G L . s c [ 2 v 4 7 6 5 0 . 0 1 3 2 : v i X r a Making Scalable Meta Learning Practical Sang Keun Choe1\u2217 Sanket Vaibhav Mehta1 Hwijeen Ahn1 Willie Neiswanger2 Pengtao Xie3,5 Emma Strubell1,4 Eric Xing1,5 1Carnegie Mellon University 2Stanford University 3UCSD 4Allen Institute for AI 5MBZUAI Abstract Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains. 1 Introduction Meta learning aims to learn the inductive biases (e.g. training data, neural architecture) of a machine learning program in such a way that a model trained with these inductive biases achieves optimal performance on user-specified objectives (e.g. fairness, quick generalization). This concept of meta learning can naturally be formulated as bilevel optimization, where the upper (meta) level problem encodes inductive biases and objectives, and the lower (base) level optimization problem represents the main machine learning program of interest, such as image classification or language modeling. Depending on the design of inductive biases and objectives, meta learning has found many applications in machine learning, including hyperparameter optimization [16], data optimization [21, 58], neural architecture search [38, 69], learned optimizers [43, 44], and few-shot learning [14, 51]. Following its versatility, numerous algorithms have been proposed to solve meta learning. Among them, gradient-based meta learning (GBML) has in particular gained considerable attention, due to its capability to optimize a wide range of high-dimensional inductive biases in an efficient manner. For example, MAML [14] finds optimal initialization weights (inductive bias) that achieve quick generalization to new tasks (objective), and L2RW [54] optimizes training sample weights (inductive bias) to achieve robustness against label noise (objective). However, the above benefits of GBML oftentimes get overshadowed by its poor scalability in practice, especially under the recent trend of large models [5, 33, 50], which arises due to several factors. First, many GBML algorithms [14, 40, 51] require inversion of a large Jacobian matrix, which suffers from both algorithmic instability as well \u2217Correspondence: sangkeuc@andrew.cmu.edu 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Constant Memory Jacobian Inverse Free Adaptive Optimizer Support (Efficient) Distributed Training Support Overall Scalability Iterative Differentiation [14, 15, 42] Recurrent Backpropagation [36] T1 \u2212 T2 [38, 41] Neumann Series [40] Conjugate Gradient [51] \u2717 \u2714 \u2714 \u2714 \u2717 \u2717 \u2714 \u2717 \u2717 \u2714 \u2714 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 SAMA (ours) \u2714 \u2714 \u2714 \u2714 \u2714 SAMA-NA 20 15 100 150 SAMA (2 GPUs) 250 SAMA CG 300 SAMA (4 GPUs) 10 200 Neumann betterNoisy Finetuning w/ BERT-base 400Throughput (samples/s) 350 25GPU Memory Usage (GB) 200 50Memory Usage (GB) 10 50 250 400Parameter Count (M) Distill-RoBERTa 30 0 300 RoBERTa-base 0 150 20 350 100 RoBERTa-largeContinued Pretraining w/ RoBERTa Variants CG A6000 Max Memory (48GB) 40 SAMA Neumann Figure 1: Top: Table showing a scalability comparison. Bottom left: Plot of throughput vs memory of different GBML algorithms on the noisy finetuning of BERT-base experiment. SAMA achieves better memory/compute efficiency overall given a fixed model, and the gap further widens by distributing compute across multiple GPUs with our efficient distributed training strategy. Bottom right: Plot of memory vs model size (i.e., # of parameters) of different GBML algorithms on the continued pretraining of RoBERTa experiment. SAMA demonstrates the least significant increase in GPU memory usage with the increasing model size compared to baseline methods. as exorbitant compute/memory costs. Second, most GBML research assumes that lower (base) level optimization is performed with SGD, whereas most large models, exemplified by Transformers [62], are by default optimized with adaptive optimizers like Adam [32]; consequently, the applicability of SGD-based GBML methods to large models trained with adaptive optimizers remains unclear. Finally, most GBML research to date has been limited to the single-GPU setup due to the lack of distributed training support [9, 20], which is essential in large-scale learning. In this work, we endeavor to resolve the aforementioned scalability issues in GBML by co-developing algorithms and systems, and explore the initial potential of scalable meta learning in diverse applica- tions. Our main contributions can be summarized as follows: 1. We investigate the root causes of substantial memory/compute costs, algorithmic instability, and a lack of distributed training support in GBML, all of which significantly limit its scalability, through a technical analysis of implicit differentiation. In doing so, we identify three major factors, namely (i) base Jacobian inversion, (ii) a lack of algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of the backward pass of meta gradient computation, and discuss how each of them contributes negatively to the above limitations in depth. 2. Taking one step further, we propose an initial solution to each of the aforementioned issues by respectively (i) approximating the base Jacobian with an identity matrix, (ii) additionally expanding the meta Jacobian via the chain rule, and (iii) devising a novel communication strategy that efficiently uses the communication-computation overlap trick"}], "doc_text": "3 2 0 2 t c O 3 2 ] G L . s c [ 2 v 4 7 6 5 0 . 0 1 3 2 : v i X r a Making Scalable Meta Learning Practical Sang Keun Choe1\u2217 Sanket Vaibhav Mehta1 Hwijeen Ahn1 Willie Neiswanger2 Pengtao Xie3,5 Emma Strubell1,4 Eric Xing1,5 1Carnegie Mellon University 2Stanford University 3UCSD 4Allen Institute for AI 5MBZUAI Abstract Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8\u00d7 increase in throughput and 2.0/3.8\u00d7 decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains. 1 Introduction Meta learning aims to learn the inductive biases (e.g. training data, neural architecture) of a machine learning program in such a way that a model trained with these inductive biases achieves optimal performance on user-specified objectives (e.g. fairness, quick generalization). This concept of meta learning can naturally be formulated as bilevel optimization, where the upper (meta) level problem encodes inductive biases and objectives, and the lower (base) level optimization problem represents the main machine learning program of interest, such as image classification or language modeling. Depending on the design of inductive biases and objectives, meta learning has found many applications in machine learning, including hyperparameter optimization [16], data optimization [21, 58], neural architecture search [38, 69], learned optimizers [43, 44], and few-shot learning [14, 51]. Following its versatility, numerous algorithms have been proposed to solve meta learning. Among them, gradient-based meta learning (GBML) has in particular gained considerable attention, due to its capability to optimize a wide range of high-dimensional inductive biases in an efficient manner. For example, MAML [14] finds optimal initialization weights (inductive bias) that achieve quick generalization to new tasks (objective), and L2RW [54] optimizes training sample weights (inductive bias) to achieve robustness against label noise (objective). However, the above benefits of GBML oftentimes get overshadowed by its poor scalability in practice, especially under the recent trend of large models [5, 33, 50], which arises due to several factors. First, many GBML algorithms [14, 40, 51] require inversion of a large Jacobian matrix, which suffers from both algorithmic instability as well \u2217Correspondence: sangkeuc@andrew.cmu.edu 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Constant Memory Jacobian Inverse Free Adaptive Optimizer Support (Efficient) Distributed Training Support Overall Scalability Iterative Differentiation [14, 15, 42] Recurrent Backpropagation [36] T1 \u2212 T2 [38, 41] Neumann Series [40] Conjugate Gradient [51] \u2717 \u2714 \u2714 \u2714 \u2717 \u2717 \u2714 \u2717 \u2717 \u2714 \u2714 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 SAMA (ours) \u2714 \u2714 \u2714 \u2714 \u2714 SAMA-NA 20 15 100 150 SAMA (2 GPUs) 250 SAMA CG 300 SAMA (4 GPUs) 10 200 Neumann betterNoisy Finetuning w/ BERT-base 400Throughput (samples/s) 350 25GPU Memory Usage (GB) 200 50Memory Usage (GB) 10 50 250 400Parameter Count (M) Distill-RoBERTa 30 0 300 RoBERTa-base 0 150 20 350 100 RoBERTa-largeContinued Pretraining w/ RoBERTa Variants CG A6000 Max Memory (48GB) 40 SAMA Neumann Figure 1: Top: Table showing a scalability comparison. Bottom left: Plot of throughput vs memory of different GBML algorithms on the noisy finetuning of BERT-base experiment. SAMA achieves better memory/compute efficiency overall given a fixed model, and the gap further widens by distributing compute across multiple GPUs with our efficient distributed training strategy. Bottom right: Plot of memory vs model size (i.e., # of parameters) of different GBML algorithms on the continued pretraining of RoBERTa experiment. SAMA demonstrates the least significant increase in GPU memory usage with the increasing model size compared to baseline methods. as exorbitant compute/memory costs. Second, most GBML research assumes that lower (base) level optimization is performed with SGD, whereas most large models, exemplified by Transformers [62], are by default optimized with adaptive optimizers like Adam [32]; consequently, the applicability of SGD-based GBML methods to large models trained with adaptive optimizers remains unclear. Finally, most GBML research to date has been limited to the single-GPU setup due to the lack of distributed training support [9, 20], which is essential in large-scale learning. In this work, we endeavor to resolve the aforementioned scalability issues in GBML by co-developing algorithms and systems, and explore the initial potential of scalable meta learning in diverse applica- tions. Our main contributions can be summarized as follows: 1. We investigate the root causes of substantial memory/compute costs, algorithmic instability, and a lack of distributed training support in GBML, all of which significantly limit its scalability, through a technical analysis of implicit differentiation. In doing so, we identify three major factors, namely (i) base Jacobian inversion, (ii) a lack of algorithmic adaptation for adaptive optimizers, and (iii) a need for the custom implementation of the backward pass of meta gradient computation, and discuss how each of them contributes negatively to the above limitations in depth. 2. Taking one step further, we propose an initial solution to each of the aforementioned issues by respectively (i) approximating the base Jacobian with an identity matrix, (ii) additionally expanding the meta Jacobian via the chain rule, and (iii) devising a novel communication strategy that efficiently uses the communication-computation overlap trick"}