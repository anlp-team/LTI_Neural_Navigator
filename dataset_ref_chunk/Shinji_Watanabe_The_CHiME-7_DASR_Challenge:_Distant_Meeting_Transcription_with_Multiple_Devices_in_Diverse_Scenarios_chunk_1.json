{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_The_CHiME-7_DASR_Challenge:_Distant_Meeting_Transcription_with_Multiple_Devices_in_Diverse_Scenarios_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the CHiME-7 DASR Challenge focused on?,answer: The CHiME-7 DASR Challenge is focused on distant meeting transcription with multiple devices in diverse scenarios.", "ref_chunk": "3 2 0 2 l u J 4 1 ] S A . s s e e [ 2 v 4 3 7 3 1 . 6 0 3 2 : v i X r a The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios Samuele Cornell1,2, Matthew Wiesner3, Shinji Watanabe2, Desh Raj3, Xuankai Chang2, Paola Garcia3, Matthew Maciejewski3, Yoshiki Masuyama4, Zhong-Qiu Wang2, Stefano Squartini1, Sanjeev Khudanpur3 1Universit`a Politecnica delle Marche, Italy 2Carnegie Mellon University, USA 3Johns Hopkins University, USA 4Tokyo Metropolitan University, Japan Abstract The CHiME challenges have played a significant role in the de- velopment and evaluation of robust automatic speech recogni- tion (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task com- prises joint ASR and diarization in far-field settings with mul- tiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse sce- narios: CHiME-6, DiPCo, and Mixer 6. The goal is for partici- pants to devise a single system that can generalize across differ- ent array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that partic- ipants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, mo- tivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology ag- nostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR). Index Terms: CHiME challenge, speech recognition, multi- channel, speaker diarization, speech separation a noisy/reverberant environment captured by multiple micro- phones. The objective in this case was to foster research in robust ASR methods, ignoring possible issues that can arise from overlapped speech, wrong segmentation, and colloquial language. These datasets comprised either fully simulated or semi-real environments, where utterances come from an audio- book speech corpus such as LibriSpeech [22]. A notable ex- ception in this regard is LibriCSS. It features a simulated, un- segmented full meeting scenario between multiple participants in semi-real conditions; it was obtained by playing LibriSpeech utterances via loudspeakers in a real room. AMI, ICSI, and Sheffield Wargames were historically among the first projects that featured real unsegmented meeting- style scenarios recorded in far-field settings. Although these are more expensive to collect compared with simulated or semi- real datasets, they better reflect possible real-world applications. Recently, this line of research has seen renewed interest, and has resulted in the collection and creation of datasets such as CH- iME-5/6, DiPCo, Alimeeting, and Ego4D. The progress made in ASR, diarization and speech separation is a significant con- tributing factor to this trend, and it suggests that reliable and robust transcription in these scenarios is within our reach in the coming years. 1. Introduction Despite significant leaps made in the last decade, reliable conversational speech recognition remains a significant chal- lenge [1,2]. This is particularly true in meeting scenarios which are often constrained to use only distant array devices [1, 3\u20136]. Besides difficulties due to noise and reverberation in far-field speech, in meeting scenarios a crucial challenge is the presence of overlapped speech, which may amount to more than 15% of total conversation [7]. A third issue are linguistic artifacts aris- ing from informal settings or domain-specific words, an often disregarded characteristic in ASR research, as most commonly used benchmark datasets feature scripted or semi-scripted con- versations [2]. The proposed CHiME-7 DASR challenge1 follows this lat- ter line of research on real, unsegmented meeting scenarios. Our main goal is to foster research in an important direction: how can we build systems that can generalize across a wide range of real-world settings and provide reliable ASR perfor- mance under adverse acoustic conditions? Additionally, we aim to incorporate recent progresses in self-supervised learning to address this problem as well as supervised DNN-based speech separation and enhancement (SSE), by allowing the use of ex- ternal data and pre-trained models. 2. Motivation As these difficulties arise from diverse factors, multi-talker distant conversational speech recognition requires a compre- hensive approach for the development and integration of both front-end and ASR back-end techniques. Over the years, sev- eral challenges and corpora have played a significant role in advancing research in this field. These include, but are not limited to, ASpiRE [8], AMI [9], ICSI [10], the CHiME (1\u2013 4) challenges [11\u201314], the Sheffield Wargames corpus [15], and DIRHA [16]. More recently, we have seen VOiCES [17], DiPCo [18], LibriCSS [3], Alimeeting [19], Ego4D [20], and the recent CHiME 5 and 6 challenges [1, 21]. Several of these datasets/challenges, such as DIRHA, VOiCES, and CHiME-4, focused primarily on acoustic robust- ness, thus featuring pre-segmented single speaker utterances in Compared to the previous CHiME editions, CHiME-7 DASR features three main novelties, motivated by recent promising di- rections in speech processing. 2.1. Diverse Scenarios To expand the breadth of evaluation conditions, we include two additional datasets (along with CHiME-6) \u2014 DiPCo [18], and Mixer 6 Speech [23]. The objective is to encourage research to- wards methods that (1) work well across different array topolo- gies: linear (CHiME-6), circular (DiPCo), or heterogeneous (Mixer 6); (2) are capable of handling variable numbers of speakers in each session; (3) account for linguistic differences 1CHiME-7 DASR website: chimechallenge.org/current/task1/index between dinner party scenarios (CHiME-6 and DiPCo) versus interviews (Mixer 6); and (4) can effectively handle diverse acoustic conditions. Such variability reflects real use-cases where such cross-domain generalization capability is highly de- sirable. 2.2. \u201cFoundation\u201d Models Pre-trained speech models trained on large datasets are in- creasingly becoming available to the public. These include supervised models such as OpenAI Whisper [24], Nvidia JASPER [25] and QuartzNet [26], and self-supervised ones such as Wav2Vec 2.0 [27], HuBERT [28], and WavLM [29]. The latter, in particular, have proved effective in many down- stream applications [30], and their integration with front-end speech enhancement has enabled state-of-the-art performance on benchmarks such as CHiME-4 [31, 32]. Leveraging these models offers unique opportunities as well as novel challenges. By"}, {"question": " What is the goal of the participants in the CHiME-7 DASR Challenge?,answer: The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information.", "ref_chunk": "3 2 0 2 l u J 4 1 ] S A . s s e e [ 2 v 4 3 7 3 1 . 6 0 3 2 : v i X r a The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios Samuele Cornell1,2, Matthew Wiesner3, Shinji Watanabe2, Desh Raj3, Xuankai Chang2, Paola Garcia3, Matthew Maciejewski3, Yoshiki Masuyama4, Zhong-Qiu Wang2, Stefano Squartini1, Sanjeev Khudanpur3 1Universit`a Politecnica delle Marche, Italy 2Carnegie Mellon University, USA 3Johns Hopkins University, USA 4Tokyo Metropolitan University, Japan Abstract The CHiME challenges have played a significant role in the de- velopment and evaluation of robust automatic speech recogni- tion (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task com- prises joint ASR and diarization in far-field settings with mul- tiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse sce- narios: CHiME-6, DiPCo, and Mixer 6. The goal is for partici- pants to devise a single system that can generalize across differ- ent array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that partic- ipants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, mo- tivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology ag- nostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR). Index Terms: CHiME challenge, speech recognition, multi- channel, speaker diarization, speech separation a noisy/reverberant environment captured by multiple micro- phones. The objective in this case was to foster research in robust ASR methods, ignoring possible issues that can arise from overlapped speech, wrong segmentation, and colloquial language. These datasets comprised either fully simulated or semi-real environments, where utterances come from an audio- book speech corpus such as LibriSpeech [22]. A notable ex- ception in this regard is LibriCSS. It features a simulated, un- segmented full meeting scenario between multiple participants in semi-real conditions; it was obtained by playing LibriSpeech utterances via loudspeakers in a real room. AMI, ICSI, and Sheffield Wargames were historically among the first projects that featured real unsegmented meeting- style scenarios recorded in far-field settings. Although these are more expensive to collect compared with simulated or semi- real datasets, they better reflect possible real-world applications. Recently, this line of research has seen renewed interest, and has resulted in the collection and creation of datasets such as CH- iME-5/6, DiPCo, Alimeeting, and Ego4D. The progress made in ASR, diarization and speech separation is a significant con- tributing factor to this trend, and it suggests that reliable and robust transcription in these scenarios is within our reach in the coming years. 1. Introduction Despite significant leaps made in the last decade, reliable conversational speech recognition remains a significant chal- lenge [1,2]. This is particularly true in meeting scenarios which are often constrained to use only distant array devices [1, 3\u20136]. Besides difficulties due to noise and reverberation in far-field speech, in meeting scenarios a crucial challenge is the presence of overlapped speech, which may amount to more than 15% of total conversation [7]. A third issue are linguistic artifacts aris- ing from informal settings or domain-specific words, an often disregarded characteristic in ASR research, as most commonly used benchmark datasets feature scripted or semi-scripted con- versations [2]. The proposed CHiME-7 DASR challenge1 follows this lat- ter line of research on real, unsegmented meeting scenarios. Our main goal is to foster research in an important direction: how can we build systems that can generalize across a wide range of real-world settings and provide reliable ASR perfor- mance under adverse acoustic conditions? Additionally, we aim to incorporate recent progresses in self-supervised learning to address this problem as well as supervised DNN-based speech separation and enhancement (SSE), by allowing the use of ex- ternal data and pre-trained models. 2. Motivation As these difficulties arise from diverse factors, multi-talker distant conversational speech recognition requires a compre- hensive approach for the development and integration of both front-end and ASR back-end techniques. Over the years, sev- eral challenges and corpora have played a significant role in advancing research in this field. These include, but are not limited to, ASpiRE [8], AMI [9], ICSI [10], the CHiME (1\u2013 4) challenges [11\u201314], the Sheffield Wargames corpus [15], and DIRHA [16]. More recently, we have seen VOiCES [17], DiPCo [18], LibriCSS [3], Alimeeting [19], Ego4D [20], and the recent CHiME 5 and 6 challenges [1, 21]. Several of these datasets/challenges, such as DIRHA, VOiCES, and CHiME-4, focused primarily on acoustic robust- ness, thus featuring pre-segmented single speaker utterances in Compared to the previous CHiME editions, CHiME-7 DASR features three main novelties, motivated by recent promising di- rections in speech processing. 2.1. Diverse Scenarios To expand the breadth of evaluation conditions, we include two additional datasets (along with CHiME-6) \u2014 DiPCo [18], and Mixer 6 Speech [23]. The objective is to encourage research to- wards methods that (1) work well across different array topolo- gies: linear (CHiME-6), circular (DiPCo), or heterogeneous (Mixer 6); (2) are capable of handling variable numbers of speakers in each session; (3) account for linguistic differences 1CHiME-7 DASR website: chimechallenge.org/current/task1/index between dinner party scenarios (CHiME-6 and DiPCo) versus interviews (Mixer 6); and (4) can effectively handle diverse acoustic conditions. Such variability reflects real use-cases where such cross-domain generalization capability is highly de- sirable. 2.2. \u201cFoundation\u201d Models Pre-trained speech models trained on large datasets are in- creasingly becoming available to the public. These include supervised models such as OpenAI Whisper [24], Nvidia JASPER [25] and QuartzNet [26], and self-supervised ones such as Wav2Vec 2.0 [27], HuBERT [28], and WavLM [29]. The latter, in particular, have proved effective in many down- stream applications [30], and their integration with front-end speech enhancement has enabled state-of-the-art performance on benchmarks such as CHiME-4 [31, 32]. Leveraging these models offers unique opportunities as well as novel challenges. By"}, {"question": " What are some of the novelties in the CHiME-7 DASR Challenge compared to previous editions?,answer: The novelties include participants being allowed to use open-source pre-trained models and datasets, evaluation on 3 diverse scenarios (CHiME-6, DiPCo, and Mixer 6), and participants are allowed to devise a single system for generalization.", "ref_chunk": "3 2 0 2 l u J 4 1 ] S A . s s e e [ 2 v 4 3 7 3 1 . 6 0 3 2 : v i X r a The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios Samuele Cornell1,2, Matthew Wiesner3, Shinji Watanabe2, Desh Raj3, Xuankai Chang2, Paola Garcia3, Matthew Maciejewski3, Yoshiki Masuyama4, Zhong-Qiu Wang2, Stefano Squartini1, Sanjeev Khudanpur3 1Universit`a Politecnica delle Marche, Italy 2Carnegie Mellon University, USA 3Johns Hopkins University, USA 4Tokyo Metropolitan University, Japan Abstract The CHiME challenges have played a significant role in the de- velopment and evaluation of robust automatic speech recogni- tion (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task com- prises joint ASR and diarization in far-field settings with mul- tiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse sce- narios: CHiME-6, DiPCo, and Mixer 6. The goal is for partici- pants to devise a single system that can generalize across differ- ent array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that partic- ipants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, mo- tivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology ag- nostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR). Index Terms: CHiME challenge, speech recognition, multi- channel, speaker diarization, speech separation a noisy/reverberant environment captured by multiple micro- phones. The objective in this case was to foster research in robust ASR methods, ignoring possible issues that can arise from overlapped speech, wrong segmentation, and colloquial language. These datasets comprised either fully simulated or semi-real environments, where utterances come from an audio- book speech corpus such as LibriSpeech [22]. A notable ex- ception in this regard is LibriCSS. It features a simulated, un- segmented full meeting scenario between multiple participants in semi-real conditions; it was obtained by playing LibriSpeech utterances via loudspeakers in a real room. AMI, ICSI, and Sheffield Wargames were historically among the first projects that featured real unsegmented meeting- style scenarios recorded in far-field settings. Although these are more expensive to collect compared with simulated or semi- real datasets, they better reflect possible real-world applications. Recently, this line of research has seen renewed interest, and has resulted in the collection and creation of datasets such as CH- iME-5/6, DiPCo, Alimeeting, and Ego4D. The progress made in ASR, diarization and speech separation is a significant con- tributing factor to this trend, and it suggests that reliable and robust transcription in these scenarios is within our reach in the coming years. 1. Introduction Despite significant leaps made in the last decade, reliable conversational speech recognition remains a significant chal- lenge [1,2]. This is particularly true in meeting scenarios which are often constrained to use only distant array devices [1, 3\u20136]. Besides difficulties due to noise and reverberation in far-field speech, in meeting scenarios a crucial challenge is the presence of overlapped speech, which may amount to more than 15% of total conversation [7]. A third issue are linguistic artifacts aris- ing from informal settings or domain-specific words, an often disregarded characteristic in ASR research, as most commonly used benchmark datasets feature scripted or semi-scripted con- versations [2]. The proposed CHiME-7 DASR challenge1 follows this lat- ter line of research on real, unsegmented meeting scenarios. Our main goal is to foster research in an important direction: how can we build systems that can generalize across a wide range of real-world settings and provide reliable ASR perfor- mance under adverse acoustic conditions? Additionally, we aim to incorporate recent progresses in self-supervised learning to address this problem as well as supervised DNN-based speech separation and enhancement (SSE), by allowing the use of ex- ternal data and pre-trained models. 2. Motivation As these difficulties arise from diverse factors, multi-talker distant conversational speech recognition requires a compre- hensive approach for the development and integration of both front-end and ASR back-end techniques. Over the years, sev- eral challenges and corpora have played a significant role in advancing research in this field. These include, but are not limited to, ASpiRE [8], AMI [9], ICSI [10], the CHiME (1\u2013 4) challenges [11\u201314], the Sheffield Wargames corpus [15], and DIRHA [16]. More recently, we have seen VOiCES [17], DiPCo [18], LibriCSS [3], Alimeeting [19], Ego4D [20], and the recent CHiME 5 and 6 challenges [1, 21]. Several of these datasets/challenges, such as DIRHA, VOiCES, and CHiME-4, focused primarily on acoustic robust- ness, thus featuring pre-segmented single speaker utterances in Compared to the previous CHiME editions, CHiME-7 DASR features three main novelties, motivated by recent promising di- rections in speech processing. 2.1. Diverse Scenarios To expand the breadth of evaluation conditions, we include two additional datasets (along with CHiME-6) \u2014 DiPCo [18], and Mixer 6 Speech [23]. The objective is to encourage research to- wards methods that (1) work well across different array topolo- gies: linear (CHiME-6), circular (DiPCo), or heterogeneous (Mixer 6); (2) are capable of handling variable numbers of speakers in each session; (3) account for linguistic differences 1CHiME-7 DASR website: chimechallenge.org/current/task1/index between dinner party scenarios (CHiME-6 and DiPCo) versus interviews (Mixer 6); and (4) can effectively handle diverse acoustic conditions. Such variability reflects real use-cases where such cross-domain generalization capability is highly de- sirable. 2.2. \u201cFoundation\u201d Models Pre-trained speech models trained on large datasets are in- creasingly becoming available to the public. These include supervised models such as OpenAI Whisper [24], Nvidia JASPER [25] and QuartzNet [26], and self-supervised ones such as Wav2Vec 2.0 [27], HuBERT [28], and WavLM [29]. The latter, in particular, have proved effective in many down- stream applications [30], and their integration with front-end speech enhancement has enabled state-of-the-art performance on benchmarks such as CHiME-4 [31, 32]. Leveraging these models offers unique opportunities as well as novel challenges. By"}, {"question": " What are some challenges in meeting scenarios that affect speech recognition?,answer: Challenges include noise and reverberation in far-field speech, overlapped speech, and linguistic artifacts from informal settings or domain-specific words.", "ref_chunk": "3 2 0 2 l u J 4 1 ] S A . s s e e [ 2 v 4 3 7 3 1 . 6 0 3 2 : v i X r a The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios Samuele Cornell1,2, Matthew Wiesner3, Shinji Watanabe2, Desh Raj3, Xuankai Chang2, Paola Garcia3, Matthew Maciejewski3, Yoshiki Masuyama4, Zhong-Qiu Wang2, Stefano Squartini1, Sanjeev Khudanpur3 1Universit`a Politecnica delle Marche, Italy 2Carnegie Mellon University, USA 3Johns Hopkins University, USA 4Tokyo Metropolitan University, Japan Abstract The CHiME challenges have played a significant role in the de- velopment and evaluation of robust automatic speech recogni- tion (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task com- prises joint ASR and diarization in far-field settings with mul- tiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse sce- narios: CHiME-6, DiPCo, and Mixer 6. The goal is for partici- pants to devise a single system that can generalize across differ- ent array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that partic- ipants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, mo- tivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology ag- nostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR). Index Terms: CHiME challenge, speech recognition, multi- channel, speaker diarization, speech separation a noisy/reverberant environment captured by multiple micro- phones. The objective in this case was to foster research in robust ASR methods, ignoring possible issues that can arise from overlapped speech, wrong segmentation, and colloquial language. These datasets comprised either fully simulated or semi-real environments, where utterances come from an audio- book speech corpus such as LibriSpeech [22]. A notable ex- ception in this regard is LibriCSS. It features a simulated, un- segmented full meeting scenario between multiple participants in semi-real conditions; it was obtained by playing LibriSpeech utterances via loudspeakers in a real room. AMI, ICSI, and Sheffield Wargames were historically among the first projects that featured real unsegmented meeting- style scenarios recorded in far-field settings. Although these are more expensive to collect compared with simulated or semi- real datasets, they better reflect possible real-world applications. Recently, this line of research has seen renewed interest, and has resulted in the collection and creation of datasets such as CH- iME-5/6, DiPCo, Alimeeting, and Ego4D. The progress made in ASR, diarization and speech separation is a significant con- tributing factor to this trend, and it suggests that reliable and robust transcription in these scenarios is within our reach in the coming years. 1. Introduction Despite significant leaps made in the last decade, reliable conversational speech recognition remains a significant chal- lenge [1,2]. This is particularly true in meeting scenarios which are often constrained to use only distant array devices [1, 3\u20136]. Besides difficulties due to noise and reverberation in far-field speech, in meeting scenarios a crucial challenge is the presence of overlapped speech, which may amount to more than 15% of total conversation [7]. A third issue are linguistic artifacts aris- ing from informal settings or domain-specific words, an often disregarded characteristic in ASR research, as most commonly used benchmark datasets feature scripted or semi-scripted con- versations [2]. The proposed CHiME-7 DASR challenge1 follows this lat- ter line of research on real, unsegmented meeting scenarios. Our main goal is to foster research in an important direction: how can we build systems that can generalize across a wide range of real-world settings and provide reliable ASR perfor- mance under adverse acoustic conditions? Additionally, we aim to incorporate recent progresses in self-supervised learning to address this problem as well as supervised DNN-based speech separation and enhancement (SSE), by allowing the use of ex- ternal data and pre-trained models. 2. Motivation As these difficulties arise from diverse factors, multi-talker distant conversational speech recognition requires a compre- hensive approach for the development and integration of both front-end and ASR back-end techniques. Over the years, sev- eral challenges and corpora have played a significant role in advancing research in this field. These include, but are not limited to, ASpiRE [8], AMI [9], ICSI [10], the CHiME (1\u2013 4) challenges [11\u201314], the Sheffield Wargames corpus [15], and DIRHA [16]. More recently, we have seen VOiCES [17], DiPCo [18], LibriCSS [3], Alimeeting [19], Ego4D [20], and the recent CHiME 5 and 6 challenges [1, 21]. Several of these datasets/challenges, such as DIRHA, VOiCES, and CHiME-4, focused primarily on acoustic robust- ness, thus featuring pre-segmented single speaker utterances in Compared to the previous CHiME editions, CHiME-7 DASR features three main novelties, motivated by recent promising di- rections in speech processing. 2.1. Diverse Scenarios To expand the breadth of evaluation conditions, we include two additional datasets (along with CHiME-6) \u2014 DiPCo [18], and Mixer 6 Speech [23]. The objective is to encourage research to- wards methods that (1) work well across different array topolo- gies: linear (CHiME-6), circular (DiPCo), or heterogeneous (Mixer 6); (2) are capable of handling variable numbers of speakers in each session; (3) account for linguistic differences 1CHiME-7 DASR website: chimechallenge.org/current/task1/index between dinner party scenarios (CHiME-6 and DiPCo) versus interviews (Mixer 6); and (4) can effectively handle diverse acoustic conditions. Such variability reflects real use-cases where such cross-domain generalization capability is highly de- sirable. 2.2. \u201cFoundation\u201d Models Pre-trained speech models trained on large datasets are in- creasingly becoming available to the public. These include supervised models such as OpenAI Whisper [24], Nvidia JASPER [25] and QuartzNet [26], and self-supervised ones such as Wav2Vec 2.0 [27], HuBERT [28], and WavLM [29]. The latter, in particular, have proved effective in many down- stream applications [30], and their integration with front-end speech enhancement has enabled state-of-the-art performance on benchmarks such as CHiME-4 [31, 32]. Leveraging these models offers unique opportunities as well as novel challenges. By"}, {"question": " What datasets were historically among the first to feature real unsegmented meeting-style scenarios recorded in far-field settings?,answer: AMI, ICSI, and Sheffield Wargames were historically among the first projects that featured real unsegmented meeting-style scenarios recorded in far-field settings.", "ref_chunk": "3 2 0 2 l u J 4 1 ] S A . s s e e [ 2 v 4 3 7 3 1 . 6 0 3 2 : v i X r a The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios Samuele Cornell1,2, Matthew Wiesner3, Shinji Watanabe2, Desh Raj3, Xuankai Chang2, Paola Garcia3, Matthew Maciejewski3, Yoshiki Masuyama4, Zhong-Qiu Wang2, Stefano Squartini1, Sanjeev Khudanpur3 1Universit`a Politecnica delle Marche, Italy 2Carnegie Mellon University, USA 3Johns Hopkins University, USA 4Tokyo Metropolitan University, Japan Abstract The CHiME challenges have played a significant role in the de- velopment and evaluation of robust automatic speech recogni- tion (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task com- prises joint ASR and diarization in far-field settings with mul- tiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse sce- narios: CHiME-6, DiPCo, and Mixer 6. The goal is for partici- pants to devise a single system that can generalize across differ- ent array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that partic- ipants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, mo- tivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology ag- nostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR). Index Terms: CHiME challenge, speech recognition, multi- channel, speaker diarization, speech separation a noisy/reverberant environment captured by multiple micro- phones. The objective in this case was to foster research in robust ASR methods, ignoring possible issues that can arise from overlapped speech, wrong segmentation, and colloquial language. These datasets comprised either fully simulated or semi-real environments, where utterances come from an audio- book speech corpus such as LibriSpeech [22]. A notable ex- ception in this regard is LibriCSS. It features a simulated, un- segmented full meeting scenario between multiple participants in semi-real conditions; it was obtained by playing LibriSpeech utterances via loudspeakers in a real room. AMI, ICSI, and Sheffield Wargames were historically among the first projects that featured real unsegmented meeting- style scenarios recorded in far-field settings. Although these are more expensive to collect compared with simulated or semi- real datasets, they better reflect possible real-world applications. Recently, this line of research has seen renewed interest, and has resulted in the collection and creation of datasets such as CH- iME-5/6, DiPCo, Alimeeting, and Ego4D. The progress made in ASR, diarization and speech separation is a significant con- tributing factor to this trend, and it suggests that reliable and robust transcription in these scenarios is within our reach in the coming years. 1. Introduction Despite significant leaps made in the last decade, reliable conversational speech recognition remains a significant chal- lenge [1,2]. This is particularly true in meeting scenarios which are often constrained to use only distant array devices [1, 3\u20136]. Besides difficulties due to noise and reverberation in far-field speech, in meeting scenarios a crucial challenge is the presence of overlapped speech, which may amount to more than 15% of total conversation [7]. A third issue are linguistic artifacts aris- ing from informal settings or domain-specific words, an often disregarded characteristic in ASR research, as most commonly used benchmark datasets feature scripted or semi-scripted con- versations [2]. The proposed CHiME-7 DASR challenge1 follows this lat- ter line of research on real, unsegmented meeting scenarios. Our main goal is to foster research in an important direction: how can we build systems that can generalize across a wide range of real-world settings and provide reliable ASR perfor- mance under adverse acoustic conditions? Additionally, we aim to incorporate recent progresses in self-supervised learning to address this problem as well as supervised DNN-based speech separation and enhancement (SSE), by allowing the use of ex- ternal data and pre-trained models. 2. Motivation As these difficulties arise from diverse factors, multi-talker distant conversational speech recognition requires a compre- hensive approach for the development and integration of both front-end and ASR back-end techniques. Over the years, sev- eral challenges and corpora have played a significant role in advancing research in this field. These include, but are not limited to, ASpiRE [8], AMI [9], ICSI [10], the CHiME (1\u2013 4) challenges [11\u201314], the Sheffield Wargames corpus [15], and DIRHA [16]. More recently, we have seen VOiCES [17], DiPCo [18], LibriCSS [3], Alimeeting [19], Ego4D [20], and the recent CHiME 5 and 6 challenges [1, 21]. Several of these datasets/challenges, such as DIRHA, VOiCES, and CHiME-4, focused primarily on acoustic robust- ness, thus featuring pre-segmented single speaker utterances in Compared to the previous CHiME editions, CHiME-7 DASR features three main novelties, motivated by recent promising di- rections in speech processing. 2.1. Diverse Scenarios To expand the breadth of evaluation conditions, we include two additional datasets (along with CHiME-6) \u2014 DiPCo [18], and Mixer 6 Speech [23]. The objective is to encourage research to- wards methods that (1) work well across different array topolo- gies: linear (CHiME-6), circular (DiPCo), or heterogeneous (Mixer 6); (2) are capable of handling variable numbers of speakers in each session; (3) account for linguistic differences 1CHiME-7 DASR website: chimechallenge.org/current/task1/index between dinner party scenarios (CHiME-6 and DiPCo) versus interviews (Mixer 6); and (4) can effectively handle diverse acoustic conditions. Such variability reflects real use-cases where such cross-domain generalization capability is highly de- sirable. 2.2. \u201cFoundation\u201d Models Pre-trained speech models trained on large datasets are in- creasingly becoming available to the public. These include supervised models such as OpenAI Whisper [24], Nvidia JASPER [25] and QuartzNet [26], and self-supervised ones such as Wav2Vec 2.0 [27], HuBERT [28], and WavLM [29]. The latter, in particular, have proved effective in many down- stream applications [30], and their integration with front-end speech enhancement has enabled state-of-the-art performance on benchmarks such as CHiME-4 [31, 32]. Leveraging these models offers unique opportunities as well as novel challenges. By"}, {"question": " What progress has been made in ASR, diarization, and speech separation in diverse scenarios?,answer: Progress in ASR, diarization, and speech separation suggests that reliable and robust transcription in diverse scenarios is achievable in the coming years.", "ref_chunk": "3 2 0 2 l u J 4 1 ] S A . s s e e [ 2 v 4 3 7 3 1 . 6 0 3 2 : v i X r a The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios Samuele Cornell1,2, Matthew Wiesner3, Shinji Watanabe2, Desh Raj3, Xuankai Chang2, Paola Garcia3, Matthew Maciejewski3, Yoshiki Masuyama4, Zhong-Qiu Wang2, Stefano Squartini1, Sanjeev Khudanpur3 1Universit`a Politecnica delle Marche, Italy 2Carnegie Mellon University, USA 3Johns Hopkins University, USA 4Tokyo Metropolitan University, Japan Abstract The CHiME challenges have played a significant role in the de- velopment and evaluation of robust automatic speech recogni- tion (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task com- prises joint ASR and diarization in far-field settings with mul- tiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse sce- narios: CHiME-6, DiPCo, and Mixer 6. The goal is for partici- pants to devise a single system that can generalize across differ- ent array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that partic- ipants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, mo- tivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology ag- nostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR). Index Terms: CHiME challenge, speech recognition, multi- channel, speaker diarization, speech separation a noisy/reverberant environment captured by multiple micro- phones. The objective in this case was to foster research in robust ASR methods, ignoring possible issues that can arise from overlapped speech, wrong segmentation, and colloquial language. These datasets comprised either fully simulated or semi-real environments, where utterances come from an audio- book speech corpus such as LibriSpeech [22]. A notable ex- ception in this regard is LibriCSS. It features a simulated, un- segmented full meeting scenario between multiple participants in semi-real conditions; it was obtained by playing LibriSpeech utterances via loudspeakers in a real room. AMI, ICSI, and Sheffield Wargames were historically among the first projects that featured real unsegmented meeting- style scenarios recorded in far-field settings. Although these are more expensive to collect compared with simulated or semi- real datasets, they better reflect possible real-world applications. Recently, this line of research has seen renewed interest, and has resulted in the collection and creation of datasets such as CH- iME-5/6, DiPCo, Alimeeting, and Ego4D. The progress made in ASR, diarization and speech separation is a significant con- tributing factor to this trend, and it suggests that reliable and robust transcription in these scenarios is within our reach in the coming years. 1. Introduction Despite significant leaps made in the last decade, reliable conversational speech recognition remains a significant chal- lenge [1,2]. This is particularly true in meeting scenarios which are often constrained to use only distant array devices [1, 3\u20136]. Besides difficulties due to noise and reverberation in far-field speech, in meeting scenarios a crucial challenge is the presence of overlapped speech, which may amount to more than 15% of total conversation [7]. A third issue are linguistic artifacts aris- ing from informal settings or domain-specific words, an often disregarded characteristic in ASR research, as most commonly used benchmark datasets feature scripted or semi-scripted con- versations [2]. The proposed CHiME-7 DASR challenge1 follows this lat- ter line of research on real, unsegmented meeting scenarios. Our main goal is to foster research in an important direction: how can we build systems that can generalize across a wide range of real-world settings and provide reliable ASR perfor- mance under adverse acoustic conditions? Additionally, we aim to incorporate recent progresses in self-supervised learning to address this problem as well as supervised DNN-based speech separation and enhancement (SSE), by allowing the use of ex- ternal data and pre-trained models. 2. Motivation As these difficulties arise from diverse factors, multi-talker distant conversational speech recognition requires a compre- hensive approach for the development and integration of both front-end and ASR back-end techniques. Over the years, sev- eral challenges and corpora have played a significant role in advancing research in this field. These include, but are not limited to, ASpiRE [8], AMI [9], ICSI [10], the CHiME (1\u2013 4) challenges [11\u201314], the Sheffield Wargames corpus [15], and DIRHA [16]. More recently, we have seen VOiCES [17], DiPCo [18], LibriCSS [3], Alimeeting [19], Ego4D [20], and the recent CHiME 5 and 6 challenges [1, 21]. Several of these datasets/challenges, such as DIRHA, VOiCES, and CHiME-4, focused primarily on acoustic robust- ness, thus featuring pre-segmented single speaker utterances in Compared to the previous CHiME editions, CHiME-7 DASR features three main novelties, motivated by recent promising di- rections in speech processing. 2.1. Diverse Scenarios To expand the breadth of evaluation conditions, we include two additional datasets (along with CHiME-6) \u2014 DiPCo [18], and Mixer 6 Speech [23]. The objective is to encourage research to- wards methods that (1) work well across different array topolo- gies: linear (CHiME-6), circular (DiPCo), or heterogeneous (Mixer 6); (2) are capable of handling variable numbers of speakers in each session; (3) account for linguistic differences 1CHiME-7 DASR website: chimechallenge.org/current/task1/index between dinner party scenarios (CHiME-6 and DiPCo) versus interviews (Mixer 6); and (4) can effectively handle diverse acoustic conditions. Such variability reflects real use-cases where such cross-domain generalization capability is highly de- sirable. 2.2. \u201cFoundation\u201d Models Pre-trained speech models trained on large datasets are in- creasingly becoming available to the public. These include supervised models such as OpenAI Whisper [24], Nvidia JASPER [25] and QuartzNet [26], and self-supervised ones such as Wav2Vec 2.0 [27], HuBERT [28], and WavLM [29]. The latter, in particular, have proved effective in many down- stream applications [30], and their integration with front-end speech enhancement has enabled state-of-the-art performance on benchmarks such as CHiME-4 [31, 32]. Leveraging these models offers unique opportunities as well as novel challenges. By"}, {"question": " What is a significant contributing factor to the renewed interest in research on meeting scenarios for ASR?,answer: A significant contributing factor is the progress made in ASR, diarization, and speech separation.", "ref_chunk": "3 2 0 2 l u J 4 1 ] S A . s s e e [ 2 v 4 3 7 3 1 . 6 0 3 2 : v i X r a The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios Samuele Cornell1,2, Matthew Wiesner3, Shinji Watanabe2, Desh Raj3, Xuankai Chang2, Paola Garcia3, Matthew Maciejewski3, Yoshiki Masuyama4, Zhong-Qiu Wang2, Stefano Squartini1, Sanjeev Khudanpur3 1Universit`a Politecnica delle Marche, Italy 2Carnegie Mellon University, USA 3Johns Hopkins University, USA 4Tokyo Metropolitan University, Japan Abstract The CHiME challenges have played a significant role in the de- velopment and evaluation of robust automatic speech recogni- tion (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task com- prises joint ASR and diarization in far-field settings with mul- tiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse sce- narios: CHiME-6, DiPCo, and Mixer 6. The goal is for partici- pants to devise a single system that can generalize across differ- ent array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that partic- ipants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, mo- tivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology ag- nostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR). Index Terms: CHiME challenge, speech recognition, multi- channel, speaker diarization, speech separation a noisy/reverberant environment captured by multiple micro- phones. The objective in this case was to foster research in robust ASR methods, ignoring possible issues that can arise from overlapped speech, wrong segmentation, and colloquial language. These datasets comprised either fully simulated or semi-real environments, where utterances come from an audio- book speech corpus such as LibriSpeech [22]. A notable ex- ception in this regard is LibriCSS. It features a simulated, un- segmented full meeting scenario between multiple participants in semi-real conditions; it was obtained by playing LibriSpeech utterances via loudspeakers in a real room. AMI, ICSI, and Sheffield Wargames were historically among the first projects that featured real unsegmented meeting- style scenarios recorded in far-field settings. Although these are more expensive to collect compared with simulated or semi- real datasets, they better reflect possible real-world applications. Recently, this line of research has seen renewed interest, and has resulted in the collection and creation of datasets such as CH- iME-5/6, DiPCo, Alimeeting, and Ego4D. The progress made in ASR, diarization and speech separation is a significant con- tributing factor to this trend, and it suggests that reliable and robust transcription in these scenarios is within our reach in the coming years. 1. Introduction Despite significant leaps made in the last decade, reliable conversational speech recognition remains a significant chal- lenge [1,2]. This is particularly true in meeting scenarios which are often constrained to use only distant array devices [1, 3\u20136]. Besides difficulties due to noise and reverberation in far-field speech, in meeting scenarios a crucial challenge is the presence of overlapped speech, which may amount to more than 15% of total conversation [7]. A third issue are linguistic artifacts aris- ing from informal settings or domain-specific words, an often disregarded characteristic in ASR research, as most commonly used benchmark datasets feature scripted or semi-scripted con- versations [2]. The proposed CHiME-7 DASR challenge1 follows this lat- ter line of research on real, unsegmented meeting scenarios. Our main goal is to foster research in an important direction: how can we build systems that can generalize across a wide range of real-world settings and provide reliable ASR perfor- mance under adverse acoustic conditions? Additionally, we aim to incorporate recent progresses in self-supervised learning to address this problem as well as supervised DNN-based speech separation and enhancement (SSE), by allowing the use of ex- ternal data and pre-trained models. 2. Motivation As these difficulties arise from diverse factors, multi-talker distant conversational speech recognition requires a compre- hensive approach for the development and integration of both front-end and ASR back-end techniques. Over the years, sev- eral challenges and corpora have played a significant role in advancing research in this field. These include, but are not limited to, ASpiRE [8], AMI [9], ICSI [10], the CHiME (1\u2013 4) challenges [11\u201314], the Sheffield Wargames corpus [15], and DIRHA [16]. More recently, we have seen VOiCES [17], DiPCo [18], LibriCSS [3], Alimeeting [19], Ego4D [20], and the recent CHiME 5 and 6 challenges [1, 21]. Several of these datasets/challenges, such as DIRHA, VOiCES, and CHiME-4, focused primarily on acoustic robust- ness, thus featuring pre-segmented single speaker utterances in Compared to the previous CHiME editions, CHiME-7 DASR features three main novelties, motivated by recent promising di- rections in speech processing. 2.1. Diverse Scenarios To expand the breadth of evaluation conditions, we include two additional datasets (along with CHiME-6) \u2014 DiPCo [18], and Mixer 6 Speech [23]. The objective is to encourage research to- wards methods that (1) work well across different array topolo- gies: linear (CHiME-6), circular (DiPCo), or heterogeneous (Mixer 6); (2) are capable of handling variable numbers of speakers in each session; (3) account for linguistic differences 1CHiME-7 DASR website: chimechallenge.org/current/task1/index between dinner party scenarios (CHiME-6 and DiPCo) versus interviews (Mixer 6); and (4) can effectively handle diverse acoustic conditions. Such variability reflects real use-cases where such cross-domain generalization capability is highly de- sirable. 2.2. \u201cFoundation\u201d Models Pre-trained speech models trained on large datasets are in- creasingly becoming available to the public. These include supervised models such as OpenAI Whisper [24], Nvidia JASPER [25] and QuartzNet [26], and self-supervised ones such as Wav2Vec 2.0 [27], HuBERT [28], and WavLM [29]. The latter, in particular, have proved effective in many down- stream applications [30], and their integration with front-end speech enhancement has enabled state-of-the-art performance on benchmarks such as CHiME-4 [31, 32]. Leveraging these models offers unique opportunities as well as novel challenges. By"}, {"question": " Why is it important to foster research in developing systems for generalizing across real-world settings in ASR?,answer: It is important to foster research in this area to provide reliable ASR performance under adverse acoustic conditions and real-world settings.", "ref_chunk": "3 2 0 2 l u J 4 1 ] S A . s s e e [ 2 v 4 3 7 3 1 . 6 0 3 2 : v i X r a The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios Samuele Cornell1,2, Matthew Wiesner3, Shinji Watanabe2, Desh Raj3, Xuankai Chang2, Paola Garcia3, Matthew Maciejewski3, Yoshiki Masuyama4, Zhong-Qiu Wang2, Stefano Squartini1, Sanjeev Khudanpur3 1Universit`a Politecnica delle Marche, Italy 2Carnegie Mellon University, USA 3Johns Hopkins University, USA 4Tokyo Metropolitan University, Japan Abstract The CHiME challenges have played a significant role in the de- velopment and evaluation of robust automatic speech recogni- tion (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task com- prises joint ASR and diarization in far-field settings with mul- tiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse sce- narios: CHiME-6, DiPCo, and Mixer 6. The goal is for partici- pants to devise a single system that can generalize across differ- ent array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that partic- ipants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, mo- tivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology ag- nostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR). Index Terms: CHiME challenge, speech recognition, multi- channel, speaker diarization, speech separation a noisy/reverberant environment captured by multiple micro- phones. The objective in this case was to foster research in robust ASR methods, ignoring possible issues that can arise from overlapped speech, wrong segmentation, and colloquial language. These datasets comprised either fully simulated or semi-real environments, where utterances come from an audio- book speech corpus such as LibriSpeech [22]. A notable ex- ception in this regard is LibriCSS. It features a simulated, un- segmented full meeting scenario between multiple participants in semi-real conditions; it was obtained by playing LibriSpeech utterances via loudspeakers in a real room. AMI, ICSI, and Sheffield Wargames were historically among the first projects that featured real unsegmented meeting- style scenarios recorded in far-field settings. Although these are more expensive to collect compared with simulated or semi- real datasets, they better reflect possible real-world applications. Recently, this line of research has seen renewed interest, and has resulted in the collection and creation of datasets such as CH- iME-5/6, DiPCo, Alimeeting, and Ego4D. The progress made in ASR, diarization and speech separation is a significant con- tributing factor to this trend, and it suggests that reliable and robust transcription in these scenarios is within our reach in the coming years. 1. Introduction Despite significant leaps made in the last decade, reliable conversational speech recognition remains a significant chal- lenge [1,2]. This is particularly true in meeting scenarios which are often constrained to use only distant array devices [1, 3\u20136]. Besides difficulties due to noise and reverberation in far-field speech, in meeting scenarios a crucial challenge is the presence of overlapped speech, which may amount to more than 15% of total conversation [7]. A third issue are linguistic artifacts aris- ing from informal settings or domain-specific words, an often disregarded characteristic in ASR research, as most commonly used benchmark datasets feature scripted or semi-scripted con- versations [2]. The proposed CHiME-7 DASR challenge1 follows this lat- ter line of research on real, unsegmented meeting scenarios. Our main goal is to foster research in an important direction: how can we build systems that can generalize across a wide range of real-world settings and provide reliable ASR perfor- mance under adverse acoustic conditions? Additionally, we aim to incorporate recent progresses in self-supervised learning to address this problem as well as supervised DNN-based speech separation and enhancement (SSE), by allowing the use of ex- ternal data and pre-trained models. 2. Motivation As these difficulties arise from diverse factors, multi-talker distant conversational speech recognition requires a compre- hensive approach for the development and integration of both front-end and ASR back-end techniques. Over the years, sev- eral challenges and corpora have played a significant role in advancing research in this field. These include, but are not limited to, ASpiRE [8], AMI [9], ICSI [10], the CHiME (1\u2013 4) challenges [11\u201314], the Sheffield Wargames corpus [15], and DIRHA [16]. More recently, we have seen VOiCES [17], DiPCo [18], LibriCSS [3], Alimeeting [19], Ego4D [20], and the recent CHiME 5 and 6 challenges [1, 21]. Several of these datasets/challenges, such as DIRHA, VOiCES, and CHiME-4, focused primarily on acoustic robust- ness, thus featuring pre-segmented single speaker utterances in Compared to the previous CHiME editions, CHiME-7 DASR features three main novelties, motivated by recent promising di- rections in speech processing. 2.1. Diverse Scenarios To expand the breadth of evaluation conditions, we include two additional datasets (along with CHiME-6) \u2014 DiPCo [18], and Mixer 6 Speech [23]. The objective is to encourage research to- wards methods that (1) work well across different array topolo- gies: linear (CHiME-6), circular (DiPCo), or heterogeneous (Mixer 6); (2) are capable of handling variable numbers of speakers in each session; (3) account for linguistic differences 1CHiME-7 DASR website: chimechallenge.org/current/task1/index between dinner party scenarios (CHiME-6 and DiPCo) versus interviews (Mixer 6); and (4) can effectively handle diverse acoustic conditions. Such variability reflects real use-cases where such cross-domain generalization capability is highly de- sirable. 2.2. \u201cFoundation\u201d Models Pre-trained speech models trained on large datasets are in- creasingly becoming available to the public. These include supervised models such as OpenAI Whisper [24], Nvidia JASPER [25] and QuartzNet [26], and self-supervised ones such as Wav2Vec 2.0 [27], HuBERT [28], and WavLM [29]. The latter, in particular, have proved effective in many down- stream applications [30], and their integration with front-end speech enhancement has enabled state-of-the-art performance on benchmarks such as CHiME-4 [31, 32]. Leveraging these models offers unique opportunities as well as novel challenges. By"}, {"question": " What are some of the diverse scenarios included in the CHiME-7 DASR Challenge?,answer: The diverse scenarios include CHiME-6, DiPCo, and Mixer 6 scenarios.", "ref_chunk": "3 2 0 2 l u J 4 1 ] S A . s s e e [ 2 v 4 3 7 3 1 . 6 0 3 2 : v i X r a The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios Samuele Cornell1,2, Matthew Wiesner3, Shinji Watanabe2, Desh Raj3, Xuankai Chang2, Paola Garcia3, Matthew Maciejewski3, Yoshiki Masuyama4, Zhong-Qiu Wang2, Stefano Squartini1, Sanjeev Khudanpur3 1Universit`a Politecnica delle Marche, Italy 2Carnegie Mellon University, USA 3Johns Hopkins University, USA 4Tokyo Metropolitan University, Japan Abstract The CHiME challenges have played a significant role in the de- velopment and evaluation of robust automatic speech recogni- tion (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task com- prises joint ASR and diarization in far-field settings with mul- tiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse sce- narios: CHiME-6, DiPCo, and Mixer 6. The goal is for partici- pants to devise a single system that can generalize across differ- ent array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that partic- ipants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, mo- tivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology ag- nostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR). Index Terms: CHiME challenge, speech recognition, multi- channel, speaker diarization, speech separation a noisy/reverberant environment captured by multiple micro- phones. The objective in this case was to foster research in robust ASR methods, ignoring possible issues that can arise from overlapped speech, wrong segmentation, and colloquial language. These datasets comprised either fully simulated or semi-real environments, where utterances come from an audio- book speech corpus such as LibriSpeech [22]. A notable ex- ception in this regard is LibriCSS. It features a simulated, un- segmented full meeting scenario between multiple participants in semi-real conditions; it was obtained by playing LibriSpeech utterances via loudspeakers in a real room. AMI, ICSI, and Sheffield Wargames were historically among the first projects that featured real unsegmented meeting- style scenarios recorded in far-field settings. Although these are more expensive to collect compared with simulated or semi- real datasets, they better reflect possible real-world applications. Recently, this line of research has seen renewed interest, and has resulted in the collection and creation of datasets such as CH- iME-5/6, DiPCo, Alimeeting, and Ego4D. The progress made in ASR, diarization and speech separation is a significant con- tributing factor to this trend, and it suggests that reliable and robust transcription in these scenarios is within our reach in the coming years. 1. Introduction Despite significant leaps made in the last decade, reliable conversational speech recognition remains a significant chal- lenge [1,2]. This is particularly true in meeting scenarios which are often constrained to use only distant array devices [1, 3\u20136]. Besides difficulties due to noise and reverberation in far-field speech, in meeting scenarios a crucial challenge is the presence of overlapped speech, which may amount to more than 15% of total conversation [7]. A third issue are linguistic artifacts aris- ing from informal settings or domain-specific words, an often disregarded characteristic in ASR research, as most commonly used benchmark datasets feature scripted or semi-scripted con- versations [2]. The proposed CHiME-7 DASR challenge1 follows this lat- ter line of research on real, unsegmented meeting scenarios. Our main goal is to foster research in an important direction: how can we build systems that can generalize across a wide range of real-world settings and provide reliable ASR perfor- mance under adverse acoustic conditions? Additionally, we aim to incorporate recent progresses in self-supervised learning to address this problem as well as supervised DNN-based speech separation and enhancement (SSE), by allowing the use of ex- ternal data and pre-trained models. 2. Motivation As these difficulties arise from diverse factors, multi-talker distant conversational speech recognition requires a compre- hensive approach for the development and integration of both front-end and ASR back-end techniques. Over the years, sev- eral challenges and corpora have played a significant role in advancing research in this field. These include, but are not limited to, ASpiRE [8], AMI [9], ICSI [10], the CHiME (1\u2013 4) challenges [11\u201314], the Sheffield Wargames corpus [15], and DIRHA [16]. More recently, we have seen VOiCES [17], DiPCo [18], LibriCSS [3], Alimeeting [19], Ego4D [20], and the recent CHiME 5 and 6 challenges [1, 21]. Several of these datasets/challenges, such as DIRHA, VOiCES, and CHiME-4, focused primarily on acoustic robust- ness, thus featuring pre-segmented single speaker utterances in Compared to the previous CHiME editions, CHiME-7 DASR features three main novelties, motivated by recent promising di- rections in speech processing. 2.1. Diverse Scenarios To expand the breadth of evaluation conditions, we include two additional datasets (along with CHiME-6) \u2014 DiPCo [18], and Mixer 6 Speech [23]. The objective is to encourage research to- wards methods that (1) work well across different array topolo- gies: linear (CHiME-6), circular (DiPCo), or heterogeneous (Mixer 6); (2) are capable of handling variable numbers of speakers in each session; (3) account for linguistic differences 1CHiME-7 DASR website: chimechallenge.org/current/task1/index between dinner party scenarios (CHiME-6 and DiPCo) versus interviews (Mixer 6); and (4) can effectively handle diverse acoustic conditions. Such variability reflects real use-cases where such cross-domain generalization capability is highly de- sirable. 2.2. \u201cFoundation\u201d Models Pre-trained speech models trained on large datasets are in- creasingly becoming available to the public. These include supervised models such as OpenAI Whisper [24], Nvidia JASPER [25] and QuartzNet [26], and self-supervised ones such as Wav2Vec 2.0 [27], HuBERT [28], and WavLM [29]. The latter, in particular, have proved effective in many down- stream applications [30], and their integration with front-end speech enhancement has enabled state-of-the-art performance on benchmarks such as CHiME-4 [31, 32]. Leveraging these models offers unique opportunities as well as novel challenges. By"}, {"question": " What are some of the pre-trained speech models that are becoming available to the public?,answer: Some pre-trained speech models include OpenAI Whisper, Nvidia JASPER, QuartzNet, Wav2Vec 2.0, HuBERT, and WavLM.", "ref_chunk": "3 2 0 2 l u J 4 1 ] S A . s s e e [ 2 v 4 3 7 3 1 . 6 0 3 2 : v i X r a The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios Samuele Cornell1,2, Matthew Wiesner3, Shinji Watanabe2, Desh Raj3, Xuankai Chang2, Paola Garcia3, Matthew Maciejewski3, Yoshiki Masuyama4, Zhong-Qiu Wang2, Stefano Squartini1, Sanjeev Khudanpur3 1Universit`a Politecnica delle Marche, Italy 2Carnegie Mellon University, USA 3Johns Hopkins University, USA 4Tokyo Metropolitan University, Japan Abstract The CHiME challenges have played a significant role in the de- velopment and evaluation of robust automatic speech recogni- tion (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task com- prises joint ASR and diarization in far-field settings with mul- tiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse sce- narios: CHiME-6, DiPCo, and Mixer 6. The goal is for partici- pants to devise a single system that can generalize across differ- ent array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that partic- ipants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, mo- tivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology ag- nostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR). Index Terms: CHiME challenge, speech recognition, multi- channel, speaker diarization, speech separation a noisy/reverberant environment captured by multiple micro- phones. The objective in this case was to foster research in robust ASR methods, ignoring possible issues that can arise from overlapped speech, wrong segmentation, and colloquial language. These datasets comprised either fully simulated or semi-real environments, where utterances come from an audio- book speech corpus such as LibriSpeech [22]. A notable ex- ception in this regard is LibriCSS. It features a simulated, un- segmented full meeting scenario between multiple participants in semi-real conditions; it was obtained by playing LibriSpeech utterances via loudspeakers in a real room. AMI, ICSI, and Sheffield Wargames were historically among the first projects that featured real unsegmented meeting- style scenarios recorded in far-field settings. Although these are more expensive to collect compared with simulated or semi- real datasets, they better reflect possible real-world applications. Recently, this line of research has seen renewed interest, and has resulted in the collection and creation of datasets such as CH- iME-5/6, DiPCo, Alimeeting, and Ego4D. The progress made in ASR, diarization and speech separation is a significant con- tributing factor to this trend, and it suggests that reliable and robust transcription in these scenarios is within our reach in the coming years. 1. Introduction Despite significant leaps made in the last decade, reliable conversational speech recognition remains a significant chal- lenge [1,2]. This is particularly true in meeting scenarios which are often constrained to use only distant array devices [1, 3\u20136]. Besides difficulties due to noise and reverberation in far-field speech, in meeting scenarios a crucial challenge is the presence of overlapped speech, which may amount to more than 15% of total conversation [7]. A third issue are linguistic artifacts aris- ing from informal settings or domain-specific words, an often disregarded characteristic in ASR research, as most commonly used benchmark datasets feature scripted or semi-scripted con- versations [2]. The proposed CHiME-7 DASR challenge1 follows this lat- ter line of research on real, unsegmented meeting scenarios. Our main goal is to foster research in an important direction: how can we build systems that can generalize across a wide range of real-world settings and provide reliable ASR perfor- mance under adverse acoustic conditions? Additionally, we aim to incorporate recent progresses in self-supervised learning to address this problem as well as supervised DNN-based speech separation and enhancement (SSE), by allowing the use of ex- ternal data and pre-trained models. 2. Motivation As these difficulties arise from diverse factors, multi-talker distant conversational speech recognition requires a compre- hensive approach for the development and integration of both front-end and ASR back-end techniques. Over the years, sev- eral challenges and corpora have played a significant role in advancing research in this field. These include, but are not limited to, ASpiRE [8], AMI [9], ICSI [10], the CHiME (1\u2013 4) challenges [11\u201314], the Sheffield Wargames corpus [15], and DIRHA [16]. More recently, we have seen VOiCES [17], DiPCo [18], LibriCSS [3], Alimeeting [19], Ego4D [20], and the recent CHiME 5 and 6 challenges [1, 21]. Several of these datasets/challenges, such as DIRHA, VOiCES, and CHiME-4, focused primarily on acoustic robust- ness, thus featuring pre-segmented single speaker utterances in Compared to the previous CHiME editions, CHiME-7 DASR features three main novelties, motivated by recent promising di- rections in speech processing. 2.1. Diverse Scenarios To expand the breadth of evaluation conditions, we include two additional datasets (along with CHiME-6) \u2014 DiPCo [18], and Mixer 6 Speech [23]. The objective is to encourage research to- wards methods that (1) work well across different array topolo- gies: linear (CHiME-6), circular (DiPCo), or heterogeneous (Mixer 6); (2) are capable of handling variable numbers of speakers in each session; (3) account for linguistic differences 1CHiME-7 DASR website: chimechallenge.org/current/task1/index between dinner party scenarios (CHiME-6 and DiPCo) versus interviews (Mixer 6); and (4) can effectively handle diverse acoustic conditions. Such variability reflects real use-cases where such cross-domain generalization capability is highly de- sirable. 2.2. \u201cFoundation\u201d Models Pre-trained speech models trained on large datasets are in- creasingly becoming available to the public. These include supervised models such as OpenAI Whisper [24], Nvidia JASPER [25] and QuartzNet [26], and self-supervised ones such as Wav2Vec 2.0 [27], HuBERT [28], and WavLM [29]. The latter, in particular, have proved effective in many down- stream applications [30], and their integration with front-end speech enhancement has enabled state-of-the-art performance on benchmarks such as CHiME-4 [31, 32]. Leveraging these models offers unique opportunities as well as novel challenges. By"}], "doc_text": "3 2 0 2 l u J 4 1 ] S A . s s e e [ 2 v 4 3 7 3 1 . 6 0 3 2 : v i X r a The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios Samuele Cornell1,2, Matthew Wiesner3, Shinji Watanabe2, Desh Raj3, Xuankai Chang2, Paola Garcia3, Matthew Maciejewski3, Yoshiki Masuyama4, Zhong-Qiu Wang2, Stefano Squartini1, Sanjeev Khudanpur3 1Universit`a Politecnica delle Marche, Italy 2Carnegie Mellon University, USA 3Johns Hopkins University, USA 4Tokyo Metropolitan University, Japan Abstract The CHiME challenges have played a significant role in the de- velopment and evaluation of robust automatic speech recogni- tion (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task com- prises joint ASR and diarization in far-field settings with mul- tiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse sce- narios: CHiME-6, DiPCo, and Mixer 6. The goal is for partici- pants to devise a single system that can generalize across differ- ent array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that partic- ipants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, mo- tivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology ag- nostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR). Index Terms: CHiME challenge, speech recognition, multi- channel, speaker diarization, speech separation a noisy/reverberant environment captured by multiple micro- phones. The objective in this case was to foster research in robust ASR methods, ignoring possible issues that can arise from overlapped speech, wrong segmentation, and colloquial language. These datasets comprised either fully simulated or semi-real environments, where utterances come from an audio- book speech corpus such as LibriSpeech [22]. A notable ex- ception in this regard is LibriCSS. It features a simulated, un- segmented full meeting scenario between multiple participants in semi-real conditions; it was obtained by playing LibriSpeech utterances via loudspeakers in a real room. AMI, ICSI, and Sheffield Wargames were historically among the first projects that featured real unsegmented meeting- style scenarios recorded in far-field settings. Although these are more expensive to collect compared with simulated or semi- real datasets, they better reflect possible real-world applications. Recently, this line of research has seen renewed interest, and has resulted in the collection and creation of datasets such as CH- iME-5/6, DiPCo, Alimeeting, and Ego4D. The progress made in ASR, diarization and speech separation is a significant con- tributing factor to this trend, and it suggests that reliable and robust transcription in these scenarios is within our reach in the coming years. 1. Introduction Despite significant leaps made in the last decade, reliable conversational speech recognition remains a significant chal- lenge [1,2]. This is particularly true in meeting scenarios which are often constrained to use only distant array devices [1, 3\u20136]. Besides difficulties due to noise and reverberation in far-field speech, in meeting scenarios a crucial challenge is the presence of overlapped speech, which may amount to more than 15% of total conversation [7]. A third issue are linguistic artifacts aris- ing from informal settings or domain-specific words, an often disregarded characteristic in ASR research, as most commonly used benchmark datasets feature scripted or semi-scripted con- versations [2]. The proposed CHiME-7 DASR challenge1 follows this lat- ter line of research on real, unsegmented meeting scenarios. Our main goal is to foster research in an important direction: how can we build systems that can generalize across a wide range of real-world settings and provide reliable ASR perfor- mance under adverse acoustic conditions? Additionally, we aim to incorporate recent progresses in self-supervised learning to address this problem as well as supervised DNN-based speech separation and enhancement (SSE), by allowing the use of ex- ternal data and pre-trained models. 2. Motivation As these difficulties arise from diverse factors, multi-talker distant conversational speech recognition requires a compre- hensive approach for the development and integration of both front-end and ASR back-end techniques. Over the years, sev- eral challenges and corpora have played a significant role in advancing research in this field. These include, but are not limited to, ASpiRE [8], AMI [9], ICSI [10], the CHiME (1\u2013 4) challenges [11\u201314], the Sheffield Wargames corpus [15], and DIRHA [16]. More recently, we have seen VOiCES [17], DiPCo [18], LibriCSS [3], Alimeeting [19], Ego4D [20], and the recent CHiME 5 and 6 challenges [1, 21]. Several of these datasets/challenges, such as DIRHA, VOiCES, and CHiME-4, focused primarily on acoustic robust- ness, thus featuring pre-segmented single speaker utterances in Compared to the previous CHiME editions, CHiME-7 DASR features three main novelties, motivated by recent promising di- rections in speech processing. 2.1. Diverse Scenarios To expand the breadth of evaluation conditions, we include two additional datasets (along with CHiME-6) \u2014 DiPCo [18], and Mixer 6 Speech [23]. The objective is to encourage research to- wards methods that (1) work well across different array topolo- gies: linear (CHiME-6), circular (DiPCo), or heterogeneous (Mixer 6); (2) are capable of handling variable numbers of speakers in each session; (3) account for linguistic differences 1CHiME-7 DASR website: chimechallenge.org/current/task1/index between dinner party scenarios (CHiME-6 and DiPCo) versus interviews (Mixer 6); and (4) can effectively handle diverse acoustic conditions. Such variability reflects real use-cases where such cross-domain generalization capability is highly de- sirable. 2.2. \u201cFoundation\u201d Models Pre-trained speech models trained on large datasets are in- creasingly becoming available to the public. These include supervised models such as OpenAI Whisper [24], Nvidia JASPER [25] and QuartzNet [26], and self-supervised ones such as Wav2Vec 2.0 [27], HuBERT [28], and WavLM [29]. The latter, in particular, have proved effective in many down- stream applications [30], and their integration with front-end speech enhancement has enabled state-of-the-art performance on benchmarks such as CHiME-4 [31, 32]. Leveraging these models offers unique opportunities as well as novel challenges. By"}