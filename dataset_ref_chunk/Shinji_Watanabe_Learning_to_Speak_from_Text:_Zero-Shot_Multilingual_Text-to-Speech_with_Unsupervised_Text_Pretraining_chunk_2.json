{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Learning_to_Speak_from_Text:_Zero-Shot_Multilingual_Text-to-Speech_with_Unsupervised_Text_Pretraining_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of unsupervised multilingual text pretraining as described in the text?", "answer": " The purpose is to use multilingual text data consisting of languages that are not included in the paired data.", "ref_chunk": "even for languages that are not present in the paired data. 2.1 Unsupervised Multilingual Text Pretraining Fig. 2(a) illustrates the unsupervised pretraining method. It uses multilingual text data consisting of languages that are not included in the paired data. Let X = (xn \u2208 V |n = 1, \u00b7 \u00b7 \u00b7 , N ) denote the input text token sequence of length N , where V denotes a vocabulary constructed for pretraining. 1https://github.com/Takaaki-Saeki/zm-text-tts 2https://takaaki-saeki.github.io/zm-tts-text demo We define Dtext as the text dataset. Let Ltext denote the set of language IDs included in Dtext. First, the masked token sequence X m and a language ID ltext \u2208 Ltext are fed to the model. Let the token embedding sequence and language em- n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and el \u2208 Rd, bedding be Z m = (zm respectively. The embedding layers output Z m and el as: Z m = Embed(X m; \u03b8T), el = Embed(ltext; \u03b8L), where \u03b8T and \u03b8L denote the model parameters of the to- ken embedding and language embedding layers, respectively. Then the token and language embeddings obtained in Eq. (1) are added and fed to a bottleneck layer to project them into a hidden input vector. Let Hin = (hin,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and Hout = (hout,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) denote hidden vectors in the encoder input and output, respectively. Then the conditional probability p(X|X\u2212\u03a0) is computed as: Hin = Bottleneck(Z m + el; \u03b8B), Hout = Encoder(Hin; \u03b8E), p(X|X\u2212\u03a0) = Softmax(PredictionNet(Hout; \u03b8P)), where \u03b8B, \u03b8E, \u03b8P denote the model parameters of the bot- tleneck layer, the encoder and a prediction network, respec- tively. In Eq. (4), Softmax(\u00b7) denotes a softmax function. We define the network with the model parameters {\u03b8B, \u03b8T, \u03b8L} as language-aware embedding layer, which jointly embeds the token sequence X and the language ID ltext as in Eq. (1) and (2). Let \u03a0 = (\u03c0k \u2208 N|k = 1, \u00b7 \u00b7 \u00b7 , K) be the indexes of the masked tokens of length K. With the probability com- puted in Eq. (4), the training objective can be defined as: Lmlm = 1 K K (cid:88) k=1 log p(x\u03c0k |X m), {\u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T, \u02c6\u03b8L} = arg min \u03b8E,\u03b8B,\u03b8T,\u03b8L Lmlm. We use UTF-8 bytes or International Phonetic Alphabet (IPA) symbols for the input token sequence X. For each to- ken type, the vocabulary V is constructed from Dtext, which includes a start/end of sentence token ([SOS/EOS]). We ex- tracted International IPA sequences using an open-source toolkit3. To obtain the masked token X m, we use the same masking ratio and category as in the original BERT pre- training [Devlin et al., 2019] for each token type. Randomly, 12 % of the tokens are replaced with the [MASK] token, and 1.5 % of them are replaced with random tokens. Also, 1.5 % of the tokens are left unchanged and Lmlm is computed as in Eq. (5) for those 15 % of tokens that have indices \u03a0. 2.2 Supervised Learning with Paired Data Fig. 2(b) illustrates the supervised learning of the TTS model with paired data. We define the paired data and the set of lan- guage IDs as Dpaired and Lpaired, respectively. Note that we assume Lpaired \u2282 Ltext. Let Y = (yt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T ) denote the speech feature sequence with the length of T . We first initialize the model parameters {\u03b8E, \u03b8B, \u03b8T, \u03b8L} with 3https://github.com/espeak-ng/espeak-ng (1) (2) (3) (4) (5) those obtained in the pretraining described in \u00a7 2.1. Let \u03b8D denote the model parameter of the decoder. The speech fea- tures are predicted with teacher forcing as: Hout = Encoder(Bottleneck(Z + el)), \u02c6Y = Decoder(Hout, Y ; \u03b8D), where Z is the unmasked token embedding sequence. Note that the unmasked token sequence is used in Eq. (6), while the masked token sequence is used in Eq. (2) Let Ltts( \u02c6Y , Y ) denote the training objective of the TTS model. Then we consider two types of schemes. Updating language-aware embedding layer We only freeze the parameter of the language embedding layer \u03b8L while updating the rest of the parameters. Therefore the train- able model parameters can be written as {\u02c6\u03b8D, \u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T} = arg min \u03b8D,\u03b8E,\u03b8B,\u03b8T Ltts( \u02c6Y , Y ). Previous work has confirmed that multilingual BERT has high cross-lingual transferability for various NLP tasks [Wu and Dredze, 2019]. This scheme corresponds to a simple fine- tuning of BERT [Wu and Dredze, 2019], which updates all the parameters during training for the downstream tasks4. Freezing language-aware embedding layer We freeze the bottleneck layer and the token embedding layer along with the language embedding, updating the encoder and decoder. The training process can be written as {\u02c6\u03b8D, \u02c6\u03b8E} = arg min Ltts( \u02c6Y , Y ). \u03b8D,\u03b8E In contrast to the scheme represented in Eq. (8), the scheme in Eq. (9) preserves the parameters of the language-aware em- bedding layer to facilitate cross-lingual transfer. In the evalu- ation, we use the scheme formulated in Eq. (9), except for the ablation study in \u00a7 3.4. 2.3 Let Lsyn denote the set of language IDs used for inference. The text token sequence X and the language ID lsyn \u2208 Lsyn are fed to the model as in Eq. (1), and the encoder output is predicted as in Eq. (6). Unlike Eq. (7), the speech features are predicted as: Inference \u02c6Y = Decoder(Hout; \u03b8D). (10) The output waveform is obtained by feeding the predicted features \u02c6Y to a pretrained neural vocoder. Fig. 2(c) illustrates the inference process. The left and right sides of the figure show the typical multilingual TTS and our zero-shot TTS. Previous work [Li et al., 2019a] has typically assumed seen languages, and the inference is performed with the language IDs Lseen \u2282 Lpaired. However, it is challenging to perform TTS for unseen languages Lunseen \u2229 Lpaired = \u2205. While other"}, {"question": " How is the input text token sequence denoted in the text?", "answer": " The input text token sequence is denoted as X = (xn \u2208 V |n = 1, \u00b7 \u00b7 \u00b7 , N ).", "ref_chunk": "even for languages that are not present in the paired data. 2.1 Unsupervised Multilingual Text Pretraining Fig. 2(a) illustrates the unsupervised pretraining method. It uses multilingual text data consisting of languages that are not included in the paired data. Let X = (xn \u2208 V |n = 1, \u00b7 \u00b7 \u00b7 , N ) denote the input text token sequence of length N , where V denotes a vocabulary constructed for pretraining. 1https://github.com/Takaaki-Saeki/zm-text-tts 2https://takaaki-saeki.github.io/zm-tts-text demo We define Dtext as the text dataset. Let Ltext denote the set of language IDs included in Dtext. First, the masked token sequence X m and a language ID ltext \u2208 Ltext are fed to the model. Let the token embedding sequence and language em- n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and el \u2208 Rd, bedding be Z m = (zm respectively. The embedding layers output Z m and el as: Z m = Embed(X m; \u03b8T), el = Embed(ltext; \u03b8L), where \u03b8T and \u03b8L denote the model parameters of the to- ken embedding and language embedding layers, respectively. Then the token and language embeddings obtained in Eq. (1) are added and fed to a bottleneck layer to project them into a hidden input vector. Let Hin = (hin,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and Hout = (hout,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) denote hidden vectors in the encoder input and output, respectively. Then the conditional probability p(X|X\u2212\u03a0) is computed as: Hin = Bottleneck(Z m + el; \u03b8B), Hout = Encoder(Hin; \u03b8E), p(X|X\u2212\u03a0) = Softmax(PredictionNet(Hout; \u03b8P)), where \u03b8B, \u03b8E, \u03b8P denote the model parameters of the bot- tleneck layer, the encoder and a prediction network, respec- tively. In Eq. (4), Softmax(\u00b7) denotes a softmax function. We define the network with the model parameters {\u03b8B, \u03b8T, \u03b8L} as language-aware embedding layer, which jointly embeds the token sequence X and the language ID ltext as in Eq. (1) and (2). Let \u03a0 = (\u03c0k \u2208 N|k = 1, \u00b7 \u00b7 \u00b7 , K) be the indexes of the masked tokens of length K. With the probability com- puted in Eq. (4), the training objective can be defined as: Lmlm = 1 K K (cid:88) k=1 log p(x\u03c0k |X m), {\u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T, \u02c6\u03b8L} = arg min \u03b8E,\u03b8B,\u03b8T,\u03b8L Lmlm. We use UTF-8 bytes or International Phonetic Alphabet (IPA) symbols for the input token sequence X. For each to- ken type, the vocabulary V is constructed from Dtext, which includes a start/end of sentence token ([SOS/EOS]). We ex- tracted International IPA sequences using an open-source toolkit3. To obtain the masked token X m, we use the same masking ratio and category as in the original BERT pre- training [Devlin et al., 2019] for each token type. Randomly, 12 % of the tokens are replaced with the [MASK] token, and 1.5 % of them are replaced with random tokens. Also, 1.5 % of the tokens are left unchanged and Lmlm is computed as in Eq. (5) for those 15 % of tokens that have indices \u03a0. 2.2 Supervised Learning with Paired Data Fig. 2(b) illustrates the supervised learning of the TTS model with paired data. We define the paired data and the set of lan- guage IDs as Dpaired and Lpaired, respectively. Note that we assume Lpaired \u2282 Ltext. Let Y = (yt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T ) denote the speech feature sequence with the length of T . We first initialize the model parameters {\u03b8E, \u03b8B, \u03b8T, \u03b8L} with 3https://github.com/espeak-ng/espeak-ng (1) (2) (3) (4) (5) those obtained in the pretraining described in \u00a7 2.1. Let \u03b8D denote the model parameter of the decoder. The speech fea- tures are predicted with teacher forcing as: Hout = Encoder(Bottleneck(Z + el)), \u02c6Y = Decoder(Hout, Y ; \u03b8D), where Z is the unmasked token embedding sequence. Note that the unmasked token sequence is used in Eq. (6), while the masked token sequence is used in Eq. (2) Let Ltts( \u02c6Y , Y ) denote the training objective of the TTS model. Then we consider two types of schemes. Updating language-aware embedding layer We only freeze the parameter of the language embedding layer \u03b8L while updating the rest of the parameters. Therefore the train- able model parameters can be written as {\u02c6\u03b8D, \u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T} = arg min \u03b8D,\u03b8E,\u03b8B,\u03b8T Ltts( \u02c6Y , Y ). Previous work has confirmed that multilingual BERT has high cross-lingual transferability for various NLP tasks [Wu and Dredze, 2019]. This scheme corresponds to a simple fine- tuning of BERT [Wu and Dredze, 2019], which updates all the parameters during training for the downstream tasks4. Freezing language-aware embedding layer We freeze the bottleneck layer and the token embedding layer along with the language embedding, updating the encoder and decoder. The training process can be written as {\u02c6\u03b8D, \u02c6\u03b8E} = arg min Ltts( \u02c6Y , Y ). \u03b8D,\u03b8E In contrast to the scheme represented in Eq. (8), the scheme in Eq. (9) preserves the parameters of the language-aware em- bedding layer to facilitate cross-lingual transfer. In the evalu- ation, we use the scheme formulated in Eq. (9), except for the ablation study in \u00a7 3.4. 2.3 Let Lsyn denote the set of language IDs used for inference. The text token sequence X and the language ID lsyn \u2208 Lsyn are fed to the model as in Eq. (1), and the encoder output is predicted as in Eq. (6). Unlike Eq. (7), the speech features are predicted as: Inference \u02c6Y = Decoder(Hout; \u03b8D). (10) The output waveform is obtained by feeding the predicted features \u02c6Y to a pretrained neural vocoder. Fig. 2(c) illustrates the inference process. The left and right sides of the figure show the typical multilingual TTS and our zero-shot TTS. Previous work [Li et al., 2019a] has typically assumed seen languages, and the inference is performed with the language IDs Lseen \u2282 Lpaired. However, it is challenging to perform TTS for unseen languages Lunseen \u2229 Lpaired = \u2205. While other"}, {"question": " What do the embedding layers output in the unsupervised pretraining method?", "answer": " The embedding layers output Z m = (zm) and el.", "ref_chunk": "even for languages that are not present in the paired data. 2.1 Unsupervised Multilingual Text Pretraining Fig. 2(a) illustrates the unsupervised pretraining method. It uses multilingual text data consisting of languages that are not included in the paired data. Let X = (xn \u2208 V |n = 1, \u00b7 \u00b7 \u00b7 , N ) denote the input text token sequence of length N , where V denotes a vocabulary constructed for pretraining. 1https://github.com/Takaaki-Saeki/zm-text-tts 2https://takaaki-saeki.github.io/zm-tts-text demo We define Dtext as the text dataset. Let Ltext denote the set of language IDs included in Dtext. First, the masked token sequence X m and a language ID ltext \u2208 Ltext are fed to the model. Let the token embedding sequence and language em- n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and el \u2208 Rd, bedding be Z m = (zm respectively. The embedding layers output Z m and el as: Z m = Embed(X m; \u03b8T), el = Embed(ltext; \u03b8L), where \u03b8T and \u03b8L denote the model parameters of the to- ken embedding and language embedding layers, respectively. Then the token and language embeddings obtained in Eq. (1) are added and fed to a bottleneck layer to project them into a hidden input vector. Let Hin = (hin,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and Hout = (hout,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) denote hidden vectors in the encoder input and output, respectively. Then the conditional probability p(X|X\u2212\u03a0) is computed as: Hin = Bottleneck(Z m + el; \u03b8B), Hout = Encoder(Hin; \u03b8E), p(X|X\u2212\u03a0) = Softmax(PredictionNet(Hout; \u03b8P)), where \u03b8B, \u03b8E, \u03b8P denote the model parameters of the bot- tleneck layer, the encoder and a prediction network, respec- tively. In Eq. (4), Softmax(\u00b7) denotes a softmax function. We define the network with the model parameters {\u03b8B, \u03b8T, \u03b8L} as language-aware embedding layer, which jointly embeds the token sequence X and the language ID ltext as in Eq. (1) and (2). Let \u03a0 = (\u03c0k \u2208 N|k = 1, \u00b7 \u00b7 \u00b7 , K) be the indexes of the masked tokens of length K. With the probability com- puted in Eq. (4), the training objective can be defined as: Lmlm = 1 K K (cid:88) k=1 log p(x\u03c0k |X m), {\u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T, \u02c6\u03b8L} = arg min \u03b8E,\u03b8B,\u03b8T,\u03b8L Lmlm. We use UTF-8 bytes or International Phonetic Alphabet (IPA) symbols for the input token sequence X. For each to- ken type, the vocabulary V is constructed from Dtext, which includes a start/end of sentence token ([SOS/EOS]). We ex- tracted International IPA sequences using an open-source toolkit3. To obtain the masked token X m, we use the same masking ratio and category as in the original BERT pre- training [Devlin et al., 2019] for each token type. Randomly, 12 % of the tokens are replaced with the [MASK] token, and 1.5 % of them are replaced with random tokens. Also, 1.5 % of the tokens are left unchanged and Lmlm is computed as in Eq. (5) for those 15 % of tokens that have indices \u03a0. 2.2 Supervised Learning with Paired Data Fig. 2(b) illustrates the supervised learning of the TTS model with paired data. We define the paired data and the set of lan- guage IDs as Dpaired and Lpaired, respectively. Note that we assume Lpaired \u2282 Ltext. Let Y = (yt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T ) denote the speech feature sequence with the length of T . We first initialize the model parameters {\u03b8E, \u03b8B, \u03b8T, \u03b8L} with 3https://github.com/espeak-ng/espeak-ng (1) (2) (3) (4) (5) those obtained in the pretraining described in \u00a7 2.1. Let \u03b8D denote the model parameter of the decoder. The speech fea- tures are predicted with teacher forcing as: Hout = Encoder(Bottleneck(Z + el)), \u02c6Y = Decoder(Hout, Y ; \u03b8D), where Z is the unmasked token embedding sequence. Note that the unmasked token sequence is used in Eq. (6), while the masked token sequence is used in Eq. (2) Let Ltts( \u02c6Y , Y ) denote the training objective of the TTS model. Then we consider two types of schemes. Updating language-aware embedding layer We only freeze the parameter of the language embedding layer \u03b8L while updating the rest of the parameters. Therefore the train- able model parameters can be written as {\u02c6\u03b8D, \u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T} = arg min \u03b8D,\u03b8E,\u03b8B,\u03b8T Ltts( \u02c6Y , Y ). Previous work has confirmed that multilingual BERT has high cross-lingual transferability for various NLP tasks [Wu and Dredze, 2019]. This scheme corresponds to a simple fine- tuning of BERT [Wu and Dredze, 2019], which updates all the parameters during training for the downstream tasks4. Freezing language-aware embedding layer We freeze the bottleneck layer and the token embedding layer along with the language embedding, updating the encoder and decoder. The training process can be written as {\u02c6\u03b8D, \u02c6\u03b8E} = arg min Ltts( \u02c6Y , Y ). \u03b8D,\u03b8E In contrast to the scheme represented in Eq. (8), the scheme in Eq. (9) preserves the parameters of the language-aware em- bedding layer to facilitate cross-lingual transfer. In the evalu- ation, we use the scheme formulated in Eq. (9), except for the ablation study in \u00a7 3.4. 2.3 Let Lsyn denote the set of language IDs used for inference. The text token sequence X and the language ID lsyn \u2208 Lsyn are fed to the model as in Eq. (1), and the encoder output is predicted as in Eq. (6). Unlike Eq. (7), the speech features are predicted as: Inference \u02c6Y = Decoder(Hout; \u03b8D). (10) The output waveform is obtained by feeding the predicted features \u02c6Y to a pretrained neural vocoder. Fig. 2(c) illustrates the inference process. The left and right sides of the figure show the typical multilingual TTS and our zero-shot TTS. Previous work [Li et al., 2019a] has typically assumed seen languages, and the inference is performed with the language IDs Lseen \u2282 Lpaired. However, it is challenging to perform TTS for unseen languages Lunseen \u2229 Lpaired = \u2205. While other"}, {"question": " What is the purpose of the bottleneck layer in the encoder input?", "answer": " The bottleneck layer is used to project the token and language embeddings into a hidden input vector.", "ref_chunk": "even for languages that are not present in the paired data. 2.1 Unsupervised Multilingual Text Pretraining Fig. 2(a) illustrates the unsupervised pretraining method. It uses multilingual text data consisting of languages that are not included in the paired data. Let X = (xn \u2208 V |n = 1, \u00b7 \u00b7 \u00b7 , N ) denote the input text token sequence of length N , where V denotes a vocabulary constructed for pretraining. 1https://github.com/Takaaki-Saeki/zm-text-tts 2https://takaaki-saeki.github.io/zm-tts-text demo We define Dtext as the text dataset. Let Ltext denote the set of language IDs included in Dtext. First, the masked token sequence X m and a language ID ltext \u2208 Ltext are fed to the model. Let the token embedding sequence and language em- n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and el \u2208 Rd, bedding be Z m = (zm respectively. The embedding layers output Z m and el as: Z m = Embed(X m; \u03b8T), el = Embed(ltext; \u03b8L), where \u03b8T and \u03b8L denote the model parameters of the to- ken embedding and language embedding layers, respectively. Then the token and language embeddings obtained in Eq. (1) are added and fed to a bottleneck layer to project them into a hidden input vector. Let Hin = (hin,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and Hout = (hout,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) denote hidden vectors in the encoder input and output, respectively. Then the conditional probability p(X|X\u2212\u03a0) is computed as: Hin = Bottleneck(Z m + el; \u03b8B), Hout = Encoder(Hin; \u03b8E), p(X|X\u2212\u03a0) = Softmax(PredictionNet(Hout; \u03b8P)), where \u03b8B, \u03b8E, \u03b8P denote the model parameters of the bot- tleneck layer, the encoder and a prediction network, respec- tively. In Eq. (4), Softmax(\u00b7) denotes a softmax function. We define the network with the model parameters {\u03b8B, \u03b8T, \u03b8L} as language-aware embedding layer, which jointly embeds the token sequence X and the language ID ltext as in Eq. (1) and (2). Let \u03a0 = (\u03c0k \u2208 N|k = 1, \u00b7 \u00b7 \u00b7 , K) be the indexes of the masked tokens of length K. With the probability com- puted in Eq. (4), the training objective can be defined as: Lmlm = 1 K K (cid:88) k=1 log p(x\u03c0k |X m), {\u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T, \u02c6\u03b8L} = arg min \u03b8E,\u03b8B,\u03b8T,\u03b8L Lmlm. We use UTF-8 bytes or International Phonetic Alphabet (IPA) symbols for the input token sequence X. For each to- ken type, the vocabulary V is constructed from Dtext, which includes a start/end of sentence token ([SOS/EOS]). We ex- tracted International IPA sequences using an open-source toolkit3. To obtain the masked token X m, we use the same masking ratio and category as in the original BERT pre- training [Devlin et al., 2019] for each token type. Randomly, 12 % of the tokens are replaced with the [MASK] token, and 1.5 % of them are replaced with random tokens. Also, 1.5 % of the tokens are left unchanged and Lmlm is computed as in Eq. (5) for those 15 % of tokens that have indices \u03a0. 2.2 Supervised Learning with Paired Data Fig. 2(b) illustrates the supervised learning of the TTS model with paired data. We define the paired data and the set of lan- guage IDs as Dpaired and Lpaired, respectively. Note that we assume Lpaired \u2282 Ltext. Let Y = (yt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T ) denote the speech feature sequence with the length of T . We first initialize the model parameters {\u03b8E, \u03b8B, \u03b8T, \u03b8L} with 3https://github.com/espeak-ng/espeak-ng (1) (2) (3) (4) (5) those obtained in the pretraining described in \u00a7 2.1. Let \u03b8D denote the model parameter of the decoder. The speech fea- tures are predicted with teacher forcing as: Hout = Encoder(Bottleneck(Z + el)), \u02c6Y = Decoder(Hout, Y ; \u03b8D), where Z is the unmasked token embedding sequence. Note that the unmasked token sequence is used in Eq. (6), while the masked token sequence is used in Eq. (2) Let Ltts( \u02c6Y , Y ) denote the training objective of the TTS model. Then we consider two types of schemes. Updating language-aware embedding layer We only freeze the parameter of the language embedding layer \u03b8L while updating the rest of the parameters. Therefore the train- able model parameters can be written as {\u02c6\u03b8D, \u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T} = arg min \u03b8D,\u03b8E,\u03b8B,\u03b8T Ltts( \u02c6Y , Y ). Previous work has confirmed that multilingual BERT has high cross-lingual transferability for various NLP tasks [Wu and Dredze, 2019]. This scheme corresponds to a simple fine- tuning of BERT [Wu and Dredze, 2019], which updates all the parameters during training for the downstream tasks4. Freezing language-aware embedding layer We freeze the bottleneck layer and the token embedding layer along with the language embedding, updating the encoder and decoder. The training process can be written as {\u02c6\u03b8D, \u02c6\u03b8E} = arg min Ltts( \u02c6Y , Y ). \u03b8D,\u03b8E In contrast to the scheme represented in Eq. (8), the scheme in Eq. (9) preserves the parameters of the language-aware em- bedding layer to facilitate cross-lingual transfer. In the evalu- ation, we use the scheme formulated in Eq. (9), except for the ablation study in \u00a7 3.4. 2.3 Let Lsyn denote the set of language IDs used for inference. The text token sequence X and the language ID lsyn \u2208 Lsyn are fed to the model as in Eq. (1), and the encoder output is predicted as in Eq. (6). Unlike Eq. (7), the speech features are predicted as: Inference \u02c6Y = Decoder(Hout; \u03b8D). (10) The output waveform is obtained by feeding the predicted features \u02c6Y to a pretrained neural vocoder. Fig. 2(c) illustrates the inference process. The left and right sides of the figure show the typical multilingual TTS and our zero-shot TTS. Previous work [Li et al., 2019a] has typically assumed seen languages, and the inference is performed with the language IDs Lseen \u2282 Lpaired. However, it is challenging to perform TTS for unseen languages Lunseen \u2229 Lpaired = \u2205. While other"}, {"question": " How is the training objective defined in the unsupervised pretraining method?", "answer": " The training objective is defined as Lmlm = 1/K \u2211_{k=1}^{K} log p(x\u03c0k |X m).", "ref_chunk": "even for languages that are not present in the paired data. 2.1 Unsupervised Multilingual Text Pretraining Fig. 2(a) illustrates the unsupervised pretraining method. It uses multilingual text data consisting of languages that are not included in the paired data. Let X = (xn \u2208 V |n = 1, \u00b7 \u00b7 \u00b7 , N ) denote the input text token sequence of length N , where V denotes a vocabulary constructed for pretraining. 1https://github.com/Takaaki-Saeki/zm-text-tts 2https://takaaki-saeki.github.io/zm-tts-text demo We define Dtext as the text dataset. Let Ltext denote the set of language IDs included in Dtext. First, the masked token sequence X m and a language ID ltext \u2208 Ltext are fed to the model. Let the token embedding sequence and language em- n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and el \u2208 Rd, bedding be Z m = (zm respectively. The embedding layers output Z m and el as: Z m = Embed(X m; \u03b8T), el = Embed(ltext; \u03b8L), where \u03b8T and \u03b8L denote the model parameters of the to- ken embedding and language embedding layers, respectively. Then the token and language embeddings obtained in Eq. (1) are added and fed to a bottleneck layer to project them into a hidden input vector. Let Hin = (hin,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and Hout = (hout,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) denote hidden vectors in the encoder input and output, respectively. Then the conditional probability p(X|X\u2212\u03a0) is computed as: Hin = Bottleneck(Z m + el; \u03b8B), Hout = Encoder(Hin; \u03b8E), p(X|X\u2212\u03a0) = Softmax(PredictionNet(Hout; \u03b8P)), where \u03b8B, \u03b8E, \u03b8P denote the model parameters of the bot- tleneck layer, the encoder and a prediction network, respec- tively. In Eq. (4), Softmax(\u00b7) denotes a softmax function. We define the network with the model parameters {\u03b8B, \u03b8T, \u03b8L} as language-aware embedding layer, which jointly embeds the token sequence X and the language ID ltext as in Eq. (1) and (2). Let \u03a0 = (\u03c0k \u2208 N|k = 1, \u00b7 \u00b7 \u00b7 , K) be the indexes of the masked tokens of length K. With the probability com- puted in Eq. (4), the training objective can be defined as: Lmlm = 1 K K (cid:88) k=1 log p(x\u03c0k |X m), {\u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T, \u02c6\u03b8L} = arg min \u03b8E,\u03b8B,\u03b8T,\u03b8L Lmlm. We use UTF-8 bytes or International Phonetic Alphabet (IPA) symbols for the input token sequence X. For each to- ken type, the vocabulary V is constructed from Dtext, which includes a start/end of sentence token ([SOS/EOS]). We ex- tracted International IPA sequences using an open-source toolkit3. To obtain the masked token X m, we use the same masking ratio and category as in the original BERT pre- training [Devlin et al., 2019] for each token type. Randomly, 12 % of the tokens are replaced with the [MASK] token, and 1.5 % of them are replaced with random tokens. Also, 1.5 % of the tokens are left unchanged and Lmlm is computed as in Eq. (5) for those 15 % of tokens that have indices \u03a0. 2.2 Supervised Learning with Paired Data Fig. 2(b) illustrates the supervised learning of the TTS model with paired data. We define the paired data and the set of lan- guage IDs as Dpaired and Lpaired, respectively. Note that we assume Lpaired \u2282 Ltext. Let Y = (yt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T ) denote the speech feature sequence with the length of T . We first initialize the model parameters {\u03b8E, \u03b8B, \u03b8T, \u03b8L} with 3https://github.com/espeak-ng/espeak-ng (1) (2) (3) (4) (5) those obtained in the pretraining described in \u00a7 2.1. Let \u03b8D denote the model parameter of the decoder. The speech fea- tures are predicted with teacher forcing as: Hout = Encoder(Bottleneck(Z + el)), \u02c6Y = Decoder(Hout, Y ; \u03b8D), where Z is the unmasked token embedding sequence. Note that the unmasked token sequence is used in Eq. (6), while the masked token sequence is used in Eq. (2) Let Ltts( \u02c6Y , Y ) denote the training objective of the TTS model. Then we consider two types of schemes. Updating language-aware embedding layer We only freeze the parameter of the language embedding layer \u03b8L while updating the rest of the parameters. Therefore the train- able model parameters can be written as {\u02c6\u03b8D, \u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T} = arg min \u03b8D,\u03b8E,\u03b8B,\u03b8T Ltts( \u02c6Y , Y ). Previous work has confirmed that multilingual BERT has high cross-lingual transferability for various NLP tasks [Wu and Dredze, 2019]. This scheme corresponds to a simple fine- tuning of BERT [Wu and Dredze, 2019], which updates all the parameters during training for the downstream tasks4. Freezing language-aware embedding layer We freeze the bottleneck layer and the token embedding layer along with the language embedding, updating the encoder and decoder. The training process can be written as {\u02c6\u03b8D, \u02c6\u03b8E} = arg min Ltts( \u02c6Y , Y ). \u03b8D,\u03b8E In contrast to the scheme represented in Eq. (8), the scheme in Eq. (9) preserves the parameters of the language-aware em- bedding layer to facilitate cross-lingual transfer. In the evalu- ation, we use the scheme formulated in Eq. (9), except for the ablation study in \u00a7 3.4. 2.3 Let Lsyn denote the set of language IDs used for inference. The text token sequence X and the language ID lsyn \u2208 Lsyn are fed to the model as in Eq. (1), and the encoder output is predicted as in Eq. (6). Unlike Eq. (7), the speech features are predicted as: Inference \u02c6Y = Decoder(Hout; \u03b8D). (10) The output waveform is obtained by feeding the predicted features \u02c6Y to a pretrained neural vocoder. Fig. 2(c) illustrates the inference process. The left and right sides of the figure show the typical multilingual TTS and our zero-shot TTS. Previous work [Li et al., 2019a] has typically assumed seen languages, and the inference is performed with the language IDs Lseen \u2282 Lpaired. However, it is challenging to perform TTS for unseen languages Lunseen \u2229 Lpaired = \u2205. While other"}, {"question": " What is the main focus in supervised learning with paired data as illustrated in the text?", "answer": " The main focus is on the supervised learning of the TTS model with paired data.", "ref_chunk": "even for languages that are not present in the paired data. 2.1 Unsupervised Multilingual Text Pretraining Fig. 2(a) illustrates the unsupervised pretraining method. It uses multilingual text data consisting of languages that are not included in the paired data. Let X = (xn \u2208 V |n = 1, \u00b7 \u00b7 \u00b7 , N ) denote the input text token sequence of length N , where V denotes a vocabulary constructed for pretraining. 1https://github.com/Takaaki-Saeki/zm-text-tts 2https://takaaki-saeki.github.io/zm-tts-text demo We define Dtext as the text dataset. Let Ltext denote the set of language IDs included in Dtext. First, the masked token sequence X m and a language ID ltext \u2208 Ltext are fed to the model. Let the token embedding sequence and language em- n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and el \u2208 Rd, bedding be Z m = (zm respectively. The embedding layers output Z m and el as: Z m = Embed(X m; \u03b8T), el = Embed(ltext; \u03b8L), where \u03b8T and \u03b8L denote the model parameters of the to- ken embedding and language embedding layers, respectively. Then the token and language embeddings obtained in Eq. (1) are added and fed to a bottleneck layer to project them into a hidden input vector. Let Hin = (hin,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and Hout = (hout,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) denote hidden vectors in the encoder input and output, respectively. Then the conditional probability p(X|X\u2212\u03a0) is computed as: Hin = Bottleneck(Z m + el; \u03b8B), Hout = Encoder(Hin; \u03b8E), p(X|X\u2212\u03a0) = Softmax(PredictionNet(Hout; \u03b8P)), where \u03b8B, \u03b8E, \u03b8P denote the model parameters of the bot- tleneck layer, the encoder and a prediction network, respec- tively. In Eq. (4), Softmax(\u00b7) denotes a softmax function. We define the network with the model parameters {\u03b8B, \u03b8T, \u03b8L} as language-aware embedding layer, which jointly embeds the token sequence X and the language ID ltext as in Eq. (1) and (2). Let \u03a0 = (\u03c0k \u2208 N|k = 1, \u00b7 \u00b7 \u00b7 , K) be the indexes of the masked tokens of length K. With the probability com- puted in Eq. (4), the training objective can be defined as: Lmlm = 1 K K (cid:88) k=1 log p(x\u03c0k |X m), {\u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T, \u02c6\u03b8L} = arg min \u03b8E,\u03b8B,\u03b8T,\u03b8L Lmlm. We use UTF-8 bytes or International Phonetic Alphabet (IPA) symbols for the input token sequence X. For each to- ken type, the vocabulary V is constructed from Dtext, which includes a start/end of sentence token ([SOS/EOS]). We ex- tracted International IPA sequences using an open-source toolkit3. To obtain the masked token X m, we use the same masking ratio and category as in the original BERT pre- training [Devlin et al., 2019] for each token type. Randomly, 12 % of the tokens are replaced with the [MASK] token, and 1.5 % of them are replaced with random tokens. Also, 1.5 % of the tokens are left unchanged and Lmlm is computed as in Eq. (5) for those 15 % of tokens that have indices \u03a0. 2.2 Supervised Learning with Paired Data Fig. 2(b) illustrates the supervised learning of the TTS model with paired data. We define the paired data and the set of lan- guage IDs as Dpaired and Lpaired, respectively. Note that we assume Lpaired \u2282 Ltext. Let Y = (yt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T ) denote the speech feature sequence with the length of T . We first initialize the model parameters {\u03b8E, \u03b8B, \u03b8T, \u03b8L} with 3https://github.com/espeak-ng/espeak-ng (1) (2) (3) (4) (5) those obtained in the pretraining described in \u00a7 2.1. Let \u03b8D denote the model parameter of the decoder. The speech fea- tures are predicted with teacher forcing as: Hout = Encoder(Bottleneck(Z + el)), \u02c6Y = Decoder(Hout, Y ; \u03b8D), where Z is the unmasked token embedding sequence. Note that the unmasked token sequence is used in Eq. (6), while the masked token sequence is used in Eq. (2) Let Ltts( \u02c6Y , Y ) denote the training objective of the TTS model. Then we consider two types of schemes. Updating language-aware embedding layer We only freeze the parameter of the language embedding layer \u03b8L while updating the rest of the parameters. Therefore the train- able model parameters can be written as {\u02c6\u03b8D, \u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T} = arg min \u03b8D,\u03b8E,\u03b8B,\u03b8T Ltts( \u02c6Y , Y ). Previous work has confirmed that multilingual BERT has high cross-lingual transferability for various NLP tasks [Wu and Dredze, 2019]. This scheme corresponds to a simple fine- tuning of BERT [Wu and Dredze, 2019], which updates all the parameters during training for the downstream tasks4. Freezing language-aware embedding layer We freeze the bottleneck layer and the token embedding layer along with the language embedding, updating the encoder and decoder. The training process can be written as {\u02c6\u03b8D, \u02c6\u03b8E} = arg min Ltts( \u02c6Y , Y ). \u03b8D,\u03b8E In contrast to the scheme represented in Eq. (8), the scheme in Eq. (9) preserves the parameters of the language-aware em- bedding layer to facilitate cross-lingual transfer. In the evalu- ation, we use the scheme formulated in Eq. (9), except for the ablation study in \u00a7 3.4. 2.3 Let Lsyn denote the set of language IDs used for inference. The text token sequence X and the language ID lsyn \u2208 Lsyn are fed to the model as in Eq. (1), and the encoder output is predicted as in Eq. (6). Unlike Eq. (7), the speech features are predicted as: Inference \u02c6Y = Decoder(Hout; \u03b8D). (10) The output waveform is obtained by feeding the predicted features \u02c6Y to a pretrained neural vocoder. Fig. 2(c) illustrates the inference process. The left and right sides of the figure show the typical multilingual TTS and our zero-shot TTS. Previous work [Li et al., 2019a] has typically assumed seen languages, and the inference is performed with the language IDs Lseen \u2282 Lpaired. However, it is challenging to perform TTS for unseen languages Lunseen \u2229 Lpaired = \u2205. While other"}, {"question": " How are the speech features predicted in the supervised learning process?", "answer": " The speech features are predicted with teacher forcing using the encoder and decoder.", "ref_chunk": "even for languages that are not present in the paired data. 2.1 Unsupervised Multilingual Text Pretraining Fig. 2(a) illustrates the unsupervised pretraining method. It uses multilingual text data consisting of languages that are not included in the paired data. Let X = (xn \u2208 V |n = 1, \u00b7 \u00b7 \u00b7 , N ) denote the input text token sequence of length N , where V denotes a vocabulary constructed for pretraining. 1https://github.com/Takaaki-Saeki/zm-text-tts 2https://takaaki-saeki.github.io/zm-tts-text demo We define Dtext as the text dataset. Let Ltext denote the set of language IDs included in Dtext. First, the masked token sequence X m and a language ID ltext \u2208 Ltext are fed to the model. Let the token embedding sequence and language em- n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and el \u2208 Rd, bedding be Z m = (zm respectively. The embedding layers output Z m and el as: Z m = Embed(X m; \u03b8T), el = Embed(ltext; \u03b8L), where \u03b8T and \u03b8L denote the model parameters of the to- ken embedding and language embedding layers, respectively. Then the token and language embeddings obtained in Eq. (1) are added and fed to a bottleneck layer to project them into a hidden input vector. Let Hin = (hin,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and Hout = (hout,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) denote hidden vectors in the encoder input and output, respectively. Then the conditional probability p(X|X\u2212\u03a0) is computed as: Hin = Bottleneck(Z m + el; \u03b8B), Hout = Encoder(Hin; \u03b8E), p(X|X\u2212\u03a0) = Softmax(PredictionNet(Hout; \u03b8P)), where \u03b8B, \u03b8E, \u03b8P denote the model parameters of the bot- tleneck layer, the encoder and a prediction network, respec- tively. In Eq. (4), Softmax(\u00b7) denotes a softmax function. We define the network with the model parameters {\u03b8B, \u03b8T, \u03b8L} as language-aware embedding layer, which jointly embeds the token sequence X and the language ID ltext as in Eq. (1) and (2). Let \u03a0 = (\u03c0k \u2208 N|k = 1, \u00b7 \u00b7 \u00b7 , K) be the indexes of the masked tokens of length K. With the probability com- puted in Eq. (4), the training objective can be defined as: Lmlm = 1 K K (cid:88) k=1 log p(x\u03c0k |X m), {\u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T, \u02c6\u03b8L} = arg min \u03b8E,\u03b8B,\u03b8T,\u03b8L Lmlm. We use UTF-8 bytes or International Phonetic Alphabet (IPA) symbols for the input token sequence X. For each to- ken type, the vocabulary V is constructed from Dtext, which includes a start/end of sentence token ([SOS/EOS]). We ex- tracted International IPA sequences using an open-source toolkit3. To obtain the masked token X m, we use the same masking ratio and category as in the original BERT pre- training [Devlin et al., 2019] for each token type. Randomly, 12 % of the tokens are replaced with the [MASK] token, and 1.5 % of them are replaced with random tokens. Also, 1.5 % of the tokens are left unchanged and Lmlm is computed as in Eq. (5) for those 15 % of tokens that have indices \u03a0. 2.2 Supervised Learning with Paired Data Fig. 2(b) illustrates the supervised learning of the TTS model with paired data. We define the paired data and the set of lan- guage IDs as Dpaired and Lpaired, respectively. Note that we assume Lpaired \u2282 Ltext. Let Y = (yt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T ) denote the speech feature sequence with the length of T . We first initialize the model parameters {\u03b8E, \u03b8B, \u03b8T, \u03b8L} with 3https://github.com/espeak-ng/espeak-ng (1) (2) (3) (4) (5) those obtained in the pretraining described in \u00a7 2.1. Let \u03b8D denote the model parameter of the decoder. The speech fea- tures are predicted with teacher forcing as: Hout = Encoder(Bottleneck(Z + el)), \u02c6Y = Decoder(Hout, Y ; \u03b8D), where Z is the unmasked token embedding sequence. Note that the unmasked token sequence is used in Eq. (6), while the masked token sequence is used in Eq. (2) Let Ltts( \u02c6Y , Y ) denote the training objective of the TTS model. Then we consider two types of schemes. Updating language-aware embedding layer We only freeze the parameter of the language embedding layer \u03b8L while updating the rest of the parameters. Therefore the train- able model parameters can be written as {\u02c6\u03b8D, \u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T} = arg min \u03b8D,\u03b8E,\u03b8B,\u03b8T Ltts( \u02c6Y , Y ). Previous work has confirmed that multilingual BERT has high cross-lingual transferability for various NLP tasks [Wu and Dredze, 2019]. This scheme corresponds to a simple fine- tuning of BERT [Wu and Dredze, 2019], which updates all the parameters during training for the downstream tasks4. Freezing language-aware embedding layer We freeze the bottleneck layer and the token embedding layer along with the language embedding, updating the encoder and decoder. The training process can be written as {\u02c6\u03b8D, \u02c6\u03b8E} = arg min Ltts( \u02c6Y , Y ). \u03b8D,\u03b8E In contrast to the scheme represented in Eq. (8), the scheme in Eq. (9) preserves the parameters of the language-aware em- bedding layer to facilitate cross-lingual transfer. In the evalu- ation, we use the scheme formulated in Eq. (9), except for the ablation study in \u00a7 3.4. 2.3 Let Lsyn denote the set of language IDs used for inference. The text token sequence X and the language ID lsyn \u2208 Lsyn are fed to the model as in Eq. (1), and the encoder output is predicted as in Eq. (6). Unlike Eq. (7), the speech features are predicted as: Inference \u02c6Y = Decoder(Hout; \u03b8D). (10) The output waveform is obtained by feeding the predicted features \u02c6Y to a pretrained neural vocoder. Fig. 2(c) illustrates the inference process. The left and right sides of the figure show the typical multilingual TTS and our zero-shot TTS. Previous work [Li et al., 2019a] has typically assumed seen languages, and the inference is performed with the language IDs Lseen \u2282 Lpaired. However, it is challenging to perform TTS for unseen languages Lunseen \u2229 Lpaired = \u2205. While other"}, {"question": " What does the scheme of freezing the language-aware embedding layer aim to achieve?", "answer": " The scheme aims to facilitate cross-lingual transfer by preserving the parameters of the language-aware embedding layer.", "ref_chunk": "even for languages that are not present in the paired data. 2.1 Unsupervised Multilingual Text Pretraining Fig. 2(a) illustrates the unsupervised pretraining method. It uses multilingual text data consisting of languages that are not included in the paired data. Let X = (xn \u2208 V |n = 1, \u00b7 \u00b7 \u00b7 , N ) denote the input text token sequence of length N , where V denotes a vocabulary constructed for pretraining. 1https://github.com/Takaaki-Saeki/zm-text-tts 2https://takaaki-saeki.github.io/zm-tts-text demo We define Dtext as the text dataset. Let Ltext denote the set of language IDs included in Dtext. First, the masked token sequence X m and a language ID ltext \u2208 Ltext are fed to the model. Let the token embedding sequence and language em- n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and el \u2208 Rd, bedding be Z m = (zm respectively. The embedding layers output Z m and el as: Z m = Embed(X m; \u03b8T), el = Embed(ltext; \u03b8L), where \u03b8T and \u03b8L denote the model parameters of the to- ken embedding and language embedding layers, respectively. Then the token and language embeddings obtained in Eq. (1) are added and fed to a bottleneck layer to project them into a hidden input vector. Let Hin = (hin,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and Hout = (hout,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) denote hidden vectors in the encoder input and output, respectively. Then the conditional probability p(X|X\u2212\u03a0) is computed as: Hin = Bottleneck(Z m + el; \u03b8B), Hout = Encoder(Hin; \u03b8E), p(X|X\u2212\u03a0) = Softmax(PredictionNet(Hout; \u03b8P)), where \u03b8B, \u03b8E, \u03b8P denote the model parameters of the bot- tleneck layer, the encoder and a prediction network, respec- tively. In Eq. (4), Softmax(\u00b7) denotes a softmax function. We define the network with the model parameters {\u03b8B, \u03b8T, \u03b8L} as language-aware embedding layer, which jointly embeds the token sequence X and the language ID ltext as in Eq. (1) and (2). Let \u03a0 = (\u03c0k \u2208 N|k = 1, \u00b7 \u00b7 \u00b7 , K) be the indexes of the masked tokens of length K. With the probability com- puted in Eq. (4), the training objective can be defined as: Lmlm = 1 K K (cid:88) k=1 log p(x\u03c0k |X m), {\u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T, \u02c6\u03b8L} = arg min \u03b8E,\u03b8B,\u03b8T,\u03b8L Lmlm. We use UTF-8 bytes or International Phonetic Alphabet (IPA) symbols for the input token sequence X. For each to- ken type, the vocabulary V is constructed from Dtext, which includes a start/end of sentence token ([SOS/EOS]). We ex- tracted International IPA sequences using an open-source toolkit3. To obtain the masked token X m, we use the same masking ratio and category as in the original BERT pre- training [Devlin et al., 2019] for each token type. Randomly, 12 % of the tokens are replaced with the [MASK] token, and 1.5 % of them are replaced with random tokens. Also, 1.5 % of the tokens are left unchanged and Lmlm is computed as in Eq. (5) for those 15 % of tokens that have indices \u03a0. 2.2 Supervised Learning with Paired Data Fig. 2(b) illustrates the supervised learning of the TTS model with paired data. We define the paired data and the set of lan- guage IDs as Dpaired and Lpaired, respectively. Note that we assume Lpaired \u2282 Ltext. Let Y = (yt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T ) denote the speech feature sequence with the length of T . We first initialize the model parameters {\u03b8E, \u03b8B, \u03b8T, \u03b8L} with 3https://github.com/espeak-ng/espeak-ng (1) (2) (3) (4) (5) those obtained in the pretraining described in \u00a7 2.1. Let \u03b8D denote the model parameter of the decoder. The speech fea- tures are predicted with teacher forcing as: Hout = Encoder(Bottleneck(Z + el)), \u02c6Y = Decoder(Hout, Y ; \u03b8D), where Z is the unmasked token embedding sequence. Note that the unmasked token sequence is used in Eq. (6), while the masked token sequence is used in Eq. (2) Let Ltts( \u02c6Y , Y ) denote the training objective of the TTS model. Then we consider two types of schemes. Updating language-aware embedding layer We only freeze the parameter of the language embedding layer \u03b8L while updating the rest of the parameters. Therefore the train- able model parameters can be written as {\u02c6\u03b8D, \u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T} = arg min \u03b8D,\u03b8E,\u03b8B,\u03b8T Ltts( \u02c6Y , Y ). Previous work has confirmed that multilingual BERT has high cross-lingual transferability for various NLP tasks [Wu and Dredze, 2019]. This scheme corresponds to a simple fine- tuning of BERT [Wu and Dredze, 2019], which updates all the parameters during training for the downstream tasks4. Freezing language-aware embedding layer We freeze the bottleneck layer and the token embedding layer along with the language embedding, updating the encoder and decoder. The training process can be written as {\u02c6\u03b8D, \u02c6\u03b8E} = arg min Ltts( \u02c6Y , Y ). \u03b8D,\u03b8E In contrast to the scheme represented in Eq. (8), the scheme in Eq. (9) preserves the parameters of the language-aware em- bedding layer to facilitate cross-lingual transfer. In the evalu- ation, we use the scheme formulated in Eq. (9), except for the ablation study in \u00a7 3.4. 2.3 Let Lsyn denote the set of language IDs used for inference. The text token sequence X and the language ID lsyn \u2208 Lsyn are fed to the model as in Eq. (1), and the encoder output is predicted as in Eq. (6). Unlike Eq. (7), the speech features are predicted as: Inference \u02c6Y = Decoder(Hout; \u03b8D). (10) The output waveform is obtained by feeding the predicted features \u02c6Y to a pretrained neural vocoder. Fig. 2(c) illustrates the inference process. The left and right sides of the figure show the typical multilingual TTS and our zero-shot TTS. Previous work [Li et al., 2019a] has typically assumed seen languages, and the inference is performed with the language IDs Lseen \u2282 Lpaired. However, it is challenging to perform TTS for unseen languages Lunseen \u2229 Lpaired = \u2205. While other"}, {"question": " What does the training process involve in the scheme of freezing the bottleneck layer and token embedding layer?", "answer": " The training process involves updating the encoder and decoder while freezing the bottleneck layer and token embedding layer.", "ref_chunk": "even for languages that are not present in the paired data. 2.1 Unsupervised Multilingual Text Pretraining Fig. 2(a) illustrates the unsupervised pretraining method. It uses multilingual text data consisting of languages that are not included in the paired data. Let X = (xn \u2208 V |n = 1, \u00b7 \u00b7 \u00b7 , N ) denote the input text token sequence of length N , where V denotes a vocabulary constructed for pretraining. 1https://github.com/Takaaki-Saeki/zm-text-tts 2https://takaaki-saeki.github.io/zm-tts-text demo We define Dtext as the text dataset. Let Ltext denote the set of language IDs included in Dtext. First, the masked token sequence X m and a language ID ltext \u2208 Ltext are fed to the model. Let the token embedding sequence and language em- n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and el \u2208 Rd, bedding be Z m = (zm respectively. The embedding layers output Z m and el as: Z m = Embed(X m; \u03b8T), el = Embed(ltext; \u03b8L), where \u03b8T and \u03b8L denote the model parameters of the to- ken embedding and language embedding layers, respectively. Then the token and language embeddings obtained in Eq. (1) are added and fed to a bottleneck layer to project them into a hidden input vector. Let Hin = (hin,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and Hout = (hout,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) denote hidden vectors in the encoder input and output, respectively. Then the conditional probability p(X|X\u2212\u03a0) is computed as: Hin = Bottleneck(Z m + el; \u03b8B), Hout = Encoder(Hin; \u03b8E), p(X|X\u2212\u03a0) = Softmax(PredictionNet(Hout; \u03b8P)), where \u03b8B, \u03b8E, \u03b8P denote the model parameters of the bot- tleneck layer, the encoder and a prediction network, respec- tively. In Eq. (4), Softmax(\u00b7) denotes a softmax function. We define the network with the model parameters {\u03b8B, \u03b8T, \u03b8L} as language-aware embedding layer, which jointly embeds the token sequence X and the language ID ltext as in Eq. (1) and (2). Let \u03a0 = (\u03c0k \u2208 N|k = 1, \u00b7 \u00b7 \u00b7 , K) be the indexes of the masked tokens of length K. With the probability com- puted in Eq. (4), the training objective can be defined as: Lmlm = 1 K K (cid:88) k=1 log p(x\u03c0k |X m), {\u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T, \u02c6\u03b8L} = arg min \u03b8E,\u03b8B,\u03b8T,\u03b8L Lmlm. We use UTF-8 bytes or International Phonetic Alphabet (IPA) symbols for the input token sequence X. For each to- ken type, the vocabulary V is constructed from Dtext, which includes a start/end of sentence token ([SOS/EOS]). We ex- tracted International IPA sequences using an open-source toolkit3. To obtain the masked token X m, we use the same masking ratio and category as in the original BERT pre- training [Devlin et al., 2019] for each token type. Randomly, 12 % of the tokens are replaced with the [MASK] token, and 1.5 % of them are replaced with random tokens. Also, 1.5 % of the tokens are left unchanged and Lmlm is computed as in Eq. (5) for those 15 % of tokens that have indices \u03a0. 2.2 Supervised Learning with Paired Data Fig. 2(b) illustrates the supervised learning of the TTS model with paired data. We define the paired data and the set of lan- guage IDs as Dpaired and Lpaired, respectively. Note that we assume Lpaired \u2282 Ltext. Let Y = (yt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T ) denote the speech feature sequence with the length of T . We first initialize the model parameters {\u03b8E, \u03b8B, \u03b8T, \u03b8L} with 3https://github.com/espeak-ng/espeak-ng (1) (2) (3) (4) (5) those obtained in the pretraining described in \u00a7 2.1. Let \u03b8D denote the model parameter of the decoder. The speech fea- tures are predicted with teacher forcing as: Hout = Encoder(Bottleneck(Z + el)), \u02c6Y = Decoder(Hout, Y ; \u03b8D), where Z is the unmasked token embedding sequence. Note that the unmasked token sequence is used in Eq. (6), while the masked token sequence is used in Eq. (2) Let Ltts( \u02c6Y , Y ) denote the training objective of the TTS model. Then we consider two types of schemes. Updating language-aware embedding layer We only freeze the parameter of the language embedding layer \u03b8L while updating the rest of the parameters. Therefore the train- able model parameters can be written as {\u02c6\u03b8D, \u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T} = arg min \u03b8D,\u03b8E,\u03b8B,\u03b8T Ltts( \u02c6Y , Y ). Previous work has confirmed that multilingual BERT has high cross-lingual transferability for various NLP tasks [Wu and Dredze, 2019]. This scheme corresponds to a simple fine- tuning of BERT [Wu and Dredze, 2019], which updates all the parameters during training for the downstream tasks4. Freezing language-aware embedding layer We freeze the bottleneck layer and the token embedding layer along with the language embedding, updating the encoder and decoder. The training process can be written as {\u02c6\u03b8D, \u02c6\u03b8E} = arg min Ltts( \u02c6Y , Y ). \u03b8D,\u03b8E In contrast to the scheme represented in Eq. (8), the scheme in Eq. (9) preserves the parameters of the language-aware em- bedding layer to facilitate cross-lingual transfer. In the evalu- ation, we use the scheme formulated in Eq. (9), except for the ablation study in \u00a7 3.4. 2.3 Let Lsyn denote the set of language IDs used for inference. The text token sequence X and the language ID lsyn \u2208 Lsyn are fed to the model as in Eq. (1), and the encoder output is predicted as in Eq. (6). Unlike Eq. (7), the speech features are predicted as: Inference \u02c6Y = Decoder(Hout; \u03b8D). (10) The output waveform is obtained by feeding the predicted features \u02c6Y to a pretrained neural vocoder. Fig. 2(c) illustrates the inference process. The left and right sides of the figure show the typical multilingual TTS and our zero-shot TTS. Previous work [Li et al., 2019a] has typically assumed seen languages, and the inference is performed with the language IDs Lseen \u2282 Lpaired. However, it is challenging to perform TTS for unseen languages Lunseen \u2229 Lpaired = \u2205. While other"}, {"question": " How are the speech features predicted in the inference process of zero-shot TTS?", "answer": " The speech features are predicted by feeding the encoder output to the decoder.", "ref_chunk": "even for languages that are not present in the paired data. 2.1 Unsupervised Multilingual Text Pretraining Fig. 2(a) illustrates the unsupervised pretraining method. It uses multilingual text data consisting of languages that are not included in the paired data. Let X = (xn \u2208 V |n = 1, \u00b7 \u00b7 \u00b7 , N ) denote the input text token sequence of length N , where V denotes a vocabulary constructed for pretraining. 1https://github.com/Takaaki-Saeki/zm-text-tts 2https://takaaki-saeki.github.io/zm-tts-text demo We define Dtext as the text dataset. Let Ltext denote the set of language IDs included in Dtext. First, the masked token sequence X m and a language ID ltext \u2208 Ltext are fed to the model. Let the token embedding sequence and language em- n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and el \u2208 Rd, bedding be Z m = (zm respectively. The embedding layers output Z m and el as: Z m = Embed(X m; \u03b8T), el = Embed(ltext; \u03b8L), where \u03b8T and \u03b8L denote the model parameters of the to- ken embedding and language embedding layers, respectively. Then the token and language embeddings obtained in Eq. (1) are added and fed to a bottleneck layer to project them into a hidden input vector. Let Hin = (hin,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and Hout = (hout,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) denote hidden vectors in the encoder input and output, respectively. Then the conditional probability p(X|X\u2212\u03a0) is computed as: Hin = Bottleneck(Z m + el; \u03b8B), Hout = Encoder(Hin; \u03b8E), p(X|X\u2212\u03a0) = Softmax(PredictionNet(Hout; \u03b8P)), where \u03b8B, \u03b8E, \u03b8P denote the model parameters of the bot- tleneck layer, the encoder and a prediction network, respec- tively. In Eq. (4), Softmax(\u00b7) denotes a softmax function. We define the network with the model parameters {\u03b8B, \u03b8T, \u03b8L} as language-aware embedding layer, which jointly embeds the token sequence X and the language ID ltext as in Eq. (1) and (2). Let \u03a0 = (\u03c0k \u2208 N|k = 1, \u00b7 \u00b7 \u00b7 , K) be the indexes of the masked tokens of length K. With the probability com- puted in Eq. (4), the training objective can be defined as: Lmlm = 1 K K (cid:88) k=1 log p(x\u03c0k |X m), {\u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T, \u02c6\u03b8L} = arg min \u03b8E,\u03b8B,\u03b8T,\u03b8L Lmlm. We use UTF-8 bytes or International Phonetic Alphabet (IPA) symbols for the input token sequence X. For each to- ken type, the vocabulary V is constructed from Dtext, which includes a start/end of sentence token ([SOS/EOS]). We ex- tracted International IPA sequences using an open-source toolkit3. To obtain the masked token X m, we use the same masking ratio and category as in the original BERT pre- training [Devlin et al., 2019] for each token type. Randomly, 12 % of the tokens are replaced with the [MASK] token, and 1.5 % of them are replaced with random tokens. Also, 1.5 % of the tokens are left unchanged and Lmlm is computed as in Eq. (5) for those 15 % of tokens that have indices \u03a0. 2.2 Supervised Learning with Paired Data Fig. 2(b) illustrates the supervised learning of the TTS model with paired data. We define the paired data and the set of lan- guage IDs as Dpaired and Lpaired, respectively. Note that we assume Lpaired \u2282 Ltext. Let Y = (yt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T ) denote the speech feature sequence with the length of T . We first initialize the model parameters {\u03b8E, \u03b8B, \u03b8T, \u03b8L} with 3https://github.com/espeak-ng/espeak-ng (1) (2) (3) (4) (5) those obtained in the pretraining described in \u00a7 2.1. Let \u03b8D denote the model parameter of the decoder. The speech fea- tures are predicted with teacher forcing as: Hout = Encoder(Bottleneck(Z + el)), \u02c6Y = Decoder(Hout, Y ; \u03b8D), where Z is the unmasked token embedding sequence. Note that the unmasked token sequence is used in Eq. (6), while the masked token sequence is used in Eq. (2) Let Ltts( \u02c6Y , Y ) denote the training objective of the TTS model. Then we consider two types of schemes. Updating language-aware embedding layer We only freeze the parameter of the language embedding layer \u03b8L while updating the rest of the parameters. Therefore the train- able model parameters can be written as {\u02c6\u03b8D, \u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T} = arg min \u03b8D,\u03b8E,\u03b8B,\u03b8T Ltts( \u02c6Y , Y ). Previous work has confirmed that multilingual BERT has high cross-lingual transferability for various NLP tasks [Wu and Dredze, 2019]. This scheme corresponds to a simple fine- tuning of BERT [Wu and Dredze, 2019], which updates all the parameters during training for the downstream tasks4. Freezing language-aware embedding layer We freeze the bottleneck layer and the token embedding layer along with the language embedding, updating the encoder and decoder. The training process can be written as {\u02c6\u03b8D, \u02c6\u03b8E} = arg min Ltts( \u02c6Y , Y ). \u03b8D,\u03b8E In contrast to the scheme represented in Eq. (8), the scheme in Eq. (9) preserves the parameters of the language-aware em- bedding layer to facilitate cross-lingual transfer. In the evalu- ation, we use the scheme formulated in Eq. (9), except for the ablation study in \u00a7 3.4. 2.3 Let Lsyn denote the set of language IDs used for inference. The text token sequence X and the language ID lsyn \u2208 Lsyn are fed to the model as in Eq. (1), and the encoder output is predicted as in Eq. (6). Unlike Eq. (7), the speech features are predicted as: Inference \u02c6Y = Decoder(Hout; \u03b8D). (10) The output waveform is obtained by feeding the predicted features \u02c6Y to a pretrained neural vocoder. Fig. 2(c) illustrates the inference process. The left and right sides of the figure show the typical multilingual TTS and our zero-shot TTS. Previous work [Li et al., 2019a] has typically assumed seen languages, and the inference is performed with the language IDs Lseen \u2282 Lpaired. However, it is challenging to perform TTS for unseen languages Lunseen \u2229 Lpaired = \u2205. While other"}], "doc_text": "even for languages that are not present in the paired data. 2.1 Unsupervised Multilingual Text Pretraining Fig. 2(a) illustrates the unsupervised pretraining method. It uses multilingual text data consisting of languages that are not included in the paired data. Let X = (xn \u2208 V |n = 1, \u00b7 \u00b7 \u00b7 , N ) denote the input text token sequence of length N , where V denotes a vocabulary constructed for pretraining. 1https://github.com/Takaaki-Saeki/zm-text-tts 2https://takaaki-saeki.github.io/zm-tts-text demo We define Dtext as the text dataset. Let Ltext denote the set of language IDs included in Dtext. First, the masked token sequence X m and a language ID ltext \u2208 Ltext are fed to the model. Let the token embedding sequence and language em- n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and el \u2208 Rd, bedding be Z m = (zm respectively. The embedding layers output Z m and el as: Z m = Embed(X m; \u03b8T), el = Embed(ltext; \u03b8L), where \u03b8T and \u03b8L denote the model parameters of the to- ken embedding and language embedding layers, respectively. Then the token and language embeddings obtained in Eq. (1) are added and fed to a bottleneck layer to project them into a hidden input vector. Let Hin = (hin,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) and Hout = (hout,n \u2208 Rd|n = 1, \u00b7 \u00b7 \u00b7 , N ) denote hidden vectors in the encoder input and output, respectively. Then the conditional probability p(X|X\u2212\u03a0) is computed as: Hin = Bottleneck(Z m + el; \u03b8B), Hout = Encoder(Hin; \u03b8E), p(X|X\u2212\u03a0) = Softmax(PredictionNet(Hout; \u03b8P)), where \u03b8B, \u03b8E, \u03b8P denote the model parameters of the bot- tleneck layer, the encoder and a prediction network, respec- tively. In Eq. (4), Softmax(\u00b7) denotes a softmax function. We define the network with the model parameters {\u03b8B, \u03b8T, \u03b8L} as language-aware embedding layer, which jointly embeds the token sequence X and the language ID ltext as in Eq. (1) and (2). Let \u03a0 = (\u03c0k \u2208 N|k = 1, \u00b7 \u00b7 \u00b7 , K) be the indexes of the masked tokens of length K. With the probability com- puted in Eq. (4), the training objective can be defined as: Lmlm = 1 K K (cid:88) k=1 log p(x\u03c0k |X m), {\u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T, \u02c6\u03b8L} = arg min \u03b8E,\u03b8B,\u03b8T,\u03b8L Lmlm. We use UTF-8 bytes or International Phonetic Alphabet (IPA) symbols for the input token sequence X. For each to- ken type, the vocabulary V is constructed from Dtext, which includes a start/end of sentence token ([SOS/EOS]). We ex- tracted International IPA sequences using an open-source toolkit3. To obtain the masked token X m, we use the same masking ratio and category as in the original BERT pre- training [Devlin et al., 2019] for each token type. Randomly, 12 % of the tokens are replaced with the [MASK] token, and 1.5 % of them are replaced with random tokens. Also, 1.5 % of the tokens are left unchanged and Lmlm is computed as in Eq. (5) for those 15 % of tokens that have indices \u03a0. 2.2 Supervised Learning with Paired Data Fig. 2(b) illustrates the supervised learning of the TTS model with paired data. We define the paired data and the set of lan- guage IDs as Dpaired and Lpaired, respectively. Note that we assume Lpaired \u2282 Ltext. Let Y = (yt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T ) denote the speech feature sequence with the length of T . We first initialize the model parameters {\u03b8E, \u03b8B, \u03b8T, \u03b8L} with 3https://github.com/espeak-ng/espeak-ng (1) (2) (3) (4) (5) those obtained in the pretraining described in \u00a7 2.1. Let \u03b8D denote the model parameter of the decoder. The speech fea- tures are predicted with teacher forcing as: Hout = Encoder(Bottleneck(Z + el)), \u02c6Y = Decoder(Hout, Y ; \u03b8D), where Z is the unmasked token embedding sequence. Note that the unmasked token sequence is used in Eq. (6), while the masked token sequence is used in Eq. (2) Let Ltts( \u02c6Y , Y ) denote the training objective of the TTS model. Then we consider two types of schemes. Updating language-aware embedding layer We only freeze the parameter of the language embedding layer \u03b8L while updating the rest of the parameters. Therefore the train- able model parameters can be written as {\u02c6\u03b8D, \u02c6\u03b8E, \u02c6\u03b8B, \u02c6\u03b8T} = arg min \u03b8D,\u03b8E,\u03b8B,\u03b8T Ltts( \u02c6Y , Y ). Previous work has confirmed that multilingual BERT has high cross-lingual transferability for various NLP tasks [Wu and Dredze, 2019]. This scheme corresponds to a simple fine- tuning of BERT [Wu and Dredze, 2019], which updates all the parameters during training for the downstream tasks4. Freezing language-aware embedding layer We freeze the bottleneck layer and the token embedding layer along with the language embedding, updating the encoder and decoder. The training process can be written as {\u02c6\u03b8D, \u02c6\u03b8E} = arg min Ltts( \u02c6Y , Y ). \u03b8D,\u03b8E In contrast to the scheme represented in Eq. (8), the scheme in Eq. (9) preserves the parameters of the language-aware em- bedding layer to facilitate cross-lingual transfer. In the evalu- ation, we use the scheme formulated in Eq. (9), except for the ablation study in \u00a7 3.4. 2.3 Let Lsyn denote the set of language IDs used for inference. The text token sequence X and the language ID lsyn \u2208 Lsyn are fed to the model as in Eq. (1), and the encoder output is predicted as in Eq. (6). Unlike Eq. (7), the speech features are predicted as: Inference \u02c6Y = Decoder(Hout; \u03b8D). (10) The output waveform is obtained by feeding the predicted features \u02c6Y to a pretrained neural vocoder. Fig. 2(c) illustrates the inference process. The left and right sides of the figure show the typical multilingual TTS and our zero-shot TTS. Previous work [Li et al., 2019a] has typically assumed seen languages, and the inference is performed with the language IDs Lseen \u2282 Lpaired. However, it is challenging to perform TTS for unseen languages Lunseen \u2229 Lpaired = \u2205. While other"}