{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_I3D:_Transformer_Architectures_with_Input-Dependent_Dynamic_Depth_for_Speech_Recognition_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the proposed I3D Transformer encoder?", "answer": " To achieve strong performance-efficiency trade-offs for end-to-end speech recognition.", "ref_chunk": "3 2 0 2 r a M 4 1 ] L C . s c [ 1 v 4 2 6 7 0 . 3 0 3 2 : v i X r a I3D: TRANSFORMER ARCHITECTURES WITH INPUT-DEPENDENT DYNAMIC DEPTH FOR SPEECH RECOGNITION Yifan Peng1, Jaesong Lee2, Shinji Watanabe1 1Carnegie Mellon University 2NAVER Corporation ABSTRACT Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it dif\ufb01cult to deploy these models in some real-world applica- tions. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a \ufb01xed architec- ture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-ef\ufb01ciency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders. Index Terms\u2014 Dynamic depth, transformer, speech recogni- tion 1. INTRODUCTION still determined on a frame-by-frame basis, which requires a spe- cial design for the \ufb01ned-grained key and query operations in self- attention. For nonstreaming (or chunk-based streaming) ASR, the frame-level prediction may be expensive and suboptimal, as it only captures frame-level local features. We propose a Transformer encoder with Input-Dependent Dynamic Depth (I3D) for end-to-end ASR. Instead of using carefully designed \ufb01ne-grained operations within submodules like attention, I3D predicts whether to skip an entire self-attention block or an entire feed-forward network through a series of local gate predictors or a single global gate predictor. The prediction is made at the ut- terance level (or chunk level if extended to streaming cases), which is easier to implement and reduces additional cost. It also captures global statistics of the input. As analyzed in Sec. 3.4, the length of an utterance affects the inference architecture. Some blocks may be useful for longer inputs. Results show that I3D models consistently outperform Transformers trained from scratch and the static pruned models via iterative layer pruning [29], when using a similar num- ber of layers for inference. We also perform interesting analysis on predicted gate probabilities and input-dependency, which helps us better understand the behavior of deep encoders. Recently, end-to-end automatic speech recognition (ASR) has gained popularity. Typical frameworks include Connectionist Tem- poral Classi\ufb01cation (CTC) [1], Attention-based Encoder-Decoder (AED) [2\u20134], and Recurrent Neural Network Transducer (RNN- T) [5]. Many types of networks can be used as encoders in these frameworks, such as Convolutional Neural Networks (CNNs), RNNs, Transformers [6] and their combinations [7\u20139]. Trans- formers have achieved great success in various benchmarks [10]. However, they usually contain many cascaded blocks and thus have high computation, which hinders deployment in some real-world ap- plications with limited resource. To reduce computation and speed up inference, researchers have investigated different approaches. 2. METHOD 2.1. Transformer encoder A Transformer [6] encoder layer contains a multi-head self-attention (MHA) module and a feed-forward network (FFN), which are com- bined sequentially. The function of the l-th layer is as follows: Y(l) = X(l\u22121) + MHA(l)(X(l\u22121)), X(l) = Y(l) + FFN(l)(Y(l)), A popular method is to compress a large pre-trained model using distillation [11\u201313], pruning [14\u201316], and quantization [15]. How- ever, the compressed model has a \ufb01xed architecture for all types of inputs, which might be suboptimal. For example, this \ufb01xed model may be too expensive for very easy utterances but insuf\ufb01cient for dif- \ufb01cult ones. To better trade off performance and computation, prior studies have explored dynamic models [17], which can adapt their architectures to different inputs. Dynamic models have shown to be effective in computer vision [18\u201323], which are mainly based on CNNs. For speech processing, [24] trains two RNN encoders of dif- ferent sizes and dynamically switches between them guided by key- word spotting. [25] proposes a dynamic encoder transducer based on layer dropout and collaborative learning. [26] adopts two RNN en- coders to tackle close-talk and far-talk speech. [27] also designs two RNN encoders that are compressed to different degrees and switches between them on a frame-by-frame basis. [28] extends this idea to Transformer-transducers and considers more \ufb02exible subnetworks, but it continues to focus on streaming ASR and the architecture is where X(l) is the output of the l-th Transformer layer and X(l\u22121) is thus the input to the l-th layer. Y(l) is the output of the MHA at the l-th layer, which is also the input of the FFN at the l-th layer. These sequences all have a length of T and a feature size of d. 2.2. Overall architecture of I3D encoders Fig. 1a shows the overall architecture of I3D encoders. A waveform is \ufb01rst converted to a feature sequence by a frontend, and further processed and downsampled by a CNN, after which positional em- beddings are added. Later, the sequence is processed by a stack of N I3D encoder layers to produce high-level features. This overall de- sign follows that of Transformer. However, Transformer always uses a \ufb01xed architecture regardless of the input. Our I3D selects different combinations of MHA and FFN depending on the input utterance. To determine whether a module should be executed or skipped, a bi- nary gate is introduced for each MHA or FFN module. The function (1) (2) Frontend I3D Encoder Layer CNN \u00d7\ud835\udc41 PositionalEmbedding Encoder Output SequenceEncoder Input Sequence Audio Waveform \u2026 Mean Multi-HeadSelf-Attention Local Gate Predictor Feed-ForwardNetwork I3D Encoder Layer Mean \u00d7\ud835\udc41\u2026Encoder Input SequenceEncoder Output Sequence Multi-HeadSelf-Attention Feed-ForwardNetwork Layer 1Layer \ud835\udc41 Global Gate Predictor \u2026 (a) Overall encoder architecture. (b) I3D encoder layer with a local gate predictor. (c) I3D encoder with a global gate predictor. Fig. 1: Architectures of our proposed I3D encoders. of the l-th layer (see Eqs. (1) and (2) for the vanilla Transformer) now becomes: Y(l) = X(l\u22121) + g(l) X(l) = Y(l) + g(l) MHA \u00b7 MHA(l)(X(l\u22121)), FFN \u00b7 FFN(l)(Y(l)), where g(l) FFN \u2208 {0, 1} are input-dependent gates. If a gate is predicted to be 0, then"}, {"question": " How does the I3D encoder differ from the vanilla Transformer and static pruned models?", "answer": " I3D predicts whether to skip an entire self-attention block or feed-forward network through gate predictors, making it more efficient.", "ref_chunk": "3 2 0 2 r a M 4 1 ] L C . s c [ 1 v 4 2 6 7 0 . 3 0 3 2 : v i X r a I3D: TRANSFORMER ARCHITECTURES WITH INPUT-DEPENDENT DYNAMIC DEPTH FOR SPEECH RECOGNITION Yifan Peng1, Jaesong Lee2, Shinji Watanabe1 1Carnegie Mellon University 2NAVER Corporation ABSTRACT Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it dif\ufb01cult to deploy these models in some real-world applica- tions. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a \ufb01xed architec- ture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-ef\ufb01ciency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders. Index Terms\u2014 Dynamic depth, transformer, speech recogni- tion 1. INTRODUCTION still determined on a frame-by-frame basis, which requires a spe- cial design for the \ufb01ned-grained key and query operations in self- attention. For nonstreaming (or chunk-based streaming) ASR, the frame-level prediction may be expensive and suboptimal, as it only captures frame-level local features. We propose a Transformer encoder with Input-Dependent Dynamic Depth (I3D) for end-to-end ASR. Instead of using carefully designed \ufb01ne-grained operations within submodules like attention, I3D predicts whether to skip an entire self-attention block or an entire feed-forward network through a series of local gate predictors or a single global gate predictor. The prediction is made at the ut- terance level (or chunk level if extended to streaming cases), which is easier to implement and reduces additional cost. It also captures global statistics of the input. As analyzed in Sec. 3.4, the length of an utterance affects the inference architecture. Some blocks may be useful for longer inputs. Results show that I3D models consistently outperform Transformers trained from scratch and the static pruned models via iterative layer pruning [29], when using a similar num- ber of layers for inference. We also perform interesting analysis on predicted gate probabilities and input-dependency, which helps us better understand the behavior of deep encoders. Recently, end-to-end automatic speech recognition (ASR) has gained popularity. Typical frameworks include Connectionist Tem- poral Classi\ufb01cation (CTC) [1], Attention-based Encoder-Decoder (AED) [2\u20134], and Recurrent Neural Network Transducer (RNN- T) [5]. Many types of networks can be used as encoders in these frameworks, such as Convolutional Neural Networks (CNNs), RNNs, Transformers [6] and their combinations [7\u20139]. Trans- formers have achieved great success in various benchmarks [10]. However, they usually contain many cascaded blocks and thus have high computation, which hinders deployment in some real-world ap- plications with limited resource. To reduce computation and speed up inference, researchers have investigated different approaches. 2. METHOD 2.1. Transformer encoder A Transformer [6] encoder layer contains a multi-head self-attention (MHA) module and a feed-forward network (FFN), which are com- bined sequentially. The function of the l-th layer is as follows: Y(l) = X(l\u22121) + MHA(l)(X(l\u22121)), X(l) = Y(l) + FFN(l)(Y(l)), A popular method is to compress a large pre-trained model using distillation [11\u201313], pruning [14\u201316], and quantization [15]. How- ever, the compressed model has a \ufb01xed architecture for all types of inputs, which might be suboptimal. For example, this \ufb01xed model may be too expensive for very easy utterances but insuf\ufb01cient for dif- \ufb01cult ones. To better trade off performance and computation, prior studies have explored dynamic models [17], which can adapt their architectures to different inputs. Dynamic models have shown to be effective in computer vision [18\u201323], which are mainly based on CNNs. For speech processing, [24] trains two RNN encoders of dif- ferent sizes and dynamically switches between them guided by key- word spotting. [25] proposes a dynamic encoder transducer based on layer dropout and collaborative learning. [26] adopts two RNN en- coders to tackle close-talk and far-talk speech. [27] also designs two RNN encoders that are compressed to different degrees and switches between them on a frame-by-frame basis. [28] extends this idea to Transformer-transducers and considers more \ufb02exible subnetworks, but it continues to focus on streaming ASR and the architecture is where X(l) is the output of the l-th Transformer layer and X(l\u22121) is thus the input to the l-th layer. Y(l) is the output of the MHA at the l-th layer, which is also the input of the FFN at the l-th layer. These sequences all have a length of T and a feature size of d. 2.2. Overall architecture of I3D encoders Fig. 1a shows the overall architecture of I3D encoders. A waveform is \ufb01rst converted to a feature sequence by a frontend, and further processed and downsampled by a CNN, after which positional em- beddings are added. Later, the sequence is processed by a stack of N I3D encoder layers to produce high-level features. This overall de- sign follows that of Transformer. However, Transformer always uses a \ufb01xed architecture regardless of the input. Our I3D selects different combinations of MHA and FFN depending on the input utterance. To determine whether a module should be executed or skipped, a bi- nary gate is introduced for each MHA or FFN module. The function (1) (2) Frontend I3D Encoder Layer CNN \u00d7\ud835\udc41 PositionalEmbedding Encoder Output SequenceEncoder Input Sequence Audio Waveform \u2026 Mean Multi-HeadSelf-Attention Local Gate Predictor Feed-ForwardNetwork I3D Encoder Layer Mean \u00d7\ud835\udc41\u2026Encoder Input SequenceEncoder Output Sequence Multi-HeadSelf-Attention Feed-ForwardNetwork Layer 1Layer \ud835\udc41 Global Gate Predictor \u2026 (a) Overall encoder architecture. (b) I3D encoder layer with a local gate predictor. (c) I3D encoder with a global gate predictor. Fig. 1: Architectures of our proposed I3D encoders. of the l-th layer (see Eqs. (1) and (2) for the vanilla Transformer) now becomes: Y(l) = X(l\u22121) + g(l) X(l) = Y(l) + g(l) MHA \u00b7 MHA(l)(X(l\u22121)), FFN \u00b7 FFN(l)(Y(l)), where g(l) FFN \u2208 {0, 1} are input-dependent gates. If a gate is predicted to be 0, then"}, {"question": " In what context is the prediction made by the I3D encoder?", "answer": " At the utterance level (or chunk level for streaming), which is easier to implement and captures global statistics.", "ref_chunk": "3 2 0 2 r a M 4 1 ] L C . s c [ 1 v 4 2 6 7 0 . 3 0 3 2 : v i X r a I3D: TRANSFORMER ARCHITECTURES WITH INPUT-DEPENDENT DYNAMIC DEPTH FOR SPEECH RECOGNITION Yifan Peng1, Jaesong Lee2, Shinji Watanabe1 1Carnegie Mellon University 2NAVER Corporation ABSTRACT Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it dif\ufb01cult to deploy these models in some real-world applica- tions. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a \ufb01xed architec- ture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-ef\ufb01ciency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders. Index Terms\u2014 Dynamic depth, transformer, speech recogni- tion 1. INTRODUCTION still determined on a frame-by-frame basis, which requires a spe- cial design for the \ufb01ned-grained key and query operations in self- attention. For nonstreaming (or chunk-based streaming) ASR, the frame-level prediction may be expensive and suboptimal, as it only captures frame-level local features. We propose a Transformer encoder with Input-Dependent Dynamic Depth (I3D) for end-to-end ASR. Instead of using carefully designed \ufb01ne-grained operations within submodules like attention, I3D predicts whether to skip an entire self-attention block or an entire feed-forward network through a series of local gate predictors or a single global gate predictor. The prediction is made at the ut- terance level (or chunk level if extended to streaming cases), which is easier to implement and reduces additional cost. It also captures global statistics of the input. As analyzed in Sec. 3.4, the length of an utterance affects the inference architecture. Some blocks may be useful for longer inputs. Results show that I3D models consistently outperform Transformers trained from scratch and the static pruned models via iterative layer pruning [29], when using a similar num- ber of layers for inference. We also perform interesting analysis on predicted gate probabilities and input-dependency, which helps us better understand the behavior of deep encoders. Recently, end-to-end automatic speech recognition (ASR) has gained popularity. Typical frameworks include Connectionist Tem- poral Classi\ufb01cation (CTC) [1], Attention-based Encoder-Decoder (AED) [2\u20134], and Recurrent Neural Network Transducer (RNN- T) [5]. Many types of networks can be used as encoders in these frameworks, such as Convolutional Neural Networks (CNNs), RNNs, Transformers [6] and their combinations [7\u20139]. Trans- formers have achieved great success in various benchmarks [10]. However, they usually contain many cascaded blocks and thus have high computation, which hinders deployment in some real-world ap- plications with limited resource. To reduce computation and speed up inference, researchers have investigated different approaches. 2. METHOD 2.1. Transformer encoder A Transformer [6] encoder layer contains a multi-head self-attention (MHA) module and a feed-forward network (FFN), which are com- bined sequentially. The function of the l-th layer is as follows: Y(l) = X(l\u22121) + MHA(l)(X(l\u22121)), X(l) = Y(l) + FFN(l)(Y(l)), A popular method is to compress a large pre-trained model using distillation [11\u201313], pruning [14\u201316], and quantization [15]. How- ever, the compressed model has a \ufb01xed architecture for all types of inputs, which might be suboptimal. For example, this \ufb01xed model may be too expensive for very easy utterances but insuf\ufb01cient for dif- \ufb01cult ones. To better trade off performance and computation, prior studies have explored dynamic models [17], which can adapt their architectures to different inputs. Dynamic models have shown to be effective in computer vision [18\u201323], which are mainly based on CNNs. For speech processing, [24] trains two RNN encoders of dif- ferent sizes and dynamically switches between them guided by key- word spotting. [25] proposes a dynamic encoder transducer based on layer dropout and collaborative learning. [26] adopts two RNN en- coders to tackle close-talk and far-talk speech. [27] also designs two RNN encoders that are compressed to different degrees and switches between them on a frame-by-frame basis. [28] extends this idea to Transformer-transducers and considers more \ufb02exible subnetworks, but it continues to focus on streaming ASR and the architecture is where X(l) is the output of the l-th Transformer layer and X(l\u22121) is thus the input to the l-th layer. Y(l) is the output of the MHA at the l-th layer, which is also the input of the FFN at the l-th layer. These sequences all have a length of T and a feature size of d. 2.2. Overall architecture of I3D encoders Fig. 1a shows the overall architecture of I3D encoders. A waveform is \ufb01rst converted to a feature sequence by a frontend, and further processed and downsampled by a CNN, after which positional em- beddings are added. Later, the sequence is processed by a stack of N I3D encoder layers to produce high-level features. This overall de- sign follows that of Transformer. However, Transformer always uses a \ufb01xed architecture regardless of the input. Our I3D selects different combinations of MHA and FFN depending on the input utterance. To determine whether a module should be executed or skipped, a bi- nary gate is introduced for each MHA or FFN module. The function (1) (2) Frontend I3D Encoder Layer CNN \u00d7\ud835\udc41 PositionalEmbedding Encoder Output SequenceEncoder Input Sequence Audio Waveform \u2026 Mean Multi-HeadSelf-Attention Local Gate Predictor Feed-ForwardNetwork I3D Encoder Layer Mean \u00d7\ud835\udc41\u2026Encoder Input SequenceEncoder Output Sequence Multi-HeadSelf-Attention Feed-ForwardNetwork Layer 1Layer \ud835\udc41 Global Gate Predictor \u2026 (a) Overall encoder architecture. (b) I3D encoder layer with a local gate predictor. (c) I3D encoder with a global gate predictor. Fig. 1: Architectures of our proposed I3D encoders. of the l-th layer (see Eqs. (1) and (2) for the vanilla Transformer) now becomes: Y(l) = X(l\u22121) + g(l) X(l) = Y(l) + g(l) MHA \u00b7 MHA(l)(X(l\u22121)), FFN \u00b7 FFN(l)(Y(l)), where g(l) FFN \u2208 {0, 1} are input-dependent gates. If a gate is predicted to be 0, then"}, {"question": " What are some typical frameworks for automatic speech recognition (ASR) mentioned in the text?", "answer": " Connectionist Temporal Classification (CTC), Attention-based Encoder-Decoder (AED), and Recurrent Neural Network Transducer (RNN-T).", "ref_chunk": "3 2 0 2 r a M 4 1 ] L C . s c [ 1 v 4 2 6 7 0 . 3 0 3 2 : v i X r a I3D: TRANSFORMER ARCHITECTURES WITH INPUT-DEPENDENT DYNAMIC DEPTH FOR SPEECH RECOGNITION Yifan Peng1, Jaesong Lee2, Shinji Watanabe1 1Carnegie Mellon University 2NAVER Corporation ABSTRACT Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it dif\ufb01cult to deploy these models in some real-world applica- tions. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a \ufb01xed architec- ture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-ef\ufb01ciency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders. Index Terms\u2014 Dynamic depth, transformer, speech recogni- tion 1. INTRODUCTION still determined on a frame-by-frame basis, which requires a spe- cial design for the \ufb01ned-grained key and query operations in self- attention. For nonstreaming (or chunk-based streaming) ASR, the frame-level prediction may be expensive and suboptimal, as it only captures frame-level local features. We propose a Transformer encoder with Input-Dependent Dynamic Depth (I3D) for end-to-end ASR. Instead of using carefully designed \ufb01ne-grained operations within submodules like attention, I3D predicts whether to skip an entire self-attention block or an entire feed-forward network through a series of local gate predictors or a single global gate predictor. The prediction is made at the ut- terance level (or chunk level if extended to streaming cases), which is easier to implement and reduces additional cost. It also captures global statistics of the input. As analyzed in Sec. 3.4, the length of an utterance affects the inference architecture. Some blocks may be useful for longer inputs. Results show that I3D models consistently outperform Transformers trained from scratch and the static pruned models via iterative layer pruning [29], when using a similar num- ber of layers for inference. We also perform interesting analysis on predicted gate probabilities and input-dependency, which helps us better understand the behavior of deep encoders. Recently, end-to-end automatic speech recognition (ASR) has gained popularity. Typical frameworks include Connectionist Tem- poral Classi\ufb01cation (CTC) [1], Attention-based Encoder-Decoder (AED) [2\u20134], and Recurrent Neural Network Transducer (RNN- T) [5]. Many types of networks can be used as encoders in these frameworks, such as Convolutional Neural Networks (CNNs), RNNs, Transformers [6] and their combinations [7\u20139]. Trans- formers have achieved great success in various benchmarks [10]. However, they usually contain many cascaded blocks and thus have high computation, which hinders deployment in some real-world ap- plications with limited resource. To reduce computation and speed up inference, researchers have investigated different approaches. 2. METHOD 2.1. Transformer encoder A Transformer [6] encoder layer contains a multi-head self-attention (MHA) module and a feed-forward network (FFN), which are com- bined sequentially. The function of the l-th layer is as follows: Y(l) = X(l\u22121) + MHA(l)(X(l\u22121)), X(l) = Y(l) + FFN(l)(Y(l)), A popular method is to compress a large pre-trained model using distillation [11\u201313], pruning [14\u201316], and quantization [15]. How- ever, the compressed model has a \ufb01xed architecture for all types of inputs, which might be suboptimal. For example, this \ufb01xed model may be too expensive for very easy utterances but insuf\ufb01cient for dif- \ufb01cult ones. To better trade off performance and computation, prior studies have explored dynamic models [17], which can adapt their architectures to different inputs. Dynamic models have shown to be effective in computer vision [18\u201323], which are mainly based on CNNs. For speech processing, [24] trains two RNN encoders of dif- ferent sizes and dynamically switches between them guided by key- word spotting. [25] proposes a dynamic encoder transducer based on layer dropout and collaborative learning. [26] adopts two RNN en- coders to tackle close-talk and far-talk speech. [27] also designs two RNN encoders that are compressed to different degrees and switches between them on a frame-by-frame basis. [28] extends this idea to Transformer-transducers and considers more \ufb02exible subnetworks, but it continues to focus on streaming ASR and the architecture is where X(l) is the output of the l-th Transformer layer and X(l\u22121) is thus the input to the l-th layer. Y(l) is the output of the MHA at the l-th layer, which is also the input of the FFN at the l-th layer. These sequences all have a length of T and a feature size of d. 2.2. Overall architecture of I3D encoders Fig. 1a shows the overall architecture of I3D encoders. A waveform is \ufb01rst converted to a feature sequence by a frontend, and further processed and downsampled by a CNN, after which positional em- beddings are added. Later, the sequence is processed by a stack of N I3D encoder layers to produce high-level features. This overall de- sign follows that of Transformer. However, Transformer always uses a \ufb01xed architecture regardless of the input. Our I3D selects different combinations of MHA and FFN depending on the input utterance. To determine whether a module should be executed or skipped, a bi- nary gate is introduced for each MHA or FFN module. The function (1) (2) Frontend I3D Encoder Layer CNN \u00d7\ud835\udc41 PositionalEmbedding Encoder Output SequenceEncoder Input Sequence Audio Waveform \u2026 Mean Multi-HeadSelf-Attention Local Gate Predictor Feed-ForwardNetwork I3D Encoder Layer Mean \u00d7\ud835\udc41\u2026Encoder Input SequenceEncoder Output Sequence Multi-HeadSelf-Attention Feed-ForwardNetwork Layer 1Layer \ud835\udc41 Global Gate Predictor \u2026 (a) Overall encoder architecture. (b) I3D encoder layer with a local gate predictor. (c) I3D encoder with a global gate predictor. Fig. 1: Architectures of our proposed I3D encoders. of the l-th layer (see Eqs. (1) and (2) for the vanilla Transformer) now becomes: Y(l) = X(l\u22121) + g(l) X(l) = Y(l) + g(l) MHA \u00b7 MHA(l)(X(l\u22121)), FFN \u00b7 FFN(l)(Y(l)), where g(l) FFN \u2208 {0, 1} are input-dependent gates. If a gate is predicted to be 0, then"}, {"question": " What are some methods used to reduce computation and speed up inference in Transformer models?", "answer": " Distillation, pruning, and quantization of large pre-trained models.", "ref_chunk": "3 2 0 2 r a M 4 1 ] L C . s c [ 1 v 4 2 6 7 0 . 3 0 3 2 : v i X r a I3D: TRANSFORMER ARCHITECTURES WITH INPUT-DEPENDENT DYNAMIC DEPTH FOR SPEECH RECOGNITION Yifan Peng1, Jaesong Lee2, Shinji Watanabe1 1Carnegie Mellon University 2NAVER Corporation ABSTRACT Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it dif\ufb01cult to deploy these models in some real-world applica- tions. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a \ufb01xed architec- ture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-ef\ufb01ciency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders. Index Terms\u2014 Dynamic depth, transformer, speech recogni- tion 1. INTRODUCTION still determined on a frame-by-frame basis, which requires a spe- cial design for the \ufb01ned-grained key and query operations in self- attention. For nonstreaming (or chunk-based streaming) ASR, the frame-level prediction may be expensive and suboptimal, as it only captures frame-level local features. We propose a Transformer encoder with Input-Dependent Dynamic Depth (I3D) for end-to-end ASR. Instead of using carefully designed \ufb01ne-grained operations within submodules like attention, I3D predicts whether to skip an entire self-attention block or an entire feed-forward network through a series of local gate predictors or a single global gate predictor. The prediction is made at the ut- terance level (or chunk level if extended to streaming cases), which is easier to implement and reduces additional cost. It also captures global statistics of the input. As analyzed in Sec. 3.4, the length of an utterance affects the inference architecture. Some blocks may be useful for longer inputs. Results show that I3D models consistently outperform Transformers trained from scratch and the static pruned models via iterative layer pruning [29], when using a similar num- ber of layers for inference. We also perform interesting analysis on predicted gate probabilities and input-dependency, which helps us better understand the behavior of deep encoders. Recently, end-to-end automatic speech recognition (ASR) has gained popularity. Typical frameworks include Connectionist Tem- poral Classi\ufb01cation (CTC) [1], Attention-based Encoder-Decoder (AED) [2\u20134], and Recurrent Neural Network Transducer (RNN- T) [5]. Many types of networks can be used as encoders in these frameworks, such as Convolutional Neural Networks (CNNs), RNNs, Transformers [6] and their combinations [7\u20139]. Trans- formers have achieved great success in various benchmarks [10]. However, they usually contain many cascaded blocks and thus have high computation, which hinders deployment in some real-world ap- plications with limited resource. To reduce computation and speed up inference, researchers have investigated different approaches. 2. METHOD 2.1. Transformer encoder A Transformer [6] encoder layer contains a multi-head self-attention (MHA) module and a feed-forward network (FFN), which are com- bined sequentially. The function of the l-th layer is as follows: Y(l) = X(l\u22121) + MHA(l)(X(l\u22121)), X(l) = Y(l) + FFN(l)(Y(l)), A popular method is to compress a large pre-trained model using distillation [11\u201313], pruning [14\u201316], and quantization [15]. How- ever, the compressed model has a \ufb01xed architecture for all types of inputs, which might be suboptimal. For example, this \ufb01xed model may be too expensive for very easy utterances but insuf\ufb01cient for dif- \ufb01cult ones. To better trade off performance and computation, prior studies have explored dynamic models [17], which can adapt their architectures to different inputs. Dynamic models have shown to be effective in computer vision [18\u201323], which are mainly based on CNNs. For speech processing, [24] trains two RNN encoders of dif- ferent sizes and dynamically switches between them guided by key- word spotting. [25] proposes a dynamic encoder transducer based on layer dropout and collaborative learning. [26] adopts two RNN en- coders to tackle close-talk and far-talk speech. [27] also designs two RNN encoders that are compressed to different degrees and switches between them on a frame-by-frame basis. [28] extends this idea to Transformer-transducers and considers more \ufb02exible subnetworks, but it continues to focus on streaming ASR and the architecture is where X(l) is the output of the l-th Transformer layer and X(l\u22121) is thus the input to the l-th layer. Y(l) is the output of the MHA at the l-th layer, which is also the input of the FFN at the l-th layer. These sequences all have a length of T and a feature size of d. 2.2. Overall architecture of I3D encoders Fig. 1a shows the overall architecture of I3D encoders. A waveform is \ufb01rst converted to a feature sequence by a frontend, and further processed and downsampled by a CNN, after which positional em- beddings are added. Later, the sequence is processed by a stack of N I3D encoder layers to produce high-level features. This overall de- sign follows that of Transformer. However, Transformer always uses a \ufb01xed architecture regardless of the input. Our I3D selects different combinations of MHA and FFN depending on the input utterance. To determine whether a module should be executed or skipped, a bi- nary gate is introduced for each MHA or FFN module. The function (1) (2) Frontend I3D Encoder Layer CNN \u00d7\ud835\udc41 PositionalEmbedding Encoder Output SequenceEncoder Input Sequence Audio Waveform \u2026 Mean Multi-HeadSelf-Attention Local Gate Predictor Feed-ForwardNetwork I3D Encoder Layer Mean \u00d7\ud835\udc41\u2026Encoder Input SequenceEncoder Output Sequence Multi-HeadSelf-Attention Feed-ForwardNetwork Layer 1Layer \ud835\udc41 Global Gate Predictor \u2026 (a) Overall encoder architecture. (b) I3D encoder layer with a local gate predictor. (c) I3D encoder with a global gate predictor. Fig. 1: Architectures of our proposed I3D encoders. of the l-th layer (see Eqs. (1) and (2) for the vanilla Transformer) now becomes: Y(l) = X(l\u22121) + g(l) X(l) = Y(l) + g(l) MHA \u00b7 MHA(l)(X(l\u22121)), FFN \u00b7 FFN(l)(Y(l)), where g(l) FFN \u2208 {0, 1} are input-dependent gates. If a gate is predicted to be 0, then"}, {"question": " How do dynamic models differ from static models in adapting their architectures to different inputs?", "answer": " Dynamic models can adjust their architectures based on input, while static models have fixed architectures for all inputs.", "ref_chunk": "3 2 0 2 r a M 4 1 ] L C . s c [ 1 v 4 2 6 7 0 . 3 0 3 2 : v i X r a I3D: TRANSFORMER ARCHITECTURES WITH INPUT-DEPENDENT DYNAMIC DEPTH FOR SPEECH RECOGNITION Yifan Peng1, Jaesong Lee2, Shinji Watanabe1 1Carnegie Mellon University 2NAVER Corporation ABSTRACT Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it dif\ufb01cult to deploy these models in some real-world applica- tions. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a \ufb01xed architec- ture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-ef\ufb01ciency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders. Index Terms\u2014 Dynamic depth, transformer, speech recogni- tion 1. INTRODUCTION still determined on a frame-by-frame basis, which requires a spe- cial design for the \ufb01ned-grained key and query operations in self- attention. For nonstreaming (or chunk-based streaming) ASR, the frame-level prediction may be expensive and suboptimal, as it only captures frame-level local features. We propose a Transformer encoder with Input-Dependent Dynamic Depth (I3D) for end-to-end ASR. Instead of using carefully designed \ufb01ne-grained operations within submodules like attention, I3D predicts whether to skip an entire self-attention block or an entire feed-forward network through a series of local gate predictors or a single global gate predictor. The prediction is made at the ut- terance level (or chunk level if extended to streaming cases), which is easier to implement and reduces additional cost. It also captures global statistics of the input. As analyzed in Sec. 3.4, the length of an utterance affects the inference architecture. Some blocks may be useful for longer inputs. Results show that I3D models consistently outperform Transformers trained from scratch and the static pruned models via iterative layer pruning [29], when using a similar num- ber of layers for inference. We also perform interesting analysis on predicted gate probabilities and input-dependency, which helps us better understand the behavior of deep encoders. Recently, end-to-end automatic speech recognition (ASR) has gained popularity. Typical frameworks include Connectionist Tem- poral Classi\ufb01cation (CTC) [1], Attention-based Encoder-Decoder (AED) [2\u20134], and Recurrent Neural Network Transducer (RNN- T) [5]. Many types of networks can be used as encoders in these frameworks, such as Convolutional Neural Networks (CNNs), RNNs, Transformers [6] and their combinations [7\u20139]. Trans- formers have achieved great success in various benchmarks [10]. However, they usually contain many cascaded blocks and thus have high computation, which hinders deployment in some real-world ap- plications with limited resource. To reduce computation and speed up inference, researchers have investigated different approaches. 2. METHOD 2.1. Transformer encoder A Transformer [6] encoder layer contains a multi-head self-attention (MHA) module and a feed-forward network (FFN), which are com- bined sequentially. The function of the l-th layer is as follows: Y(l) = X(l\u22121) + MHA(l)(X(l\u22121)), X(l) = Y(l) + FFN(l)(Y(l)), A popular method is to compress a large pre-trained model using distillation [11\u201313], pruning [14\u201316], and quantization [15]. How- ever, the compressed model has a \ufb01xed architecture for all types of inputs, which might be suboptimal. For example, this \ufb01xed model may be too expensive for very easy utterances but insuf\ufb01cient for dif- \ufb01cult ones. To better trade off performance and computation, prior studies have explored dynamic models [17], which can adapt their architectures to different inputs. Dynamic models have shown to be effective in computer vision [18\u201323], which are mainly based on CNNs. For speech processing, [24] trains two RNN encoders of dif- ferent sizes and dynamically switches between them guided by key- word spotting. [25] proposes a dynamic encoder transducer based on layer dropout and collaborative learning. [26] adopts two RNN en- coders to tackle close-talk and far-talk speech. [27] also designs two RNN encoders that are compressed to different degrees and switches between them on a frame-by-frame basis. [28] extends this idea to Transformer-transducers and considers more \ufb02exible subnetworks, but it continues to focus on streaming ASR and the architecture is where X(l) is the output of the l-th Transformer layer and X(l\u22121) is thus the input to the l-th layer. Y(l) is the output of the MHA at the l-th layer, which is also the input of the FFN at the l-th layer. These sequences all have a length of T and a feature size of d. 2.2. Overall architecture of I3D encoders Fig. 1a shows the overall architecture of I3D encoders. A waveform is \ufb01rst converted to a feature sequence by a frontend, and further processed and downsampled by a CNN, after which positional em- beddings are added. Later, the sequence is processed by a stack of N I3D encoder layers to produce high-level features. This overall de- sign follows that of Transformer. However, Transformer always uses a \ufb01xed architecture regardless of the input. Our I3D selects different combinations of MHA and FFN depending on the input utterance. To determine whether a module should be executed or skipped, a bi- nary gate is introduced for each MHA or FFN module. The function (1) (2) Frontend I3D Encoder Layer CNN \u00d7\ud835\udc41 PositionalEmbedding Encoder Output SequenceEncoder Input Sequence Audio Waveform \u2026 Mean Multi-HeadSelf-Attention Local Gate Predictor Feed-ForwardNetwork I3D Encoder Layer Mean \u00d7\ud835\udc41\u2026Encoder Input SequenceEncoder Output Sequence Multi-HeadSelf-Attention Feed-ForwardNetwork Layer 1Layer \ud835\udc41 Global Gate Predictor \u2026 (a) Overall encoder architecture. (b) I3D encoder layer with a local gate predictor. (c) I3D encoder with a global gate predictor. Fig. 1: Architectures of our proposed I3D encoders. of the l-th layer (see Eqs. (1) and (2) for the vanilla Transformer) now becomes: Y(l) = X(l\u22121) + g(l) X(l) = Y(l) + g(l) MHA \u00b7 MHA(l)(X(l\u22121)), FFN \u00b7 FFN(l)(Y(l)), where g(l) FFN \u2208 {0, 1} are input-dependent gates. If a gate is predicted to be 0, then"}, {"question": " What is the function of a Transformer encoder layer?", "answer": " It consists of a multi-head self-attention module and feed-forward network, combined sequentially.", "ref_chunk": "3 2 0 2 r a M 4 1 ] L C . s c [ 1 v 4 2 6 7 0 . 3 0 3 2 : v i X r a I3D: TRANSFORMER ARCHITECTURES WITH INPUT-DEPENDENT DYNAMIC DEPTH FOR SPEECH RECOGNITION Yifan Peng1, Jaesong Lee2, Shinji Watanabe1 1Carnegie Mellon University 2NAVER Corporation ABSTRACT Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it dif\ufb01cult to deploy these models in some real-world applica- tions. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a \ufb01xed architec- ture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-ef\ufb01ciency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders. Index Terms\u2014 Dynamic depth, transformer, speech recogni- tion 1. INTRODUCTION still determined on a frame-by-frame basis, which requires a spe- cial design for the \ufb01ned-grained key and query operations in self- attention. For nonstreaming (or chunk-based streaming) ASR, the frame-level prediction may be expensive and suboptimal, as it only captures frame-level local features. We propose a Transformer encoder with Input-Dependent Dynamic Depth (I3D) for end-to-end ASR. Instead of using carefully designed \ufb01ne-grained operations within submodules like attention, I3D predicts whether to skip an entire self-attention block or an entire feed-forward network through a series of local gate predictors or a single global gate predictor. The prediction is made at the ut- terance level (or chunk level if extended to streaming cases), which is easier to implement and reduces additional cost. It also captures global statistics of the input. As analyzed in Sec. 3.4, the length of an utterance affects the inference architecture. Some blocks may be useful for longer inputs. Results show that I3D models consistently outperform Transformers trained from scratch and the static pruned models via iterative layer pruning [29], when using a similar num- ber of layers for inference. We also perform interesting analysis on predicted gate probabilities and input-dependency, which helps us better understand the behavior of deep encoders. Recently, end-to-end automatic speech recognition (ASR) has gained popularity. Typical frameworks include Connectionist Tem- poral Classi\ufb01cation (CTC) [1], Attention-based Encoder-Decoder (AED) [2\u20134], and Recurrent Neural Network Transducer (RNN- T) [5]. Many types of networks can be used as encoders in these frameworks, such as Convolutional Neural Networks (CNNs), RNNs, Transformers [6] and their combinations [7\u20139]. Trans- formers have achieved great success in various benchmarks [10]. However, they usually contain many cascaded blocks and thus have high computation, which hinders deployment in some real-world ap- plications with limited resource. To reduce computation and speed up inference, researchers have investigated different approaches. 2. METHOD 2.1. Transformer encoder A Transformer [6] encoder layer contains a multi-head self-attention (MHA) module and a feed-forward network (FFN), which are com- bined sequentially. The function of the l-th layer is as follows: Y(l) = X(l\u22121) + MHA(l)(X(l\u22121)), X(l) = Y(l) + FFN(l)(Y(l)), A popular method is to compress a large pre-trained model using distillation [11\u201313], pruning [14\u201316], and quantization [15]. How- ever, the compressed model has a \ufb01xed architecture for all types of inputs, which might be suboptimal. For example, this \ufb01xed model may be too expensive for very easy utterances but insuf\ufb01cient for dif- \ufb01cult ones. To better trade off performance and computation, prior studies have explored dynamic models [17], which can adapt their architectures to different inputs. Dynamic models have shown to be effective in computer vision [18\u201323], which are mainly based on CNNs. For speech processing, [24] trains two RNN encoders of dif- ferent sizes and dynamically switches between them guided by key- word spotting. [25] proposes a dynamic encoder transducer based on layer dropout and collaborative learning. [26] adopts two RNN en- coders to tackle close-talk and far-talk speech. [27] also designs two RNN encoders that are compressed to different degrees and switches between them on a frame-by-frame basis. [28] extends this idea to Transformer-transducers and considers more \ufb02exible subnetworks, but it continues to focus on streaming ASR and the architecture is where X(l) is the output of the l-th Transformer layer and X(l\u22121) is thus the input to the l-th layer. Y(l) is the output of the MHA at the l-th layer, which is also the input of the FFN at the l-th layer. These sequences all have a length of T and a feature size of d. 2.2. Overall architecture of I3D encoders Fig. 1a shows the overall architecture of I3D encoders. A waveform is \ufb01rst converted to a feature sequence by a frontend, and further processed and downsampled by a CNN, after which positional em- beddings are added. Later, the sequence is processed by a stack of N I3D encoder layers to produce high-level features. This overall de- sign follows that of Transformer. However, Transformer always uses a \ufb01xed architecture regardless of the input. Our I3D selects different combinations of MHA and FFN depending on the input utterance. To determine whether a module should be executed or skipped, a bi- nary gate is introduced for each MHA or FFN module. The function (1) (2) Frontend I3D Encoder Layer CNN \u00d7\ud835\udc41 PositionalEmbedding Encoder Output SequenceEncoder Input Sequence Audio Waveform \u2026 Mean Multi-HeadSelf-Attention Local Gate Predictor Feed-ForwardNetwork I3D Encoder Layer Mean \u00d7\ud835\udc41\u2026Encoder Input SequenceEncoder Output Sequence Multi-HeadSelf-Attention Feed-ForwardNetwork Layer 1Layer \ud835\udc41 Global Gate Predictor \u2026 (a) Overall encoder architecture. (b) I3D encoder layer with a local gate predictor. (c) I3D encoder with a global gate predictor. Fig. 1: Architectures of our proposed I3D encoders. of the l-th layer (see Eqs. (1) and (2) for the vanilla Transformer) now becomes: Y(l) = X(l\u22121) + g(l) X(l) = Y(l) + g(l) MHA \u00b7 MHA(l)(X(l\u22121)), FFN \u00b7 FFN(l)(Y(l)), where g(l) FFN \u2208 {0, 1} are input-dependent gates. If a gate is predicted to be 0, then"}, {"question": " How does the I3D encoder select different combinations of modules for execution?", "answer": " By using binary gates for each multi-head self-attention or feed-forward network module.", "ref_chunk": "3 2 0 2 r a M 4 1 ] L C . s c [ 1 v 4 2 6 7 0 . 3 0 3 2 : v i X r a I3D: TRANSFORMER ARCHITECTURES WITH INPUT-DEPENDENT DYNAMIC DEPTH FOR SPEECH RECOGNITION Yifan Peng1, Jaesong Lee2, Shinji Watanabe1 1Carnegie Mellon University 2NAVER Corporation ABSTRACT Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it dif\ufb01cult to deploy these models in some real-world applica- tions. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a \ufb01xed architec- ture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-ef\ufb01ciency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders. Index Terms\u2014 Dynamic depth, transformer, speech recogni- tion 1. INTRODUCTION still determined on a frame-by-frame basis, which requires a spe- cial design for the \ufb01ned-grained key and query operations in self- attention. For nonstreaming (or chunk-based streaming) ASR, the frame-level prediction may be expensive and suboptimal, as it only captures frame-level local features. We propose a Transformer encoder with Input-Dependent Dynamic Depth (I3D) for end-to-end ASR. Instead of using carefully designed \ufb01ne-grained operations within submodules like attention, I3D predicts whether to skip an entire self-attention block or an entire feed-forward network through a series of local gate predictors or a single global gate predictor. The prediction is made at the ut- terance level (or chunk level if extended to streaming cases), which is easier to implement and reduces additional cost. It also captures global statistics of the input. As analyzed in Sec. 3.4, the length of an utterance affects the inference architecture. Some blocks may be useful for longer inputs. Results show that I3D models consistently outperform Transformers trained from scratch and the static pruned models via iterative layer pruning [29], when using a similar num- ber of layers for inference. We also perform interesting analysis on predicted gate probabilities and input-dependency, which helps us better understand the behavior of deep encoders. Recently, end-to-end automatic speech recognition (ASR) has gained popularity. Typical frameworks include Connectionist Tem- poral Classi\ufb01cation (CTC) [1], Attention-based Encoder-Decoder (AED) [2\u20134], and Recurrent Neural Network Transducer (RNN- T) [5]. Many types of networks can be used as encoders in these frameworks, such as Convolutional Neural Networks (CNNs), RNNs, Transformers [6] and their combinations [7\u20139]. Trans- formers have achieved great success in various benchmarks [10]. However, they usually contain many cascaded blocks and thus have high computation, which hinders deployment in some real-world ap- plications with limited resource. To reduce computation and speed up inference, researchers have investigated different approaches. 2. METHOD 2.1. Transformer encoder A Transformer [6] encoder layer contains a multi-head self-attention (MHA) module and a feed-forward network (FFN), which are com- bined sequentially. The function of the l-th layer is as follows: Y(l) = X(l\u22121) + MHA(l)(X(l\u22121)), X(l) = Y(l) + FFN(l)(Y(l)), A popular method is to compress a large pre-trained model using distillation [11\u201313], pruning [14\u201316], and quantization [15]. How- ever, the compressed model has a \ufb01xed architecture for all types of inputs, which might be suboptimal. For example, this \ufb01xed model may be too expensive for very easy utterances but insuf\ufb01cient for dif- \ufb01cult ones. To better trade off performance and computation, prior studies have explored dynamic models [17], which can adapt their architectures to different inputs. Dynamic models have shown to be effective in computer vision [18\u201323], which are mainly based on CNNs. For speech processing, [24] trains two RNN encoders of dif- ferent sizes and dynamically switches between them guided by key- word spotting. [25] proposes a dynamic encoder transducer based on layer dropout and collaborative learning. [26] adopts two RNN en- coders to tackle close-talk and far-talk speech. [27] also designs two RNN encoders that are compressed to different degrees and switches between them on a frame-by-frame basis. [28] extends this idea to Transformer-transducers and considers more \ufb02exible subnetworks, but it continues to focus on streaming ASR and the architecture is where X(l) is the output of the l-th Transformer layer and X(l\u22121) is thus the input to the l-th layer. Y(l) is the output of the MHA at the l-th layer, which is also the input of the FFN at the l-th layer. These sequences all have a length of T and a feature size of d. 2.2. Overall architecture of I3D encoders Fig. 1a shows the overall architecture of I3D encoders. A waveform is \ufb01rst converted to a feature sequence by a frontend, and further processed and downsampled by a CNN, after which positional em- beddings are added. Later, the sequence is processed by a stack of N I3D encoder layers to produce high-level features. This overall de- sign follows that of Transformer. However, Transformer always uses a \ufb01xed architecture regardless of the input. Our I3D selects different combinations of MHA and FFN depending on the input utterance. To determine whether a module should be executed or skipped, a bi- nary gate is introduced for each MHA or FFN module. The function (1) (2) Frontend I3D Encoder Layer CNN \u00d7\ud835\udc41 PositionalEmbedding Encoder Output SequenceEncoder Input Sequence Audio Waveform \u2026 Mean Multi-HeadSelf-Attention Local Gate Predictor Feed-ForwardNetwork I3D Encoder Layer Mean \u00d7\ud835\udc41\u2026Encoder Input SequenceEncoder Output Sequence Multi-HeadSelf-Attention Feed-ForwardNetwork Layer 1Layer \ud835\udc41 Global Gate Predictor \u2026 (a) Overall encoder architecture. (b) I3D encoder layer with a local gate predictor. (c) I3D encoder with a global gate predictor. Fig. 1: Architectures of our proposed I3D encoders. of the l-th layer (see Eqs. (1) and (2) for the vanilla Transformer) now becomes: Y(l) = X(l\u22121) + g(l) X(l) = Y(l) + g(l) MHA \u00b7 MHA(l)(X(l\u22121)), FFN \u00b7 FFN(l)(Y(l)), where g(l) FFN \u2208 {0, 1} are input-dependent gates. If a gate is predicted to be 0, then"}, {"question": " What is the overall architecture of the I3D encoder based on the provided text?", "answer": " It processes an input sequence using a stack of I3D encoder layers that dynamically select modules based on the input.", "ref_chunk": "3 2 0 2 r a M 4 1 ] L C . s c [ 1 v 4 2 6 7 0 . 3 0 3 2 : v i X r a I3D: TRANSFORMER ARCHITECTURES WITH INPUT-DEPENDENT DYNAMIC DEPTH FOR SPEECH RECOGNITION Yifan Peng1, Jaesong Lee2, Shinji Watanabe1 1Carnegie Mellon University 2NAVER Corporation ABSTRACT Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it dif\ufb01cult to deploy these models in some real-world applica- tions. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a \ufb01xed architec- ture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-ef\ufb01ciency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders. Index Terms\u2014 Dynamic depth, transformer, speech recogni- tion 1. INTRODUCTION still determined on a frame-by-frame basis, which requires a spe- cial design for the \ufb01ned-grained key and query operations in self- attention. For nonstreaming (or chunk-based streaming) ASR, the frame-level prediction may be expensive and suboptimal, as it only captures frame-level local features. We propose a Transformer encoder with Input-Dependent Dynamic Depth (I3D) for end-to-end ASR. Instead of using carefully designed \ufb01ne-grained operations within submodules like attention, I3D predicts whether to skip an entire self-attention block or an entire feed-forward network through a series of local gate predictors or a single global gate predictor. The prediction is made at the ut- terance level (or chunk level if extended to streaming cases), which is easier to implement and reduces additional cost. It also captures global statistics of the input. As analyzed in Sec. 3.4, the length of an utterance affects the inference architecture. Some blocks may be useful for longer inputs. Results show that I3D models consistently outperform Transformers trained from scratch and the static pruned models via iterative layer pruning [29], when using a similar num- ber of layers for inference. We also perform interesting analysis on predicted gate probabilities and input-dependency, which helps us better understand the behavior of deep encoders. Recently, end-to-end automatic speech recognition (ASR) has gained popularity. Typical frameworks include Connectionist Tem- poral Classi\ufb01cation (CTC) [1], Attention-based Encoder-Decoder (AED) [2\u20134], and Recurrent Neural Network Transducer (RNN- T) [5]. Many types of networks can be used as encoders in these frameworks, such as Convolutional Neural Networks (CNNs), RNNs, Transformers [6] and their combinations [7\u20139]. Trans- formers have achieved great success in various benchmarks [10]. However, they usually contain many cascaded blocks and thus have high computation, which hinders deployment in some real-world ap- plications with limited resource. To reduce computation and speed up inference, researchers have investigated different approaches. 2. METHOD 2.1. Transformer encoder A Transformer [6] encoder layer contains a multi-head self-attention (MHA) module and a feed-forward network (FFN), which are com- bined sequentially. The function of the l-th layer is as follows: Y(l) = X(l\u22121) + MHA(l)(X(l\u22121)), X(l) = Y(l) + FFN(l)(Y(l)), A popular method is to compress a large pre-trained model using distillation [11\u201313], pruning [14\u201316], and quantization [15]. How- ever, the compressed model has a \ufb01xed architecture for all types of inputs, which might be suboptimal. For example, this \ufb01xed model may be too expensive for very easy utterances but insuf\ufb01cient for dif- \ufb01cult ones. To better trade off performance and computation, prior studies have explored dynamic models [17], which can adapt their architectures to different inputs. Dynamic models have shown to be effective in computer vision [18\u201323], which are mainly based on CNNs. For speech processing, [24] trains two RNN encoders of dif- ferent sizes and dynamically switches between them guided by key- word spotting. [25] proposes a dynamic encoder transducer based on layer dropout and collaborative learning. [26] adopts two RNN en- coders to tackle close-talk and far-talk speech. [27] also designs two RNN encoders that are compressed to different degrees and switches between them on a frame-by-frame basis. [28] extends this idea to Transformer-transducers and considers more \ufb02exible subnetworks, but it continues to focus on streaming ASR and the architecture is where X(l) is the output of the l-th Transformer layer and X(l\u22121) is thus the input to the l-th layer. Y(l) is the output of the MHA at the l-th layer, which is also the input of the FFN at the l-th layer. These sequences all have a length of T and a feature size of d. 2.2. Overall architecture of I3D encoders Fig. 1a shows the overall architecture of I3D encoders. A waveform is \ufb01rst converted to a feature sequence by a frontend, and further processed and downsampled by a CNN, after which positional em- beddings are added. Later, the sequence is processed by a stack of N I3D encoder layers to produce high-level features. This overall de- sign follows that of Transformer. However, Transformer always uses a \ufb01xed architecture regardless of the input. Our I3D selects different combinations of MHA and FFN depending on the input utterance. To determine whether a module should be executed or skipped, a bi- nary gate is introduced for each MHA or FFN module. The function (1) (2) Frontend I3D Encoder Layer CNN \u00d7\ud835\udc41 PositionalEmbedding Encoder Output SequenceEncoder Input Sequence Audio Waveform \u2026 Mean Multi-HeadSelf-Attention Local Gate Predictor Feed-ForwardNetwork I3D Encoder Layer Mean \u00d7\ud835\udc41\u2026Encoder Input SequenceEncoder Output Sequence Multi-HeadSelf-Attention Feed-ForwardNetwork Layer 1Layer \ud835\udc41 Global Gate Predictor \u2026 (a) Overall encoder architecture. (b) I3D encoder layer with a local gate predictor. (c) I3D encoder with a global gate predictor. Fig. 1: Architectures of our proposed I3D encoders. of the l-th layer (see Eqs. (1) and (2) for the vanilla Transformer) now becomes: Y(l) = X(l\u22121) + g(l) X(l) = Y(l) + g(l) MHA \u00b7 MHA(l)(X(l\u22121)), FFN \u00b7 FFN(l)(Y(l)), where g(l) FFN \u2208 {0, 1} are input-dependent gates. If a gate is predicted to be 0, then"}, {"question": " Why is the I3D encoder considered to outperform other models mentioned in the text?", "answer": " It achieves better performance-efficiency trade-offs by adapting the depth of the encoder based on the input.", "ref_chunk": "3 2 0 2 r a M 4 1 ] L C . s c [ 1 v 4 2 6 7 0 . 3 0 3 2 : v i X r a I3D: TRANSFORMER ARCHITECTURES WITH INPUT-DEPENDENT DYNAMIC DEPTH FOR SPEECH RECOGNITION Yifan Peng1, Jaesong Lee2, Shinji Watanabe1 1Carnegie Mellon University 2NAVER Corporation ABSTRACT Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it dif\ufb01cult to deploy these models in some real-world applica- tions. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a \ufb01xed architec- ture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-ef\ufb01ciency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders. Index Terms\u2014 Dynamic depth, transformer, speech recogni- tion 1. INTRODUCTION still determined on a frame-by-frame basis, which requires a spe- cial design for the \ufb01ned-grained key and query operations in self- attention. For nonstreaming (or chunk-based streaming) ASR, the frame-level prediction may be expensive and suboptimal, as it only captures frame-level local features. We propose a Transformer encoder with Input-Dependent Dynamic Depth (I3D) for end-to-end ASR. Instead of using carefully designed \ufb01ne-grained operations within submodules like attention, I3D predicts whether to skip an entire self-attention block or an entire feed-forward network through a series of local gate predictors or a single global gate predictor. The prediction is made at the ut- terance level (or chunk level if extended to streaming cases), which is easier to implement and reduces additional cost. It also captures global statistics of the input. As analyzed in Sec. 3.4, the length of an utterance affects the inference architecture. Some blocks may be useful for longer inputs. Results show that I3D models consistently outperform Transformers trained from scratch and the static pruned models via iterative layer pruning [29], when using a similar num- ber of layers for inference. We also perform interesting analysis on predicted gate probabilities and input-dependency, which helps us better understand the behavior of deep encoders. Recently, end-to-end automatic speech recognition (ASR) has gained popularity. Typical frameworks include Connectionist Tem- poral Classi\ufb01cation (CTC) [1], Attention-based Encoder-Decoder (AED) [2\u20134], and Recurrent Neural Network Transducer (RNN- T) [5]. Many types of networks can be used as encoders in these frameworks, such as Convolutional Neural Networks (CNNs), RNNs, Transformers [6] and their combinations [7\u20139]. Trans- formers have achieved great success in various benchmarks [10]. However, they usually contain many cascaded blocks and thus have high computation, which hinders deployment in some real-world ap- plications with limited resource. To reduce computation and speed up inference, researchers have investigated different approaches. 2. METHOD 2.1. Transformer encoder A Transformer [6] encoder layer contains a multi-head self-attention (MHA) module and a feed-forward network (FFN), which are com- bined sequentially. The function of the l-th layer is as follows: Y(l) = X(l\u22121) + MHA(l)(X(l\u22121)), X(l) = Y(l) + FFN(l)(Y(l)), A popular method is to compress a large pre-trained model using distillation [11\u201313], pruning [14\u201316], and quantization [15]. How- ever, the compressed model has a \ufb01xed architecture for all types of inputs, which might be suboptimal. For example, this \ufb01xed model may be too expensive for very easy utterances but insuf\ufb01cient for dif- \ufb01cult ones. To better trade off performance and computation, prior studies have explored dynamic models [17], which can adapt their architectures to different inputs. Dynamic models have shown to be effective in computer vision [18\u201323], which are mainly based on CNNs. For speech processing, [24] trains two RNN encoders of dif- ferent sizes and dynamically switches between them guided by key- word spotting. [25] proposes a dynamic encoder transducer based on layer dropout and collaborative learning. [26] adopts two RNN en- coders to tackle close-talk and far-talk speech. [27] also designs two RNN encoders that are compressed to different degrees and switches between them on a frame-by-frame basis. [28] extends this idea to Transformer-transducers and considers more \ufb02exible subnetworks, but it continues to focus on streaming ASR and the architecture is where X(l) is the output of the l-th Transformer layer and X(l\u22121) is thus the input to the l-th layer. Y(l) is the output of the MHA at the l-th layer, which is also the input of the FFN at the l-th layer. These sequences all have a length of T and a feature size of d. 2.2. Overall architecture of I3D encoders Fig. 1a shows the overall architecture of I3D encoders. A waveform is \ufb01rst converted to a feature sequence by a frontend, and further processed and downsampled by a CNN, after which positional em- beddings are added. Later, the sequence is processed by a stack of N I3D encoder layers to produce high-level features. This overall de- sign follows that of Transformer. However, Transformer always uses a \ufb01xed architecture regardless of the input. Our I3D selects different combinations of MHA and FFN depending on the input utterance. To determine whether a module should be executed or skipped, a bi- nary gate is introduced for each MHA or FFN module. The function (1) (2) Frontend I3D Encoder Layer CNN \u00d7\ud835\udc41 PositionalEmbedding Encoder Output SequenceEncoder Input Sequence Audio Waveform \u2026 Mean Multi-HeadSelf-Attention Local Gate Predictor Feed-ForwardNetwork I3D Encoder Layer Mean \u00d7\ud835\udc41\u2026Encoder Input SequenceEncoder Output Sequence Multi-HeadSelf-Attention Feed-ForwardNetwork Layer 1Layer \ud835\udc41 Global Gate Predictor \u2026 (a) Overall encoder architecture. (b) I3D encoder layer with a local gate predictor. (c) I3D encoder with a global gate predictor. Fig. 1: Architectures of our proposed I3D encoders. of the l-th layer (see Eqs. (1) and (2) for the vanilla Transformer) now becomes: Y(l) = X(l\u22121) + g(l) X(l) = Y(l) + g(l) MHA \u00b7 MHA(l)(X(l\u22121)), FFN \u00b7 FFN(l)(Y(l)), where g(l) FFN \u2208 {0, 1} are input-dependent gates. If a gate is predicted to be 0, then"}], "doc_text": "3 2 0 2 r a M 4 1 ] L C . s c [ 1 v 4 2 6 7 0 . 3 0 3 2 : v i X r a I3D: TRANSFORMER ARCHITECTURES WITH INPUT-DEPENDENT DYNAMIC DEPTH FOR SPEECH RECOGNITION Yifan Peng1, Jaesong Lee2, Shinji Watanabe1 1Carnegie Mellon University 2NAVER Corporation ABSTRACT Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it dif\ufb01cult to deploy these models in some real-world applica- tions. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a \ufb01xed architec- ture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-ef\ufb01ciency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders. Index Terms\u2014 Dynamic depth, transformer, speech recogni- tion 1. INTRODUCTION still determined on a frame-by-frame basis, which requires a spe- cial design for the \ufb01ned-grained key and query operations in self- attention. For nonstreaming (or chunk-based streaming) ASR, the frame-level prediction may be expensive and suboptimal, as it only captures frame-level local features. We propose a Transformer encoder with Input-Dependent Dynamic Depth (I3D) for end-to-end ASR. Instead of using carefully designed \ufb01ne-grained operations within submodules like attention, I3D predicts whether to skip an entire self-attention block or an entire feed-forward network through a series of local gate predictors or a single global gate predictor. The prediction is made at the ut- terance level (or chunk level if extended to streaming cases), which is easier to implement and reduces additional cost. It also captures global statistics of the input. As analyzed in Sec. 3.4, the length of an utterance affects the inference architecture. Some blocks may be useful for longer inputs. Results show that I3D models consistently outperform Transformers trained from scratch and the static pruned models via iterative layer pruning [29], when using a similar num- ber of layers for inference. We also perform interesting analysis on predicted gate probabilities and input-dependency, which helps us better understand the behavior of deep encoders. Recently, end-to-end automatic speech recognition (ASR) has gained popularity. Typical frameworks include Connectionist Tem- poral Classi\ufb01cation (CTC) [1], Attention-based Encoder-Decoder (AED) [2\u20134], and Recurrent Neural Network Transducer (RNN- T) [5]. Many types of networks can be used as encoders in these frameworks, such as Convolutional Neural Networks (CNNs), RNNs, Transformers [6] and their combinations [7\u20139]. Trans- formers have achieved great success in various benchmarks [10]. However, they usually contain many cascaded blocks and thus have high computation, which hinders deployment in some real-world ap- plications with limited resource. To reduce computation and speed up inference, researchers have investigated different approaches. 2. METHOD 2.1. Transformer encoder A Transformer [6] encoder layer contains a multi-head self-attention (MHA) module and a feed-forward network (FFN), which are com- bined sequentially. The function of the l-th layer is as follows: Y(l) = X(l\u22121) + MHA(l)(X(l\u22121)), X(l) = Y(l) + FFN(l)(Y(l)), A popular method is to compress a large pre-trained model using distillation [11\u201313], pruning [14\u201316], and quantization [15]. How- ever, the compressed model has a \ufb01xed architecture for all types of inputs, which might be suboptimal. For example, this \ufb01xed model may be too expensive for very easy utterances but insuf\ufb01cient for dif- \ufb01cult ones. To better trade off performance and computation, prior studies have explored dynamic models [17], which can adapt their architectures to different inputs. Dynamic models have shown to be effective in computer vision [18\u201323], which are mainly based on CNNs. For speech processing, [24] trains two RNN encoders of dif- ferent sizes and dynamically switches between them guided by key- word spotting. [25] proposes a dynamic encoder transducer based on layer dropout and collaborative learning. [26] adopts two RNN en- coders to tackle close-talk and far-talk speech. [27] also designs two RNN encoders that are compressed to different degrees and switches between them on a frame-by-frame basis. [28] extends this idea to Transformer-transducers and considers more \ufb02exible subnetworks, but it continues to focus on streaming ASR and the architecture is where X(l) is the output of the l-th Transformer layer and X(l\u22121) is thus the input to the l-th layer. Y(l) is the output of the MHA at the l-th layer, which is also the input of the FFN at the l-th layer. These sequences all have a length of T and a feature size of d. 2.2. Overall architecture of I3D encoders Fig. 1a shows the overall architecture of I3D encoders. A waveform is \ufb01rst converted to a feature sequence by a frontend, and further processed and downsampled by a CNN, after which positional em- beddings are added. Later, the sequence is processed by a stack of N I3D encoder layers to produce high-level features. This overall de- sign follows that of Transformer. However, Transformer always uses a \ufb01xed architecture regardless of the input. Our I3D selects different combinations of MHA and FFN depending on the input utterance. To determine whether a module should be executed or skipped, a bi- nary gate is introduced for each MHA or FFN module. The function (1) (2) Frontend I3D Encoder Layer CNN \u00d7\ud835\udc41 PositionalEmbedding Encoder Output SequenceEncoder Input Sequence Audio Waveform \u2026 Mean Multi-HeadSelf-Attention Local Gate Predictor Feed-ForwardNetwork I3D Encoder Layer Mean \u00d7\ud835\udc41\u2026Encoder Input SequenceEncoder Output Sequence Multi-HeadSelf-Attention Feed-ForwardNetwork Layer 1Layer \ud835\udc41 Global Gate Predictor \u2026 (a) Overall encoder architecture. (b) I3D encoder layer with a local gate predictor. (c) I3D encoder with a global gate predictor. Fig. 1: Architectures of our proposed I3D encoders. of the l-th layer (see Eqs. (1) and (2) for the vanilla Transformer) now becomes: Y(l) = X(l\u22121) + g(l) X(l) = Y(l) + g(l) MHA \u00b7 MHA(l)(X(l\u22121)), FFN \u00b7 FFN(l)(Y(l)), where g(l) FFN \u2208 {0, 1} are input-dependent gates. If a gate is predicted to be 0, then"}