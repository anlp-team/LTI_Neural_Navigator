{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Joint_Prediction_and_Denoising_for_Large-Scale_Multilingual_Self-Supervised_Learning_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the title of the research paper discussed in the text?", "answer": " JOINT PREDICTION AND DENOISING FOR LARGE-SCALE MULTILINGUAL SELF-SUPERVISED LEARNING", "ref_chunk": "3 2 0 2 p e S 8 2 ] L C . s c [ 2 v 7 1 3 5 1 . 9 0 3 2 : v i X r a JOINT PREDICTION AND DENOISING FOR LARGE-SCALE MULTILINGUAL SELF-SUPERVISED LEARNING William Chen1, Jiatong Shi1, Brian Yan1, Dan Berrebbi1, Wangyou Zhang1,2, Yifan Peng1, Xuankai Chang1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China ABSTRACT Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This fur- ther harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We pro- pose WavLabLM, which extends WavLM\u2019s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multi- lingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than 10% of the train- ing data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain 94% of XLS-R\u2019s performance with only 3% of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet. els across an array of speech processing tasks for English. While masked-prediction methods such as HuBERT [12] and WavLM [3] reign supreme on SUPERB, they are notably ab- sent in multilingual studies [17\u201322]. Instead, popular multilin- gual SSL models such as XLSR 53 [23], XLS-R 128 [24], and MMS [25] are built upon the wav2vec framework [10, 11]1, which is outperformed by the mentioned masked-prediction models above [3, 12, 15, 27, 28]. Our goal is to build a large-scale multilingual SSL model that uses state-of-the-art (SOTA) SSL methods while main- taining a reasonable computational budget. We believe that using more powerful techniques can actually lead to more effi- cient pre-training, as less data and compute is needed to reach similar levels of performance. By showing the feasibility of large-scale multilingual SSL without industrial compute, we hope to encourage more community-driven efforts in building state-of-the-art multilingual models and help pave the way for extending speech processing to new languages [7, 29]. Index Terms\u2014 SSL, HuBERT, WavLM, Multilingual 1. INTRODUCTION Recent advances in NLP and speech processing have been driven by large scale self-supervised learning (SSL) models trained on gigantic amounts of unlabeled or weakly labeled data [1\u20139]. In speech processing specifically, SSL research has largely centered around speech representation learning, accomplished by training large speech encoders [3, 10\u201313] on tens of thousands of hours of unlabeled audio. By leveraging the abundant amounts of unlabeled data, SSL presents a scalable solution for extending the reach of speech technologies to new audiences. This can be most di- rectly accomplished via cross-lingual transfer learning, which aims to leverage information from other languages within a single model. However, due to the increased complexity and computational expenses of training with multiple languages, the methodologies used in multilingual SSL have often lagged behind the state-of-the-art, which remain centered around high- resource languages such as English [1, 3, 4, 14, 15]. As such, we propose a new massively multilingual SSL model: WavLabLM. WavLabLM extends the WavLM frame- work of jointly learning masked speech prediction and denois- ing to nearly 40k hours of speech across 136 languages. De- spite its relatively small dataset size [23, 24, 30], WavLabLM\u2019s pre-training data is curated for wide geographical coverage, the 15 languages with the most data span 7 different regions across the globe. For contrast, the entire top 15 languages used for XLS-R 128 [24] are European. To pre-train WavLabLM within academic constraints, we explore various techniques to make SSL more efficient. We first experimented with continual learning from English-based SSL models, halving the resources required for pre-training at the cost of performance in certain tasks. For multilingual SSL models trained from scratch, we propose a novel multi-stage pre-training approach to counteract imbalances in the language distribution of the data. This can be done by simply training on a large unbalanced dataset, before continual learning on a smaller dataset with a balanced language distribution. As a result, WavLabLM achieves comparable performance to XLS- R 128 on the 10-minute track of ML-SUPERB, despite only This can be evidenced via the SUPERB Benchmark [16], which aims to evaluate the effectiveness of different SSL mod- 1A mHuBERT [26] model was publicly released, but was only pre-trained on 3 languages: English, Spanish, and French, and on a much smaller scale. \u00a9 IEEE 2023 To appear in Proc. ASRU2023, December 16-20, 2023, Beitou, Taipei using 10% of the pre-training data, and even outperforms it in several tasks. To better fit SSL pre-training into academic constraints, we further slim down our approach. Using only 4 GPUs, we find that a simple HuBERT Base model is surprisingly efficient at multilingual SSL. Our CV-HuBERT, trained only on 92 languages from Common Voice [31], maintains up to 94% of the performance of XLS-R with only 3% of the training data and a third of the model parameters. In summary, our methods and results can be outlined as follows: We propose WavLabLM, an SSL model for 136 lan- guages and the first extension of joint denoising and prediction to multilingual SSL. We propose novel SSL techniques to make pre-training more efficient: continual pre-training from English mod- els and multi-stage pre-training for language balancing. We explore multi-resolution inputs in the multilingual setting, and show significant efficiency gains can be achieved with only a vanilla HuBERT Base model. Overall, WavLabLM achieves comparable performance to XLS-R 128 with only 10% of the pre-training data. Our CV-HuBERT maintains up to 94% of the perfor- mance with only 3% of the data. Finally, we include detailed descriptions of our engineering work that was required for scaling SSL, as a community con- tribution."}, {"question": " Which universities are mentioned as affiliations for some of the authors?", "answer": " Carnegie Mellon University, USA and Shanghai Jiao Tong University, China", "ref_chunk": "3 2 0 2 p e S 8 2 ] L C . s c [ 2 v 7 1 3 5 1 . 9 0 3 2 : v i X r a JOINT PREDICTION AND DENOISING FOR LARGE-SCALE MULTILINGUAL SELF-SUPERVISED LEARNING William Chen1, Jiatong Shi1, Brian Yan1, Dan Berrebbi1, Wangyou Zhang1,2, Yifan Peng1, Xuankai Chang1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China ABSTRACT Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This fur- ther harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We pro- pose WavLabLM, which extends WavLM\u2019s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multi- lingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than 10% of the train- ing data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain 94% of XLS-R\u2019s performance with only 3% of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet. els across an array of speech processing tasks for English. While masked-prediction methods such as HuBERT [12] and WavLM [3] reign supreme on SUPERB, they are notably ab- sent in multilingual studies [17\u201322]. Instead, popular multilin- gual SSL models such as XLSR 53 [23], XLS-R 128 [24], and MMS [25] are built upon the wav2vec framework [10, 11]1, which is outperformed by the mentioned masked-prediction models above [3, 12, 15, 27, 28]. Our goal is to build a large-scale multilingual SSL model that uses state-of-the-art (SOTA) SSL methods while main- taining a reasonable computational budget. We believe that using more powerful techniques can actually lead to more effi- cient pre-training, as less data and compute is needed to reach similar levels of performance. By showing the feasibility of large-scale multilingual SSL without industrial compute, we hope to encourage more community-driven efforts in building state-of-the-art multilingual models and help pave the way for extending speech processing to new languages [7, 29]. Index Terms\u2014 SSL, HuBERT, WavLM, Multilingual 1. INTRODUCTION Recent advances in NLP and speech processing have been driven by large scale self-supervised learning (SSL) models trained on gigantic amounts of unlabeled or weakly labeled data [1\u20139]. In speech processing specifically, SSL research has largely centered around speech representation learning, accomplished by training large speech encoders [3, 10\u201313] on tens of thousands of hours of unlabeled audio. By leveraging the abundant amounts of unlabeled data, SSL presents a scalable solution for extending the reach of speech technologies to new audiences. This can be most di- rectly accomplished via cross-lingual transfer learning, which aims to leverage information from other languages within a single model. However, due to the increased complexity and computational expenses of training with multiple languages, the methodologies used in multilingual SSL have often lagged behind the state-of-the-art, which remain centered around high- resource languages such as English [1, 3, 4, 14, 15]. As such, we propose a new massively multilingual SSL model: WavLabLM. WavLabLM extends the WavLM frame- work of jointly learning masked speech prediction and denois- ing to nearly 40k hours of speech across 136 languages. De- spite its relatively small dataset size [23, 24, 30], WavLabLM\u2019s pre-training data is curated for wide geographical coverage, the 15 languages with the most data span 7 different regions across the globe. For contrast, the entire top 15 languages used for XLS-R 128 [24] are European. To pre-train WavLabLM within academic constraints, we explore various techniques to make SSL more efficient. We first experimented with continual learning from English-based SSL models, halving the resources required for pre-training at the cost of performance in certain tasks. For multilingual SSL models trained from scratch, we propose a novel multi-stage pre-training approach to counteract imbalances in the language distribution of the data. This can be done by simply training on a large unbalanced dataset, before continual learning on a smaller dataset with a balanced language distribution. As a result, WavLabLM achieves comparable performance to XLS- R 128 on the 10-minute track of ML-SUPERB, despite only This can be evidenced via the SUPERB Benchmark [16], which aims to evaluate the effectiveness of different SSL mod- 1A mHuBERT [26] model was publicly released, but was only pre-trained on 3 languages: English, Spanish, and French, and on a much smaller scale. \u00a9 IEEE 2023 To appear in Proc. ASRU2023, December 16-20, 2023, Beitou, Taipei using 10% of the pre-training data, and even outperforms it in several tasks. To better fit SSL pre-training into academic constraints, we further slim down our approach. Using only 4 GPUs, we find that a simple HuBERT Base model is surprisingly efficient at multilingual SSL. Our CV-HuBERT, trained only on 92 languages from Common Voice [31], maintains up to 94% of the performance of XLS-R with only 3% of the training data and a third of the model parameters. In summary, our methods and results can be outlined as follows: We propose WavLabLM, an SSL model for 136 lan- guages and the first extension of joint denoising and prediction to multilingual SSL. We propose novel SSL techniques to make pre-training more efficient: continual pre-training from English mod- els and multi-stage pre-training for language balancing. We explore multi-resolution inputs in the multilingual setting, and show significant efficiency gains can be achieved with only a vanilla HuBERT Base model. Overall, WavLabLM achieves comparable performance to XLS-R 128 with only 10% of the pre-training data. Our CV-HuBERT maintains up to 94% of the perfor- mance with only 3% of the data. Finally, we include detailed descriptions of our engineering work that was required for scaling SSL, as a community con- tribution."}, {"question": " How many languages does WavLabLM extend its joint prediction and denoising to?", "answer": " 136 languages", "ref_chunk": "3 2 0 2 p e S 8 2 ] L C . s c [ 2 v 7 1 3 5 1 . 9 0 3 2 : v i X r a JOINT PREDICTION AND DENOISING FOR LARGE-SCALE MULTILINGUAL SELF-SUPERVISED LEARNING William Chen1, Jiatong Shi1, Brian Yan1, Dan Berrebbi1, Wangyou Zhang1,2, Yifan Peng1, Xuankai Chang1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China ABSTRACT Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This fur- ther harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We pro- pose WavLabLM, which extends WavLM\u2019s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multi- lingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than 10% of the train- ing data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain 94% of XLS-R\u2019s performance with only 3% of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet. els across an array of speech processing tasks for English. While masked-prediction methods such as HuBERT [12] and WavLM [3] reign supreme on SUPERB, they are notably ab- sent in multilingual studies [17\u201322]. Instead, popular multilin- gual SSL models such as XLSR 53 [23], XLS-R 128 [24], and MMS [25] are built upon the wav2vec framework [10, 11]1, which is outperformed by the mentioned masked-prediction models above [3, 12, 15, 27, 28]. Our goal is to build a large-scale multilingual SSL model that uses state-of-the-art (SOTA) SSL methods while main- taining a reasonable computational budget. We believe that using more powerful techniques can actually lead to more effi- cient pre-training, as less data and compute is needed to reach similar levels of performance. By showing the feasibility of large-scale multilingual SSL without industrial compute, we hope to encourage more community-driven efforts in building state-of-the-art multilingual models and help pave the way for extending speech processing to new languages [7, 29]. Index Terms\u2014 SSL, HuBERT, WavLM, Multilingual 1. INTRODUCTION Recent advances in NLP and speech processing have been driven by large scale self-supervised learning (SSL) models trained on gigantic amounts of unlabeled or weakly labeled data [1\u20139]. In speech processing specifically, SSL research has largely centered around speech representation learning, accomplished by training large speech encoders [3, 10\u201313] on tens of thousands of hours of unlabeled audio. By leveraging the abundant amounts of unlabeled data, SSL presents a scalable solution for extending the reach of speech technologies to new audiences. This can be most di- rectly accomplished via cross-lingual transfer learning, which aims to leverage information from other languages within a single model. However, due to the increased complexity and computational expenses of training with multiple languages, the methodologies used in multilingual SSL have often lagged behind the state-of-the-art, which remain centered around high- resource languages such as English [1, 3, 4, 14, 15]. As such, we propose a new massively multilingual SSL model: WavLabLM. WavLabLM extends the WavLM frame- work of jointly learning masked speech prediction and denois- ing to nearly 40k hours of speech across 136 languages. De- spite its relatively small dataset size [23, 24, 30], WavLabLM\u2019s pre-training data is curated for wide geographical coverage, the 15 languages with the most data span 7 different regions across the globe. For contrast, the entire top 15 languages used for XLS-R 128 [24] are European. To pre-train WavLabLM within academic constraints, we explore various techniques to make SSL more efficient. We first experimented with continual learning from English-based SSL models, halving the resources required for pre-training at the cost of performance in certain tasks. For multilingual SSL models trained from scratch, we propose a novel multi-stage pre-training approach to counteract imbalances in the language distribution of the data. This can be done by simply training on a large unbalanced dataset, before continual learning on a smaller dataset with a balanced language distribution. As a result, WavLabLM achieves comparable performance to XLS- R 128 on the 10-minute track of ML-SUPERB, despite only This can be evidenced via the SUPERB Benchmark [16], which aims to evaluate the effectiveness of different SSL mod- 1A mHuBERT [26] model was publicly released, but was only pre-trained on 3 languages: English, Spanish, and French, and on a much smaller scale. \u00a9 IEEE 2023 To appear in Proc. ASRU2023, December 16-20, 2023, Beitou, Taipei using 10% of the pre-training data, and even outperforms it in several tasks. To better fit SSL pre-training into academic constraints, we further slim down our approach. Using only 4 GPUs, we find that a simple HuBERT Base model is surprisingly efficient at multilingual SSL. Our CV-HuBERT, trained only on 92 languages from Common Voice [31], maintains up to 94% of the performance of XLS-R with only 3% of the training data and a third of the model parameters. In summary, our methods and results can be outlined as follows: We propose WavLabLM, an SSL model for 136 lan- guages and the first extension of joint denoising and prediction to multilingual SSL. We propose novel SSL techniques to make pre-training more efficient: continual pre-training from English mod- els and multi-stage pre-training for language balancing. We explore multi-resolution inputs in the multilingual setting, and show significant efficiency gains can be achieved with only a vanilla HuBERT Base model. Overall, WavLabLM achieves comparable performance to XLS-R 128 with only 10% of the pre-training data. Our CV-HuBERT maintains up to 94% of the perfor- mance with only 3% of the data. Finally, we include detailed descriptions of our engineering work that was required for scaling SSL, as a community con- tribution."}, {"question": " What percentage of the training data does WavLabLM need to achieve comparable performance to XLS-R on ML-SUPERB?", "answer": " Less than 10%", "ref_chunk": "3 2 0 2 p e S 8 2 ] L C . s c [ 2 v 7 1 3 5 1 . 9 0 3 2 : v i X r a JOINT PREDICTION AND DENOISING FOR LARGE-SCALE MULTILINGUAL SELF-SUPERVISED LEARNING William Chen1, Jiatong Shi1, Brian Yan1, Dan Berrebbi1, Wangyou Zhang1,2, Yifan Peng1, Xuankai Chang1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China ABSTRACT Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This fur- ther harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We pro- pose WavLabLM, which extends WavLM\u2019s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multi- lingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than 10% of the train- ing data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain 94% of XLS-R\u2019s performance with only 3% of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet. els across an array of speech processing tasks for English. While masked-prediction methods such as HuBERT [12] and WavLM [3] reign supreme on SUPERB, they are notably ab- sent in multilingual studies [17\u201322]. Instead, popular multilin- gual SSL models such as XLSR 53 [23], XLS-R 128 [24], and MMS [25] are built upon the wav2vec framework [10, 11]1, which is outperformed by the mentioned masked-prediction models above [3, 12, 15, 27, 28]. Our goal is to build a large-scale multilingual SSL model that uses state-of-the-art (SOTA) SSL methods while main- taining a reasonable computational budget. We believe that using more powerful techniques can actually lead to more effi- cient pre-training, as less data and compute is needed to reach similar levels of performance. By showing the feasibility of large-scale multilingual SSL without industrial compute, we hope to encourage more community-driven efforts in building state-of-the-art multilingual models and help pave the way for extending speech processing to new languages [7, 29]. Index Terms\u2014 SSL, HuBERT, WavLM, Multilingual 1. INTRODUCTION Recent advances in NLP and speech processing have been driven by large scale self-supervised learning (SSL) models trained on gigantic amounts of unlabeled or weakly labeled data [1\u20139]. In speech processing specifically, SSL research has largely centered around speech representation learning, accomplished by training large speech encoders [3, 10\u201313] on tens of thousands of hours of unlabeled audio. By leveraging the abundant amounts of unlabeled data, SSL presents a scalable solution for extending the reach of speech technologies to new audiences. This can be most di- rectly accomplished via cross-lingual transfer learning, which aims to leverage information from other languages within a single model. However, due to the increased complexity and computational expenses of training with multiple languages, the methodologies used in multilingual SSL have often lagged behind the state-of-the-art, which remain centered around high- resource languages such as English [1, 3, 4, 14, 15]. As such, we propose a new massively multilingual SSL model: WavLabLM. WavLabLM extends the WavLM frame- work of jointly learning masked speech prediction and denois- ing to nearly 40k hours of speech across 136 languages. De- spite its relatively small dataset size [23, 24, 30], WavLabLM\u2019s pre-training data is curated for wide geographical coverage, the 15 languages with the most data span 7 different regions across the globe. For contrast, the entire top 15 languages used for XLS-R 128 [24] are European. To pre-train WavLabLM within academic constraints, we explore various techniques to make SSL more efficient. We first experimented with continual learning from English-based SSL models, halving the resources required for pre-training at the cost of performance in certain tasks. For multilingual SSL models trained from scratch, we propose a novel multi-stage pre-training approach to counteract imbalances in the language distribution of the data. This can be done by simply training on a large unbalanced dataset, before continual learning on a smaller dataset with a balanced language distribution. As a result, WavLabLM achieves comparable performance to XLS- R 128 on the 10-minute track of ML-SUPERB, despite only This can be evidenced via the SUPERB Benchmark [16], which aims to evaluate the effectiveness of different SSL mod- 1A mHuBERT [26] model was publicly released, but was only pre-trained on 3 languages: English, Spanish, and French, and on a much smaller scale. \u00a9 IEEE 2023 To appear in Proc. ASRU2023, December 16-20, 2023, Beitou, Taipei using 10% of the pre-training data, and even outperforms it in several tasks. To better fit SSL pre-training into academic constraints, we further slim down our approach. Using only 4 GPUs, we find that a simple HuBERT Base model is surprisingly efficient at multilingual SSL. Our CV-HuBERT, trained only on 92 languages from Common Voice [31], maintains up to 94% of the performance of XLS-R with only 3% of the training data and a third of the model parameters. In summary, our methods and results can be outlined as follows: We propose WavLabLM, an SSL model for 136 lan- guages and the first extension of joint denoising and prediction to multilingual SSL. We propose novel SSL techniques to make pre-training more efficient: continual pre-training from English mod- els and multi-stage pre-training for language balancing. We explore multi-resolution inputs in the multilingual setting, and show significant efficiency gains can be achieved with only a vanilla HuBERT Base model. Overall, WavLabLM achieves comparable performance to XLS-R 128 with only 10% of the pre-training data. Our CV-HuBERT maintains up to 94% of the perfor- mance with only 3% of the data. Finally, we include detailed descriptions of our engineering work that was required for scaling SSL, as a community con- tribution."}, {"question": " What are some popular multilingual SSL models mentioned in the text?", "answer": " XLSR 53, XLS-R 128, and MMS", "ref_chunk": "3 2 0 2 p e S 8 2 ] L C . s c [ 2 v 7 1 3 5 1 . 9 0 3 2 : v i X r a JOINT PREDICTION AND DENOISING FOR LARGE-SCALE MULTILINGUAL SELF-SUPERVISED LEARNING William Chen1, Jiatong Shi1, Brian Yan1, Dan Berrebbi1, Wangyou Zhang1,2, Yifan Peng1, Xuankai Chang1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China ABSTRACT Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This fur- ther harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We pro- pose WavLabLM, which extends WavLM\u2019s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multi- lingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than 10% of the train- ing data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain 94% of XLS-R\u2019s performance with only 3% of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet. els across an array of speech processing tasks for English. While masked-prediction methods such as HuBERT [12] and WavLM [3] reign supreme on SUPERB, they are notably ab- sent in multilingual studies [17\u201322]. Instead, popular multilin- gual SSL models such as XLSR 53 [23], XLS-R 128 [24], and MMS [25] are built upon the wav2vec framework [10, 11]1, which is outperformed by the mentioned masked-prediction models above [3, 12, 15, 27, 28]. Our goal is to build a large-scale multilingual SSL model that uses state-of-the-art (SOTA) SSL methods while main- taining a reasonable computational budget. We believe that using more powerful techniques can actually lead to more effi- cient pre-training, as less data and compute is needed to reach similar levels of performance. By showing the feasibility of large-scale multilingual SSL without industrial compute, we hope to encourage more community-driven efforts in building state-of-the-art multilingual models and help pave the way for extending speech processing to new languages [7, 29]. Index Terms\u2014 SSL, HuBERT, WavLM, Multilingual 1. INTRODUCTION Recent advances in NLP and speech processing have been driven by large scale self-supervised learning (SSL) models trained on gigantic amounts of unlabeled or weakly labeled data [1\u20139]. In speech processing specifically, SSL research has largely centered around speech representation learning, accomplished by training large speech encoders [3, 10\u201313] on tens of thousands of hours of unlabeled audio. By leveraging the abundant amounts of unlabeled data, SSL presents a scalable solution for extending the reach of speech technologies to new audiences. This can be most di- rectly accomplished via cross-lingual transfer learning, which aims to leverage information from other languages within a single model. However, due to the increased complexity and computational expenses of training with multiple languages, the methodologies used in multilingual SSL have often lagged behind the state-of-the-art, which remain centered around high- resource languages such as English [1, 3, 4, 14, 15]. As such, we propose a new massively multilingual SSL model: WavLabLM. WavLabLM extends the WavLM frame- work of jointly learning masked speech prediction and denois- ing to nearly 40k hours of speech across 136 languages. De- spite its relatively small dataset size [23, 24, 30], WavLabLM\u2019s pre-training data is curated for wide geographical coverage, the 15 languages with the most data span 7 different regions across the globe. For contrast, the entire top 15 languages used for XLS-R 128 [24] are European. To pre-train WavLabLM within academic constraints, we explore various techniques to make SSL more efficient. We first experimented with continual learning from English-based SSL models, halving the resources required for pre-training at the cost of performance in certain tasks. For multilingual SSL models trained from scratch, we propose a novel multi-stage pre-training approach to counteract imbalances in the language distribution of the data. This can be done by simply training on a large unbalanced dataset, before continual learning on a smaller dataset with a balanced language distribution. As a result, WavLabLM achieves comparable performance to XLS- R 128 on the 10-minute track of ML-SUPERB, despite only This can be evidenced via the SUPERB Benchmark [16], which aims to evaluate the effectiveness of different SSL mod- 1A mHuBERT [26] model was publicly released, but was only pre-trained on 3 languages: English, Spanish, and French, and on a much smaller scale. \u00a9 IEEE 2023 To appear in Proc. ASRU2023, December 16-20, 2023, Beitou, Taipei using 10% of the pre-training data, and even outperforms it in several tasks. To better fit SSL pre-training into academic constraints, we further slim down our approach. Using only 4 GPUs, we find that a simple HuBERT Base model is surprisingly efficient at multilingual SSL. Our CV-HuBERT, trained only on 92 languages from Common Voice [31], maintains up to 94% of the performance of XLS-R with only 3% of the training data and a third of the model parameters. In summary, our methods and results can be outlined as follows: We propose WavLabLM, an SSL model for 136 lan- guages and the first extension of joint denoising and prediction to multilingual SSL. We propose novel SSL techniques to make pre-training more efficient: continual pre-training from English mod- els and multi-stage pre-training for language balancing. We explore multi-resolution inputs in the multilingual setting, and show significant efficiency gains can be achieved with only a vanilla HuBERT Base model. Overall, WavLabLM achieves comparable performance to XLS-R 128 with only 10% of the pre-training data. Our CV-HuBERT maintains up to 94% of the perfor- mance with only 3% of the data. Finally, we include detailed descriptions of our engineering work that was required for scaling SSL, as a community con- tribution."}, {"question": " What is the primary goal of using more powerful techniques in large-scale multilingual self-supervised learning?", "answer": " To achieve more efficient pre-training with less data and compute", "ref_chunk": "3 2 0 2 p e S 8 2 ] L C . s c [ 2 v 7 1 3 5 1 . 9 0 3 2 : v i X r a JOINT PREDICTION AND DENOISING FOR LARGE-SCALE MULTILINGUAL SELF-SUPERVISED LEARNING William Chen1, Jiatong Shi1, Brian Yan1, Dan Berrebbi1, Wangyou Zhang1,2, Yifan Peng1, Xuankai Chang1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China ABSTRACT Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This fur- ther harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We pro- pose WavLabLM, which extends WavLM\u2019s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multi- lingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than 10% of the train- ing data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain 94% of XLS-R\u2019s performance with only 3% of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet. els across an array of speech processing tasks for English. While masked-prediction methods such as HuBERT [12] and WavLM [3] reign supreme on SUPERB, they are notably ab- sent in multilingual studies [17\u201322]. Instead, popular multilin- gual SSL models such as XLSR 53 [23], XLS-R 128 [24], and MMS [25] are built upon the wav2vec framework [10, 11]1, which is outperformed by the mentioned masked-prediction models above [3, 12, 15, 27, 28]. Our goal is to build a large-scale multilingual SSL model that uses state-of-the-art (SOTA) SSL methods while main- taining a reasonable computational budget. We believe that using more powerful techniques can actually lead to more effi- cient pre-training, as less data and compute is needed to reach similar levels of performance. By showing the feasibility of large-scale multilingual SSL without industrial compute, we hope to encourage more community-driven efforts in building state-of-the-art multilingual models and help pave the way for extending speech processing to new languages [7, 29]. Index Terms\u2014 SSL, HuBERT, WavLM, Multilingual 1. INTRODUCTION Recent advances in NLP and speech processing have been driven by large scale self-supervised learning (SSL) models trained on gigantic amounts of unlabeled or weakly labeled data [1\u20139]. In speech processing specifically, SSL research has largely centered around speech representation learning, accomplished by training large speech encoders [3, 10\u201313] on tens of thousands of hours of unlabeled audio. By leveraging the abundant amounts of unlabeled data, SSL presents a scalable solution for extending the reach of speech technologies to new audiences. This can be most di- rectly accomplished via cross-lingual transfer learning, which aims to leverage information from other languages within a single model. However, due to the increased complexity and computational expenses of training with multiple languages, the methodologies used in multilingual SSL have often lagged behind the state-of-the-art, which remain centered around high- resource languages such as English [1, 3, 4, 14, 15]. As such, we propose a new massively multilingual SSL model: WavLabLM. WavLabLM extends the WavLM frame- work of jointly learning masked speech prediction and denois- ing to nearly 40k hours of speech across 136 languages. De- spite its relatively small dataset size [23, 24, 30], WavLabLM\u2019s pre-training data is curated for wide geographical coverage, the 15 languages with the most data span 7 different regions across the globe. For contrast, the entire top 15 languages used for XLS-R 128 [24] are European. To pre-train WavLabLM within academic constraints, we explore various techniques to make SSL more efficient. We first experimented with continual learning from English-based SSL models, halving the resources required for pre-training at the cost of performance in certain tasks. For multilingual SSL models trained from scratch, we propose a novel multi-stage pre-training approach to counteract imbalances in the language distribution of the data. This can be done by simply training on a large unbalanced dataset, before continual learning on a smaller dataset with a balanced language distribution. As a result, WavLabLM achieves comparable performance to XLS- R 128 on the 10-minute track of ML-SUPERB, despite only This can be evidenced via the SUPERB Benchmark [16], which aims to evaluate the effectiveness of different SSL mod- 1A mHuBERT [26] model was publicly released, but was only pre-trained on 3 languages: English, Spanish, and French, and on a much smaller scale. \u00a9 IEEE 2023 To appear in Proc. ASRU2023, December 16-20, 2023, Beitou, Taipei using 10% of the pre-training data, and even outperforms it in several tasks. To better fit SSL pre-training into academic constraints, we further slim down our approach. Using only 4 GPUs, we find that a simple HuBERT Base model is surprisingly efficient at multilingual SSL. Our CV-HuBERT, trained only on 92 languages from Common Voice [31], maintains up to 94% of the performance of XLS-R with only 3% of the training data and a third of the model parameters. In summary, our methods and results can be outlined as follows: We propose WavLabLM, an SSL model for 136 lan- guages and the first extension of joint denoising and prediction to multilingual SSL. We propose novel SSL techniques to make pre-training more efficient: continual pre-training from English mod- els and multi-stage pre-training for language balancing. We explore multi-resolution inputs in the multilingual setting, and show significant efficiency gains can be achieved with only a vanilla HuBERT Base model. Overall, WavLabLM achieves comparable performance to XLS-R 128 with only 10% of the pre-training data. Our CV-HuBERT maintains up to 94% of the perfor- mance with only 3% of the data. Finally, we include detailed descriptions of our engineering work that was required for scaling SSL, as a community con- tribution."}, {"question": " What is the main focus of SSL research in speech processing?", "answer": " Speech representation learning", "ref_chunk": "3 2 0 2 p e S 8 2 ] L C . s c [ 2 v 7 1 3 5 1 . 9 0 3 2 : v i X r a JOINT PREDICTION AND DENOISING FOR LARGE-SCALE MULTILINGUAL SELF-SUPERVISED LEARNING William Chen1, Jiatong Shi1, Brian Yan1, Dan Berrebbi1, Wangyou Zhang1,2, Yifan Peng1, Xuankai Chang1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China ABSTRACT Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This fur- ther harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We pro- pose WavLabLM, which extends WavLM\u2019s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multi- lingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than 10% of the train- ing data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain 94% of XLS-R\u2019s performance with only 3% of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet. els across an array of speech processing tasks for English. While masked-prediction methods such as HuBERT [12] and WavLM [3] reign supreme on SUPERB, they are notably ab- sent in multilingual studies [17\u201322]. Instead, popular multilin- gual SSL models such as XLSR 53 [23], XLS-R 128 [24], and MMS [25] are built upon the wav2vec framework [10, 11]1, which is outperformed by the mentioned masked-prediction models above [3, 12, 15, 27, 28]. Our goal is to build a large-scale multilingual SSL model that uses state-of-the-art (SOTA) SSL methods while main- taining a reasonable computational budget. We believe that using more powerful techniques can actually lead to more effi- cient pre-training, as less data and compute is needed to reach similar levels of performance. By showing the feasibility of large-scale multilingual SSL without industrial compute, we hope to encourage more community-driven efforts in building state-of-the-art multilingual models and help pave the way for extending speech processing to new languages [7, 29]. Index Terms\u2014 SSL, HuBERT, WavLM, Multilingual 1. INTRODUCTION Recent advances in NLP and speech processing have been driven by large scale self-supervised learning (SSL) models trained on gigantic amounts of unlabeled or weakly labeled data [1\u20139]. In speech processing specifically, SSL research has largely centered around speech representation learning, accomplished by training large speech encoders [3, 10\u201313] on tens of thousands of hours of unlabeled audio. By leveraging the abundant amounts of unlabeled data, SSL presents a scalable solution for extending the reach of speech technologies to new audiences. This can be most di- rectly accomplished via cross-lingual transfer learning, which aims to leverage information from other languages within a single model. However, due to the increased complexity and computational expenses of training with multiple languages, the methodologies used in multilingual SSL have often lagged behind the state-of-the-art, which remain centered around high- resource languages such as English [1, 3, 4, 14, 15]. As such, we propose a new massively multilingual SSL model: WavLabLM. WavLabLM extends the WavLM frame- work of jointly learning masked speech prediction and denois- ing to nearly 40k hours of speech across 136 languages. De- spite its relatively small dataset size [23, 24, 30], WavLabLM\u2019s pre-training data is curated for wide geographical coverage, the 15 languages with the most data span 7 different regions across the globe. For contrast, the entire top 15 languages used for XLS-R 128 [24] are European. To pre-train WavLabLM within academic constraints, we explore various techniques to make SSL more efficient. We first experimented with continual learning from English-based SSL models, halving the resources required for pre-training at the cost of performance in certain tasks. For multilingual SSL models trained from scratch, we propose a novel multi-stage pre-training approach to counteract imbalances in the language distribution of the data. This can be done by simply training on a large unbalanced dataset, before continual learning on a smaller dataset with a balanced language distribution. As a result, WavLabLM achieves comparable performance to XLS- R 128 on the 10-minute track of ML-SUPERB, despite only This can be evidenced via the SUPERB Benchmark [16], which aims to evaluate the effectiveness of different SSL mod- 1A mHuBERT [26] model was publicly released, but was only pre-trained on 3 languages: English, Spanish, and French, and on a much smaller scale. \u00a9 IEEE 2023 To appear in Proc. ASRU2023, December 16-20, 2023, Beitou, Taipei using 10% of the pre-training data, and even outperforms it in several tasks. To better fit SSL pre-training into academic constraints, we further slim down our approach. Using only 4 GPUs, we find that a simple HuBERT Base model is surprisingly efficient at multilingual SSL. Our CV-HuBERT, trained only on 92 languages from Common Voice [31], maintains up to 94% of the performance of XLS-R with only 3% of the training data and a third of the model parameters. In summary, our methods and results can be outlined as follows: We propose WavLabLM, an SSL model for 136 lan- guages and the first extension of joint denoising and prediction to multilingual SSL. We propose novel SSL techniques to make pre-training more efficient: continual pre-training from English mod- els and multi-stage pre-training for language balancing. We explore multi-resolution inputs in the multilingual setting, and show significant efficiency gains can be achieved with only a vanilla HuBERT Base model. Overall, WavLabLM achieves comparable performance to XLS-R 128 with only 10% of the pre-training data. Our CV-HuBERT maintains up to 94% of the perfor- mance with only 3% of the data. Finally, we include detailed descriptions of our engineering work that was required for scaling SSL, as a community con- tribution."}, {"question": " What strategy does the text propose to counteract imbalances in the language distribution of the data?", "answer": " A novel multi-stage pre-training approach", "ref_chunk": "3 2 0 2 p e S 8 2 ] L C . s c [ 2 v 7 1 3 5 1 . 9 0 3 2 : v i X r a JOINT PREDICTION AND DENOISING FOR LARGE-SCALE MULTILINGUAL SELF-SUPERVISED LEARNING William Chen1, Jiatong Shi1, Brian Yan1, Dan Berrebbi1, Wangyou Zhang1,2, Yifan Peng1, Xuankai Chang1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China ABSTRACT Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This fur- ther harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We pro- pose WavLabLM, which extends WavLM\u2019s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multi- lingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than 10% of the train- ing data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain 94% of XLS-R\u2019s performance with only 3% of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet. els across an array of speech processing tasks for English. While masked-prediction methods such as HuBERT [12] and WavLM [3] reign supreme on SUPERB, they are notably ab- sent in multilingual studies [17\u201322]. Instead, popular multilin- gual SSL models such as XLSR 53 [23], XLS-R 128 [24], and MMS [25] are built upon the wav2vec framework [10, 11]1, which is outperformed by the mentioned masked-prediction models above [3, 12, 15, 27, 28]. Our goal is to build a large-scale multilingual SSL model that uses state-of-the-art (SOTA) SSL methods while main- taining a reasonable computational budget. We believe that using more powerful techniques can actually lead to more effi- cient pre-training, as less data and compute is needed to reach similar levels of performance. By showing the feasibility of large-scale multilingual SSL without industrial compute, we hope to encourage more community-driven efforts in building state-of-the-art multilingual models and help pave the way for extending speech processing to new languages [7, 29]. Index Terms\u2014 SSL, HuBERT, WavLM, Multilingual 1. INTRODUCTION Recent advances in NLP and speech processing have been driven by large scale self-supervised learning (SSL) models trained on gigantic amounts of unlabeled or weakly labeled data [1\u20139]. In speech processing specifically, SSL research has largely centered around speech representation learning, accomplished by training large speech encoders [3, 10\u201313] on tens of thousands of hours of unlabeled audio. By leveraging the abundant amounts of unlabeled data, SSL presents a scalable solution for extending the reach of speech technologies to new audiences. This can be most di- rectly accomplished via cross-lingual transfer learning, which aims to leverage information from other languages within a single model. However, due to the increased complexity and computational expenses of training with multiple languages, the methodologies used in multilingual SSL have often lagged behind the state-of-the-art, which remain centered around high- resource languages such as English [1, 3, 4, 14, 15]. As such, we propose a new massively multilingual SSL model: WavLabLM. WavLabLM extends the WavLM frame- work of jointly learning masked speech prediction and denois- ing to nearly 40k hours of speech across 136 languages. De- spite its relatively small dataset size [23, 24, 30], WavLabLM\u2019s pre-training data is curated for wide geographical coverage, the 15 languages with the most data span 7 different regions across the globe. For contrast, the entire top 15 languages used for XLS-R 128 [24] are European. To pre-train WavLabLM within academic constraints, we explore various techniques to make SSL more efficient. We first experimented with continual learning from English-based SSL models, halving the resources required for pre-training at the cost of performance in certain tasks. For multilingual SSL models trained from scratch, we propose a novel multi-stage pre-training approach to counteract imbalances in the language distribution of the data. This can be done by simply training on a large unbalanced dataset, before continual learning on a smaller dataset with a balanced language distribution. As a result, WavLabLM achieves comparable performance to XLS- R 128 on the 10-minute track of ML-SUPERB, despite only This can be evidenced via the SUPERB Benchmark [16], which aims to evaluate the effectiveness of different SSL mod- 1A mHuBERT [26] model was publicly released, but was only pre-trained on 3 languages: English, Spanish, and French, and on a much smaller scale. \u00a9 IEEE 2023 To appear in Proc. ASRU2023, December 16-20, 2023, Beitou, Taipei using 10% of the pre-training data, and even outperforms it in several tasks. To better fit SSL pre-training into academic constraints, we further slim down our approach. Using only 4 GPUs, we find that a simple HuBERT Base model is surprisingly efficient at multilingual SSL. Our CV-HuBERT, trained only on 92 languages from Common Voice [31], maintains up to 94% of the performance of XLS-R with only 3% of the training data and a third of the model parameters. In summary, our methods and results can be outlined as follows: We propose WavLabLM, an SSL model for 136 lan- guages and the first extension of joint denoising and prediction to multilingual SSL. We propose novel SSL techniques to make pre-training more efficient: continual pre-training from English mod- els and multi-stage pre-training for language balancing. We explore multi-resolution inputs in the multilingual setting, and show significant efficiency gains can be achieved with only a vanilla HuBERT Base model. Overall, WavLabLM achieves comparable performance to XLS-R 128 with only 10% of the pre-training data. Our CV-HuBERT maintains up to 94% of the perfor- mance with only 3% of the data. Finally, we include detailed descriptions of our engineering work that was required for scaling SSL, as a community con- tribution."}, {"question": " What model is mentioned as being surprisingly efficient at multilingual SSL with only 4 GPUs?", "answer": " HuBERT Base model", "ref_chunk": "3 2 0 2 p e S 8 2 ] L C . s c [ 2 v 7 1 3 5 1 . 9 0 3 2 : v i X r a JOINT PREDICTION AND DENOISING FOR LARGE-SCALE MULTILINGUAL SELF-SUPERVISED LEARNING William Chen1, Jiatong Shi1, Brian Yan1, Dan Berrebbi1, Wangyou Zhang1,2, Yifan Peng1, Xuankai Chang1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China ABSTRACT Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This fur- ther harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We pro- pose WavLabLM, which extends WavLM\u2019s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multi- lingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than 10% of the train- ing data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain 94% of XLS-R\u2019s performance with only 3% of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet. els across an array of speech processing tasks for English. While masked-prediction methods such as HuBERT [12] and WavLM [3] reign supreme on SUPERB, they are notably ab- sent in multilingual studies [17\u201322]. Instead, popular multilin- gual SSL models such as XLSR 53 [23], XLS-R 128 [24], and MMS [25] are built upon the wav2vec framework [10, 11]1, which is outperformed by the mentioned masked-prediction models above [3, 12, 15, 27, 28]. Our goal is to build a large-scale multilingual SSL model that uses state-of-the-art (SOTA) SSL methods while main- taining a reasonable computational budget. We believe that using more powerful techniques can actually lead to more effi- cient pre-training, as less data and compute is needed to reach similar levels of performance. By showing the feasibility of large-scale multilingual SSL without industrial compute, we hope to encourage more community-driven efforts in building state-of-the-art multilingual models and help pave the way for extending speech processing to new languages [7, 29]. Index Terms\u2014 SSL, HuBERT, WavLM, Multilingual 1. INTRODUCTION Recent advances in NLP and speech processing have been driven by large scale self-supervised learning (SSL) models trained on gigantic amounts of unlabeled or weakly labeled data [1\u20139]. In speech processing specifically, SSL research has largely centered around speech representation learning, accomplished by training large speech encoders [3, 10\u201313] on tens of thousands of hours of unlabeled audio. By leveraging the abundant amounts of unlabeled data, SSL presents a scalable solution for extending the reach of speech technologies to new audiences. This can be most di- rectly accomplished via cross-lingual transfer learning, which aims to leverage information from other languages within a single model. However, due to the increased complexity and computational expenses of training with multiple languages, the methodologies used in multilingual SSL have often lagged behind the state-of-the-art, which remain centered around high- resource languages such as English [1, 3, 4, 14, 15]. As such, we propose a new massively multilingual SSL model: WavLabLM. WavLabLM extends the WavLM frame- work of jointly learning masked speech prediction and denois- ing to nearly 40k hours of speech across 136 languages. De- spite its relatively small dataset size [23, 24, 30], WavLabLM\u2019s pre-training data is curated for wide geographical coverage, the 15 languages with the most data span 7 different regions across the globe. For contrast, the entire top 15 languages used for XLS-R 128 [24] are European. To pre-train WavLabLM within academic constraints, we explore various techniques to make SSL more efficient. We first experimented with continual learning from English-based SSL models, halving the resources required for pre-training at the cost of performance in certain tasks. For multilingual SSL models trained from scratch, we propose a novel multi-stage pre-training approach to counteract imbalances in the language distribution of the data. This can be done by simply training on a large unbalanced dataset, before continual learning on a smaller dataset with a balanced language distribution. As a result, WavLabLM achieves comparable performance to XLS- R 128 on the 10-minute track of ML-SUPERB, despite only This can be evidenced via the SUPERB Benchmark [16], which aims to evaluate the effectiveness of different SSL mod- 1A mHuBERT [26] model was publicly released, but was only pre-trained on 3 languages: English, Spanish, and French, and on a much smaller scale. \u00a9 IEEE 2023 To appear in Proc. ASRU2023, December 16-20, 2023, Beitou, Taipei using 10% of the pre-training data, and even outperforms it in several tasks. To better fit SSL pre-training into academic constraints, we further slim down our approach. Using only 4 GPUs, we find that a simple HuBERT Base model is surprisingly efficient at multilingual SSL. Our CV-HuBERT, trained only on 92 languages from Common Voice [31], maintains up to 94% of the performance of XLS-R with only 3% of the training data and a third of the model parameters. In summary, our methods and results can be outlined as follows: We propose WavLabLM, an SSL model for 136 lan- guages and the first extension of joint denoising and prediction to multilingual SSL. We propose novel SSL techniques to make pre-training more efficient: continual pre-training from English mod- els and multi-stage pre-training for language balancing. We explore multi-resolution inputs in the multilingual setting, and show significant efficiency gains can be achieved with only a vanilla HuBERT Base model. Overall, WavLabLM achieves comparable performance to XLS-R 128 with only 10% of the pre-training data. Our CV-HuBERT maintains up to 94% of the perfor- mance with only 3% of the data. Finally, we include detailed descriptions of our engineering work that was required for scaling SSL, as a community con- tribution."}, {"question": " What percentage of the performance of XLS-R does CV-HuBERT maintain with only 3% of the training data?", "answer": " Up to 94%", "ref_chunk": "3 2 0 2 p e S 8 2 ] L C . s c [ 2 v 7 1 3 5 1 . 9 0 3 2 : v i X r a JOINT PREDICTION AND DENOISING FOR LARGE-SCALE MULTILINGUAL SELF-SUPERVISED LEARNING William Chen1, Jiatong Shi1, Brian Yan1, Dan Berrebbi1, Wangyou Zhang1,2, Yifan Peng1, Xuankai Chang1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China ABSTRACT Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This fur- ther harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We pro- pose WavLabLM, which extends WavLM\u2019s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multi- lingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than 10% of the train- ing data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain 94% of XLS-R\u2019s performance with only 3% of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet. els across an array of speech processing tasks for English. While masked-prediction methods such as HuBERT [12] and WavLM [3] reign supreme on SUPERB, they are notably ab- sent in multilingual studies [17\u201322]. Instead, popular multilin- gual SSL models such as XLSR 53 [23], XLS-R 128 [24], and MMS [25] are built upon the wav2vec framework [10, 11]1, which is outperformed by the mentioned masked-prediction models above [3, 12, 15, 27, 28]. Our goal is to build a large-scale multilingual SSL model that uses state-of-the-art (SOTA) SSL methods while main- taining a reasonable computational budget. We believe that using more powerful techniques can actually lead to more effi- cient pre-training, as less data and compute is needed to reach similar levels of performance. By showing the feasibility of large-scale multilingual SSL without industrial compute, we hope to encourage more community-driven efforts in building state-of-the-art multilingual models and help pave the way for extending speech processing to new languages [7, 29]. Index Terms\u2014 SSL, HuBERT, WavLM, Multilingual 1. INTRODUCTION Recent advances in NLP and speech processing have been driven by large scale self-supervised learning (SSL) models trained on gigantic amounts of unlabeled or weakly labeled data [1\u20139]. In speech processing specifically, SSL research has largely centered around speech representation learning, accomplished by training large speech encoders [3, 10\u201313] on tens of thousands of hours of unlabeled audio. By leveraging the abundant amounts of unlabeled data, SSL presents a scalable solution for extending the reach of speech technologies to new audiences. This can be most di- rectly accomplished via cross-lingual transfer learning, which aims to leverage information from other languages within a single model. However, due to the increased complexity and computational expenses of training with multiple languages, the methodologies used in multilingual SSL have often lagged behind the state-of-the-art, which remain centered around high- resource languages such as English [1, 3, 4, 14, 15]. As such, we propose a new massively multilingual SSL model: WavLabLM. WavLabLM extends the WavLM frame- work of jointly learning masked speech prediction and denois- ing to nearly 40k hours of speech across 136 languages. De- spite its relatively small dataset size [23, 24, 30], WavLabLM\u2019s pre-training data is curated for wide geographical coverage, the 15 languages with the most data span 7 different regions across the globe. For contrast, the entire top 15 languages used for XLS-R 128 [24] are European. To pre-train WavLabLM within academic constraints, we explore various techniques to make SSL more efficient. We first experimented with continual learning from English-based SSL models, halving the resources required for pre-training at the cost of performance in certain tasks. For multilingual SSL models trained from scratch, we propose a novel multi-stage pre-training approach to counteract imbalances in the language distribution of the data. This can be done by simply training on a large unbalanced dataset, before continual learning on a smaller dataset with a balanced language distribution. As a result, WavLabLM achieves comparable performance to XLS- R 128 on the 10-minute track of ML-SUPERB, despite only This can be evidenced via the SUPERB Benchmark [16], which aims to evaluate the effectiveness of different SSL mod- 1A mHuBERT [26] model was publicly released, but was only pre-trained on 3 languages: English, Spanish, and French, and on a much smaller scale. \u00a9 IEEE 2023 To appear in Proc. ASRU2023, December 16-20, 2023, Beitou, Taipei using 10% of the pre-training data, and even outperforms it in several tasks. To better fit SSL pre-training into academic constraints, we further slim down our approach. Using only 4 GPUs, we find that a simple HuBERT Base model is surprisingly efficient at multilingual SSL. Our CV-HuBERT, trained only on 92 languages from Common Voice [31], maintains up to 94% of the performance of XLS-R with only 3% of the training data and a third of the model parameters. In summary, our methods and results can be outlined as follows: We propose WavLabLM, an SSL model for 136 lan- guages and the first extension of joint denoising and prediction to multilingual SSL. We propose novel SSL techniques to make pre-training more efficient: continual pre-training from English mod- els and multi-stage pre-training for language balancing. We explore multi-resolution inputs in the multilingual setting, and show significant efficiency gains can be achieved with only a vanilla HuBERT Base model. Overall, WavLabLM achieves comparable performance to XLS-R 128 with only 10% of the pre-training data. Our CV-HuBERT maintains up to 94% of the perfor- mance with only 3% of the data. Finally, we include detailed descriptions of our engineering work that was required for scaling SSL, as a community con- tribution."}], "doc_text": "3 2 0 2 p e S 8 2 ] L C . s c [ 2 v 7 1 3 5 1 . 9 0 3 2 : v i X r a JOINT PREDICTION AND DENOISING FOR LARGE-SCALE MULTILINGUAL SELF-SUPERVISED LEARNING William Chen1, Jiatong Shi1, Brian Yan1, Dan Berrebbi1, Wangyou Zhang1,2, Yifan Peng1, Xuankai Chang1, Soumi Maiti1, Shinji Watanabe1 1Carnegie Mellon University, USA 2Shanghai Jiao Tong University, China ABSTRACT Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This fur- ther harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We pro- pose WavLabLM, which extends WavLM\u2019s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multi- lingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than 10% of the train- ing data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain 94% of XLS-R\u2019s performance with only 3% of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet. els across an array of speech processing tasks for English. While masked-prediction methods such as HuBERT [12] and WavLM [3] reign supreme on SUPERB, they are notably ab- sent in multilingual studies [17\u201322]. Instead, popular multilin- gual SSL models such as XLSR 53 [23], XLS-R 128 [24], and MMS [25] are built upon the wav2vec framework [10, 11]1, which is outperformed by the mentioned masked-prediction models above [3, 12, 15, 27, 28]. Our goal is to build a large-scale multilingual SSL model that uses state-of-the-art (SOTA) SSL methods while main- taining a reasonable computational budget. We believe that using more powerful techniques can actually lead to more effi- cient pre-training, as less data and compute is needed to reach similar levels of performance. By showing the feasibility of large-scale multilingual SSL without industrial compute, we hope to encourage more community-driven efforts in building state-of-the-art multilingual models and help pave the way for extending speech processing to new languages [7, 29]. Index Terms\u2014 SSL, HuBERT, WavLM, Multilingual 1. INTRODUCTION Recent advances in NLP and speech processing have been driven by large scale self-supervised learning (SSL) models trained on gigantic amounts of unlabeled or weakly labeled data [1\u20139]. In speech processing specifically, SSL research has largely centered around speech representation learning, accomplished by training large speech encoders [3, 10\u201313] on tens of thousands of hours of unlabeled audio. By leveraging the abundant amounts of unlabeled data, SSL presents a scalable solution for extending the reach of speech technologies to new audiences. This can be most di- rectly accomplished via cross-lingual transfer learning, which aims to leverage information from other languages within a single model. However, due to the increased complexity and computational expenses of training with multiple languages, the methodologies used in multilingual SSL have often lagged behind the state-of-the-art, which remain centered around high- resource languages such as English [1, 3, 4, 14, 15]. As such, we propose a new massively multilingual SSL model: WavLabLM. WavLabLM extends the WavLM frame- work of jointly learning masked speech prediction and denois- ing to nearly 40k hours of speech across 136 languages. De- spite its relatively small dataset size [23, 24, 30], WavLabLM\u2019s pre-training data is curated for wide geographical coverage, the 15 languages with the most data span 7 different regions across the globe. For contrast, the entire top 15 languages used for XLS-R 128 [24] are European. To pre-train WavLabLM within academic constraints, we explore various techniques to make SSL more efficient. We first experimented with continual learning from English-based SSL models, halving the resources required for pre-training at the cost of performance in certain tasks. For multilingual SSL models trained from scratch, we propose a novel multi-stage pre-training approach to counteract imbalances in the language distribution of the data. This can be done by simply training on a large unbalanced dataset, before continual learning on a smaller dataset with a balanced language distribution. As a result, WavLabLM achieves comparable performance to XLS- R 128 on the 10-minute track of ML-SUPERB, despite only This can be evidenced via the SUPERB Benchmark [16], which aims to evaluate the effectiveness of different SSL mod- 1A mHuBERT [26] model was publicly released, but was only pre-trained on 3 languages: English, Spanish, and French, and on a much smaller scale. \u00a9 IEEE 2023 To appear in Proc. ASRU2023, December 16-20, 2023, Beitou, Taipei using 10% of the pre-training data, and even outperforms it in several tasks. To better fit SSL pre-training into academic constraints, we further slim down our approach. Using only 4 GPUs, we find that a simple HuBERT Base model is surprisingly efficient at multilingual SSL. Our CV-HuBERT, trained only on 92 languages from Common Voice [31], maintains up to 94% of the performance of XLS-R with only 3% of the training data and a third of the model parameters. In summary, our methods and results can be outlined as follows: We propose WavLabLM, an SSL model for 136 lan- guages and the first extension of joint denoising and prediction to multilingual SSL. We propose novel SSL techniques to make pre-training more efficient: continual pre-training from English mod- els and multi-stage pre-training for language balancing. We explore multi-resolution inputs in the multilingual setting, and show significant efficiency gains can be achieved with only a vanilla HuBERT Base model. Overall, WavLabLM achieves comparable performance to XLS-R 128 with only 10% of the pre-training data. Our CV-HuBERT maintains up to 94% of the perfor- mance with only 3% of the data. Finally, we include detailed descriptions of our engineering work that was required for scaling SSL, as a community con- tribution."}