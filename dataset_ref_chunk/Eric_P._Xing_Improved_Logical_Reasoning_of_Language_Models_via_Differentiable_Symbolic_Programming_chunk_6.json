{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Improved_Logical_Reasoning_of_Language_Models_via_Differentiable_Symbolic_Programming_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " According to Table 5, how does the word2vec-based model compare to the transformer-based models in terms of performance?", "answer": " The word2vec-based model still has better performance than all other baselines.", "ref_chunk": "the reasoning performance. As shown in Table 5, the two transformer-based models have on-par perfor- mance and outperform the word2vec one. However, note that the word2vec-based model still has bet- ter performance than all other baselines. Besides higher \ufb01nal accuracy, the pre-trained transformer- based language model also accelerates the train- ing process. Both DSR-LM-RoBERTa and DSR- LM-DeBERTa reach their best performance within 20 epochs, while it takes DSR-w2v-BiLSTM 40 epochs to peak. Model DSR-LM (RoBERTa) DSR-LM-DeBERTa DSR-w2v-BiLSTM Accuracy (%) 60.98 \u00b1 2.64 60.92 \u00b1 2.72 40.39 \u00b1 0.06 Table 5: Ablation study about neural backbones of DSR-LM. We compare the CLUTRR performance of DSR-LM using different LMs. Incorporate domain knowledge. DSR-LM al- lows injecting domain speci\ufb01c knowledge. In DSR- LM-with-Rule, we manually crafted 92 rules for kinship reasoning to replace the learnt rules. As shown in Table 6, it obtained a 0.36% performance gain over DSR-LM. The fact that the improvement is marginal implies our method extracts useful rules to obtain on-par performance with manually crafted ones. DSR-LM-without-IC, our model without in- tegrity constraints speci\ufb01ed on predicted relations and rules, performs worse than DSR-LM, suggest- ing that logical integrity constraints are essential component for improving the model robustness. The impact of the relation extractor. To under- stand what causes the failure case of DSR-LM, we study the performance of our relation classi\ufb01cation Model DSR-LM DSR-LM-without-IC DSR-LM-with-Manual-Rule Accuracy (%) 60.98 \u00b1 2.64 51.48 \u00b1 0.57 61.34 \u00b1 1.56 Table 6: Ablation study. We compare our model\u2019s per- formance on CLUTRR with different setups. model separately. We isolate the trained relation extractor and found that it reaches 84.69% accu- racy on the single relation classi\ufb01cation task. For comparison, we train a relation extractor using all the intermediate labels in the training dataset, and it reaches 85.32% accuracy. It shows that even using only weak supervision (i.e., the \ufb01nal answers to multi-hop questions), our approach can reach on- par performance as supervised relation extraction. Reasoning over structured KBs. To understand the rule learning capability of our approach, we de- sign our ablation model DSR-without-LM to take as input ground-truth KBs instead of natural lan- guage. In this case, rule weights are not initialized by LM but randomized. As shown in Table 7, our model outperforms GAT and CTP which also op- erates on structured KBs. It demonstrates that our differentiable rule learning paradigm learns rules to reason about KBs consistently. Model GAT CTP DSR-without-LM Accuracy (%) 39.05 95.57 98.81 Table 7: DSR-without-LM compared against GAT and CTP on reasoning with ground truth KBs. For this com- parison we train on k \u2208 [2, 3] and test on k \u2208 [4, 10]. Failure cases of DSR-LM. We showcase in Ap- pendix Table 8 that even state-of-the-art large LMs are prone to logical fallacies. On the other hand, the failure case of our method usually occurs in the stage of relation extraction. For example, for the following sentence \u201cChristopher and Guillermina are having a father-daughter dance\u201d, our RoBERTa based relation extractor fails to recognize the father- daughter relationship but rather thinks C and G have a husband-wife relationship. We require most of the relation extraction to be correct in order to avoid cascading error. As the error rate on individ- ual relation extraction accumulates, it leads to the observed drop in accuracy as k becomes larger. 5 Concluding Remarks We investigate how to improve LMs\u2019 logical rea- soning capability using differentiable symbolic rea- soning. Through extensive experiments, we demon- strate the effectiveness of DSR-LM over challeng- ing scenarios where widely deployed large LMs fail to reason reliably. We hope our work can lay the groundwork for exploring neuro-symbolic pro- gramming techniques to improve the robustness of LMs on reasoning problems. References Gregor Betz, Christian Voigt, and Kyle Richardson. 2020. Critical thinking for language models. arXiv preprint arXiv:2009.07185. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. NeurIPS. Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Nat- ural language inference with natural language expla- nations. Advances in Neural Information Process- ing Systems, 31. Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, et al. 2021. Neurosymbolic Programming. Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In IJCAI. Antonia Creswell, Murray Shanahan, and Irina Hig- gins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi- lasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze, and Yoav Goldberg. 2021. Measuring and im- proving consistency in pretrained language models. TACL. Richard Evans and Edward Grefenstette. 2018. Learn- ing explanatory rules from noisy data. Journal of Arti\ufb01cial Intelligence Research, 61:1\u201364. Klaus Greff, Sjoerd Van Steenkiste, and J\u00fcrgen On the binding problem arXiv preprint Schmidhuber. 2020. in arti\ufb01cial neural networks. arXiv:2012.05208. Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2021. Do language mod- els have beliefs? methods for detecting, updat- ing, and visualizing model beliefs. arXiv preprint arXiv:2111.13654. Chadi Helwe, Chlo\u00e9 Clavel, and Fabian M. Suchanek. 2021. Reasoning with transformer-based models: Deep learning, but shallow reasoning. In AKBC. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation. Jiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, and Xujie Si. 2021. Scallop: From probabilistic deductive databases to scalable differentiable reasoning. NeurIPS. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec- tional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991. Drew A Hudson and Christopher D Manning. 2018. Compositional attention networks for machine rea- soning. In"}, {"question": " In the ablation study about neural backbones of DSR-LM, how many epochs does it take for DSR-w2v-BiLSTM to reach its peak performance?", "answer": " DSR-w2v-BiLSTM takes 40 epochs to peak.", "ref_chunk": "the reasoning performance. As shown in Table 5, the two transformer-based models have on-par perfor- mance and outperform the word2vec one. However, note that the word2vec-based model still has bet- ter performance than all other baselines. Besides higher \ufb01nal accuracy, the pre-trained transformer- based language model also accelerates the train- ing process. Both DSR-LM-RoBERTa and DSR- LM-DeBERTa reach their best performance within 20 epochs, while it takes DSR-w2v-BiLSTM 40 epochs to peak. Model DSR-LM (RoBERTa) DSR-LM-DeBERTa DSR-w2v-BiLSTM Accuracy (%) 60.98 \u00b1 2.64 60.92 \u00b1 2.72 40.39 \u00b1 0.06 Table 5: Ablation study about neural backbones of DSR-LM. We compare the CLUTRR performance of DSR-LM using different LMs. Incorporate domain knowledge. DSR-LM al- lows injecting domain speci\ufb01c knowledge. In DSR- LM-with-Rule, we manually crafted 92 rules for kinship reasoning to replace the learnt rules. As shown in Table 6, it obtained a 0.36% performance gain over DSR-LM. The fact that the improvement is marginal implies our method extracts useful rules to obtain on-par performance with manually crafted ones. DSR-LM-without-IC, our model without in- tegrity constraints speci\ufb01ed on predicted relations and rules, performs worse than DSR-LM, suggest- ing that logical integrity constraints are essential component for improving the model robustness. The impact of the relation extractor. To under- stand what causes the failure case of DSR-LM, we study the performance of our relation classi\ufb01cation Model DSR-LM DSR-LM-without-IC DSR-LM-with-Manual-Rule Accuracy (%) 60.98 \u00b1 2.64 51.48 \u00b1 0.57 61.34 \u00b1 1.56 Table 6: Ablation study. We compare our model\u2019s per- formance on CLUTRR with different setups. model separately. We isolate the trained relation extractor and found that it reaches 84.69% accu- racy on the single relation classi\ufb01cation task. For comparison, we train a relation extractor using all the intermediate labels in the training dataset, and it reaches 85.32% accuracy. It shows that even using only weak supervision (i.e., the \ufb01nal answers to multi-hop questions), our approach can reach on- par performance as supervised relation extraction. Reasoning over structured KBs. To understand the rule learning capability of our approach, we de- sign our ablation model DSR-without-LM to take as input ground-truth KBs instead of natural lan- guage. In this case, rule weights are not initialized by LM but randomized. As shown in Table 7, our model outperforms GAT and CTP which also op- erates on structured KBs. It demonstrates that our differentiable rule learning paradigm learns rules to reason about KBs consistently. Model GAT CTP DSR-without-LM Accuracy (%) 39.05 95.57 98.81 Table 7: DSR-without-LM compared against GAT and CTP on reasoning with ground truth KBs. For this com- parison we train on k \u2208 [2, 3] and test on k \u2208 [4, 10]. Failure cases of DSR-LM. We showcase in Ap- pendix Table 8 that even state-of-the-art large LMs are prone to logical fallacies. On the other hand, the failure case of our method usually occurs in the stage of relation extraction. For example, for the following sentence \u201cChristopher and Guillermina are having a father-daughter dance\u201d, our RoBERTa based relation extractor fails to recognize the father- daughter relationship but rather thinks C and G have a husband-wife relationship. We require most of the relation extraction to be correct in order to avoid cascading error. As the error rate on individ- ual relation extraction accumulates, it leads to the observed drop in accuracy as k becomes larger. 5 Concluding Remarks We investigate how to improve LMs\u2019 logical rea- soning capability using differentiable symbolic rea- soning. Through extensive experiments, we demon- strate the effectiveness of DSR-LM over challeng- ing scenarios where widely deployed large LMs fail to reason reliably. We hope our work can lay the groundwork for exploring neuro-symbolic pro- gramming techniques to improve the robustness of LMs on reasoning problems. References Gregor Betz, Christian Voigt, and Kyle Richardson. 2020. Critical thinking for language models. arXiv preprint arXiv:2009.07185. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. NeurIPS. Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Nat- ural language inference with natural language expla- nations. Advances in Neural Information Process- ing Systems, 31. Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, et al. 2021. Neurosymbolic Programming. Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In IJCAI. Antonia Creswell, Murray Shanahan, and Irina Hig- gins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi- lasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze, and Yoav Goldberg. 2021. Measuring and im- proving consistency in pretrained language models. TACL. Richard Evans and Edward Grefenstette. 2018. Learn- ing explanatory rules from noisy data. Journal of Arti\ufb01cial Intelligence Research, 61:1\u201364. Klaus Greff, Sjoerd Van Steenkiste, and J\u00fcrgen On the binding problem arXiv preprint Schmidhuber. 2020. in arti\ufb01cial neural networks. arXiv:2012.05208. Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2021. Do language mod- els have beliefs? methods for detecting, updat- ing, and visualizing model beliefs. arXiv preprint arXiv:2111.13654. Chadi Helwe, Chlo\u00e9 Clavel, and Fabian M. Suchanek. 2021. Reasoning with transformer-based models: Deep learning, but shallow reasoning. In AKBC. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation. Jiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, and Xujie Si. 2021. Scallop: From probabilistic deductive databases to scalable differentiable reasoning. NeurIPS. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec- tional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991. Drew A Hudson and Christopher D Manning. 2018. Compositional attention networks for machine rea- soning. In"}, {"question": " What is the accuracy of DSR-LM-DeBERTa model according to Table 5?", "answer": " 60.92 \u00b1 2.72", "ref_chunk": "the reasoning performance. As shown in Table 5, the two transformer-based models have on-par perfor- mance and outperform the word2vec one. However, note that the word2vec-based model still has bet- ter performance than all other baselines. Besides higher \ufb01nal accuracy, the pre-trained transformer- based language model also accelerates the train- ing process. Both DSR-LM-RoBERTa and DSR- LM-DeBERTa reach their best performance within 20 epochs, while it takes DSR-w2v-BiLSTM 40 epochs to peak. Model DSR-LM (RoBERTa) DSR-LM-DeBERTa DSR-w2v-BiLSTM Accuracy (%) 60.98 \u00b1 2.64 60.92 \u00b1 2.72 40.39 \u00b1 0.06 Table 5: Ablation study about neural backbones of DSR-LM. We compare the CLUTRR performance of DSR-LM using different LMs. Incorporate domain knowledge. DSR-LM al- lows injecting domain speci\ufb01c knowledge. In DSR- LM-with-Rule, we manually crafted 92 rules for kinship reasoning to replace the learnt rules. As shown in Table 6, it obtained a 0.36% performance gain over DSR-LM. The fact that the improvement is marginal implies our method extracts useful rules to obtain on-par performance with manually crafted ones. DSR-LM-without-IC, our model without in- tegrity constraints speci\ufb01ed on predicted relations and rules, performs worse than DSR-LM, suggest- ing that logical integrity constraints are essential component for improving the model robustness. The impact of the relation extractor. To under- stand what causes the failure case of DSR-LM, we study the performance of our relation classi\ufb01cation Model DSR-LM DSR-LM-without-IC DSR-LM-with-Manual-Rule Accuracy (%) 60.98 \u00b1 2.64 51.48 \u00b1 0.57 61.34 \u00b1 1.56 Table 6: Ablation study. We compare our model\u2019s per- formance on CLUTRR with different setups. model separately. We isolate the trained relation extractor and found that it reaches 84.69% accu- racy on the single relation classi\ufb01cation task. For comparison, we train a relation extractor using all the intermediate labels in the training dataset, and it reaches 85.32% accuracy. It shows that even using only weak supervision (i.e., the \ufb01nal answers to multi-hop questions), our approach can reach on- par performance as supervised relation extraction. Reasoning over structured KBs. To understand the rule learning capability of our approach, we de- sign our ablation model DSR-without-LM to take as input ground-truth KBs instead of natural lan- guage. In this case, rule weights are not initialized by LM but randomized. As shown in Table 7, our model outperforms GAT and CTP which also op- erates on structured KBs. It demonstrates that our differentiable rule learning paradigm learns rules to reason about KBs consistently. Model GAT CTP DSR-without-LM Accuracy (%) 39.05 95.57 98.81 Table 7: DSR-without-LM compared against GAT and CTP on reasoning with ground truth KBs. For this com- parison we train on k \u2208 [2, 3] and test on k \u2208 [4, 10]. Failure cases of DSR-LM. We showcase in Ap- pendix Table 8 that even state-of-the-art large LMs are prone to logical fallacies. On the other hand, the failure case of our method usually occurs in the stage of relation extraction. For example, for the following sentence \u201cChristopher and Guillermina are having a father-daughter dance\u201d, our RoBERTa based relation extractor fails to recognize the father- daughter relationship but rather thinks C and G have a husband-wife relationship. We require most of the relation extraction to be correct in order to avoid cascading error. As the error rate on individ- ual relation extraction accumulates, it leads to the observed drop in accuracy as k becomes larger. 5 Concluding Remarks We investigate how to improve LMs\u2019 logical rea- soning capability using differentiable symbolic rea- soning. Through extensive experiments, we demon- strate the effectiveness of DSR-LM over challeng- ing scenarios where widely deployed large LMs fail to reason reliably. We hope our work can lay the groundwork for exploring neuro-symbolic pro- gramming techniques to improve the robustness of LMs on reasoning problems. References Gregor Betz, Christian Voigt, and Kyle Richardson. 2020. Critical thinking for language models. arXiv preprint arXiv:2009.07185. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. NeurIPS. Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Nat- ural language inference with natural language expla- nations. Advances in Neural Information Process- ing Systems, 31. Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, et al. 2021. Neurosymbolic Programming. Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In IJCAI. Antonia Creswell, Murray Shanahan, and Irina Hig- gins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi- lasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze, and Yoav Goldberg. 2021. Measuring and im- proving consistency in pretrained language models. TACL. Richard Evans and Edward Grefenstette. 2018. Learn- ing explanatory rules from noisy data. Journal of Arti\ufb01cial Intelligence Research, 61:1\u201364. Klaus Greff, Sjoerd Van Steenkiste, and J\u00fcrgen On the binding problem arXiv preprint Schmidhuber. 2020. in arti\ufb01cial neural networks. arXiv:2012.05208. Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2021. Do language mod- els have beliefs? methods for detecting, updat- ing, and visualizing model beliefs. arXiv preprint arXiv:2111.13654. Chadi Helwe, Chlo\u00e9 Clavel, and Fabian M. Suchanek. 2021. Reasoning with transformer-based models: Deep learning, but shallow reasoning. In AKBC. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation. Jiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, and Xujie Si. 2021. Scallop: From probabilistic deductive databases to scalable differentiable reasoning. NeurIPS. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec- tional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991. Drew A Hudson and Christopher D Manning. 2018. Compositional attention networks for machine rea- soning. In"}, {"question": " What does DSR-LM allow in terms of injecting domain specific knowledge?", "answer": " DSR-LM allows injecting domain specific knowledge.", "ref_chunk": "the reasoning performance. As shown in Table 5, the two transformer-based models have on-par perfor- mance and outperform the word2vec one. However, note that the word2vec-based model still has bet- ter performance than all other baselines. Besides higher \ufb01nal accuracy, the pre-trained transformer- based language model also accelerates the train- ing process. Both DSR-LM-RoBERTa and DSR- LM-DeBERTa reach their best performance within 20 epochs, while it takes DSR-w2v-BiLSTM 40 epochs to peak. Model DSR-LM (RoBERTa) DSR-LM-DeBERTa DSR-w2v-BiLSTM Accuracy (%) 60.98 \u00b1 2.64 60.92 \u00b1 2.72 40.39 \u00b1 0.06 Table 5: Ablation study about neural backbones of DSR-LM. We compare the CLUTRR performance of DSR-LM using different LMs. Incorporate domain knowledge. DSR-LM al- lows injecting domain speci\ufb01c knowledge. In DSR- LM-with-Rule, we manually crafted 92 rules for kinship reasoning to replace the learnt rules. As shown in Table 6, it obtained a 0.36% performance gain over DSR-LM. The fact that the improvement is marginal implies our method extracts useful rules to obtain on-par performance with manually crafted ones. DSR-LM-without-IC, our model without in- tegrity constraints speci\ufb01ed on predicted relations and rules, performs worse than DSR-LM, suggest- ing that logical integrity constraints are essential component for improving the model robustness. The impact of the relation extractor. To under- stand what causes the failure case of DSR-LM, we study the performance of our relation classi\ufb01cation Model DSR-LM DSR-LM-without-IC DSR-LM-with-Manual-Rule Accuracy (%) 60.98 \u00b1 2.64 51.48 \u00b1 0.57 61.34 \u00b1 1.56 Table 6: Ablation study. We compare our model\u2019s per- formance on CLUTRR with different setups. model separately. We isolate the trained relation extractor and found that it reaches 84.69% accu- racy on the single relation classi\ufb01cation task. For comparison, we train a relation extractor using all the intermediate labels in the training dataset, and it reaches 85.32% accuracy. It shows that even using only weak supervision (i.e., the \ufb01nal answers to multi-hop questions), our approach can reach on- par performance as supervised relation extraction. Reasoning over structured KBs. To understand the rule learning capability of our approach, we de- sign our ablation model DSR-without-LM to take as input ground-truth KBs instead of natural lan- guage. In this case, rule weights are not initialized by LM but randomized. As shown in Table 7, our model outperforms GAT and CTP which also op- erates on structured KBs. It demonstrates that our differentiable rule learning paradigm learns rules to reason about KBs consistently. Model GAT CTP DSR-without-LM Accuracy (%) 39.05 95.57 98.81 Table 7: DSR-without-LM compared against GAT and CTP on reasoning with ground truth KBs. For this com- parison we train on k \u2208 [2, 3] and test on k \u2208 [4, 10]. Failure cases of DSR-LM. We showcase in Ap- pendix Table 8 that even state-of-the-art large LMs are prone to logical fallacies. On the other hand, the failure case of our method usually occurs in the stage of relation extraction. For example, for the following sentence \u201cChristopher and Guillermina are having a father-daughter dance\u201d, our RoBERTa based relation extractor fails to recognize the father- daughter relationship but rather thinks C and G have a husband-wife relationship. We require most of the relation extraction to be correct in order to avoid cascading error. As the error rate on individ- ual relation extraction accumulates, it leads to the observed drop in accuracy as k becomes larger. 5 Concluding Remarks We investigate how to improve LMs\u2019 logical rea- soning capability using differentiable symbolic rea- soning. Through extensive experiments, we demon- strate the effectiveness of DSR-LM over challeng- ing scenarios where widely deployed large LMs fail to reason reliably. We hope our work can lay the groundwork for exploring neuro-symbolic pro- gramming techniques to improve the robustness of LMs on reasoning problems. References Gregor Betz, Christian Voigt, and Kyle Richardson. 2020. Critical thinking for language models. arXiv preprint arXiv:2009.07185. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. NeurIPS. Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Nat- ural language inference with natural language expla- nations. Advances in Neural Information Process- ing Systems, 31. Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, et al. 2021. Neurosymbolic Programming. Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In IJCAI. Antonia Creswell, Murray Shanahan, and Irina Hig- gins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi- lasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze, and Yoav Goldberg. 2021. Measuring and im- proving consistency in pretrained language models. TACL. Richard Evans and Edward Grefenstette. 2018. Learn- ing explanatory rules from noisy data. Journal of Arti\ufb01cial Intelligence Research, 61:1\u201364. Klaus Greff, Sjoerd Van Steenkiste, and J\u00fcrgen On the binding problem arXiv preprint Schmidhuber. 2020. in arti\ufb01cial neural networks. arXiv:2012.05208. Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2021. Do language mod- els have beliefs? methods for detecting, updat- ing, and visualizing model beliefs. arXiv preprint arXiv:2111.13654. Chadi Helwe, Chlo\u00e9 Clavel, and Fabian M. Suchanek. 2021. Reasoning with transformer-based models: Deep learning, but shallow reasoning. In AKBC. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation. Jiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, and Xujie Si. 2021. Scallop: From probabilistic deductive databases to scalable differentiable reasoning. NeurIPS. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec- tional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991. Drew A Hudson and Christopher D Manning. 2018. Compositional attention networks for machine rea- soning. In"}, {"question": " According to the study on failure cases of DSR-LM, what usually causes the failure of the method?", "answer": " The failure case of the method usually occurs in the stage of relation extraction.", "ref_chunk": "the reasoning performance. As shown in Table 5, the two transformer-based models have on-par perfor- mance and outperform the word2vec one. However, note that the word2vec-based model still has bet- ter performance than all other baselines. Besides higher \ufb01nal accuracy, the pre-trained transformer- based language model also accelerates the train- ing process. Both DSR-LM-RoBERTa and DSR- LM-DeBERTa reach their best performance within 20 epochs, while it takes DSR-w2v-BiLSTM 40 epochs to peak. Model DSR-LM (RoBERTa) DSR-LM-DeBERTa DSR-w2v-BiLSTM Accuracy (%) 60.98 \u00b1 2.64 60.92 \u00b1 2.72 40.39 \u00b1 0.06 Table 5: Ablation study about neural backbones of DSR-LM. We compare the CLUTRR performance of DSR-LM using different LMs. Incorporate domain knowledge. DSR-LM al- lows injecting domain speci\ufb01c knowledge. In DSR- LM-with-Rule, we manually crafted 92 rules for kinship reasoning to replace the learnt rules. As shown in Table 6, it obtained a 0.36% performance gain over DSR-LM. The fact that the improvement is marginal implies our method extracts useful rules to obtain on-par performance with manually crafted ones. DSR-LM-without-IC, our model without in- tegrity constraints speci\ufb01ed on predicted relations and rules, performs worse than DSR-LM, suggest- ing that logical integrity constraints are essential component for improving the model robustness. The impact of the relation extractor. To under- stand what causes the failure case of DSR-LM, we study the performance of our relation classi\ufb01cation Model DSR-LM DSR-LM-without-IC DSR-LM-with-Manual-Rule Accuracy (%) 60.98 \u00b1 2.64 51.48 \u00b1 0.57 61.34 \u00b1 1.56 Table 6: Ablation study. We compare our model\u2019s per- formance on CLUTRR with different setups. model separately. We isolate the trained relation extractor and found that it reaches 84.69% accu- racy on the single relation classi\ufb01cation task. For comparison, we train a relation extractor using all the intermediate labels in the training dataset, and it reaches 85.32% accuracy. It shows that even using only weak supervision (i.e., the \ufb01nal answers to multi-hop questions), our approach can reach on- par performance as supervised relation extraction. Reasoning over structured KBs. To understand the rule learning capability of our approach, we de- sign our ablation model DSR-without-LM to take as input ground-truth KBs instead of natural lan- guage. In this case, rule weights are not initialized by LM but randomized. As shown in Table 7, our model outperforms GAT and CTP which also op- erates on structured KBs. It demonstrates that our differentiable rule learning paradigm learns rules to reason about KBs consistently. Model GAT CTP DSR-without-LM Accuracy (%) 39.05 95.57 98.81 Table 7: DSR-without-LM compared against GAT and CTP on reasoning with ground truth KBs. For this com- parison we train on k \u2208 [2, 3] and test on k \u2208 [4, 10]. Failure cases of DSR-LM. We showcase in Ap- pendix Table 8 that even state-of-the-art large LMs are prone to logical fallacies. On the other hand, the failure case of our method usually occurs in the stage of relation extraction. For example, for the following sentence \u201cChristopher and Guillermina are having a father-daughter dance\u201d, our RoBERTa based relation extractor fails to recognize the father- daughter relationship but rather thinks C and G have a husband-wife relationship. We require most of the relation extraction to be correct in order to avoid cascading error. As the error rate on individ- ual relation extraction accumulates, it leads to the observed drop in accuracy as k becomes larger. 5 Concluding Remarks We investigate how to improve LMs\u2019 logical rea- soning capability using differentiable symbolic rea- soning. Through extensive experiments, we demon- strate the effectiveness of DSR-LM over challeng- ing scenarios where widely deployed large LMs fail to reason reliably. We hope our work can lay the groundwork for exploring neuro-symbolic pro- gramming techniques to improve the robustness of LMs on reasoning problems. References Gregor Betz, Christian Voigt, and Kyle Richardson. 2020. Critical thinking for language models. arXiv preprint arXiv:2009.07185. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. NeurIPS. Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Nat- ural language inference with natural language expla- nations. Advances in Neural Information Process- ing Systems, 31. Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, et al. 2021. Neurosymbolic Programming. Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In IJCAI. Antonia Creswell, Murray Shanahan, and Irina Hig- gins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi- lasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze, and Yoav Goldberg. 2021. Measuring and im- proving consistency in pretrained language models. TACL. Richard Evans and Edward Grefenstette. 2018. Learn- ing explanatory rules from noisy data. Journal of Arti\ufb01cial Intelligence Research, 61:1\u201364. Klaus Greff, Sjoerd Van Steenkiste, and J\u00fcrgen On the binding problem arXiv preprint Schmidhuber. 2020. in arti\ufb01cial neural networks. arXiv:2012.05208. Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2021. Do language mod- els have beliefs? methods for detecting, updat- ing, and visualizing model beliefs. arXiv preprint arXiv:2111.13654. Chadi Helwe, Chlo\u00e9 Clavel, and Fabian M. Suchanek. 2021. Reasoning with transformer-based models: Deep learning, but shallow reasoning. In AKBC. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation. Jiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, and Xujie Si. 2021. Scallop: From probabilistic deductive databases to scalable differentiable reasoning. NeurIPS. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec- tional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991. Drew A Hudson and Christopher D Manning. 2018. Compositional attention networks for machine rea- soning. In"}, {"question": " In the comparison of DSR-without-LM against GAT and CTP on reasoning with ground truth KBs, which model has the highest accuracy?", "answer": " DSR-without-LM has the highest accuracy with 98.81%.", "ref_chunk": "the reasoning performance. As shown in Table 5, the two transformer-based models have on-par perfor- mance and outperform the word2vec one. However, note that the word2vec-based model still has bet- ter performance than all other baselines. Besides higher \ufb01nal accuracy, the pre-trained transformer- based language model also accelerates the train- ing process. Both DSR-LM-RoBERTa and DSR- LM-DeBERTa reach their best performance within 20 epochs, while it takes DSR-w2v-BiLSTM 40 epochs to peak. Model DSR-LM (RoBERTa) DSR-LM-DeBERTa DSR-w2v-BiLSTM Accuracy (%) 60.98 \u00b1 2.64 60.92 \u00b1 2.72 40.39 \u00b1 0.06 Table 5: Ablation study about neural backbones of DSR-LM. We compare the CLUTRR performance of DSR-LM using different LMs. Incorporate domain knowledge. DSR-LM al- lows injecting domain speci\ufb01c knowledge. In DSR- LM-with-Rule, we manually crafted 92 rules for kinship reasoning to replace the learnt rules. As shown in Table 6, it obtained a 0.36% performance gain over DSR-LM. The fact that the improvement is marginal implies our method extracts useful rules to obtain on-par performance with manually crafted ones. DSR-LM-without-IC, our model without in- tegrity constraints speci\ufb01ed on predicted relations and rules, performs worse than DSR-LM, suggest- ing that logical integrity constraints are essential component for improving the model robustness. The impact of the relation extractor. To under- stand what causes the failure case of DSR-LM, we study the performance of our relation classi\ufb01cation Model DSR-LM DSR-LM-without-IC DSR-LM-with-Manual-Rule Accuracy (%) 60.98 \u00b1 2.64 51.48 \u00b1 0.57 61.34 \u00b1 1.56 Table 6: Ablation study. We compare our model\u2019s per- formance on CLUTRR with different setups. model separately. We isolate the trained relation extractor and found that it reaches 84.69% accu- racy on the single relation classi\ufb01cation task. For comparison, we train a relation extractor using all the intermediate labels in the training dataset, and it reaches 85.32% accuracy. It shows that even using only weak supervision (i.e., the \ufb01nal answers to multi-hop questions), our approach can reach on- par performance as supervised relation extraction. Reasoning over structured KBs. To understand the rule learning capability of our approach, we de- sign our ablation model DSR-without-LM to take as input ground-truth KBs instead of natural lan- guage. In this case, rule weights are not initialized by LM but randomized. As shown in Table 7, our model outperforms GAT and CTP which also op- erates on structured KBs. It demonstrates that our differentiable rule learning paradigm learns rules to reason about KBs consistently. Model GAT CTP DSR-without-LM Accuracy (%) 39.05 95.57 98.81 Table 7: DSR-without-LM compared against GAT and CTP on reasoning with ground truth KBs. For this com- parison we train on k \u2208 [2, 3] and test on k \u2208 [4, 10]. Failure cases of DSR-LM. We showcase in Ap- pendix Table 8 that even state-of-the-art large LMs are prone to logical fallacies. On the other hand, the failure case of our method usually occurs in the stage of relation extraction. For example, for the following sentence \u201cChristopher and Guillermina are having a father-daughter dance\u201d, our RoBERTa based relation extractor fails to recognize the father- daughter relationship but rather thinks C and G have a husband-wife relationship. We require most of the relation extraction to be correct in order to avoid cascading error. As the error rate on individ- ual relation extraction accumulates, it leads to the observed drop in accuracy as k becomes larger. 5 Concluding Remarks We investigate how to improve LMs\u2019 logical rea- soning capability using differentiable symbolic rea- soning. Through extensive experiments, we demon- strate the effectiveness of DSR-LM over challeng- ing scenarios where widely deployed large LMs fail to reason reliably. We hope our work can lay the groundwork for exploring neuro-symbolic pro- gramming techniques to improve the robustness of LMs on reasoning problems. References Gregor Betz, Christian Voigt, and Kyle Richardson. 2020. Critical thinking for language models. arXiv preprint arXiv:2009.07185. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. NeurIPS. Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Nat- ural language inference with natural language expla- nations. Advances in Neural Information Process- ing Systems, 31. Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, et al. 2021. Neurosymbolic Programming. Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In IJCAI. Antonia Creswell, Murray Shanahan, and Irina Hig- gins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi- lasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze, and Yoav Goldberg. 2021. Measuring and im- proving consistency in pretrained language models. TACL. Richard Evans and Edward Grefenstette. 2018. Learn- ing explanatory rules from noisy data. Journal of Arti\ufb01cial Intelligence Research, 61:1\u201364. Klaus Greff, Sjoerd Van Steenkiste, and J\u00fcrgen On the binding problem arXiv preprint Schmidhuber. 2020. in arti\ufb01cial neural networks. arXiv:2012.05208. Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2021. Do language mod- els have beliefs? methods for detecting, updat- ing, and visualizing model beliefs. arXiv preprint arXiv:2111.13654. Chadi Helwe, Chlo\u00e9 Clavel, and Fabian M. Suchanek. 2021. Reasoning with transformer-based models: Deep learning, but shallow reasoning. In AKBC. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation. Jiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, and Xujie Si. 2021. Scallop: From probabilistic deductive databases to scalable differentiable reasoning. NeurIPS. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec- tional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991. Drew A Hudson and Christopher D Manning. 2018. Compositional attention networks for machine rea- soning. In"}, {"question": " What is the purpose of the ablation model DSR-without-LM in terms of input?", "answer": " The model takes ground-truth KBs instead of natural language as input.", "ref_chunk": "the reasoning performance. As shown in Table 5, the two transformer-based models have on-par perfor- mance and outperform the word2vec one. However, note that the word2vec-based model still has bet- ter performance than all other baselines. Besides higher \ufb01nal accuracy, the pre-trained transformer- based language model also accelerates the train- ing process. Both DSR-LM-RoBERTa and DSR- LM-DeBERTa reach their best performance within 20 epochs, while it takes DSR-w2v-BiLSTM 40 epochs to peak. Model DSR-LM (RoBERTa) DSR-LM-DeBERTa DSR-w2v-BiLSTM Accuracy (%) 60.98 \u00b1 2.64 60.92 \u00b1 2.72 40.39 \u00b1 0.06 Table 5: Ablation study about neural backbones of DSR-LM. We compare the CLUTRR performance of DSR-LM using different LMs. Incorporate domain knowledge. DSR-LM al- lows injecting domain speci\ufb01c knowledge. In DSR- LM-with-Rule, we manually crafted 92 rules for kinship reasoning to replace the learnt rules. As shown in Table 6, it obtained a 0.36% performance gain over DSR-LM. The fact that the improvement is marginal implies our method extracts useful rules to obtain on-par performance with manually crafted ones. DSR-LM-without-IC, our model without in- tegrity constraints speci\ufb01ed on predicted relations and rules, performs worse than DSR-LM, suggest- ing that logical integrity constraints are essential component for improving the model robustness. The impact of the relation extractor. To under- stand what causes the failure case of DSR-LM, we study the performance of our relation classi\ufb01cation Model DSR-LM DSR-LM-without-IC DSR-LM-with-Manual-Rule Accuracy (%) 60.98 \u00b1 2.64 51.48 \u00b1 0.57 61.34 \u00b1 1.56 Table 6: Ablation study. We compare our model\u2019s per- formance on CLUTRR with different setups. model separately. We isolate the trained relation extractor and found that it reaches 84.69% accu- racy on the single relation classi\ufb01cation task. For comparison, we train a relation extractor using all the intermediate labels in the training dataset, and it reaches 85.32% accuracy. It shows that even using only weak supervision (i.e., the \ufb01nal answers to multi-hop questions), our approach can reach on- par performance as supervised relation extraction. Reasoning over structured KBs. To understand the rule learning capability of our approach, we de- sign our ablation model DSR-without-LM to take as input ground-truth KBs instead of natural lan- guage. In this case, rule weights are not initialized by LM but randomized. As shown in Table 7, our model outperforms GAT and CTP which also op- erates on structured KBs. It demonstrates that our differentiable rule learning paradigm learns rules to reason about KBs consistently. Model GAT CTP DSR-without-LM Accuracy (%) 39.05 95.57 98.81 Table 7: DSR-without-LM compared against GAT and CTP on reasoning with ground truth KBs. For this com- parison we train on k \u2208 [2, 3] and test on k \u2208 [4, 10]. Failure cases of DSR-LM. We showcase in Ap- pendix Table 8 that even state-of-the-art large LMs are prone to logical fallacies. On the other hand, the failure case of our method usually occurs in the stage of relation extraction. For example, for the following sentence \u201cChristopher and Guillermina are having a father-daughter dance\u201d, our RoBERTa based relation extractor fails to recognize the father- daughter relationship but rather thinks C and G have a husband-wife relationship. We require most of the relation extraction to be correct in order to avoid cascading error. As the error rate on individ- ual relation extraction accumulates, it leads to the observed drop in accuracy as k becomes larger. 5 Concluding Remarks We investigate how to improve LMs\u2019 logical rea- soning capability using differentiable symbolic rea- soning. Through extensive experiments, we demon- strate the effectiveness of DSR-LM over challeng- ing scenarios where widely deployed large LMs fail to reason reliably. We hope our work can lay the groundwork for exploring neuro-symbolic pro- gramming techniques to improve the robustness of LMs on reasoning problems. References Gregor Betz, Christian Voigt, and Kyle Richardson. 2020. Critical thinking for language models. arXiv preprint arXiv:2009.07185. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. NeurIPS. Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Nat- ural language inference with natural language expla- nations. Advances in Neural Information Process- ing Systems, 31. Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, et al. 2021. Neurosymbolic Programming. Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In IJCAI. Antonia Creswell, Murray Shanahan, and Irina Hig- gins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi- lasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze, and Yoav Goldberg. 2021. Measuring and im- proving consistency in pretrained language models. TACL. Richard Evans and Edward Grefenstette. 2018. Learn- ing explanatory rules from noisy data. Journal of Arti\ufb01cial Intelligence Research, 61:1\u201364. Klaus Greff, Sjoerd Van Steenkiste, and J\u00fcrgen On the binding problem arXiv preprint Schmidhuber. 2020. in arti\ufb01cial neural networks. arXiv:2012.05208. Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2021. Do language mod- els have beliefs? methods for detecting, updat- ing, and visualizing model beliefs. arXiv preprint arXiv:2111.13654. Chadi Helwe, Chlo\u00e9 Clavel, and Fabian M. Suchanek. 2021. Reasoning with transformer-based models: Deep learning, but shallow reasoning. In AKBC. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation. Jiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, and Xujie Si. 2021. Scallop: From probabilistic deductive databases to scalable differentiable reasoning. NeurIPS. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec- tional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991. Drew A Hudson and Christopher D Manning. 2018. Compositional attention networks for machine rea- soning. In"}, {"question": " In the conclusion, what does the text hope to achieve through the work on DSR-LM?", "answer": " The text hopes the work can lay the groundwork for exploring neuro-symbolic programming techniques to improve the robustness of LMs on reasoning problems.", "ref_chunk": "the reasoning performance. As shown in Table 5, the two transformer-based models have on-par perfor- mance and outperform the word2vec one. However, note that the word2vec-based model still has bet- ter performance than all other baselines. Besides higher \ufb01nal accuracy, the pre-trained transformer- based language model also accelerates the train- ing process. Both DSR-LM-RoBERTa and DSR- LM-DeBERTa reach their best performance within 20 epochs, while it takes DSR-w2v-BiLSTM 40 epochs to peak. Model DSR-LM (RoBERTa) DSR-LM-DeBERTa DSR-w2v-BiLSTM Accuracy (%) 60.98 \u00b1 2.64 60.92 \u00b1 2.72 40.39 \u00b1 0.06 Table 5: Ablation study about neural backbones of DSR-LM. We compare the CLUTRR performance of DSR-LM using different LMs. Incorporate domain knowledge. DSR-LM al- lows injecting domain speci\ufb01c knowledge. In DSR- LM-with-Rule, we manually crafted 92 rules for kinship reasoning to replace the learnt rules. As shown in Table 6, it obtained a 0.36% performance gain over DSR-LM. The fact that the improvement is marginal implies our method extracts useful rules to obtain on-par performance with manually crafted ones. DSR-LM-without-IC, our model without in- tegrity constraints speci\ufb01ed on predicted relations and rules, performs worse than DSR-LM, suggest- ing that logical integrity constraints are essential component for improving the model robustness. The impact of the relation extractor. To under- stand what causes the failure case of DSR-LM, we study the performance of our relation classi\ufb01cation Model DSR-LM DSR-LM-without-IC DSR-LM-with-Manual-Rule Accuracy (%) 60.98 \u00b1 2.64 51.48 \u00b1 0.57 61.34 \u00b1 1.56 Table 6: Ablation study. We compare our model\u2019s per- formance on CLUTRR with different setups. model separately. We isolate the trained relation extractor and found that it reaches 84.69% accu- racy on the single relation classi\ufb01cation task. For comparison, we train a relation extractor using all the intermediate labels in the training dataset, and it reaches 85.32% accuracy. It shows that even using only weak supervision (i.e., the \ufb01nal answers to multi-hop questions), our approach can reach on- par performance as supervised relation extraction. Reasoning over structured KBs. To understand the rule learning capability of our approach, we de- sign our ablation model DSR-without-LM to take as input ground-truth KBs instead of natural lan- guage. In this case, rule weights are not initialized by LM but randomized. As shown in Table 7, our model outperforms GAT and CTP which also op- erates on structured KBs. It demonstrates that our differentiable rule learning paradigm learns rules to reason about KBs consistently. Model GAT CTP DSR-without-LM Accuracy (%) 39.05 95.57 98.81 Table 7: DSR-without-LM compared against GAT and CTP on reasoning with ground truth KBs. For this com- parison we train on k \u2208 [2, 3] and test on k \u2208 [4, 10]. Failure cases of DSR-LM. We showcase in Ap- pendix Table 8 that even state-of-the-art large LMs are prone to logical fallacies. On the other hand, the failure case of our method usually occurs in the stage of relation extraction. For example, for the following sentence \u201cChristopher and Guillermina are having a father-daughter dance\u201d, our RoBERTa based relation extractor fails to recognize the father- daughter relationship but rather thinks C and G have a husband-wife relationship. We require most of the relation extraction to be correct in order to avoid cascading error. As the error rate on individ- ual relation extraction accumulates, it leads to the observed drop in accuracy as k becomes larger. 5 Concluding Remarks We investigate how to improve LMs\u2019 logical rea- soning capability using differentiable symbolic rea- soning. Through extensive experiments, we demon- strate the effectiveness of DSR-LM over challeng- ing scenarios where widely deployed large LMs fail to reason reliably. We hope our work can lay the groundwork for exploring neuro-symbolic pro- gramming techniques to improve the robustness of LMs on reasoning problems. References Gregor Betz, Christian Voigt, and Kyle Richardson. 2020. Critical thinking for language models. arXiv preprint arXiv:2009.07185. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. NeurIPS. Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Nat- ural language inference with natural language expla- nations. Advances in Neural Information Process- ing Systems, 31. Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, et al. 2021. Neurosymbolic Programming. Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In IJCAI. Antonia Creswell, Murray Shanahan, and Irina Hig- gins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi- lasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze, and Yoav Goldberg. 2021. Measuring and im- proving consistency in pretrained language models. TACL. Richard Evans and Edward Grefenstette. 2018. Learn- ing explanatory rules from noisy data. Journal of Arti\ufb01cial Intelligence Research, 61:1\u201364. Klaus Greff, Sjoerd Van Steenkiste, and J\u00fcrgen On the binding problem arXiv preprint Schmidhuber. 2020. in arti\ufb01cial neural networks. arXiv:2012.05208. Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2021. Do language mod- els have beliefs? methods for detecting, updat- ing, and visualizing model beliefs. arXiv preprint arXiv:2111.13654. Chadi Helwe, Chlo\u00e9 Clavel, and Fabian M. Suchanek. 2021. Reasoning with transformer-based models: Deep learning, but shallow reasoning. In AKBC. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation. Jiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, and Xujie Si. 2021. Scallop: From probabilistic deductive databases to scalable differentiable reasoning. NeurIPS. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec- tional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991. Drew A Hudson and Christopher D Manning. 2018. Compositional attention networks for machine rea- soning. In"}, {"question": " Which reference discusses the topic of language models as few-shot learners?", "answer": " Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. in NeurIPS 2020.", "ref_chunk": "the reasoning performance. As shown in Table 5, the two transformer-based models have on-par perfor- mance and outperform the word2vec one. However, note that the word2vec-based model still has bet- ter performance than all other baselines. Besides higher \ufb01nal accuracy, the pre-trained transformer- based language model also accelerates the train- ing process. Both DSR-LM-RoBERTa and DSR- LM-DeBERTa reach their best performance within 20 epochs, while it takes DSR-w2v-BiLSTM 40 epochs to peak. Model DSR-LM (RoBERTa) DSR-LM-DeBERTa DSR-w2v-BiLSTM Accuracy (%) 60.98 \u00b1 2.64 60.92 \u00b1 2.72 40.39 \u00b1 0.06 Table 5: Ablation study about neural backbones of DSR-LM. We compare the CLUTRR performance of DSR-LM using different LMs. Incorporate domain knowledge. DSR-LM al- lows injecting domain speci\ufb01c knowledge. In DSR- LM-with-Rule, we manually crafted 92 rules for kinship reasoning to replace the learnt rules. As shown in Table 6, it obtained a 0.36% performance gain over DSR-LM. The fact that the improvement is marginal implies our method extracts useful rules to obtain on-par performance with manually crafted ones. DSR-LM-without-IC, our model without in- tegrity constraints speci\ufb01ed on predicted relations and rules, performs worse than DSR-LM, suggest- ing that logical integrity constraints are essential component for improving the model robustness. The impact of the relation extractor. To under- stand what causes the failure case of DSR-LM, we study the performance of our relation classi\ufb01cation Model DSR-LM DSR-LM-without-IC DSR-LM-with-Manual-Rule Accuracy (%) 60.98 \u00b1 2.64 51.48 \u00b1 0.57 61.34 \u00b1 1.56 Table 6: Ablation study. We compare our model\u2019s per- formance on CLUTRR with different setups. model separately. We isolate the trained relation extractor and found that it reaches 84.69% accu- racy on the single relation classi\ufb01cation task. For comparison, we train a relation extractor using all the intermediate labels in the training dataset, and it reaches 85.32% accuracy. It shows that even using only weak supervision (i.e., the \ufb01nal answers to multi-hop questions), our approach can reach on- par performance as supervised relation extraction. Reasoning over structured KBs. To understand the rule learning capability of our approach, we de- sign our ablation model DSR-without-LM to take as input ground-truth KBs instead of natural lan- guage. In this case, rule weights are not initialized by LM but randomized. As shown in Table 7, our model outperforms GAT and CTP which also op- erates on structured KBs. It demonstrates that our differentiable rule learning paradigm learns rules to reason about KBs consistently. Model GAT CTP DSR-without-LM Accuracy (%) 39.05 95.57 98.81 Table 7: DSR-without-LM compared against GAT and CTP on reasoning with ground truth KBs. For this com- parison we train on k \u2208 [2, 3] and test on k \u2208 [4, 10]. Failure cases of DSR-LM. We showcase in Ap- pendix Table 8 that even state-of-the-art large LMs are prone to logical fallacies. On the other hand, the failure case of our method usually occurs in the stage of relation extraction. For example, for the following sentence \u201cChristopher and Guillermina are having a father-daughter dance\u201d, our RoBERTa based relation extractor fails to recognize the father- daughter relationship but rather thinks C and G have a husband-wife relationship. We require most of the relation extraction to be correct in order to avoid cascading error. As the error rate on individ- ual relation extraction accumulates, it leads to the observed drop in accuracy as k becomes larger. 5 Concluding Remarks We investigate how to improve LMs\u2019 logical rea- soning capability using differentiable symbolic rea- soning. Through extensive experiments, we demon- strate the effectiveness of DSR-LM over challeng- ing scenarios where widely deployed large LMs fail to reason reliably. We hope our work can lay the groundwork for exploring neuro-symbolic pro- gramming techniques to improve the robustness of LMs on reasoning problems. References Gregor Betz, Christian Voigt, and Kyle Richardson. 2020. Critical thinking for language models. arXiv preprint arXiv:2009.07185. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. NeurIPS. Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Nat- ural language inference with natural language expla- nations. Advances in Neural Information Process- ing Systems, 31. Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, et al. 2021. Neurosymbolic Programming. Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In IJCAI. Antonia Creswell, Murray Shanahan, and Irina Hig- gins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi- lasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze, and Yoav Goldberg. 2021. Measuring and im- proving consistency in pretrained language models. TACL. Richard Evans and Edward Grefenstette. 2018. Learn- ing explanatory rules from noisy data. Journal of Arti\ufb01cial Intelligence Research, 61:1\u201364. Klaus Greff, Sjoerd Van Steenkiste, and J\u00fcrgen On the binding problem arXiv preprint Schmidhuber. 2020. in arti\ufb01cial neural networks. arXiv:2012.05208. Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2021. Do language mod- els have beliefs? methods for detecting, updat- ing, and visualizing model beliefs. arXiv preprint arXiv:2111.13654. Chadi Helwe, Chlo\u00e9 Clavel, and Fabian M. Suchanek. 2021. Reasoning with transformer-based models: Deep learning, but shallow reasoning. In AKBC. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation. Jiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, and Xujie Si. 2021. Scallop: From probabilistic deductive databases to scalable differentiable reasoning. NeurIPS. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec- tional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991. Drew A Hudson and Christopher D Manning. 2018. Compositional attention networks for machine rea- soning. In"}, {"question": " What is the accuracy of the trained relation extractor on the single relation classification task?", "answer": " The trained relation extractor reaches 84.69% accuracy on the single relation classification task.", "ref_chunk": "the reasoning performance. As shown in Table 5, the two transformer-based models have on-par perfor- mance and outperform the word2vec one. However, note that the word2vec-based model still has bet- ter performance than all other baselines. Besides higher \ufb01nal accuracy, the pre-trained transformer- based language model also accelerates the train- ing process. Both DSR-LM-RoBERTa and DSR- LM-DeBERTa reach their best performance within 20 epochs, while it takes DSR-w2v-BiLSTM 40 epochs to peak. Model DSR-LM (RoBERTa) DSR-LM-DeBERTa DSR-w2v-BiLSTM Accuracy (%) 60.98 \u00b1 2.64 60.92 \u00b1 2.72 40.39 \u00b1 0.06 Table 5: Ablation study about neural backbones of DSR-LM. We compare the CLUTRR performance of DSR-LM using different LMs. Incorporate domain knowledge. DSR-LM al- lows injecting domain speci\ufb01c knowledge. In DSR- LM-with-Rule, we manually crafted 92 rules for kinship reasoning to replace the learnt rules. As shown in Table 6, it obtained a 0.36% performance gain over DSR-LM. The fact that the improvement is marginal implies our method extracts useful rules to obtain on-par performance with manually crafted ones. DSR-LM-without-IC, our model without in- tegrity constraints speci\ufb01ed on predicted relations and rules, performs worse than DSR-LM, suggest- ing that logical integrity constraints are essential component for improving the model robustness. The impact of the relation extractor. To under- stand what causes the failure case of DSR-LM, we study the performance of our relation classi\ufb01cation Model DSR-LM DSR-LM-without-IC DSR-LM-with-Manual-Rule Accuracy (%) 60.98 \u00b1 2.64 51.48 \u00b1 0.57 61.34 \u00b1 1.56 Table 6: Ablation study. We compare our model\u2019s per- formance on CLUTRR with different setups. model separately. We isolate the trained relation extractor and found that it reaches 84.69% accu- racy on the single relation classi\ufb01cation task. For comparison, we train a relation extractor using all the intermediate labels in the training dataset, and it reaches 85.32% accuracy. It shows that even using only weak supervision (i.e., the \ufb01nal answers to multi-hop questions), our approach can reach on- par performance as supervised relation extraction. Reasoning over structured KBs. To understand the rule learning capability of our approach, we de- sign our ablation model DSR-without-LM to take as input ground-truth KBs instead of natural lan- guage. In this case, rule weights are not initialized by LM but randomized. As shown in Table 7, our model outperforms GAT and CTP which also op- erates on structured KBs. It demonstrates that our differentiable rule learning paradigm learns rules to reason about KBs consistently. Model GAT CTP DSR-without-LM Accuracy (%) 39.05 95.57 98.81 Table 7: DSR-without-LM compared against GAT and CTP on reasoning with ground truth KBs. For this com- parison we train on k \u2208 [2, 3] and test on k \u2208 [4, 10]. Failure cases of DSR-LM. We showcase in Ap- pendix Table 8 that even state-of-the-art large LMs are prone to logical fallacies. On the other hand, the failure case of our method usually occurs in the stage of relation extraction. For example, for the following sentence \u201cChristopher and Guillermina are having a father-daughter dance\u201d, our RoBERTa based relation extractor fails to recognize the father- daughter relationship but rather thinks C and G have a husband-wife relationship. We require most of the relation extraction to be correct in order to avoid cascading error. As the error rate on individ- ual relation extraction accumulates, it leads to the observed drop in accuracy as k becomes larger. 5 Concluding Remarks We investigate how to improve LMs\u2019 logical rea- soning capability using differentiable symbolic rea- soning. Through extensive experiments, we demon- strate the effectiveness of DSR-LM over challeng- ing scenarios where widely deployed large LMs fail to reason reliably. We hope our work can lay the groundwork for exploring neuro-symbolic pro- gramming techniques to improve the robustness of LMs on reasoning problems. References Gregor Betz, Christian Voigt, and Kyle Richardson. 2020. Critical thinking for language models. arXiv preprint arXiv:2009.07185. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. NeurIPS. Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Nat- ural language inference with natural language expla- nations. Advances in Neural Information Process- ing Systems, 31. Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, et al. 2021. Neurosymbolic Programming. Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In IJCAI. Antonia Creswell, Murray Shanahan, and Irina Hig- gins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi- lasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze, and Yoav Goldberg. 2021. Measuring and im- proving consistency in pretrained language models. TACL. Richard Evans and Edward Grefenstette. 2018. Learn- ing explanatory rules from noisy data. Journal of Arti\ufb01cial Intelligence Research, 61:1\u201364. Klaus Greff, Sjoerd Van Steenkiste, and J\u00fcrgen On the binding problem arXiv preprint Schmidhuber. 2020. in arti\ufb01cial neural networks. arXiv:2012.05208. Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2021. Do language mod- els have beliefs? methods for detecting, updat- ing, and visualizing model beliefs. arXiv preprint arXiv:2111.13654. Chadi Helwe, Chlo\u00e9 Clavel, and Fabian M. Suchanek. 2021. Reasoning with transformer-based models: Deep learning, but shallow reasoning. In AKBC. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation. Jiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, and Xujie Si. 2021. Scallop: From probabilistic deductive databases to scalable differentiable reasoning. NeurIPS. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec- tional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991. Drew A Hudson and Christopher D Manning. 2018. Compositional attention networks for machine rea- soning. In"}], "doc_text": "the reasoning performance. As shown in Table 5, the two transformer-based models have on-par perfor- mance and outperform the word2vec one. However, note that the word2vec-based model still has bet- ter performance than all other baselines. Besides higher \ufb01nal accuracy, the pre-trained transformer- based language model also accelerates the train- ing process. Both DSR-LM-RoBERTa and DSR- LM-DeBERTa reach their best performance within 20 epochs, while it takes DSR-w2v-BiLSTM 40 epochs to peak. Model DSR-LM (RoBERTa) DSR-LM-DeBERTa DSR-w2v-BiLSTM Accuracy (%) 60.98 \u00b1 2.64 60.92 \u00b1 2.72 40.39 \u00b1 0.06 Table 5: Ablation study about neural backbones of DSR-LM. We compare the CLUTRR performance of DSR-LM using different LMs. Incorporate domain knowledge. DSR-LM al- lows injecting domain speci\ufb01c knowledge. In DSR- LM-with-Rule, we manually crafted 92 rules for kinship reasoning to replace the learnt rules. As shown in Table 6, it obtained a 0.36% performance gain over DSR-LM. The fact that the improvement is marginal implies our method extracts useful rules to obtain on-par performance with manually crafted ones. DSR-LM-without-IC, our model without in- tegrity constraints speci\ufb01ed on predicted relations and rules, performs worse than DSR-LM, suggest- ing that logical integrity constraints are essential component for improving the model robustness. The impact of the relation extractor. To under- stand what causes the failure case of DSR-LM, we study the performance of our relation classi\ufb01cation Model DSR-LM DSR-LM-without-IC DSR-LM-with-Manual-Rule Accuracy (%) 60.98 \u00b1 2.64 51.48 \u00b1 0.57 61.34 \u00b1 1.56 Table 6: Ablation study. We compare our model\u2019s per- formance on CLUTRR with different setups. model separately. We isolate the trained relation extractor and found that it reaches 84.69% accu- racy on the single relation classi\ufb01cation task. For comparison, we train a relation extractor using all the intermediate labels in the training dataset, and it reaches 85.32% accuracy. It shows that even using only weak supervision (i.e., the \ufb01nal answers to multi-hop questions), our approach can reach on- par performance as supervised relation extraction. Reasoning over structured KBs. To understand the rule learning capability of our approach, we de- sign our ablation model DSR-without-LM to take as input ground-truth KBs instead of natural lan- guage. In this case, rule weights are not initialized by LM but randomized. As shown in Table 7, our model outperforms GAT and CTP which also op- erates on structured KBs. It demonstrates that our differentiable rule learning paradigm learns rules to reason about KBs consistently. Model GAT CTP DSR-without-LM Accuracy (%) 39.05 95.57 98.81 Table 7: DSR-without-LM compared against GAT and CTP on reasoning with ground truth KBs. For this com- parison we train on k \u2208 [2, 3] and test on k \u2208 [4, 10]. Failure cases of DSR-LM. We showcase in Ap- pendix Table 8 that even state-of-the-art large LMs are prone to logical fallacies. On the other hand, the failure case of our method usually occurs in the stage of relation extraction. For example, for the following sentence \u201cChristopher and Guillermina are having a father-daughter dance\u201d, our RoBERTa based relation extractor fails to recognize the father- daughter relationship but rather thinks C and G have a husband-wife relationship. We require most of the relation extraction to be correct in order to avoid cascading error. As the error rate on individ- ual relation extraction accumulates, it leads to the observed drop in accuracy as k becomes larger. 5 Concluding Remarks We investigate how to improve LMs\u2019 logical rea- soning capability using differentiable symbolic rea- soning. Through extensive experiments, we demon- strate the effectiveness of DSR-LM over challeng- ing scenarios where widely deployed large LMs fail to reason reliably. We hope our work can lay the groundwork for exploring neuro-symbolic pro- gramming techniques to improve the robustness of LMs on reasoning problems. References Gregor Betz, Christian Voigt, and Kyle Richardson. 2020. Critical thinking for language models. arXiv preprint arXiv:2009.07185. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. NeurIPS. Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Nat- ural language inference with natural language expla- nations. Advances in Neural Information Process- ing Systems, 31. Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, et al. 2021. Neurosymbolic Programming. Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In IJCAI. Antonia Creswell, Murray Shanahan, and Irina Hig- gins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi- lasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze, and Yoav Goldberg. 2021. Measuring and im- proving consistency in pretrained language models. TACL. Richard Evans and Edward Grefenstette. 2018. Learn- ing explanatory rules from noisy data. Journal of Arti\ufb01cial Intelligence Research, 61:1\u201364. Klaus Greff, Sjoerd Van Steenkiste, and J\u00fcrgen On the binding problem arXiv preprint Schmidhuber. 2020. in arti\ufb01cial neural networks. arXiv:2012.05208. Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2021. Do language mod- els have beliefs? methods for detecting, updat- ing, and visualizing model beliefs. arXiv preprint arXiv:2111.13654. Chadi Helwe, Chlo\u00e9 Clavel, and Fabian M. Suchanek. 2021. Reasoning with transformer-based models: Deep learning, but shallow reasoning. In AKBC. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation. Jiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, and Xujie Si. 2021. Scallop: From probabilistic deductive databases to scalable differentiable reasoning. NeurIPS. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec- tional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991. Drew A Hudson and Christopher D Manning. 2018. Compositional attention networks for machine rea- soning. In"}