{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_Rethinking_Voice-Face_Correlation:_A_Geometry_View_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of the phonatory module in the analysis pipeline for voice-face correlation?", "answer": " The phonatory module equips a diffusion-based voice generation model with a voice code \ud835\udc52 as a condition to conduct voice style cloning to help us understand the relationship between face geometry and voice characteristics.", "ref_chunk": ") maps \ud835\udc63 Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 \ud835\udc65! \ud835\udc65\"#$ \ud835\udc65~\ud835\udca9(0, 1) \ud835\udc65! \ud835\udf19 Figure 2: Illustration of our analysis pipeline for voice-face correlation. We randomly pick two voice recordings with shared speaker identity as \ud835\udc63 and \ud835\udc63 \u2032. We then analyze the relationship between each AM and voice by predicting each AM from voice with an estimator and an intermediate voice code \ud835\udc52. The optional phonatory module equips a diffusion-based voice generation model with a voice code \ud835\udc52 as a condition to conduct voice style cloning to help us understand the relationship between face geometry and voice characteristics, which serves as an additional constraint to enforce the estimator learn voice identity. We analyze and select AMs with hypothesis testing. The statically significantly predictable AMs are utilized for 3D facial shape reconstruction for further analysis. (a) Landmarks (b) Proportion(c) Angle(d) Distance Figure 3: Examples of summarized AMs. We summarized three types of AM: proportion, angle and distance. Those AMs are computed from the predefined landmark on the 3D face repre- sentation. into the mean of the \ud835\udc56-th predicted AM. Similarly, we define an uncertainty estimator \ud835\udc3a\ud835\udc3f (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) : \ud835\udc63 \u21a6\u2192 R+ \u222a {0} that \ud835\udc63 into the variance of the \ud835\udc58-th predicted AM. Again, E\ud835\udc58 and \ud835\udf19\ud835\udc58 are the learn- able parameters. The predicted AM and its ground truth become N (\ud835\udc39\ud835\udc58 (\ud835\udc63), \ud835\udc3a\ud835\udc58 (\ud835\udc63)) and N (\ud835\udc5a (0), 0) respectively [20]. Given two ran- dom variables, a more reasonable learning objective is to minimize their KL divergence. AMs when the predicted variances are small, and defer the voice recordings to human experts otherwise. An extreme case is \ud835\udc3a\ud835\udc58 (\ud835\udc63) \u2261 1 where the uncertainty learning objective degrades to the regular regression model. Temporal aggregation. In practice, following the convention of voice understanding, the long voice recording \ud835\udc63 is fed into the net- work in the form of multiple short segments {\ud835\udc63 (1), \u00b7 \u00b7 \u00b7 , \ud835\udc63 (\ud835\udc3f) }. We obtain a sequence of means and variances of the predicted AM. During training, we compute the loss for each segment individually and average them as the training loss. While during evaluation, the predicted AM and its uncertainty are given by aggregating the pre- dictions among all segments. Assuming the short segments from a long recording are class-conditionally independent, the formulations of aggregation are \u02c6\ud835\udc5a (\ud835\udc58 ) = 1 \ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc64 (\ud835\udc58 ) \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) 1 \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) \ud835\udc39\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ), {E\u2217 \ud835\udc58, \ud835\udf14\u2217 \ud835\udc58, \ud835\udf19 \u2217 \ud835\udc58 } = arg min E\ud835\udc58 ,\ud835\udf14\ud835\udc58 ,\ud835\udf19\ud835\udc58 1 |D\ud835\udc61 | \u2211\ufe01 (\ud835\udc63,\ud835\udc5a (\ud835\udc58 ) ) \u2208 D\ud835\udc61 (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) +ln\ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) (2) For a fixed (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2, there is an optimal variance \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) = (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 such that the loss function is minimized. Thereby the uncertainty estimator \ud835\udc3a\ud835\udc58 is learned to produce a small variance if the prediction error is small and vice versa. On the contrary, a smaller variance indicates that the predicted AM is more likely to yield a small prediction error,i.e., close to the ground truth. In this way, we can choose to trust the predicted where \u02c6\ud835\udc5a (\ud835\udc58 ) is the aggregated mean and also the predicted \ud835\udc58-th AM. However, the aggregated variance \ud835\udc64 (\ud835\udc58 ) is not used as the uncer- tainty of the predicted \ud835\udc58-th AM since the conditional independence assumption does not always hold in cases such as noises, silences, the computed aggregated variance will be biased by the number of voice segments in the long recording. So we calibrate the uncertainty as \u02c6\ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u00b7 \ud835\udc64 (\ud835\udc58 ) . Predictable AM identification. We have collected a number of AMs and trained estimators for predicting them. However, only a few of the AMs are actually predictable from voice, which we had anticipated while designing the task. To identify those AMs, we (3) Rethinking Voice-Face Correlation: A Geometry View use hypothesis testing to them. Formally, we can write the null and alternative hypotheses for the \ud835\udc58-th AM as \ud835\udc3b0 : the AM \ud835\udc5a (\ud835\udc58 ) is NOT predictable from voice \ud835\udc3b1 : the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice In order to reject \ud835\udc3b0, we only need to find a counterexample to show that voice is indeed useful in predicting AM \ud835\udc5a (\ud835\udc58 ) . An effective example is to compare the estimators with and without the voice input. If there exists a learned estimator \ud835\udc39\ud835\udc58 (\ud835\udc63) performing better than the chance-level estimator \ud835\udc36\ud835\udc58 without using voice input and the results are statistically significant, we can successfully reject \ud835\udc3b0 and accept \ud835\udc3b1. Here the chance-level estimator for the \ud835\udc58-th AM is \ud835\udc5a (\ud835\udc58 ) , which is the mean \ud835\udc5a (\ud835\udc58 ) of a constant \ud835\udc36\ud835\udc58 = 1 | D\ud835\udc61 | the training set D\ud835\udc61 . So the null and alternative hypothesis can be rewritten as (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc61 \ud835\udc3b0 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc3b1 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u2264 1 \ud835\udc58 ) \u2265 1 where \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 are the mean square errors of estimators with and without voice inputs on validation set D\ud835\udc632, respectively. The formula- tions of \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 ( \u02c6\ud835\udc5a (\ud835\udc58 ) \u2212\ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 and \ud835\udf16\ud835\udc36 (\ud835\udc36\ud835\udc58 \u2212 \ud835\udc5a (\ud835\udc58 ) )2. Since the true variance \ud835\udc58 = of \ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 is unknown, the type of hypothesis testing is one-sided paired-sample t-test. The upper bound of the confidence interval (CI) is given by (cid:205) 1 | D\ud835\udc632 | \ud835\udc58 are given as \ud835\udf16\ud835\udc58 = (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 1 | D\ud835\udc632 | \ud835\udc36\ud835\udc3c\ud835\udc62 = \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) + \ud835\udc611\u2212\ud835\udefc,\ud835\udf08 \u00b7"}, {"question": " What types of Abstract Measures (AMs) are computed from the predefined landmarks on the 3D face representation?", "answer": " The three types of AMs computed are proportion, angle, and distance.", "ref_chunk": ") maps \ud835\udc63 Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 \ud835\udc65! \ud835\udc65\"#$ \ud835\udc65~\ud835\udca9(0, 1) \ud835\udc65! \ud835\udf19 Figure 2: Illustration of our analysis pipeline for voice-face correlation. We randomly pick two voice recordings with shared speaker identity as \ud835\udc63 and \ud835\udc63 \u2032. We then analyze the relationship between each AM and voice by predicting each AM from voice with an estimator and an intermediate voice code \ud835\udc52. The optional phonatory module equips a diffusion-based voice generation model with a voice code \ud835\udc52 as a condition to conduct voice style cloning to help us understand the relationship between face geometry and voice characteristics, which serves as an additional constraint to enforce the estimator learn voice identity. We analyze and select AMs with hypothesis testing. The statically significantly predictable AMs are utilized for 3D facial shape reconstruction for further analysis. (a) Landmarks (b) Proportion(c) Angle(d) Distance Figure 3: Examples of summarized AMs. We summarized three types of AM: proportion, angle and distance. Those AMs are computed from the predefined landmark on the 3D face repre- sentation. into the mean of the \ud835\udc56-th predicted AM. Similarly, we define an uncertainty estimator \ud835\udc3a\ud835\udc3f (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) : \ud835\udc63 \u21a6\u2192 R+ \u222a {0} that \ud835\udc63 into the variance of the \ud835\udc58-th predicted AM. Again, E\ud835\udc58 and \ud835\udf19\ud835\udc58 are the learn- able parameters. The predicted AM and its ground truth become N (\ud835\udc39\ud835\udc58 (\ud835\udc63), \ud835\udc3a\ud835\udc58 (\ud835\udc63)) and N (\ud835\udc5a (0), 0) respectively [20]. Given two ran- dom variables, a more reasonable learning objective is to minimize their KL divergence. AMs when the predicted variances are small, and defer the voice recordings to human experts otherwise. An extreme case is \ud835\udc3a\ud835\udc58 (\ud835\udc63) \u2261 1 where the uncertainty learning objective degrades to the regular regression model. Temporal aggregation. In practice, following the convention of voice understanding, the long voice recording \ud835\udc63 is fed into the net- work in the form of multiple short segments {\ud835\udc63 (1), \u00b7 \u00b7 \u00b7 , \ud835\udc63 (\ud835\udc3f) }. We obtain a sequence of means and variances of the predicted AM. During training, we compute the loss for each segment individually and average them as the training loss. While during evaluation, the predicted AM and its uncertainty are given by aggregating the pre- dictions among all segments. Assuming the short segments from a long recording are class-conditionally independent, the formulations of aggregation are \u02c6\ud835\udc5a (\ud835\udc58 ) = 1 \ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc64 (\ud835\udc58 ) \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) 1 \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) \ud835\udc39\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ), {E\u2217 \ud835\udc58, \ud835\udf14\u2217 \ud835\udc58, \ud835\udf19 \u2217 \ud835\udc58 } = arg min E\ud835\udc58 ,\ud835\udf14\ud835\udc58 ,\ud835\udf19\ud835\udc58 1 |D\ud835\udc61 | \u2211\ufe01 (\ud835\udc63,\ud835\udc5a (\ud835\udc58 ) ) \u2208 D\ud835\udc61 (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) +ln\ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) (2) For a fixed (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2, there is an optimal variance \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) = (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 such that the loss function is minimized. Thereby the uncertainty estimator \ud835\udc3a\ud835\udc58 is learned to produce a small variance if the prediction error is small and vice versa. On the contrary, a smaller variance indicates that the predicted AM is more likely to yield a small prediction error,i.e., close to the ground truth. In this way, we can choose to trust the predicted where \u02c6\ud835\udc5a (\ud835\udc58 ) is the aggregated mean and also the predicted \ud835\udc58-th AM. However, the aggregated variance \ud835\udc64 (\ud835\udc58 ) is not used as the uncer- tainty of the predicted \ud835\udc58-th AM since the conditional independence assumption does not always hold in cases such as noises, silences, the computed aggregated variance will be biased by the number of voice segments in the long recording. So we calibrate the uncertainty as \u02c6\ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u00b7 \ud835\udc64 (\ud835\udc58 ) . Predictable AM identification. We have collected a number of AMs and trained estimators for predicting them. However, only a few of the AMs are actually predictable from voice, which we had anticipated while designing the task. To identify those AMs, we (3) Rethinking Voice-Face Correlation: A Geometry View use hypothesis testing to them. Formally, we can write the null and alternative hypotheses for the \ud835\udc58-th AM as \ud835\udc3b0 : the AM \ud835\udc5a (\ud835\udc58 ) is NOT predictable from voice \ud835\udc3b1 : the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice In order to reject \ud835\udc3b0, we only need to find a counterexample to show that voice is indeed useful in predicting AM \ud835\udc5a (\ud835\udc58 ) . An effective example is to compare the estimators with and without the voice input. If there exists a learned estimator \ud835\udc39\ud835\udc58 (\ud835\udc63) performing better than the chance-level estimator \ud835\udc36\ud835\udc58 without using voice input and the results are statistically significant, we can successfully reject \ud835\udc3b0 and accept \ud835\udc3b1. Here the chance-level estimator for the \ud835\udc58-th AM is \ud835\udc5a (\ud835\udc58 ) , which is the mean \ud835\udc5a (\ud835\udc58 ) of a constant \ud835\udc36\ud835\udc58 = 1 | D\ud835\udc61 | the training set D\ud835\udc61 . So the null and alternative hypothesis can be rewritten as (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc61 \ud835\udc3b0 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc3b1 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u2264 1 \ud835\udc58 ) \u2265 1 where \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 are the mean square errors of estimators with and without voice inputs on validation set D\ud835\udc632, respectively. The formula- tions of \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 ( \u02c6\ud835\udc5a (\ud835\udc58 ) \u2212\ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 and \ud835\udf16\ud835\udc36 (\ud835\udc36\ud835\udc58 \u2212 \ud835\udc5a (\ud835\udc58 ) )2. Since the true variance \ud835\udc58 = of \ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 is unknown, the type of hypothesis testing is one-sided paired-sample t-test. The upper bound of the confidence interval (CI) is given by (cid:205) 1 | D\ud835\udc632 | \ud835\udc58 are given as \ud835\udf16\ud835\udc58 = (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 1 | D\ud835\udc632 | \ud835\udc36\ud835\udc3c\ud835\udc62 = \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) + \ud835\udc611\u2212\ud835\udefc,\ud835\udf08 \u00b7"}, {"question": " How is the loss computed during training for the voice recordings?", "answer": " During training, the loss for each segment is computed individually and then averaged as the training loss.", "ref_chunk": ") maps \ud835\udc63 Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 \ud835\udc65! \ud835\udc65\"#$ \ud835\udc65~\ud835\udca9(0, 1) \ud835\udc65! \ud835\udf19 Figure 2: Illustration of our analysis pipeline for voice-face correlation. We randomly pick two voice recordings with shared speaker identity as \ud835\udc63 and \ud835\udc63 \u2032. We then analyze the relationship between each AM and voice by predicting each AM from voice with an estimator and an intermediate voice code \ud835\udc52. The optional phonatory module equips a diffusion-based voice generation model with a voice code \ud835\udc52 as a condition to conduct voice style cloning to help us understand the relationship between face geometry and voice characteristics, which serves as an additional constraint to enforce the estimator learn voice identity. We analyze and select AMs with hypothesis testing. The statically significantly predictable AMs are utilized for 3D facial shape reconstruction for further analysis. (a) Landmarks (b) Proportion(c) Angle(d) Distance Figure 3: Examples of summarized AMs. We summarized three types of AM: proportion, angle and distance. Those AMs are computed from the predefined landmark on the 3D face repre- sentation. into the mean of the \ud835\udc56-th predicted AM. Similarly, we define an uncertainty estimator \ud835\udc3a\ud835\udc3f (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) : \ud835\udc63 \u21a6\u2192 R+ \u222a {0} that \ud835\udc63 into the variance of the \ud835\udc58-th predicted AM. Again, E\ud835\udc58 and \ud835\udf19\ud835\udc58 are the learn- able parameters. The predicted AM and its ground truth become N (\ud835\udc39\ud835\udc58 (\ud835\udc63), \ud835\udc3a\ud835\udc58 (\ud835\udc63)) and N (\ud835\udc5a (0), 0) respectively [20]. Given two ran- dom variables, a more reasonable learning objective is to minimize their KL divergence. AMs when the predicted variances are small, and defer the voice recordings to human experts otherwise. An extreme case is \ud835\udc3a\ud835\udc58 (\ud835\udc63) \u2261 1 where the uncertainty learning objective degrades to the regular regression model. Temporal aggregation. In practice, following the convention of voice understanding, the long voice recording \ud835\udc63 is fed into the net- work in the form of multiple short segments {\ud835\udc63 (1), \u00b7 \u00b7 \u00b7 , \ud835\udc63 (\ud835\udc3f) }. We obtain a sequence of means and variances of the predicted AM. During training, we compute the loss for each segment individually and average them as the training loss. While during evaluation, the predicted AM and its uncertainty are given by aggregating the pre- dictions among all segments. Assuming the short segments from a long recording are class-conditionally independent, the formulations of aggregation are \u02c6\ud835\udc5a (\ud835\udc58 ) = 1 \ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc64 (\ud835\udc58 ) \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) 1 \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) \ud835\udc39\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ), {E\u2217 \ud835\udc58, \ud835\udf14\u2217 \ud835\udc58, \ud835\udf19 \u2217 \ud835\udc58 } = arg min E\ud835\udc58 ,\ud835\udf14\ud835\udc58 ,\ud835\udf19\ud835\udc58 1 |D\ud835\udc61 | \u2211\ufe01 (\ud835\udc63,\ud835\udc5a (\ud835\udc58 ) ) \u2208 D\ud835\udc61 (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) +ln\ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) (2) For a fixed (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2, there is an optimal variance \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) = (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 such that the loss function is minimized. Thereby the uncertainty estimator \ud835\udc3a\ud835\udc58 is learned to produce a small variance if the prediction error is small and vice versa. On the contrary, a smaller variance indicates that the predicted AM is more likely to yield a small prediction error,i.e., close to the ground truth. In this way, we can choose to trust the predicted where \u02c6\ud835\udc5a (\ud835\udc58 ) is the aggregated mean and also the predicted \ud835\udc58-th AM. However, the aggregated variance \ud835\udc64 (\ud835\udc58 ) is not used as the uncer- tainty of the predicted \ud835\udc58-th AM since the conditional independence assumption does not always hold in cases such as noises, silences, the computed aggregated variance will be biased by the number of voice segments in the long recording. So we calibrate the uncertainty as \u02c6\ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u00b7 \ud835\udc64 (\ud835\udc58 ) . Predictable AM identification. We have collected a number of AMs and trained estimators for predicting them. However, only a few of the AMs are actually predictable from voice, which we had anticipated while designing the task. To identify those AMs, we (3) Rethinking Voice-Face Correlation: A Geometry View use hypothesis testing to them. Formally, we can write the null and alternative hypotheses for the \ud835\udc58-th AM as \ud835\udc3b0 : the AM \ud835\udc5a (\ud835\udc58 ) is NOT predictable from voice \ud835\udc3b1 : the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice In order to reject \ud835\udc3b0, we only need to find a counterexample to show that voice is indeed useful in predicting AM \ud835\udc5a (\ud835\udc58 ) . An effective example is to compare the estimators with and without the voice input. If there exists a learned estimator \ud835\udc39\ud835\udc58 (\ud835\udc63) performing better than the chance-level estimator \ud835\udc36\ud835\udc58 without using voice input and the results are statistically significant, we can successfully reject \ud835\udc3b0 and accept \ud835\udc3b1. Here the chance-level estimator for the \ud835\udc58-th AM is \ud835\udc5a (\ud835\udc58 ) , which is the mean \ud835\udc5a (\ud835\udc58 ) of a constant \ud835\udc36\ud835\udc58 = 1 | D\ud835\udc61 | the training set D\ud835\udc61 . So the null and alternative hypothesis can be rewritten as (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc61 \ud835\udc3b0 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc3b1 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u2264 1 \ud835\udc58 ) \u2265 1 where \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 are the mean square errors of estimators with and without voice inputs on validation set D\ud835\udc632, respectively. The formula- tions of \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 ( \u02c6\ud835\udc5a (\ud835\udc58 ) \u2212\ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 and \ud835\udf16\ud835\udc36 (\ud835\udc36\ud835\udc58 \u2212 \ud835\udc5a (\ud835\udc58 ) )2. Since the true variance \ud835\udc58 = of \ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 is unknown, the type of hypothesis testing is one-sided paired-sample t-test. The upper bound of the confidence interval (CI) is given by (cid:205) 1 | D\ud835\udc632 | \ud835\udc58 are given as \ud835\udf16\ud835\udc58 = (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 1 | D\ud835\udc632 | \ud835\udc36\ud835\udc3c\ud835\udc62 = \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) + \ud835\udc611\u2212\ud835\udefc,\ud835\udf08 \u00b7"}, {"question": " What is the objective of learning the uncertainty estimator \ud835\udc3a\ud835\udc3f?", "answer": " The objective is to produce a small variance for the uncertainty estimator if the prediction error is small and vice versa, indicating a close prediction to the ground truth.", "ref_chunk": ") maps \ud835\udc63 Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 \ud835\udc65! \ud835\udc65\"#$ \ud835\udc65~\ud835\udca9(0, 1) \ud835\udc65! \ud835\udf19 Figure 2: Illustration of our analysis pipeline for voice-face correlation. We randomly pick two voice recordings with shared speaker identity as \ud835\udc63 and \ud835\udc63 \u2032. We then analyze the relationship between each AM and voice by predicting each AM from voice with an estimator and an intermediate voice code \ud835\udc52. The optional phonatory module equips a diffusion-based voice generation model with a voice code \ud835\udc52 as a condition to conduct voice style cloning to help us understand the relationship between face geometry and voice characteristics, which serves as an additional constraint to enforce the estimator learn voice identity. We analyze and select AMs with hypothesis testing. The statically significantly predictable AMs are utilized for 3D facial shape reconstruction for further analysis. (a) Landmarks (b) Proportion(c) Angle(d) Distance Figure 3: Examples of summarized AMs. We summarized three types of AM: proportion, angle and distance. Those AMs are computed from the predefined landmark on the 3D face repre- sentation. into the mean of the \ud835\udc56-th predicted AM. Similarly, we define an uncertainty estimator \ud835\udc3a\ud835\udc3f (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) : \ud835\udc63 \u21a6\u2192 R+ \u222a {0} that \ud835\udc63 into the variance of the \ud835\udc58-th predicted AM. Again, E\ud835\udc58 and \ud835\udf19\ud835\udc58 are the learn- able parameters. The predicted AM and its ground truth become N (\ud835\udc39\ud835\udc58 (\ud835\udc63), \ud835\udc3a\ud835\udc58 (\ud835\udc63)) and N (\ud835\udc5a (0), 0) respectively [20]. Given two ran- dom variables, a more reasonable learning objective is to minimize their KL divergence. AMs when the predicted variances are small, and defer the voice recordings to human experts otherwise. An extreme case is \ud835\udc3a\ud835\udc58 (\ud835\udc63) \u2261 1 where the uncertainty learning objective degrades to the regular regression model. Temporal aggregation. In practice, following the convention of voice understanding, the long voice recording \ud835\udc63 is fed into the net- work in the form of multiple short segments {\ud835\udc63 (1), \u00b7 \u00b7 \u00b7 , \ud835\udc63 (\ud835\udc3f) }. We obtain a sequence of means and variances of the predicted AM. During training, we compute the loss for each segment individually and average them as the training loss. While during evaluation, the predicted AM and its uncertainty are given by aggregating the pre- dictions among all segments. Assuming the short segments from a long recording are class-conditionally independent, the formulations of aggregation are \u02c6\ud835\udc5a (\ud835\udc58 ) = 1 \ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc64 (\ud835\udc58 ) \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) 1 \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) \ud835\udc39\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ), {E\u2217 \ud835\udc58, \ud835\udf14\u2217 \ud835\udc58, \ud835\udf19 \u2217 \ud835\udc58 } = arg min E\ud835\udc58 ,\ud835\udf14\ud835\udc58 ,\ud835\udf19\ud835\udc58 1 |D\ud835\udc61 | \u2211\ufe01 (\ud835\udc63,\ud835\udc5a (\ud835\udc58 ) ) \u2208 D\ud835\udc61 (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) +ln\ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) (2) For a fixed (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2, there is an optimal variance \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) = (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 such that the loss function is minimized. Thereby the uncertainty estimator \ud835\udc3a\ud835\udc58 is learned to produce a small variance if the prediction error is small and vice versa. On the contrary, a smaller variance indicates that the predicted AM is more likely to yield a small prediction error,i.e., close to the ground truth. In this way, we can choose to trust the predicted where \u02c6\ud835\udc5a (\ud835\udc58 ) is the aggregated mean and also the predicted \ud835\udc58-th AM. However, the aggregated variance \ud835\udc64 (\ud835\udc58 ) is not used as the uncer- tainty of the predicted \ud835\udc58-th AM since the conditional independence assumption does not always hold in cases such as noises, silences, the computed aggregated variance will be biased by the number of voice segments in the long recording. So we calibrate the uncertainty as \u02c6\ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u00b7 \ud835\udc64 (\ud835\udc58 ) . Predictable AM identification. We have collected a number of AMs and trained estimators for predicting them. However, only a few of the AMs are actually predictable from voice, which we had anticipated while designing the task. To identify those AMs, we (3) Rethinking Voice-Face Correlation: A Geometry View use hypothesis testing to them. Formally, we can write the null and alternative hypotheses for the \ud835\udc58-th AM as \ud835\udc3b0 : the AM \ud835\udc5a (\ud835\udc58 ) is NOT predictable from voice \ud835\udc3b1 : the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice In order to reject \ud835\udc3b0, we only need to find a counterexample to show that voice is indeed useful in predicting AM \ud835\udc5a (\ud835\udc58 ) . An effective example is to compare the estimators with and without the voice input. If there exists a learned estimator \ud835\udc39\ud835\udc58 (\ud835\udc63) performing better than the chance-level estimator \ud835\udc36\ud835\udc58 without using voice input and the results are statistically significant, we can successfully reject \ud835\udc3b0 and accept \ud835\udc3b1. Here the chance-level estimator for the \ud835\udc58-th AM is \ud835\udc5a (\ud835\udc58 ) , which is the mean \ud835\udc5a (\ud835\udc58 ) of a constant \ud835\udc36\ud835\udc58 = 1 | D\ud835\udc61 | the training set D\ud835\udc61 . So the null and alternative hypothesis can be rewritten as (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc61 \ud835\udc3b0 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc3b1 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u2264 1 \ud835\udc58 ) \u2265 1 where \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 are the mean square errors of estimators with and without voice inputs on validation set D\ud835\udc632, respectively. The formula- tions of \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 ( \u02c6\ud835\udc5a (\ud835\udc58 ) \u2212\ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 and \ud835\udf16\ud835\udc36 (\ud835\udc36\ud835\udc58 \u2212 \ud835\udc5a (\ud835\udc58 ) )2. Since the true variance \ud835\udc58 = of \ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 is unknown, the type of hypothesis testing is one-sided paired-sample t-test. The upper bound of the confidence interval (CI) is given by (cid:205) 1 | D\ud835\udc632 | \ud835\udc58 are given as \ud835\udf16\ud835\udc58 = (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 1 | D\ud835\udc632 | \ud835\udc36\ud835\udc3c\ud835\udc62 = \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) + \ud835\udc611\u2212\ud835\udefc,\ud835\udf08 \u00b7"}, {"question": " How is the aggregation of predictions from short voice segments handled during evaluation?", "answer": " The predicted AM and its uncertainty are given by aggregating the predictions among all segments, assuming the short segments are class-conditionally independent.", "ref_chunk": ") maps \ud835\udc63 Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 \ud835\udc65! \ud835\udc65\"#$ \ud835\udc65~\ud835\udca9(0, 1) \ud835\udc65! \ud835\udf19 Figure 2: Illustration of our analysis pipeline for voice-face correlation. We randomly pick two voice recordings with shared speaker identity as \ud835\udc63 and \ud835\udc63 \u2032. We then analyze the relationship between each AM and voice by predicting each AM from voice with an estimator and an intermediate voice code \ud835\udc52. The optional phonatory module equips a diffusion-based voice generation model with a voice code \ud835\udc52 as a condition to conduct voice style cloning to help us understand the relationship between face geometry and voice characteristics, which serves as an additional constraint to enforce the estimator learn voice identity. We analyze and select AMs with hypothesis testing. The statically significantly predictable AMs are utilized for 3D facial shape reconstruction for further analysis. (a) Landmarks (b) Proportion(c) Angle(d) Distance Figure 3: Examples of summarized AMs. We summarized three types of AM: proportion, angle and distance. Those AMs are computed from the predefined landmark on the 3D face repre- sentation. into the mean of the \ud835\udc56-th predicted AM. Similarly, we define an uncertainty estimator \ud835\udc3a\ud835\udc3f (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) : \ud835\udc63 \u21a6\u2192 R+ \u222a {0} that \ud835\udc63 into the variance of the \ud835\udc58-th predicted AM. Again, E\ud835\udc58 and \ud835\udf19\ud835\udc58 are the learn- able parameters. The predicted AM and its ground truth become N (\ud835\udc39\ud835\udc58 (\ud835\udc63), \ud835\udc3a\ud835\udc58 (\ud835\udc63)) and N (\ud835\udc5a (0), 0) respectively [20]. Given two ran- dom variables, a more reasonable learning objective is to minimize their KL divergence. AMs when the predicted variances are small, and defer the voice recordings to human experts otherwise. An extreme case is \ud835\udc3a\ud835\udc58 (\ud835\udc63) \u2261 1 where the uncertainty learning objective degrades to the regular regression model. Temporal aggregation. In practice, following the convention of voice understanding, the long voice recording \ud835\udc63 is fed into the net- work in the form of multiple short segments {\ud835\udc63 (1), \u00b7 \u00b7 \u00b7 , \ud835\udc63 (\ud835\udc3f) }. We obtain a sequence of means and variances of the predicted AM. During training, we compute the loss for each segment individually and average them as the training loss. While during evaluation, the predicted AM and its uncertainty are given by aggregating the pre- dictions among all segments. Assuming the short segments from a long recording are class-conditionally independent, the formulations of aggregation are \u02c6\ud835\udc5a (\ud835\udc58 ) = 1 \ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc64 (\ud835\udc58 ) \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) 1 \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) \ud835\udc39\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ), {E\u2217 \ud835\udc58, \ud835\udf14\u2217 \ud835\udc58, \ud835\udf19 \u2217 \ud835\udc58 } = arg min E\ud835\udc58 ,\ud835\udf14\ud835\udc58 ,\ud835\udf19\ud835\udc58 1 |D\ud835\udc61 | \u2211\ufe01 (\ud835\udc63,\ud835\udc5a (\ud835\udc58 ) ) \u2208 D\ud835\udc61 (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) +ln\ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) (2) For a fixed (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2, there is an optimal variance \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) = (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 such that the loss function is minimized. Thereby the uncertainty estimator \ud835\udc3a\ud835\udc58 is learned to produce a small variance if the prediction error is small and vice versa. On the contrary, a smaller variance indicates that the predicted AM is more likely to yield a small prediction error,i.e., close to the ground truth. In this way, we can choose to trust the predicted where \u02c6\ud835\udc5a (\ud835\udc58 ) is the aggregated mean and also the predicted \ud835\udc58-th AM. However, the aggregated variance \ud835\udc64 (\ud835\udc58 ) is not used as the uncer- tainty of the predicted \ud835\udc58-th AM since the conditional independence assumption does not always hold in cases such as noises, silences, the computed aggregated variance will be biased by the number of voice segments in the long recording. So we calibrate the uncertainty as \u02c6\ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u00b7 \ud835\udc64 (\ud835\udc58 ) . Predictable AM identification. We have collected a number of AMs and trained estimators for predicting them. However, only a few of the AMs are actually predictable from voice, which we had anticipated while designing the task. To identify those AMs, we (3) Rethinking Voice-Face Correlation: A Geometry View use hypothesis testing to them. Formally, we can write the null and alternative hypotheses for the \ud835\udc58-th AM as \ud835\udc3b0 : the AM \ud835\udc5a (\ud835\udc58 ) is NOT predictable from voice \ud835\udc3b1 : the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice In order to reject \ud835\udc3b0, we only need to find a counterexample to show that voice is indeed useful in predicting AM \ud835\udc5a (\ud835\udc58 ) . An effective example is to compare the estimators with and without the voice input. If there exists a learned estimator \ud835\udc39\ud835\udc58 (\ud835\udc63) performing better than the chance-level estimator \ud835\udc36\ud835\udc58 without using voice input and the results are statistically significant, we can successfully reject \ud835\udc3b0 and accept \ud835\udc3b1. Here the chance-level estimator for the \ud835\udc58-th AM is \ud835\udc5a (\ud835\udc58 ) , which is the mean \ud835\udc5a (\ud835\udc58 ) of a constant \ud835\udc36\ud835\udc58 = 1 | D\ud835\udc61 | the training set D\ud835\udc61 . So the null and alternative hypothesis can be rewritten as (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc61 \ud835\udc3b0 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc3b1 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u2264 1 \ud835\udc58 ) \u2265 1 where \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 are the mean square errors of estimators with and without voice inputs on validation set D\ud835\udc632, respectively. The formula- tions of \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 ( \u02c6\ud835\udc5a (\ud835\udc58 ) \u2212\ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 and \ud835\udf16\ud835\udc36 (\ud835\udc36\ud835\udc58 \u2212 \ud835\udc5a (\ud835\udc58 ) )2. Since the true variance \ud835\udc58 = of \ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 is unknown, the type of hypothesis testing is one-sided paired-sample t-test. The upper bound of the confidence interval (CI) is given by (cid:205) 1 | D\ud835\udc632 | \ud835\udc58 are given as \ud835\udf16\ud835\udc58 = (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 1 | D\ud835\udc632 | \ud835\udc36\ud835\udc3c\ud835\udc62 = \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) + \ud835\udc611\u2212\ud835\udefc,\ud835\udf08 \u00b7"}, {"question": " What is the condition to successfully reject the null hypothesis in predictable AM identification?", "answer": " To successfully reject the null hypothesis, a learned estimator \ud835\udc39\ud835\udc58 (\ud835\udc63) must perform better than the chance-level estimator \ud835\udc36\ud835\udc58 without using voice input, with statistically significant results.", "ref_chunk": ") maps \ud835\udc63 Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 \ud835\udc65! \ud835\udc65\"#$ \ud835\udc65~\ud835\udca9(0, 1) \ud835\udc65! \ud835\udf19 Figure 2: Illustration of our analysis pipeline for voice-face correlation. We randomly pick two voice recordings with shared speaker identity as \ud835\udc63 and \ud835\udc63 \u2032. We then analyze the relationship between each AM and voice by predicting each AM from voice with an estimator and an intermediate voice code \ud835\udc52. The optional phonatory module equips a diffusion-based voice generation model with a voice code \ud835\udc52 as a condition to conduct voice style cloning to help us understand the relationship between face geometry and voice characteristics, which serves as an additional constraint to enforce the estimator learn voice identity. We analyze and select AMs with hypothesis testing. The statically significantly predictable AMs are utilized for 3D facial shape reconstruction for further analysis. (a) Landmarks (b) Proportion(c) Angle(d) Distance Figure 3: Examples of summarized AMs. We summarized three types of AM: proportion, angle and distance. Those AMs are computed from the predefined landmark on the 3D face repre- sentation. into the mean of the \ud835\udc56-th predicted AM. Similarly, we define an uncertainty estimator \ud835\udc3a\ud835\udc3f (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) : \ud835\udc63 \u21a6\u2192 R+ \u222a {0} that \ud835\udc63 into the variance of the \ud835\udc58-th predicted AM. Again, E\ud835\udc58 and \ud835\udf19\ud835\udc58 are the learn- able parameters. The predicted AM and its ground truth become N (\ud835\udc39\ud835\udc58 (\ud835\udc63), \ud835\udc3a\ud835\udc58 (\ud835\udc63)) and N (\ud835\udc5a (0), 0) respectively [20]. Given two ran- dom variables, a more reasonable learning objective is to minimize their KL divergence. AMs when the predicted variances are small, and defer the voice recordings to human experts otherwise. An extreme case is \ud835\udc3a\ud835\udc58 (\ud835\udc63) \u2261 1 where the uncertainty learning objective degrades to the regular regression model. Temporal aggregation. In practice, following the convention of voice understanding, the long voice recording \ud835\udc63 is fed into the net- work in the form of multiple short segments {\ud835\udc63 (1), \u00b7 \u00b7 \u00b7 , \ud835\udc63 (\ud835\udc3f) }. We obtain a sequence of means and variances of the predicted AM. During training, we compute the loss for each segment individually and average them as the training loss. While during evaluation, the predicted AM and its uncertainty are given by aggregating the pre- dictions among all segments. Assuming the short segments from a long recording are class-conditionally independent, the formulations of aggregation are \u02c6\ud835\udc5a (\ud835\udc58 ) = 1 \ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc64 (\ud835\udc58 ) \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) 1 \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) \ud835\udc39\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ), {E\u2217 \ud835\udc58, \ud835\udf14\u2217 \ud835\udc58, \ud835\udf19 \u2217 \ud835\udc58 } = arg min E\ud835\udc58 ,\ud835\udf14\ud835\udc58 ,\ud835\udf19\ud835\udc58 1 |D\ud835\udc61 | \u2211\ufe01 (\ud835\udc63,\ud835\udc5a (\ud835\udc58 ) ) \u2208 D\ud835\udc61 (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) +ln\ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) (2) For a fixed (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2, there is an optimal variance \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) = (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 such that the loss function is minimized. Thereby the uncertainty estimator \ud835\udc3a\ud835\udc58 is learned to produce a small variance if the prediction error is small and vice versa. On the contrary, a smaller variance indicates that the predicted AM is more likely to yield a small prediction error,i.e., close to the ground truth. In this way, we can choose to trust the predicted where \u02c6\ud835\udc5a (\ud835\udc58 ) is the aggregated mean and also the predicted \ud835\udc58-th AM. However, the aggregated variance \ud835\udc64 (\ud835\udc58 ) is not used as the uncer- tainty of the predicted \ud835\udc58-th AM since the conditional independence assumption does not always hold in cases such as noises, silences, the computed aggregated variance will be biased by the number of voice segments in the long recording. So we calibrate the uncertainty as \u02c6\ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u00b7 \ud835\udc64 (\ud835\udc58 ) . Predictable AM identification. We have collected a number of AMs and trained estimators for predicting them. However, only a few of the AMs are actually predictable from voice, which we had anticipated while designing the task. To identify those AMs, we (3) Rethinking Voice-Face Correlation: A Geometry View use hypothesis testing to them. Formally, we can write the null and alternative hypotheses for the \ud835\udc58-th AM as \ud835\udc3b0 : the AM \ud835\udc5a (\ud835\udc58 ) is NOT predictable from voice \ud835\udc3b1 : the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice In order to reject \ud835\udc3b0, we only need to find a counterexample to show that voice is indeed useful in predicting AM \ud835\udc5a (\ud835\udc58 ) . An effective example is to compare the estimators with and without the voice input. If there exists a learned estimator \ud835\udc39\ud835\udc58 (\ud835\udc63) performing better than the chance-level estimator \ud835\udc36\ud835\udc58 without using voice input and the results are statistically significant, we can successfully reject \ud835\udc3b0 and accept \ud835\udc3b1. Here the chance-level estimator for the \ud835\udc58-th AM is \ud835\udc5a (\ud835\udc58 ) , which is the mean \ud835\udc5a (\ud835\udc58 ) of a constant \ud835\udc36\ud835\udc58 = 1 | D\ud835\udc61 | the training set D\ud835\udc61 . So the null and alternative hypothesis can be rewritten as (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc61 \ud835\udc3b0 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc3b1 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u2264 1 \ud835\udc58 ) \u2265 1 where \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 are the mean square errors of estimators with and without voice inputs on validation set D\ud835\udc632, respectively. The formula- tions of \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 ( \u02c6\ud835\udc5a (\ud835\udc58 ) \u2212\ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 and \ud835\udf16\ud835\udc36 (\ud835\udc36\ud835\udc58 \u2212 \ud835\udc5a (\ud835\udc58 ) )2. Since the true variance \ud835\udc58 = of \ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 is unknown, the type of hypothesis testing is one-sided paired-sample t-test. The upper bound of the confidence interval (CI) is given by (cid:205) 1 | D\ud835\udc632 | \ud835\udc58 are given as \ud835\udf16\ud835\udc58 = (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 1 | D\ud835\udc632 | \ud835\udc36\ud835\udc3c\ud835\udc62 = \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) + \ud835\udc611\u2212\ud835\udefc,\ud835\udf08 \u00b7"}, {"question": " What is the null hypothesis in predictable AM identification related to voice input?", "answer": " The null hypothesis states that the Abstract Measure (AM) \ud835\udc5a (\ud835\udc58 ) is NOT predictable from voice.", "ref_chunk": ") maps \ud835\udc63 Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 \ud835\udc65! \ud835\udc65\"#$ \ud835\udc65~\ud835\udca9(0, 1) \ud835\udc65! \ud835\udf19 Figure 2: Illustration of our analysis pipeline for voice-face correlation. We randomly pick two voice recordings with shared speaker identity as \ud835\udc63 and \ud835\udc63 \u2032. We then analyze the relationship between each AM and voice by predicting each AM from voice with an estimator and an intermediate voice code \ud835\udc52. The optional phonatory module equips a diffusion-based voice generation model with a voice code \ud835\udc52 as a condition to conduct voice style cloning to help us understand the relationship between face geometry and voice characteristics, which serves as an additional constraint to enforce the estimator learn voice identity. We analyze and select AMs with hypothesis testing. The statically significantly predictable AMs are utilized for 3D facial shape reconstruction for further analysis. (a) Landmarks (b) Proportion(c) Angle(d) Distance Figure 3: Examples of summarized AMs. We summarized three types of AM: proportion, angle and distance. Those AMs are computed from the predefined landmark on the 3D face repre- sentation. into the mean of the \ud835\udc56-th predicted AM. Similarly, we define an uncertainty estimator \ud835\udc3a\ud835\udc3f (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) : \ud835\udc63 \u21a6\u2192 R+ \u222a {0} that \ud835\udc63 into the variance of the \ud835\udc58-th predicted AM. Again, E\ud835\udc58 and \ud835\udf19\ud835\udc58 are the learn- able parameters. The predicted AM and its ground truth become N (\ud835\udc39\ud835\udc58 (\ud835\udc63), \ud835\udc3a\ud835\udc58 (\ud835\udc63)) and N (\ud835\udc5a (0), 0) respectively [20]. Given two ran- dom variables, a more reasonable learning objective is to minimize their KL divergence. AMs when the predicted variances are small, and defer the voice recordings to human experts otherwise. An extreme case is \ud835\udc3a\ud835\udc58 (\ud835\udc63) \u2261 1 where the uncertainty learning objective degrades to the regular regression model. Temporal aggregation. In practice, following the convention of voice understanding, the long voice recording \ud835\udc63 is fed into the net- work in the form of multiple short segments {\ud835\udc63 (1), \u00b7 \u00b7 \u00b7 , \ud835\udc63 (\ud835\udc3f) }. We obtain a sequence of means and variances of the predicted AM. During training, we compute the loss for each segment individually and average them as the training loss. While during evaluation, the predicted AM and its uncertainty are given by aggregating the pre- dictions among all segments. Assuming the short segments from a long recording are class-conditionally independent, the formulations of aggregation are \u02c6\ud835\udc5a (\ud835\udc58 ) = 1 \ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc64 (\ud835\udc58 ) \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) 1 \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) \ud835\udc39\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ), {E\u2217 \ud835\udc58, \ud835\udf14\u2217 \ud835\udc58, \ud835\udf19 \u2217 \ud835\udc58 } = arg min E\ud835\udc58 ,\ud835\udf14\ud835\udc58 ,\ud835\udf19\ud835\udc58 1 |D\ud835\udc61 | \u2211\ufe01 (\ud835\udc63,\ud835\udc5a (\ud835\udc58 ) ) \u2208 D\ud835\udc61 (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) +ln\ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) (2) For a fixed (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2, there is an optimal variance \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) = (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 such that the loss function is minimized. Thereby the uncertainty estimator \ud835\udc3a\ud835\udc58 is learned to produce a small variance if the prediction error is small and vice versa. On the contrary, a smaller variance indicates that the predicted AM is more likely to yield a small prediction error,i.e., close to the ground truth. In this way, we can choose to trust the predicted where \u02c6\ud835\udc5a (\ud835\udc58 ) is the aggregated mean and also the predicted \ud835\udc58-th AM. However, the aggregated variance \ud835\udc64 (\ud835\udc58 ) is not used as the uncer- tainty of the predicted \ud835\udc58-th AM since the conditional independence assumption does not always hold in cases such as noises, silences, the computed aggregated variance will be biased by the number of voice segments in the long recording. So we calibrate the uncertainty as \u02c6\ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u00b7 \ud835\udc64 (\ud835\udc58 ) . Predictable AM identification. We have collected a number of AMs and trained estimators for predicting them. However, only a few of the AMs are actually predictable from voice, which we had anticipated while designing the task. To identify those AMs, we (3) Rethinking Voice-Face Correlation: A Geometry View use hypothesis testing to them. Formally, we can write the null and alternative hypotheses for the \ud835\udc58-th AM as \ud835\udc3b0 : the AM \ud835\udc5a (\ud835\udc58 ) is NOT predictable from voice \ud835\udc3b1 : the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice In order to reject \ud835\udc3b0, we only need to find a counterexample to show that voice is indeed useful in predicting AM \ud835\udc5a (\ud835\udc58 ) . An effective example is to compare the estimators with and without the voice input. If there exists a learned estimator \ud835\udc39\ud835\udc58 (\ud835\udc63) performing better than the chance-level estimator \ud835\udc36\ud835\udc58 without using voice input and the results are statistically significant, we can successfully reject \ud835\udc3b0 and accept \ud835\udc3b1. Here the chance-level estimator for the \ud835\udc58-th AM is \ud835\udc5a (\ud835\udc58 ) , which is the mean \ud835\udc5a (\ud835\udc58 ) of a constant \ud835\udc36\ud835\udc58 = 1 | D\ud835\udc61 | the training set D\ud835\udc61 . So the null and alternative hypothesis can be rewritten as (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc61 \ud835\udc3b0 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc3b1 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u2264 1 \ud835\udc58 ) \u2265 1 where \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 are the mean square errors of estimators with and without voice inputs on validation set D\ud835\udc632, respectively. The formula- tions of \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 ( \u02c6\ud835\udc5a (\ud835\udc58 ) \u2212\ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 and \ud835\udf16\ud835\udc36 (\ud835\udc36\ud835\udc58 \u2212 \ud835\udc5a (\ud835\udc58 ) )2. Since the true variance \ud835\udc58 = of \ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 is unknown, the type of hypothesis testing is one-sided paired-sample t-test. The upper bound of the confidence interval (CI) is given by (cid:205) 1 | D\ud835\udc632 | \ud835\udc58 are given as \ud835\udf16\ud835\udc58 = (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 1 | D\ud835\udc632 | \ud835\udc36\ud835\udc3c\ud835\udc62 = \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) + \ud835\udc611\u2212\ud835\udefc,\ud835\udf08 \u00b7"}, {"question": " How is the uncertainty of the predicted \ud835\udc58-th AM calibrated?", "answer": " The uncertainty of the predicted \ud835\udc58-th AM is calibrated by multiplying the computed aggregated variance by the number of voice segments in the long recording.", "ref_chunk": ") maps \ud835\udc63 Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 \ud835\udc65! \ud835\udc65\"#$ \ud835\udc65~\ud835\udca9(0, 1) \ud835\udc65! \ud835\udf19 Figure 2: Illustration of our analysis pipeline for voice-face correlation. We randomly pick two voice recordings with shared speaker identity as \ud835\udc63 and \ud835\udc63 \u2032. We then analyze the relationship between each AM and voice by predicting each AM from voice with an estimator and an intermediate voice code \ud835\udc52. The optional phonatory module equips a diffusion-based voice generation model with a voice code \ud835\udc52 as a condition to conduct voice style cloning to help us understand the relationship between face geometry and voice characteristics, which serves as an additional constraint to enforce the estimator learn voice identity. We analyze and select AMs with hypothesis testing. The statically significantly predictable AMs are utilized for 3D facial shape reconstruction for further analysis. (a) Landmarks (b) Proportion(c) Angle(d) Distance Figure 3: Examples of summarized AMs. We summarized three types of AM: proportion, angle and distance. Those AMs are computed from the predefined landmark on the 3D face repre- sentation. into the mean of the \ud835\udc56-th predicted AM. Similarly, we define an uncertainty estimator \ud835\udc3a\ud835\udc3f (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) : \ud835\udc63 \u21a6\u2192 R+ \u222a {0} that \ud835\udc63 into the variance of the \ud835\udc58-th predicted AM. Again, E\ud835\udc58 and \ud835\udf19\ud835\udc58 are the learn- able parameters. The predicted AM and its ground truth become N (\ud835\udc39\ud835\udc58 (\ud835\udc63), \ud835\udc3a\ud835\udc58 (\ud835\udc63)) and N (\ud835\udc5a (0), 0) respectively [20]. Given two ran- dom variables, a more reasonable learning objective is to minimize their KL divergence. AMs when the predicted variances are small, and defer the voice recordings to human experts otherwise. An extreme case is \ud835\udc3a\ud835\udc58 (\ud835\udc63) \u2261 1 where the uncertainty learning objective degrades to the regular regression model. Temporal aggregation. In practice, following the convention of voice understanding, the long voice recording \ud835\udc63 is fed into the net- work in the form of multiple short segments {\ud835\udc63 (1), \u00b7 \u00b7 \u00b7 , \ud835\udc63 (\ud835\udc3f) }. We obtain a sequence of means and variances of the predicted AM. During training, we compute the loss for each segment individually and average them as the training loss. While during evaluation, the predicted AM and its uncertainty are given by aggregating the pre- dictions among all segments. Assuming the short segments from a long recording are class-conditionally independent, the formulations of aggregation are \u02c6\ud835\udc5a (\ud835\udc58 ) = 1 \ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc64 (\ud835\udc58 ) \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) 1 \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) \ud835\udc39\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ), {E\u2217 \ud835\udc58, \ud835\udf14\u2217 \ud835\udc58, \ud835\udf19 \u2217 \ud835\udc58 } = arg min E\ud835\udc58 ,\ud835\udf14\ud835\udc58 ,\ud835\udf19\ud835\udc58 1 |D\ud835\udc61 | \u2211\ufe01 (\ud835\udc63,\ud835\udc5a (\ud835\udc58 ) ) \u2208 D\ud835\udc61 (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) +ln\ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) (2) For a fixed (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2, there is an optimal variance \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) = (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 such that the loss function is minimized. Thereby the uncertainty estimator \ud835\udc3a\ud835\udc58 is learned to produce a small variance if the prediction error is small and vice versa. On the contrary, a smaller variance indicates that the predicted AM is more likely to yield a small prediction error,i.e., close to the ground truth. In this way, we can choose to trust the predicted where \u02c6\ud835\udc5a (\ud835\udc58 ) is the aggregated mean and also the predicted \ud835\udc58-th AM. However, the aggregated variance \ud835\udc64 (\ud835\udc58 ) is not used as the uncer- tainty of the predicted \ud835\udc58-th AM since the conditional independence assumption does not always hold in cases such as noises, silences, the computed aggregated variance will be biased by the number of voice segments in the long recording. So we calibrate the uncertainty as \u02c6\ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u00b7 \ud835\udc64 (\ud835\udc58 ) . Predictable AM identification. We have collected a number of AMs and trained estimators for predicting them. However, only a few of the AMs are actually predictable from voice, which we had anticipated while designing the task. To identify those AMs, we (3) Rethinking Voice-Face Correlation: A Geometry View use hypothesis testing to them. Formally, we can write the null and alternative hypotheses for the \ud835\udc58-th AM as \ud835\udc3b0 : the AM \ud835\udc5a (\ud835\udc58 ) is NOT predictable from voice \ud835\udc3b1 : the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice In order to reject \ud835\udc3b0, we only need to find a counterexample to show that voice is indeed useful in predicting AM \ud835\udc5a (\ud835\udc58 ) . An effective example is to compare the estimators with and without the voice input. If there exists a learned estimator \ud835\udc39\ud835\udc58 (\ud835\udc63) performing better than the chance-level estimator \ud835\udc36\ud835\udc58 without using voice input and the results are statistically significant, we can successfully reject \ud835\udc3b0 and accept \ud835\udc3b1. Here the chance-level estimator for the \ud835\udc58-th AM is \ud835\udc5a (\ud835\udc58 ) , which is the mean \ud835\udc5a (\ud835\udc58 ) of a constant \ud835\udc36\ud835\udc58 = 1 | D\ud835\udc61 | the training set D\ud835\udc61 . So the null and alternative hypothesis can be rewritten as (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc61 \ud835\udc3b0 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc3b1 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u2264 1 \ud835\udc58 ) \u2265 1 where \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 are the mean square errors of estimators with and without voice inputs on validation set D\ud835\udc632, respectively. The formula- tions of \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 ( \u02c6\ud835\udc5a (\ud835\udc58 ) \u2212\ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 and \ud835\udf16\ud835\udc36 (\ud835\udc36\ud835\udc58 \u2212 \ud835\udc5a (\ud835\udc58 ) )2. Since the true variance \ud835\udc58 = of \ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 is unknown, the type of hypothesis testing is one-sided paired-sample t-test. The upper bound of the confidence interval (CI) is given by (cid:205) 1 | D\ud835\udc632 | \ud835\udc58 are given as \ud835\udf16\ud835\udc58 = (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 1 | D\ud835\udc632 | \ud835\udc36\ud835\udc3c\ud835\udc62 = \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) + \ud835\udc611\u2212\ud835\udefc,\ud835\udf08 \u00b7"}, {"question": " What is the learning objective when given two random variables in the analysis?", "answer": " The more reasonable learning objective is to minimize the Kullback-Leibler (KL) divergence between the predicted variances when the predicted variances are small.", "ref_chunk": ") maps \ud835\udc63 Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 \ud835\udc65! \ud835\udc65\"#$ \ud835\udc65~\ud835\udca9(0, 1) \ud835\udc65! \ud835\udf19 Figure 2: Illustration of our analysis pipeline for voice-face correlation. We randomly pick two voice recordings with shared speaker identity as \ud835\udc63 and \ud835\udc63 \u2032. We then analyze the relationship between each AM and voice by predicting each AM from voice with an estimator and an intermediate voice code \ud835\udc52. The optional phonatory module equips a diffusion-based voice generation model with a voice code \ud835\udc52 as a condition to conduct voice style cloning to help us understand the relationship between face geometry and voice characteristics, which serves as an additional constraint to enforce the estimator learn voice identity. We analyze and select AMs with hypothesis testing. The statically significantly predictable AMs are utilized for 3D facial shape reconstruction for further analysis. (a) Landmarks (b) Proportion(c) Angle(d) Distance Figure 3: Examples of summarized AMs. We summarized three types of AM: proportion, angle and distance. Those AMs are computed from the predefined landmark on the 3D face repre- sentation. into the mean of the \ud835\udc56-th predicted AM. Similarly, we define an uncertainty estimator \ud835\udc3a\ud835\udc3f (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) : \ud835\udc63 \u21a6\u2192 R+ \u222a {0} that \ud835\udc63 into the variance of the \ud835\udc58-th predicted AM. Again, E\ud835\udc58 and \ud835\udf19\ud835\udc58 are the learn- able parameters. The predicted AM and its ground truth become N (\ud835\udc39\ud835\udc58 (\ud835\udc63), \ud835\udc3a\ud835\udc58 (\ud835\udc63)) and N (\ud835\udc5a (0), 0) respectively [20]. Given two ran- dom variables, a more reasonable learning objective is to minimize their KL divergence. AMs when the predicted variances are small, and defer the voice recordings to human experts otherwise. An extreme case is \ud835\udc3a\ud835\udc58 (\ud835\udc63) \u2261 1 where the uncertainty learning objective degrades to the regular regression model. Temporal aggregation. In practice, following the convention of voice understanding, the long voice recording \ud835\udc63 is fed into the net- work in the form of multiple short segments {\ud835\udc63 (1), \u00b7 \u00b7 \u00b7 , \ud835\udc63 (\ud835\udc3f) }. We obtain a sequence of means and variances of the predicted AM. During training, we compute the loss for each segment individually and average them as the training loss. While during evaluation, the predicted AM and its uncertainty are given by aggregating the pre- dictions among all segments. Assuming the short segments from a long recording are class-conditionally independent, the formulations of aggregation are \u02c6\ud835\udc5a (\ud835\udc58 ) = 1 \ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc64 (\ud835\udc58 ) \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) 1 \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) \ud835\udc39\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ), {E\u2217 \ud835\udc58, \ud835\udf14\u2217 \ud835\udc58, \ud835\udf19 \u2217 \ud835\udc58 } = arg min E\ud835\udc58 ,\ud835\udf14\ud835\udc58 ,\ud835\udf19\ud835\udc58 1 |D\ud835\udc61 | \u2211\ufe01 (\ud835\udc63,\ud835\udc5a (\ud835\udc58 ) ) \u2208 D\ud835\udc61 (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) +ln\ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) (2) For a fixed (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2, there is an optimal variance \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) = (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 such that the loss function is minimized. Thereby the uncertainty estimator \ud835\udc3a\ud835\udc58 is learned to produce a small variance if the prediction error is small and vice versa. On the contrary, a smaller variance indicates that the predicted AM is more likely to yield a small prediction error,i.e., close to the ground truth. In this way, we can choose to trust the predicted where \u02c6\ud835\udc5a (\ud835\udc58 ) is the aggregated mean and also the predicted \ud835\udc58-th AM. However, the aggregated variance \ud835\udc64 (\ud835\udc58 ) is not used as the uncer- tainty of the predicted \ud835\udc58-th AM since the conditional independence assumption does not always hold in cases such as noises, silences, the computed aggregated variance will be biased by the number of voice segments in the long recording. So we calibrate the uncertainty as \u02c6\ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u00b7 \ud835\udc64 (\ud835\udc58 ) . Predictable AM identification. We have collected a number of AMs and trained estimators for predicting them. However, only a few of the AMs are actually predictable from voice, which we had anticipated while designing the task. To identify those AMs, we (3) Rethinking Voice-Face Correlation: A Geometry View use hypothesis testing to them. Formally, we can write the null and alternative hypotheses for the \ud835\udc58-th AM as \ud835\udc3b0 : the AM \ud835\udc5a (\ud835\udc58 ) is NOT predictable from voice \ud835\udc3b1 : the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice In order to reject \ud835\udc3b0, we only need to find a counterexample to show that voice is indeed useful in predicting AM \ud835\udc5a (\ud835\udc58 ) . An effective example is to compare the estimators with and without the voice input. If there exists a learned estimator \ud835\udc39\ud835\udc58 (\ud835\udc63) performing better than the chance-level estimator \ud835\udc36\ud835\udc58 without using voice input and the results are statistically significant, we can successfully reject \ud835\udc3b0 and accept \ud835\udc3b1. Here the chance-level estimator for the \ud835\udc58-th AM is \ud835\udc5a (\ud835\udc58 ) , which is the mean \ud835\udc5a (\ud835\udc58 ) of a constant \ud835\udc36\ud835\udc58 = 1 | D\ud835\udc61 | the training set D\ud835\udc61 . So the null and alternative hypothesis can be rewritten as (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc61 \ud835\udc3b0 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc3b1 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u2264 1 \ud835\udc58 ) \u2265 1 where \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 are the mean square errors of estimators with and without voice inputs on validation set D\ud835\udc632, respectively. The formula- tions of \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 ( \u02c6\ud835\udc5a (\ud835\udc58 ) \u2212\ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 and \ud835\udf16\ud835\udc36 (\ud835\udc36\ud835\udc58 \u2212 \ud835\udc5a (\ud835\udc58 ) )2. Since the true variance \ud835\udc58 = of \ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 is unknown, the type of hypothesis testing is one-sided paired-sample t-test. The upper bound of the confidence interval (CI) is given by (cid:205) 1 | D\ud835\udc632 | \ud835\udc58 are given as \ud835\udf16\ud835\udc58 = (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 1 | D\ud835\udc632 | \ud835\udc36\ud835\udc3c\ud835\udc62 = \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) + \ud835\udc611\u2212\ud835\udefc,\ud835\udf08 \u00b7"}, {"question": " What is the purpose of the hypothesis testing in predictable AM identification?", "answer": " The purpose is to identify AMs that are predictable from voice by comparing the performance of estimators with and without the voice input, rejecting the null hypothesis for AMs that are predictable.", "ref_chunk": ") maps \ud835\udc63 Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 \ud835\udc65! \ud835\udc65\"#$ \ud835\udc65~\ud835\udca9(0, 1) \ud835\udc65! \ud835\udf19 Figure 2: Illustration of our analysis pipeline for voice-face correlation. We randomly pick two voice recordings with shared speaker identity as \ud835\udc63 and \ud835\udc63 \u2032. We then analyze the relationship between each AM and voice by predicting each AM from voice with an estimator and an intermediate voice code \ud835\udc52. The optional phonatory module equips a diffusion-based voice generation model with a voice code \ud835\udc52 as a condition to conduct voice style cloning to help us understand the relationship between face geometry and voice characteristics, which serves as an additional constraint to enforce the estimator learn voice identity. We analyze and select AMs with hypothesis testing. The statically significantly predictable AMs are utilized for 3D facial shape reconstruction for further analysis. (a) Landmarks (b) Proportion(c) Angle(d) Distance Figure 3: Examples of summarized AMs. We summarized three types of AM: proportion, angle and distance. Those AMs are computed from the predefined landmark on the 3D face repre- sentation. into the mean of the \ud835\udc56-th predicted AM. Similarly, we define an uncertainty estimator \ud835\udc3a\ud835\udc3f (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) : \ud835\udc63 \u21a6\u2192 R+ \u222a {0} that \ud835\udc63 into the variance of the \ud835\udc58-th predicted AM. Again, E\ud835\udc58 and \ud835\udf19\ud835\udc58 are the learn- able parameters. The predicted AM and its ground truth become N (\ud835\udc39\ud835\udc58 (\ud835\udc63), \ud835\udc3a\ud835\udc58 (\ud835\udc63)) and N (\ud835\udc5a (0), 0) respectively [20]. Given two ran- dom variables, a more reasonable learning objective is to minimize their KL divergence. AMs when the predicted variances are small, and defer the voice recordings to human experts otherwise. An extreme case is \ud835\udc3a\ud835\udc58 (\ud835\udc63) \u2261 1 where the uncertainty learning objective degrades to the regular regression model. Temporal aggregation. In practice, following the convention of voice understanding, the long voice recording \ud835\udc63 is fed into the net- work in the form of multiple short segments {\ud835\udc63 (1), \u00b7 \u00b7 \u00b7 , \ud835\udc63 (\ud835\udc3f) }. We obtain a sequence of means and variances of the predicted AM. During training, we compute the loss for each segment individually and average them as the training loss. While during evaluation, the predicted AM and its uncertainty are given by aggregating the pre- dictions among all segments. Assuming the short segments from a long recording are class-conditionally independent, the formulations of aggregation are \u02c6\ud835\udc5a (\ud835\udc58 ) = 1 \ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc64 (\ud835\udc58 ) \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) 1 \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) \ud835\udc39\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ), {E\u2217 \ud835\udc58, \ud835\udf14\u2217 \ud835\udc58, \ud835\udf19 \u2217 \ud835\udc58 } = arg min E\ud835\udc58 ,\ud835\udf14\ud835\udc58 ,\ud835\udf19\ud835\udc58 1 |D\ud835\udc61 | \u2211\ufe01 (\ud835\udc63,\ud835\udc5a (\ud835\udc58 ) ) \u2208 D\ud835\udc61 (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) +ln\ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) (2) For a fixed (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2, there is an optimal variance \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) = (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 such that the loss function is minimized. Thereby the uncertainty estimator \ud835\udc3a\ud835\udc58 is learned to produce a small variance if the prediction error is small and vice versa. On the contrary, a smaller variance indicates that the predicted AM is more likely to yield a small prediction error,i.e., close to the ground truth. In this way, we can choose to trust the predicted where \u02c6\ud835\udc5a (\ud835\udc58 ) is the aggregated mean and also the predicted \ud835\udc58-th AM. However, the aggregated variance \ud835\udc64 (\ud835\udc58 ) is not used as the uncer- tainty of the predicted \ud835\udc58-th AM since the conditional independence assumption does not always hold in cases such as noises, silences, the computed aggregated variance will be biased by the number of voice segments in the long recording. So we calibrate the uncertainty as \u02c6\ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u00b7 \ud835\udc64 (\ud835\udc58 ) . Predictable AM identification. We have collected a number of AMs and trained estimators for predicting them. However, only a few of the AMs are actually predictable from voice, which we had anticipated while designing the task. To identify those AMs, we (3) Rethinking Voice-Face Correlation: A Geometry View use hypothesis testing to them. Formally, we can write the null and alternative hypotheses for the \ud835\udc58-th AM as \ud835\udc3b0 : the AM \ud835\udc5a (\ud835\udc58 ) is NOT predictable from voice \ud835\udc3b1 : the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice In order to reject \ud835\udc3b0, we only need to find a counterexample to show that voice is indeed useful in predicting AM \ud835\udc5a (\ud835\udc58 ) . An effective example is to compare the estimators with and without the voice input. If there exists a learned estimator \ud835\udc39\ud835\udc58 (\ud835\udc63) performing better than the chance-level estimator \ud835\udc36\ud835\udc58 without using voice input and the results are statistically significant, we can successfully reject \ud835\udc3b0 and accept \ud835\udc3b1. Here the chance-level estimator for the \ud835\udc58-th AM is \ud835\udc5a (\ud835\udc58 ) , which is the mean \ud835\udc5a (\ud835\udc58 ) of a constant \ud835\udc36\ud835\udc58 = 1 | D\ud835\udc61 | the training set D\ud835\udc61 . So the null and alternative hypothesis can be rewritten as (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc61 \ud835\udc3b0 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc3b1 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u2264 1 \ud835\udc58 ) \u2265 1 where \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 are the mean square errors of estimators with and without voice inputs on validation set D\ud835\udc632, respectively. The formula- tions of \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 ( \u02c6\ud835\udc5a (\ud835\udc58 ) \u2212\ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 and \ud835\udf16\ud835\udc36 (\ud835\udc36\ud835\udc58 \u2212 \ud835\udc5a (\ud835\udc58 ) )2. Since the true variance \ud835\udc58 = of \ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 is unknown, the type of hypothesis testing is one-sided paired-sample t-test. The upper bound of the confidence interval (CI) is given by (cid:205) 1 | D\ud835\udc632 | \ud835\udc58 are given as \ud835\udf16\ud835\udc58 = (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 1 | D\ud835\udc632 | \ud835\udc36\ud835\udc3c\ud835\udc62 = \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) + \ud835\udc611\u2212\ud835\udefc,\ud835\udf08 \u00b7"}], "doc_text": ") maps \ud835\udc63 Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 \ud835\udc65! \ud835\udc65\"#$ \ud835\udc65~\ud835\udca9(0, 1) \ud835\udc65! \ud835\udf19 Figure 2: Illustration of our analysis pipeline for voice-face correlation. We randomly pick two voice recordings with shared speaker identity as \ud835\udc63 and \ud835\udc63 \u2032. We then analyze the relationship between each AM and voice by predicting each AM from voice with an estimator and an intermediate voice code \ud835\udc52. The optional phonatory module equips a diffusion-based voice generation model with a voice code \ud835\udc52 as a condition to conduct voice style cloning to help us understand the relationship between face geometry and voice characteristics, which serves as an additional constraint to enforce the estimator learn voice identity. We analyze and select AMs with hypothesis testing. The statically significantly predictable AMs are utilized for 3D facial shape reconstruction for further analysis. (a) Landmarks (b) Proportion(c) Angle(d) Distance Figure 3: Examples of summarized AMs. We summarized three types of AM: proportion, angle and distance. Those AMs are computed from the predefined landmark on the 3D face repre- sentation. into the mean of the \ud835\udc56-th predicted AM. Similarly, we define an uncertainty estimator \ud835\udc3a\ud835\udc3f (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) : \ud835\udc63 \u21a6\u2192 R+ \u222a {0} that \ud835\udc63 into the variance of the \ud835\udc58-th predicted AM. Again, E\ud835\udc58 and \ud835\udf19\ud835\udc58 are the learn- able parameters. The predicted AM and its ground truth become N (\ud835\udc39\ud835\udc58 (\ud835\udc63), \ud835\udc3a\ud835\udc58 (\ud835\udc63)) and N (\ud835\udc5a (0), 0) respectively [20]. Given two ran- dom variables, a more reasonable learning objective is to minimize their KL divergence. AMs when the predicted variances are small, and defer the voice recordings to human experts otherwise. An extreme case is \ud835\udc3a\ud835\udc58 (\ud835\udc63) \u2261 1 where the uncertainty learning objective degrades to the regular regression model. Temporal aggregation. In practice, following the convention of voice understanding, the long voice recording \ud835\udc63 is fed into the net- work in the form of multiple short segments {\ud835\udc63 (1), \u00b7 \u00b7 \u00b7 , \ud835\udc63 (\ud835\udc3f) }. We obtain a sequence of means and variances of the predicted AM. During training, we compute the loss for each segment individually and average them as the training loss. While during evaluation, the predicted AM and its uncertainty are given by aggregating the pre- dictions among all segments. Assuming the short segments from a long recording are class-conditionally independent, the formulations of aggregation are \u02c6\ud835\udc5a (\ud835\udc58 ) = 1 \ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc3f \u2211\ufe01 \ud835\udc59=1 \ud835\udc64 (\ud835\udc58 ) \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) 1 \ud835\udc3a\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ) \ud835\udc39\ud835\udc58 (\ud835\udc63 (\ud835\udc59 ) ), {E\u2217 \ud835\udc58, \ud835\udf14\u2217 \ud835\udc58, \ud835\udf19 \u2217 \ud835\udc58 } = arg min E\ud835\udc58 ,\ud835\udf14\ud835\udc58 ,\ud835\udf19\ud835\udc58 1 |D\ud835\udc61 | \u2211\ufe01 (\ud835\udc63,\ud835\udc5a (\ud835\udc58 ) ) \u2208 D\ud835\udc61 (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) +ln\ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) (2) For a fixed (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2, there is an optimal variance \ud835\udc3a\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf19\ud835\udc58 ) = (\ud835\udc39\ud835\udc58 (\ud835\udc63; E\ud835\udc58, \ud835\udf14\ud835\udc58 ) \u2212 \ud835\udc5a (\ud835\udc58 ) )2 such that the loss function is minimized. Thereby the uncertainty estimator \ud835\udc3a\ud835\udc58 is learned to produce a small variance if the prediction error is small and vice versa. On the contrary, a smaller variance indicates that the predicted AM is more likely to yield a small prediction error,i.e., close to the ground truth. In this way, we can choose to trust the predicted where \u02c6\ud835\udc5a (\ud835\udc58 ) is the aggregated mean and also the predicted \ud835\udc58-th AM. However, the aggregated variance \ud835\udc64 (\ud835\udc58 ) is not used as the uncer- tainty of the predicted \ud835\udc58-th AM since the conditional independence assumption does not always hold in cases such as noises, silences, the computed aggregated variance will be biased by the number of voice segments in the long recording. So we calibrate the uncertainty as \u02c6\ud835\udc64 (\ud835\udc58 ) = \ud835\udc3f \u00b7 \ud835\udc64 (\ud835\udc58 ) . Predictable AM identification. We have collected a number of AMs and trained estimators for predicting them. However, only a few of the AMs are actually predictable from voice, which we had anticipated while designing the task. To identify those AMs, we (3) Rethinking Voice-Face Correlation: A Geometry View use hypothesis testing to them. Formally, we can write the null and alternative hypotheses for the \ud835\udc58-th AM as \ud835\udc3b0 : the AM \ud835\udc5a (\ud835\udc58 ) is NOT predictable from voice \ud835\udc3b1 : the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice In order to reject \ud835\udc3b0, we only need to find a counterexample to show that voice is indeed useful in predicting AM \ud835\udc5a (\ud835\udc58 ) . An effective example is to compare the estimators with and without the voice input. If there exists a learned estimator \ud835\udc39\ud835\udc58 (\ud835\udc63) performing better than the chance-level estimator \ud835\udc36\ud835\udc58 without using voice input and the results are statistically significant, we can successfully reject \ud835\udc3b0 and accept \ud835\udc3b1. Here the chance-level estimator for the \ud835\udc58-th AM is \ud835\udc5a (\ud835\udc58 ) , which is the mean \ud835\udc5a (\ud835\udc58 ) of a constant \ud835\udc36\ud835\udc58 = 1 | D\ud835\udc61 | the training set D\ud835\udc61 . So the null and alternative hypothesis can be rewritten as (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc61 \ud835\udc3b0 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc3b1 : \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u2264 1 \ud835\udc58 ) \u2265 1 where \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 are the mean square errors of estimators with and without voice inputs on validation set D\ud835\udc632, respectively. The formula- tions of \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 ( \u02c6\ud835\udc5a (\ud835\udc58 ) \u2212\ud835\udc5a (\ud835\udc58 ) )2 \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 and \ud835\udf16\ud835\udc36 (\ud835\udc36\ud835\udc58 \u2212 \ud835\udc5a (\ud835\udc58 ) )2. Since the true variance \ud835\udc58 = of \ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 is unknown, the type of hypothesis testing is one-sided paired-sample t-test. The upper bound of the confidence interval (CI) is given by (cid:205) 1 | D\ud835\udc632 | \ud835\udc58 are given as \ud835\udf16\ud835\udc58 = (cid:205) \ud835\udc5a (\ud835\udc58 ) \u2208 D\ud835\udc632 1 | D\ud835\udc632 | \ud835\udc36\ud835\udc3c\ud835\udc62 = \ud835\udf07 (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) + \ud835\udc611\u2212\ud835\udefc,\ud835\udf08 \u00b7"}