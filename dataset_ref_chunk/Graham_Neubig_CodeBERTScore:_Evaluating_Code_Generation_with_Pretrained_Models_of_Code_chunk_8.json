{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_CodeBERTScore:_Evaluating_Code_Generation_with_Pretrained_Models_of_Code_chunk_8.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the function of the distinguishability metric mentioned in the text?", "answer": " The function of the distinguishability metric is to measure how well CodeBERTScore can differentiate between similar and dissimilar code snippets.", "ref_chunk": "such that yi, yj \u2208 Ck} Pairinter ={(yi, yj) | \u2203k such that yi \u2208 Ck, yj /\u2208 Ck} where Ck is the k-th cluster with semantically Intuitively, a similar- equivalent code snippets. ity function f that can distinguish between similar and dissimilar code will produce d larger than 1, meaning that a pair of code snippets from the same semantic cluster has a higher similarity score than a pair of snippets from different clusters. Since the number of intra-class and inter-class pairs grows quadratically with the number of code snippets, in our experiments we followed Eghbali and Pradel (2022) to sample N inter- and N intra-class pairs instead. F.2 Dataset with Semantically equivalent clusters We follow Eghbali and Pradel (2022) to evaluate whether CodeBERTScore can distinguish similar and dissimilar code mined from ShareCode9, an online coding competition platform. Semantically equivalent code snippets are from the same coding problem, and they all pass the unit tests provided by the platform. The dataset consists 6958 code 9https://sharecode.io/ Java C++ Python JavaScript Metric \u03c4 rs \u03c4 rs \u03c4 rs \u03c4 rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481(\u00b1.030) .496(\u00b1.034) .516(\u00b1.052) .525(\u00b1.049) .508(\u00b1.060) .558(\u00b1.058) .532(\u00b1.067) .471(\u00b1.024) .361(\u00b1.037) .324(\u00b1.037) .318(\u00b1.043) .315(\u00b1.047) .344(\u00b1.038) .383(\u00b1.027) .319(\u00b1.035) .273(\u00b1.067) .112(\u00b1.059) .175(\u00b1.021) .262(\u00b1.073) .270(\u00b1.073) .258(\u00b1.091) .301(\u00b1.061) .319(\u00b1.056) .046(\u00b1.009) .301(\u00b1.054) .201(\u00b1.037) .260(\u00b1.024) .273(\u00b1.036) .288(\u00b1.027) .321(\u00b1.023) .321(\u00b1.020) .095(\u00b1.064) .393(\u00b1.083) .366(\u00b1.079) .368(\u00b1.092) .365(\u00b1.094) .338(\u00b1.103) .418(\u00b1.090) .394(\u00b1.096) .391(\u00b1.080) .352(\u00b1.064) .326(\u00b1.075) .334(\u00b1.054) .322(\u00b1.077) .350(\u00b1.064) .402(\u00b1.049) .379(\u00b1.058) .309(\u00b1.073) .248(\u00b1.075) .261(\u00b1.065) .279(\u00b1.092) .261(\u00b1.077) .271(\u00b1.078) .324(\u00b1.075) .302(\u00b1.073) .118(\u00b1.057) .343(\u00b1.052) .299(\u00b1.043) .280(\u00b1.068) .292(\u00b1.057) .293(\u00b1.046) .415(\u00b1.022) .374(\u00b1.044) .059(\u00b1.069) CodeBERTScore .553(\u00b1.068) .369(\u00b1.049) .327(\u00b1.086) .393(\u00b1.048) .422(\u00b1.090) .415(\u00b1.071) .319(\u00b1.054) .402(\u00b1.030) Table 3: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs, along with the standard deviation. Metric \u03c4 rp rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .374(\u00b1.025) .350(\u00b1.037) .397(\u00b1.023) .429(\u00b1.025) .420(\u00b1.037) .366(\u00b1.033) .470(\u00b1.029) .411(\u00b1.030) .604(\u00b1.016) .539(\u00b1.033) .604(\u00b1.016) .629(\u00b1.015) .619(\u00b1.014) .581(\u00b1.016) .635(\u00b1.023) .598(\u00b1.019) .543(\u00b1.018) .495(\u00b1.037) .570(\u00b1.018) .588(\u00b1.022) .574(\u00b1.022) .540(\u00b1.022) .623(\u00b1.018) .576(\u00b1.034) CodeBertScore .517(\u00b1.024) .674(\u00b1.012) .662(\u00b1.012) Table 4: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spearman (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Numbers inside parentheses indicate the standard deviations. Metric BLEU CodeBLEU CrystalBLEU Java 2.36 1.44 5.96 C++ 2.51 1.42 6.94 This result confirms that CodeBERTScore assigns higher similarity scores to semantically similar code pairs, compared to randomly paired snippets that belong to different semantic classes. CodeBERTScore 9.56 9.13 Table 5: Distinguishability with different metrics as the similarity function. CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, on the same datasets. snippets covering 278 problems in Java and C++. We use CodeBERTScore to calculate the similar- ity score for code pairs that share the same seman- tic class and code pairs that do not. We then mea- sure the distinguishability of CodeBERTScore ac- cording to Equation 7. The results are shown in Table 5. Table 5 shows that CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, in both Java and C++. CodeBERTScore achieves distinguisha- bility scores of 9.56 in Java while CrystalBLEU achieves 5.96; in C++, CodeBERTScore achieves 9.13 while CrystalBLEU achieves only 6.94. Can We Hack the Distinguishability Metric? Despite the encouraging results in Table 5, we also found that distinguishability can be easily manipu- lated since it compares absolute scores across dif- ferent metrics. For example, while CrystalBLEU achieves a distinguishability score of 5.96, we can craft a variant of CodeBERTScore that achieves a distinguishability score of 120,000 by simple ex- ponentiation of CodeBERTScore\u2019s output score. To illustrate this, we conducted a distinguisha- bility evaluation with the same configurations as before, but with a variant of CodeBERTScore that we call CodeBERTScorek, and defined as the composition of CodeBERTScore with the f (x) = xk function, that is: CodeBERTScorek (y1, y2) = (CodeBERTScore (y1, y2))k. As Figure 8 shows, distinguishability of CodeBERTScorek increases almost exponentially while increasing k, although the base Code- BERTScore metric has not changed. 130,000 120,000 110,000 100,000 90,000 80,000 70,000 60,000 50,000 40,000 30,000 20,000 10,000 0 CodeBERTScore k 0 10 20 30 40 k Figure 8: Distinguishability by exponentiating the orig- inal CodeBERTScore by k. We thus argue that distinguishability is not a reliable meta-metric and is no substitute for execution-based- or human-rating. We further sus- pect that any meta-metric that compares exact, ab- solute, scores across different metrics is suscepti- ble to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. of CodeBERTScorek with different values of k are shown in Figure 8. As Figure 8 shows, the distinguishability increases almost exponentially with the increasing value of k. We thus argue that distinguishability is not a reliable meta- metric and is no substitute for execution-based- or human-rating. We further suspect that any meta-metric that compares exact, absolute, scores across different metrics is susceptible to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. The distinguishability results G Additional Examples In this section, we provide additional examples in which CodeBERTScore prefers the functionally correct prediction, while the best baseline metric in each language ranks higher a functionally in- correct prediction, which is inequivalent to the ref- erence. Figure 9 shows an example in Java, and Figure 10 shows a C++ example. 50 Natural Language Question: Reference: /** Find how many times a given substring can be found in the original string. Count overlaping cases. >>> howManyTimes(\"\", \"a\") 0 >>> howManyTimes(\"aaa\", \"a\") 3 >>> howManyTimes(\"aaaa\", \"aa\") 3 */ public static int howManyTimes(String string, String substring) { int times = 0; for (int i = 0; i < string.length() - substring.length() + 1; i++) { if (string.substring(i, i + substring.length()) .equals(substring)) { times += 1; } } return times; } (a) The natural language question. (b) The ground truth reference. Non-equivalent candidate: Equivalent candidate: public"}, {"question": " How does the distinguishability metric in Table 5 compare CodeBERTScore with CrystalBLEU?", "answer": " The distinguishability metric shows that CodeBERTScore achieves higher distinguishability scores than CrystalBLEU for both Java and C++ datasets.", "ref_chunk": "such that yi, yj \u2208 Ck} Pairinter ={(yi, yj) | \u2203k such that yi \u2208 Ck, yj /\u2208 Ck} where Ck is the k-th cluster with semantically Intuitively, a similar- equivalent code snippets. ity function f that can distinguish between similar and dissimilar code will produce d larger than 1, meaning that a pair of code snippets from the same semantic cluster has a higher similarity score than a pair of snippets from different clusters. Since the number of intra-class and inter-class pairs grows quadratically with the number of code snippets, in our experiments we followed Eghbali and Pradel (2022) to sample N inter- and N intra-class pairs instead. F.2 Dataset with Semantically equivalent clusters We follow Eghbali and Pradel (2022) to evaluate whether CodeBERTScore can distinguish similar and dissimilar code mined from ShareCode9, an online coding competition platform. Semantically equivalent code snippets are from the same coding problem, and they all pass the unit tests provided by the platform. The dataset consists 6958 code 9https://sharecode.io/ Java C++ Python JavaScript Metric \u03c4 rs \u03c4 rs \u03c4 rs \u03c4 rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481(\u00b1.030) .496(\u00b1.034) .516(\u00b1.052) .525(\u00b1.049) .508(\u00b1.060) .558(\u00b1.058) .532(\u00b1.067) .471(\u00b1.024) .361(\u00b1.037) .324(\u00b1.037) .318(\u00b1.043) .315(\u00b1.047) .344(\u00b1.038) .383(\u00b1.027) .319(\u00b1.035) .273(\u00b1.067) .112(\u00b1.059) .175(\u00b1.021) .262(\u00b1.073) .270(\u00b1.073) .258(\u00b1.091) .301(\u00b1.061) .319(\u00b1.056) .046(\u00b1.009) .301(\u00b1.054) .201(\u00b1.037) .260(\u00b1.024) .273(\u00b1.036) .288(\u00b1.027) .321(\u00b1.023) .321(\u00b1.020) .095(\u00b1.064) .393(\u00b1.083) .366(\u00b1.079) .368(\u00b1.092) .365(\u00b1.094) .338(\u00b1.103) .418(\u00b1.090) .394(\u00b1.096) .391(\u00b1.080) .352(\u00b1.064) .326(\u00b1.075) .334(\u00b1.054) .322(\u00b1.077) .350(\u00b1.064) .402(\u00b1.049) .379(\u00b1.058) .309(\u00b1.073) .248(\u00b1.075) .261(\u00b1.065) .279(\u00b1.092) .261(\u00b1.077) .271(\u00b1.078) .324(\u00b1.075) .302(\u00b1.073) .118(\u00b1.057) .343(\u00b1.052) .299(\u00b1.043) .280(\u00b1.068) .292(\u00b1.057) .293(\u00b1.046) .415(\u00b1.022) .374(\u00b1.044) .059(\u00b1.069) CodeBERTScore .553(\u00b1.068) .369(\u00b1.049) .327(\u00b1.086) .393(\u00b1.048) .422(\u00b1.090) .415(\u00b1.071) .319(\u00b1.054) .402(\u00b1.030) Table 3: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs, along with the standard deviation. Metric \u03c4 rp rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .374(\u00b1.025) .350(\u00b1.037) .397(\u00b1.023) .429(\u00b1.025) .420(\u00b1.037) .366(\u00b1.033) .470(\u00b1.029) .411(\u00b1.030) .604(\u00b1.016) .539(\u00b1.033) .604(\u00b1.016) .629(\u00b1.015) .619(\u00b1.014) .581(\u00b1.016) .635(\u00b1.023) .598(\u00b1.019) .543(\u00b1.018) .495(\u00b1.037) .570(\u00b1.018) .588(\u00b1.022) .574(\u00b1.022) .540(\u00b1.022) .623(\u00b1.018) .576(\u00b1.034) CodeBertScore .517(\u00b1.024) .674(\u00b1.012) .662(\u00b1.012) Table 4: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spearman (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Numbers inside parentheses indicate the standard deviations. Metric BLEU CodeBLEU CrystalBLEU Java 2.36 1.44 5.96 C++ 2.51 1.42 6.94 This result confirms that CodeBERTScore assigns higher similarity scores to semantically similar code pairs, compared to randomly paired snippets that belong to different semantic classes. CodeBERTScore 9.56 9.13 Table 5: Distinguishability with different metrics as the similarity function. CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, on the same datasets. snippets covering 278 problems in Java and C++. We use CodeBERTScore to calculate the similar- ity score for code pairs that share the same seman- tic class and code pairs that do not. We then mea- sure the distinguishability of CodeBERTScore ac- cording to Equation 7. The results are shown in Table 5. Table 5 shows that CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, in both Java and C++. CodeBERTScore achieves distinguisha- bility scores of 9.56 in Java while CrystalBLEU achieves 5.96; in C++, CodeBERTScore achieves 9.13 while CrystalBLEU achieves only 6.94. Can We Hack the Distinguishability Metric? Despite the encouraging results in Table 5, we also found that distinguishability can be easily manipu- lated since it compares absolute scores across dif- ferent metrics. For example, while CrystalBLEU achieves a distinguishability score of 5.96, we can craft a variant of CodeBERTScore that achieves a distinguishability score of 120,000 by simple ex- ponentiation of CodeBERTScore\u2019s output score. To illustrate this, we conducted a distinguisha- bility evaluation with the same configurations as before, but with a variant of CodeBERTScore that we call CodeBERTScorek, and defined as the composition of CodeBERTScore with the f (x) = xk function, that is: CodeBERTScorek (y1, y2) = (CodeBERTScore (y1, y2))k. As Figure 8 shows, distinguishability of CodeBERTScorek increases almost exponentially while increasing k, although the base Code- BERTScore metric has not changed. 130,000 120,000 110,000 100,000 90,000 80,000 70,000 60,000 50,000 40,000 30,000 20,000 10,000 0 CodeBERTScore k 0 10 20 30 40 k Figure 8: Distinguishability by exponentiating the orig- inal CodeBERTScore by k. We thus argue that distinguishability is not a reliable meta-metric and is no substitute for execution-based- or human-rating. We further sus- pect that any meta-metric that compares exact, ab- solute, scores across different metrics is suscepti- ble to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. of CodeBERTScorek with different values of k are shown in Figure 8. As Figure 8 shows, the distinguishability increases almost exponentially with the increasing value of k. We thus argue that distinguishability is not a reliable meta- metric and is no substitute for execution-based- or human-rating. We further suspect that any meta-metric that compares exact, absolute, scores across different metrics is susceptible to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. The distinguishability results G Additional Examples In this section, we provide additional examples in which CodeBERTScore prefers the functionally correct prediction, while the best baseline metric in each language ranks higher a functionally in- correct prediction, which is inequivalent to the ref- erence. Figure 9 shows an example in Java, and Figure 10 shows a C++ example. 50 Natural Language Question: Reference: /** Find how many times a given substring can be found in the original string. Count overlaping cases. >>> howManyTimes(\"\", \"a\") 0 >>> howManyTimes(\"aaa\", \"a\") 3 >>> howManyTimes(\"aaaa\", \"aa\") 3 */ public static int howManyTimes(String string, String substring) { int times = 0; for (int i = 0; i < string.length() - substring.length() + 1; i++) { if (string.substring(i, i + substring.length()) .equals(substring)) { times += 1; } } return times; } (a) The natural language question. (b) The ground truth reference. Non-equivalent candidate: Equivalent candidate: public"}, {"question": " How can the distinguishability metric be manipulated according to the text?", "answer": " The distinguishability metric can be manipulated by exponentiating the output score of CodeBERTScore, resulting in inflated distinguishability scores.", "ref_chunk": "such that yi, yj \u2208 Ck} Pairinter ={(yi, yj) | \u2203k such that yi \u2208 Ck, yj /\u2208 Ck} where Ck is the k-th cluster with semantically Intuitively, a similar- equivalent code snippets. ity function f that can distinguish between similar and dissimilar code will produce d larger than 1, meaning that a pair of code snippets from the same semantic cluster has a higher similarity score than a pair of snippets from different clusters. Since the number of intra-class and inter-class pairs grows quadratically with the number of code snippets, in our experiments we followed Eghbali and Pradel (2022) to sample N inter- and N intra-class pairs instead. F.2 Dataset with Semantically equivalent clusters We follow Eghbali and Pradel (2022) to evaluate whether CodeBERTScore can distinguish similar and dissimilar code mined from ShareCode9, an online coding competition platform. Semantically equivalent code snippets are from the same coding problem, and they all pass the unit tests provided by the platform. The dataset consists 6958 code 9https://sharecode.io/ Java C++ Python JavaScript Metric \u03c4 rs \u03c4 rs \u03c4 rs \u03c4 rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481(\u00b1.030) .496(\u00b1.034) .516(\u00b1.052) .525(\u00b1.049) .508(\u00b1.060) .558(\u00b1.058) .532(\u00b1.067) .471(\u00b1.024) .361(\u00b1.037) .324(\u00b1.037) .318(\u00b1.043) .315(\u00b1.047) .344(\u00b1.038) .383(\u00b1.027) .319(\u00b1.035) .273(\u00b1.067) .112(\u00b1.059) .175(\u00b1.021) .262(\u00b1.073) .270(\u00b1.073) .258(\u00b1.091) .301(\u00b1.061) .319(\u00b1.056) .046(\u00b1.009) .301(\u00b1.054) .201(\u00b1.037) .260(\u00b1.024) .273(\u00b1.036) .288(\u00b1.027) .321(\u00b1.023) .321(\u00b1.020) .095(\u00b1.064) .393(\u00b1.083) .366(\u00b1.079) .368(\u00b1.092) .365(\u00b1.094) .338(\u00b1.103) .418(\u00b1.090) .394(\u00b1.096) .391(\u00b1.080) .352(\u00b1.064) .326(\u00b1.075) .334(\u00b1.054) .322(\u00b1.077) .350(\u00b1.064) .402(\u00b1.049) .379(\u00b1.058) .309(\u00b1.073) .248(\u00b1.075) .261(\u00b1.065) .279(\u00b1.092) .261(\u00b1.077) .271(\u00b1.078) .324(\u00b1.075) .302(\u00b1.073) .118(\u00b1.057) .343(\u00b1.052) .299(\u00b1.043) .280(\u00b1.068) .292(\u00b1.057) .293(\u00b1.046) .415(\u00b1.022) .374(\u00b1.044) .059(\u00b1.069) CodeBERTScore .553(\u00b1.068) .369(\u00b1.049) .327(\u00b1.086) .393(\u00b1.048) .422(\u00b1.090) .415(\u00b1.071) .319(\u00b1.054) .402(\u00b1.030) Table 3: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs, along with the standard deviation. Metric \u03c4 rp rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .374(\u00b1.025) .350(\u00b1.037) .397(\u00b1.023) .429(\u00b1.025) .420(\u00b1.037) .366(\u00b1.033) .470(\u00b1.029) .411(\u00b1.030) .604(\u00b1.016) .539(\u00b1.033) .604(\u00b1.016) .629(\u00b1.015) .619(\u00b1.014) .581(\u00b1.016) .635(\u00b1.023) .598(\u00b1.019) .543(\u00b1.018) .495(\u00b1.037) .570(\u00b1.018) .588(\u00b1.022) .574(\u00b1.022) .540(\u00b1.022) .623(\u00b1.018) .576(\u00b1.034) CodeBertScore .517(\u00b1.024) .674(\u00b1.012) .662(\u00b1.012) Table 4: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spearman (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Numbers inside parentheses indicate the standard deviations. Metric BLEU CodeBLEU CrystalBLEU Java 2.36 1.44 5.96 C++ 2.51 1.42 6.94 This result confirms that CodeBERTScore assigns higher similarity scores to semantically similar code pairs, compared to randomly paired snippets that belong to different semantic classes. CodeBERTScore 9.56 9.13 Table 5: Distinguishability with different metrics as the similarity function. CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, on the same datasets. snippets covering 278 problems in Java and C++. We use CodeBERTScore to calculate the similar- ity score for code pairs that share the same seman- tic class and code pairs that do not. We then mea- sure the distinguishability of CodeBERTScore ac- cording to Equation 7. The results are shown in Table 5. Table 5 shows that CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, in both Java and C++. CodeBERTScore achieves distinguisha- bility scores of 9.56 in Java while CrystalBLEU achieves 5.96; in C++, CodeBERTScore achieves 9.13 while CrystalBLEU achieves only 6.94. Can We Hack the Distinguishability Metric? Despite the encouraging results in Table 5, we also found that distinguishability can be easily manipu- lated since it compares absolute scores across dif- ferent metrics. For example, while CrystalBLEU achieves a distinguishability score of 5.96, we can craft a variant of CodeBERTScore that achieves a distinguishability score of 120,000 by simple ex- ponentiation of CodeBERTScore\u2019s output score. To illustrate this, we conducted a distinguisha- bility evaluation with the same configurations as before, but with a variant of CodeBERTScore that we call CodeBERTScorek, and defined as the composition of CodeBERTScore with the f (x) = xk function, that is: CodeBERTScorek (y1, y2) = (CodeBERTScore (y1, y2))k. As Figure 8 shows, distinguishability of CodeBERTScorek increases almost exponentially while increasing k, although the base Code- BERTScore metric has not changed. 130,000 120,000 110,000 100,000 90,000 80,000 70,000 60,000 50,000 40,000 30,000 20,000 10,000 0 CodeBERTScore k 0 10 20 30 40 k Figure 8: Distinguishability by exponentiating the orig- inal CodeBERTScore by k. We thus argue that distinguishability is not a reliable meta-metric and is no substitute for execution-based- or human-rating. We further sus- pect that any meta-metric that compares exact, ab- solute, scores across different metrics is suscepti- ble to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. of CodeBERTScorek with different values of k are shown in Figure 8. As Figure 8 shows, the distinguishability increases almost exponentially with the increasing value of k. We thus argue that distinguishability is not a reliable meta- metric and is no substitute for execution-based- or human-rating. We further suspect that any meta-metric that compares exact, absolute, scores across different metrics is susceptible to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. The distinguishability results G Additional Examples In this section, we provide additional examples in which CodeBERTScore prefers the functionally correct prediction, while the best baseline metric in each language ranks higher a functionally in- correct prediction, which is inequivalent to the ref- erence. Figure 9 shows an example in Java, and Figure 10 shows a C++ example. 50 Natural Language Question: Reference: /** Find how many times a given substring can be found in the original string. Count overlaping cases. >>> howManyTimes(\"\", \"a\") 0 >>> howManyTimes(\"aaa\", \"a\") 3 >>> howManyTimes(\"aaaa\", \"aa\") 3 */ public static int howManyTimes(String string, String substring) { int times = 0; for (int i = 0; i < string.length() - substring.length() + 1; i++) { if (string.substring(i, i + substring.length()) .equals(substring)) { times += 1; } } return times; } (a) The natural language question. (b) The ground truth reference. Non-equivalent candidate: Equivalent candidate: public"}, {"question": " Why is distinguishability not considered a reliable meta-metric?", "answer": " Distinguishability is not considered reliable because it can be easily manipulated by comparing absolute scores across different metrics.", "ref_chunk": "such that yi, yj \u2208 Ck} Pairinter ={(yi, yj) | \u2203k such that yi \u2208 Ck, yj /\u2208 Ck} where Ck is the k-th cluster with semantically Intuitively, a similar- equivalent code snippets. ity function f that can distinguish between similar and dissimilar code will produce d larger than 1, meaning that a pair of code snippets from the same semantic cluster has a higher similarity score than a pair of snippets from different clusters. Since the number of intra-class and inter-class pairs grows quadratically with the number of code snippets, in our experiments we followed Eghbali and Pradel (2022) to sample N inter- and N intra-class pairs instead. F.2 Dataset with Semantically equivalent clusters We follow Eghbali and Pradel (2022) to evaluate whether CodeBERTScore can distinguish similar and dissimilar code mined from ShareCode9, an online coding competition platform. Semantically equivalent code snippets are from the same coding problem, and they all pass the unit tests provided by the platform. The dataset consists 6958 code 9https://sharecode.io/ Java C++ Python JavaScript Metric \u03c4 rs \u03c4 rs \u03c4 rs \u03c4 rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481(\u00b1.030) .496(\u00b1.034) .516(\u00b1.052) .525(\u00b1.049) .508(\u00b1.060) .558(\u00b1.058) .532(\u00b1.067) .471(\u00b1.024) .361(\u00b1.037) .324(\u00b1.037) .318(\u00b1.043) .315(\u00b1.047) .344(\u00b1.038) .383(\u00b1.027) .319(\u00b1.035) .273(\u00b1.067) .112(\u00b1.059) .175(\u00b1.021) .262(\u00b1.073) .270(\u00b1.073) .258(\u00b1.091) .301(\u00b1.061) .319(\u00b1.056) .046(\u00b1.009) .301(\u00b1.054) .201(\u00b1.037) .260(\u00b1.024) .273(\u00b1.036) .288(\u00b1.027) .321(\u00b1.023) .321(\u00b1.020) .095(\u00b1.064) .393(\u00b1.083) .366(\u00b1.079) .368(\u00b1.092) .365(\u00b1.094) .338(\u00b1.103) .418(\u00b1.090) .394(\u00b1.096) .391(\u00b1.080) .352(\u00b1.064) .326(\u00b1.075) .334(\u00b1.054) .322(\u00b1.077) .350(\u00b1.064) .402(\u00b1.049) .379(\u00b1.058) .309(\u00b1.073) .248(\u00b1.075) .261(\u00b1.065) .279(\u00b1.092) .261(\u00b1.077) .271(\u00b1.078) .324(\u00b1.075) .302(\u00b1.073) .118(\u00b1.057) .343(\u00b1.052) .299(\u00b1.043) .280(\u00b1.068) .292(\u00b1.057) .293(\u00b1.046) .415(\u00b1.022) .374(\u00b1.044) .059(\u00b1.069) CodeBERTScore .553(\u00b1.068) .369(\u00b1.049) .327(\u00b1.086) .393(\u00b1.048) .422(\u00b1.090) .415(\u00b1.071) .319(\u00b1.054) .402(\u00b1.030) Table 3: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs, along with the standard deviation. Metric \u03c4 rp rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .374(\u00b1.025) .350(\u00b1.037) .397(\u00b1.023) .429(\u00b1.025) .420(\u00b1.037) .366(\u00b1.033) .470(\u00b1.029) .411(\u00b1.030) .604(\u00b1.016) .539(\u00b1.033) .604(\u00b1.016) .629(\u00b1.015) .619(\u00b1.014) .581(\u00b1.016) .635(\u00b1.023) .598(\u00b1.019) .543(\u00b1.018) .495(\u00b1.037) .570(\u00b1.018) .588(\u00b1.022) .574(\u00b1.022) .540(\u00b1.022) .623(\u00b1.018) .576(\u00b1.034) CodeBertScore .517(\u00b1.024) .674(\u00b1.012) .662(\u00b1.012) Table 4: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spearman (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Numbers inside parentheses indicate the standard deviations. Metric BLEU CodeBLEU CrystalBLEU Java 2.36 1.44 5.96 C++ 2.51 1.42 6.94 This result confirms that CodeBERTScore assigns higher similarity scores to semantically similar code pairs, compared to randomly paired snippets that belong to different semantic classes. CodeBERTScore 9.56 9.13 Table 5: Distinguishability with different metrics as the similarity function. CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, on the same datasets. snippets covering 278 problems in Java and C++. We use CodeBERTScore to calculate the similar- ity score for code pairs that share the same seman- tic class and code pairs that do not. We then mea- sure the distinguishability of CodeBERTScore ac- cording to Equation 7. The results are shown in Table 5. Table 5 shows that CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, in both Java and C++. CodeBERTScore achieves distinguisha- bility scores of 9.56 in Java while CrystalBLEU achieves 5.96; in C++, CodeBERTScore achieves 9.13 while CrystalBLEU achieves only 6.94. Can We Hack the Distinguishability Metric? Despite the encouraging results in Table 5, we also found that distinguishability can be easily manipu- lated since it compares absolute scores across dif- ferent metrics. For example, while CrystalBLEU achieves a distinguishability score of 5.96, we can craft a variant of CodeBERTScore that achieves a distinguishability score of 120,000 by simple ex- ponentiation of CodeBERTScore\u2019s output score. To illustrate this, we conducted a distinguisha- bility evaluation with the same configurations as before, but with a variant of CodeBERTScore that we call CodeBERTScorek, and defined as the composition of CodeBERTScore with the f (x) = xk function, that is: CodeBERTScorek (y1, y2) = (CodeBERTScore (y1, y2))k. As Figure 8 shows, distinguishability of CodeBERTScorek increases almost exponentially while increasing k, although the base Code- BERTScore metric has not changed. 130,000 120,000 110,000 100,000 90,000 80,000 70,000 60,000 50,000 40,000 30,000 20,000 10,000 0 CodeBERTScore k 0 10 20 30 40 k Figure 8: Distinguishability by exponentiating the orig- inal CodeBERTScore by k. We thus argue that distinguishability is not a reliable meta-metric and is no substitute for execution-based- or human-rating. We further sus- pect that any meta-metric that compares exact, ab- solute, scores across different metrics is suscepti- ble to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. of CodeBERTScorek with different values of k are shown in Figure 8. As Figure 8 shows, the distinguishability increases almost exponentially with the increasing value of k. We thus argue that distinguishability is not a reliable meta- metric and is no substitute for execution-based- or human-rating. We further suspect that any meta-metric that compares exact, absolute, scores across different metrics is susceptible to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. The distinguishability results G Additional Examples In this section, we provide additional examples in which CodeBERTScore prefers the functionally correct prediction, while the best baseline metric in each language ranks higher a functionally in- correct prediction, which is inequivalent to the ref- erence. Figure 9 shows an example in Java, and Figure 10 shows a C++ example. 50 Natural Language Question: Reference: /** Find how many times a given substring can be found in the original string. Count overlaping cases. >>> howManyTimes(\"\", \"a\") 0 >>> howManyTimes(\"aaa\", \"a\") 3 >>> howManyTimes(\"aaaa\", \"aa\") 3 */ public static int howManyTimes(String string, String substring) { int times = 0; for (int i = 0; i < string.length() - substring.length() + 1; i++) { if (string.substring(i, i + substring.length()) .equals(substring)) { times += 1; } } return times; } (a) The natural language question. (b) The ground truth reference. Non-equivalent candidate: Equivalent candidate: public"}, {"question": " What does the text suggest is a more reliable way to compare metrics?", "answer": " The text suggests that comparing metrics based on how they rank different examples is more reliable than comparing them based on exact scores.", "ref_chunk": "such that yi, yj \u2208 Ck} Pairinter ={(yi, yj) | \u2203k such that yi \u2208 Ck, yj /\u2208 Ck} where Ck is the k-th cluster with semantically Intuitively, a similar- equivalent code snippets. ity function f that can distinguish between similar and dissimilar code will produce d larger than 1, meaning that a pair of code snippets from the same semantic cluster has a higher similarity score than a pair of snippets from different clusters. Since the number of intra-class and inter-class pairs grows quadratically with the number of code snippets, in our experiments we followed Eghbali and Pradel (2022) to sample N inter- and N intra-class pairs instead. F.2 Dataset with Semantically equivalent clusters We follow Eghbali and Pradel (2022) to evaluate whether CodeBERTScore can distinguish similar and dissimilar code mined from ShareCode9, an online coding competition platform. Semantically equivalent code snippets are from the same coding problem, and they all pass the unit tests provided by the platform. The dataset consists 6958 code 9https://sharecode.io/ Java C++ Python JavaScript Metric \u03c4 rs \u03c4 rs \u03c4 rs \u03c4 rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481(\u00b1.030) .496(\u00b1.034) .516(\u00b1.052) .525(\u00b1.049) .508(\u00b1.060) .558(\u00b1.058) .532(\u00b1.067) .471(\u00b1.024) .361(\u00b1.037) .324(\u00b1.037) .318(\u00b1.043) .315(\u00b1.047) .344(\u00b1.038) .383(\u00b1.027) .319(\u00b1.035) .273(\u00b1.067) .112(\u00b1.059) .175(\u00b1.021) .262(\u00b1.073) .270(\u00b1.073) .258(\u00b1.091) .301(\u00b1.061) .319(\u00b1.056) .046(\u00b1.009) .301(\u00b1.054) .201(\u00b1.037) .260(\u00b1.024) .273(\u00b1.036) .288(\u00b1.027) .321(\u00b1.023) .321(\u00b1.020) .095(\u00b1.064) .393(\u00b1.083) .366(\u00b1.079) .368(\u00b1.092) .365(\u00b1.094) .338(\u00b1.103) .418(\u00b1.090) .394(\u00b1.096) .391(\u00b1.080) .352(\u00b1.064) .326(\u00b1.075) .334(\u00b1.054) .322(\u00b1.077) .350(\u00b1.064) .402(\u00b1.049) .379(\u00b1.058) .309(\u00b1.073) .248(\u00b1.075) .261(\u00b1.065) .279(\u00b1.092) .261(\u00b1.077) .271(\u00b1.078) .324(\u00b1.075) .302(\u00b1.073) .118(\u00b1.057) .343(\u00b1.052) .299(\u00b1.043) .280(\u00b1.068) .292(\u00b1.057) .293(\u00b1.046) .415(\u00b1.022) .374(\u00b1.044) .059(\u00b1.069) CodeBERTScore .553(\u00b1.068) .369(\u00b1.049) .327(\u00b1.086) .393(\u00b1.048) .422(\u00b1.090) .415(\u00b1.071) .319(\u00b1.054) .402(\u00b1.030) Table 3: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs, along with the standard deviation. Metric \u03c4 rp rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .374(\u00b1.025) .350(\u00b1.037) .397(\u00b1.023) .429(\u00b1.025) .420(\u00b1.037) .366(\u00b1.033) .470(\u00b1.029) .411(\u00b1.030) .604(\u00b1.016) .539(\u00b1.033) .604(\u00b1.016) .629(\u00b1.015) .619(\u00b1.014) .581(\u00b1.016) .635(\u00b1.023) .598(\u00b1.019) .543(\u00b1.018) .495(\u00b1.037) .570(\u00b1.018) .588(\u00b1.022) .574(\u00b1.022) .540(\u00b1.022) .623(\u00b1.018) .576(\u00b1.034) CodeBertScore .517(\u00b1.024) .674(\u00b1.012) .662(\u00b1.012) Table 4: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spearman (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Numbers inside parentheses indicate the standard deviations. Metric BLEU CodeBLEU CrystalBLEU Java 2.36 1.44 5.96 C++ 2.51 1.42 6.94 This result confirms that CodeBERTScore assigns higher similarity scores to semantically similar code pairs, compared to randomly paired snippets that belong to different semantic classes. CodeBERTScore 9.56 9.13 Table 5: Distinguishability with different metrics as the similarity function. CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, on the same datasets. snippets covering 278 problems in Java and C++. We use CodeBERTScore to calculate the similar- ity score for code pairs that share the same seman- tic class and code pairs that do not. We then mea- sure the distinguishability of CodeBERTScore ac- cording to Equation 7. The results are shown in Table 5. Table 5 shows that CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, in both Java and C++. CodeBERTScore achieves distinguisha- bility scores of 9.56 in Java while CrystalBLEU achieves 5.96; in C++, CodeBERTScore achieves 9.13 while CrystalBLEU achieves only 6.94. Can We Hack the Distinguishability Metric? Despite the encouraging results in Table 5, we also found that distinguishability can be easily manipu- lated since it compares absolute scores across dif- ferent metrics. For example, while CrystalBLEU achieves a distinguishability score of 5.96, we can craft a variant of CodeBERTScore that achieves a distinguishability score of 120,000 by simple ex- ponentiation of CodeBERTScore\u2019s output score. To illustrate this, we conducted a distinguisha- bility evaluation with the same configurations as before, but with a variant of CodeBERTScore that we call CodeBERTScorek, and defined as the composition of CodeBERTScore with the f (x) = xk function, that is: CodeBERTScorek (y1, y2) = (CodeBERTScore (y1, y2))k. As Figure 8 shows, distinguishability of CodeBERTScorek increases almost exponentially while increasing k, although the base Code- BERTScore metric has not changed. 130,000 120,000 110,000 100,000 90,000 80,000 70,000 60,000 50,000 40,000 30,000 20,000 10,000 0 CodeBERTScore k 0 10 20 30 40 k Figure 8: Distinguishability by exponentiating the orig- inal CodeBERTScore by k. We thus argue that distinguishability is not a reliable meta-metric and is no substitute for execution-based- or human-rating. We further sus- pect that any meta-metric that compares exact, ab- solute, scores across different metrics is suscepti- ble to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. of CodeBERTScorek with different values of k are shown in Figure 8. As Figure 8 shows, the distinguishability increases almost exponentially with the increasing value of k. We thus argue that distinguishability is not a reliable meta- metric and is no substitute for execution-based- or human-rating. We further suspect that any meta-metric that compares exact, absolute, scores across different metrics is susceptible to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. The distinguishability results G Additional Examples In this section, we provide additional examples in which CodeBERTScore prefers the functionally correct prediction, while the best baseline metric in each language ranks higher a functionally in- correct prediction, which is inequivalent to the ref- erence. Figure 9 shows an example in Java, and Figure 10 shows a C++ example. 50 Natural Language Question: Reference: /** Find how many times a given substring can be found in the original string. Count overlaping cases. >>> howManyTimes(\"\", \"a\") 0 >>> howManyTimes(\"aaa\", \"a\") 3 >>> howManyTimes(\"aaaa\", \"aa\") 3 */ public static int howManyTimes(String string, String substring) { int times = 0; for (int i = 0; i < string.length() - substring.length() + 1; i++) { if (string.substring(i, i + substring.length()) .equals(substring)) { times += 1; } } return times; } (a) The natural language question. (b) The ground truth reference. Non-equivalent candidate: Equivalent candidate: public"}, {"question": " What does Figure 8 in the text illustrate?", "answer": " Figure 8 illustrates how distinguishability increases exponentially when the original CodeBERTScore is exponentiated by different values of k.", "ref_chunk": "such that yi, yj \u2208 Ck} Pairinter ={(yi, yj) | \u2203k such that yi \u2208 Ck, yj /\u2208 Ck} where Ck is the k-th cluster with semantically Intuitively, a similar- equivalent code snippets. ity function f that can distinguish between similar and dissimilar code will produce d larger than 1, meaning that a pair of code snippets from the same semantic cluster has a higher similarity score than a pair of snippets from different clusters. Since the number of intra-class and inter-class pairs grows quadratically with the number of code snippets, in our experiments we followed Eghbali and Pradel (2022) to sample N inter- and N intra-class pairs instead. F.2 Dataset with Semantically equivalent clusters We follow Eghbali and Pradel (2022) to evaluate whether CodeBERTScore can distinguish similar and dissimilar code mined from ShareCode9, an online coding competition platform. Semantically equivalent code snippets are from the same coding problem, and they all pass the unit tests provided by the platform. The dataset consists 6958 code 9https://sharecode.io/ Java C++ Python JavaScript Metric \u03c4 rs \u03c4 rs \u03c4 rs \u03c4 rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481(\u00b1.030) .496(\u00b1.034) .516(\u00b1.052) .525(\u00b1.049) .508(\u00b1.060) .558(\u00b1.058) .532(\u00b1.067) .471(\u00b1.024) .361(\u00b1.037) .324(\u00b1.037) .318(\u00b1.043) .315(\u00b1.047) .344(\u00b1.038) .383(\u00b1.027) .319(\u00b1.035) .273(\u00b1.067) .112(\u00b1.059) .175(\u00b1.021) .262(\u00b1.073) .270(\u00b1.073) .258(\u00b1.091) .301(\u00b1.061) .319(\u00b1.056) .046(\u00b1.009) .301(\u00b1.054) .201(\u00b1.037) .260(\u00b1.024) .273(\u00b1.036) .288(\u00b1.027) .321(\u00b1.023) .321(\u00b1.020) .095(\u00b1.064) .393(\u00b1.083) .366(\u00b1.079) .368(\u00b1.092) .365(\u00b1.094) .338(\u00b1.103) .418(\u00b1.090) .394(\u00b1.096) .391(\u00b1.080) .352(\u00b1.064) .326(\u00b1.075) .334(\u00b1.054) .322(\u00b1.077) .350(\u00b1.064) .402(\u00b1.049) .379(\u00b1.058) .309(\u00b1.073) .248(\u00b1.075) .261(\u00b1.065) .279(\u00b1.092) .261(\u00b1.077) .271(\u00b1.078) .324(\u00b1.075) .302(\u00b1.073) .118(\u00b1.057) .343(\u00b1.052) .299(\u00b1.043) .280(\u00b1.068) .292(\u00b1.057) .293(\u00b1.046) .415(\u00b1.022) .374(\u00b1.044) .059(\u00b1.069) CodeBERTScore .553(\u00b1.068) .369(\u00b1.049) .327(\u00b1.086) .393(\u00b1.048) .422(\u00b1.090) .415(\u00b1.071) .319(\u00b1.054) .402(\u00b1.030) Table 3: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs, along with the standard deviation. Metric \u03c4 rp rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .374(\u00b1.025) .350(\u00b1.037) .397(\u00b1.023) .429(\u00b1.025) .420(\u00b1.037) .366(\u00b1.033) .470(\u00b1.029) .411(\u00b1.030) .604(\u00b1.016) .539(\u00b1.033) .604(\u00b1.016) .629(\u00b1.015) .619(\u00b1.014) .581(\u00b1.016) .635(\u00b1.023) .598(\u00b1.019) .543(\u00b1.018) .495(\u00b1.037) .570(\u00b1.018) .588(\u00b1.022) .574(\u00b1.022) .540(\u00b1.022) .623(\u00b1.018) .576(\u00b1.034) CodeBertScore .517(\u00b1.024) .674(\u00b1.012) .662(\u00b1.012) Table 4: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spearman (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Numbers inside parentheses indicate the standard deviations. Metric BLEU CodeBLEU CrystalBLEU Java 2.36 1.44 5.96 C++ 2.51 1.42 6.94 This result confirms that CodeBERTScore assigns higher similarity scores to semantically similar code pairs, compared to randomly paired snippets that belong to different semantic classes. CodeBERTScore 9.56 9.13 Table 5: Distinguishability with different metrics as the similarity function. CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, on the same datasets. snippets covering 278 problems in Java and C++. We use CodeBERTScore to calculate the similar- ity score for code pairs that share the same seman- tic class and code pairs that do not. We then mea- sure the distinguishability of CodeBERTScore ac- cording to Equation 7. The results are shown in Table 5. Table 5 shows that CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, in both Java and C++. CodeBERTScore achieves distinguisha- bility scores of 9.56 in Java while CrystalBLEU achieves 5.96; in C++, CodeBERTScore achieves 9.13 while CrystalBLEU achieves only 6.94. Can We Hack the Distinguishability Metric? Despite the encouraging results in Table 5, we also found that distinguishability can be easily manipu- lated since it compares absolute scores across dif- ferent metrics. For example, while CrystalBLEU achieves a distinguishability score of 5.96, we can craft a variant of CodeBERTScore that achieves a distinguishability score of 120,000 by simple ex- ponentiation of CodeBERTScore\u2019s output score. To illustrate this, we conducted a distinguisha- bility evaluation with the same configurations as before, but with a variant of CodeBERTScore that we call CodeBERTScorek, and defined as the composition of CodeBERTScore with the f (x) = xk function, that is: CodeBERTScorek (y1, y2) = (CodeBERTScore (y1, y2))k. As Figure 8 shows, distinguishability of CodeBERTScorek increases almost exponentially while increasing k, although the base Code- BERTScore metric has not changed. 130,000 120,000 110,000 100,000 90,000 80,000 70,000 60,000 50,000 40,000 30,000 20,000 10,000 0 CodeBERTScore k 0 10 20 30 40 k Figure 8: Distinguishability by exponentiating the orig- inal CodeBERTScore by k. We thus argue that distinguishability is not a reliable meta-metric and is no substitute for execution-based- or human-rating. We further sus- pect that any meta-metric that compares exact, ab- solute, scores across different metrics is suscepti- ble to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. of CodeBERTScorek with different values of k are shown in Figure 8. As Figure 8 shows, the distinguishability increases almost exponentially with the increasing value of k. We thus argue that distinguishability is not a reliable meta- metric and is no substitute for execution-based- or human-rating. We further suspect that any meta-metric that compares exact, absolute, scores across different metrics is susceptible to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. The distinguishability results G Additional Examples In this section, we provide additional examples in which CodeBERTScore prefers the functionally correct prediction, while the best baseline metric in each language ranks higher a functionally in- correct prediction, which is inequivalent to the ref- erence. Figure 9 shows an example in Java, and Figure 10 shows a C++ example. 50 Natural Language Question: Reference: /** Find how many times a given substring can be found in the original string. Count overlaping cases. >>> howManyTimes(\"\", \"a\") 0 >>> howManyTimes(\"aaa\", \"a\") 3 >>> howManyTimes(\"aaaa\", \"aa\") 3 */ public static int howManyTimes(String string, String substring) { int times = 0; for (int i = 0; i < string.length() - substring.length() + 1; i++) { if (string.substring(i, i + substring.length()) .equals(substring)) { times += 1; } } return times; } (a) The natural language question. (b) The ground truth reference. Non-equivalent candidate: Equivalent candidate: public"}, {"question": " How does the text recommend comparing metrics to ensure reliability?", "answer": " The text recommends comparing metrics based on how they rank different examples rather than relying on exact scores for a more reliable comparison.", "ref_chunk": "such that yi, yj \u2208 Ck} Pairinter ={(yi, yj) | \u2203k such that yi \u2208 Ck, yj /\u2208 Ck} where Ck is the k-th cluster with semantically Intuitively, a similar- equivalent code snippets. ity function f that can distinguish between similar and dissimilar code will produce d larger than 1, meaning that a pair of code snippets from the same semantic cluster has a higher similarity score than a pair of snippets from different clusters. Since the number of intra-class and inter-class pairs grows quadratically with the number of code snippets, in our experiments we followed Eghbali and Pradel (2022) to sample N inter- and N intra-class pairs instead. F.2 Dataset with Semantically equivalent clusters We follow Eghbali and Pradel (2022) to evaluate whether CodeBERTScore can distinguish similar and dissimilar code mined from ShareCode9, an online coding competition platform. Semantically equivalent code snippets are from the same coding problem, and they all pass the unit tests provided by the platform. The dataset consists 6958 code 9https://sharecode.io/ Java C++ Python JavaScript Metric \u03c4 rs \u03c4 rs \u03c4 rs \u03c4 rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481(\u00b1.030) .496(\u00b1.034) .516(\u00b1.052) .525(\u00b1.049) .508(\u00b1.060) .558(\u00b1.058) .532(\u00b1.067) .471(\u00b1.024) .361(\u00b1.037) .324(\u00b1.037) .318(\u00b1.043) .315(\u00b1.047) .344(\u00b1.038) .383(\u00b1.027) .319(\u00b1.035) .273(\u00b1.067) .112(\u00b1.059) .175(\u00b1.021) .262(\u00b1.073) .270(\u00b1.073) .258(\u00b1.091) .301(\u00b1.061) .319(\u00b1.056) .046(\u00b1.009) .301(\u00b1.054) .201(\u00b1.037) .260(\u00b1.024) .273(\u00b1.036) .288(\u00b1.027) .321(\u00b1.023) .321(\u00b1.020) .095(\u00b1.064) .393(\u00b1.083) .366(\u00b1.079) .368(\u00b1.092) .365(\u00b1.094) .338(\u00b1.103) .418(\u00b1.090) .394(\u00b1.096) .391(\u00b1.080) .352(\u00b1.064) .326(\u00b1.075) .334(\u00b1.054) .322(\u00b1.077) .350(\u00b1.064) .402(\u00b1.049) .379(\u00b1.058) .309(\u00b1.073) .248(\u00b1.075) .261(\u00b1.065) .279(\u00b1.092) .261(\u00b1.077) .271(\u00b1.078) .324(\u00b1.075) .302(\u00b1.073) .118(\u00b1.057) .343(\u00b1.052) .299(\u00b1.043) .280(\u00b1.068) .292(\u00b1.057) .293(\u00b1.046) .415(\u00b1.022) .374(\u00b1.044) .059(\u00b1.069) CodeBERTScore .553(\u00b1.068) .369(\u00b1.049) .327(\u00b1.086) .393(\u00b1.048) .422(\u00b1.090) .415(\u00b1.071) .319(\u00b1.054) .402(\u00b1.030) Table 3: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs, along with the standard deviation. Metric \u03c4 rp rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .374(\u00b1.025) .350(\u00b1.037) .397(\u00b1.023) .429(\u00b1.025) .420(\u00b1.037) .366(\u00b1.033) .470(\u00b1.029) .411(\u00b1.030) .604(\u00b1.016) .539(\u00b1.033) .604(\u00b1.016) .629(\u00b1.015) .619(\u00b1.014) .581(\u00b1.016) .635(\u00b1.023) .598(\u00b1.019) .543(\u00b1.018) .495(\u00b1.037) .570(\u00b1.018) .588(\u00b1.022) .574(\u00b1.022) .540(\u00b1.022) .623(\u00b1.018) .576(\u00b1.034) CodeBertScore .517(\u00b1.024) .674(\u00b1.012) .662(\u00b1.012) Table 4: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spearman (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Numbers inside parentheses indicate the standard deviations. Metric BLEU CodeBLEU CrystalBLEU Java 2.36 1.44 5.96 C++ 2.51 1.42 6.94 This result confirms that CodeBERTScore assigns higher similarity scores to semantically similar code pairs, compared to randomly paired snippets that belong to different semantic classes. CodeBERTScore 9.56 9.13 Table 5: Distinguishability with different metrics as the similarity function. CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, on the same datasets. snippets covering 278 problems in Java and C++. We use CodeBERTScore to calculate the similar- ity score for code pairs that share the same seman- tic class and code pairs that do not. We then mea- sure the distinguishability of CodeBERTScore ac- cording to Equation 7. The results are shown in Table 5. Table 5 shows that CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, in both Java and C++. CodeBERTScore achieves distinguisha- bility scores of 9.56 in Java while CrystalBLEU achieves 5.96; in C++, CodeBERTScore achieves 9.13 while CrystalBLEU achieves only 6.94. Can We Hack the Distinguishability Metric? Despite the encouraging results in Table 5, we also found that distinguishability can be easily manipu- lated since it compares absolute scores across dif- ferent metrics. For example, while CrystalBLEU achieves a distinguishability score of 5.96, we can craft a variant of CodeBERTScore that achieves a distinguishability score of 120,000 by simple ex- ponentiation of CodeBERTScore\u2019s output score. To illustrate this, we conducted a distinguisha- bility evaluation with the same configurations as before, but with a variant of CodeBERTScore that we call CodeBERTScorek, and defined as the composition of CodeBERTScore with the f (x) = xk function, that is: CodeBERTScorek (y1, y2) = (CodeBERTScore (y1, y2))k. As Figure 8 shows, distinguishability of CodeBERTScorek increases almost exponentially while increasing k, although the base Code- BERTScore metric has not changed. 130,000 120,000 110,000 100,000 90,000 80,000 70,000 60,000 50,000 40,000 30,000 20,000 10,000 0 CodeBERTScore k 0 10 20 30 40 k Figure 8: Distinguishability by exponentiating the orig- inal CodeBERTScore by k. We thus argue that distinguishability is not a reliable meta-metric and is no substitute for execution-based- or human-rating. We further sus- pect that any meta-metric that compares exact, ab- solute, scores across different metrics is suscepti- ble to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. of CodeBERTScorek with different values of k are shown in Figure 8. As Figure 8 shows, the distinguishability increases almost exponentially with the increasing value of k. We thus argue that distinguishability is not a reliable meta- metric and is no substitute for execution-based- or human-rating. We further suspect that any meta-metric that compares exact, absolute, scores across different metrics is susceptible to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. The distinguishability results G Additional Examples In this section, we provide additional examples in which CodeBERTScore prefers the functionally correct prediction, while the best baseline metric in each language ranks higher a functionally in- correct prediction, which is inequivalent to the ref- erence. Figure 9 shows an example in Java, and Figure 10 shows a C++ example. 50 Natural Language Question: Reference: /** Find how many times a given substring can be found in the original string. Count overlaping cases. >>> howManyTimes(\"\", \"a\") 0 >>> howManyTimes(\"aaa\", \"a\") 3 >>> howManyTimes(\"aaaa\", \"aa\") 3 */ public static int howManyTimes(String string, String substring) { int times = 0; for (int i = 0; i < string.length() - substring.length() + 1; i++) { if (string.substring(i, i + substring.length()) .equals(substring)) { times += 1; } } return times; } (a) The natural language question. (b) The ground truth reference. Non-equivalent candidate: Equivalent candidate: public"}, {"question": " What is the purpose of providing additional examples in the text?", "answer": " The purpose of providing additional examples is to showcase scenarios where CodeBERTScore outperforms the best baseline metric by preferring functionally correct predictions.", "ref_chunk": "such that yi, yj \u2208 Ck} Pairinter ={(yi, yj) | \u2203k such that yi \u2208 Ck, yj /\u2208 Ck} where Ck is the k-th cluster with semantically Intuitively, a similar- equivalent code snippets. ity function f that can distinguish between similar and dissimilar code will produce d larger than 1, meaning that a pair of code snippets from the same semantic cluster has a higher similarity score than a pair of snippets from different clusters. Since the number of intra-class and inter-class pairs grows quadratically with the number of code snippets, in our experiments we followed Eghbali and Pradel (2022) to sample N inter- and N intra-class pairs instead. F.2 Dataset with Semantically equivalent clusters We follow Eghbali and Pradel (2022) to evaluate whether CodeBERTScore can distinguish similar and dissimilar code mined from ShareCode9, an online coding competition platform. Semantically equivalent code snippets are from the same coding problem, and they all pass the unit tests provided by the platform. The dataset consists 6958 code 9https://sharecode.io/ Java C++ Python JavaScript Metric \u03c4 rs \u03c4 rs \u03c4 rs \u03c4 rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481(\u00b1.030) .496(\u00b1.034) .516(\u00b1.052) .525(\u00b1.049) .508(\u00b1.060) .558(\u00b1.058) .532(\u00b1.067) .471(\u00b1.024) .361(\u00b1.037) .324(\u00b1.037) .318(\u00b1.043) .315(\u00b1.047) .344(\u00b1.038) .383(\u00b1.027) .319(\u00b1.035) .273(\u00b1.067) .112(\u00b1.059) .175(\u00b1.021) .262(\u00b1.073) .270(\u00b1.073) .258(\u00b1.091) .301(\u00b1.061) .319(\u00b1.056) .046(\u00b1.009) .301(\u00b1.054) .201(\u00b1.037) .260(\u00b1.024) .273(\u00b1.036) .288(\u00b1.027) .321(\u00b1.023) .321(\u00b1.020) .095(\u00b1.064) .393(\u00b1.083) .366(\u00b1.079) .368(\u00b1.092) .365(\u00b1.094) .338(\u00b1.103) .418(\u00b1.090) .394(\u00b1.096) .391(\u00b1.080) .352(\u00b1.064) .326(\u00b1.075) .334(\u00b1.054) .322(\u00b1.077) .350(\u00b1.064) .402(\u00b1.049) .379(\u00b1.058) .309(\u00b1.073) .248(\u00b1.075) .261(\u00b1.065) .279(\u00b1.092) .261(\u00b1.077) .271(\u00b1.078) .324(\u00b1.075) .302(\u00b1.073) .118(\u00b1.057) .343(\u00b1.052) .299(\u00b1.043) .280(\u00b1.068) .292(\u00b1.057) .293(\u00b1.046) .415(\u00b1.022) .374(\u00b1.044) .059(\u00b1.069) CodeBERTScore .553(\u00b1.068) .369(\u00b1.049) .327(\u00b1.086) .393(\u00b1.048) .422(\u00b1.090) .415(\u00b1.071) .319(\u00b1.054) .402(\u00b1.030) Table 3: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs, along with the standard deviation. Metric \u03c4 rp rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .374(\u00b1.025) .350(\u00b1.037) .397(\u00b1.023) .429(\u00b1.025) .420(\u00b1.037) .366(\u00b1.033) .470(\u00b1.029) .411(\u00b1.030) .604(\u00b1.016) .539(\u00b1.033) .604(\u00b1.016) .629(\u00b1.015) .619(\u00b1.014) .581(\u00b1.016) .635(\u00b1.023) .598(\u00b1.019) .543(\u00b1.018) .495(\u00b1.037) .570(\u00b1.018) .588(\u00b1.022) .574(\u00b1.022) .540(\u00b1.022) .623(\u00b1.018) .576(\u00b1.034) CodeBertScore .517(\u00b1.024) .674(\u00b1.012) .662(\u00b1.012) Table 4: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spearman (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Numbers inside parentheses indicate the standard deviations. Metric BLEU CodeBLEU CrystalBLEU Java 2.36 1.44 5.96 C++ 2.51 1.42 6.94 This result confirms that CodeBERTScore assigns higher similarity scores to semantically similar code pairs, compared to randomly paired snippets that belong to different semantic classes. CodeBERTScore 9.56 9.13 Table 5: Distinguishability with different metrics as the similarity function. CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, on the same datasets. snippets covering 278 problems in Java and C++. We use CodeBERTScore to calculate the similar- ity score for code pairs that share the same seman- tic class and code pairs that do not. We then mea- sure the distinguishability of CodeBERTScore ac- cording to Equation 7. The results are shown in Table 5. Table 5 shows that CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, in both Java and C++. CodeBERTScore achieves distinguisha- bility scores of 9.56 in Java while CrystalBLEU achieves 5.96; in C++, CodeBERTScore achieves 9.13 while CrystalBLEU achieves only 6.94. Can We Hack the Distinguishability Metric? Despite the encouraging results in Table 5, we also found that distinguishability can be easily manipu- lated since it compares absolute scores across dif- ferent metrics. For example, while CrystalBLEU achieves a distinguishability score of 5.96, we can craft a variant of CodeBERTScore that achieves a distinguishability score of 120,000 by simple ex- ponentiation of CodeBERTScore\u2019s output score. To illustrate this, we conducted a distinguisha- bility evaluation with the same configurations as before, but with a variant of CodeBERTScore that we call CodeBERTScorek, and defined as the composition of CodeBERTScore with the f (x) = xk function, that is: CodeBERTScorek (y1, y2) = (CodeBERTScore (y1, y2))k. As Figure 8 shows, distinguishability of CodeBERTScorek increases almost exponentially while increasing k, although the base Code- BERTScore metric has not changed. 130,000 120,000 110,000 100,000 90,000 80,000 70,000 60,000 50,000 40,000 30,000 20,000 10,000 0 CodeBERTScore k 0 10 20 30 40 k Figure 8: Distinguishability by exponentiating the orig- inal CodeBERTScore by k. We thus argue that distinguishability is not a reliable meta-metric and is no substitute for execution-based- or human-rating. We further sus- pect that any meta-metric that compares exact, ab- solute, scores across different metrics is suscepti- ble to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. of CodeBERTScorek with different values of k are shown in Figure 8. As Figure 8 shows, the distinguishability increases almost exponentially with the increasing value of k. We thus argue that distinguishability is not a reliable meta- metric and is no substitute for execution-based- or human-rating. We further suspect that any meta-metric that compares exact, absolute, scores across different metrics is susceptible to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. The distinguishability results G Additional Examples In this section, we provide additional examples in which CodeBERTScore prefers the functionally correct prediction, while the best baseline metric in each language ranks higher a functionally in- correct prediction, which is inequivalent to the ref- erence. Figure 9 shows an example in Java, and Figure 10 shows a C++ example. 50 Natural Language Question: Reference: /** Find how many times a given substring can be found in the original string. Count overlaping cases. >>> howManyTimes(\"\", \"a\") 0 >>> howManyTimes(\"aaa\", \"a\") 3 >>> howManyTimes(\"aaaa\", \"aa\") 3 */ public static int howManyTimes(String string, String substring) { int times = 0; for (int i = 0; i < string.length() - substring.length() + 1; i++) { if (string.substring(i, i + substring.length()) .equals(substring)) { times += 1; } } return times; } (a) The natural language question. (b) The ground truth reference. Non-equivalent candidate: Equivalent candidate: public"}, {"question": " What is the motivation behind showing the C++ example in Figure 10?", "answer": " The motivation behind showing the C++ example in Figure 10 is to highlight a scenario where CodeBERTScore ranks a functionally correct prediction higher than the best baseline metric.", "ref_chunk": "such that yi, yj \u2208 Ck} Pairinter ={(yi, yj) | \u2203k such that yi \u2208 Ck, yj /\u2208 Ck} where Ck is the k-th cluster with semantically Intuitively, a similar- equivalent code snippets. ity function f that can distinguish between similar and dissimilar code will produce d larger than 1, meaning that a pair of code snippets from the same semantic cluster has a higher similarity score than a pair of snippets from different clusters. Since the number of intra-class and inter-class pairs grows quadratically with the number of code snippets, in our experiments we followed Eghbali and Pradel (2022) to sample N inter- and N intra-class pairs instead. F.2 Dataset with Semantically equivalent clusters We follow Eghbali and Pradel (2022) to evaluate whether CodeBERTScore can distinguish similar and dissimilar code mined from ShareCode9, an online coding competition platform. Semantically equivalent code snippets are from the same coding problem, and they all pass the unit tests provided by the platform. The dataset consists 6958 code 9https://sharecode.io/ Java C++ Python JavaScript Metric \u03c4 rs \u03c4 rs \u03c4 rs \u03c4 rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481(\u00b1.030) .496(\u00b1.034) .516(\u00b1.052) .525(\u00b1.049) .508(\u00b1.060) .558(\u00b1.058) .532(\u00b1.067) .471(\u00b1.024) .361(\u00b1.037) .324(\u00b1.037) .318(\u00b1.043) .315(\u00b1.047) .344(\u00b1.038) .383(\u00b1.027) .319(\u00b1.035) .273(\u00b1.067) .112(\u00b1.059) .175(\u00b1.021) .262(\u00b1.073) .270(\u00b1.073) .258(\u00b1.091) .301(\u00b1.061) .319(\u00b1.056) .046(\u00b1.009) .301(\u00b1.054) .201(\u00b1.037) .260(\u00b1.024) .273(\u00b1.036) .288(\u00b1.027) .321(\u00b1.023) .321(\u00b1.020) .095(\u00b1.064) .393(\u00b1.083) .366(\u00b1.079) .368(\u00b1.092) .365(\u00b1.094) .338(\u00b1.103) .418(\u00b1.090) .394(\u00b1.096) .391(\u00b1.080) .352(\u00b1.064) .326(\u00b1.075) .334(\u00b1.054) .322(\u00b1.077) .350(\u00b1.064) .402(\u00b1.049) .379(\u00b1.058) .309(\u00b1.073) .248(\u00b1.075) .261(\u00b1.065) .279(\u00b1.092) .261(\u00b1.077) .271(\u00b1.078) .324(\u00b1.075) .302(\u00b1.073) .118(\u00b1.057) .343(\u00b1.052) .299(\u00b1.043) .280(\u00b1.068) .292(\u00b1.057) .293(\u00b1.046) .415(\u00b1.022) .374(\u00b1.044) .059(\u00b1.069) CodeBERTScore .553(\u00b1.068) .369(\u00b1.049) .327(\u00b1.086) .393(\u00b1.048) .422(\u00b1.090) .415(\u00b1.071) .319(\u00b1.054) .402(\u00b1.030) Table 3: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs, along with the standard deviation. Metric \u03c4 rp rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .374(\u00b1.025) .350(\u00b1.037) .397(\u00b1.023) .429(\u00b1.025) .420(\u00b1.037) .366(\u00b1.033) .470(\u00b1.029) .411(\u00b1.030) .604(\u00b1.016) .539(\u00b1.033) .604(\u00b1.016) .629(\u00b1.015) .619(\u00b1.014) .581(\u00b1.016) .635(\u00b1.023) .598(\u00b1.019) .543(\u00b1.018) .495(\u00b1.037) .570(\u00b1.018) .588(\u00b1.022) .574(\u00b1.022) .540(\u00b1.022) .623(\u00b1.018) .576(\u00b1.034) CodeBertScore .517(\u00b1.024) .674(\u00b1.012) .662(\u00b1.012) Table 4: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spearman (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Numbers inside parentheses indicate the standard deviations. Metric BLEU CodeBLEU CrystalBLEU Java 2.36 1.44 5.96 C++ 2.51 1.42 6.94 This result confirms that CodeBERTScore assigns higher similarity scores to semantically similar code pairs, compared to randomly paired snippets that belong to different semantic classes. CodeBERTScore 9.56 9.13 Table 5: Distinguishability with different metrics as the similarity function. CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, on the same datasets. snippets covering 278 problems in Java and C++. We use CodeBERTScore to calculate the similar- ity score for code pairs that share the same seman- tic class and code pairs that do not. We then mea- sure the distinguishability of CodeBERTScore ac- cording to Equation 7. The results are shown in Table 5. Table 5 shows that CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, in both Java and C++. CodeBERTScore achieves distinguisha- bility scores of 9.56 in Java while CrystalBLEU achieves 5.96; in C++, CodeBERTScore achieves 9.13 while CrystalBLEU achieves only 6.94. Can We Hack the Distinguishability Metric? Despite the encouraging results in Table 5, we also found that distinguishability can be easily manipu- lated since it compares absolute scores across dif- ferent metrics. For example, while CrystalBLEU achieves a distinguishability score of 5.96, we can craft a variant of CodeBERTScore that achieves a distinguishability score of 120,000 by simple ex- ponentiation of CodeBERTScore\u2019s output score. To illustrate this, we conducted a distinguisha- bility evaluation with the same configurations as before, but with a variant of CodeBERTScore that we call CodeBERTScorek, and defined as the composition of CodeBERTScore with the f (x) = xk function, that is: CodeBERTScorek (y1, y2) = (CodeBERTScore (y1, y2))k. As Figure 8 shows, distinguishability of CodeBERTScorek increases almost exponentially while increasing k, although the base Code- BERTScore metric has not changed. 130,000 120,000 110,000 100,000 90,000 80,000 70,000 60,000 50,000 40,000 30,000 20,000 10,000 0 CodeBERTScore k 0 10 20 30 40 k Figure 8: Distinguishability by exponentiating the orig- inal CodeBERTScore by k. We thus argue that distinguishability is not a reliable meta-metric and is no substitute for execution-based- or human-rating. We further sus- pect that any meta-metric that compares exact, ab- solute, scores across different metrics is suscepti- ble to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. of CodeBERTScorek with different values of k are shown in Figure 8. As Figure 8 shows, the distinguishability increases almost exponentially with the increasing value of k. We thus argue that distinguishability is not a reliable meta- metric and is no substitute for execution-based- or human-rating. We further suspect that any meta-metric that compares exact, absolute, scores across different metrics is susceptible to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. The distinguishability results G Additional Examples In this section, we provide additional examples in which CodeBERTScore prefers the functionally correct prediction, while the best baseline metric in each language ranks higher a functionally in- correct prediction, which is inequivalent to the ref- erence. Figure 9 shows an example in Java, and Figure 10 shows a C++ example. 50 Natural Language Question: Reference: /** Find how many times a given substring can be found in the original string. Count overlaping cases. >>> howManyTimes(\"\", \"a\") 0 >>> howManyTimes(\"aaa\", \"a\") 3 >>> howManyTimes(\"aaaa\", \"aa\") 3 */ public static int howManyTimes(String string, String substring) { int times = 0; for (int i = 0; i < string.length() - substring.length() + 1; i++) { if (string.substring(i, i + substring.length()) .equals(substring)) { times += 1; } } return times; } (a) The natural language question. (b) The ground truth reference. Non-equivalent candidate: Equivalent candidate: public"}, {"question": " What does the Java example in Figure 9 aim to demonstrate?", "answer": " The Java example in Figure 9 aims to demonstrate how CodeBERTScore favors the functionally correct prediction over an incorrect one by ranking it higher.", "ref_chunk": "such that yi, yj \u2208 Ck} Pairinter ={(yi, yj) | \u2203k such that yi \u2208 Ck, yj /\u2208 Ck} where Ck is the k-th cluster with semantically Intuitively, a similar- equivalent code snippets. ity function f that can distinguish between similar and dissimilar code will produce d larger than 1, meaning that a pair of code snippets from the same semantic cluster has a higher similarity score than a pair of snippets from different clusters. Since the number of intra-class and inter-class pairs grows quadratically with the number of code snippets, in our experiments we followed Eghbali and Pradel (2022) to sample N inter- and N intra-class pairs instead. F.2 Dataset with Semantically equivalent clusters We follow Eghbali and Pradel (2022) to evaluate whether CodeBERTScore can distinguish similar and dissimilar code mined from ShareCode9, an online coding competition platform. Semantically equivalent code snippets are from the same coding problem, and they all pass the unit tests provided by the platform. The dataset consists 6958 code 9https://sharecode.io/ Java C++ Python JavaScript Metric \u03c4 rs \u03c4 rs \u03c4 rs \u03c4 rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481(\u00b1.030) .496(\u00b1.034) .516(\u00b1.052) .525(\u00b1.049) .508(\u00b1.060) .558(\u00b1.058) .532(\u00b1.067) .471(\u00b1.024) .361(\u00b1.037) .324(\u00b1.037) .318(\u00b1.043) .315(\u00b1.047) .344(\u00b1.038) .383(\u00b1.027) .319(\u00b1.035) .273(\u00b1.067) .112(\u00b1.059) .175(\u00b1.021) .262(\u00b1.073) .270(\u00b1.073) .258(\u00b1.091) .301(\u00b1.061) .319(\u00b1.056) .046(\u00b1.009) .301(\u00b1.054) .201(\u00b1.037) .260(\u00b1.024) .273(\u00b1.036) .288(\u00b1.027) .321(\u00b1.023) .321(\u00b1.020) .095(\u00b1.064) .393(\u00b1.083) .366(\u00b1.079) .368(\u00b1.092) .365(\u00b1.094) .338(\u00b1.103) .418(\u00b1.090) .394(\u00b1.096) .391(\u00b1.080) .352(\u00b1.064) .326(\u00b1.075) .334(\u00b1.054) .322(\u00b1.077) .350(\u00b1.064) .402(\u00b1.049) .379(\u00b1.058) .309(\u00b1.073) .248(\u00b1.075) .261(\u00b1.065) .279(\u00b1.092) .261(\u00b1.077) .271(\u00b1.078) .324(\u00b1.075) .302(\u00b1.073) .118(\u00b1.057) .343(\u00b1.052) .299(\u00b1.043) .280(\u00b1.068) .292(\u00b1.057) .293(\u00b1.046) .415(\u00b1.022) .374(\u00b1.044) .059(\u00b1.069) CodeBERTScore .553(\u00b1.068) .369(\u00b1.049) .327(\u00b1.086) .393(\u00b1.048) .422(\u00b1.090) .415(\u00b1.071) .319(\u00b1.054) .402(\u00b1.030) Table 3: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs, along with the standard deviation. Metric \u03c4 rp rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .374(\u00b1.025) .350(\u00b1.037) .397(\u00b1.023) .429(\u00b1.025) .420(\u00b1.037) .366(\u00b1.033) .470(\u00b1.029) .411(\u00b1.030) .604(\u00b1.016) .539(\u00b1.033) .604(\u00b1.016) .629(\u00b1.015) .619(\u00b1.014) .581(\u00b1.016) .635(\u00b1.023) .598(\u00b1.019) .543(\u00b1.018) .495(\u00b1.037) .570(\u00b1.018) .588(\u00b1.022) .574(\u00b1.022) .540(\u00b1.022) .623(\u00b1.018) .576(\u00b1.034) CodeBertScore .517(\u00b1.024) .674(\u00b1.012) .662(\u00b1.012) Table 4: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spearman (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Numbers inside parentheses indicate the standard deviations. Metric BLEU CodeBLEU CrystalBLEU Java 2.36 1.44 5.96 C++ 2.51 1.42 6.94 This result confirms that CodeBERTScore assigns higher similarity scores to semantically similar code pairs, compared to randomly paired snippets that belong to different semantic classes. CodeBERTScore 9.56 9.13 Table 5: Distinguishability with different metrics as the similarity function. CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, on the same datasets. snippets covering 278 problems in Java and C++. We use CodeBERTScore to calculate the similar- ity score for code pairs that share the same seman- tic class and code pairs that do not. We then mea- sure the distinguishability of CodeBERTScore ac- cording to Equation 7. The results are shown in Table 5. Table 5 shows that CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, in both Java and C++. CodeBERTScore achieves distinguisha- bility scores of 9.56 in Java while CrystalBLEU achieves 5.96; in C++, CodeBERTScore achieves 9.13 while CrystalBLEU achieves only 6.94. Can We Hack the Distinguishability Metric? Despite the encouraging results in Table 5, we also found that distinguishability can be easily manipu- lated since it compares absolute scores across dif- ferent metrics. For example, while CrystalBLEU achieves a distinguishability score of 5.96, we can craft a variant of CodeBERTScore that achieves a distinguishability score of 120,000 by simple ex- ponentiation of CodeBERTScore\u2019s output score. To illustrate this, we conducted a distinguisha- bility evaluation with the same configurations as before, but with a variant of CodeBERTScore that we call CodeBERTScorek, and defined as the composition of CodeBERTScore with the f (x) = xk function, that is: CodeBERTScorek (y1, y2) = (CodeBERTScore (y1, y2))k. As Figure 8 shows, distinguishability of CodeBERTScorek increases almost exponentially while increasing k, although the base Code- BERTScore metric has not changed. 130,000 120,000 110,000 100,000 90,000 80,000 70,000 60,000 50,000 40,000 30,000 20,000 10,000 0 CodeBERTScore k 0 10 20 30 40 k Figure 8: Distinguishability by exponentiating the orig- inal CodeBERTScore by k. We thus argue that distinguishability is not a reliable meta-metric and is no substitute for execution-based- or human-rating. We further sus- pect that any meta-metric that compares exact, ab- solute, scores across different metrics is suscepti- ble to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. of CodeBERTScorek with different values of k are shown in Figure 8. As Figure 8 shows, the distinguishability increases almost exponentially with the increasing value of k. We thus argue that distinguishability is not a reliable meta- metric and is no substitute for execution-based- or human-rating. We further suspect that any meta-metric that compares exact, absolute, scores across different metrics is susceptible to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. The distinguishability results G Additional Examples In this section, we provide additional examples in which CodeBERTScore prefers the functionally correct prediction, while the best baseline metric in each language ranks higher a functionally in- correct prediction, which is inequivalent to the ref- erence. Figure 9 shows an example in Java, and Figure 10 shows a C++ example. 50 Natural Language Question: Reference: /** Find how many times a given substring can be found in the original string. Count overlaping cases. >>> howManyTimes(\"\", \"a\") 0 >>> howManyTimes(\"aaa\", \"a\") 3 >>> howManyTimes(\"aaaa\", \"aa\") 3 */ public static int howManyTimes(String string, String substring) { int times = 0; for (int i = 0; i < string.length() - substring.length() + 1; i++) { if (string.substring(i, i + substring.length()) .equals(substring)) { times += 1; } } return times; } (a) The natural language question. (b) The ground truth reference. Non-equivalent candidate: Equivalent candidate: public"}], "doc_text": "such that yi, yj \u2208 Ck} Pairinter ={(yi, yj) | \u2203k such that yi \u2208 Ck, yj /\u2208 Ck} where Ck is the k-th cluster with semantically Intuitively, a similar- equivalent code snippets. ity function f that can distinguish between similar and dissimilar code will produce d larger than 1, meaning that a pair of code snippets from the same semantic cluster has a higher similarity score than a pair of snippets from different clusters. Since the number of intra-class and inter-class pairs grows quadratically with the number of code snippets, in our experiments we followed Eghbali and Pradel (2022) to sample N inter- and N intra-class pairs instead. F.2 Dataset with Semantically equivalent clusters We follow Eghbali and Pradel (2022) to evaluate whether CodeBERTScore can distinguish similar and dissimilar code mined from ShareCode9, an online coding competition platform. Semantically equivalent code snippets are from the same coding problem, and they all pass the unit tests provided by the platform. The dataset consists 6958 code 9https://sharecode.io/ Java C++ Python JavaScript Metric \u03c4 rs \u03c4 rs \u03c4 rs \u03c4 rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481(\u00b1.030) .496(\u00b1.034) .516(\u00b1.052) .525(\u00b1.049) .508(\u00b1.060) .558(\u00b1.058) .532(\u00b1.067) .471(\u00b1.024) .361(\u00b1.037) .324(\u00b1.037) .318(\u00b1.043) .315(\u00b1.047) .344(\u00b1.038) .383(\u00b1.027) .319(\u00b1.035) .273(\u00b1.067) .112(\u00b1.059) .175(\u00b1.021) .262(\u00b1.073) .270(\u00b1.073) .258(\u00b1.091) .301(\u00b1.061) .319(\u00b1.056) .046(\u00b1.009) .301(\u00b1.054) .201(\u00b1.037) .260(\u00b1.024) .273(\u00b1.036) .288(\u00b1.027) .321(\u00b1.023) .321(\u00b1.020) .095(\u00b1.064) .393(\u00b1.083) .366(\u00b1.079) .368(\u00b1.092) .365(\u00b1.094) .338(\u00b1.103) .418(\u00b1.090) .394(\u00b1.096) .391(\u00b1.080) .352(\u00b1.064) .326(\u00b1.075) .334(\u00b1.054) .322(\u00b1.077) .350(\u00b1.064) .402(\u00b1.049) .379(\u00b1.058) .309(\u00b1.073) .248(\u00b1.075) .261(\u00b1.065) .279(\u00b1.092) .261(\u00b1.077) .271(\u00b1.078) .324(\u00b1.075) .302(\u00b1.073) .118(\u00b1.057) .343(\u00b1.052) .299(\u00b1.043) .280(\u00b1.068) .292(\u00b1.057) .293(\u00b1.046) .415(\u00b1.022) .374(\u00b1.044) .059(\u00b1.069) CodeBERTScore .553(\u00b1.068) .369(\u00b1.049) .327(\u00b1.086) .393(\u00b1.048) .422(\u00b1.090) .415(\u00b1.071) .319(\u00b1.054) .402(\u00b1.030) Table 3: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs, along with the standard deviation. Metric \u03c4 rp rs BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .374(\u00b1.025) .350(\u00b1.037) .397(\u00b1.023) .429(\u00b1.025) .420(\u00b1.037) .366(\u00b1.033) .470(\u00b1.029) .411(\u00b1.030) .604(\u00b1.016) .539(\u00b1.033) .604(\u00b1.016) .629(\u00b1.015) .619(\u00b1.014) .581(\u00b1.016) .635(\u00b1.023) .598(\u00b1.019) .543(\u00b1.018) .495(\u00b1.037) .570(\u00b1.018) .588(\u00b1.022) .574(\u00b1.022) .540(\u00b1.022) .623(\u00b1.018) .576(\u00b1.034) CodeBertScore .517(\u00b1.024) .674(\u00b1.012) .662(\u00b1.012) Table 4: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spearman (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Numbers inside parentheses indicate the standard deviations. Metric BLEU CodeBLEU CrystalBLEU Java 2.36 1.44 5.96 C++ 2.51 1.42 6.94 This result confirms that CodeBERTScore assigns higher similarity scores to semantically similar code pairs, compared to randomly paired snippets that belong to different semantic classes. CodeBERTScore 9.56 9.13 Table 5: Distinguishability with different metrics as the similarity function. CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, on the same datasets. snippets covering 278 problems in Java and C++. We use CodeBERTScore to calculate the similar- ity score for code pairs that share the same seman- tic class and code pairs that do not. We then mea- sure the distinguishability of CodeBERTScore ac- cording to Equation 7. The results are shown in Table 5. Table 5 shows that CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, in both Java and C++. CodeBERTScore achieves distinguisha- bility scores of 9.56 in Java while CrystalBLEU achieves 5.96; in C++, CodeBERTScore achieves 9.13 while CrystalBLEU achieves only 6.94. Can We Hack the Distinguishability Metric? Despite the encouraging results in Table 5, we also found that distinguishability can be easily manipu- lated since it compares absolute scores across dif- ferent metrics. For example, while CrystalBLEU achieves a distinguishability score of 5.96, we can craft a variant of CodeBERTScore that achieves a distinguishability score of 120,000 by simple ex- ponentiation of CodeBERTScore\u2019s output score. To illustrate this, we conducted a distinguisha- bility evaluation with the same configurations as before, but with a variant of CodeBERTScore that we call CodeBERTScorek, and defined as the composition of CodeBERTScore with the f (x) = xk function, that is: CodeBERTScorek (y1, y2) = (CodeBERTScore (y1, y2))k. As Figure 8 shows, distinguishability of CodeBERTScorek increases almost exponentially while increasing k, although the base Code- BERTScore metric has not changed. 130,000 120,000 110,000 100,000 90,000 80,000 70,000 60,000 50,000 40,000 30,000 20,000 10,000 0 CodeBERTScore k 0 10 20 30 40 k Figure 8: Distinguishability by exponentiating the orig- inal CodeBERTScore by k. We thus argue that distinguishability is not a reliable meta-metric and is no substitute for execution-based- or human-rating. We further sus- pect that any meta-metric that compares exact, ab- solute, scores across different metrics is suscepti- ble to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. of CodeBERTScorek with different values of k are shown in Figure 8. As Figure 8 shows, the distinguishability increases almost exponentially with the increasing value of k. We thus argue that distinguishability is not a reliable meta- metric and is no substitute for execution-based- or human-rating. We further suspect that any meta-metric that compares exact, absolute, scores across different metrics is susceptible to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores. The distinguishability results G Additional Examples In this section, we provide additional examples in which CodeBERTScore prefers the functionally correct prediction, while the best baseline metric in each language ranks higher a functionally in- correct prediction, which is inequivalent to the ref- erence. Figure 9 shows an example in Java, and Figure 10 shows a C++ example. 50 Natural Language Question: Reference: /** Find how many times a given substring can be found in the original string. Count overlaping cases. >>> howManyTimes(\"\", \"a\") 0 >>> howManyTimes(\"aaa\", \"a\") 3 >>> howManyTimes(\"aaaa\", \"aa\") 3 */ public static int howManyTimes(String string, String substring) { int times = 0; for (int i = 0; i < string.length() - substring.length() + 1; i++) { if (string.substring(i, i + substring.length()) .equals(substring)) { times += 1; } } return times; } (a) The natural language question. (b) The ground truth reference. Non-equivalent candidate: Equivalent candidate: public"}