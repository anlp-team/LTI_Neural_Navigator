{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Tensor_decomposition_for_minimization_of_E2E_SLU_model_toward_on-device_processing_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the focus of the paper by Yosuke Kashiwagi et al.?", "answer": " The paper focuses on the end-to-end (E2E) SLU model for minimization of computational cost towards on-device processing.", "ref_chunk": "3 2 0 2 n u J 2 ] S A . s s e e [ 1 v 7 4 2 1 0 . 6 0 3 2 : v i X r a TENSOR DECOMPOSITION FOR MINIMIZATION OF E2E SLU MODEL TOWARD ON-DEVICE PROCESSING Yosuke Kashiwagi1, Siddhant Arora2, Hayato Futami1, Jessica Huynh2, Shih-Lun Wu2, Yifan Peng2, Brian Yan2, Emiru Tsunoo1, Shinji Watanabe2 1 Sony Group Corporation, Japan, 2 Carnegie Mellon University, USA yosuke.kashiwagi@sony.com Abstract high accuracy in the SLU task [17, 22]. Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency prop- erty, unlike a cascade system, and aims to minimize the com- putational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architec- tures used in our E2E SLU models. We propose to apply singu- lar value decomposition to linear layers and the Tucker decom- position to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposi- tion to the Tucker decomposition. Since the E2E model is rep- resented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 mil- lion parameters. Index Terms: spoken language understanding, E2E, on-device, sequential distillation, tensor decomposition, STOP Tensor decomposition techniques are widely used for model miniaturization [23]. Although various tensor decompo- sition techniques have been proposed, we mainly explore SVD, and Tucker decomposition [24] to target on-device fast process- ing. They enable inference without reconstructing the original parameter tensors from the factored tensors. Decomposition with smaller ranks can then be used to reduce computational complexity, enabling a reduction in the number of parameters and faster computation [25]. The paper demonstrates effective combinations of model compression techniques for Conformer and E-Branchformer, specifically through the use of tensor de- compositions such as Tucker decomposition. This is because these models have convolution layers with higher-order tensors as parameters, making them more efficiently compressed us- ing these techniques. By defining the model compression ra- tio, we show that the model size can be flexibly changed by determining the rank of the decomposition from this ratio. Fur- thermore, we evaluate CANDECOMP/PARAFAC (CP) decom- position [26] and Tensor-Train decomposition [27] when used instead of the Tucker decomposition. 1. Introduction 2. Tensor decomposition Spoken language understanding (SLU) is one of the essential applications of speech recognition. SLU is used in voice in- terfaces such as smart speakers, and improving its accuracy is required for usability [1\u20135]. On the other hand, these voice in- terfaces often work on edge devices due to latency and privacy issues. Such on-device processing requires smaller models to preserve power consumption. Therefore, miniaturization of the SLU model for on-device processing is a critical issue for voice interfaces [6\u201315]. For example, Radfar et al. reduce parameters by sharing the audio encoder and estimate slot tags, values, and intents [2, 6]. Also, Le et al. model NLU efficiently based on RNN-T us- ing embedded features of both the encoder and the predictor of RNN-T [7]. In addition, Tyagi et al. propose early decision- making using the BranchyNet scheme to address the latency and computational complexity issues [9]. Although these are prac- tical approaches, many are based on the traditional, relatively simple E2E model structure. On the other hand, recent E2E models have been used with more complex structures by com- bining convolution and self-attention operations such as Con- former [16] and E-Branchformer [17,18]. Therefore, in addition to using singular value decomposition (SVD) [19\u201321] for two- dimensional matrices in the self-attention network, convolution network having high-order tensors requires tensor decomposi- tion techniques. We specifically target the Conformer and E- Branchformer-based E2E SLU model, which has demonstrated 2.1. Singular value decomposition The SVD-based parameter reduction technique is stable and widely used [19\u201321]. The SVD is applied to the weight ma- trix W \u2208 RI\u00d7J with low-rank bases as: W = U SV , where S is a diagonal matrix. The matrix size of each U , S and V is I \u00d7 R, R \u00d7 R and R \u00d7 J, respectively. The parameter size can be controlled by changing R. Since S is a square matrix, U S or SV can be pre-composed to reduce the parameters. In our model, we composed SV . Thus, the final number of de- composed parameters can be reduced into IR + RJ. Therefore, the parameter compression ratio \u03b3svd can be described as: \u03b3svd = IR + RJ IJ . 2.2. Tucker decomposition Tucker decomposition [24] is known to be effective for apply- ing high-order tensors. Here we discuss the case of 1d convo- lution, which has a 3-dimensional parameter tensor. The pa- rameter tensor of the convolution W \u2208 RI\u00d7J\u00d7K can be de- composed into a core tensor C \u2208 RR\u00d7S\u00d7T and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7S and U 3 \u2208 RK\u00d7T using Tucker (1) (2) Algorithm 1 Size halving for Tucker decomposition Require: \u02c6\u03b3tucker > 0.0 1: R \u2190 I, S \u2190 J, T \u2190 K 2: \u03b3tucker \u2190 \u221e 3: while \u03b3tucker > \u02c6\u03b3tucker do 4: R \u2190 R/2, S \u2190 S/2, T \u2190 T /2 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end while if R < 1 then R \u2190 1 end if if S < 1 then S \u2190 1 end if if T < 1 then T \u2190 1 end if \u03b3tucker \u2190 RST +IR+JS+KT IJK decomposition as: W = (cid:88) Cr,s,t \u00d7 U 1 r \u2297 U 2 s \u2297 U 3 t , r,s,t where \u2297 describes a Kronecker product. The parameter com- pression ratio \u03b3tucker can be described as: \u03b3tucker = RST + IR + JS + KT IJK . 2.3. CP decomposition CP"}, {"question": " What are two tensor decomposition techniques mentioned in the text for model miniaturization?", "answer": " SVD and Tucker decomposition are mentioned for model miniaturization.", "ref_chunk": "3 2 0 2 n u J 2 ] S A . s s e e [ 1 v 7 4 2 1 0 . 6 0 3 2 : v i X r a TENSOR DECOMPOSITION FOR MINIMIZATION OF E2E SLU MODEL TOWARD ON-DEVICE PROCESSING Yosuke Kashiwagi1, Siddhant Arora2, Hayato Futami1, Jessica Huynh2, Shih-Lun Wu2, Yifan Peng2, Brian Yan2, Emiru Tsunoo1, Shinji Watanabe2 1 Sony Group Corporation, Japan, 2 Carnegie Mellon University, USA yosuke.kashiwagi@sony.com Abstract high accuracy in the SLU task [17, 22]. Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency prop- erty, unlike a cascade system, and aims to minimize the com- putational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architec- tures used in our E2E SLU models. We propose to apply singu- lar value decomposition to linear layers and the Tucker decom- position to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposi- tion to the Tucker decomposition. Since the E2E model is rep- resented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 mil- lion parameters. Index Terms: spoken language understanding, E2E, on-device, sequential distillation, tensor decomposition, STOP Tensor decomposition techniques are widely used for model miniaturization [23]. Although various tensor decompo- sition techniques have been proposed, we mainly explore SVD, and Tucker decomposition [24] to target on-device fast process- ing. They enable inference without reconstructing the original parameter tensors from the factored tensors. Decomposition with smaller ranks can then be used to reduce computational complexity, enabling a reduction in the number of parameters and faster computation [25]. The paper demonstrates effective combinations of model compression techniques for Conformer and E-Branchformer, specifically through the use of tensor de- compositions such as Tucker decomposition. This is because these models have convolution layers with higher-order tensors as parameters, making them more efficiently compressed us- ing these techniques. By defining the model compression ra- tio, we show that the model size can be flexibly changed by determining the rank of the decomposition from this ratio. Fur- thermore, we evaluate CANDECOMP/PARAFAC (CP) decom- position [26] and Tensor-Train decomposition [27] when used instead of the Tucker decomposition. 1. Introduction 2. Tensor decomposition Spoken language understanding (SLU) is one of the essential applications of speech recognition. SLU is used in voice in- terfaces such as smart speakers, and improving its accuracy is required for usability [1\u20135]. On the other hand, these voice in- terfaces often work on edge devices due to latency and privacy issues. Such on-device processing requires smaller models to preserve power consumption. Therefore, miniaturization of the SLU model for on-device processing is a critical issue for voice interfaces [6\u201315]. For example, Radfar et al. reduce parameters by sharing the audio encoder and estimate slot tags, values, and intents [2, 6]. Also, Le et al. model NLU efficiently based on RNN-T us- ing embedded features of both the encoder and the predictor of RNN-T [7]. In addition, Tyagi et al. propose early decision- making using the BranchyNet scheme to address the latency and computational complexity issues [9]. Although these are prac- tical approaches, many are based on the traditional, relatively simple E2E model structure. On the other hand, recent E2E models have been used with more complex structures by com- bining convolution and self-attention operations such as Con- former [16] and E-Branchformer [17,18]. Therefore, in addition to using singular value decomposition (SVD) [19\u201321] for two- dimensional matrices in the self-attention network, convolution network having high-order tensors requires tensor decomposi- tion techniques. We specifically target the Conformer and E- Branchformer-based E2E SLU model, which has demonstrated 2.1. Singular value decomposition The SVD-based parameter reduction technique is stable and widely used [19\u201321]. The SVD is applied to the weight ma- trix W \u2208 RI\u00d7J with low-rank bases as: W = U SV , where S is a diagonal matrix. The matrix size of each U , S and V is I \u00d7 R, R \u00d7 R and R \u00d7 J, respectively. The parameter size can be controlled by changing R. Since S is a square matrix, U S or SV can be pre-composed to reduce the parameters. In our model, we composed SV . Thus, the final number of de- composed parameters can be reduced into IR + RJ. Therefore, the parameter compression ratio \u03b3svd can be described as: \u03b3svd = IR + RJ IJ . 2.2. Tucker decomposition Tucker decomposition [24] is known to be effective for apply- ing high-order tensors. Here we discuss the case of 1d convo- lution, which has a 3-dimensional parameter tensor. The pa- rameter tensor of the convolution W \u2208 RI\u00d7J\u00d7K can be de- composed into a core tensor C \u2208 RR\u00d7S\u00d7T and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7S and U 3 \u2208 RK\u00d7T using Tucker (1) (2) Algorithm 1 Size halving for Tucker decomposition Require: \u02c6\u03b3tucker > 0.0 1: R \u2190 I, S \u2190 J, T \u2190 K 2: \u03b3tucker \u2190 \u221e 3: while \u03b3tucker > \u02c6\u03b3tucker do 4: R \u2190 R/2, S \u2190 S/2, T \u2190 T /2 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end while if R < 1 then R \u2190 1 end if if S < 1 then S \u2190 1 end if if T < 1 then T \u2190 1 end if \u03b3tucker \u2190 RST +IR+JS+KT IJK decomposition as: W = (cid:88) Cr,s,t \u00d7 U 1 r \u2297 U 2 s \u2297 U 3 t , r,s,t where \u2297 describes a Kronecker product. The parameter com- pression ratio \u03b3tucker can be described as: \u03b3tucker = RST + IR + JS + KT IJK . 2.3. CP decomposition CP"}, {"question": " How does tensor decomposition help in reducing the model size?", "answer": " Tensor decomposition helps reduce the model size by applying singular value decomposition to linear layers and Tucker decomposition to convolution layers.", "ref_chunk": "3 2 0 2 n u J 2 ] S A . s s e e [ 1 v 7 4 2 1 0 . 6 0 3 2 : v i X r a TENSOR DECOMPOSITION FOR MINIMIZATION OF E2E SLU MODEL TOWARD ON-DEVICE PROCESSING Yosuke Kashiwagi1, Siddhant Arora2, Hayato Futami1, Jessica Huynh2, Shih-Lun Wu2, Yifan Peng2, Brian Yan2, Emiru Tsunoo1, Shinji Watanabe2 1 Sony Group Corporation, Japan, 2 Carnegie Mellon University, USA yosuke.kashiwagi@sony.com Abstract high accuracy in the SLU task [17, 22]. Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency prop- erty, unlike a cascade system, and aims to minimize the com- putational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architec- tures used in our E2E SLU models. We propose to apply singu- lar value decomposition to linear layers and the Tucker decom- position to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposi- tion to the Tucker decomposition. Since the E2E model is rep- resented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 mil- lion parameters. Index Terms: spoken language understanding, E2E, on-device, sequential distillation, tensor decomposition, STOP Tensor decomposition techniques are widely used for model miniaturization [23]. Although various tensor decompo- sition techniques have been proposed, we mainly explore SVD, and Tucker decomposition [24] to target on-device fast process- ing. They enable inference without reconstructing the original parameter tensors from the factored tensors. Decomposition with smaller ranks can then be used to reduce computational complexity, enabling a reduction in the number of parameters and faster computation [25]. The paper demonstrates effective combinations of model compression techniques for Conformer and E-Branchformer, specifically through the use of tensor de- compositions such as Tucker decomposition. This is because these models have convolution layers with higher-order tensors as parameters, making them more efficiently compressed us- ing these techniques. By defining the model compression ra- tio, we show that the model size can be flexibly changed by determining the rank of the decomposition from this ratio. Fur- thermore, we evaluate CANDECOMP/PARAFAC (CP) decom- position [26] and Tensor-Train decomposition [27] when used instead of the Tucker decomposition. 1. Introduction 2. Tensor decomposition Spoken language understanding (SLU) is one of the essential applications of speech recognition. SLU is used in voice in- terfaces such as smart speakers, and improving its accuracy is required for usability [1\u20135]. On the other hand, these voice in- terfaces often work on edge devices due to latency and privacy issues. Such on-device processing requires smaller models to preserve power consumption. Therefore, miniaturization of the SLU model for on-device processing is a critical issue for voice interfaces [6\u201315]. For example, Radfar et al. reduce parameters by sharing the audio encoder and estimate slot tags, values, and intents [2, 6]. Also, Le et al. model NLU efficiently based on RNN-T us- ing embedded features of both the encoder and the predictor of RNN-T [7]. In addition, Tyagi et al. propose early decision- making using the BranchyNet scheme to address the latency and computational complexity issues [9]. Although these are prac- tical approaches, many are based on the traditional, relatively simple E2E model structure. On the other hand, recent E2E models have been used with more complex structures by com- bining convolution and self-attention operations such as Con- former [16] and E-Branchformer [17,18]. Therefore, in addition to using singular value decomposition (SVD) [19\u201321] for two- dimensional matrices in the self-attention network, convolution network having high-order tensors requires tensor decomposi- tion techniques. We specifically target the Conformer and E- Branchformer-based E2E SLU model, which has demonstrated 2.1. Singular value decomposition The SVD-based parameter reduction technique is stable and widely used [19\u201321]. The SVD is applied to the weight ma- trix W \u2208 RI\u00d7J with low-rank bases as: W = U SV , where S is a diagonal matrix. The matrix size of each U , S and V is I \u00d7 R, R \u00d7 R and R \u00d7 J, respectively. The parameter size can be controlled by changing R. Since S is a square matrix, U S or SV can be pre-composed to reduce the parameters. In our model, we composed SV . Thus, the final number of de- composed parameters can be reduced into IR + RJ. Therefore, the parameter compression ratio \u03b3svd can be described as: \u03b3svd = IR + RJ IJ . 2.2. Tucker decomposition Tucker decomposition [24] is known to be effective for apply- ing high-order tensors. Here we discuss the case of 1d convo- lution, which has a 3-dimensional parameter tensor. The pa- rameter tensor of the convolution W \u2208 RI\u00d7J\u00d7K can be de- composed into a core tensor C \u2208 RR\u00d7S\u00d7T and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7S and U 3 \u2208 RK\u00d7T using Tucker (1) (2) Algorithm 1 Size halving for Tucker decomposition Require: \u02c6\u03b3tucker > 0.0 1: R \u2190 I, S \u2190 J, T \u2190 K 2: \u03b3tucker \u2190 \u221e 3: while \u03b3tucker > \u02c6\u03b3tucker do 4: R \u2190 R/2, S \u2190 S/2, T \u2190 T /2 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end while if R < 1 then R \u2190 1 end if if S < 1 then S \u2190 1 end if if T < 1 then T \u2190 1 end if \u03b3tucker \u2190 RST +IR+JS+KT IJK decomposition as: W = (cid:88) Cr,s,t \u00d7 U 1 r \u2297 U 2 s \u2297 U 3 t , r,s,t where \u2297 describes a Kronecker product. The parameter com- pression ratio \u03b3tucker can be described as: \u03b3tucker = RST + IR + JS + KT IJK . 2.3. CP decomposition CP"}, {"question": " What is the purpose of using tensor decompositions in the E2E model?", "answer": " The purpose of using tensor decompositions in the E2E model is to flexibly control the number of parameters without changing feature dimensions.", "ref_chunk": "3 2 0 2 n u J 2 ] S A . s s e e [ 1 v 7 4 2 1 0 . 6 0 3 2 : v i X r a TENSOR DECOMPOSITION FOR MINIMIZATION OF E2E SLU MODEL TOWARD ON-DEVICE PROCESSING Yosuke Kashiwagi1, Siddhant Arora2, Hayato Futami1, Jessica Huynh2, Shih-Lun Wu2, Yifan Peng2, Brian Yan2, Emiru Tsunoo1, Shinji Watanabe2 1 Sony Group Corporation, Japan, 2 Carnegie Mellon University, USA yosuke.kashiwagi@sony.com Abstract high accuracy in the SLU task [17, 22]. Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency prop- erty, unlike a cascade system, and aims to minimize the com- putational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architec- tures used in our E2E SLU models. We propose to apply singu- lar value decomposition to linear layers and the Tucker decom- position to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposi- tion to the Tucker decomposition. Since the E2E model is rep- resented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 mil- lion parameters. Index Terms: spoken language understanding, E2E, on-device, sequential distillation, tensor decomposition, STOP Tensor decomposition techniques are widely used for model miniaturization [23]. Although various tensor decompo- sition techniques have been proposed, we mainly explore SVD, and Tucker decomposition [24] to target on-device fast process- ing. They enable inference without reconstructing the original parameter tensors from the factored tensors. Decomposition with smaller ranks can then be used to reduce computational complexity, enabling a reduction in the number of parameters and faster computation [25]. The paper demonstrates effective combinations of model compression techniques for Conformer and E-Branchformer, specifically through the use of tensor de- compositions such as Tucker decomposition. This is because these models have convolution layers with higher-order tensors as parameters, making them more efficiently compressed us- ing these techniques. By defining the model compression ra- tio, we show that the model size can be flexibly changed by determining the rank of the decomposition from this ratio. Fur- thermore, we evaluate CANDECOMP/PARAFAC (CP) decom- position [26] and Tensor-Train decomposition [27] when used instead of the Tucker decomposition. 1. Introduction 2. Tensor decomposition Spoken language understanding (SLU) is one of the essential applications of speech recognition. SLU is used in voice in- terfaces such as smart speakers, and improving its accuracy is required for usability [1\u20135]. On the other hand, these voice in- terfaces often work on edge devices due to latency and privacy issues. Such on-device processing requires smaller models to preserve power consumption. Therefore, miniaturization of the SLU model for on-device processing is a critical issue for voice interfaces [6\u201315]. For example, Radfar et al. reduce parameters by sharing the audio encoder and estimate slot tags, values, and intents [2, 6]. Also, Le et al. model NLU efficiently based on RNN-T us- ing embedded features of both the encoder and the predictor of RNN-T [7]. In addition, Tyagi et al. propose early decision- making using the BranchyNet scheme to address the latency and computational complexity issues [9]. Although these are prac- tical approaches, many are based on the traditional, relatively simple E2E model structure. On the other hand, recent E2E models have been used with more complex structures by com- bining convolution and self-attention operations such as Con- former [16] and E-Branchformer [17,18]. Therefore, in addition to using singular value decomposition (SVD) [19\u201321] for two- dimensional matrices in the self-attention network, convolution network having high-order tensors requires tensor decomposi- tion techniques. We specifically target the Conformer and E- Branchformer-based E2E SLU model, which has demonstrated 2.1. Singular value decomposition The SVD-based parameter reduction technique is stable and widely used [19\u201321]. The SVD is applied to the weight ma- trix W \u2208 RI\u00d7J with low-rank bases as: W = U SV , where S is a diagonal matrix. The matrix size of each U , S and V is I \u00d7 R, R \u00d7 R and R \u00d7 J, respectively. The parameter size can be controlled by changing R. Since S is a square matrix, U S or SV can be pre-composed to reduce the parameters. In our model, we composed SV . Thus, the final number of de- composed parameters can be reduced into IR + RJ. Therefore, the parameter compression ratio \u03b3svd can be described as: \u03b3svd = IR + RJ IJ . 2.2. Tucker decomposition Tucker decomposition [24] is known to be effective for apply- ing high-order tensors. Here we discuss the case of 1d convo- lution, which has a 3-dimensional parameter tensor. The pa- rameter tensor of the convolution W \u2208 RI\u00d7J\u00d7K can be de- composed into a core tensor C \u2208 RR\u00d7S\u00d7T and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7S and U 3 \u2208 RK\u00d7T using Tucker (1) (2) Algorithm 1 Size halving for Tucker decomposition Require: \u02c6\u03b3tucker > 0.0 1: R \u2190 I, S \u2190 J, T \u2190 K 2: \u03b3tucker \u2190 \u221e 3: while \u03b3tucker > \u02c6\u03b3tucker do 4: R \u2190 R/2, S \u2190 S/2, T \u2190 T /2 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end while if R < 1 then R \u2190 1 end if if S < 1 then S \u2190 1 end if if T < 1 then T \u2190 1 end if \u03b3tucker \u2190 RST +IR+JS+KT IJK decomposition as: W = (cid:88) Cr,s,t \u00d7 U 1 r \u2297 U 2 s \u2297 U 3 t , r,s,t where \u2297 describes a Kronecker product. The parameter com- pression ratio \u03b3tucker can be described as: \u03b3tucker = RST + IR + JS + KT IJK . 2.3. CP decomposition CP"}, {"question": " What parameter reduction technique is stable and widely used according to the text?", "answer": " Singular value decomposition (SVD) is stable and widely used for parameter reduction.", "ref_chunk": "3 2 0 2 n u J 2 ] S A . s s e e [ 1 v 7 4 2 1 0 . 6 0 3 2 : v i X r a TENSOR DECOMPOSITION FOR MINIMIZATION OF E2E SLU MODEL TOWARD ON-DEVICE PROCESSING Yosuke Kashiwagi1, Siddhant Arora2, Hayato Futami1, Jessica Huynh2, Shih-Lun Wu2, Yifan Peng2, Brian Yan2, Emiru Tsunoo1, Shinji Watanabe2 1 Sony Group Corporation, Japan, 2 Carnegie Mellon University, USA yosuke.kashiwagi@sony.com Abstract high accuracy in the SLU task [17, 22]. Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency prop- erty, unlike a cascade system, and aims to minimize the com- putational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architec- tures used in our E2E SLU models. We propose to apply singu- lar value decomposition to linear layers and the Tucker decom- position to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposi- tion to the Tucker decomposition. Since the E2E model is rep- resented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 mil- lion parameters. Index Terms: spoken language understanding, E2E, on-device, sequential distillation, tensor decomposition, STOP Tensor decomposition techniques are widely used for model miniaturization [23]. Although various tensor decompo- sition techniques have been proposed, we mainly explore SVD, and Tucker decomposition [24] to target on-device fast process- ing. They enable inference without reconstructing the original parameter tensors from the factored tensors. Decomposition with smaller ranks can then be used to reduce computational complexity, enabling a reduction in the number of parameters and faster computation [25]. The paper demonstrates effective combinations of model compression techniques for Conformer and E-Branchformer, specifically through the use of tensor de- compositions such as Tucker decomposition. This is because these models have convolution layers with higher-order tensors as parameters, making them more efficiently compressed us- ing these techniques. By defining the model compression ra- tio, we show that the model size can be flexibly changed by determining the rank of the decomposition from this ratio. Fur- thermore, we evaluate CANDECOMP/PARAFAC (CP) decom- position [26] and Tensor-Train decomposition [27] when used instead of the Tucker decomposition. 1. Introduction 2. Tensor decomposition Spoken language understanding (SLU) is one of the essential applications of speech recognition. SLU is used in voice in- terfaces such as smart speakers, and improving its accuracy is required for usability [1\u20135]. On the other hand, these voice in- terfaces often work on edge devices due to latency and privacy issues. Such on-device processing requires smaller models to preserve power consumption. Therefore, miniaturization of the SLU model for on-device processing is a critical issue for voice interfaces [6\u201315]. For example, Radfar et al. reduce parameters by sharing the audio encoder and estimate slot tags, values, and intents [2, 6]. Also, Le et al. model NLU efficiently based on RNN-T us- ing embedded features of both the encoder and the predictor of RNN-T [7]. In addition, Tyagi et al. propose early decision- making using the BranchyNet scheme to address the latency and computational complexity issues [9]. Although these are prac- tical approaches, many are based on the traditional, relatively simple E2E model structure. On the other hand, recent E2E models have been used with more complex structures by com- bining convolution and self-attention operations such as Con- former [16] and E-Branchformer [17,18]. Therefore, in addition to using singular value decomposition (SVD) [19\u201321] for two- dimensional matrices in the self-attention network, convolution network having high-order tensors requires tensor decomposi- tion techniques. We specifically target the Conformer and E- Branchformer-based E2E SLU model, which has demonstrated 2.1. Singular value decomposition The SVD-based parameter reduction technique is stable and widely used [19\u201321]. The SVD is applied to the weight ma- trix W \u2208 RI\u00d7J with low-rank bases as: W = U SV , where S is a diagonal matrix. The matrix size of each U , S and V is I \u00d7 R, R \u00d7 R and R \u00d7 J, respectively. The parameter size can be controlled by changing R. Since S is a square matrix, U S or SV can be pre-composed to reduce the parameters. In our model, we composed SV . Thus, the final number of de- composed parameters can be reduced into IR + RJ. Therefore, the parameter compression ratio \u03b3svd can be described as: \u03b3svd = IR + RJ IJ . 2.2. Tucker decomposition Tucker decomposition [24] is known to be effective for apply- ing high-order tensors. Here we discuss the case of 1d convo- lution, which has a 3-dimensional parameter tensor. The pa- rameter tensor of the convolution W \u2208 RI\u00d7J\u00d7K can be de- composed into a core tensor C \u2208 RR\u00d7S\u00d7T and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7S and U 3 \u2208 RK\u00d7T using Tucker (1) (2) Algorithm 1 Size halving for Tucker decomposition Require: \u02c6\u03b3tucker > 0.0 1: R \u2190 I, S \u2190 J, T \u2190 K 2: \u03b3tucker \u2190 \u221e 3: while \u03b3tucker > \u02c6\u03b3tucker do 4: R \u2190 R/2, S \u2190 S/2, T \u2190 T /2 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end while if R < 1 then R \u2190 1 end if if S < 1 then S \u2190 1 end if if T < 1 then T \u2190 1 end if \u03b3tucker \u2190 RST +IR+JS+KT IJK decomposition as: W = (cid:88) Cr,s,t \u00d7 U 1 r \u2297 U 2 s \u2297 U 3 t , r,s,t where \u2297 describes a Kronecker product. The parameter com- pression ratio \u03b3tucker can be described as: \u03b3tucker = RST + IR + JS + KT IJK . 2.3. CP decomposition CP"}, {"question": " How is the parameter size controlled in Tucker decomposition?", "answer": " In Tucker decomposition, the parameter size is controlled by decomposing the parameter tensor into a core tensor and factor matrices.", "ref_chunk": "3 2 0 2 n u J 2 ] S A . s s e e [ 1 v 7 4 2 1 0 . 6 0 3 2 : v i X r a TENSOR DECOMPOSITION FOR MINIMIZATION OF E2E SLU MODEL TOWARD ON-DEVICE PROCESSING Yosuke Kashiwagi1, Siddhant Arora2, Hayato Futami1, Jessica Huynh2, Shih-Lun Wu2, Yifan Peng2, Brian Yan2, Emiru Tsunoo1, Shinji Watanabe2 1 Sony Group Corporation, Japan, 2 Carnegie Mellon University, USA yosuke.kashiwagi@sony.com Abstract high accuracy in the SLU task [17, 22]. Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency prop- erty, unlike a cascade system, and aims to minimize the com- putational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architec- tures used in our E2E SLU models. We propose to apply singu- lar value decomposition to linear layers and the Tucker decom- position to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposi- tion to the Tucker decomposition. Since the E2E model is rep- resented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 mil- lion parameters. Index Terms: spoken language understanding, E2E, on-device, sequential distillation, tensor decomposition, STOP Tensor decomposition techniques are widely used for model miniaturization [23]. Although various tensor decompo- sition techniques have been proposed, we mainly explore SVD, and Tucker decomposition [24] to target on-device fast process- ing. They enable inference without reconstructing the original parameter tensors from the factored tensors. Decomposition with smaller ranks can then be used to reduce computational complexity, enabling a reduction in the number of parameters and faster computation [25]. The paper demonstrates effective combinations of model compression techniques for Conformer and E-Branchformer, specifically through the use of tensor de- compositions such as Tucker decomposition. This is because these models have convolution layers with higher-order tensors as parameters, making them more efficiently compressed us- ing these techniques. By defining the model compression ra- tio, we show that the model size can be flexibly changed by determining the rank of the decomposition from this ratio. Fur- thermore, we evaluate CANDECOMP/PARAFAC (CP) decom- position [26] and Tensor-Train decomposition [27] when used instead of the Tucker decomposition. 1. Introduction 2. Tensor decomposition Spoken language understanding (SLU) is one of the essential applications of speech recognition. SLU is used in voice in- terfaces such as smart speakers, and improving its accuracy is required for usability [1\u20135]. On the other hand, these voice in- terfaces often work on edge devices due to latency and privacy issues. Such on-device processing requires smaller models to preserve power consumption. Therefore, miniaturization of the SLU model for on-device processing is a critical issue for voice interfaces [6\u201315]. For example, Radfar et al. reduce parameters by sharing the audio encoder and estimate slot tags, values, and intents [2, 6]. Also, Le et al. model NLU efficiently based on RNN-T us- ing embedded features of both the encoder and the predictor of RNN-T [7]. In addition, Tyagi et al. propose early decision- making using the BranchyNet scheme to address the latency and computational complexity issues [9]. Although these are prac- tical approaches, many are based on the traditional, relatively simple E2E model structure. On the other hand, recent E2E models have been used with more complex structures by com- bining convolution and self-attention operations such as Con- former [16] and E-Branchformer [17,18]. Therefore, in addition to using singular value decomposition (SVD) [19\u201321] for two- dimensional matrices in the self-attention network, convolution network having high-order tensors requires tensor decomposi- tion techniques. We specifically target the Conformer and E- Branchformer-based E2E SLU model, which has demonstrated 2.1. Singular value decomposition The SVD-based parameter reduction technique is stable and widely used [19\u201321]. The SVD is applied to the weight ma- trix W \u2208 RI\u00d7J with low-rank bases as: W = U SV , where S is a diagonal matrix. The matrix size of each U , S and V is I \u00d7 R, R \u00d7 R and R \u00d7 J, respectively. The parameter size can be controlled by changing R. Since S is a square matrix, U S or SV can be pre-composed to reduce the parameters. In our model, we composed SV . Thus, the final number of de- composed parameters can be reduced into IR + RJ. Therefore, the parameter compression ratio \u03b3svd can be described as: \u03b3svd = IR + RJ IJ . 2.2. Tucker decomposition Tucker decomposition [24] is known to be effective for apply- ing high-order tensors. Here we discuss the case of 1d convo- lution, which has a 3-dimensional parameter tensor. The pa- rameter tensor of the convolution W \u2208 RI\u00d7J\u00d7K can be de- composed into a core tensor C \u2208 RR\u00d7S\u00d7T and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7S and U 3 \u2208 RK\u00d7T using Tucker (1) (2) Algorithm 1 Size halving for Tucker decomposition Require: \u02c6\u03b3tucker > 0.0 1: R \u2190 I, S \u2190 J, T \u2190 K 2: \u03b3tucker \u2190 \u221e 3: while \u03b3tucker > \u02c6\u03b3tucker do 4: R \u2190 R/2, S \u2190 S/2, T \u2190 T /2 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end while if R < 1 then R \u2190 1 end if if S < 1 then S \u2190 1 end if if T < 1 then T \u2190 1 end if \u03b3tucker \u2190 RST +IR+JS+KT IJK decomposition as: W = (cid:88) Cr,s,t \u00d7 U 1 r \u2297 U 2 s \u2297 U 3 t , r,s,t where \u2297 describes a Kronecker product. The parameter com- pression ratio \u03b3tucker can be described as: \u03b3tucker = RST + IR + JS + KT IJK . 2.3. CP decomposition CP"}, {"question": " What is the aim of the algorithm presented for Tucker decomposition in the text?", "answer": " The algorithm aims to halve the size for Tucker decomposition by adjusting the dimensions R, S, and T.", "ref_chunk": "3 2 0 2 n u J 2 ] S A . s s e e [ 1 v 7 4 2 1 0 . 6 0 3 2 : v i X r a TENSOR DECOMPOSITION FOR MINIMIZATION OF E2E SLU MODEL TOWARD ON-DEVICE PROCESSING Yosuke Kashiwagi1, Siddhant Arora2, Hayato Futami1, Jessica Huynh2, Shih-Lun Wu2, Yifan Peng2, Brian Yan2, Emiru Tsunoo1, Shinji Watanabe2 1 Sony Group Corporation, Japan, 2 Carnegie Mellon University, USA yosuke.kashiwagi@sony.com Abstract high accuracy in the SLU task [17, 22]. Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency prop- erty, unlike a cascade system, and aims to minimize the com- putational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architec- tures used in our E2E SLU models. We propose to apply singu- lar value decomposition to linear layers and the Tucker decom- position to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposi- tion to the Tucker decomposition. Since the E2E model is rep- resented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 mil- lion parameters. Index Terms: spoken language understanding, E2E, on-device, sequential distillation, tensor decomposition, STOP Tensor decomposition techniques are widely used for model miniaturization [23]. Although various tensor decompo- sition techniques have been proposed, we mainly explore SVD, and Tucker decomposition [24] to target on-device fast process- ing. They enable inference without reconstructing the original parameter tensors from the factored tensors. Decomposition with smaller ranks can then be used to reduce computational complexity, enabling a reduction in the number of parameters and faster computation [25]. The paper demonstrates effective combinations of model compression techniques for Conformer and E-Branchformer, specifically through the use of tensor de- compositions such as Tucker decomposition. This is because these models have convolution layers with higher-order tensors as parameters, making them more efficiently compressed us- ing these techniques. By defining the model compression ra- tio, we show that the model size can be flexibly changed by determining the rank of the decomposition from this ratio. Fur- thermore, we evaluate CANDECOMP/PARAFAC (CP) decom- position [26] and Tensor-Train decomposition [27] when used instead of the Tucker decomposition. 1. Introduction 2. Tensor decomposition Spoken language understanding (SLU) is one of the essential applications of speech recognition. SLU is used in voice in- terfaces such as smart speakers, and improving its accuracy is required for usability [1\u20135]. On the other hand, these voice in- terfaces often work on edge devices due to latency and privacy issues. Such on-device processing requires smaller models to preserve power consumption. Therefore, miniaturization of the SLU model for on-device processing is a critical issue for voice interfaces [6\u201315]. For example, Radfar et al. reduce parameters by sharing the audio encoder and estimate slot tags, values, and intents [2, 6]. Also, Le et al. model NLU efficiently based on RNN-T us- ing embedded features of both the encoder and the predictor of RNN-T [7]. In addition, Tyagi et al. propose early decision- making using the BranchyNet scheme to address the latency and computational complexity issues [9]. Although these are prac- tical approaches, many are based on the traditional, relatively simple E2E model structure. On the other hand, recent E2E models have been used with more complex structures by com- bining convolution and self-attention operations such as Con- former [16] and E-Branchformer [17,18]. Therefore, in addition to using singular value decomposition (SVD) [19\u201321] for two- dimensional matrices in the self-attention network, convolution network having high-order tensors requires tensor decomposi- tion techniques. We specifically target the Conformer and E- Branchformer-based E2E SLU model, which has demonstrated 2.1. Singular value decomposition The SVD-based parameter reduction technique is stable and widely used [19\u201321]. The SVD is applied to the weight ma- trix W \u2208 RI\u00d7J with low-rank bases as: W = U SV , where S is a diagonal matrix. The matrix size of each U , S and V is I \u00d7 R, R \u00d7 R and R \u00d7 J, respectively. The parameter size can be controlled by changing R. Since S is a square matrix, U S or SV can be pre-composed to reduce the parameters. In our model, we composed SV . Thus, the final number of de- composed parameters can be reduced into IR + RJ. Therefore, the parameter compression ratio \u03b3svd can be described as: \u03b3svd = IR + RJ IJ . 2.2. Tucker decomposition Tucker decomposition [24] is known to be effective for apply- ing high-order tensors. Here we discuss the case of 1d convo- lution, which has a 3-dimensional parameter tensor. The pa- rameter tensor of the convolution W \u2208 RI\u00d7J\u00d7K can be de- composed into a core tensor C \u2208 RR\u00d7S\u00d7T and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7S and U 3 \u2208 RK\u00d7T using Tucker (1) (2) Algorithm 1 Size halving for Tucker decomposition Require: \u02c6\u03b3tucker > 0.0 1: R \u2190 I, S \u2190 J, T \u2190 K 2: \u03b3tucker \u2190 \u221e 3: while \u03b3tucker > \u02c6\u03b3tucker do 4: R \u2190 R/2, S \u2190 S/2, T \u2190 T /2 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end while if R < 1 then R \u2190 1 end if if S < 1 then S \u2190 1 end if if T < 1 then T \u2190 1 end if \u03b3tucker \u2190 RST +IR+JS+KT IJK decomposition as: W = (cid:88) Cr,s,t \u00d7 U 1 r \u2297 U 2 s \u2297 U 3 t , r,s,t where \u2297 describes a Kronecker product. The parameter com- pression ratio \u03b3tucker can be described as: \u03b3tucker = RST + IR + JS + KT IJK . 2.3. CP decomposition CP"}, {"question": " What is CP decomposition used for in the context of tensor decompositions?", "answer": " CP decomposition is explored for applying high-order tensors in the context of model compression.", "ref_chunk": "3 2 0 2 n u J 2 ] S A . s s e e [ 1 v 7 4 2 1 0 . 6 0 3 2 : v i X r a TENSOR DECOMPOSITION FOR MINIMIZATION OF E2E SLU MODEL TOWARD ON-DEVICE PROCESSING Yosuke Kashiwagi1, Siddhant Arora2, Hayato Futami1, Jessica Huynh2, Shih-Lun Wu2, Yifan Peng2, Brian Yan2, Emiru Tsunoo1, Shinji Watanabe2 1 Sony Group Corporation, Japan, 2 Carnegie Mellon University, USA yosuke.kashiwagi@sony.com Abstract high accuracy in the SLU task [17, 22]. Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency prop- erty, unlike a cascade system, and aims to minimize the com- putational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architec- tures used in our E2E SLU models. We propose to apply singu- lar value decomposition to linear layers and the Tucker decom- position to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposi- tion to the Tucker decomposition. Since the E2E model is rep- resented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 mil- lion parameters. Index Terms: spoken language understanding, E2E, on-device, sequential distillation, tensor decomposition, STOP Tensor decomposition techniques are widely used for model miniaturization [23]. Although various tensor decompo- sition techniques have been proposed, we mainly explore SVD, and Tucker decomposition [24] to target on-device fast process- ing. They enable inference without reconstructing the original parameter tensors from the factored tensors. Decomposition with smaller ranks can then be used to reduce computational complexity, enabling a reduction in the number of parameters and faster computation [25]. The paper demonstrates effective combinations of model compression techniques for Conformer and E-Branchformer, specifically through the use of tensor de- compositions such as Tucker decomposition. This is because these models have convolution layers with higher-order tensors as parameters, making them more efficiently compressed us- ing these techniques. By defining the model compression ra- tio, we show that the model size can be flexibly changed by determining the rank of the decomposition from this ratio. Fur- thermore, we evaluate CANDECOMP/PARAFAC (CP) decom- position [26] and Tensor-Train decomposition [27] when used instead of the Tucker decomposition. 1. Introduction 2. Tensor decomposition Spoken language understanding (SLU) is one of the essential applications of speech recognition. SLU is used in voice in- terfaces such as smart speakers, and improving its accuracy is required for usability [1\u20135]. On the other hand, these voice in- terfaces often work on edge devices due to latency and privacy issues. Such on-device processing requires smaller models to preserve power consumption. Therefore, miniaturization of the SLU model for on-device processing is a critical issue for voice interfaces [6\u201315]. For example, Radfar et al. reduce parameters by sharing the audio encoder and estimate slot tags, values, and intents [2, 6]. Also, Le et al. model NLU efficiently based on RNN-T us- ing embedded features of both the encoder and the predictor of RNN-T [7]. In addition, Tyagi et al. propose early decision- making using the BranchyNet scheme to address the latency and computational complexity issues [9]. Although these are prac- tical approaches, many are based on the traditional, relatively simple E2E model structure. On the other hand, recent E2E models have been used with more complex structures by com- bining convolution and self-attention operations such as Con- former [16] and E-Branchformer [17,18]. Therefore, in addition to using singular value decomposition (SVD) [19\u201321] for two- dimensional matrices in the self-attention network, convolution network having high-order tensors requires tensor decomposi- tion techniques. We specifically target the Conformer and E- Branchformer-based E2E SLU model, which has demonstrated 2.1. Singular value decomposition The SVD-based parameter reduction technique is stable and widely used [19\u201321]. The SVD is applied to the weight ma- trix W \u2208 RI\u00d7J with low-rank bases as: W = U SV , where S is a diagonal matrix. The matrix size of each U , S and V is I \u00d7 R, R \u00d7 R and R \u00d7 J, respectively. The parameter size can be controlled by changing R. Since S is a square matrix, U S or SV can be pre-composed to reduce the parameters. In our model, we composed SV . Thus, the final number of de- composed parameters can be reduced into IR + RJ. Therefore, the parameter compression ratio \u03b3svd can be described as: \u03b3svd = IR + RJ IJ . 2.2. Tucker decomposition Tucker decomposition [24] is known to be effective for apply- ing high-order tensors. Here we discuss the case of 1d convo- lution, which has a 3-dimensional parameter tensor. The pa- rameter tensor of the convolution W \u2208 RI\u00d7J\u00d7K can be de- composed into a core tensor C \u2208 RR\u00d7S\u00d7T and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7S and U 3 \u2208 RK\u00d7T using Tucker (1) (2) Algorithm 1 Size halving for Tucker decomposition Require: \u02c6\u03b3tucker > 0.0 1: R \u2190 I, S \u2190 J, T \u2190 K 2: \u03b3tucker \u2190 \u221e 3: while \u03b3tucker > \u02c6\u03b3tucker do 4: R \u2190 R/2, S \u2190 S/2, T \u2190 T /2 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end while if R < 1 then R \u2190 1 end if if S < 1 then S \u2190 1 end if if T < 1 then T \u2190 1 end if \u03b3tucker \u2190 RST +IR+JS+KT IJK decomposition as: W = (cid:88) Cr,s,t \u00d7 U 1 r \u2297 U 2 s \u2297 U 3 t , r,s,t where \u2297 describes a Kronecker product. The parameter com- pression ratio \u03b3tucker can be described as: \u03b3tucker = RST + IR + JS + KT IJK . 2.3. CP decomposition CP"}, {"question": " Why is model compression essential for on-device processing according to the text?", "answer": " Model compression is essential for on-device processing to reduce power consumption and enable faster computation on edge devices.", "ref_chunk": "3 2 0 2 n u J 2 ] S A . s s e e [ 1 v 7 4 2 1 0 . 6 0 3 2 : v i X r a TENSOR DECOMPOSITION FOR MINIMIZATION OF E2E SLU MODEL TOWARD ON-DEVICE PROCESSING Yosuke Kashiwagi1, Siddhant Arora2, Hayato Futami1, Jessica Huynh2, Shih-Lun Wu2, Yifan Peng2, Brian Yan2, Emiru Tsunoo1, Shinji Watanabe2 1 Sony Group Corporation, Japan, 2 Carnegie Mellon University, USA yosuke.kashiwagi@sony.com Abstract high accuracy in the SLU task [17, 22]. Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency prop- erty, unlike a cascade system, and aims to minimize the com- putational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architec- tures used in our E2E SLU models. We propose to apply singu- lar value decomposition to linear layers and the Tucker decom- position to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposi- tion to the Tucker decomposition. Since the E2E model is rep- resented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 mil- lion parameters. Index Terms: spoken language understanding, E2E, on-device, sequential distillation, tensor decomposition, STOP Tensor decomposition techniques are widely used for model miniaturization [23]. Although various tensor decompo- sition techniques have been proposed, we mainly explore SVD, and Tucker decomposition [24] to target on-device fast process- ing. They enable inference without reconstructing the original parameter tensors from the factored tensors. Decomposition with smaller ranks can then be used to reduce computational complexity, enabling a reduction in the number of parameters and faster computation [25]. The paper demonstrates effective combinations of model compression techniques for Conformer and E-Branchformer, specifically through the use of tensor de- compositions such as Tucker decomposition. This is because these models have convolution layers with higher-order tensors as parameters, making them more efficiently compressed us- ing these techniques. By defining the model compression ra- tio, we show that the model size can be flexibly changed by determining the rank of the decomposition from this ratio. Fur- thermore, we evaluate CANDECOMP/PARAFAC (CP) decom- position [26] and Tensor-Train decomposition [27] when used instead of the Tucker decomposition. 1. Introduction 2. Tensor decomposition Spoken language understanding (SLU) is one of the essential applications of speech recognition. SLU is used in voice in- terfaces such as smart speakers, and improving its accuracy is required for usability [1\u20135]. On the other hand, these voice in- terfaces often work on edge devices due to latency and privacy issues. Such on-device processing requires smaller models to preserve power consumption. Therefore, miniaturization of the SLU model for on-device processing is a critical issue for voice interfaces [6\u201315]. For example, Radfar et al. reduce parameters by sharing the audio encoder and estimate slot tags, values, and intents [2, 6]. Also, Le et al. model NLU efficiently based on RNN-T us- ing embedded features of both the encoder and the predictor of RNN-T [7]. In addition, Tyagi et al. propose early decision- making using the BranchyNet scheme to address the latency and computational complexity issues [9]. Although these are prac- tical approaches, many are based on the traditional, relatively simple E2E model structure. On the other hand, recent E2E models have been used with more complex structures by com- bining convolution and self-attention operations such as Con- former [16] and E-Branchformer [17,18]. Therefore, in addition to using singular value decomposition (SVD) [19\u201321] for two- dimensional matrices in the self-attention network, convolution network having high-order tensors requires tensor decomposi- tion techniques. We specifically target the Conformer and E- Branchformer-based E2E SLU model, which has demonstrated 2.1. Singular value decomposition The SVD-based parameter reduction technique is stable and widely used [19\u201321]. The SVD is applied to the weight ma- trix W \u2208 RI\u00d7J with low-rank bases as: W = U SV , where S is a diagonal matrix. The matrix size of each U , S and V is I \u00d7 R, R \u00d7 R and R \u00d7 J, respectively. The parameter size can be controlled by changing R. Since S is a square matrix, U S or SV can be pre-composed to reduce the parameters. In our model, we composed SV . Thus, the final number of de- composed parameters can be reduced into IR + RJ. Therefore, the parameter compression ratio \u03b3svd can be described as: \u03b3svd = IR + RJ IJ . 2.2. Tucker decomposition Tucker decomposition [24] is known to be effective for apply- ing high-order tensors. Here we discuss the case of 1d convo- lution, which has a 3-dimensional parameter tensor. The pa- rameter tensor of the convolution W \u2208 RI\u00d7J\u00d7K can be de- composed into a core tensor C \u2208 RR\u00d7S\u00d7T and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7S and U 3 \u2208 RK\u00d7T using Tucker (1) (2) Algorithm 1 Size halving for Tucker decomposition Require: \u02c6\u03b3tucker > 0.0 1: R \u2190 I, S \u2190 J, T \u2190 K 2: \u03b3tucker \u2190 \u221e 3: while \u03b3tucker > \u02c6\u03b3tucker do 4: R \u2190 R/2, S \u2190 S/2, T \u2190 T /2 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end while if R < 1 then R \u2190 1 end if if S < 1 then S \u2190 1 end if if T < 1 then T \u2190 1 end if \u03b3tucker \u2190 RST +IR+JS+KT IJK decomposition as: W = (cid:88) Cr,s,t \u00d7 U 1 r \u2297 U 2 s \u2297 U 3 t , r,s,t where \u2297 describes a Kronecker product. The parameter com- pression ratio \u03b3tucker can be described as: \u03b3tucker = RST + IR + JS + KT IJK . 2.3. CP decomposition CP"}, {"question": " What is the significance of combining convolution and self-attention operations in the Conformer and E-Branchformer models?", "answer": " Combining convolution and self-attention operations in these models makes them more efficiently compressed using tensor decomposition techniques.", "ref_chunk": "3 2 0 2 n u J 2 ] S A . s s e e [ 1 v 7 4 2 1 0 . 6 0 3 2 : v i X r a TENSOR DECOMPOSITION FOR MINIMIZATION OF E2E SLU MODEL TOWARD ON-DEVICE PROCESSING Yosuke Kashiwagi1, Siddhant Arora2, Hayato Futami1, Jessica Huynh2, Shih-Lun Wu2, Yifan Peng2, Brian Yan2, Emiru Tsunoo1, Shinji Watanabe2 1 Sony Group Corporation, Japan, 2 Carnegie Mellon University, USA yosuke.kashiwagi@sony.com Abstract high accuracy in the SLU task [17, 22]. Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency prop- erty, unlike a cascade system, and aims to minimize the com- putational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architec- tures used in our E2E SLU models. We propose to apply singu- lar value decomposition to linear layers and the Tucker decom- position to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposi- tion to the Tucker decomposition. Since the E2E model is rep- resented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 mil- lion parameters. Index Terms: spoken language understanding, E2E, on-device, sequential distillation, tensor decomposition, STOP Tensor decomposition techniques are widely used for model miniaturization [23]. Although various tensor decompo- sition techniques have been proposed, we mainly explore SVD, and Tucker decomposition [24] to target on-device fast process- ing. They enable inference without reconstructing the original parameter tensors from the factored tensors. Decomposition with smaller ranks can then be used to reduce computational complexity, enabling a reduction in the number of parameters and faster computation [25]. The paper demonstrates effective combinations of model compression techniques for Conformer and E-Branchformer, specifically through the use of tensor de- compositions such as Tucker decomposition. This is because these models have convolution layers with higher-order tensors as parameters, making them more efficiently compressed us- ing these techniques. By defining the model compression ra- tio, we show that the model size can be flexibly changed by determining the rank of the decomposition from this ratio. Fur- thermore, we evaluate CANDECOMP/PARAFAC (CP) decom- position [26] and Tensor-Train decomposition [27] when used instead of the Tucker decomposition. 1. Introduction 2. Tensor decomposition Spoken language understanding (SLU) is one of the essential applications of speech recognition. SLU is used in voice in- terfaces such as smart speakers, and improving its accuracy is required for usability [1\u20135]. On the other hand, these voice in- terfaces often work on edge devices due to latency and privacy issues. Such on-device processing requires smaller models to preserve power consumption. Therefore, miniaturization of the SLU model for on-device processing is a critical issue for voice interfaces [6\u201315]. For example, Radfar et al. reduce parameters by sharing the audio encoder and estimate slot tags, values, and intents [2, 6]. Also, Le et al. model NLU efficiently based on RNN-T us- ing embedded features of both the encoder and the predictor of RNN-T [7]. In addition, Tyagi et al. propose early decision- making using the BranchyNet scheme to address the latency and computational complexity issues [9]. Although these are prac- tical approaches, many are based on the traditional, relatively simple E2E model structure. On the other hand, recent E2E models have been used with more complex structures by com- bining convolution and self-attention operations such as Con- former [16] and E-Branchformer [17,18]. Therefore, in addition to using singular value decomposition (SVD) [19\u201321] for two- dimensional matrices in the self-attention network, convolution network having high-order tensors requires tensor decomposi- tion techniques. We specifically target the Conformer and E- Branchformer-based E2E SLU model, which has demonstrated 2.1. Singular value decomposition The SVD-based parameter reduction technique is stable and widely used [19\u201321]. The SVD is applied to the weight ma- trix W \u2208 RI\u00d7J with low-rank bases as: W = U SV , where S is a diagonal matrix. The matrix size of each U , S and V is I \u00d7 R, R \u00d7 R and R \u00d7 J, respectively. The parameter size can be controlled by changing R. Since S is a square matrix, U S or SV can be pre-composed to reduce the parameters. In our model, we composed SV . Thus, the final number of de- composed parameters can be reduced into IR + RJ. Therefore, the parameter compression ratio \u03b3svd can be described as: \u03b3svd = IR + RJ IJ . 2.2. Tucker decomposition Tucker decomposition [24] is known to be effective for apply- ing high-order tensors. Here we discuss the case of 1d convo- lution, which has a 3-dimensional parameter tensor. The pa- rameter tensor of the convolution W \u2208 RI\u00d7J\u00d7K can be de- composed into a core tensor C \u2208 RR\u00d7S\u00d7T and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7S and U 3 \u2208 RK\u00d7T using Tucker (1) (2) Algorithm 1 Size halving for Tucker decomposition Require: \u02c6\u03b3tucker > 0.0 1: R \u2190 I, S \u2190 J, T \u2190 K 2: \u03b3tucker \u2190 \u221e 3: while \u03b3tucker > \u02c6\u03b3tucker do 4: R \u2190 R/2, S \u2190 S/2, T \u2190 T /2 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end while if R < 1 then R \u2190 1 end if if S < 1 then S \u2190 1 end if if T < 1 then T \u2190 1 end if \u03b3tucker \u2190 RST +IR+JS+KT IJK decomposition as: W = (cid:88) Cr,s,t \u00d7 U 1 r \u2297 U 2 s \u2297 U 3 t , r,s,t where \u2297 describes a Kronecker product. The parameter com- pression ratio \u03b3tucker can be described as: \u03b3tucker = RST + IR + JS + KT IJK . 2.3. CP decomposition CP"}], "doc_text": "3 2 0 2 n u J 2 ] S A . s s e e [ 1 v 7 4 2 1 0 . 6 0 3 2 : v i X r a TENSOR DECOMPOSITION FOR MINIMIZATION OF E2E SLU MODEL TOWARD ON-DEVICE PROCESSING Yosuke Kashiwagi1, Siddhant Arora2, Hayato Futami1, Jessica Huynh2, Shih-Lun Wu2, Yifan Peng2, Brian Yan2, Emiru Tsunoo1, Shinji Watanabe2 1 Sony Group Corporation, Japan, 2 Carnegie Mellon University, USA yosuke.kashiwagi@sony.com Abstract high accuracy in the SLU task [17, 22]. Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency prop- erty, unlike a cascade system, and aims to minimize the com- putational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architec- tures used in our E2E SLU models. We propose to apply singu- lar value decomposition to linear layers and the Tucker decom- position to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposi- tion to the Tucker decomposition. Since the E2E model is rep- resented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 mil- lion parameters. Index Terms: spoken language understanding, E2E, on-device, sequential distillation, tensor decomposition, STOP Tensor decomposition techniques are widely used for model miniaturization [23]. Although various tensor decompo- sition techniques have been proposed, we mainly explore SVD, and Tucker decomposition [24] to target on-device fast process- ing. They enable inference without reconstructing the original parameter tensors from the factored tensors. Decomposition with smaller ranks can then be used to reduce computational complexity, enabling a reduction in the number of parameters and faster computation [25]. The paper demonstrates effective combinations of model compression techniques for Conformer and E-Branchformer, specifically through the use of tensor de- compositions such as Tucker decomposition. This is because these models have convolution layers with higher-order tensors as parameters, making them more efficiently compressed us- ing these techniques. By defining the model compression ra- tio, we show that the model size can be flexibly changed by determining the rank of the decomposition from this ratio. Fur- thermore, we evaluate CANDECOMP/PARAFAC (CP) decom- position [26] and Tensor-Train decomposition [27] when used instead of the Tucker decomposition. 1. Introduction 2. Tensor decomposition Spoken language understanding (SLU) is one of the essential applications of speech recognition. SLU is used in voice in- terfaces such as smart speakers, and improving its accuracy is required for usability [1\u20135]. On the other hand, these voice in- terfaces often work on edge devices due to latency and privacy issues. Such on-device processing requires smaller models to preserve power consumption. Therefore, miniaturization of the SLU model for on-device processing is a critical issue for voice interfaces [6\u201315]. For example, Radfar et al. reduce parameters by sharing the audio encoder and estimate slot tags, values, and intents [2, 6]. Also, Le et al. model NLU efficiently based on RNN-T us- ing embedded features of both the encoder and the predictor of RNN-T [7]. In addition, Tyagi et al. propose early decision- making using the BranchyNet scheme to address the latency and computational complexity issues [9]. Although these are prac- tical approaches, many are based on the traditional, relatively simple E2E model structure. On the other hand, recent E2E models have been used with more complex structures by com- bining convolution and self-attention operations such as Con- former [16] and E-Branchformer [17,18]. Therefore, in addition to using singular value decomposition (SVD) [19\u201321] for two- dimensional matrices in the self-attention network, convolution network having high-order tensors requires tensor decomposi- tion techniques. We specifically target the Conformer and E- Branchformer-based E2E SLU model, which has demonstrated 2.1. Singular value decomposition The SVD-based parameter reduction technique is stable and widely used [19\u201321]. The SVD is applied to the weight ma- trix W \u2208 RI\u00d7J with low-rank bases as: W = U SV , where S is a diagonal matrix. The matrix size of each U , S and V is I \u00d7 R, R \u00d7 R and R \u00d7 J, respectively. The parameter size can be controlled by changing R. Since S is a square matrix, U S or SV can be pre-composed to reduce the parameters. In our model, we composed SV . Thus, the final number of de- composed parameters can be reduced into IR + RJ. Therefore, the parameter compression ratio \u03b3svd can be described as: \u03b3svd = IR + RJ IJ . 2.2. Tucker decomposition Tucker decomposition [24] is known to be effective for apply- ing high-order tensors. Here we discuss the case of 1d convo- lution, which has a 3-dimensional parameter tensor. The pa- rameter tensor of the convolution W \u2208 RI\u00d7J\u00d7K can be de- composed into a core tensor C \u2208 RR\u00d7S\u00d7T and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7S and U 3 \u2208 RK\u00d7T using Tucker (1) (2) Algorithm 1 Size halving for Tucker decomposition Require: \u02c6\u03b3tucker > 0.0 1: R \u2190 I, S \u2190 J, T \u2190 K 2: \u03b3tucker \u2190 \u221e 3: while \u03b3tucker > \u02c6\u03b3tucker do 4: R \u2190 R/2, S \u2190 S/2, T \u2190 T /2 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end while if R < 1 then R \u2190 1 end if if S < 1 then S \u2190 1 end if if T < 1 then T \u2190 1 end if \u03b3tucker \u2190 RST +IR+JS+KT IJK decomposition as: W = (cid:88) Cr,s,t \u00d7 U 1 r \u2297 U 2 s \u2297 U 3 t , r,s,t where \u2297 describes a Kronecker product. The parameter com- pression ratio \u03b3tucker can be described as: \u03b3tucker = RST + IR + JS + KT IJK . 2.3. CP decomposition CP"}