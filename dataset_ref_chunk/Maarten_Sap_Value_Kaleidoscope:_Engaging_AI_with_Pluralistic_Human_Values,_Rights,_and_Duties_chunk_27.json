{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_Value_Kaleidoscope:_Engaging_AI_with_Pluralistic_Human_Values,_Rights,_and_Duties_chunk_27.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the minimum hourly wage paid to crowdworkers for tasks related to the dataset?,        answer: A minimum hourly wage of $15-25 USD.    ", "ref_chunk": "through demo users and the OpenAI API. However, to understand the dataset\u2019s quality and represen- tativeness, we do carry out several human studies on subsets of the data (see Section 4.1 and 4.2). We ensured that, for all tasks, crowdworkers were paid a minimum hourly wage of $15-25 USD. Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame? The situations were collected from 2021-2023 on the Delphi user demo, and the values, rights, and duties were generated using the OpenAI API from May 2023-July 2023. How was the data associated with each instance ac- quired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey re- sponses), or indirectly inferred/derived from other data (e.g., part of speech tags; model-based guesses for age or language)? If the latter two, were they validated/verified and if so how? The data is only associated in that the situations came from the demo and the remaining data from the OpenAI API. Does the dataset contain all possible instances? Or is it, for instance, a sample (not necessarily random) from a larger set of instances? If the dataset is a sample, then what is the population? What was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Is the sample representative of the larger set (e.g., geographic coverage)? If not, why not (e.g., to cover a more diverse range of instances)? How does this affect possible uses? We source our 31k situations about which to reason from a set of 1.3M user-submitted situations, and curate the dataset by filtering out situations that are not are not actions or un- related to morality (as labeled in a few-shot manner25 by Flan-T5 (Chung et al. 2022)). We also filter out any ques- tions using keyword matching. We note that an outsize proportion of the dataset involves toxic, NSFW, or sexually explicit content. In the interest of having a diversity of situations, we label for these attributes1 25Few-shot filtering prompts are found in Appendix M.1. using Flan-T5 (Chung et al. 2022). We take 95% of our situ- ations determinnistically from those that have less toxic/NS- FW/explicit content, and sample the other 5% uniformly from the rest of the data so as to include the entire spec- trum of inputs. We find that this succeeds in increasing the diversity of the dataset, as measured by unique n-grams di- vided by the length of the dataset (dist-2: .23\u2192.36, dist-3: .54\u2192.67). Is there information missing from the dataset and why? (this does not include intentionally dropped instances; it might include, e.g., redacted text, withheld documents) Is this data missing because it was unavailable? No, there is no known data missing from the dataset, al- though we do not claim or believe that the dataset is neces- sarily a comprehensive set of representative human values. Are there any known errors, sources of noise, or redun- dancies in the data? No known errors, sources of noise, or redundancies, al- though we hope future work will help to shed more light on weaknesses. N.4 Data Preprocessing What preprocessing/cleaning was done? (e.g., discretiza- tion or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, process- ing of missing values, etc.) The main preprocessing was extraction of the features from raw text output from GPT-4 to the semi-structured dataset that we have. We used regex expressions for this ex- traction. Was the \u201craw\u201d data saved in addition to the prepro- cessed/cleaned data? (e.g., to support unanticipated fu- ture uses) Yes, the raw GPT-4 outputs were saved in addition to the cleaned data. Is the preprocessing software available? Yes, all preprocessing software will be available at https: //github.com/tsor13/kaleido. Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet? It achieves the goal of 1) trying to understand what plu- ralistic human values, rights, and duties are currently em- bedded in GPT-4 (although not other LLMs). It achieves the goal of taking a first step to modeling human values, rights, and duties computationally, as manifested by KALEIDO, but we do not claim that it necessarily does so with accuracy and complete representativeness. N.5 Dataset Distribution How is the dataset distributed? (e.g., website, API, etc.; does the data have a DOI; is it archived redundantly?) We plan on distributing the dataset via Huggingface Datasets, but it will be gated for individual-approval and in- tended for research-use only in an attempt to prevent misuse. When will the dataset be released/first distributed? (Is there a canonical paper/reference for this dataset?) We plan on distributng the dataset in September 2023, with this manuscript as a reference. What license (if any) is it distributed under? Are there any copyrights on the data? We plan on distributing VALUEPRISM under the ImpACT license (Allen Insitute for AI 2023) as a \u201cmedium-risk arti- fact\u201d. Users must agree to all terms and restrictions of the license before accessing or using the dataset. Are there any fees or access/export restrictions? No, the dataset is distributed at no cost. However, we do gate access by individual request and access is predicated on acceptance of the license. N.6 Dataset Maintenance Who is supporting/hosting/maintaining the dataset? How does one contact the owner/curator/manager of the dataset (e.g. email address, or other contact info)? The Allen Institute for AI supports the dataset and it will be hosted on Huggingface. Corresponding authors are Taylor Sorensen (tsor13@cs.washington.edu) and Yejin Choi (yejin@cs.washington.edu). Will the dataset be updated? How often and by whom? How will updates/revisions be documented and commu- nicated (e.g., mailing list, GitHub)? Is there an erra- tum? We do not plan on updating the dataset. If the dataset becomes obsolete how will this be commu- nicated? Is there a repository to link to any/all paper- s/systems that use this dataset? We do not expect the dataset to become obselete as it does not depend on external sources. Users of VALUEPRISM should cite this manuscript. If others want to extend/augment/build on this dataset,"}, {"question": " From which years were the situations collected on the Delphi user demo?,        answer: The situations were collected from 2021-2023.    ", "ref_chunk": "through demo users and the OpenAI API. However, to understand the dataset\u2019s quality and represen- tativeness, we do carry out several human studies on subsets of the data (see Section 4.1 and 4.2). We ensured that, for all tasks, crowdworkers were paid a minimum hourly wage of $15-25 USD. Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame? The situations were collected from 2021-2023 on the Delphi user demo, and the values, rights, and duties were generated using the OpenAI API from May 2023-July 2023. How was the data associated with each instance ac- quired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey re- sponses), or indirectly inferred/derived from other data (e.g., part of speech tags; model-based guesses for age or language)? If the latter two, were they validated/verified and if so how? The data is only associated in that the situations came from the demo and the remaining data from the OpenAI API. Does the dataset contain all possible instances? Or is it, for instance, a sample (not necessarily random) from a larger set of instances? If the dataset is a sample, then what is the population? What was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Is the sample representative of the larger set (e.g., geographic coverage)? If not, why not (e.g., to cover a more diverse range of instances)? How does this affect possible uses? We source our 31k situations about which to reason from a set of 1.3M user-submitted situations, and curate the dataset by filtering out situations that are not are not actions or un- related to morality (as labeled in a few-shot manner25 by Flan-T5 (Chung et al. 2022)). We also filter out any ques- tions using keyword matching. We note that an outsize proportion of the dataset involves toxic, NSFW, or sexually explicit content. In the interest of having a diversity of situations, we label for these attributes1 25Few-shot filtering prompts are found in Appendix M.1. using Flan-T5 (Chung et al. 2022). We take 95% of our situ- ations determinnistically from those that have less toxic/NS- FW/explicit content, and sample the other 5% uniformly from the rest of the data so as to include the entire spec- trum of inputs. We find that this succeeds in increasing the diversity of the dataset, as measured by unique n-grams di- vided by the length of the dataset (dist-2: .23\u2192.36, dist-3: .54\u2192.67). Is there information missing from the dataset and why? (this does not include intentionally dropped instances; it might include, e.g., redacted text, withheld documents) Is this data missing because it was unavailable? No, there is no known data missing from the dataset, al- though we do not claim or believe that the dataset is neces- sarily a comprehensive set of representative human values. Are there any known errors, sources of noise, or redun- dancies in the data? No known errors, sources of noise, or redundancies, al- though we hope future work will help to shed more light on weaknesses. N.4 Data Preprocessing What preprocessing/cleaning was done? (e.g., discretiza- tion or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, process- ing of missing values, etc.) The main preprocessing was extraction of the features from raw text output from GPT-4 to the semi-structured dataset that we have. We used regex expressions for this ex- traction. Was the \u201craw\u201d data saved in addition to the prepro- cessed/cleaned data? (e.g., to support unanticipated fu- ture uses) Yes, the raw GPT-4 outputs were saved in addition to the cleaned data. Is the preprocessing software available? Yes, all preprocessing software will be available at https: //github.com/tsor13/kaleido. Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet? It achieves the goal of 1) trying to understand what plu- ralistic human values, rights, and duties are currently em- bedded in GPT-4 (although not other LLMs). It achieves the goal of taking a first step to modeling human values, rights, and duties computationally, as manifested by KALEIDO, but we do not claim that it necessarily does so with accuracy and complete representativeness. N.5 Dataset Distribution How is the dataset distributed? (e.g., website, API, etc.; does the data have a DOI; is it archived redundantly?) We plan on distributing the dataset via Huggingface Datasets, but it will be gated for individual-approval and in- tended for research-use only in an attempt to prevent misuse. When will the dataset be released/first distributed? (Is there a canonical paper/reference for this dataset?) We plan on distributng the dataset in September 2023, with this manuscript as a reference. What license (if any) is it distributed under? Are there any copyrights on the data? We plan on distributing VALUEPRISM under the ImpACT license (Allen Insitute for AI 2023) as a \u201cmedium-risk arti- fact\u201d. Users must agree to all terms and restrictions of the license before accessing or using the dataset. Are there any fees or access/export restrictions? No, the dataset is distributed at no cost. However, we do gate access by individual request and access is predicated on acceptance of the license. N.6 Dataset Maintenance Who is supporting/hosting/maintaining the dataset? How does one contact the owner/curator/manager of the dataset (e.g. email address, or other contact info)? The Allen Institute for AI supports the dataset and it will be hosted on Huggingface. Corresponding authors are Taylor Sorensen (tsor13@cs.washington.edu) and Yejin Choi (yejin@cs.washington.edu). Will the dataset be updated? How often and by whom? How will updates/revisions be documented and commu- nicated (e.g., mailing list, GitHub)? Is there an erra- tum? We do not plan on updating the dataset. If the dataset becomes obsolete how will this be commu- nicated? Is there a repository to link to any/all paper- s/systems that use this dataset? We do not expect the dataset to become obselete as it does not depend on external sources. Users of VALUEPRISM should cite this manuscript. If others want to extend/augment/build on this dataset,"}, {"question": " How was the data associated with each instance acquired?,        answer: The data was either directly observable, reported by subjects, or indirectly inferred/derived from other data.    ", "ref_chunk": "through demo users and the OpenAI API. However, to understand the dataset\u2019s quality and represen- tativeness, we do carry out several human studies on subsets of the data (see Section 4.1 and 4.2). We ensured that, for all tasks, crowdworkers were paid a minimum hourly wage of $15-25 USD. Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame? The situations were collected from 2021-2023 on the Delphi user demo, and the values, rights, and duties were generated using the OpenAI API from May 2023-July 2023. How was the data associated with each instance ac- quired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey re- sponses), or indirectly inferred/derived from other data (e.g., part of speech tags; model-based guesses for age or language)? If the latter two, were they validated/verified and if so how? The data is only associated in that the situations came from the demo and the remaining data from the OpenAI API. Does the dataset contain all possible instances? Or is it, for instance, a sample (not necessarily random) from a larger set of instances? If the dataset is a sample, then what is the population? What was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Is the sample representative of the larger set (e.g., geographic coverage)? If not, why not (e.g., to cover a more diverse range of instances)? How does this affect possible uses? We source our 31k situations about which to reason from a set of 1.3M user-submitted situations, and curate the dataset by filtering out situations that are not are not actions or un- related to morality (as labeled in a few-shot manner25 by Flan-T5 (Chung et al. 2022)). We also filter out any ques- tions using keyword matching. We note that an outsize proportion of the dataset involves toxic, NSFW, or sexually explicit content. In the interest of having a diversity of situations, we label for these attributes1 25Few-shot filtering prompts are found in Appendix M.1. using Flan-T5 (Chung et al. 2022). We take 95% of our situ- ations determinnistically from those that have less toxic/NS- FW/explicit content, and sample the other 5% uniformly from the rest of the data so as to include the entire spec- trum of inputs. We find that this succeeds in increasing the diversity of the dataset, as measured by unique n-grams di- vided by the length of the dataset (dist-2: .23\u2192.36, dist-3: .54\u2192.67). Is there information missing from the dataset and why? (this does not include intentionally dropped instances; it might include, e.g., redacted text, withheld documents) Is this data missing because it was unavailable? No, there is no known data missing from the dataset, al- though we do not claim or believe that the dataset is neces- sarily a comprehensive set of representative human values. Are there any known errors, sources of noise, or redun- dancies in the data? No known errors, sources of noise, or redundancies, al- though we hope future work will help to shed more light on weaknesses. N.4 Data Preprocessing What preprocessing/cleaning was done? (e.g., discretiza- tion or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, process- ing of missing values, etc.) The main preprocessing was extraction of the features from raw text output from GPT-4 to the semi-structured dataset that we have. We used regex expressions for this ex- traction. Was the \u201craw\u201d data saved in addition to the prepro- cessed/cleaned data? (e.g., to support unanticipated fu- ture uses) Yes, the raw GPT-4 outputs were saved in addition to the cleaned data. Is the preprocessing software available? Yes, all preprocessing software will be available at https: //github.com/tsor13/kaleido. Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet? It achieves the goal of 1) trying to understand what plu- ralistic human values, rights, and duties are currently em- bedded in GPT-4 (although not other LLMs). It achieves the goal of taking a first step to modeling human values, rights, and duties computationally, as manifested by KALEIDO, but we do not claim that it necessarily does so with accuracy and complete representativeness. N.5 Dataset Distribution How is the dataset distributed? (e.g., website, API, etc.; does the data have a DOI; is it archived redundantly?) We plan on distributing the dataset via Huggingface Datasets, but it will be gated for individual-approval and in- tended for research-use only in an attempt to prevent misuse. When will the dataset be released/first distributed? (Is there a canonical paper/reference for this dataset?) We plan on distributng the dataset in September 2023, with this manuscript as a reference. What license (if any) is it distributed under? Are there any copyrights on the data? We plan on distributing VALUEPRISM under the ImpACT license (Allen Insitute for AI 2023) as a \u201cmedium-risk arti- fact\u201d. Users must agree to all terms and restrictions of the license before accessing or using the dataset. Are there any fees or access/export restrictions? No, the dataset is distributed at no cost. However, we do gate access by individual request and access is predicated on acceptance of the license. N.6 Dataset Maintenance Who is supporting/hosting/maintaining the dataset? How does one contact the owner/curator/manager of the dataset (e.g. email address, or other contact info)? The Allen Institute for AI supports the dataset and it will be hosted on Huggingface. Corresponding authors are Taylor Sorensen (tsor13@cs.washington.edu) and Yejin Choi (yejin@cs.washington.edu). Will the dataset be updated? How often and by whom? How will updates/revisions be documented and commu- nicated (e.g., mailing list, GitHub)? Is there an erra- tum? We do not plan on updating the dataset. If the dataset becomes obsolete how will this be commu- nicated? Is there a repository to link to any/all paper- s/systems that use this dataset? We do not expect the dataset to become obselete as it does not depend on external sources. Users of VALUEPRISM should cite this manuscript. If others want to extend/augment/build on this dataset,"}, {"question": " How did the dataset creators ensure diversity in the dataset?,        answer: By filtering out toxic, NSFW, or sexually explicit content and including a mixture of less toxic content and uniformly sampled content.    ", "ref_chunk": "through demo users and the OpenAI API. However, to understand the dataset\u2019s quality and represen- tativeness, we do carry out several human studies on subsets of the data (see Section 4.1 and 4.2). We ensured that, for all tasks, crowdworkers were paid a minimum hourly wage of $15-25 USD. Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame? The situations were collected from 2021-2023 on the Delphi user demo, and the values, rights, and duties were generated using the OpenAI API from May 2023-July 2023. How was the data associated with each instance ac- quired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey re- sponses), or indirectly inferred/derived from other data (e.g., part of speech tags; model-based guesses for age or language)? If the latter two, were they validated/verified and if so how? The data is only associated in that the situations came from the demo and the remaining data from the OpenAI API. Does the dataset contain all possible instances? Or is it, for instance, a sample (not necessarily random) from a larger set of instances? If the dataset is a sample, then what is the population? What was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Is the sample representative of the larger set (e.g., geographic coverage)? If not, why not (e.g., to cover a more diverse range of instances)? How does this affect possible uses? We source our 31k situations about which to reason from a set of 1.3M user-submitted situations, and curate the dataset by filtering out situations that are not are not actions or un- related to morality (as labeled in a few-shot manner25 by Flan-T5 (Chung et al. 2022)). We also filter out any ques- tions using keyword matching. We note that an outsize proportion of the dataset involves toxic, NSFW, or sexually explicit content. In the interest of having a diversity of situations, we label for these attributes1 25Few-shot filtering prompts are found in Appendix M.1. using Flan-T5 (Chung et al. 2022). We take 95% of our situ- ations determinnistically from those that have less toxic/NS- FW/explicit content, and sample the other 5% uniformly from the rest of the data so as to include the entire spec- trum of inputs. We find that this succeeds in increasing the diversity of the dataset, as measured by unique n-grams di- vided by the length of the dataset (dist-2: .23\u2192.36, dist-3: .54\u2192.67). Is there information missing from the dataset and why? (this does not include intentionally dropped instances; it might include, e.g., redacted text, withheld documents) Is this data missing because it was unavailable? No, there is no known data missing from the dataset, al- though we do not claim or believe that the dataset is neces- sarily a comprehensive set of representative human values. Are there any known errors, sources of noise, or redun- dancies in the data? No known errors, sources of noise, or redundancies, al- though we hope future work will help to shed more light on weaknesses. N.4 Data Preprocessing What preprocessing/cleaning was done? (e.g., discretiza- tion or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, process- ing of missing values, etc.) The main preprocessing was extraction of the features from raw text output from GPT-4 to the semi-structured dataset that we have. We used regex expressions for this ex- traction. Was the \u201craw\u201d data saved in addition to the prepro- cessed/cleaned data? (e.g., to support unanticipated fu- ture uses) Yes, the raw GPT-4 outputs were saved in addition to the cleaned data. Is the preprocessing software available? Yes, all preprocessing software will be available at https: //github.com/tsor13/kaleido. Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet? It achieves the goal of 1) trying to understand what plu- ralistic human values, rights, and duties are currently em- bedded in GPT-4 (although not other LLMs). It achieves the goal of taking a first step to modeling human values, rights, and duties computationally, as manifested by KALEIDO, but we do not claim that it necessarily does so with accuracy and complete representativeness. N.5 Dataset Distribution How is the dataset distributed? (e.g., website, API, etc.; does the data have a DOI; is it archived redundantly?) We plan on distributing the dataset via Huggingface Datasets, but it will be gated for individual-approval and in- tended for research-use only in an attempt to prevent misuse. When will the dataset be released/first distributed? (Is there a canonical paper/reference for this dataset?) We plan on distributng the dataset in September 2023, with this manuscript as a reference. What license (if any) is it distributed under? Are there any copyrights on the data? We plan on distributing VALUEPRISM under the ImpACT license (Allen Insitute for AI 2023) as a \u201cmedium-risk arti- fact\u201d. Users must agree to all terms and restrictions of the license before accessing or using the dataset. Are there any fees or access/export restrictions? No, the dataset is distributed at no cost. However, we do gate access by individual request and access is predicated on acceptance of the license. N.6 Dataset Maintenance Who is supporting/hosting/maintaining the dataset? How does one contact the owner/curator/manager of the dataset (e.g. email address, or other contact info)? The Allen Institute for AI supports the dataset and it will be hosted on Huggingface. Corresponding authors are Taylor Sorensen (tsor13@cs.washington.edu) and Yejin Choi (yejin@cs.washington.edu). Will the dataset be updated? How often and by whom? How will updates/revisions be documented and commu- nicated (e.g., mailing list, GitHub)? Is there an erra- tum? We do not plan on updating the dataset. If the dataset becomes obsolete how will this be commu- nicated? Is there a repository to link to any/all paper- s/systems that use this dataset? We do not expect the dataset to become obselete as it does not depend on external sources. Users of VALUEPRISM should cite this manuscript. If others want to extend/augment/build on this dataset,"}, {"question": " Is there any missing information in the dataset? Why?,        answer: No, there is no known missing data in the dataset.    ", "ref_chunk": "through demo users and the OpenAI API. However, to understand the dataset\u2019s quality and represen- tativeness, we do carry out several human studies on subsets of the data (see Section 4.1 and 4.2). We ensured that, for all tasks, crowdworkers were paid a minimum hourly wage of $15-25 USD. Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame? The situations were collected from 2021-2023 on the Delphi user demo, and the values, rights, and duties were generated using the OpenAI API from May 2023-July 2023. How was the data associated with each instance ac- quired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey re- sponses), or indirectly inferred/derived from other data (e.g., part of speech tags; model-based guesses for age or language)? If the latter two, were they validated/verified and if so how? The data is only associated in that the situations came from the demo and the remaining data from the OpenAI API. Does the dataset contain all possible instances? Or is it, for instance, a sample (not necessarily random) from a larger set of instances? If the dataset is a sample, then what is the population? What was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Is the sample representative of the larger set (e.g., geographic coverage)? If not, why not (e.g., to cover a more diverse range of instances)? How does this affect possible uses? We source our 31k situations about which to reason from a set of 1.3M user-submitted situations, and curate the dataset by filtering out situations that are not are not actions or un- related to morality (as labeled in a few-shot manner25 by Flan-T5 (Chung et al. 2022)). We also filter out any ques- tions using keyword matching. We note that an outsize proportion of the dataset involves toxic, NSFW, or sexually explicit content. In the interest of having a diversity of situations, we label for these attributes1 25Few-shot filtering prompts are found in Appendix M.1. using Flan-T5 (Chung et al. 2022). We take 95% of our situ- ations determinnistically from those that have less toxic/NS- FW/explicit content, and sample the other 5% uniformly from the rest of the data so as to include the entire spec- trum of inputs. We find that this succeeds in increasing the diversity of the dataset, as measured by unique n-grams di- vided by the length of the dataset (dist-2: .23\u2192.36, dist-3: .54\u2192.67). Is there information missing from the dataset and why? (this does not include intentionally dropped instances; it might include, e.g., redacted text, withheld documents) Is this data missing because it was unavailable? No, there is no known data missing from the dataset, al- though we do not claim or believe that the dataset is neces- sarily a comprehensive set of representative human values. Are there any known errors, sources of noise, or redun- dancies in the data? No known errors, sources of noise, or redundancies, al- though we hope future work will help to shed more light on weaknesses. N.4 Data Preprocessing What preprocessing/cleaning was done? (e.g., discretiza- tion or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, process- ing of missing values, etc.) The main preprocessing was extraction of the features from raw text output from GPT-4 to the semi-structured dataset that we have. We used regex expressions for this ex- traction. Was the \u201craw\u201d data saved in addition to the prepro- cessed/cleaned data? (e.g., to support unanticipated fu- ture uses) Yes, the raw GPT-4 outputs were saved in addition to the cleaned data. Is the preprocessing software available? Yes, all preprocessing software will be available at https: //github.com/tsor13/kaleido. Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet? It achieves the goal of 1) trying to understand what plu- ralistic human values, rights, and duties are currently em- bedded in GPT-4 (although not other LLMs). It achieves the goal of taking a first step to modeling human values, rights, and duties computationally, as manifested by KALEIDO, but we do not claim that it necessarily does so with accuracy and complete representativeness. N.5 Dataset Distribution How is the dataset distributed? (e.g., website, API, etc.; does the data have a DOI; is it archived redundantly?) We plan on distributing the dataset via Huggingface Datasets, but it will be gated for individual-approval and in- tended for research-use only in an attempt to prevent misuse. When will the dataset be released/first distributed? (Is there a canonical paper/reference for this dataset?) We plan on distributng the dataset in September 2023, with this manuscript as a reference. What license (if any) is it distributed under? Are there any copyrights on the data? We plan on distributing VALUEPRISM under the ImpACT license (Allen Insitute for AI 2023) as a \u201cmedium-risk arti- fact\u201d. Users must agree to all terms and restrictions of the license before accessing or using the dataset. Are there any fees or access/export restrictions? No, the dataset is distributed at no cost. However, we do gate access by individual request and access is predicated on acceptance of the license. N.6 Dataset Maintenance Who is supporting/hosting/maintaining the dataset? How does one contact the owner/curator/manager of the dataset (e.g. email address, or other contact info)? The Allen Institute for AI supports the dataset and it will be hosted on Huggingface. Corresponding authors are Taylor Sorensen (tsor13@cs.washington.edu) and Yejin Choi (yejin@cs.washington.edu). Will the dataset be updated? How often and by whom? How will updates/revisions be documented and commu- nicated (e.g., mailing list, GitHub)? Is there an erra- tum? We do not plan on updating the dataset. If the dataset becomes obsolete how will this be commu- nicated? Is there a repository to link to any/all paper- s/systems that use this dataset? We do not expect the dataset to become obselete as it does not depend on external sources. Users of VALUEPRISM should cite this manuscript. If others want to extend/augment/build on this dataset,"}, {"question": " Was any preprocessing done on the data? If yes, what was the main preprocessing method used?,        answer: Yes, the main preprocessing involved extracting features from raw text using regex expressions.    ", "ref_chunk": "through demo users and the OpenAI API. However, to understand the dataset\u2019s quality and represen- tativeness, we do carry out several human studies on subsets of the data (see Section 4.1 and 4.2). We ensured that, for all tasks, crowdworkers were paid a minimum hourly wage of $15-25 USD. Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame? The situations were collected from 2021-2023 on the Delphi user demo, and the values, rights, and duties were generated using the OpenAI API from May 2023-July 2023. How was the data associated with each instance ac- quired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey re- sponses), or indirectly inferred/derived from other data (e.g., part of speech tags; model-based guesses for age or language)? If the latter two, were they validated/verified and if so how? The data is only associated in that the situations came from the demo and the remaining data from the OpenAI API. Does the dataset contain all possible instances? Or is it, for instance, a sample (not necessarily random) from a larger set of instances? If the dataset is a sample, then what is the population? What was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Is the sample representative of the larger set (e.g., geographic coverage)? If not, why not (e.g., to cover a more diverse range of instances)? How does this affect possible uses? We source our 31k situations about which to reason from a set of 1.3M user-submitted situations, and curate the dataset by filtering out situations that are not are not actions or un- related to morality (as labeled in a few-shot manner25 by Flan-T5 (Chung et al. 2022)). We also filter out any ques- tions using keyword matching. We note that an outsize proportion of the dataset involves toxic, NSFW, or sexually explicit content. In the interest of having a diversity of situations, we label for these attributes1 25Few-shot filtering prompts are found in Appendix M.1. using Flan-T5 (Chung et al. 2022). We take 95% of our situ- ations determinnistically from those that have less toxic/NS- FW/explicit content, and sample the other 5% uniformly from the rest of the data so as to include the entire spec- trum of inputs. We find that this succeeds in increasing the diversity of the dataset, as measured by unique n-grams di- vided by the length of the dataset (dist-2: .23\u2192.36, dist-3: .54\u2192.67). Is there information missing from the dataset and why? (this does not include intentionally dropped instances; it might include, e.g., redacted text, withheld documents) Is this data missing because it was unavailable? No, there is no known data missing from the dataset, al- though we do not claim or believe that the dataset is neces- sarily a comprehensive set of representative human values. Are there any known errors, sources of noise, or redun- dancies in the data? No known errors, sources of noise, or redundancies, al- though we hope future work will help to shed more light on weaknesses. N.4 Data Preprocessing What preprocessing/cleaning was done? (e.g., discretiza- tion or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, process- ing of missing values, etc.) The main preprocessing was extraction of the features from raw text output from GPT-4 to the semi-structured dataset that we have. We used regex expressions for this ex- traction. Was the \u201craw\u201d data saved in addition to the prepro- cessed/cleaned data? (e.g., to support unanticipated fu- ture uses) Yes, the raw GPT-4 outputs were saved in addition to the cleaned data. Is the preprocessing software available? Yes, all preprocessing software will be available at https: //github.com/tsor13/kaleido. Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet? It achieves the goal of 1) trying to understand what plu- ralistic human values, rights, and duties are currently em- bedded in GPT-4 (although not other LLMs). It achieves the goal of taking a first step to modeling human values, rights, and duties computationally, as manifested by KALEIDO, but we do not claim that it necessarily does so with accuracy and complete representativeness. N.5 Dataset Distribution How is the dataset distributed? (e.g., website, API, etc.; does the data have a DOI; is it archived redundantly?) We plan on distributing the dataset via Huggingface Datasets, but it will be gated for individual-approval and in- tended for research-use only in an attempt to prevent misuse. When will the dataset be released/first distributed? (Is there a canonical paper/reference for this dataset?) We plan on distributng the dataset in September 2023, with this manuscript as a reference. What license (if any) is it distributed under? Are there any copyrights on the data? We plan on distributing VALUEPRISM under the ImpACT license (Allen Insitute for AI 2023) as a \u201cmedium-risk arti- fact\u201d. Users must agree to all terms and restrictions of the license before accessing or using the dataset. Are there any fees or access/export restrictions? No, the dataset is distributed at no cost. However, we do gate access by individual request and access is predicated on acceptance of the license. N.6 Dataset Maintenance Who is supporting/hosting/maintaining the dataset? How does one contact the owner/curator/manager of the dataset (e.g. email address, or other contact info)? The Allen Institute for AI supports the dataset and it will be hosted on Huggingface. Corresponding authors are Taylor Sorensen (tsor13@cs.washington.edu) and Yejin Choi (yejin@cs.washington.edu). Will the dataset be updated? How often and by whom? How will updates/revisions be documented and commu- nicated (e.g., mailing list, GitHub)? Is there an erra- tum? We do not plan on updating the dataset. If the dataset becomes obsolete how will this be commu- nicated? Is there a repository to link to any/all paper- s/systems that use this dataset? We do not expect the dataset to become obselete as it does not depend on external sources. Users of VALUEPRISM should cite this manuscript. If others want to extend/augment/build on this dataset,"}, {"question": " Is the raw data saved in addition to the cleaned data?,        answer: Yes, the raw GPT-4 outputs were saved along with the cleaned data.    ", "ref_chunk": "through demo users and the OpenAI API. However, to understand the dataset\u2019s quality and represen- tativeness, we do carry out several human studies on subsets of the data (see Section 4.1 and 4.2). We ensured that, for all tasks, crowdworkers were paid a minimum hourly wage of $15-25 USD. Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame? The situations were collected from 2021-2023 on the Delphi user demo, and the values, rights, and duties were generated using the OpenAI API from May 2023-July 2023. How was the data associated with each instance ac- quired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey re- sponses), or indirectly inferred/derived from other data (e.g., part of speech tags; model-based guesses for age or language)? If the latter two, were they validated/verified and if so how? The data is only associated in that the situations came from the demo and the remaining data from the OpenAI API. Does the dataset contain all possible instances? Or is it, for instance, a sample (not necessarily random) from a larger set of instances? If the dataset is a sample, then what is the population? What was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Is the sample representative of the larger set (e.g., geographic coverage)? If not, why not (e.g., to cover a more diverse range of instances)? How does this affect possible uses? We source our 31k situations about which to reason from a set of 1.3M user-submitted situations, and curate the dataset by filtering out situations that are not are not actions or un- related to morality (as labeled in a few-shot manner25 by Flan-T5 (Chung et al. 2022)). We also filter out any ques- tions using keyword matching. We note that an outsize proportion of the dataset involves toxic, NSFW, or sexually explicit content. In the interest of having a diversity of situations, we label for these attributes1 25Few-shot filtering prompts are found in Appendix M.1. using Flan-T5 (Chung et al. 2022). We take 95% of our situ- ations determinnistically from those that have less toxic/NS- FW/explicit content, and sample the other 5% uniformly from the rest of the data so as to include the entire spec- trum of inputs. We find that this succeeds in increasing the diversity of the dataset, as measured by unique n-grams di- vided by the length of the dataset (dist-2: .23\u2192.36, dist-3: .54\u2192.67). Is there information missing from the dataset and why? (this does not include intentionally dropped instances; it might include, e.g., redacted text, withheld documents) Is this data missing because it was unavailable? No, there is no known data missing from the dataset, al- though we do not claim or believe that the dataset is neces- sarily a comprehensive set of representative human values. Are there any known errors, sources of noise, or redun- dancies in the data? No known errors, sources of noise, or redundancies, al- though we hope future work will help to shed more light on weaknesses. N.4 Data Preprocessing What preprocessing/cleaning was done? (e.g., discretiza- tion or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, process- ing of missing values, etc.) The main preprocessing was extraction of the features from raw text output from GPT-4 to the semi-structured dataset that we have. We used regex expressions for this ex- traction. Was the \u201craw\u201d data saved in addition to the prepro- cessed/cleaned data? (e.g., to support unanticipated fu- ture uses) Yes, the raw GPT-4 outputs were saved in addition to the cleaned data. Is the preprocessing software available? Yes, all preprocessing software will be available at https: //github.com/tsor13/kaleido. Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet? It achieves the goal of 1) trying to understand what plu- ralistic human values, rights, and duties are currently em- bedded in GPT-4 (although not other LLMs). It achieves the goal of taking a first step to modeling human values, rights, and duties computationally, as manifested by KALEIDO, but we do not claim that it necessarily does so with accuracy and complete representativeness. N.5 Dataset Distribution How is the dataset distributed? (e.g., website, API, etc.; does the data have a DOI; is it archived redundantly?) We plan on distributing the dataset via Huggingface Datasets, but it will be gated for individual-approval and in- tended for research-use only in an attempt to prevent misuse. When will the dataset be released/first distributed? (Is there a canonical paper/reference for this dataset?) We plan on distributng the dataset in September 2023, with this manuscript as a reference. What license (if any) is it distributed under? Are there any copyrights on the data? We plan on distributing VALUEPRISM under the ImpACT license (Allen Insitute for AI 2023) as a \u201cmedium-risk arti- fact\u201d. Users must agree to all terms and restrictions of the license before accessing or using the dataset. Are there any fees or access/export restrictions? No, the dataset is distributed at no cost. However, we do gate access by individual request and access is predicated on acceptance of the license. N.6 Dataset Maintenance Who is supporting/hosting/maintaining the dataset? How does one contact the owner/curator/manager of the dataset (e.g. email address, or other contact info)? The Allen Institute for AI supports the dataset and it will be hosted on Huggingface. Corresponding authors are Taylor Sorensen (tsor13@cs.washington.edu) and Yejin Choi (yejin@cs.washington.edu). Will the dataset be updated? How often and by whom? How will updates/revisions be documented and commu- nicated (e.g., mailing list, GitHub)? Is there an erra- tum? We do not plan on updating the dataset. If the dataset becomes obsolete how will this be commu- nicated? Is there a repository to link to any/all paper- s/systems that use this dataset? We do not expect the dataset to become obselete as it does not depend on external sources. Users of VALUEPRISM should cite this manuscript. If others want to extend/augment/build on this dataset,"}, {"question": " How is the dataset distributed?,        answer: The dataset is planned to be distributed via Huggingface Datasets, with individual approval required for research use.    ", "ref_chunk": "through demo users and the OpenAI API. However, to understand the dataset\u2019s quality and represen- tativeness, we do carry out several human studies on subsets of the data (see Section 4.1 and 4.2). We ensured that, for all tasks, crowdworkers were paid a minimum hourly wage of $15-25 USD. Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame? The situations were collected from 2021-2023 on the Delphi user demo, and the values, rights, and duties were generated using the OpenAI API from May 2023-July 2023. How was the data associated with each instance ac- quired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey re- sponses), or indirectly inferred/derived from other data (e.g., part of speech tags; model-based guesses for age or language)? If the latter two, were they validated/verified and if so how? The data is only associated in that the situations came from the demo and the remaining data from the OpenAI API. Does the dataset contain all possible instances? Or is it, for instance, a sample (not necessarily random) from a larger set of instances? If the dataset is a sample, then what is the population? What was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Is the sample representative of the larger set (e.g., geographic coverage)? If not, why not (e.g., to cover a more diverse range of instances)? How does this affect possible uses? We source our 31k situations about which to reason from a set of 1.3M user-submitted situations, and curate the dataset by filtering out situations that are not are not actions or un- related to morality (as labeled in a few-shot manner25 by Flan-T5 (Chung et al. 2022)). We also filter out any ques- tions using keyword matching. We note that an outsize proportion of the dataset involves toxic, NSFW, or sexually explicit content. In the interest of having a diversity of situations, we label for these attributes1 25Few-shot filtering prompts are found in Appendix M.1. using Flan-T5 (Chung et al. 2022). We take 95% of our situ- ations determinnistically from those that have less toxic/NS- FW/explicit content, and sample the other 5% uniformly from the rest of the data so as to include the entire spec- trum of inputs. We find that this succeeds in increasing the diversity of the dataset, as measured by unique n-grams di- vided by the length of the dataset (dist-2: .23\u2192.36, dist-3: .54\u2192.67). Is there information missing from the dataset and why? (this does not include intentionally dropped instances; it might include, e.g., redacted text, withheld documents) Is this data missing because it was unavailable? No, there is no known data missing from the dataset, al- though we do not claim or believe that the dataset is neces- sarily a comprehensive set of representative human values. Are there any known errors, sources of noise, or redun- dancies in the data? No known errors, sources of noise, or redundancies, al- though we hope future work will help to shed more light on weaknesses. N.4 Data Preprocessing What preprocessing/cleaning was done? (e.g., discretiza- tion or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, process- ing of missing values, etc.) The main preprocessing was extraction of the features from raw text output from GPT-4 to the semi-structured dataset that we have. We used regex expressions for this ex- traction. Was the \u201craw\u201d data saved in addition to the prepro- cessed/cleaned data? (e.g., to support unanticipated fu- ture uses) Yes, the raw GPT-4 outputs were saved in addition to the cleaned data. Is the preprocessing software available? Yes, all preprocessing software will be available at https: //github.com/tsor13/kaleido. Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet? It achieves the goal of 1) trying to understand what plu- ralistic human values, rights, and duties are currently em- bedded in GPT-4 (although not other LLMs). It achieves the goal of taking a first step to modeling human values, rights, and duties computationally, as manifested by KALEIDO, but we do not claim that it necessarily does so with accuracy and complete representativeness. N.5 Dataset Distribution How is the dataset distributed? (e.g., website, API, etc.; does the data have a DOI; is it archived redundantly?) We plan on distributing the dataset via Huggingface Datasets, but it will be gated for individual-approval and in- tended for research-use only in an attempt to prevent misuse. When will the dataset be released/first distributed? (Is there a canonical paper/reference for this dataset?) We plan on distributng the dataset in September 2023, with this manuscript as a reference. What license (if any) is it distributed under? Are there any copyrights on the data? We plan on distributing VALUEPRISM under the ImpACT license (Allen Insitute for AI 2023) as a \u201cmedium-risk arti- fact\u201d. Users must agree to all terms and restrictions of the license before accessing or using the dataset. Are there any fees or access/export restrictions? No, the dataset is distributed at no cost. However, we do gate access by individual request and access is predicated on acceptance of the license. N.6 Dataset Maintenance Who is supporting/hosting/maintaining the dataset? How does one contact the owner/curator/manager of the dataset (e.g. email address, or other contact info)? The Allen Institute for AI supports the dataset and it will be hosted on Huggingface. Corresponding authors are Taylor Sorensen (tsor13@cs.washington.edu) and Yejin Choi (yejin@cs.washington.edu). Will the dataset be updated? How often and by whom? How will updates/revisions be documented and commu- nicated (e.g., mailing list, GitHub)? Is there an erra- tum? We do not plan on updating the dataset. If the dataset becomes obsolete how will this be commu- nicated? Is there a repository to link to any/all paper- s/systems that use this dataset? We do not expect the dataset to become obselete as it does not depend on external sources. Users of VALUEPRISM should cite this manuscript. If others want to extend/augment/build on this dataset,"}, {"question": " Under what license is the dataset distributed?,        answer: The dataset is planned to be distributed under the ImpACT license by the Allen Insitute for AI.    ", "ref_chunk": "through demo users and the OpenAI API. However, to understand the dataset\u2019s quality and represen- tativeness, we do carry out several human studies on subsets of the data (see Section 4.1 and 4.2). We ensured that, for all tasks, crowdworkers were paid a minimum hourly wage of $15-25 USD. Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame? The situations were collected from 2021-2023 on the Delphi user demo, and the values, rights, and duties were generated using the OpenAI API from May 2023-July 2023. How was the data associated with each instance ac- quired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey re- sponses), or indirectly inferred/derived from other data (e.g., part of speech tags; model-based guesses for age or language)? If the latter two, were they validated/verified and if so how? The data is only associated in that the situations came from the demo and the remaining data from the OpenAI API. Does the dataset contain all possible instances? Or is it, for instance, a sample (not necessarily random) from a larger set of instances? If the dataset is a sample, then what is the population? What was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Is the sample representative of the larger set (e.g., geographic coverage)? If not, why not (e.g., to cover a more diverse range of instances)? How does this affect possible uses? We source our 31k situations about which to reason from a set of 1.3M user-submitted situations, and curate the dataset by filtering out situations that are not are not actions or un- related to morality (as labeled in a few-shot manner25 by Flan-T5 (Chung et al. 2022)). We also filter out any ques- tions using keyword matching. We note that an outsize proportion of the dataset involves toxic, NSFW, or sexually explicit content. In the interest of having a diversity of situations, we label for these attributes1 25Few-shot filtering prompts are found in Appendix M.1. using Flan-T5 (Chung et al. 2022). We take 95% of our situ- ations determinnistically from those that have less toxic/NS- FW/explicit content, and sample the other 5% uniformly from the rest of the data so as to include the entire spec- trum of inputs. We find that this succeeds in increasing the diversity of the dataset, as measured by unique n-grams di- vided by the length of the dataset (dist-2: .23\u2192.36, dist-3: .54\u2192.67). Is there information missing from the dataset and why? (this does not include intentionally dropped instances; it might include, e.g., redacted text, withheld documents) Is this data missing because it was unavailable? No, there is no known data missing from the dataset, al- though we do not claim or believe that the dataset is neces- sarily a comprehensive set of representative human values. Are there any known errors, sources of noise, or redun- dancies in the data? No known errors, sources of noise, or redundancies, al- though we hope future work will help to shed more light on weaknesses. N.4 Data Preprocessing What preprocessing/cleaning was done? (e.g., discretiza- tion or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, process- ing of missing values, etc.) The main preprocessing was extraction of the features from raw text output from GPT-4 to the semi-structured dataset that we have. We used regex expressions for this ex- traction. Was the \u201craw\u201d data saved in addition to the prepro- cessed/cleaned data? (e.g., to support unanticipated fu- ture uses) Yes, the raw GPT-4 outputs were saved in addition to the cleaned data. Is the preprocessing software available? Yes, all preprocessing software will be available at https: //github.com/tsor13/kaleido. Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet? It achieves the goal of 1) trying to understand what plu- ralistic human values, rights, and duties are currently em- bedded in GPT-4 (although not other LLMs). It achieves the goal of taking a first step to modeling human values, rights, and duties computationally, as manifested by KALEIDO, but we do not claim that it necessarily does so with accuracy and complete representativeness. N.5 Dataset Distribution How is the dataset distributed? (e.g., website, API, etc.; does the data have a DOI; is it archived redundantly?) We plan on distributing the dataset via Huggingface Datasets, but it will be gated for individual-approval and in- tended for research-use only in an attempt to prevent misuse. When will the dataset be released/first distributed? (Is there a canonical paper/reference for this dataset?) We plan on distributng the dataset in September 2023, with this manuscript as a reference. What license (if any) is it distributed under? Are there any copyrights on the data? We plan on distributing VALUEPRISM under the ImpACT license (Allen Insitute for AI 2023) as a \u201cmedium-risk arti- fact\u201d. Users must agree to all terms and restrictions of the license before accessing or using the dataset. Are there any fees or access/export restrictions? No, the dataset is distributed at no cost. However, we do gate access by individual request and access is predicated on acceptance of the license. N.6 Dataset Maintenance Who is supporting/hosting/maintaining the dataset? How does one contact the owner/curator/manager of the dataset (e.g. email address, or other contact info)? The Allen Institute for AI supports the dataset and it will be hosted on Huggingface. Corresponding authors are Taylor Sorensen (tsor13@cs.washington.edu) and Yejin Choi (yejin@cs.washington.edu). Will the dataset be updated? How often and by whom? How will updates/revisions be documented and commu- nicated (e.g., mailing list, GitHub)? Is there an erra- tum? We do not plan on updating the dataset. If the dataset becomes obsolete how will this be commu- nicated? Is there a repository to link to any/all paper- s/systems that use this dataset? We do not expect the dataset to become obselete as it does not depend on external sources. Users of VALUEPRISM should cite this manuscript. If others want to extend/augment/build on this dataset,"}, {"question": " Who is supporting and maintaining the dataset?,        answer: The dataset is supported by the Allen Institute for AI and hosted on Huggingface. Contact information is provided for the corresponding authors.    ", "ref_chunk": "through demo users and the OpenAI API. However, to understand the dataset\u2019s quality and represen- tativeness, we do carry out several human studies on subsets of the data (see Section 4.1 and 4.2). We ensured that, for all tasks, crowdworkers were paid a minimum hourly wage of $15-25 USD. Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame? The situations were collected from 2021-2023 on the Delphi user demo, and the values, rights, and duties were generated using the OpenAI API from May 2023-July 2023. How was the data associated with each instance ac- quired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey re- sponses), or indirectly inferred/derived from other data (e.g., part of speech tags; model-based guesses for age or language)? If the latter two, were they validated/verified and if so how? The data is only associated in that the situations came from the demo and the remaining data from the OpenAI API. Does the dataset contain all possible instances? Or is it, for instance, a sample (not necessarily random) from a larger set of instances? If the dataset is a sample, then what is the population? What was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Is the sample representative of the larger set (e.g., geographic coverage)? If not, why not (e.g., to cover a more diverse range of instances)? How does this affect possible uses? We source our 31k situations about which to reason from a set of 1.3M user-submitted situations, and curate the dataset by filtering out situations that are not are not actions or un- related to morality (as labeled in a few-shot manner25 by Flan-T5 (Chung et al. 2022)). We also filter out any ques- tions using keyword matching. We note that an outsize proportion of the dataset involves toxic, NSFW, or sexually explicit content. In the interest of having a diversity of situations, we label for these attributes1 25Few-shot filtering prompts are found in Appendix M.1. using Flan-T5 (Chung et al. 2022). We take 95% of our situ- ations determinnistically from those that have less toxic/NS- FW/explicit content, and sample the other 5% uniformly from the rest of the data so as to include the entire spec- trum of inputs. We find that this succeeds in increasing the diversity of the dataset, as measured by unique n-grams di- vided by the length of the dataset (dist-2: .23\u2192.36, dist-3: .54\u2192.67). Is there information missing from the dataset and why? (this does not include intentionally dropped instances; it might include, e.g., redacted text, withheld documents) Is this data missing because it was unavailable? No, there is no known data missing from the dataset, al- though we do not claim or believe that the dataset is neces- sarily a comprehensive set of representative human values. Are there any known errors, sources of noise, or redun- dancies in the data? No known errors, sources of noise, or redundancies, al- though we hope future work will help to shed more light on weaknesses. N.4 Data Preprocessing What preprocessing/cleaning was done? (e.g., discretiza- tion or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, process- ing of missing values, etc.) The main preprocessing was extraction of the features from raw text output from GPT-4 to the semi-structured dataset that we have. We used regex expressions for this ex- traction. Was the \u201craw\u201d data saved in addition to the prepro- cessed/cleaned data? (e.g., to support unanticipated fu- ture uses) Yes, the raw GPT-4 outputs were saved in addition to the cleaned data. Is the preprocessing software available? Yes, all preprocessing software will be available at https: //github.com/tsor13/kaleido. Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet? It achieves the goal of 1) trying to understand what plu- ralistic human values, rights, and duties are currently em- bedded in GPT-4 (although not other LLMs). It achieves the goal of taking a first step to modeling human values, rights, and duties computationally, as manifested by KALEIDO, but we do not claim that it necessarily does so with accuracy and complete representativeness. N.5 Dataset Distribution How is the dataset distributed? (e.g., website, API, etc.; does the data have a DOI; is it archived redundantly?) We plan on distributing the dataset via Huggingface Datasets, but it will be gated for individual-approval and in- tended for research-use only in an attempt to prevent misuse. When will the dataset be released/first distributed? (Is there a canonical paper/reference for this dataset?) We plan on distributng the dataset in September 2023, with this manuscript as a reference. What license (if any) is it distributed under? Are there any copyrights on the data? We plan on distributing VALUEPRISM under the ImpACT license (Allen Insitute for AI 2023) as a \u201cmedium-risk arti- fact\u201d. Users must agree to all terms and restrictions of the license before accessing or using the dataset. Are there any fees or access/export restrictions? No, the dataset is distributed at no cost. However, we do gate access by individual request and access is predicated on acceptance of the license. N.6 Dataset Maintenance Who is supporting/hosting/maintaining the dataset? How does one contact the owner/curator/manager of the dataset (e.g. email address, or other contact info)? The Allen Institute for AI supports the dataset and it will be hosted on Huggingface. Corresponding authors are Taylor Sorensen (tsor13@cs.washington.edu) and Yejin Choi (yejin@cs.washington.edu). Will the dataset be updated? How often and by whom? How will updates/revisions be documented and commu- nicated (e.g., mailing list, GitHub)? Is there an erra- tum? We do not plan on updating the dataset. If the dataset becomes obsolete how will this be commu- nicated? Is there a repository to link to any/all paper- s/systems that use this dataset? We do not expect the dataset to become obselete as it does not depend on external sources. Users of VALUEPRISM should cite this manuscript. If others want to extend/augment/build on this dataset,"}], "doc_text": "through demo users and the OpenAI API. However, to understand the dataset\u2019s quality and represen- tativeness, we do carry out several human studies on subsets of the data (see Section 4.1 and 4.2). We ensured that, for all tasks, crowdworkers were paid a minimum hourly wage of $15-25 USD. Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame? The situations were collected from 2021-2023 on the Delphi user demo, and the values, rights, and duties were generated using the OpenAI API from May 2023-July 2023. How was the data associated with each instance ac- quired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey re- sponses), or indirectly inferred/derived from other data (e.g., part of speech tags; model-based guesses for age or language)? If the latter two, were they validated/verified and if so how? The data is only associated in that the situations came from the demo and the remaining data from the OpenAI API. Does the dataset contain all possible instances? Or is it, for instance, a sample (not necessarily random) from a larger set of instances? If the dataset is a sample, then what is the population? What was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Is the sample representative of the larger set (e.g., geographic coverage)? If not, why not (e.g., to cover a more diverse range of instances)? How does this affect possible uses? We source our 31k situations about which to reason from a set of 1.3M user-submitted situations, and curate the dataset by filtering out situations that are not are not actions or un- related to morality (as labeled in a few-shot manner25 by Flan-T5 (Chung et al. 2022)). We also filter out any ques- tions using keyword matching. We note that an outsize proportion of the dataset involves toxic, NSFW, or sexually explicit content. In the interest of having a diversity of situations, we label for these attributes1 25Few-shot filtering prompts are found in Appendix M.1. using Flan-T5 (Chung et al. 2022). We take 95% of our situ- ations determinnistically from those that have less toxic/NS- FW/explicit content, and sample the other 5% uniformly from the rest of the data so as to include the entire spec- trum of inputs. We find that this succeeds in increasing the diversity of the dataset, as measured by unique n-grams di- vided by the length of the dataset (dist-2: .23\u2192.36, dist-3: .54\u2192.67). Is there information missing from the dataset and why? (this does not include intentionally dropped instances; it might include, e.g., redacted text, withheld documents) Is this data missing because it was unavailable? No, there is no known data missing from the dataset, al- though we do not claim or believe that the dataset is neces- sarily a comprehensive set of representative human values. Are there any known errors, sources of noise, or redun- dancies in the data? No known errors, sources of noise, or redundancies, al- though we hope future work will help to shed more light on weaknesses. N.4 Data Preprocessing What preprocessing/cleaning was done? (e.g., discretiza- tion or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, process- ing of missing values, etc.) The main preprocessing was extraction of the features from raw text output from GPT-4 to the semi-structured dataset that we have. We used regex expressions for this ex- traction. Was the \u201craw\u201d data saved in addition to the prepro- cessed/cleaned data? (e.g., to support unanticipated fu- ture uses) Yes, the raw GPT-4 outputs were saved in addition to the cleaned data. Is the preprocessing software available? Yes, all preprocessing software will be available at https: //github.com/tsor13/kaleido. Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet? It achieves the goal of 1) trying to understand what plu- ralistic human values, rights, and duties are currently em- bedded in GPT-4 (although not other LLMs). It achieves the goal of taking a first step to modeling human values, rights, and duties computationally, as manifested by KALEIDO, but we do not claim that it necessarily does so with accuracy and complete representativeness. N.5 Dataset Distribution How is the dataset distributed? (e.g., website, API, etc.; does the data have a DOI; is it archived redundantly?) We plan on distributing the dataset via Huggingface Datasets, but it will be gated for individual-approval and in- tended for research-use only in an attempt to prevent misuse. When will the dataset be released/first distributed? (Is there a canonical paper/reference for this dataset?) We plan on distributng the dataset in September 2023, with this manuscript as a reference. What license (if any) is it distributed under? Are there any copyrights on the data? We plan on distributing VALUEPRISM under the ImpACT license (Allen Insitute for AI 2023) as a \u201cmedium-risk arti- fact\u201d. Users must agree to all terms and restrictions of the license before accessing or using the dataset. Are there any fees or access/export restrictions? No, the dataset is distributed at no cost. However, we do gate access by individual request and access is predicated on acceptance of the license. N.6 Dataset Maintenance Who is supporting/hosting/maintaining the dataset? How does one contact the owner/curator/manager of the dataset (e.g. email address, or other contact info)? The Allen Institute for AI supports the dataset and it will be hosted on Huggingface. Corresponding authors are Taylor Sorensen (tsor13@cs.washington.edu) and Yejin Choi (yejin@cs.washington.edu). Will the dataset be updated? How often and by whom? How will updates/revisions be documented and commu- nicated (e.g., mailing list, GitHub)? Is there an erra- tum? We do not plan on updating the dataset. If the dataset becomes obsolete how will this be commu- nicated? Is there a repository to link to any/all paper- s/systems that use this dataset? We do not expect the dataset to become obselete as it does not depend on external sources. Users of VALUEPRISM should cite this manuscript. If others want to extend/augment/build on this dataset,"}