{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Does_compressing_activations_help_model_parallel_training?_chunk_7.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does Figure (a) in the text show?,        answer: Figure (a) shows the real and predicted computation time with the increase of the hidden size.    ", "ref_chunk": "that our prediction model is accurate compared with the real experimental results. The model we use here has only one transformer layer and the tensor model-parallel size is 4. In speci\ufb01c, Figure (a) shows the real and predicted computation time with the increase of the hidden size. Figure (b) presents the real and predicted communication time between tensor parallelism by varying the hidden size. As for the Figure (c), it presents the computation time of AE with the increase of hidden size. In the end, Figure (d) show the total speedup when we use AE to compress activations over tensor parallelism. that these two steps can hardly overlap because , the reason behind it is that the all-reduce communication depends on the previous computational results: T = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bsh) (1) In AE, Toverhead is the encoder and Modeling Toverhead decoder computation time. It is a batched matrix multiplica- tion with input dimension B \u00d7 s \u00d7 h and h \u00d7 e. Assuming e is kept constant, it can be modeled as Toverhead = \u03b3Bsh. The \ufb01tting result is shown in 5(c). Modeling Tcomp We model Tcomp as a linear function of FLOPs with the coef\ufb01cient \u03b1 that corresponds to the peak performance of the GPU. In particular, we estimate \u03b1 using ground truth wall clock time of the largest hidden size we can \ufb01t, where the GPU is more likely to be of the peak utilization (Williams et al., 2009). During experiments, we found that \ufb01tting \u03b1 using time of smaller hidden sizes can result in a 30x higher prediction time when we scale up the hidden size because of low GPU utilization. Our prediction versus the ground truth time is plotted in Figure 5(a). Since each Transformer layer has identical con\ufb01gurations in popular Transformer models (Devlin et al., 2018; Radford et al., 2018), the overall speedup ratio is the same as we vary the number of layers. Thus, we can estimate the speedup of different hidden sizes of any number of Transformer layers using T TAE . We provide the \ufb01tting result in Figure 5(d). Understanding the trend We consider the asymptotic behavior of large hidden size h: T TAE \u2248 \u03b1(96Bsh2 + 16Bs2h) + \u03b2Bsh \u03b1(96Bsh2 + 16Bs2h) + \u03b3Bsh + c Modeling Tcomm we model Tcomm as a piece-wise func- tion of the message size (Agarwal et al., 2022). Formally, Thus, we can see that as hidden layer size increases, the bene\ufb01ts from compression diminish. Tcomm(Bsh) = (cid:40) if Bsh < d c \u03b2Bsh if Bsh \u2265 d If the message size is smaller than a threshold d, then Tcomm(Bsh) is a constant c because the worker needs to launch one communication round (Li et al., 2020). Other- wise, the number of communication rounds is proportional to the message size. The \ufb01tting result is in Figure 5(b). Using AE as the compression method and a \ufb01xed encoder dimension e (we set e to 100 in this section), the total time of a single Transformer layer is: TAE = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bse) + Toverhead Compared with the setting without compression, the compu- tation time remains unchanged. In addition, Tcomm(Bse) is roughly equal to c because Bse is usually smaller than the threshold d. In our experiments, the threshold d = 16 \u00d7 128 \u00d7 100 = 409600 and c \u2248 0.2. Scaling up the cluster size Next we analyze the speedup when scaling up the cluster size by combining the pipeline parallelism cost model developed in (Li et al., 2022; Zheng et al., 2022). Formally, the running time is modeled as a sum of per-micro-batch pipeline communication time, per- micro-batch of non-straggler pipeline execution time, and the per-mini-batch straggler pipeline execution time. To use the cost model, we denote the micro-batch size as m, the number of nodes n, the number of layers L, the pipeline communication time p or pAE. We use the default pipeline layer assignment strategy in (Shoeybi et al., 2019), which balances the number of transformer layers. Thus, every stage takes the same time in our scenario: L n TAE. We use the pipeline communi- cation model in (Jia et al., 2019; Li et al., 2022), p = Bsh w , pAE = Bse w , where w is the bandwidth. Thus the overall speedup can be written as: n T or L ( m\u22121 n + 1) \u00d7 LT + (n \u2212 1) \u00d7 Bsh n + 1) \u00d7 LTAE + (n \u2212 1) \u00d7 Bse ( m\u22121 w w (2) (3) Does compressing activations help model parallel training? From the Table 10, we see that we can maintain a \u223c1.5x speedup as we scale the hidden size to 25600. This shows that if we increase the number of nodes when we increase in hidden size, AE compression retains its bene\ufb01ts. However, it is possible to avoid the diminishing speedup by properly scaling up the number of nodes n, where the speedup will asymptotically converge to h e . In summary, compression in model parallelism has dimin- ishing returns if we only scale up the model on a \ufb01xed cluster. To gain bene\ufb01ts from compression methods, one needs to also properly manage other parameters in the cost model, e.g. also scaling up the number of nodes and use the pipeline parallelism. 5 RELATED WORK In this section, we \ufb01rst introduce work related to the de- velopment of large Transformer models. Then, we discuss strategies to train these models at scale. In the end, we discuss prior work that accelerates distributed ML models training by using compression techniques. Transformer Models. Transformer models were \ufb01rst intro- duced by Vaswani et al. (2017) in the machine translation context. It has been shown to be effective in various other language understanding tasks such as text generation, text classi\ufb01cation and question answering (Devlin et al., 2018; Radford et al., 2018; Wang et al., 2018a; Rajpurkar et al., 2016). Recent research has also successfully applied Trans- former models to images (Dosovitskiy et al., 2020;"}, {"question": " What is shown in Figure (b) in the text?,        answer: Figure (b) presents the real and predicted communication time between tensor parallelism by varying the hidden size.    ", "ref_chunk": "that our prediction model is accurate compared with the real experimental results. The model we use here has only one transformer layer and the tensor model-parallel size is 4. In speci\ufb01c, Figure (a) shows the real and predicted computation time with the increase of the hidden size. Figure (b) presents the real and predicted communication time between tensor parallelism by varying the hidden size. As for the Figure (c), it presents the computation time of AE with the increase of hidden size. In the end, Figure (d) show the total speedup when we use AE to compress activations over tensor parallelism. that these two steps can hardly overlap because , the reason behind it is that the all-reduce communication depends on the previous computational results: T = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bsh) (1) In AE, Toverhead is the encoder and Modeling Toverhead decoder computation time. It is a batched matrix multiplica- tion with input dimension B \u00d7 s \u00d7 h and h \u00d7 e. Assuming e is kept constant, it can be modeled as Toverhead = \u03b3Bsh. The \ufb01tting result is shown in 5(c). Modeling Tcomp We model Tcomp as a linear function of FLOPs with the coef\ufb01cient \u03b1 that corresponds to the peak performance of the GPU. In particular, we estimate \u03b1 using ground truth wall clock time of the largest hidden size we can \ufb01t, where the GPU is more likely to be of the peak utilization (Williams et al., 2009). During experiments, we found that \ufb01tting \u03b1 using time of smaller hidden sizes can result in a 30x higher prediction time when we scale up the hidden size because of low GPU utilization. Our prediction versus the ground truth time is plotted in Figure 5(a). Since each Transformer layer has identical con\ufb01gurations in popular Transformer models (Devlin et al., 2018; Radford et al., 2018), the overall speedup ratio is the same as we vary the number of layers. Thus, we can estimate the speedup of different hidden sizes of any number of Transformer layers using T TAE . We provide the \ufb01tting result in Figure 5(d). Understanding the trend We consider the asymptotic behavior of large hidden size h: T TAE \u2248 \u03b1(96Bsh2 + 16Bs2h) + \u03b2Bsh \u03b1(96Bsh2 + 16Bs2h) + \u03b3Bsh + c Modeling Tcomm we model Tcomm as a piece-wise func- tion of the message size (Agarwal et al., 2022). Formally, Thus, we can see that as hidden layer size increases, the bene\ufb01ts from compression diminish. Tcomm(Bsh) = (cid:40) if Bsh < d c \u03b2Bsh if Bsh \u2265 d If the message size is smaller than a threshold d, then Tcomm(Bsh) is a constant c because the worker needs to launch one communication round (Li et al., 2020). Other- wise, the number of communication rounds is proportional to the message size. The \ufb01tting result is in Figure 5(b). Using AE as the compression method and a \ufb01xed encoder dimension e (we set e to 100 in this section), the total time of a single Transformer layer is: TAE = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bse) + Toverhead Compared with the setting without compression, the compu- tation time remains unchanged. In addition, Tcomm(Bse) is roughly equal to c because Bse is usually smaller than the threshold d. In our experiments, the threshold d = 16 \u00d7 128 \u00d7 100 = 409600 and c \u2248 0.2. Scaling up the cluster size Next we analyze the speedup when scaling up the cluster size by combining the pipeline parallelism cost model developed in (Li et al., 2022; Zheng et al., 2022). Formally, the running time is modeled as a sum of per-micro-batch pipeline communication time, per- micro-batch of non-straggler pipeline execution time, and the per-mini-batch straggler pipeline execution time. To use the cost model, we denote the micro-batch size as m, the number of nodes n, the number of layers L, the pipeline communication time p or pAE. We use the default pipeline layer assignment strategy in (Shoeybi et al., 2019), which balances the number of transformer layers. Thus, every stage takes the same time in our scenario: L n TAE. We use the pipeline communi- cation model in (Jia et al., 2019; Li et al., 2022), p = Bsh w , pAE = Bse w , where w is the bandwidth. Thus the overall speedup can be written as: n T or L ( m\u22121 n + 1) \u00d7 LT + (n \u2212 1) \u00d7 Bsh n + 1) \u00d7 LTAE + (n \u2212 1) \u00d7 Bse ( m\u22121 w w (2) (3) Does compressing activations help model parallel training? From the Table 10, we see that we can maintain a \u223c1.5x speedup as we scale the hidden size to 25600. This shows that if we increase the number of nodes when we increase in hidden size, AE compression retains its bene\ufb01ts. However, it is possible to avoid the diminishing speedup by properly scaling up the number of nodes n, where the speedup will asymptotically converge to h e . In summary, compression in model parallelism has dimin- ishing returns if we only scale up the model on a \ufb01xed cluster. To gain bene\ufb01ts from compression methods, one needs to also properly manage other parameters in the cost model, e.g. also scaling up the number of nodes and use the pipeline parallelism. 5 RELATED WORK In this section, we \ufb01rst introduce work related to the de- velopment of large Transformer models. Then, we discuss strategies to train these models at scale. In the end, we discuss prior work that accelerates distributed ML models training by using compression techniques. Transformer Models. Transformer models were \ufb01rst intro- duced by Vaswani et al. (2017) in the machine translation context. It has been shown to be effective in various other language understanding tasks such as text generation, text classi\ufb01cation and question answering (Devlin et al., 2018; Radford et al., 2018; Wang et al., 2018a; Rajpurkar et al., 2016). Recent research has also successfully applied Trans- former models to images (Dosovitskiy et al., 2020;"}, {"question": " What information is presented in Figure (c) in the text?,        answer: Figure (c) presents the computation time of AE with the increase of hidden size.    ", "ref_chunk": "that our prediction model is accurate compared with the real experimental results. The model we use here has only one transformer layer and the tensor model-parallel size is 4. In speci\ufb01c, Figure (a) shows the real and predicted computation time with the increase of the hidden size. Figure (b) presents the real and predicted communication time between tensor parallelism by varying the hidden size. As for the Figure (c), it presents the computation time of AE with the increase of hidden size. In the end, Figure (d) show the total speedup when we use AE to compress activations over tensor parallelism. that these two steps can hardly overlap because , the reason behind it is that the all-reduce communication depends on the previous computational results: T = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bsh) (1) In AE, Toverhead is the encoder and Modeling Toverhead decoder computation time. It is a batched matrix multiplica- tion with input dimension B \u00d7 s \u00d7 h and h \u00d7 e. Assuming e is kept constant, it can be modeled as Toverhead = \u03b3Bsh. The \ufb01tting result is shown in 5(c). Modeling Tcomp We model Tcomp as a linear function of FLOPs with the coef\ufb01cient \u03b1 that corresponds to the peak performance of the GPU. In particular, we estimate \u03b1 using ground truth wall clock time of the largest hidden size we can \ufb01t, where the GPU is more likely to be of the peak utilization (Williams et al., 2009). During experiments, we found that \ufb01tting \u03b1 using time of smaller hidden sizes can result in a 30x higher prediction time when we scale up the hidden size because of low GPU utilization. Our prediction versus the ground truth time is plotted in Figure 5(a). Since each Transformer layer has identical con\ufb01gurations in popular Transformer models (Devlin et al., 2018; Radford et al., 2018), the overall speedup ratio is the same as we vary the number of layers. Thus, we can estimate the speedup of different hidden sizes of any number of Transformer layers using T TAE . We provide the \ufb01tting result in Figure 5(d). Understanding the trend We consider the asymptotic behavior of large hidden size h: T TAE \u2248 \u03b1(96Bsh2 + 16Bs2h) + \u03b2Bsh \u03b1(96Bsh2 + 16Bs2h) + \u03b3Bsh + c Modeling Tcomm we model Tcomm as a piece-wise func- tion of the message size (Agarwal et al., 2022). Formally, Thus, we can see that as hidden layer size increases, the bene\ufb01ts from compression diminish. Tcomm(Bsh) = (cid:40) if Bsh < d c \u03b2Bsh if Bsh \u2265 d If the message size is smaller than a threshold d, then Tcomm(Bsh) is a constant c because the worker needs to launch one communication round (Li et al., 2020). Other- wise, the number of communication rounds is proportional to the message size. The \ufb01tting result is in Figure 5(b). Using AE as the compression method and a \ufb01xed encoder dimension e (we set e to 100 in this section), the total time of a single Transformer layer is: TAE = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bse) + Toverhead Compared with the setting without compression, the compu- tation time remains unchanged. In addition, Tcomm(Bse) is roughly equal to c because Bse is usually smaller than the threshold d. In our experiments, the threshold d = 16 \u00d7 128 \u00d7 100 = 409600 and c \u2248 0.2. Scaling up the cluster size Next we analyze the speedup when scaling up the cluster size by combining the pipeline parallelism cost model developed in (Li et al., 2022; Zheng et al., 2022). Formally, the running time is modeled as a sum of per-micro-batch pipeline communication time, per- micro-batch of non-straggler pipeline execution time, and the per-mini-batch straggler pipeline execution time. To use the cost model, we denote the micro-batch size as m, the number of nodes n, the number of layers L, the pipeline communication time p or pAE. We use the default pipeline layer assignment strategy in (Shoeybi et al., 2019), which balances the number of transformer layers. Thus, every stage takes the same time in our scenario: L n TAE. We use the pipeline communi- cation model in (Jia et al., 2019; Li et al., 2022), p = Bsh w , pAE = Bse w , where w is the bandwidth. Thus the overall speedup can be written as: n T or L ( m\u22121 n + 1) \u00d7 LT + (n \u2212 1) \u00d7 Bsh n + 1) \u00d7 LTAE + (n \u2212 1) \u00d7 Bse ( m\u22121 w w (2) (3) Does compressing activations help model parallel training? From the Table 10, we see that we can maintain a \u223c1.5x speedup as we scale the hidden size to 25600. This shows that if we increase the number of nodes when we increase in hidden size, AE compression retains its bene\ufb01ts. However, it is possible to avoid the diminishing speedup by properly scaling up the number of nodes n, where the speedup will asymptotically converge to h e . In summary, compression in model parallelism has dimin- ishing returns if we only scale up the model on a \ufb01xed cluster. To gain bene\ufb01ts from compression methods, one needs to also properly manage other parameters in the cost model, e.g. also scaling up the number of nodes and use the pipeline parallelism. 5 RELATED WORK In this section, we \ufb01rst introduce work related to the de- velopment of large Transformer models. Then, we discuss strategies to train these models at scale. In the end, we discuss prior work that accelerates distributed ML models training by using compression techniques. Transformer Models. Transformer models were \ufb01rst intro- duced by Vaswani et al. (2017) in the machine translation context. It has been shown to be effective in various other language understanding tasks such as text generation, text classi\ufb01cation and question answering (Devlin et al., 2018; Radford et al., 2018; Wang et al., 2018a; Rajpurkar et al., 2016). Recent research has also successfully applied Trans- former models to images (Dosovitskiy et al., 2020;"}, {"question": " What does Figure (d) show when using AE to compress activations over tensor parallelism?,        answer: Figure (d) shows the total speedup.    ", "ref_chunk": "that our prediction model is accurate compared with the real experimental results. The model we use here has only one transformer layer and the tensor model-parallel size is 4. In speci\ufb01c, Figure (a) shows the real and predicted computation time with the increase of the hidden size. Figure (b) presents the real and predicted communication time between tensor parallelism by varying the hidden size. As for the Figure (c), it presents the computation time of AE with the increase of hidden size. In the end, Figure (d) show the total speedup when we use AE to compress activations over tensor parallelism. that these two steps can hardly overlap because , the reason behind it is that the all-reduce communication depends on the previous computational results: T = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bsh) (1) In AE, Toverhead is the encoder and Modeling Toverhead decoder computation time. It is a batched matrix multiplica- tion with input dimension B \u00d7 s \u00d7 h and h \u00d7 e. Assuming e is kept constant, it can be modeled as Toverhead = \u03b3Bsh. The \ufb01tting result is shown in 5(c). Modeling Tcomp We model Tcomp as a linear function of FLOPs with the coef\ufb01cient \u03b1 that corresponds to the peak performance of the GPU. In particular, we estimate \u03b1 using ground truth wall clock time of the largest hidden size we can \ufb01t, where the GPU is more likely to be of the peak utilization (Williams et al., 2009). During experiments, we found that \ufb01tting \u03b1 using time of smaller hidden sizes can result in a 30x higher prediction time when we scale up the hidden size because of low GPU utilization. Our prediction versus the ground truth time is plotted in Figure 5(a). Since each Transformer layer has identical con\ufb01gurations in popular Transformer models (Devlin et al., 2018; Radford et al., 2018), the overall speedup ratio is the same as we vary the number of layers. Thus, we can estimate the speedup of different hidden sizes of any number of Transformer layers using T TAE . We provide the \ufb01tting result in Figure 5(d). Understanding the trend We consider the asymptotic behavior of large hidden size h: T TAE \u2248 \u03b1(96Bsh2 + 16Bs2h) + \u03b2Bsh \u03b1(96Bsh2 + 16Bs2h) + \u03b3Bsh + c Modeling Tcomm we model Tcomm as a piece-wise func- tion of the message size (Agarwal et al., 2022). Formally, Thus, we can see that as hidden layer size increases, the bene\ufb01ts from compression diminish. Tcomm(Bsh) = (cid:40) if Bsh < d c \u03b2Bsh if Bsh \u2265 d If the message size is smaller than a threshold d, then Tcomm(Bsh) is a constant c because the worker needs to launch one communication round (Li et al., 2020). Other- wise, the number of communication rounds is proportional to the message size. The \ufb01tting result is in Figure 5(b). Using AE as the compression method and a \ufb01xed encoder dimension e (we set e to 100 in this section), the total time of a single Transformer layer is: TAE = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bse) + Toverhead Compared with the setting without compression, the compu- tation time remains unchanged. In addition, Tcomm(Bse) is roughly equal to c because Bse is usually smaller than the threshold d. In our experiments, the threshold d = 16 \u00d7 128 \u00d7 100 = 409600 and c \u2248 0.2. Scaling up the cluster size Next we analyze the speedup when scaling up the cluster size by combining the pipeline parallelism cost model developed in (Li et al., 2022; Zheng et al., 2022). Formally, the running time is modeled as a sum of per-micro-batch pipeline communication time, per- micro-batch of non-straggler pipeline execution time, and the per-mini-batch straggler pipeline execution time. To use the cost model, we denote the micro-batch size as m, the number of nodes n, the number of layers L, the pipeline communication time p or pAE. We use the default pipeline layer assignment strategy in (Shoeybi et al., 2019), which balances the number of transformer layers. Thus, every stage takes the same time in our scenario: L n TAE. We use the pipeline communi- cation model in (Jia et al., 2019; Li et al., 2022), p = Bsh w , pAE = Bse w , where w is the bandwidth. Thus the overall speedup can be written as: n T or L ( m\u22121 n + 1) \u00d7 LT + (n \u2212 1) \u00d7 Bsh n + 1) \u00d7 LTAE + (n \u2212 1) \u00d7 Bse ( m\u22121 w w (2) (3) Does compressing activations help model parallel training? From the Table 10, we see that we can maintain a \u223c1.5x speedup as we scale the hidden size to 25600. This shows that if we increase the number of nodes when we increase in hidden size, AE compression retains its bene\ufb01ts. However, it is possible to avoid the diminishing speedup by properly scaling up the number of nodes n, where the speedup will asymptotically converge to h e . In summary, compression in model parallelism has dimin- ishing returns if we only scale up the model on a \ufb01xed cluster. To gain bene\ufb01ts from compression methods, one needs to also properly manage other parameters in the cost model, e.g. also scaling up the number of nodes and use the pipeline parallelism. 5 RELATED WORK In this section, we \ufb01rst introduce work related to the de- velopment of large Transformer models. Then, we discuss strategies to train these models at scale. In the end, we discuss prior work that accelerates distributed ML models training by using compression techniques. Transformer Models. Transformer models were \ufb01rst intro- duced by Vaswani et al. (2017) in the machine translation context. It has been shown to be effective in various other language understanding tasks such as text generation, text classi\ufb01cation and question answering (Devlin et al., 2018; Radford et al., 2018; Wang et al., 2018a; Rajpurkar et al., 2016). Recent research has also successfully applied Trans- former models to images (Dosovitskiy et al., 2020;"}, {"question": " How is the message size related to the communication time in the text?,        answer: As the hidden layer size increases, the benefits from compression diminish.    ", "ref_chunk": "that our prediction model is accurate compared with the real experimental results. The model we use here has only one transformer layer and the tensor model-parallel size is 4. In speci\ufb01c, Figure (a) shows the real and predicted computation time with the increase of the hidden size. Figure (b) presents the real and predicted communication time between tensor parallelism by varying the hidden size. As for the Figure (c), it presents the computation time of AE with the increase of hidden size. In the end, Figure (d) show the total speedup when we use AE to compress activations over tensor parallelism. that these two steps can hardly overlap because , the reason behind it is that the all-reduce communication depends on the previous computational results: T = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bsh) (1) In AE, Toverhead is the encoder and Modeling Toverhead decoder computation time. It is a batched matrix multiplica- tion with input dimension B \u00d7 s \u00d7 h and h \u00d7 e. Assuming e is kept constant, it can be modeled as Toverhead = \u03b3Bsh. The \ufb01tting result is shown in 5(c). Modeling Tcomp We model Tcomp as a linear function of FLOPs with the coef\ufb01cient \u03b1 that corresponds to the peak performance of the GPU. In particular, we estimate \u03b1 using ground truth wall clock time of the largest hidden size we can \ufb01t, where the GPU is more likely to be of the peak utilization (Williams et al., 2009). During experiments, we found that \ufb01tting \u03b1 using time of smaller hidden sizes can result in a 30x higher prediction time when we scale up the hidden size because of low GPU utilization. Our prediction versus the ground truth time is plotted in Figure 5(a). Since each Transformer layer has identical con\ufb01gurations in popular Transformer models (Devlin et al., 2018; Radford et al., 2018), the overall speedup ratio is the same as we vary the number of layers. Thus, we can estimate the speedup of different hidden sizes of any number of Transformer layers using T TAE . We provide the \ufb01tting result in Figure 5(d). Understanding the trend We consider the asymptotic behavior of large hidden size h: T TAE \u2248 \u03b1(96Bsh2 + 16Bs2h) + \u03b2Bsh \u03b1(96Bsh2 + 16Bs2h) + \u03b3Bsh + c Modeling Tcomm we model Tcomm as a piece-wise func- tion of the message size (Agarwal et al., 2022). Formally, Thus, we can see that as hidden layer size increases, the bene\ufb01ts from compression diminish. Tcomm(Bsh) = (cid:40) if Bsh < d c \u03b2Bsh if Bsh \u2265 d If the message size is smaller than a threshold d, then Tcomm(Bsh) is a constant c because the worker needs to launch one communication round (Li et al., 2020). Other- wise, the number of communication rounds is proportional to the message size. The \ufb01tting result is in Figure 5(b). Using AE as the compression method and a \ufb01xed encoder dimension e (we set e to 100 in this section), the total time of a single Transformer layer is: TAE = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bse) + Toverhead Compared with the setting without compression, the compu- tation time remains unchanged. In addition, Tcomm(Bse) is roughly equal to c because Bse is usually smaller than the threshold d. In our experiments, the threshold d = 16 \u00d7 128 \u00d7 100 = 409600 and c \u2248 0.2. Scaling up the cluster size Next we analyze the speedup when scaling up the cluster size by combining the pipeline parallelism cost model developed in (Li et al., 2022; Zheng et al., 2022). Formally, the running time is modeled as a sum of per-micro-batch pipeline communication time, per- micro-batch of non-straggler pipeline execution time, and the per-mini-batch straggler pipeline execution time. To use the cost model, we denote the micro-batch size as m, the number of nodes n, the number of layers L, the pipeline communication time p or pAE. We use the default pipeline layer assignment strategy in (Shoeybi et al., 2019), which balances the number of transformer layers. Thus, every stage takes the same time in our scenario: L n TAE. We use the pipeline communi- cation model in (Jia et al., 2019; Li et al., 2022), p = Bsh w , pAE = Bse w , where w is the bandwidth. Thus the overall speedup can be written as: n T or L ( m\u22121 n + 1) \u00d7 LT + (n \u2212 1) \u00d7 Bsh n + 1) \u00d7 LTAE + (n \u2212 1) \u00d7 Bse ( m\u22121 w w (2) (3) Does compressing activations help model parallel training? From the Table 10, we see that we can maintain a \u223c1.5x speedup as we scale the hidden size to 25600. This shows that if we increase the number of nodes when we increase in hidden size, AE compression retains its bene\ufb01ts. However, it is possible to avoid the diminishing speedup by properly scaling up the number of nodes n, where the speedup will asymptotically converge to h e . In summary, compression in model parallelism has dimin- ishing returns if we only scale up the model on a \ufb01xed cluster. To gain bene\ufb01ts from compression methods, one needs to also properly manage other parameters in the cost model, e.g. also scaling up the number of nodes and use the pipeline parallelism. 5 RELATED WORK In this section, we \ufb01rst introduce work related to the de- velopment of large Transformer models. Then, we discuss strategies to train these models at scale. In the end, we discuss prior work that accelerates distributed ML models training by using compression techniques. Transformer Models. Transformer models were \ufb01rst intro- duced by Vaswani et al. (2017) in the machine translation context. It has been shown to be effective in various other language understanding tasks such as text generation, text classi\ufb01cation and question answering (Devlin et al., 2018; Radford et al., 2018; Wang et al., 2018a; Rajpurkar et al., 2016). Recent research has also successfully applied Trans- former models to images (Dosovitskiy et al., 2020;"}, {"question": " What is the formula for the total time of a single Transformer layer when using AE as the compression method?,        answer: TAE = Tcomp(96Bsh^2 + 16Bs^2h) + Tcomm(Bse) + Toverhead.    ", "ref_chunk": "that our prediction model is accurate compared with the real experimental results. The model we use here has only one transformer layer and the tensor model-parallel size is 4. In speci\ufb01c, Figure (a) shows the real and predicted computation time with the increase of the hidden size. Figure (b) presents the real and predicted communication time between tensor parallelism by varying the hidden size. As for the Figure (c), it presents the computation time of AE with the increase of hidden size. In the end, Figure (d) show the total speedup when we use AE to compress activations over tensor parallelism. that these two steps can hardly overlap because , the reason behind it is that the all-reduce communication depends on the previous computational results: T = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bsh) (1) In AE, Toverhead is the encoder and Modeling Toverhead decoder computation time. It is a batched matrix multiplica- tion with input dimension B \u00d7 s \u00d7 h and h \u00d7 e. Assuming e is kept constant, it can be modeled as Toverhead = \u03b3Bsh. The \ufb01tting result is shown in 5(c). Modeling Tcomp We model Tcomp as a linear function of FLOPs with the coef\ufb01cient \u03b1 that corresponds to the peak performance of the GPU. In particular, we estimate \u03b1 using ground truth wall clock time of the largest hidden size we can \ufb01t, where the GPU is more likely to be of the peak utilization (Williams et al., 2009). During experiments, we found that \ufb01tting \u03b1 using time of smaller hidden sizes can result in a 30x higher prediction time when we scale up the hidden size because of low GPU utilization. Our prediction versus the ground truth time is plotted in Figure 5(a). Since each Transformer layer has identical con\ufb01gurations in popular Transformer models (Devlin et al., 2018; Radford et al., 2018), the overall speedup ratio is the same as we vary the number of layers. Thus, we can estimate the speedup of different hidden sizes of any number of Transformer layers using T TAE . We provide the \ufb01tting result in Figure 5(d). Understanding the trend We consider the asymptotic behavior of large hidden size h: T TAE \u2248 \u03b1(96Bsh2 + 16Bs2h) + \u03b2Bsh \u03b1(96Bsh2 + 16Bs2h) + \u03b3Bsh + c Modeling Tcomm we model Tcomm as a piece-wise func- tion of the message size (Agarwal et al., 2022). Formally, Thus, we can see that as hidden layer size increases, the bene\ufb01ts from compression diminish. Tcomm(Bsh) = (cid:40) if Bsh < d c \u03b2Bsh if Bsh \u2265 d If the message size is smaller than a threshold d, then Tcomm(Bsh) is a constant c because the worker needs to launch one communication round (Li et al., 2020). Other- wise, the number of communication rounds is proportional to the message size. The \ufb01tting result is in Figure 5(b). Using AE as the compression method and a \ufb01xed encoder dimension e (we set e to 100 in this section), the total time of a single Transformer layer is: TAE = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bse) + Toverhead Compared with the setting without compression, the compu- tation time remains unchanged. In addition, Tcomm(Bse) is roughly equal to c because Bse is usually smaller than the threshold d. In our experiments, the threshold d = 16 \u00d7 128 \u00d7 100 = 409600 and c \u2248 0.2. Scaling up the cluster size Next we analyze the speedup when scaling up the cluster size by combining the pipeline parallelism cost model developed in (Li et al., 2022; Zheng et al., 2022). Formally, the running time is modeled as a sum of per-micro-batch pipeline communication time, per- micro-batch of non-straggler pipeline execution time, and the per-mini-batch straggler pipeline execution time. To use the cost model, we denote the micro-batch size as m, the number of nodes n, the number of layers L, the pipeline communication time p or pAE. We use the default pipeline layer assignment strategy in (Shoeybi et al., 2019), which balances the number of transformer layers. Thus, every stage takes the same time in our scenario: L n TAE. We use the pipeline communi- cation model in (Jia et al., 2019; Li et al., 2022), p = Bsh w , pAE = Bse w , where w is the bandwidth. Thus the overall speedup can be written as: n T or L ( m\u22121 n + 1) \u00d7 LT + (n \u2212 1) \u00d7 Bsh n + 1) \u00d7 LTAE + (n \u2212 1) \u00d7 Bse ( m\u22121 w w (2) (3) Does compressing activations help model parallel training? From the Table 10, we see that we can maintain a \u223c1.5x speedup as we scale the hidden size to 25600. This shows that if we increase the number of nodes when we increase in hidden size, AE compression retains its bene\ufb01ts. However, it is possible to avoid the diminishing speedup by properly scaling up the number of nodes n, where the speedup will asymptotically converge to h e . In summary, compression in model parallelism has dimin- ishing returns if we only scale up the model on a \ufb01xed cluster. To gain bene\ufb01ts from compression methods, one needs to also properly manage other parameters in the cost model, e.g. also scaling up the number of nodes and use the pipeline parallelism. 5 RELATED WORK In this section, we \ufb01rst introduce work related to the de- velopment of large Transformer models. Then, we discuss strategies to train these models at scale. In the end, we discuss prior work that accelerates distributed ML models training by using compression techniques. Transformer Models. Transformer models were \ufb01rst intro- duced by Vaswani et al. (2017) in the machine translation context. It has been shown to be effective in various other language understanding tasks such as text generation, text classi\ufb01cation and question answering (Devlin et al., 2018; Radford et al., 2018; Wang et al., 2018a; Rajpurkar et al., 2016). Recent research has also successfully applied Trans- former models to images (Dosovitskiy et al., 2020;"}, {"question": " How is the overall speedup when scaling up the cluster size calculated?,        answer: The overall speedup is calculated as a sum of per-micro-batch pipeline communication time, per-micro-batch of non-straggler pipeline execution time, and the per-mini-batch straggler pipeline execution time.    ", "ref_chunk": "that our prediction model is accurate compared with the real experimental results. The model we use here has only one transformer layer and the tensor model-parallel size is 4. In speci\ufb01c, Figure (a) shows the real and predicted computation time with the increase of the hidden size. Figure (b) presents the real and predicted communication time between tensor parallelism by varying the hidden size. As for the Figure (c), it presents the computation time of AE with the increase of hidden size. In the end, Figure (d) show the total speedup when we use AE to compress activations over tensor parallelism. that these two steps can hardly overlap because , the reason behind it is that the all-reduce communication depends on the previous computational results: T = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bsh) (1) In AE, Toverhead is the encoder and Modeling Toverhead decoder computation time. It is a batched matrix multiplica- tion with input dimension B \u00d7 s \u00d7 h and h \u00d7 e. Assuming e is kept constant, it can be modeled as Toverhead = \u03b3Bsh. The \ufb01tting result is shown in 5(c). Modeling Tcomp We model Tcomp as a linear function of FLOPs with the coef\ufb01cient \u03b1 that corresponds to the peak performance of the GPU. In particular, we estimate \u03b1 using ground truth wall clock time of the largest hidden size we can \ufb01t, where the GPU is more likely to be of the peak utilization (Williams et al., 2009). During experiments, we found that \ufb01tting \u03b1 using time of smaller hidden sizes can result in a 30x higher prediction time when we scale up the hidden size because of low GPU utilization. Our prediction versus the ground truth time is plotted in Figure 5(a). Since each Transformer layer has identical con\ufb01gurations in popular Transformer models (Devlin et al., 2018; Radford et al., 2018), the overall speedup ratio is the same as we vary the number of layers. Thus, we can estimate the speedup of different hidden sizes of any number of Transformer layers using T TAE . We provide the \ufb01tting result in Figure 5(d). Understanding the trend We consider the asymptotic behavior of large hidden size h: T TAE \u2248 \u03b1(96Bsh2 + 16Bs2h) + \u03b2Bsh \u03b1(96Bsh2 + 16Bs2h) + \u03b3Bsh + c Modeling Tcomm we model Tcomm as a piece-wise func- tion of the message size (Agarwal et al., 2022). Formally, Thus, we can see that as hidden layer size increases, the bene\ufb01ts from compression diminish. Tcomm(Bsh) = (cid:40) if Bsh < d c \u03b2Bsh if Bsh \u2265 d If the message size is smaller than a threshold d, then Tcomm(Bsh) is a constant c because the worker needs to launch one communication round (Li et al., 2020). Other- wise, the number of communication rounds is proportional to the message size. The \ufb01tting result is in Figure 5(b). Using AE as the compression method and a \ufb01xed encoder dimension e (we set e to 100 in this section), the total time of a single Transformer layer is: TAE = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bse) + Toverhead Compared with the setting without compression, the compu- tation time remains unchanged. In addition, Tcomm(Bse) is roughly equal to c because Bse is usually smaller than the threshold d. In our experiments, the threshold d = 16 \u00d7 128 \u00d7 100 = 409600 and c \u2248 0.2. Scaling up the cluster size Next we analyze the speedup when scaling up the cluster size by combining the pipeline parallelism cost model developed in (Li et al., 2022; Zheng et al., 2022). Formally, the running time is modeled as a sum of per-micro-batch pipeline communication time, per- micro-batch of non-straggler pipeline execution time, and the per-mini-batch straggler pipeline execution time. To use the cost model, we denote the micro-batch size as m, the number of nodes n, the number of layers L, the pipeline communication time p or pAE. We use the default pipeline layer assignment strategy in (Shoeybi et al., 2019), which balances the number of transformer layers. Thus, every stage takes the same time in our scenario: L n TAE. We use the pipeline communi- cation model in (Jia et al., 2019; Li et al., 2022), p = Bsh w , pAE = Bse w , where w is the bandwidth. Thus the overall speedup can be written as: n T or L ( m\u22121 n + 1) \u00d7 LT + (n \u2212 1) \u00d7 Bsh n + 1) \u00d7 LTAE + (n \u2212 1) \u00d7 Bse ( m\u22121 w w (2) (3) Does compressing activations help model parallel training? From the Table 10, we see that we can maintain a \u223c1.5x speedup as we scale the hidden size to 25600. This shows that if we increase the number of nodes when we increase in hidden size, AE compression retains its bene\ufb01ts. However, it is possible to avoid the diminishing speedup by properly scaling up the number of nodes n, where the speedup will asymptotically converge to h e . In summary, compression in model parallelism has dimin- ishing returns if we only scale up the model on a \ufb01xed cluster. To gain bene\ufb01ts from compression methods, one needs to also properly manage other parameters in the cost model, e.g. also scaling up the number of nodes and use the pipeline parallelism. 5 RELATED WORK In this section, we \ufb01rst introduce work related to the de- velopment of large Transformer models. Then, we discuss strategies to train these models at scale. In the end, we discuss prior work that accelerates distributed ML models training by using compression techniques. Transformer Models. Transformer models were \ufb01rst intro- duced by Vaswani et al. (2017) in the machine translation context. It has been shown to be effective in various other language understanding tasks such as text generation, text classi\ufb01cation and question answering (Devlin et al., 2018; Radford et al., 2018; Wang et al., 2018a; Rajpurkar et al., 2016). Recent research has also successfully applied Trans- former models to images (Dosovitskiy et al., 2020;"}, {"question": " According to the text, does compressing activations help model parallel training?,        answer: Yes, compressing activations can help maintain a \u223c1.5x speedup when scaling the hidden size to 25600.    ", "ref_chunk": "that our prediction model is accurate compared with the real experimental results. The model we use here has only one transformer layer and the tensor model-parallel size is 4. In speci\ufb01c, Figure (a) shows the real and predicted computation time with the increase of the hidden size. Figure (b) presents the real and predicted communication time between tensor parallelism by varying the hidden size. As for the Figure (c), it presents the computation time of AE with the increase of hidden size. In the end, Figure (d) show the total speedup when we use AE to compress activations over tensor parallelism. that these two steps can hardly overlap because , the reason behind it is that the all-reduce communication depends on the previous computational results: T = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bsh) (1) In AE, Toverhead is the encoder and Modeling Toverhead decoder computation time. It is a batched matrix multiplica- tion with input dimension B \u00d7 s \u00d7 h and h \u00d7 e. Assuming e is kept constant, it can be modeled as Toverhead = \u03b3Bsh. The \ufb01tting result is shown in 5(c). Modeling Tcomp We model Tcomp as a linear function of FLOPs with the coef\ufb01cient \u03b1 that corresponds to the peak performance of the GPU. In particular, we estimate \u03b1 using ground truth wall clock time of the largest hidden size we can \ufb01t, where the GPU is more likely to be of the peak utilization (Williams et al., 2009). During experiments, we found that \ufb01tting \u03b1 using time of smaller hidden sizes can result in a 30x higher prediction time when we scale up the hidden size because of low GPU utilization. Our prediction versus the ground truth time is plotted in Figure 5(a). Since each Transformer layer has identical con\ufb01gurations in popular Transformer models (Devlin et al., 2018; Radford et al., 2018), the overall speedup ratio is the same as we vary the number of layers. Thus, we can estimate the speedup of different hidden sizes of any number of Transformer layers using T TAE . We provide the \ufb01tting result in Figure 5(d). Understanding the trend We consider the asymptotic behavior of large hidden size h: T TAE \u2248 \u03b1(96Bsh2 + 16Bs2h) + \u03b2Bsh \u03b1(96Bsh2 + 16Bs2h) + \u03b3Bsh + c Modeling Tcomm we model Tcomm as a piece-wise func- tion of the message size (Agarwal et al., 2022). Formally, Thus, we can see that as hidden layer size increases, the bene\ufb01ts from compression diminish. Tcomm(Bsh) = (cid:40) if Bsh < d c \u03b2Bsh if Bsh \u2265 d If the message size is smaller than a threshold d, then Tcomm(Bsh) is a constant c because the worker needs to launch one communication round (Li et al., 2020). Other- wise, the number of communication rounds is proportional to the message size. The \ufb01tting result is in Figure 5(b). Using AE as the compression method and a \ufb01xed encoder dimension e (we set e to 100 in this section), the total time of a single Transformer layer is: TAE = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bse) + Toverhead Compared with the setting without compression, the compu- tation time remains unchanged. In addition, Tcomm(Bse) is roughly equal to c because Bse is usually smaller than the threshold d. In our experiments, the threshold d = 16 \u00d7 128 \u00d7 100 = 409600 and c \u2248 0.2. Scaling up the cluster size Next we analyze the speedup when scaling up the cluster size by combining the pipeline parallelism cost model developed in (Li et al., 2022; Zheng et al., 2022). Formally, the running time is modeled as a sum of per-micro-batch pipeline communication time, per- micro-batch of non-straggler pipeline execution time, and the per-mini-batch straggler pipeline execution time. To use the cost model, we denote the micro-batch size as m, the number of nodes n, the number of layers L, the pipeline communication time p or pAE. We use the default pipeline layer assignment strategy in (Shoeybi et al., 2019), which balances the number of transformer layers. Thus, every stage takes the same time in our scenario: L n TAE. We use the pipeline communi- cation model in (Jia et al., 2019; Li et al., 2022), p = Bsh w , pAE = Bse w , where w is the bandwidth. Thus the overall speedup can be written as: n T or L ( m\u22121 n + 1) \u00d7 LT + (n \u2212 1) \u00d7 Bsh n + 1) \u00d7 LTAE + (n \u2212 1) \u00d7 Bse ( m\u22121 w w (2) (3) Does compressing activations help model parallel training? From the Table 10, we see that we can maintain a \u223c1.5x speedup as we scale the hidden size to 25600. This shows that if we increase the number of nodes when we increase in hidden size, AE compression retains its bene\ufb01ts. However, it is possible to avoid the diminishing speedup by properly scaling up the number of nodes n, where the speedup will asymptotically converge to h e . In summary, compression in model parallelism has dimin- ishing returns if we only scale up the model on a \ufb01xed cluster. To gain bene\ufb01ts from compression methods, one needs to also properly manage other parameters in the cost model, e.g. also scaling up the number of nodes and use the pipeline parallelism. 5 RELATED WORK In this section, we \ufb01rst introduce work related to the de- velopment of large Transformer models. Then, we discuss strategies to train these models at scale. In the end, we discuss prior work that accelerates distributed ML models training by using compression techniques. Transformer Models. Transformer models were \ufb01rst intro- duced by Vaswani et al. (2017) in the machine translation context. It has been shown to be effective in various other language understanding tasks such as text generation, text classi\ufb01cation and question answering (Devlin et al., 2018; Radford et al., 2018; Wang et al., 2018a; Rajpurkar et al., 2016). Recent research has also successfully applied Trans- former models to images (Dosovitskiy et al., 2020;"}, {"question": " What is the primary purpose of using the pipeline parallelism cost model mentioned in the text?,        answer: The cost model is used to analyze the speedup when scaling up the cluster size.    ", "ref_chunk": "that our prediction model is accurate compared with the real experimental results. The model we use here has only one transformer layer and the tensor model-parallel size is 4. In speci\ufb01c, Figure (a) shows the real and predicted computation time with the increase of the hidden size. Figure (b) presents the real and predicted communication time between tensor parallelism by varying the hidden size. As for the Figure (c), it presents the computation time of AE with the increase of hidden size. In the end, Figure (d) show the total speedup when we use AE to compress activations over tensor parallelism. that these two steps can hardly overlap because , the reason behind it is that the all-reduce communication depends on the previous computational results: T = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bsh) (1) In AE, Toverhead is the encoder and Modeling Toverhead decoder computation time. It is a batched matrix multiplica- tion with input dimension B \u00d7 s \u00d7 h and h \u00d7 e. Assuming e is kept constant, it can be modeled as Toverhead = \u03b3Bsh. The \ufb01tting result is shown in 5(c). Modeling Tcomp We model Tcomp as a linear function of FLOPs with the coef\ufb01cient \u03b1 that corresponds to the peak performance of the GPU. In particular, we estimate \u03b1 using ground truth wall clock time of the largest hidden size we can \ufb01t, where the GPU is more likely to be of the peak utilization (Williams et al., 2009). During experiments, we found that \ufb01tting \u03b1 using time of smaller hidden sizes can result in a 30x higher prediction time when we scale up the hidden size because of low GPU utilization. Our prediction versus the ground truth time is plotted in Figure 5(a). Since each Transformer layer has identical con\ufb01gurations in popular Transformer models (Devlin et al., 2018; Radford et al., 2018), the overall speedup ratio is the same as we vary the number of layers. Thus, we can estimate the speedup of different hidden sizes of any number of Transformer layers using T TAE . We provide the \ufb01tting result in Figure 5(d). Understanding the trend We consider the asymptotic behavior of large hidden size h: T TAE \u2248 \u03b1(96Bsh2 + 16Bs2h) + \u03b2Bsh \u03b1(96Bsh2 + 16Bs2h) + \u03b3Bsh + c Modeling Tcomm we model Tcomm as a piece-wise func- tion of the message size (Agarwal et al., 2022). Formally, Thus, we can see that as hidden layer size increases, the bene\ufb01ts from compression diminish. Tcomm(Bsh) = (cid:40) if Bsh < d c \u03b2Bsh if Bsh \u2265 d If the message size is smaller than a threshold d, then Tcomm(Bsh) is a constant c because the worker needs to launch one communication round (Li et al., 2020). Other- wise, the number of communication rounds is proportional to the message size. The \ufb01tting result is in Figure 5(b). Using AE as the compression method and a \ufb01xed encoder dimension e (we set e to 100 in this section), the total time of a single Transformer layer is: TAE = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bse) + Toverhead Compared with the setting without compression, the compu- tation time remains unchanged. In addition, Tcomm(Bse) is roughly equal to c because Bse is usually smaller than the threshold d. In our experiments, the threshold d = 16 \u00d7 128 \u00d7 100 = 409600 and c \u2248 0.2. Scaling up the cluster size Next we analyze the speedup when scaling up the cluster size by combining the pipeline parallelism cost model developed in (Li et al., 2022; Zheng et al., 2022). Formally, the running time is modeled as a sum of per-micro-batch pipeline communication time, per- micro-batch of non-straggler pipeline execution time, and the per-mini-batch straggler pipeline execution time. To use the cost model, we denote the micro-batch size as m, the number of nodes n, the number of layers L, the pipeline communication time p or pAE. We use the default pipeline layer assignment strategy in (Shoeybi et al., 2019), which balances the number of transformer layers. Thus, every stage takes the same time in our scenario: L n TAE. We use the pipeline communi- cation model in (Jia et al., 2019; Li et al., 2022), p = Bsh w , pAE = Bse w , where w is the bandwidth. Thus the overall speedup can be written as: n T or L ( m\u22121 n + 1) \u00d7 LT + (n \u2212 1) \u00d7 Bsh n + 1) \u00d7 LTAE + (n \u2212 1) \u00d7 Bse ( m\u22121 w w (2) (3) Does compressing activations help model parallel training? From the Table 10, we see that we can maintain a \u223c1.5x speedup as we scale the hidden size to 25600. This shows that if we increase the number of nodes when we increase in hidden size, AE compression retains its bene\ufb01ts. However, it is possible to avoid the diminishing speedup by properly scaling up the number of nodes n, where the speedup will asymptotically converge to h e . In summary, compression in model parallelism has dimin- ishing returns if we only scale up the model on a \ufb01xed cluster. To gain bene\ufb01ts from compression methods, one needs to also properly manage other parameters in the cost model, e.g. also scaling up the number of nodes and use the pipeline parallelism. 5 RELATED WORK In this section, we \ufb01rst introduce work related to the de- velopment of large Transformer models. Then, we discuss strategies to train these models at scale. In the end, we discuss prior work that accelerates distributed ML models training by using compression techniques. Transformer Models. Transformer models were \ufb01rst intro- duced by Vaswani et al. (2017) in the machine translation context. It has been shown to be effective in various other language understanding tasks such as text generation, text classi\ufb01cation and question answering (Devlin et al., 2018; Radford et al., 2018; Wang et al., 2018a; Rajpurkar et al., 2016). Recent research has also successfully applied Trans- former models to images (Dosovitskiy et al., 2020;"}, {"question": " What are some strategies suggested in the text to gain benefits from compression methods in model parallelism?,        answer: Properly scaling up the number of nodes and using pipeline parallelism are suggested to gain benefits from compression methods in model parallelism.    ", "ref_chunk": "that our prediction model is accurate compared with the real experimental results. The model we use here has only one transformer layer and the tensor model-parallel size is 4. In speci\ufb01c, Figure (a) shows the real and predicted computation time with the increase of the hidden size. Figure (b) presents the real and predicted communication time between tensor parallelism by varying the hidden size. As for the Figure (c), it presents the computation time of AE with the increase of hidden size. In the end, Figure (d) show the total speedup when we use AE to compress activations over tensor parallelism. that these two steps can hardly overlap because , the reason behind it is that the all-reduce communication depends on the previous computational results: T = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bsh) (1) In AE, Toverhead is the encoder and Modeling Toverhead decoder computation time. It is a batched matrix multiplica- tion with input dimension B \u00d7 s \u00d7 h and h \u00d7 e. Assuming e is kept constant, it can be modeled as Toverhead = \u03b3Bsh. The \ufb01tting result is shown in 5(c). Modeling Tcomp We model Tcomp as a linear function of FLOPs with the coef\ufb01cient \u03b1 that corresponds to the peak performance of the GPU. In particular, we estimate \u03b1 using ground truth wall clock time of the largest hidden size we can \ufb01t, where the GPU is more likely to be of the peak utilization (Williams et al., 2009). During experiments, we found that \ufb01tting \u03b1 using time of smaller hidden sizes can result in a 30x higher prediction time when we scale up the hidden size because of low GPU utilization. Our prediction versus the ground truth time is plotted in Figure 5(a). Since each Transformer layer has identical con\ufb01gurations in popular Transformer models (Devlin et al., 2018; Radford et al., 2018), the overall speedup ratio is the same as we vary the number of layers. Thus, we can estimate the speedup of different hidden sizes of any number of Transformer layers using T TAE . We provide the \ufb01tting result in Figure 5(d). Understanding the trend We consider the asymptotic behavior of large hidden size h: T TAE \u2248 \u03b1(96Bsh2 + 16Bs2h) + \u03b2Bsh \u03b1(96Bsh2 + 16Bs2h) + \u03b3Bsh + c Modeling Tcomm we model Tcomm as a piece-wise func- tion of the message size (Agarwal et al., 2022). Formally, Thus, we can see that as hidden layer size increases, the bene\ufb01ts from compression diminish. Tcomm(Bsh) = (cid:40) if Bsh < d c \u03b2Bsh if Bsh \u2265 d If the message size is smaller than a threshold d, then Tcomm(Bsh) is a constant c because the worker needs to launch one communication round (Li et al., 2020). Other- wise, the number of communication rounds is proportional to the message size. The \ufb01tting result is in Figure 5(b). Using AE as the compression method and a \ufb01xed encoder dimension e (we set e to 100 in this section), the total time of a single Transformer layer is: TAE = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bse) + Toverhead Compared with the setting without compression, the compu- tation time remains unchanged. In addition, Tcomm(Bse) is roughly equal to c because Bse is usually smaller than the threshold d. In our experiments, the threshold d = 16 \u00d7 128 \u00d7 100 = 409600 and c \u2248 0.2. Scaling up the cluster size Next we analyze the speedup when scaling up the cluster size by combining the pipeline parallelism cost model developed in (Li et al., 2022; Zheng et al., 2022). Formally, the running time is modeled as a sum of per-micro-batch pipeline communication time, per- micro-batch of non-straggler pipeline execution time, and the per-mini-batch straggler pipeline execution time. To use the cost model, we denote the micro-batch size as m, the number of nodes n, the number of layers L, the pipeline communication time p or pAE. We use the default pipeline layer assignment strategy in (Shoeybi et al., 2019), which balances the number of transformer layers. Thus, every stage takes the same time in our scenario: L n TAE. We use the pipeline communi- cation model in (Jia et al., 2019; Li et al., 2022), p = Bsh w , pAE = Bse w , where w is the bandwidth. Thus the overall speedup can be written as: n T or L ( m\u22121 n + 1) \u00d7 LT + (n \u2212 1) \u00d7 Bsh n + 1) \u00d7 LTAE + (n \u2212 1) \u00d7 Bse ( m\u22121 w w (2) (3) Does compressing activations help model parallel training? From the Table 10, we see that we can maintain a \u223c1.5x speedup as we scale the hidden size to 25600. This shows that if we increase the number of nodes when we increase in hidden size, AE compression retains its bene\ufb01ts. However, it is possible to avoid the diminishing speedup by properly scaling up the number of nodes n, where the speedup will asymptotically converge to h e . In summary, compression in model parallelism has dimin- ishing returns if we only scale up the model on a \ufb01xed cluster. To gain bene\ufb01ts from compression methods, one needs to also properly manage other parameters in the cost model, e.g. also scaling up the number of nodes and use the pipeline parallelism. 5 RELATED WORK In this section, we \ufb01rst introduce work related to the de- velopment of large Transformer models. Then, we discuss strategies to train these models at scale. In the end, we discuss prior work that accelerates distributed ML models training by using compression techniques. Transformer Models. Transformer models were \ufb01rst intro- duced by Vaswani et al. (2017) in the machine translation context. It has been shown to be effective in various other language understanding tasks such as text generation, text classi\ufb01cation and question answering (Devlin et al., 2018; Radford et al., 2018; Wang et al., 2018a; Rajpurkar et al., 2016). Recent research has also successfully applied Trans- former models to images (Dosovitskiy et al., 2020;"}], "doc_text": "that our prediction model is accurate compared with the real experimental results. The model we use here has only one transformer layer and the tensor model-parallel size is 4. In speci\ufb01c, Figure (a) shows the real and predicted computation time with the increase of the hidden size. Figure (b) presents the real and predicted communication time between tensor parallelism by varying the hidden size. As for the Figure (c), it presents the computation time of AE with the increase of hidden size. In the end, Figure (d) show the total speedup when we use AE to compress activations over tensor parallelism. that these two steps can hardly overlap because , the reason behind it is that the all-reduce communication depends on the previous computational results: T = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bsh) (1) In AE, Toverhead is the encoder and Modeling Toverhead decoder computation time. It is a batched matrix multiplica- tion with input dimension B \u00d7 s \u00d7 h and h \u00d7 e. Assuming e is kept constant, it can be modeled as Toverhead = \u03b3Bsh. The \ufb01tting result is shown in 5(c). Modeling Tcomp We model Tcomp as a linear function of FLOPs with the coef\ufb01cient \u03b1 that corresponds to the peak performance of the GPU. In particular, we estimate \u03b1 using ground truth wall clock time of the largest hidden size we can \ufb01t, where the GPU is more likely to be of the peak utilization (Williams et al., 2009). During experiments, we found that \ufb01tting \u03b1 using time of smaller hidden sizes can result in a 30x higher prediction time when we scale up the hidden size because of low GPU utilization. Our prediction versus the ground truth time is plotted in Figure 5(a). Since each Transformer layer has identical con\ufb01gurations in popular Transformer models (Devlin et al., 2018; Radford et al., 2018), the overall speedup ratio is the same as we vary the number of layers. Thus, we can estimate the speedup of different hidden sizes of any number of Transformer layers using T TAE . We provide the \ufb01tting result in Figure 5(d). Understanding the trend We consider the asymptotic behavior of large hidden size h: T TAE \u2248 \u03b1(96Bsh2 + 16Bs2h) + \u03b2Bsh \u03b1(96Bsh2 + 16Bs2h) + \u03b3Bsh + c Modeling Tcomm we model Tcomm as a piece-wise func- tion of the message size (Agarwal et al., 2022). Formally, Thus, we can see that as hidden layer size increases, the bene\ufb01ts from compression diminish. Tcomm(Bsh) = (cid:40) if Bsh < d c \u03b2Bsh if Bsh \u2265 d If the message size is smaller than a threshold d, then Tcomm(Bsh) is a constant c because the worker needs to launch one communication round (Li et al., 2020). Other- wise, the number of communication rounds is proportional to the message size. The \ufb01tting result is in Figure 5(b). Using AE as the compression method and a \ufb01xed encoder dimension e (we set e to 100 in this section), the total time of a single Transformer layer is: TAE = Tcomp(96Bsh2 + 16Bs2h) + Tcomm(Bse) + Toverhead Compared with the setting without compression, the compu- tation time remains unchanged. In addition, Tcomm(Bse) is roughly equal to c because Bse is usually smaller than the threshold d. In our experiments, the threshold d = 16 \u00d7 128 \u00d7 100 = 409600 and c \u2248 0.2. Scaling up the cluster size Next we analyze the speedup when scaling up the cluster size by combining the pipeline parallelism cost model developed in (Li et al., 2022; Zheng et al., 2022). Formally, the running time is modeled as a sum of per-micro-batch pipeline communication time, per- micro-batch of non-straggler pipeline execution time, and the per-mini-batch straggler pipeline execution time. To use the cost model, we denote the micro-batch size as m, the number of nodes n, the number of layers L, the pipeline communication time p or pAE. We use the default pipeline layer assignment strategy in (Shoeybi et al., 2019), which balances the number of transformer layers. Thus, every stage takes the same time in our scenario: L n TAE. We use the pipeline communi- cation model in (Jia et al., 2019; Li et al., 2022), p = Bsh w , pAE = Bse w , where w is the bandwidth. Thus the overall speedup can be written as: n T or L ( m\u22121 n + 1) \u00d7 LT + (n \u2212 1) \u00d7 Bsh n + 1) \u00d7 LTAE + (n \u2212 1) \u00d7 Bse ( m\u22121 w w (2) (3) Does compressing activations help model parallel training? From the Table 10, we see that we can maintain a \u223c1.5x speedup as we scale the hidden size to 25600. This shows that if we increase the number of nodes when we increase in hidden size, AE compression retains its bene\ufb01ts. However, it is possible to avoid the diminishing speedup by properly scaling up the number of nodes n, where the speedup will asymptotically converge to h e . In summary, compression in model parallelism has dimin- ishing returns if we only scale up the model on a \ufb01xed cluster. To gain bene\ufb01ts from compression methods, one needs to also properly manage other parameters in the cost model, e.g. also scaling up the number of nodes and use the pipeline parallelism. 5 RELATED WORK In this section, we \ufb01rst introduce work related to the de- velopment of large Transformer models. Then, we discuss strategies to train these models at scale. In the end, we discuss prior work that accelerates distributed ML models training by using compression techniques. Transformer Models. Transformer models were \ufb01rst intro- duced by Vaswani et al. (2017) in the machine translation context. It has been shown to be effective in various other language understanding tasks such as text generation, text classi\ufb01cation and question answering (Devlin et al., 2018; Radford et al., 2018; Wang et al., 2018a; Rajpurkar et al., 2016). Recent research has also successfully applied Trans- former models to images (Dosovitskiy et al., 2020;"}