{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Federated_Learning_as_Variational_Inference:_A_Scalable_Expectation_Propagation_Approach_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the probabilistic view that motivates an alternative formulation of federated learning?", "answer": " The global posterior p(\u03b8 | D) given a collection of datasets factorizes as the prior times the local likelihood associated with each data partition.", "ref_chunk": "up expectation propaga- tion to contemporary benchmarks (e.g., models with many millions of parameters and datasets with hundreds of thousands of clients). When applied on top of modern FL benchmarks, our approach outperforms strong FedAvg and FedPA baselines. 2 FEDERATED LEARNING WITH EXPECTATION PROPAGATION The probabilistic view from Eq. 1 motivates an alternative formulation of federated learning based on variational inference. First observe that the global posterior p (\u03b8 | D) given a collection of datasets D = (cid:83) k\u2208[K] Dk factorizes as, K \u220f k=1 K \u220f k=0 p (\u03b8 | D) \u221d p(\u03b8) p(Dk | \u03b8) = pk(\u03b8), where for convenience we de\ufb01ne p0(\u03b8) := p(\u03b8) to be the prior and further use pk(\u03b8) := p(Dk | \u03b8) to refer to the local likelihood associated with k-th data partition. To simplify notation we hereon refer to the global posterior as pglobal(\u03b8) and drop the conditioning on D. Now consider an approximating global posterior qglobal(\u03b8) that admits the same factorization as the above, i.e., qglobal(\u03b8) \u221d \u220fK k=0 qk(\u03b8). Plugging in these terms into Eq. 1 gives the following objective, (cid:19) (cid:18) K \u220f k=0 K \u220f k=0 K \u220f k=0 qk(\u03b8), where {qk(\u03b8)}K k=0 = arg min pk(\u03b8) (cid:107) qk(\u03b8) arg max \u03b8 D qk\u2208Q Here Q is the variational family, which is assumed to be the same for all clients. This global objec- tive is in general intractable; evaluating \u220fk pk(\u03b8) requires accessing all clients\u2019 data and violates the standard FL assumption. This section presents a probabilistic message-passing algorithm based on expectation propagation (EP, Minka, 2001). 2.1 EXPECTATION PROPAGATION EP is an iterative algorithm in which an intractable target density pglobal(\u03b8) is approximated by a tractable density qglobal(\u03b8) using a collection of localized inference procedures. In EP, each local inference problem is a function of just pk and the current global estimate, making it appropriate for the FL setting. 2 . (1) (2) Published as a conference paper at ICLR 2023 Algorithm 1 Federated Learning as Inference 1: for round t = 1, . . . , T do 2: 3: 4: 5: 6: 7: 8: 9: end for 10: Return \u00b5global. Sample a subset of clients K. Broadcast qglobal(\u03b8) to the selected clients. for each client k \u2208 K in parallel do \u2206qk(\u03b8) \u2190 ClientInfer(qglobal(\u03b8)) end for Collect \u2206qk(\u03b8) from the selected clients. qglobal(\u03b8) \u2190 ServerInfer({\u2206qk(\u03b8)}k) Algorithm 2 Approximate Inference: MCMC 1: Input: q\\k(\u03b8; Dk, \u03b7\u2212k, \u039b\u2212k) 2: Sk \u2190 {} 3: for i = 1, . . . , N do 4: k \u2190 SGDEpoch(\u2212 log q\\k, \u03b8(i\u22121) \u03b8(i) Sk \u2190 Sk \u222a \u03b8(i) k 5: 6: end for 7: \u03b7\\k, \u039b 8: Output: (cid:98)q\\k(\u03b8; \u03b7\\k, \u039b k \\k \u2190 EstimateMoments(Sk) \\k) ) Algorithm 3 Gaussian EP: Server Inference 1: Receive: {\u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b \u221d qglobal \u220fk (\u2206qk)\u03b4 2: qnew k)}k global \u03b7global \u2190 \u03b7global + \u03b4 ServerOptim(\u2211 k \u039bglobal \u2190 \u039bglobal + \u03b4 ServerOptim(\u2211 // Sec. 2.2.3 \u2206\u03b7k) \u2206\u039b k) k 3: Send: qglobal(\u03b8; \u03b7global, \u039bglobal) Algorithm 4 Gaussian EP: Client Inference 1: Receive: qglobal(\u03b8; \u03b7global, \u039bglobal) 2: q\u2212k \u221d qglobal/qk \u03b7\u2212k \u2190 \u03b7global \u2212 \u03b7k, \u221d pk q\u2212k // cavity distribution \u039b\u2212k \u2190 \u039bglobal \u2212 \u039b k // tilted inference (Sec. 2.2.2) 3: (cid:98)q\\k \u2248 q\\k \u03b7\\k, \u039b \u221d \u221d pk q\u2212k) \\k \u2190 ApproxInference(q\\k (cid:98)q\\k/qglobal \u2206\u03b7k \u2190 \u03b7\\k \u2212 \u03b7global, \u221d qk (\u2206qk)\u03b4 4: \u2206qk // client deltas (Sec. A.1) \\k \u2212 \u039bglobal k \u2190 \u039b \u2206\u039b 5: qnew k \u03b7k \u2190 \u03b7k + \u03b4 ClientOptim(\u2206\u03b7k) k + \u03b4 ClientOptim(\u2206\u039b \u039b k) // local update (Sec. 2.2.3) k \u2190 \u039b 6: Send: \u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b k) Concretely, EP iteratively solves the following problem (either in sequence or parallel), qnew k (\u03b8) = arg min q\u2208Q D (cid:18) pk(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:124) \u221d q\\k(\u03b8) (cid:107) q(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:98)q\\k(\u03b8) (cid:124) \u221d (cid:19) , where q\u2212k(\u03b8) \u221d qglobal(\u03b8) qk(\u03b8) . (3) Here qglobal(\u03b8) and qk(\u03b8) are the global/local distributions from the current iteration. (See Sec. A.2 for further details). In the EP literature, q\u2212k(\u03b8) is referred to as the cavity distribution and q\\k(\u03b8) and (cid:98)q\\k(\u03b8) are referred to as the target/approximate tilted distributions. EP then uses qnew (\u03b8) to derive qnew global(\u03b8). While the theoretical properties of EP are still not well understood (Minka, 2001; Dehaene & Barthelm\u00b4e, 2015; 2018), it has empirically been shown to produce good posterior ap- proximations in many cases (Li et al., 2015; Vehtari et al., 2020). When applied to FL, the central server initiates the update by sending the parameters of the current global approximation qglobal(\u03b8) as messages to the subset of clients K. Upon receiving these messages, each client updates the re- spective local approximation qnew (\u03b8) and sends back the changes in parameters as messages, which is then aggregated by the server. Algorithms 1-4 illustrate the probabilistic message passing with the Gaussian variational family in more detail. k k Remark. Consider the case where we set q\u2212k(\u03b8) \u221d 1 (i.e., an improper uniform distribu- tion that ignores the current estimate of the global parameters). Then Eq. 3 reduces to fed- erated learning with posterior averaging (FedPA) from Al-Shedivat et al. (2021), qnew (\u03b8) = arg minq\u2208Q D (pk(\u03b8) (cid:107) q(\u03b8)). Hence, FedEP improves upon FedPA by taking into account the global parameters and the previous local estimate while deriving the local posterior.2 k 2.2 SCALABLE EXPECTATION PROPAGATION While federated learning with expectation propagation is conceptually straightforward, scaling up FedEP to modern models and datasets is challenging. For one, the high dimensionality of the param- eter space of contemporary models can make local inference dif\ufb01cult even with simple mean-\ufb01eld Gaussian variational families. This is compounded by the fact that classic expectation propaga- tion is stateful and therefore requires that each client always maintains its local contribution to the global posterior. These factors make classic EP potentially an unideal approach in settings where 2When the parameters of qglobal(\u03b8) and qk(\u03b8)\u2019s are initialized as improper uniform distributions, the \ufb01rst round (but only the \ufb01rst round) of FedEP and FedPA is identical. 3 Published as a conference paper"}, {"question": " What is the variational family assumed to be the same for all clients in federated learning?", "answer": " Q", "ref_chunk": "up expectation propaga- tion to contemporary benchmarks (e.g., models with many millions of parameters and datasets with hundreds of thousands of clients). When applied on top of modern FL benchmarks, our approach outperforms strong FedAvg and FedPA baselines. 2 FEDERATED LEARNING WITH EXPECTATION PROPAGATION The probabilistic view from Eq. 1 motivates an alternative formulation of federated learning based on variational inference. First observe that the global posterior p (\u03b8 | D) given a collection of datasets D = (cid:83) k\u2208[K] Dk factorizes as, K \u220f k=1 K \u220f k=0 p (\u03b8 | D) \u221d p(\u03b8) p(Dk | \u03b8) = pk(\u03b8), where for convenience we de\ufb01ne p0(\u03b8) := p(\u03b8) to be the prior and further use pk(\u03b8) := p(Dk | \u03b8) to refer to the local likelihood associated with k-th data partition. To simplify notation we hereon refer to the global posterior as pglobal(\u03b8) and drop the conditioning on D. Now consider an approximating global posterior qglobal(\u03b8) that admits the same factorization as the above, i.e., qglobal(\u03b8) \u221d \u220fK k=0 qk(\u03b8). Plugging in these terms into Eq. 1 gives the following objective, (cid:19) (cid:18) K \u220f k=0 K \u220f k=0 K \u220f k=0 qk(\u03b8), where {qk(\u03b8)}K k=0 = arg min pk(\u03b8) (cid:107) qk(\u03b8) arg max \u03b8 D qk\u2208Q Here Q is the variational family, which is assumed to be the same for all clients. This global objec- tive is in general intractable; evaluating \u220fk pk(\u03b8) requires accessing all clients\u2019 data and violates the standard FL assumption. This section presents a probabilistic message-passing algorithm based on expectation propagation (EP, Minka, 2001). 2.1 EXPECTATION PROPAGATION EP is an iterative algorithm in which an intractable target density pglobal(\u03b8) is approximated by a tractable density qglobal(\u03b8) using a collection of localized inference procedures. In EP, each local inference problem is a function of just pk and the current global estimate, making it appropriate for the FL setting. 2 . (1) (2) Published as a conference paper at ICLR 2023 Algorithm 1 Federated Learning as Inference 1: for round t = 1, . . . , T do 2: 3: 4: 5: 6: 7: 8: 9: end for 10: Return \u00b5global. Sample a subset of clients K. Broadcast qglobal(\u03b8) to the selected clients. for each client k \u2208 K in parallel do \u2206qk(\u03b8) \u2190 ClientInfer(qglobal(\u03b8)) end for Collect \u2206qk(\u03b8) from the selected clients. qglobal(\u03b8) \u2190 ServerInfer({\u2206qk(\u03b8)}k) Algorithm 2 Approximate Inference: MCMC 1: Input: q\\k(\u03b8; Dk, \u03b7\u2212k, \u039b\u2212k) 2: Sk \u2190 {} 3: for i = 1, . . . , N do 4: k \u2190 SGDEpoch(\u2212 log q\\k, \u03b8(i\u22121) \u03b8(i) Sk \u2190 Sk \u222a \u03b8(i) k 5: 6: end for 7: \u03b7\\k, \u039b 8: Output: (cid:98)q\\k(\u03b8; \u03b7\\k, \u039b k \\k \u2190 EstimateMoments(Sk) \\k) ) Algorithm 3 Gaussian EP: Server Inference 1: Receive: {\u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b \u221d qglobal \u220fk (\u2206qk)\u03b4 2: qnew k)}k global \u03b7global \u2190 \u03b7global + \u03b4 ServerOptim(\u2211 k \u039bglobal \u2190 \u039bglobal + \u03b4 ServerOptim(\u2211 // Sec. 2.2.3 \u2206\u03b7k) \u2206\u039b k) k 3: Send: qglobal(\u03b8; \u03b7global, \u039bglobal) Algorithm 4 Gaussian EP: Client Inference 1: Receive: qglobal(\u03b8; \u03b7global, \u039bglobal) 2: q\u2212k \u221d qglobal/qk \u03b7\u2212k \u2190 \u03b7global \u2212 \u03b7k, \u221d pk q\u2212k // cavity distribution \u039b\u2212k \u2190 \u039bglobal \u2212 \u039b k // tilted inference (Sec. 2.2.2) 3: (cid:98)q\\k \u2248 q\\k \u03b7\\k, \u039b \u221d \u221d pk q\u2212k) \\k \u2190 ApproxInference(q\\k (cid:98)q\\k/qglobal \u2206\u03b7k \u2190 \u03b7\\k \u2212 \u03b7global, \u221d qk (\u2206qk)\u03b4 4: \u2206qk // client deltas (Sec. A.1) \\k \u2212 \u039bglobal k \u2190 \u039b \u2206\u039b 5: qnew k \u03b7k \u2190 \u03b7k + \u03b4 ClientOptim(\u2206\u03b7k) k + \u03b4 ClientOptim(\u2206\u039b \u039b k) // local update (Sec. 2.2.3) k \u2190 \u039b 6: Send: \u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b k) Concretely, EP iteratively solves the following problem (either in sequence or parallel), qnew k (\u03b8) = arg min q\u2208Q D (cid:18) pk(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:124) \u221d q\\k(\u03b8) (cid:107) q(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:98)q\\k(\u03b8) (cid:124) \u221d (cid:19) , where q\u2212k(\u03b8) \u221d qglobal(\u03b8) qk(\u03b8) . (3) Here qglobal(\u03b8) and qk(\u03b8) are the global/local distributions from the current iteration. (See Sec. A.2 for further details). In the EP literature, q\u2212k(\u03b8) is referred to as the cavity distribution and q\\k(\u03b8) and (cid:98)q\\k(\u03b8) are referred to as the target/approximate tilted distributions. EP then uses qnew (\u03b8) to derive qnew global(\u03b8). While the theoretical properties of EP are still not well understood (Minka, 2001; Dehaene & Barthelm\u00b4e, 2015; 2018), it has empirically been shown to produce good posterior ap- proximations in many cases (Li et al., 2015; Vehtari et al., 2020). When applied to FL, the central server initiates the update by sending the parameters of the current global approximation qglobal(\u03b8) as messages to the subset of clients K. Upon receiving these messages, each client updates the re- spective local approximation qnew (\u03b8) and sends back the changes in parameters as messages, which is then aggregated by the server. Algorithms 1-4 illustrate the probabilistic message passing with the Gaussian variational family in more detail. k k Remark. Consider the case where we set q\u2212k(\u03b8) \u221d 1 (i.e., an improper uniform distribu- tion that ignores the current estimate of the global parameters). Then Eq. 3 reduces to fed- erated learning with posterior averaging (FedPA) from Al-Shedivat et al. (2021), qnew (\u03b8) = arg minq\u2208Q D (pk(\u03b8) (cid:107) q(\u03b8)). Hence, FedEP improves upon FedPA by taking into account the global parameters and the previous local estimate while deriving the local posterior.2 k 2.2 SCALABLE EXPECTATION PROPAGATION While federated learning with expectation propagation is conceptually straightforward, scaling up FedEP to modern models and datasets is challenging. For one, the high dimensionality of the param- eter space of contemporary models can make local inference dif\ufb01cult even with simple mean-\ufb01eld Gaussian variational families. This is compounded by the fact that classic expectation propaga- tion is stateful and therefore requires that each client always maintains its local contribution to the global posterior. These factors make classic EP potentially an unideal approach in settings where 2When the parameters of qglobal(\u03b8) and qk(\u03b8)\u2019s are initialized as improper uniform distributions, the \ufb01rst round (but only the \ufb01rst round) of FedEP and FedPA is identical. 3 Published as a conference paper"}, {"question": " What is the objective of the global probabilistic message-passing algorithm based on expectation propagation?", "answer": " To approximate the global posterior p(\u03b8 | D) using a tractable density q(\u03b8) that admits the same factorization.", "ref_chunk": "up expectation propaga- tion to contemporary benchmarks (e.g., models with many millions of parameters and datasets with hundreds of thousands of clients). When applied on top of modern FL benchmarks, our approach outperforms strong FedAvg and FedPA baselines. 2 FEDERATED LEARNING WITH EXPECTATION PROPAGATION The probabilistic view from Eq. 1 motivates an alternative formulation of federated learning based on variational inference. First observe that the global posterior p (\u03b8 | D) given a collection of datasets D = (cid:83) k\u2208[K] Dk factorizes as, K \u220f k=1 K \u220f k=0 p (\u03b8 | D) \u221d p(\u03b8) p(Dk | \u03b8) = pk(\u03b8), where for convenience we de\ufb01ne p0(\u03b8) := p(\u03b8) to be the prior and further use pk(\u03b8) := p(Dk | \u03b8) to refer to the local likelihood associated with k-th data partition. To simplify notation we hereon refer to the global posterior as pglobal(\u03b8) and drop the conditioning on D. Now consider an approximating global posterior qglobal(\u03b8) that admits the same factorization as the above, i.e., qglobal(\u03b8) \u221d \u220fK k=0 qk(\u03b8). Plugging in these terms into Eq. 1 gives the following objective, (cid:19) (cid:18) K \u220f k=0 K \u220f k=0 K \u220f k=0 qk(\u03b8), where {qk(\u03b8)}K k=0 = arg min pk(\u03b8) (cid:107) qk(\u03b8) arg max \u03b8 D qk\u2208Q Here Q is the variational family, which is assumed to be the same for all clients. This global objec- tive is in general intractable; evaluating \u220fk pk(\u03b8) requires accessing all clients\u2019 data and violates the standard FL assumption. This section presents a probabilistic message-passing algorithm based on expectation propagation (EP, Minka, 2001). 2.1 EXPECTATION PROPAGATION EP is an iterative algorithm in which an intractable target density pglobal(\u03b8) is approximated by a tractable density qglobal(\u03b8) using a collection of localized inference procedures. In EP, each local inference problem is a function of just pk and the current global estimate, making it appropriate for the FL setting. 2 . (1) (2) Published as a conference paper at ICLR 2023 Algorithm 1 Federated Learning as Inference 1: for round t = 1, . . . , T do 2: 3: 4: 5: 6: 7: 8: 9: end for 10: Return \u00b5global. Sample a subset of clients K. Broadcast qglobal(\u03b8) to the selected clients. for each client k \u2208 K in parallel do \u2206qk(\u03b8) \u2190 ClientInfer(qglobal(\u03b8)) end for Collect \u2206qk(\u03b8) from the selected clients. qglobal(\u03b8) \u2190 ServerInfer({\u2206qk(\u03b8)}k) Algorithm 2 Approximate Inference: MCMC 1: Input: q\\k(\u03b8; Dk, \u03b7\u2212k, \u039b\u2212k) 2: Sk \u2190 {} 3: for i = 1, . . . , N do 4: k \u2190 SGDEpoch(\u2212 log q\\k, \u03b8(i\u22121) \u03b8(i) Sk \u2190 Sk \u222a \u03b8(i) k 5: 6: end for 7: \u03b7\\k, \u039b 8: Output: (cid:98)q\\k(\u03b8; \u03b7\\k, \u039b k \\k \u2190 EstimateMoments(Sk) \\k) ) Algorithm 3 Gaussian EP: Server Inference 1: Receive: {\u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b \u221d qglobal \u220fk (\u2206qk)\u03b4 2: qnew k)}k global \u03b7global \u2190 \u03b7global + \u03b4 ServerOptim(\u2211 k \u039bglobal \u2190 \u039bglobal + \u03b4 ServerOptim(\u2211 // Sec. 2.2.3 \u2206\u03b7k) \u2206\u039b k) k 3: Send: qglobal(\u03b8; \u03b7global, \u039bglobal) Algorithm 4 Gaussian EP: Client Inference 1: Receive: qglobal(\u03b8; \u03b7global, \u039bglobal) 2: q\u2212k \u221d qglobal/qk \u03b7\u2212k \u2190 \u03b7global \u2212 \u03b7k, \u221d pk q\u2212k // cavity distribution \u039b\u2212k \u2190 \u039bglobal \u2212 \u039b k // tilted inference (Sec. 2.2.2) 3: (cid:98)q\\k \u2248 q\\k \u03b7\\k, \u039b \u221d \u221d pk q\u2212k) \\k \u2190 ApproxInference(q\\k (cid:98)q\\k/qglobal \u2206\u03b7k \u2190 \u03b7\\k \u2212 \u03b7global, \u221d qk (\u2206qk)\u03b4 4: \u2206qk // client deltas (Sec. A.1) \\k \u2212 \u039bglobal k \u2190 \u039b \u2206\u039b 5: qnew k \u03b7k \u2190 \u03b7k + \u03b4 ClientOptim(\u2206\u03b7k) k + \u03b4 ClientOptim(\u2206\u039b \u039b k) // local update (Sec. 2.2.3) k \u2190 \u039b 6: Send: \u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b k) Concretely, EP iteratively solves the following problem (either in sequence or parallel), qnew k (\u03b8) = arg min q\u2208Q D (cid:18) pk(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:124) \u221d q\\k(\u03b8) (cid:107) q(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:98)q\\k(\u03b8) (cid:124) \u221d (cid:19) , where q\u2212k(\u03b8) \u221d qglobal(\u03b8) qk(\u03b8) . (3) Here qglobal(\u03b8) and qk(\u03b8) are the global/local distributions from the current iteration. (See Sec. A.2 for further details). In the EP literature, q\u2212k(\u03b8) is referred to as the cavity distribution and q\\k(\u03b8) and (cid:98)q\\k(\u03b8) are referred to as the target/approximate tilted distributions. EP then uses qnew (\u03b8) to derive qnew global(\u03b8). While the theoretical properties of EP are still not well understood (Minka, 2001; Dehaene & Barthelm\u00b4e, 2015; 2018), it has empirically been shown to produce good posterior ap- proximations in many cases (Li et al., 2015; Vehtari et al., 2020). When applied to FL, the central server initiates the update by sending the parameters of the current global approximation qglobal(\u03b8) as messages to the subset of clients K. Upon receiving these messages, each client updates the re- spective local approximation qnew (\u03b8) and sends back the changes in parameters as messages, which is then aggregated by the server. Algorithms 1-4 illustrate the probabilistic message passing with the Gaussian variational family in more detail. k k Remark. Consider the case where we set q\u2212k(\u03b8) \u221d 1 (i.e., an improper uniform distribu- tion that ignores the current estimate of the global parameters). Then Eq. 3 reduces to fed- erated learning with posterior averaging (FedPA) from Al-Shedivat et al. (2021), qnew (\u03b8) = arg minq\u2208Q D (pk(\u03b8) (cid:107) q(\u03b8)). Hence, FedEP improves upon FedPA by taking into account the global parameters and the previous local estimate while deriving the local posterior.2 k 2.2 SCALABLE EXPECTATION PROPAGATION While federated learning with expectation propagation is conceptually straightforward, scaling up FedEP to modern models and datasets is challenging. For one, the high dimensionality of the param- eter space of contemporary models can make local inference dif\ufb01cult even with simple mean-\ufb01eld Gaussian variational families. This is compounded by the fact that classic expectation propaga- tion is stateful and therefore requires that each client always maintains its local contribution to the global posterior. These factors make classic EP potentially an unideal approach in settings where 2When the parameters of qglobal(\u03b8) and qk(\u03b8)\u2019s are initialized as improper uniform distributions, the \ufb01rst round (but only the \ufb01rst round) of FedEP and FedPA is identical. 3 Published as a conference paper"}, {"question": " What is the key feature of the expectation propagation algorithm in the federated learning setting?", "answer": " Each local inference problem is a function of just the local likelihood and the current global estimate.", "ref_chunk": "up expectation propaga- tion to contemporary benchmarks (e.g., models with many millions of parameters and datasets with hundreds of thousands of clients). When applied on top of modern FL benchmarks, our approach outperforms strong FedAvg and FedPA baselines. 2 FEDERATED LEARNING WITH EXPECTATION PROPAGATION The probabilistic view from Eq. 1 motivates an alternative formulation of federated learning based on variational inference. First observe that the global posterior p (\u03b8 | D) given a collection of datasets D = (cid:83) k\u2208[K] Dk factorizes as, K \u220f k=1 K \u220f k=0 p (\u03b8 | D) \u221d p(\u03b8) p(Dk | \u03b8) = pk(\u03b8), where for convenience we de\ufb01ne p0(\u03b8) := p(\u03b8) to be the prior and further use pk(\u03b8) := p(Dk | \u03b8) to refer to the local likelihood associated with k-th data partition. To simplify notation we hereon refer to the global posterior as pglobal(\u03b8) and drop the conditioning on D. Now consider an approximating global posterior qglobal(\u03b8) that admits the same factorization as the above, i.e., qglobal(\u03b8) \u221d \u220fK k=0 qk(\u03b8). Plugging in these terms into Eq. 1 gives the following objective, (cid:19) (cid:18) K \u220f k=0 K \u220f k=0 K \u220f k=0 qk(\u03b8), where {qk(\u03b8)}K k=0 = arg min pk(\u03b8) (cid:107) qk(\u03b8) arg max \u03b8 D qk\u2208Q Here Q is the variational family, which is assumed to be the same for all clients. This global objec- tive is in general intractable; evaluating \u220fk pk(\u03b8) requires accessing all clients\u2019 data and violates the standard FL assumption. This section presents a probabilistic message-passing algorithm based on expectation propagation (EP, Minka, 2001). 2.1 EXPECTATION PROPAGATION EP is an iterative algorithm in which an intractable target density pglobal(\u03b8) is approximated by a tractable density qglobal(\u03b8) using a collection of localized inference procedures. In EP, each local inference problem is a function of just pk and the current global estimate, making it appropriate for the FL setting. 2 . (1) (2) Published as a conference paper at ICLR 2023 Algorithm 1 Federated Learning as Inference 1: for round t = 1, . . . , T do 2: 3: 4: 5: 6: 7: 8: 9: end for 10: Return \u00b5global. Sample a subset of clients K. Broadcast qglobal(\u03b8) to the selected clients. for each client k \u2208 K in parallel do \u2206qk(\u03b8) \u2190 ClientInfer(qglobal(\u03b8)) end for Collect \u2206qk(\u03b8) from the selected clients. qglobal(\u03b8) \u2190 ServerInfer({\u2206qk(\u03b8)}k) Algorithm 2 Approximate Inference: MCMC 1: Input: q\\k(\u03b8; Dk, \u03b7\u2212k, \u039b\u2212k) 2: Sk \u2190 {} 3: for i = 1, . . . , N do 4: k \u2190 SGDEpoch(\u2212 log q\\k, \u03b8(i\u22121) \u03b8(i) Sk \u2190 Sk \u222a \u03b8(i) k 5: 6: end for 7: \u03b7\\k, \u039b 8: Output: (cid:98)q\\k(\u03b8; \u03b7\\k, \u039b k \\k \u2190 EstimateMoments(Sk) \\k) ) Algorithm 3 Gaussian EP: Server Inference 1: Receive: {\u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b \u221d qglobal \u220fk (\u2206qk)\u03b4 2: qnew k)}k global \u03b7global \u2190 \u03b7global + \u03b4 ServerOptim(\u2211 k \u039bglobal \u2190 \u039bglobal + \u03b4 ServerOptim(\u2211 // Sec. 2.2.3 \u2206\u03b7k) \u2206\u039b k) k 3: Send: qglobal(\u03b8; \u03b7global, \u039bglobal) Algorithm 4 Gaussian EP: Client Inference 1: Receive: qglobal(\u03b8; \u03b7global, \u039bglobal) 2: q\u2212k \u221d qglobal/qk \u03b7\u2212k \u2190 \u03b7global \u2212 \u03b7k, \u221d pk q\u2212k // cavity distribution \u039b\u2212k \u2190 \u039bglobal \u2212 \u039b k // tilted inference (Sec. 2.2.2) 3: (cid:98)q\\k \u2248 q\\k \u03b7\\k, \u039b \u221d \u221d pk q\u2212k) \\k \u2190 ApproxInference(q\\k (cid:98)q\\k/qglobal \u2206\u03b7k \u2190 \u03b7\\k \u2212 \u03b7global, \u221d qk (\u2206qk)\u03b4 4: \u2206qk // client deltas (Sec. A.1) \\k \u2212 \u039bglobal k \u2190 \u039b \u2206\u039b 5: qnew k \u03b7k \u2190 \u03b7k + \u03b4 ClientOptim(\u2206\u03b7k) k + \u03b4 ClientOptim(\u2206\u039b \u039b k) // local update (Sec. 2.2.3) k \u2190 \u039b 6: Send: \u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b k) Concretely, EP iteratively solves the following problem (either in sequence or parallel), qnew k (\u03b8) = arg min q\u2208Q D (cid:18) pk(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:124) \u221d q\\k(\u03b8) (cid:107) q(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:98)q\\k(\u03b8) (cid:124) \u221d (cid:19) , where q\u2212k(\u03b8) \u221d qglobal(\u03b8) qk(\u03b8) . (3) Here qglobal(\u03b8) and qk(\u03b8) are the global/local distributions from the current iteration. (See Sec. A.2 for further details). In the EP literature, q\u2212k(\u03b8) is referred to as the cavity distribution and q\\k(\u03b8) and (cid:98)q\\k(\u03b8) are referred to as the target/approximate tilted distributions. EP then uses qnew (\u03b8) to derive qnew global(\u03b8). While the theoretical properties of EP are still not well understood (Minka, 2001; Dehaene & Barthelm\u00b4e, 2015; 2018), it has empirically been shown to produce good posterior ap- proximations in many cases (Li et al., 2015; Vehtari et al., 2020). When applied to FL, the central server initiates the update by sending the parameters of the current global approximation qglobal(\u03b8) as messages to the subset of clients K. Upon receiving these messages, each client updates the re- spective local approximation qnew (\u03b8) and sends back the changes in parameters as messages, which is then aggregated by the server. Algorithms 1-4 illustrate the probabilistic message passing with the Gaussian variational family in more detail. k k Remark. Consider the case where we set q\u2212k(\u03b8) \u221d 1 (i.e., an improper uniform distribu- tion that ignores the current estimate of the global parameters). Then Eq. 3 reduces to fed- erated learning with posterior averaging (FedPA) from Al-Shedivat et al. (2021), qnew (\u03b8) = arg minq\u2208Q D (pk(\u03b8) (cid:107) q(\u03b8)). Hence, FedEP improves upon FedPA by taking into account the global parameters and the previous local estimate while deriving the local posterior.2 k 2.2 SCALABLE EXPECTATION PROPAGATION While federated learning with expectation propagation is conceptually straightforward, scaling up FedEP to modern models and datasets is challenging. For one, the high dimensionality of the param- eter space of contemporary models can make local inference dif\ufb01cult even with simple mean-\ufb01eld Gaussian variational families. This is compounded by the fact that classic expectation propaga- tion is stateful and therefore requires that each client always maintains its local contribution to the global posterior. These factors make classic EP potentially an unideal approach in settings where 2When the parameters of qglobal(\u03b8) and qk(\u03b8)\u2019s are initialized as improper uniform distributions, the \ufb01rst round (but only the \ufb01rst round) of FedEP and FedPA is identical. 3 Published as a conference paper"}, {"question": " How is federated learning implemented as inference in the provided algorithm?", "answer": " By iterating over rounds, sampling a subset of clients, updating client parameters in parallel, and collecting and aggregating the client updates.", "ref_chunk": "up expectation propaga- tion to contemporary benchmarks (e.g., models with many millions of parameters and datasets with hundreds of thousands of clients). When applied on top of modern FL benchmarks, our approach outperforms strong FedAvg and FedPA baselines. 2 FEDERATED LEARNING WITH EXPECTATION PROPAGATION The probabilistic view from Eq. 1 motivates an alternative formulation of federated learning based on variational inference. First observe that the global posterior p (\u03b8 | D) given a collection of datasets D = (cid:83) k\u2208[K] Dk factorizes as, K \u220f k=1 K \u220f k=0 p (\u03b8 | D) \u221d p(\u03b8) p(Dk | \u03b8) = pk(\u03b8), where for convenience we de\ufb01ne p0(\u03b8) := p(\u03b8) to be the prior and further use pk(\u03b8) := p(Dk | \u03b8) to refer to the local likelihood associated with k-th data partition. To simplify notation we hereon refer to the global posterior as pglobal(\u03b8) and drop the conditioning on D. Now consider an approximating global posterior qglobal(\u03b8) that admits the same factorization as the above, i.e., qglobal(\u03b8) \u221d \u220fK k=0 qk(\u03b8). Plugging in these terms into Eq. 1 gives the following objective, (cid:19) (cid:18) K \u220f k=0 K \u220f k=0 K \u220f k=0 qk(\u03b8), where {qk(\u03b8)}K k=0 = arg min pk(\u03b8) (cid:107) qk(\u03b8) arg max \u03b8 D qk\u2208Q Here Q is the variational family, which is assumed to be the same for all clients. This global objec- tive is in general intractable; evaluating \u220fk pk(\u03b8) requires accessing all clients\u2019 data and violates the standard FL assumption. This section presents a probabilistic message-passing algorithm based on expectation propagation (EP, Minka, 2001). 2.1 EXPECTATION PROPAGATION EP is an iterative algorithm in which an intractable target density pglobal(\u03b8) is approximated by a tractable density qglobal(\u03b8) using a collection of localized inference procedures. In EP, each local inference problem is a function of just pk and the current global estimate, making it appropriate for the FL setting. 2 . (1) (2) Published as a conference paper at ICLR 2023 Algorithm 1 Federated Learning as Inference 1: for round t = 1, . . . , T do 2: 3: 4: 5: 6: 7: 8: 9: end for 10: Return \u00b5global. Sample a subset of clients K. Broadcast qglobal(\u03b8) to the selected clients. for each client k \u2208 K in parallel do \u2206qk(\u03b8) \u2190 ClientInfer(qglobal(\u03b8)) end for Collect \u2206qk(\u03b8) from the selected clients. qglobal(\u03b8) \u2190 ServerInfer({\u2206qk(\u03b8)}k) Algorithm 2 Approximate Inference: MCMC 1: Input: q\\k(\u03b8; Dk, \u03b7\u2212k, \u039b\u2212k) 2: Sk \u2190 {} 3: for i = 1, . . . , N do 4: k \u2190 SGDEpoch(\u2212 log q\\k, \u03b8(i\u22121) \u03b8(i) Sk \u2190 Sk \u222a \u03b8(i) k 5: 6: end for 7: \u03b7\\k, \u039b 8: Output: (cid:98)q\\k(\u03b8; \u03b7\\k, \u039b k \\k \u2190 EstimateMoments(Sk) \\k) ) Algorithm 3 Gaussian EP: Server Inference 1: Receive: {\u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b \u221d qglobal \u220fk (\u2206qk)\u03b4 2: qnew k)}k global \u03b7global \u2190 \u03b7global + \u03b4 ServerOptim(\u2211 k \u039bglobal \u2190 \u039bglobal + \u03b4 ServerOptim(\u2211 // Sec. 2.2.3 \u2206\u03b7k) \u2206\u039b k) k 3: Send: qglobal(\u03b8; \u03b7global, \u039bglobal) Algorithm 4 Gaussian EP: Client Inference 1: Receive: qglobal(\u03b8; \u03b7global, \u039bglobal) 2: q\u2212k \u221d qglobal/qk \u03b7\u2212k \u2190 \u03b7global \u2212 \u03b7k, \u221d pk q\u2212k // cavity distribution \u039b\u2212k \u2190 \u039bglobal \u2212 \u039b k // tilted inference (Sec. 2.2.2) 3: (cid:98)q\\k \u2248 q\\k \u03b7\\k, \u039b \u221d \u221d pk q\u2212k) \\k \u2190 ApproxInference(q\\k (cid:98)q\\k/qglobal \u2206\u03b7k \u2190 \u03b7\\k \u2212 \u03b7global, \u221d qk (\u2206qk)\u03b4 4: \u2206qk // client deltas (Sec. A.1) \\k \u2212 \u039bglobal k \u2190 \u039b \u2206\u039b 5: qnew k \u03b7k \u2190 \u03b7k + \u03b4 ClientOptim(\u2206\u03b7k) k + \u03b4 ClientOptim(\u2206\u039b \u039b k) // local update (Sec. 2.2.3) k \u2190 \u039b 6: Send: \u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b k) Concretely, EP iteratively solves the following problem (either in sequence or parallel), qnew k (\u03b8) = arg min q\u2208Q D (cid:18) pk(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:124) \u221d q\\k(\u03b8) (cid:107) q(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:98)q\\k(\u03b8) (cid:124) \u221d (cid:19) , where q\u2212k(\u03b8) \u221d qglobal(\u03b8) qk(\u03b8) . (3) Here qglobal(\u03b8) and qk(\u03b8) are the global/local distributions from the current iteration. (See Sec. A.2 for further details). In the EP literature, q\u2212k(\u03b8) is referred to as the cavity distribution and q\\k(\u03b8) and (cid:98)q\\k(\u03b8) are referred to as the target/approximate tilted distributions. EP then uses qnew (\u03b8) to derive qnew global(\u03b8). While the theoretical properties of EP are still not well understood (Minka, 2001; Dehaene & Barthelm\u00b4e, 2015; 2018), it has empirically been shown to produce good posterior ap- proximations in many cases (Li et al., 2015; Vehtari et al., 2020). When applied to FL, the central server initiates the update by sending the parameters of the current global approximation qglobal(\u03b8) as messages to the subset of clients K. Upon receiving these messages, each client updates the re- spective local approximation qnew (\u03b8) and sends back the changes in parameters as messages, which is then aggregated by the server. Algorithms 1-4 illustrate the probabilistic message passing with the Gaussian variational family in more detail. k k Remark. Consider the case where we set q\u2212k(\u03b8) \u221d 1 (i.e., an improper uniform distribu- tion that ignores the current estimate of the global parameters). Then Eq. 3 reduces to fed- erated learning with posterior averaging (FedPA) from Al-Shedivat et al. (2021), qnew (\u03b8) = arg minq\u2208Q D (pk(\u03b8) (cid:107) q(\u03b8)). Hence, FedEP improves upon FedPA by taking into account the global parameters and the previous local estimate while deriving the local posterior.2 k 2.2 SCALABLE EXPECTATION PROPAGATION While federated learning with expectation propagation is conceptually straightforward, scaling up FedEP to modern models and datasets is challenging. For one, the high dimensionality of the param- eter space of contemporary models can make local inference dif\ufb01cult even with simple mean-\ufb01eld Gaussian variational families. This is compounded by the fact that classic expectation propaga- tion is stateful and therefore requires that each client always maintains its local contribution to the global posterior. These factors make classic EP potentially an unideal approach in settings where 2When the parameters of qglobal(\u03b8) and qk(\u03b8)\u2019s are initialized as improper uniform distributions, the \ufb01rst round (but only the \ufb01rst round) of FedEP and FedPA is identical. 3 Published as a conference paper"}, {"question": " What is the main challenge in scaling up federated learning with expectation propagation to modern models and datasets?", "answer": " The high dimensionality of the parameter space and the statefulness of classic expectation propagation.", "ref_chunk": "up expectation propaga- tion to contemporary benchmarks (e.g., models with many millions of parameters and datasets with hundreds of thousands of clients). When applied on top of modern FL benchmarks, our approach outperforms strong FedAvg and FedPA baselines. 2 FEDERATED LEARNING WITH EXPECTATION PROPAGATION The probabilistic view from Eq. 1 motivates an alternative formulation of federated learning based on variational inference. First observe that the global posterior p (\u03b8 | D) given a collection of datasets D = (cid:83) k\u2208[K] Dk factorizes as, K \u220f k=1 K \u220f k=0 p (\u03b8 | D) \u221d p(\u03b8) p(Dk | \u03b8) = pk(\u03b8), where for convenience we de\ufb01ne p0(\u03b8) := p(\u03b8) to be the prior and further use pk(\u03b8) := p(Dk | \u03b8) to refer to the local likelihood associated with k-th data partition. To simplify notation we hereon refer to the global posterior as pglobal(\u03b8) and drop the conditioning on D. Now consider an approximating global posterior qglobal(\u03b8) that admits the same factorization as the above, i.e., qglobal(\u03b8) \u221d \u220fK k=0 qk(\u03b8). Plugging in these terms into Eq. 1 gives the following objective, (cid:19) (cid:18) K \u220f k=0 K \u220f k=0 K \u220f k=0 qk(\u03b8), where {qk(\u03b8)}K k=0 = arg min pk(\u03b8) (cid:107) qk(\u03b8) arg max \u03b8 D qk\u2208Q Here Q is the variational family, which is assumed to be the same for all clients. This global objec- tive is in general intractable; evaluating \u220fk pk(\u03b8) requires accessing all clients\u2019 data and violates the standard FL assumption. This section presents a probabilistic message-passing algorithm based on expectation propagation (EP, Minka, 2001). 2.1 EXPECTATION PROPAGATION EP is an iterative algorithm in which an intractable target density pglobal(\u03b8) is approximated by a tractable density qglobal(\u03b8) using a collection of localized inference procedures. In EP, each local inference problem is a function of just pk and the current global estimate, making it appropriate for the FL setting. 2 . (1) (2) Published as a conference paper at ICLR 2023 Algorithm 1 Federated Learning as Inference 1: for round t = 1, . . . , T do 2: 3: 4: 5: 6: 7: 8: 9: end for 10: Return \u00b5global. Sample a subset of clients K. Broadcast qglobal(\u03b8) to the selected clients. for each client k \u2208 K in parallel do \u2206qk(\u03b8) \u2190 ClientInfer(qglobal(\u03b8)) end for Collect \u2206qk(\u03b8) from the selected clients. qglobal(\u03b8) \u2190 ServerInfer({\u2206qk(\u03b8)}k) Algorithm 2 Approximate Inference: MCMC 1: Input: q\\k(\u03b8; Dk, \u03b7\u2212k, \u039b\u2212k) 2: Sk \u2190 {} 3: for i = 1, . . . , N do 4: k \u2190 SGDEpoch(\u2212 log q\\k, \u03b8(i\u22121) \u03b8(i) Sk \u2190 Sk \u222a \u03b8(i) k 5: 6: end for 7: \u03b7\\k, \u039b 8: Output: (cid:98)q\\k(\u03b8; \u03b7\\k, \u039b k \\k \u2190 EstimateMoments(Sk) \\k) ) Algorithm 3 Gaussian EP: Server Inference 1: Receive: {\u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b \u221d qglobal \u220fk (\u2206qk)\u03b4 2: qnew k)}k global \u03b7global \u2190 \u03b7global + \u03b4 ServerOptim(\u2211 k \u039bglobal \u2190 \u039bglobal + \u03b4 ServerOptim(\u2211 // Sec. 2.2.3 \u2206\u03b7k) \u2206\u039b k) k 3: Send: qglobal(\u03b8; \u03b7global, \u039bglobal) Algorithm 4 Gaussian EP: Client Inference 1: Receive: qglobal(\u03b8; \u03b7global, \u039bglobal) 2: q\u2212k \u221d qglobal/qk \u03b7\u2212k \u2190 \u03b7global \u2212 \u03b7k, \u221d pk q\u2212k // cavity distribution \u039b\u2212k \u2190 \u039bglobal \u2212 \u039b k // tilted inference (Sec. 2.2.2) 3: (cid:98)q\\k \u2248 q\\k \u03b7\\k, \u039b \u221d \u221d pk q\u2212k) \\k \u2190 ApproxInference(q\\k (cid:98)q\\k/qglobal \u2206\u03b7k \u2190 \u03b7\\k \u2212 \u03b7global, \u221d qk (\u2206qk)\u03b4 4: \u2206qk // client deltas (Sec. A.1) \\k \u2212 \u039bglobal k \u2190 \u039b \u2206\u039b 5: qnew k \u03b7k \u2190 \u03b7k + \u03b4 ClientOptim(\u2206\u03b7k) k + \u03b4 ClientOptim(\u2206\u039b \u039b k) // local update (Sec. 2.2.3) k \u2190 \u039b 6: Send: \u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b k) Concretely, EP iteratively solves the following problem (either in sequence or parallel), qnew k (\u03b8) = arg min q\u2208Q D (cid:18) pk(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:124) \u221d q\\k(\u03b8) (cid:107) q(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:98)q\\k(\u03b8) (cid:124) \u221d (cid:19) , where q\u2212k(\u03b8) \u221d qglobal(\u03b8) qk(\u03b8) . (3) Here qglobal(\u03b8) and qk(\u03b8) are the global/local distributions from the current iteration. (See Sec. A.2 for further details). In the EP literature, q\u2212k(\u03b8) is referred to as the cavity distribution and q\\k(\u03b8) and (cid:98)q\\k(\u03b8) are referred to as the target/approximate tilted distributions. EP then uses qnew (\u03b8) to derive qnew global(\u03b8). While the theoretical properties of EP are still not well understood (Minka, 2001; Dehaene & Barthelm\u00b4e, 2015; 2018), it has empirically been shown to produce good posterior ap- proximations in many cases (Li et al., 2015; Vehtari et al., 2020). When applied to FL, the central server initiates the update by sending the parameters of the current global approximation qglobal(\u03b8) as messages to the subset of clients K. Upon receiving these messages, each client updates the re- spective local approximation qnew (\u03b8) and sends back the changes in parameters as messages, which is then aggregated by the server. Algorithms 1-4 illustrate the probabilistic message passing with the Gaussian variational family in more detail. k k Remark. Consider the case where we set q\u2212k(\u03b8) \u221d 1 (i.e., an improper uniform distribu- tion that ignores the current estimate of the global parameters). Then Eq. 3 reduces to fed- erated learning with posterior averaging (FedPA) from Al-Shedivat et al. (2021), qnew (\u03b8) = arg minq\u2208Q D (pk(\u03b8) (cid:107) q(\u03b8)). Hence, FedEP improves upon FedPA by taking into account the global parameters and the previous local estimate while deriving the local posterior.2 k 2.2 SCALABLE EXPECTATION PROPAGATION While federated learning with expectation propagation is conceptually straightforward, scaling up FedEP to modern models and datasets is challenging. For one, the high dimensionality of the param- eter space of contemporary models can make local inference dif\ufb01cult even with simple mean-\ufb01eld Gaussian variational families. This is compounded by the fact that classic expectation propaga- tion is stateful and therefore requires that each client always maintains its local contribution to the global posterior. These factors make classic EP potentially an unideal approach in settings where 2When the parameters of qglobal(\u03b8) and qk(\u03b8)\u2019s are initialized as improper uniform distributions, the \ufb01rst round (but only the \ufb01rst round) of FedEP and FedPA is identical. 3 Published as a conference paper"}, {"question": " What is the cavity distribution referred to in the EP literature?", "answer": " The distribution q\u2212k(\u03b8) that is proportional to the global distribution and the local distribution.", "ref_chunk": "up expectation propaga- tion to contemporary benchmarks (e.g., models with many millions of parameters and datasets with hundreds of thousands of clients). When applied on top of modern FL benchmarks, our approach outperforms strong FedAvg and FedPA baselines. 2 FEDERATED LEARNING WITH EXPECTATION PROPAGATION The probabilistic view from Eq. 1 motivates an alternative formulation of federated learning based on variational inference. First observe that the global posterior p (\u03b8 | D) given a collection of datasets D = (cid:83) k\u2208[K] Dk factorizes as, K \u220f k=1 K \u220f k=0 p (\u03b8 | D) \u221d p(\u03b8) p(Dk | \u03b8) = pk(\u03b8), where for convenience we de\ufb01ne p0(\u03b8) := p(\u03b8) to be the prior and further use pk(\u03b8) := p(Dk | \u03b8) to refer to the local likelihood associated with k-th data partition. To simplify notation we hereon refer to the global posterior as pglobal(\u03b8) and drop the conditioning on D. Now consider an approximating global posterior qglobal(\u03b8) that admits the same factorization as the above, i.e., qglobal(\u03b8) \u221d \u220fK k=0 qk(\u03b8). Plugging in these terms into Eq. 1 gives the following objective, (cid:19) (cid:18) K \u220f k=0 K \u220f k=0 K \u220f k=0 qk(\u03b8), where {qk(\u03b8)}K k=0 = arg min pk(\u03b8) (cid:107) qk(\u03b8) arg max \u03b8 D qk\u2208Q Here Q is the variational family, which is assumed to be the same for all clients. This global objec- tive is in general intractable; evaluating \u220fk pk(\u03b8) requires accessing all clients\u2019 data and violates the standard FL assumption. This section presents a probabilistic message-passing algorithm based on expectation propagation (EP, Minka, 2001). 2.1 EXPECTATION PROPAGATION EP is an iterative algorithm in which an intractable target density pglobal(\u03b8) is approximated by a tractable density qglobal(\u03b8) using a collection of localized inference procedures. In EP, each local inference problem is a function of just pk and the current global estimate, making it appropriate for the FL setting. 2 . (1) (2) Published as a conference paper at ICLR 2023 Algorithm 1 Federated Learning as Inference 1: for round t = 1, . . . , T do 2: 3: 4: 5: 6: 7: 8: 9: end for 10: Return \u00b5global. Sample a subset of clients K. Broadcast qglobal(\u03b8) to the selected clients. for each client k \u2208 K in parallel do \u2206qk(\u03b8) \u2190 ClientInfer(qglobal(\u03b8)) end for Collect \u2206qk(\u03b8) from the selected clients. qglobal(\u03b8) \u2190 ServerInfer({\u2206qk(\u03b8)}k) Algorithm 2 Approximate Inference: MCMC 1: Input: q\\k(\u03b8; Dk, \u03b7\u2212k, \u039b\u2212k) 2: Sk \u2190 {} 3: for i = 1, . . . , N do 4: k \u2190 SGDEpoch(\u2212 log q\\k, \u03b8(i\u22121) \u03b8(i) Sk \u2190 Sk \u222a \u03b8(i) k 5: 6: end for 7: \u03b7\\k, \u039b 8: Output: (cid:98)q\\k(\u03b8; \u03b7\\k, \u039b k \\k \u2190 EstimateMoments(Sk) \\k) ) Algorithm 3 Gaussian EP: Server Inference 1: Receive: {\u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b \u221d qglobal \u220fk (\u2206qk)\u03b4 2: qnew k)}k global \u03b7global \u2190 \u03b7global + \u03b4 ServerOptim(\u2211 k \u039bglobal \u2190 \u039bglobal + \u03b4 ServerOptim(\u2211 // Sec. 2.2.3 \u2206\u03b7k) \u2206\u039b k) k 3: Send: qglobal(\u03b8; \u03b7global, \u039bglobal) Algorithm 4 Gaussian EP: Client Inference 1: Receive: qglobal(\u03b8; \u03b7global, \u039bglobal) 2: q\u2212k \u221d qglobal/qk \u03b7\u2212k \u2190 \u03b7global \u2212 \u03b7k, \u221d pk q\u2212k // cavity distribution \u039b\u2212k \u2190 \u039bglobal \u2212 \u039b k // tilted inference (Sec. 2.2.2) 3: (cid:98)q\\k \u2248 q\\k \u03b7\\k, \u039b \u221d \u221d pk q\u2212k) \\k \u2190 ApproxInference(q\\k (cid:98)q\\k/qglobal \u2206\u03b7k \u2190 \u03b7\\k \u2212 \u03b7global, \u221d qk (\u2206qk)\u03b4 4: \u2206qk // client deltas (Sec. A.1) \\k \u2212 \u039bglobal k \u2190 \u039b \u2206\u039b 5: qnew k \u03b7k \u2190 \u03b7k + \u03b4 ClientOptim(\u2206\u03b7k) k + \u03b4 ClientOptim(\u2206\u039b \u039b k) // local update (Sec. 2.2.3) k \u2190 \u039b 6: Send: \u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b k) Concretely, EP iteratively solves the following problem (either in sequence or parallel), qnew k (\u03b8) = arg min q\u2208Q D (cid:18) pk(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:124) \u221d q\\k(\u03b8) (cid:107) q(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:98)q\\k(\u03b8) (cid:124) \u221d (cid:19) , where q\u2212k(\u03b8) \u221d qglobal(\u03b8) qk(\u03b8) . (3) Here qglobal(\u03b8) and qk(\u03b8) are the global/local distributions from the current iteration. (See Sec. A.2 for further details). In the EP literature, q\u2212k(\u03b8) is referred to as the cavity distribution and q\\k(\u03b8) and (cid:98)q\\k(\u03b8) are referred to as the target/approximate tilted distributions. EP then uses qnew (\u03b8) to derive qnew global(\u03b8). While the theoretical properties of EP are still not well understood (Minka, 2001; Dehaene & Barthelm\u00b4e, 2015; 2018), it has empirically been shown to produce good posterior ap- proximations in many cases (Li et al., 2015; Vehtari et al., 2020). When applied to FL, the central server initiates the update by sending the parameters of the current global approximation qglobal(\u03b8) as messages to the subset of clients K. Upon receiving these messages, each client updates the re- spective local approximation qnew (\u03b8) and sends back the changes in parameters as messages, which is then aggregated by the server. Algorithms 1-4 illustrate the probabilistic message passing with the Gaussian variational family in more detail. k k Remark. Consider the case where we set q\u2212k(\u03b8) \u221d 1 (i.e., an improper uniform distribu- tion that ignores the current estimate of the global parameters). Then Eq. 3 reduces to fed- erated learning with posterior averaging (FedPA) from Al-Shedivat et al. (2021), qnew (\u03b8) = arg minq\u2208Q D (pk(\u03b8) (cid:107) q(\u03b8)). Hence, FedEP improves upon FedPA by taking into account the global parameters and the previous local estimate while deriving the local posterior.2 k 2.2 SCALABLE EXPECTATION PROPAGATION While federated learning with expectation propagation is conceptually straightforward, scaling up FedEP to modern models and datasets is challenging. For one, the high dimensionality of the param- eter space of contemporary models can make local inference dif\ufb01cult even with simple mean-\ufb01eld Gaussian variational families. This is compounded by the fact that classic expectation propaga- tion is stateful and therefore requires that each client always maintains its local contribution to the global posterior. These factors make classic EP potentially an unideal approach in settings where 2When the parameters of qglobal(\u03b8) and qk(\u03b8)\u2019s are initialized as improper uniform distributions, the \ufb01rst round (but only the \ufb01rst round) of FedEP and FedPA is identical. 3 Published as a conference paper"}, {"question": " How does FedEP improve upon FedPA in the context of federated learning?", "answer": " FedEP takes into account the global parameters and the previous local estimate while deriving the local posterior.", "ref_chunk": "up expectation propaga- tion to contemporary benchmarks (e.g., models with many millions of parameters and datasets with hundreds of thousands of clients). When applied on top of modern FL benchmarks, our approach outperforms strong FedAvg and FedPA baselines. 2 FEDERATED LEARNING WITH EXPECTATION PROPAGATION The probabilistic view from Eq. 1 motivates an alternative formulation of federated learning based on variational inference. First observe that the global posterior p (\u03b8 | D) given a collection of datasets D = (cid:83) k\u2208[K] Dk factorizes as, K \u220f k=1 K \u220f k=0 p (\u03b8 | D) \u221d p(\u03b8) p(Dk | \u03b8) = pk(\u03b8), where for convenience we de\ufb01ne p0(\u03b8) := p(\u03b8) to be the prior and further use pk(\u03b8) := p(Dk | \u03b8) to refer to the local likelihood associated with k-th data partition. To simplify notation we hereon refer to the global posterior as pglobal(\u03b8) and drop the conditioning on D. Now consider an approximating global posterior qglobal(\u03b8) that admits the same factorization as the above, i.e., qglobal(\u03b8) \u221d \u220fK k=0 qk(\u03b8). Plugging in these terms into Eq. 1 gives the following objective, (cid:19) (cid:18) K \u220f k=0 K \u220f k=0 K \u220f k=0 qk(\u03b8), where {qk(\u03b8)}K k=0 = arg min pk(\u03b8) (cid:107) qk(\u03b8) arg max \u03b8 D qk\u2208Q Here Q is the variational family, which is assumed to be the same for all clients. This global objec- tive is in general intractable; evaluating \u220fk pk(\u03b8) requires accessing all clients\u2019 data and violates the standard FL assumption. This section presents a probabilistic message-passing algorithm based on expectation propagation (EP, Minka, 2001). 2.1 EXPECTATION PROPAGATION EP is an iterative algorithm in which an intractable target density pglobal(\u03b8) is approximated by a tractable density qglobal(\u03b8) using a collection of localized inference procedures. In EP, each local inference problem is a function of just pk and the current global estimate, making it appropriate for the FL setting. 2 . (1) (2) Published as a conference paper at ICLR 2023 Algorithm 1 Federated Learning as Inference 1: for round t = 1, . . . , T do 2: 3: 4: 5: 6: 7: 8: 9: end for 10: Return \u00b5global. Sample a subset of clients K. Broadcast qglobal(\u03b8) to the selected clients. for each client k \u2208 K in parallel do \u2206qk(\u03b8) \u2190 ClientInfer(qglobal(\u03b8)) end for Collect \u2206qk(\u03b8) from the selected clients. qglobal(\u03b8) \u2190 ServerInfer({\u2206qk(\u03b8)}k) Algorithm 2 Approximate Inference: MCMC 1: Input: q\\k(\u03b8; Dk, \u03b7\u2212k, \u039b\u2212k) 2: Sk \u2190 {} 3: for i = 1, . . . , N do 4: k \u2190 SGDEpoch(\u2212 log q\\k, \u03b8(i\u22121) \u03b8(i) Sk \u2190 Sk \u222a \u03b8(i) k 5: 6: end for 7: \u03b7\\k, \u039b 8: Output: (cid:98)q\\k(\u03b8; \u03b7\\k, \u039b k \\k \u2190 EstimateMoments(Sk) \\k) ) Algorithm 3 Gaussian EP: Server Inference 1: Receive: {\u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b \u221d qglobal \u220fk (\u2206qk)\u03b4 2: qnew k)}k global \u03b7global \u2190 \u03b7global + \u03b4 ServerOptim(\u2211 k \u039bglobal \u2190 \u039bglobal + \u03b4 ServerOptim(\u2211 // Sec. 2.2.3 \u2206\u03b7k) \u2206\u039b k) k 3: Send: qglobal(\u03b8; \u03b7global, \u039bglobal) Algorithm 4 Gaussian EP: Client Inference 1: Receive: qglobal(\u03b8; \u03b7global, \u039bglobal) 2: q\u2212k \u221d qglobal/qk \u03b7\u2212k \u2190 \u03b7global \u2212 \u03b7k, \u221d pk q\u2212k // cavity distribution \u039b\u2212k \u2190 \u039bglobal \u2212 \u039b k // tilted inference (Sec. 2.2.2) 3: (cid:98)q\\k \u2248 q\\k \u03b7\\k, \u039b \u221d \u221d pk q\u2212k) \\k \u2190 ApproxInference(q\\k (cid:98)q\\k/qglobal \u2206\u03b7k \u2190 \u03b7\\k \u2212 \u03b7global, \u221d qk (\u2206qk)\u03b4 4: \u2206qk // client deltas (Sec. A.1) \\k \u2212 \u039bglobal k \u2190 \u039b \u2206\u039b 5: qnew k \u03b7k \u2190 \u03b7k + \u03b4 ClientOptim(\u2206\u03b7k) k + \u03b4 ClientOptim(\u2206\u039b \u039b k) // local update (Sec. 2.2.3) k \u2190 \u039b 6: Send: \u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b k) Concretely, EP iteratively solves the following problem (either in sequence or parallel), qnew k (\u03b8) = arg min q\u2208Q D (cid:18) pk(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:124) \u221d q\\k(\u03b8) (cid:107) q(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:98)q\\k(\u03b8) (cid:124) \u221d (cid:19) , where q\u2212k(\u03b8) \u221d qglobal(\u03b8) qk(\u03b8) . (3) Here qglobal(\u03b8) and qk(\u03b8) are the global/local distributions from the current iteration. (See Sec. A.2 for further details). In the EP literature, q\u2212k(\u03b8) is referred to as the cavity distribution and q\\k(\u03b8) and (cid:98)q\\k(\u03b8) are referred to as the target/approximate tilted distributions. EP then uses qnew (\u03b8) to derive qnew global(\u03b8). While the theoretical properties of EP are still not well understood (Minka, 2001; Dehaene & Barthelm\u00b4e, 2015; 2018), it has empirically been shown to produce good posterior ap- proximations in many cases (Li et al., 2015; Vehtari et al., 2020). When applied to FL, the central server initiates the update by sending the parameters of the current global approximation qglobal(\u03b8) as messages to the subset of clients K. Upon receiving these messages, each client updates the re- spective local approximation qnew (\u03b8) and sends back the changes in parameters as messages, which is then aggregated by the server. Algorithms 1-4 illustrate the probabilistic message passing with the Gaussian variational family in more detail. k k Remark. Consider the case where we set q\u2212k(\u03b8) \u221d 1 (i.e., an improper uniform distribu- tion that ignores the current estimate of the global parameters). Then Eq. 3 reduces to fed- erated learning with posterior averaging (FedPA) from Al-Shedivat et al. (2021), qnew (\u03b8) = arg minq\u2208Q D (pk(\u03b8) (cid:107) q(\u03b8)). Hence, FedEP improves upon FedPA by taking into account the global parameters and the previous local estimate while deriving the local posterior.2 k 2.2 SCALABLE EXPECTATION PROPAGATION While federated learning with expectation propagation is conceptually straightforward, scaling up FedEP to modern models and datasets is challenging. For one, the high dimensionality of the param- eter space of contemporary models can make local inference dif\ufb01cult even with simple mean-\ufb01eld Gaussian variational families. This is compounded by the fact that classic expectation propaga- tion is stateful and therefore requires that each client always maintains its local contribution to the global posterior. These factors make classic EP potentially an unideal approach in settings where 2When the parameters of qglobal(\u03b8) and qk(\u03b8)\u2019s are initialized as improper uniform distributions, the \ufb01rst round (but only the \ufb01rst round) of FedEP and FedPA is identical. 3 Published as a conference paper"}, {"question": " What does EP use to derive qnew(\u03b8) in the approximate inference algorithm?", "answer": " The cavity distribution and the approximate tilted distributions.", "ref_chunk": "up expectation propaga- tion to contemporary benchmarks (e.g., models with many millions of parameters and datasets with hundreds of thousands of clients). When applied on top of modern FL benchmarks, our approach outperforms strong FedAvg and FedPA baselines. 2 FEDERATED LEARNING WITH EXPECTATION PROPAGATION The probabilistic view from Eq. 1 motivates an alternative formulation of federated learning based on variational inference. First observe that the global posterior p (\u03b8 | D) given a collection of datasets D = (cid:83) k\u2208[K] Dk factorizes as, K \u220f k=1 K \u220f k=0 p (\u03b8 | D) \u221d p(\u03b8) p(Dk | \u03b8) = pk(\u03b8), where for convenience we de\ufb01ne p0(\u03b8) := p(\u03b8) to be the prior and further use pk(\u03b8) := p(Dk | \u03b8) to refer to the local likelihood associated with k-th data partition. To simplify notation we hereon refer to the global posterior as pglobal(\u03b8) and drop the conditioning on D. Now consider an approximating global posterior qglobal(\u03b8) that admits the same factorization as the above, i.e., qglobal(\u03b8) \u221d \u220fK k=0 qk(\u03b8). Plugging in these terms into Eq. 1 gives the following objective, (cid:19) (cid:18) K \u220f k=0 K \u220f k=0 K \u220f k=0 qk(\u03b8), where {qk(\u03b8)}K k=0 = arg min pk(\u03b8) (cid:107) qk(\u03b8) arg max \u03b8 D qk\u2208Q Here Q is the variational family, which is assumed to be the same for all clients. This global objec- tive is in general intractable; evaluating \u220fk pk(\u03b8) requires accessing all clients\u2019 data and violates the standard FL assumption. This section presents a probabilistic message-passing algorithm based on expectation propagation (EP, Minka, 2001). 2.1 EXPECTATION PROPAGATION EP is an iterative algorithm in which an intractable target density pglobal(\u03b8) is approximated by a tractable density qglobal(\u03b8) using a collection of localized inference procedures. In EP, each local inference problem is a function of just pk and the current global estimate, making it appropriate for the FL setting. 2 . (1) (2) Published as a conference paper at ICLR 2023 Algorithm 1 Federated Learning as Inference 1: for round t = 1, . . . , T do 2: 3: 4: 5: 6: 7: 8: 9: end for 10: Return \u00b5global. Sample a subset of clients K. Broadcast qglobal(\u03b8) to the selected clients. for each client k \u2208 K in parallel do \u2206qk(\u03b8) \u2190 ClientInfer(qglobal(\u03b8)) end for Collect \u2206qk(\u03b8) from the selected clients. qglobal(\u03b8) \u2190 ServerInfer({\u2206qk(\u03b8)}k) Algorithm 2 Approximate Inference: MCMC 1: Input: q\\k(\u03b8; Dk, \u03b7\u2212k, \u039b\u2212k) 2: Sk \u2190 {} 3: for i = 1, . . . , N do 4: k \u2190 SGDEpoch(\u2212 log q\\k, \u03b8(i\u22121) \u03b8(i) Sk \u2190 Sk \u222a \u03b8(i) k 5: 6: end for 7: \u03b7\\k, \u039b 8: Output: (cid:98)q\\k(\u03b8; \u03b7\\k, \u039b k \\k \u2190 EstimateMoments(Sk) \\k) ) Algorithm 3 Gaussian EP: Server Inference 1: Receive: {\u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b \u221d qglobal \u220fk (\u2206qk)\u03b4 2: qnew k)}k global \u03b7global \u2190 \u03b7global + \u03b4 ServerOptim(\u2211 k \u039bglobal \u2190 \u039bglobal + \u03b4 ServerOptim(\u2211 // Sec. 2.2.3 \u2206\u03b7k) \u2206\u039b k) k 3: Send: qglobal(\u03b8; \u03b7global, \u039bglobal) Algorithm 4 Gaussian EP: Client Inference 1: Receive: qglobal(\u03b8; \u03b7global, \u039bglobal) 2: q\u2212k \u221d qglobal/qk \u03b7\u2212k \u2190 \u03b7global \u2212 \u03b7k, \u221d pk q\u2212k // cavity distribution \u039b\u2212k \u2190 \u039bglobal \u2212 \u039b k // tilted inference (Sec. 2.2.2) 3: (cid:98)q\\k \u2248 q\\k \u03b7\\k, \u039b \u221d \u221d pk q\u2212k) \\k \u2190 ApproxInference(q\\k (cid:98)q\\k/qglobal \u2206\u03b7k \u2190 \u03b7\\k \u2212 \u03b7global, \u221d qk (\u2206qk)\u03b4 4: \u2206qk // client deltas (Sec. A.1) \\k \u2212 \u039bglobal k \u2190 \u039b \u2206\u039b 5: qnew k \u03b7k \u2190 \u03b7k + \u03b4 ClientOptim(\u2206\u03b7k) k + \u03b4 ClientOptim(\u2206\u039b \u039b k) // local update (Sec. 2.2.3) k \u2190 \u039b 6: Send: \u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b k) Concretely, EP iteratively solves the following problem (either in sequence or parallel), qnew k (\u03b8) = arg min q\u2208Q D (cid:18) pk(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:124) \u221d q\\k(\u03b8) (cid:107) q(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:98)q\\k(\u03b8) (cid:124) \u221d (cid:19) , where q\u2212k(\u03b8) \u221d qglobal(\u03b8) qk(\u03b8) . (3) Here qglobal(\u03b8) and qk(\u03b8) are the global/local distributions from the current iteration. (See Sec. A.2 for further details). In the EP literature, q\u2212k(\u03b8) is referred to as the cavity distribution and q\\k(\u03b8) and (cid:98)q\\k(\u03b8) are referred to as the target/approximate tilted distributions. EP then uses qnew (\u03b8) to derive qnew global(\u03b8). While the theoretical properties of EP are still not well understood (Minka, 2001; Dehaene & Barthelm\u00b4e, 2015; 2018), it has empirically been shown to produce good posterior ap- proximations in many cases (Li et al., 2015; Vehtari et al., 2020). When applied to FL, the central server initiates the update by sending the parameters of the current global approximation qglobal(\u03b8) as messages to the subset of clients K. Upon receiving these messages, each client updates the re- spective local approximation qnew (\u03b8) and sends back the changes in parameters as messages, which is then aggregated by the server. Algorithms 1-4 illustrate the probabilistic message passing with the Gaussian variational family in more detail. k k Remark. Consider the case where we set q\u2212k(\u03b8) \u221d 1 (i.e., an improper uniform distribu- tion that ignores the current estimate of the global parameters). Then Eq. 3 reduces to fed- erated learning with posterior averaging (FedPA) from Al-Shedivat et al. (2021), qnew (\u03b8) = arg minq\u2208Q D (pk(\u03b8) (cid:107) q(\u03b8)). Hence, FedEP improves upon FedPA by taking into account the global parameters and the previous local estimate while deriving the local posterior.2 k 2.2 SCALABLE EXPECTATION PROPAGATION While federated learning with expectation propagation is conceptually straightforward, scaling up FedEP to modern models and datasets is challenging. For one, the high dimensionality of the param- eter space of contemporary models can make local inference dif\ufb01cult even with simple mean-\ufb01eld Gaussian variational families. This is compounded by the fact that classic expectation propaga- tion is stateful and therefore requires that each client always maintains its local contribution to the global posterior. These factors make classic EP potentially an unideal approach in settings where 2When the parameters of qglobal(\u03b8) and qk(\u03b8)\u2019s are initialized as improper uniform distributions, the \ufb01rst round (but only the \ufb01rst round) of FedEP and FedPA is identical. 3 Published as a conference paper"}, {"question": " What is the probabilistic message passing algorithm based on in the discussed scenario?", "answer": " The Gaussian variational family.", "ref_chunk": "up expectation propaga- tion to contemporary benchmarks (e.g., models with many millions of parameters and datasets with hundreds of thousands of clients). When applied on top of modern FL benchmarks, our approach outperforms strong FedAvg and FedPA baselines. 2 FEDERATED LEARNING WITH EXPECTATION PROPAGATION The probabilistic view from Eq. 1 motivates an alternative formulation of federated learning based on variational inference. First observe that the global posterior p (\u03b8 | D) given a collection of datasets D = (cid:83) k\u2208[K] Dk factorizes as, K \u220f k=1 K \u220f k=0 p (\u03b8 | D) \u221d p(\u03b8) p(Dk | \u03b8) = pk(\u03b8), where for convenience we de\ufb01ne p0(\u03b8) := p(\u03b8) to be the prior and further use pk(\u03b8) := p(Dk | \u03b8) to refer to the local likelihood associated with k-th data partition. To simplify notation we hereon refer to the global posterior as pglobal(\u03b8) and drop the conditioning on D. Now consider an approximating global posterior qglobal(\u03b8) that admits the same factorization as the above, i.e., qglobal(\u03b8) \u221d \u220fK k=0 qk(\u03b8). Plugging in these terms into Eq. 1 gives the following objective, (cid:19) (cid:18) K \u220f k=0 K \u220f k=0 K \u220f k=0 qk(\u03b8), where {qk(\u03b8)}K k=0 = arg min pk(\u03b8) (cid:107) qk(\u03b8) arg max \u03b8 D qk\u2208Q Here Q is the variational family, which is assumed to be the same for all clients. This global objec- tive is in general intractable; evaluating \u220fk pk(\u03b8) requires accessing all clients\u2019 data and violates the standard FL assumption. This section presents a probabilistic message-passing algorithm based on expectation propagation (EP, Minka, 2001). 2.1 EXPECTATION PROPAGATION EP is an iterative algorithm in which an intractable target density pglobal(\u03b8) is approximated by a tractable density qglobal(\u03b8) using a collection of localized inference procedures. In EP, each local inference problem is a function of just pk and the current global estimate, making it appropriate for the FL setting. 2 . (1) (2) Published as a conference paper at ICLR 2023 Algorithm 1 Federated Learning as Inference 1: for round t = 1, . . . , T do 2: 3: 4: 5: 6: 7: 8: 9: end for 10: Return \u00b5global. Sample a subset of clients K. Broadcast qglobal(\u03b8) to the selected clients. for each client k \u2208 K in parallel do \u2206qk(\u03b8) \u2190 ClientInfer(qglobal(\u03b8)) end for Collect \u2206qk(\u03b8) from the selected clients. qglobal(\u03b8) \u2190 ServerInfer({\u2206qk(\u03b8)}k) Algorithm 2 Approximate Inference: MCMC 1: Input: q\\k(\u03b8; Dk, \u03b7\u2212k, \u039b\u2212k) 2: Sk \u2190 {} 3: for i = 1, . . . , N do 4: k \u2190 SGDEpoch(\u2212 log q\\k, \u03b8(i\u22121) \u03b8(i) Sk \u2190 Sk \u222a \u03b8(i) k 5: 6: end for 7: \u03b7\\k, \u039b 8: Output: (cid:98)q\\k(\u03b8; \u03b7\\k, \u039b k \\k \u2190 EstimateMoments(Sk) \\k) ) Algorithm 3 Gaussian EP: Server Inference 1: Receive: {\u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b \u221d qglobal \u220fk (\u2206qk)\u03b4 2: qnew k)}k global \u03b7global \u2190 \u03b7global + \u03b4 ServerOptim(\u2211 k \u039bglobal \u2190 \u039bglobal + \u03b4 ServerOptim(\u2211 // Sec. 2.2.3 \u2206\u03b7k) \u2206\u039b k) k 3: Send: qglobal(\u03b8; \u03b7global, \u039bglobal) Algorithm 4 Gaussian EP: Client Inference 1: Receive: qglobal(\u03b8; \u03b7global, \u039bglobal) 2: q\u2212k \u221d qglobal/qk \u03b7\u2212k \u2190 \u03b7global \u2212 \u03b7k, \u221d pk q\u2212k // cavity distribution \u039b\u2212k \u2190 \u039bglobal \u2212 \u039b k // tilted inference (Sec. 2.2.2) 3: (cid:98)q\\k \u2248 q\\k \u03b7\\k, \u039b \u221d \u221d pk q\u2212k) \\k \u2190 ApproxInference(q\\k (cid:98)q\\k/qglobal \u2206\u03b7k \u2190 \u03b7\\k \u2212 \u03b7global, \u221d qk (\u2206qk)\u03b4 4: \u2206qk // client deltas (Sec. A.1) \\k \u2212 \u039bglobal k \u2190 \u039b \u2206\u039b 5: qnew k \u03b7k \u2190 \u03b7k + \u03b4 ClientOptim(\u2206\u03b7k) k + \u03b4 ClientOptim(\u2206\u039b \u039b k) // local update (Sec. 2.2.3) k \u2190 \u039b 6: Send: \u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b k) Concretely, EP iteratively solves the following problem (either in sequence or parallel), qnew k (\u03b8) = arg min q\u2208Q D (cid:18) pk(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:124) \u221d q\\k(\u03b8) (cid:107) q(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:98)q\\k(\u03b8) (cid:124) \u221d (cid:19) , where q\u2212k(\u03b8) \u221d qglobal(\u03b8) qk(\u03b8) . (3) Here qglobal(\u03b8) and qk(\u03b8) are the global/local distributions from the current iteration. (See Sec. A.2 for further details). In the EP literature, q\u2212k(\u03b8) is referred to as the cavity distribution and q\\k(\u03b8) and (cid:98)q\\k(\u03b8) are referred to as the target/approximate tilted distributions. EP then uses qnew (\u03b8) to derive qnew global(\u03b8). While the theoretical properties of EP are still not well understood (Minka, 2001; Dehaene & Barthelm\u00b4e, 2015; 2018), it has empirically been shown to produce good posterior ap- proximations in many cases (Li et al., 2015; Vehtari et al., 2020). When applied to FL, the central server initiates the update by sending the parameters of the current global approximation qglobal(\u03b8) as messages to the subset of clients K. Upon receiving these messages, each client updates the re- spective local approximation qnew (\u03b8) and sends back the changes in parameters as messages, which is then aggregated by the server. Algorithms 1-4 illustrate the probabilistic message passing with the Gaussian variational family in more detail. k k Remark. Consider the case where we set q\u2212k(\u03b8) \u221d 1 (i.e., an improper uniform distribu- tion that ignores the current estimate of the global parameters). Then Eq. 3 reduces to fed- erated learning with posterior averaging (FedPA) from Al-Shedivat et al. (2021), qnew (\u03b8) = arg minq\u2208Q D (pk(\u03b8) (cid:107) q(\u03b8)). Hence, FedEP improves upon FedPA by taking into account the global parameters and the previous local estimate while deriving the local posterior.2 k 2.2 SCALABLE EXPECTATION PROPAGATION While federated learning with expectation propagation is conceptually straightforward, scaling up FedEP to modern models and datasets is challenging. For one, the high dimensionality of the param- eter space of contemporary models can make local inference dif\ufb01cult even with simple mean-\ufb01eld Gaussian variational families. This is compounded by the fact that classic expectation propaga- tion is stateful and therefore requires that each client always maintains its local contribution to the global posterior. These factors make classic EP potentially an unideal approach in settings where 2When the parameters of qglobal(\u03b8) and qk(\u03b8)\u2019s are initialized as improper uniform distributions, the \ufb01rst round (but only the \ufb01rst round) of FedEP and FedPA is identical. 3 Published as a conference paper"}], "doc_text": "up expectation propaga- tion to contemporary benchmarks (e.g., models with many millions of parameters and datasets with hundreds of thousands of clients). When applied on top of modern FL benchmarks, our approach outperforms strong FedAvg and FedPA baselines. 2 FEDERATED LEARNING WITH EXPECTATION PROPAGATION The probabilistic view from Eq. 1 motivates an alternative formulation of federated learning based on variational inference. First observe that the global posterior p (\u03b8 | D) given a collection of datasets D = (cid:83) k\u2208[K] Dk factorizes as, K \u220f k=1 K \u220f k=0 p (\u03b8 | D) \u221d p(\u03b8) p(Dk | \u03b8) = pk(\u03b8), where for convenience we de\ufb01ne p0(\u03b8) := p(\u03b8) to be the prior and further use pk(\u03b8) := p(Dk | \u03b8) to refer to the local likelihood associated with k-th data partition. To simplify notation we hereon refer to the global posterior as pglobal(\u03b8) and drop the conditioning on D. Now consider an approximating global posterior qglobal(\u03b8) that admits the same factorization as the above, i.e., qglobal(\u03b8) \u221d \u220fK k=0 qk(\u03b8). Plugging in these terms into Eq. 1 gives the following objective, (cid:19) (cid:18) K \u220f k=0 K \u220f k=0 K \u220f k=0 qk(\u03b8), where {qk(\u03b8)}K k=0 = arg min pk(\u03b8) (cid:107) qk(\u03b8) arg max \u03b8 D qk\u2208Q Here Q is the variational family, which is assumed to be the same for all clients. This global objec- tive is in general intractable; evaluating \u220fk pk(\u03b8) requires accessing all clients\u2019 data and violates the standard FL assumption. This section presents a probabilistic message-passing algorithm based on expectation propagation (EP, Minka, 2001). 2.1 EXPECTATION PROPAGATION EP is an iterative algorithm in which an intractable target density pglobal(\u03b8) is approximated by a tractable density qglobal(\u03b8) using a collection of localized inference procedures. In EP, each local inference problem is a function of just pk and the current global estimate, making it appropriate for the FL setting. 2 . (1) (2) Published as a conference paper at ICLR 2023 Algorithm 1 Federated Learning as Inference 1: for round t = 1, . . . , T do 2: 3: 4: 5: 6: 7: 8: 9: end for 10: Return \u00b5global. Sample a subset of clients K. Broadcast qglobal(\u03b8) to the selected clients. for each client k \u2208 K in parallel do \u2206qk(\u03b8) \u2190 ClientInfer(qglobal(\u03b8)) end for Collect \u2206qk(\u03b8) from the selected clients. qglobal(\u03b8) \u2190 ServerInfer({\u2206qk(\u03b8)}k) Algorithm 2 Approximate Inference: MCMC 1: Input: q\\k(\u03b8; Dk, \u03b7\u2212k, \u039b\u2212k) 2: Sk \u2190 {} 3: for i = 1, . . . , N do 4: k \u2190 SGDEpoch(\u2212 log q\\k, \u03b8(i\u22121) \u03b8(i) Sk \u2190 Sk \u222a \u03b8(i) k 5: 6: end for 7: \u03b7\\k, \u039b 8: Output: (cid:98)q\\k(\u03b8; \u03b7\\k, \u039b k \\k \u2190 EstimateMoments(Sk) \\k) ) Algorithm 3 Gaussian EP: Server Inference 1: Receive: {\u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b \u221d qglobal \u220fk (\u2206qk)\u03b4 2: qnew k)}k global \u03b7global \u2190 \u03b7global + \u03b4 ServerOptim(\u2211 k \u039bglobal \u2190 \u039bglobal + \u03b4 ServerOptim(\u2211 // Sec. 2.2.3 \u2206\u03b7k) \u2206\u039b k) k 3: Send: qglobal(\u03b8; \u03b7global, \u039bglobal) Algorithm 4 Gaussian EP: Client Inference 1: Receive: qglobal(\u03b8; \u03b7global, \u039bglobal) 2: q\u2212k \u221d qglobal/qk \u03b7\u2212k \u2190 \u03b7global \u2212 \u03b7k, \u221d pk q\u2212k // cavity distribution \u039b\u2212k \u2190 \u039bglobal \u2212 \u039b k // tilted inference (Sec. 2.2.2) 3: (cid:98)q\\k \u2248 q\\k \u03b7\\k, \u039b \u221d \u221d pk q\u2212k) \\k \u2190 ApproxInference(q\\k (cid:98)q\\k/qglobal \u2206\u03b7k \u2190 \u03b7\\k \u2212 \u03b7global, \u221d qk (\u2206qk)\u03b4 4: \u2206qk // client deltas (Sec. A.1) \\k \u2212 \u039bglobal k \u2190 \u039b \u2206\u039b 5: qnew k \u03b7k \u2190 \u03b7k + \u03b4 ClientOptim(\u2206\u03b7k) k + \u03b4 ClientOptim(\u2206\u039b \u039b k) // local update (Sec. 2.2.3) k \u2190 \u039b 6: Send: \u2206qk(\u03b8; \u2206\u03b7k, \u2206\u039b k) Concretely, EP iteratively solves the following problem (either in sequence or parallel), qnew k (\u03b8) = arg min q\u2208Q D (cid:18) pk(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:124) \u221d q\\k(\u03b8) (cid:107) q(\u03b8) q\u2212k(\u03b8) (cid:123)(cid:122) (cid:125) (cid:98)q\\k(\u03b8) (cid:124) \u221d (cid:19) , where q\u2212k(\u03b8) \u221d qglobal(\u03b8) qk(\u03b8) . (3) Here qglobal(\u03b8) and qk(\u03b8) are the global/local distributions from the current iteration. (See Sec. A.2 for further details). In the EP literature, q\u2212k(\u03b8) is referred to as the cavity distribution and q\\k(\u03b8) and (cid:98)q\\k(\u03b8) are referred to as the target/approximate tilted distributions. EP then uses qnew (\u03b8) to derive qnew global(\u03b8). While the theoretical properties of EP are still not well understood (Minka, 2001; Dehaene & Barthelm\u00b4e, 2015; 2018), it has empirically been shown to produce good posterior ap- proximations in many cases (Li et al., 2015; Vehtari et al., 2020). When applied to FL, the central server initiates the update by sending the parameters of the current global approximation qglobal(\u03b8) as messages to the subset of clients K. Upon receiving these messages, each client updates the re- spective local approximation qnew (\u03b8) and sends back the changes in parameters as messages, which is then aggregated by the server. Algorithms 1-4 illustrate the probabilistic message passing with the Gaussian variational family in more detail. k k Remark. Consider the case where we set q\u2212k(\u03b8) \u221d 1 (i.e., an improper uniform distribu- tion that ignores the current estimate of the global parameters). Then Eq. 3 reduces to fed- erated learning with posterior averaging (FedPA) from Al-Shedivat et al. (2021), qnew (\u03b8) = arg minq\u2208Q D (pk(\u03b8) (cid:107) q(\u03b8)). Hence, FedEP improves upon FedPA by taking into account the global parameters and the previous local estimate while deriving the local posterior.2 k 2.2 SCALABLE EXPECTATION PROPAGATION While federated learning with expectation propagation is conceptually straightforward, scaling up FedEP to modern models and datasets is challenging. For one, the high dimensionality of the param- eter space of contemporary models can make local inference dif\ufb01cult even with simple mean-\ufb01eld Gaussian variational families. This is compounded by the fact that classic expectation propaga- tion is stateful and therefore requires that each client always maintains its local contribution to the global posterior. These factors make classic EP potentially an unideal approach in settings where 2When the parameters of qglobal(\u03b8) and qk(\u03b8)\u2019s are initialized as improper uniform distributions, the \ufb01rst round (but only the \ufb01rst round) of FedEP and FedPA is identical. 3 Published as a conference paper"}