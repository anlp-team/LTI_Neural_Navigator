{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_One-for-All:_Generalized_LoRA_for_Parameter-Efficient_Fine-tuning_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the hypothesis space Hi in the problem context?", "answer": " The hypothesis space Hi is a subset of Huni.", "ref_chunk": "problem context, where the hypothesis space Hi is a subset of Huni in our context. Huni encompasses all possible shattered scenarios of Hi. For the extreme case where the VC dimension dvc(Ho) (Ho is the difference set of Huni and Hi) is 0, the error \u03f5 will be zero. As per learning theory, a higher VC dimension implies greater model flexibility and capability of our approach. Clearly, Theorem 1 holds for GLoRA and thus it experiences a greater model capacity. 3 EXPERIMENTS Datasets. We thoroughly evaluate GLoRA on VTAB-1K (Zhai et al., 2020) benchmark for various parameter budgets. VTAB-1K comprises 19 image classification tasks clustered into three domains: (i) Natural images; (ii) Specialized tasks consisting of remote sensing and medical datasets; and 5 (12) (13) (iii) Structured tasks focusing on scene structure understanding. To examine the ability on few-shot learning, we evaluate GLoRA on five fine-grained visual recognition few-shot datasets: Food101 (Bossard et al., 2014), OxfordFlowers102 (Nilsback & Zisserman, 2006), StandfordCars (Krause et al., 2013), OxfordPets (Parkhi et al., 2012), and FGVCAircraft (Maji et al., 2013). Following previous work (Jie & Deng, 2022), we evaluate 1, 2, 4, 8, and 16-shot settings. Next, to show the domain generalization capabilities of GLoRA, we train it on ImageNet (Deng et al., 2009b) for a 16-shot setting and test on four out-of-domain datasets including ImageNetV2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet- R (Hendrycks et al., 2021a). Finally, we show the performance of GLoRA on the Open LLM Leaderboard which consists of four datasets with varying prompt shots, namely AI2 Reasoning Challenge (25-shot) (Clark et al., 2018), TruthfulQA (0-shot) (Lin et al., 2022), HellaSwag (10- shot) (Zellers et al., 2019) and MMLU (5-shot) (Hendrycks et al., 2020). Network Architecture and Implementation Details. For all the vision experiments, we utilize ViT-B (Dosovitskiy et al., 2021), a model pre-trained on ImageNet-21K, as our foundational model. For the language experiments, we consider two foundational base models: LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b). Our supernets undergo a training process spanning 500 epochs and 15 epochs for vision and lan- guage datasets respectively, operating with a constant batch size of 64 and a cosine learning rate scheduler. It is crucial to highlight that this precise policy demonstrates robust efficacy across all settings, regardless of the dataset in use. Post the training of supernet, we perform an evolutionary search on the validation set to pinpoint the optimal task-specific subnet, finalized for implemen- tation. Finally, we report the performance of the searched subnet on the test set. In Appendix, we provide further insights into dataset-specific learning rates and specific settings for different datasets. 3.1 RESULTS ON VTAB-1K We train three different GLoRA supernet configurations to vary the number of trainable parameters. The difference among them is only the LoRA dimensions in the search space which varies from 8 and 4 in the largest model, 4 and 2 in the intermediate model, and 2 in the smallest model. This added parameter flexibility in our approach allows for user-defined trainable parameter count in the final models. Results on the VTAB-1K benchmark are shown in Table 1. We push the state-of-the- art in parameter-efficient transfer learning by up to 2.9%. Impressively, our smallest model already surpasses all existing approaches by a significant margin. It is worth noting that GLoRA performs competitively across datasets, in contrast, prior all existing works tend to fail on at least one, proving GLoRA\u2019s high generalization capabilities. GLoRA pushes the state of the art in as many as 14 out of 19 datasets under VTAB-1K while maintaining commendable performance on the others. Table 1: Full results on VTAB-1K benchmark. \u201c# params\u201d specifies the number of trainable pa- rameters in backbones. Average accuracy and # params are averaged over group-wise mean values. Natural Specialized Structured ) M ( m a r a p # t s o C e c n e r e f n I 0 0 1 r a f i C 1 0 1 h c e t l a C D T D 2 0 1 r e w o l F s t e P N H V S 7 9 3 n u S n o y l e m a C T A S o r u E 5 4 c s i s e R y h t a p o n i t e R t n u o C r v e l C t s i D r v e l C b a L M D t s i D I T T I K c o L - r p S d i r O r p S d m i z A B R O N s Traditional Finetuning Full Linear PEFT methods BitFit 0.10 VPT-Shallow 0.06 0.53 VPT-Deep 0.16 Adapter 0.16 AdaptFormer 0.29 LoRA 0.36 NOAH 0.07 FacT 0.24 SSF 0.22 RepAdapter GLoRA 0.86 GLoRA 0.44 GLoRA 0.29 85.8 0 - \u2191 \u2191 \u2191 \u2191 - \u2191 - - - - - - 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.9 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.6 72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 65.2 77.7 86.9 62.6 97.5 87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 67.8 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 72.0 69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 73.9 70.8 91.2 70.5 99.1 90.9 86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3 76.3 45.7 31.7 41.1 74.7 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1"}, {"question": " What does the extreme case where the VC dimension is 0 imply?", "answer": " In the extreme case where the VC dimension is 0, the error \u03f5 will be zero.", "ref_chunk": "problem context, where the hypothesis space Hi is a subset of Huni in our context. Huni encompasses all possible shattered scenarios of Hi. For the extreme case where the VC dimension dvc(Ho) (Ho is the difference set of Huni and Hi) is 0, the error \u03f5 will be zero. As per learning theory, a higher VC dimension implies greater model flexibility and capability of our approach. Clearly, Theorem 1 holds for GLoRA and thus it experiences a greater model capacity. 3 EXPERIMENTS Datasets. We thoroughly evaluate GLoRA on VTAB-1K (Zhai et al., 2020) benchmark for various parameter budgets. VTAB-1K comprises 19 image classification tasks clustered into three domains: (i) Natural images; (ii) Specialized tasks consisting of remote sensing and medical datasets; and 5 (12) (13) (iii) Structured tasks focusing on scene structure understanding. To examine the ability on few-shot learning, we evaluate GLoRA on five fine-grained visual recognition few-shot datasets: Food101 (Bossard et al., 2014), OxfordFlowers102 (Nilsback & Zisserman, 2006), StandfordCars (Krause et al., 2013), OxfordPets (Parkhi et al., 2012), and FGVCAircraft (Maji et al., 2013). Following previous work (Jie & Deng, 2022), we evaluate 1, 2, 4, 8, and 16-shot settings. Next, to show the domain generalization capabilities of GLoRA, we train it on ImageNet (Deng et al., 2009b) for a 16-shot setting and test on four out-of-domain datasets including ImageNetV2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet- R (Hendrycks et al., 2021a). Finally, we show the performance of GLoRA on the Open LLM Leaderboard which consists of four datasets with varying prompt shots, namely AI2 Reasoning Challenge (25-shot) (Clark et al., 2018), TruthfulQA (0-shot) (Lin et al., 2022), HellaSwag (10- shot) (Zellers et al., 2019) and MMLU (5-shot) (Hendrycks et al., 2020). Network Architecture and Implementation Details. For all the vision experiments, we utilize ViT-B (Dosovitskiy et al., 2021), a model pre-trained on ImageNet-21K, as our foundational model. For the language experiments, we consider two foundational base models: LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b). Our supernets undergo a training process spanning 500 epochs and 15 epochs for vision and lan- guage datasets respectively, operating with a constant batch size of 64 and a cosine learning rate scheduler. It is crucial to highlight that this precise policy demonstrates robust efficacy across all settings, regardless of the dataset in use. Post the training of supernet, we perform an evolutionary search on the validation set to pinpoint the optimal task-specific subnet, finalized for implemen- tation. Finally, we report the performance of the searched subnet on the test set. In Appendix, we provide further insights into dataset-specific learning rates and specific settings for different datasets. 3.1 RESULTS ON VTAB-1K We train three different GLoRA supernet configurations to vary the number of trainable parameters. The difference among them is only the LoRA dimensions in the search space which varies from 8 and 4 in the largest model, 4 and 2 in the intermediate model, and 2 in the smallest model. This added parameter flexibility in our approach allows for user-defined trainable parameter count in the final models. Results on the VTAB-1K benchmark are shown in Table 1. We push the state-of-the- art in parameter-efficient transfer learning by up to 2.9%. Impressively, our smallest model already surpasses all existing approaches by a significant margin. It is worth noting that GLoRA performs competitively across datasets, in contrast, prior all existing works tend to fail on at least one, proving GLoRA\u2019s high generalization capabilities. GLoRA pushes the state of the art in as many as 14 out of 19 datasets under VTAB-1K while maintaining commendable performance on the others. Table 1: Full results on VTAB-1K benchmark. \u201c# params\u201d specifies the number of trainable pa- rameters in backbones. Average accuracy and # params are averaged over group-wise mean values. Natural Specialized Structured ) M ( m a r a p # t s o C e c n e r e f n I 0 0 1 r a f i C 1 0 1 h c e t l a C D T D 2 0 1 r e w o l F s t e P N H V S 7 9 3 n u S n o y l e m a C T A S o r u E 5 4 c s i s e R y h t a p o n i t e R t n u o C r v e l C t s i D r v e l C b a L M D t s i D I T T I K c o L - r p S d i r O r p S d m i z A B R O N s Traditional Finetuning Full Linear PEFT methods BitFit 0.10 VPT-Shallow 0.06 0.53 VPT-Deep 0.16 Adapter 0.16 AdaptFormer 0.29 LoRA 0.36 NOAH 0.07 FacT 0.24 SSF 0.22 RepAdapter GLoRA 0.86 GLoRA 0.44 GLoRA 0.29 85.8 0 - \u2191 \u2191 \u2191 \u2191 - \u2191 - - - - - - 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.9 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.6 72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 65.2 77.7 86.9 62.6 97.5 87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 67.8 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 72.0 69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 73.9 70.8 91.2 70.5 99.1 90.9 86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3 76.3 45.7 31.7 41.1 74.7 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1"}, {"question": " According to learning theory, what does a higher VC dimension imply?", "answer": " A higher VC dimension implies greater model flexibility and capability of the approach.", "ref_chunk": "problem context, where the hypothesis space Hi is a subset of Huni in our context. Huni encompasses all possible shattered scenarios of Hi. For the extreme case where the VC dimension dvc(Ho) (Ho is the difference set of Huni and Hi) is 0, the error \u03f5 will be zero. As per learning theory, a higher VC dimension implies greater model flexibility and capability of our approach. Clearly, Theorem 1 holds for GLoRA and thus it experiences a greater model capacity. 3 EXPERIMENTS Datasets. We thoroughly evaluate GLoRA on VTAB-1K (Zhai et al., 2020) benchmark for various parameter budgets. VTAB-1K comprises 19 image classification tasks clustered into three domains: (i) Natural images; (ii) Specialized tasks consisting of remote sensing and medical datasets; and 5 (12) (13) (iii) Structured tasks focusing on scene structure understanding. To examine the ability on few-shot learning, we evaluate GLoRA on five fine-grained visual recognition few-shot datasets: Food101 (Bossard et al., 2014), OxfordFlowers102 (Nilsback & Zisserman, 2006), StandfordCars (Krause et al., 2013), OxfordPets (Parkhi et al., 2012), and FGVCAircraft (Maji et al., 2013). Following previous work (Jie & Deng, 2022), we evaluate 1, 2, 4, 8, and 16-shot settings. Next, to show the domain generalization capabilities of GLoRA, we train it on ImageNet (Deng et al., 2009b) for a 16-shot setting and test on four out-of-domain datasets including ImageNetV2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet- R (Hendrycks et al., 2021a). Finally, we show the performance of GLoRA on the Open LLM Leaderboard which consists of four datasets with varying prompt shots, namely AI2 Reasoning Challenge (25-shot) (Clark et al., 2018), TruthfulQA (0-shot) (Lin et al., 2022), HellaSwag (10- shot) (Zellers et al., 2019) and MMLU (5-shot) (Hendrycks et al., 2020). Network Architecture and Implementation Details. For all the vision experiments, we utilize ViT-B (Dosovitskiy et al., 2021), a model pre-trained on ImageNet-21K, as our foundational model. For the language experiments, we consider two foundational base models: LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b). Our supernets undergo a training process spanning 500 epochs and 15 epochs for vision and lan- guage datasets respectively, operating with a constant batch size of 64 and a cosine learning rate scheduler. It is crucial to highlight that this precise policy demonstrates robust efficacy across all settings, regardless of the dataset in use. Post the training of supernet, we perform an evolutionary search on the validation set to pinpoint the optimal task-specific subnet, finalized for implemen- tation. Finally, we report the performance of the searched subnet on the test set. In Appendix, we provide further insights into dataset-specific learning rates and specific settings for different datasets. 3.1 RESULTS ON VTAB-1K We train three different GLoRA supernet configurations to vary the number of trainable parameters. The difference among them is only the LoRA dimensions in the search space which varies from 8 and 4 in the largest model, 4 and 2 in the intermediate model, and 2 in the smallest model. This added parameter flexibility in our approach allows for user-defined trainable parameter count in the final models. Results on the VTAB-1K benchmark are shown in Table 1. We push the state-of-the- art in parameter-efficient transfer learning by up to 2.9%. Impressively, our smallest model already surpasses all existing approaches by a significant margin. It is worth noting that GLoRA performs competitively across datasets, in contrast, prior all existing works tend to fail on at least one, proving GLoRA\u2019s high generalization capabilities. GLoRA pushes the state of the art in as many as 14 out of 19 datasets under VTAB-1K while maintaining commendable performance on the others. Table 1: Full results on VTAB-1K benchmark. \u201c# params\u201d specifies the number of trainable pa- rameters in backbones. Average accuracy and # params are averaged over group-wise mean values. Natural Specialized Structured ) M ( m a r a p # t s o C e c n e r e f n I 0 0 1 r a f i C 1 0 1 h c e t l a C D T D 2 0 1 r e w o l F s t e P N H V S 7 9 3 n u S n o y l e m a C T A S o r u E 5 4 c s i s e R y h t a p o n i t e R t n u o C r v e l C t s i D r v e l C b a L M D t s i D I T T I K c o L - r p S d i r O r p S d m i z A B R O N s Traditional Finetuning Full Linear PEFT methods BitFit 0.10 VPT-Shallow 0.06 0.53 VPT-Deep 0.16 Adapter 0.16 AdaptFormer 0.29 LoRA 0.36 NOAH 0.07 FacT 0.24 SSF 0.22 RepAdapter GLoRA 0.86 GLoRA 0.44 GLoRA 0.29 85.8 0 - \u2191 \u2191 \u2191 \u2191 - \u2191 - - - - - - 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.9 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.6 72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 65.2 77.7 86.9 62.6 97.5 87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 67.8 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 72.0 69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 73.9 70.8 91.2 70.5 99.1 90.9 86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3 76.3 45.7 31.7 41.1 74.7 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1"}, {"question": " Which datasets are evaluated in GLoRA on VTAB-1K benchmark?", "answer": " GLoRA is evaluated on VTAB-1K benchmark comprising 19 image classification tasks clustered into three domains: Natural images, Specialized tasks, and Structured tasks.", "ref_chunk": "problem context, where the hypothesis space Hi is a subset of Huni in our context. Huni encompasses all possible shattered scenarios of Hi. For the extreme case where the VC dimension dvc(Ho) (Ho is the difference set of Huni and Hi) is 0, the error \u03f5 will be zero. As per learning theory, a higher VC dimension implies greater model flexibility and capability of our approach. Clearly, Theorem 1 holds for GLoRA and thus it experiences a greater model capacity. 3 EXPERIMENTS Datasets. We thoroughly evaluate GLoRA on VTAB-1K (Zhai et al., 2020) benchmark for various parameter budgets. VTAB-1K comprises 19 image classification tasks clustered into three domains: (i) Natural images; (ii) Specialized tasks consisting of remote sensing and medical datasets; and 5 (12) (13) (iii) Structured tasks focusing on scene structure understanding. To examine the ability on few-shot learning, we evaluate GLoRA on five fine-grained visual recognition few-shot datasets: Food101 (Bossard et al., 2014), OxfordFlowers102 (Nilsback & Zisserman, 2006), StandfordCars (Krause et al., 2013), OxfordPets (Parkhi et al., 2012), and FGVCAircraft (Maji et al., 2013). Following previous work (Jie & Deng, 2022), we evaluate 1, 2, 4, 8, and 16-shot settings. Next, to show the domain generalization capabilities of GLoRA, we train it on ImageNet (Deng et al., 2009b) for a 16-shot setting and test on four out-of-domain datasets including ImageNetV2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet- R (Hendrycks et al., 2021a). Finally, we show the performance of GLoRA on the Open LLM Leaderboard which consists of four datasets with varying prompt shots, namely AI2 Reasoning Challenge (25-shot) (Clark et al., 2018), TruthfulQA (0-shot) (Lin et al., 2022), HellaSwag (10- shot) (Zellers et al., 2019) and MMLU (5-shot) (Hendrycks et al., 2020). Network Architecture and Implementation Details. For all the vision experiments, we utilize ViT-B (Dosovitskiy et al., 2021), a model pre-trained on ImageNet-21K, as our foundational model. For the language experiments, we consider two foundational base models: LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b). Our supernets undergo a training process spanning 500 epochs and 15 epochs for vision and lan- guage datasets respectively, operating with a constant batch size of 64 and a cosine learning rate scheduler. It is crucial to highlight that this precise policy demonstrates robust efficacy across all settings, regardless of the dataset in use. Post the training of supernet, we perform an evolutionary search on the validation set to pinpoint the optimal task-specific subnet, finalized for implemen- tation. Finally, we report the performance of the searched subnet on the test set. In Appendix, we provide further insights into dataset-specific learning rates and specific settings for different datasets. 3.1 RESULTS ON VTAB-1K We train three different GLoRA supernet configurations to vary the number of trainable parameters. The difference among them is only the LoRA dimensions in the search space which varies from 8 and 4 in the largest model, 4 and 2 in the intermediate model, and 2 in the smallest model. This added parameter flexibility in our approach allows for user-defined trainable parameter count in the final models. Results on the VTAB-1K benchmark are shown in Table 1. We push the state-of-the- art in parameter-efficient transfer learning by up to 2.9%. Impressively, our smallest model already surpasses all existing approaches by a significant margin. It is worth noting that GLoRA performs competitively across datasets, in contrast, prior all existing works tend to fail on at least one, proving GLoRA\u2019s high generalization capabilities. GLoRA pushes the state of the art in as many as 14 out of 19 datasets under VTAB-1K while maintaining commendable performance on the others. Table 1: Full results on VTAB-1K benchmark. \u201c# params\u201d specifies the number of trainable pa- rameters in backbones. Average accuracy and # params are averaged over group-wise mean values. Natural Specialized Structured ) M ( m a r a p # t s o C e c n e r e f n I 0 0 1 r a f i C 1 0 1 h c e t l a C D T D 2 0 1 r e w o l F s t e P N H V S 7 9 3 n u S n o y l e m a C T A S o r u E 5 4 c s i s e R y h t a p o n i t e R t n u o C r v e l C t s i D r v e l C b a L M D t s i D I T T I K c o L - r p S d i r O r p S d m i z A B R O N s Traditional Finetuning Full Linear PEFT methods BitFit 0.10 VPT-Shallow 0.06 0.53 VPT-Deep 0.16 Adapter 0.16 AdaptFormer 0.29 LoRA 0.36 NOAH 0.07 FacT 0.24 SSF 0.22 RepAdapter GLoRA 0.86 GLoRA 0.44 GLoRA 0.29 85.8 0 - \u2191 \u2191 \u2191 \u2191 - \u2191 - - - - - - 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.9 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.6 72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 65.2 77.7 86.9 62.6 97.5 87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 67.8 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 72.0 69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 73.9 70.8 91.2 70.5 99.1 90.9 86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3 76.3 45.7 31.7 41.1 74.7 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1"}, {"question": " Which datasets are used to evaluate GLoRA on fine-grained visual recognition few-shot datasets?", "answer": " GLoRA is evaluated on five fine-grained visual recognition few-shot datasets: Food101, OxfordFlowers102, StandfordCars, OxfordPets, and FGVCAircraft.", "ref_chunk": "problem context, where the hypothesis space Hi is a subset of Huni in our context. Huni encompasses all possible shattered scenarios of Hi. For the extreme case where the VC dimension dvc(Ho) (Ho is the difference set of Huni and Hi) is 0, the error \u03f5 will be zero. As per learning theory, a higher VC dimension implies greater model flexibility and capability of our approach. Clearly, Theorem 1 holds for GLoRA and thus it experiences a greater model capacity. 3 EXPERIMENTS Datasets. We thoroughly evaluate GLoRA on VTAB-1K (Zhai et al., 2020) benchmark for various parameter budgets. VTAB-1K comprises 19 image classification tasks clustered into three domains: (i) Natural images; (ii) Specialized tasks consisting of remote sensing and medical datasets; and 5 (12) (13) (iii) Structured tasks focusing on scene structure understanding. To examine the ability on few-shot learning, we evaluate GLoRA on five fine-grained visual recognition few-shot datasets: Food101 (Bossard et al., 2014), OxfordFlowers102 (Nilsback & Zisserman, 2006), StandfordCars (Krause et al., 2013), OxfordPets (Parkhi et al., 2012), and FGVCAircraft (Maji et al., 2013). Following previous work (Jie & Deng, 2022), we evaluate 1, 2, 4, 8, and 16-shot settings. Next, to show the domain generalization capabilities of GLoRA, we train it on ImageNet (Deng et al., 2009b) for a 16-shot setting and test on four out-of-domain datasets including ImageNetV2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet- R (Hendrycks et al., 2021a). Finally, we show the performance of GLoRA on the Open LLM Leaderboard which consists of four datasets with varying prompt shots, namely AI2 Reasoning Challenge (25-shot) (Clark et al., 2018), TruthfulQA (0-shot) (Lin et al., 2022), HellaSwag (10- shot) (Zellers et al., 2019) and MMLU (5-shot) (Hendrycks et al., 2020). Network Architecture and Implementation Details. For all the vision experiments, we utilize ViT-B (Dosovitskiy et al., 2021), a model pre-trained on ImageNet-21K, as our foundational model. For the language experiments, we consider two foundational base models: LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b). Our supernets undergo a training process spanning 500 epochs and 15 epochs for vision and lan- guage datasets respectively, operating with a constant batch size of 64 and a cosine learning rate scheduler. It is crucial to highlight that this precise policy demonstrates robust efficacy across all settings, regardless of the dataset in use. Post the training of supernet, we perform an evolutionary search on the validation set to pinpoint the optimal task-specific subnet, finalized for implemen- tation. Finally, we report the performance of the searched subnet on the test set. In Appendix, we provide further insights into dataset-specific learning rates and specific settings for different datasets. 3.1 RESULTS ON VTAB-1K We train three different GLoRA supernet configurations to vary the number of trainable parameters. The difference among them is only the LoRA dimensions in the search space which varies from 8 and 4 in the largest model, 4 and 2 in the intermediate model, and 2 in the smallest model. This added parameter flexibility in our approach allows for user-defined trainable parameter count in the final models. Results on the VTAB-1K benchmark are shown in Table 1. We push the state-of-the- art in parameter-efficient transfer learning by up to 2.9%. Impressively, our smallest model already surpasses all existing approaches by a significant margin. It is worth noting that GLoRA performs competitively across datasets, in contrast, prior all existing works tend to fail on at least one, proving GLoRA\u2019s high generalization capabilities. GLoRA pushes the state of the art in as many as 14 out of 19 datasets under VTAB-1K while maintaining commendable performance on the others. Table 1: Full results on VTAB-1K benchmark. \u201c# params\u201d specifies the number of trainable pa- rameters in backbones. Average accuracy and # params are averaged over group-wise mean values. Natural Specialized Structured ) M ( m a r a p # t s o C e c n e r e f n I 0 0 1 r a f i C 1 0 1 h c e t l a C D T D 2 0 1 r e w o l F s t e P N H V S 7 9 3 n u S n o y l e m a C T A S o r u E 5 4 c s i s e R y h t a p o n i t e R t n u o C r v e l C t s i D r v e l C b a L M D t s i D I T T I K c o L - r p S d i r O r p S d m i z A B R O N s Traditional Finetuning Full Linear PEFT methods BitFit 0.10 VPT-Shallow 0.06 0.53 VPT-Deep 0.16 Adapter 0.16 AdaptFormer 0.29 LoRA 0.36 NOAH 0.07 FacT 0.24 SSF 0.22 RepAdapter GLoRA 0.86 GLoRA 0.44 GLoRA 0.29 85.8 0 - \u2191 \u2191 \u2191 \u2191 - \u2191 - - - - - - 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.9 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.6 72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 65.2 77.7 86.9 62.6 97.5 87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 67.8 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 72.0 69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 73.9 70.8 91.2 70.5 99.1 90.9 86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3 76.3 45.7 31.7 41.1 74.7 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1"}, {"question": " What models are utilized in vision experiments for GLoRA?", "answer": " ViT-B model pre-trained on ImageNet-21K is utilized for vision experiments in GLoRA.", "ref_chunk": "problem context, where the hypothesis space Hi is a subset of Huni in our context. Huni encompasses all possible shattered scenarios of Hi. For the extreme case where the VC dimension dvc(Ho) (Ho is the difference set of Huni and Hi) is 0, the error \u03f5 will be zero. As per learning theory, a higher VC dimension implies greater model flexibility and capability of our approach. Clearly, Theorem 1 holds for GLoRA and thus it experiences a greater model capacity. 3 EXPERIMENTS Datasets. We thoroughly evaluate GLoRA on VTAB-1K (Zhai et al., 2020) benchmark for various parameter budgets. VTAB-1K comprises 19 image classification tasks clustered into three domains: (i) Natural images; (ii) Specialized tasks consisting of remote sensing and medical datasets; and 5 (12) (13) (iii) Structured tasks focusing on scene structure understanding. To examine the ability on few-shot learning, we evaluate GLoRA on five fine-grained visual recognition few-shot datasets: Food101 (Bossard et al., 2014), OxfordFlowers102 (Nilsback & Zisserman, 2006), StandfordCars (Krause et al., 2013), OxfordPets (Parkhi et al., 2012), and FGVCAircraft (Maji et al., 2013). Following previous work (Jie & Deng, 2022), we evaluate 1, 2, 4, 8, and 16-shot settings. Next, to show the domain generalization capabilities of GLoRA, we train it on ImageNet (Deng et al., 2009b) for a 16-shot setting and test on four out-of-domain datasets including ImageNetV2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet- R (Hendrycks et al., 2021a). Finally, we show the performance of GLoRA on the Open LLM Leaderboard which consists of four datasets with varying prompt shots, namely AI2 Reasoning Challenge (25-shot) (Clark et al., 2018), TruthfulQA (0-shot) (Lin et al., 2022), HellaSwag (10- shot) (Zellers et al., 2019) and MMLU (5-shot) (Hendrycks et al., 2020). Network Architecture and Implementation Details. For all the vision experiments, we utilize ViT-B (Dosovitskiy et al., 2021), a model pre-trained on ImageNet-21K, as our foundational model. For the language experiments, we consider two foundational base models: LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b). Our supernets undergo a training process spanning 500 epochs and 15 epochs for vision and lan- guage datasets respectively, operating with a constant batch size of 64 and a cosine learning rate scheduler. It is crucial to highlight that this precise policy demonstrates robust efficacy across all settings, regardless of the dataset in use. Post the training of supernet, we perform an evolutionary search on the validation set to pinpoint the optimal task-specific subnet, finalized for implemen- tation. Finally, we report the performance of the searched subnet on the test set. In Appendix, we provide further insights into dataset-specific learning rates and specific settings for different datasets. 3.1 RESULTS ON VTAB-1K We train three different GLoRA supernet configurations to vary the number of trainable parameters. The difference among them is only the LoRA dimensions in the search space which varies from 8 and 4 in the largest model, 4 and 2 in the intermediate model, and 2 in the smallest model. This added parameter flexibility in our approach allows for user-defined trainable parameter count in the final models. Results on the VTAB-1K benchmark are shown in Table 1. We push the state-of-the- art in parameter-efficient transfer learning by up to 2.9%. Impressively, our smallest model already surpasses all existing approaches by a significant margin. It is worth noting that GLoRA performs competitively across datasets, in contrast, prior all existing works tend to fail on at least one, proving GLoRA\u2019s high generalization capabilities. GLoRA pushes the state of the art in as many as 14 out of 19 datasets under VTAB-1K while maintaining commendable performance on the others. Table 1: Full results on VTAB-1K benchmark. \u201c# params\u201d specifies the number of trainable pa- rameters in backbones. Average accuracy and # params are averaged over group-wise mean values. Natural Specialized Structured ) M ( m a r a p # t s o C e c n e r e f n I 0 0 1 r a f i C 1 0 1 h c e t l a C D T D 2 0 1 r e w o l F s t e P N H V S 7 9 3 n u S n o y l e m a C T A S o r u E 5 4 c s i s e R y h t a p o n i t e R t n u o C r v e l C t s i D r v e l C b a L M D t s i D I T T I K c o L - r p S d i r O r p S d m i z A B R O N s Traditional Finetuning Full Linear PEFT methods BitFit 0.10 VPT-Shallow 0.06 0.53 VPT-Deep 0.16 Adapter 0.16 AdaptFormer 0.29 LoRA 0.36 NOAH 0.07 FacT 0.24 SSF 0.22 RepAdapter GLoRA 0.86 GLoRA 0.44 GLoRA 0.29 85.8 0 - \u2191 \u2191 \u2191 \u2191 - \u2191 - - - - - - 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.9 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.6 72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 65.2 77.7 86.9 62.6 97.5 87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 67.8 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 72.0 69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 73.9 70.8 91.2 70.5 99.1 90.9 86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3 76.3 45.7 31.7 41.1 74.7 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1"}, {"question": " How many trainable parameters are there in the largest GLoRA model configuration?", "answer": " The largest GLoRA model configuration has 8 and 4 LoRA dimensions in the search space.", "ref_chunk": "problem context, where the hypothesis space Hi is a subset of Huni in our context. Huni encompasses all possible shattered scenarios of Hi. For the extreme case where the VC dimension dvc(Ho) (Ho is the difference set of Huni and Hi) is 0, the error \u03f5 will be zero. As per learning theory, a higher VC dimension implies greater model flexibility and capability of our approach. Clearly, Theorem 1 holds for GLoRA and thus it experiences a greater model capacity. 3 EXPERIMENTS Datasets. We thoroughly evaluate GLoRA on VTAB-1K (Zhai et al., 2020) benchmark for various parameter budgets. VTAB-1K comprises 19 image classification tasks clustered into three domains: (i) Natural images; (ii) Specialized tasks consisting of remote sensing and medical datasets; and 5 (12) (13) (iii) Structured tasks focusing on scene structure understanding. To examine the ability on few-shot learning, we evaluate GLoRA on five fine-grained visual recognition few-shot datasets: Food101 (Bossard et al., 2014), OxfordFlowers102 (Nilsback & Zisserman, 2006), StandfordCars (Krause et al., 2013), OxfordPets (Parkhi et al., 2012), and FGVCAircraft (Maji et al., 2013). Following previous work (Jie & Deng, 2022), we evaluate 1, 2, 4, 8, and 16-shot settings. Next, to show the domain generalization capabilities of GLoRA, we train it on ImageNet (Deng et al., 2009b) for a 16-shot setting and test on four out-of-domain datasets including ImageNetV2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet- R (Hendrycks et al., 2021a). Finally, we show the performance of GLoRA on the Open LLM Leaderboard which consists of four datasets with varying prompt shots, namely AI2 Reasoning Challenge (25-shot) (Clark et al., 2018), TruthfulQA (0-shot) (Lin et al., 2022), HellaSwag (10- shot) (Zellers et al., 2019) and MMLU (5-shot) (Hendrycks et al., 2020). Network Architecture and Implementation Details. For all the vision experiments, we utilize ViT-B (Dosovitskiy et al., 2021), a model pre-trained on ImageNet-21K, as our foundational model. For the language experiments, we consider two foundational base models: LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b). Our supernets undergo a training process spanning 500 epochs and 15 epochs for vision and lan- guage datasets respectively, operating with a constant batch size of 64 and a cosine learning rate scheduler. It is crucial to highlight that this precise policy demonstrates robust efficacy across all settings, regardless of the dataset in use. Post the training of supernet, we perform an evolutionary search on the validation set to pinpoint the optimal task-specific subnet, finalized for implemen- tation. Finally, we report the performance of the searched subnet on the test set. In Appendix, we provide further insights into dataset-specific learning rates and specific settings for different datasets. 3.1 RESULTS ON VTAB-1K We train three different GLoRA supernet configurations to vary the number of trainable parameters. The difference among them is only the LoRA dimensions in the search space which varies from 8 and 4 in the largest model, 4 and 2 in the intermediate model, and 2 in the smallest model. This added parameter flexibility in our approach allows for user-defined trainable parameter count in the final models. Results on the VTAB-1K benchmark are shown in Table 1. We push the state-of-the- art in parameter-efficient transfer learning by up to 2.9%. Impressively, our smallest model already surpasses all existing approaches by a significant margin. It is worth noting that GLoRA performs competitively across datasets, in contrast, prior all existing works tend to fail on at least one, proving GLoRA\u2019s high generalization capabilities. GLoRA pushes the state of the art in as many as 14 out of 19 datasets under VTAB-1K while maintaining commendable performance on the others. Table 1: Full results on VTAB-1K benchmark. \u201c# params\u201d specifies the number of trainable pa- rameters in backbones. Average accuracy and # params are averaged over group-wise mean values. Natural Specialized Structured ) M ( m a r a p # t s o C e c n e r e f n I 0 0 1 r a f i C 1 0 1 h c e t l a C D T D 2 0 1 r e w o l F s t e P N H V S 7 9 3 n u S n o y l e m a C T A S o r u E 5 4 c s i s e R y h t a p o n i t e R t n u o C r v e l C t s i D r v e l C b a L M D t s i D I T T I K c o L - r p S d i r O r p S d m i z A B R O N s Traditional Finetuning Full Linear PEFT methods BitFit 0.10 VPT-Shallow 0.06 0.53 VPT-Deep 0.16 Adapter 0.16 AdaptFormer 0.29 LoRA 0.36 NOAH 0.07 FacT 0.24 SSF 0.22 RepAdapter GLoRA 0.86 GLoRA 0.44 GLoRA 0.29 85.8 0 - \u2191 \u2191 \u2191 \u2191 - \u2191 - - - - - - 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.9 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.6 72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 65.2 77.7 86.9 62.6 97.5 87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 67.8 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 72.0 69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 73.9 70.8 91.2 70.5 99.1 90.9 86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3 76.3 45.7 31.7 41.1 74.7 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1"}, {"question": " What is the significance of the added parameter flexibility in GLoRA approach?", "answer": " The added parameter flexibility allows for user-defined trainable parameter count in the final models.", "ref_chunk": "problem context, where the hypothesis space Hi is a subset of Huni in our context. Huni encompasses all possible shattered scenarios of Hi. For the extreme case where the VC dimension dvc(Ho) (Ho is the difference set of Huni and Hi) is 0, the error \u03f5 will be zero. As per learning theory, a higher VC dimension implies greater model flexibility and capability of our approach. Clearly, Theorem 1 holds for GLoRA and thus it experiences a greater model capacity. 3 EXPERIMENTS Datasets. We thoroughly evaluate GLoRA on VTAB-1K (Zhai et al., 2020) benchmark for various parameter budgets. VTAB-1K comprises 19 image classification tasks clustered into three domains: (i) Natural images; (ii) Specialized tasks consisting of remote sensing and medical datasets; and 5 (12) (13) (iii) Structured tasks focusing on scene structure understanding. To examine the ability on few-shot learning, we evaluate GLoRA on five fine-grained visual recognition few-shot datasets: Food101 (Bossard et al., 2014), OxfordFlowers102 (Nilsback & Zisserman, 2006), StandfordCars (Krause et al., 2013), OxfordPets (Parkhi et al., 2012), and FGVCAircraft (Maji et al., 2013). Following previous work (Jie & Deng, 2022), we evaluate 1, 2, 4, 8, and 16-shot settings. Next, to show the domain generalization capabilities of GLoRA, we train it on ImageNet (Deng et al., 2009b) for a 16-shot setting and test on four out-of-domain datasets including ImageNetV2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet- R (Hendrycks et al., 2021a). Finally, we show the performance of GLoRA on the Open LLM Leaderboard which consists of four datasets with varying prompt shots, namely AI2 Reasoning Challenge (25-shot) (Clark et al., 2018), TruthfulQA (0-shot) (Lin et al., 2022), HellaSwag (10- shot) (Zellers et al., 2019) and MMLU (5-shot) (Hendrycks et al., 2020). Network Architecture and Implementation Details. For all the vision experiments, we utilize ViT-B (Dosovitskiy et al., 2021), a model pre-trained on ImageNet-21K, as our foundational model. For the language experiments, we consider two foundational base models: LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b). Our supernets undergo a training process spanning 500 epochs and 15 epochs for vision and lan- guage datasets respectively, operating with a constant batch size of 64 and a cosine learning rate scheduler. It is crucial to highlight that this precise policy demonstrates robust efficacy across all settings, regardless of the dataset in use. Post the training of supernet, we perform an evolutionary search on the validation set to pinpoint the optimal task-specific subnet, finalized for implemen- tation. Finally, we report the performance of the searched subnet on the test set. In Appendix, we provide further insights into dataset-specific learning rates and specific settings for different datasets. 3.1 RESULTS ON VTAB-1K We train three different GLoRA supernet configurations to vary the number of trainable parameters. The difference among them is only the LoRA dimensions in the search space which varies from 8 and 4 in the largest model, 4 and 2 in the intermediate model, and 2 in the smallest model. This added parameter flexibility in our approach allows for user-defined trainable parameter count in the final models. Results on the VTAB-1K benchmark are shown in Table 1. We push the state-of-the- art in parameter-efficient transfer learning by up to 2.9%. Impressively, our smallest model already surpasses all existing approaches by a significant margin. It is worth noting that GLoRA performs competitively across datasets, in contrast, prior all existing works tend to fail on at least one, proving GLoRA\u2019s high generalization capabilities. GLoRA pushes the state of the art in as many as 14 out of 19 datasets under VTAB-1K while maintaining commendable performance on the others. Table 1: Full results on VTAB-1K benchmark. \u201c# params\u201d specifies the number of trainable pa- rameters in backbones. Average accuracy and # params are averaged over group-wise mean values. Natural Specialized Structured ) M ( m a r a p # t s o C e c n e r e f n I 0 0 1 r a f i C 1 0 1 h c e t l a C D T D 2 0 1 r e w o l F s t e P N H V S 7 9 3 n u S n o y l e m a C T A S o r u E 5 4 c s i s e R y h t a p o n i t e R t n u o C r v e l C t s i D r v e l C b a L M D t s i D I T T I K c o L - r p S d i r O r p S d m i z A B R O N s Traditional Finetuning Full Linear PEFT methods BitFit 0.10 VPT-Shallow 0.06 0.53 VPT-Deep 0.16 Adapter 0.16 AdaptFormer 0.29 LoRA 0.36 NOAH 0.07 FacT 0.24 SSF 0.22 RepAdapter GLoRA 0.86 GLoRA 0.44 GLoRA 0.29 85.8 0 - \u2191 \u2191 \u2191 \u2191 - \u2191 - - - - - - 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.9 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.6 72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 65.2 77.7 86.9 62.6 97.5 87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 67.8 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 72.0 69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 73.9 70.8 91.2 70.5 99.1 90.9 86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3 76.3 45.7 31.7 41.1 74.7 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1"}, {"question": " In which datasets does GLoRA perform competitively according to the text?", "answer": " GLoRA performs competitively across datasets in contrast to prior works and tends to fail on at least one dataset.", "ref_chunk": "problem context, where the hypothesis space Hi is a subset of Huni in our context. Huni encompasses all possible shattered scenarios of Hi. For the extreme case where the VC dimension dvc(Ho) (Ho is the difference set of Huni and Hi) is 0, the error \u03f5 will be zero. As per learning theory, a higher VC dimension implies greater model flexibility and capability of our approach. Clearly, Theorem 1 holds for GLoRA and thus it experiences a greater model capacity. 3 EXPERIMENTS Datasets. We thoroughly evaluate GLoRA on VTAB-1K (Zhai et al., 2020) benchmark for various parameter budgets. VTAB-1K comprises 19 image classification tasks clustered into three domains: (i) Natural images; (ii) Specialized tasks consisting of remote sensing and medical datasets; and 5 (12) (13) (iii) Structured tasks focusing on scene structure understanding. To examine the ability on few-shot learning, we evaluate GLoRA on five fine-grained visual recognition few-shot datasets: Food101 (Bossard et al., 2014), OxfordFlowers102 (Nilsback & Zisserman, 2006), StandfordCars (Krause et al., 2013), OxfordPets (Parkhi et al., 2012), and FGVCAircraft (Maji et al., 2013). Following previous work (Jie & Deng, 2022), we evaluate 1, 2, 4, 8, and 16-shot settings. Next, to show the domain generalization capabilities of GLoRA, we train it on ImageNet (Deng et al., 2009b) for a 16-shot setting and test on four out-of-domain datasets including ImageNetV2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet- R (Hendrycks et al., 2021a). Finally, we show the performance of GLoRA on the Open LLM Leaderboard which consists of four datasets with varying prompt shots, namely AI2 Reasoning Challenge (25-shot) (Clark et al., 2018), TruthfulQA (0-shot) (Lin et al., 2022), HellaSwag (10- shot) (Zellers et al., 2019) and MMLU (5-shot) (Hendrycks et al., 2020). Network Architecture and Implementation Details. For all the vision experiments, we utilize ViT-B (Dosovitskiy et al., 2021), a model pre-trained on ImageNet-21K, as our foundational model. For the language experiments, we consider two foundational base models: LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b). Our supernets undergo a training process spanning 500 epochs and 15 epochs for vision and lan- guage datasets respectively, operating with a constant batch size of 64 and a cosine learning rate scheduler. It is crucial to highlight that this precise policy demonstrates robust efficacy across all settings, regardless of the dataset in use. Post the training of supernet, we perform an evolutionary search on the validation set to pinpoint the optimal task-specific subnet, finalized for implemen- tation. Finally, we report the performance of the searched subnet on the test set. In Appendix, we provide further insights into dataset-specific learning rates and specific settings for different datasets. 3.1 RESULTS ON VTAB-1K We train three different GLoRA supernet configurations to vary the number of trainable parameters. The difference among them is only the LoRA dimensions in the search space which varies from 8 and 4 in the largest model, 4 and 2 in the intermediate model, and 2 in the smallest model. This added parameter flexibility in our approach allows for user-defined trainable parameter count in the final models. Results on the VTAB-1K benchmark are shown in Table 1. We push the state-of-the- art in parameter-efficient transfer learning by up to 2.9%. Impressively, our smallest model already surpasses all existing approaches by a significant margin. It is worth noting that GLoRA performs competitively across datasets, in contrast, prior all existing works tend to fail on at least one, proving GLoRA\u2019s high generalization capabilities. GLoRA pushes the state of the art in as many as 14 out of 19 datasets under VTAB-1K while maintaining commendable performance on the others. Table 1: Full results on VTAB-1K benchmark. \u201c# params\u201d specifies the number of trainable pa- rameters in backbones. Average accuracy and # params are averaged over group-wise mean values. Natural Specialized Structured ) M ( m a r a p # t s o C e c n e r e f n I 0 0 1 r a f i C 1 0 1 h c e t l a C D T D 2 0 1 r e w o l F s t e P N H V S 7 9 3 n u S n o y l e m a C T A S o r u E 5 4 c s i s e R y h t a p o n i t e R t n u o C r v e l C t s i D r v e l C b a L M D t s i D I T T I K c o L - r p S d i r O r p S d m i z A B R O N s Traditional Finetuning Full Linear PEFT methods BitFit 0.10 VPT-Shallow 0.06 0.53 VPT-Deep 0.16 Adapter 0.16 AdaptFormer 0.29 LoRA 0.36 NOAH 0.07 FacT 0.24 SSF 0.22 RepAdapter GLoRA 0.86 GLoRA 0.44 GLoRA 0.29 85.8 0 - \u2191 \u2191 \u2191 \u2191 - \u2191 - - - - - - 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.9 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.6 72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 65.2 77.7 86.9 62.6 97.5 87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 67.8 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 72.0 69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 73.9 70.8 91.2 70.5 99.1 90.9 86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3 76.3 45.7 31.7 41.1 74.7 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1"}, {"question": " How many datasets does GLoRA push the state of the art in under VTAB-1K according to the text?", "answer": " GLoRA pushes the state of the art in as many as 14 out of 19 datasets under VTAB-1K.", "ref_chunk": "problem context, where the hypothesis space Hi is a subset of Huni in our context. Huni encompasses all possible shattered scenarios of Hi. For the extreme case where the VC dimension dvc(Ho) (Ho is the difference set of Huni and Hi) is 0, the error \u03f5 will be zero. As per learning theory, a higher VC dimension implies greater model flexibility and capability of our approach. Clearly, Theorem 1 holds for GLoRA and thus it experiences a greater model capacity. 3 EXPERIMENTS Datasets. We thoroughly evaluate GLoRA on VTAB-1K (Zhai et al., 2020) benchmark for various parameter budgets. VTAB-1K comprises 19 image classification tasks clustered into three domains: (i) Natural images; (ii) Specialized tasks consisting of remote sensing and medical datasets; and 5 (12) (13) (iii) Structured tasks focusing on scene structure understanding. To examine the ability on few-shot learning, we evaluate GLoRA on five fine-grained visual recognition few-shot datasets: Food101 (Bossard et al., 2014), OxfordFlowers102 (Nilsback & Zisserman, 2006), StandfordCars (Krause et al., 2013), OxfordPets (Parkhi et al., 2012), and FGVCAircraft (Maji et al., 2013). Following previous work (Jie & Deng, 2022), we evaluate 1, 2, 4, 8, and 16-shot settings. Next, to show the domain generalization capabilities of GLoRA, we train it on ImageNet (Deng et al., 2009b) for a 16-shot setting and test on four out-of-domain datasets including ImageNetV2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet- R (Hendrycks et al., 2021a). Finally, we show the performance of GLoRA on the Open LLM Leaderboard which consists of four datasets with varying prompt shots, namely AI2 Reasoning Challenge (25-shot) (Clark et al., 2018), TruthfulQA (0-shot) (Lin et al., 2022), HellaSwag (10- shot) (Zellers et al., 2019) and MMLU (5-shot) (Hendrycks et al., 2020). Network Architecture and Implementation Details. For all the vision experiments, we utilize ViT-B (Dosovitskiy et al., 2021), a model pre-trained on ImageNet-21K, as our foundational model. For the language experiments, we consider two foundational base models: LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b). Our supernets undergo a training process spanning 500 epochs and 15 epochs for vision and lan- guage datasets respectively, operating with a constant batch size of 64 and a cosine learning rate scheduler. It is crucial to highlight that this precise policy demonstrates robust efficacy across all settings, regardless of the dataset in use. Post the training of supernet, we perform an evolutionary search on the validation set to pinpoint the optimal task-specific subnet, finalized for implemen- tation. Finally, we report the performance of the searched subnet on the test set. In Appendix, we provide further insights into dataset-specific learning rates and specific settings for different datasets. 3.1 RESULTS ON VTAB-1K We train three different GLoRA supernet configurations to vary the number of trainable parameters. The difference among them is only the LoRA dimensions in the search space which varies from 8 and 4 in the largest model, 4 and 2 in the intermediate model, and 2 in the smallest model. This added parameter flexibility in our approach allows for user-defined trainable parameter count in the final models. Results on the VTAB-1K benchmark are shown in Table 1. We push the state-of-the- art in parameter-efficient transfer learning by up to 2.9%. Impressively, our smallest model already surpasses all existing approaches by a significant margin. It is worth noting that GLoRA performs competitively across datasets, in contrast, prior all existing works tend to fail on at least one, proving GLoRA\u2019s high generalization capabilities. GLoRA pushes the state of the art in as many as 14 out of 19 datasets under VTAB-1K while maintaining commendable performance on the others. Table 1: Full results on VTAB-1K benchmark. \u201c# params\u201d specifies the number of trainable pa- rameters in backbones. Average accuracy and # params are averaged over group-wise mean values. Natural Specialized Structured ) M ( m a r a p # t s o C e c n e r e f n I 0 0 1 r a f i C 1 0 1 h c e t l a C D T D 2 0 1 r e w o l F s t e P N H V S 7 9 3 n u S n o y l e m a C T A S o r u E 5 4 c s i s e R y h t a p o n i t e R t n u o C r v e l C t s i D r v e l C b a L M D t s i D I T T I K c o L - r p S d i r O r p S d m i z A B R O N s Traditional Finetuning Full Linear PEFT methods BitFit 0.10 VPT-Shallow 0.06 0.53 VPT-Deep 0.16 Adapter 0.16 AdaptFormer 0.29 LoRA 0.36 NOAH 0.07 FacT 0.24 SSF 0.22 RepAdapter GLoRA 0.86 GLoRA 0.44 GLoRA 0.29 85.8 0 - \u2191 \u2191 \u2191 \u2191 - \u2191 - - - - - - 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.9 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.6 72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 65.2 77.7 86.9 62.6 97.5 87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 67.8 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 72.0 69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 73.9 70.8 91.2 70.5 99.1 90.9 86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3 76.3 45.7 31.7 41.1 74.7 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1"}], "doc_text": "problem context, where the hypothesis space Hi is a subset of Huni in our context. Huni encompasses all possible shattered scenarios of Hi. For the extreme case where the VC dimension dvc(Ho) (Ho is the difference set of Huni and Hi) is 0, the error \u03f5 will be zero. As per learning theory, a higher VC dimension implies greater model flexibility and capability of our approach. Clearly, Theorem 1 holds for GLoRA and thus it experiences a greater model capacity. 3 EXPERIMENTS Datasets. We thoroughly evaluate GLoRA on VTAB-1K (Zhai et al., 2020) benchmark for various parameter budgets. VTAB-1K comprises 19 image classification tasks clustered into three domains: (i) Natural images; (ii) Specialized tasks consisting of remote sensing and medical datasets; and 5 (12) (13) (iii) Structured tasks focusing on scene structure understanding. To examine the ability on few-shot learning, we evaluate GLoRA on five fine-grained visual recognition few-shot datasets: Food101 (Bossard et al., 2014), OxfordFlowers102 (Nilsback & Zisserman, 2006), StandfordCars (Krause et al., 2013), OxfordPets (Parkhi et al., 2012), and FGVCAircraft (Maji et al., 2013). Following previous work (Jie & Deng, 2022), we evaluate 1, 2, 4, 8, and 16-shot settings. Next, to show the domain generalization capabilities of GLoRA, we train it on ImageNet (Deng et al., 2009b) for a 16-shot setting and test on four out-of-domain datasets including ImageNetV2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet- R (Hendrycks et al., 2021a). Finally, we show the performance of GLoRA on the Open LLM Leaderboard which consists of four datasets with varying prompt shots, namely AI2 Reasoning Challenge (25-shot) (Clark et al., 2018), TruthfulQA (0-shot) (Lin et al., 2022), HellaSwag (10- shot) (Zellers et al., 2019) and MMLU (5-shot) (Hendrycks et al., 2020). Network Architecture and Implementation Details. For all the vision experiments, we utilize ViT-B (Dosovitskiy et al., 2021), a model pre-trained on ImageNet-21K, as our foundational model. For the language experiments, we consider two foundational base models: LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b). Our supernets undergo a training process spanning 500 epochs and 15 epochs for vision and lan- guage datasets respectively, operating with a constant batch size of 64 and a cosine learning rate scheduler. It is crucial to highlight that this precise policy demonstrates robust efficacy across all settings, regardless of the dataset in use. Post the training of supernet, we perform an evolutionary search on the validation set to pinpoint the optimal task-specific subnet, finalized for implemen- tation. Finally, we report the performance of the searched subnet on the test set. In Appendix, we provide further insights into dataset-specific learning rates and specific settings for different datasets. 3.1 RESULTS ON VTAB-1K We train three different GLoRA supernet configurations to vary the number of trainable parameters. The difference among them is only the LoRA dimensions in the search space which varies from 8 and 4 in the largest model, 4 and 2 in the intermediate model, and 2 in the smallest model. This added parameter flexibility in our approach allows for user-defined trainable parameter count in the final models. Results on the VTAB-1K benchmark are shown in Table 1. We push the state-of-the- art in parameter-efficient transfer learning by up to 2.9%. Impressively, our smallest model already surpasses all existing approaches by a significant margin. It is worth noting that GLoRA performs competitively across datasets, in contrast, prior all existing works tend to fail on at least one, proving GLoRA\u2019s high generalization capabilities. GLoRA pushes the state of the art in as many as 14 out of 19 datasets under VTAB-1K while maintaining commendable performance on the others. Table 1: Full results on VTAB-1K benchmark. \u201c# params\u201d specifies the number of trainable pa- rameters in backbones. Average accuracy and # params are averaged over group-wise mean values. Natural Specialized Structured ) M ( m a r a p # t s o C e c n e r e f n I 0 0 1 r a f i C 1 0 1 h c e t l a C D T D 2 0 1 r e w o l F s t e P N H V S 7 9 3 n u S n o y l e m a C T A S o r u E 5 4 c s i s e R y h t a p o n i t e R t n u o C r v e l C t s i D r v e l C b a L M D t s i D I T T I K c o L - r p S d i r O r p S d m i z A B R O N s Traditional Finetuning Full Linear PEFT methods BitFit 0.10 VPT-Shallow 0.06 0.53 VPT-Deep 0.16 Adapter 0.16 AdaptFormer 0.29 LoRA 0.36 NOAH 0.07 FacT 0.24 SSF 0.22 RepAdapter GLoRA 0.86 GLoRA 0.44 GLoRA 0.29 85.8 0 - \u2191 \u2191 \u2191 \u2191 - \u2191 - - - - - - 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.9 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.6 72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 65.2 77.7 86.9 62.6 97.5 87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 67.8 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 72.0 69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 73.9 70.8 91.2 70.5 99.1 90.9 86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3 76.3 45.7 31.7 41.1 74.7 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1"}