{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daphne_Ippolito_Extracting_Training_Data_from_Diffusion_Models_chunk_16.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is used to find memorized training samples in the study?", "answer": " Normalized (cid:96)2 distance in pixel space", "ref_chunk": "and diffusion (DDPM) models. We use normalized (cid:96)2 distance in pixel space to \ufb01nd memorized training samples. In each pair of images, left and right image corresponds to real and it closely synthetic image. For StyleGAN-ADA and DDPM model we display 120 pairs with smallest normalized (cid:96)2 distance. For others we display all memorized training images. 1M generations 31"}, {"question": " How are real and synthetic images represented in the pairs of images?", "answer": " Left and right correspond to real and closely synthetic images", "ref_chunk": "and diffusion (DDPM) models. We use normalized (cid:96)2 distance in pixel space to \ufb01nd memorized training samples. In each pair of images, left and right image corresponds to real and it closely synthetic image. For StyleGAN-ADA and DDPM model we display 120 pairs with smallest normalized (cid:96)2 distance. For others we display all memorized training images. 1M generations 31"}, {"question": " How many pairs with the smallest normalized (cid:96)2 distance are displayed for StyleGAN-ADA and DDPM model?", "answer": " 120 pairs", "ref_chunk": "and diffusion (DDPM) models. We use normalized (cid:96)2 distance in pixel space to \ufb01nd memorized training samples. In each pair of images, left and right image corresponds to real and it closely synthetic image. For StyleGAN-ADA and DDPM model we display 120 pairs with smallest normalized (cid:96)2 distance. For others we display all memorized training images. 1M generations 31"}, {"question": " For which models are all memorized training images displayed?", "answer": " For others", "ref_chunk": "and diffusion (DDPM) models. We use normalized (cid:96)2 distance in pixel space to \ufb01nd memorized training samples. In each pair of images, left and right image corresponds to real and it closely synthetic image. For StyleGAN-ADA and DDPM model we display 120 pairs with smallest normalized (cid:96)2 distance. For others we display all memorized training images. 1M generations 31"}, {"question": " How many generations are mentioned in the study?", "answer": " 1 million (1M)", "ref_chunk": "and diffusion (DDPM) models. We use normalized (cid:96)2 distance in pixel space to \ufb01nd memorized training samples. In each pair of images, left and right image corresponds to real and it closely synthetic image. For StyleGAN-ADA and DDPM model we display 120 pairs with smallest normalized (cid:96)2 distance. For others we display all memorized training images. 1M generations 31"}, {"question": " What type of models are compared in the study?", "answer": " StyleGAN-ADA and DDPM models", "ref_chunk": "and diffusion (DDPM) models. We use normalized (cid:96)2 distance in pixel space to \ufb01nd memorized training samples. In each pair of images, left and right image corresponds to real and it closely synthetic image. For StyleGAN-ADA and DDPM model we display 120 pairs with smallest normalized (cid:96)2 distance. For others we display all memorized training images. 1M generations 31"}, {"question": " What distance metric is used to find memorized training samples?", "answer": " (cid:96)2 distance", "ref_chunk": "and diffusion (DDPM) models. We use normalized (cid:96)2 distance in pixel space to \ufb01nd memorized training samples. In each pair of images, left and right image corresponds to real and it closely synthetic image. For StyleGAN-ADA and DDPM model we display 120 pairs with smallest normalized (cid:96)2 distance. For others we display all memorized training images. 1M generations 31"}, {"question": " How are real and synthetic images differentiated in the displayed pairs?", "answer": " Left and right correspond to real and closely synthetic images", "ref_chunk": "and diffusion (DDPM) models. We use normalized (cid:96)2 distance in pixel space to \ufb01nd memorized training samples. In each pair of images, left and right image corresponds to real and it closely synthetic image. For StyleGAN-ADA and DDPM model we display 120 pairs with smallest normalized (cid:96)2 distance. For others we display all memorized training images. 1M generations 31"}, {"question": " How many pairs are displayed for StyleGAN-ADA and DDPM models?", "answer": " 120 pairs with smallest distance", "ref_chunk": "and diffusion (DDPM) models. We use normalized (cid:96)2 distance in pixel space to \ufb01nd memorized training samples. In each pair of images, left and right image corresponds to real and it closely synthetic image. For StyleGAN-ADA and DDPM model we display 120 pairs with smallest normalized (cid:96)2 distance. For others we display all memorized training images. 1M generations 31"}, {"question": " How many total pairs of images are displayed in the study?", "answer": " 120 pairs for StyleGAN-ADA and DDPM, all memorized images for others", "ref_chunk": "and diffusion (DDPM) models. We use normalized (cid:96)2 distance in pixel space to \ufb01nd memorized training samples. In each pair of images, left and right image corresponds to real and it closely synthetic image. For StyleGAN-ADA and DDPM model we display 120 pairs with smallest normalized (cid:96)2 distance. For others we display all memorized training images. 1M generations 31"}], "doc_text": "and diffusion (DDPM) models. We use normalized (cid:96)2 distance in pixel space to \ufb01nd memorized training samples. In each pair of images, left and right image corresponds to real and it closely synthetic image. For StyleGAN-ADA and DDPM model we display 120 pairs with smallest normalized (cid:96)2 distance. For others we display all memorized training images. 1M generations 31"}