{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yonatan_Bisk_The_Framework_Tax:_Disparities_Between_Inference_Efficiency_in_Research_and_Deployment_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the dimensions shown in Table 1 for the BERT PyTorch models on RTX-8000?", "answer": " 768, 768, 768, 1536, 3072, 3072, 3072, 6144", "ref_chunk": "is bot- SA Dim FC Dim Batch Seq Latency TP 768 768 768 1536 3072 3072 3072 6144 1 4 1 1 128 128 512 128 0.0136 0.0134 0.0134 0.0134 0.0136 0.0034 0.0134 0.0134 Table 1: Latency and throughput (TP) of BERT PyTorch models on RTX-8000. Scaling along batch sizes and model width shows no increase in latency. tlenecked by framework overhead, the batch size or input size can be increased without increases in overall runtime. In practice, this can lead to the pro- cessing of larger batch sizes and sequence lengths at no additional latency cost until kernel operations saturate framework overhead. Model width can be increased at no cost for framework-bound models. For a given batch size, individual layers of a framework bound model can be made wider by increasing hidden dimension or filter size without impacting latency. Wider mod- els are known to exhibit better task performance due to increased parameter count and expressivity (Zagoruyko and Komodakis, 2016). Model designers can leverage wider architec- tures with their target inference framework and hardware setting in mind to achieve higher utiliza- tion. For example, models processing few exam- ples during inference can leverage wider layers to avoid framework bottlenecks. See Table 1. Using higher-performing hardware does not necessarily improve end-to-end performance. Framework overhead limits the impact of improved hardware as it limits utilization. This trend will con- tinue as ML-specific hardware advances without ef- forts to address software bottlenecks. For example, single-example inference with both BERT is slower using an A100 than using a V100 GPU despite a 2.75x increase in peak computational throughput. 6 Conclusion We conduct an extensive study of neural networks from the convolutional and transformer architec- ture paradigms across a variety of software and hardware platforms. We show that inference per- formed with these large neural networks, which was previously assumed to be compute bounded, is in fact limited by overhead incurred by deep learning frameworks. While wider transformer ar- chitectures (e.g. BERT-Base) exhibit less bounded- ness behaviors than narrower, deeper CNNs (e.g. ResNet-50), we show that all models exhibit frame- work boundedness. Additionally, we observe that these inefficiencies are becoming more apparent as hardware accelerator speeds increase. We introduce the concept of the framework tax to describe when improvements in hardware speed and reductions in required computation fail to trans- late to speedups in model latency and throughput due to bottlenecks incurred by deep learning frame- works. We hope that these observations raise aware- ness of the impact and limitations created by choice of deep learning frameworks on model develop- ment and deployment. 7 Limitations In this work, we study the inference efficiency of speech and language models using GPU hardware accelerators. While GPUs are the most common general purpose hardware accelerators, there ex- ist domain specific architectures such as Google TPU\u2019s, GraphCore IPUs, and custom ASICs which present additional settings for future investigation. Additionally, our study evaluates efficiency via model latency and our claims do not necessarily translate to other metrics of efficiency, such as power consumption or power output. As models continue to scale in size, they often require model or data parallelism techniques that require computation across several nodes which introduce overhead from multi-device synchroniza- tion and network communication. Additionally, we do not study latency in the training setting where the per-layer computation is larger due to the com- putation of gradients and losses. Acknowledgements This work was supported in part by a grant from the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE2140739. We thank the anonymous reviewers for their valuable feedback. We would also like to thank lab members and faculty for helpful feed- back during discussions and revisions, including: Daniel Fried, Han Guo, Jeremiah Milbauer, Sanket Vaibhav Mehta, and Saujas Vaduguru. References Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fr\u00e9d\u00e9ric Bastien, Justin Bayer, Anatoly Be- likov, Alexander Belopolsky, et al. 2016. Theano: A python framework for fast computation of mathemat- ical expressions. arXiv e-prints, pages arXiv\u20131605. Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. 2022. Revisiting neural scaling laws in language and vision. Advances in Neural Informa- tion Processing Systems, 35:22300\u201322312. Reza Yazdani Aminabadi, Samyam Rajbhandari, Am- mar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeed-inference: enabling efficient inference of transformer models at unprece- In Proceedings of the International dented scale. Conference on High Performance Computing, Net- working, Storage and Analysis, pages 1\u201315. Jeff Barr. 2019. Amazon ec2 update-infl instances with aws inferentia chips for high performance cost- effective inferencing. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. Han Cai, Ligeng Zhu, and Song Han. 2018. Proxy- lessnas: Direct neural architecture search on target task and hardware. In International Conference on Learning Representations. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Se- lected Topics in Signal Processing, 16(6):1505\u20131518. Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexi- ble and efficient machine learning library for het- arXiv preprint erogeneous distributed systems. arXiv:1512.01274. Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Sympo- sium on Operating Systems Design and Implementa- tion (OSDI 18), pages 578\u2013594. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Ad-"}, {"question": " According to the text, can the batch size or input size be increased without increases in overall runtime for framework-bound models?", "answer": " Yes, the batch size or input size can be increased without increases in overall runtime for framework-bound models.", "ref_chunk": "is bot- SA Dim FC Dim Batch Seq Latency TP 768 768 768 1536 3072 3072 3072 6144 1 4 1 1 128 128 512 128 0.0136 0.0134 0.0134 0.0134 0.0136 0.0034 0.0134 0.0134 Table 1: Latency and throughput (TP) of BERT PyTorch models on RTX-8000. Scaling along batch sizes and model width shows no increase in latency. tlenecked by framework overhead, the batch size or input size can be increased without increases in overall runtime. In practice, this can lead to the pro- cessing of larger batch sizes and sequence lengths at no additional latency cost until kernel operations saturate framework overhead. Model width can be increased at no cost for framework-bound models. For a given batch size, individual layers of a framework bound model can be made wider by increasing hidden dimension or filter size without impacting latency. Wider mod- els are known to exhibit better task performance due to increased parameter count and expressivity (Zagoruyko and Komodakis, 2016). Model designers can leverage wider architec- tures with their target inference framework and hardware setting in mind to achieve higher utiliza- tion. For example, models processing few exam- ples during inference can leverage wider layers to avoid framework bottlenecks. See Table 1. Using higher-performing hardware does not necessarily improve end-to-end performance. Framework overhead limits the impact of improved hardware as it limits utilization. This trend will con- tinue as ML-specific hardware advances without ef- forts to address software bottlenecks. For example, single-example inference with both BERT is slower using an A100 than using a V100 GPU despite a 2.75x increase in peak computational throughput. 6 Conclusion We conduct an extensive study of neural networks from the convolutional and transformer architec- ture paradigms across a variety of software and hardware platforms. We show that inference per- formed with these large neural networks, which was previously assumed to be compute bounded, is in fact limited by overhead incurred by deep learning frameworks. While wider transformer ar- chitectures (e.g. BERT-Base) exhibit less bounded- ness behaviors than narrower, deeper CNNs (e.g. ResNet-50), we show that all models exhibit frame- work boundedness. Additionally, we observe that these inefficiencies are becoming more apparent as hardware accelerator speeds increase. We introduce the concept of the framework tax to describe when improvements in hardware speed and reductions in required computation fail to trans- late to speedups in model latency and throughput due to bottlenecks incurred by deep learning frame- works. We hope that these observations raise aware- ness of the impact and limitations created by choice of deep learning frameworks on model develop- ment and deployment. 7 Limitations In this work, we study the inference efficiency of speech and language models using GPU hardware accelerators. While GPUs are the most common general purpose hardware accelerators, there ex- ist domain specific architectures such as Google TPU\u2019s, GraphCore IPUs, and custom ASICs which present additional settings for future investigation. Additionally, our study evaluates efficiency via model latency and our claims do not necessarily translate to other metrics of efficiency, such as power consumption or power output. As models continue to scale in size, they often require model or data parallelism techniques that require computation across several nodes which introduce overhead from multi-device synchroniza- tion and network communication. Additionally, we do not study latency in the training setting where the per-layer computation is larger due to the com- putation of gradients and losses. Acknowledgements This work was supported in part by a grant from the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE2140739. We thank the anonymous reviewers for their valuable feedback. We would also like to thank lab members and faculty for helpful feed- back during discussions and revisions, including: Daniel Fried, Han Guo, Jeremiah Milbauer, Sanket Vaibhav Mehta, and Saujas Vaduguru. References Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fr\u00e9d\u00e9ric Bastien, Justin Bayer, Anatoly Be- likov, Alexander Belopolsky, et al. 2016. Theano: A python framework for fast computation of mathemat- ical expressions. arXiv e-prints, pages arXiv\u20131605. Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. 2022. Revisiting neural scaling laws in language and vision. Advances in Neural Informa- tion Processing Systems, 35:22300\u201322312. Reza Yazdani Aminabadi, Samyam Rajbhandari, Am- mar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeed-inference: enabling efficient inference of transformer models at unprece- In Proceedings of the International dented scale. Conference on High Performance Computing, Net- working, Storage and Analysis, pages 1\u201315. Jeff Barr. 2019. Amazon ec2 update-infl instances with aws inferentia chips for high performance cost- effective inferencing. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. Han Cai, Ligeng Zhu, and Song Han. 2018. Proxy- lessnas: Direct neural architecture search on target task and hardware. In International Conference on Learning Representations. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Se- lected Topics in Signal Processing, 16(6):1505\u20131518. Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexi- ble and efficient machine learning library for het- arXiv preprint erogeneous distributed systems. arXiv:1512.01274. Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Sympo- sium on Operating Systems Design and Implementa- tion (OSDI 18), pages 578\u2013594. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Ad-"}, {"question": " How do wider models help improve task performance according to the text?", "answer": " Wider models exhibit better task performance due to increased parameter count and expressivity.", "ref_chunk": "is bot- SA Dim FC Dim Batch Seq Latency TP 768 768 768 1536 3072 3072 3072 6144 1 4 1 1 128 128 512 128 0.0136 0.0134 0.0134 0.0134 0.0136 0.0034 0.0134 0.0134 Table 1: Latency and throughput (TP) of BERT PyTorch models on RTX-8000. Scaling along batch sizes and model width shows no increase in latency. tlenecked by framework overhead, the batch size or input size can be increased without increases in overall runtime. In practice, this can lead to the pro- cessing of larger batch sizes and sequence lengths at no additional latency cost until kernel operations saturate framework overhead. Model width can be increased at no cost for framework-bound models. For a given batch size, individual layers of a framework bound model can be made wider by increasing hidden dimension or filter size without impacting latency. Wider mod- els are known to exhibit better task performance due to increased parameter count and expressivity (Zagoruyko and Komodakis, 2016). Model designers can leverage wider architec- tures with their target inference framework and hardware setting in mind to achieve higher utiliza- tion. For example, models processing few exam- ples during inference can leverage wider layers to avoid framework bottlenecks. See Table 1. Using higher-performing hardware does not necessarily improve end-to-end performance. Framework overhead limits the impact of improved hardware as it limits utilization. This trend will con- tinue as ML-specific hardware advances without ef- forts to address software bottlenecks. For example, single-example inference with both BERT is slower using an A100 than using a V100 GPU despite a 2.75x increase in peak computational throughput. 6 Conclusion We conduct an extensive study of neural networks from the convolutional and transformer architec- ture paradigms across a variety of software and hardware platforms. We show that inference per- formed with these large neural networks, which was previously assumed to be compute bounded, is in fact limited by overhead incurred by deep learning frameworks. While wider transformer ar- chitectures (e.g. BERT-Base) exhibit less bounded- ness behaviors than narrower, deeper CNNs (e.g. ResNet-50), we show that all models exhibit frame- work boundedness. Additionally, we observe that these inefficiencies are becoming more apparent as hardware accelerator speeds increase. We introduce the concept of the framework tax to describe when improvements in hardware speed and reductions in required computation fail to trans- late to speedups in model latency and throughput due to bottlenecks incurred by deep learning frame- works. We hope that these observations raise aware- ness of the impact and limitations created by choice of deep learning frameworks on model develop- ment and deployment. 7 Limitations In this work, we study the inference efficiency of speech and language models using GPU hardware accelerators. While GPUs are the most common general purpose hardware accelerators, there ex- ist domain specific architectures such as Google TPU\u2019s, GraphCore IPUs, and custom ASICs which present additional settings for future investigation. Additionally, our study evaluates efficiency via model latency and our claims do not necessarily translate to other metrics of efficiency, such as power consumption or power output. As models continue to scale in size, they often require model or data parallelism techniques that require computation across several nodes which introduce overhead from multi-device synchroniza- tion and network communication. Additionally, we do not study latency in the training setting where the per-layer computation is larger due to the com- putation of gradients and losses. Acknowledgements This work was supported in part by a grant from the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE2140739. We thank the anonymous reviewers for their valuable feedback. We would also like to thank lab members and faculty for helpful feed- back during discussions and revisions, including: Daniel Fried, Han Guo, Jeremiah Milbauer, Sanket Vaibhav Mehta, and Saujas Vaduguru. References Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fr\u00e9d\u00e9ric Bastien, Justin Bayer, Anatoly Be- likov, Alexander Belopolsky, et al. 2016. Theano: A python framework for fast computation of mathemat- ical expressions. arXiv e-prints, pages arXiv\u20131605. Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. 2022. Revisiting neural scaling laws in language and vision. Advances in Neural Informa- tion Processing Systems, 35:22300\u201322312. Reza Yazdani Aminabadi, Samyam Rajbhandari, Am- mar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeed-inference: enabling efficient inference of transformer models at unprece- In Proceedings of the International dented scale. Conference on High Performance Computing, Net- working, Storage and Analysis, pages 1\u201315. Jeff Barr. 2019. Amazon ec2 update-infl instances with aws inferentia chips for high performance cost- effective inferencing. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. Han Cai, Ligeng Zhu, and Song Han. 2018. Proxy- lessnas: Direct neural architecture search on target task and hardware. In International Conference on Learning Representations. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Se- lected Topics in Signal Processing, 16(6):1505\u20131518. Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexi- ble and efficient machine learning library for het- arXiv preprint erogeneous distributed systems. arXiv:1512.01274. Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Sympo- sium on Operating Systems Design and Implementa- tion (OSDI 18), pages 578\u2013594. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Ad-"}, {"question": " What does the text mention about the impact of using higher-performing hardware on end-to-end performance?", "answer": " Using higher-performing hardware does not necessarily improve end-to-end performance due to framework overhead.", "ref_chunk": "is bot- SA Dim FC Dim Batch Seq Latency TP 768 768 768 1536 3072 3072 3072 6144 1 4 1 1 128 128 512 128 0.0136 0.0134 0.0134 0.0134 0.0136 0.0034 0.0134 0.0134 Table 1: Latency and throughput (TP) of BERT PyTorch models on RTX-8000. Scaling along batch sizes and model width shows no increase in latency. tlenecked by framework overhead, the batch size or input size can be increased without increases in overall runtime. In practice, this can lead to the pro- cessing of larger batch sizes and sequence lengths at no additional latency cost until kernel operations saturate framework overhead. Model width can be increased at no cost for framework-bound models. For a given batch size, individual layers of a framework bound model can be made wider by increasing hidden dimension or filter size without impacting latency. Wider mod- els are known to exhibit better task performance due to increased parameter count and expressivity (Zagoruyko and Komodakis, 2016). Model designers can leverage wider architec- tures with their target inference framework and hardware setting in mind to achieve higher utiliza- tion. For example, models processing few exam- ples during inference can leverage wider layers to avoid framework bottlenecks. See Table 1. Using higher-performing hardware does not necessarily improve end-to-end performance. Framework overhead limits the impact of improved hardware as it limits utilization. This trend will con- tinue as ML-specific hardware advances without ef- forts to address software bottlenecks. For example, single-example inference with both BERT is slower using an A100 than using a V100 GPU despite a 2.75x increase in peak computational throughput. 6 Conclusion We conduct an extensive study of neural networks from the convolutional and transformer architec- ture paradigms across a variety of software and hardware platforms. We show that inference per- formed with these large neural networks, which was previously assumed to be compute bounded, is in fact limited by overhead incurred by deep learning frameworks. While wider transformer ar- chitectures (e.g. BERT-Base) exhibit less bounded- ness behaviors than narrower, deeper CNNs (e.g. ResNet-50), we show that all models exhibit frame- work boundedness. Additionally, we observe that these inefficiencies are becoming more apparent as hardware accelerator speeds increase. We introduce the concept of the framework tax to describe when improvements in hardware speed and reductions in required computation fail to trans- late to speedups in model latency and throughput due to bottlenecks incurred by deep learning frame- works. We hope that these observations raise aware- ness of the impact and limitations created by choice of deep learning frameworks on model develop- ment and deployment. 7 Limitations In this work, we study the inference efficiency of speech and language models using GPU hardware accelerators. While GPUs are the most common general purpose hardware accelerators, there ex- ist domain specific architectures such as Google TPU\u2019s, GraphCore IPUs, and custom ASICs which present additional settings for future investigation. Additionally, our study evaluates efficiency via model latency and our claims do not necessarily translate to other metrics of efficiency, such as power consumption or power output. As models continue to scale in size, they often require model or data parallelism techniques that require computation across several nodes which introduce overhead from multi-device synchroniza- tion and network communication. Additionally, we do not study latency in the training setting where the per-layer computation is larger due to the com- putation of gradients and losses. Acknowledgements This work was supported in part by a grant from the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE2140739. We thank the anonymous reviewers for their valuable feedback. We would also like to thank lab members and faculty for helpful feed- back during discussions and revisions, including: Daniel Fried, Han Guo, Jeremiah Milbauer, Sanket Vaibhav Mehta, and Saujas Vaduguru. References Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fr\u00e9d\u00e9ric Bastien, Justin Bayer, Anatoly Be- likov, Alexander Belopolsky, et al. 2016. Theano: A python framework for fast computation of mathemat- ical expressions. arXiv e-prints, pages arXiv\u20131605. Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. 2022. Revisiting neural scaling laws in language and vision. Advances in Neural Informa- tion Processing Systems, 35:22300\u201322312. Reza Yazdani Aminabadi, Samyam Rajbhandari, Am- mar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeed-inference: enabling efficient inference of transformer models at unprece- In Proceedings of the International dented scale. Conference on High Performance Computing, Net- working, Storage and Analysis, pages 1\u201315. Jeff Barr. 2019. Amazon ec2 update-infl instances with aws inferentia chips for high performance cost- effective inferencing. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. Han Cai, Ligeng Zhu, and Song Han. 2018. Proxy- lessnas: Direct neural architecture search on target task and hardware. In International Conference on Learning Representations. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Se- lected Topics in Signal Processing, 16(6):1505\u20131518. Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexi- ble and efficient machine learning library for het- arXiv preprint erogeneous distributed systems. arXiv:1512.01274. Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Sympo- sium on Operating Systems Design and Implementa- tion (OSDI 18), pages 578\u2013594. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Ad-"}, {"question": " What term is introduced in the text to describe when improvements in hardware speed and reductions in required computation fail to translate to speedups in model latency and throughput?", "answer": " The framework tax", "ref_chunk": "is bot- SA Dim FC Dim Batch Seq Latency TP 768 768 768 1536 3072 3072 3072 6144 1 4 1 1 128 128 512 128 0.0136 0.0134 0.0134 0.0134 0.0136 0.0034 0.0134 0.0134 Table 1: Latency and throughput (TP) of BERT PyTorch models on RTX-8000. Scaling along batch sizes and model width shows no increase in latency. tlenecked by framework overhead, the batch size or input size can be increased without increases in overall runtime. In practice, this can lead to the pro- cessing of larger batch sizes and sequence lengths at no additional latency cost until kernel operations saturate framework overhead. Model width can be increased at no cost for framework-bound models. For a given batch size, individual layers of a framework bound model can be made wider by increasing hidden dimension or filter size without impacting latency. Wider mod- els are known to exhibit better task performance due to increased parameter count and expressivity (Zagoruyko and Komodakis, 2016). Model designers can leverage wider architec- tures with their target inference framework and hardware setting in mind to achieve higher utiliza- tion. For example, models processing few exam- ples during inference can leverage wider layers to avoid framework bottlenecks. See Table 1. Using higher-performing hardware does not necessarily improve end-to-end performance. Framework overhead limits the impact of improved hardware as it limits utilization. This trend will con- tinue as ML-specific hardware advances without ef- forts to address software bottlenecks. For example, single-example inference with both BERT is slower using an A100 than using a V100 GPU despite a 2.75x increase in peak computational throughput. 6 Conclusion We conduct an extensive study of neural networks from the convolutional and transformer architec- ture paradigms across a variety of software and hardware platforms. We show that inference per- formed with these large neural networks, which was previously assumed to be compute bounded, is in fact limited by overhead incurred by deep learning frameworks. While wider transformer ar- chitectures (e.g. BERT-Base) exhibit less bounded- ness behaviors than narrower, deeper CNNs (e.g. ResNet-50), we show that all models exhibit frame- work boundedness. Additionally, we observe that these inefficiencies are becoming more apparent as hardware accelerator speeds increase. We introduce the concept of the framework tax to describe when improvements in hardware speed and reductions in required computation fail to trans- late to speedups in model latency and throughput due to bottlenecks incurred by deep learning frame- works. We hope that these observations raise aware- ness of the impact and limitations created by choice of deep learning frameworks on model develop- ment and deployment. 7 Limitations In this work, we study the inference efficiency of speech and language models using GPU hardware accelerators. While GPUs are the most common general purpose hardware accelerators, there ex- ist domain specific architectures such as Google TPU\u2019s, GraphCore IPUs, and custom ASICs which present additional settings for future investigation. Additionally, our study evaluates efficiency via model latency and our claims do not necessarily translate to other metrics of efficiency, such as power consumption or power output. As models continue to scale in size, they often require model or data parallelism techniques that require computation across several nodes which introduce overhead from multi-device synchroniza- tion and network communication. Additionally, we do not study latency in the training setting where the per-layer computation is larger due to the com- putation of gradients and losses. Acknowledgements This work was supported in part by a grant from the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE2140739. We thank the anonymous reviewers for their valuable feedback. We would also like to thank lab members and faculty for helpful feed- back during discussions and revisions, including: Daniel Fried, Han Guo, Jeremiah Milbauer, Sanket Vaibhav Mehta, and Saujas Vaduguru. References Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fr\u00e9d\u00e9ric Bastien, Justin Bayer, Anatoly Be- likov, Alexander Belopolsky, et al. 2016. Theano: A python framework for fast computation of mathemat- ical expressions. arXiv e-prints, pages arXiv\u20131605. Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. 2022. Revisiting neural scaling laws in language and vision. Advances in Neural Informa- tion Processing Systems, 35:22300\u201322312. Reza Yazdani Aminabadi, Samyam Rajbhandari, Am- mar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeed-inference: enabling efficient inference of transformer models at unprece- In Proceedings of the International dented scale. Conference on High Performance Computing, Net- working, Storage and Analysis, pages 1\u201315. Jeff Barr. 2019. Amazon ec2 update-infl instances with aws inferentia chips for high performance cost- effective inferencing. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. Han Cai, Ligeng Zhu, and Song Han. 2018. Proxy- lessnas: Direct neural architecture search on target task and hardware. In International Conference on Learning Representations. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Se- lected Topics in Signal Processing, 16(6):1505\u20131518. Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexi- ble and efficient machine learning library for het- arXiv preprint erogeneous distributed systems. arXiv:1512.01274. Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Sympo- sium on Operating Systems Design and Implementa- tion (OSDI 18), pages 578\u2013594. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Ad-"}, {"question": " What does the text mention about the inference efficiency of speech and language models using GPU hardware accelerators?", "answer": " The inference efficiency is limited by overhead incurred by deep learning frameworks, not by computational boundaries.", "ref_chunk": "is bot- SA Dim FC Dim Batch Seq Latency TP 768 768 768 1536 3072 3072 3072 6144 1 4 1 1 128 128 512 128 0.0136 0.0134 0.0134 0.0134 0.0136 0.0034 0.0134 0.0134 Table 1: Latency and throughput (TP) of BERT PyTorch models on RTX-8000. Scaling along batch sizes and model width shows no increase in latency. tlenecked by framework overhead, the batch size or input size can be increased without increases in overall runtime. In practice, this can lead to the pro- cessing of larger batch sizes and sequence lengths at no additional latency cost until kernel operations saturate framework overhead. Model width can be increased at no cost for framework-bound models. For a given batch size, individual layers of a framework bound model can be made wider by increasing hidden dimension or filter size without impacting latency. Wider mod- els are known to exhibit better task performance due to increased parameter count and expressivity (Zagoruyko and Komodakis, 2016). Model designers can leverage wider architec- tures with their target inference framework and hardware setting in mind to achieve higher utiliza- tion. For example, models processing few exam- ples during inference can leverage wider layers to avoid framework bottlenecks. See Table 1. Using higher-performing hardware does not necessarily improve end-to-end performance. Framework overhead limits the impact of improved hardware as it limits utilization. This trend will con- tinue as ML-specific hardware advances without ef- forts to address software bottlenecks. For example, single-example inference with both BERT is slower using an A100 than using a V100 GPU despite a 2.75x increase in peak computational throughput. 6 Conclusion We conduct an extensive study of neural networks from the convolutional and transformer architec- ture paradigms across a variety of software and hardware platforms. We show that inference per- formed with these large neural networks, which was previously assumed to be compute bounded, is in fact limited by overhead incurred by deep learning frameworks. While wider transformer ar- chitectures (e.g. BERT-Base) exhibit less bounded- ness behaviors than narrower, deeper CNNs (e.g. ResNet-50), we show that all models exhibit frame- work boundedness. Additionally, we observe that these inefficiencies are becoming more apparent as hardware accelerator speeds increase. We introduce the concept of the framework tax to describe when improvements in hardware speed and reductions in required computation fail to trans- late to speedups in model latency and throughput due to bottlenecks incurred by deep learning frame- works. We hope that these observations raise aware- ness of the impact and limitations created by choice of deep learning frameworks on model develop- ment and deployment. 7 Limitations In this work, we study the inference efficiency of speech and language models using GPU hardware accelerators. While GPUs are the most common general purpose hardware accelerators, there ex- ist domain specific architectures such as Google TPU\u2019s, GraphCore IPUs, and custom ASICs which present additional settings for future investigation. Additionally, our study evaluates efficiency via model latency and our claims do not necessarily translate to other metrics of efficiency, such as power consumption or power output. As models continue to scale in size, they often require model or data parallelism techniques that require computation across several nodes which introduce overhead from multi-device synchroniza- tion and network communication. Additionally, we do not study latency in the training setting where the per-layer computation is larger due to the com- putation of gradients and losses. Acknowledgements This work was supported in part by a grant from the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE2140739. We thank the anonymous reviewers for their valuable feedback. We would also like to thank lab members and faculty for helpful feed- back during discussions and revisions, including: Daniel Fried, Han Guo, Jeremiah Milbauer, Sanket Vaibhav Mehta, and Saujas Vaduguru. References Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fr\u00e9d\u00e9ric Bastien, Justin Bayer, Anatoly Be- likov, Alexander Belopolsky, et al. 2016. Theano: A python framework for fast computation of mathemat- ical expressions. arXiv e-prints, pages arXiv\u20131605. Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. 2022. Revisiting neural scaling laws in language and vision. Advances in Neural Informa- tion Processing Systems, 35:22300\u201322312. Reza Yazdani Aminabadi, Samyam Rajbhandari, Am- mar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeed-inference: enabling efficient inference of transformer models at unprece- In Proceedings of the International dented scale. Conference on High Performance Computing, Net- working, Storage and Analysis, pages 1\u201315. Jeff Barr. 2019. Amazon ec2 update-infl instances with aws inferentia chips for high performance cost- effective inferencing. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. Han Cai, Ligeng Zhu, and Song Han. 2018. Proxy- lessnas: Direct neural architecture search on target task and hardware. In International Conference on Learning Representations. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Se- lected Topics in Signal Processing, 16(6):1505\u20131518. Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexi- ble and efficient machine learning library for het- arXiv preprint erogeneous distributed systems. arXiv:1512.01274. Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Sympo- sium on Operating Systems Design and Implementa- tion (OSDI 18), pages 578\u2013594. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Ad-"}, {"question": " What type of architectures are mentioned as domain-specific alternatives to GPUs in the text?", "answer": " Domain-specific architectures such as Google TPU\u2019s, GraphCore IPUs, and custom ASICs.", "ref_chunk": "is bot- SA Dim FC Dim Batch Seq Latency TP 768 768 768 1536 3072 3072 3072 6144 1 4 1 1 128 128 512 128 0.0136 0.0134 0.0134 0.0134 0.0136 0.0034 0.0134 0.0134 Table 1: Latency and throughput (TP) of BERT PyTorch models on RTX-8000. Scaling along batch sizes and model width shows no increase in latency. tlenecked by framework overhead, the batch size or input size can be increased without increases in overall runtime. In practice, this can lead to the pro- cessing of larger batch sizes and sequence lengths at no additional latency cost until kernel operations saturate framework overhead. Model width can be increased at no cost for framework-bound models. For a given batch size, individual layers of a framework bound model can be made wider by increasing hidden dimension or filter size without impacting latency. Wider mod- els are known to exhibit better task performance due to increased parameter count and expressivity (Zagoruyko and Komodakis, 2016). Model designers can leverage wider architec- tures with their target inference framework and hardware setting in mind to achieve higher utiliza- tion. For example, models processing few exam- ples during inference can leverage wider layers to avoid framework bottlenecks. See Table 1. Using higher-performing hardware does not necessarily improve end-to-end performance. Framework overhead limits the impact of improved hardware as it limits utilization. This trend will con- tinue as ML-specific hardware advances without ef- forts to address software bottlenecks. For example, single-example inference with both BERT is slower using an A100 than using a V100 GPU despite a 2.75x increase in peak computational throughput. 6 Conclusion We conduct an extensive study of neural networks from the convolutional and transformer architec- ture paradigms across a variety of software and hardware platforms. We show that inference per- formed with these large neural networks, which was previously assumed to be compute bounded, is in fact limited by overhead incurred by deep learning frameworks. While wider transformer ar- chitectures (e.g. BERT-Base) exhibit less bounded- ness behaviors than narrower, deeper CNNs (e.g. ResNet-50), we show that all models exhibit frame- work boundedness. Additionally, we observe that these inefficiencies are becoming more apparent as hardware accelerator speeds increase. We introduce the concept of the framework tax to describe when improvements in hardware speed and reductions in required computation fail to trans- late to speedups in model latency and throughput due to bottlenecks incurred by deep learning frame- works. We hope that these observations raise aware- ness of the impact and limitations created by choice of deep learning frameworks on model develop- ment and deployment. 7 Limitations In this work, we study the inference efficiency of speech and language models using GPU hardware accelerators. While GPUs are the most common general purpose hardware accelerators, there ex- ist domain specific architectures such as Google TPU\u2019s, GraphCore IPUs, and custom ASICs which present additional settings for future investigation. Additionally, our study evaluates efficiency via model latency and our claims do not necessarily translate to other metrics of efficiency, such as power consumption or power output. As models continue to scale in size, they often require model or data parallelism techniques that require computation across several nodes which introduce overhead from multi-device synchroniza- tion and network communication. Additionally, we do not study latency in the training setting where the per-layer computation is larger due to the com- putation of gradients and losses. Acknowledgements This work was supported in part by a grant from the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE2140739. We thank the anonymous reviewers for their valuable feedback. We would also like to thank lab members and faculty for helpful feed- back during discussions and revisions, including: Daniel Fried, Han Guo, Jeremiah Milbauer, Sanket Vaibhav Mehta, and Saujas Vaduguru. References Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fr\u00e9d\u00e9ric Bastien, Justin Bayer, Anatoly Be- likov, Alexander Belopolsky, et al. 2016. Theano: A python framework for fast computation of mathemat- ical expressions. arXiv e-prints, pages arXiv\u20131605. Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. 2022. Revisiting neural scaling laws in language and vision. Advances in Neural Informa- tion Processing Systems, 35:22300\u201322312. Reza Yazdani Aminabadi, Samyam Rajbhandari, Am- mar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeed-inference: enabling efficient inference of transformer models at unprece- In Proceedings of the International dented scale. Conference on High Performance Computing, Net- working, Storage and Analysis, pages 1\u201315. Jeff Barr. 2019. Amazon ec2 update-infl instances with aws inferentia chips for high performance cost- effective inferencing. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. Han Cai, Ligeng Zhu, and Song Han. 2018. Proxy- lessnas: Direct neural architecture search on target task and hardware. In International Conference on Learning Representations. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Se- lected Topics in Signal Processing, 16(6):1505\u20131518. Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexi- ble and efficient machine learning library for het- arXiv preprint erogeneous distributed systems. arXiv:1512.01274. Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Sympo- sium on Operating Systems Design and Implementa- tion (OSDI 18), pages 578\u2013594. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Ad-"}, {"question": " What are some limitations mentioned in the text related to efficiency evaluation via model latency?", "answer": " The study does not necessarily translate to other metrics of efficiency, such as power consumption or power output.", "ref_chunk": "is bot- SA Dim FC Dim Batch Seq Latency TP 768 768 768 1536 3072 3072 3072 6144 1 4 1 1 128 128 512 128 0.0136 0.0134 0.0134 0.0134 0.0136 0.0034 0.0134 0.0134 Table 1: Latency and throughput (TP) of BERT PyTorch models on RTX-8000. Scaling along batch sizes and model width shows no increase in latency. tlenecked by framework overhead, the batch size or input size can be increased without increases in overall runtime. In practice, this can lead to the pro- cessing of larger batch sizes and sequence lengths at no additional latency cost until kernel operations saturate framework overhead. Model width can be increased at no cost for framework-bound models. For a given batch size, individual layers of a framework bound model can be made wider by increasing hidden dimension or filter size without impacting latency. Wider mod- els are known to exhibit better task performance due to increased parameter count and expressivity (Zagoruyko and Komodakis, 2016). Model designers can leverage wider architec- tures with their target inference framework and hardware setting in mind to achieve higher utiliza- tion. For example, models processing few exam- ples during inference can leverage wider layers to avoid framework bottlenecks. See Table 1. Using higher-performing hardware does not necessarily improve end-to-end performance. Framework overhead limits the impact of improved hardware as it limits utilization. This trend will con- tinue as ML-specific hardware advances without ef- forts to address software bottlenecks. For example, single-example inference with both BERT is slower using an A100 than using a V100 GPU despite a 2.75x increase in peak computational throughput. 6 Conclusion We conduct an extensive study of neural networks from the convolutional and transformer architec- ture paradigms across a variety of software and hardware platforms. We show that inference per- formed with these large neural networks, which was previously assumed to be compute bounded, is in fact limited by overhead incurred by deep learning frameworks. While wider transformer ar- chitectures (e.g. BERT-Base) exhibit less bounded- ness behaviors than narrower, deeper CNNs (e.g. ResNet-50), we show that all models exhibit frame- work boundedness. Additionally, we observe that these inefficiencies are becoming more apparent as hardware accelerator speeds increase. We introduce the concept of the framework tax to describe when improvements in hardware speed and reductions in required computation fail to trans- late to speedups in model latency and throughput due to bottlenecks incurred by deep learning frame- works. We hope that these observations raise aware- ness of the impact and limitations created by choice of deep learning frameworks on model develop- ment and deployment. 7 Limitations In this work, we study the inference efficiency of speech and language models using GPU hardware accelerators. While GPUs are the most common general purpose hardware accelerators, there ex- ist domain specific architectures such as Google TPU\u2019s, GraphCore IPUs, and custom ASICs which present additional settings for future investigation. Additionally, our study evaluates efficiency via model latency and our claims do not necessarily translate to other metrics of efficiency, such as power consumption or power output. As models continue to scale in size, they often require model or data parallelism techniques that require computation across several nodes which introduce overhead from multi-device synchroniza- tion and network communication. Additionally, we do not study latency in the training setting where the per-layer computation is larger due to the com- putation of gradients and losses. Acknowledgements This work was supported in part by a grant from the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE2140739. We thank the anonymous reviewers for their valuable feedback. We would also like to thank lab members and faculty for helpful feed- back during discussions and revisions, including: Daniel Fried, Han Guo, Jeremiah Milbauer, Sanket Vaibhav Mehta, and Saujas Vaduguru. References Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fr\u00e9d\u00e9ric Bastien, Justin Bayer, Anatoly Be- likov, Alexander Belopolsky, et al. 2016. Theano: A python framework for fast computation of mathemat- ical expressions. arXiv e-prints, pages arXiv\u20131605. Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. 2022. Revisiting neural scaling laws in language and vision. Advances in Neural Informa- tion Processing Systems, 35:22300\u201322312. Reza Yazdani Aminabadi, Samyam Rajbhandari, Am- mar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeed-inference: enabling efficient inference of transformer models at unprece- In Proceedings of the International dented scale. Conference on High Performance Computing, Net- working, Storage and Analysis, pages 1\u201315. Jeff Barr. 2019. Amazon ec2 update-infl instances with aws inferentia chips for high performance cost- effective inferencing. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. Han Cai, Ligeng Zhu, and Song Han. 2018. Proxy- lessnas: Direct neural architecture search on target task and hardware. In International Conference on Learning Representations. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Se- lected Topics in Signal Processing, 16(6):1505\u20131518. Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexi- ble and efficient machine learning library for het- arXiv preprint erogeneous distributed systems. arXiv:1512.01274. Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Sympo- sium on Operating Systems Design and Implementa- tion (OSDI 18), pages 578\u2013594. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Ad-"}, {"question": " What is the work supported by a grant from, as mentioned in the text?", "answer": " The work was supported in part by a grant from the National Science Foundation Graduate Research Fellowship Program.", "ref_chunk": "is bot- SA Dim FC Dim Batch Seq Latency TP 768 768 768 1536 3072 3072 3072 6144 1 4 1 1 128 128 512 128 0.0136 0.0134 0.0134 0.0134 0.0136 0.0034 0.0134 0.0134 Table 1: Latency and throughput (TP) of BERT PyTorch models on RTX-8000. Scaling along batch sizes and model width shows no increase in latency. tlenecked by framework overhead, the batch size or input size can be increased without increases in overall runtime. In practice, this can lead to the pro- cessing of larger batch sizes and sequence lengths at no additional latency cost until kernel operations saturate framework overhead. Model width can be increased at no cost for framework-bound models. For a given batch size, individual layers of a framework bound model can be made wider by increasing hidden dimension or filter size without impacting latency. Wider mod- els are known to exhibit better task performance due to increased parameter count and expressivity (Zagoruyko and Komodakis, 2016). Model designers can leverage wider architec- tures with their target inference framework and hardware setting in mind to achieve higher utiliza- tion. For example, models processing few exam- ples during inference can leverage wider layers to avoid framework bottlenecks. See Table 1. Using higher-performing hardware does not necessarily improve end-to-end performance. Framework overhead limits the impact of improved hardware as it limits utilization. This trend will con- tinue as ML-specific hardware advances without ef- forts to address software bottlenecks. For example, single-example inference with both BERT is slower using an A100 than using a V100 GPU despite a 2.75x increase in peak computational throughput. 6 Conclusion We conduct an extensive study of neural networks from the convolutional and transformer architec- ture paradigms across a variety of software and hardware platforms. We show that inference per- formed with these large neural networks, which was previously assumed to be compute bounded, is in fact limited by overhead incurred by deep learning frameworks. While wider transformer ar- chitectures (e.g. BERT-Base) exhibit less bounded- ness behaviors than narrower, deeper CNNs (e.g. ResNet-50), we show that all models exhibit frame- work boundedness. Additionally, we observe that these inefficiencies are becoming more apparent as hardware accelerator speeds increase. We introduce the concept of the framework tax to describe when improvements in hardware speed and reductions in required computation fail to trans- late to speedups in model latency and throughput due to bottlenecks incurred by deep learning frame- works. We hope that these observations raise aware- ness of the impact and limitations created by choice of deep learning frameworks on model develop- ment and deployment. 7 Limitations In this work, we study the inference efficiency of speech and language models using GPU hardware accelerators. While GPUs are the most common general purpose hardware accelerators, there ex- ist domain specific architectures such as Google TPU\u2019s, GraphCore IPUs, and custom ASICs which present additional settings for future investigation. Additionally, our study evaluates efficiency via model latency and our claims do not necessarily translate to other metrics of efficiency, such as power consumption or power output. As models continue to scale in size, they often require model or data parallelism techniques that require computation across several nodes which introduce overhead from multi-device synchroniza- tion and network communication. Additionally, we do not study latency in the training setting where the per-layer computation is larger due to the com- putation of gradients and losses. Acknowledgements This work was supported in part by a grant from the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE2140739. We thank the anonymous reviewers for their valuable feedback. We would also like to thank lab members and faculty for helpful feed- back during discussions and revisions, including: Daniel Fried, Han Guo, Jeremiah Milbauer, Sanket Vaibhav Mehta, and Saujas Vaduguru. References Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fr\u00e9d\u00e9ric Bastien, Justin Bayer, Anatoly Be- likov, Alexander Belopolsky, et al. 2016. Theano: A python framework for fast computation of mathemat- ical expressions. arXiv e-prints, pages arXiv\u20131605. Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. 2022. Revisiting neural scaling laws in language and vision. Advances in Neural Informa- tion Processing Systems, 35:22300\u201322312. Reza Yazdani Aminabadi, Samyam Rajbhandari, Am- mar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeed-inference: enabling efficient inference of transformer models at unprece- In Proceedings of the International dented scale. Conference on High Performance Computing, Net- working, Storage and Analysis, pages 1\u201315. Jeff Barr. 2019. Amazon ec2 update-infl instances with aws inferentia chips for high performance cost- effective inferencing. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. Han Cai, Ligeng Zhu, and Song Han. 2018. Proxy- lessnas: Direct neural architecture search on target task and hardware. In International Conference on Learning Representations. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Se- lected Topics in Signal Processing, 16(6):1505\u20131518. Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexi- ble and efficient machine learning library for het- arXiv preprint erogeneous distributed systems. arXiv:1512.01274. Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Sympo- sium on Operating Systems Design and Implementa- tion (OSDI 18), pages 578\u2013594. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Ad-"}, {"question": " Who are some of the lab members and faculty mentioned in the text for their contributions?", "answer": " Some of the lab members and faculty mentioned include Daniel Fried, Han Guo, Jeremiah Milbauer, Sanket Vaibhav Mehta, and Saujas Vaduguru.", "ref_chunk": "is bot- SA Dim FC Dim Batch Seq Latency TP 768 768 768 1536 3072 3072 3072 6144 1 4 1 1 128 128 512 128 0.0136 0.0134 0.0134 0.0134 0.0136 0.0034 0.0134 0.0134 Table 1: Latency and throughput (TP) of BERT PyTorch models on RTX-8000. Scaling along batch sizes and model width shows no increase in latency. tlenecked by framework overhead, the batch size or input size can be increased without increases in overall runtime. In practice, this can lead to the pro- cessing of larger batch sizes and sequence lengths at no additional latency cost until kernel operations saturate framework overhead. Model width can be increased at no cost for framework-bound models. For a given batch size, individual layers of a framework bound model can be made wider by increasing hidden dimension or filter size without impacting latency. Wider mod- els are known to exhibit better task performance due to increased parameter count and expressivity (Zagoruyko and Komodakis, 2016). Model designers can leverage wider architec- tures with their target inference framework and hardware setting in mind to achieve higher utiliza- tion. For example, models processing few exam- ples during inference can leverage wider layers to avoid framework bottlenecks. See Table 1. Using higher-performing hardware does not necessarily improve end-to-end performance. Framework overhead limits the impact of improved hardware as it limits utilization. This trend will con- tinue as ML-specific hardware advances without ef- forts to address software bottlenecks. For example, single-example inference with both BERT is slower using an A100 than using a V100 GPU despite a 2.75x increase in peak computational throughput. 6 Conclusion We conduct an extensive study of neural networks from the convolutional and transformer architec- ture paradigms across a variety of software and hardware platforms. We show that inference per- formed with these large neural networks, which was previously assumed to be compute bounded, is in fact limited by overhead incurred by deep learning frameworks. While wider transformer ar- chitectures (e.g. BERT-Base) exhibit less bounded- ness behaviors than narrower, deeper CNNs (e.g. ResNet-50), we show that all models exhibit frame- work boundedness. Additionally, we observe that these inefficiencies are becoming more apparent as hardware accelerator speeds increase. We introduce the concept of the framework tax to describe when improvements in hardware speed and reductions in required computation fail to trans- late to speedups in model latency and throughput due to bottlenecks incurred by deep learning frame- works. We hope that these observations raise aware- ness of the impact and limitations created by choice of deep learning frameworks on model develop- ment and deployment. 7 Limitations In this work, we study the inference efficiency of speech and language models using GPU hardware accelerators. While GPUs are the most common general purpose hardware accelerators, there ex- ist domain specific architectures such as Google TPU\u2019s, GraphCore IPUs, and custom ASICs which present additional settings for future investigation. Additionally, our study evaluates efficiency via model latency and our claims do not necessarily translate to other metrics of efficiency, such as power consumption or power output. As models continue to scale in size, they often require model or data parallelism techniques that require computation across several nodes which introduce overhead from multi-device synchroniza- tion and network communication. Additionally, we do not study latency in the training setting where the per-layer computation is larger due to the com- putation of gradients and losses. Acknowledgements This work was supported in part by a grant from the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE2140739. We thank the anonymous reviewers for their valuable feedback. We would also like to thank lab members and faculty for helpful feed- back during discussions and revisions, including: Daniel Fried, Han Guo, Jeremiah Milbauer, Sanket Vaibhav Mehta, and Saujas Vaduguru. References Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fr\u00e9d\u00e9ric Bastien, Justin Bayer, Anatoly Be- likov, Alexander Belopolsky, et al. 2016. Theano: A python framework for fast computation of mathemat- ical expressions. arXiv e-prints, pages arXiv\u20131605. Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. 2022. Revisiting neural scaling laws in language and vision. Advances in Neural Informa- tion Processing Systems, 35:22300\u201322312. Reza Yazdani Aminabadi, Samyam Rajbhandari, Am- mar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeed-inference: enabling efficient inference of transformer models at unprece- In Proceedings of the International dented scale. Conference on High Performance Computing, Net- working, Storage and Analysis, pages 1\u201315. Jeff Barr. 2019. Amazon ec2 update-infl instances with aws inferentia chips for high performance cost- effective inferencing. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. Han Cai, Ligeng Zhu, and Song Han. 2018. Proxy- lessnas: Direct neural architecture search on target task and hardware. In International Conference on Learning Representations. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Se- lected Topics in Signal Processing, 16(6):1505\u20131518. Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexi- ble and efficient machine learning library for het- arXiv preprint erogeneous distributed systems. arXiv:1512.01274. Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Sympo- sium on Operating Systems Design and Implementa- tion (OSDI 18), pages 578\u2013594. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Ad-"}], "doc_text": "is bot- SA Dim FC Dim Batch Seq Latency TP 768 768 768 1536 3072 3072 3072 6144 1 4 1 1 128 128 512 128 0.0136 0.0134 0.0134 0.0134 0.0136 0.0034 0.0134 0.0134 Table 1: Latency and throughput (TP) of BERT PyTorch models on RTX-8000. Scaling along batch sizes and model width shows no increase in latency. tlenecked by framework overhead, the batch size or input size can be increased without increases in overall runtime. In practice, this can lead to the pro- cessing of larger batch sizes and sequence lengths at no additional latency cost until kernel operations saturate framework overhead. Model width can be increased at no cost for framework-bound models. For a given batch size, individual layers of a framework bound model can be made wider by increasing hidden dimension or filter size without impacting latency. Wider mod- els are known to exhibit better task performance due to increased parameter count and expressivity (Zagoruyko and Komodakis, 2016). Model designers can leverage wider architec- tures with their target inference framework and hardware setting in mind to achieve higher utiliza- tion. For example, models processing few exam- ples during inference can leverage wider layers to avoid framework bottlenecks. See Table 1. Using higher-performing hardware does not necessarily improve end-to-end performance. Framework overhead limits the impact of improved hardware as it limits utilization. This trend will con- tinue as ML-specific hardware advances without ef- forts to address software bottlenecks. For example, single-example inference with both BERT is slower using an A100 than using a V100 GPU despite a 2.75x increase in peak computational throughput. 6 Conclusion We conduct an extensive study of neural networks from the convolutional and transformer architec- ture paradigms across a variety of software and hardware platforms. We show that inference per- formed with these large neural networks, which was previously assumed to be compute bounded, is in fact limited by overhead incurred by deep learning frameworks. While wider transformer ar- chitectures (e.g. BERT-Base) exhibit less bounded- ness behaviors than narrower, deeper CNNs (e.g. ResNet-50), we show that all models exhibit frame- work boundedness. Additionally, we observe that these inefficiencies are becoming more apparent as hardware accelerator speeds increase. We introduce the concept of the framework tax to describe when improvements in hardware speed and reductions in required computation fail to trans- late to speedups in model latency and throughput due to bottlenecks incurred by deep learning frame- works. We hope that these observations raise aware- ness of the impact and limitations created by choice of deep learning frameworks on model develop- ment and deployment. 7 Limitations In this work, we study the inference efficiency of speech and language models using GPU hardware accelerators. While GPUs are the most common general purpose hardware accelerators, there ex- ist domain specific architectures such as Google TPU\u2019s, GraphCore IPUs, and custom ASICs which present additional settings for future investigation. Additionally, our study evaluates efficiency via model latency and our claims do not necessarily translate to other metrics of efficiency, such as power consumption or power output. As models continue to scale in size, they often require model or data parallelism techniques that require computation across several nodes which introduce overhead from multi-device synchroniza- tion and network communication. Additionally, we do not study latency in the training setting where the per-layer computation is larger due to the com- putation of gradients and losses. Acknowledgements This work was supported in part by a grant from the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE2140739. We thank the anonymous reviewers for their valuable feedback. We would also like to thank lab members and faculty for helpful feed- back during discussions and revisions, including: Daniel Fried, Han Guo, Jeremiah Milbauer, Sanket Vaibhav Mehta, and Saujas Vaduguru. References Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fr\u00e9d\u00e9ric Bastien, Justin Bayer, Anatoly Be- likov, Alexander Belopolsky, et al. 2016. Theano: A python framework for fast computation of mathemat- ical expressions. arXiv e-prints, pages arXiv\u20131605. Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. 2022. Revisiting neural scaling laws in language and vision. Advances in Neural Informa- tion Processing Systems, 35:22300\u201322312. Reza Yazdani Aminabadi, Samyam Rajbhandari, Am- mar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeed-inference: enabling efficient inference of transformer models at unprece- In Proceedings of the International dented scale. Conference on High Performance Computing, Net- working, Storage and Analysis, pages 1\u201315. Jeff Barr. 2019. Amazon ec2 update-infl instances with aws inferentia chips for high performance cost- effective inferencing. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. Han Cai, Ligeng Zhu, and Song Han. 2018. Proxy- lessnas: Direct neural architecture search on target task and hardware. In International Conference on Learning Representations. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Se- lected Topics in Signal Processing, 16(6):1505\u20131518. Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexi- ble and efficient machine learning library for het- arXiv preprint erogeneous distributed systems. arXiv:1512.01274. Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Sympo- sium on Operating Systems Design and Implementa- tion (OSDI 18), pages 578\u2013594. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Ad-"}