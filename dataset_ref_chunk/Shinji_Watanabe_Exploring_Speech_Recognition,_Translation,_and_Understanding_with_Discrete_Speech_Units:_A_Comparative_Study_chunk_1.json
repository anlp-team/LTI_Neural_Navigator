{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Exploring_Speech_Recognition,_Translation,_and_Understanding_with_Discrete_Speech_Units:_A_Comparative_Study_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of using discrete speech units in speech processing models?", "answer": " To significantly compress the size of speech data and reduce training time while retaining performance.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 0 0 8 5 1 . 9 0 3 2 : v i X r a EXPLORING SPEECH RECOGNITION, TRANSLATION, AND UNDERSTANDING WITH DISCRETE SPEECH UNITS: A COMPARATIVE STUDY Xuankai Chang1, Brian Yan1, Kwanghee Choi1, Jeeweon Jung1, Yichen Lu1, Soumi Maiti1, Roshan Sharma1, Jiatong Shi1, Jinchuan Tian1, Shinji Watanabe1\u2020, Yuya Fujita2, Takashi Maekaku2, Pengcheng Guo3, Yao-Fei Cheng4, Pavel Denisov5, Kohei Saijo6, Hsiu-Hsuan Wang7\u2217 1Carnegie Mellon University, 2Yahoo Japan Corporation, 3Northwestern Polytechnical University, 4University of Washington,5University of Stuttgart, 6Waseda University,7National Taiwan University ABSTRACT Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of dis- crete speech units derived from self-supervised learning representa- tions, which significantly compresses the size of speech data. Apply- ing various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech pro- cessing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts. Fig. 1: Illustration of the E2E speech processing model with discrete speech units. The speech discretization process is shown on the left. On the right side is a Seq2Seq model that takes discrete units to output target text. Index Terms\u2014 Discrete units, end-to-end, speech recognition, speech translation, spoken language understanding 1. INTRODUCTION Significant progress has been made in the field of automatic speech recognition (ASR) over the past few decades, largely attributed to the evolution of deep neural networks [1, 2]. Since the emergence of end-to-end (E2E) ASR models [3\u20135], there have been many exciting outcomes. Among these achievements, a slew of potent architec- tures [6\u20139] has boosted the performance of speech tasks, including ASR, speech translation (ST), and spoken language understanding (SLU). Also, novel training paradigms have demonstrated improved performance and generalization, including self-supervised learning (SSL) models [10\u201314] and Whisper [15]. In the majority of prior endeavors, high-dimensional features are derived from raw wave- forms as the input. Conventionally, spectral speech features are ex- tracted from a fixed-length temporal window, such as Mel Frequency Cepstral Coefficients (MFCC) or log Mel filter banks (FBANK). Re- cently, data-driven methods become popular to learn feature extrac- tion using neural networks [12, 13, 16]. However, in most cases, the data storage and transmission efficiency are similar among raw waveforms and speech features [17]. It is not trivial to improve the efficiency of computation without performance degradation. Recently, a few studies have proposed the use of discrete speech units to represent speech signals, where the information of speech signals in a short window is represented by a single token or a few levels from restricted vocabulary [11, 17, 18], as opposed to employing high-dimensional continuous speech features within the ASR task. For instance, in [17], researchers propose to utilize clus- tering indices derived from features of SSL models as input. This approach condenses the information considerably, reducing the orig- inal 1024-sized float vector to a mere 12-bit binary number: over 3000 times less. Such a compression process substantially reduces data storage and transmission size while maintaining predictive per- formance comparable to conventional high-dimensional features. Moreover, input sequence lengths can be drastically reduced via de-duplication and subword modeling on discrete units, leading to more than 2X faster training and inference. Notably, discrete unit representations can be regarded as a spoken language similar to text data in NLP tasks, which is more straightforward to unify the tasks or models. Being an alternative to traditional speech representation, this paradigm has already been applied in other domains, such as speech translation [19, 20] and audio generation [21\u201325]. To provide an extensive guide for discrete speech units, we con- ducted a comprehensive exploration into the effectiveness of discrete speech units across various speech processing tasks. We summarize our experiments and findings as follows: A comparative analysis is conducted under fair conditions, eval- uating the performance and training time reduction using discrete speech units as opposed to traditional speech features. \u2217Authors are ordered by the organizations. \u2020Corresponding author. A diverse range of benchmarks, including 12 ASR (Section 3.2), 3 ST (Section 3.3), and 1 SLU (Section 3.4) corpora, are mostly evaluated for the first time. To demonstrate wide applicability of the discrete units, we adopted noisy speech, spontaneous speech, telephony speech, and several multi-lingual speech corpora, which would be the first work to explore these aspects (Section 3.2.1). We show the versatility of discrete units in various E2E frame- works, including connectionist temporal classification (CTC) [3], attention-based encoder-decoder (AED) [5], and RNN-Transducer [4] (Table 3). We share various tips based on our investigations to get better performance, including SSL feature choice and discretization. Selecting SSL features based on canonical correlation analysis (CCA) [26] improves performance significantly compared to prior work [17] (Section 3.1). We also explore other possible choices of discrete units, includ- ing clustering SSL [14] or supervised representations, or vector quantization of neural codec models [27] (Section 3.2.5). We will release fully reproducible recipes and trained models on ESPnet [28], which can significantly benefit the community. 2. SPEECH PROCESSING WITH DISCRETE TOKENS This section elaborates on the details of our speech processing mod- els, which take discrete units as input. Figure 1 summarizes the whole pipeline. Leveraging the sequence-to-sequence (Seq2Seq) paradigm as our backbone framework enables broad applicability across a spectrum of tasks involving the transformation of speech signals into diverse target outputs. 2.1. Speech Discretization The discretization process is the pivotal step, which transforms the speech signals into discrete representations."}, {"question": " How are high-dimensional speech features typically used in speech processing models?", "answer": " They are often used as input for the subsequent model, but can still be redundant.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 0 0 8 5 1 . 9 0 3 2 : v i X r a EXPLORING SPEECH RECOGNITION, TRANSLATION, AND UNDERSTANDING WITH DISCRETE SPEECH UNITS: A COMPARATIVE STUDY Xuankai Chang1, Brian Yan1, Kwanghee Choi1, Jeeweon Jung1, Yichen Lu1, Soumi Maiti1, Roshan Sharma1, Jiatong Shi1, Jinchuan Tian1, Shinji Watanabe1\u2020, Yuya Fujita2, Takashi Maekaku2, Pengcheng Guo3, Yao-Fei Cheng4, Pavel Denisov5, Kohei Saijo6, Hsiu-Hsuan Wang7\u2217 1Carnegie Mellon University, 2Yahoo Japan Corporation, 3Northwestern Polytechnical University, 4University of Washington,5University of Stuttgart, 6Waseda University,7National Taiwan University ABSTRACT Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of dis- crete speech units derived from self-supervised learning representa- tions, which significantly compresses the size of speech data. Apply- ing various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech pro- cessing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts. Fig. 1: Illustration of the E2E speech processing model with discrete speech units. The speech discretization process is shown on the left. On the right side is a Seq2Seq model that takes discrete units to output target text. Index Terms\u2014 Discrete units, end-to-end, speech recognition, speech translation, spoken language understanding 1. INTRODUCTION Significant progress has been made in the field of automatic speech recognition (ASR) over the past few decades, largely attributed to the evolution of deep neural networks [1, 2]. Since the emergence of end-to-end (E2E) ASR models [3\u20135], there have been many exciting outcomes. Among these achievements, a slew of potent architec- tures [6\u20139] has boosted the performance of speech tasks, including ASR, speech translation (ST), and spoken language understanding (SLU). Also, novel training paradigms have demonstrated improved performance and generalization, including self-supervised learning (SSL) models [10\u201314] and Whisper [15]. In the majority of prior endeavors, high-dimensional features are derived from raw wave- forms as the input. Conventionally, spectral speech features are ex- tracted from a fixed-length temporal window, such as Mel Frequency Cepstral Coefficients (MFCC) or log Mel filter banks (FBANK). Re- cently, data-driven methods become popular to learn feature extrac- tion using neural networks [12, 13, 16]. However, in most cases, the data storage and transmission efficiency are similar among raw waveforms and speech features [17]. It is not trivial to improve the efficiency of computation without performance degradation. Recently, a few studies have proposed the use of discrete speech units to represent speech signals, where the information of speech signals in a short window is represented by a single token or a few levels from restricted vocabulary [11, 17, 18], as opposed to employing high-dimensional continuous speech features within the ASR task. For instance, in [17], researchers propose to utilize clus- tering indices derived from features of SSL models as input. This approach condenses the information considerably, reducing the orig- inal 1024-sized float vector to a mere 12-bit binary number: over 3000 times less. Such a compression process substantially reduces data storage and transmission size while maintaining predictive per- formance comparable to conventional high-dimensional features. Moreover, input sequence lengths can be drastically reduced via de-duplication and subword modeling on discrete units, leading to more than 2X faster training and inference. Notably, discrete unit representations can be regarded as a spoken language similar to text data in NLP tasks, which is more straightforward to unify the tasks or models. Being an alternative to traditional speech representation, this paradigm has already been applied in other domains, such as speech translation [19, 20] and audio generation [21\u201325]. To provide an extensive guide for discrete speech units, we con- ducted a comprehensive exploration into the effectiveness of discrete speech units across various speech processing tasks. We summarize our experiments and findings as follows: A comparative analysis is conducted under fair conditions, eval- uating the performance and training time reduction using discrete speech units as opposed to traditional speech features. \u2217Authors are ordered by the organizations. \u2020Corresponding author. A diverse range of benchmarks, including 12 ASR (Section 3.2), 3 ST (Section 3.3), and 1 SLU (Section 3.4) corpora, are mostly evaluated for the first time. To demonstrate wide applicability of the discrete units, we adopted noisy speech, spontaneous speech, telephony speech, and several multi-lingual speech corpora, which would be the first work to explore these aspects (Section 3.2.1). We show the versatility of discrete units in various E2E frame- works, including connectionist temporal classification (CTC) [3], attention-based encoder-decoder (AED) [5], and RNN-Transducer [4] (Table 3). We share various tips based on our investigations to get better performance, including SSL feature choice and discretization. Selecting SSL features based on canonical correlation analysis (CCA) [26] improves performance significantly compared to prior work [17] (Section 3.1). We also explore other possible choices of discrete units, includ- ing clustering SSL [14] or supervised representations, or vector quantization of neural codec models [27] (Section 3.2.5). We will release fully reproducible recipes and trained models on ESPnet [28], which can significantly benefit the community. 2. SPEECH PROCESSING WITH DISCRETE TOKENS This section elaborates on the details of our speech processing mod- els, which take discrete units as input. Figure 1 summarizes the whole pipeline. Leveraging the sequence-to-sequence (Seq2Seq) paradigm as our backbone framework enables broad applicability across a spectrum of tasks involving the transformation of speech signals into diverse target outputs. 2.1. Speech Discretization The discretization process is the pivotal step, which transforms the speech signals into discrete representations."}, {"question": " What methods can further compress the speech sequence length when using discrete speech units?", "answer": " Methods such as de-duplication and subword modeling.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 0 0 8 5 1 . 9 0 3 2 : v i X r a EXPLORING SPEECH RECOGNITION, TRANSLATION, AND UNDERSTANDING WITH DISCRETE SPEECH UNITS: A COMPARATIVE STUDY Xuankai Chang1, Brian Yan1, Kwanghee Choi1, Jeeweon Jung1, Yichen Lu1, Soumi Maiti1, Roshan Sharma1, Jiatong Shi1, Jinchuan Tian1, Shinji Watanabe1\u2020, Yuya Fujita2, Takashi Maekaku2, Pengcheng Guo3, Yao-Fei Cheng4, Pavel Denisov5, Kohei Saijo6, Hsiu-Hsuan Wang7\u2217 1Carnegie Mellon University, 2Yahoo Japan Corporation, 3Northwestern Polytechnical University, 4University of Washington,5University of Stuttgart, 6Waseda University,7National Taiwan University ABSTRACT Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of dis- crete speech units derived from self-supervised learning representa- tions, which significantly compresses the size of speech data. Apply- ing various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech pro- cessing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts. Fig. 1: Illustration of the E2E speech processing model with discrete speech units. The speech discretization process is shown on the left. On the right side is a Seq2Seq model that takes discrete units to output target text. Index Terms\u2014 Discrete units, end-to-end, speech recognition, speech translation, spoken language understanding 1. INTRODUCTION Significant progress has been made in the field of automatic speech recognition (ASR) over the past few decades, largely attributed to the evolution of deep neural networks [1, 2]. Since the emergence of end-to-end (E2E) ASR models [3\u20135], there have been many exciting outcomes. Among these achievements, a slew of potent architec- tures [6\u20139] has boosted the performance of speech tasks, including ASR, speech translation (ST), and spoken language understanding (SLU). Also, novel training paradigms have demonstrated improved performance and generalization, including self-supervised learning (SSL) models [10\u201314] and Whisper [15]. In the majority of prior endeavors, high-dimensional features are derived from raw wave- forms as the input. Conventionally, spectral speech features are ex- tracted from a fixed-length temporal window, such as Mel Frequency Cepstral Coefficients (MFCC) or log Mel filter banks (FBANK). Re- cently, data-driven methods become popular to learn feature extrac- tion using neural networks [12, 13, 16]. However, in most cases, the data storage and transmission efficiency are similar among raw waveforms and speech features [17]. It is not trivial to improve the efficiency of computation without performance degradation. Recently, a few studies have proposed the use of discrete speech units to represent speech signals, where the information of speech signals in a short window is represented by a single token or a few levels from restricted vocabulary [11, 17, 18], as opposed to employing high-dimensional continuous speech features within the ASR task. For instance, in [17], researchers propose to utilize clus- tering indices derived from features of SSL models as input. This approach condenses the information considerably, reducing the orig- inal 1024-sized float vector to a mere 12-bit binary number: over 3000 times less. Such a compression process substantially reduces data storage and transmission size while maintaining predictive per- formance comparable to conventional high-dimensional features. Moreover, input sequence lengths can be drastically reduced via de-duplication and subword modeling on discrete units, leading to more than 2X faster training and inference. Notably, discrete unit representations can be regarded as a spoken language similar to text data in NLP tasks, which is more straightforward to unify the tasks or models. Being an alternative to traditional speech representation, this paradigm has already been applied in other domains, such as speech translation [19, 20] and audio generation [21\u201325]. To provide an extensive guide for discrete speech units, we con- ducted a comprehensive exploration into the effectiveness of discrete speech units across various speech processing tasks. We summarize our experiments and findings as follows: A comparative analysis is conducted under fair conditions, eval- uating the performance and training time reduction using discrete speech units as opposed to traditional speech features. \u2217Authors are ordered by the organizations. \u2020Corresponding author. A diverse range of benchmarks, including 12 ASR (Section 3.2), 3 ST (Section 3.3), and 1 SLU (Section 3.4) corpora, are mostly evaluated for the first time. To demonstrate wide applicability of the discrete units, we adopted noisy speech, spontaneous speech, telephony speech, and several multi-lingual speech corpora, which would be the first work to explore these aspects (Section 3.2.1). We show the versatility of discrete units in various E2E frame- works, including connectionist temporal classification (CTC) [3], attention-based encoder-decoder (AED) [5], and RNN-Transducer [4] (Table 3). We share various tips based on our investigations to get better performance, including SSL feature choice and discretization. Selecting SSL features based on canonical correlation analysis (CCA) [26] improves performance significantly compared to prior work [17] (Section 3.1). We also explore other possible choices of discrete units, includ- ing clustering SSL [14] or supervised representations, or vector quantization of neural codec models [27] (Section 3.2.5). We will release fully reproducible recipes and trained models on ESPnet [28], which can significantly benefit the community. 2. SPEECH PROCESSING WITH DISCRETE TOKENS This section elaborates on the details of our speech processing mod- els, which take discrete units as input. Figure 1 summarizes the whole pipeline. Leveraging the sequence-to-sequence (Seq2Seq) paradigm as our backbone framework enables broad applicability across a spectrum of tasks involving the transformation of speech signals into diverse target outputs. 2.1. Speech Discretization The discretization process is the pivotal step, which transforms the speech signals into discrete representations."}, {"question": " What is the outcome of the experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora with discrete speech units?", "answer": " Discrete units achieve reasonably good results in almost all the settings.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 0 0 8 5 1 . 9 0 3 2 : v i X r a EXPLORING SPEECH RECOGNITION, TRANSLATION, AND UNDERSTANDING WITH DISCRETE SPEECH UNITS: A COMPARATIVE STUDY Xuankai Chang1, Brian Yan1, Kwanghee Choi1, Jeeweon Jung1, Yichen Lu1, Soumi Maiti1, Roshan Sharma1, Jiatong Shi1, Jinchuan Tian1, Shinji Watanabe1\u2020, Yuya Fujita2, Takashi Maekaku2, Pengcheng Guo3, Yao-Fei Cheng4, Pavel Denisov5, Kohei Saijo6, Hsiu-Hsuan Wang7\u2217 1Carnegie Mellon University, 2Yahoo Japan Corporation, 3Northwestern Polytechnical University, 4University of Washington,5University of Stuttgart, 6Waseda University,7National Taiwan University ABSTRACT Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of dis- crete speech units derived from self-supervised learning representa- tions, which significantly compresses the size of speech data. Apply- ing various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech pro- cessing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts. Fig. 1: Illustration of the E2E speech processing model with discrete speech units. The speech discretization process is shown on the left. On the right side is a Seq2Seq model that takes discrete units to output target text. Index Terms\u2014 Discrete units, end-to-end, speech recognition, speech translation, spoken language understanding 1. INTRODUCTION Significant progress has been made in the field of automatic speech recognition (ASR) over the past few decades, largely attributed to the evolution of deep neural networks [1, 2]. Since the emergence of end-to-end (E2E) ASR models [3\u20135], there have been many exciting outcomes. Among these achievements, a slew of potent architec- tures [6\u20139] has boosted the performance of speech tasks, including ASR, speech translation (ST), and spoken language understanding (SLU). Also, novel training paradigms have demonstrated improved performance and generalization, including self-supervised learning (SSL) models [10\u201314] and Whisper [15]. In the majority of prior endeavors, high-dimensional features are derived from raw wave- forms as the input. Conventionally, spectral speech features are ex- tracted from a fixed-length temporal window, such as Mel Frequency Cepstral Coefficients (MFCC) or log Mel filter banks (FBANK). Re- cently, data-driven methods become popular to learn feature extrac- tion using neural networks [12, 13, 16]. However, in most cases, the data storage and transmission efficiency are similar among raw waveforms and speech features [17]. It is not trivial to improve the efficiency of computation without performance degradation. Recently, a few studies have proposed the use of discrete speech units to represent speech signals, where the information of speech signals in a short window is represented by a single token or a few levels from restricted vocabulary [11, 17, 18], as opposed to employing high-dimensional continuous speech features within the ASR task. For instance, in [17], researchers propose to utilize clus- tering indices derived from features of SSL models as input. This approach condenses the information considerably, reducing the orig- inal 1024-sized float vector to a mere 12-bit binary number: over 3000 times less. Such a compression process substantially reduces data storage and transmission size while maintaining predictive per- formance comparable to conventional high-dimensional features. Moreover, input sequence lengths can be drastically reduced via de-duplication and subword modeling on discrete units, leading to more than 2X faster training and inference. Notably, discrete unit representations can be regarded as a spoken language similar to text data in NLP tasks, which is more straightforward to unify the tasks or models. Being an alternative to traditional speech representation, this paradigm has already been applied in other domains, such as speech translation [19, 20] and audio generation [21\u201325]. To provide an extensive guide for discrete speech units, we con- ducted a comprehensive exploration into the effectiveness of discrete speech units across various speech processing tasks. We summarize our experiments and findings as follows: A comparative analysis is conducted under fair conditions, eval- uating the performance and training time reduction using discrete speech units as opposed to traditional speech features. \u2217Authors are ordered by the organizations. \u2020Corresponding author. A diverse range of benchmarks, including 12 ASR (Section 3.2), 3 ST (Section 3.3), and 1 SLU (Section 3.4) corpora, are mostly evaluated for the first time. To demonstrate wide applicability of the discrete units, we adopted noisy speech, spontaneous speech, telephony speech, and several multi-lingual speech corpora, which would be the first work to explore these aspects (Section 3.2.1). We show the versatility of discrete units in various E2E frame- works, including connectionist temporal classification (CTC) [3], attention-based encoder-decoder (AED) [5], and RNN-Transducer [4] (Table 3). We share various tips based on our investigations to get better performance, including SSL feature choice and discretization. Selecting SSL features based on canonical correlation analysis (CCA) [26] improves performance significantly compared to prior work [17] (Section 3.1). We also explore other possible choices of discrete units, includ- ing clustering SSL [14] or supervised representations, or vector quantization of neural codec models [27] (Section 3.2.5). We will release fully reproducible recipes and trained models on ESPnet [28], which can significantly benefit the community. 2. SPEECH PROCESSING WITH DISCRETE TOKENS This section elaborates on the details of our speech processing mod- els, which take discrete units as input. Figure 1 summarizes the whole pipeline. Leveraging the sequence-to-sequence (Seq2Seq) paradigm as our backbone framework enables broad applicability across a spectrum of tasks involving the transformation of speech signals into diverse target outputs. 2.1. Speech Discretization The discretization process is the pivotal step, which transforms the speech signals into discrete representations."}, {"question": " How do discrete speech units represent speech signals, as described in the text?", "answer": " The information of speech signals in a short window is represented by a single token or a few levels from restricted vocabulary.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 0 0 8 5 1 . 9 0 3 2 : v i X r a EXPLORING SPEECH RECOGNITION, TRANSLATION, AND UNDERSTANDING WITH DISCRETE SPEECH UNITS: A COMPARATIVE STUDY Xuankai Chang1, Brian Yan1, Kwanghee Choi1, Jeeweon Jung1, Yichen Lu1, Soumi Maiti1, Roshan Sharma1, Jiatong Shi1, Jinchuan Tian1, Shinji Watanabe1\u2020, Yuya Fujita2, Takashi Maekaku2, Pengcheng Guo3, Yao-Fei Cheng4, Pavel Denisov5, Kohei Saijo6, Hsiu-Hsuan Wang7\u2217 1Carnegie Mellon University, 2Yahoo Japan Corporation, 3Northwestern Polytechnical University, 4University of Washington,5University of Stuttgart, 6Waseda University,7National Taiwan University ABSTRACT Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of dis- crete speech units derived from self-supervised learning representa- tions, which significantly compresses the size of speech data. Apply- ing various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech pro- cessing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts. Fig. 1: Illustration of the E2E speech processing model with discrete speech units. The speech discretization process is shown on the left. On the right side is a Seq2Seq model that takes discrete units to output target text. Index Terms\u2014 Discrete units, end-to-end, speech recognition, speech translation, spoken language understanding 1. INTRODUCTION Significant progress has been made in the field of automatic speech recognition (ASR) over the past few decades, largely attributed to the evolution of deep neural networks [1, 2]. Since the emergence of end-to-end (E2E) ASR models [3\u20135], there have been many exciting outcomes. Among these achievements, a slew of potent architec- tures [6\u20139] has boosted the performance of speech tasks, including ASR, speech translation (ST), and spoken language understanding (SLU). Also, novel training paradigms have demonstrated improved performance and generalization, including self-supervised learning (SSL) models [10\u201314] and Whisper [15]. In the majority of prior endeavors, high-dimensional features are derived from raw wave- forms as the input. Conventionally, spectral speech features are ex- tracted from a fixed-length temporal window, such as Mel Frequency Cepstral Coefficients (MFCC) or log Mel filter banks (FBANK). Re- cently, data-driven methods become popular to learn feature extrac- tion using neural networks [12, 13, 16]. However, in most cases, the data storage and transmission efficiency are similar among raw waveforms and speech features [17]. It is not trivial to improve the efficiency of computation without performance degradation. Recently, a few studies have proposed the use of discrete speech units to represent speech signals, where the information of speech signals in a short window is represented by a single token or a few levels from restricted vocabulary [11, 17, 18], as opposed to employing high-dimensional continuous speech features within the ASR task. For instance, in [17], researchers propose to utilize clus- tering indices derived from features of SSL models as input. This approach condenses the information considerably, reducing the orig- inal 1024-sized float vector to a mere 12-bit binary number: over 3000 times less. Such a compression process substantially reduces data storage and transmission size while maintaining predictive per- formance comparable to conventional high-dimensional features. Moreover, input sequence lengths can be drastically reduced via de-duplication and subword modeling on discrete units, leading to more than 2X faster training and inference. Notably, discrete unit representations can be regarded as a spoken language similar to text data in NLP tasks, which is more straightforward to unify the tasks or models. Being an alternative to traditional speech representation, this paradigm has already been applied in other domains, such as speech translation [19, 20] and audio generation [21\u201325]. To provide an extensive guide for discrete speech units, we con- ducted a comprehensive exploration into the effectiveness of discrete speech units across various speech processing tasks. We summarize our experiments and findings as follows: A comparative analysis is conducted under fair conditions, eval- uating the performance and training time reduction using discrete speech units as opposed to traditional speech features. \u2217Authors are ordered by the organizations. \u2020Corresponding author. A diverse range of benchmarks, including 12 ASR (Section 3.2), 3 ST (Section 3.3), and 1 SLU (Section 3.4) corpora, are mostly evaluated for the first time. To demonstrate wide applicability of the discrete units, we adopted noisy speech, spontaneous speech, telephony speech, and several multi-lingual speech corpora, which would be the first work to explore these aspects (Section 3.2.1). We show the versatility of discrete units in various E2E frame- works, including connectionist temporal classification (CTC) [3], attention-based encoder-decoder (AED) [5], and RNN-Transducer [4] (Table 3). We share various tips based on our investigations to get better performance, including SSL feature choice and discretization. Selecting SSL features based on canonical correlation analysis (CCA) [26] improves performance significantly compared to prior work [17] (Section 3.1). We also explore other possible choices of discrete units, includ- ing clustering SSL [14] or supervised representations, or vector quantization of neural codec models [27] (Section 3.2.5). We will release fully reproducible recipes and trained models on ESPnet [28], which can significantly benefit the community. 2. SPEECH PROCESSING WITH DISCRETE TOKENS This section elaborates on the details of our speech processing mod- els, which take discrete units as input. Figure 1 summarizes the whole pipeline. Leveraging the sequence-to-sequence (Seq2Seq) paradigm as our backbone framework enables broad applicability across a spectrum of tasks involving the transformation of speech signals into diverse target outputs. 2.1. Speech Discretization The discretization process is the pivotal step, which transforms the speech signals into discrete representations."}, {"question": " What is the advantage of utilizing discrete speech units over high-dimensional continuous speech features in speech processing tasks?", "answer": " Discrete units substantially reduce data storage and transmission size while maintaining performance comparable to conventional features.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 0 0 8 5 1 . 9 0 3 2 : v i X r a EXPLORING SPEECH RECOGNITION, TRANSLATION, AND UNDERSTANDING WITH DISCRETE SPEECH UNITS: A COMPARATIVE STUDY Xuankai Chang1, Brian Yan1, Kwanghee Choi1, Jeeweon Jung1, Yichen Lu1, Soumi Maiti1, Roshan Sharma1, Jiatong Shi1, Jinchuan Tian1, Shinji Watanabe1\u2020, Yuya Fujita2, Takashi Maekaku2, Pengcheng Guo3, Yao-Fei Cheng4, Pavel Denisov5, Kohei Saijo6, Hsiu-Hsuan Wang7\u2217 1Carnegie Mellon University, 2Yahoo Japan Corporation, 3Northwestern Polytechnical University, 4University of Washington,5University of Stuttgart, 6Waseda University,7National Taiwan University ABSTRACT Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of dis- crete speech units derived from self-supervised learning representa- tions, which significantly compresses the size of speech data. Apply- ing various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech pro- cessing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts. Fig. 1: Illustration of the E2E speech processing model with discrete speech units. The speech discretization process is shown on the left. On the right side is a Seq2Seq model that takes discrete units to output target text. Index Terms\u2014 Discrete units, end-to-end, speech recognition, speech translation, spoken language understanding 1. INTRODUCTION Significant progress has been made in the field of automatic speech recognition (ASR) over the past few decades, largely attributed to the evolution of deep neural networks [1, 2]. Since the emergence of end-to-end (E2E) ASR models [3\u20135], there have been many exciting outcomes. Among these achievements, a slew of potent architec- tures [6\u20139] has boosted the performance of speech tasks, including ASR, speech translation (ST), and spoken language understanding (SLU). Also, novel training paradigms have demonstrated improved performance and generalization, including self-supervised learning (SSL) models [10\u201314] and Whisper [15]. In the majority of prior endeavors, high-dimensional features are derived from raw wave- forms as the input. Conventionally, spectral speech features are ex- tracted from a fixed-length temporal window, such as Mel Frequency Cepstral Coefficients (MFCC) or log Mel filter banks (FBANK). Re- cently, data-driven methods become popular to learn feature extrac- tion using neural networks [12, 13, 16]. However, in most cases, the data storage and transmission efficiency are similar among raw waveforms and speech features [17]. It is not trivial to improve the efficiency of computation without performance degradation. Recently, a few studies have proposed the use of discrete speech units to represent speech signals, where the information of speech signals in a short window is represented by a single token or a few levels from restricted vocabulary [11, 17, 18], as opposed to employing high-dimensional continuous speech features within the ASR task. For instance, in [17], researchers propose to utilize clus- tering indices derived from features of SSL models as input. This approach condenses the information considerably, reducing the orig- inal 1024-sized float vector to a mere 12-bit binary number: over 3000 times less. Such a compression process substantially reduces data storage and transmission size while maintaining predictive per- formance comparable to conventional high-dimensional features. Moreover, input sequence lengths can be drastically reduced via de-duplication and subword modeling on discrete units, leading to more than 2X faster training and inference. Notably, discrete unit representations can be regarded as a spoken language similar to text data in NLP tasks, which is more straightforward to unify the tasks or models. Being an alternative to traditional speech representation, this paradigm has already been applied in other domains, such as speech translation [19, 20] and audio generation [21\u201325]. To provide an extensive guide for discrete speech units, we con- ducted a comprehensive exploration into the effectiveness of discrete speech units across various speech processing tasks. We summarize our experiments and findings as follows: A comparative analysis is conducted under fair conditions, eval- uating the performance and training time reduction using discrete speech units as opposed to traditional speech features. \u2217Authors are ordered by the organizations. \u2020Corresponding author. A diverse range of benchmarks, including 12 ASR (Section 3.2), 3 ST (Section 3.3), and 1 SLU (Section 3.4) corpora, are mostly evaluated for the first time. To demonstrate wide applicability of the discrete units, we adopted noisy speech, spontaneous speech, telephony speech, and several multi-lingual speech corpora, which would be the first work to explore these aspects (Section 3.2.1). We show the versatility of discrete units in various E2E frame- works, including connectionist temporal classification (CTC) [3], attention-based encoder-decoder (AED) [5], and RNN-Transducer [4] (Table 3). We share various tips based on our investigations to get better performance, including SSL feature choice and discretization. Selecting SSL features based on canonical correlation analysis (CCA) [26] improves performance significantly compared to prior work [17] (Section 3.1). We also explore other possible choices of discrete units, includ- ing clustering SSL [14] or supervised representations, or vector quantization of neural codec models [27] (Section 3.2.5). We will release fully reproducible recipes and trained models on ESPnet [28], which can significantly benefit the community. 2. SPEECH PROCESSING WITH DISCRETE TOKENS This section elaborates on the details of our speech processing mod- els, which take discrete units as input. Figure 1 summarizes the whole pipeline. Leveraging the sequence-to-sequence (Seq2Seq) paradigm as our backbone framework enables broad applicability across a spectrum of tasks involving the transformation of speech signals into diverse target outputs. 2.1. Speech Discretization The discretization process is the pivotal step, which transforms the speech signals into discrete representations."}, {"question": " How can input sequence lengths be reduced when using discrete speech units?", "answer": " Input sequence lengths can be drastically reduced via de-duplication and subword modeling on discrete units.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 0 0 8 5 1 . 9 0 3 2 : v i X r a EXPLORING SPEECH RECOGNITION, TRANSLATION, AND UNDERSTANDING WITH DISCRETE SPEECH UNITS: A COMPARATIVE STUDY Xuankai Chang1, Brian Yan1, Kwanghee Choi1, Jeeweon Jung1, Yichen Lu1, Soumi Maiti1, Roshan Sharma1, Jiatong Shi1, Jinchuan Tian1, Shinji Watanabe1\u2020, Yuya Fujita2, Takashi Maekaku2, Pengcheng Guo3, Yao-Fei Cheng4, Pavel Denisov5, Kohei Saijo6, Hsiu-Hsuan Wang7\u2217 1Carnegie Mellon University, 2Yahoo Japan Corporation, 3Northwestern Polytechnical University, 4University of Washington,5University of Stuttgart, 6Waseda University,7National Taiwan University ABSTRACT Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of dis- crete speech units derived from self-supervised learning representa- tions, which significantly compresses the size of speech data. Apply- ing various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech pro- cessing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts. Fig. 1: Illustration of the E2E speech processing model with discrete speech units. The speech discretization process is shown on the left. On the right side is a Seq2Seq model that takes discrete units to output target text. Index Terms\u2014 Discrete units, end-to-end, speech recognition, speech translation, spoken language understanding 1. INTRODUCTION Significant progress has been made in the field of automatic speech recognition (ASR) over the past few decades, largely attributed to the evolution of deep neural networks [1, 2]. Since the emergence of end-to-end (E2E) ASR models [3\u20135], there have been many exciting outcomes. Among these achievements, a slew of potent architec- tures [6\u20139] has boosted the performance of speech tasks, including ASR, speech translation (ST), and spoken language understanding (SLU). Also, novel training paradigms have demonstrated improved performance and generalization, including self-supervised learning (SSL) models [10\u201314] and Whisper [15]. In the majority of prior endeavors, high-dimensional features are derived from raw wave- forms as the input. Conventionally, spectral speech features are ex- tracted from a fixed-length temporal window, such as Mel Frequency Cepstral Coefficients (MFCC) or log Mel filter banks (FBANK). Re- cently, data-driven methods become popular to learn feature extrac- tion using neural networks [12, 13, 16]. However, in most cases, the data storage and transmission efficiency are similar among raw waveforms and speech features [17]. It is not trivial to improve the efficiency of computation without performance degradation. Recently, a few studies have proposed the use of discrete speech units to represent speech signals, where the information of speech signals in a short window is represented by a single token or a few levels from restricted vocabulary [11, 17, 18], as opposed to employing high-dimensional continuous speech features within the ASR task. For instance, in [17], researchers propose to utilize clus- tering indices derived from features of SSL models as input. This approach condenses the information considerably, reducing the orig- inal 1024-sized float vector to a mere 12-bit binary number: over 3000 times less. Such a compression process substantially reduces data storage and transmission size while maintaining predictive per- formance comparable to conventional high-dimensional features. Moreover, input sequence lengths can be drastically reduced via de-duplication and subword modeling on discrete units, leading to more than 2X faster training and inference. Notably, discrete unit representations can be regarded as a spoken language similar to text data in NLP tasks, which is more straightforward to unify the tasks or models. Being an alternative to traditional speech representation, this paradigm has already been applied in other domains, such as speech translation [19, 20] and audio generation [21\u201325]. To provide an extensive guide for discrete speech units, we con- ducted a comprehensive exploration into the effectiveness of discrete speech units across various speech processing tasks. We summarize our experiments and findings as follows: A comparative analysis is conducted under fair conditions, eval- uating the performance and training time reduction using discrete speech units as opposed to traditional speech features. \u2217Authors are ordered by the organizations. \u2020Corresponding author. A diverse range of benchmarks, including 12 ASR (Section 3.2), 3 ST (Section 3.3), and 1 SLU (Section 3.4) corpora, are mostly evaluated for the first time. To demonstrate wide applicability of the discrete units, we adopted noisy speech, spontaneous speech, telephony speech, and several multi-lingual speech corpora, which would be the first work to explore these aspects (Section 3.2.1). We show the versatility of discrete units in various E2E frame- works, including connectionist temporal classification (CTC) [3], attention-based encoder-decoder (AED) [5], and RNN-Transducer [4] (Table 3). We share various tips based on our investigations to get better performance, including SSL feature choice and discretization. Selecting SSL features based on canonical correlation analysis (CCA) [26] improves performance significantly compared to prior work [17] (Section 3.1). We also explore other possible choices of discrete units, includ- ing clustering SSL [14] or supervised representations, or vector quantization of neural codec models [27] (Section 3.2.5). We will release fully reproducible recipes and trained models on ESPnet [28], which can significantly benefit the community. 2. SPEECH PROCESSING WITH DISCRETE TOKENS This section elaborates on the details of our speech processing mod- els, which take discrete units as input. Figure 1 summarizes the whole pipeline. Leveraging the sequence-to-sequence (Seq2Seq) paradigm as our backbone framework enables broad applicability across a spectrum of tasks involving the transformation of speech signals into diverse target outputs. 2.1. Speech Discretization The discretization process is the pivotal step, which transforms the speech signals into discrete representations."}, {"question": " How are traditional speech features typically derived in speech processing models?", "answer": " Conventionally, spectral speech features are extracted from a fixed-length temporal window.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 0 0 8 5 1 . 9 0 3 2 : v i X r a EXPLORING SPEECH RECOGNITION, TRANSLATION, AND UNDERSTANDING WITH DISCRETE SPEECH UNITS: A COMPARATIVE STUDY Xuankai Chang1, Brian Yan1, Kwanghee Choi1, Jeeweon Jung1, Yichen Lu1, Soumi Maiti1, Roshan Sharma1, Jiatong Shi1, Jinchuan Tian1, Shinji Watanabe1\u2020, Yuya Fujita2, Takashi Maekaku2, Pengcheng Guo3, Yao-Fei Cheng4, Pavel Denisov5, Kohei Saijo6, Hsiu-Hsuan Wang7\u2217 1Carnegie Mellon University, 2Yahoo Japan Corporation, 3Northwestern Polytechnical University, 4University of Washington,5University of Stuttgart, 6Waseda University,7National Taiwan University ABSTRACT Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of dis- crete speech units derived from self-supervised learning representa- tions, which significantly compresses the size of speech data. Apply- ing various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech pro- cessing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts. Fig. 1: Illustration of the E2E speech processing model with discrete speech units. The speech discretization process is shown on the left. On the right side is a Seq2Seq model that takes discrete units to output target text. Index Terms\u2014 Discrete units, end-to-end, speech recognition, speech translation, spoken language understanding 1. INTRODUCTION Significant progress has been made in the field of automatic speech recognition (ASR) over the past few decades, largely attributed to the evolution of deep neural networks [1, 2]. Since the emergence of end-to-end (E2E) ASR models [3\u20135], there have been many exciting outcomes. Among these achievements, a slew of potent architec- tures [6\u20139] has boosted the performance of speech tasks, including ASR, speech translation (ST), and spoken language understanding (SLU). Also, novel training paradigms have demonstrated improved performance and generalization, including self-supervised learning (SSL) models [10\u201314] and Whisper [15]. In the majority of prior endeavors, high-dimensional features are derived from raw wave- forms as the input. Conventionally, spectral speech features are ex- tracted from a fixed-length temporal window, such as Mel Frequency Cepstral Coefficients (MFCC) or log Mel filter banks (FBANK). Re- cently, data-driven methods become popular to learn feature extrac- tion using neural networks [12, 13, 16]. However, in most cases, the data storage and transmission efficiency are similar among raw waveforms and speech features [17]. It is not trivial to improve the efficiency of computation without performance degradation. Recently, a few studies have proposed the use of discrete speech units to represent speech signals, where the information of speech signals in a short window is represented by a single token or a few levels from restricted vocabulary [11, 17, 18], as opposed to employing high-dimensional continuous speech features within the ASR task. For instance, in [17], researchers propose to utilize clus- tering indices derived from features of SSL models as input. This approach condenses the information considerably, reducing the orig- inal 1024-sized float vector to a mere 12-bit binary number: over 3000 times less. Such a compression process substantially reduces data storage and transmission size while maintaining predictive per- formance comparable to conventional high-dimensional features. Moreover, input sequence lengths can be drastically reduced via de-duplication and subword modeling on discrete units, leading to more than 2X faster training and inference. Notably, discrete unit representations can be regarded as a spoken language similar to text data in NLP tasks, which is more straightforward to unify the tasks or models. Being an alternative to traditional speech representation, this paradigm has already been applied in other domains, such as speech translation [19, 20] and audio generation [21\u201325]. To provide an extensive guide for discrete speech units, we con- ducted a comprehensive exploration into the effectiveness of discrete speech units across various speech processing tasks. We summarize our experiments and findings as follows: A comparative analysis is conducted under fair conditions, eval- uating the performance and training time reduction using discrete speech units as opposed to traditional speech features. \u2217Authors are ordered by the organizations. \u2020Corresponding author. A diverse range of benchmarks, including 12 ASR (Section 3.2), 3 ST (Section 3.3), and 1 SLU (Section 3.4) corpora, are mostly evaluated for the first time. To demonstrate wide applicability of the discrete units, we adopted noisy speech, spontaneous speech, telephony speech, and several multi-lingual speech corpora, which would be the first work to explore these aspects (Section 3.2.1). We show the versatility of discrete units in various E2E frame- works, including connectionist temporal classification (CTC) [3], attention-based encoder-decoder (AED) [5], and RNN-Transducer [4] (Table 3). We share various tips based on our investigations to get better performance, including SSL feature choice and discretization. Selecting SSL features based on canonical correlation analysis (CCA) [26] improves performance significantly compared to prior work [17] (Section 3.1). We also explore other possible choices of discrete units, includ- ing clustering SSL [14] or supervised representations, or vector quantization of neural codec models [27] (Section 3.2.5). We will release fully reproducible recipes and trained models on ESPnet [28], which can significantly benefit the community. 2. SPEECH PROCESSING WITH DISCRETE TOKENS This section elaborates on the details of our speech processing mod- els, which take discrete units as input. Figure 1 summarizes the whole pipeline. Leveraging the sequence-to-sequence (Seq2Seq) paradigm as our backbone framework enables broad applicability across a spectrum of tasks involving the transformation of speech signals into diverse target outputs. 2.1. Speech Discretization The discretization process is the pivotal step, which transforms the speech signals into discrete representations."}, {"question": " What are some of the frameworks mentioned in the text for E2E speech processing models?", "answer": " Frameworks mentioned include connectionist temporal classification (CTC), attention-based encoder-decoder (AED), and RNN-Transducer.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 0 0 8 5 1 . 9 0 3 2 : v i X r a EXPLORING SPEECH RECOGNITION, TRANSLATION, AND UNDERSTANDING WITH DISCRETE SPEECH UNITS: A COMPARATIVE STUDY Xuankai Chang1, Brian Yan1, Kwanghee Choi1, Jeeweon Jung1, Yichen Lu1, Soumi Maiti1, Roshan Sharma1, Jiatong Shi1, Jinchuan Tian1, Shinji Watanabe1\u2020, Yuya Fujita2, Takashi Maekaku2, Pengcheng Guo3, Yao-Fei Cheng4, Pavel Denisov5, Kohei Saijo6, Hsiu-Hsuan Wang7\u2217 1Carnegie Mellon University, 2Yahoo Japan Corporation, 3Northwestern Polytechnical University, 4University of Washington,5University of Stuttgart, 6Waseda University,7National Taiwan University ABSTRACT Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of dis- crete speech units derived from self-supervised learning representa- tions, which significantly compresses the size of speech data. Apply- ing various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech pro- cessing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts. Fig. 1: Illustration of the E2E speech processing model with discrete speech units. The speech discretization process is shown on the left. On the right side is a Seq2Seq model that takes discrete units to output target text. Index Terms\u2014 Discrete units, end-to-end, speech recognition, speech translation, spoken language understanding 1. INTRODUCTION Significant progress has been made in the field of automatic speech recognition (ASR) over the past few decades, largely attributed to the evolution of deep neural networks [1, 2]. Since the emergence of end-to-end (E2E) ASR models [3\u20135], there have been many exciting outcomes. Among these achievements, a slew of potent architec- tures [6\u20139] has boosted the performance of speech tasks, including ASR, speech translation (ST), and spoken language understanding (SLU). Also, novel training paradigms have demonstrated improved performance and generalization, including self-supervised learning (SSL) models [10\u201314] and Whisper [15]. In the majority of prior endeavors, high-dimensional features are derived from raw wave- forms as the input. Conventionally, spectral speech features are ex- tracted from a fixed-length temporal window, such as Mel Frequency Cepstral Coefficients (MFCC) or log Mel filter banks (FBANK). Re- cently, data-driven methods become popular to learn feature extrac- tion using neural networks [12, 13, 16]. However, in most cases, the data storage and transmission efficiency are similar among raw waveforms and speech features [17]. It is not trivial to improve the efficiency of computation without performance degradation. Recently, a few studies have proposed the use of discrete speech units to represent speech signals, where the information of speech signals in a short window is represented by a single token or a few levels from restricted vocabulary [11, 17, 18], as opposed to employing high-dimensional continuous speech features within the ASR task. For instance, in [17], researchers propose to utilize clus- tering indices derived from features of SSL models as input. This approach condenses the information considerably, reducing the orig- inal 1024-sized float vector to a mere 12-bit binary number: over 3000 times less. Such a compression process substantially reduces data storage and transmission size while maintaining predictive per- formance comparable to conventional high-dimensional features. Moreover, input sequence lengths can be drastically reduced via de-duplication and subword modeling on discrete units, leading to more than 2X faster training and inference. Notably, discrete unit representations can be regarded as a spoken language similar to text data in NLP tasks, which is more straightforward to unify the tasks or models. Being an alternative to traditional speech representation, this paradigm has already been applied in other domains, such as speech translation [19, 20] and audio generation [21\u201325]. To provide an extensive guide for discrete speech units, we con- ducted a comprehensive exploration into the effectiveness of discrete speech units across various speech processing tasks. We summarize our experiments and findings as follows: A comparative analysis is conducted under fair conditions, eval- uating the performance and training time reduction using discrete speech units as opposed to traditional speech features. \u2217Authors are ordered by the organizations. \u2020Corresponding author. A diverse range of benchmarks, including 12 ASR (Section 3.2), 3 ST (Section 3.3), and 1 SLU (Section 3.4) corpora, are mostly evaluated for the first time. To demonstrate wide applicability of the discrete units, we adopted noisy speech, spontaneous speech, telephony speech, and several multi-lingual speech corpora, which would be the first work to explore these aspects (Section 3.2.1). We show the versatility of discrete units in various E2E frame- works, including connectionist temporal classification (CTC) [3], attention-based encoder-decoder (AED) [5], and RNN-Transducer [4] (Table 3). We share various tips based on our investigations to get better performance, including SSL feature choice and discretization. Selecting SSL features based on canonical correlation analysis (CCA) [26] improves performance significantly compared to prior work [17] (Section 3.1). We also explore other possible choices of discrete units, includ- ing clustering SSL [14] or supervised representations, or vector quantization of neural codec models [27] (Section 3.2.5). We will release fully reproducible recipes and trained models on ESPnet [28], which can significantly benefit the community. 2. SPEECH PROCESSING WITH DISCRETE TOKENS This section elaborates on the details of our speech processing mod- els, which take discrete units as input. Figure 1 summarizes the whole pipeline. Leveraging the sequence-to-sequence (Seq2Seq) paradigm as our backbone framework enables broad applicability across a spectrum of tasks involving the transformation of speech signals into diverse target outputs. 2.1. Speech Discretization The discretization process is the pivotal step, which transforms the speech signals into discrete representations."}, {"question": " What tips are shared based on the investigations in the text to improve performance when using discrete speech units?", "answer": " Tips include selecting SSL features based on canonical correlation analysis (CCA) and exploring other possible choices of discrete units.", "ref_chunk": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 0 0 8 5 1 . 9 0 3 2 : v i X r a EXPLORING SPEECH RECOGNITION, TRANSLATION, AND UNDERSTANDING WITH DISCRETE SPEECH UNITS: A COMPARATIVE STUDY Xuankai Chang1, Brian Yan1, Kwanghee Choi1, Jeeweon Jung1, Yichen Lu1, Soumi Maiti1, Roshan Sharma1, Jiatong Shi1, Jinchuan Tian1, Shinji Watanabe1\u2020, Yuya Fujita2, Takashi Maekaku2, Pengcheng Guo3, Yao-Fei Cheng4, Pavel Denisov5, Kohei Saijo6, Hsiu-Hsuan Wang7\u2217 1Carnegie Mellon University, 2Yahoo Japan Corporation, 3Northwestern Polytechnical University, 4University of Washington,5University of Stuttgart, 6Waseda University,7National Taiwan University ABSTRACT Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of dis- crete speech units derived from self-supervised learning representa- tions, which significantly compresses the size of speech data. Apply- ing various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech pro- cessing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts. Fig. 1: Illustration of the E2E speech processing model with discrete speech units. The speech discretization process is shown on the left. On the right side is a Seq2Seq model that takes discrete units to output target text. Index Terms\u2014 Discrete units, end-to-end, speech recognition, speech translation, spoken language understanding 1. INTRODUCTION Significant progress has been made in the field of automatic speech recognition (ASR) over the past few decades, largely attributed to the evolution of deep neural networks [1, 2]. Since the emergence of end-to-end (E2E) ASR models [3\u20135], there have been many exciting outcomes. Among these achievements, a slew of potent architec- tures [6\u20139] has boosted the performance of speech tasks, including ASR, speech translation (ST), and spoken language understanding (SLU). Also, novel training paradigms have demonstrated improved performance and generalization, including self-supervised learning (SSL) models [10\u201314] and Whisper [15]. In the majority of prior endeavors, high-dimensional features are derived from raw wave- forms as the input. Conventionally, spectral speech features are ex- tracted from a fixed-length temporal window, such as Mel Frequency Cepstral Coefficients (MFCC) or log Mel filter banks (FBANK). Re- cently, data-driven methods become popular to learn feature extrac- tion using neural networks [12, 13, 16]. However, in most cases, the data storage and transmission efficiency are similar among raw waveforms and speech features [17]. It is not trivial to improve the efficiency of computation without performance degradation. Recently, a few studies have proposed the use of discrete speech units to represent speech signals, where the information of speech signals in a short window is represented by a single token or a few levels from restricted vocabulary [11, 17, 18], as opposed to employing high-dimensional continuous speech features within the ASR task. For instance, in [17], researchers propose to utilize clus- tering indices derived from features of SSL models as input. This approach condenses the information considerably, reducing the orig- inal 1024-sized float vector to a mere 12-bit binary number: over 3000 times less. Such a compression process substantially reduces data storage and transmission size while maintaining predictive per- formance comparable to conventional high-dimensional features. Moreover, input sequence lengths can be drastically reduced via de-duplication and subword modeling on discrete units, leading to more than 2X faster training and inference. Notably, discrete unit representations can be regarded as a spoken language similar to text data in NLP tasks, which is more straightforward to unify the tasks or models. Being an alternative to traditional speech representation, this paradigm has already been applied in other domains, such as speech translation [19, 20] and audio generation [21\u201325]. To provide an extensive guide for discrete speech units, we con- ducted a comprehensive exploration into the effectiveness of discrete speech units across various speech processing tasks. We summarize our experiments and findings as follows: A comparative analysis is conducted under fair conditions, eval- uating the performance and training time reduction using discrete speech units as opposed to traditional speech features. \u2217Authors are ordered by the organizations. \u2020Corresponding author. A diverse range of benchmarks, including 12 ASR (Section 3.2), 3 ST (Section 3.3), and 1 SLU (Section 3.4) corpora, are mostly evaluated for the first time. To demonstrate wide applicability of the discrete units, we adopted noisy speech, spontaneous speech, telephony speech, and several multi-lingual speech corpora, which would be the first work to explore these aspects (Section 3.2.1). We show the versatility of discrete units in various E2E frame- works, including connectionist temporal classification (CTC) [3], attention-based encoder-decoder (AED) [5], and RNN-Transducer [4] (Table 3). We share various tips based on our investigations to get better performance, including SSL feature choice and discretization. Selecting SSL features based on canonical correlation analysis (CCA) [26] improves performance significantly compared to prior work [17] (Section 3.1). We also explore other possible choices of discrete units, includ- ing clustering SSL [14] or supervised representations, or vector quantization of neural codec models [27] (Section 3.2.5). We will release fully reproducible recipes and trained models on ESPnet [28], which can significantly benefit the community. 2. SPEECH PROCESSING WITH DISCRETE TOKENS This section elaborates on the details of our speech processing mod- els, which take discrete units as input. Figure 1 summarizes the whole pipeline. Leveraging the sequence-to-sequence (Seq2Seq) paradigm as our backbone framework enables broad applicability across a spectrum of tasks involving the transformation of speech signals into diverse target outputs. 2.1. Speech Discretization The discretization process is the pivotal step, which transforms the speech signals into discrete representations."}], "doc_text": "3 2 0 2 p e S 7 2 ] L C . s c [ 1 v 0 0 8 5 1 . 9 0 3 2 : v i X r a EXPLORING SPEECH RECOGNITION, TRANSLATION, AND UNDERSTANDING WITH DISCRETE SPEECH UNITS: A COMPARATIVE STUDY Xuankai Chang1, Brian Yan1, Kwanghee Choi1, Jeeweon Jung1, Yichen Lu1, Soumi Maiti1, Roshan Sharma1, Jiatong Shi1, Jinchuan Tian1, Shinji Watanabe1\u2020, Yuya Fujita2, Takashi Maekaku2, Pengcheng Guo3, Yao-Fei Cheng4, Pavel Denisov5, Kohei Saijo6, Hsiu-Hsuan Wang7\u2217 1Carnegie Mellon University, 2Yahoo Japan Corporation, 3Northwestern Polytechnical University, 4University of Washington,5University of Stuttgart, 6Waseda University,7National Taiwan University ABSTRACT Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of dis- crete speech units derived from self-supervised learning representa- tions, which significantly compresses the size of speech data. Apply- ing various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech pro- cessing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts. Fig. 1: Illustration of the E2E speech processing model with discrete speech units. The speech discretization process is shown on the left. On the right side is a Seq2Seq model that takes discrete units to output target text. Index Terms\u2014 Discrete units, end-to-end, speech recognition, speech translation, spoken language understanding 1. INTRODUCTION Significant progress has been made in the field of automatic speech recognition (ASR) over the past few decades, largely attributed to the evolution of deep neural networks [1, 2]. Since the emergence of end-to-end (E2E) ASR models [3\u20135], there have been many exciting outcomes. Among these achievements, a slew of potent architec- tures [6\u20139] has boosted the performance of speech tasks, including ASR, speech translation (ST), and spoken language understanding (SLU). Also, novel training paradigms have demonstrated improved performance and generalization, including self-supervised learning (SSL) models [10\u201314] and Whisper [15]. In the majority of prior endeavors, high-dimensional features are derived from raw wave- forms as the input. Conventionally, spectral speech features are ex- tracted from a fixed-length temporal window, such as Mel Frequency Cepstral Coefficients (MFCC) or log Mel filter banks (FBANK). Re- cently, data-driven methods become popular to learn feature extrac- tion using neural networks [12, 13, 16]. However, in most cases, the data storage and transmission efficiency are similar among raw waveforms and speech features [17]. It is not trivial to improve the efficiency of computation without performance degradation. Recently, a few studies have proposed the use of discrete speech units to represent speech signals, where the information of speech signals in a short window is represented by a single token or a few levels from restricted vocabulary [11, 17, 18], as opposed to employing high-dimensional continuous speech features within the ASR task. For instance, in [17], researchers propose to utilize clus- tering indices derived from features of SSL models as input. This approach condenses the information considerably, reducing the orig- inal 1024-sized float vector to a mere 12-bit binary number: over 3000 times less. Such a compression process substantially reduces data storage and transmission size while maintaining predictive per- formance comparable to conventional high-dimensional features. Moreover, input sequence lengths can be drastically reduced via de-duplication and subword modeling on discrete units, leading to more than 2X faster training and inference. Notably, discrete unit representations can be regarded as a spoken language similar to text data in NLP tasks, which is more straightforward to unify the tasks or models. Being an alternative to traditional speech representation, this paradigm has already been applied in other domains, such as speech translation [19, 20] and audio generation [21\u201325]. To provide an extensive guide for discrete speech units, we con- ducted a comprehensive exploration into the effectiveness of discrete speech units across various speech processing tasks. We summarize our experiments and findings as follows: A comparative analysis is conducted under fair conditions, eval- uating the performance and training time reduction using discrete speech units as opposed to traditional speech features. \u2217Authors are ordered by the organizations. \u2020Corresponding author. A diverse range of benchmarks, including 12 ASR (Section 3.2), 3 ST (Section 3.3), and 1 SLU (Section 3.4) corpora, are mostly evaluated for the first time. To demonstrate wide applicability of the discrete units, we adopted noisy speech, spontaneous speech, telephony speech, and several multi-lingual speech corpora, which would be the first work to explore these aspects (Section 3.2.1). We show the versatility of discrete units in various E2E frame- works, including connectionist temporal classification (CTC) [3], attention-based encoder-decoder (AED) [5], and RNN-Transducer [4] (Table 3). We share various tips based on our investigations to get better performance, including SSL feature choice and discretization. Selecting SSL features based on canonical correlation analysis (CCA) [26] improves performance significantly compared to prior work [17] (Section 3.1). We also explore other possible choices of discrete units, includ- ing clustering SSL [14] or supervised representations, or vector quantization of neural codec models [27] (Section 3.2.5). We will release fully reproducible recipes and trained models on ESPnet [28], which can significantly benefit the community. 2. SPEECH PROCESSING WITH DISCRETE TOKENS This section elaborates on the details of our speech processing mod- els, which take discrete units as input. Figure 1 summarizes the whole pipeline. Leveraging the sequence-to-sequence (Seq2Seq) paradigm as our backbone framework enables broad applicability across a spectrum of tasks involving the transformation of speech signals into diverse target outputs. 2.1. Speech Discretization The discretization process is the pivotal step, which transforms the speech signals into discrete representations."}