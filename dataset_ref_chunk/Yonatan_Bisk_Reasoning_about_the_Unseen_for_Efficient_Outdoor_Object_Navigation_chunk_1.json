{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yonatan_Bisk_Reasoning_about_the_Unseen_for_Efficient_Outdoor_Object_Navigation_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the focus of recent advancements in Object Goal Navigation (OGN)?", "answer": " Navigating in indoor environments", "ref_chunk": "3 2 0 2 p e S 8 1 ] O R . s c [ 1 v 3 0 1 0 1 . 9 0 3 2 : v i X r a Reasoning about the Unseen for Efficient Outdoor Object Navigation Quanting Xie1 Tianyi Zhang1 Kedi Xu1 Matthew Johnson-Roberson1 Yonatan Bisk1 Fig. 1. Direct application of Language Models in embodied agents navigating outdoor environments suffers from short-sightedness and limited environment comprehension. Our approach augments the LLM by enabling it to expand imaginary nodes in space, enhancing feasibility for outdoor navigation. Abstract\u2014 Robots should exist anywhere humans do: in- doors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation (OGN)[1]\u2013[3] has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches. I. INTRODUCTION Advancements in Object Goal Navigation (OGN) [1]\u2013[3] have enhanced the proficiency of robotic agents in navigating indoor environments by leveraging spatial and semantic cues. Agents that can guide humans (e.g. the visually impaired [4]) are an important enabling technology, but need to move beyond restricted indoor spaces to the full richness of outdoor navigation. Outdoor environments are substantially larger 1Carnegie Mellon University, Pittsburgh, PA USA. {quantinx, tianyiz4, kedix, mkj, ybisk}@andrew.cmu.edu 2https://github.com/quantingxie/ReasonedExplorer than handled by current semantic mapping approaches [5], lack clear [6], have complex terrains [7], and, crucially, semantic delineations. Not only is sensing simplified indoors, but so is reasoning as rooms are easily distinguished and semantically categorized. Outdoor environments still have semantic distinctions but visually identical spaces might be a soccer field, a picnic area, or the pit of an outdoor orchestra depending on the time of day. Additionally, outdoor naviga- tional tasks typically demand that robotic agents engage in roles with more granular goal specifications. For instance, in the context of search and rescue operations, the objective is not merely to navigate to a general category of \u2018people\u2019 but to pinpoint casualties potentially trapped under a car. [8]\u2013[10] trained on expansive internet datasets are serving as adapt- able policies in embodied platforms, making them proficient in addressing a wider range of tasks [11]\u2013[13]. The existing work has primarily focused on high-level task-planning with predefined skills in constrained environments. Despite this, we have seem very promising skill demonstrations in indoor object-scenarios made possible by these models [14]\u2013[16]. While some emergent behavior has been identified, the language and vision communities have begun harnessing these LLMs for their reasoning capabilities due to the vast world knowledge and models stored in their parameters. Recently, Large Language Models (LLMs) Thus, we posit that outdoor navigation offers a promising avenue to test and refine the foundational navigation and reasoning abilities of LLMs. This paper aims to formulate an elementary navigation policy and evaluate its efficacy in diverse and challenging outdoor environments, providing insights into the potential of LLMs as embodied agents. Fig. 2. Above are example queries at varying levels of complexity and a representative scene in our OUTDOOR task. Our primary contribution in this work are: 1) We introduce the OUTDOOR (Outdoor Underspecified Task Descriptions Of Objects and Regions) task, which dramatically increases the complexity inherent in ob- ject goal navigation for outdoor settings. 2) We introduce a novel use of LLMs as a planning agent to traverse real-world outdoor terrains. Our approach imagines future notes for a RRT (Rapidly-exploring Random Tree) to improve agent success (+50.4%). 3) We introduce the CASR (Computationally Adjusted Success Rate) metric, that trades off planning costs with time spent \u201cthinking\u201d (i.e. querying LLMs). II. TASK DEFINITION A. OUTDOOR: Outdoor Underspecified Task Descriptions Of Objects and Regions In traditional Object Goal Navigation, users specify dis- tinct goal categories that can be automatically evaluated by the system and do not include contraints or handle underspecified goals (e.g. reference by affordance). However, real-world outdoor environments like parks present more complex scenarios. For example, if the goal is to find a place to eat, it could refer to any bench or table, rather than a specific one. OUTDOOR embraces ambiguity, generalizing to a more nuanced and realistic navigational challenge. We categorize the instruction complexity into four levels: 1) Level 1: Navigate to obj X [aka traditional object-nav] 2) Level 2: Navigate to obj X conditioned on obj Y 3) Level 3: Navigate to obj X conditioned on path P 4) Level 4: Navigate to underspecific abstraction A Human intervention is essential for evaluating success across all levels from 1 to 4. While the goals in Levels 1 to 3 are specific and relatively straightforward to assess, Level 4 presents a more abstract goal. For example, a directive like \u201dFind me somewhere to take a nap?\u201d makes the evaluation more nuanced, potentially necessitating human evaluation. Agents start an episode from a pose s0 and are given a linguistic goal x from one of the aforementioned levels. The agent\u2019s challenge is to reconcile real-time environmental observations with its interpretation of x and understand the semantic and spatial relationships between the objects and regions present. Operating autonomously, the agent must then navigate the environment, with the path being repre- sented as {s0, a0, s1, a1, . . . , sT , aT }, where each action at transitions to a pose st+1. The episode terminates when"}, {"question": " Why do outdoor environments present new challenges compared to indoor environments?", "answer": " Outdoor environments lack clear spatial delineations and are riddled with semantic ambiguities", "ref_chunk": "3 2 0 2 p e S 8 1 ] O R . s c [ 1 v 3 0 1 0 1 . 9 0 3 2 : v i X r a Reasoning about the Unseen for Efficient Outdoor Object Navigation Quanting Xie1 Tianyi Zhang1 Kedi Xu1 Matthew Johnson-Roberson1 Yonatan Bisk1 Fig. 1. Direct application of Language Models in embodied agents navigating outdoor environments suffers from short-sightedness and limited environment comprehension. Our approach augments the LLM by enabling it to expand imaginary nodes in space, enhancing feasibility for outdoor navigation. Abstract\u2014 Robots should exist anywhere humans do: in- doors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation (OGN)[1]\u2013[3] has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches. I. INTRODUCTION Advancements in Object Goal Navigation (OGN) [1]\u2013[3] have enhanced the proficiency of robotic agents in navigating indoor environments by leveraging spatial and semantic cues. Agents that can guide humans (e.g. the visually impaired [4]) are an important enabling technology, but need to move beyond restricted indoor spaces to the full richness of outdoor navigation. Outdoor environments are substantially larger 1Carnegie Mellon University, Pittsburgh, PA USA. {quantinx, tianyiz4, kedix, mkj, ybisk}@andrew.cmu.edu 2https://github.com/quantingxie/ReasonedExplorer than handled by current semantic mapping approaches [5], lack clear [6], have complex terrains [7], and, crucially, semantic delineations. Not only is sensing simplified indoors, but so is reasoning as rooms are easily distinguished and semantically categorized. Outdoor environments still have semantic distinctions but visually identical spaces might be a soccer field, a picnic area, or the pit of an outdoor orchestra depending on the time of day. Additionally, outdoor naviga- tional tasks typically demand that robotic agents engage in roles with more granular goal specifications. For instance, in the context of search and rescue operations, the objective is not merely to navigate to a general category of \u2018people\u2019 but to pinpoint casualties potentially trapped under a car. [8]\u2013[10] trained on expansive internet datasets are serving as adapt- able policies in embodied platforms, making them proficient in addressing a wider range of tasks [11]\u2013[13]. The existing work has primarily focused on high-level task-planning with predefined skills in constrained environments. Despite this, we have seem very promising skill demonstrations in indoor object-scenarios made possible by these models [14]\u2013[16]. While some emergent behavior has been identified, the language and vision communities have begun harnessing these LLMs for their reasoning capabilities due to the vast world knowledge and models stored in their parameters. Recently, Large Language Models (LLMs) Thus, we posit that outdoor navigation offers a promising avenue to test and refine the foundational navigation and reasoning abilities of LLMs. This paper aims to formulate an elementary navigation policy and evaluate its efficacy in diverse and challenging outdoor environments, providing insights into the potential of LLMs as embodied agents. Fig. 2. Above are example queries at varying levels of complexity and a representative scene in our OUTDOOR task. Our primary contribution in this work are: 1) We introduce the OUTDOOR (Outdoor Underspecified Task Descriptions Of Objects and Regions) task, which dramatically increases the complexity inherent in ob- ject goal navigation for outdoor settings. 2) We introduce a novel use of LLMs as a planning agent to traverse real-world outdoor terrains. Our approach imagines future notes for a RRT (Rapidly-exploring Random Tree) to improve agent success (+50.4%). 3) We introduce the CASR (Computationally Adjusted Success Rate) metric, that trades off planning costs with time spent \u201cthinking\u201d (i.e. querying LLMs). II. TASK DEFINITION A. OUTDOOR: Outdoor Underspecified Task Descriptions Of Objects and Regions In traditional Object Goal Navigation, users specify dis- tinct goal categories that can be automatically evaluated by the system and do not include contraints or handle underspecified goals (e.g. reference by affordance). However, real-world outdoor environments like parks present more complex scenarios. For example, if the goal is to find a place to eat, it could refer to any bench or table, rather than a specific one. OUTDOOR embraces ambiguity, generalizing to a more nuanced and realistic navigational challenge. We categorize the instruction complexity into four levels: 1) Level 1: Navigate to obj X [aka traditional object-nav] 2) Level 2: Navigate to obj X conditioned on obj Y 3) Level 3: Navigate to obj X conditioned on path P 4) Level 4: Navigate to underspecific abstraction A Human intervention is essential for evaluating success across all levels from 1 to 4. While the goals in Levels 1 to 3 are specific and relatively straightforward to assess, Level 4 presents a more abstract goal. For example, a directive like \u201dFind me somewhere to take a nap?\u201d makes the evaluation more nuanced, potentially necessitating human evaluation. Agents start an episode from a pose s0 and are given a linguistic goal x from one of the aforementioned levels. The agent\u2019s challenge is to reconcile real-time environmental observations with its interpretation of x and understand the semantic and spatial relationships between the objects and regions present. Operating autonomously, the agent must then navigate the environment, with the path being repre- sented as {s0, a0, s1, a1, . . . , sT , aT }, where each action at transitions to a pose st+1. The episode terminates when"}, {"question": " What is the main advantage that allows humans to navigate outdoor environments with ease?", "answer": " Humans can reason about the unseen", "ref_chunk": "3 2 0 2 p e S 8 1 ] O R . s c [ 1 v 3 0 1 0 1 . 9 0 3 2 : v i X r a Reasoning about the Unseen for Efficient Outdoor Object Navigation Quanting Xie1 Tianyi Zhang1 Kedi Xu1 Matthew Johnson-Roberson1 Yonatan Bisk1 Fig. 1. Direct application of Language Models in embodied agents navigating outdoor environments suffers from short-sightedness and limited environment comprehension. Our approach augments the LLM by enabling it to expand imaginary nodes in space, enhancing feasibility for outdoor navigation. Abstract\u2014 Robots should exist anywhere humans do: in- doors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation (OGN)[1]\u2013[3] has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches. I. INTRODUCTION Advancements in Object Goal Navigation (OGN) [1]\u2013[3] have enhanced the proficiency of robotic agents in navigating indoor environments by leveraging spatial and semantic cues. Agents that can guide humans (e.g. the visually impaired [4]) are an important enabling technology, but need to move beyond restricted indoor spaces to the full richness of outdoor navigation. Outdoor environments are substantially larger 1Carnegie Mellon University, Pittsburgh, PA USA. {quantinx, tianyiz4, kedix, mkj, ybisk}@andrew.cmu.edu 2https://github.com/quantingxie/ReasonedExplorer than handled by current semantic mapping approaches [5], lack clear [6], have complex terrains [7], and, crucially, semantic delineations. Not only is sensing simplified indoors, but so is reasoning as rooms are easily distinguished and semantically categorized. Outdoor environments still have semantic distinctions but visually identical spaces might be a soccer field, a picnic area, or the pit of an outdoor orchestra depending on the time of day. Additionally, outdoor naviga- tional tasks typically demand that robotic agents engage in roles with more granular goal specifications. For instance, in the context of search and rescue operations, the objective is not merely to navigate to a general category of \u2018people\u2019 but to pinpoint casualties potentially trapped under a car. [8]\u2013[10] trained on expansive internet datasets are serving as adapt- able policies in embodied platforms, making them proficient in addressing a wider range of tasks [11]\u2013[13]. The existing work has primarily focused on high-level task-planning with predefined skills in constrained environments. Despite this, we have seem very promising skill demonstrations in indoor object-scenarios made possible by these models [14]\u2013[16]. While some emergent behavior has been identified, the language and vision communities have begun harnessing these LLMs for their reasoning capabilities due to the vast world knowledge and models stored in their parameters. Recently, Large Language Models (LLMs) Thus, we posit that outdoor navigation offers a promising avenue to test and refine the foundational navigation and reasoning abilities of LLMs. This paper aims to formulate an elementary navigation policy and evaluate its efficacy in diverse and challenging outdoor environments, providing insights into the potential of LLMs as embodied agents. Fig. 2. Above are example queries at varying levels of complexity and a representative scene in our OUTDOOR task. Our primary contribution in this work are: 1) We introduce the OUTDOOR (Outdoor Underspecified Task Descriptions Of Objects and Regions) task, which dramatically increases the complexity inherent in ob- ject goal navigation for outdoor settings. 2) We introduce a novel use of LLMs as a planning agent to traverse real-world outdoor terrains. Our approach imagines future notes for a RRT (Rapidly-exploring Random Tree) to improve agent success (+50.4%). 3) We introduce the CASR (Computationally Adjusted Success Rate) metric, that trades off planning costs with time spent \u201cthinking\u201d (i.e. querying LLMs). II. TASK DEFINITION A. OUTDOOR: Outdoor Underspecified Task Descriptions Of Objects and Regions In traditional Object Goal Navigation, users specify dis- tinct goal categories that can be automatically evaluated by the system and do not include contraints or handle underspecified goals (e.g. reference by affordance). However, real-world outdoor environments like parks present more complex scenarios. For example, if the goal is to find a place to eat, it could refer to any bench or table, rather than a specific one. OUTDOOR embraces ambiguity, generalizing to a more nuanced and realistic navigational challenge. We categorize the instruction complexity into four levels: 1) Level 1: Navigate to obj X [aka traditional object-nav] 2) Level 2: Navigate to obj X conditioned on obj Y 3) Level 3: Navigate to obj X conditioned on path P 4) Level 4: Navigate to underspecific abstraction A Human intervention is essential for evaluating success across all levels from 1 to 4. While the goals in Levels 1 to 3 are specific and relatively straightforward to assess, Level 4 presents a more abstract goal. For example, a directive like \u201dFind me somewhere to take a nap?\u201d makes the evaluation more nuanced, potentially necessitating human evaluation. Agents start an episode from a pose s0 and are given a linguistic goal x from one of the aforementioned levels. The agent\u2019s challenge is to reconcile real-time environmental observations with its interpretation of x and understand the semantic and spatial relationships between the objects and regions present. Operating autonomously, the agent must then navigate the environment, with the path being repre- sented as {s0, a0, s1, a1, . . . , sT , aT }, where each action at transitions to a pose st+1. The episode terminates when"}, {"question": " What is the new task introduced in the text?", "answer": " OUTDOOR (Outdoor Underspecified Task Descriptions Of Objects and Regions)", "ref_chunk": "3 2 0 2 p e S 8 1 ] O R . s c [ 1 v 3 0 1 0 1 . 9 0 3 2 : v i X r a Reasoning about the Unseen for Efficient Outdoor Object Navigation Quanting Xie1 Tianyi Zhang1 Kedi Xu1 Matthew Johnson-Roberson1 Yonatan Bisk1 Fig. 1. Direct application of Language Models in embodied agents navigating outdoor environments suffers from short-sightedness and limited environment comprehension. Our approach augments the LLM by enabling it to expand imaginary nodes in space, enhancing feasibility for outdoor navigation. Abstract\u2014 Robots should exist anywhere humans do: in- doors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation (OGN)[1]\u2013[3] has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches. I. INTRODUCTION Advancements in Object Goal Navigation (OGN) [1]\u2013[3] have enhanced the proficiency of robotic agents in navigating indoor environments by leveraging spatial and semantic cues. Agents that can guide humans (e.g. the visually impaired [4]) are an important enabling technology, but need to move beyond restricted indoor spaces to the full richness of outdoor navigation. Outdoor environments are substantially larger 1Carnegie Mellon University, Pittsburgh, PA USA. {quantinx, tianyiz4, kedix, mkj, ybisk}@andrew.cmu.edu 2https://github.com/quantingxie/ReasonedExplorer than handled by current semantic mapping approaches [5], lack clear [6], have complex terrains [7], and, crucially, semantic delineations. Not only is sensing simplified indoors, but so is reasoning as rooms are easily distinguished and semantically categorized. Outdoor environments still have semantic distinctions but visually identical spaces might be a soccer field, a picnic area, or the pit of an outdoor orchestra depending on the time of day. Additionally, outdoor naviga- tional tasks typically demand that robotic agents engage in roles with more granular goal specifications. For instance, in the context of search and rescue operations, the objective is not merely to navigate to a general category of \u2018people\u2019 but to pinpoint casualties potentially trapped under a car. [8]\u2013[10] trained on expansive internet datasets are serving as adapt- able policies in embodied platforms, making them proficient in addressing a wider range of tasks [11]\u2013[13]. The existing work has primarily focused on high-level task-planning with predefined skills in constrained environments. Despite this, we have seem very promising skill demonstrations in indoor object-scenarios made possible by these models [14]\u2013[16]. While some emergent behavior has been identified, the language and vision communities have begun harnessing these LLMs for their reasoning capabilities due to the vast world knowledge and models stored in their parameters. Recently, Large Language Models (LLMs) Thus, we posit that outdoor navigation offers a promising avenue to test and refine the foundational navigation and reasoning abilities of LLMs. This paper aims to formulate an elementary navigation policy and evaluate its efficacy in diverse and challenging outdoor environments, providing insights into the potential of LLMs as embodied agents. Fig. 2. Above are example queries at varying levels of complexity and a representative scene in our OUTDOOR task. Our primary contribution in this work are: 1) We introduce the OUTDOOR (Outdoor Underspecified Task Descriptions Of Objects and Regions) task, which dramatically increases the complexity inherent in ob- ject goal navigation for outdoor settings. 2) We introduce a novel use of LLMs as a planning agent to traverse real-world outdoor terrains. Our approach imagines future notes for a RRT (Rapidly-exploring Random Tree) to improve agent success (+50.4%). 3) We introduce the CASR (Computationally Adjusted Success Rate) metric, that trades off planning costs with time spent \u201cthinking\u201d (i.e. querying LLMs). II. TASK DEFINITION A. OUTDOOR: Outdoor Underspecified Task Descriptions Of Objects and Regions In traditional Object Goal Navigation, users specify dis- tinct goal categories that can be automatically evaluated by the system and do not include contraints or handle underspecified goals (e.g. reference by affordance). However, real-world outdoor environments like parks present more complex scenarios. For example, if the goal is to find a place to eat, it could refer to any bench or table, rather than a specific one. OUTDOOR embraces ambiguity, generalizing to a more nuanced and realistic navigational challenge. We categorize the instruction complexity into four levels: 1) Level 1: Navigate to obj X [aka traditional object-nav] 2) Level 2: Navigate to obj X conditioned on obj Y 3) Level 3: Navigate to obj X conditioned on path P 4) Level 4: Navigate to underspecific abstraction A Human intervention is essential for evaluating success across all levels from 1 to 4. While the goals in Levels 1 to 3 are specific and relatively straightforward to assess, Level 4 presents a more abstract goal. For example, a directive like \u201dFind me somewhere to take a nap?\u201d makes the evaluation more nuanced, potentially necessitating human evaluation. Agents start an episode from a pose s0 and are given a linguistic goal x from one of the aforementioned levels. The agent\u2019s challenge is to reconcile real-time environmental observations with its interpretation of x and understand the semantic and spatial relationships between the objects and regions present. Operating autonomously, the agent must then navigate the environment, with the path being repre- sented as {s0, a0, s1, a1, . . . , sT , aT }, where each action at transitions to a pose st+1. The episode terminates when"}, {"question": " How does the text suggest improving the feasibility of outdoor navigation for agents?", "answer": " By enabling agents to expand imaginary nodes in space", "ref_chunk": "3 2 0 2 p e S 8 1 ] O R . s c [ 1 v 3 0 1 0 1 . 9 0 3 2 : v i X r a Reasoning about the Unseen for Efficient Outdoor Object Navigation Quanting Xie1 Tianyi Zhang1 Kedi Xu1 Matthew Johnson-Roberson1 Yonatan Bisk1 Fig. 1. Direct application of Language Models in embodied agents navigating outdoor environments suffers from short-sightedness and limited environment comprehension. Our approach augments the LLM by enabling it to expand imaginary nodes in space, enhancing feasibility for outdoor navigation. Abstract\u2014 Robots should exist anywhere humans do: in- doors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation (OGN)[1]\u2013[3] has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches. I. INTRODUCTION Advancements in Object Goal Navigation (OGN) [1]\u2013[3] have enhanced the proficiency of robotic agents in navigating indoor environments by leveraging spatial and semantic cues. Agents that can guide humans (e.g. the visually impaired [4]) are an important enabling technology, but need to move beyond restricted indoor spaces to the full richness of outdoor navigation. Outdoor environments are substantially larger 1Carnegie Mellon University, Pittsburgh, PA USA. {quantinx, tianyiz4, kedix, mkj, ybisk}@andrew.cmu.edu 2https://github.com/quantingxie/ReasonedExplorer than handled by current semantic mapping approaches [5], lack clear [6], have complex terrains [7], and, crucially, semantic delineations. Not only is sensing simplified indoors, but so is reasoning as rooms are easily distinguished and semantically categorized. Outdoor environments still have semantic distinctions but visually identical spaces might be a soccer field, a picnic area, or the pit of an outdoor orchestra depending on the time of day. Additionally, outdoor naviga- tional tasks typically demand that robotic agents engage in roles with more granular goal specifications. For instance, in the context of search and rescue operations, the objective is not merely to navigate to a general category of \u2018people\u2019 but to pinpoint casualties potentially trapped under a car. [8]\u2013[10] trained on expansive internet datasets are serving as adapt- able policies in embodied platforms, making them proficient in addressing a wider range of tasks [11]\u2013[13]. The existing work has primarily focused on high-level task-planning with predefined skills in constrained environments. Despite this, we have seem very promising skill demonstrations in indoor object-scenarios made possible by these models [14]\u2013[16]. While some emergent behavior has been identified, the language and vision communities have begun harnessing these LLMs for their reasoning capabilities due to the vast world knowledge and models stored in their parameters. Recently, Large Language Models (LLMs) Thus, we posit that outdoor navigation offers a promising avenue to test and refine the foundational navigation and reasoning abilities of LLMs. This paper aims to formulate an elementary navigation policy and evaluate its efficacy in diverse and challenging outdoor environments, providing insights into the potential of LLMs as embodied agents. Fig. 2. Above are example queries at varying levels of complexity and a representative scene in our OUTDOOR task. Our primary contribution in this work are: 1) We introduce the OUTDOOR (Outdoor Underspecified Task Descriptions Of Objects and Regions) task, which dramatically increases the complexity inherent in ob- ject goal navigation for outdoor settings. 2) We introduce a novel use of LLMs as a planning agent to traverse real-world outdoor terrains. Our approach imagines future notes for a RRT (Rapidly-exploring Random Tree) to improve agent success (+50.4%). 3) We introduce the CASR (Computationally Adjusted Success Rate) metric, that trades off planning costs with time spent \u201cthinking\u201d (i.e. querying LLMs). II. TASK DEFINITION A. OUTDOOR: Outdoor Underspecified Task Descriptions Of Objects and Regions In traditional Object Goal Navigation, users specify dis- tinct goal categories that can be automatically evaluated by the system and do not include contraints or handle underspecified goals (e.g. reference by affordance). However, real-world outdoor environments like parks present more complex scenarios. For example, if the goal is to find a place to eat, it could refer to any bench or table, rather than a specific one. OUTDOOR embraces ambiguity, generalizing to a more nuanced and realistic navigational challenge. We categorize the instruction complexity into four levels: 1) Level 1: Navigate to obj X [aka traditional object-nav] 2) Level 2: Navigate to obj X conditioned on obj Y 3) Level 3: Navigate to obj X conditioned on path P 4) Level 4: Navigate to underspecific abstraction A Human intervention is essential for evaluating success across all levels from 1 to 4. While the goals in Levels 1 to 3 are specific and relatively straightforward to assess, Level 4 presents a more abstract goal. For example, a directive like \u201dFind me somewhere to take a nap?\u201d makes the evaluation more nuanced, potentially necessitating human evaluation. Agents start an episode from a pose s0 and are given a linguistic goal x from one of the aforementioned levels. The agent\u2019s challenge is to reconcile real-time environmental observations with its interpretation of x and understand the semantic and spatial relationships between the objects and regions present. Operating autonomously, the agent must then navigate the environment, with the path being repre- sented as {s0, a0, s1, a1, . . . , sT , aT }, where each action at transitions to a pose st+1. The episode terminates when"}, {"question": " What is the primary contribution of the work described in the text?", "answer": " Introducing the OUTDOOR task and using LLMs for outdoor navigation", "ref_chunk": "3 2 0 2 p e S 8 1 ] O R . s c [ 1 v 3 0 1 0 1 . 9 0 3 2 : v i X r a Reasoning about the Unseen for Efficient Outdoor Object Navigation Quanting Xie1 Tianyi Zhang1 Kedi Xu1 Matthew Johnson-Roberson1 Yonatan Bisk1 Fig. 1. Direct application of Language Models in embodied agents navigating outdoor environments suffers from short-sightedness and limited environment comprehension. Our approach augments the LLM by enabling it to expand imaginary nodes in space, enhancing feasibility for outdoor navigation. Abstract\u2014 Robots should exist anywhere humans do: in- doors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation (OGN)[1]\u2013[3] has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches. I. INTRODUCTION Advancements in Object Goal Navigation (OGN) [1]\u2013[3] have enhanced the proficiency of robotic agents in navigating indoor environments by leveraging spatial and semantic cues. Agents that can guide humans (e.g. the visually impaired [4]) are an important enabling technology, but need to move beyond restricted indoor spaces to the full richness of outdoor navigation. Outdoor environments are substantially larger 1Carnegie Mellon University, Pittsburgh, PA USA. {quantinx, tianyiz4, kedix, mkj, ybisk}@andrew.cmu.edu 2https://github.com/quantingxie/ReasonedExplorer than handled by current semantic mapping approaches [5], lack clear [6], have complex terrains [7], and, crucially, semantic delineations. Not only is sensing simplified indoors, but so is reasoning as rooms are easily distinguished and semantically categorized. Outdoor environments still have semantic distinctions but visually identical spaces might be a soccer field, a picnic area, or the pit of an outdoor orchestra depending on the time of day. Additionally, outdoor naviga- tional tasks typically demand that robotic agents engage in roles with more granular goal specifications. For instance, in the context of search and rescue operations, the objective is not merely to navigate to a general category of \u2018people\u2019 but to pinpoint casualties potentially trapped under a car. [8]\u2013[10] trained on expansive internet datasets are serving as adapt- able policies in embodied platforms, making them proficient in addressing a wider range of tasks [11]\u2013[13]. The existing work has primarily focused on high-level task-planning with predefined skills in constrained environments. Despite this, we have seem very promising skill demonstrations in indoor object-scenarios made possible by these models [14]\u2013[16]. While some emergent behavior has been identified, the language and vision communities have begun harnessing these LLMs for their reasoning capabilities due to the vast world knowledge and models stored in their parameters. Recently, Large Language Models (LLMs) Thus, we posit that outdoor navigation offers a promising avenue to test and refine the foundational navigation and reasoning abilities of LLMs. This paper aims to formulate an elementary navigation policy and evaluate its efficacy in diverse and challenging outdoor environments, providing insights into the potential of LLMs as embodied agents. Fig. 2. Above are example queries at varying levels of complexity and a representative scene in our OUTDOOR task. Our primary contribution in this work are: 1) We introduce the OUTDOOR (Outdoor Underspecified Task Descriptions Of Objects and Regions) task, which dramatically increases the complexity inherent in ob- ject goal navigation for outdoor settings. 2) We introduce a novel use of LLMs as a planning agent to traverse real-world outdoor terrains. Our approach imagines future notes for a RRT (Rapidly-exploring Random Tree) to improve agent success (+50.4%). 3) We introduce the CASR (Computationally Adjusted Success Rate) metric, that trades off planning costs with time spent \u201cthinking\u201d (i.e. querying LLMs). II. TASK DEFINITION A. OUTDOOR: Outdoor Underspecified Task Descriptions Of Objects and Regions In traditional Object Goal Navigation, users specify dis- tinct goal categories that can be automatically evaluated by the system and do not include contraints or handle underspecified goals (e.g. reference by affordance). However, real-world outdoor environments like parks present more complex scenarios. For example, if the goal is to find a place to eat, it could refer to any bench or table, rather than a specific one. OUTDOOR embraces ambiguity, generalizing to a more nuanced and realistic navigational challenge. We categorize the instruction complexity into four levels: 1) Level 1: Navigate to obj X [aka traditional object-nav] 2) Level 2: Navigate to obj X conditioned on obj Y 3) Level 3: Navigate to obj X conditioned on path P 4) Level 4: Navigate to underspecific abstraction A Human intervention is essential for evaluating success across all levels from 1 to 4. While the goals in Levels 1 to 3 are specific and relatively straightforward to assess, Level 4 presents a more abstract goal. For example, a directive like \u201dFind me somewhere to take a nap?\u201d makes the evaluation more nuanced, potentially necessitating human evaluation. Agents start an episode from a pose s0 and are given a linguistic goal x from one of the aforementioned levels. The agent\u2019s challenge is to reconcile real-time environmental observations with its interpretation of x and understand the semantic and spatial relationships between the objects and regions present. Operating autonomously, the agent must then navigate the environment, with the path being repre- sented as {s0, a0, s1, a1, . . . , sT , aT }, where each action at transitions to a pose st+1. The episode terminates when"}, {"question": " What type of metric is introduced to measure the success rate of the agent in outdoor navigation?", "answer": " CASR (Computationally Adjusted Success Rate)", "ref_chunk": "3 2 0 2 p e S 8 1 ] O R . s c [ 1 v 3 0 1 0 1 . 9 0 3 2 : v i X r a Reasoning about the Unseen for Efficient Outdoor Object Navigation Quanting Xie1 Tianyi Zhang1 Kedi Xu1 Matthew Johnson-Roberson1 Yonatan Bisk1 Fig. 1. Direct application of Language Models in embodied agents navigating outdoor environments suffers from short-sightedness and limited environment comprehension. Our approach augments the LLM by enabling it to expand imaginary nodes in space, enhancing feasibility for outdoor navigation. Abstract\u2014 Robots should exist anywhere humans do: in- doors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation (OGN)[1]\u2013[3] has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches. I. INTRODUCTION Advancements in Object Goal Navigation (OGN) [1]\u2013[3] have enhanced the proficiency of robotic agents in navigating indoor environments by leveraging spatial and semantic cues. Agents that can guide humans (e.g. the visually impaired [4]) are an important enabling technology, but need to move beyond restricted indoor spaces to the full richness of outdoor navigation. Outdoor environments are substantially larger 1Carnegie Mellon University, Pittsburgh, PA USA. {quantinx, tianyiz4, kedix, mkj, ybisk}@andrew.cmu.edu 2https://github.com/quantingxie/ReasonedExplorer than handled by current semantic mapping approaches [5], lack clear [6], have complex terrains [7], and, crucially, semantic delineations. Not only is sensing simplified indoors, but so is reasoning as rooms are easily distinguished and semantically categorized. Outdoor environments still have semantic distinctions but visually identical spaces might be a soccer field, a picnic area, or the pit of an outdoor orchestra depending on the time of day. Additionally, outdoor naviga- tional tasks typically demand that robotic agents engage in roles with more granular goal specifications. For instance, in the context of search and rescue operations, the objective is not merely to navigate to a general category of \u2018people\u2019 but to pinpoint casualties potentially trapped under a car. [8]\u2013[10] trained on expansive internet datasets are serving as adapt- able policies in embodied platforms, making them proficient in addressing a wider range of tasks [11]\u2013[13]. The existing work has primarily focused on high-level task-planning with predefined skills in constrained environments. Despite this, we have seem very promising skill demonstrations in indoor object-scenarios made possible by these models [14]\u2013[16]. While some emergent behavior has been identified, the language and vision communities have begun harnessing these LLMs for their reasoning capabilities due to the vast world knowledge and models stored in their parameters. Recently, Large Language Models (LLMs) Thus, we posit that outdoor navigation offers a promising avenue to test and refine the foundational navigation and reasoning abilities of LLMs. This paper aims to formulate an elementary navigation policy and evaluate its efficacy in diverse and challenging outdoor environments, providing insights into the potential of LLMs as embodied agents. Fig. 2. Above are example queries at varying levels of complexity and a representative scene in our OUTDOOR task. Our primary contribution in this work are: 1) We introduce the OUTDOOR (Outdoor Underspecified Task Descriptions Of Objects and Regions) task, which dramatically increases the complexity inherent in ob- ject goal navigation for outdoor settings. 2) We introduce a novel use of LLMs as a planning agent to traverse real-world outdoor terrains. Our approach imagines future notes for a RRT (Rapidly-exploring Random Tree) to improve agent success (+50.4%). 3) We introduce the CASR (Computationally Adjusted Success Rate) metric, that trades off planning costs with time spent \u201cthinking\u201d (i.e. querying LLMs). II. TASK DEFINITION A. OUTDOOR: Outdoor Underspecified Task Descriptions Of Objects and Regions In traditional Object Goal Navigation, users specify dis- tinct goal categories that can be automatically evaluated by the system and do not include contraints or handle underspecified goals (e.g. reference by affordance). However, real-world outdoor environments like parks present more complex scenarios. For example, if the goal is to find a place to eat, it could refer to any bench or table, rather than a specific one. OUTDOOR embraces ambiguity, generalizing to a more nuanced and realistic navigational challenge. We categorize the instruction complexity into four levels: 1) Level 1: Navigate to obj X [aka traditional object-nav] 2) Level 2: Navigate to obj X conditioned on obj Y 3) Level 3: Navigate to obj X conditioned on path P 4) Level 4: Navigate to underspecific abstraction A Human intervention is essential for evaluating success across all levels from 1 to 4. While the goals in Levels 1 to 3 are specific and relatively straightforward to assess, Level 4 presents a more abstract goal. For example, a directive like \u201dFind me somewhere to take a nap?\u201d makes the evaluation more nuanced, potentially necessitating human evaluation. Agents start an episode from a pose s0 and are given a linguistic goal x from one of the aforementioned levels. The agent\u2019s challenge is to reconcile real-time environmental observations with its interpretation of x and understand the semantic and spatial relationships between the objects and regions present. Operating autonomously, the agent must then navigate the environment, with the path being repre- sented as {s0, a0, s1, a1, . . . , sT , aT }, where each action at transitions to a pose st+1. The episode terminates when"}, {"question": " What are the four levels of instruction complexity in the OUTDOOR task?", "answer": " Level 1: Navigate to obj X, Level 2: Navigate to obj X conditioned on obj Y, Level 3: Navigate to obj X conditioned on path P, Level 4: Navigate to underspecific abstraction A", "ref_chunk": "3 2 0 2 p e S 8 1 ] O R . s c [ 1 v 3 0 1 0 1 . 9 0 3 2 : v i X r a Reasoning about the Unseen for Efficient Outdoor Object Navigation Quanting Xie1 Tianyi Zhang1 Kedi Xu1 Matthew Johnson-Roberson1 Yonatan Bisk1 Fig. 1. Direct application of Language Models in embodied agents navigating outdoor environments suffers from short-sightedness and limited environment comprehension. Our approach augments the LLM by enabling it to expand imaginary nodes in space, enhancing feasibility for outdoor navigation. Abstract\u2014 Robots should exist anywhere humans do: in- doors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation (OGN)[1]\u2013[3] has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches. I. INTRODUCTION Advancements in Object Goal Navigation (OGN) [1]\u2013[3] have enhanced the proficiency of robotic agents in navigating indoor environments by leveraging spatial and semantic cues. Agents that can guide humans (e.g. the visually impaired [4]) are an important enabling technology, but need to move beyond restricted indoor spaces to the full richness of outdoor navigation. Outdoor environments are substantially larger 1Carnegie Mellon University, Pittsburgh, PA USA. {quantinx, tianyiz4, kedix, mkj, ybisk}@andrew.cmu.edu 2https://github.com/quantingxie/ReasonedExplorer than handled by current semantic mapping approaches [5], lack clear [6], have complex terrains [7], and, crucially, semantic delineations. Not only is sensing simplified indoors, but so is reasoning as rooms are easily distinguished and semantically categorized. Outdoor environments still have semantic distinctions but visually identical spaces might be a soccer field, a picnic area, or the pit of an outdoor orchestra depending on the time of day. Additionally, outdoor naviga- tional tasks typically demand that robotic agents engage in roles with more granular goal specifications. For instance, in the context of search and rescue operations, the objective is not merely to navigate to a general category of \u2018people\u2019 but to pinpoint casualties potentially trapped under a car. [8]\u2013[10] trained on expansive internet datasets are serving as adapt- able policies in embodied platforms, making them proficient in addressing a wider range of tasks [11]\u2013[13]. The existing work has primarily focused on high-level task-planning with predefined skills in constrained environments. Despite this, we have seem very promising skill demonstrations in indoor object-scenarios made possible by these models [14]\u2013[16]. While some emergent behavior has been identified, the language and vision communities have begun harnessing these LLMs for their reasoning capabilities due to the vast world knowledge and models stored in their parameters. Recently, Large Language Models (LLMs) Thus, we posit that outdoor navigation offers a promising avenue to test and refine the foundational navigation and reasoning abilities of LLMs. This paper aims to formulate an elementary navigation policy and evaluate its efficacy in diverse and challenging outdoor environments, providing insights into the potential of LLMs as embodied agents. Fig. 2. Above are example queries at varying levels of complexity and a representative scene in our OUTDOOR task. Our primary contribution in this work are: 1) We introduce the OUTDOOR (Outdoor Underspecified Task Descriptions Of Objects and Regions) task, which dramatically increases the complexity inherent in ob- ject goal navigation for outdoor settings. 2) We introduce a novel use of LLMs as a planning agent to traverse real-world outdoor terrains. Our approach imagines future notes for a RRT (Rapidly-exploring Random Tree) to improve agent success (+50.4%). 3) We introduce the CASR (Computationally Adjusted Success Rate) metric, that trades off planning costs with time spent \u201cthinking\u201d (i.e. querying LLMs). II. TASK DEFINITION A. OUTDOOR: Outdoor Underspecified Task Descriptions Of Objects and Regions In traditional Object Goal Navigation, users specify dis- tinct goal categories that can be automatically evaluated by the system and do not include contraints or handle underspecified goals (e.g. reference by affordance). However, real-world outdoor environments like parks present more complex scenarios. For example, if the goal is to find a place to eat, it could refer to any bench or table, rather than a specific one. OUTDOOR embraces ambiguity, generalizing to a more nuanced and realistic navigational challenge. We categorize the instruction complexity into four levels: 1) Level 1: Navigate to obj X [aka traditional object-nav] 2) Level 2: Navigate to obj X conditioned on obj Y 3) Level 3: Navigate to obj X conditioned on path P 4) Level 4: Navigate to underspecific abstraction A Human intervention is essential for evaluating success across all levels from 1 to 4. While the goals in Levels 1 to 3 are specific and relatively straightforward to assess, Level 4 presents a more abstract goal. For example, a directive like \u201dFind me somewhere to take a nap?\u201d makes the evaluation more nuanced, potentially necessitating human evaluation. Agents start an episode from a pose s0 and are given a linguistic goal x from one of the aforementioned levels. The agent\u2019s challenge is to reconcile real-time environmental observations with its interpretation of x and understand the semantic and spatial relationships between the objects and regions present. Operating autonomously, the agent must then navigate the environment, with the path being repre- sented as {s0, a0, s1, a1, . . . , sT , aT }, where each action at transitions to a pose st+1. The episode terminates when"}, {"question": " In which types of applications are Large Language Models (LLMs) being used as adaptable policies?", "answer": " Embodied platforms", "ref_chunk": "3 2 0 2 p e S 8 1 ] O R . s c [ 1 v 3 0 1 0 1 . 9 0 3 2 : v i X r a Reasoning about the Unseen for Efficient Outdoor Object Navigation Quanting Xie1 Tianyi Zhang1 Kedi Xu1 Matthew Johnson-Roberson1 Yonatan Bisk1 Fig. 1. Direct application of Language Models in embodied agents navigating outdoor environments suffers from short-sightedness and limited environment comprehension. Our approach augments the LLM by enabling it to expand imaginary nodes in space, enhancing feasibility for outdoor navigation. Abstract\u2014 Robots should exist anywhere humans do: in- doors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation (OGN)[1]\u2013[3] has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches. I. INTRODUCTION Advancements in Object Goal Navigation (OGN) [1]\u2013[3] have enhanced the proficiency of robotic agents in navigating indoor environments by leveraging spatial and semantic cues. Agents that can guide humans (e.g. the visually impaired [4]) are an important enabling technology, but need to move beyond restricted indoor spaces to the full richness of outdoor navigation. Outdoor environments are substantially larger 1Carnegie Mellon University, Pittsburgh, PA USA. {quantinx, tianyiz4, kedix, mkj, ybisk}@andrew.cmu.edu 2https://github.com/quantingxie/ReasonedExplorer than handled by current semantic mapping approaches [5], lack clear [6], have complex terrains [7], and, crucially, semantic delineations. Not only is sensing simplified indoors, but so is reasoning as rooms are easily distinguished and semantically categorized. Outdoor environments still have semantic distinctions but visually identical spaces might be a soccer field, a picnic area, or the pit of an outdoor orchestra depending on the time of day. Additionally, outdoor naviga- tional tasks typically demand that robotic agents engage in roles with more granular goal specifications. For instance, in the context of search and rescue operations, the objective is not merely to navigate to a general category of \u2018people\u2019 but to pinpoint casualties potentially trapped under a car. [8]\u2013[10] trained on expansive internet datasets are serving as adapt- able policies in embodied platforms, making them proficient in addressing a wider range of tasks [11]\u2013[13]. The existing work has primarily focused on high-level task-planning with predefined skills in constrained environments. Despite this, we have seem very promising skill demonstrations in indoor object-scenarios made possible by these models [14]\u2013[16]. While some emergent behavior has been identified, the language and vision communities have begun harnessing these LLMs for their reasoning capabilities due to the vast world knowledge and models stored in their parameters. Recently, Large Language Models (LLMs) Thus, we posit that outdoor navigation offers a promising avenue to test and refine the foundational navigation and reasoning abilities of LLMs. This paper aims to formulate an elementary navigation policy and evaluate its efficacy in diverse and challenging outdoor environments, providing insights into the potential of LLMs as embodied agents. Fig. 2. Above are example queries at varying levels of complexity and a representative scene in our OUTDOOR task. Our primary contribution in this work are: 1) We introduce the OUTDOOR (Outdoor Underspecified Task Descriptions Of Objects and Regions) task, which dramatically increases the complexity inherent in ob- ject goal navigation for outdoor settings. 2) We introduce a novel use of LLMs as a planning agent to traverse real-world outdoor terrains. Our approach imagines future notes for a RRT (Rapidly-exploring Random Tree) to improve agent success (+50.4%). 3) We introduce the CASR (Computationally Adjusted Success Rate) metric, that trades off planning costs with time spent \u201cthinking\u201d (i.e. querying LLMs). II. TASK DEFINITION A. OUTDOOR: Outdoor Underspecified Task Descriptions Of Objects and Regions In traditional Object Goal Navigation, users specify dis- tinct goal categories that can be automatically evaluated by the system and do not include contraints or handle underspecified goals (e.g. reference by affordance). However, real-world outdoor environments like parks present more complex scenarios. For example, if the goal is to find a place to eat, it could refer to any bench or table, rather than a specific one. OUTDOOR embraces ambiguity, generalizing to a more nuanced and realistic navigational challenge. We categorize the instruction complexity into four levels: 1) Level 1: Navigate to obj X [aka traditional object-nav] 2) Level 2: Navigate to obj X conditioned on obj Y 3) Level 3: Navigate to obj X conditioned on path P 4) Level 4: Navigate to underspecific abstraction A Human intervention is essential for evaluating success across all levels from 1 to 4. While the goals in Levels 1 to 3 are specific and relatively straightforward to assess, Level 4 presents a more abstract goal. For example, a directive like \u201dFind me somewhere to take a nap?\u201d makes the evaluation more nuanced, potentially necessitating human evaluation. Agents start an episode from a pose s0 and are given a linguistic goal x from one of the aforementioned levels. The agent\u2019s challenge is to reconcile real-time environmental observations with its interpretation of x and understand the semantic and spatial relationships between the objects and regions present. Operating autonomously, the agent must then navigate the environment, with the path being repre- sented as {s0, a0, s1, a1, . . . , sT , aT }, where each action at transitions to a pose st+1. The episode terminates when"}, {"question": " What is the goal of testing and refining LLMs in outdoor navigation?", "answer": " To refine the foundational navigation and reasoning abilities of LLMs", "ref_chunk": "3 2 0 2 p e S 8 1 ] O R . s c [ 1 v 3 0 1 0 1 . 9 0 3 2 : v i X r a Reasoning about the Unseen for Efficient Outdoor Object Navigation Quanting Xie1 Tianyi Zhang1 Kedi Xu1 Matthew Johnson-Roberson1 Yonatan Bisk1 Fig. 1. Direct application of Language Models in embodied agents navigating outdoor environments suffers from short-sightedness and limited environment comprehension. Our approach augments the LLM by enabling it to expand imaginary nodes in space, enhancing feasibility for outdoor navigation. Abstract\u2014 Robots should exist anywhere humans do: in- doors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation (OGN)[1]\u2013[3] has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches. I. INTRODUCTION Advancements in Object Goal Navigation (OGN) [1]\u2013[3] have enhanced the proficiency of robotic agents in navigating indoor environments by leveraging spatial and semantic cues. Agents that can guide humans (e.g. the visually impaired [4]) are an important enabling technology, but need to move beyond restricted indoor spaces to the full richness of outdoor navigation. Outdoor environments are substantially larger 1Carnegie Mellon University, Pittsburgh, PA USA. {quantinx, tianyiz4, kedix, mkj, ybisk}@andrew.cmu.edu 2https://github.com/quantingxie/ReasonedExplorer than handled by current semantic mapping approaches [5], lack clear [6], have complex terrains [7], and, crucially, semantic delineations. Not only is sensing simplified indoors, but so is reasoning as rooms are easily distinguished and semantically categorized. Outdoor environments still have semantic distinctions but visually identical spaces might be a soccer field, a picnic area, or the pit of an outdoor orchestra depending on the time of day. Additionally, outdoor naviga- tional tasks typically demand that robotic agents engage in roles with more granular goal specifications. For instance, in the context of search and rescue operations, the objective is not merely to navigate to a general category of \u2018people\u2019 but to pinpoint casualties potentially trapped under a car. [8]\u2013[10] trained on expansive internet datasets are serving as adapt- able policies in embodied platforms, making them proficient in addressing a wider range of tasks [11]\u2013[13]. The existing work has primarily focused on high-level task-planning with predefined skills in constrained environments. Despite this, we have seem very promising skill demonstrations in indoor object-scenarios made possible by these models [14]\u2013[16]. While some emergent behavior has been identified, the language and vision communities have begun harnessing these LLMs for their reasoning capabilities due to the vast world knowledge and models stored in their parameters. Recently, Large Language Models (LLMs) Thus, we posit that outdoor navigation offers a promising avenue to test and refine the foundational navigation and reasoning abilities of LLMs. This paper aims to formulate an elementary navigation policy and evaluate its efficacy in diverse and challenging outdoor environments, providing insights into the potential of LLMs as embodied agents. Fig. 2. Above are example queries at varying levels of complexity and a representative scene in our OUTDOOR task. Our primary contribution in this work are: 1) We introduce the OUTDOOR (Outdoor Underspecified Task Descriptions Of Objects and Regions) task, which dramatically increases the complexity inherent in ob- ject goal navigation for outdoor settings. 2) We introduce a novel use of LLMs as a planning agent to traverse real-world outdoor terrains. Our approach imagines future notes for a RRT (Rapidly-exploring Random Tree) to improve agent success (+50.4%). 3) We introduce the CASR (Computationally Adjusted Success Rate) metric, that trades off planning costs with time spent \u201cthinking\u201d (i.e. querying LLMs). II. TASK DEFINITION A. OUTDOOR: Outdoor Underspecified Task Descriptions Of Objects and Regions In traditional Object Goal Navigation, users specify dis- tinct goal categories that can be automatically evaluated by the system and do not include contraints or handle underspecified goals (e.g. reference by affordance). However, real-world outdoor environments like parks present more complex scenarios. For example, if the goal is to find a place to eat, it could refer to any bench or table, rather than a specific one. OUTDOOR embraces ambiguity, generalizing to a more nuanced and realistic navigational challenge. We categorize the instruction complexity into four levels: 1) Level 1: Navigate to obj X [aka traditional object-nav] 2) Level 2: Navigate to obj X conditioned on obj Y 3) Level 3: Navigate to obj X conditioned on path P 4) Level 4: Navigate to underspecific abstraction A Human intervention is essential for evaluating success across all levels from 1 to 4. While the goals in Levels 1 to 3 are specific and relatively straightforward to assess, Level 4 presents a more abstract goal. For example, a directive like \u201dFind me somewhere to take a nap?\u201d makes the evaluation more nuanced, potentially necessitating human evaluation. Agents start an episode from a pose s0 and are given a linguistic goal x from one of the aforementioned levels. The agent\u2019s challenge is to reconcile real-time environmental observations with its interpretation of x and understand the semantic and spatial relationships between the objects and regions present. Operating autonomously, the agent must then navigate the environment, with the path being repre- sented as {s0, a0, s1, a1, . . . , sT , aT }, where each action at transitions to a pose st+1. The episode terminates when"}], "doc_text": "3 2 0 2 p e S 8 1 ] O R . s c [ 1 v 3 0 1 0 1 . 9 0 3 2 : v i X r a Reasoning about the Unseen for Efficient Outdoor Object Navigation Quanting Xie1 Tianyi Zhang1 Kedi Xu1 Matthew Johnson-Roberson1 Yonatan Bisk1 Fig. 1. Direct application of Language Models in embodied agents navigating outdoor environments suffers from short-sightedness and limited environment comprehension. Our approach augments the LLM by enabling it to expand imaginary nodes in space, enhancing feasibility for outdoor navigation. Abstract\u2014 Robots should exist anywhere humans do: in- doors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation (OGN)[1]\u2013[3] has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches. I. INTRODUCTION Advancements in Object Goal Navigation (OGN) [1]\u2013[3] have enhanced the proficiency of robotic agents in navigating indoor environments by leveraging spatial and semantic cues. Agents that can guide humans (e.g. the visually impaired [4]) are an important enabling technology, but need to move beyond restricted indoor spaces to the full richness of outdoor navigation. Outdoor environments are substantially larger 1Carnegie Mellon University, Pittsburgh, PA USA. {quantinx, tianyiz4, kedix, mkj, ybisk}@andrew.cmu.edu 2https://github.com/quantingxie/ReasonedExplorer than handled by current semantic mapping approaches [5], lack clear [6], have complex terrains [7], and, crucially, semantic delineations. Not only is sensing simplified indoors, but so is reasoning as rooms are easily distinguished and semantically categorized. Outdoor environments still have semantic distinctions but visually identical spaces might be a soccer field, a picnic area, or the pit of an outdoor orchestra depending on the time of day. Additionally, outdoor naviga- tional tasks typically demand that robotic agents engage in roles with more granular goal specifications. For instance, in the context of search and rescue operations, the objective is not merely to navigate to a general category of \u2018people\u2019 but to pinpoint casualties potentially trapped under a car. [8]\u2013[10] trained on expansive internet datasets are serving as adapt- able policies in embodied platforms, making them proficient in addressing a wider range of tasks [11]\u2013[13]. The existing work has primarily focused on high-level task-planning with predefined skills in constrained environments. Despite this, we have seem very promising skill demonstrations in indoor object-scenarios made possible by these models [14]\u2013[16]. While some emergent behavior has been identified, the language and vision communities have begun harnessing these LLMs for their reasoning capabilities due to the vast world knowledge and models stored in their parameters. Recently, Large Language Models (LLMs) Thus, we posit that outdoor navigation offers a promising avenue to test and refine the foundational navigation and reasoning abilities of LLMs. This paper aims to formulate an elementary navigation policy and evaluate its efficacy in diverse and challenging outdoor environments, providing insights into the potential of LLMs as embodied agents. Fig. 2. Above are example queries at varying levels of complexity and a representative scene in our OUTDOOR task. Our primary contribution in this work are: 1) We introduce the OUTDOOR (Outdoor Underspecified Task Descriptions Of Objects and Regions) task, which dramatically increases the complexity inherent in ob- ject goal navigation for outdoor settings. 2) We introduce a novel use of LLMs as a planning agent to traverse real-world outdoor terrains. Our approach imagines future notes for a RRT (Rapidly-exploring Random Tree) to improve agent success (+50.4%). 3) We introduce the CASR (Computationally Adjusted Success Rate) metric, that trades off planning costs with time spent \u201cthinking\u201d (i.e. querying LLMs). II. TASK DEFINITION A. OUTDOOR: Outdoor Underspecified Task Descriptions Of Objects and Regions In traditional Object Goal Navigation, users specify dis- tinct goal categories that can be automatically evaluated by the system and do not include contraints or handle underspecified goals (e.g. reference by affordance). However, real-world outdoor environments like parks present more complex scenarios. For example, if the goal is to find a place to eat, it could refer to any bench or table, rather than a specific one. OUTDOOR embraces ambiguity, generalizing to a more nuanced and realistic navigational challenge. We categorize the instruction complexity into four levels: 1) Level 1: Navigate to obj X [aka traditional object-nav] 2) Level 2: Navigate to obj X conditioned on obj Y 3) Level 3: Navigate to obj X conditioned on path P 4) Level 4: Navigate to underspecific abstraction A Human intervention is essential for evaluating success across all levels from 1 to 4. While the goals in Levels 1 to 3 are specific and relatively straightforward to assess, Level 4 presents a more abstract goal. For example, a directive like \u201dFind me somewhere to take a nap?\u201d makes the evaluation more nuanced, potentially necessitating human evaluation. Agents start an episode from a pose s0 and are given a linguistic goal x from one of the aforementioned levels. The agent\u2019s challenge is to reconcile real-time environmental observations with its interpretation of x and understand the semantic and spatial relationships between the objects and regions present. Operating autonomously, the agent must then navigate the environment, with the path being repre- sented as {s0, a0, s1, a1, . . . , sT , aT }, where each action at transitions to a pose st+1. The episode terminates when"}