{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Improving_Massively_Multilingual_ASR_with_Auxiliary_CTC_Objectives_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What was the additional hours of data that XLS-R was pretrained on compared to w2v-bert?", "answer": " 6.6K hours of data", "ref_chunk": "can vary as much as 22.7 CER. These large dis- crepancies are likely derived from differences in SSL pre-training. Compared to w2v-bert (600M parameters), XLS-R (300M parame- ters) was pretrained on an additional 6.6K hours of data (436K total) that extended its language coverage by 77. We suspect that the larger parameter size and smaller pool of languages allowed w2v-bert to learn better representations in the languages that it covered, which carried over to ASR. Similarly, Table 5 compares the languages with the largest change in LID accuracy between our two Conformer mod- 1One concurrent work [45] further improves CER by 1.4, albeit with additional training data [17, 18, 46, 47], while another [48] was evaluated zero-shot on a subset of languages. Table 5. Languages with largest differences in LID accuracy (\u2191) between HierLIDutt and SC-CTC Conformer: Zulu (Zu), Hindi (Hi), Bosnian (Bs), Occitan (Oc), Swedish (Sw), and Umbundu (Umb). Umb Hi 91.7 80.4 91.4 60.0 ID Model D1 D2 HierLIDutt Zu 66.8 83.6 Bs 32.1 42.9 Sv 95.3 75.9 Oc 48.1 35.4 SC-CTC Table 6. Average Conformer CER (\u2193) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 10.7 10.6 9.0 9.3 EE 9.9 10.0 7.5 7.5 CMN SSA 15.6 14.5 16.4 14.8 9.1 12.6 12.0 9.2 SA 17.4 19.2 16.3 15.5 SEA CJK 25.0 14.7 24.6 14.9 17.9 14.6 13.5 18.3 Table 7. Average Conformer LID % accuracy (\u2191) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 85.3 84.6 94.1 92.5 EE 78.4 81.3 95.1 94.2 CMN SSA 59.1 72.9 62.2 75.9 96.6 98.9 96.4 97.7 SA 52.0 51.7 89.6 90.5 SEA CJK 89.7 65.7 87.8 73.4 99.3 94.1 95.4 98.9 els. We found that degradations in LID accuracy were often caused by confusion with a related language. However, this was generally accompanied by improvements in the other language, such as with the case of Serbian and Bosnian. In extreme cases, misclassi\ufb01cations considerably affected CER, such as for Swedish and Occitan (Tables 4 and 5), which were frequently misidenti\ufb01ed as Norwegian and French respectively. We also performed a region-level analysis. Table 6 shows the CERs for each group in FLEURS: Western Europe (WE), Eastern Europe (EE), Central-Asian, Middle-East and North-Africa (CMN), Sub-Saharan Africa (SSA), South Asia (SA), South-East Asia (SEA), and East Asia (CJK). Both Conformer models improve across-the- board compared to prior work [21], with notable CER reductions in the CJK and CMN language groups. The HierLIDutt technique is particularly effective on the SSA, SA, and SEA language groups compared to SC-CTC, with a small performance cost in WE, CMN, and CJK (D1 vs. D2). Table 7 makes a similar comparison using LID accuracy across language groups. Both Conformer models again out-perform previous work across all language groups, but the LID accuracy of HierLIDutt degrades in all but two language groups when compared to SC-CTC (D1 vs. D2). 6. CONCLUSION Improving multilingual ASR can help extend speech technologies to new languages. However, these models face the challenge of handling the typological diversity of so many languages. To help handle this, we introduce a framework using hierarchical CTC that can leverage language identity throughout the entire encoder-decoder network, hy- pothesizing that correctly identifying the language eases transcription modelling. We evaluate our technique on the 102-language FLEURS dataset to show its effectiveness and improve over the results of prior work. In the future, we hope to extend our approach to an even larger set of languages [4] and data [11], so that these trained models can also in downstream tools, such as with speech alignment and data cleaning, that can further help extend speech technologies to more languages. 7. ACKNOWLEDGEMENTS This work used the Bridges2 system [49], supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. 8. REFERENCES [1] S. Watanabe, T. Hori, and J. R. Hershey, \u201cLanguage independent end-to-end architecture for joint language identi\ufb01cation and speech recognition,\u201d in Proc. ASRU, 2017, pp. 265\u2013271. [2] A. Bapna, C. Cherry, Y. Zhang, et al., \u201cmSLAM: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [3] Y. Lu, M. Huang, X. Qu, et al., \u201cLanguage adaptive cross-lingual speech rep- resentation learning with sparse sharing sub-networks,\u201d in Proc. ICASSP, 2022, pp. 6882\u20136886. [4] X. Li, F. Metze, D. R. Mortensen, et al., \u201cASR2K: Speech Recognition for Around 2000 Languages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [5] P. Ogayo, G. Neubig, and A. W Black, \u201cBuilding African Voices,\u201d in Proc. In- terspeech, 2022, pp. 1263\u20131267. [6] C. Zhang, B. Li, T. Sainath, et al., \u201cStreaming end-to-end multilingual speech recognition with joint language identi\ufb01cation,\u201d Proc. Interspeech, 2022. [7] J. Bai, B. Li, Y. Zhang, et al., \u201cJoint unsupervised and supervised training for multilingual asr,\u201d in Proc. ICASSP, 2022. [8] B. Li, R. Pang, Y. Zhang, et al., \u201cMassively multilingual asr: A lifelong learning solution,\u201d in Proc. ICASSP, 2022, pp. 6397\u20136401. [9] O. Adams, M. Wiesner, S. Watanabe, et al., \u201cMassively multilingual adversarial speech recognition,\u201d in Proc. NAACL-HLT, 2019, pp. 96\u2013108. [10] V. Pratap, A. Sriram, P. Tomasello, et al., \u201cMassively Multilingual ASR: 50 Lan- guages, 1 Model, 1 Billion Parameters,\u201d in Proc. Interspeech, 2020, pp. 4751\u2013 4755. [11] W. Hou, Y. Dong, B. Zhuang, et al., \u201cLarge-Scale End-to-End Multilingual Speech Recognition and Language Identi\ufb01cation with Multi-Task Learning,\u201d in Proc. Interspeech, 2020, pp. 1037\u20131041. [12] X. Li, S. Dalmia, J. Li, et al., \u201cUniversal phone recognition with a multilingual allophone system,\u201d in Proc. ICASSP, 2020, pp. 8249\u20138253. [13] A. Conneau, A. Baevski, R. Collobert, et al., \u201cUnsupervised Cross-Lingual Representation Learning for Speech Recognition,\u201d in Proc. Interspeech, 2021, pp. 2426\u20132430. [14] B. Li, R. Pang, T. N. Sainath, et al., \u201cScaling end-to-end models for large-scale multilingual asr,\u201d in Proc. ASRU, 2021, pp. 1011\u20131018. [15] B. Yan, S. Dalmia, D. R. Mortensen, et al., \u201cDifferentiable allophone graphs for language-universal speech recognition,\u201d in Proc. Interspeech, 2021. [16] L. Zhou, J. Li, E. Sun, et al.,"}, {"question": " How much did the XLS-R parameter size differ from w2v-bert?", "answer": " 300M parameters", "ref_chunk": "can vary as much as 22.7 CER. These large dis- crepancies are likely derived from differences in SSL pre-training. Compared to w2v-bert (600M parameters), XLS-R (300M parame- ters) was pretrained on an additional 6.6K hours of data (436K total) that extended its language coverage by 77. We suspect that the larger parameter size and smaller pool of languages allowed w2v-bert to learn better representations in the languages that it covered, which carried over to ASR. Similarly, Table 5 compares the languages with the largest change in LID accuracy between our two Conformer mod- 1One concurrent work [45] further improves CER by 1.4, albeit with additional training data [17, 18, 46, 47], while another [48] was evaluated zero-shot on a subset of languages. Table 5. Languages with largest differences in LID accuracy (\u2191) between HierLIDutt and SC-CTC Conformer: Zulu (Zu), Hindi (Hi), Bosnian (Bs), Occitan (Oc), Swedish (Sw), and Umbundu (Umb). Umb Hi 91.7 80.4 91.4 60.0 ID Model D1 D2 HierLIDutt Zu 66.8 83.6 Bs 32.1 42.9 Sv 95.3 75.9 Oc 48.1 35.4 SC-CTC Table 6. Average Conformer CER (\u2193) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 10.7 10.6 9.0 9.3 EE 9.9 10.0 7.5 7.5 CMN SSA 15.6 14.5 16.4 14.8 9.1 12.6 12.0 9.2 SA 17.4 19.2 16.3 15.5 SEA CJK 25.0 14.7 24.6 14.9 17.9 14.6 13.5 18.3 Table 7. Average Conformer LID % accuracy (\u2191) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 85.3 84.6 94.1 92.5 EE 78.4 81.3 95.1 94.2 CMN SSA 59.1 72.9 62.2 75.9 96.6 98.9 96.4 97.7 SA 52.0 51.7 89.6 90.5 SEA CJK 89.7 65.7 87.8 73.4 99.3 94.1 95.4 98.9 els. We found that degradations in LID accuracy were often caused by confusion with a related language. However, this was generally accompanied by improvements in the other language, such as with the case of Serbian and Bosnian. In extreme cases, misclassi\ufb01cations considerably affected CER, such as for Swedish and Occitan (Tables 4 and 5), which were frequently misidenti\ufb01ed as Norwegian and French respectively. We also performed a region-level analysis. Table 6 shows the CERs for each group in FLEURS: Western Europe (WE), Eastern Europe (EE), Central-Asian, Middle-East and North-Africa (CMN), Sub-Saharan Africa (SSA), South Asia (SA), South-East Asia (SEA), and East Asia (CJK). Both Conformer models improve across-the- board compared to prior work [21], with notable CER reductions in the CJK and CMN language groups. The HierLIDutt technique is particularly effective on the SSA, SA, and SEA language groups compared to SC-CTC, with a small performance cost in WE, CMN, and CJK (D1 vs. D2). Table 7 makes a similar comparison using LID accuracy across language groups. Both Conformer models again out-perform previous work across all language groups, but the LID accuracy of HierLIDutt degrades in all but two language groups when compared to SC-CTC (D1 vs. D2). 6. CONCLUSION Improving multilingual ASR can help extend speech technologies to new languages. However, these models face the challenge of handling the typological diversity of so many languages. To help handle this, we introduce a framework using hierarchical CTC that can leverage language identity throughout the entire encoder-decoder network, hy- pothesizing that correctly identifying the language eases transcription modelling. We evaluate our technique on the 102-language FLEURS dataset to show its effectiveness and improve over the results of prior work. In the future, we hope to extend our approach to an even larger set of languages [4] and data [11], so that these trained models can also in downstream tools, such as with speech alignment and data cleaning, that can further help extend speech technologies to more languages. 7. ACKNOWLEDGEMENTS This work used the Bridges2 system [49], supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. 8. REFERENCES [1] S. Watanabe, T. Hori, and J. R. Hershey, \u201cLanguage independent end-to-end architecture for joint language identi\ufb01cation and speech recognition,\u201d in Proc. ASRU, 2017, pp. 265\u2013271. [2] A. Bapna, C. Cherry, Y. Zhang, et al., \u201cmSLAM: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [3] Y. Lu, M. Huang, X. Qu, et al., \u201cLanguage adaptive cross-lingual speech rep- resentation learning with sparse sharing sub-networks,\u201d in Proc. ICASSP, 2022, pp. 6882\u20136886. [4] X. Li, F. Metze, D. R. Mortensen, et al., \u201cASR2K: Speech Recognition for Around 2000 Languages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [5] P. Ogayo, G. Neubig, and A. W Black, \u201cBuilding African Voices,\u201d in Proc. In- terspeech, 2022, pp. 1263\u20131267. [6] C. Zhang, B. Li, T. Sainath, et al., \u201cStreaming end-to-end multilingual speech recognition with joint language identi\ufb01cation,\u201d Proc. Interspeech, 2022. [7] J. Bai, B. Li, Y. Zhang, et al., \u201cJoint unsupervised and supervised training for multilingual asr,\u201d in Proc. ICASSP, 2022. [8] B. Li, R. Pang, Y. Zhang, et al., \u201cMassively multilingual asr: A lifelong learning solution,\u201d in Proc. ICASSP, 2022, pp. 6397\u20136401. [9] O. Adams, M. Wiesner, S. Watanabe, et al., \u201cMassively multilingual adversarial speech recognition,\u201d in Proc. NAACL-HLT, 2019, pp. 96\u2013108. [10] V. Pratap, A. Sriram, P. Tomasello, et al., \u201cMassively Multilingual ASR: 50 Lan- guages, 1 Model, 1 Billion Parameters,\u201d in Proc. Interspeech, 2020, pp. 4751\u2013 4755. [11] W. Hou, Y. Dong, B. Zhuang, et al., \u201cLarge-Scale End-to-End Multilingual Speech Recognition and Language Identi\ufb01cation with Multi-Task Learning,\u201d in Proc. Interspeech, 2020, pp. 1037\u20131041. [12] X. Li, S. Dalmia, J. Li, et al., \u201cUniversal phone recognition with a multilingual allophone system,\u201d in Proc. ICASSP, 2020, pp. 8249\u20138253. [13] A. Conneau, A. Baevski, R. Collobert, et al., \u201cUnsupervised Cross-Lingual Representation Learning for Speech Recognition,\u201d in Proc. Interspeech, 2021, pp. 2426\u20132430. [14] B. Li, R. Pang, T. N. Sainath, et al., \u201cScaling end-to-end models for large-scale multilingual asr,\u201d in Proc. ASRU, 2021, pp. 1011\u20131018. [15] B. Yan, S. Dalmia, D. R. Mortensen, et al., \u201cDifferentiable allophone graphs for language-universal speech recognition,\u201d in Proc. Interspeech, 2021. [16] L. Zhou, J. Li, E. Sun, et al.,"}, {"question": " What improvement did the larger parameter size of w2v-bert allow in language representations?", "answer": " Better representations in the languages it covered", "ref_chunk": "can vary as much as 22.7 CER. These large dis- crepancies are likely derived from differences in SSL pre-training. Compared to w2v-bert (600M parameters), XLS-R (300M parame- ters) was pretrained on an additional 6.6K hours of data (436K total) that extended its language coverage by 77. We suspect that the larger parameter size and smaller pool of languages allowed w2v-bert to learn better representations in the languages that it covered, which carried over to ASR. Similarly, Table 5 compares the languages with the largest change in LID accuracy between our two Conformer mod- 1One concurrent work [45] further improves CER by 1.4, albeit with additional training data [17, 18, 46, 47], while another [48] was evaluated zero-shot on a subset of languages. Table 5. Languages with largest differences in LID accuracy (\u2191) between HierLIDutt and SC-CTC Conformer: Zulu (Zu), Hindi (Hi), Bosnian (Bs), Occitan (Oc), Swedish (Sw), and Umbundu (Umb). Umb Hi 91.7 80.4 91.4 60.0 ID Model D1 D2 HierLIDutt Zu 66.8 83.6 Bs 32.1 42.9 Sv 95.3 75.9 Oc 48.1 35.4 SC-CTC Table 6. Average Conformer CER (\u2193) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 10.7 10.6 9.0 9.3 EE 9.9 10.0 7.5 7.5 CMN SSA 15.6 14.5 16.4 14.8 9.1 12.6 12.0 9.2 SA 17.4 19.2 16.3 15.5 SEA CJK 25.0 14.7 24.6 14.9 17.9 14.6 13.5 18.3 Table 7. Average Conformer LID % accuracy (\u2191) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 85.3 84.6 94.1 92.5 EE 78.4 81.3 95.1 94.2 CMN SSA 59.1 72.9 62.2 75.9 96.6 98.9 96.4 97.7 SA 52.0 51.7 89.6 90.5 SEA CJK 89.7 65.7 87.8 73.4 99.3 94.1 95.4 98.9 els. We found that degradations in LID accuracy were often caused by confusion with a related language. However, this was generally accompanied by improvements in the other language, such as with the case of Serbian and Bosnian. In extreme cases, misclassi\ufb01cations considerably affected CER, such as for Swedish and Occitan (Tables 4 and 5), which were frequently misidenti\ufb01ed as Norwegian and French respectively. We also performed a region-level analysis. Table 6 shows the CERs for each group in FLEURS: Western Europe (WE), Eastern Europe (EE), Central-Asian, Middle-East and North-Africa (CMN), Sub-Saharan Africa (SSA), South Asia (SA), South-East Asia (SEA), and East Asia (CJK). Both Conformer models improve across-the- board compared to prior work [21], with notable CER reductions in the CJK and CMN language groups. The HierLIDutt technique is particularly effective on the SSA, SA, and SEA language groups compared to SC-CTC, with a small performance cost in WE, CMN, and CJK (D1 vs. D2). Table 7 makes a similar comparison using LID accuracy across language groups. Both Conformer models again out-perform previous work across all language groups, but the LID accuracy of HierLIDutt degrades in all but two language groups when compared to SC-CTC (D1 vs. D2). 6. CONCLUSION Improving multilingual ASR can help extend speech technologies to new languages. However, these models face the challenge of handling the typological diversity of so many languages. To help handle this, we introduce a framework using hierarchical CTC that can leverage language identity throughout the entire encoder-decoder network, hy- pothesizing that correctly identifying the language eases transcription modelling. We evaluate our technique on the 102-language FLEURS dataset to show its effectiveness and improve over the results of prior work. In the future, we hope to extend our approach to an even larger set of languages [4] and data [11], so that these trained models can also in downstream tools, such as with speech alignment and data cleaning, that can further help extend speech technologies to more languages. 7. ACKNOWLEDGEMENTS This work used the Bridges2 system [49], supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. 8. REFERENCES [1] S. Watanabe, T. Hori, and J. R. Hershey, \u201cLanguage independent end-to-end architecture for joint language identi\ufb01cation and speech recognition,\u201d in Proc. ASRU, 2017, pp. 265\u2013271. [2] A. Bapna, C. Cherry, Y. Zhang, et al., \u201cmSLAM: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [3] Y. Lu, M. Huang, X. Qu, et al., \u201cLanguage adaptive cross-lingual speech rep- resentation learning with sparse sharing sub-networks,\u201d in Proc. ICASSP, 2022, pp. 6882\u20136886. [4] X. Li, F. Metze, D. R. Mortensen, et al., \u201cASR2K: Speech Recognition for Around 2000 Languages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [5] P. Ogayo, G. Neubig, and A. W Black, \u201cBuilding African Voices,\u201d in Proc. In- terspeech, 2022, pp. 1263\u20131267. [6] C. Zhang, B. Li, T. Sainath, et al., \u201cStreaming end-to-end multilingual speech recognition with joint language identi\ufb01cation,\u201d Proc. Interspeech, 2022. [7] J. Bai, B. Li, Y. Zhang, et al., \u201cJoint unsupervised and supervised training for multilingual asr,\u201d in Proc. ICASSP, 2022. [8] B. Li, R. Pang, Y. Zhang, et al., \u201cMassively multilingual asr: A lifelong learning solution,\u201d in Proc. ICASSP, 2022, pp. 6397\u20136401. [9] O. Adams, M. Wiesner, S. Watanabe, et al., \u201cMassively multilingual adversarial speech recognition,\u201d in Proc. NAACL-HLT, 2019, pp. 96\u2013108. [10] V. Pratap, A. Sriram, P. Tomasello, et al., \u201cMassively Multilingual ASR: 50 Lan- guages, 1 Model, 1 Billion Parameters,\u201d in Proc. Interspeech, 2020, pp. 4751\u2013 4755. [11] W. Hou, Y. Dong, B. Zhuang, et al., \u201cLarge-Scale End-to-End Multilingual Speech Recognition and Language Identi\ufb01cation with Multi-Task Learning,\u201d in Proc. Interspeech, 2020, pp. 1037\u20131041. [12] X. Li, S. Dalmia, J. Li, et al., \u201cUniversal phone recognition with a multilingual allophone system,\u201d in Proc. ICASSP, 2020, pp. 8249\u20138253. [13] A. Conneau, A. Baevski, R. Collobert, et al., \u201cUnsupervised Cross-Lingual Representation Learning for Speech Recognition,\u201d in Proc. Interspeech, 2021, pp. 2426\u20132430. [14] B. Li, R. Pang, T. N. Sainath, et al., \u201cScaling end-to-end models for large-scale multilingual asr,\u201d in Proc. ASRU, 2021, pp. 1011\u20131018. [15] B. Yan, S. Dalmia, D. R. Mortensen, et al., \u201cDifferentiable allophone graphs for language-universal speech recognition,\u201d in Proc. Interspeech, 2021. [16] L. Zhou, J. Li, E. Sun, et al.,"}, {"question": " Which languages showed the largest differences in LID accuracy between HierLIDutt and SC-CTC Conformer?", "answer": " Zulu (Zu), Hindi (Hi), Bosnian (Bs), Occitan (Oc), Swedish (Sw), and Umbundu (Umb)", "ref_chunk": "can vary as much as 22.7 CER. These large dis- crepancies are likely derived from differences in SSL pre-training. Compared to w2v-bert (600M parameters), XLS-R (300M parame- ters) was pretrained on an additional 6.6K hours of data (436K total) that extended its language coverage by 77. We suspect that the larger parameter size and smaller pool of languages allowed w2v-bert to learn better representations in the languages that it covered, which carried over to ASR. Similarly, Table 5 compares the languages with the largest change in LID accuracy between our two Conformer mod- 1One concurrent work [45] further improves CER by 1.4, albeit with additional training data [17, 18, 46, 47], while another [48] was evaluated zero-shot on a subset of languages. Table 5. Languages with largest differences in LID accuracy (\u2191) between HierLIDutt and SC-CTC Conformer: Zulu (Zu), Hindi (Hi), Bosnian (Bs), Occitan (Oc), Swedish (Sw), and Umbundu (Umb). Umb Hi 91.7 80.4 91.4 60.0 ID Model D1 D2 HierLIDutt Zu 66.8 83.6 Bs 32.1 42.9 Sv 95.3 75.9 Oc 48.1 35.4 SC-CTC Table 6. Average Conformer CER (\u2193) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 10.7 10.6 9.0 9.3 EE 9.9 10.0 7.5 7.5 CMN SSA 15.6 14.5 16.4 14.8 9.1 12.6 12.0 9.2 SA 17.4 19.2 16.3 15.5 SEA CJK 25.0 14.7 24.6 14.9 17.9 14.6 13.5 18.3 Table 7. Average Conformer LID % accuracy (\u2191) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 85.3 84.6 94.1 92.5 EE 78.4 81.3 95.1 94.2 CMN SSA 59.1 72.9 62.2 75.9 96.6 98.9 96.4 97.7 SA 52.0 51.7 89.6 90.5 SEA CJK 89.7 65.7 87.8 73.4 99.3 94.1 95.4 98.9 els. We found that degradations in LID accuracy were often caused by confusion with a related language. However, this was generally accompanied by improvements in the other language, such as with the case of Serbian and Bosnian. In extreme cases, misclassi\ufb01cations considerably affected CER, such as for Swedish and Occitan (Tables 4 and 5), which were frequently misidenti\ufb01ed as Norwegian and French respectively. We also performed a region-level analysis. Table 6 shows the CERs for each group in FLEURS: Western Europe (WE), Eastern Europe (EE), Central-Asian, Middle-East and North-Africa (CMN), Sub-Saharan Africa (SSA), South Asia (SA), South-East Asia (SEA), and East Asia (CJK). Both Conformer models improve across-the- board compared to prior work [21], with notable CER reductions in the CJK and CMN language groups. The HierLIDutt technique is particularly effective on the SSA, SA, and SEA language groups compared to SC-CTC, with a small performance cost in WE, CMN, and CJK (D1 vs. D2). Table 7 makes a similar comparison using LID accuracy across language groups. Both Conformer models again out-perform previous work across all language groups, but the LID accuracy of HierLIDutt degrades in all but two language groups when compared to SC-CTC (D1 vs. D2). 6. CONCLUSION Improving multilingual ASR can help extend speech technologies to new languages. However, these models face the challenge of handling the typological diversity of so many languages. To help handle this, we introduce a framework using hierarchical CTC that can leverage language identity throughout the entire encoder-decoder network, hy- pothesizing that correctly identifying the language eases transcription modelling. We evaluate our technique on the 102-language FLEURS dataset to show its effectiveness and improve over the results of prior work. In the future, we hope to extend our approach to an even larger set of languages [4] and data [11], so that these trained models can also in downstream tools, such as with speech alignment and data cleaning, that can further help extend speech technologies to more languages. 7. ACKNOWLEDGEMENTS This work used the Bridges2 system [49], supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. 8. REFERENCES [1] S. Watanabe, T. Hori, and J. R. Hershey, \u201cLanguage independent end-to-end architecture for joint language identi\ufb01cation and speech recognition,\u201d in Proc. ASRU, 2017, pp. 265\u2013271. [2] A. Bapna, C. Cherry, Y. Zhang, et al., \u201cmSLAM: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [3] Y. Lu, M. Huang, X. Qu, et al., \u201cLanguage adaptive cross-lingual speech rep- resentation learning with sparse sharing sub-networks,\u201d in Proc. ICASSP, 2022, pp. 6882\u20136886. [4] X. Li, F. Metze, D. R. Mortensen, et al., \u201cASR2K: Speech Recognition for Around 2000 Languages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [5] P. Ogayo, G. Neubig, and A. W Black, \u201cBuilding African Voices,\u201d in Proc. In- terspeech, 2022, pp. 1263\u20131267. [6] C. Zhang, B. Li, T. Sainath, et al., \u201cStreaming end-to-end multilingual speech recognition with joint language identi\ufb01cation,\u201d Proc. Interspeech, 2022. [7] J. Bai, B. Li, Y. Zhang, et al., \u201cJoint unsupervised and supervised training for multilingual asr,\u201d in Proc. ICASSP, 2022. [8] B. Li, R. Pang, Y. Zhang, et al., \u201cMassively multilingual asr: A lifelong learning solution,\u201d in Proc. ICASSP, 2022, pp. 6397\u20136401. [9] O. Adams, M. Wiesner, S. Watanabe, et al., \u201cMassively multilingual adversarial speech recognition,\u201d in Proc. NAACL-HLT, 2019, pp. 96\u2013108. [10] V. Pratap, A. Sriram, P. Tomasello, et al., \u201cMassively Multilingual ASR: 50 Lan- guages, 1 Model, 1 Billion Parameters,\u201d in Proc. Interspeech, 2020, pp. 4751\u2013 4755. [11] W. Hou, Y. Dong, B. Zhuang, et al., \u201cLarge-Scale End-to-End Multilingual Speech Recognition and Language Identi\ufb01cation with Multi-Task Learning,\u201d in Proc. Interspeech, 2020, pp. 1037\u20131041. [12] X. Li, S. Dalmia, J. Li, et al., \u201cUniversal phone recognition with a multilingual allophone system,\u201d in Proc. ICASSP, 2020, pp. 8249\u20138253. [13] A. Conneau, A. Baevski, R. Collobert, et al., \u201cUnsupervised Cross-Lingual Representation Learning for Speech Recognition,\u201d in Proc. Interspeech, 2021, pp. 2426\u20132430. [14] B. Li, R. Pang, T. N. Sainath, et al., \u201cScaling end-to-end models for large-scale multilingual asr,\u201d in Proc. ASRU, 2021, pp. 1011\u20131018. [15] B. Yan, S. Dalmia, D. R. Mortensen, et al., \u201cDifferentiable allophone graphs for language-universal speech recognition,\u201d in Proc. Interspeech, 2021. [16] L. Zhou, J. Li, E. Sun, et al.,"}, {"question": " What is the average Conformer CER reduction compared to prior work for the CJK language group?", "answer": " 10.4", "ref_chunk": "can vary as much as 22.7 CER. These large dis- crepancies are likely derived from differences in SSL pre-training. Compared to w2v-bert (600M parameters), XLS-R (300M parame- ters) was pretrained on an additional 6.6K hours of data (436K total) that extended its language coverage by 77. We suspect that the larger parameter size and smaller pool of languages allowed w2v-bert to learn better representations in the languages that it covered, which carried over to ASR. Similarly, Table 5 compares the languages with the largest change in LID accuracy between our two Conformer mod- 1One concurrent work [45] further improves CER by 1.4, albeit with additional training data [17, 18, 46, 47], while another [48] was evaluated zero-shot on a subset of languages. Table 5. Languages with largest differences in LID accuracy (\u2191) between HierLIDutt and SC-CTC Conformer: Zulu (Zu), Hindi (Hi), Bosnian (Bs), Occitan (Oc), Swedish (Sw), and Umbundu (Umb). Umb Hi 91.7 80.4 91.4 60.0 ID Model D1 D2 HierLIDutt Zu 66.8 83.6 Bs 32.1 42.9 Sv 95.3 75.9 Oc 48.1 35.4 SC-CTC Table 6. Average Conformer CER (\u2193) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 10.7 10.6 9.0 9.3 EE 9.9 10.0 7.5 7.5 CMN SSA 15.6 14.5 16.4 14.8 9.1 12.6 12.0 9.2 SA 17.4 19.2 16.3 15.5 SEA CJK 25.0 14.7 24.6 14.9 17.9 14.6 13.5 18.3 Table 7. Average Conformer LID % accuracy (\u2191) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 85.3 84.6 94.1 92.5 EE 78.4 81.3 95.1 94.2 CMN SSA 59.1 72.9 62.2 75.9 96.6 98.9 96.4 97.7 SA 52.0 51.7 89.6 90.5 SEA CJK 89.7 65.7 87.8 73.4 99.3 94.1 95.4 98.9 els. We found that degradations in LID accuracy were often caused by confusion with a related language. However, this was generally accompanied by improvements in the other language, such as with the case of Serbian and Bosnian. In extreme cases, misclassi\ufb01cations considerably affected CER, such as for Swedish and Occitan (Tables 4 and 5), which were frequently misidenti\ufb01ed as Norwegian and French respectively. We also performed a region-level analysis. Table 6 shows the CERs for each group in FLEURS: Western Europe (WE), Eastern Europe (EE), Central-Asian, Middle-East and North-Africa (CMN), Sub-Saharan Africa (SSA), South Asia (SA), South-East Asia (SEA), and East Asia (CJK). Both Conformer models improve across-the- board compared to prior work [21], with notable CER reductions in the CJK and CMN language groups. The HierLIDutt technique is particularly effective on the SSA, SA, and SEA language groups compared to SC-CTC, with a small performance cost in WE, CMN, and CJK (D1 vs. D2). Table 7 makes a similar comparison using LID accuracy across language groups. Both Conformer models again out-perform previous work across all language groups, but the LID accuracy of HierLIDutt degrades in all but two language groups when compared to SC-CTC (D1 vs. D2). 6. CONCLUSION Improving multilingual ASR can help extend speech technologies to new languages. However, these models face the challenge of handling the typological diversity of so many languages. To help handle this, we introduce a framework using hierarchical CTC that can leverage language identity throughout the entire encoder-decoder network, hy- pothesizing that correctly identifying the language eases transcription modelling. We evaluate our technique on the 102-language FLEURS dataset to show its effectiveness and improve over the results of prior work. In the future, we hope to extend our approach to an even larger set of languages [4] and data [11], so that these trained models can also in downstream tools, such as with speech alignment and data cleaning, that can further help extend speech technologies to more languages. 7. ACKNOWLEDGEMENTS This work used the Bridges2 system [49], supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. 8. REFERENCES [1] S. Watanabe, T. Hori, and J. R. Hershey, \u201cLanguage independent end-to-end architecture for joint language identi\ufb01cation and speech recognition,\u201d in Proc. ASRU, 2017, pp. 265\u2013271. [2] A. Bapna, C. Cherry, Y. Zhang, et al., \u201cmSLAM: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [3] Y. Lu, M. Huang, X. Qu, et al., \u201cLanguage adaptive cross-lingual speech rep- resentation learning with sparse sharing sub-networks,\u201d in Proc. ICASSP, 2022, pp. 6882\u20136886. [4] X. Li, F. Metze, D. R. Mortensen, et al., \u201cASR2K: Speech Recognition for Around 2000 Languages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [5] P. Ogayo, G. Neubig, and A. W Black, \u201cBuilding African Voices,\u201d in Proc. In- terspeech, 2022, pp. 1263\u20131267. [6] C. Zhang, B. Li, T. Sainath, et al., \u201cStreaming end-to-end multilingual speech recognition with joint language identi\ufb01cation,\u201d Proc. Interspeech, 2022. [7] J. Bai, B. Li, Y. Zhang, et al., \u201cJoint unsupervised and supervised training for multilingual asr,\u201d in Proc. ICASSP, 2022. [8] B. Li, R. Pang, Y. Zhang, et al., \u201cMassively multilingual asr: A lifelong learning solution,\u201d in Proc. ICASSP, 2022, pp. 6397\u20136401. [9] O. Adams, M. Wiesner, S. Watanabe, et al., \u201cMassively multilingual adversarial speech recognition,\u201d in Proc. NAACL-HLT, 2019, pp. 96\u2013108. [10] V. Pratap, A. Sriram, P. Tomasello, et al., \u201cMassively Multilingual ASR: 50 Lan- guages, 1 Model, 1 Billion Parameters,\u201d in Proc. Interspeech, 2020, pp. 4751\u2013 4755. [11] W. Hou, Y. Dong, B. Zhuang, et al., \u201cLarge-Scale End-to-End Multilingual Speech Recognition and Language Identi\ufb01cation with Multi-Task Learning,\u201d in Proc. Interspeech, 2020, pp. 1037\u20131041. [12] X. Li, S. Dalmia, J. Li, et al., \u201cUniversal phone recognition with a multilingual allophone system,\u201d in Proc. ICASSP, 2020, pp. 8249\u20138253. [13] A. Conneau, A. Baevski, R. Collobert, et al., \u201cUnsupervised Cross-Lingual Representation Learning for Speech Recognition,\u201d in Proc. Interspeech, 2021, pp. 2426\u20132430. [14] B. Li, R. Pang, T. N. Sainath, et al., \u201cScaling end-to-end models for large-scale multilingual asr,\u201d in Proc. ASRU, 2021, pp. 1011\u20131018. [15] B. Yan, S. Dalmia, D. R. Mortensen, et al., \u201cDifferentiable allophone graphs for language-universal speech recognition,\u201d in Proc. Interspeech, 2021. [16] L. Zhou, J. Li, E. Sun, et al.,"}, {"question": " What is the main conclusion regarding multilingual ASR in the text?", "answer": " Improving multilingual ASR can help extend speech technologies to new languages", "ref_chunk": "can vary as much as 22.7 CER. These large dis- crepancies are likely derived from differences in SSL pre-training. Compared to w2v-bert (600M parameters), XLS-R (300M parame- ters) was pretrained on an additional 6.6K hours of data (436K total) that extended its language coverage by 77. We suspect that the larger parameter size and smaller pool of languages allowed w2v-bert to learn better representations in the languages that it covered, which carried over to ASR. Similarly, Table 5 compares the languages with the largest change in LID accuracy between our two Conformer mod- 1One concurrent work [45] further improves CER by 1.4, albeit with additional training data [17, 18, 46, 47], while another [48] was evaluated zero-shot on a subset of languages. Table 5. Languages with largest differences in LID accuracy (\u2191) between HierLIDutt and SC-CTC Conformer: Zulu (Zu), Hindi (Hi), Bosnian (Bs), Occitan (Oc), Swedish (Sw), and Umbundu (Umb). Umb Hi 91.7 80.4 91.4 60.0 ID Model D1 D2 HierLIDutt Zu 66.8 83.6 Bs 32.1 42.9 Sv 95.3 75.9 Oc 48.1 35.4 SC-CTC Table 6. Average Conformer CER (\u2193) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 10.7 10.6 9.0 9.3 EE 9.9 10.0 7.5 7.5 CMN SSA 15.6 14.5 16.4 14.8 9.1 12.6 12.0 9.2 SA 17.4 19.2 16.3 15.5 SEA CJK 25.0 14.7 24.6 14.9 17.9 14.6 13.5 18.3 Table 7. Average Conformer LID % accuracy (\u2191) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 85.3 84.6 94.1 92.5 EE 78.4 81.3 95.1 94.2 CMN SSA 59.1 72.9 62.2 75.9 96.6 98.9 96.4 97.7 SA 52.0 51.7 89.6 90.5 SEA CJK 89.7 65.7 87.8 73.4 99.3 94.1 95.4 98.9 els. We found that degradations in LID accuracy were often caused by confusion with a related language. However, this was generally accompanied by improvements in the other language, such as with the case of Serbian and Bosnian. In extreme cases, misclassi\ufb01cations considerably affected CER, such as for Swedish and Occitan (Tables 4 and 5), which were frequently misidenti\ufb01ed as Norwegian and French respectively. We also performed a region-level analysis. Table 6 shows the CERs for each group in FLEURS: Western Europe (WE), Eastern Europe (EE), Central-Asian, Middle-East and North-Africa (CMN), Sub-Saharan Africa (SSA), South Asia (SA), South-East Asia (SEA), and East Asia (CJK). Both Conformer models improve across-the- board compared to prior work [21], with notable CER reductions in the CJK and CMN language groups. The HierLIDutt technique is particularly effective on the SSA, SA, and SEA language groups compared to SC-CTC, with a small performance cost in WE, CMN, and CJK (D1 vs. D2). Table 7 makes a similar comparison using LID accuracy across language groups. Both Conformer models again out-perform previous work across all language groups, but the LID accuracy of HierLIDutt degrades in all but two language groups when compared to SC-CTC (D1 vs. D2). 6. CONCLUSION Improving multilingual ASR can help extend speech technologies to new languages. However, these models face the challenge of handling the typological diversity of so many languages. To help handle this, we introduce a framework using hierarchical CTC that can leverage language identity throughout the entire encoder-decoder network, hy- pothesizing that correctly identifying the language eases transcription modelling. We evaluate our technique on the 102-language FLEURS dataset to show its effectiveness and improve over the results of prior work. In the future, we hope to extend our approach to an even larger set of languages [4] and data [11], so that these trained models can also in downstream tools, such as with speech alignment and data cleaning, that can further help extend speech technologies to more languages. 7. ACKNOWLEDGEMENTS This work used the Bridges2 system [49], supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. 8. REFERENCES [1] S. Watanabe, T. Hori, and J. R. Hershey, \u201cLanguage independent end-to-end architecture for joint language identi\ufb01cation and speech recognition,\u201d in Proc. ASRU, 2017, pp. 265\u2013271. [2] A. Bapna, C. Cherry, Y. Zhang, et al., \u201cmSLAM: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [3] Y. Lu, M. Huang, X. Qu, et al., \u201cLanguage adaptive cross-lingual speech rep- resentation learning with sparse sharing sub-networks,\u201d in Proc. ICASSP, 2022, pp. 6882\u20136886. [4] X. Li, F. Metze, D. R. Mortensen, et al., \u201cASR2K: Speech Recognition for Around 2000 Languages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [5] P. Ogayo, G. Neubig, and A. W Black, \u201cBuilding African Voices,\u201d in Proc. In- terspeech, 2022, pp. 1263\u20131267. [6] C. Zhang, B. Li, T. Sainath, et al., \u201cStreaming end-to-end multilingual speech recognition with joint language identi\ufb01cation,\u201d Proc. Interspeech, 2022. [7] J. Bai, B. Li, Y. Zhang, et al., \u201cJoint unsupervised and supervised training for multilingual asr,\u201d in Proc. ICASSP, 2022. [8] B. Li, R. Pang, Y. Zhang, et al., \u201cMassively multilingual asr: A lifelong learning solution,\u201d in Proc. ICASSP, 2022, pp. 6397\u20136401. [9] O. Adams, M. Wiesner, S. Watanabe, et al., \u201cMassively multilingual adversarial speech recognition,\u201d in Proc. NAACL-HLT, 2019, pp. 96\u2013108. [10] V. Pratap, A. Sriram, P. Tomasello, et al., \u201cMassively Multilingual ASR: 50 Lan- guages, 1 Model, 1 Billion Parameters,\u201d in Proc. Interspeech, 2020, pp. 4751\u2013 4755. [11] W. Hou, Y. Dong, B. Zhuang, et al., \u201cLarge-Scale End-to-End Multilingual Speech Recognition and Language Identi\ufb01cation with Multi-Task Learning,\u201d in Proc. Interspeech, 2020, pp. 1037\u20131041. [12] X. Li, S. Dalmia, J. Li, et al., \u201cUniversal phone recognition with a multilingual allophone system,\u201d in Proc. ICASSP, 2020, pp. 8249\u20138253. [13] A. Conneau, A. Baevski, R. Collobert, et al., \u201cUnsupervised Cross-Lingual Representation Learning for Speech Recognition,\u201d in Proc. Interspeech, 2021, pp. 2426\u20132430. [14] B. Li, R. Pang, T. N. Sainath, et al., \u201cScaling end-to-end models for large-scale multilingual asr,\u201d in Proc. ASRU, 2021, pp. 1011\u20131018. [15] B. Yan, S. Dalmia, D. R. Mortensen, et al., \u201cDifferentiable allophone graphs for language-universal speech recognition,\u201d in Proc. Interspeech, 2021. [16] L. Zhou, J. Li, E. Sun, et al.,"}, {"question": " What framework was introduced to handle the typological diversity of many languages?", "answer": " Hierarchical CTC framework", "ref_chunk": "can vary as much as 22.7 CER. These large dis- crepancies are likely derived from differences in SSL pre-training. Compared to w2v-bert (600M parameters), XLS-R (300M parame- ters) was pretrained on an additional 6.6K hours of data (436K total) that extended its language coverage by 77. We suspect that the larger parameter size and smaller pool of languages allowed w2v-bert to learn better representations in the languages that it covered, which carried over to ASR. Similarly, Table 5 compares the languages with the largest change in LID accuracy between our two Conformer mod- 1One concurrent work [45] further improves CER by 1.4, albeit with additional training data [17, 18, 46, 47], while another [48] was evaluated zero-shot on a subset of languages. Table 5. Languages with largest differences in LID accuracy (\u2191) between HierLIDutt and SC-CTC Conformer: Zulu (Zu), Hindi (Hi), Bosnian (Bs), Occitan (Oc), Swedish (Sw), and Umbundu (Umb). Umb Hi 91.7 80.4 91.4 60.0 ID Model D1 D2 HierLIDutt Zu 66.8 83.6 Bs 32.1 42.9 Sv 95.3 75.9 Oc 48.1 35.4 SC-CTC Table 6. Average Conformer CER (\u2193) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 10.7 10.6 9.0 9.3 EE 9.9 10.0 7.5 7.5 CMN SSA 15.6 14.5 16.4 14.8 9.1 12.6 12.0 9.2 SA 17.4 19.2 16.3 15.5 SEA CJK 25.0 14.7 24.6 14.9 17.9 14.6 13.5 18.3 Table 7. Average Conformer LID % accuracy (\u2191) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 85.3 84.6 94.1 92.5 EE 78.4 81.3 95.1 94.2 CMN SSA 59.1 72.9 62.2 75.9 96.6 98.9 96.4 97.7 SA 52.0 51.7 89.6 90.5 SEA CJK 89.7 65.7 87.8 73.4 99.3 94.1 95.4 98.9 els. We found that degradations in LID accuracy were often caused by confusion with a related language. However, this was generally accompanied by improvements in the other language, such as with the case of Serbian and Bosnian. In extreme cases, misclassi\ufb01cations considerably affected CER, such as for Swedish and Occitan (Tables 4 and 5), which were frequently misidenti\ufb01ed as Norwegian and French respectively. We also performed a region-level analysis. Table 6 shows the CERs for each group in FLEURS: Western Europe (WE), Eastern Europe (EE), Central-Asian, Middle-East and North-Africa (CMN), Sub-Saharan Africa (SSA), South Asia (SA), South-East Asia (SEA), and East Asia (CJK). Both Conformer models improve across-the- board compared to prior work [21], with notable CER reductions in the CJK and CMN language groups. The HierLIDutt technique is particularly effective on the SSA, SA, and SEA language groups compared to SC-CTC, with a small performance cost in WE, CMN, and CJK (D1 vs. D2). Table 7 makes a similar comparison using LID accuracy across language groups. Both Conformer models again out-perform previous work across all language groups, but the LID accuracy of HierLIDutt degrades in all but two language groups when compared to SC-CTC (D1 vs. D2). 6. CONCLUSION Improving multilingual ASR can help extend speech technologies to new languages. However, these models face the challenge of handling the typological diversity of so many languages. To help handle this, we introduce a framework using hierarchical CTC that can leverage language identity throughout the entire encoder-decoder network, hy- pothesizing that correctly identifying the language eases transcription modelling. We evaluate our technique on the 102-language FLEURS dataset to show its effectiveness and improve over the results of prior work. In the future, we hope to extend our approach to an even larger set of languages [4] and data [11], so that these trained models can also in downstream tools, such as with speech alignment and data cleaning, that can further help extend speech technologies to more languages. 7. ACKNOWLEDGEMENTS This work used the Bridges2 system [49], supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. 8. REFERENCES [1] S. Watanabe, T. Hori, and J. R. Hershey, \u201cLanguage independent end-to-end architecture for joint language identi\ufb01cation and speech recognition,\u201d in Proc. ASRU, 2017, pp. 265\u2013271. [2] A. Bapna, C. Cherry, Y. Zhang, et al., \u201cmSLAM: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [3] Y. Lu, M. Huang, X. Qu, et al., \u201cLanguage adaptive cross-lingual speech rep- resentation learning with sparse sharing sub-networks,\u201d in Proc. ICASSP, 2022, pp. 6882\u20136886. [4] X. Li, F. Metze, D. R. Mortensen, et al., \u201cASR2K: Speech Recognition for Around 2000 Languages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [5] P. Ogayo, G. Neubig, and A. W Black, \u201cBuilding African Voices,\u201d in Proc. In- terspeech, 2022, pp. 1263\u20131267. [6] C. Zhang, B. Li, T. Sainath, et al., \u201cStreaming end-to-end multilingual speech recognition with joint language identi\ufb01cation,\u201d Proc. Interspeech, 2022. [7] J. Bai, B. Li, Y. Zhang, et al., \u201cJoint unsupervised and supervised training for multilingual asr,\u201d in Proc. ICASSP, 2022. [8] B. Li, R. Pang, Y. Zhang, et al., \u201cMassively multilingual asr: A lifelong learning solution,\u201d in Proc. ICASSP, 2022, pp. 6397\u20136401. [9] O. Adams, M. Wiesner, S. Watanabe, et al., \u201cMassively multilingual adversarial speech recognition,\u201d in Proc. NAACL-HLT, 2019, pp. 96\u2013108. [10] V. Pratap, A. Sriram, P. Tomasello, et al., \u201cMassively Multilingual ASR: 50 Lan- guages, 1 Model, 1 Billion Parameters,\u201d in Proc. Interspeech, 2020, pp. 4751\u2013 4755. [11] W. Hou, Y. Dong, B. Zhuang, et al., \u201cLarge-Scale End-to-End Multilingual Speech Recognition and Language Identi\ufb01cation with Multi-Task Learning,\u201d in Proc. Interspeech, 2020, pp. 1037\u20131041. [12] X. Li, S. Dalmia, J. Li, et al., \u201cUniversal phone recognition with a multilingual allophone system,\u201d in Proc. ICASSP, 2020, pp. 8249\u20138253. [13] A. Conneau, A. Baevski, R. Collobert, et al., \u201cUnsupervised Cross-Lingual Representation Learning for Speech Recognition,\u201d in Proc. Interspeech, 2021, pp. 2426\u20132430. [14] B. Li, R. Pang, T. N. Sainath, et al., \u201cScaling end-to-end models for large-scale multilingual asr,\u201d in Proc. ASRU, 2021, pp. 1011\u20131018. [15] B. Yan, S. Dalmia, D. R. Mortensen, et al., \u201cDifferentiable allophone graphs for language-universal speech recognition,\u201d in Proc. Interspeech, 2021. [16] L. Zhou, J. Li, E. Sun, et al.,"}, {"question": " What did the authors hypothesize regarding correctly identifying the language in transcription modeling?", "answer": " Identifying the language eases transcription modeling", "ref_chunk": "can vary as much as 22.7 CER. These large dis- crepancies are likely derived from differences in SSL pre-training. Compared to w2v-bert (600M parameters), XLS-R (300M parame- ters) was pretrained on an additional 6.6K hours of data (436K total) that extended its language coverage by 77. We suspect that the larger parameter size and smaller pool of languages allowed w2v-bert to learn better representations in the languages that it covered, which carried over to ASR. Similarly, Table 5 compares the languages with the largest change in LID accuracy between our two Conformer mod- 1One concurrent work [45] further improves CER by 1.4, albeit with additional training data [17, 18, 46, 47], while another [48] was evaluated zero-shot on a subset of languages. Table 5. Languages with largest differences in LID accuracy (\u2191) between HierLIDutt and SC-CTC Conformer: Zulu (Zu), Hindi (Hi), Bosnian (Bs), Occitan (Oc), Swedish (Sw), and Umbundu (Umb). Umb Hi 91.7 80.4 91.4 60.0 ID Model D1 D2 HierLIDutt Zu 66.8 83.6 Bs 32.1 42.9 Sv 95.3 75.9 Oc 48.1 35.4 SC-CTC Table 6. Average Conformer CER (\u2193) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 10.7 10.6 9.0 9.3 EE 9.9 10.0 7.5 7.5 CMN SSA 15.6 14.5 16.4 14.8 9.1 12.6 12.0 9.2 SA 17.4 19.2 16.3 15.5 SEA CJK 25.0 14.7 24.6 14.9 17.9 14.6 13.5 18.3 Table 7. Average Conformer LID % accuracy (\u2191) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 85.3 84.6 94.1 92.5 EE 78.4 81.3 95.1 94.2 CMN SSA 59.1 72.9 62.2 75.9 96.6 98.9 96.4 97.7 SA 52.0 51.7 89.6 90.5 SEA CJK 89.7 65.7 87.8 73.4 99.3 94.1 95.4 98.9 els. We found that degradations in LID accuracy were often caused by confusion with a related language. However, this was generally accompanied by improvements in the other language, such as with the case of Serbian and Bosnian. In extreme cases, misclassi\ufb01cations considerably affected CER, such as for Swedish and Occitan (Tables 4 and 5), which were frequently misidenti\ufb01ed as Norwegian and French respectively. We also performed a region-level analysis. Table 6 shows the CERs for each group in FLEURS: Western Europe (WE), Eastern Europe (EE), Central-Asian, Middle-East and North-Africa (CMN), Sub-Saharan Africa (SSA), South Asia (SA), South-East Asia (SEA), and East Asia (CJK). Both Conformer models improve across-the- board compared to prior work [21], with notable CER reductions in the CJK and CMN language groups. The HierLIDutt technique is particularly effective on the SSA, SA, and SEA language groups compared to SC-CTC, with a small performance cost in WE, CMN, and CJK (D1 vs. D2). Table 7 makes a similar comparison using LID accuracy across language groups. Both Conformer models again out-perform previous work across all language groups, but the LID accuracy of HierLIDutt degrades in all but two language groups when compared to SC-CTC (D1 vs. D2). 6. CONCLUSION Improving multilingual ASR can help extend speech technologies to new languages. However, these models face the challenge of handling the typological diversity of so many languages. To help handle this, we introduce a framework using hierarchical CTC that can leverage language identity throughout the entire encoder-decoder network, hy- pothesizing that correctly identifying the language eases transcription modelling. We evaluate our technique on the 102-language FLEURS dataset to show its effectiveness and improve over the results of prior work. In the future, we hope to extend our approach to an even larger set of languages [4] and data [11], so that these trained models can also in downstream tools, such as with speech alignment and data cleaning, that can further help extend speech technologies to more languages. 7. ACKNOWLEDGEMENTS This work used the Bridges2 system [49], supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. 8. REFERENCES [1] S. Watanabe, T. Hori, and J. R. Hershey, \u201cLanguage independent end-to-end architecture for joint language identi\ufb01cation and speech recognition,\u201d in Proc. ASRU, 2017, pp. 265\u2013271. [2] A. Bapna, C. Cherry, Y. Zhang, et al., \u201cmSLAM: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [3] Y. Lu, M. Huang, X. Qu, et al., \u201cLanguage adaptive cross-lingual speech rep- resentation learning with sparse sharing sub-networks,\u201d in Proc. ICASSP, 2022, pp. 6882\u20136886. [4] X. Li, F. Metze, D. R. Mortensen, et al., \u201cASR2K: Speech Recognition for Around 2000 Languages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [5] P. Ogayo, G. Neubig, and A. W Black, \u201cBuilding African Voices,\u201d in Proc. In- terspeech, 2022, pp. 1263\u20131267. [6] C. Zhang, B. Li, T. Sainath, et al., \u201cStreaming end-to-end multilingual speech recognition with joint language identi\ufb01cation,\u201d Proc. Interspeech, 2022. [7] J. Bai, B. Li, Y. Zhang, et al., \u201cJoint unsupervised and supervised training for multilingual asr,\u201d in Proc. ICASSP, 2022. [8] B. Li, R. Pang, Y. Zhang, et al., \u201cMassively multilingual asr: A lifelong learning solution,\u201d in Proc. ICASSP, 2022, pp. 6397\u20136401. [9] O. Adams, M. Wiesner, S. Watanabe, et al., \u201cMassively multilingual adversarial speech recognition,\u201d in Proc. NAACL-HLT, 2019, pp. 96\u2013108. [10] V. Pratap, A. Sriram, P. Tomasello, et al., \u201cMassively Multilingual ASR: 50 Lan- guages, 1 Model, 1 Billion Parameters,\u201d in Proc. Interspeech, 2020, pp. 4751\u2013 4755. [11] W. Hou, Y. Dong, B. Zhuang, et al., \u201cLarge-Scale End-to-End Multilingual Speech Recognition and Language Identi\ufb01cation with Multi-Task Learning,\u201d in Proc. Interspeech, 2020, pp. 1037\u20131041. [12] X. Li, S. Dalmia, J. Li, et al., \u201cUniversal phone recognition with a multilingual allophone system,\u201d in Proc. ICASSP, 2020, pp. 8249\u20138253. [13] A. Conneau, A. Baevski, R. Collobert, et al., \u201cUnsupervised Cross-Lingual Representation Learning for Speech Recognition,\u201d in Proc. Interspeech, 2021, pp. 2426\u20132430. [14] B. Li, R. Pang, T. N. Sainath, et al., \u201cScaling end-to-end models for large-scale multilingual asr,\u201d in Proc. ASRU, 2021, pp. 1011\u20131018. [15] B. Yan, S. Dalmia, D. R. Mortensen, et al., \u201cDifferentiable allophone graphs for language-universal speech recognition,\u201d in Proc. Interspeech, 2021. [16] L. Zhou, J. Li, E. Sun, et al.,"}, {"question": " What system was used in the research, supported by NSF award number ACI-1445606?", "answer": " Bridges2 system", "ref_chunk": "can vary as much as 22.7 CER. These large dis- crepancies are likely derived from differences in SSL pre-training. Compared to w2v-bert (600M parameters), XLS-R (300M parame- ters) was pretrained on an additional 6.6K hours of data (436K total) that extended its language coverage by 77. We suspect that the larger parameter size and smaller pool of languages allowed w2v-bert to learn better representations in the languages that it covered, which carried over to ASR. Similarly, Table 5 compares the languages with the largest change in LID accuracy between our two Conformer mod- 1One concurrent work [45] further improves CER by 1.4, albeit with additional training data [17, 18, 46, 47], while another [48] was evaluated zero-shot on a subset of languages. Table 5. Languages with largest differences in LID accuracy (\u2191) between HierLIDutt and SC-CTC Conformer: Zulu (Zu), Hindi (Hi), Bosnian (Bs), Occitan (Oc), Swedish (Sw), and Umbundu (Umb). Umb Hi 91.7 80.4 91.4 60.0 ID Model D1 D2 HierLIDutt Zu 66.8 83.6 Bs 32.1 42.9 Sv 95.3 75.9 Oc 48.1 35.4 SC-CTC Table 6. Average Conformer CER (\u2193) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 10.7 10.6 9.0 9.3 EE 9.9 10.0 7.5 7.5 CMN SSA 15.6 14.5 16.4 14.8 9.1 12.6 12.0 9.2 SA 17.4 19.2 16.3 15.5 SEA CJK 25.0 14.7 24.6 14.9 17.9 14.6 13.5 18.3 Table 7. Average Conformer LID % accuracy (\u2191) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 85.3 84.6 94.1 92.5 EE 78.4 81.3 95.1 94.2 CMN SSA 59.1 72.9 62.2 75.9 96.6 98.9 96.4 97.7 SA 52.0 51.7 89.6 90.5 SEA CJK 89.7 65.7 87.8 73.4 99.3 94.1 95.4 98.9 els. We found that degradations in LID accuracy were often caused by confusion with a related language. However, this was generally accompanied by improvements in the other language, such as with the case of Serbian and Bosnian. In extreme cases, misclassi\ufb01cations considerably affected CER, such as for Swedish and Occitan (Tables 4 and 5), which were frequently misidenti\ufb01ed as Norwegian and French respectively. We also performed a region-level analysis. Table 6 shows the CERs for each group in FLEURS: Western Europe (WE), Eastern Europe (EE), Central-Asian, Middle-East and North-Africa (CMN), Sub-Saharan Africa (SSA), South Asia (SA), South-East Asia (SEA), and East Asia (CJK). Both Conformer models improve across-the- board compared to prior work [21], with notable CER reductions in the CJK and CMN language groups. The HierLIDutt technique is particularly effective on the SSA, SA, and SEA language groups compared to SC-CTC, with a small performance cost in WE, CMN, and CJK (D1 vs. D2). Table 7 makes a similar comparison using LID accuracy across language groups. Both Conformer models again out-perform previous work across all language groups, but the LID accuracy of HierLIDutt degrades in all but two language groups when compared to SC-CTC (D1 vs. D2). 6. CONCLUSION Improving multilingual ASR can help extend speech technologies to new languages. However, these models face the challenge of handling the typological diversity of so many languages. To help handle this, we introduce a framework using hierarchical CTC that can leverage language identity throughout the entire encoder-decoder network, hy- pothesizing that correctly identifying the language eases transcription modelling. We evaluate our technique on the 102-language FLEURS dataset to show its effectiveness and improve over the results of prior work. In the future, we hope to extend our approach to an even larger set of languages [4] and data [11], so that these trained models can also in downstream tools, such as with speech alignment and data cleaning, that can further help extend speech technologies to more languages. 7. ACKNOWLEDGEMENTS This work used the Bridges2 system [49], supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. 8. REFERENCES [1] S. Watanabe, T. Hori, and J. R. Hershey, \u201cLanguage independent end-to-end architecture for joint language identi\ufb01cation and speech recognition,\u201d in Proc. ASRU, 2017, pp. 265\u2013271. [2] A. Bapna, C. Cherry, Y. Zhang, et al., \u201cmSLAM: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [3] Y. Lu, M. Huang, X. Qu, et al., \u201cLanguage adaptive cross-lingual speech rep- resentation learning with sparse sharing sub-networks,\u201d in Proc. ICASSP, 2022, pp. 6882\u20136886. [4] X. Li, F. Metze, D. R. Mortensen, et al., \u201cASR2K: Speech Recognition for Around 2000 Languages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [5] P. Ogayo, G. Neubig, and A. W Black, \u201cBuilding African Voices,\u201d in Proc. In- terspeech, 2022, pp. 1263\u20131267. [6] C. Zhang, B. Li, T. Sainath, et al., \u201cStreaming end-to-end multilingual speech recognition with joint language identi\ufb01cation,\u201d Proc. Interspeech, 2022. [7] J. Bai, B. Li, Y. Zhang, et al., \u201cJoint unsupervised and supervised training for multilingual asr,\u201d in Proc. ICASSP, 2022. [8] B. Li, R. Pang, Y. Zhang, et al., \u201cMassively multilingual asr: A lifelong learning solution,\u201d in Proc. ICASSP, 2022, pp. 6397\u20136401. [9] O. Adams, M. Wiesner, S. Watanabe, et al., \u201cMassively multilingual adversarial speech recognition,\u201d in Proc. NAACL-HLT, 2019, pp. 96\u2013108. [10] V. Pratap, A. Sriram, P. Tomasello, et al., \u201cMassively Multilingual ASR: 50 Lan- guages, 1 Model, 1 Billion Parameters,\u201d in Proc. Interspeech, 2020, pp. 4751\u2013 4755. [11] W. Hou, Y. Dong, B. Zhuang, et al., \u201cLarge-Scale End-to-End Multilingual Speech Recognition and Language Identi\ufb01cation with Multi-Task Learning,\u201d in Proc. Interspeech, 2020, pp. 1037\u20131041. [12] X. Li, S. Dalmia, J. Li, et al., \u201cUniversal phone recognition with a multilingual allophone system,\u201d in Proc. ICASSP, 2020, pp. 8249\u20138253. [13] A. Conneau, A. Baevski, R. Collobert, et al., \u201cUnsupervised Cross-Lingual Representation Learning for Speech Recognition,\u201d in Proc. Interspeech, 2021, pp. 2426\u20132430. [14] B. Li, R. Pang, T. N. Sainath, et al., \u201cScaling end-to-end models for large-scale multilingual asr,\u201d in Proc. ASRU, 2021, pp. 1011\u20131018. [15] B. Yan, S. Dalmia, D. R. Mortensen, et al., \u201cDifferentiable allophone graphs for language-universal speech recognition,\u201d in Proc. Interspeech, 2021. [16] L. Zhou, J. Li, E. Sun, et al.,"}, {"question": " What type of award supported the Bridges2 system used in the research?", "answer": " NSF award", "ref_chunk": "can vary as much as 22.7 CER. These large dis- crepancies are likely derived from differences in SSL pre-training. Compared to w2v-bert (600M parameters), XLS-R (300M parame- ters) was pretrained on an additional 6.6K hours of data (436K total) that extended its language coverage by 77. We suspect that the larger parameter size and smaller pool of languages allowed w2v-bert to learn better representations in the languages that it covered, which carried over to ASR. Similarly, Table 5 compares the languages with the largest change in LID accuracy between our two Conformer mod- 1One concurrent work [45] further improves CER by 1.4, albeit with additional training data [17, 18, 46, 47], while another [48] was evaluated zero-shot on a subset of languages. Table 5. Languages with largest differences in LID accuracy (\u2191) between HierLIDutt and SC-CTC Conformer: Zulu (Zu), Hindi (Hi), Bosnian (Bs), Occitan (Oc), Swedish (Sw), and Umbundu (Umb). Umb Hi 91.7 80.4 91.4 60.0 ID Model D1 D2 HierLIDutt Zu 66.8 83.6 Bs 32.1 42.9 Sv 95.3 75.9 Oc 48.1 35.4 SC-CTC Table 6. Average Conformer CER (\u2193) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 10.7 10.6 9.0 9.3 EE 9.9 10.0 7.5 7.5 CMN SSA 15.6 14.5 16.4 14.8 9.1 12.6 12.0 9.2 SA 17.4 19.2 16.3 15.5 SEA CJK 25.0 14.7 24.6 14.9 17.9 14.6 13.5 18.3 Table 7. Average Conformer LID % accuracy (\u2191) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 85.3 84.6 94.1 92.5 EE 78.4 81.3 95.1 94.2 CMN SSA 59.1 72.9 62.2 75.9 96.6 98.9 96.4 97.7 SA 52.0 51.7 89.6 90.5 SEA CJK 89.7 65.7 87.8 73.4 99.3 94.1 95.4 98.9 els. We found that degradations in LID accuracy were often caused by confusion with a related language. However, this was generally accompanied by improvements in the other language, such as with the case of Serbian and Bosnian. In extreme cases, misclassi\ufb01cations considerably affected CER, such as for Swedish and Occitan (Tables 4 and 5), which were frequently misidenti\ufb01ed as Norwegian and French respectively. We also performed a region-level analysis. Table 6 shows the CERs for each group in FLEURS: Western Europe (WE), Eastern Europe (EE), Central-Asian, Middle-East and North-Africa (CMN), Sub-Saharan Africa (SSA), South Asia (SA), South-East Asia (SEA), and East Asia (CJK). Both Conformer models improve across-the- board compared to prior work [21], with notable CER reductions in the CJK and CMN language groups. The HierLIDutt technique is particularly effective on the SSA, SA, and SEA language groups compared to SC-CTC, with a small performance cost in WE, CMN, and CJK (D1 vs. D2). Table 7 makes a similar comparison using LID accuracy across language groups. Both Conformer models again out-perform previous work across all language groups, but the LID accuracy of HierLIDutt degrades in all but two language groups when compared to SC-CTC (D1 vs. D2). 6. CONCLUSION Improving multilingual ASR can help extend speech technologies to new languages. However, these models face the challenge of handling the typological diversity of so many languages. To help handle this, we introduce a framework using hierarchical CTC that can leverage language identity throughout the entire encoder-decoder network, hy- pothesizing that correctly identifying the language eases transcription modelling. We evaluate our technique on the 102-language FLEURS dataset to show its effectiveness and improve over the results of prior work. In the future, we hope to extend our approach to an even larger set of languages [4] and data [11], so that these trained models can also in downstream tools, such as with speech alignment and data cleaning, that can further help extend speech technologies to more languages. 7. ACKNOWLEDGEMENTS This work used the Bridges2 system [49], supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. 8. REFERENCES [1] S. Watanabe, T. Hori, and J. R. Hershey, \u201cLanguage independent end-to-end architecture for joint language identi\ufb01cation and speech recognition,\u201d in Proc. ASRU, 2017, pp. 265\u2013271. [2] A. Bapna, C. Cherry, Y. Zhang, et al., \u201cmSLAM: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [3] Y. Lu, M. Huang, X. Qu, et al., \u201cLanguage adaptive cross-lingual speech rep- resentation learning with sparse sharing sub-networks,\u201d in Proc. ICASSP, 2022, pp. 6882\u20136886. [4] X. Li, F. Metze, D. R. Mortensen, et al., \u201cASR2K: Speech Recognition for Around 2000 Languages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [5] P. Ogayo, G. Neubig, and A. W Black, \u201cBuilding African Voices,\u201d in Proc. In- terspeech, 2022, pp. 1263\u20131267. [6] C. Zhang, B. Li, T. Sainath, et al., \u201cStreaming end-to-end multilingual speech recognition with joint language identi\ufb01cation,\u201d Proc. Interspeech, 2022. [7] J. Bai, B. Li, Y. Zhang, et al., \u201cJoint unsupervised and supervised training for multilingual asr,\u201d in Proc. ICASSP, 2022. [8] B. Li, R. Pang, Y. Zhang, et al., \u201cMassively multilingual asr: A lifelong learning solution,\u201d in Proc. ICASSP, 2022, pp. 6397\u20136401. [9] O. Adams, M. Wiesner, S. Watanabe, et al., \u201cMassively multilingual adversarial speech recognition,\u201d in Proc. NAACL-HLT, 2019, pp. 96\u2013108. [10] V. Pratap, A. Sriram, P. Tomasello, et al., \u201cMassively Multilingual ASR: 50 Lan- guages, 1 Model, 1 Billion Parameters,\u201d in Proc. Interspeech, 2020, pp. 4751\u2013 4755. [11] W. Hou, Y. Dong, B. Zhuang, et al., \u201cLarge-Scale End-to-End Multilingual Speech Recognition and Language Identi\ufb01cation with Multi-Task Learning,\u201d in Proc. Interspeech, 2020, pp. 1037\u20131041. [12] X. Li, S. Dalmia, J. Li, et al., \u201cUniversal phone recognition with a multilingual allophone system,\u201d in Proc. ICASSP, 2020, pp. 8249\u20138253. [13] A. Conneau, A. Baevski, R. Collobert, et al., \u201cUnsupervised Cross-Lingual Representation Learning for Speech Recognition,\u201d in Proc. Interspeech, 2021, pp. 2426\u20132430. [14] B. Li, R. Pang, T. N. Sainath, et al., \u201cScaling end-to-end models for large-scale multilingual asr,\u201d in Proc. ASRU, 2021, pp. 1011\u20131018. [15] B. Yan, S. Dalmia, D. R. Mortensen, et al., \u201cDifferentiable allophone graphs for language-universal speech recognition,\u201d in Proc. Interspeech, 2021. [16] L. Zhou, J. Li, E. Sun, et al.,"}], "doc_text": "can vary as much as 22.7 CER. These large dis- crepancies are likely derived from differences in SSL pre-training. Compared to w2v-bert (600M parameters), XLS-R (300M parame- ters) was pretrained on an additional 6.6K hours of data (436K total) that extended its language coverage by 77. We suspect that the larger parameter size and smaller pool of languages allowed w2v-bert to learn better representations in the languages that it covered, which carried over to ASR. Similarly, Table 5 compares the languages with the largest change in LID accuracy between our two Conformer mod- 1One concurrent work [45] further improves CER by 1.4, albeit with additional training data [17, 18, 46, 47], while another [48] was evaluated zero-shot on a subset of languages. Table 5. Languages with largest differences in LID accuracy (\u2191) between HierLIDutt and SC-CTC Conformer: Zulu (Zu), Hindi (Hi), Bosnian (Bs), Occitan (Oc), Swedish (Sw), and Umbundu (Umb). Umb Hi 91.7 80.4 91.4 60.0 ID Model D1 D2 HierLIDutt Zu 66.8 83.6 Bs 32.1 42.9 Sv 95.3 75.9 Oc 48.1 35.4 SC-CTC Table 6. Average Conformer CER (\u2193) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 10.7 10.6 9.0 9.3 EE 9.9 10.0 7.5 7.5 CMN SSA 15.6 14.5 16.4 14.8 9.1 12.6 12.0 9.2 SA 17.4 19.2 16.3 15.5 SEA CJK 25.0 14.7 24.6 14.9 17.9 14.6 13.5 18.3 Table 7. Average Conformer LID % accuracy (\u2191) compared to prior work for each language group. ID Model Z1 w2v-bert-51 [21] Z2 mSLAM-101 [21] D1 D2 HierLIDutt SC-CTC WE 85.3 84.6 94.1 92.5 EE 78.4 81.3 95.1 94.2 CMN SSA 59.1 72.9 62.2 75.9 96.6 98.9 96.4 97.7 SA 52.0 51.7 89.6 90.5 SEA CJK 89.7 65.7 87.8 73.4 99.3 94.1 95.4 98.9 els. We found that degradations in LID accuracy were often caused by confusion with a related language. However, this was generally accompanied by improvements in the other language, such as with the case of Serbian and Bosnian. In extreme cases, misclassi\ufb01cations considerably affected CER, such as for Swedish and Occitan (Tables 4 and 5), which were frequently misidenti\ufb01ed as Norwegian and French respectively. We also performed a region-level analysis. Table 6 shows the CERs for each group in FLEURS: Western Europe (WE), Eastern Europe (EE), Central-Asian, Middle-East and North-Africa (CMN), Sub-Saharan Africa (SSA), South Asia (SA), South-East Asia (SEA), and East Asia (CJK). Both Conformer models improve across-the- board compared to prior work [21], with notable CER reductions in the CJK and CMN language groups. The HierLIDutt technique is particularly effective on the SSA, SA, and SEA language groups compared to SC-CTC, with a small performance cost in WE, CMN, and CJK (D1 vs. D2). Table 7 makes a similar comparison using LID accuracy across language groups. Both Conformer models again out-perform previous work across all language groups, but the LID accuracy of HierLIDutt degrades in all but two language groups when compared to SC-CTC (D1 vs. D2). 6. CONCLUSION Improving multilingual ASR can help extend speech technologies to new languages. However, these models face the challenge of handling the typological diversity of so many languages. To help handle this, we introduce a framework using hierarchical CTC that can leverage language identity throughout the entire encoder-decoder network, hy- pothesizing that correctly identifying the language eases transcription modelling. We evaluate our technique on the 102-language FLEURS dataset to show its effectiveness and improve over the results of prior work. In the future, we hope to extend our approach to an even larger set of languages [4] and data [11], so that these trained models can also in downstream tools, such as with speech alignment and data cleaning, that can further help extend speech technologies to more languages. 7. ACKNOWLEDGEMENTS This work used the Bridges2 system [49], supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. 8. REFERENCES [1] S. Watanabe, T. Hori, and J. R. Hershey, \u201cLanguage independent end-to-end architecture for joint language identi\ufb01cation and speech recognition,\u201d in Proc. ASRU, 2017, pp. 265\u2013271. [2] A. Bapna, C. Cherry, Y. Zhang, et al., \u201cmSLAM: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [3] Y. Lu, M. Huang, X. Qu, et al., \u201cLanguage adaptive cross-lingual speech rep- resentation learning with sparse sharing sub-networks,\u201d in Proc. ICASSP, 2022, pp. 6882\u20136886. [4] X. Li, F. Metze, D. R. Mortensen, et al., \u201cASR2K: Speech Recognition for Around 2000 Languages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [5] P. Ogayo, G. Neubig, and A. W Black, \u201cBuilding African Voices,\u201d in Proc. In- terspeech, 2022, pp. 1263\u20131267. [6] C. Zhang, B. Li, T. Sainath, et al., \u201cStreaming end-to-end multilingual speech recognition with joint language identi\ufb01cation,\u201d Proc. Interspeech, 2022. [7] J. Bai, B. Li, Y. Zhang, et al., \u201cJoint unsupervised and supervised training for multilingual asr,\u201d in Proc. ICASSP, 2022. [8] B. Li, R. Pang, Y. Zhang, et al., \u201cMassively multilingual asr: A lifelong learning solution,\u201d in Proc. ICASSP, 2022, pp. 6397\u20136401. [9] O. Adams, M. Wiesner, S. Watanabe, et al., \u201cMassively multilingual adversarial speech recognition,\u201d in Proc. NAACL-HLT, 2019, pp. 96\u2013108. [10] V. Pratap, A. Sriram, P. Tomasello, et al., \u201cMassively Multilingual ASR: 50 Lan- guages, 1 Model, 1 Billion Parameters,\u201d in Proc. Interspeech, 2020, pp. 4751\u2013 4755. [11] W. Hou, Y. Dong, B. Zhuang, et al., \u201cLarge-Scale End-to-End Multilingual Speech Recognition and Language Identi\ufb01cation with Multi-Task Learning,\u201d in Proc. Interspeech, 2020, pp. 1037\u20131041. [12] X. Li, S. Dalmia, J. Li, et al., \u201cUniversal phone recognition with a multilingual allophone system,\u201d in Proc. ICASSP, 2020, pp. 8249\u20138253. [13] A. Conneau, A. Baevski, R. Collobert, et al., \u201cUnsupervised Cross-Lingual Representation Learning for Speech Recognition,\u201d in Proc. Interspeech, 2021, pp. 2426\u20132430. [14] B. Li, R. Pang, T. N. Sainath, et al., \u201cScaling end-to-end models for large-scale multilingual asr,\u201d in Proc. ASRU, 2021, pp. 1011\u20131018. [15] B. Yan, S. Dalmia, D. R. Mortensen, et al., \u201cDifferentiable allophone graphs for language-universal speech recognition,\u201d in Proc. Interspeech, 2021. [16] L. Zhou, J. Li, E. Sun, et al.,"}