{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_StarCoder:_may_the_source_be_with_you!_chunk_8.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What was the overall precision of the PII detection tool?,        answer: Below 4%    ", "ref_chunk": "C-sharp that weren\u2019t well represented in the regex evaluation. Consequently, the overall precision of the tool was below 4%. Post-processing Before applying the best PII detection model to the full dataset, we observed a couple of frequent detection errors. We added the following post-processing techniques to reduce the number of false positives: Ignore secrets with fewer than 4 characters. Detect full names only by requiring at least one space within the name. 13 F1 Published in Transactions on Machine Learning Research (12/2023) Method Name Username Password Prec. Recall F1 Prec. Recall F1 Prec. Recall NER 83.66% 95.52% 89.19% 48.93% 75.55% 59.39% 59.16% 96.62% 73.39% + pseudo labels 86.45% 97.38% 91.59% 52.20% 74.81% 61.49% 70.94% 95.96% 81.57% Table 9: Comparison of PII detection performance: NER Pipeline with Annotated Data vs. Annotated Data + Pseudo-Labels Ignore detected keys with fewer than 9 characters or that are not gibberish using a gibberish-detector.11 Ignore IP addresses that aren\u2019t valid or are private (non-Internet facing) using the ipaddress python package. We also ignore IP addresses from popular DNS servers. We use the same list as in Ben Allal et al. (2023). PII placeholders We replaced the detected PII entities with the following tokens: <NAME>, <EMAIL>, <KEY>, <PASSWORD> To mask IP addresses, we randomly selected an IP address from 5 synthetic, private, non-internet-facing IP addresses of the same type that can be found in Appendix C. Github issues We already employed a regex approach to detect keys, IP addresses, and emails in the Github issues, so we only used the PII detection model to redact names. We anonymized the usernames of the authors by replacing them with a participant counter within the conversation, e.g. username_1 to refer to second participant (see Section 5.1 for formatting details). We prepend these pseudonyms to the beginning of each comment such that we preserve the speaker identity of the author. In addition, we redact all mentions of these usernames in the messages. Note that we only mask the usernames of active participants in the conversation and mentions of non-participating users are not anonymized. Compute resources We used the PII detection model to identify PII across all programming languages in the training dataset, including GitHub issues (names only), Git commits, and Jupyter notebooks. The total dataset amounts to 815 GB in size. We ran inference on multiple NVIDIA A100 80 GB GPUs, which required 800 GPU-hours. 5 Model training This section presents information on the training process of the StarCoder models. Before we proceed, we first clarify the differences between the two models: StarCoderBase is the first model trained on 1 trillion tokens sourced from the curated dataset described in Section 3. StarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2 epochs). Throughout the following, we show how we formatted the training data (Section 5.1), decontaminated the training data (Section 5.2), and provide details regarding the tokenizer (Section 5.3), the model architecture (Section 5.4), the training process (Section 5.5), multi-node GPU setup (Section 5.6), and CO2 emissions (Section 5.7). 11https://github.com/domanchi/gibberish-detector 14 F1 Published in Transactions on Machine Learning Research (12/2023) 5.1 Data formatting We present the formatting guidelines for each of the data sources below. We provide the templates below in which <token> refers to a sentinel token, and metadata and data refer to placeholders for data fields, respectively. Code We prepend the repository name, file name, and the number of stars to the context of the code file. To not overfit on the exact number of stars, we categorized GitHub stars into five buckets: 0, 1\u201310, 10\u2013100, 100\u20131000, 1000+. To enable the model to operate without this metadata during inference, we prefixed the repository name, filename, and stars independently at random, each with a probability of 0.2. <reponame>reponame<filename>filename<gh_stars>stars\\ncode<|endoftext|> To the source code in this template (i.e. code), we apply the fill-in-the-middle transformation (FIM; Bavarian et al., 2022). More precisely, we apply FIM at the character-level to the source code files with a FIM-rate of 0.5, and use PSM mode with probability .5 and SPMv2 mode with probability .5. Issues We use sentinel tokens to mark the opening of an issue and subsequently include its title. We separate the sequence of comments by a <issue_comment> token and include a anonymized speaker identifier before the comment. Specifically, we refer to authors by their participant counter within the conversation, e.g. username_1 to refer to second participant in the issue. To distinguish between the different turns, we use comment1, id1 to refer to the second comment and its anonymized speaker id, respectively. <issue_start>Title: title\\nusername_id0:comment0<issue_comment>username_id1:comment1 ... <issue_closed (optional)><|endoftext|> Jupyter \u2013 scripts Jupyter scripts were formatted in the same manner as code. Jupyter \u2013 structured Parsed Jupyter notebooks come in chains of text, code, and outputs, and we separated them with sentinel tokens. Note that we use text2, code2, output2 to refer to the 3rd triplet in the notebook. <jupyter_start><jupyter_text>text0<jupyter_code>code0 <jupyter_output>output0<jupyter_text> ... <|endoftext|> Git commits We separate the code before the commit, the commit message, and the code after the commit with sentinel tokens. As explained in Section 3.4, we use the full files with 20% probability and otherwise use a small window (0-32 lines) around the changed lines. <commit_before>code_before<commit_msg>message<commit_after>code_after<|endoftext|> We summarize all sentinel tokens in Table 10. 5.2 Training data decontamination The code training data was decontaminated by removing files that contained docstrings or solutions from HumanEval and MBPP, docstrings from APPS, questions from GSM8K, or prompts from DS1000. (These benchmarks are further described in Section 6.) To give an indication of the amount of data removed by decontamination, Python is the language with the highest number of matches, with 558 files removed. 15 Published in Transactions on Machine Learning Research (12/2023) Token Description <|endoftext|> <fim_prefix> <fim_middle> <fim_suffix> <fim_pad> <reponame> <filename> <gh_stars> <issue_start> <issue_comment> <issue_closed> <jupyter_start> <jupyter_text> <jupyter_code> <jupyter_output> <empty_output> <commit_before> <commit_msg> <commit_after> end of text/sequence FIM prefix FIM middle FIM suffix FIM pad repository name file name GitHub stars start of GitHub issue start of GitHub issue comment GitHub issue closed event start of Jupyter notebook start of"}, {"question": " What post-processing techniques were added to reduce false positives?,        answer: Ignore secrets with fewer than 4 characters and detect full names only by requiring at least one space within the name    ", "ref_chunk": "C-sharp that weren\u2019t well represented in the regex evaluation. Consequently, the overall precision of the tool was below 4%. Post-processing Before applying the best PII detection model to the full dataset, we observed a couple of frequent detection errors. We added the following post-processing techniques to reduce the number of false positives: Ignore secrets with fewer than 4 characters. Detect full names only by requiring at least one space within the name. 13 F1 Published in Transactions on Machine Learning Research (12/2023) Method Name Username Password Prec. Recall F1 Prec. Recall F1 Prec. Recall NER 83.66% 95.52% 89.19% 48.93% 75.55% 59.39% 59.16% 96.62% 73.39% + pseudo labels 86.45% 97.38% 91.59% 52.20% 74.81% 61.49% 70.94% 95.96% 81.57% Table 9: Comparison of PII detection performance: NER Pipeline with Annotated Data vs. Annotated Data + Pseudo-Labels Ignore detected keys with fewer than 9 characters or that are not gibberish using a gibberish-detector.11 Ignore IP addresses that aren\u2019t valid or are private (non-Internet facing) using the ipaddress python package. We also ignore IP addresses from popular DNS servers. We use the same list as in Ben Allal et al. (2023). PII placeholders We replaced the detected PII entities with the following tokens: <NAME>, <EMAIL>, <KEY>, <PASSWORD> To mask IP addresses, we randomly selected an IP address from 5 synthetic, private, non-internet-facing IP addresses of the same type that can be found in Appendix C. Github issues We already employed a regex approach to detect keys, IP addresses, and emails in the Github issues, so we only used the PII detection model to redact names. We anonymized the usernames of the authors by replacing them with a participant counter within the conversation, e.g. username_1 to refer to second participant (see Section 5.1 for formatting details). We prepend these pseudonyms to the beginning of each comment such that we preserve the speaker identity of the author. In addition, we redact all mentions of these usernames in the messages. Note that we only mask the usernames of active participants in the conversation and mentions of non-participating users are not anonymized. Compute resources We used the PII detection model to identify PII across all programming languages in the training dataset, including GitHub issues (names only), Git commits, and Jupyter notebooks. The total dataset amounts to 815 GB in size. We ran inference on multiple NVIDIA A100 80 GB GPUs, which required 800 GPU-hours. 5 Model training This section presents information on the training process of the StarCoder models. Before we proceed, we first clarify the differences between the two models: StarCoderBase is the first model trained on 1 trillion tokens sourced from the curated dataset described in Section 3. StarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2 epochs). Throughout the following, we show how we formatted the training data (Section 5.1), decontaminated the training data (Section 5.2), and provide details regarding the tokenizer (Section 5.3), the model architecture (Section 5.4), the training process (Section 5.5), multi-node GPU setup (Section 5.6), and CO2 emissions (Section 5.7). 11https://github.com/domanchi/gibberish-detector 14 F1 Published in Transactions on Machine Learning Research (12/2023) 5.1 Data formatting We present the formatting guidelines for each of the data sources below. We provide the templates below in which <token> refers to a sentinel token, and metadata and data refer to placeholders for data fields, respectively. Code We prepend the repository name, file name, and the number of stars to the context of the code file. To not overfit on the exact number of stars, we categorized GitHub stars into five buckets: 0, 1\u201310, 10\u2013100, 100\u20131000, 1000+. To enable the model to operate without this metadata during inference, we prefixed the repository name, filename, and stars independently at random, each with a probability of 0.2. <reponame>reponame<filename>filename<gh_stars>stars\\ncode<|endoftext|> To the source code in this template (i.e. code), we apply the fill-in-the-middle transformation (FIM; Bavarian et al., 2022). More precisely, we apply FIM at the character-level to the source code files with a FIM-rate of 0.5, and use PSM mode with probability .5 and SPMv2 mode with probability .5. Issues We use sentinel tokens to mark the opening of an issue and subsequently include its title. We separate the sequence of comments by a <issue_comment> token and include a anonymized speaker identifier before the comment. Specifically, we refer to authors by their participant counter within the conversation, e.g. username_1 to refer to second participant in the issue. To distinguish between the different turns, we use comment1, id1 to refer to the second comment and its anonymized speaker id, respectively. <issue_start>Title: title\\nusername_id0:comment0<issue_comment>username_id1:comment1 ... <issue_closed (optional)><|endoftext|> Jupyter \u2013 scripts Jupyter scripts were formatted in the same manner as code. Jupyter \u2013 structured Parsed Jupyter notebooks come in chains of text, code, and outputs, and we separated them with sentinel tokens. Note that we use text2, code2, output2 to refer to the 3rd triplet in the notebook. <jupyter_start><jupyter_text>text0<jupyter_code>code0 <jupyter_output>output0<jupyter_text> ... <|endoftext|> Git commits We separate the code before the commit, the commit message, and the code after the commit with sentinel tokens. As explained in Section 3.4, we use the full files with 20% probability and otherwise use a small window (0-32 lines) around the changed lines. <commit_before>code_before<commit_msg>message<commit_after>code_after<|endoftext|> We summarize all sentinel tokens in Table 10. 5.2 Training data decontamination The code training data was decontaminated by removing files that contained docstrings or solutions from HumanEval and MBPP, docstrings from APPS, questions from GSM8K, or prompts from DS1000. (These benchmarks are further described in Section 6.) To give an indication of the amount of data removed by decontamination, Python is the language with the highest number of matches, with 558 files removed. 15 Published in Transactions on Machine Learning Research (12/2023) Token Description <|endoftext|> <fim_prefix> <fim_middle> <fim_suffix> <fim_pad> <reponame> <filename> <gh_stars> <issue_start> <issue_comment> <issue_closed> <jupyter_start> <jupyter_text> <jupyter_code> <jupyter_output> <empty_output> <commit_before> <commit_msg> <commit_after> end of text/sequence FIM prefix FIM middle FIM suffix FIM pad repository name file name GitHub stars start of GitHub issue start of GitHub issue comment GitHub issue closed event start of Jupyter notebook start of"}, {"question": " How were keys with fewer than 9 characters or that are gibberish handled in the PII detection process?,        answer: They were ignored using a gibberish-detector    ", "ref_chunk": "C-sharp that weren\u2019t well represented in the regex evaluation. Consequently, the overall precision of the tool was below 4%. Post-processing Before applying the best PII detection model to the full dataset, we observed a couple of frequent detection errors. We added the following post-processing techniques to reduce the number of false positives: Ignore secrets with fewer than 4 characters. Detect full names only by requiring at least one space within the name. 13 F1 Published in Transactions on Machine Learning Research (12/2023) Method Name Username Password Prec. Recall F1 Prec. Recall F1 Prec. Recall NER 83.66% 95.52% 89.19% 48.93% 75.55% 59.39% 59.16% 96.62% 73.39% + pseudo labels 86.45% 97.38% 91.59% 52.20% 74.81% 61.49% 70.94% 95.96% 81.57% Table 9: Comparison of PII detection performance: NER Pipeline with Annotated Data vs. Annotated Data + Pseudo-Labels Ignore detected keys with fewer than 9 characters or that are not gibberish using a gibberish-detector.11 Ignore IP addresses that aren\u2019t valid or are private (non-Internet facing) using the ipaddress python package. We also ignore IP addresses from popular DNS servers. We use the same list as in Ben Allal et al. (2023). PII placeholders We replaced the detected PII entities with the following tokens: <NAME>, <EMAIL>, <KEY>, <PASSWORD> To mask IP addresses, we randomly selected an IP address from 5 synthetic, private, non-internet-facing IP addresses of the same type that can be found in Appendix C. Github issues We already employed a regex approach to detect keys, IP addresses, and emails in the Github issues, so we only used the PII detection model to redact names. We anonymized the usernames of the authors by replacing them with a participant counter within the conversation, e.g. username_1 to refer to second participant (see Section 5.1 for formatting details). We prepend these pseudonyms to the beginning of each comment such that we preserve the speaker identity of the author. In addition, we redact all mentions of these usernames in the messages. Note that we only mask the usernames of active participants in the conversation and mentions of non-participating users are not anonymized. Compute resources We used the PII detection model to identify PII across all programming languages in the training dataset, including GitHub issues (names only), Git commits, and Jupyter notebooks. The total dataset amounts to 815 GB in size. We ran inference on multiple NVIDIA A100 80 GB GPUs, which required 800 GPU-hours. 5 Model training This section presents information on the training process of the StarCoder models. Before we proceed, we first clarify the differences between the two models: StarCoderBase is the first model trained on 1 trillion tokens sourced from the curated dataset described in Section 3. StarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2 epochs). Throughout the following, we show how we formatted the training data (Section 5.1), decontaminated the training data (Section 5.2), and provide details regarding the tokenizer (Section 5.3), the model architecture (Section 5.4), the training process (Section 5.5), multi-node GPU setup (Section 5.6), and CO2 emissions (Section 5.7). 11https://github.com/domanchi/gibberish-detector 14 F1 Published in Transactions on Machine Learning Research (12/2023) 5.1 Data formatting We present the formatting guidelines for each of the data sources below. We provide the templates below in which <token> refers to a sentinel token, and metadata and data refer to placeholders for data fields, respectively. Code We prepend the repository name, file name, and the number of stars to the context of the code file. To not overfit on the exact number of stars, we categorized GitHub stars into five buckets: 0, 1\u201310, 10\u2013100, 100\u20131000, 1000+. To enable the model to operate without this metadata during inference, we prefixed the repository name, filename, and stars independently at random, each with a probability of 0.2. <reponame>reponame<filename>filename<gh_stars>stars\\ncode<|endoftext|> To the source code in this template (i.e. code), we apply the fill-in-the-middle transformation (FIM; Bavarian et al., 2022). More precisely, we apply FIM at the character-level to the source code files with a FIM-rate of 0.5, and use PSM mode with probability .5 and SPMv2 mode with probability .5. Issues We use sentinel tokens to mark the opening of an issue and subsequently include its title. We separate the sequence of comments by a <issue_comment> token and include a anonymized speaker identifier before the comment. Specifically, we refer to authors by their participant counter within the conversation, e.g. username_1 to refer to second participant in the issue. To distinguish between the different turns, we use comment1, id1 to refer to the second comment and its anonymized speaker id, respectively. <issue_start>Title: title\\nusername_id0:comment0<issue_comment>username_id1:comment1 ... <issue_closed (optional)><|endoftext|> Jupyter \u2013 scripts Jupyter scripts were formatted in the same manner as code. Jupyter \u2013 structured Parsed Jupyter notebooks come in chains of text, code, and outputs, and we separated them with sentinel tokens. Note that we use text2, code2, output2 to refer to the 3rd triplet in the notebook. <jupyter_start><jupyter_text>text0<jupyter_code>code0 <jupyter_output>output0<jupyter_text> ... <|endoftext|> Git commits We separate the code before the commit, the commit message, and the code after the commit with sentinel tokens. As explained in Section 3.4, we use the full files with 20% probability and otherwise use a small window (0-32 lines) around the changed lines. <commit_before>code_before<commit_msg>message<commit_after>code_after<|endoftext|> We summarize all sentinel tokens in Table 10. 5.2 Training data decontamination The code training data was decontaminated by removing files that contained docstrings or solutions from HumanEval and MBPP, docstrings from APPS, questions from GSM8K, or prompts from DS1000. (These benchmarks are further described in Section 6.) To give an indication of the amount of data removed by decontamination, Python is the language with the highest number of matches, with 558 files removed. 15 Published in Transactions on Machine Learning Research (12/2023) Token Description <|endoftext|> <fim_prefix> <fim_middle> <fim_suffix> <fim_pad> <reponame> <filename> <gh_stars> <issue_start> <issue_comment> <issue_closed> <jupyter_start> <jupyter_text> <jupyter_code> <jupyter_output> <empty_output> <commit_before> <commit_msg> <commit_after> end of text/sequence FIM prefix FIM middle FIM suffix FIM pad repository name file name GitHub stars start of GitHub issue start of GitHub issue comment GitHub issue closed event start of Jupyter notebook start of"}, {"question": " How were invalid or private IP addresses handled in the PII detection process?,        answer: They were ignored using the ipaddress python package    ", "ref_chunk": "C-sharp that weren\u2019t well represented in the regex evaluation. Consequently, the overall precision of the tool was below 4%. Post-processing Before applying the best PII detection model to the full dataset, we observed a couple of frequent detection errors. We added the following post-processing techniques to reduce the number of false positives: Ignore secrets with fewer than 4 characters. Detect full names only by requiring at least one space within the name. 13 F1 Published in Transactions on Machine Learning Research (12/2023) Method Name Username Password Prec. Recall F1 Prec. Recall F1 Prec. Recall NER 83.66% 95.52% 89.19% 48.93% 75.55% 59.39% 59.16% 96.62% 73.39% + pseudo labels 86.45% 97.38% 91.59% 52.20% 74.81% 61.49% 70.94% 95.96% 81.57% Table 9: Comparison of PII detection performance: NER Pipeline with Annotated Data vs. Annotated Data + Pseudo-Labels Ignore detected keys with fewer than 9 characters or that are not gibberish using a gibberish-detector.11 Ignore IP addresses that aren\u2019t valid or are private (non-Internet facing) using the ipaddress python package. We also ignore IP addresses from popular DNS servers. We use the same list as in Ben Allal et al. (2023). PII placeholders We replaced the detected PII entities with the following tokens: <NAME>, <EMAIL>, <KEY>, <PASSWORD> To mask IP addresses, we randomly selected an IP address from 5 synthetic, private, non-internet-facing IP addresses of the same type that can be found in Appendix C. Github issues We already employed a regex approach to detect keys, IP addresses, and emails in the Github issues, so we only used the PII detection model to redact names. We anonymized the usernames of the authors by replacing them with a participant counter within the conversation, e.g. username_1 to refer to second participant (see Section 5.1 for formatting details). We prepend these pseudonyms to the beginning of each comment such that we preserve the speaker identity of the author. In addition, we redact all mentions of these usernames in the messages. Note that we only mask the usernames of active participants in the conversation and mentions of non-participating users are not anonymized. Compute resources We used the PII detection model to identify PII across all programming languages in the training dataset, including GitHub issues (names only), Git commits, and Jupyter notebooks. The total dataset amounts to 815 GB in size. We ran inference on multiple NVIDIA A100 80 GB GPUs, which required 800 GPU-hours. 5 Model training This section presents information on the training process of the StarCoder models. Before we proceed, we first clarify the differences between the two models: StarCoderBase is the first model trained on 1 trillion tokens sourced from the curated dataset described in Section 3. StarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2 epochs). Throughout the following, we show how we formatted the training data (Section 5.1), decontaminated the training data (Section 5.2), and provide details regarding the tokenizer (Section 5.3), the model architecture (Section 5.4), the training process (Section 5.5), multi-node GPU setup (Section 5.6), and CO2 emissions (Section 5.7). 11https://github.com/domanchi/gibberish-detector 14 F1 Published in Transactions on Machine Learning Research (12/2023) 5.1 Data formatting We present the formatting guidelines for each of the data sources below. We provide the templates below in which <token> refers to a sentinel token, and metadata and data refer to placeholders for data fields, respectively. Code We prepend the repository name, file name, and the number of stars to the context of the code file. To not overfit on the exact number of stars, we categorized GitHub stars into five buckets: 0, 1\u201310, 10\u2013100, 100\u20131000, 1000+. To enable the model to operate without this metadata during inference, we prefixed the repository name, filename, and stars independently at random, each with a probability of 0.2. <reponame>reponame<filename>filename<gh_stars>stars\\ncode<|endoftext|> To the source code in this template (i.e. code), we apply the fill-in-the-middle transformation (FIM; Bavarian et al., 2022). More precisely, we apply FIM at the character-level to the source code files with a FIM-rate of 0.5, and use PSM mode with probability .5 and SPMv2 mode with probability .5. Issues We use sentinel tokens to mark the opening of an issue and subsequently include its title. We separate the sequence of comments by a <issue_comment> token and include a anonymized speaker identifier before the comment. Specifically, we refer to authors by their participant counter within the conversation, e.g. username_1 to refer to second participant in the issue. To distinguish between the different turns, we use comment1, id1 to refer to the second comment and its anonymized speaker id, respectively. <issue_start>Title: title\\nusername_id0:comment0<issue_comment>username_id1:comment1 ... <issue_closed (optional)><|endoftext|> Jupyter \u2013 scripts Jupyter scripts were formatted in the same manner as code. Jupyter \u2013 structured Parsed Jupyter notebooks come in chains of text, code, and outputs, and we separated them with sentinel tokens. Note that we use text2, code2, output2 to refer to the 3rd triplet in the notebook. <jupyter_start><jupyter_text>text0<jupyter_code>code0 <jupyter_output>output0<jupyter_text> ... <|endoftext|> Git commits We separate the code before the commit, the commit message, and the code after the commit with sentinel tokens. As explained in Section 3.4, we use the full files with 20% probability and otherwise use a small window (0-32 lines) around the changed lines. <commit_before>code_before<commit_msg>message<commit_after>code_after<|endoftext|> We summarize all sentinel tokens in Table 10. 5.2 Training data decontamination The code training data was decontaminated by removing files that contained docstrings or solutions from HumanEval and MBPP, docstrings from APPS, questions from GSM8K, or prompts from DS1000. (These benchmarks are further described in Section 6.) To give an indication of the amount of data removed by decontamination, Python is the language with the highest number of matches, with 558 files removed. 15 Published in Transactions on Machine Learning Research (12/2023) Token Description <|endoftext|> <fim_prefix> <fim_middle> <fim_suffix> <fim_pad> <reponame> <filename> <gh_stars> <issue_start> <issue_comment> <issue_closed> <jupyter_start> <jupyter_text> <jupyter_code> <jupyter_output> <empty_output> <commit_before> <commit_msg> <commit_after> end of text/sequence FIM prefix FIM middle FIM suffix FIM pad repository name file name GitHub stars start of GitHub issue start of GitHub issue comment GitHub issue closed event start of Jupyter notebook start of"}, {"question": " What placeholders were used to replace the detected PII entities?,        answer: <NAME>, <EMAIL>, <KEY>, <PASSWORD>    ", "ref_chunk": "C-sharp that weren\u2019t well represented in the regex evaluation. Consequently, the overall precision of the tool was below 4%. Post-processing Before applying the best PII detection model to the full dataset, we observed a couple of frequent detection errors. We added the following post-processing techniques to reduce the number of false positives: Ignore secrets with fewer than 4 characters. Detect full names only by requiring at least one space within the name. 13 F1 Published in Transactions on Machine Learning Research (12/2023) Method Name Username Password Prec. Recall F1 Prec. Recall F1 Prec. Recall NER 83.66% 95.52% 89.19% 48.93% 75.55% 59.39% 59.16% 96.62% 73.39% + pseudo labels 86.45% 97.38% 91.59% 52.20% 74.81% 61.49% 70.94% 95.96% 81.57% Table 9: Comparison of PII detection performance: NER Pipeline with Annotated Data vs. Annotated Data + Pseudo-Labels Ignore detected keys with fewer than 9 characters or that are not gibberish using a gibberish-detector.11 Ignore IP addresses that aren\u2019t valid or are private (non-Internet facing) using the ipaddress python package. We also ignore IP addresses from popular DNS servers. We use the same list as in Ben Allal et al. (2023). PII placeholders We replaced the detected PII entities with the following tokens: <NAME>, <EMAIL>, <KEY>, <PASSWORD> To mask IP addresses, we randomly selected an IP address from 5 synthetic, private, non-internet-facing IP addresses of the same type that can be found in Appendix C. Github issues We already employed a regex approach to detect keys, IP addresses, and emails in the Github issues, so we only used the PII detection model to redact names. We anonymized the usernames of the authors by replacing them with a participant counter within the conversation, e.g. username_1 to refer to second participant (see Section 5.1 for formatting details). We prepend these pseudonyms to the beginning of each comment such that we preserve the speaker identity of the author. In addition, we redact all mentions of these usernames in the messages. Note that we only mask the usernames of active participants in the conversation and mentions of non-participating users are not anonymized. Compute resources We used the PII detection model to identify PII across all programming languages in the training dataset, including GitHub issues (names only), Git commits, and Jupyter notebooks. The total dataset amounts to 815 GB in size. We ran inference on multiple NVIDIA A100 80 GB GPUs, which required 800 GPU-hours. 5 Model training This section presents information on the training process of the StarCoder models. Before we proceed, we first clarify the differences between the two models: StarCoderBase is the first model trained on 1 trillion tokens sourced from the curated dataset described in Section 3. StarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2 epochs). Throughout the following, we show how we formatted the training data (Section 5.1), decontaminated the training data (Section 5.2), and provide details regarding the tokenizer (Section 5.3), the model architecture (Section 5.4), the training process (Section 5.5), multi-node GPU setup (Section 5.6), and CO2 emissions (Section 5.7). 11https://github.com/domanchi/gibberish-detector 14 F1 Published in Transactions on Machine Learning Research (12/2023) 5.1 Data formatting We present the formatting guidelines for each of the data sources below. We provide the templates below in which <token> refers to a sentinel token, and metadata and data refer to placeholders for data fields, respectively. Code We prepend the repository name, file name, and the number of stars to the context of the code file. To not overfit on the exact number of stars, we categorized GitHub stars into five buckets: 0, 1\u201310, 10\u2013100, 100\u20131000, 1000+. To enable the model to operate without this metadata during inference, we prefixed the repository name, filename, and stars independently at random, each with a probability of 0.2. <reponame>reponame<filename>filename<gh_stars>stars\\ncode<|endoftext|> To the source code in this template (i.e. code), we apply the fill-in-the-middle transformation (FIM; Bavarian et al., 2022). More precisely, we apply FIM at the character-level to the source code files with a FIM-rate of 0.5, and use PSM mode with probability .5 and SPMv2 mode with probability .5. Issues We use sentinel tokens to mark the opening of an issue and subsequently include its title. We separate the sequence of comments by a <issue_comment> token and include a anonymized speaker identifier before the comment. Specifically, we refer to authors by their participant counter within the conversation, e.g. username_1 to refer to second participant in the issue. To distinguish between the different turns, we use comment1, id1 to refer to the second comment and its anonymized speaker id, respectively. <issue_start>Title: title\\nusername_id0:comment0<issue_comment>username_id1:comment1 ... <issue_closed (optional)><|endoftext|> Jupyter \u2013 scripts Jupyter scripts were formatted in the same manner as code. Jupyter \u2013 structured Parsed Jupyter notebooks come in chains of text, code, and outputs, and we separated them with sentinel tokens. Note that we use text2, code2, output2 to refer to the 3rd triplet in the notebook. <jupyter_start><jupyter_text>text0<jupyter_code>code0 <jupyter_output>output0<jupyter_text> ... <|endoftext|> Git commits We separate the code before the commit, the commit message, and the code after the commit with sentinel tokens. As explained in Section 3.4, we use the full files with 20% probability and otherwise use a small window (0-32 lines) around the changed lines. <commit_before>code_before<commit_msg>message<commit_after>code_after<|endoftext|> We summarize all sentinel tokens in Table 10. 5.2 Training data decontamination The code training data was decontaminated by removing files that contained docstrings or solutions from HumanEval and MBPP, docstrings from APPS, questions from GSM8K, or prompts from DS1000. (These benchmarks are further described in Section 6.) To give an indication of the amount of data removed by decontamination, Python is the language with the highest number of matches, with 558 files removed. 15 Published in Transactions on Machine Learning Research (12/2023) Token Description <|endoftext|> <fim_prefix> <fim_middle> <fim_suffix> <fim_pad> <reponame> <filename> <gh_stars> <issue_start> <issue_comment> <issue_closed> <jupyter_start> <jupyter_text> <jupyter_code> <jupyter_output> <empty_output> <commit_before> <commit_msg> <commit_after> end of text/sequence FIM prefix FIM middle FIM suffix FIM pad repository name file name GitHub stars start of GitHub issue start of GitHub issue comment GitHub issue closed event start of Jupyter notebook start of"}, {"question": " What data sources did the PII detection model identify PII across?,        answer: All programming languages in the training dataset, including GitHub issues, Git commits, and Jupyter notebooks    ", "ref_chunk": "C-sharp that weren\u2019t well represented in the regex evaluation. Consequently, the overall precision of the tool was below 4%. Post-processing Before applying the best PII detection model to the full dataset, we observed a couple of frequent detection errors. We added the following post-processing techniques to reduce the number of false positives: Ignore secrets with fewer than 4 characters. Detect full names only by requiring at least one space within the name. 13 F1 Published in Transactions on Machine Learning Research (12/2023) Method Name Username Password Prec. Recall F1 Prec. Recall F1 Prec. Recall NER 83.66% 95.52% 89.19% 48.93% 75.55% 59.39% 59.16% 96.62% 73.39% + pseudo labels 86.45% 97.38% 91.59% 52.20% 74.81% 61.49% 70.94% 95.96% 81.57% Table 9: Comparison of PII detection performance: NER Pipeline with Annotated Data vs. Annotated Data + Pseudo-Labels Ignore detected keys with fewer than 9 characters or that are not gibberish using a gibberish-detector.11 Ignore IP addresses that aren\u2019t valid or are private (non-Internet facing) using the ipaddress python package. We also ignore IP addresses from popular DNS servers. We use the same list as in Ben Allal et al. (2023). PII placeholders We replaced the detected PII entities with the following tokens: <NAME>, <EMAIL>, <KEY>, <PASSWORD> To mask IP addresses, we randomly selected an IP address from 5 synthetic, private, non-internet-facing IP addresses of the same type that can be found in Appendix C. Github issues We already employed a regex approach to detect keys, IP addresses, and emails in the Github issues, so we only used the PII detection model to redact names. We anonymized the usernames of the authors by replacing them with a participant counter within the conversation, e.g. username_1 to refer to second participant (see Section 5.1 for formatting details). We prepend these pseudonyms to the beginning of each comment such that we preserve the speaker identity of the author. In addition, we redact all mentions of these usernames in the messages. Note that we only mask the usernames of active participants in the conversation and mentions of non-participating users are not anonymized. Compute resources We used the PII detection model to identify PII across all programming languages in the training dataset, including GitHub issues (names only), Git commits, and Jupyter notebooks. The total dataset amounts to 815 GB in size. We ran inference on multiple NVIDIA A100 80 GB GPUs, which required 800 GPU-hours. 5 Model training This section presents information on the training process of the StarCoder models. Before we proceed, we first clarify the differences between the two models: StarCoderBase is the first model trained on 1 trillion tokens sourced from the curated dataset described in Section 3. StarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2 epochs). Throughout the following, we show how we formatted the training data (Section 5.1), decontaminated the training data (Section 5.2), and provide details regarding the tokenizer (Section 5.3), the model architecture (Section 5.4), the training process (Section 5.5), multi-node GPU setup (Section 5.6), and CO2 emissions (Section 5.7). 11https://github.com/domanchi/gibberish-detector 14 F1 Published in Transactions on Machine Learning Research (12/2023) 5.1 Data formatting We present the formatting guidelines for each of the data sources below. We provide the templates below in which <token> refers to a sentinel token, and metadata and data refer to placeholders for data fields, respectively. Code We prepend the repository name, file name, and the number of stars to the context of the code file. To not overfit on the exact number of stars, we categorized GitHub stars into five buckets: 0, 1\u201310, 10\u2013100, 100\u20131000, 1000+. To enable the model to operate without this metadata during inference, we prefixed the repository name, filename, and stars independently at random, each with a probability of 0.2. <reponame>reponame<filename>filename<gh_stars>stars\\ncode<|endoftext|> To the source code in this template (i.e. code), we apply the fill-in-the-middle transformation (FIM; Bavarian et al., 2022). More precisely, we apply FIM at the character-level to the source code files with a FIM-rate of 0.5, and use PSM mode with probability .5 and SPMv2 mode with probability .5. Issues We use sentinel tokens to mark the opening of an issue and subsequently include its title. We separate the sequence of comments by a <issue_comment> token and include a anonymized speaker identifier before the comment. Specifically, we refer to authors by their participant counter within the conversation, e.g. username_1 to refer to second participant in the issue. To distinguish between the different turns, we use comment1, id1 to refer to the second comment and its anonymized speaker id, respectively. <issue_start>Title: title\\nusername_id0:comment0<issue_comment>username_id1:comment1 ... <issue_closed (optional)><|endoftext|> Jupyter \u2013 scripts Jupyter scripts were formatted in the same manner as code. Jupyter \u2013 structured Parsed Jupyter notebooks come in chains of text, code, and outputs, and we separated them with sentinel tokens. Note that we use text2, code2, output2 to refer to the 3rd triplet in the notebook. <jupyter_start><jupyter_text>text0<jupyter_code>code0 <jupyter_output>output0<jupyter_text> ... <|endoftext|> Git commits We separate the code before the commit, the commit message, and the code after the commit with sentinel tokens. As explained in Section 3.4, we use the full files with 20% probability and otherwise use a small window (0-32 lines) around the changed lines. <commit_before>code_before<commit_msg>message<commit_after>code_after<|endoftext|> We summarize all sentinel tokens in Table 10. 5.2 Training data decontamination The code training data was decontaminated by removing files that contained docstrings or solutions from HumanEval and MBPP, docstrings from APPS, questions from GSM8K, or prompts from DS1000. (These benchmarks are further described in Section 6.) To give an indication of the amount of data removed by decontamination, Python is the language with the highest number of matches, with 558 files removed. 15 Published in Transactions on Machine Learning Research (12/2023) Token Description <|endoftext|> <fim_prefix> <fim_middle> <fim_suffix> <fim_pad> <reponame> <filename> <gh_stars> <issue_start> <issue_comment> <issue_closed> <jupyter_start> <jupyter_text> <jupyter_code> <jupyter_output> <empty_output> <commit_before> <commit_msg> <commit_after> end of text/sequence FIM prefix FIM middle FIM suffix FIM pad repository name file name GitHub stars start of GitHub issue start of GitHub issue comment GitHub issue closed event start of Jupyter notebook start of"}, {"question": " What were the differences between StarCoderBase and StarCoder models?,        answer: StarCoderBase was trained on 1 trillion tokens while StarCoder was the fine-tuned version trained on another 35B Python tokens    ", "ref_chunk": "C-sharp that weren\u2019t well represented in the regex evaluation. Consequently, the overall precision of the tool was below 4%. Post-processing Before applying the best PII detection model to the full dataset, we observed a couple of frequent detection errors. We added the following post-processing techniques to reduce the number of false positives: Ignore secrets with fewer than 4 characters. Detect full names only by requiring at least one space within the name. 13 F1 Published in Transactions on Machine Learning Research (12/2023) Method Name Username Password Prec. Recall F1 Prec. Recall F1 Prec. Recall NER 83.66% 95.52% 89.19% 48.93% 75.55% 59.39% 59.16% 96.62% 73.39% + pseudo labels 86.45% 97.38% 91.59% 52.20% 74.81% 61.49% 70.94% 95.96% 81.57% Table 9: Comparison of PII detection performance: NER Pipeline with Annotated Data vs. Annotated Data + Pseudo-Labels Ignore detected keys with fewer than 9 characters or that are not gibberish using a gibberish-detector.11 Ignore IP addresses that aren\u2019t valid or are private (non-Internet facing) using the ipaddress python package. We also ignore IP addresses from popular DNS servers. We use the same list as in Ben Allal et al. (2023). PII placeholders We replaced the detected PII entities with the following tokens: <NAME>, <EMAIL>, <KEY>, <PASSWORD> To mask IP addresses, we randomly selected an IP address from 5 synthetic, private, non-internet-facing IP addresses of the same type that can be found in Appendix C. Github issues We already employed a regex approach to detect keys, IP addresses, and emails in the Github issues, so we only used the PII detection model to redact names. We anonymized the usernames of the authors by replacing them with a participant counter within the conversation, e.g. username_1 to refer to second participant (see Section 5.1 for formatting details). We prepend these pseudonyms to the beginning of each comment such that we preserve the speaker identity of the author. In addition, we redact all mentions of these usernames in the messages. Note that we only mask the usernames of active participants in the conversation and mentions of non-participating users are not anonymized. Compute resources We used the PII detection model to identify PII across all programming languages in the training dataset, including GitHub issues (names only), Git commits, and Jupyter notebooks. The total dataset amounts to 815 GB in size. We ran inference on multiple NVIDIA A100 80 GB GPUs, which required 800 GPU-hours. 5 Model training This section presents information on the training process of the StarCoder models. Before we proceed, we first clarify the differences between the two models: StarCoderBase is the first model trained on 1 trillion tokens sourced from the curated dataset described in Section 3. StarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2 epochs). Throughout the following, we show how we formatted the training data (Section 5.1), decontaminated the training data (Section 5.2), and provide details regarding the tokenizer (Section 5.3), the model architecture (Section 5.4), the training process (Section 5.5), multi-node GPU setup (Section 5.6), and CO2 emissions (Section 5.7). 11https://github.com/domanchi/gibberish-detector 14 F1 Published in Transactions on Machine Learning Research (12/2023) 5.1 Data formatting We present the formatting guidelines for each of the data sources below. We provide the templates below in which <token> refers to a sentinel token, and metadata and data refer to placeholders for data fields, respectively. Code We prepend the repository name, file name, and the number of stars to the context of the code file. To not overfit on the exact number of stars, we categorized GitHub stars into five buckets: 0, 1\u201310, 10\u2013100, 100\u20131000, 1000+. To enable the model to operate without this metadata during inference, we prefixed the repository name, filename, and stars independently at random, each with a probability of 0.2. <reponame>reponame<filename>filename<gh_stars>stars\\ncode<|endoftext|> To the source code in this template (i.e. code), we apply the fill-in-the-middle transformation (FIM; Bavarian et al., 2022). More precisely, we apply FIM at the character-level to the source code files with a FIM-rate of 0.5, and use PSM mode with probability .5 and SPMv2 mode with probability .5. Issues We use sentinel tokens to mark the opening of an issue and subsequently include its title. We separate the sequence of comments by a <issue_comment> token and include a anonymized speaker identifier before the comment. Specifically, we refer to authors by their participant counter within the conversation, e.g. username_1 to refer to second participant in the issue. To distinguish between the different turns, we use comment1, id1 to refer to the second comment and its anonymized speaker id, respectively. <issue_start>Title: title\\nusername_id0:comment0<issue_comment>username_id1:comment1 ... <issue_closed (optional)><|endoftext|> Jupyter \u2013 scripts Jupyter scripts were formatted in the same manner as code. Jupyter \u2013 structured Parsed Jupyter notebooks come in chains of text, code, and outputs, and we separated them with sentinel tokens. Note that we use text2, code2, output2 to refer to the 3rd triplet in the notebook. <jupyter_start><jupyter_text>text0<jupyter_code>code0 <jupyter_output>output0<jupyter_text> ... <|endoftext|> Git commits We separate the code before the commit, the commit message, and the code after the commit with sentinel tokens. As explained in Section 3.4, we use the full files with 20% probability and otherwise use a small window (0-32 lines) around the changed lines. <commit_before>code_before<commit_msg>message<commit_after>code_after<|endoftext|> We summarize all sentinel tokens in Table 10. 5.2 Training data decontamination The code training data was decontaminated by removing files that contained docstrings or solutions from HumanEval and MBPP, docstrings from APPS, questions from GSM8K, or prompts from DS1000. (These benchmarks are further described in Section 6.) To give an indication of the amount of data removed by decontamination, Python is the language with the highest number of matches, with 558 files removed. 15 Published in Transactions on Machine Learning Research (12/2023) Token Description <|endoftext|> <fim_prefix> <fim_middle> <fim_suffix> <fim_pad> <reponame> <filename> <gh_stars> <issue_start> <issue_comment> <issue_closed> <jupyter_start> <jupyter_text> <jupyter_code> <jupyter_output> <empty_output> <commit_before> <commit_msg> <commit_after> end of text/sequence FIM prefix FIM middle FIM suffix FIM pad repository name file name GitHub stars start of GitHub issue start of GitHub issue comment GitHub issue closed event start of Jupyter notebook start of"}, {"question": " What transformation was applied to the source code files in the training data?,        answer: The fill-in-the-middle transformation (FIM)    ", "ref_chunk": "C-sharp that weren\u2019t well represented in the regex evaluation. Consequently, the overall precision of the tool was below 4%. Post-processing Before applying the best PII detection model to the full dataset, we observed a couple of frequent detection errors. We added the following post-processing techniques to reduce the number of false positives: Ignore secrets with fewer than 4 characters. Detect full names only by requiring at least one space within the name. 13 F1 Published in Transactions on Machine Learning Research (12/2023) Method Name Username Password Prec. Recall F1 Prec. Recall F1 Prec. Recall NER 83.66% 95.52% 89.19% 48.93% 75.55% 59.39% 59.16% 96.62% 73.39% + pseudo labels 86.45% 97.38% 91.59% 52.20% 74.81% 61.49% 70.94% 95.96% 81.57% Table 9: Comparison of PII detection performance: NER Pipeline with Annotated Data vs. Annotated Data + Pseudo-Labels Ignore detected keys with fewer than 9 characters or that are not gibberish using a gibberish-detector.11 Ignore IP addresses that aren\u2019t valid or are private (non-Internet facing) using the ipaddress python package. We also ignore IP addresses from popular DNS servers. We use the same list as in Ben Allal et al. (2023). PII placeholders We replaced the detected PII entities with the following tokens: <NAME>, <EMAIL>, <KEY>, <PASSWORD> To mask IP addresses, we randomly selected an IP address from 5 synthetic, private, non-internet-facing IP addresses of the same type that can be found in Appendix C. Github issues We already employed a regex approach to detect keys, IP addresses, and emails in the Github issues, so we only used the PII detection model to redact names. We anonymized the usernames of the authors by replacing them with a participant counter within the conversation, e.g. username_1 to refer to second participant (see Section 5.1 for formatting details). We prepend these pseudonyms to the beginning of each comment such that we preserve the speaker identity of the author. In addition, we redact all mentions of these usernames in the messages. Note that we only mask the usernames of active participants in the conversation and mentions of non-participating users are not anonymized. Compute resources We used the PII detection model to identify PII across all programming languages in the training dataset, including GitHub issues (names only), Git commits, and Jupyter notebooks. The total dataset amounts to 815 GB in size. We ran inference on multiple NVIDIA A100 80 GB GPUs, which required 800 GPU-hours. 5 Model training This section presents information on the training process of the StarCoder models. Before we proceed, we first clarify the differences between the two models: StarCoderBase is the first model trained on 1 trillion tokens sourced from the curated dataset described in Section 3. StarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2 epochs). Throughout the following, we show how we formatted the training data (Section 5.1), decontaminated the training data (Section 5.2), and provide details regarding the tokenizer (Section 5.3), the model architecture (Section 5.4), the training process (Section 5.5), multi-node GPU setup (Section 5.6), and CO2 emissions (Section 5.7). 11https://github.com/domanchi/gibberish-detector 14 F1 Published in Transactions on Machine Learning Research (12/2023) 5.1 Data formatting We present the formatting guidelines for each of the data sources below. We provide the templates below in which <token> refers to a sentinel token, and metadata and data refer to placeholders for data fields, respectively. Code We prepend the repository name, file name, and the number of stars to the context of the code file. To not overfit on the exact number of stars, we categorized GitHub stars into five buckets: 0, 1\u201310, 10\u2013100, 100\u20131000, 1000+. To enable the model to operate without this metadata during inference, we prefixed the repository name, filename, and stars independently at random, each with a probability of 0.2. <reponame>reponame<filename>filename<gh_stars>stars\\ncode<|endoftext|> To the source code in this template (i.e. code), we apply the fill-in-the-middle transformation (FIM; Bavarian et al., 2022). More precisely, we apply FIM at the character-level to the source code files with a FIM-rate of 0.5, and use PSM mode with probability .5 and SPMv2 mode with probability .5. Issues We use sentinel tokens to mark the opening of an issue and subsequently include its title. We separate the sequence of comments by a <issue_comment> token and include a anonymized speaker identifier before the comment. Specifically, we refer to authors by their participant counter within the conversation, e.g. username_1 to refer to second participant in the issue. To distinguish between the different turns, we use comment1, id1 to refer to the second comment and its anonymized speaker id, respectively. <issue_start>Title: title\\nusername_id0:comment0<issue_comment>username_id1:comment1 ... <issue_closed (optional)><|endoftext|> Jupyter \u2013 scripts Jupyter scripts were formatted in the same manner as code. Jupyter \u2013 structured Parsed Jupyter notebooks come in chains of text, code, and outputs, and we separated them with sentinel tokens. Note that we use text2, code2, output2 to refer to the 3rd triplet in the notebook. <jupyter_start><jupyter_text>text0<jupyter_code>code0 <jupyter_output>output0<jupyter_text> ... <|endoftext|> Git commits We separate the code before the commit, the commit message, and the code after the commit with sentinel tokens. As explained in Section 3.4, we use the full files with 20% probability and otherwise use a small window (0-32 lines) around the changed lines. <commit_before>code_before<commit_msg>message<commit_after>code_after<|endoftext|> We summarize all sentinel tokens in Table 10. 5.2 Training data decontamination The code training data was decontaminated by removing files that contained docstrings or solutions from HumanEval and MBPP, docstrings from APPS, questions from GSM8K, or prompts from DS1000. (These benchmarks are further described in Section 6.) To give an indication of the amount of data removed by decontamination, Python is the language with the highest number of matches, with 558 files removed. 15 Published in Transactions on Machine Learning Research (12/2023) Token Description <|endoftext|> <fim_prefix> <fim_middle> <fim_suffix> <fim_pad> <reponame> <filename> <gh_stars> <issue_start> <issue_comment> <issue_closed> <jupyter_start> <jupyter_text> <jupyter_code> <jupyter_output> <empty_output> <commit_before> <commit_msg> <commit_after> end of text/sequence FIM prefix FIM middle FIM suffix FIM pad repository name file name GitHub stars start of GitHub issue start of GitHub issue comment GitHub issue closed event start of Jupyter notebook start of"}, {"question": " How was the training data decontaminated?,        answer: By removing files that contained docstrings or solutions from various benchmarks    ", "ref_chunk": "C-sharp that weren\u2019t well represented in the regex evaluation. Consequently, the overall precision of the tool was below 4%. Post-processing Before applying the best PII detection model to the full dataset, we observed a couple of frequent detection errors. We added the following post-processing techniques to reduce the number of false positives: Ignore secrets with fewer than 4 characters. Detect full names only by requiring at least one space within the name. 13 F1 Published in Transactions on Machine Learning Research (12/2023) Method Name Username Password Prec. Recall F1 Prec. Recall F1 Prec. Recall NER 83.66% 95.52% 89.19% 48.93% 75.55% 59.39% 59.16% 96.62% 73.39% + pseudo labels 86.45% 97.38% 91.59% 52.20% 74.81% 61.49% 70.94% 95.96% 81.57% Table 9: Comparison of PII detection performance: NER Pipeline with Annotated Data vs. Annotated Data + Pseudo-Labels Ignore detected keys with fewer than 9 characters or that are not gibberish using a gibberish-detector.11 Ignore IP addresses that aren\u2019t valid or are private (non-Internet facing) using the ipaddress python package. We also ignore IP addresses from popular DNS servers. We use the same list as in Ben Allal et al. (2023). PII placeholders We replaced the detected PII entities with the following tokens: <NAME>, <EMAIL>, <KEY>, <PASSWORD> To mask IP addresses, we randomly selected an IP address from 5 synthetic, private, non-internet-facing IP addresses of the same type that can be found in Appendix C. Github issues We already employed a regex approach to detect keys, IP addresses, and emails in the Github issues, so we only used the PII detection model to redact names. We anonymized the usernames of the authors by replacing them with a participant counter within the conversation, e.g. username_1 to refer to second participant (see Section 5.1 for formatting details). We prepend these pseudonyms to the beginning of each comment such that we preserve the speaker identity of the author. In addition, we redact all mentions of these usernames in the messages. Note that we only mask the usernames of active participants in the conversation and mentions of non-participating users are not anonymized. Compute resources We used the PII detection model to identify PII across all programming languages in the training dataset, including GitHub issues (names only), Git commits, and Jupyter notebooks. The total dataset amounts to 815 GB in size. We ran inference on multiple NVIDIA A100 80 GB GPUs, which required 800 GPU-hours. 5 Model training This section presents information on the training process of the StarCoder models. Before we proceed, we first clarify the differences between the two models: StarCoderBase is the first model trained on 1 trillion tokens sourced from the curated dataset described in Section 3. StarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2 epochs). Throughout the following, we show how we formatted the training data (Section 5.1), decontaminated the training data (Section 5.2), and provide details regarding the tokenizer (Section 5.3), the model architecture (Section 5.4), the training process (Section 5.5), multi-node GPU setup (Section 5.6), and CO2 emissions (Section 5.7). 11https://github.com/domanchi/gibberish-detector 14 F1 Published in Transactions on Machine Learning Research (12/2023) 5.1 Data formatting We present the formatting guidelines for each of the data sources below. We provide the templates below in which <token> refers to a sentinel token, and metadata and data refer to placeholders for data fields, respectively. Code We prepend the repository name, file name, and the number of stars to the context of the code file. To not overfit on the exact number of stars, we categorized GitHub stars into five buckets: 0, 1\u201310, 10\u2013100, 100\u20131000, 1000+. To enable the model to operate without this metadata during inference, we prefixed the repository name, filename, and stars independently at random, each with a probability of 0.2. <reponame>reponame<filename>filename<gh_stars>stars\\ncode<|endoftext|> To the source code in this template (i.e. code), we apply the fill-in-the-middle transformation (FIM; Bavarian et al., 2022). More precisely, we apply FIM at the character-level to the source code files with a FIM-rate of 0.5, and use PSM mode with probability .5 and SPMv2 mode with probability .5. Issues We use sentinel tokens to mark the opening of an issue and subsequently include its title. We separate the sequence of comments by a <issue_comment> token and include a anonymized speaker identifier before the comment. Specifically, we refer to authors by their participant counter within the conversation, e.g. username_1 to refer to second participant in the issue. To distinguish between the different turns, we use comment1, id1 to refer to the second comment and its anonymized speaker id, respectively. <issue_start>Title: title\\nusername_id0:comment0<issue_comment>username_id1:comment1 ... <issue_closed (optional)><|endoftext|> Jupyter \u2013 scripts Jupyter scripts were formatted in the same manner as code. Jupyter \u2013 structured Parsed Jupyter notebooks come in chains of text, code, and outputs, and we separated them with sentinel tokens. Note that we use text2, code2, output2 to refer to the 3rd triplet in the notebook. <jupyter_start><jupyter_text>text0<jupyter_code>code0 <jupyter_output>output0<jupyter_text> ... <|endoftext|> Git commits We separate the code before the commit, the commit message, and the code after the commit with sentinel tokens. As explained in Section 3.4, we use the full files with 20% probability and otherwise use a small window (0-32 lines) around the changed lines. <commit_before>code_before<commit_msg>message<commit_after>code_after<|endoftext|> We summarize all sentinel tokens in Table 10. 5.2 Training data decontamination The code training data was decontaminated by removing files that contained docstrings or solutions from HumanEval and MBPP, docstrings from APPS, questions from GSM8K, or prompts from DS1000. (These benchmarks are further described in Section 6.) To give an indication of the amount of data removed by decontamination, Python is the language with the highest number of matches, with 558 files removed. 15 Published in Transactions on Machine Learning Research (12/2023) Token Description <|endoftext|> <fim_prefix> <fim_middle> <fim_suffix> <fim_pad> <reponame> <filename> <gh_stars> <issue_start> <issue_comment> <issue_closed> <jupyter_start> <jupyter_text> <jupyter_code> <jupyter_output> <empty_output> <commit_before> <commit_msg> <commit_after> end of text/sequence FIM prefix FIM middle FIM suffix FIM pad repository name file name GitHub stars start of GitHub issue start of GitHub issue comment GitHub issue closed event start of Jupyter notebook start of"}, {"question": " What was the language with the highest number of matches in the removed training data files?,        answer: Python    ", "ref_chunk": "C-sharp that weren\u2019t well represented in the regex evaluation. Consequently, the overall precision of the tool was below 4%. Post-processing Before applying the best PII detection model to the full dataset, we observed a couple of frequent detection errors. We added the following post-processing techniques to reduce the number of false positives: Ignore secrets with fewer than 4 characters. Detect full names only by requiring at least one space within the name. 13 F1 Published in Transactions on Machine Learning Research (12/2023) Method Name Username Password Prec. Recall F1 Prec. Recall F1 Prec. Recall NER 83.66% 95.52% 89.19% 48.93% 75.55% 59.39% 59.16% 96.62% 73.39% + pseudo labels 86.45% 97.38% 91.59% 52.20% 74.81% 61.49% 70.94% 95.96% 81.57% Table 9: Comparison of PII detection performance: NER Pipeline with Annotated Data vs. Annotated Data + Pseudo-Labels Ignore detected keys with fewer than 9 characters or that are not gibberish using a gibberish-detector.11 Ignore IP addresses that aren\u2019t valid or are private (non-Internet facing) using the ipaddress python package. We also ignore IP addresses from popular DNS servers. We use the same list as in Ben Allal et al. (2023). PII placeholders We replaced the detected PII entities with the following tokens: <NAME>, <EMAIL>, <KEY>, <PASSWORD> To mask IP addresses, we randomly selected an IP address from 5 synthetic, private, non-internet-facing IP addresses of the same type that can be found in Appendix C. Github issues We already employed a regex approach to detect keys, IP addresses, and emails in the Github issues, so we only used the PII detection model to redact names. We anonymized the usernames of the authors by replacing them with a participant counter within the conversation, e.g. username_1 to refer to second participant (see Section 5.1 for formatting details). We prepend these pseudonyms to the beginning of each comment such that we preserve the speaker identity of the author. In addition, we redact all mentions of these usernames in the messages. Note that we only mask the usernames of active participants in the conversation and mentions of non-participating users are not anonymized. Compute resources We used the PII detection model to identify PII across all programming languages in the training dataset, including GitHub issues (names only), Git commits, and Jupyter notebooks. The total dataset amounts to 815 GB in size. We ran inference on multiple NVIDIA A100 80 GB GPUs, which required 800 GPU-hours. 5 Model training This section presents information on the training process of the StarCoder models. Before we proceed, we first clarify the differences between the two models: StarCoderBase is the first model trained on 1 trillion tokens sourced from the curated dataset described in Section 3. StarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2 epochs). Throughout the following, we show how we formatted the training data (Section 5.1), decontaminated the training data (Section 5.2), and provide details regarding the tokenizer (Section 5.3), the model architecture (Section 5.4), the training process (Section 5.5), multi-node GPU setup (Section 5.6), and CO2 emissions (Section 5.7). 11https://github.com/domanchi/gibberish-detector 14 F1 Published in Transactions on Machine Learning Research (12/2023) 5.1 Data formatting We present the formatting guidelines for each of the data sources below. We provide the templates below in which <token> refers to a sentinel token, and metadata and data refer to placeholders for data fields, respectively. Code We prepend the repository name, file name, and the number of stars to the context of the code file. To not overfit on the exact number of stars, we categorized GitHub stars into five buckets: 0, 1\u201310, 10\u2013100, 100\u20131000, 1000+. To enable the model to operate without this metadata during inference, we prefixed the repository name, filename, and stars independently at random, each with a probability of 0.2. <reponame>reponame<filename>filename<gh_stars>stars\\ncode<|endoftext|> To the source code in this template (i.e. code), we apply the fill-in-the-middle transformation (FIM; Bavarian et al., 2022). More precisely, we apply FIM at the character-level to the source code files with a FIM-rate of 0.5, and use PSM mode with probability .5 and SPMv2 mode with probability .5. Issues We use sentinel tokens to mark the opening of an issue and subsequently include its title. We separate the sequence of comments by a <issue_comment> token and include a anonymized speaker identifier before the comment. Specifically, we refer to authors by their participant counter within the conversation, e.g. username_1 to refer to second participant in the issue. To distinguish between the different turns, we use comment1, id1 to refer to the second comment and its anonymized speaker id, respectively. <issue_start>Title: title\\nusername_id0:comment0<issue_comment>username_id1:comment1 ... <issue_closed (optional)><|endoftext|> Jupyter \u2013 scripts Jupyter scripts were formatted in the same manner as code. Jupyter \u2013 structured Parsed Jupyter notebooks come in chains of text, code, and outputs, and we separated them with sentinel tokens. Note that we use text2, code2, output2 to refer to the 3rd triplet in the notebook. <jupyter_start><jupyter_text>text0<jupyter_code>code0 <jupyter_output>output0<jupyter_text> ... <|endoftext|> Git commits We separate the code before the commit, the commit message, and the code after the commit with sentinel tokens. As explained in Section 3.4, we use the full files with 20% probability and otherwise use a small window (0-32 lines) around the changed lines. <commit_before>code_before<commit_msg>message<commit_after>code_after<|endoftext|> We summarize all sentinel tokens in Table 10. 5.2 Training data decontamination The code training data was decontaminated by removing files that contained docstrings or solutions from HumanEval and MBPP, docstrings from APPS, questions from GSM8K, or prompts from DS1000. (These benchmarks are further described in Section 6.) To give an indication of the amount of data removed by decontamination, Python is the language with the highest number of matches, with 558 files removed. 15 Published in Transactions on Machine Learning Research (12/2023) Token Description <|endoftext|> <fim_prefix> <fim_middle> <fim_suffix> <fim_pad> <reponame> <filename> <gh_stars> <issue_start> <issue_comment> <issue_closed> <jupyter_start> <jupyter_text> <jupyter_code> <jupyter_output> <empty_output> <commit_before> <commit_msg> <commit_after> end of text/sequence FIM prefix FIM middle FIM suffix FIM pad repository name file name GitHub stars start of GitHub issue start of GitHub issue comment GitHub issue closed event start of Jupyter notebook start of"}], "doc_text": "C-sharp that weren\u2019t well represented in the regex evaluation. Consequently, the overall precision of the tool was below 4%. Post-processing Before applying the best PII detection model to the full dataset, we observed a couple of frequent detection errors. We added the following post-processing techniques to reduce the number of false positives: Ignore secrets with fewer than 4 characters. Detect full names only by requiring at least one space within the name. 13 F1 Published in Transactions on Machine Learning Research (12/2023) Method Name Username Password Prec. Recall F1 Prec. Recall F1 Prec. Recall NER 83.66% 95.52% 89.19% 48.93% 75.55% 59.39% 59.16% 96.62% 73.39% + pseudo labels 86.45% 97.38% 91.59% 52.20% 74.81% 61.49% 70.94% 95.96% 81.57% Table 9: Comparison of PII detection performance: NER Pipeline with Annotated Data vs. Annotated Data + Pseudo-Labels Ignore detected keys with fewer than 9 characters or that are not gibberish using a gibberish-detector.11 Ignore IP addresses that aren\u2019t valid or are private (non-Internet facing) using the ipaddress python package. We also ignore IP addresses from popular DNS servers. We use the same list as in Ben Allal et al. (2023). PII placeholders We replaced the detected PII entities with the following tokens: <NAME>, <EMAIL>, <KEY>, <PASSWORD> To mask IP addresses, we randomly selected an IP address from 5 synthetic, private, non-internet-facing IP addresses of the same type that can be found in Appendix C. Github issues We already employed a regex approach to detect keys, IP addresses, and emails in the Github issues, so we only used the PII detection model to redact names. We anonymized the usernames of the authors by replacing them with a participant counter within the conversation, e.g. username_1 to refer to second participant (see Section 5.1 for formatting details). We prepend these pseudonyms to the beginning of each comment such that we preserve the speaker identity of the author. In addition, we redact all mentions of these usernames in the messages. Note that we only mask the usernames of active participants in the conversation and mentions of non-participating users are not anonymized. Compute resources We used the PII detection model to identify PII across all programming languages in the training dataset, including GitHub issues (names only), Git commits, and Jupyter notebooks. The total dataset amounts to 815 GB in size. We ran inference on multiple NVIDIA A100 80 GB GPUs, which required 800 GPU-hours. 5 Model training This section presents information on the training process of the StarCoder models. Before we proceed, we first clarify the differences between the two models: StarCoderBase is the first model trained on 1 trillion tokens sourced from the curated dataset described in Section 3. StarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2 epochs). Throughout the following, we show how we formatted the training data (Section 5.1), decontaminated the training data (Section 5.2), and provide details regarding the tokenizer (Section 5.3), the model architecture (Section 5.4), the training process (Section 5.5), multi-node GPU setup (Section 5.6), and CO2 emissions (Section 5.7). 11https://github.com/domanchi/gibberish-detector 14 F1 Published in Transactions on Machine Learning Research (12/2023) 5.1 Data formatting We present the formatting guidelines for each of the data sources below. We provide the templates below in which <token> refers to a sentinel token, and metadata and data refer to placeholders for data fields, respectively. Code We prepend the repository name, file name, and the number of stars to the context of the code file. To not overfit on the exact number of stars, we categorized GitHub stars into five buckets: 0, 1\u201310, 10\u2013100, 100\u20131000, 1000+. To enable the model to operate without this metadata during inference, we prefixed the repository name, filename, and stars independently at random, each with a probability of 0.2. <reponame>reponame<filename>filename<gh_stars>stars\\ncode<|endoftext|> To the source code in this template (i.e. code), we apply the fill-in-the-middle transformation (FIM; Bavarian et al., 2022). More precisely, we apply FIM at the character-level to the source code files with a FIM-rate of 0.5, and use PSM mode with probability .5 and SPMv2 mode with probability .5. Issues We use sentinel tokens to mark the opening of an issue and subsequently include its title. We separate the sequence of comments by a <issue_comment> token and include a anonymized speaker identifier before the comment. Specifically, we refer to authors by their participant counter within the conversation, e.g. username_1 to refer to second participant in the issue. To distinguish between the different turns, we use comment1, id1 to refer to the second comment and its anonymized speaker id, respectively. <issue_start>Title: title\\nusername_id0:comment0<issue_comment>username_id1:comment1 ... <issue_closed (optional)><|endoftext|> Jupyter \u2013 scripts Jupyter scripts were formatted in the same manner as code. Jupyter \u2013 structured Parsed Jupyter notebooks come in chains of text, code, and outputs, and we separated them with sentinel tokens. Note that we use text2, code2, output2 to refer to the 3rd triplet in the notebook. <jupyter_start><jupyter_text>text0<jupyter_code>code0 <jupyter_output>output0<jupyter_text> ... <|endoftext|> Git commits We separate the code before the commit, the commit message, and the code after the commit with sentinel tokens. As explained in Section 3.4, we use the full files with 20% probability and otherwise use a small window (0-32 lines) around the changed lines. <commit_before>code_before<commit_msg>message<commit_after>code_after<|endoftext|> We summarize all sentinel tokens in Table 10. 5.2 Training data decontamination The code training data was decontaminated by removing files that contained docstrings or solutions from HumanEval and MBPP, docstrings from APPS, questions from GSM8K, or prompts from DS1000. (These benchmarks are further described in Section 6.) To give an indication of the amount of data removed by decontamination, Python is the language with the highest number of matches, with 558 files removed. 15 Published in Transactions on Machine Learning Research (12/2023) Token Description <|endoftext|> <fim_prefix> <fim_middle> <fim_suffix> <fim_pad> <reponame> <filename> <gh_stars> <issue_start> <issue_comment> <issue_closed> <jupyter_start> <jupyter_text> <jupyter_code> <jupyter_output> <empty_output> <commit_before> <commit_msg> <commit_after> end of text/sequence FIM prefix FIM middle FIM suffix FIM pad repository name file name GitHub stars start of GitHub issue start of GitHub issue comment GitHub issue closed event start of Jupyter notebook start of"}