{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Deep_Speech_Synthesis_from_MRI-Based_Articulatory_Representations_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What type of GPU was used in the experiments?", "answer": " RTX A5000 GPU", "ref_chunk": "model and the base- lines, summarized in Table 1. GPU trials use one RTX A5000 GPU, and CPU trials use none. Like Wu et al. [13], we report inference time as the mean and standard deviation of five tri- 2https://github.com/kan-bayashi/ ParallelWaveGAN 3https://podcast.adobe.com/enhance Table 1: Average inference time and number of parameters for MRI-to-speech models. See Section 4 for details. Model CPU (s) \u2193 GPU (s) \u2193 Params. \u2193 CBL (Spe.) [14] CBL (Hub.) HiFi-CAR .66 \u00b1 .05 .69 \u00b1 .04 .58 \u00b1 .03 .081 \u00b1 .009 .090 \u00b1 .016 .061 \u00b1 .015 1.9 \u2217 107 2.3 \u2217 107 1.5 \u2217 107 Table 2: AB test results. See Section 5.1 for details. Baseline Type AB Test Votes Baseline Ours Same CBL (Spe.) [14] + Denoising 1 18 53 33 0 3 als, each calculating the average time to synthesize an utterance in our test. Our model is faster and uses less parameters than both intermediate-representation baselines, reinforcing the idea that directly mapping articulatory features to speech is more ef- ficient than relying on an intermediate representations. 5. Synthesis Quality 5.1. Subjective Fidelity Evaluation We perform a subjective AB preference test on Amazon Me- chanical Turk (MTurk). In this evaluation, each participant is asked to distinguish between the utterances generated by our method and the baselines in terms of naturalness. We compared our model with two baselines: (1) Yu et al. [14], detailed in Section 3.2, and (2) [14] trained with our denoised waveforms described in Section 3.4 as targets. For each of the two AB tests, we asked 6 native English listeners to rank a total of 9 random samples from the test set in this study. To prevent listeners from randomly submitting results, we added an audio pair consisting of one audio sample consisting entirely of noise and another au- dio sample containing high-fidelity speech. In Table 2, we sum- marize the total number of votes for each of the three options for both AB tests. Our model outperforms both baselines, receiv- ing the most votes. The almost unanimous vote for our model in the AB test with the non-denoised baseline highlights the im- portance of denoising waveforms accompanying MRI data. Our model also noticeably outperforms the denoised baseline, sug- gesting that direct synthesis approach described in Section 3.3 is well-suited for articulatory synthesis. 5.2. Objective Fidelity Evaluation We perform an objective evaluation of synthesis quality by ana- lyzing the mel-cepstral distortions (MCD) [35] between ground truths and synthesized samples, as in Wu et al. [13]. Table 3 summarizes these results, reporting the mean and standard deviation of the MCDs across utterances. Our HiFi-CAR ap- proach outperforms both intermediate-representation baselines, suggesting that our direct modelling method is suitable for the MRI-to-speech task. 5.3. Transcription We also compare our method with the baselines in terms of speech intelligibility. Specifically, we compute the character error rate (CER) for speech transcription. We use Whisper [36], Table 3: MCD between MRI-to-speech model outputs and de- noised ground truths. See Section 5.2 for details. Model MCD \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 7.31 \u00b1 0.45 8.84 \u00b1 1.00 6.64 \u00b1 0.64 Table 4: ASR CER for MRI-to-speech model outputs. See Sec- tion 5.3 for details. Model CER \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 84.7% \u00b1 36.4% 84.2% \u00b1 15.7% 69.2% \u00b1 28.1% a state-of-the-art automatic speech recognition (ASR) model, to generate text from the speech synthesized using each method and all test set utterances. Table 4 summarizes these results. Like in Table 3, our model outperforms both baselines, reinforc- ing the suitability of our model for MRI-to-speech synthesis. 6. Comparing MRI and EMA Features As mentioned in Section 1, MRI provides much more informa- tion about the vocal tract than EMA. MRI is a superset of EMA. Specifically, EMA has one x-y coordinate for each of the fol- lowing locations: upper lip, lower lip, lower incisor, tongue tip, tongue body, and tongue dorsum. Points at all of these locations are present in the MRI data, so we can actually approximate EMA features from MRI by choosing one MRI point at each In this figure, seg- EMA location, as visualized in Figure 2. ments are the connected MRI points and the shaped dots are the estimated EMA locations. We compare these two articulatory feature sets by comparing the outputs of our proposed MRI- to-speech model with those of this model trained to synthesize speech from our estimated 12-dimensional EMA features. The test set predictions of this EMA-to-speech model yielded an MCD of 6.986\u00b10.587 and ASR CER of 73.2%\u00b16.7%. Both of these values are worse than those of our MRI-to-speech model, summarized in Tables 3 and 4. This suggests that MRI features are more complete representations of the human vocal tract than EMA features. Thus, articulatory synthesis models should in- corporate features beyond EMA in order to achieve human-like fidelity across all utterances, with MRI features being a poten- tial feature set to extend towards. We identify which of the MRI features would be the most valuable to add to the articulatory feature set in Section 7. 7. Identifying Important MRI Features We also study which of the MRI features are the most useful for synthesis in order to provide insight into which features should be present in an ideal articulatory feature set for artic- ulatory synthesis. Specifically, we created 50 subsets of our 230-dimensional MRI feature set, each composed of a random 90% subset of the 230 features. With each feature subset, we masked the 23 MRI features not in the subset to 0.0 and syn- thesized the test set utterances. Then, we computed the average MCD between the test set ground truths and the synthesized waveforms. For each MRI feature, we assign it a score equal 100 arytenoid lower lip upper lip 25 lower teeth chin 150 tongue hard palate 225 50 75 epiglottis 175 125 velum 200 pharynx Figure 4: Importance of each MRI feature for MRI-to-speech synthesis. See Section 6 for details. to"}, {"question": " How many intermediate-representation baselines were compared to the proposed model?", "answer": " Two", "ref_chunk": "model and the base- lines, summarized in Table 1. GPU trials use one RTX A5000 GPU, and CPU trials use none. Like Wu et al. [13], we report inference time as the mean and standard deviation of five tri- 2https://github.com/kan-bayashi/ ParallelWaveGAN 3https://podcast.adobe.com/enhance Table 1: Average inference time and number of parameters for MRI-to-speech models. See Section 4 for details. Model CPU (s) \u2193 GPU (s) \u2193 Params. \u2193 CBL (Spe.) [14] CBL (Hub.) HiFi-CAR .66 \u00b1 .05 .69 \u00b1 .04 .58 \u00b1 .03 .081 \u00b1 .009 .090 \u00b1 .016 .061 \u00b1 .015 1.9 \u2217 107 2.3 \u2217 107 1.5 \u2217 107 Table 2: AB test results. See Section 5.1 for details. Baseline Type AB Test Votes Baseline Ours Same CBL (Spe.) [14] + Denoising 1 18 53 33 0 3 als, each calculating the average time to synthesize an utterance in our test. Our model is faster and uses less parameters than both intermediate-representation baselines, reinforcing the idea that directly mapping articulatory features to speech is more ef- ficient than relying on an intermediate representations. 5. Synthesis Quality 5.1. Subjective Fidelity Evaluation We perform a subjective AB preference test on Amazon Me- chanical Turk (MTurk). In this evaluation, each participant is asked to distinguish between the utterances generated by our method and the baselines in terms of naturalness. We compared our model with two baselines: (1) Yu et al. [14], detailed in Section 3.2, and (2) [14] trained with our denoised waveforms described in Section 3.4 as targets. For each of the two AB tests, we asked 6 native English listeners to rank a total of 9 random samples from the test set in this study. To prevent listeners from randomly submitting results, we added an audio pair consisting of one audio sample consisting entirely of noise and another au- dio sample containing high-fidelity speech. In Table 2, we sum- marize the total number of votes for each of the three options for both AB tests. Our model outperforms both baselines, receiv- ing the most votes. The almost unanimous vote for our model in the AB test with the non-denoised baseline highlights the im- portance of denoising waveforms accompanying MRI data. Our model also noticeably outperforms the denoised baseline, sug- gesting that direct synthesis approach described in Section 3.3 is well-suited for articulatory synthesis. 5.2. Objective Fidelity Evaluation We perform an objective evaluation of synthesis quality by ana- lyzing the mel-cepstral distortions (MCD) [35] between ground truths and synthesized samples, as in Wu et al. [13]. Table 3 summarizes these results, reporting the mean and standard deviation of the MCDs across utterances. Our HiFi-CAR ap- proach outperforms both intermediate-representation baselines, suggesting that our direct modelling method is suitable for the MRI-to-speech task. 5.3. Transcription We also compare our method with the baselines in terms of speech intelligibility. Specifically, we compute the character error rate (CER) for speech transcription. We use Whisper [36], Table 3: MCD between MRI-to-speech model outputs and de- noised ground truths. See Section 5.2 for details. Model MCD \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 7.31 \u00b1 0.45 8.84 \u00b1 1.00 6.64 \u00b1 0.64 Table 4: ASR CER for MRI-to-speech model outputs. See Sec- tion 5.3 for details. Model CER \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 84.7% \u00b1 36.4% 84.2% \u00b1 15.7% 69.2% \u00b1 28.1% a state-of-the-art automatic speech recognition (ASR) model, to generate text from the speech synthesized using each method and all test set utterances. Table 4 summarizes these results. Like in Table 3, our model outperforms both baselines, reinforc- ing the suitability of our model for MRI-to-speech synthesis. 6. Comparing MRI and EMA Features As mentioned in Section 1, MRI provides much more informa- tion about the vocal tract than EMA. MRI is a superset of EMA. Specifically, EMA has one x-y coordinate for each of the fol- lowing locations: upper lip, lower lip, lower incisor, tongue tip, tongue body, and tongue dorsum. Points at all of these locations are present in the MRI data, so we can actually approximate EMA features from MRI by choosing one MRI point at each In this figure, seg- EMA location, as visualized in Figure 2. ments are the connected MRI points and the shaped dots are the estimated EMA locations. We compare these two articulatory feature sets by comparing the outputs of our proposed MRI- to-speech model with those of this model trained to synthesize speech from our estimated 12-dimensional EMA features. The test set predictions of this EMA-to-speech model yielded an MCD of 6.986\u00b10.587 and ASR CER of 73.2%\u00b16.7%. Both of these values are worse than those of our MRI-to-speech model, summarized in Tables 3 and 4. This suggests that MRI features are more complete representations of the human vocal tract than EMA features. Thus, articulatory synthesis models should in- corporate features beyond EMA in order to achieve human-like fidelity across all utterances, with MRI features being a poten- tial feature set to extend towards. We identify which of the MRI features would be the most valuable to add to the articulatory feature set in Section 7. 7. Identifying Important MRI Features We also study which of the MRI features are the most useful for synthesis in order to provide insight into which features should be present in an ideal articulatory feature set for artic- ulatory synthesis. Specifically, we created 50 subsets of our 230-dimensional MRI feature set, each composed of a random 90% subset of the 230 features. With each feature subset, we masked the 23 MRI features not in the subset to 0.0 and syn- thesized the test set utterances. Then, we computed the average MCD between the test set ground truths and the synthesized waveforms. For each MRI feature, we assign it a score equal 100 arytenoid lower lip upper lip 25 lower teeth chin 150 tongue hard palate 225 50 75 epiglottis 175 125 velum 200 pharynx Figure 4: Importance of each MRI feature for MRI-to-speech synthesis. See Section 6 for details. to"}, {"question": " What evaluation method was used to compare the naturalness of utterances generated by different models?", "answer": " Subjective AB preference test on Amazon Mechanical Turk (MTurk)", "ref_chunk": "model and the base- lines, summarized in Table 1. GPU trials use one RTX A5000 GPU, and CPU trials use none. Like Wu et al. [13], we report inference time as the mean and standard deviation of five tri- 2https://github.com/kan-bayashi/ ParallelWaveGAN 3https://podcast.adobe.com/enhance Table 1: Average inference time and number of parameters for MRI-to-speech models. See Section 4 for details. Model CPU (s) \u2193 GPU (s) \u2193 Params. \u2193 CBL (Spe.) [14] CBL (Hub.) HiFi-CAR .66 \u00b1 .05 .69 \u00b1 .04 .58 \u00b1 .03 .081 \u00b1 .009 .090 \u00b1 .016 .061 \u00b1 .015 1.9 \u2217 107 2.3 \u2217 107 1.5 \u2217 107 Table 2: AB test results. See Section 5.1 for details. Baseline Type AB Test Votes Baseline Ours Same CBL (Spe.) [14] + Denoising 1 18 53 33 0 3 als, each calculating the average time to synthesize an utterance in our test. Our model is faster and uses less parameters than both intermediate-representation baselines, reinforcing the idea that directly mapping articulatory features to speech is more ef- ficient than relying on an intermediate representations. 5. Synthesis Quality 5.1. Subjective Fidelity Evaluation We perform a subjective AB preference test on Amazon Me- chanical Turk (MTurk). In this evaluation, each participant is asked to distinguish between the utterances generated by our method and the baselines in terms of naturalness. We compared our model with two baselines: (1) Yu et al. [14], detailed in Section 3.2, and (2) [14] trained with our denoised waveforms described in Section 3.4 as targets. For each of the two AB tests, we asked 6 native English listeners to rank a total of 9 random samples from the test set in this study. To prevent listeners from randomly submitting results, we added an audio pair consisting of one audio sample consisting entirely of noise and another au- dio sample containing high-fidelity speech. In Table 2, we sum- marize the total number of votes for each of the three options for both AB tests. Our model outperforms both baselines, receiv- ing the most votes. The almost unanimous vote for our model in the AB test with the non-denoised baseline highlights the im- portance of denoising waveforms accompanying MRI data. Our model also noticeably outperforms the denoised baseline, sug- gesting that direct synthesis approach described in Section 3.3 is well-suited for articulatory synthesis. 5.2. Objective Fidelity Evaluation We perform an objective evaluation of synthesis quality by ana- lyzing the mel-cepstral distortions (MCD) [35] between ground truths and synthesized samples, as in Wu et al. [13]. Table 3 summarizes these results, reporting the mean and standard deviation of the MCDs across utterances. Our HiFi-CAR ap- proach outperforms both intermediate-representation baselines, suggesting that our direct modelling method is suitable for the MRI-to-speech task. 5.3. Transcription We also compare our method with the baselines in terms of speech intelligibility. Specifically, we compute the character error rate (CER) for speech transcription. We use Whisper [36], Table 3: MCD between MRI-to-speech model outputs and de- noised ground truths. See Section 5.2 for details. Model MCD \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 7.31 \u00b1 0.45 8.84 \u00b1 1.00 6.64 \u00b1 0.64 Table 4: ASR CER for MRI-to-speech model outputs. See Sec- tion 5.3 for details. Model CER \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 84.7% \u00b1 36.4% 84.2% \u00b1 15.7% 69.2% \u00b1 28.1% a state-of-the-art automatic speech recognition (ASR) model, to generate text from the speech synthesized using each method and all test set utterances. Table 4 summarizes these results. Like in Table 3, our model outperforms both baselines, reinforc- ing the suitability of our model for MRI-to-speech synthesis. 6. Comparing MRI and EMA Features As mentioned in Section 1, MRI provides much more informa- tion about the vocal tract than EMA. MRI is a superset of EMA. Specifically, EMA has one x-y coordinate for each of the fol- lowing locations: upper lip, lower lip, lower incisor, tongue tip, tongue body, and tongue dorsum. Points at all of these locations are present in the MRI data, so we can actually approximate EMA features from MRI by choosing one MRI point at each In this figure, seg- EMA location, as visualized in Figure 2. ments are the connected MRI points and the shaped dots are the estimated EMA locations. We compare these two articulatory feature sets by comparing the outputs of our proposed MRI- to-speech model with those of this model trained to synthesize speech from our estimated 12-dimensional EMA features. The test set predictions of this EMA-to-speech model yielded an MCD of 6.986\u00b10.587 and ASR CER of 73.2%\u00b16.7%. Both of these values are worse than those of our MRI-to-speech model, summarized in Tables 3 and 4. This suggests that MRI features are more complete representations of the human vocal tract than EMA features. Thus, articulatory synthesis models should in- corporate features beyond EMA in order to achieve human-like fidelity across all utterances, with MRI features being a poten- tial feature set to extend towards. We identify which of the MRI features would be the most valuable to add to the articulatory feature set in Section 7. 7. Identifying Important MRI Features We also study which of the MRI features are the most useful for synthesis in order to provide insight into which features should be present in an ideal articulatory feature set for artic- ulatory synthesis. Specifically, we created 50 subsets of our 230-dimensional MRI feature set, each composed of a random 90% subset of the 230 features. With each feature subset, we masked the 23 MRI features not in the subset to 0.0 and syn- thesized the test set utterances. Then, we computed the average MCD between the test set ground truths and the synthesized waveforms. For each MRI feature, we assign it a score equal 100 arytenoid lower lip upper lip 25 lower teeth chin 150 tongue hard palate 225 50 75 epiglottis 175 125 velum 200 pharynx Figure 4: Importance of each MRI feature for MRI-to-speech synthesis. See Section 6 for details. to"}, {"question": " How many random samples were ranked by participants in the subjective AB preference test?", "answer": " 9", "ref_chunk": "model and the base- lines, summarized in Table 1. GPU trials use one RTX A5000 GPU, and CPU trials use none. Like Wu et al. [13], we report inference time as the mean and standard deviation of five tri- 2https://github.com/kan-bayashi/ ParallelWaveGAN 3https://podcast.adobe.com/enhance Table 1: Average inference time and number of parameters for MRI-to-speech models. See Section 4 for details. Model CPU (s) \u2193 GPU (s) \u2193 Params. \u2193 CBL (Spe.) [14] CBL (Hub.) HiFi-CAR .66 \u00b1 .05 .69 \u00b1 .04 .58 \u00b1 .03 .081 \u00b1 .009 .090 \u00b1 .016 .061 \u00b1 .015 1.9 \u2217 107 2.3 \u2217 107 1.5 \u2217 107 Table 2: AB test results. See Section 5.1 for details. Baseline Type AB Test Votes Baseline Ours Same CBL (Spe.) [14] + Denoising 1 18 53 33 0 3 als, each calculating the average time to synthesize an utterance in our test. Our model is faster and uses less parameters than both intermediate-representation baselines, reinforcing the idea that directly mapping articulatory features to speech is more ef- ficient than relying on an intermediate representations. 5. Synthesis Quality 5.1. Subjective Fidelity Evaluation We perform a subjective AB preference test on Amazon Me- chanical Turk (MTurk). In this evaluation, each participant is asked to distinguish between the utterances generated by our method and the baselines in terms of naturalness. We compared our model with two baselines: (1) Yu et al. [14], detailed in Section 3.2, and (2) [14] trained with our denoised waveforms described in Section 3.4 as targets. For each of the two AB tests, we asked 6 native English listeners to rank a total of 9 random samples from the test set in this study. To prevent listeners from randomly submitting results, we added an audio pair consisting of one audio sample consisting entirely of noise and another au- dio sample containing high-fidelity speech. In Table 2, we sum- marize the total number of votes for each of the three options for both AB tests. Our model outperforms both baselines, receiv- ing the most votes. The almost unanimous vote for our model in the AB test with the non-denoised baseline highlights the im- portance of denoising waveforms accompanying MRI data. Our model also noticeably outperforms the denoised baseline, sug- gesting that direct synthesis approach described in Section 3.3 is well-suited for articulatory synthesis. 5.2. Objective Fidelity Evaluation We perform an objective evaluation of synthesis quality by ana- lyzing the mel-cepstral distortions (MCD) [35] between ground truths and synthesized samples, as in Wu et al. [13]. Table 3 summarizes these results, reporting the mean and standard deviation of the MCDs across utterances. Our HiFi-CAR ap- proach outperforms both intermediate-representation baselines, suggesting that our direct modelling method is suitable for the MRI-to-speech task. 5.3. Transcription We also compare our method with the baselines in terms of speech intelligibility. Specifically, we compute the character error rate (CER) for speech transcription. We use Whisper [36], Table 3: MCD between MRI-to-speech model outputs and de- noised ground truths. See Section 5.2 for details. Model MCD \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 7.31 \u00b1 0.45 8.84 \u00b1 1.00 6.64 \u00b1 0.64 Table 4: ASR CER for MRI-to-speech model outputs. See Sec- tion 5.3 for details. Model CER \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 84.7% \u00b1 36.4% 84.2% \u00b1 15.7% 69.2% \u00b1 28.1% a state-of-the-art automatic speech recognition (ASR) model, to generate text from the speech synthesized using each method and all test set utterances. Table 4 summarizes these results. Like in Table 3, our model outperforms both baselines, reinforc- ing the suitability of our model for MRI-to-speech synthesis. 6. Comparing MRI and EMA Features As mentioned in Section 1, MRI provides much more informa- tion about the vocal tract than EMA. MRI is a superset of EMA. Specifically, EMA has one x-y coordinate for each of the fol- lowing locations: upper lip, lower lip, lower incisor, tongue tip, tongue body, and tongue dorsum. Points at all of these locations are present in the MRI data, so we can actually approximate EMA features from MRI by choosing one MRI point at each In this figure, seg- EMA location, as visualized in Figure 2. ments are the connected MRI points and the shaped dots are the estimated EMA locations. We compare these two articulatory feature sets by comparing the outputs of our proposed MRI- to-speech model with those of this model trained to synthesize speech from our estimated 12-dimensional EMA features. The test set predictions of this EMA-to-speech model yielded an MCD of 6.986\u00b10.587 and ASR CER of 73.2%\u00b16.7%. Both of these values are worse than those of our MRI-to-speech model, summarized in Tables 3 and 4. This suggests that MRI features are more complete representations of the human vocal tract than EMA features. Thus, articulatory synthesis models should in- corporate features beyond EMA in order to achieve human-like fidelity across all utterances, with MRI features being a poten- tial feature set to extend towards. We identify which of the MRI features would be the most valuable to add to the articulatory feature set in Section 7. 7. Identifying Important MRI Features We also study which of the MRI features are the most useful for synthesis in order to provide insight into which features should be present in an ideal articulatory feature set for artic- ulatory synthesis. Specifically, we created 50 subsets of our 230-dimensional MRI feature set, each composed of a random 90% subset of the 230 features. With each feature subset, we masked the 23 MRI features not in the subset to 0.0 and syn- thesized the test set utterances. Then, we computed the average MCD between the test set ground truths and the synthesized waveforms. For each MRI feature, we assign it a score equal 100 arytenoid lower lip upper lip 25 lower teeth chin 150 tongue hard palate 225 50 75 epiglottis 175 125 velum 200 pharynx Figure 4: Importance of each MRI feature for MRI-to-speech synthesis. See Section 6 for details. to"}, {"question": " What method was used to analyze the mel-cepstral distortions between ground truths and synthesized samples?", "answer": " MCD (Mel-cepstral distortions)", "ref_chunk": "model and the base- lines, summarized in Table 1. GPU trials use one RTX A5000 GPU, and CPU trials use none. Like Wu et al. [13], we report inference time as the mean and standard deviation of five tri- 2https://github.com/kan-bayashi/ ParallelWaveGAN 3https://podcast.adobe.com/enhance Table 1: Average inference time and number of parameters for MRI-to-speech models. See Section 4 for details. Model CPU (s) \u2193 GPU (s) \u2193 Params. \u2193 CBL (Spe.) [14] CBL (Hub.) HiFi-CAR .66 \u00b1 .05 .69 \u00b1 .04 .58 \u00b1 .03 .081 \u00b1 .009 .090 \u00b1 .016 .061 \u00b1 .015 1.9 \u2217 107 2.3 \u2217 107 1.5 \u2217 107 Table 2: AB test results. See Section 5.1 for details. Baseline Type AB Test Votes Baseline Ours Same CBL (Spe.) [14] + Denoising 1 18 53 33 0 3 als, each calculating the average time to synthesize an utterance in our test. Our model is faster and uses less parameters than both intermediate-representation baselines, reinforcing the idea that directly mapping articulatory features to speech is more ef- ficient than relying on an intermediate representations. 5. Synthesis Quality 5.1. Subjective Fidelity Evaluation We perform a subjective AB preference test on Amazon Me- chanical Turk (MTurk). In this evaluation, each participant is asked to distinguish between the utterances generated by our method and the baselines in terms of naturalness. We compared our model with two baselines: (1) Yu et al. [14], detailed in Section 3.2, and (2) [14] trained with our denoised waveforms described in Section 3.4 as targets. For each of the two AB tests, we asked 6 native English listeners to rank a total of 9 random samples from the test set in this study. To prevent listeners from randomly submitting results, we added an audio pair consisting of one audio sample consisting entirely of noise and another au- dio sample containing high-fidelity speech. In Table 2, we sum- marize the total number of votes for each of the three options for both AB tests. Our model outperforms both baselines, receiv- ing the most votes. The almost unanimous vote for our model in the AB test with the non-denoised baseline highlights the im- portance of denoising waveforms accompanying MRI data. Our model also noticeably outperforms the denoised baseline, sug- gesting that direct synthesis approach described in Section 3.3 is well-suited for articulatory synthesis. 5.2. Objective Fidelity Evaluation We perform an objective evaluation of synthesis quality by ana- lyzing the mel-cepstral distortions (MCD) [35] between ground truths and synthesized samples, as in Wu et al. [13]. Table 3 summarizes these results, reporting the mean and standard deviation of the MCDs across utterances. Our HiFi-CAR ap- proach outperforms both intermediate-representation baselines, suggesting that our direct modelling method is suitable for the MRI-to-speech task. 5.3. Transcription We also compare our method with the baselines in terms of speech intelligibility. Specifically, we compute the character error rate (CER) for speech transcription. We use Whisper [36], Table 3: MCD between MRI-to-speech model outputs and de- noised ground truths. See Section 5.2 for details. Model MCD \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 7.31 \u00b1 0.45 8.84 \u00b1 1.00 6.64 \u00b1 0.64 Table 4: ASR CER for MRI-to-speech model outputs. See Sec- tion 5.3 for details. Model CER \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 84.7% \u00b1 36.4% 84.2% \u00b1 15.7% 69.2% \u00b1 28.1% a state-of-the-art automatic speech recognition (ASR) model, to generate text from the speech synthesized using each method and all test set utterances. Table 4 summarizes these results. Like in Table 3, our model outperforms both baselines, reinforc- ing the suitability of our model for MRI-to-speech synthesis. 6. Comparing MRI and EMA Features As mentioned in Section 1, MRI provides much more informa- tion about the vocal tract than EMA. MRI is a superset of EMA. Specifically, EMA has one x-y coordinate for each of the fol- lowing locations: upper lip, lower lip, lower incisor, tongue tip, tongue body, and tongue dorsum. Points at all of these locations are present in the MRI data, so we can actually approximate EMA features from MRI by choosing one MRI point at each In this figure, seg- EMA location, as visualized in Figure 2. ments are the connected MRI points and the shaped dots are the estimated EMA locations. We compare these two articulatory feature sets by comparing the outputs of our proposed MRI- to-speech model with those of this model trained to synthesize speech from our estimated 12-dimensional EMA features. The test set predictions of this EMA-to-speech model yielded an MCD of 6.986\u00b10.587 and ASR CER of 73.2%\u00b16.7%. Both of these values are worse than those of our MRI-to-speech model, summarized in Tables 3 and 4. This suggests that MRI features are more complete representations of the human vocal tract than EMA features. Thus, articulatory synthesis models should in- corporate features beyond EMA in order to achieve human-like fidelity across all utterances, with MRI features being a poten- tial feature set to extend towards. We identify which of the MRI features would be the most valuable to add to the articulatory feature set in Section 7. 7. Identifying Important MRI Features We also study which of the MRI features are the most useful for synthesis in order to provide insight into which features should be present in an ideal articulatory feature set for artic- ulatory synthesis. Specifically, we created 50 subsets of our 230-dimensional MRI feature set, each composed of a random 90% subset of the 230 features. With each feature subset, we masked the 23 MRI features not in the subset to 0.0 and syn- thesized the test set utterances. Then, we computed the average MCD between the test set ground truths and the synthesized waveforms. For each MRI feature, we assign it a score equal 100 arytenoid lower lip upper lip 25 lower teeth chin 150 tongue hard palate 225 50 75 epiglottis 175 125 velum 200 pharynx Figure 4: Importance of each MRI feature for MRI-to-speech synthesis. See Section 6 for details. to"}, {"question": " What evaluation metric was used to assess speech intelligibility?", "answer": " Character error rate (CER)", "ref_chunk": "model and the base- lines, summarized in Table 1. GPU trials use one RTX A5000 GPU, and CPU trials use none. Like Wu et al. [13], we report inference time as the mean and standard deviation of five tri- 2https://github.com/kan-bayashi/ ParallelWaveGAN 3https://podcast.adobe.com/enhance Table 1: Average inference time and number of parameters for MRI-to-speech models. See Section 4 for details. Model CPU (s) \u2193 GPU (s) \u2193 Params. \u2193 CBL (Spe.) [14] CBL (Hub.) HiFi-CAR .66 \u00b1 .05 .69 \u00b1 .04 .58 \u00b1 .03 .081 \u00b1 .009 .090 \u00b1 .016 .061 \u00b1 .015 1.9 \u2217 107 2.3 \u2217 107 1.5 \u2217 107 Table 2: AB test results. See Section 5.1 for details. Baseline Type AB Test Votes Baseline Ours Same CBL (Spe.) [14] + Denoising 1 18 53 33 0 3 als, each calculating the average time to synthesize an utterance in our test. Our model is faster and uses less parameters than both intermediate-representation baselines, reinforcing the idea that directly mapping articulatory features to speech is more ef- ficient than relying on an intermediate representations. 5. Synthesis Quality 5.1. Subjective Fidelity Evaluation We perform a subjective AB preference test on Amazon Me- chanical Turk (MTurk). In this evaluation, each participant is asked to distinguish between the utterances generated by our method and the baselines in terms of naturalness. We compared our model with two baselines: (1) Yu et al. [14], detailed in Section 3.2, and (2) [14] trained with our denoised waveforms described in Section 3.4 as targets. For each of the two AB tests, we asked 6 native English listeners to rank a total of 9 random samples from the test set in this study. To prevent listeners from randomly submitting results, we added an audio pair consisting of one audio sample consisting entirely of noise and another au- dio sample containing high-fidelity speech. In Table 2, we sum- marize the total number of votes for each of the three options for both AB tests. Our model outperforms both baselines, receiv- ing the most votes. The almost unanimous vote for our model in the AB test with the non-denoised baseline highlights the im- portance of denoising waveforms accompanying MRI data. Our model also noticeably outperforms the denoised baseline, sug- gesting that direct synthesis approach described in Section 3.3 is well-suited for articulatory synthesis. 5.2. Objective Fidelity Evaluation We perform an objective evaluation of synthesis quality by ana- lyzing the mel-cepstral distortions (MCD) [35] between ground truths and synthesized samples, as in Wu et al. [13]. Table 3 summarizes these results, reporting the mean and standard deviation of the MCDs across utterances. Our HiFi-CAR ap- proach outperforms both intermediate-representation baselines, suggesting that our direct modelling method is suitable for the MRI-to-speech task. 5.3. Transcription We also compare our method with the baselines in terms of speech intelligibility. Specifically, we compute the character error rate (CER) for speech transcription. We use Whisper [36], Table 3: MCD between MRI-to-speech model outputs and de- noised ground truths. See Section 5.2 for details. Model MCD \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 7.31 \u00b1 0.45 8.84 \u00b1 1.00 6.64 \u00b1 0.64 Table 4: ASR CER for MRI-to-speech model outputs. See Sec- tion 5.3 for details. Model CER \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 84.7% \u00b1 36.4% 84.2% \u00b1 15.7% 69.2% \u00b1 28.1% a state-of-the-art automatic speech recognition (ASR) model, to generate text from the speech synthesized using each method and all test set utterances. Table 4 summarizes these results. Like in Table 3, our model outperforms both baselines, reinforc- ing the suitability of our model for MRI-to-speech synthesis. 6. Comparing MRI and EMA Features As mentioned in Section 1, MRI provides much more informa- tion about the vocal tract than EMA. MRI is a superset of EMA. Specifically, EMA has one x-y coordinate for each of the fol- lowing locations: upper lip, lower lip, lower incisor, tongue tip, tongue body, and tongue dorsum. Points at all of these locations are present in the MRI data, so we can actually approximate EMA features from MRI by choosing one MRI point at each In this figure, seg- EMA location, as visualized in Figure 2. ments are the connected MRI points and the shaped dots are the estimated EMA locations. We compare these two articulatory feature sets by comparing the outputs of our proposed MRI- to-speech model with those of this model trained to synthesize speech from our estimated 12-dimensional EMA features. The test set predictions of this EMA-to-speech model yielded an MCD of 6.986\u00b10.587 and ASR CER of 73.2%\u00b16.7%. Both of these values are worse than those of our MRI-to-speech model, summarized in Tables 3 and 4. This suggests that MRI features are more complete representations of the human vocal tract than EMA features. Thus, articulatory synthesis models should in- corporate features beyond EMA in order to achieve human-like fidelity across all utterances, with MRI features being a poten- tial feature set to extend towards. We identify which of the MRI features would be the most valuable to add to the articulatory feature set in Section 7. 7. Identifying Important MRI Features We also study which of the MRI features are the most useful for synthesis in order to provide insight into which features should be present in an ideal articulatory feature set for artic- ulatory synthesis. Specifically, we created 50 subsets of our 230-dimensional MRI feature set, each composed of a random 90% subset of the 230 features. With each feature subset, we masked the 23 MRI features not in the subset to 0.0 and syn- thesized the test set utterances. Then, we computed the average MCD between the test set ground truths and the synthesized waveforms. For each MRI feature, we assign it a score equal 100 arytenoid lower lip upper lip 25 lower teeth chin 150 tongue hard palate 225 50 75 epiglottis 175 125 velum 200 pharynx Figure 4: Importance of each MRI feature for MRI-to-speech synthesis. See Section 6 for details. to"}, {"question": " What is the CER for the HiFi-CAR model?", "answer": " 69.2% \u00b1 28.1%", "ref_chunk": "model and the base- lines, summarized in Table 1. GPU trials use one RTX A5000 GPU, and CPU trials use none. Like Wu et al. [13], we report inference time as the mean and standard deviation of five tri- 2https://github.com/kan-bayashi/ ParallelWaveGAN 3https://podcast.adobe.com/enhance Table 1: Average inference time and number of parameters for MRI-to-speech models. See Section 4 for details. Model CPU (s) \u2193 GPU (s) \u2193 Params. \u2193 CBL (Spe.) [14] CBL (Hub.) HiFi-CAR .66 \u00b1 .05 .69 \u00b1 .04 .58 \u00b1 .03 .081 \u00b1 .009 .090 \u00b1 .016 .061 \u00b1 .015 1.9 \u2217 107 2.3 \u2217 107 1.5 \u2217 107 Table 2: AB test results. See Section 5.1 for details. Baseline Type AB Test Votes Baseline Ours Same CBL (Spe.) [14] + Denoising 1 18 53 33 0 3 als, each calculating the average time to synthesize an utterance in our test. Our model is faster and uses less parameters than both intermediate-representation baselines, reinforcing the idea that directly mapping articulatory features to speech is more ef- ficient than relying on an intermediate representations. 5. Synthesis Quality 5.1. Subjective Fidelity Evaluation We perform a subjective AB preference test on Amazon Me- chanical Turk (MTurk). In this evaluation, each participant is asked to distinguish between the utterances generated by our method and the baselines in terms of naturalness. We compared our model with two baselines: (1) Yu et al. [14], detailed in Section 3.2, and (2) [14] trained with our denoised waveforms described in Section 3.4 as targets. For each of the two AB tests, we asked 6 native English listeners to rank a total of 9 random samples from the test set in this study. To prevent listeners from randomly submitting results, we added an audio pair consisting of one audio sample consisting entirely of noise and another au- dio sample containing high-fidelity speech. In Table 2, we sum- marize the total number of votes for each of the three options for both AB tests. Our model outperforms both baselines, receiv- ing the most votes. The almost unanimous vote for our model in the AB test with the non-denoised baseline highlights the im- portance of denoising waveforms accompanying MRI data. Our model also noticeably outperforms the denoised baseline, sug- gesting that direct synthesis approach described in Section 3.3 is well-suited for articulatory synthesis. 5.2. Objective Fidelity Evaluation We perform an objective evaluation of synthesis quality by ana- lyzing the mel-cepstral distortions (MCD) [35] between ground truths and synthesized samples, as in Wu et al. [13]. Table 3 summarizes these results, reporting the mean and standard deviation of the MCDs across utterances. Our HiFi-CAR ap- proach outperforms both intermediate-representation baselines, suggesting that our direct modelling method is suitable for the MRI-to-speech task. 5.3. Transcription We also compare our method with the baselines in terms of speech intelligibility. Specifically, we compute the character error rate (CER) for speech transcription. We use Whisper [36], Table 3: MCD between MRI-to-speech model outputs and de- noised ground truths. See Section 5.2 for details. Model MCD \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 7.31 \u00b1 0.45 8.84 \u00b1 1.00 6.64 \u00b1 0.64 Table 4: ASR CER for MRI-to-speech model outputs. See Sec- tion 5.3 for details. Model CER \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 84.7% \u00b1 36.4% 84.2% \u00b1 15.7% 69.2% \u00b1 28.1% a state-of-the-art automatic speech recognition (ASR) model, to generate text from the speech synthesized using each method and all test set utterances. Table 4 summarizes these results. Like in Table 3, our model outperforms both baselines, reinforc- ing the suitability of our model for MRI-to-speech synthesis. 6. Comparing MRI and EMA Features As mentioned in Section 1, MRI provides much more informa- tion about the vocal tract than EMA. MRI is a superset of EMA. Specifically, EMA has one x-y coordinate for each of the fol- lowing locations: upper lip, lower lip, lower incisor, tongue tip, tongue body, and tongue dorsum. Points at all of these locations are present in the MRI data, so we can actually approximate EMA features from MRI by choosing one MRI point at each In this figure, seg- EMA location, as visualized in Figure 2. ments are the connected MRI points and the shaped dots are the estimated EMA locations. We compare these two articulatory feature sets by comparing the outputs of our proposed MRI- to-speech model with those of this model trained to synthesize speech from our estimated 12-dimensional EMA features. The test set predictions of this EMA-to-speech model yielded an MCD of 6.986\u00b10.587 and ASR CER of 73.2%\u00b16.7%. Both of these values are worse than those of our MRI-to-speech model, summarized in Tables 3 and 4. This suggests that MRI features are more complete representations of the human vocal tract than EMA features. Thus, articulatory synthesis models should in- corporate features beyond EMA in order to achieve human-like fidelity across all utterances, with MRI features being a poten- tial feature set to extend towards. We identify which of the MRI features would be the most valuable to add to the articulatory feature set in Section 7. 7. Identifying Important MRI Features We also study which of the MRI features are the most useful for synthesis in order to provide insight into which features should be present in an ideal articulatory feature set for artic- ulatory synthesis. Specifically, we created 50 subsets of our 230-dimensional MRI feature set, each composed of a random 90% subset of the 230 features. With each feature subset, we masked the 23 MRI features not in the subset to 0.0 and syn- thesized the test set utterances. Then, we computed the average MCD between the test set ground truths and the synthesized waveforms. For each MRI feature, we assign it a score equal 100 arytenoid lower lip upper lip 25 lower teeth chin 150 tongue hard palate 225 50 75 epiglottis 175 125 velum 200 pharynx Figure 4: Importance of each MRI feature for MRI-to-speech synthesis. See Section 6 for details. to"}, {"question": " Why is incorporating MRI features suggested for articulatory synthesis models?", "answer": " To achieve human-like fidelity across all utterances", "ref_chunk": "model and the base- lines, summarized in Table 1. GPU trials use one RTX A5000 GPU, and CPU trials use none. Like Wu et al. [13], we report inference time as the mean and standard deviation of five tri- 2https://github.com/kan-bayashi/ ParallelWaveGAN 3https://podcast.adobe.com/enhance Table 1: Average inference time and number of parameters for MRI-to-speech models. See Section 4 for details. Model CPU (s) \u2193 GPU (s) \u2193 Params. \u2193 CBL (Spe.) [14] CBL (Hub.) HiFi-CAR .66 \u00b1 .05 .69 \u00b1 .04 .58 \u00b1 .03 .081 \u00b1 .009 .090 \u00b1 .016 .061 \u00b1 .015 1.9 \u2217 107 2.3 \u2217 107 1.5 \u2217 107 Table 2: AB test results. See Section 5.1 for details. Baseline Type AB Test Votes Baseline Ours Same CBL (Spe.) [14] + Denoising 1 18 53 33 0 3 als, each calculating the average time to synthesize an utterance in our test. Our model is faster and uses less parameters than both intermediate-representation baselines, reinforcing the idea that directly mapping articulatory features to speech is more ef- ficient than relying on an intermediate representations. 5. Synthesis Quality 5.1. Subjective Fidelity Evaluation We perform a subjective AB preference test on Amazon Me- chanical Turk (MTurk). In this evaluation, each participant is asked to distinguish between the utterances generated by our method and the baselines in terms of naturalness. We compared our model with two baselines: (1) Yu et al. [14], detailed in Section 3.2, and (2) [14] trained with our denoised waveforms described in Section 3.4 as targets. For each of the two AB tests, we asked 6 native English listeners to rank a total of 9 random samples from the test set in this study. To prevent listeners from randomly submitting results, we added an audio pair consisting of one audio sample consisting entirely of noise and another au- dio sample containing high-fidelity speech. In Table 2, we sum- marize the total number of votes for each of the three options for both AB tests. Our model outperforms both baselines, receiv- ing the most votes. The almost unanimous vote for our model in the AB test with the non-denoised baseline highlights the im- portance of denoising waveforms accompanying MRI data. Our model also noticeably outperforms the denoised baseline, sug- gesting that direct synthesis approach described in Section 3.3 is well-suited for articulatory synthesis. 5.2. Objective Fidelity Evaluation We perform an objective evaluation of synthesis quality by ana- lyzing the mel-cepstral distortions (MCD) [35] between ground truths and synthesized samples, as in Wu et al. [13]. Table 3 summarizes these results, reporting the mean and standard deviation of the MCDs across utterances. Our HiFi-CAR ap- proach outperforms both intermediate-representation baselines, suggesting that our direct modelling method is suitable for the MRI-to-speech task. 5.3. Transcription We also compare our method with the baselines in terms of speech intelligibility. Specifically, we compute the character error rate (CER) for speech transcription. We use Whisper [36], Table 3: MCD between MRI-to-speech model outputs and de- noised ground truths. See Section 5.2 for details. Model MCD \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 7.31 \u00b1 0.45 8.84 \u00b1 1.00 6.64 \u00b1 0.64 Table 4: ASR CER for MRI-to-speech model outputs. See Sec- tion 5.3 for details. Model CER \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 84.7% \u00b1 36.4% 84.2% \u00b1 15.7% 69.2% \u00b1 28.1% a state-of-the-art automatic speech recognition (ASR) model, to generate text from the speech synthesized using each method and all test set utterances. Table 4 summarizes these results. Like in Table 3, our model outperforms both baselines, reinforc- ing the suitability of our model for MRI-to-speech synthesis. 6. Comparing MRI and EMA Features As mentioned in Section 1, MRI provides much more informa- tion about the vocal tract than EMA. MRI is a superset of EMA. Specifically, EMA has one x-y coordinate for each of the fol- lowing locations: upper lip, lower lip, lower incisor, tongue tip, tongue body, and tongue dorsum. Points at all of these locations are present in the MRI data, so we can actually approximate EMA features from MRI by choosing one MRI point at each In this figure, seg- EMA location, as visualized in Figure 2. ments are the connected MRI points and the shaped dots are the estimated EMA locations. We compare these two articulatory feature sets by comparing the outputs of our proposed MRI- to-speech model with those of this model trained to synthesize speech from our estimated 12-dimensional EMA features. The test set predictions of this EMA-to-speech model yielded an MCD of 6.986\u00b10.587 and ASR CER of 73.2%\u00b16.7%. Both of these values are worse than those of our MRI-to-speech model, summarized in Tables 3 and 4. This suggests that MRI features are more complete representations of the human vocal tract than EMA features. Thus, articulatory synthesis models should in- corporate features beyond EMA in order to achieve human-like fidelity across all utterances, with MRI features being a poten- tial feature set to extend towards. We identify which of the MRI features would be the most valuable to add to the articulatory feature set in Section 7. 7. Identifying Important MRI Features We also study which of the MRI features are the most useful for synthesis in order to provide insight into which features should be present in an ideal articulatory feature set for artic- ulatory synthesis. Specifically, we created 50 subsets of our 230-dimensional MRI feature set, each composed of a random 90% subset of the 230 features. With each feature subset, we masked the 23 MRI features not in the subset to 0.0 and syn- thesized the test set utterances. Then, we computed the average MCD between the test set ground truths and the synthesized waveforms. For each MRI feature, we assign it a score equal 100 arytenoid lower lip upper lip 25 lower teeth chin 150 tongue hard palate 225 50 75 epiglottis 175 125 velum 200 pharynx Figure 4: Importance of each MRI feature for MRI-to-speech synthesis. See Section 6 for details. to"}, {"question": " What was the average MCD between ground truths and synthesized waveforms for each MRI feature subset?", "answer": " Computed and assigned a score", "ref_chunk": "model and the base- lines, summarized in Table 1. GPU trials use one RTX A5000 GPU, and CPU trials use none. Like Wu et al. [13], we report inference time as the mean and standard deviation of five tri- 2https://github.com/kan-bayashi/ ParallelWaveGAN 3https://podcast.adobe.com/enhance Table 1: Average inference time and number of parameters for MRI-to-speech models. See Section 4 for details. Model CPU (s) \u2193 GPU (s) \u2193 Params. \u2193 CBL (Spe.) [14] CBL (Hub.) HiFi-CAR .66 \u00b1 .05 .69 \u00b1 .04 .58 \u00b1 .03 .081 \u00b1 .009 .090 \u00b1 .016 .061 \u00b1 .015 1.9 \u2217 107 2.3 \u2217 107 1.5 \u2217 107 Table 2: AB test results. See Section 5.1 for details. Baseline Type AB Test Votes Baseline Ours Same CBL (Spe.) [14] + Denoising 1 18 53 33 0 3 als, each calculating the average time to synthesize an utterance in our test. Our model is faster and uses less parameters than both intermediate-representation baselines, reinforcing the idea that directly mapping articulatory features to speech is more ef- ficient than relying on an intermediate representations. 5. Synthesis Quality 5.1. Subjective Fidelity Evaluation We perform a subjective AB preference test on Amazon Me- chanical Turk (MTurk). In this evaluation, each participant is asked to distinguish between the utterances generated by our method and the baselines in terms of naturalness. We compared our model with two baselines: (1) Yu et al. [14], detailed in Section 3.2, and (2) [14] trained with our denoised waveforms described in Section 3.4 as targets. For each of the two AB tests, we asked 6 native English listeners to rank a total of 9 random samples from the test set in this study. To prevent listeners from randomly submitting results, we added an audio pair consisting of one audio sample consisting entirely of noise and another au- dio sample containing high-fidelity speech. In Table 2, we sum- marize the total number of votes for each of the three options for both AB tests. Our model outperforms both baselines, receiv- ing the most votes. The almost unanimous vote for our model in the AB test with the non-denoised baseline highlights the im- portance of denoising waveforms accompanying MRI data. Our model also noticeably outperforms the denoised baseline, sug- gesting that direct synthesis approach described in Section 3.3 is well-suited for articulatory synthesis. 5.2. Objective Fidelity Evaluation We perform an objective evaluation of synthesis quality by ana- lyzing the mel-cepstral distortions (MCD) [35] between ground truths and synthesized samples, as in Wu et al. [13]. Table 3 summarizes these results, reporting the mean and standard deviation of the MCDs across utterances. Our HiFi-CAR ap- proach outperforms both intermediate-representation baselines, suggesting that our direct modelling method is suitable for the MRI-to-speech task. 5.3. Transcription We also compare our method with the baselines in terms of speech intelligibility. Specifically, we compute the character error rate (CER) for speech transcription. We use Whisper [36], Table 3: MCD between MRI-to-speech model outputs and de- noised ground truths. See Section 5.2 for details. Model MCD \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 7.31 \u00b1 0.45 8.84 \u00b1 1.00 6.64 \u00b1 0.64 Table 4: ASR CER for MRI-to-speech model outputs. See Sec- tion 5.3 for details. Model CER \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 84.7% \u00b1 36.4% 84.2% \u00b1 15.7% 69.2% \u00b1 28.1% a state-of-the-art automatic speech recognition (ASR) model, to generate text from the speech synthesized using each method and all test set utterances. Table 4 summarizes these results. Like in Table 3, our model outperforms both baselines, reinforc- ing the suitability of our model for MRI-to-speech synthesis. 6. Comparing MRI and EMA Features As mentioned in Section 1, MRI provides much more informa- tion about the vocal tract than EMA. MRI is a superset of EMA. Specifically, EMA has one x-y coordinate for each of the fol- lowing locations: upper lip, lower lip, lower incisor, tongue tip, tongue body, and tongue dorsum. Points at all of these locations are present in the MRI data, so we can actually approximate EMA features from MRI by choosing one MRI point at each In this figure, seg- EMA location, as visualized in Figure 2. ments are the connected MRI points and the shaped dots are the estimated EMA locations. We compare these two articulatory feature sets by comparing the outputs of our proposed MRI- to-speech model with those of this model trained to synthesize speech from our estimated 12-dimensional EMA features. The test set predictions of this EMA-to-speech model yielded an MCD of 6.986\u00b10.587 and ASR CER of 73.2%\u00b16.7%. Both of these values are worse than those of our MRI-to-speech model, summarized in Tables 3 and 4. This suggests that MRI features are more complete representations of the human vocal tract than EMA features. Thus, articulatory synthesis models should in- corporate features beyond EMA in order to achieve human-like fidelity across all utterances, with MRI features being a poten- tial feature set to extend towards. We identify which of the MRI features would be the most valuable to add to the articulatory feature set in Section 7. 7. Identifying Important MRI Features We also study which of the MRI features are the most useful for synthesis in order to provide insight into which features should be present in an ideal articulatory feature set for artic- ulatory synthesis. Specifically, we created 50 subsets of our 230-dimensional MRI feature set, each composed of a random 90% subset of the 230 features. With each feature subset, we masked the 23 MRI features not in the subset to 0.0 and syn- thesized the test set utterances. Then, we computed the average MCD between the test set ground truths and the synthesized waveforms. For each MRI feature, we assign it a score equal 100 arytenoid lower lip upper lip 25 lower teeth chin 150 tongue hard palate 225 50 75 epiglottis 175 125 velum 200 pharynx Figure 4: Importance of each MRI feature for MRI-to-speech synthesis. See Section 6 for details. to"}, {"question": " What insight do the studies on MRI features provide for articulatory synthesis?", "answer": " Which features should be present in an ideal articulatory feature set", "ref_chunk": "model and the base- lines, summarized in Table 1. GPU trials use one RTX A5000 GPU, and CPU trials use none. Like Wu et al. [13], we report inference time as the mean and standard deviation of five tri- 2https://github.com/kan-bayashi/ ParallelWaveGAN 3https://podcast.adobe.com/enhance Table 1: Average inference time and number of parameters for MRI-to-speech models. See Section 4 for details. Model CPU (s) \u2193 GPU (s) \u2193 Params. \u2193 CBL (Spe.) [14] CBL (Hub.) HiFi-CAR .66 \u00b1 .05 .69 \u00b1 .04 .58 \u00b1 .03 .081 \u00b1 .009 .090 \u00b1 .016 .061 \u00b1 .015 1.9 \u2217 107 2.3 \u2217 107 1.5 \u2217 107 Table 2: AB test results. See Section 5.1 for details. Baseline Type AB Test Votes Baseline Ours Same CBL (Spe.) [14] + Denoising 1 18 53 33 0 3 als, each calculating the average time to synthesize an utterance in our test. Our model is faster and uses less parameters than both intermediate-representation baselines, reinforcing the idea that directly mapping articulatory features to speech is more ef- ficient than relying on an intermediate representations. 5. Synthesis Quality 5.1. Subjective Fidelity Evaluation We perform a subjective AB preference test on Amazon Me- chanical Turk (MTurk). In this evaluation, each participant is asked to distinguish between the utterances generated by our method and the baselines in terms of naturalness. We compared our model with two baselines: (1) Yu et al. [14], detailed in Section 3.2, and (2) [14] trained with our denoised waveforms described in Section 3.4 as targets. For each of the two AB tests, we asked 6 native English listeners to rank a total of 9 random samples from the test set in this study. To prevent listeners from randomly submitting results, we added an audio pair consisting of one audio sample consisting entirely of noise and another au- dio sample containing high-fidelity speech. In Table 2, we sum- marize the total number of votes for each of the three options for both AB tests. Our model outperforms both baselines, receiv- ing the most votes. The almost unanimous vote for our model in the AB test with the non-denoised baseline highlights the im- portance of denoising waveforms accompanying MRI data. Our model also noticeably outperforms the denoised baseline, sug- gesting that direct synthesis approach described in Section 3.3 is well-suited for articulatory synthesis. 5.2. Objective Fidelity Evaluation We perform an objective evaluation of synthesis quality by ana- lyzing the mel-cepstral distortions (MCD) [35] between ground truths and synthesized samples, as in Wu et al. [13]. Table 3 summarizes these results, reporting the mean and standard deviation of the MCDs across utterances. Our HiFi-CAR ap- proach outperforms both intermediate-representation baselines, suggesting that our direct modelling method is suitable for the MRI-to-speech task. 5.3. Transcription We also compare our method with the baselines in terms of speech intelligibility. Specifically, we compute the character error rate (CER) for speech transcription. We use Whisper [36], Table 3: MCD between MRI-to-speech model outputs and de- noised ground truths. See Section 5.2 for details. Model MCD \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 7.31 \u00b1 0.45 8.84 \u00b1 1.00 6.64 \u00b1 0.64 Table 4: ASR CER for MRI-to-speech model outputs. See Sec- tion 5.3 for details. Model CER \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 84.7% \u00b1 36.4% 84.2% \u00b1 15.7% 69.2% \u00b1 28.1% a state-of-the-art automatic speech recognition (ASR) model, to generate text from the speech synthesized using each method and all test set utterances. Table 4 summarizes these results. Like in Table 3, our model outperforms both baselines, reinforc- ing the suitability of our model for MRI-to-speech synthesis. 6. Comparing MRI and EMA Features As mentioned in Section 1, MRI provides much more informa- tion about the vocal tract than EMA. MRI is a superset of EMA. Specifically, EMA has one x-y coordinate for each of the fol- lowing locations: upper lip, lower lip, lower incisor, tongue tip, tongue body, and tongue dorsum. Points at all of these locations are present in the MRI data, so we can actually approximate EMA features from MRI by choosing one MRI point at each In this figure, seg- EMA location, as visualized in Figure 2. ments are the connected MRI points and the shaped dots are the estimated EMA locations. We compare these two articulatory feature sets by comparing the outputs of our proposed MRI- to-speech model with those of this model trained to synthesize speech from our estimated 12-dimensional EMA features. The test set predictions of this EMA-to-speech model yielded an MCD of 6.986\u00b10.587 and ASR CER of 73.2%\u00b16.7%. Both of these values are worse than those of our MRI-to-speech model, summarized in Tables 3 and 4. This suggests that MRI features are more complete representations of the human vocal tract than EMA features. Thus, articulatory synthesis models should in- corporate features beyond EMA in order to achieve human-like fidelity across all utterances, with MRI features being a poten- tial feature set to extend towards. We identify which of the MRI features would be the most valuable to add to the articulatory feature set in Section 7. 7. Identifying Important MRI Features We also study which of the MRI features are the most useful for synthesis in order to provide insight into which features should be present in an ideal articulatory feature set for artic- ulatory synthesis. Specifically, we created 50 subsets of our 230-dimensional MRI feature set, each composed of a random 90% subset of the 230 features. With each feature subset, we masked the 23 MRI features not in the subset to 0.0 and syn- thesized the test set utterances. Then, we computed the average MCD between the test set ground truths and the synthesized waveforms. For each MRI feature, we assign it a score equal 100 arytenoid lower lip upper lip 25 lower teeth chin 150 tongue hard palate 225 50 75 epiglottis 175 125 velum 200 pharynx Figure 4: Importance of each MRI feature for MRI-to-speech synthesis. See Section 6 for details. to"}], "doc_text": "model and the base- lines, summarized in Table 1. GPU trials use one RTX A5000 GPU, and CPU trials use none. Like Wu et al. [13], we report inference time as the mean and standard deviation of five tri- 2https://github.com/kan-bayashi/ ParallelWaveGAN 3https://podcast.adobe.com/enhance Table 1: Average inference time and number of parameters for MRI-to-speech models. See Section 4 for details. Model CPU (s) \u2193 GPU (s) \u2193 Params. \u2193 CBL (Spe.) [14] CBL (Hub.) HiFi-CAR .66 \u00b1 .05 .69 \u00b1 .04 .58 \u00b1 .03 .081 \u00b1 .009 .090 \u00b1 .016 .061 \u00b1 .015 1.9 \u2217 107 2.3 \u2217 107 1.5 \u2217 107 Table 2: AB test results. See Section 5.1 for details. Baseline Type AB Test Votes Baseline Ours Same CBL (Spe.) [14] + Denoising 1 18 53 33 0 3 als, each calculating the average time to synthesize an utterance in our test. Our model is faster and uses less parameters than both intermediate-representation baselines, reinforcing the idea that directly mapping articulatory features to speech is more ef- ficient than relying on an intermediate representations. 5. Synthesis Quality 5.1. Subjective Fidelity Evaluation We perform a subjective AB preference test on Amazon Me- chanical Turk (MTurk). In this evaluation, each participant is asked to distinguish between the utterances generated by our method and the baselines in terms of naturalness. We compared our model with two baselines: (1) Yu et al. [14], detailed in Section 3.2, and (2) [14] trained with our denoised waveforms described in Section 3.4 as targets. For each of the two AB tests, we asked 6 native English listeners to rank a total of 9 random samples from the test set in this study. To prevent listeners from randomly submitting results, we added an audio pair consisting of one audio sample consisting entirely of noise and another au- dio sample containing high-fidelity speech. In Table 2, we sum- marize the total number of votes for each of the three options for both AB tests. Our model outperforms both baselines, receiv- ing the most votes. The almost unanimous vote for our model in the AB test with the non-denoised baseline highlights the im- portance of denoising waveforms accompanying MRI data. Our model also noticeably outperforms the denoised baseline, sug- gesting that direct synthesis approach described in Section 3.3 is well-suited for articulatory synthesis. 5.2. Objective Fidelity Evaluation We perform an objective evaluation of synthesis quality by ana- lyzing the mel-cepstral distortions (MCD) [35] between ground truths and synthesized samples, as in Wu et al. [13]. Table 3 summarizes these results, reporting the mean and standard deviation of the MCDs across utterances. Our HiFi-CAR ap- proach outperforms both intermediate-representation baselines, suggesting that our direct modelling method is suitable for the MRI-to-speech task. 5.3. Transcription We also compare our method with the baselines in terms of speech intelligibility. Specifically, we compute the character error rate (CER) for speech transcription. We use Whisper [36], Table 3: MCD between MRI-to-speech model outputs and de- noised ground truths. See Section 5.2 for details. Model MCD \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 7.31 \u00b1 0.45 8.84 \u00b1 1.00 6.64 \u00b1 0.64 Table 4: ASR CER for MRI-to-speech model outputs. See Sec- tion 5.3 for details. Model CER \u2193 CBL (Spe.) [14] + Denoising CBL (Hub.) + Denoising HiFi-CAR 84.7% \u00b1 36.4% 84.2% \u00b1 15.7% 69.2% \u00b1 28.1% a state-of-the-art automatic speech recognition (ASR) model, to generate text from the speech synthesized using each method and all test set utterances. Table 4 summarizes these results. Like in Table 3, our model outperforms both baselines, reinforc- ing the suitability of our model for MRI-to-speech synthesis. 6. Comparing MRI and EMA Features As mentioned in Section 1, MRI provides much more informa- tion about the vocal tract than EMA. MRI is a superset of EMA. Specifically, EMA has one x-y coordinate for each of the fol- lowing locations: upper lip, lower lip, lower incisor, tongue tip, tongue body, and tongue dorsum. Points at all of these locations are present in the MRI data, so we can actually approximate EMA features from MRI by choosing one MRI point at each In this figure, seg- EMA location, as visualized in Figure 2. ments are the connected MRI points and the shaped dots are the estimated EMA locations. We compare these two articulatory feature sets by comparing the outputs of our proposed MRI- to-speech model with those of this model trained to synthesize speech from our estimated 12-dimensional EMA features. The test set predictions of this EMA-to-speech model yielded an MCD of 6.986\u00b10.587 and ASR CER of 73.2%\u00b16.7%. Both of these values are worse than those of our MRI-to-speech model, summarized in Tables 3 and 4. This suggests that MRI features are more complete representations of the human vocal tract than EMA features. Thus, articulatory synthesis models should in- corporate features beyond EMA in order to achieve human-like fidelity across all utterances, with MRI features being a poten- tial feature set to extend towards. We identify which of the MRI features would be the most valuable to add to the articulatory feature set in Section 7. 7. Identifying Important MRI Features We also study which of the MRI features are the most useful for synthesis in order to provide insight into which features should be present in an ideal articulatory feature set for artic- ulatory synthesis. Specifically, we created 50 subsets of our 230-dimensional MRI feature set, each composed of a random 90% subset of the 230 features. With each feature subset, we masked the 23 MRI features not in the subset to 0.0 and syn- thesized the test set utterances. Then, we computed the average MCD between the test set ground truths and the synthesized waveforms. For each MRI feature, we assign it a score equal 100 arytenoid lower lip upper lip 25 lower teeth chin 150 tongue hard palate 225 50 75 epiglottis 175 125 velum 200 pharynx Figure 4: Importance of each MRI feature for MRI-to-speech synthesis. See Section 6 for details. to"}