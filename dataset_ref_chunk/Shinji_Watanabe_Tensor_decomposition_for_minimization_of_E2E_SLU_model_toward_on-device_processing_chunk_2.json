{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Tensor_decomposition_for_minimization_of_E2E_SLU_model_toward_on-device_processing_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the CP decomposition and how is it different from the Tucker decomposition?", "answer": " The CP decomposition is a special case of the Tucker decomposition where the core tensor has only diagonal component values and the same rank for each dimension, allowing for more parameter reduction but being less expressive.", "ref_chunk": "decomposition [26] decomposes tensor W \u2208 RI\u00d7J\u00d7K into a weight vector \u03bb \u2208 RR and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7R and U 3 \u2208 RK\u00d7R as: W = (cid:88) \u03bbr \u00d7 U 1 r \u2297 U 2 r \u2297 U 3 r . r The parameter compression ratio \u03b3cp can be described as: \u03b3cp = R(1 + I + J + K) IJK . The CP decomposition is the special case where the core tensor of the Tucker decomposition has only diagonal component val- ues and the same rank for each dimension in eq. (3). Therefore, it allows for more parameter reduction but is less expressive than the Tucker decomposition. Furthermore, the rank of the CP decomposition is restricted to the smallest dimension. Since the kernel size of the convolution layer is often small compared to the number of channels, the rank is often limited to the ker- nel size. Therefore, CP decomposition is difficult to adjust with flexible parameter sizes. 2.4. Tensor-Train decomposition Tensor-Train [27] is another decomposition method for high- order tensor. The original tensor W \u2208 RI\u00d7J\u00d7K is decom- 1,i,r \u2208 R1\u00d7I\u00d7R, posed into a product of 3-order tensors G1 s,k,1 \u2208 RS\u00d7K\u00d71 by the Tensor-Train G2 decomposition as: r,j,s \u2208 RR\u00d7J\u00d7S and G3 Wi,j,k = (cid:88) G1 1,i,r \u00d7 G2 r,j,s \u00d7 G3 s,k,1. r,s (3) (4) (5) (6) Figure 1: Training procedure. The parameter compression ratio \u03b3tt can be described as: \u03b3tt = IR + RJS + SK IJK . Tensor-Train decomposition provides flexible parameter reduc- tion, as it involves N \u2212 1 hyperparameters corresponding to the rank of N -th order tensors. However, unlike Tucker decompo- sition, it cannot accelerate inference because the original tensor must be reconstructed during inference. 3. Decomposed E2E SLU model 3.1. Training overview We investigate the tensor decomposition techniques in E2E SLU models with higher-order tensors as parameters. E2E SLU is a task that estimates semantic label sequences Y directly from input speech feature sequences X. Fig.1 describes the overview of the training procedure of our model. We use two- step sequential distillation to train a small model. Recent SOTA models use self-supervised learning techniques, which have a large number of parameters (over 300M) [28]. However, on- device processing requires a smaller model, typically around 30M or less. Therefore, We do not apply tensor decomposi- tion directly to the large model because the teacher model is too large to minimize within small parameters with sufficient rank. We train small model through a middle-size model to achieve the target size with a compression ratio near 0.3. As shown in the blue boxes in Fig.1, the training procedure has 5 steps. 1. A large teacher model is trained with ground truth labels. We use the E2E Conformer model with Hubert-based feature extractor1 [28], which has achieved the best performance in prior study [29]. 2. The training data is decoded using the trained teacher model to generate teacher labels. 3. The middle model is trained with both the distilled and ground truth labels. The middle model is used as the seed model to miniaturize the final small model using the decom- position, so its structural type is the same as the final small model. 4. Tensor decomposition and SVD techniques are applied to the middle model to reduce the parameters. We set the target size to 15M or 30M as in a previous study [7]. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 1https://dl.fbaipublicfiles.com/hubert/hubert large ll60k.pt (8) Table 1: Experimental results on STOP dataset within 15M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small Small 0.300 / 0.295 0.250 / 0.300 0.250 / 0.300 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 200 < 15M 431M 14M 47M 15M 15M 53M 15M 15M \u2014\u2013 / 67.9 68.8 / 69.4 34.7 / 34.8 62.6 / 62.5 65.0 / 65.4 60.9 / 61.4 68.6 / 69.0 69.6 / 70.1 70.4 / 70.9 Table 2: Experimental results on STOP dataset within 30M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small 0.650 / 0.650 0.550 / 0.600 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 < 30M \u2014\u2013 / 73.87 68.8 / 69.4 45.3 / 45.7 62.6 / 62.5 65.4 / 65.7 66.4 / 66.8 68.6 / 69.0 71.0 / 71.4 431M 29M 47M 30M 30M 53M 30M The middle model and small model have Conformer or E- Branchformer encoders. Conformer and E-Branchformer in- clude convolution layers that have 3rd or 4th-order tensor pa- rameters. Thus, they can be efficiently compressed by tensor decomposition techniques such as Tucker decomposition. The number of nodes in the middle of the SVD and the size of the core tensor of the Tucker decomposition are determined from the given compression ratio (eq. (2)). In the case of SVD, the number of middle nodes R can be calculated as follows: 3.2. Sequential distillation R = \u03b3svd IJ I + J . Sequential distillation is used to downsize sequential models such as encoder-decoder models [30]. Frame-by-frame distil- lation is performed by the KL-divergence minimization crite- rion using softmax outputs with temperature hyperparameters. In contrast, sequential distillation uses the labels obtained by in- ference of the teacher model as target labels for student model training. It is also reported to be more effective when extended to N-best from 1-best [31]. However, in our setting, when ex- tended to 5-best, the performance is slightly degraded. Instead, our system uses sequential distillation, which combines the 1- best obtained by the teacher model and ground truth labels. 3.3. Determination of rank from compression ratio On the other hand, since the"}, {"question": " How is the rank of the CP decomposition restricted?", "answer": " The rank of the CP decomposition is restricted to the smallest dimension.", "ref_chunk": "decomposition [26] decomposes tensor W \u2208 RI\u00d7J\u00d7K into a weight vector \u03bb \u2208 RR and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7R and U 3 \u2208 RK\u00d7R as: W = (cid:88) \u03bbr \u00d7 U 1 r \u2297 U 2 r \u2297 U 3 r . r The parameter compression ratio \u03b3cp can be described as: \u03b3cp = R(1 + I + J + K) IJK . The CP decomposition is the special case where the core tensor of the Tucker decomposition has only diagonal component val- ues and the same rank for each dimension in eq. (3). Therefore, it allows for more parameter reduction but is less expressive than the Tucker decomposition. Furthermore, the rank of the CP decomposition is restricted to the smallest dimension. Since the kernel size of the convolution layer is often small compared to the number of channels, the rank is often limited to the ker- nel size. Therefore, CP decomposition is difficult to adjust with flexible parameter sizes. 2.4. Tensor-Train decomposition Tensor-Train [27] is another decomposition method for high- order tensor. The original tensor W \u2208 RI\u00d7J\u00d7K is decom- 1,i,r \u2208 R1\u00d7I\u00d7R, posed into a product of 3-order tensors G1 s,k,1 \u2208 RS\u00d7K\u00d71 by the Tensor-Train G2 decomposition as: r,j,s \u2208 RR\u00d7J\u00d7S and G3 Wi,j,k = (cid:88) G1 1,i,r \u00d7 G2 r,j,s \u00d7 G3 s,k,1. r,s (3) (4) (5) (6) Figure 1: Training procedure. The parameter compression ratio \u03b3tt can be described as: \u03b3tt = IR + RJS + SK IJK . Tensor-Train decomposition provides flexible parameter reduc- tion, as it involves N \u2212 1 hyperparameters corresponding to the rank of N -th order tensors. However, unlike Tucker decompo- sition, it cannot accelerate inference because the original tensor must be reconstructed during inference. 3. Decomposed E2E SLU model 3.1. Training overview We investigate the tensor decomposition techniques in E2E SLU models with higher-order tensors as parameters. E2E SLU is a task that estimates semantic label sequences Y directly from input speech feature sequences X. Fig.1 describes the overview of the training procedure of our model. We use two- step sequential distillation to train a small model. Recent SOTA models use self-supervised learning techniques, which have a large number of parameters (over 300M) [28]. However, on- device processing requires a smaller model, typically around 30M or less. Therefore, We do not apply tensor decomposi- tion directly to the large model because the teacher model is too large to minimize within small parameters with sufficient rank. We train small model through a middle-size model to achieve the target size with a compression ratio near 0.3. As shown in the blue boxes in Fig.1, the training procedure has 5 steps. 1. A large teacher model is trained with ground truth labels. We use the E2E Conformer model with Hubert-based feature extractor1 [28], which has achieved the best performance in prior study [29]. 2. The training data is decoded using the trained teacher model to generate teacher labels. 3. The middle model is trained with both the distilled and ground truth labels. The middle model is used as the seed model to miniaturize the final small model using the decom- position, so its structural type is the same as the final small model. 4. Tensor decomposition and SVD techniques are applied to the middle model to reduce the parameters. We set the target size to 15M or 30M as in a previous study [7]. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 1https://dl.fbaipublicfiles.com/hubert/hubert large ll60k.pt (8) Table 1: Experimental results on STOP dataset within 15M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small Small 0.300 / 0.295 0.250 / 0.300 0.250 / 0.300 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 200 < 15M 431M 14M 47M 15M 15M 53M 15M 15M \u2014\u2013 / 67.9 68.8 / 69.4 34.7 / 34.8 62.6 / 62.5 65.0 / 65.4 60.9 / 61.4 68.6 / 69.0 69.6 / 70.1 70.4 / 70.9 Table 2: Experimental results on STOP dataset within 30M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small 0.650 / 0.650 0.550 / 0.600 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 < 30M \u2014\u2013 / 73.87 68.8 / 69.4 45.3 / 45.7 62.6 / 62.5 65.4 / 65.7 66.4 / 66.8 68.6 / 69.0 71.0 / 71.4 431M 29M 47M 30M 30M 53M 30M The middle model and small model have Conformer or E- Branchformer encoders. Conformer and E-Branchformer in- clude convolution layers that have 3rd or 4th-order tensor pa- rameters. Thus, they can be efficiently compressed by tensor decomposition techniques such as Tucker decomposition. The number of nodes in the middle of the SVD and the size of the core tensor of the Tucker decomposition are determined from the given compression ratio (eq. (2)). In the case of SVD, the number of middle nodes R can be calculated as follows: 3.2. Sequential distillation R = \u03b3svd IJ I + J . Sequential distillation is used to downsize sequential models such as encoder-decoder models [30]. Frame-by-frame distil- lation is performed by the KL-divergence minimization crite- rion using softmax outputs with temperature hyperparameters. In contrast, sequential distillation uses the labels obtained by in- ference of the teacher model as target labels for student model training. It is also reported to be more effective when extended to N-best from 1-best [31]. However, in our setting, when ex- tended to 5-best, the performance is slightly degraded. Instead, our system uses sequential distillation, which combines the 1- best obtained by the teacher model and ground truth labels. 3.3. Determination of rank from compression ratio On the other hand, since the"}, {"question": " Why is CP decomposition difficult to adjust with flexible parameter sizes?", "answer": " CP decomposition is difficult to adjust with flexible parameter sizes because the rank is often limited to the kernel size, and the kernel size of the convolution layer is typically small compared to the number of channels.", "ref_chunk": "decomposition [26] decomposes tensor W \u2208 RI\u00d7J\u00d7K into a weight vector \u03bb \u2208 RR and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7R and U 3 \u2208 RK\u00d7R as: W = (cid:88) \u03bbr \u00d7 U 1 r \u2297 U 2 r \u2297 U 3 r . r The parameter compression ratio \u03b3cp can be described as: \u03b3cp = R(1 + I + J + K) IJK . The CP decomposition is the special case where the core tensor of the Tucker decomposition has only diagonal component val- ues and the same rank for each dimension in eq. (3). Therefore, it allows for more parameter reduction but is less expressive than the Tucker decomposition. Furthermore, the rank of the CP decomposition is restricted to the smallest dimension. Since the kernel size of the convolution layer is often small compared to the number of channels, the rank is often limited to the ker- nel size. Therefore, CP decomposition is difficult to adjust with flexible parameter sizes. 2.4. Tensor-Train decomposition Tensor-Train [27] is another decomposition method for high- order tensor. The original tensor W \u2208 RI\u00d7J\u00d7K is decom- 1,i,r \u2208 R1\u00d7I\u00d7R, posed into a product of 3-order tensors G1 s,k,1 \u2208 RS\u00d7K\u00d71 by the Tensor-Train G2 decomposition as: r,j,s \u2208 RR\u00d7J\u00d7S and G3 Wi,j,k = (cid:88) G1 1,i,r \u00d7 G2 r,j,s \u00d7 G3 s,k,1. r,s (3) (4) (5) (6) Figure 1: Training procedure. The parameter compression ratio \u03b3tt can be described as: \u03b3tt = IR + RJS + SK IJK . Tensor-Train decomposition provides flexible parameter reduc- tion, as it involves N \u2212 1 hyperparameters corresponding to the rank of N -th order tensors. However, unlike Tucker decompo- sition, it cannot accelerate inference because the original tensor must be reconstructed during inference. 3. Decomposed E2E SLU model 3.1. Training overview We investigate the tensor decomposition techniques in E2E SLU models with higher-order tensors as parameters. E2E SLU is a task that estimates semantic label sequences Y directly from input speech feature sequences X. Fig.1 describes the overview of the training procedure of our model. We use two- step sequential distillation to train a small model. Recent SOTA models use self-supervised learning techniques, which have a large number of parameters (over 300M) [28]. However, on- device processing requires a smaller model, typically around 30M or less. Therefore, We do not apply tensor decomposi- tion directly to the large model because the teacher model is too large to minimize within small parameters with sufficient rank. We train small model through a middle-size model to achieve the target size with a compression ratio near 0.3. As shown in the blue boxes in Fig.1, the training procedure has 5 steps. 1. A large teacher model is trained with ground truth labels. We use the E2E Conformer model with Hubert-based feature extractor1 [28], which has achieved the best performance in prior study [29]. 2. The training data is decoded using the trained teacher model to generate teacher labels. 3. The middle model is trained with both the distilled and ground truth labels. The middle model is used as the seed model to miniaturize the final small model using the decom- position, so its structural type is the same as the final small model. 4. Tensor decomposition and SVD techniques are applied to the middle model to reduce the parameters. We set the target size to 15M or 30M as in a previous study [7]. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 1https://dl.fbaipublicfiles.com/hubert/hubert large ll60k.pt (8) Table 1: Experimental results on STOP dataset within 15M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small Small 0.300 / 0.295 0.250 / 0.300 0.250 / 0.300 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 200 < 15M 431M 14M 47M 15M 15M 53M 15M 15M \u2014\u2013 / 67.9 68.8 / 69.4 34.7 / 34.8 62.6 / 62.5 65.0 / 65.4 60.9 / 61.4 68.6 / 69.0 69.6 / 70.1 70.4 / 70.9 Table 2: Experimental results on STOP dataset within 30M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small 0.650 / 0.650 0.550 / 0.600 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 < 30M \u2014\u2013 / 73.87 68.8 / 69.4 45.3 / 45.7 62.6 / 62.5 65.4 / 65.7 66.4 / 66.8 68.6 / 69.0 71.0 / 71.4 431M 29M 47M 30M 30M 53M 30M The middle model and small model have Conformer or E- Branchformer encoders. Conformer and E-Branchformer in- clude convolution layers that have 3rd or 4th-order tensor pa- rameters. Thus, they can be efficiently compressed by tensor decomposition techniques such as Tucker decomposition. The number of nodes in the middle of the SVD and the size of the core tensor of the Tucker decomposition are determined from the given compression ratio (eq. (2)). In the case of SVD, the number of middle nodes R can be calculated as follows: 3.2. Sequential distillation R = \u03b3svd IJ I + J . Sequential distillation is used to downsize sequential models such as encoder-decoder models [30]. Frame-by-frame distil- lation is performed by the KL-divergence minimization crite- rion using softmax outputs with temperature hyperparameters. In contrast, sequential distillation uses the labels obtained by in- ference of the teacher model as target labels for student model training. It is also reported to be more effective when extended to N-best from 1-best [31]. However, in our setting, when ex- tended to 5-best, the performance is slightly degraded. Instead, our system uses sequential distillation, which combines the 1- best obtained by the teacher model and ground truth labels. 3.3. Determination of rank from compression ratio On the other hand, since the"}, {"question": " What is the Tensor-Train decomposition and how does it differ from the Tucker decomposition?", "answer": " The Tensor-Train decomposition is another method for high-order tensors, involving a product of 3-order tensors. It provides flexible parameter reduction but cannot accelerate inference like the Tucker decomposition.", "ref_chunk": "decomposition [26] decomposes tensor W \u2208 RI\u00d7J\u00d7K into a weight vector \u03bb \u2208 RR and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7R and U 3 \u2208 RK\u00d7R as: W = (cid:88) \u03bbr \u00d7 U 1 r \u2297 U 2 r \u2297 U 3 r . r The parameter compression ratio \u03b3cp can be described as: \u03b3cp = R(1 + I + J + K) IJK . The CP decomposition is the special case where the core tensor of the Tucker decomposition has only diagonal component val- ues and the same rank for each dimension in eq. (3). Therefore, it allows for more parameter reduction but is less expressive than the Tucker decomposition. Furthermore, the rank of the CP decomposition is restricted to the smallest dimension. Since the kernel size of the convolution layer is often small compared to the number of channels, the rank is often limited to the ker- nel size. Therefore, CP decomposition is difficult to adjust with flexible parameter sizes. 2.4. Tensor-Train decomposition Tensor-Train [27] is another decomposition method for high- order tensor. The original tensor W \u2208 RI\u00d7J\u00d7K is decom- 1,i,r \u2208 R1\u00d7I\u00d7R, posed into a product of 3-order tensors G1 s,k,1 \u2208 RS\u00d7K\u00d71 by the Tensor-Train G2 decomposition as: r,j,s \u2208 RR\u00d7J\u00d7S and G3 Wi,j,k = (cid:88) G1 1,i,r \u00d7 G2 r,j,s \u00d7 G3 s,k,1. r,s (3) (4) (5) (6) Figure 1: Training procedure. The parameter compression ratio \u03b3tt can be described as: \u03b3tt = IR + RJS + SK IJK . Tensor-Train decomposition provides flexible parameter reduc- tion, as it involves N \u2212 1 hyperparameters corresponding to the rank of N -th order tensors. However, unlike Tucker decompo- sition, it cannot accelerate inference because the original tensor must be reconstructed during inference. 3. Decomposed E2E SLU model 3.1. Training overview We investigate the tensor decomposition techniques in E2E SLU models with higher-order tensors as parameters. E2E SLU is a task that estimates semantic label sequences Y directly from input speech feature sequences X. Fig.1 describes the overview of the training procedure of our model. We use two- step sequential distillation to train a small model. Recent SOTA models use self-supervised learning techniques, which have a large number of parameters (over 300M) [28]. However, on- device processing requires a smaller model, typically around 30M or less. Therefore, We do not apply tensor decomposi- tion directly to the large model because the teacher model is too large to minimize within small parameters with sufficient rank. We train small model through a middle-size model to achieve the target size with a compression ratio near 0.3. As shown in the blue boxes in Fig.1, the training procedure has 5 steps. 1. A large teacher model is trained with ground truth labels. We use the E2E Conformer model with Hubert-based feature extractor1 [28], which has achieved the best performance in prior study [29]. 2. The training data is decoded using the trained teacher model to generate teacher labels. 3. The middle model is trained with both the distilled and ground truth labels. The middle model is used as the seed model to miniaturize the final small model using the decom- position, so its structural type is the same as the final small model. 4. Tensor decomposition and SVD techniques are applied to the middle model to reduce the parameters. We set the target size to 15M or 30M as in a previous study [7]. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 1https://dl.fbaipublicfiles.com/hubert/hubert large ll60k.pt (8) Table 1: Experimental results on STOP dataset within 15M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small Small 0.300 / 0.295 0.250 / 0.300 0.250 / 0.300 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 200 < 15M 431M 14M 47M 15M 15M 53M 15M 15M \u2014\u2013 / 67.9 68.8 / 69.4 34.7 / 34.8 62.6 / 62.5 65.0 / 65.4 60.9 / 61.4 68.6 / 69.0 69.6 / 70.1 70.4 / 70.9 Table 2: Experimental results on STOP dataset within 30M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small 0.650 / 0.650 0.550 / 0.600 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 < 30M \u2014\u2013 / 73.87 68.8 / 69.4 45.3 / 45.7 62.6 / 62.5 65.4 / 65.7 66.4 / 66.8 68.6 / 69.0 71.0 / 71.4 431M 29M 47M 30M 30M 53M 30M The middle model and small model have Conformer or E- Branchformer encoders. Conformer and E-Branchformer in- clude convolution layers that have 3rd or 4th-order tensor pa- rameters. Thus, they can be efficiently compressed by tensor decomposition techniques such as Tucker decomposition. The number of nodes in the middle of the SVD and the size of the core tensor of the Tucker decomposition are determined from the given compression ratio (eq. (2)). In the case of SVD, the number of middle nodes R can be calculated as follows: 3.2. Sequential distillation R = \u03b3svd IJ I + J . Sequential distillation is used to downsize sequential models such as encoder-decoder models [30]. Frame-by-frame distil- lation is performed by the KL-divergence minimization crite- rion using softmax outputs with temperature hyperparameters. In contrast, sequential distillation uses the labels obtained by in- ference of the teacher model as target labels for student model training. It is also reported to be more effective when extended to N-best from 1-best [31]. However, in our setting, when ex- tended to 5-best, the performance is slightly degraded. Instead, our system uses sequential distillation, which combines the 1- best obtained by the teacher model and ground truth labels. 3.3. Determination of rank from compression ratio On the other hand, since the"}, {"question": " How many hyperparameters does the Tensor-Train decomposition involve?", "answer": " The Tensor-Train decomposition involves N - 1 hyperparameters corresponding to the rank of N-th order tensors.", "ref_chunk": "decomposition [26] decomposes tensor W \u2208 RI\u00d7J\u00d7K into a weight vector \u03bb \u2208 RR and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7R and U 3 \u2208 RK\u00d7R as: W = (cid:88) \u03bbr \u00d7 U 1 r \u2297 U 2 r \u2297 U 3 r . r The parameter compression ratio \u03b3cp can be described as: \u03b3cp = R(1 + I + J + K) IJK . The CP decomposition is the special case where the core tensor of the Tucker decomposition has only diagonal component val- ues and the same rank for each dimension in eq. (3). Therefore, it allows for more parameter reduction but is less expressive than the Tucker decomposition. Furthermore, the rank of the CP decomposition is restricted to the smallest dimension. Since the kernel size of the convolution layer is often small compared to the number of channels, the rank is often limited to the ker- nel size. Therefore, CP decomposition is difficult to adjust with flexible parameter sizes. 2.4. Tensor-Train decomposition Tensor-Train [27] is another decomposition method for high- order tensor. The original tensor W \u2208 RI\u00d7J\u00d7K is decom- 1,i,r \u2208 R1\u00d7I\u00d7R, posed into a product of 3-order tensors G1 s,k,1 \u2208 RS\u00d7K\u00d71 by the Tensor-Train G2 decomposition as: r,j,s \u2208 RR\u00d7J\u00d7S and G3 Wi,j,k = (cid:88) G1 1,i,r \u00d7 G2 r,j,s \u00d7 G3 s,k,1. r,s (3) (4) (5) (6) Figure 1: Training procedure. The parameter compression ratio \u03b3tt can be described as: \u03b3tt = IR + RJS + SK IJK . Tensor-Train decomposition provides flexible parameter reduc- tion, as it involves N \u2212 1 hyperparameters corresponding to the rank of N -th order tensors. However, unlike Tucker decompo- sition, it cannot accelerate inference because the original tensor must be reconstructed during inference. 3. Decomposed E2E SLU model 3.1. Training overview We investigate the tensor decomposition techniques in E2E SLU models with higher-order tensors as parameters. E2E SLU is a task that estimates semantic label sequences Y directly from input speech feature sequences X. Fig.1 describes the overview of the training procedure of our model. We use two- step sequential distillation to train a small model. Recent SOTA models use self-supervised learning techniques, which have a large number of parameters (over 300M) [28]. However, on- device processing requires a smaller model, typically around 30M or less. Therefore, We do not apply tensor decomposi- tion directly to the large model because the teacher model is too large to minimize within small parameters with sufficient rank. We train small model through a middle-size model to achieve the target size with a compression ratio near 0.3. As shown in the blue boxes in Fig.1, the training procedure has 5 steps. 1. A large teacher model is trained with ground truth labels. We use the E2E Conformer model with Hubert-based feature extractor1 [28], which has achieved the best performance in prior study [29]. 2. The training data is decoded using the trained teacher model to generate teacher labels. 3. The middle model is trained with both the distilled and ground truth labels. The middle model is used as the seed model to miniaturize the final small model using the decom- position, so its structural type is the same as the final small model. 4. Tensor decomposition and SVD techniques are applied to the middle model to reduce the parameters. We set the target size to 15M or 30M as in a previous study [7]. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 1https://dl.fbaipublicfiles.com/hubert/hubert large ll60k.pt (8) Table 1: Experimental results on STOP dataset within 15M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small Small 0.300 / 0.295 0.250 / 0.300 0.250 / 0.300 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 200 < 15M 431M 14M 47M 15M 15M 53M 15M 15M \u2014\u2013 / 67.9 68.8 / 69.4 34.7 / 34.8 62.6 / 62.5 65.0 / 65.4 60.9 / 61.4 68.6 / 69.0 69.6 / 70.1 70.4 / 70.9 Table 2: Experimental results on STOP dataset within 30M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small 0.650 / 0.650 0.550 / 0.600 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 < 30M \u2014\u2013 / 73.87 68.8 / 69.4 45.3 / 45.7 62.6 / 62.5 65.4 / 65.7 66.4 / 66.8 68.6 / 69.0 71.0 / 71.4 431M 29M 47M 30M 30M 53M 30M The middle model and small model have Conformer or E- Branchformer encoders. Conformer and E-Branchformer in- clude convolution layers that have 3rd or 4th-order tensor pa- rameters. Thus, they can be efficiently compressed by tensor decomposition techniques such as Tucker decomposition. The number of nodes in the middle of the SVD and the size of the core tensor of the Tucker decomposition are determined from the given compression ratio (eq. (2)). In the case of SVD, the number of middle nodes R can be calculated as follows: 3.2. Sequential distillation R = \u03b3svd IJ I + J . Sequential distillation is used to downsize sequential models such as encoder-decoder models [30]. Frame-by-frame distil- lation is performed by the KL-divergence minimization crite- rion using softmax outputs with temperature hyperparameters. In contrast, sequential distillation uses the labels obtained by in- ference of the teacher model as target labels for student model training. It is also reported to be more effective when extended to N-best from 1-best [31]. However, in our setting, when ex- tended to 5-best, the performance is slightly degraded. Instead, our system uses sequential distillation, which combines the 1- best obtained by the teacher model and ground truth labels. 3.3. Determination of rank from compression ratio On the other hand, since the"}, {"question": " What is the purpose of the E2E SLU model described in the text?", "answer": " The E2E SLU model aims to estimate semantic label sequences directly from input speech feature sequences, using tensor decomposition techniques for parameter reduction.", "ref_chunk": "decomposition [26] decomposes tensor W \u2208 RI\u00d7J\u00d7K into a weight vector \u03bb \u2208 RR and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7R and U 3 \u2208 RK\u00d7R as: W = (cid:88) \u03bbr \u00d7 U 1 r \u2297 U 2 r \u2297 U 3 r . r The parameter compression ratio \u03b3cp can be described as: \u03b3cp = R(1 + I + J + K) IJK . The CP decomposition is the special case where the core tensor of the Tucker decomposition has only diagonal component val- ues and the same rank for each dimension in eq. (3). Therefore, it allows for more parameter reduction but is less expressive than the Tucker decomposition. Furthermore, the rank of the CP decomposition is restricted to the smallest dimension. Since the kernel size of the convolution layer is often small compared to the number of channels, the rank is often limited to the ker- nel size. Therefore, CP decomposition is difficult to adjust with flexible parameter sizes. 2.4. Tensor-Train decomposition Tensor-Train [27] is another decomposition method for high- order tensor. The original tensor W \u2208 RI\u00d7J\u00d7K is decom- 1,i,r \u2208 R1\u00d7I\u00d7R, posed into a product of 3-order tensors G1 s,k,1 \u2208 RS\u00d7K\u00d71 by the Tensor-Train G2 decomposition as: r,j,s \u2208 RR\u00d7J\u00d7S and G3 Wi,j,k = (cid:88) G1 1,i,r \u00d7 G2 r,j,s \u00d7 G3 s,k,1. r,s (3) (4) (5) (6) Figure 1: Training procedure. The parameter compression ratio \u03b3tt can be described as: \u03b3tt = IR + RJS + SK IJK . Tensor-Train decomposition provides flexible parameter reduc- tion, as it involves N \u2212 1 hyperparameters corresponding to the rank of N -th order tensors. However, unlike Tucker decompo- sition, it cannot accelerate inference because the original tensor must be reconstructed during inference. 3. Decomposed E2E SLU model 3.1. Training overview We investigate the tensor decomposition techniques in E2E SLU models with higher-order tensors as parameters. E2E SLU is a task that estimates semantic label sequences Y directly from input speech feature sequences X. Fig.1 describes the overview of the training procedure of our model. We use two- step sequential distillation to train a small model. Recent SOTA models use self-supervised learning techniques, which have a large number of parameters (over 300M) [28]. However, on- device processing requires a smaller model, typically around 30M or less. Therefore, We do not apply tensor decomposi- tion directly to the large model because the teacher model is too large to minimize within small parameters with sufficient rank. We train small model through a middle-size model to achieve the target size with a compression ratio near 0.3. As shown in the blue boxes in Fig.1, the training procedure has 5 steps. 1. A large teacher model is trained with ground truth labels. We use the E2E Conformer model with Hubert-based feature extractor1 [28], which has achieved the best performance in prior study [29]. 2. The training data is decoded using the trained teacher model to generate teacher labels. 3. The middle model is trained with both the distilled and ground truth labels. The middle model is used as the seed model to miniaturize the final small model using the decom- position, so its structural type is the same as the final small model. 4. Tensor decomposition and SVD techniques are applied to the middle model to reduce the parameters. We set the target size to 15M or 30M as in a previous study [7]. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 1https://dl.fbaipublicfiles.com/hubert/hubert large ll60k.pt (8) Table 1: Experimental results on STOP dataset within 15M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small Small 0.300 / 0.295 0.250 / 0.300 0.250 / 0.300 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 200 < 15M 431M 14M 47M 15M 15M 53M 15M 15M \u2014\u2013 / 67.9 68.8 / 69.4 34.7 / 34.8 62.6 / 62.5 65.0 / 65.4 60.9 / 61.4 68.6 / 69.0 69.6 / 70.1 70.4 / 70.9 Table 2: Experimental results on STOP dataset within 30M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small 0.650 / 0.650 0.550 / 0.600 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 < 30M \u2014\u2013 / 73.87 68.8 / 69.4 45.3 / 45.7 62.6 / 62.5 65.4 / 65.7 66.4 / 66.8 68.6 / 69.0 71.0 / 71.4 431M 29M 47M 30M 30M 53M 30M The middle model and small model have Conformer or E- Branchformer encoders. Conformer and E-Branchformer in- clude convolution layers that have 3rd or 4th-order tensor pa- rameters. Thus, they can be efficiently compressed by tensor decomposition techniques such as Tucker decomposition. The number of nodes in the middle of the SVD and the size of the core tensor of the Tucker decomposition are determined from the given compression ratio (eq. (2)). In the case of SVD, the number of middle nodes R can be calculated as follows: 3.2. Sequential distillation R = \u03b3svd IJ I + J . Sequential distillation is used to downsize sequential models such as encoder-decoder models [30]. Frame-by-frame distil- lation is performed by the KL-divergence minimization crite- rion using softmax outputs with temperature hyperparameters. In contrast, sequential distillation uses the labels obtained by in- ference of the teacher model as target labels for student model training. It is also reported to be more effective when extended to N-best from 1-best [31]. However, in our setting, when ex- tended to 5-best, the performance is slightly degraded. Instead, our system uses sequential distillation, which combines the 1- best obtained by the teacher model and ground truth labels. 3.3. Determination of rank from compression ratio On the other hand, since the"}, {"question": " What is the role of the middle model in the training procedure of the E2E SLU model?", "answer": " The middle model is used as the seed model to miniaturize the final small model using tensor decomposition techniques, ensuring that the structural type remains the same.", "ref_chunk": "decomposition [26] decomposes tensor W \u2208 RI\u00d7J\u00d7K into a weight vector \u03bb \u2208 RR and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7R and U 3 \u2208 RK\u00d7R as: W = (cid:88) \u03bbr \u00d7 U 1 r \u2297 U 2 r \u2297 U 3 r . r The parameter compression ratio \u03b3cp can be described as: \u03b3cp = R(1 + I + J + K) IJK . The CP decomposition is the special case where the core tensor of the Tucker decomposition has only diagonal component val- ues and the same rank for each dimension in eq. (3). Therefore, it allows for more parameter reduction but is less expressive than the Tucker decomposition. Furthermore, the rank of the CP decomposition is restricted to the smallest dimension. Since the kernel size of the convolution layer is often small compared to the number of channels, the rank is often limited to the ker- nel size. Therefore, CP decomposition is difficult to adjust with flexible parameter sizes. 2.4. Tensor-Train decomposition Tensor-Train [27] is another decomposition method for high- order tensor. The original tensor W \u2208 RI\u00d7J\u00d7K is decom- 1,i,r \u2208 R1\u00d7I\u00d7R, posed into a product of 3-order tensors G1 s,k,1 \u2208 RS\u00d7K\u00d71 by the Tensor-Train G2 decomposition as: r,j,s \u2208 RR\u00d7J\u00d7S and G3 Wi,j,k = (cid:88) G1 1,i,r \u00d7 G2 r,j,s \u00d7 G3 s,k,1. r,s (3) (4) (5) (6) Figure 1: Training procedure. The parameter compression ratio \u03b3tt can be described as: \u03b3tt = IR + RJS + SK IJK . Tensor-Train decomposition provides flexible parameter reduc- tion, as it involves N \u2212 1 hyperparameters corresponding to the rank of N -th order tensors. However, unlike Tucker decompo- sition, it cannot accelerate inference because the original tensor must be reconstructed during inference. 3. Decomposed E2E SLU model 3.1. Training overview We investigate the tensor decomposition techniques in E2E SLU models with higher-order tensors as parameters. E2E SLU is a task that estimates semantic label sequences Y directly from input speech feature sequences X. Fig.1 describes the overview of the training procedure of our model. We use two- step sequential distillation to train a small model. Recent SOTA models use self-supervised learning techniques, which have a large number of parameters (over 300M) [28]. However, on- device processing requires a smaller model, typically around 30M or less. Therefore, We do not apply tensor decomposi- tion directly to the large model because the teacher model is too large to minimize within small parameters with sufficient rank. We train small model through a middle-size model to achieve the target size with a compression ratio near 0.3. As shown in the blue boxes in Fig.1, the training procedure has 5 steps. 1. A large teacher model is trained with ground truth labels. We use the E2E Conformer model with Hubert-based feature extractor1 [28], which has achieved the best performance in prior study [29]. 2. The training data is decoded using the trained teacher model to generate teacher labels. 3. The middle model is trained with both the distilled and ground truth labels. The middle model is used as the seed model to miniaturize the final small model using the decom- position, so its structural type is the same as the final small model. 4. Tensor decomposition and SVD techniques are applied to the middle model to reduce the parameters. We set the target size to 15M or 30M as in a previous study [7]. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 1https://dl.fbaipublicfiles.com/hubert/hubert large ll60k.pt (8) Table 1: Experimental results on STOP dataset within 15M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small Small 0.300 / 0.295 0.250 / 0.300 0.250 / 0.300 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 200 < 15M 431M 14M 47M 15M 15M 53M 15M 15M \u2014\u2013 / 67.9 68.8 / 69.4 34.7 / 34.8 62.6 / 62.5 65.0 / 65.4 60.9 / 61.4 68.6 / 69.0 69.6 / 70.1 70.4 / 70.9 Table 2: Experimental results on STOP dataset within 30M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small 0.650 / 0.650 0.550 / 0.600 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 < 30M \u2014\u2013 / 73.87 68.8 / 69.4 45.3 / 45.7 62.6 / 62.5 65.4 / 65.7 66.4 / 66.8 68.6 / 69.0 71.0 / 71.4 431M 29M 47M 30M 30M 53M 30M The middle model and small model have Conformer or E- Branchformer encoders. Conformer and E-Branchformer in- clude convolution layers that have 3rd or 4th-order tensor pa- rameters. Thus, they can be efficiently compressed by tensor decomposition techniques such as Tucker decomposition. The number of nodes in the middle of the SVD and the size of the core tensor of the Tucker decomposition are determined from the given compression ratio (eq. (2)). In the case of SVD, the number of middle nodes R can be calculated as follows: 3.2. Sequential distillation R = \u03b3svd IJ I + J . Sequential distillation is used to downsize sequential models such as encoder-decoder models [30]. Frame-by-frame distil- lation is performed by the KL-divergence minimization crite- rion using softmax outputs with temperature hyperparameters. In contrast, sequential distillation uses the labels obtained by in- ference of the teacher model as target labels for student model training. It is also reported to be more effective when extended to N-best from 1-best [31]. However, in our setting, when ex- tended to 5-best, the performance is slightly degraded. Instead, our system uses sequential distillation, which combines the 1- best obtained by the teacher model and ground truth labels. 3.3. Determination of rank from compression ratio On the other hand, since the"}, {"question": " How is the number of nodes in the middle model and the size of the core tensor determined?", "answer": " The number of nodes in the middle model and the size of the core tensor of the Tucker decomposition are determined from the given compression ratio.", "ref_chunk": "decomposition [26] decomposes tensor W \u2208 RI\u00d7J\u00d7K into a weight vector \u03bb \u2208 RR and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7R and U 3 \u2208 RK\u00d7R as: W = (cid:88) \u03bbr \u00d7 U 1 r \u2297 U 2 r \u2297 U 3 r . r The parameter compression ratio \u03b3cp can be described as: \u03b3cp = R(1 + I + J + K) IJK . The CP decomposition is the special case where the core tensor of the Tucker decomposition has only diagonal component val- ues and the same rank for each dimension in eq. (3). Therefore, it allows for more parameter reduction but is less expressive than the Tucker decomposition. Furthermore, the rank of the CP decomposition is restricted to the smallest dimension. Since the kernel size of the convolution layer is often small compared to the number of channels, the rank is often limited to the ker- nel size. Therefore, CP decomposition is difficult to adjust with flexible parameter sizes. 2.4. Tensor-Train decomposition Tensor-Train [27] is another decomposition method for high- order tensor. The original tensor W \u2208 RI\u00d7J\u00d7K is decom- 1,i,r \u2208 R1\u00d7I\u00d7R, posed into a product of 3-order tensors G1 s,k,1 \u2208 RS\u00d7K\u00d71 by the Tensor-Train G2 decomposition as: r,j,s \u2208 RR\u00d7J\u00d7S and G3 Wi,j,k = (cid:88) G1 1,i,r \u00d7 G2 r,j,s \u00d7 G3 s,k,1. r,s (3) (4) (5) (6) Figure 1: Training procedure. The parameter compression ratio \u03b3tt can be described as: \u03b3tt = IR + RJS + SK IJK . Tensor-Train decomposition provides flexible parameter reduc- tion, as it involves N \u2212 1 hyperparameters corresponding to the rank of N -th order tensors. However, unlike Tucker decompo- sition, it cannot accelerate inference because the original tensor must be reconstructed during inference. 3. Decomposed E2E SLU model 3.1. Training overview We investigate the tensor decomposition techniques in E2E SLU models with higher-order tensors as parameters. E2E SLU is a task that estimates semantic label sequences Y directly from input speech feature sequences X. Fig.1 describes the overview of the training procedure of our model. We use two- step sequential distillation to train a small model. Recent SOTA models use self-supervised learning techniques, which have a large number of parameters (over 300M) [28]. However, on- device processing requires a smaller model, typically around 30M or less. Therefore, We do not apply tensor decomposi- tion directly to the large model because the teacher model is too large to minimize within small parameters with sufficient rank. We train small model through a middle-size model to achieve the target size with a compression ratio near 0.3. As shown in the blue boxes in Fig.1, the training procedure has 5 steps. 1. A large teacher model is trained with ground truth labels. We use the E2E Conformer model with Hubert-based feature extractor1 [28], which has achieved the best performance in prior study [29]. 2. The training data is decoded using the trained teacher model to generate teacher labels. 3. The middle model is trained with both the distilled and ground truth labels. The middle model is used as the seed model to miniaturize the final small model using the decom- position, so its structural type is the same as the final small model. 4. Tensor decomposition and SVD techniques are applied to the middle model to reduce the parameters. We set the target size to 15M or 30M as in a previous study [7]. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 1https://dl.fbaipublicfiles.com/hubert/hubert large ll60k.pt (8) Table 1: Experimental results on STOP dataset within 15M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small Small 0.300 / 0.295 0.250 / 0.300 0.250 / 0.300 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 200 < 15M 431M 14M 47M 15M 15M 53M 15M 15M \u2014\u2013 / 67.9 68.8 / 69.4 34.7 / 34.8 62.6 / 62.5 65.0 / 65.4 60.9 / 61.4 68.6 / 69.0 69.6 / 70.1 70.4 / 70.9 Table 2: Experimental results on STOP dataset within 30M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small 0.650 / 0.650 0.550 / 0.600 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 < 30M \u2014\u2013 / 73.87 68.8 / 69.4 45.3 / 45.7 62.6 / 62.5 65.4 / 65.7 66.4 / 66.8 68.6 / 69.0 71.0 / 71.4 431M 29M 47M 30M 30M 53M 30M The middle model and small model have Conformer or E- Branchformer encoders. Conformer and E-Branchformer in- clude convolution layers that have 3rd or 4th-order tensor pa- rameters. Thus, they can be efficiently compressed by tensor decomposition techniques such as Tucker decomposition. The number of nodes in the middle of the SVD and the size of the core tensor of the Tucker decomposition are determined from the given compression ratio (eq. (2)). In the case of SVD, the number of middle nodes R can be calculated as follows: 3.2. Sequential distillation R = \u03b3svd IJ I + J . Sequential distillation is used to downsize sequential models such as encoder-decoder models [30]. Frame-by-frame distil- lation is performed by the KL-divergence minimization crite- rion using softmax outputs with temperature hyperparameters. In contrast, sequential distillation uses the labels obtained by in- ference of the teacher model as target labels for student model training. It is also reported to be more effective when extended to N-best from 1-best [31]. However, in our setting, when ex- tended to 5-best, the performance is slightly degraded. Instead, our system uses sequential distillation, which combines the 1- best obtained by the teacher model and ground truth labels. 3.3. Determination of rank from compression ratio On the other hand, since the"}, {"question": " What is sequential distillation, and how is it used in downsizing sequential models?", "answer": " Sequential distillation is a technique used to downsize sequential models like encoder-decoder models by minimizing the KL-divergence criterion using softmax outputs with temperature hyperparameters.", "ref_chunk": "decomposition [26] decomposes tensor W \u2208 RI\u00d7J\u00d7K into a weight vector \u03bb \u2208 RR and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7R and U 3 \u2208 RK\u00d7R as: W = (cid:88) \u03bbr \u00d7 U 1 r \u2297 U 2 r \u2297 U 3 r . r The parameter compression ratio \u03b3cp can be described as: \u03b3cp = R(1 + I + J + K) IJK . The CP decomposition is the special case where the core tensor of the Tucker decomposition has only diagonal component val- ues and the same rank for each dimension in eq. (3). Therefore, it allows for more parameter reduction but is less expressive than the Tucker decomposition. Furthermore, the rank of the CP decomposition is restricted to the smallest dimension. Since the kernel size of the convolution layer is often small compared to the number of channels, the rank is often limited to the ker- nel size. Therefore, CP decomposition is difficult to adjust with flexible parameter sizes. 2.4. Tensor-Train decomposition Tensor-Train [27] is another decomposition method for high- order tensor. The original tensor W \u2208 RI\u00d7J\u00d7K is decom- 1,i,r \u2208 R1\u00d7I\u00d7R, posed into a product of 3-order tensors G1 s,k,1 \u2208 RS\u00d7K\u00d71 by the Tensor-Train G2 decomposition as: r,j,s \u2208 RR\u00d7J\u00d7S and G3 Wi,j,k = (cid:88) G1 1,i,r \u00d7 G2 r,j,s \u00d7 G3 s,k,1. r,s (3) (4) (5) (6) Figure 1: Training procedure. The parameter compression ratio \u03b3tt can be described as: \u03b3tt = IR + RJS + SK IJK . Tensor-Train decomposition provides flexible parameter reduc- tion, as it involves N \u2212 1 hyperparameters corresponding to the rank of N -th order tensors. However, unlike Tucker decompo- sition, it cannot accelerate inference because the original tensor must be reconstructed during inference. 3. Decomposed E2E SLU model 3.1. Training overview We investigate the tensor decomposition techniques in E2E SLU models with higher-order tensors as parameters. E2E SLU is a task that estimates semantic label sequences Y directly from input speech feature sequences X. Fig.1 describes the overview of the training procedure of our model. We use two- step sequential distillation to train a small model. Recent SOTA models use self-supervised learning techniques, which have a large number of parameters (over 300M) [28]. However, on- device processing requires a smaller model, typically around 30M or less. Therefore, We do not apply tensor decomposi- tion directly to the large model because the teacher model is too large to minimize within small parameters with sufficient rank. We train small model through a middle-size model to achieve the target size with a compression ratio near 0.3. As shown in the blue boxes in Fig.1, the training procedure has 5 steps. 1. A large teacher model is trained with ground truth labels. We use the E2E Conformer model with Hubert-based feature extractor1 [28], which has achieved the best performance in prior study [29]. 2. The training data is decoded using the trained teacher model to generate teacher labels. 3. The middle model is trained with both the distilled and ground truth labels. The middle model is used as the seed model to miniaturize the final small model using the decom- position, so its structural type is the same as the final small model. 4. Tensor decomposition and SVD techniques are applied to the middle model to reduce the parameters. We set the target size to 15M or 30M as in a previous study [7]. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 1https://dl.fbaipublicfiles.com/hubert/hubert large ll60k.pt (8) Table 1: Experimental results on STOP dataset within 15M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small Small 0.300 / 0.295 0.250 / 0.300 0.250 / 0.300 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 200 < 15M 431M 14M 47M 15M 15M 53M 15M 15M \u2014\u2013 / 67.9 68.8 / 69.4 34.7 / 34.8 62.6 / 62.5 65.0 / 65.4 60.9 / 61.4 68.6 / 69.0 69.6 / 70.1 70.4 / 70.9 Table 2: Experimental results on STOP dataset within 30M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small 0.650 / 0.650 0.550 / 0.600 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 < 30M \u2014\u2013 / 73.87 68.8 / 69.4 45.3 / 45.7 62.6 / 62.5 65.4 / 65.7 66.4 / 66.8 68.6 / 69.0 71.0 / 71.4 431M 29M 47M 30M 30M 53M 30M The middle model and small model have Conformer or E- Branchformer encoders. Conformer and E-Branchformer in- clude convolution layers that have 3rd or 4th-order tensor pa- rameters. Thus, they can be efficiently compressed by tensor decomposition techniques such as Tucker decomposition. The number of nodes in the middle of the SVD and the size of the core tensor of the Tucker decomposition are determined from the given compression ratio (eq. (2)). In the case of SVD, the number of middle nodes R can be calculated as follows: 3.2. Sequential distillation R = \u03b3svd IJ I + J . Sequential distillation is used to downsize sequential models such as encoder-decoder models [30]. Frame-by-frame distil- lation is performed by the KL-divergence minimization crite- rion using softmax outputs with temperature hyperparameters. In contrast, sequential distillation uses the labels obtained by in- ference of the teacher model as target labels for student model training. It is also reported to be more effective when extended to N-best from 1-best [31]. However, in our setting, when ex- tended to 5-best, the performance is slightly degraded. Instead, our system uses sequential distillation, which combines the 1- best obtained by the teacher model and ground truth labels. 3.3. Determination of rank from compression ratio On the other hand, since the"}, {"question": " How is the rank determined from the compression ratio in tensor decomposition?", "answer": " The rank is determined from the compression ratio in tensor decomposition by calculating the number of middle nodes or using specific formulas depending on the decomposition technique.", "ref_chunk": "decomposition [26] decomposes tensor W \u2208 RI\u00d7J\u00d7K into a weight vector \u03bb \u2208 RR and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7R and U 3 \u2208 RK\u00d7R as: W = (cid:88) \u03bbr \u00d7 U 1 r \u2297 U 2 r \u2297 U 3 r . r The parameter compression ratio \u03b3cp can be described as: \u03b3cp = R(1 + I + J + K) IJK . The CP decomposition is the special case where the core tensor of the Tucker decomposition has only diagonal component val- ues and the same rank for each dimension in eq. (3). Therefore, it allows for more parameter reduction but is less expressive than the Tucker decomposition. Furthermore, the rank of the CP decomposition is restricted to the smallest dimension. Since the kernel size of the convolution layer is often small compared to the number of channels, the rank is often limited to the ker- nel size. Therefore, CP decomposition is difficult to adjust with flexible parameter sizes. 2.4. Tensor-Train decomposition Tensor-Train [27] is another decomposition method for high- order tensor. The original tensor W \u2208 RI\u00d7J\u00d7K is decom- 1,i,r \u2208 R1\u00d7I\u00d7R, posed into a product of 3-order tensors G1 s,k,1 \u2208 RS\u00d7K\u00d71 by the Tensor-Train G2 decomposition as: r,j,s \u2208 RR\u00d7J\u00d7S and G3 Wi,j,k = (cid:88) G1 1,i,r \u00d7 G2 r,j,s \u00d7 G3 s,k,1. r,s (3) (4) (5) (6) Figure 1: Training procedure. The parameter compression ratio \u03b3tt can be described as: \u03b3tt = IR + RJS + SK IJK . Tensor-Train decomposition provides flexible parameter reduc- tion, as it involves N \u2212 1 hyperparameters corresponding to the rank of N -th order tensors. However, unlike Tucker decompo- sition, it cannot accelerate inference because the original tensor must be reconstructed during inference. 3. Decomposed E2E SLU model 3.1. Training overview We investigate the tensor decomposition techniques in E2E SLU models with higher-order tensors as parameters. E2E SLU is a task that estimates semantic label sequences Y directly from input speech feature sequences X. Fig.1 describes the overview of the training procedure of our model. We use two- step sequential distillation to train a small model. Recent SOTA models use self-supervised learning techniques, which have a large number of parameters (over 300M) [28]. However, on- device processing requires a smaller model, typically around 30M or less. Therefore, We do not apply tensor decomposi- tion directly to the large model because the teacher model is too large to minimize within small parameters with sufficient rank. We train small model through a middle-size model to achieve the target size with a compression ratio near 0.3. As shown in the blue boxes in Fig.1, the training procedure has 5 steps. 1. A large teacher model is trained with ground truth labels. We use the E2E Conformer model with Hubert-based feature extractor1 [28], which has achieved the best performance in prior study [29]. 2. The training data is decoded using the trained teacher model to generate teacher labels. 3. The middle model is trained with both the distilled and ground truth labels. The middle model is used as the seed model to miniaturize the final small model using the decom- position, so its structural type is the same as the final small model. 4. Tensor decomposition and SVD techniques are applied to the middle model to reduce the parameters. We set the target size to 15M or 30M as in a previous study [7]. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 1https://dl.fbaipublicfiles.com/hubert/hubert large ll60k.pt (8) Table 1: Experimental results on STOP dataset within 15M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small Small 0.300 / 0.295 0.250 / 0.300 0.250 / 0.300 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 200 < 15M 431M 14M 47M 15M 15M 53M 15M 15M \u2014\u2013 / 67.9 68.8 / 69.4 34.7 / 34.8 62.6 / 62.5 65.0 / 65.4 60.9 / 61.4 68.6 / 69.0 69.6 / 70.1 70.4 / 70.9 Table 2: Experimental results on STOP dataset within 30M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small 0.650 / 0.650 0.550 / 0.600 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 < 30M \u2014\u2013 / 73.87 68.8 / 69.4 45.3 / 45.7 62.6 / 62.5 65.4 / 65.7 66.4 / 66.8 68.6 / 69.0 71.0 / 71.4 431M 29M 47M 30M 30M 53M 30M The middle model and small model have Conformer or E- Branchformer encoders. Conformer and E-Branchformer in- clude convolution layers that have 3rd or 4th-order tensor pa- rameters. Thus, they can be efficiently compressed by tensor decomposition techniques such as Tucker decomposition. The number of nodes in the middle of the SVD and the size of the core tensor of the Tucker decomposition are determined from the given compression ratio (eq. (2)). In the case of SVD, the number of middle nodes R can be calculated as follows: 3.2. Sequential distillation R = \u03b3svd IJ I + J . Sequential distillation is used to downsize sequential models such as encoder-decoder models [30]. Frame-by-frame distil- lation is performed by the KL-divergence minimization crite- rion using softmax outputs with temperature hyperparameters. In contrast, sequential distillation uses the labels obtained by in- ference of the teacher model as target labels for student model training. It is also reported to be more effective when extended to N-best from 1-best [31]. However, in our setting, when ex- tended to 5-best, the performance is slightly degraded. Instead, our system uses sequential distillation, which combines the 1- best obtained by the teacher model and ground truth labels. 3.3. Determination of rank from compression ratio On the other hand, since the"}], "doc_text": "decomposition [26] decomposes tensor W \u2208 RI\u00d7J\u00d7K into a weight vector \u03bb \u2208 RR and factor matrices U 1 \u2208 RI\u00d7R, U 2 \u2208 RJ\u00d7R and U 3 \u2208 RK\u00d7R as: W = (cid:88) \u03bbr \u00d7 U 1 r \u2297 U 2 r \u2297 U 3 r . r The parameter compression ratio \u03b3cp can be described as: \u03b3cp = R(1 + I + J + K) IJK . The CP decomposition is the special case where the core tensor of the Tucker decomposition has only diagonal component val- ues and the same rank for each dimension in eq. (3). Therefore, it allows for more parameter reduction but is less expressive than the Tucker decomposition. Furthermore, the rank of the CP decomposition is restricted to the smallest dimension. Since the kernel size of the convolution layer is often small compared to the number of channels, the rank is often limited to the ker- nel size. Therefore, CP decomposition is difficult to adjust with flexible parameter sizes. 2.4. Tensor-Train decomposition Tensor-Train [27] is another decomposition method for high- order tensor. The original tensor W \u2208 RI\u00d7J\u00d7K is decom- 1,i,r \u2208 R1\u00d7I\u00d7R, posed into a product of 3-order tensors G1 s,k,1 \u2208 RS\u00d7K\u00d71 by the Tensor-Train G2 decomposition as: r,j,s \u2208 RR\u00d7J\u00d7S and G3 Wi,j,k = (cid:88) G1 1,i,r \u00d7 G2 r,j,s \u00d7 G3 s,k,1. r,s (3) (4) (5) (6) Figure 1: Training procedure. The parameter compression ratio \u03b3tt can be described as: \u03b3tt = IR + RJS + SK IJK . Tensor-Train decomposition provides flexible parameter reduc- tion, as it involves N \u2212 1 hyperparameters corresponding to the rank of N -th order tensors. However, unlike Tucker decompo- sition, it cannot accelerate inference because the original tensor must be reconstructed during inference. 3. Decomposed E2E SLU model 3.1. Training overview We investigate the tensor decomposition techniques in E2E SLU models with higher-order tensors as parameters. E2E SLU is a task that estimates semantic label sequences Y directly from input speech feature sequences X. Fig.1 describes the overview of the training procedure of our model. We use two- step sequential distillation to train a small model. Recent SOTA models use self-supervised learning techniques, which have a large number of parameters (over 300M) [28]. However, on- device processing requires a smaller model, typically around 30M or less. Therefore, We do not apply tensor decomposi- tion directly to the large model because the teacher model is too large to minimize within small parameters with sufficient rank. We train small model through a middle-size model to achieve the target size with a compression ratio near 0.3. As shown in the blue boxes in Fig.1, the training procedure has 5 steps. 1. A large teacher model is trained with ground truth labels. We use the E2E Conformer model with Hubert-based feature extractor1 [28], which has achieved the best performance in prior study [29]. 2. The training data is decoded using the trained teacher model to generate teacher labels. 3. The middle model is trained with both the distilled and ground truth labels. The middle model is used as the seed model to miniaturize the final small model using the decom- position, so its structural type is the same as the final small model. 4. Tensor decomposition and SVD techniques are applied to the middle model to reduce the parameters. We set the target size to 15M or 30M as in a previous study [7]. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 5. Finally, the decomposed model is finetuned with distillation to generate the small model. 1https://dl.fbaipublicfiles.com/hubert/hubert large ll60k.pt (8) Table 1: Experimental results on STOP dataset within 15M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small Small 0.300 / 0.295 0.250 / 0.300 0.250 / 0.300 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 200 < 15M 431M 14M 47M 15M 15M 53M 15M 15M \u2014\u2013 / 67.9 68.8 / 69.4 34.7 / 34.8 62.6 / 62.5 65.0 / 65.4 60.9 / 61.4 68.6 / 69.0 69.6 / 70.1 70.4 / 70.9 Table 2: Experimental results on STOP dataset within 30M parameters using Tucker decomposition. Compression ratio encoder / decoder Distillation # Epochs # Parameters Valid/Test EM E2E Deliberation SLU [7] Conformer E-Branchformer Teacher Scratch Middle Small Scratch Middle Small 0.650 / 0.650 0.550 / 0.600 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 50 50 50 50 50 50 < 30M \u2014\u2013 / 73.87 68.8 / 69.4 45.3 / 45.7 62.6 / 62.5 65.4 / 65.7 66.4 / 66.8 68.6 / 69.0 71.0 / 71.4 431M 29M 47M 30M 30M 53M 30M The middle model and small model have Conformer or E- Branchformer encoders. Conformer and E-Branchformer in- clude convolution layers that have 3rd or 4th-order tensor pa- rameters. Thus, they can be efficiently compressed by tensor decomposition techniques such as Tucker decomposition. The number of nodes in the middle of the SVD and the size of the core tensor of the Tucker decomposition are determined from the given compression ratio (eq. (2)). In the case of SVD, the number of middle nodes R can be calculated as follows: 3.2. Sequential distillation R = \u03b3svd IJ I + J . Sequential distillation is used to downsize sequential models such as encoder-decoder models [30]. Frame-by-frame distil- lation is performed by the KL-divergence minimization crite- rion using softmax outputs with temperature hyperparameters. In contrast, sequential distillation uses the labels obtained by in- ference of the teacher model as target labels for student model training. It is also reported to be more effective when extended to N-best from 1-best [31]. However, in our setting, when ex- tended to 5-best, the performance is slightly degraded. Instead, our system uses sequential distillation, which combines the 1- best obtained by the teacher model and ground truth labels. 3.3. Determination of rank from compression ratio On the other hand, since the"}