{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Chenyan_Xiong_Text_Matching_Improves_Sequential_Recommendation_by_Reducing_Popularity_Biases_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What evaluation strategy is used after data processing?", "answer": " Leave-one-out evaluation strategy", "ref_chunk": "After data processing, each example can be a user-item interaction sequence H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc47 }. Then we use the leave-one-out evaluation strategy [7, 52, 58, 71], and separate the processed datasets into training, development, and testing sets. We construct the testing and development sets by using \ud835\udc631,...,\ud835\udc47 \u22121 to predict \ud835\udc63\ud835\udc47 and using \ud835\udc631,...,\ud835\udc47 \u22122 to predict \ud835\udc63\ud835\udc47 \u22121, respectively. For the training set, we fol- low Zhao et al. [70] and Xie et al. [58] to use interaction history \ud835\udc631,...,\ud835\udc56 \u22121 to predict \ud835\udc63\ud835\udc56 , where 1 < \ud835\udc56 < \ud835\udc47 \u2212 1. Evaluation Metrics. We utilize the same evaluation metrics as DIF-SR [58] and use Recall@10/20 and NDCG@10/20 to evaluate the recommendation performance of different models. Statistic significances are tested by permutation test with P< 0.05. During evaluating models, we follow DIF-SR [58] and employ a full ranking testing scenario [12, 32]. Some work evaluates model performance on a small item subset by randomly sampling or sam- pling items according to item popularity, making the evaluation results inconsistent of the same model. Instead of reranking items in the sampled subset, some work [12, 32] builds a more realis- tic recommendation evaluation setting by ranking all items and choosing the top-ranked items as recommendation results. Baselines. Following our main baseline model [58], we com- pare TASTE with several widely used sequential recommendation models. GRU4Rec [24] uses RNN to model user-item interaction sequences for recommendation. SASRec [28] and Bert4Rec [52] employ the self-attention mechanism to capture user preferences from user-item interaction sequences. To better capture relevance 1https://www.yelp.com/dataset 2http://jmcauley.ucsd.edu/data/amazon/ Text Matching Improves Sequential Recommendation by Reducing Popularity Biases CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom Table 2: Overall Performance. We keep the same experimental settings and report the scores of baselines from previous work [58]. Underlined scores are the highest results of baselines. \u2020, \u00a7, \u2021 indicate statistically significant improvements over DIF-SR\u2020, T5-DPR\u00a7 and T5-ID\u2021, respectively. We also show relative improvements over DIF-SR. Dataset Beauty Sports Toys Yelp Metrics Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 GRU4Rec Bert4Rec 0.0530 0.0839 0.0266 0.0344 0.0312 0.0482 0.0157 0.0200 0.0370 0.0588 0.0184 0.0239 0.0361 0.0592 0.0184 0.0243 0.0529 0.0815 0.0237 0.0309 0.0295 0.0465 0.0130 0.0173 0.0533 0.0787 0.0234 0.0297 0.0524 0.0756 0.0327 0.0385 SASRec 0.0828 0.1197 0.0371 0.0464 0.0526 0.0773 0.0233 0.0295 0.0831 0.1168 0.0375 0.0460 0.0650 0.0928 0.0401 0.0471 S3Rec NOVA ICAI-SR DIF-SR T5-DPR 0.0716 0.0868 0.1082 0.1236 0.0345 0.0439 0.0437 0.0531 0.0329 0.0517 0.0554 0.0758 0.0159 0.0249 0.0216 0.0310 0.0805\u2021 0.0967 0.1243\u2021 0.1349 0.0375 0.0475 0.0485 0.0571 0.0460 0.0589 0.0740 0.0902 0.0250\u2021 0.0338 0.0320 0.0416 0.0887 0.1237 0.0439 0.0527 0.0534 0.0759 0.0250 0.0307 0.0978 0.1322 0.0480 0.0567 0.0681 0.0964 0.0412 0.0483 0.0879 0.1231 0.0439 0.0528 0.0527 0.0762 0.0243 0.0302 0.0972 0.1303 0.0478 0.0561 0.0663 0.0940 0.0400 0.0470 0.0908 0.1284 0.0446 0.0541 0.0556 0.0800 0.0264 0.0325 0.1013 0.1382 0.0504 0.0597 0.0698 0.1003 0.0419 0.0496 T5-ID 0.0785\u00a7 0.1138\u00a7 0.0440\u00a7 0.0529\u00a7 0.0464\u00a7 0.0689\u00a7 0.0252\u00a7 0.0308\u00a7 0.0754 0.1065 0.0421\u00a7 0.0499 0.0450 0.0745 0.0235 0.0309 TASTE 0.1030\u2020\u2021\u00a7 0.1550\u2020\u2021\u00a7 0.0517\u2020\u2021\u00a7 0.0649\u2020\u2021\u00a7 0.0633\u2020\u2021\u00a7 0.0964\u2020\u2021\u00a7 0.0338\u2020\u2021\u00a7 0.0421\u2020\u2021\u00a7 0.1232\u2020\u2021\u00a7 0.1789\u2020\u2021\u00a7 0.0640\u2020\u2021\u00a7 0.0780\u2020\u2021\u00a7 0.0738\u2020\u2021\u00a7 0.1156\u2020\u2021\u00a7 0.0397\u2021\u00a7 0.0502\u2021\u00a7 13.44% 20.72% 15.92% 19.96% 13.85% 20.50% 28.03% 29.54% 21.62% 29.45% 26.98% 30.65% 5.73% 15.25% -5.25% 1.21% among attributes, items, and users, S3Rec [71] comes up with four self-supervised methods to pretrain self-attention modules. ICAI- SR [64] is compared, which proposes a heterogeneous graph to represent the relations between items and attributes to model item relevance. Besides, NOVA [38] is also compared, which builds at- tention modules to incorporate item attributes as the side informa- tion. DIF-SR [58] is the previous state-of-the-art, which builds a non-invasive attention mechanism to fuse information from item attributes during modeling user behaviors. 5 EVALUATION RESULT In this section, we first evaluate the recommendation performance of TASTE and conduct ablation studies. Then we study the effective- ness of different item verbalization methods, the ability of TASTE in reducing the popularity bias, the advantages of TASTE in repre- senting long-tail items, and modeling user behaviors using longer user-item interactions. Finally, we present several case studies. Besides, we implement a T5-ID model, which encodes user-item interaction sequences to generate the identifier of the next item [20]. We follow the previous dense retrieval model, DPR [29], to imple- ment T5-DPR, which regards the text sequences of user-item inter- action history and items as queries and documents, encodes them with T5 and is trained with in-batch sampled negatives. Implementation Details. Different from previous work [28, 52, 58], we represent items using full-text sequences (Eq. 1) instead of randomly initialized item embeddings. In our experiments, we use names and addresses to represent locations in the Yelp dataset. And we only use product names to represent shopping products in Amazon Product datasets, because the product name usually contains the item attributes, such as \u201cWAWO 15 Color Professional Makeup Eyeshadow Camouflage Facial Concealer Neutral Palette\u201d contains both category and brand. Our models are implemented with OpenMatch [41, 62]. In our experiments, we truncate the text representations of items by 32 tokens and set the max length of the text representation of user-item interactions to 512. The user-item interaction sequence is split into two subsequences to make the attention sparse in T5-encoder (Eq. 6). TASTE is initialized with the T5-base checkpoint from huggingface transformers [56]. During training, we use Adam optimizer and set learning rate=1e-4, warm up proportion=0.1, and batch size=8. Besides, we use in-batch negatives and randomly sampled negatives to optimize TASTE. We randomly sample 9 negative items for each training example and in-batch train TASTE model. 5.1 Overall Performance The recommendation performance of TASTE is shown in Table 2. Overall, TASTE significantly outperforms baseline models on all datasets by achieving 18% improvements. Compared with item id embedding based recommendation mod- els, e.g. Bert4Rec, TASTE almost doubles its recommendation per- formance, thriving on modeling relevance between users and items through text-matching signals. TASTE also outperforms the previ- ous state-of-the-art recommendation model, DIF-SR, which lever- ages item attributes as side information to help"}, {"question": " How are the processed datasets separated?", "answer": " Into training, development, and testing sets", "ref_chunk": "After data processing, each example can be a user-item interaction sequence H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc47 }. Then we use the leave-one-out evaluation strategy [7, 52, 58, 71], and separate the processed datasets into training, development, and testing sets. We construct the testing and development sets by using \ud835\udc631,...,\ud835\udc47 \u22121 to predict \ud835\udc63\ud835\udc47 and using \ud835\udc631,...,\ud835\udc47 \u22122 to predict \ud835\udc63\ud835\udc47 \u22121, respectively. For the training set, we fol- low Zhao et al. [70] and Xie et al. [58] to use interaction history \ud835\udc631,...,\ud835\udc56 \u22121 to predict \ud835\udc63\ud835\udc56 , where 1 < \ud835\udc56 < \ud835\udc47 \u2212 1. Evaluation Metrics. We utilize the same evaluation metrics as DIF-SR [58] and use Recall@10/20 and NDCG@10/20 to evaluate the recommendation performance of different models. Statistic significances are tested by permutation test with P< 0.05. During evaluating models, we follow DIF-SR [58] and employ a full ranking testing scenario [12, 32]. Some work evaluates model performance on a small item subset by randomly sampling or sam- pling items according to item popularity, making the evaluation results inconsistent of the same model. Instead of reranking items in the sampled subset, some work [12, 32] builds a more realis- tic recommendation evaluation setting by ranking all items and choosing the top-ranked items as recommendation results. Baselines. Following our main baseline model [58], we com- pare TASTE with several widely used sequential recommendation models. GRU4Rec [24] uses RNN to model user-item interaction sequences for recommendation. SASRec [28] and Bert4Rec [52] employ the self-attention mechanism to capture user preferences from user-item interaction sequences. To better capture relevance 1https://www.yelp.com/dataset 2http://jmcauley.ucsd.edu/data/amazon/ Text Matching Improves Sequential Recommendation by Reducing Popularity Biases CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom Table 2: Overall Performance. We keep the same experimental settings and report the scores of baselines from previous work [58]. Underlined scores are the highest results of baselines. \u2020, \u00a7, \u2021 indicate statistically significant improvements over DIF-SR\u2020, T5-DPR\u00a7 and T5-ID\u2021, respectively. We also show relative improvements over DIF-SR. Dataset Beauty Sports Toys Yelp Metrics Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 GRU4Rec Bert4Rec 0.0530 0.0839 0.0266 0.0344 0.0312 0.0482 0.0157 0.0200 0.0370 0.0588 0.0184 0.0239 0.0361 0.0592 0.0184 0.0243 0.0529 0.0815 0.0237 0.0309 0.0295 0.0465 0.0130 0.0173 0.0533 0.0787 0.0234 0.0297 0.0524 0.0756 0.0327 0.0385 SASRec 0.0828 0.1197 0.0371 0.0464 0.0526 0.0773 0.0233 0.0295 0.0831 0.1168 0.0375 0.0460 0.0650 0.0928 0.0401 0.0471 S3Rec NOVA ICAI-SR DIF-SR T5-DPR 0.0716 0.0868 0.1082 0.1236 0.0345 0.0439 0.0437 0.0531 0.0329 0.0517 0.0554 0.0758 0.0159 0.0249 0.0216 0.0310 0.0805\u2021 0.0967 0.1243\u2021 0.1349 0.0375 0.0475 0.0485 0.0571 0.0460 0.0589 0.0740 0.0902 0.0250\u2021 0.0338 0.0320 0.0416 0.0887 0.1237 0.0439 0.0527 0.0534 0.0759 0.0250 0.0307 0.0978 0.1322 0.0480 0.0567 0.0681 0.0964 0.0412 0.0483 0.0879 0.1231 0.0439 0.0528 0.0527 0.0762 0.0243 0.0302 0.0972 0.1303 0.0478 0.0561 0.0663 0.0940 0.0400 0.0470 0.0908 0.1284 0.0446 0.0541 0.0556 0.0800 0.0264 0.0325 0.1013 0.1382 0.0504 0.0597 0.0698 0.1003 0.0419 0.0496 T5-ID 0.0785\u00a7 0.1138\u00a7 0.0440\u00a7 0.0529\u00a7 0.0464\u00a7 0.0689\u00a7 0.0252\u00a7 0.0308\u00a7 0.0754 0.1065 0.0421\u00a7 0.0499 0.0450 0.0745 0.0235 0.0309 TASTE 0.1030\u2020\u2021\u00a7 0.1550\u2020\u2021\u00a7 0.0517\u2020\u2021\u00a7 0.0649\u2020\u2021\u00a7 0.0633\u2020\u2021\u00a7 0.0964\u2020\u2021\u00a7 0.0338\u2020\u2021\u00a7 0.0421\u2020\u2021\u00a7 0.1232\u2020\u2021\u00a7 0.1789\u2020\u2021\u00a7 0.0640\u2020\u2021\u00a7 0.0780\u2020\u2021\u00a7 0.0738\u2020\u2021\u00a7 0.1156\u2020\u2021\u00a7 0.0397\u2021\u00a7 0.0502\u2021\u00a7 13.44% 20.72% 15.92% 19.96% 13.85% 20.50% 28.03% 29.54% 21.62% 29.45% 26.98% 30.65% 5.73% 15.25% -5.25% 1.21% among attributes, items, and users, S3Rec [71] comes up with four self-supervised methods to pretrain self-attention modules. ICAI- SR [64] is compared, which proposes a heterogeneous graph to represent the relations between items and attributes to model item relevance. Besides, NOVA [38] is also compared, which builds at- tention modules to incorporate item attributes as the side informa- tion. DIF-SR [58] is the previous state-of-the-art, which builds a non-invasive attention mechanism to fuse information from item attributes during modeling user behaviors. 5 EVALUATION RESULT In this section, we first evaluate the recommendation performance of TASTE and conduct ablation studies. Then we study the effective- ness of different item verbalization methods, the ability of TASTE in reducing the popularity bias, the advantages of TASTE in repre- senting long-tail items, and modeling user behaviors using longer user-item interactions. Finally, we present several case studies. Besides, we implement a T5-ID model, which encodes user-item interaction sequences to generate the identifier of the next item [20]. We follow the previous dense retrieval model, DPR [29], to imple- ment T5-DPR, which regards the text sequences of user-item inter- action history and items as queries and documents, encodes them with T5 and is trained with in-batch sampled negatives. Implementation Details. Different from previous work [28, 52, 58], we represent items using full-text sequences (Eq. 1) instead of randomly initialized item embeddings. In our experiments, we use names and addresses to represent locations in the Yelp dataset. And we only use product names to represent shopping products in Amazon Product datasets, because the product name usually contains the item attributes, such as \u201cWAWO 15 Color Professional Makeup Eyeshadow Camouflage Facial Concealer Neutral Palette\u201d contains both category and brand. Our models are implemented with OpenMatch [41, 62]. In our experiments, we truncate the text representations of items by 32 tokens and set the max length of the text representation of user-item interactions to 512. The user-item interaction sequence is split into two subsequences to make the attention sparse in T5-encoder (Eq. 6). TASTE is initialized with the T5-base checkpoint from huggingface transformers [56]. During training, we use Adam optimizer and set learning rate=1e-4, warm up proportion=0.1, and batch size=8. Besides, we use in-batch negatives and randomly sampled negatives to optimize TASTE. We randomly sample 9 negative items for each training example and in-batch train TASTE model. 5.1 Overall Performance The recommendation performance of TASTE is shown in Table 2. Overall, TASTE significantly outperforms baseline models on all datasets by achieving 18% improvements. Compared with item id embedding based recommendation mod- els, e.g. Bert4Rec, TASTE almost doubles its recommendation per- formance, thriving on modeling relevance between users and items through text-matching signals. TASTE also outperforms the previ- ous state-of-the-art recommendation model, DIF-SR, which lever- ages item attributes as side information to help"}, {"question": " What interaction history is used for training set prediction?", "answer": " \ud835\udc631,...,\ud835\udc56 \u22121 to predict \ud835\udc63\ud835\udc56, where 1 < \ud835\udc56 < \ud835\udc47 \u2212 1", "ref_chunk": "After data processing, each example can be a user-item interaction sequence H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc47 }. Then we use the leave-one-out evaluation strategy [7, 52, 58, 71], and separate the processed datasets into training, development, and testing sets. We construct the testing and development sets by using \ud835\udc631,...,\ud835\udc47 \u22121 to predict \ud835\udc63\ud835\udc47 and using \ud835\udc631,...,\ud835\udc47 \u22122 to predict \ud835\udc63\ud835\udc47 \u22121, respectively. For the training set, we fol- low Zhao et al. [70] and Xie et al. [58] to use interaction history \ud835\udc631,...,\ud835\udc56 \u22121 to predict \ud835\udc63\ud835\udc56 , where 1 < \ud835\udc56 < \ud835\udc47 \u2212 1. Evaluation Metrics. We utilize the same evaluation metrics as DIF-SR [58] and use Recall@10/20 and NDCG@10/20 to evaluate the recommendation performance of different models. Statistic significances are tested by permutation test with P< 0.05. During evaluating models, we follow DIF-SR [58] and employ a full ranking testing scenario [12, 32]. Some work evaluates model performance on a small item subset by randomly sampling or sam- pling items according to item popularity, making the evaluation results inconsistent of the same model. Instead of reranking items in the sampled subset, some work [12, 32] builds a more realis- tic recommendation evaluation setting by ranking all items and choosing the top-ranked items as recommendation results. Baselines. Following our main baseline model [58], we com- pare TASTE with several widely used sequential recommendation models. GRU4Rec [24] uses RNN to model user-item interaction sequences for recommendation. SASRec [28] and Bert4Rec [52] employ the self-attention mechanism to capture user preferences from user-item interaction sequences. To better capture relevance 1https://www.yelp.com/dataset 2http://jmcauley.ucsd.edu/data/amazon/ Text Matching Improves Sequential Recommendation by Reducing Popularity Biases CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom Table 2: Overall Performance. We keep the same experimental settings and report the scores of baselines from previous work [58]. Underlined scores are the highest results of baselines. \u2020, \u00a7, \u2021 indicate statistically significant improvements over DIF-SR\u2020, T5-DPR\u00a7 and T5-ID\u2021, respectively. We also show relative improvements over DIF-SR. Dataset Beauty Sports Toys Yelp Metrics Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 GRU4Rec Bert4Rec 0.0530 0.0839 0.0266 0.0344 0.0312 0.0482 0.0157 0.0200 0.0370 0.0588 0.0184 0.0239 0.0361 0.0592 0.0184 0.0243 0.0529 0.0815 0.0237 0.0309 0.0295 0.0465 0.0130 0.0173 0.0533 0.0787 0.0234 0.0297 0.0524 0.0756 0.0327 0.0385 SASRec 0.0828 0.1197 0.0371 0.0464 0.0526 0.0773 0.0233 0.0295 0.0831 0.1168 0.0375 0.0460 0.0650 0.0928 0.0401 0.0471 S3Rec NOVA ICAI-SR DIF-SR T5-DPR 0.0716 0.0868 0.1082 0.1236 0.0345 0.0439 0.0437 0.0531 0.0329 0.0517 0.0554 0.0758 0.0159 0.0249 0.0216 0.0310 0.0805\u2021 0.0967 0.1243\u2021 0.1349 0.0375 0.0475 0.0485 0.0571 0.0460 0.0589 0.0740 0.0902 0.0250\u2021 0.0338 0.0320 0.0416 0.0887 0.1237 0.0439 0.0527 0.0534 0.0759 0.0250 0.0307 0.0978 0.1322 0.0480 0.0567 0.0681 0.0964 0.0412 0.0483 0.0879 0.1231 0.0439 0.0528 0.0527 0.0762 0.0243 0.0302 0.0972 0.1303 0.0478 0.0561 0.0663 0.0940 0.0400 0.0470 0.0908 0.1284 0.0446 0.0541 0.0556 0.0800 0.0264 0.0325 0.1013 0.1382 0.0504 0.0597 0.0698 0.1003 0.0419 0.0496 T5-ID 0.0785\u00a7 0.1138\u00a7 0.0440\u00a7 0.0529\u00a7 0.0464\u00a7 0.0689\u00a7 0.0252\u00a7 0.0308\u00a7 0.0754 0.1065 0.0421\u00a7 0.0499 0.0450 0.0745 0.0235 0.0309 TASTE 0.1030\u2020\u2021\u00a7 0.1550\u2020\u2021\u00a7 0.0517\u2020\u2021\u00a7 0.0649\u2020\u2021\u00a7 0.0633\u2020\u2021\u00a7 0.0964\u2020\u2021\u00a7 0.0338\u2020\u2021\u00a7 0.0421\u2020\u2021\u00a7 0.1232\u2020\u2021\u00a7 0.1789\u2020\u2021\u00a7 0.0640\u2020\u2021\u00a7 0.0780\u2020\u2021\u00a7 0.0738\u2020\u2021\u00a7 0.1156\u2020\u2021\u00a7 0.0397\u2021\u00a7 0.0502\u2021\u00a7 13.44% 20.72% 15.92% 19.96% 13.85% 20.50% 28.03% 29.54% 21.62% 29.45% 26.98% 30.65% 5.73% 15.25% -5.25% 1.21% among attributes, items, and users, S3Rec [71] comes up with four self-supervised methods to pretrain self-attention modules. ICAI- SR [64] is compared, which proposes a heterogeneous graph to represent the relations between items and attributes to model item relevance. Besides, NOVA [38] is also compared, which builds at- tention modules to incorporate item attributes as the side informa- tion. DIF-SR [58] is the previous state-of-the-art, which builds a non-invasive attention mechanism to fuse information from item attributes during modeling user behaviors. 5 EVALUATION RESULT In this section, we first evaluate the recommendation performance of TASTE and conduct ablation studies. Then we study the effective- ness of different item verbalization methods, the ability of TASTE in reducing the popularity bias, the advantages of TASTE in repre- senting long-tail items, and modeling user behaviors using longer user-item interactions. Finally, we present several case studies. Besides, we implement a T5-ID model, which encodes user-item interaction sequences to generate the identifier of the next item [20]. We follow the previous dense retrieval model, DPR [29], to imple- ment T5-DPR, which regards the text sequences of user-item inter- action history and items as queries and documents, encodes them with T5 and is trained with in-batch sampled negatives. Implementation Details. Different from previous work [28, 52, 58], we represent items using full-text sequences (Eq. 1) instead of randomly initialized item embeddings. In our experiments, we use names and addresses to represent locations in the Yelp dataset. And we only use product names to represent shopping products in Amazon Product datasets, because the product name usually contains the item attributes, such as \u201cWAWO 15 Color Professional Makeup Eyeshadow Camouflage Facial Concealer Neutral Palette\u201d contains both category and brand. Our models are implemented with OpenMatch [41, 62]. In our experiments, we truncate the text representations of items by 32 tokens and set the max length of the text representation of user-item interactions to 512. The user-item interaction sequence is split into two subsequences to make the attention sparse in T5-encoder (Eq. 6). TASTE is initialized with the T5-base checkpoint from huggingface transformers [56]. During training, we use Adam optimizer and set learning rate=1e-4, warm up proportion=0.1, and batch size=8. Besides, we use in-batch negatives and randomly sampled negatives to optimize TASTE. We randomly sample 9 negative items for each training example and in-batch train TASTE model. 5.1 Overall Performance The recommendation performance of TASTE is shown in Table 2. Overall, TASTE significantly outperforms baseline models on all datasets by achieving 18% improvements. Compared with item id embedding based recommendation mod- els, e.g. Bert4Rec, TASTE almost doubles its recommendation per- formance, thriving on modeling relevance between users and items through text-matching signals. TASTE also outperforms the previ- ous state-of-the-art recommendation model, DIF-SR, which lever- ages item attributes as side information to help"}, {"question": " What evaluation metrics are used to evaluate the recommendation performance of different models?", "answer": " Recall@10/20 and NDCG@10/20", "ref_chunk": "After data processing, each example can be a user-item interaction sequence H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc47 }. Then we use the leave-one-out evaluation strategy [7, 52, 58, 71], and separate the processed datasets into training, development, and testing sets. We construct the testing and development sets by using \ud835\udc631,...,\ud835\udc47 \u22121 to predict \ud835\udc63\ud835\udc47 and using \ud835\udc631,...,\ud835\udc47 \u22122 to predict \ud835\udc63\ud835\udc47 \u22121, respectively. For the training set, we fol- low Zhao et al. [70] and Xie et al. [58] to use interaction history \ud835\udc631,...,\ud835\udc56 \u22121 to predict \ud835\udc63\ud835\udc56 , where 1 < \ud835\udc56 < \ud835\udc47 \u2212 1. Evaluation Metrics. We utilize the same evaluation metrics as DIF-SR [58] and use Recall@10/20 and NDCG@10/20 to evaluate the recommendation performance of different models. Statistic significances are tested by permutation test with P< 0.05. During evaluating models, we follow DIF-SR [58] and employ a full ranking testing scenario [12, 32]. Some work evaluates model performance on a small item subset by randomly sampling or sam- pling items according to item popularity, making the evaluation results inconsistent of the same model. Instead of reranking items in the sampled subset, some work [12, 32] builds a more realis- tic recommendation evaluation setting by ranking all items and choosing the top-ranked items as recommendation results. Baselines. Following our main baseline model [58], we com- pare TASTE with several widely used sequential recommendation models. GRU4Rec [24] uses RNN to model user-item interaction sequences for recommendation. SASRec [28] and Bert4Rec [52] employ the self-attention mechanism to capture user preferences from user-item interaction sequences. To better capture relevance 1https://www.yelp.com/dataset 2http://jmcauley.ucsd.edu/data/amazon/ Text Matching Improves Sequential Recommendation by Reducing Popularity Biases CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom Table 2: Overall Performance. We keep the same experimental settings and report the scores of baselines from previous work [58]. Underlined scores are the highest results of baselines. \u2020, \u00a7, \u2021 indicate statistically significant improvements over DIF-SR\u2020, T5-DPR\u00a7 and T5-ID\u2021, respectively. We also show relative improvements over DIF-SR. Dataset Beauty Sports Toys Yelp Metrics Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 GRU4Rec Bert4Rec 0.0530 0.0839 0.0266 0.0344 0.0312 0.0482 0.0157 0.0200 0.0370 0.0588 0.0184 0.0239 0.0361 0.0592 0.0184 0.0243 0.0529 0.0815 0.0237 0.0309 0.0295 0.0465 0.0130 0.0173 0.0533 0.0787 0.0234 0.0297 0.0524 0.0756 0.0327 0.0385 SASRec 0.0828 0.1197 0.0371 0.0464 0.0526 0.0773 0.0233 0.0295 0.0831 0.1168 0.0375 0.0460 0.0650 0.0928 0.0401 0.0471 S3Rec NOVA ICAI-SR DIF-SR T5-DPR 0.0716 0.0868 0.1082 0.1236 0.0345 0.0439 0.0437 0.0531 0.0329 0.0517 0.0554 0.0758 0.0159 0.0249 0.0216 0.0310 0.0805\u2021 0.0967 0.1243\u2021 0.1349 0.0375 0.0475 0.0485 0.0571 0.0460 0.0589 0.0740 0.0902 0.0250\u2021 0.0338 0.0320 0.0416 0.0887 0.1237 0.0439 0.0527 0.0534 0.0759 0.0250 0.0307 0.0978 0.1322 0.0480 0.0567 0.0681 0.0964 0.0412 0.0483 0.0879 0.1231 0.0439 0.0528 0.0527 0.0762 0.0243 0.0302 0.0972 0.1303 0.0478 0.0561 0.0663 0.0940 0.0400 0.0470 0.0908 0.1284 0.0446 0.0541 0.0556 0.0800 0.0264 0.0325 0.1013 0.1382 0.0504 0.0597 0.0698 0.1003 0.0419 0.0496 T5-ID 0.0785\u00a7 0.1138\u00a7 0.0440\u00a7 0.0529\u00a7 0.0464\u00a7 0.0689\u00a7 0.0252\u00a7 0.0308\u00a7 0.0754 0.1065 0.0421\u00a7 0.0499 0.0450 0.0745 0.0235 0.0309 TASTE 0.1030\u2020\u2021\u00a7 0.1550\u2020\u2021\u00a7 0.0517\u2020\u2021\u00a7 0.0649\u2020\u2021\u00a7 0.0633\u2020\u2021\u00a7 0.0964\u2020\u2021\u00a7 0.0338\u2020\u2021\u00a7 0.0421\u2020\u2021\u00a7 0.1232\u2020\u2021\u00a7 0.1789\u2020\u2021\u00a7 0.0640\u2020\u2021\u00a7 0.0780\u2020\u2021\u00a7 0.0738\u2020\u2021\u00a7 0.1156\u2020\u2021\u00a7 0.0397\u2021\u00a7 0.0502\u2021\u00a7 13.44% 20.72% 15.92% 19.96% 13.85% 20.50% 28.03% 29.54% 21.62% 29.45% 26.98% 30.65% 5.73% 15.25% -5.25% 1.21% among attributes, items, and users, S3Rec [71] comes up with four self-supervised methods to pretrain self-attention modules. ICAI- SR [64] is compared, which proposes a heterogeneous graph to represent the relations between items and attributes to model item relevance. Besides, NOVA [38] is also compared, which builds at- tention modules to incorporate item attributes as the side informa- tion. DIF-SR [58] is the previous state-of-the-art, which builds a non-invasive attention mechanism to fuse information from item attributes during modeling user behaviors. 5 EVALUATION RESULT In this section, we first evaluate the recommendation performance of TASTE and conduct ablation studies. Then we study the effective- ness of different item verbalization methods, the ability of TASTE in reducing the popularity bias, the advantages of TASTE in repre- senting long-tail items, and modeling user behaviors using longer user-item interactions. Finally, we present several case studies. Besides, we implement a T5-ID model, which encodes user-item interaction sequences to generate the identifier of the next item [20]. We follow the previous dense retrieval model, DPR [29], to imple- ment T5-DPR, which regards the text sequences of user-item inter- action history and items as queries and documents, encodes them with T5 and is trained with in-batch sampled negatives. Implementation Details. Different from previous work [28, 52, 58], we represent items using full-text sequences (Eq. 1) instead of randomly initialized item embeddings. In our experiments, we use names and addresses to represent locations in the Yelp dataset. And we only use product names to represent shopping products in Amazon Product datasets, because the product name usually contains the item attributes, such as \u201cWAWO 15 Color Professional Makeup Eyeshadow Camouflage Facial Concealer Neutral Palette\u201d contains both category and brand. Our models are implemented with OpenMatch [41, 62]. In our experiments, we truncate the text representations of items by 32 tokens and set the max length of the text representation of user-item interactions to 512. The user-item interaction sequence is split into two subsequences to make the attention sparse in T5-encoder (Eq. 6). TASTE is initialized with the T5-base checkpoint from huggingface transformers [56]. During training, we use Adam optimizer and set learning rate=1e-4, warm up proportion=0.1, and batch size=8. Besides, we use in-batch negatives and randomly sampled negatives to optimize TASTE. We randomly sample 9 negative items for each training example and in-batch train TASTE model. 5.1 Overall Performance The recommendation performance of TASTE is shown in Table 2. Overall, TASTE significantly outperforms baseline models on all datasets by achieving 18% improvements. Compared with item id embedding based recommendation mod- els, e.g. Bert4Rec, TASTE almost doubles its recommendation per- formance, thriving on modeling relevance between users and items through text-matching signals. TASTE also outperforms the previ- ous state-of-the-art recommendation model, DIF-SR, which lever- ages item attributes as side information to help"}, {"question": " How are statistic significances tested during evaluation?", "answer": " By permutation test with P< 0.05", "ref_chunk": "After data processing, each example can be a user-item interaction sequence H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc47 }. Then we use the leave-one-out evaluation strategy [7, 52, 58, 71], and separate the processed datasets into training, development, and testing sets. We construct the testing and development sets by using \ud835\udc631,...,\ud835\udc47 \u22121 to predict \ud835\udc63\ud835\udc47 and using \ud835\udc631,...,\ud835\udc47 \u22122 to predict \ud835\udc63\ud835\udc47 \u22121, respectively. For the training set, we fol- low Zhao et al. [70] and Xie et al. [58] to use interaction history \ud835\udc631,...,\ud835\udc56 \u22121 to predict \ud835\udc63\ud835\udc56 , where 1 < \ud835\udc56 < \ud835\udc47 \u2212 1. Evaluation Metrics. We utilize the same evaluation metrics as DIF-SR [58] and use Recall@10/20 and NDCG@10/20 to evaluate the recommendation performance of different models. Statistic significances are tested by permutation test with P< 0.05. During evaluating models, we follow DIF-SR [58] and employ a full ranking testing scenario [12, 32]. Some work evaluates model performance on a small item subset by randomly sampling or sam- pling items according to item popularity, making the evaluation results inconsistent of the same model. Instead of reranking items in the sampled subset, some work [12, 32] builds a more realis- tic recommendation evaluation setting by ranking all items and choosing the top-ranked items as recommendation results. Baselines. Following our main baseline model [58], we com- pare TASTE with several widely used sequential recommendation models. GRU4Rec [24] uses RNN to model user-item interaction sequences for recommendation. SASRec [28] and Bert4Rec [52] employ the self-attention mechanism to capture user preferences from user-item interaction sequences. To better capture relevance 1https://www.yelp.com/dataset 2http://jmcauley.ucsd.edu/data/amazon/ Text Matching Improves Sequential Recommendation by Reducing Popularity Biases CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom Table 2: Overall Performance. We keep the same experimental settings and report the scores of baselines from previous work [58]. Underlined scores are the highest results of baselines. \u2020, \u00a7, \u2021 indicate statistically significant improvements over DIF-SR\u2020, T5-DPR\u00a7 and T5-ID\u2021, respectively. We also show relative improvements over DIF-SR. Dataset Beauty Sports Toys Yelp Metrics Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 GRU4Rec Bert4Rec 0.0530 0.0839 0.0266 0.0344 0.0312 0.0482 0.0157 0.0200 0.0370 0.0588 0.0184 0.0239 0.0361 0.0592 0.0184 0.0243 0.0529 0.0815 0.0237 0.0309 0.0295 0.0465 0.0130 0.0173 0.0533 0.0787 0.0234 0.0297 0.0524 0.0756 0.0327 0.0385 SASRec 0.0828 0.1197 0.0371 0.0464 0.0526 0.0773 0.0233 0.0295 0.0831 0.1168 0.0375 0.0460 0.0650 0.0928 0.0401 0.0471 S3Rec NOVA ICAI-SR DIF-SR T5-DPR 0.0716 0.0868 0.1082 0.1236 0.0345 0.0439 0.0437 0.0531 0.0329 0.0517 0.0554 0.0758 0.0159 0.0249 0.0216 0.0310 0.0805\u2021 0.0967 0.1243\u2021 0.1349 0.0375 0.0475 0.0485 0.0571 0.0460 0.0589 0.0740 0.0902 0.0250\u2021 0.0338 0.0320 0.0416 0.0887 0.1237 0.0439 0.0527 0.0534 0.0759 0.0250 0.0307 0.0978 0.1322 0.0480 0.0567 0.0681 0.0964 0.0412 0.0483 0.0879 0.1231 0.0439 0.0528 0.0527 0.0762 0.0243 0.0302 0.0972 0.1303 0.0478 0.0561 0.0663 0.0940 0.0400 0.0470 0.0908 0.1284 0.0446 0.0541 0.0556 0.0800 0.0264 0.0325 0.1013 0.1382 0.0504 0.0597 0.0698 0.1003 0.0419 0.0496 T5-ID 0.0785\u00a7 0.1138\u00a7 0.0440\u00a7 0.0529\u00a7 0.0464\u00a7 0.0689\u00a7 0.0252\u00a7 0.0308\u00a7 0.0754 0.1065 0.0421\u00a7 0.0499 0.0450 0.0745 0.0235 0.0309 TASTE 0.1030\u2020\u2021\u00a7 0.1550\u2020\u2021\u00a7 0.0517\u2020\u2021\u00a7 0.0649\u2020\u2021\u00a7 0.0633\u2020\u2021\u00a7 0.0964\u2020\u2021\u00a7 0.0338\u2020\u2021\u00a7 0.0421\u2020\u2021\u00a7 0.1232\u2020\u2021\u00a7 0.1789\u2020\u2021\u00a7 0.0640\u2020\u2021\u00a7 0.0780\u2020\u2021\u00a7 0.0738\u2020\u2021\u00a7 0.1156\u2020\u2021\u00a7 0.0397\u2021\u00a7 0.0502\u2021\u00a7 13.44% 20.72% 15.92% 19.96% 13.85% 20.50% 28.03% 29.54% 21.62% 29.45% 26.98% 30.65% 5.73% 15.25% -5.25% 1.21% among attributes, items, and users, S3Rec [71] comes up with four self-supervised methods to pretrain self-attention modules. ICAI- SR [64] is compared, which proposes a heterogeneous graph to represent the relations between items and attributes to model item relevance. Besides, NOVA [38] is also compared, which builds at- tention modules to incorporate item attributes as the side informa- tion. DIF-SR [58] is the previous state-of-the-art, which builds a non-invasive attention mechanism to fuse information from item attributes during modeling user behaviors. 5 EVALUATION RESULT In this section, we first evaluate the recommendation performance of TASTE and conduct ablation studies. Then we study the effective- ness of different item verbalization methods, the ability of TASTE in reducing the popularity bias, the advantages of TASTE in repre- senting long-tail items, and modeling user behaviors using longer user-item interactions. Finally, we present several case studies. Besides, we implement a T5-ID model, which encodes user-item interaction sequences to generate the identifier of the next item [20]. We follow the previous dense retrieval model, DPR [29], to imple- ment T5-DPR, which regards the text sequences of user-item inter- action history and items as queries and documents, encodes them with T5 and is trained with in-batch sampled negatives. Implementation Details. Different from previous work [28, 52, 58], we represent items using full-text sequences (Eq. 1) instead of randomly initialized item embeddings. In our experiments, we use names and addresses to represent locations in the Yelp dataset. And we only use product names to represent shopping products in Amazon Product datasets, because the product name usually contains the item attributes, such as \u201cWAWO 15 Color Professional Makeup Eyeshadow Camouflage Facial Concealer Neutral Palette\u201d contains both category and brand. Our models are implemented with OpenMatch [41, 62]. In our experiments, we truncate the text representations of items by 32 tokens and set the max length of the text representation of user-item interactions to 512. The user-item interaction sequence is split into two subsequences to make the attention sparse in T5-encoder (Eq. 6). TASTE is initialized with the T5-base checkpoint from huggingface transformers [56]. During training, we use Adam optimizer and set learning rate=1e-4, warm up proportion=0.1, and batch size=8. Besides, we use in-batch negatives and randomly sampled negatives to optimize TASTE. We randomly sample 9 negative items for each training example and in-batch train TASTE model. 5.1 Overall Performance The recommendation performance of TASTE is shown in Table 2. Overall, TASTE significantly outperforms baseline models on all datasets by achieving 18% improvements. Compared with item id embedding based recommendation mod- els, e.g. Bert4Rec, TASTE almost doubles its recommendation per- formance, thriving on modeling relevance between users and items through text-matching signals. TASTE also outperforms the previ- ous state-of-the-art recommendation model, DIF-SR, which lever- ages item attributes as side information to help"}, {"question": " What method does GRU4Rec use to model user-item interaction sequences for recommendation?", "answer": " RNN (Recurrent Neural Network)", "ref_chunk": "After data processing, each example can be a user-item interaction sequence H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc47 }. Then we use the leave-one-out evaluation strategy [7, 52, 58, 71], and separate the processed datasets into training, development, and testing sets. We construct the testing and development sets by using \ud835\udc631,...,\ud835\udc47 \u22121 to predict \ud835\udc63\ud835\udc47 and using \ud835\udc631,...,\ud835\udc47 \u22122 to predict \ud835\udc63\ud835\udc47 \u22121, respectively. For the training set, we fol- low Zhao et al. [70] and Xie et al. [58] to use interaction history \ud835\udc631,...,\ud835\udc56 \u22121 to predict \ud835\udc63\ud835\udc56 , where 1 < \ud835\udc56 < \ud835\udc47 \u2212 1. Evaluation Metrics. We utilize the same evaluation metrics as DIF-SR [58] and use Recall@10/20 and NDCG@10/20 to evaluate the recommendation performance of different models. Statistic significances are tested by permutation test with P< 0.05. During evaluating models, we follow DIF-SR [58] and employ a full ranking testing scenario [12, 32]. Some work evaluates model performance on a small item subset by randomly sampling or sam- pling items according to item popularity, making the evaluation results inconsistent of the same model. Instead of reranking items in the sampled subset, some work [12, 32] builds a more realis- tic recommendation evaluation setting by ranking all items and choosing the top-ranked items as recommendation results. Baselines. Following our main baseline model [58], we com- pare TASTE with several widely used sequential recommendation models. GRU4Rec [24] uses RNN to model user-item interaction sequences for recommendation. SASRec [28] and Bert4Rec [52] employ the self-attention mechanism to capture user preferences from user-item interaction sequences. To better capture relevance 1https://www.yelp.com/dataset 2http://jmcauley.ucsd.edu/data/amazon/ Text Matching Improves Sequential Recommendation by Reducing Popularity Biases CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom Table 2: Overall Performance. We keep the same experimental settings and report the scores of baselines from previous work [58]. Underlined scores are the highest results of baselines. \u2020, \u00a7, \u2021 indicate statistically significant improvements over DIF-SR\u2020, T5-DPR\u00a7 and T5-ID\u2021, respectively. We also show relative improvements over DIF-SR. Dataset Beauty Sports Toys Yelp Metrics Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 GRU4Rec Bert4Rec 0.0530 0.0839 0.0266 0.0344 0.0312 0.0482 0.0157 0.0200 0.0370 0.0588 0.0184 0.0239 0.0361 0.0592 0.0184 0.0243 0.0529 0.0815 0.0237 0.0309 0.0295 0.0465 0.0130 0.0173 0.0533 0.0787 0.0234 0.0297 0.0524 0.0756 0.0327 0.0385 SASRec 0.0828 0.1197 0.0371 0.0464 0.0526 0.0773 0.0233 0.0295 0.0831 0.1168 0.0375 0.0460 0.0650 0.0928 0.0401 0.0471 S3Rec NOVA ICAI-SR DIF-SR T5-DPR 0.0716 0.0868 0.1082 0.1236 0.0345 0.0439 0.0437 0.0531 0.0329 0.0517 0.0554 0.0758 0.0159 0.0249 0.0216 0.0310 0.0805\u2021 0.0967 0.1243\u2021 0.1349 0.0375 0.0475 0.0485 0.0571 0.0460 0.0589 0.0740 0.0902 0.0250\u2021 0.0338 0.0320 0.0416 0.0887 0.1237 0.0439 0.0527 0.0534 0.0759 0.0250 0.0307 0.0978 0.1322 0.0480 0.0567 0.0681 0.0964 0.0412 0.0483 0.0879 0.1231 0.0439 0.0528 0.0527 0.0762 0.0243 0.0302 0.0972 0.1303 0.0478 0.0561 0.0663 0.0940 0.0400 0.0470 0.0908 0.1284 0.0446 0.0541 0.0556 0.0800 0.0264 0.0325 0.1013 0.1382 0.0504 0.0597 0.0698 0.1003 0.0419 0.0496 T5-ID 0.0785\u00a7 0.1138\u00a7 0.0440\u00a7 0.0529\u00a7 0.0464\u00a7 0.0689\u00a7 0.0252\u00a7 0.0308\u00a7 0.0754 0.1065 0.0421\u00a7 0.0499 0.0450 0.0745 0.0235 0.0309 TASTE 0.1030\u2020\u2021\u00a7 0.1550\u2020\u2021\u00a7 0.0517\u2020\u2021\u00a7 0.0649\u2020\u2021\u00a7 0.0633\u2020\u2021\u00a7 0.0964\u2020\u2021\u00a7 0.0338\u2020\u2021\u00a7 0.0421\u2020\u2021\u00a7 0.1232\u2020\u2021\u00a7 0.1789\u2020\u2021\u00a7 0.0640\u2020\u2021\u00a7 0.0780\u2020\u2021\u00a7 0.0738\u2020\u2021\u00a7 0.1156\u2020\u2021\u00a7 0.0397\u2021\u00a7 0.0502\u2021\u00a7 13.44% 20.72% 15.92% 19.96% 13.85% 20.50% 28.03% 29.54% 21.62% 29.45% 26.98% 30.65% 5.73% 15.25% -5.25% 1.21% among attributes, items, and users, S3Rec [71] comes up with four self-supervised methods to pretrain self-attention modules. ICAI- SR [64] is compared, which proposes a heterogeneous graph to represent the relations between items and attributes to model item relevance. Besides, NOVA [38] is also compared, which builds at- tention modules to incorporate item attributes as the side informa- tion. DIF-SR [58] is the previous state-of-the-art, which builds a non-invasive attention mechanism to fuse information from item attributes during modeling user behaviors. 5 EVALUATION RESULT In this section, we first evaluate the recommendation performance of TASTE and conduct ablation studies. Then we study the effective- ness of different item verbalization methods, the ability of TASTE in reducing the popularity bias, the advantages of TASTE in repre- senting long-tail items, and modeling user behaviors using longer user-item interactions. Finally, we present several case studies. Besides, we implement a T5-ID model, which encodes user-item interaction sequences to generate the identifier of the next item [20]. We follow the previous dense retrieval model, DPR [29], to imple- ment T5-DPR, which regards the text sequences of user-item inter- action history and items as queries and documents, encodes them with T5 and is trained with in-batch sampled negatives. Implementation Details. Different from previous work [28, 52, 58], we represent items using full-text sequences (Eq. 1) instead of randomly initialized item embeddings. In our experiments, we use names and addresses to represent locations in the Yelp dataset. And we only use product names to represent shopping products in Amazon Product datasets, because the product name usually contains the item attributes, such as \u201cWAWO 15 Color Professional Makeup Eyeshadow Camouflage Facial Concealer Neutral Palette\u201d contains both category and brand. Our models are implemented with OpenMatch [41, 62]. In our experiments, we truncate the text representations of items by 32 tokens and set the max length of the text representation of user-item interactions to 512. The user-item interaction sequence is split into two subsequences to make the attention sparse in T5-encoder (Eq. 6). TASTE is initialized with the T5-base checkpoint from huggingface transformers [56]. During training, we use Adam optimizer and set learning rate=1e-4, warm up proportion=0.1, and batch size=8. Besides, we use in-batch negatives and randomly sampled negatives to optimize TASTE. We randomly sample 9 negative items for each training example and in-batch train TASTE model. 5.1 Overall Performance The recommendation performance of TASTE is shown in Table 2. Overall, TASTE significantly outperforms baseline models on all datasets by achieving 18% improvements. Compared with item id embedding based recommendation mod- els, e.g. Bert4Rec, TASTE almost doubles its recommendation per- formance, thriving on modeling relevance between users and items through text-matching signals. TASTE also outperforms the previ- ous state-of-the-art recommendation model, DIF-SR, which lever- ages item attributes as side information to help"}, {"question": " What is the main focus of SASRec and Bert4Rec?", "answer": " To capture user preferences from user-item interaction sequences using the self-attention mechanism", "ref_chunk": "After data processing, each example can be a user-item interaction sequence H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc47 }. Then we use the leave-one-out evaluation strategy [7, 52, 58, 71], and separate the processed datasets into training, development, and testing sets. We construct the testing and development sets by using \ud835\udc631,...,\ud835\udc47 \u22121 to predict \ud835\udc63\ud835\udc47 and using \ud835\udc631,...,\ud835\udc47 \u22122 to predict \ud835\udc63\ud835\udc47 \u22121, respectively. For the training set, we fol- low Zhao et al. [70] and Xie et al. [58] to use interaction history \ud835\udc631,...,\ud835\udc56 \u22121 to predict \ud835\udc63\ud835\udc56 , where 1 < \ud835\udc56 < \ud835\udc47 \u2212 1. Evaluation Metrics. We utilize the same evaluation metrics as DIF-SR [58] and use Recall@10/20 and NDCG@10/20 to evaluate the recommendation performance of different models. Statistic significances are tested by permutation test with P< 0.05. During evaluating models, we follow DIF-SR [58] and employ a full ranking testing scenario [12, 32]. Some work evaluates model performance on a small item subset by randomly sampling or sam- pling items according to item popularity, making the evaluation results inconsistent of the same model. Instead of reranking items in the sampled subset, some work [12, 32] builds a more realis- tic recommendation evaluation setting by ranking all items and choosing the top-ranked items as recommendation results. Baselines. Following our main baseline model [58], we com- pare TASTE with several widely used sequential recommendation models. GRU4Rec [24] uses RNN to model user-item interaction sequences for recommendation. SASRec [28] and Bert4Rec [52] employ the self-attention mechanism to capture user preferences from user-item interaction sequences. To better capture relevance 1https://www.yelp.com/dataset 2http://jmcauley.ucsd.edu/data/amazon/ Text Matching Improves Sequential Recommendation by Reducing Popularity Biases CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom Table 2: Overall Performance. We keep the same experimental settings and report the scores of baselines from previous work [58]. Underlined scores are the highest results of baselines. \u2020, \u00a7, \u2021 indicate statistically significant improvements over DIF-SR\u2020, T5-DPR\u00a7 and T5-ID\u2021, respectively. We also show relative improvements over DIF-SR. Dataset Beauty Sports Toys Yelp Metrics Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 GRU4Rec Bert4Rec 0.0530 0.0839 0.0266 0.0344 0.0312 0.0482 0.0157 0.0200 0.0370 0.0588 0.0184 0.0239 0.0361 0.0592 0.0184 0.0243 0.0529 0.0815 0.0237 0.0309 0.0295 0.0465 0.0130 0.0173 0.0533 0.0787 0.0234 0.0297 0.0524 0.0756 0.0327 0.0385 SASRec 0.0828 0.1197 0.0371 0.0464 0.0526 0.0773 0.0233 0.0295 0.0831 0.1168 0.0375 0.0460 0.0650 0.0928 0.0401 0.0471 S3Rec NOVA ICAI-SR DIF-SR T5-DPR 0.0716 0.0868 0.1082 0.1236 0.0345 0.0439 0.0437 0.0531 0.0329 0.0517 0.0554 0.0758 0.0159 0.0249 0.0216 0.0310 0.0805\u2021 0.0967 0.1243\u2021 0.1349 0.0375 0.0475 0.0485 0.0571 0.0460 0.0589 0.0740 0.0902 0.0250\u2021 0.0338 0.0320 0.0416 0.0887 0.1237 0.0439 0.0527 0.0534 0.0759 0.0250 0.0307 0.0978 0.1322 0.0480 0.0567 0.0681 0.0964 0.0412 0.0483 0.0879 0.1231 0.0439 0.0528 0.0527 0.0762 0.0243 0.0302 0.0972 0.1303 0.0478 0.0561 0.0663 0.0940 0.0400 0.0470 0.0908 0.1284 0.0446 0.0541 0.0556 0.0800 0.0264 0.0325 0.1013 0.1382 0.0504 0.0597 0.0698 0.1003 0.0419 0.0496 T5-ID 0.0785\u00a7 0.1138\u00a7 0.0440\u00a7 0.0529\u00a7 0.0464\u00a7 0.0689\u00a7 0.0252\u00a7 0.0308\u00a7 0.0754 0.1065 0.0421\u00a7 0.0499 0.0450 0.0745 0.0235 0.0309 TASTE 0.1030\u2020\u2021\u00a7 0.1550\u2020\u2021\u00a7 0.0517\u2020\u2021\u00a7 0.0649\u2020\u2021\u00a7 0.0633\u2020\u2021\u00a7 0.0964\u2020\u2021\u00a7 0.0338\u2020\u2021\u00a7 0.0421\u2020\u2021\u00a7 0.1232\u2020\u2021\u00a7 0.1789\u2020\u2021\u00a7 0.0640\u2020\u2021\u00a7 0.0780\u2020\u2021\u00a7 0.0738\u2020\u2021\u00a7 0.1156\u2020\u2021\u00a7 0.0397\u2021\u00a7 0.0502\u2021\u00a7 13.44% 20.72% 15.92% 19.96% 13.85% 20.50% 28.03% 29.54% 21.62% 29.45% 26.98% 30.65% 5.73% 15.25% -5.25% 1.21% among attributes, items, and users, S3Rec [71] comes up with four self-supervised methods to pretrain self-attention modules. ICAI- SR [64] is compared, which proposes a heterogeneous graph to represent the relations between items and attributes to model item relevance. Besides, NOVA [38] is also compared, which builds at- tention modules to incorporate item attributes as the side informa- tion. DIF-SR [58] is the previous state-of-the-art, which builds a non-invasive attention mechanism to fuse information from item attributes during modeling user behaviors. 5 EVALUATION RESULT In this section, we first evaluate the recommendation performance of TASTE and conduct ablation studies. Then we study the effective- ness of different item verbalization methods, the ability of TASTE in reducing the popularity bias, the advantages of TASTE in repre- senting long-tail items, and modeling user behaviors using longer user-item interactions. Finally, we present several case studies. Besides, we implement a T5-ID model, which encodes user-item interaction sequences to generate the identifier of the next item [20]. We follow the previous dense retrieval model, DPR [29], to imple- ment T5-DPR, which regards the text sequences of user-item inter- action history and items as queries and documents, encodes them with T5 and is trained with in-batch sampled negatives. Implementation Details. Different from previous work [28, 52, 58], we represent items using full-text sequences (Eq. 1) instead of randomly initialized item embeddings. In our experiments, we use names and addresses to represent locations in the Yelp dataset. And we only use product names to represent shopping products in Amazon Product datasets, because the product name usually contains the item attributes, such as \u201cWAWO 15 Color Professional Makeup Eyeshadow Camouflage Facial Concealer Neutral Palette\u201d contains both category and brand. Our models are implemented with OpenMatch [41, 62]. In our experiments, we truncate the text representations of items by 32 tokens and set the max length of the text representation of user-item interactions to 512. The user-item interaction sequence is split into two subsequences to make the attention sparse in T5-encoder (Eq. 6). TASTE is initialized with the T5-base checkpoint from huggingface transformers [56]. During training, we use Adam optimizer and set learning rate=1e-4, warm up proportion=0.1, and batch size=8. Besides, we use in-batch negatives and randomly sampled negatives to optimize TASTE. We randomly sample 9 negative items for each training example and in-batch train TASTE model. 5.1 Overall Performance The recommendation performance of TASTE is shown in Table 2. Overall, TASTE significantly outperforms baseline models on all datasets by achieving 18% improvements. Compared with item id embedding based recommendation mod- els, e.g. Bert4Rec, TASTE almost doubles its recommendation per- formance, thriving on modeling relevance between users and items through text-matching signals. TASTE also outperforms the previ- ous state-of-the-art recommendation model, DIF-SR, which lever- ages item attributes as side information to help"}, {"question": " What is the main advantage of TASTE over other models?", "answer": " Modeling relevance between users and items through text-matching signals", "ref_chunk": "After data processing, each example can be a user-item interaction sequence H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc47 }. Then we use the leave-one-out evaluation strategy [7, 52, 58, 71], and separate the processed datasets into training, development, and testing sets. We construct the testing and development sets by using \ud835\udc631,...,\ud835\udc47 \u22121 to predict \ud835\udc63\ud835\udc47 and using \ud835\udc631,...,\ud835\udc47 \u22122 to predict \ud835\udc63\ud835\udc47 \u22121, respectively. For the training set, we fol- low Zhao et al. [70] and Xie et al. [58] to use interaction history \ud835\udc631,...,\ud835\udc56 \u22121 to predict \ud835\udc63\ud835\udc56 , where 1 < \ud835\udc56 < \ud835\udc47 \u2212 1. Evaluation Metrics. We utilize the same evaluation metrics as DIF-SR [58] and use Recall@10/20 and NDCG@10/20 to evaluate the recommendation performance of different models. Statistic significances are tested by permutation test with P< 0.05. During evaluating models, we follow DIF-SR [58] and employ a full ranking testing scenario [12, 32]. Some work evaluates model performance on a small item subset by randomly sampling or sam- pling items according to item popularity, making the evaluation results inconsistent of the same model. Instead of reranking items in the sampled subset, some work [12, 32] builds a more realis- tic recommendation evaluation setting by ranking all items and choosing the top-ranked items as recommendation results. Baselines. Following our main baseline model [58], we com- pare TASTE with several widely used sequential recommendation models. GRU4Rec [24] uses RNN to model user-item interaction sequences for recommendation. SASRec [28] and Bert4Rec [52] employ the self-attention mechanism to capture user preferences from user-item interaction sequences. To better capture relevance 1https://www.yelp.com/dataset 2http://jmcauley.ucsd.edu/data/amazon/ Text Matching Improves Sequential Recommendation by Reducing Popularity Biases CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom Table 2: Overall Performance. We keep the same experimental settings and report the scores of baselines from previous work [58]. Underlined scores are the highest results of baselines. \u2020, \u00a7, \u2021 indicate statistically significant improvements over DIF-SR\u2020, T5-DPR\u00a7 and T5-ID\u2021, respectively. We also show relative improvements over DIF-SR. Dataset Beauty Sports Toys Yelp Metrics Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 GRU4Rec Bert4Rec 0.0530 0.0839 0.0266 0.0344 0.0312 0.0482 0.0157 0.0200 0.0370 0.0588 0.0184 0.0239 0.0361 0.0592 0.0184 0.0243 0.0529 0.0815 0.0237 0.0309 0.0295 0.0465 0.0130 0.0173 0.0533 0.0787 0.0234 0.0297 0.0524 0.0756 0.0327 0.0385 SASRec 0.0828 0.1197 0.0371 0.0464 0.0526 0.0773 0.0233 0.0295 0.0831 0.1168 0.0375 0.0460 0.0650 0.0928 0.0401 0.0471 S3Rec NOVA ICAI-SR DIF-SR T5-DPR 0.0716 0.0868 0.1082 0.1236 0.0345 0.0439 0.0437 0.0531 0.0329 0.0517 0.0554 0.0758 0.0159 0.0249 0.0216 0.0310 0.0805\u2021 0.0967 0.1243\u2021 0.1349 0.0375 0.0475 0.0485 0.0571 0.0460 0.0589 0.0740 0.0902 0.0250\u2021 0.0338 0.0320 0.0416 0.0887 0.1237 0.0439 0.0527 0.0534 0.0759 0.0250 0.0307 0.0978 0.1322 0.0480 0.0567 0.0681 0.0964 0.0412 0.0483 0.0879 0.1231 0.0439 0.0528 0.0527 0.0762 0.0243 0.0302 0.0972 0.1303 0.0478 0.0561 0.0663 0.0940 0.0400 0.0470 0.0908 0.1284 0.0446 0.0541 0.0556 0.0800 0.0264 0.0325 0.1013 0.1382 0.0504 0.0597 0.0698 0.1003 0.0419 0.0496 T5-ID 0.0785\u00a7 0.1138\u00a7 0.0440\u00a7 0.0529\u00a7 0.0464\u00a7 0.0689\u00a7 0.0252\u00a7 0.0308\u00a7 0.0754 0.1065 0.0421\u00a7 0.0499 0.0450 0.0745 0.0235 0.0309 TASTE 0.1030\u2020\u2021\u00a7 0.1550\u2020\u2021\u00a7 0.0517\u2020\u2021\u00a7 0.0649\u2020\u2021\u00a7 0.0633\u2020\u2021\u00a7 0.0964\u2020\u2021\u00a7 0.0338\u2020\u2021\u00a7 0.0421\u2020\u2021\u00a7 0.1232\u2020\u2021\u00a7 0.1789\u2020\u2021\u00a7 0.0640\u2020\u2021\u00a7 0.0780\u2020\u2021\u00a7 0.0738\u2020\u2021\u00a7 0.1156\u2020\u2021\u00a7 0.0397\u2021\u00a7 0.0502\u2021\u00a7 13.44% 20.72% 15.92% 19.96% 13.85% 20.50% 28.03% 29.54% 21.62% 29.45% 26.98% 30.65% 5.73% 15.25% -5.25% 1.21% among attributes, items, and users, S3Rec [71] comes up with four self-supervised methods to pretrain self-attention modules. ICAI- SR [64] is compared, which proposes a heterogeneous graph to represent the relations between items and attributes to model item relevance. Besides, NOVA [38] is also compared, which builds at- tention modules to incorporate item attributes as the side informa- tion. DIF-SR [58] is the previous state-of-the-art, which builds a non-invasive attention mechanism to fuse information from item attributes during modeling user behaviors. 5 EVALUATION RESULT In this section, we first evaluate the recommendation performance of TASTE and conduct ablation studies. Then we study the effective- ness of different item verbalization methods, the ability of TASTE in reducing the popularity bias, the advantages of TASTE in repre- senting long-tail items, and modeling user behaviors using longer user-item interactions. Finally, we present several case studies. Besides, we implement a T5-ID model, which encodes user-item interaction sequences to generate the identifier of the next item [20]. We follow the previous dense retrieval model, DPR [29], to imple- ment T5-DPR, which regards the text sequences of user-item inter- action history and items as queries and documents, encodes them with T5 and is trained with in-batch sampled negatives. Implementation Details. Different from previous work [28, 52, 58], we represent items using full-text sequences (Eq. 1) instead of randomly initialized item embeddings. In our experiments, we use names and addresses to represent locations in the Yelp dataset. And we only use product names to represent shopping products in Amazon Product datasets, because the product name usually contains the item attributes, such as \u201cWAWO 15 Color Professional Makeup Eyeshadow Camouflage Facial Concealer Neutral Palette\u201d contains both category and brand. Our models are implemented with OpenMatch [41, 62]. In our experiments, we truncate the text representations of items by 32 tokens and set the max length of the text representation of user-item interactions to 512. The user-item interaction sequence is split into two subsequences to make the attention sparse in T5-encoder (Eq. 6). TASTE is initialized with the T5-base checkpoint from huggingface transformers [56]. During training, we use Adam optimizer and set learning rate=1e-4, warm up proportion=0.1, and batch size=8. Besides, we use in-batch negatives and randomly sampled negatives to optimize TASTE. We randomly sample 9 negative items for each training example and in-batch train TASTE model. 5.1 Overall Performance The recommendation performance of TASTE is shown in Table 2. Overall, TASTE significantly outperforms baseline models on all datasets by achieving 18% improvements. Compared with item id embedding based recommendation mod- els, e.g. Bert4Rec, TASTE almost doubles its recommendation per- formance, thriving on modeling relevance between users and items through text-matching signals. TASTE also outperforms the previ- ous state-of-the-art recommendation model, DIF-SR, which lever- ages item attributes as side information to help"}, {"question": " What are the key components of the S3Rec model?", "answer": " Four self-supervised methods to pretrain self-attention modules", "ref_chunk": "After data processing, each example can be a user-item interaction sequence H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc47 }. Then we use the leave-one-out evaluation strategy [7, 52, 58, 71], and separate the processed datasets into training, development, and testing sets. We construct the testing and development sets by using \ud835\udc631,...,\ud835\udc47 \u22121 to predict \ud835\udc63\ud835\udc47 and using \ud835\udc631,...,\ud835\udc47 \u22122 to predict \ud835\udc63\ud835\udc47 \u22121, respectively. For the training set, we fol- low Zhao et al. [70] and Xie et al. [58] to use interaction history \ud835\udc631,...,\ud835\udc56 \u22121 to predict \ud835\udc63\ud835\udc56 , where 1 < \ud835\udc56 < \ud835\udc47 \u2212 1. Evaluation Metrics. We utilize the same evaluation metrics as DIF-SR [58] and use Recall@10/20 and NDCG@10/20 to evaluate the recommendation performance of different models. Statistic significances are tested by permutation test with P< 0.05. During evaluating models, we follow DIF-SR [58] and employ a full ranking testing scenario [12, 32]. Some work evaluates model performance on a small item subset by randomly sampling or sam- pling items according to item popularity, making the evaluation results inconsistent of the same model. Instead of reranking items in the sampled subset, some work [12, 32] builds a more realis- tic recommendation evaluation setting by ranking all items and choosing the top-ranked items as recommendation results. Baselines. Following our main baseline model [58], we com- pare TASTE with several widely used sequential recommendation models. GRU4Rec [24] uses RNN to model user-item interaction sequences for recommendation. SASRec [28] and Bert4Rec [52] employ the self-attention mechanism to capture user preferences from user-item interaction sequences. To better capture relevance 1https://www.yelp.com/dataset 2http://jmcauley.ucsd.edu/data/amazon/ Text Matching Improves Sequential Recommendation by Reducing Popularity Biases CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom Table 2: Overall Performance. We keep the same experimental settings and report the scores of baselines from previous work [58]. Underlined scores are the highest results of baselines. \u2020, \u00a7, \u2021 indicate statistically significant improvements over DIF-SR\u2020, T5-DPR\u00a7 and T5-ID\u2021, respectively. We also show relative improvements over DIF-SR. Dataset Beauty Sports Toys Yelp Metrics Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 GRU4Rec Bert4Rec 0.0530 0.0839 0.0266 0.0344 0.0312 0.0482 0.0157 0.0200 0.0370 0.0588 0.0184 0.0239 0.0361 0.0592 0.0184 0.0243 0.0529 0.0815 0.0237 0.0309 0.0295 0.0465 0.0130 0.0173 0.0533 0.0787 0.0234 0.0297 0.0524 0.0756 0.0327 0.0385 SASRec 0.0828 0.1197 0.0371 0.0464 0.0526 0.0773 0.0233 0.0295 0.0831 0.1168 0.0375 0.0460 0.0650 0.0928 0.0401 0.0471 S3Rec NOVA ICAI-SR DIF-SR T5-DPR 0.0716 0.0868 0.1082 0.1236 0.0345 0.0439 0.0437 0.0531 0.0329 0.0517 0.0554 0.0758 0.0159 0.0249 0.0216 0.0310 0.0805\u2021 0.0967 0.1243\u2021 0.1349 0.0375 0.0475 0.0485 0.0571 0.0460 0.0589 0.0740 0.0902 0.0250\u2021 0.0338 0.0320 0.0416 0.0887 0.1237 0.0439 0.0527 0.0534 0.0759 0.0250 0.0307 0.0978 0.1322 0.0480 0.0567 0.0681 0.0964 0.0412 0.0483 0.0879 0.1231 0.0439 0.0528 0.0527 0.0762 0.0243 0.0302 0.0972 0.1303 0.0478 0.0561 0.0663 0.0940 0.0400 0.0470 0.0908 0.1284 0.0446 0.0541 0.0556 0.0800 0.0264 0.0325 0.1013 0.1382 0.0504 0.0597 0.0698 0.1003 0.0419 0.0496 T5-ID 0.0785\u00a7 0.1138\u00a7 0.0440\u00a7 0.0529\u00a7 0.0464\u00a7 0.0689\u00a7 0.0252\u00a7 0.0308\u00a7 0.0754 0.1065 0.0421\u00a7 0.0499 0.0450 0.0745 0.0235 0.0309 TASTE 0.1030\u2020\u2021\u00a7 0.1550\u2020\u2021\u00a7 0.0517\u2020\u2021\u00a7 0.0649\u2020\u2021\u00a7 0.0633\u2020\u2021\u00a7 0.0964\u2020\u2021\u00a7 0.0338\u2020\u2021\u00a7 0.0421\u2020\u2021\u00a7 0.1232\u2020\u2021\u00a7 0.1789\u2020\u2021\u00a7 0.0640\u2020\u2021\u00a7 0.0780\u2020\u2021\u00a7 0.0738\u2020\u2021\u00a7 0.1156\u2020\u2021\u00a7 0.0397\u2021\u00a7 0.0502\u2021\u00a7 13.44% 20.72% 15.92% 19.96% 13.85% 20.50% 28.03% 29.54% 21.62% 29.45% 26.98% 30.65% 5.73% 15.25% -5.25% 1.21% among attributes, items, and users, S3Rec [71] comes up with four self-supervised methods to pretrain self-attention modules. ICAI- SR [64] is compared, which proposes a heterogeneous graph to represent the relations between items and attributes to model item relevance. Besides, NOVA [38] is also compared, which builds at- tention modules to incorporate item attributes as the side informa- tion. DIF-SR [58] is the previous state-of-the-art, which builds a non-invasive attention mechanism to fuse information from item attributes during modeling user behaviors. 5 EVALUATION RESULT In this section, we first evaluate the recommendation performance of TASTE and conduct ablation studies. Then we study the effective- ness of different item verbalization methods, the ability of TASTE in reducing the popularity bias, the advantages of TASTE in repre- senting long-tail items, and modeling user behaviors using longer user-item interactions. Finally, we present several case studies. Besides, we implement a T5-ID model, which encodes user-item interaction sequences to generate the identifier of the next item [20]. We follow the previous dense retrieval model, DPR [29], to imple- ment T5-DPR, which regards the text sequences of user-item inter- action history and items as queries and documents, encodes them with T5 and is trained with in-batch sampled negatives. Implementation Details. Different from previous work [28, 52, 58], we represent items using full-text sequences (Eq. 1) instead of randomly initialized item embeddings. In our experiments, we use names and addresses to represent locations in the Yelp dataset. And we only use product names to represent shopping products in Amazon Product datasets, because the product name usually contains the item attributes, such as \u201cWAWO 15 Color Professional Makeup Eyeshadow Camouflage Facial Concealer Neutral Palette\u201d contains both category and brand. Our models are implemented with OpenMatch [41, 62]. In our experiments, we truncate the text representations of items by 32 tokens and set the max length of the text representation of user-item interactions to 512. The user-item interaction sequence is split into two subsequences to make the attention sparse in T5-encoder (Eq. 6). TASTE is initialized with the T5-base checkpoint from huggingface transformers [56]. During training, we use Adam optimizer and set learning rate=1e-4, warm up proportion=0.1, and batch size=8. Besides, we use in-batch negatives and randomly sampled negatives to optimize TASTE. We randomly sample 9 negative items for each training example and in-batch train TASTE model. 5.1 Overall Performance The recommendation performance of TASTE is shown in Table 2. Overall, TASTE significantly outperforms baseline models on all datasets by achieving 18% improvements. Compared with item id embedding based recommendation mod- els, e.g. Bert4Rec, TASTE almost doubles its recommendation per- formance, thriving on modeling relevance between users and items through text-matching signals. TASTE also outperforms the previ- ous state-of-the-art recommendation model, DIF-SR, which lever- ages item attributes as side information to help"}, {"question": " How is the evaluation performance of TASTE compared to baseline models in Table 2?", "answer": " TASTE significantly outperforms baseline models on all datasets by achieving 18% improvements", "ref_chunk": "After data processing, each example can be a user-item interaction sequence H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc47 }. Then we use the leave-one-out evaluation strategy [7, 52, 58, 71], and separate the processed datasets into training, development, and testing sets. We construct the testing and development sets by using \ud835\udc631,...,\ud835\udc47 \u22121 to predict \ud835\udc63\ud835\udc47 and using \ud835\udc631,...,\ud835\udc47 \u22122 to predict \ud835\udc63\ud835\udc47 \u22121, respectively. For the training set, we fol- low Zhao et al. [70] and Xie et al. [58] to use interaction history \ud835\udc631,...,\ud835\udc56 \u22121 to predict \ud835\udc63\ud835\udc56 , where 1 < \ud835\udc56 < \ud835\udc47 \u2212 1. Evaluation Metrics. We utilize the same evaluation metrics as DIF-SR [58] and use Recall@10/20 and NDCG@10/20 to evaluate the recommendation performance of different models. Statistic significances are tested by permutation test with P< 0.05. During evaluating models, we follow DIF-SR [58] and employ a full ranking testing scenario [12, 32]. Some work evaluates model performance on a small item subset by randomly sampling or sam- pling items according to item popularity, making the evaluation results inconsistent of the same model. Instead of reranking items in the sampled subset, some work [12, 32] builds a more realis- tic recommendation evaluation setting by ranking all items and choosing the top-ranked items as recommendation results. Baselines. Following our main baseline model [58], we com- pare TASTE with several widely used sequential recommendation models. GRU4Rec [24] uses RNN to model user-item interaction sequences for recommendation. SASRec [28] and Bert4Rec [52] employ the self-attention mechanism to capture user preferences from user-item interaction sequences. To better capture relevance 1https://www.yelp.com/dataset 2http://jmcauley.ucsd.edu/data/amazon/ Text Matching Improves Sequential Recommendation by Reducing Popularity Biases CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom Table 2: Overall Performance. We keep the same experimental settings and report the scores of baselines from previous work [58]. Underlined scores are the highest results of baselines. \u2020, \u00a7, \u2021 indicate statistically significant improvements over DIF-SR\u2020, T5-DPR\u00a7 and T5-ID\u2021, respectively. We also show relative improvements over DIF-SR. Dataset Beauty Sports Toys Yelp Metrics Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 GRU4Rec Bert4Rec 0.0530 0.0839 0.0266 0.0344 0.0312 0.0482 0.0157 0.0200 0.0370 0.0588 0.0184 0.0239 0.0361 0.0592 0.0184 0.0243 0.0529 0.0815 0.0237 0.0309 0.0295 0.0465 0.0130 0.0173 0.0533 0.0787 0.0234 0.0297 0.0524 0.0756 0.0327 0.0385 SASRec 0.0828 0.1197 0.0371 0.0464 0.0526 0.0773 0.0233 0.0295 0.0831 0.1168 0.0375 0.0460 0.0650 0.0928 0.0401 0.0471 S3Rec NOVA ICAI-SR DIF-SR T5-DPR 0.0716 0.0868 0.1082 0.1236 0.0345 0.0439 0.0437 0.0531 0.0329 0.0517 0.0554 0.0758 0.0159 0.0249 0.0216 0.0310 0.0805\u2021 0.0967 0.1243\u2021 0.1349 0.0375 0.0475 0.0485 0.0571 0.0460 0.0589 0.0740 0.0902 0.0250\u2021 0.0338 0.0320 0.0416 0.0887 0.1237 0.0439 0.0527 0.0534 0.0759 0.0250 0.0307 0.0978 0.1322 0.0480 0.0567 0.0681 0.0964 0.0412 0.0483 0.0879 0.1231 0.0439 0.0528 0.0527 0.0762 0.0243 0.0302 0.0972 0.1303 0.0478 0.0561 0.0663 0.0940 0.0400 0.0470 0.0908 0.1284 0.0446 0.0541 0.0556 0.0800 0.0264 0.0325 0.1013 0.1382 0.0504 0.0597 0.0698 0.1003 0.0419 0.0496 T5-ID 0.0785\u00a7 0.1138\u00a7 0.0440\u00a7 0.0529\u00a7 0.0464\u00a7 0.0689\u00a7 0.0252\u00a7 0.0308\u00a7 0.0754 0.1065 0.0421\u00a7 0.0499 0.0450 0.0745 0.0235 0.0309 TASTE 0.1030\u2020\u2021\u00a7 0.1550\u2020\u2021\u00a7 0.0517\u2020\u2021\u00a7 0.0649\u2020\u2021\u00a7 0.0633\u2020\u2021\u00a7 0.0964\u2020\u2021\u00a7 0.0338\u2020\u2021\u00a7 0.0421\u2020\u2021\u00a7 0.1232\u2020\u2021\u00a7 0.1789\u2020\u2021\u00a7 0.0640\u2020\u2021\u00a7 0.0780\u2020\u2021\u00a7 0.0738\u2020\u2021\u00a7 0.1156\u2020\u2021\u00a7 0.0397\u2021\u00a7 0.0502\u2021\u00a7 13.44% 20.72% 15.92% 19.96% 13.85% 20.50% 28.03% 29.54% 21.62% 29.45% 26.98% 30.65% 5.73% 15.25% -5.25% 1.21% among attributes, items, and users, S3Rec [71] comes up with four self-supervised methods to pretrain self-attention modules. ICAI- SR [64] is compared, which proposes a heterogeneous graph to represent the relations between items and attributes to model item relevance. Besides, NOVA [38] is also compared, which builds at- tention modules to incorporate item attributes as the side informa- tion. DIF-SR [58] is the previous state-of-the-art, which builds a non-invasive attention mechanism to fuse information from item attributes during modeling user behaviors. 5 EVALUATION RESULT In this section, we first evaluate the recommendation performance of TASTE and conduct ablation studies. Then we study the effective- ness of different item verbalization methods, the ability of TASTE in reducing the popularity bias, the advantages of TASTE in repre- senting long-tail items, and modeling user behaviors using longer user-item interactions. Finally, we present several case studies. Besides, we implement a T5-ID model, which encodes user-item interaction sequences to generate the identifier of the next item [20]. We follow the previous dense retrieval model, DPR [29], to imple- ment T5-DPR, which regards the text sequences of user-item inter- action history and items as queries and documents, encodes them with T5 and is trained with in-batch sampled negatives. Implementation Details. Different from previous work [28, 52, 58], we represent items using full-text sequences (Eq. 1) instead of randomly initialized item embeddings. In our experiments, we use names and addresses to represent locations in the Yelp dataset. And we only use product names to represent shopping products in Amazon Product datasets, because the product name usually contains the item attributes, such as \u201cWAWO 15 Color Professional Makeup Eyeshadow Camouflage Facial Concealer Neutral Palette\u201d contains both category and brand. Our models are implemented with OpenMatch [41, 62]. In our experiments, we truncate the text representations of items by 32 tokens and set the max length of the text representation of user-item interactions to 512. The user-item interaction sequence is split into two subsequences to make the attention sparse in T5-encoder (Eq. 6). TASTE is initialized with the T5-base checkpoint from huggingface transformers [56]. During training, we use Adam optimizer and set learning rate=1e-4, warm up proportion=0.1, and batch size=8. Besides, we use in-batch negatives and randomly sampled negatives to optimize TASTE. We randomly sample 9 negative items for each training example and in-batch train TASTE model. 5.1 Overall Performance The recommendation performance of TASTE is shown in Table 2. Overall, TASTE significantly outperforms baseline models on all datasets by achieving 18% improvements. Compared with item id embedding based recommendation mod- els, e.g. Bert4Rec, TASTE almost doubles its recommendation per- formance, thriving on modeling relevance between users and items through text-matching signals. TASTE also outperforms the previ- ous state-of-the-art recommendation model, DIF-SR, which lever- ages item attributes as side information to help"}], "doc_text": "After data processing, each example can be a user-item interaction sequence H = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc47 }. Then we use the leave-one-out evaluation strategy [7, 52, 58, 71], and separate the processed datasets into training, development, and testing sets. We construct the testing and development sets by using \ud835\udc631,...,\ud835\udc47 \u22121 to predict \ud835\udc63\ud835\udc47 and using \ud835\udc631,...,\ud835\udc47 \u22122 to predict \ud835\udc63\ud835\udc47 \u22121, respectively. For the training set, we fol- low Zhao et al. [70] and Xie et al. [58] to use interaction history \ud835\udc631,...,\ud835\udc56 \u22121 to predict \ud835\udc63\ud835\udc56 , where 1 < \ud835\udc56 < \ud835\udc47 \u2212 1. Evaluation Metrics. We utilize the same evaluation metrics as DIF-SR [58] and use Recall@10/20 and NDCG@10/20 to evaluate the recommendation performance of different models. Statistic significances are tested by permutation test with P< 0.05. During evaluating models, we follow DIF-SR [58] and employ a full ranking testing scenario [12, 32]. Some work evaluates model performance on a small item subset by randomly sampling or sam- pling items according to item popularity, making the evaluation results inconsistent of the same model. Instead of reranking items in the sampled subset, some work [12, 32] builds a more realis- tic recommendation evaluation setting by ranking all items and choosing the top-ranked items as recommendation results. Baselines. Following our main baseline model [58], we com- pare TASTE with several widely used sequential recommendation models. GRU4Rec [24] uses RNN to model user-item interaction sequences for recommendation. SASRec [28] and Bert4Rec [52] employ the self-attention mechanism to capture user preferences from user-item interaction sequences. To better capture relevance 1https://www.yelp.com/dataset 2http://jmcauley.ucsd.edu/data/amazon/ Text Matching Improves Sequential Recommendation by Reducing Popularity Biases CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom Table 2: Overall Performance. We keep the same experimental settings and report the scores of baselines from previous work [58]. Underlined scores are the highest results of baselines. \u2020, \u00a7, \u2021 indicate statistically significant improvements over DIF-SR\u2020, T5-DPR\u00a7 and T5-ID\u2021, respectively. We also show relative improvements over DIF-SR. Dataset Beauty Sports Toys Yelp Metrics Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 GRU4Rec Bert4Rec 0.0530 0.0839 0.0266 0.0344 0.0312 0.0482 0.0157 0.0200 0.0370 0.0588 0.0184 0.0239 0.0361 0.0592 0.0184 0.0243 0.0529 0.0815 0.0237 0.0309 0.0295 0.0465 0.0130 0.0173 0.0533 0.0787 0.0234 0.0297 0.0524 0.0756 0.0327 0.0385 SASRec 0.0828 0.1197 0.0371 0.0464 0.0526 0.0773 0.0233 0.0295 0.0831 0.1168 0.0375 0.0460 0.0650 0.0928 0.0401 0.0471 S3Rec NOVA ICAI-SR DIF-SR T5-DPR 0.0716 0.0868 0.1082 0.1236 0.0345 0.0439 0.0437 0.0531 0.0329 0.0517 0.0554 0.0758 0.0159 0.0249 0.0216 0.0310 0.0805\u2021 0.0967 0.1243\u2021 0.1349 0.0375 0.0475 0.0485 0.0571 0.0460 0.0589 0.0740 0.0902 0.0250\u2021 0.0338 0.0320 0.0416 0.0887 0.1237 0.0439 0.0527 0.0534 0.0759 0.0250 0.0307 0.0978 0.1322 0.0480 0.0567 0.0681 0.0964 0.0412 0.0483 0.0879 0.1231 0.0439 0.0528 0.0527 0.0762 0.0243 0.0302 0.0972 0.1303 0.0478 0.0561 0.0663 0.0940 0.0400 0.0470 0.0908 0.1284 0.0446 0.0541 0.0556 0.0800 0.0264 0.0325 0.1013 0.1382 0.0504 0.0597 0.0698 0.1003 0.0419 0.0496 T5-ID 0.0785\u00a7 0.1138\u00a7 0.0440\u00a7 0.0529\u00a7 0.0464\u00a7 0.0689\u00a7 0.0252\u00a7 0.0308\u00a7 0.0754 0.1065 0.0421\u00a7 0.0499 0.0450 0.0745 0.0235 0.0309 TASTE 0.1030\u2020\u2021\u00a7 0.1550\u2020\u2021\u00a7 0.0517\u2020\u2021\u00a7 0.0649\u2020\u2021\u00a7 0.0633\u2020\u2021\u00a7 0.0964\u2020\u2021\u00a7 0.0338\u2020\u2021\u00a7 0.0421\u2020\u2021\u00a7 0.1232\u2020\u2021\u00a7 0.1789\u2020\u2021\u00a7 0.0640\u2020\u2021\u00a7 0.0780\u2020\u2021\u00a7 0.0738\u2020\u2021\u00a7 0.1156\u2020\u2021\u00a7 0.0397\u2021\u00a7 0.0502\u2021\u00a7 13.44% 20.72% 15.92% 19.96% 13.85% 20.50% 28.03% 29.54% 21.62% 29.45% 26.98% 30.65% 5.73% 15.25% -5.25% 1.21% among attributes, items, and users, S3Rec [71] comes up with four self-supervised methods to pretrain self-attention modules. ICAI- SR [64] is compared, which proposes a heterogeneous graph to represent the relations between items and attributes to model item relevance. Besides, NOVA [38] is also compared, which builds at- tention modules to incorporate item attributes as the side informa- tion. DIF-SR [58] is the previous state-of-the-art, which builds a non-invasive attention mechanism to fuse information from item attributes during modeling user behaviors. 5 EVALUATION RESULT In this section, we first evaluate the recommendation performance of TASTE and conduct ablation studies. Then we study the effective- ness of different item verbalization methods, the ability of TASTE in reducing the popularity bias, the advantages of TASTE in repre- senting long-tail items, and modeling user behaviors using longer user-item interactions. Finally, we present several case studies. Besides, we implement a T5-ID model, which encodes user-item interaction sequences to generate the identifier of the next item [20]. We follow the previous dense retrieval model, DPR [29], to imple- ment T5-DPR, which regards the text sequences of user-item inter- action history and items as queries and documents, encodes them with T5 and is trained with in-batch sampled negatives. Implementation Details. Different from previous work [28, 52, 58], we represent items using full-text sequences (Eq. 1) instead of randomly initialized item embeddings. In our experiments, we use names and addresses to represent locations in the Yelp dataset. And we only use product names to represent shopping products in Amazon Product datasets, because the product name usually contains the item attributes, such as \u201cWAWO 15 Color Professional Makeup Eyeshadow Camouflage Facial Concealer Neutral Palette\u201d contains both category and brand. Our models are implemented with OpenMatch [41, 62]. In our experiments, we truncate the text representations of items by 32 tokens and set the max length of the text representation of user-item interactions to 512. The user-item interaction sequence is split into two subsequences to make the attention sparse in T5-encoder (Eq. 6). TASTE is initialized with the T5-base checkpoint from huggingface transformers [56]. During training, we use Adam optimizer and set learning rate=1e-4, warm up proportion=0.1, and batch size=8. Besides, we use in-batch negatives and randomly sampled negatives to optimize TASTE. We randomly sample 9 negative items for each training example and in-batch train TASTE model. 5.1 Overall Performance The recommendation performance of TASTE is shown in Table 2. Overall, TASTE significantly outperforms baseline models on all datasets by achieving 18% improvements. Compared with item id embedding based recommendation mod- els, e.g. Bert4Rec, TASTE almost doubles its recommendation per- formance, thriving on modeling relevance between users and items through text-matching signals. TASTE also outperforms the previ- ous state-of-the-art recommendation model, DIF-SR, which lever- ages item attributes as side information to help"}