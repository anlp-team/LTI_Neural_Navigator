{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yonatan_Bisk_WebArena:_A_Realistic_Web_Environment_for_Building_Autonomous_Agents_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the evaluation method mentioned in the text focus on?,answer: The evaluation method focuses on evaluating the functional correctness of tasks to determine if the execution result achieves the desired goal.", "ref_chunk": "shown in the upper left of Figure 1. We focus on evaluating the functional correctness of these tasks, i.e., does the result of the execution actually achieve the desired goal (\u00a73.2). For instance, to evaluate the example in Figure 2, our evaluation method verifies the concrete contents in the designated repository. This evaluation is not only more reliable (Zhong et al., 2017; Chen et al., 2021; Wang et al., 2022) than comparing the textual surface-form action sequences (Puig et al., 2018; Deng et al., 2023) but also accommodate a range of potential valid paths to achieve the same goal, which is a ubiquitous phenomenon in sufficiently complex tasks. We use this benchmark to evaluate several agents that can follow NL command and perform web- based tasks (\u00a74). These agents are implemented in a few-shot in-context learning fashion with powerful large language models (LLMs) such as GPT-4 and PALM-2. Experiment results show that the best GPT-4 agent performance is somewhat limited, with an end-to-end task success rate of only 14.41%, while the human performance is 78.24%. We hypothesize that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex tasks (\u00a75.2). These outcomes underscore the necessity for further development towards robust and effective agents (LeCun, 2022) in WebArena. 2 webarena.wikipedia.com Search for each art museum on the Map webarena.gitlab.com \u201c\u201d webarena.openstreetmap.comRecord the optimized results to the repo \u2026 Create an efficient itinerary to visit all of Pittsburgh's art museums with minimal driving distance starting from Schenley Park. Log the order in my \u201cawesome-northeast-us-travel\u201d repository Search for museums in Pittsburgh Under review Figure 2: A high-level task that can be fully executed in WebArena. Success requires sophisticated, long-term planning and reasoning. To accomplish the goal (top), an agent needs to (1) find Pittsburgh art museums on Wikipedia, (2) identify their locations on a map (while optimizing the itinerary), and (3) update the README file in the appropriate repository with the planned route. 2 WE BAR E N A: WEBSITES AS AN ENVIRONMENT FOR AUTONOMOUS AGENTS Our goal is to create a realistic and reproducible web environment. We achieve reproducibility by making the environment standalone, without relying on live websites. This circumvents technical challenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and configuration changes, which obstruct a fair comparison across different systems over time. We achieve realism by using open-source libraries that underlie many in-use sites from several popular categories and importing data to our environment from their real-world counterparts. 2.1 CONTROLLING AGENTS THROUGH HIGH-LEVEL NATURAL LANGUAGE The WebArena environment is denoted as E= \u27e8S, A, O, T \u27e9 with state space S, action space A (\u00a72.4) and observation space O (\u00a72.3). The transition function T : S \u00d7 A\u2212\u2192 S is deterministic, and it is defined by the underlying implementation of each website in the environment. Performing a task described by a natural language intent i can be formulated as a partially observable Markov decision process (POMDP): at each time step t, an agent issues an action at\u2208 A given the partial observation ot\u2208 O. Consequently, the action results in a new state st+1\u2208 S and its corresponding observation ot+1\u2208 O. We propose a reward function r(a, s) to measure the success of a task execution, where a represents the sequence of actions, and s denotes all intermediate states. This reward function assesses if state transitions align with the expectations of the intents. For example, with an intent to place an order, it verifies whether an order has been placed. Additionally, it evaluates the accuracy of the agent\u2019s actions, such as checking the correctness of the predicted answer. 2.2 WEBSITE SELECTION To decide which categories of websites to use, we first analyzed approximately 200 examples from the authors\u2019 actual web browser histories. Each author delved into their browsing histories, summarizing the goal of particular segments of their browser session. Based on this, we classified the visited websites into abstract categories. We then identified the four most salient categories and implemented one instance per category based on this analysis: (1) E-commerce platforms supporting online shopping activities (e.g., Amazon, eBay), (2) social forum platforms for opinion exchanges (e.g., Reddit, StackExchange), (3) collaborative development platforms for software development (e.g., GitLab), and (4) content management systems (CMS) that manage the creation and revision of the digital content (e.g., online store management). In addition to these platforms, we selected three utility-style tools that are frequently used in web- based tasks: (1) a map for navigation and searching for information about points of interest (POIs) such as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes. As information- seeking and knowledge acquisition are critical in web-based tasks, we also incorporated various knowledge resources into WebArena. These resources range from general information hubs, such as the English Wikipedia, to more specialized knowledge bases, such as the website user manuals. 3 <li> <div> <a href=\"...\"><img src=\"...\"></a> <div class> <a href=\"...\">Outdoor Patio \u2026</a> <div> <span>Rating:</span> <div> <span>82%</span> </div> <a href=\u201c\u2026#reviews\">12 <span>Reviews</span></a> webarena.onestopshop.com webarena.onestopshop.com webarena.onestopshop.com RootWebArea \u2018Patio, Lawn ..\u2019 link 'Image' img 'Image' link 'Outdoor Patio..\u2019 LayoutTable '' StaticText 'Rating:' generic '82%' link '12 Reviews' StaticText \u2018$49.99' button 'Add to Cart\u2019 focusable: True button 'Wish List\u2019 focusable: \u2026 button 'Compare\u2019 focusable: \u2026 Under review Figure 3: We design the observation to be the URL and the content of a web page, with options to represent the content as a screenshot (left), HTML DOM tree (middle), and accessibility tree (right). The content of the middle and right figures are trimmed to save space. Implementation We leveraged open-source libraries relevant to each category to build our own versions of an E-commerce website (OneStopShop), GitLab, Reddit, an online store content manage- ment system (CMS), a map, and an English Wikipedia. Then we imported sampled data from their real-world counterparts. As an example, our version of GitLab was developed based on the actual GitLab project.1 We carefully emulated the features of a typical code repository"}, {"question": " What is the success rate of the best GPT-4 agent in performing end-to-end tasks?,answer: The best GPT-4 agent has an end-to-end task success rate of only 14.41%.", "ref_chunk": "shown in the upper left of Figure 1. We focus on evaluating the functional correctness of these tasks, i.e., does the result of the execution actually achieve the desired goal (\u00a73.2). For instance, to evaluate the example in Figure 2, our evaluation method verifies the concrete contents in the designated repository. This evaluation is not only more reliable (Zhong et al., 2017; Chen et al., 2021; Wang et al., 2022) than comparing the textual surface-form action sequences (Puig et al., 2018; Deng et al., 2023) but also accommodate a range of potential valid paths to achieve the same goal, which is a ubiquitous phenomenon in sufficiently complex tasks. We use this benchmark to evaluate several agents that can follow NL command and perform web- based tasks (\u00a74). These agents are implemented in a few-shot in-context learning fashion with powerful large language models (LLMs) such as GPT-4 and PALM-2. Experiment results show that the best GPT-4 agent performance is somewhat limited, with an end-to-end task success rate of only 14.41%, while the human performance is 78.24%. We hypothesize that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex tasks (\u00a75.2). These outcomes underscore the necessity for further development towards robust and effective agents (LeCun, 2022) in WebArena. 2 webarena.wikipedia.com Search for each art museum on the Map webarena.gitlab.com \u201c\u201d webarena.openstreetmap.comRecord the optimized results to the repo \u2026 Create an efficient itinerary to visit all of Pittsburgh's art museums with minimal driving distance starting from Schenley Park. Log the order in my \u201cawesome-northeast-us-travel\u201d repository Search for museums in Pittsburgh Under review Figure 2: A high-level task that can be fully executed in WebArena. Success requires sophisticated, long-term planning and reasoning. To accomplish the goal (top), an agent needs to (1) find Pittsburgh art museums on Wikipedia, (2) identify their locations on a map (while optimizing the itinerary), and (3) update the README file in the appropriate repository with the planned route. 2 WE BAR E N A: WEBSITES AS AN ENVIRONMENT FOR AUTONOMOUS AGENTS Our goal is to create a realistic and reproducible web environment. We achieve reproducibility by making the environment standalone, without relying on live websites. This circumvents technical challenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and configuration changes, which obstruct a fair comparison across different systems over time. We achieve realism by using open-source libraries that underlie many in-use sites from several popular categories and importing data to our environment from their real-world counterparts. 2.1 CONTROLLING AGENTS THROUGH HIGH-LEVEL NATURAL LANGUAGE The WebArena environment is denoted as E= \u27e8S, A, O, T \u27e9 with state space S, action space A (\u00a72.4) and observation space O (\u00a72.3). The transition function T : S \u00d7 A\u2212\u2192 S is deterministic, and it is defined by the underlying implementation of each website in the environment. Performing a task described by a natural language intent i can be formulated as a partially observable Markov decision process (POMDP): at each time step t, an agent issues an action at\u2208 A given the partial observation ot\u2208 O. Consequently, the action results in a new state st+1\u2208 S and its corresponding observation ot+1\u2208 O. We propose a reward function r(a, s) to measure the success of a task execution, where a represents the sequence of actions, and s denotes all intermediate states. This reward function assesses if state transitions align with the expectations of the intents. For example, with an intent to place an order, it verifies whether an order has been placed. Additionally, it evaluates the accuracy of the agent\u2019s actions, such as checking the correctness of the predicted answer. 2.2 WEBSITE SELECTION To decide which categories of websites to use, we first analyzed approximately 200 examples from the authors\u2019 actual web browser histories. Each author delved into their browsing histories, summarizing the goal of particular segments of their browser session. Based on this, we classified the visited websites into abstract categories. We then identified the four most salient categories and implemented one instance per category based on this analysis: (1) E-commerce platforms supporting online shopping activities (e.g., Amazon, eBay), (2) social forum platforms for opinion exchanges (e.g., Reddit, StackExchange), (3) collaborative development platforms for software development (e.g., GitLab), and (4) content management systems (CMS) that manage the creation and revision of the digital content (e.g., online store management). In addition to these platforms, we selected three utility-style tools that are frequently used in web- based tasks: (1) a map for navigation and searching for information about points of interest (POIs) such as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes. As information- seeking and knowledge acquisition are critical in web-based tasks, we also incorporated various knowledge resources into WebArena. These resources range from general information hubs, such as the English Wikipedia, to more specialized knowledge bases, such as the website user manuals. 3 <li> <div> <a href=\"...\"><img src=\"...\"></a> <div class> <a href=\"...\">Outdoor Patio \u2026</a> <div> <span>Rating:</span> <div> <span>82%</span> </div> <a href=\u201c\u2026#reviews\">12 <span>Reviews</span></a> webarena.onestopshop.com webarena.onestopshop.com webarena.onestopshop.com RootWebArea \u2018Patio, Lawn ..\u2019 link 'Image' img 'Image' link 'Outdoor Patio..\u2019 LayoutTable '' StaticText 'Rating:' generic '82%' link '12 Reviews' StaticText \u2018$49.99' button 'Add to Cart\u2019 focusable: True button 'Wish List\u2019 focusable: \u2026 button 'Compare\u2019 focusable: \u2026 Under review Figure 3: We design the observation to be the URL and the content of a web page, with options to represent the content as a screenshot (left), HTML DOM tree (middle), and accessibility tree (right). The content of the middle and right figures are trimmed to save space. Implementation We leveraged open-source libraries relevant to each category to build our own versions of an E-commerce website (OneStopShop), GitLab, Reddit, an online store content manage- ment system (CMS), a map, and an English Wikipedia. Then we imported sampled data from their real-world counterparts. As an example, our version of GitLab was developed based on the actual GitLab project.1 We carefully emulated the features of a typical code repository"}, {"question": " What hypothesis is mentioned regarding the limited performance of current LLMs?,answer: The hypothesis is that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex tasks.", "ref_chunk": "shown in the upper left of Figure 1. We focus on evaluating the functional correctness of these tasks, i.e., does the result of the execution actually achieve the desired goal (\u00a73.2). For instance, to evaluate the example in Figure 2, our evaluation method verifies the concrete contents in the designated repository. This evaluation is not only more reliable (Zhong et al., 2017; Chen et al., 2021; Wang et al., 2022) than comparing the textual surface-form action sequences (Puig et al., 2018; Deng et al., 2023) but also accommodate a range of potential valid paths to achieve the same goal, which is a ubiquitous phenomenon in sufficiently complex tasks. We use this benchmark to evaluate several agents that can follow NL command and perform web- based tasks (\u00a74). These agents are implemented in a few-shot in-context learning fashion with powerful large language models (LLMs) such as GPT-4 and PALM-2. Experiment results show that the best GPT-4 agent performance is somewhat limited, with an end-to-end task success rate of only 14.41%, while the human performance is 78.24%. We hypothesize that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex tasks (\u00a75.2). These outcomes underscore the necessity for further development towards robust and effective agents (LeCun, 2022) in WebArena. 2 webarena.wikipedia.com Search for each art museum on the Map webarena.gitlab.com \u201c\u201d webarena.openstreetmap.comRecord the optimized results to the repo \u2026 Create an efficient itinerary to visit all of Pittsburgh's art museums with minimal driving distance starting from Schenley Park. Log the order in my \u201cawesome-northeast-us-travel\u201d repository Search for museums in Pittsburgh Under review Figure 2: A high-level task that can be fully executed in WebArena. Success requires sophisticated, long-term planning and reasoning. To accomplish the goal (top), an agent needs to (1) find Pittsburgh art museums on Wikipedia, (2) identify their locations on a map (while optimizing the itinerary), and (3) update the README file in the appropriate repository with the planned route. 2 WE BAR E N A: WEBSITES AS AN ENVIRONMENT FOR AUTONOMOUS AGENTS Our goal is to create a realistic and reproducible web environment. We achieve reproducibility by making the environment standalone, without relying on live websites. This circumvents technical challenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and configuration changes, which obstruct a fair comparison across different systems over time. We achieve realism by using open-source libraries that underlie many in-use sites from several popular categories and importing data to our environment from their real-world counterparts. 2.1 CONTROLLING AGENTS THROUGH HIGH-LEVEL NATURAL LANGUAGE The WebArena environment is denoted as E= \u27e8S, A, O, T \u27e9 with state space S, action space A (\u00a72.4) and observation space O (\u00a72.3). The transition function T : S \u00d7 A\u2212\u2192 S is deterministic, and it is defined by the underlying implementation of each website in the environment. Performing a task described by a natural language intent i can be formulated as a partially observable Markov decision process (POMDP): at each time step t, an agent issues an action at\u2208 A given the partial observation ot\u2208 O. Consequently, the action results in a new state st+1\u2208 S and its corresponding observation ot+1\u2208 O. We propose a reward function r(a, s) to measure the success of a task execution, where a represents the sequence of actions, and s denotes all intermediate states. This reward function assesses if state transitions align with the expectations of the intents. For example, with an intent to place an order, it verifies whether an order has been placed. Additionally, it evaluates the accuracy of the agent\u2019s actions, such as checking the correctness of the predicted answer. 2.2 WEBSITE SELECTION To decide which categories of websites to use, we first analyzed approximately 200 examples from the authors\u2019 actual web browser histories. Each author delved into their browsing histories, summarizing the goal of particular segments of their browser session. Based on this, we classified the visited websites into abstract categories. We then identified the four most salient categories and implemented one instance per category based on this analysis: (1) E-commerce platforms supporting online shopping activities (e.g., Amazon, eBay), (2) social forum platforms for opinion exchanges (e.g., Reddit, StackExchange), (3) collaborative development platforms for software development (e.g., GitLab), and (4) content management systems (CMS) that manage the creation and revision of the digital content (e.g., online store management). In addition to these platforms, we selected three utility-style tools that are frequently used in web- based tasks: (1) a map for navigation and searching for information about points of interest (POIs) such as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes. As information- seeking and knowledge acquisition are critical in web-based tasks, we also incorporated various knowledge resources into WebArena. These resources range from general information hubs, such as the English Wikipedia, to more specialized knowledge bases, such as the website user manuals. 3 <li> <div> <a href=\"...\"><img src=\"...\"></a> <div class> <a href=\"...\">Outdoor Patio \u2026</a> <div> <span>Rating:</span> <div> <span>82%</span> </div> <a href=\u201c\u2026#reviews\">12 <span>Reviews</span></a> webarena.onestopshop.com webarena.onestopshop.com webarena.onestopshop.com RootWebArea \u2018Patio, Lawn ..\u2019 link 'Image' img 'Image' link 'Outdoor Patio..\u2019 LayoutTable '' StaticText 'Rating:' generic '82%' link '12 Reviews' StaticText \u2018$49.99' button 'Add to Cart\u2019 focusable: True button 'Wish List\u2019 focusable: \u2026 button 'Compare\u2019 focusable: \u2026 Under review Figure 3: We design the observation to be the URL and the content of a web page, with options to represent the content as a screenshot (left), HTML DOM tree (middle), and accessibility tree (right). The content of the middle and right figures are trimmed to save space. Implementation We leveraged open-source libraries relevant to each category to build our own versions of an E-commerce website (OneStopShop), GitLab, Reddit, an online store content manage- ment system (CMS), a map, and an English Wikipedia. Then we imported sampled data from their real-world counterparts. As an example, our version of GitLab was developed based on the actual GitLab project.1 We carefully emulated the features of a typical code repository"}, {"question": " How is the WebArena environment achieved by the authors?,answer: The authors achieve the WebArena environment by making it standalone, without relying on live websites, to ensure reproducibility, and by using open-source libraries from popular categories.", "ref_chunk": "shown in the upper left of Figure 1. We focus on evaluating the functional correctness of these tasks, i.e., does the result of the execution actually achieve the desired goal (\u00a73.2). For instance, to evaluate the example in Figure 2, our evaluation method verifies the concrete contents in the designated repository. This evaluation is not only more reliable (Zhong et al., 2017; Chen et al., 2021; Wang et al., 2022) than comparing the textual surface-form action sequences (Puig et al., 2018; Deng et al., 2023) but also accommodate a range of potential valid paths to achieve the same goal, which is a ubiquitous phenomenon in sufficiently complex tasks. We use this benchmark to evaluate several agents that can follow NL command and perform web- based tasks (\u00a74). These agents are implemented in a few-shot in-context learning fashion with powerful large language models (LLMs) such as GPT-4 and PALM-2. Experiment results show that the best GPT-4 agent performance is somewhat limited, with an end-to-end task success rate of only 14.41%, while the human performance is 78.24%. We hypothesize that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex tasks (\u00a75.2). These outcomes underscore the necessity for further development towards robust and effective agents (LeCun, 2022) in WebArena. 2 webarena.wikipedia.com Search for each art museum on the Map webarena.gitlab.com \u201c\u201d webarena.openstreetmap.comRecord the optimized results to the repo \u2026 Create an efficient itinerary to visit all of Pittsburgh's art museums with minimal driving distance starting from Schenley Park. Log the order in my \u201cawesome-northeast-us-travel\u201d repository Search for museums in Pittsburgh Under review Figure 2: A high-level task that can be fully executed in WebArena. Success requires sophisticated, long-term planning and reasoning. To accomplish the goal (top), an agent needs to (1) find Pittsburgh art museums on Wikipedia, (2) identify their locations on a map (while optimizing the itinerary), and (3) update the README file in the appropriate repository with the planned route. 2 WE BAR E N A: WEBSITES AS AN ENVIRONMENT FOR AUTONOMOUS AGENTS Our goal is to create a realistic and reproducible web environment. We achieve reproducibility by making the environment standalone, without relying on live websites. This circumvents technical challenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and configuration changes, which obstruct a fair comparison across different systems over time. We achieve realism by using open-source libraries that underlie many in-use sites from several popular categories and importing data to our environment from their real-world counterparts. 2.1 CONTROLLING AGENTS THROUGH HIGH-LEVEL NATURAL LANGUAGE The WebArena environment is denoted as E= \u27e8S, A, O, T \u27e9 with state space S, action space A (\u00a72.4) and observation space O (\u00a72.3). The transition function T : S \u00d7 A\u2212\u2192 S is deterministic, and it is defined by the underlying implementation of each website in the environment. Performing a task described by a natural language intent i can be formulated as a partially observable Markov decision process (POMDP): at each time step t, an agent issues an action at\u2208 A given the partial observation ot\u2208 O. Consequently, the action results in a new state st+1\u2208 S and its corresponding observation ot+1\u2208 O. We propose a reward function r(a, s) to measure the success of a task execution, where a represents the sequence of actions, and s denotes all intermediate states. This reward function assesses if state transitions align with the expectations of the intents. For example, with an intent to place an order, it verifies whether an order has been placed. Additionally, it evaluates the accuracy of the agent\u2019s actions, such as checking the correctness of the predicted answer. 2.2 WEBSITE SELECTION To decide which categories of websites to use, we first analyzed approximately 200 examples from the authors\u2019 actual web browser histories. Each author delved into their browsing histories, summarizing the goal of particular segments of their browser session. Based on this, we classified the visited websites into abstract categories. We then identified the four most salient categories and implemented one instance per category based on this analysis: (1) E-commerce platforms supporting online shopping activities (e.g., Amazon, eBay), (2) social forum platforms for opinion exchanges (e.g., Reddit, StackExchange), (3) collaborative development platforms for software development (e.g., GitLab), and (4) content management systems (CMS) that manage the creation and revision of the digital content (e.g., online store management). In addition to these platforms, we selected three utility-style tools that are frequently used in web- based tasks: (1) a map for navigation and searching for information about points of interest (POIs) such as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes. As information- seeking and knowledge acquisition are critical in web-based tasks, we also incorporated various knowledge resources into WebArena. These resources range from general information hubs, such as the English Wikipedia, to more specialized knowledge bases, such as the website user manuals. 3 <li> <div> <a href=\"...\"><img src=\"...\"></a> <div class> <a href=\"...\">Outdoor Patio \u2026</a> <div> <span>Rating:</span> <div> <span>82%</span> </div> <a href=\u201c\u2026#reviews\">12 <span>Reviews</span></a> webarena.onestopshop.com webarena.onestopshop.com webarena.onestopshop.com RootWebArea \u2018Patio, Lawn ..\u2019 link 'Image' img 'Image' link 'Outdoor Patio..\u2019 LayoutTable '' StaticText 'Rating:' generic '82%' link '12 Reviews' StaticText \u2018$49.99' button 'Add to Cart\u2019 focusable: True button 'Wish List\u2019 focusable: \u2026 button 'Compare\u2019 focusable: \u2026 Under review Figure 3: We design the observation to be the URL and the content of a web page, with options to represent the content as a screenshot (left), HTML DOM tree (middle), and accessibility tree (right). The content of the middle and right figures are trimmed to save space. Implementation We leveraged open-source libraries relevant to each category to build our own versions of an E-commerce website (OneStopShop), GitLab, Reddit, an online store content manage- ment system (CMS), a map, and an English Wikipedia. Then we imported sampled data from their real-world counterparts. As an example, our version of GitLab was developed based on the actual GitLab project.1 We carefully emulated the features of a typical code repository"}, {"question": " What is denoted as E= \u27e8S, A, O, T \u27e9 in the WebArena environment?,answer: E= \u27e8S, A, O, T \u27e9 in the WebArena environment denotes the state space S, action space A, observation space O, and transition function T.", "ref_chunk": "shown in the upper left of Figure 1. We focus on evaluating the functional correctness of these tasks, i.e., does the result of the execution actually achieve the desired goal (\u00a73.2). For instance, to evaluate the example in Figure 2, our evaluation method verifies the concrete contents in the designated repository. This evaluation is not only more reliable (Zhong et al., 2017; Chen et al., 2021; Wang et al., 2022) than comparing the textual surface-form action sequences (Puig et al., 2018; Deng et al., 2023) but also accommodate a range of potential valid paths to achieve the same goal, which is a ubiquitous phenomenon in sufficiently complex tasks. We use this benchmark to evaluate several agents that can follow NL command and perform web- based tasks (\u00a74). These agents are implemented in a few-shot in-context learning fashion with powerful large language models (LLMs) such as GPT-4 and PALM-2. Experiment results show that the best GPT-4 agent performance is somewhat limited, with an end-to-end task success rate of only 14.41%, while the human performance is 78.24%. We hypothesize that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex tasks (\u00a75.2). These outcomes underscore the necessity for further development towards robust and effective agents (LeCun, 2022) in WebArena. 2 webarena.wikipedia.com Search for each art museum on the Map webarena.gitlab.com \u201c\u201d webarena.openstreetmap.comRecord the optimized results to the repo \u2026 Create an efficient itinerary to visit all of Pittsburgh's art museums with minimal driving distance starting from Schenley Park. Log the order in my \u201cawesome-northeast-us-travel\u201d repository Search for museums in Pittsburgh Under review Figure 2: A high-level task that can be fully executed in WebArena. Success requires sophisticated, long-term planning and reasoning. To accomplish the goal (top), an agent needs to (1) find Pittsburgh art museums on Wikipedia, (2) identify their locations on a map (while optimizing the itinerary), and (3) update the README file in the appropriate repository with the planned route. 2 WE BAR E N A: WEBSITES AS AN ENVIRONMENT FOR AUTONOMOUS AGENTS Our goal is to create a realistic and reproducible web environment. We achieve reproducibility by making the environment standalone, without relying on live websites. This circumvents technical challenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and configuration changes, which obstruct a fair comparison across different systems over time. We achieve realism by using open-source libraries that underlie many in-use sites from several popular categories and importing data to our environment from their real-world counterparts. 2.1 CONTROLLING AGENTS THROUGH HIGH-LEVEL NATURAL LANGUAGE The WebArena environment is denoted as E= \u27e8S, A, O, T \u27e9 with state space S, action space A (\u00a72.4) and observation space O (\u00a72.3). The transition function T : S \u00d7 A\u2212\u2192 S is deterministic, and it is defined by the underlying implementation of each website in the environment. Performing a task described by a natural language intent i can be formulated as a partially observable Markov decision process (POMDP): at each time step t, an agent issues an action at\u2208 A given the partial observation ot\u2208 O. Consequently, the action results in a new state st+1\u2208 S and its corresponding observation ot+1\u2208 O. We propose a reward function r(a, s) to measure the success of a task execution, where a represents the sequence of actions, and s denotes all intermediate states. This reward function assesses if state transitions align with the expectations of the intents. For example, with an intent to place an order, it verifies whether an order has been placed. Additionally, it evaluates the accuracy of the agent\u2019s actions, such as checking the correctness of the predicted answer. 2.2 WEBSITE SELECTION To decide which categories of websites to use, we first analyzed approximately 200 examples from the authors\u2019 actual web browser histories. Each author delved into their browsing histories, summarizing the goal of particular segments of their browser session. Based on this, we classified the visited websites into abstract categories. We then identified the four most salient categories and implemented one instance per category based on this analysis: (1) E-commerce platforms supporting online shopping activities (e.g., Amazon, eBay), (2) social forum platforms for opinion exchanges (e.g., Reddit, StackExchange), (3) collaborative development platforms for software development (e.g., GitLab), and (4) content management systems (CMS) that manage the creation and revision of the digital content (e.g., online store management). In addition to these platforms, we selected three utility-style tools that are frequently used in web- based tasks: (1) a map for navigation and searching for information about points of interest (POIs) such as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes. As information- seeking and knowledge acquisition are critical in web-based tasks, we also incorporated various knowledge resources into WebArena. These resources range from general information hubs, such as the English Wikipedia, to more specialized knowledge bases, such as the website user manuals. 3 <li> <div> <a href=\"...\"><img src=\"...\"></a> <div class> <a href=\"...\">Outdoor Patio \u2026</a> <div> <span>Rating:</span> <div> <span>82%</span> </div> <a href=\u201c\u2026#reviews\">12 <span>Reviews</span></a> webarena.onestopshop.com webarena.onestopshop.com webarena.onestopshop.com RootWebArea \u2018Patio, Lawn ..\u2019 link 'Image' img 'Image' link 'Outdoor Patio..\u2019 LayoutTable '' StaticText 'Rating:' generic '82%' link '12 Reviews' StaticText \u2018$49.99' button 'Add to Cart\u2019 focusable: True button 'Wish List\u2019 focusable: \u2026 button 'Compare\u2019 focusable: \u2026 Under review Figure 3: We design the observation to be the URL and the content of a web page, with options to represent the content as a screenshot (left), HTML DOM tree (middle), and accessibility tree (right). The content of the middle and right figures are trimmed to save space. Implementation We leveraged open-source libraries relevant to each category to build our own versions of an E-commerce website (OneStopShop), GitLab, Reddit, an online store content manage- ment system (CMS), a map, and an English Wikipedia. Then we imported sampled data from their real-world counterparts. As an example, our version of GitLab was developed based on the actual GitLab project.1 We carefully emulated the features of a typical code repository"}, {"question": " How is a task described by a natural language intent formulated in the WebArena environment?,answer: A task described by a natural language intent is formulated as a partially observable Markov decision process (POMDP) in the WebArena environment.", "ref_chunk": "shown in the upper left of Figure 1. We focus on evaluating the functional correctness of these tasks, i.e., does the result of the execution actually achieve the desired goal (\u00a73.2). For instance, to evaluate the example in Figure 2, our evaluation method verifies the concrete contents in the designated repository. This evaluation is not only more reliable (Zhong et al., 2017; Chen et al., 2021; Wang et al., 2022) than comparing the textual surface-form action sequences (Puig et al., 2018; Deng et al., 2023) but also accommodate a range of potential valid paths to achieve the same goal, which is a ubiquitous phenomenon in sufficiently complex tasks. We use this benchmark to evaluate several agents that can follow NL command and perform web- based tasks (\u00a74). These agents are implemented in a few-shot in-context learning fashion with powerful large language models (LLMs) such as GPT-4 and PALM-2. Experiment results show that the best GPT-4 agent performance is somewhat limited, with an end-to-end task success rate of only 14.41%, while the human performance is 78.24%. We hypothesize that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex tasks (\u00a75.2). These outcomes underscore the necessity for further development towards robust and effective agents (LeCun, 2022) in WebArena. 2 webarena.wikipedia.com Search for each art museum on the Map webarena.gitlab.com \u201c\u201d webarena.openstreetmap.comRecord the optimized results to the repo \u2026 Create an efficient itinerary to visit all of Pittsburgh's art museums with minimal driving distance starting from Schenley Park. Log the order in my \u201cawesome-northeast-us-travel\u201d repository Search for museums in Pittsburgh Under review Figure 2: A high-level task that can be fully executed in WebArena. Success requires sophisticated, long-term planning and reasoning. To accomplish the goal (top), an agent needs to (1) find Pittsburgh art museums on Wikipedia, (2) identify their locations on a map (while optimizing the itinerary), and (3) update the README file in the appropriate repository with the planned route. 2 WE BAR E N A: WEBSITES AS AN ENVIRONMENT FOR AUTONOMOUS AGENTS Our goal is to create a realistic and reproducible web environment. We achieve reproducibility by making the environment standalone, without relying on live websites. This circumvents technical challenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and configuration changes, which obstruct a fair comparison across different systems over time. We achieve realism by using open-source libraries that underlie many in-use sites from several popular categories and importing data to our environment from their real-world counterparts. 2.1 CONTROLLING AGENTS THROUGH HIGH-LEVEL NATURAL LANGUAGE The WebArena environment is denoted as E= \u27e8S, A, O, T \u27e9 with state space S, action space A (\u00a72.4) and observation space O (\u00a72.3). The transition function T : S \u00d7 A\u2212\u2192 S is deterministic, and it is defined by the underlying implementation of each website in the environment. Performing a task described by a natural language intent i can be formulated as a partially observable Markov decision process (POMDP): at each time step t, an agent issues an action at\u2208 A given the partial observation ot\u2208 O. Consequently, the action results in a new state st+1\u2208 S and its corresponding observation ot+1\u2208 O. We propose a reward function r(a, s) to measure the success of a task execution, where a represents the sequence of actions, and s denotes all intermediate states. This reward function assesses if state transitions align with the expectations of the intents. For example, with an intent to place an order, it verifies whether an order has been placed. Additionally, it evaluates the accuracy of the agent\u2019s actions, such as checking the correctness of the predicted answer. 2.2 WEBSITE SELECTION To decide which categories of websites to use, we first analyzed approximately 200 examples from the authors\u2019 actual web browser histories. Each author delved into their browsing histories, summarizing the goal of particular segments of their browser session. Based on this, we classified the visited websites into abstract categories. We then identified the four most salient categories and implemented one instance per category based on this analysis: (1) E-commerce platforms supporting online shopping activities (e.g., Amazon, eBay), (2) social forum platforms for opinion exchanges (e.g., Reddit, StackExchange), (3) collaborative development platforms for software development (e.g., GitLab), and (4) content management systems (CMS) that manage the creation and revision of the digital content (e.g., online store management). In addition to these platforms, we selected three utility-style tools that are frequently used in web- based tasks: (1) a map for navigation and searching for information about points of interest (POIs) such as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes. As information- seeking and knowledge acquisition are critical in web-based tasks, we also incorporated various knowledge resources into WebArena. These resources range from general information hubs, such as the English Wikipedia, to more specialized knowledge bases, such as the website user manuals. 3 <li> <div> <a href=\"...\"><img src=\"...\"></a> <div class> <a href=\"...\">Outdoor Patio \u2026</a> <div> <span>Rating:</span> <div> <span>82%</span> </div> <a href=\u201c\u2026#reviews\">12 <span>Reviews</span></a> webarena.onestopshop.com webarena.onestopshop.com webarena.onestopshop.com RootWebArea \u2018Patio, Lawn ..\u2019 link 'Image' img 'Image' link 'Outdoor Patio..\u2019 LayoutTable '' StaticText 'Rating:' generic '82%' link '12 Reviews' StaticText \u2018$49.99' button 'Add to Cart\u2019 focusable: True button 'Wish List\u2019 focusable: \u2026 button 'Compare\u2019 focusable: \u2026 Under review Figure 3: We design the observation to be the URL and the content of a web page, with options to represent the content as a screenshot (left), HTML DOM tree (middle), and accessibility tree (right). The content of the middle and right figures are trimmed to save space. Implementation We leveraged open-source libraries relevant to each category to build our own versions of an E-commerce website (OneStopShop), GitLab, Reddit, an online store content manage- ment system (CMS), a map, and an English Wikipedia. Then we imported sampled data from their real-world counterparts. As an example, our version of GitLab was developed based on the actual GitLab project.1 We carefully emulated the features of a typical code repository"}, {"question": " What method is proposed to measure the success of a task execution in the WebArena environment?,answer: A reward function r(a, s) is proposed to measure the success of a task execution in the WebArena environment.", "ref_chunk": "shown in the upper left of Figure 1. We focus on evaluating the functional correctness of these tasks, i.e., does the result of the execution actually achieve the desired goal (\u00a73.2). For instance, to evaluate the example in Figure 2, our evaluation method verifies the concrete contents in the designated repository. This evaluation is not only more reliable (Zhong et al., 2017; Chen et al., 2021; Wang et al., 2022) than comparing the textual surface-form action sequences (Puig et al., 2018; Deng et al., 2023) but also accommodate a range of potential valid paths to achieve the same goal, which is a ubiquitous phenomenon in sufficiently complex tasks. We use this benchmark to evaluate several agents that can follow NL command and perform web- based tasks (\u00a74). These agents are implemented in a few-shot in-context learning fashion with powerful large language models (LLMs) such as GPT-4 and PALM-2. Experiment results show that the best GPT-4 agent performance is somewhat limited, with an end-to-end task success rate of only 14.41%, while the human performance is 78.24%. We hypothesize that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex tasks (\u00a75.2). These outcomes underscore the necessity for further development towards robust and effective agents (LeCun, 2022) in WebArena. 2 webarena.wikipedia.com Search for each art museum on the Map webarena.gitlab.com \u201c\u201d webarena.openstreetmap.comRecord the optimized results to the repo \u2026 Create an efficient itinerary to visit all of Pittsburgh's art museums with minimal driving distance starting from Schenley Park. Log the order in my \u201cawesome-northeast-us-travel\u201d repository Search for museums in Pittsburgh Under review Figure 2: A high-level task that can be fully executed in WebArena. Success requires sophisticated, long-term planning and reasoning. To accomplish the goal (top), an agent needs to (1) find Pittsburgh art museums on Wikipedia, (2) identify their locations on a map (while optimizing the itinerary), and (3) update the README file in the appropriate repository with the planned route. 2 WE BAR E N A: WEBSITES AS AN ENVIRONMENT FOR AUTONOMOUS AGENTS Our goal is to create a realistic and reproducible web environment. We achieve reproducibility by making the environment standalone, without relying on live websites. This circumvents technical challenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and configuration changes, which obstruct a fair comparison across different systems over time. We achieve realism by using open-source libraries that underlie many in-use sites from several popular categories and importing data to our environment from their real-world counterparts. 2.1 CONTROLLING AGENTS THROUGH HIGH-LEVEL NATURAL LANGUAGE The WebArena environment is denoted as E= \u27e8S, A, O, T \u27e9 with state space S, action space A (\u00a72.4) and observation space O (\u00a72.3). The transition function T : S \u00d7 A\u2212\u2192 S is deterministic, and it is defined by the underlying implementation of each website in the environment. Performing a task described by a natural language intent i can be formulated as a partially observable Markov decision process (POMDP): at each time step t, an agent issues an action at\u2208 A given the partial observation ot\u2208 O. Consequently, the action results in a new state st+1\u2208 S and its corresponding observation ot+1\u2208 O. We propose a reward function r(a, s) to measure the success of a task execution, where a represents the sequence of actions, and s denotes all intermediate states. This reward function assesses if state transitions align with the expectations of the intents. For example, with an intent to place an order, it verifies whether an order has been placed. Additionally, it evaluates the accuracy of the agent\u2019s actions, such as checking the correctness of the predicted answer. 2.2 WEBSITE SELECTION To decide which categories of websites to use, we first analyzed approximately 200 examples from the authors\u2019 actual web browser histories. Each author delved into their browsing histories, summarizing the goal of particular segments of their browser session. Based on this, we classified the visited websites into abstract categories. We then identified the four most salient categories and implemented one instance per category based on this analysis: (1) E-commerce platforms supporting online shopping activities (e.g., Amazon, eBay), (2) social forum platforms for opinion exchanges (e.g., Reddit, StackExchange), (3) collaborative development platforms for software development (e.g., GitLab), and (4) content management systems (CMS) that manage the creation and revision of the digital content (e.g., online store management). In addition to these platforms, we selected three utility-style tools that are frequently used in web- based tasks: (1) a map for navigation and searching for information about points of interest (POIs) such as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes. As information- seeking and knowledge acquisition are critical in web-based tasks, we also incorporated various knowledge resources into WebArena. These resources range from general information hubs, such as the English Wikipedia, to more specialized knowledge bases, such as the website user manuals. 3 <li> <div> <a href=\"...\"><img src=\"...\"></a> <div class> <a href=\"...\">Outdoor Patio \u2026</a> <div> <span>Rating:</span> <div> <span>82%</span> </div> <a href=\u201c\u2026#reviews\">12 <span>Reviews</span></a> webarena.onestopshop.com webarena.onestopshop.com webarena.onestopshop.com RootWebArea \u2018Patio, Lawn ..\u2019 link 'Image' img 'Image' link 'Outdoor Patio..\u2019 LayoutTable '' StaticText 'Rating:' generic '82%' link '12 Reviews' StaticText \u2018$49.99' button 'Add to Cart\u2019 focusable: True button 'Wish List\u2019 focusable: \u2026 button 'Compare\u2019 focusable: \u2026 Under review Figure 3: We design the observation to be the URL and the content of a web page, with options to represent the content as a screenshot (left), HTML DOM tree (middle), and accessibility tree (right). The content of the middle and right figures are trimmed to save space. Implementation We leveraged open-source libraries relevant to each category to build our own versions of an E-commerce website (OneStopShop), GitLab, Reddit, an online store content manage- ment system (CMS), a map, and an English Wikipedia. Then we imported sampled data from their real-world counterparts. As an example, our version of GitLab was developed based on the actual GitLab project.1 We carefully emulated the features of a typical code repository"}, {"question": " How did the authors decide which categories of websites to use in WebArena?,answer: The authors analyzed their browsing histories and identified categories such as E-commerce platforms, social forum platforms, collaborative development platforms, and content management systems, among others.", "ref_chunk": "shown in the upper left of Figure 1. We focus on evaluating the functional correctness of these tasks, i.e., does the result of the execution actually achieve the desired goal (\u00a73.2). For instance, to evaluate the example in Figure 2, our evaluation method verifies the concrete contents in the designated repository. This evaluation is not only more reliable (Zhong et al., 2017; Chen et al., 2021; Wang et al., 2022) than comparing the textual surface-form action sequences (Puig et al., 2018; Deng et al., 2023) but also accommodate a range of potential valid paths to achieve the same goal, which is a ubiquitous phenomenon in sufficiently complex tasks. We use this benchmark to evaluate several agents that can follow NL command and perform web- based tasks (\u00a74). These agents are implemented in a few-shot in-context learning fashion with powerful large language models (LLMs) such as GPT-4 and PALM-2. Experiment results show that the best GPT-4 agent performance is somewhat limited, with an end-to-end task success rate of only 14.41%, while the human performance is 78.24%. We hypothesize that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex tasks (\u00a75.2). These outcomes underscore the necessity for further development towards robust and effective agents (LeCun, 2022) in WebArena. 2 webarena.wikipedia.com Search for each art museum on the Map webarena.gitlab.com \u201c\u201d webarena.openstreetmap.comRecord the optimized results to the repo \u2026 Create an efficient itinerary to visit all of Pittsburgh's art museums with minimal driving distance starting from Schenley Park. Log the order in my \u201cawesome-northeast-us-travel\u201d repository Search for museums in Pittsburgh Under review Figure 2: A high-level task that can be fully executed in WebArena. Success requires sophisticated, long-term planning and reasoning. To accomplish the goal (top), an agent needs to (1) find Pittsburgh art museums on Wikipedia, (2) identify their locations on a map (while optimizing the itinerary), and (3) update the README file in the appropriate repository with the planned route. 2 WE BAR E N A: WEBSITES AS AN ENVIRONMENT FOR AUTONOMOUS AGENTS Our goal is to create a realistic and reproducible web environment. We achieve reproducibility by making the environment standalone, without relying on live websites. This circumvents technical challenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and configuration changes, which obstruct a fair comparison across different systems over time. We achieve realism by using open-source libraries that underlie many in-use sites from several popular categories and importing data to our environment from their real-world counterparts. 2.1 CONTROLLING AGENTS THROUGH HIGH-LEVEL NATURAL LANGUAGE The WebArena environment is denoted as E= \u27e8S, A, O, T \u27e9 with state space S, action space A (\u00a72.4) and observation space O (\u00a72.3). The transition function T : S \u00d7 A\u2212\u2192 S is deterministic, and it is defined by the underlying implementation of each website in the environment. Performing a task described by a natural language intent i can be formulated as a partially observable Markov decision process (POMDP): at each time step t, an agent issues an action at\u2208 A given the partial observation ot\u2208 O. Consequently, the action results in a new state st+1\u2208 S and its corresponding observation ot+1\u2208 O. We propose a reward function r(a, s) to measure the success of a task execution, where a represents the sequence of actions, and s denotes all intermediate states. This reward function assesses if state transitions align with the expectations of the intents. For example, with an intent to place an order, it verifies whether an order has been placed. Additionally, it evaluates the accuracy of the agent\u2019s actions, such as checking the correctness of the predicted answer. 2.2 WEBSITE SELECTION To decide which categories of websites to use, we first analyzed approximately 200 examples from the authors\u2019 actual web browser histories. Each author delved into their browsing histories, summarizing the goal of particular segments of their browser session. Based on this, we classified the visited websites into abstract categories. We then identified the four most salient categories and implemented one instance per category based on this analysis: (1) E-commerce platforms supporting online shopping activities (e.g., Amazon, eBay), (2) social forum platforms for opinion exchanges (e.g., Reddit, StackExchange), (3) collaborative development platforms for software development (e.g., GitLab), and (4) content management systems (CMS) that manage the creation and revision of the digital content (e.g., online store management). In addition to these platforms, we selected three utility-style tools that are frequently used in web- based tasks: (1) a map for navigation and searching for information about points of interest (POIs) such as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes. As information- seeking and knowledge acquisition are critical in web-based tasks, we also incorporated various knowledge resources into WebArena. These resources range from general information hubs, such as the English Wikipedia, to more specialized knowledge bases, such as the website user manuals. 3 <li> <div> <a href=\"...\"><img src=\"...\"></a> <div class> <a href=\"...\">Outdoor Patio \u2026</a> <div> <span>Rating:</span> <div> <span>82%</span> </div> <a href=\u201c\u2026#reviews\">12 <span>Reviews</span></a> webarena.onestopshop.com webarena.onestopshop.com webarena.onestopshop.com RootWebArea \u2018Patio, Lawn ..\u2019 link 'Image' img 'Image' link 'Outdoor Patio..\u2019 LayoutTable '' StaticText 'Rating:' generic '82%' link '12 Reviews' StaticText \u2018$49.99' button 'Add to Cart\u2019 focusable: True button 'Wish List\u2019 focusable: \u2026 button 'Compare\u2019 focusable: \u2026 Under review Figure 3: We design the observation to be the URL and the content of a web page, with options to represent the content as a screenshot (left), HTML DOM tree (middle), and accessibility tree (right). The content of the middle and right figures are trimmed to save space. Implementation We leveraged open-source libraries relevant to each category to build our own versions of an E-commerce website (OneStopShop), GitLab, Reddit, an online store content manage- ment system (CMS), a map, and an English Wikipedia. Then we imported sampled data from their real-world counterparts. As an example, our version of GitLab was developed based on the actual GitLab project.1 We carefully emulated the features of a typical code repository"}, {"question": " What utility-style tools were selected in addition to the platforms for use in web-based tasks in WebArena?,answer: In addition to platforms, utility-style tools such as a map for navigation, a calculator, and a scratchpad for taking notes were selected for use in web-based tasks.", "ref_chunk": "shown in the upper left of Figure 1. We focus on evaluating the functional correctness of these tasks, i.e., does the result of the execution actually achieve the desired goal (\u00a73.2). For instance, to evaluate the example in Figure 2, our evaluation method verifies the concrete contents in the designated repository. This evaluation is not only more reliable (Zhong et al., 2017; Chen et al., 2021; Wang et al., 2022) than comparing the textual surface-form action sequences (Puig et al., 2018; Deng et al., 2023) but also accommodate a range of potential valid paths to achieve the same goal, which is a ubiquitous phenomenon in sufficiently complex tasks. We use this benchmark to evaluate several agents that can follow NL command and perform web- based tasks (\u00a74). These agents are implemented in a few-shot in-context learning fashion with powerful large language models (LLMs) such as GPT-4 and PALM-2. Experiment results show that the best GPT-4 agent performance is somewhat limited, with an end-to-end task success rate of only 14.41%, while the human performance is 78.24%. We hypothesize that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex tasks (\u00a75.2). These outcomes underscore the necessity for further development towards robust and effective agents (LeCun, 2022) in WebArena. 2 webarena.wikipedia.com Search for each art museum on the Map webarena.gitlab.com \u201c\u201d webarena.openstreetmap.comRecord the optimized results to the repo \u2026 Create an efficient itinerary to visit all of Pittsburgh's art museums with minimal driving distance starting from Schenley Park. Log the order in my \u201cawesome-northeast-us-travel\u201d repository Search for museums in Pittsburgh Under review Figure 2: A high-level task that can be fully executed in WebArena. Success requires sophisticated, long-term planning and reasoning. To accomplish the goal (top), an agent needs to (1) find Pittsburgh art museums on Wikipedia, (2) identify their locations on a map (while optimizing the itinerary), and (3) update the README file in the appropriate repository with the planned route. 2 WE BAR E N A: WEBSITES AS AN ENVIRONMENT FOR AUTONOMOUS AGENTS Our goal is to create a realistic and reproducible web environment. We achieve reproducibility by making the environment standalone, without relying on live websites. This circumvents technical challenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and configuration changes, which obstruct a fair comparison across different systems over time. We achieve realism by using open-source libraries that underlie many in-use sites from several popular categories and importing data to our environment from their real-world counterparts. 2.1 CONTROLLING AGENTS THROUGH HIGH-LEVEL NATURAL LANGUAGE The WebArena environment is denoted as E= \u27e8S, A, O, T \u27e9 with state space S, action space A (\u00a72.4) and observation space O (\u00a72.3). The transition function T : S \u00d7 A\u2212\u2192 S is deterministic, and it is defined by the underlying implementation of each website in the environment. Performing a task described by a natural language intent i can be formulated as a partially observable Markov decision process (POMDP): at each time step t, an agent issues an action at\u2208 A given the partial observation ot\u2208 O. Consequently, the action results in a new state st+1\u2208 S and its corresponding observation ot+1\u2208 O. We propose a reward function r(a, s) to measure the success of a task execution, where a represents the sequence of actions, and s denotes all intermediate states. This reward function assesses if state transitions align with the expectations of the intents. For example, with an intent to place an order, it verifies whether an order has been placed. Additionally, it evaluates the accuracy of the agent\u2019s actions, such as checking the correctness of the predicted answer. 2.2 WEBSITE SELECTION To decide which categories of websites to use, we first analyzed approximately 200 examples from the authors\u2019 actual web browser histories. Each author delved into their browsing histories, summarizing the goal of particular segments of their browser session. Based on this, we classified the visited websites into abstract categories. We then identified the four most salient categories and implemented one instance per category based on this analysis: (1) E-commerce platforms supporting online shopping activities (e.g., Amazon, eBay), (2) social forum platforms for opinion exchanges (e.g., Reddit, StackExchange), (3) collaborative development platforms for software development (e.g., GitLab), and (4) content management systems (CMS) that manage the creation and revision of the digital content (e.g., online store management). In addition to these platforms, we selected three utility-style tools that are frequently used in web- based tasks: (1) a map for navigation and searching for information about points of interest (POIs) such as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes. As information- seeking and knowledge acquisition are critical in web-based tasks, we also incorporated various knowledge resources into WebArena. These resources range from general information hubs, such as the English Wikipedia, to more specialized knowledge bases, such as the website user manuals. 3 <li> <div> <a href=\"...\"><img src=\"...\"></a> <div class> <a href=\"...\">Outdoor Patio \u2026</a> <div> <span>Rating:</span> <div> <span>82%</span> </div> <a href=\u201c\u2026#reviews\">12 <span>Reviews</span></a> webarena.onestopshop.com webarena.onestopshop.com webarena.onestopshop.com RootWebArea \u2018Patio, Lawn ..\u2019 link 'Image' img 'Image' link 'Outdoor Patio..\u2019 LayoutTable '' StaticText 'Rating:' generic '82%' link '12 Reviews' StaticText \u2018$49.99' button 'Add to Cart\u2019 focusable: True button 'Wish List\u2019 focusable: \u2026 button 'Compare\u2019 focusable: \u2026 Under review Figure 3: We design the observation to be the URL and the content of a web page, with options to represent the content as a screenshot (left), HTML DOM tree (middle), and accessibility tree (right). The content of the middle and right figures are trimmed to save space. Implementation We leveraged open-source libraries relevant to each category to build our own versions of an E-commerce website (OneStopShop), GitLab, Reddit, an online store content manage- ment system (CMS), a map, and an English Wikipedia. Then we imported sampled data from their real-world counterparts. As an example, our version of GitLab was developed based on the actual GitLab project.1 We carefully emulated the features of a typical code repository"}, {"question": " What range of knowledge resources were incorporated into WebArena?,answer: Various knowledge resources ranging from general information hubs like English Wikipedia to specialized knowledge bases such as website user manuals were incorporated into WebArena.", "ref_chunk": "shown in the upper left of Figure 1. We focus on evaluating the functional correctness of these tasks, i.e., does the result of the execution actually achieve the desired goal (\u00a73.2). For instance, to evaluate the example in Figure 2, our evaluation method verifies the concrete contents in the designated repository. This evaluation is not only more reliable (Zhong et al., 2017; Chen et al., 2021; Wang et al., 2022) than comparing the textual surface-form action sequences (Puig et al., 2018; Deng et al., 2023) but also accommodate a range of potential valid paths to achieve the same goal, which is a ubiquitous phenomenon in sufficiently complex tasks. We use this benchmark to evaluate several agents that can follow NL command and perform web- based tasks (\u00a74). These agents are implemented in a few-shot in-context learning fashion with powerful large language models (LLMs) such as GPT-4 and PALM-2. Experiment results show that the best GPT-4 agent performance is somewhat limited, with an end-to-end task success rate of only 14.41%, while the human performance is 78.24%. We hypothesize that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex tasks (\u00a75.2). These outcomes underscore the necessity for further development towards robust and effective agents (LeCun, 2022) in WebArena. 2 webarena.wikipedia.com Search for each art museum on the Map webarena.gitlab.com \u201c\u201d webarena.openstreetmap.comRecord the optimized results to the repo \u2026 Create an efficient itinerary to visit all of Pittsburgh's art museums with minimal driving distance starting from Schenley Park. Log the order in my \u201cawesome-northeast-us-travel\u201d repository Search for museums in Pittsburgh Under review Figure 2: A high-level task that can be fully executed in WebArena. Success requires sophisticated, long-term planning and reasoning. To accomplish the goal (top), an agent needs to (1) find Pittsburgh art museums on Wikipedia, (2) identify their locations on a map (while optimizing the itinerary), and (3) update the README file in the appropriate repository with the planned route. 2 WE BAR E N A: WEBSITES AS AN ENVIRONMENT FOR AUTONOMOUS AGENTS Our goal is to create a realistic and reproducible web environment. We achieve reproducibility by making the environment standalone, without relying on live websites. This circumvents technical challenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and configuration changes, which obstruct a fair comparison across different systems over time. We achieve realism by using open-source libraries that underlie many in-use sites from several popular categories and importing data to our environment from their real-world counterparts. 2.1 CONTROLLING AGENTS THROUGH HIGH-LEVEL NATURAL LANGUAGE The WebArena environment is denoted as E= \u27e8S, A, O, T \u27e9 with state space S, action space A (\u00a72.4) and observation space O (\u00a72.3). The transition function T : S \u00d7 A\u2212\u2192 S is deterministic, and it is defined by the underlying implementation of each website in the environment. Performing a task described by a natural language intent i can be formulated as a partially observable Markov decision process (POMDP): at each time step t, an agent issues an action at\u2208 A given the partial observation ot\u2208 O. Consequently, the action results in a new state st+1\u2208 S and its corresponding observation ot+1\u2208 O. We propose a reward function r(a, s) to measure the success of a task execution, where a represents the sequence of actions, and s denotes all intermediate states. This reward function assesses if state transitions align with the expectations of the intents. For example, with an intent to place an order, it verifies whether an order has been placed. Additionally, it evaluates the accuracy of the agent\u2019s actions, such as checking the correctness of the predicted answer. 2.2 WEBSITE SELECTION To decide which categories of websites to use, we first analyzed approximately 200 examples from the authors\u2019 actual web browser histories. Each author delved into their browsing histories, summarizing the goal of particular segments of their browser session. Based on this, we classified the visited websites into abstract categories. We then identified the four most salient categories and implemented one instance per category based on this analysis: (1) E-commerce platforms supporting online shopping activities (e.g., Amazon, eBay), (2) social forum platforms for opinion exchanges (e.g., Reddit, StackExchange), (3) collaborative development platforms for software development (e.g., GitLab), and (4) content management systems (CMS) that manage the creation and revision of the digital content (e.g., online store management). In addition to these platforms, we selected three utility-style tools that are frequently used in web- based tasks: (1) a map for navigation and searching for information about points of interest (POIs) such as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes. As information- seeking and knowledge acquisition are critical in web-based tasks, we also incorporated various knowledge resources into WebArena. These resources range from general information hubs, such as the English Wikipedia, to more specialized knowledge bases, such as the website user manuals. 3 <li> <div> <a href=\"...\"><img src=\"...\"></a> <div class> <a href=\"...\">Outdoor Patio \u2026</a> <div> <span>Rating:</span> <div> <span>82%</span> </div> <a href=\u201c\u2026#reviews\">12 <span>Reviews</span></a> webarena.onestopshop.com webarena.onestopshop.com webarena.onestopshop.com RootWebArea \u2018Patio, Lawn ..\u2019 link 'Image' img 'Image' link 'Outdoor Patio..\u2019 LayoutTable '' StaticText 'Rating:' generic '82%' link '12 Reviews' StaticText \u2018$49.99' button 'Add to Cart\u2019 focusable: True button 'Wish List\u2019 focusable: \u2026 button 'Compare\u2019 focusable: \u2026 Under review Figure 3: We design the observation to be the URL and the content of a web page, with options to represent the content as a screenshot (left), HTML DOM tree (middle), and accessibility tree (right). The content of the middle and right figures are trimmed to save space. Implementation We leveraged open-source libraries relevant to each category to build our own versions of an E-commerce website (OneStopShop), GitLab, Reddit, an online store content manage- ment system (CMS), a map, and an English Wikipedia. Then we imported sampled data from their real-world counterparts. As an example, our version of GitLab was developed based on the actual GitLab project.1 We carefully emulated the features of a typical code repository"}], "doc_text": "shown in the upper left of Figure 1. We focus on evaluating the functional correctness of these tasks, i.e., does the result of the execution actually achieve the desired goal (\u00a73.2). For instance, to evaluate the example in Figure 2, our evaluation method verifies the concrete contents in the designated repository. This evaluation is not only more reliable (Zhong et al., 2017; Chen et al., 2021; Wang et al., 2022) than comparing the textual surface-form action sequences (Puig et al., 2018; Deng et al., 2023) but also accommodate a range of potential valid paths to achieve the same goal, which is a ubiquitous phenomenon in sufficiently complex tasks. We use this benchmark to evaluate several agents that can follow NL command and perform web- based tasks (\u00a74). These agents are implemented in a few-shot in-context learning fashion with powerful large language models (LLMs) such as GPT-4 and PALM-2. Experiment results show that the best GPT-4 agent performance is somewhat limited, with an end-to-end task success rate of only 14.41%, while the human performance is 78.24%. We hypothesize that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex tasks (\u00a75.2). These outcomes underscore the necessity for further development towards robust and effective agents (LeCun, 2022) in WebArena. 2 webarena.wikipedia.com Search for each art museum on the Map webarena.gitlab.com \u201c\u201d webarena.openstreetmap.comRecord the optimized results to the repo \u2026 Create an efficient itinerary to visit all of Pittsburgh's art museums with minimal driving distance starting from Schenley Park. Log the order in my \u201cawesome-northeast-us-travel\u201d repository Search for museums in Pittsburgh Under review Figure 2: A high-level task that can be fully executed in WebArena. Success requires sophisticated, long-term planning and reasoning. To accomplish the goal (top), an agent needs to (1) find Pittsburgh art museums on Wikipedia, (2) identify their locations on a map (while optimizing the itinerary), and (3) update the README file in the appropriate repository with the planned route. 2 WE BAR E N A: WEBSITES AS AN ENVIRONMENT FOR AUTONOMOUS AGENTS Our goal is to create a realistic and reproducible web environment. We achieve reproducibility by making the environment standalone, without relying on live websites. This circumvents technical challenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and configuration changes, which obstruct a fair comparison across different systems over time. We achieve realism by using open-source libraries that underlie many in-use sites from several popular categories and importing data to our environment from their real-world counterparts. 2.1 CONTROLLING AGENTS THROUGH HIGH-LEVEL NATURAL LANGUAGE The WebArena environment is denoted as E= \u27e8S, A, O, T \u27e9 with state space S, action space A (\u00a72.4) and observation space O (\u00a72.3). The transition function T : S \u00d7 A\u2212\u2192 S is deterministic, and it is defined by the underlying implementation of each website in the environment. Performing a task described by a natural language intent i can be formulated as a partially observable Markov decision process (POMDP): at each time step t, an agent issues an action at\u2208 A given the partial observation ot\u2208 O. Consequently, the action results in a new state st+1\u2208 S and its corresponding observation ot+1\u2208 O. We propose a reward function r(a, s) to measure the success of a task execution, where a represents the sequence of actions, and s denotes all intermediate states. This reward function assesses if state transitions align with the expectations of the intents. For example, with an intent to place an order, it verifies whether an order has been placed. Additionally, it evaluates the accuracy of the agent\u2019s actions, such as checking the correctness of the predicted answer. 2.2 WEBSITE SELECTION To decide which categories of websites to use, we first analyzed approximately 200 examples from the authors\u2019 actual web browser histories. Each author delved into their browsing histories, summarizing the goal of particular segments of their browser session. Based on this, we classified the visited websites into abstract categories. We then identified the four most salient categories and implemented one instance per category based on this analysis: (1) E-commerce platforms supporting online shopping activities (e.g., Amazon, eBay), (2) social forum platforms for opinion exchanges (e.g., Reddit, StackExchange), (3) collaborative development platforms for software development (e.g., GitLab), and (4) content management systems (CMS) that manage the creation and revision of the digital content (e.g., online store management). In addition to these platforms, we selected three utility-style tools that are frequently used in web- based tasks: (1) a map for navigation and searching for information about points of interest (POIs) such as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes. As information- seeking and knowledge acquisition are critical in web-based tasks, we also incorporated various knowledge resources into WebArena. These resources range from general information hubs, such as the English Wikipedia, to more specialized knowledge bases, such as the website user manuals. 3 <li> <div> <a href=\"...\"><img src=\"...\"></a> <div class> <a href=\"...\">Outdoor Patio \u2026</a> <div> <span>Rating:</span> <div> <span>82%</span> </div> <a href=\u201c\u2026#reviews\">12 <span>Reviews</span></a> webarena.onestopshop.com webarena.onestopshop.com webarena.onestopshop.com RootWebArea \u2018Patio, Lawn ..\u2019 link 'Image' img 'Image' link 'Outdoor Patio..\u2019 LayoutTable '' StaticText 'Rating:' generic '82%' link '12 Reviews' StaticText \u2018$49.99' button 'Add to Cart\u2019 focusable: True button 'Wish List\u2019 focusable: \u2026 button 'Compare\u2019 focusable: \u2026 Under review Figure 3: We design the observation to be the URL and the content of a web page, with options to represent the content as a screenshot (left), HTML DOM tree (middle), and accessibility tree (right). The content of the middle and right figures are trimmed to save space. Implementation We leveraged open-source libraries relevant to each category to build our own versions of an E-commerce website (OneStopShop), GitLab, Reddit, an online store content manage- ment system (CMS), a map, and an English Wikipedia. Then we imported sampled data from their real-world counterparts. As an example, our version of GitLab was developed based on the actual GitLab project.1 We carefully emulated the features of a typical code repository"}