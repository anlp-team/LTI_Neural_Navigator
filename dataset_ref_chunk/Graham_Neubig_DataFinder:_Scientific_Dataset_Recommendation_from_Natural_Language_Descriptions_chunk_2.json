{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_DataFinder:_Scientific_Dataset_Recommendation_from_Natural_Language_Descriptions_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the new task established in the text?", "answer": " Automatically recommending relevant datasets given a description of a data-driven system.", "ref_chunk": "2 Dataset Recommendation Task We establish a new task for automatically recom- mending relevant datasets given a description of a data-driven system. Given a query q and a set of datasets D, retrieve the most relevant subset R \u2282 D one could use to test the idea described in q. Figure 1 illustrates this with a real query written by a graduate student. The query q can take two forms: either a key- word query (the predominant interface for dataset search today (Chapman et al., 2019)) or a full- sentence description. Textual descriptions offer a more flexible input to the recommendation system, with the ability to implicitly specify constraints based on what a researcher wants to study, without needing to carefully construct keywords a priori. Evaluation Metrics Our task framing naturally leads to evaluation by information retrieval met- rics that estimate search relevance. In our experi- ments, we use four common metrics included in the trec_eval package,4 a standard evaluation tool used in the IR community: Precision@k: The proportion of relevant items in top k retrieved datasets. If P@k is 1, then every retrieved document is valuable. Recall@k: The fraction of relevant items that are retrieved. If R@k is 1, then the search results are comprehensive. Mean Average Precision (MAP): Assuming we have m relevant datasets in total, and ki is the rank of the ith relevant dataset, MAP is calcu- lated as (cid:80)m i P@ki/m (Manning et al., 2005). High MAP indicates strong average search qual- ity over all relevant datasets. Mean Reciprocal Rank (MRR): The average of the inverse of the ranks at which the first relevant item was retrieved. Assuming Ri is the rank of the i-th relevant item in the retrieved result, M RR is calculated as (cid:80)m i Ri/m. High MRR means a user sees at least some relevant datasets early in the search results. 4https://github.com/usnistgov/trec_eval. We use the -c flag for the trec_eval command. Galactica(Taylor et al, 2022) Evaluation Dataset Tagger Negative Examples Papers with Code 3 Ranking1) MARCO2) CNN/DM3) SQuAD 4 Struct. Metadata Training Citances Collection of Datasets Human Annotation S2ORC(Lo et al, 2020) Queries 7 1 Intro. Paper Title Qry1 Qry1 Positive Examples Dataset Description Qry2 Qry2 RelevantDatasets NegativeMining Ranker 1 2 2 2 Relevant Datasets Testing 5 5 5 Training Queries SciREX(Jain et al, 2020) Figure 3: We search against all datasets on Papers With Code. Our system is trained on a set of simulated queries and target datasets and evaluated on a set of expert- written queries with hand-annotated target datasets. 3 The DataFinder Dataset To support this task, we construct a dataset called The DataFinder Dataset consisting of (q, R) pairs extracted from published English-language scien- tific proceedings, where each q is either a full- sentence description or a keyword query. We collect a large training set through an automated method (for scalability), and we collect a smaller test set using real users\u2019 annotations (for reliable and realistic model evaluation). In both cases, our data collection contains two primary steps: (1) col- lecting search queries q that a user would use to describe their dataset needs, and (2) identifying relevant datasets R that match the query. Our final training and test sets contain 17495 and 392 queries, respectively. Figure 3 summarizes our data collection approach. We explain the details below and provide further discussion of the limitations of our dataset in the Limitations section. We will re- lease our data under a permissive CC-BY License. 3.1 Collection of Datasets In our task definition, we search over the collec- tion of datasets listed on Papers With Code, a large public index of papers which includes metadata for over 7000 datasets and benchmarks. For most datasets, Papers With Code Datasets stores a short human-written dataset description, a list of differ- ent names used to refer to the dataset (known as \u201cvariants\u201d), and structured metadata such as the year released, the number of papers reported as using the dataset, the tasks contained, and the the modal- ity of data. Many datasets also include the paper that introduced the dataset. We used the dataset de- scription, structured metadata, and the introducing paper\u2019s title to textually represent each dataset, and we analyze this design decision in \u00a75.4. 3.2 Training Set Construction To ensure scalability for the training set, we rely on a large corpus of scientific papers, S2ORC (Lo et al., 2020). We extract nearly 20,000 abstracts from AI papers that use datasets. To overcome the high cost of manually-annotating queries or relevant datasets, we instead simulate annotations with few-shot-learning and rule-based methods. Query Collection We extract queries from pa- per abstracts because, intuitively, an abstract will contain the most salient characteristics behind a research idea or contribution. As a result, it is an ideal source for comprehensively collecting poten- tial implicit constraints as shown in Figure 1. We simulate query collection with the 6.7B pa- rameter version of Galactica (Taylor et al., 2022), a large scientific language model that supports few- shot learning. In our prompt, we give the model an abstract and ask it to first extract five keyphrases: the tasks mentioned in paper, the task domain of the paper (e.g., biomedical or aerial), the modal- ity of data required, the language of data or labels required, and the length of text required (sentence- level, paragraph-level, or none mentioned). We then ask Galactica to generate a full query contain- ing any salient keyphrases. We perform few-shot learning using 3 examples in the prompt to guide the model. Our prompt is shown in Appendix A. Relevant Datasets For our training set, relevant datasets are automatically labeled using the body text of a paper.5 We apply a rule-based procedure to identify the dataset used in a given paper (cor- responding to an abstract whose query has been auto-labeled). For each paper, we tag all datasets that satisfy two conditions: the paper must cite the paper that introduces the dataset, and the paper must mention the dataset by name twice.6 5Note that our queries are"}, {"question": " What two forms can the query q take in the context of the described task?", "answer": " Keyword query or a full-sentence description.", "ref_chunk": "2 Dataset Recommendation Task We establish a new task for automatically recom- mending relevant datasets given a description of a data-driven system. Given a query q and a set of datasets D, retrieve the most relevant subset R \u2282 D one could use to test the idea described in q. Figure 1 illustrates this with a real query written by a graduate student. The query q can take two forms: either a key- word query (the predominant interface for dataset search today (Chapman et al., 2019)) or a full- sentence description. Textual descriptions offer a more flexible input to the recommendation system, with the ability to implicitly specify constraints based on what a researcher wants to study, without needing to carefully construct keywords a priori. Evaluation Metrics Our task framing naturally leads to evaluation by information retrieval met- rics that estimate search relevance. In our experi- ments, we use four common metrics included in the trec_eval package,4 a standard evaluation tool used in the IR community: Precision@k: The proportion of relevant items in top k retrieved datasets. If P@k is 1, then every retrieved document is valuable. Recall@k: The fraction of relevant items that are retrieved. If R@k is 1, then the search results are comprehensive. Mean Average Precision (MAP): Assuming we have m relevant datasets in total, and ki is the rank of the ith relevant dataset, MAP is calcu- lated as (cid:80)m i P@ki/m (Manning et al., 2005). High MAP indicates strong average search qual- ity over all relevant datasets. Mean Reciprocal Rank (MRR): The average of the inverse of the ranks at which the first relevant item was retrieved. Assuming Ri is the rank of the i-th relevant item in the retrieved result, M RR is calculated as (cid:80)m i Ri/m. High MRR means a user sees at least some relevant datasets early in the search results. 4https://github.com/usnistgov/trec_eval. We use the -c flag for the trec_eval command. Galactica(Taylor et al, 2022) Evaluation Dataset Tagger Negative Examples Papers with Code 3 Ranking1) MARCO2) CNN/DM3) SQuAD 4 Struct. Metadata Training Citances Collection of Datasets Human Annotation S2ORC(Lo et al, 2020) Queries 7 1 Intro. Paper Title Qry1 Qry1 Positive Examples Dataset Description Qry2 Qry2 RelevantDatasets NegativeMining Ranker 1 2 2 2 Relevant Datasets Testing 5 5 5 Training Queries SciREX(Jain et al, 2020) Figure 3: We search against all datasets on Papers With Code. Our system is trained on a set of simulated queries and target datasets and evaluated on a set of expert- written queries with hand-annotated target datasets. 3 The DataFinder Dataset To support this task, we construct a dataset called The DataFinder Dataset consisting of (q, R) pairs extracted from published English-language scien- tific proceedings, where each q is either a full- sentence description or a keyword query. We collect a large training set through an automated method (for scalability), and we collect a smaller test set using real users\u2019 annotations (for reliable and realistic model evaluation). In both cases, our data collection contains two primary steps: (1) col- lecting search queries q that a user would use to describe their dataset needs, and (2) identifying relevant datasets R that match the query. Our final training and test sets contain 17495 and 392 queries, respectively. Figure 3 summarizes our data collection approach. We explain the details below and provide further discussion of the limitations of our dataset in the Limitations section. We will re- lease our data under a permissive CC-BY License. 3.1 Collection of Datasets In our task definition, we search over the collec- tion of datasets listed on Papers With Code, a large public index of papers which includes metadata for over 7000 datasets and benchmarks. For most datasets, Papers With Code Datasets stores a short human-written dataset description, a list of differ- ent names used to refer to the dataset (known as \u201cvariants\u201d), and structured metadata such as the year released, the number of papers reported as using the dataset, the tasks contained, and the the modal- ity of data. Many datasets also include the paper that introduced the dataset. We used the dataset de- scription, structured metadata, and the introducing paper\u2019s title to textually represent each dataset, and we analyze this design decision in \u00a75.4. 3.2 Training Set Construction To ensure scalability for the training set, we rely on a large corpus of scientific papers, S2ORC (Lo et al., 2020). We extract nearly 20,000 abstracts from AI papers that use datasets. To overcome the high cost of manually-annotating queries or relevant datasets, we instead simulate annotations with few-shot-learning and rule-based methods. Query Collection We extract queries from pa- per abstracts because, intuitively, an abstract will contain the most salient characteristics behind a research idea or contribution. As a result, it is an ideal source for comprehensively collecting poten- tial implicit constraints as shown in Figure 1. We simulate query collection with the 6.7B pa- rameter version of Galactica (Taylor et al., 2022), a large scientific language model that supports few- shot learning. In our prompt, we give the model an abstract and ask it to first extract five keyphrases: the tasks mentioned in paper, the task domain of the paper (e.g., biomedical or aerial), the modal- ity of data required, the language of data or labels required, and the length of text required (sentence- level, paragraph-level, or none mentioned). We then ask Galactica to generate a full query contain- ing any salient keyphrases. We perform few-shot learning using 3 examples in the prompt to guide the model. Our prompt is shown in Appendix A. Relevant Datasets For our training set, relevant datasets are automatically labeled using the body text of a paper.5 We apply a rule-based procedure to identify the dataset used in a given paper (cor- responding to an abstract whose query has been auto-labeled). For each paper, we tag all datasets that satisfy two conditions: the paper must cite the paper that introduces the dataset, and the paper must mention the dataset by name twice.6 5Note that our queries are"}, {"question": " What are the four common evaluation metrics used in the experiments mentioned in the text?", "answer": " Precision@k, Recall@k, Mean Average Precision (MAP), Mean Reciprocal Rank (MRR).", "ref_chunk": "2 Dataset Recommendation Task We establish a new task for automatically recom- mending relevant datasets given a description of a data-driven system. Given a query q and a set of datasets D, retrieve the most relevant subset R \u2282 D one could use to test the idea described in q. Figure 1 illustrates this with a real query written by a graduate student. The query q can take two forms: either a key- word query (the predominant interface for dataset search today (Chapman et al., 2019)) or a full- sentence description. Textual descriptions offer a more flexible input to the recommendation system, with the ability to implicitly specify constraints based on what a researcher wants to study, without needing to carefully construct keywords a priori. Evaluation Metrics Our task framing naturally leads to evaluation by information retrieval met- rics that estimate search relevance. In our experi- ments, we use four common metrics included in the trec_eval package,4 a standard evaluation tool used in the IR community: Precision@k: The proportion of relevant items in top k retrieved datasets. If P@k is 1, then every retrieved document is valuable. Recall@k: The fraction of relevant items that are retrieved. If R@k is 1, then the search results are comprehensive. Mean Average Precision (MAP): Assuming we have m relevant datasets in total, and ki is the rank of the ith relevant dataset, MAP is calcu- lated as (cid:80)m i P@ki/m (Manning et al., 2005). High MAP indicates strong average search qual- ity over all relevant datasets. Mean Reciprocal Rank (MRR): The average of the inverse of the ranks at which the first relevant item was retrieved. Assuming Ri is the rank of the i-th relevant item in the retrieved result, M RR is calculated as (cid:80)m i Ri/m. High MRR means a user sees at least some relevant datasets early in the search results. 4https://github.com/usnistgov/trec_eval. We use the -c flag for the trec_eval command. Galactica(Taylor et al, 2022) Evaluation Dataset Tagger Negative Examples Papers with Code 3 Ranking1) MARCO2) CNN/DM3) SQuAD 4 Struct. Metadata Training Citances Collection of Datasets Human Annotation S2ORC(Lo et al, 2020) Queries 7 1 Intro. Paper Title Qry1 Qry1 Positive Examples Dataset Description Qry2 Qry2 RelevantDatasets NegativeMining Ranker 1 2 2 2 Relevant Datasets Testing 5 5 5 Training Queries SciREX(Jain et al, 2020) Figure 3: We search against all datasets on Papers With Code. Our system is trained on a set of simulated queries and target datasets and evaluated on a set of expert- written queries with hand-annotated target datasets. 3 The DataFinder Dataset To support this task, we construct a dataset called The DataFinder Dataset consisting of (q, R) pairs extracted from published English-language scien- tific proceedings, where each q is either a full- sentence description or a keyword query. We collect a large training set through an automated method (for scalability), and we collect a smaller test set using real users\u2019 annotations (for reliable and realistic model evaluation). In both cases, our data collection contains two primary steps: (1) col- lecting search queries q that a user would use to describe their dataset needs, and (2) identifying relevant datasets R that match the query. Our final training and test sets contain 17495 and 392 queries, respectively. Figure 3 summarizes our data collection approach. We explain the details below and provide further discussion of the limitations of our dataset in the Limitations section. We will re- lease our data under a permissive CC-BY License. 3.1 Collection of Datasets In our task definition, we search over the collec- tion of datasets listed on Papers With Code, a large public index of papers which includes metadata for over 7000 datasets and benchmarks. For most datasets, Papers With Code Datasets stores a short human-written dataset description, a list of differ- ent names used to refer to the dataset (known as \u201cvariants\u201d), and structured metadata such as the year released, the number of papers reported as using the dataset, the tasks contained, and the the modal- ity of data. Many datasets also include the paper that introduced the dataset. We used the dataset de- scription, structured metadata, and the introducing paper\u2019s title to textually represent each dataset, and we analyze this design decision in \u00a75.4. 3.2 Training Set Construction To ensure scalability for the training set, we rely on a large corpus of scientific papers, S2ORC (Lo et al., 2020). We extract nearly 20,000 abstracts from AI papers that use datasets. To overcome the high cost of manually-annotating queries or relevant datasets, we instead simulate annotations with few-shot-learning and rule-based methods. Query Collection We extract queries from pa- per abstracts because, intuitively, an abstract will contain the most salient characteristics behind a research idea or contribution. As a result, it is an ideal source for comprehensively collecting poten- tial implicit constraints as shown in Figure 1. We simulate query collection with the 6.7B pa- rameter version of Galactica (Taylor et al., 2022), a large scientific language model that supports few- shot learning. In our prompt, we give the model an abstract and ask it to first extract five keyphrases: the tasks mentioned in paper, the task domain of the paper (e.g., biomedical or aerial), the modal- ity of data required, the language of data or labels required, and the length of text required (sentence- level, paragraph-level, or none mentioned). We then ask Galactica to generate a full query contain- ing any salient keyphrases. We perform few-shot learning using 3 examples in the prompt to guide the model. Our prompt is shown in Appendix A. Relevant Datasets For our training set, relevant datasets are automatically labeled using the body text of a paper.5 We apply a rule-based procedure to identify the dataset used in a given paper (cor- responding to an abstract whose query has been auto-labeled). For each paper, we tag all datasets that satisfy two conditions: the paper must cite the paper that introduces the dataset, and the paper must mention the dataset by name twice.6 5Note that our queries are"}, {"question": " What does Precision@k measure in the evaluation?", "answer": " The proportion of relevant items in the top k retrieved datasets.", "ref_chunk": "2 Dataset Recommendation Task We establish a new task for automatically recom- mending relevant datasets given a description of a data-driven system. Given a query q and a set of datasets D, retrieve the most relevant subset R \u2282 D one could use to test the idea described in q. Figure 1 illustrates this with a real query written by a graduate student. The query q can take two forms: either a key- word query (the predominant interface for dataset search today (Chapman et al., 2019)) or a full- sentence description. Textual descriptions offer a more flexible input to the recommendation system, with the ability to implicitly specify constraints based on what a researcher wants to study, without needing to carefully construct keywords a priori. Evaluation Metrics Our task framing naturally leads to evaluation by information retrieval met- rics that estimate search relevance. In our experi- ments, we use four common metrics included in the trec_eval package,4 a standard evaluation tool used in the IR community: Precision@k: The proportion of relevant items in top k retrieved datasets. If P@k is 1, then every retrieved document is valuable. Recall@k: The fraction of relevant items that are retrieved. If R@k is 1, then the search results are comprehensive. Mean Average Precision (MAP): Assuming we have m relevant datasets in total, and ki is the rank of the ith relevant dataset, MAP is calcu- lated as (cid:80)m i P@ki/m (Manning et al., 2005). High MAP indicates strong average search qual- ity over all relevant datasets. Mean Reciprocal Rank (MRR): The average of the inverse of the ranks at which the first relevant item was retrieved. Assuming Ri is the rank of the i-th relevant item in the retrieved result, M RR is calculated as (cid:80)m i Ri/m. High MRR means a user sees at least some relevant datasets early in the search results. 4https://github.com/usnistgov/trec_eval. We use the -c flag for the trec_eval command. Galactica(Taylor et al, 2022) Evaluation Dataset Tagger Negative Examples Papers with Code 3 Ranking1) MARCO2) CNN/DM3) SQuAD 4 Struct. Metadata Training Citances Collection of Datasets Human Annotation S2ORC(Lo et al, 2020) Queries 7 1 Intro. Paper Title Qry1 Qry1 Positive Examples Dataset Description Qry2 Qry2 RelevantDatasets NegativeMining Ranker 1 2 2 2 Relevant Datasets Testing 5 5 5 Training Queries SciREX(Jain et al, 2020) Figure 3: We search against all datasets on Papers With Code. Our system is trained on a set of simulated queries and target datasets and evaluated on a set of expert- written queries with hand-annotated target datasets. 3 The DataFinder Dataset To support this task, we construct a dataset called The DataFinder Dataset consisting of (q, R) pairs extracted from published English-language scien- tific proceedings, where each q is either a full- sentence description or a keyword query. We collect a large training set through an automated method (for scalability), and we collect a smaller test set using real users\u2019 annotations (for reliable and realistic model evaluation). In both cases, our data collection contains two primary steps: (1) col- lecting search queries q that a user would use to describe their dataset needs, and (2) identifying relevant datasets R that match the query. Our final training and test sets contain 17495 and 392 queries, respectively. Figure 3 summarizes our data collection approach. We explain the details below and provide further discussion of the limitations of our dataset in the Limitations section. We will re- lease our data under a permissive CC-BY License. 3.1 Collection of Datasets In our task definition, we search over the collec- tion of datasets listed on Papers With Code, a large public index of papers which includes metadata for over 7000 datasets and benchmarks. For most datasets, Papers With Code Datasets stores a short human-written dataset description, a list of differ- ent names used to refer to the dataset (known as \u201cvariants\u201d), and structured metadata such as the year released, the number of papers reported as using the dataset, the tasks contained, and the the modal- ity of data. Many datasets also include the paper that introduced the dataset. We used the dataset de- scription, structured metadata, and the introducing paper\u2019s title to textually represent each dataset, and we analyze this design decision in \u00a75.4. 3.2 Training Set Construction To ensure scalability for the training set, we rely on a large corpus of scientific papers, S2ORC (Lo et al., 2020). We extract nearly 20,000 abstracts from AI papers that use datasets. To overcome the high cost of manually-annotating queries or relevant datasets, we instead simulate annotations with few-shot-learning and rule-based methods. Query Collection We extract queries from pa- per abstracts because, intuitively, an abstract will contain the most salient characteristics behind a research idea or contribution. As a result, it is an ideal source for comprehensively collecting poten- tial implicit constraints as shown in Figure 1. We simulate query collection with the 6.7B pa- rameter version of Galactica (Taylor et al., 2022), a large scientific language model that supports few- shot learning. In our prompt, we give the model an abstract and ask it to first extract five keyphrases: the tasks mentioned in paper, the task domain of the paper (e.g., biomedical or aerial), the modal- ity of data required, the language of data or labels required, and the length of text required (sentence- level, paragraph-level, or none mentioned). We then ask Galactica to generate a full query contain- ing any salient keyphrases. We perform few-shot learning using 3 examples in the prompt to guide the model. Our prompt is shown in Appendix A. Relevant Datasets For our training set, relevant datasets are automatically labeled using the body text of a paper.5 We apply a rule-based procedure to identify the dataset used in a given paper (cor- responding to an abstract whose query has been auto-labeled). For each paper, we tag all datasets that satisfy two conditions: the paper must cite the paper that introduces the dataset, and the paper must mention the dataset by name twice.6 5Note that our queries are"}, {"question": " What does MRR stand for and how is it calculated?", "answer": " Mean Reciprocal Rank. It is calculated as the average of the inverse of the ranks at which the first relevant item was retrieved.", "ref_chunk": "2 Dataset Recommendation Task We establish a new task for automatically recom- mending relevant datasets given a description of a data-driven system. Given a query q and a set of datasets D, retrieve the most relevant subset R \u2282 D one could use to test the idea described in q. Figure 1 illustrates this with a real query written by a graduate student. The query q can take two forms: either a key- word query (the predominant interface for dataset search today (Chapman et al., 2019)) or a full- sentence description. Textual descriptions offer a more flexible input to the recommendation system, with the ability to implicitly specify constraints based on what a researcher wants to study, without needing to carefully construct keywords a priori. Evaluation Metrics Our task framing naturally leads to evaluation by information retrieval met- rics that estimate search relevance. In our experi- ments, we use four common metrics included in the trec_eval package,4 a standard evaluation tool used in the IR community: Precision@k: The proportion of relevant items in top k retrieved datasets. If P@k is 1, then every retrieved document is valuable. Recall@k: The fraction of relevant items that are retrieved. If R@k is 1, then the search results are comprehensive. Mean Average Precision (MAP): Assuming we have m relevant datasets in total, and ki is the rank of the ith relevant dataset, MAP is calcu- lated as (cid:80)m i P@ki/m (Manning et al., 2005). High MAP indicates strong average search qual- ity over all relevant datasets. Mean Reciprocal Rank (MRR): The average of the inverse of the ranks at which the first relevant item was retrieved. Assuming Ri is the rank of the i-th relevant item in the retrieved result, M RR is calculated as (cid:80)m i Ri/m. High MRR means a user sees at least some relevant datasets early in the search results. 4https://github.com/usnistgov/trec_eval. We use the -c flag for the trec_eval command. Galactica(Taylor et al, 2022) Evaluation Dataset Tagger Negative Examples Papers with Code 3 Ranking1) MARCO2) CNN/DM3) SQuAD 4 Struct. Metadata Training Citances Collection of Datasets Human Annotation S2ORC(Lo et al, 2020) Queries 7 1 Intro. Paper Title Qry1 Qry1 Positive Examples Dataset Description Qry2 Qry2 RelevantDatasets NegativeMining Ranker 1 2 2 2 Relevant Datasets Testing 5 5 5 Training Queries SciREX(Jain et al, 2020) Figure 3: We search against all datasets on Papers With Code. Our system is trained on a set of simulated queries and target datasets and evaluated on a set of expert- written queries with hand-annotated target datasets. 3 The DataFinder Dataset To support this task, we construct a dataset called The DataFinder Dataset consisting of (q, R) pairs extracted from published English-language scien- tific proceedings, where each q is either a full- sentence description or a keyword query. We collect a large training set through an automated method (for scalability), and we collect a smaller test set using real users\u2019 annotations (for reliable and realistic model evaluation). In both cases, our data collection contains two primary steps: (1) col- lecting search queries q that a user would use to describe their dataset needs, and (2) identifying relevant datasets R that match the query. Our final training and test sets contain 17495 and 392 queries, respectively. Figure 3 summarizes our data collection approach. We explain the details below and provide further discussion of the limitations of our dataset in the Limitations section. We will re- lease our data under a permissive CC-BY License. 3.1 Collection of Datasets In our task definition, we search over the collec- tion of datasets listed on Papers With Code, a large public index of papers which includes metadata for over 7000 datasets and benchmarks. For most datasets, Papers With Code Datasets stores a short human-written dataset description, a list of differ- ent names used to refer to the dataset (known as \u201cvariants\u201d), and structured metadata such as the year released, the number of papers reported as using the dataset, the tasks contained, and the the modal- ity of data. Many datasets also include the paper that introduced the dataset. We used the dataset de- scription, structured metadata, and the introducing paper\u2019s title to textually represent each dataset, and we analyze this design decision in \u00a75.4. 3.2 Training Set Construction To ensure scalability for the training set, we rely on a large corpus of scientific papers, S2ORC (Lo et al., 2020). We extract nearly 20,000 abstracts from AI papers that use datasets. To overcome the high cost of manually-annotating queries or relevant datasets, we instead simulate annotations with few-shot-learning and rule-based methods. Query Collection We extract queries from pa- per abstracts because, intuitively, an abstract will contain the most salient characteristics behind a research idea or contribution. As a result, it is an ideal source for comprehensively collecting poten- tial implicit constraints as shown in Figure 1. We simulate query collection with the 6.7B pa- rameter version of Galactica (Taylor et al., 2022), a large scientific language model that supports few- shot learning. In our prompt, we give the model an abstract and ask it to first extract five keyphrases: the tasks mentioned in paper, the task domain of the paper (e.g., biomedical or aerial), the modal- ity of data required, the language of data or labels required, and the length of text required (sentence- level, paragraph-level, or none mentioned). We then ask Galactica to generate a full query contain- ing any salient keyphrases. We perform few-shot learning using 3 examples in the prompt to guide the model. Our prompt is shown in Appendix A. Relevant Datasets For our training set, relevant datasets are automatically labeled using the body text of a paper.5 We apply a rule-based procedure to identify the dataset used in a given paper (cor- responding to an abstract whose query has been auto-labeled). For each paper, we tag all datasets that satisfy two conditions: the paper must cite the paper that introduces the dataset, and the paper must mention the dataset by name twice.6 5Note that our queries are"}, {"question": " What dataset is constructed to support the task mentioned in the text?", "answer": " The DataFinder Dataset.", "ref_chunk": "2 Dataset Recommendation Task We establish a new task for automatically recom- mending relevant datasets given a description of a data-driven system. Given a query q and a set of datasets D, retrieve the most relevant subset R \u2282 D one could use to test the idea described in q. Figure 1 illustrates this with a real query written by a graduate student. The query q can take two forms: either a key- word query (the predominant interface for dataset search today (Chapman et al., 2019)) or a full- sentence description. Textual descriptions offer a more flexible input to the recommendation system, with the ability to implicitly specify constraints based on what a researcher wants to study, without needing to carefully construct keywords a priori. Evaluation Metrics Our task framing naturally leads to evaluation by information retrieval met- rics that estimate search relevance. In our experi- ments, we use four common metrics included in the trec_eval package,4 a standard evaluation tool used in the IR community: Precision@k: The proportion of relevant items in top k retrieved datasets. If P@k is 1, then every retrieved document is valuable. Recall@k: The fraction of relevant items that are retrieved. If R@k is 1, then the search results are comprehensive. Mean Average Precision (MAP): Assuming we have m relevant datasets in total, and ki is the rank of the ith relevant dataset, MAP is calcu- lated as (cid:80)m i P@ki/m (Manning et al., 2005). High MAP indicates strong average search qual- ity over all relevant datasets. Mean Reciprocal Rank (MRR): The average of the inverse of the ranks at which the first relevant item was retrieved. Assuming Ri is the rank of the i-th relevant item in the retrieved result, M RR is calculated as (cid:80)m i Ri/m. High MRR means a user sees at least some relevant datasets early in the search results. 4https://github.com/usnistgov/trec_eval. We use the -c flag for the trec_eval command. Galactica(Taylor et al, 2022) Evaluation Dataset Tagger Negative Examples Papers with Code 3 Ranking1) MARCO2) CNN/DM3) SQuAD 4 Struct. Metadata Training Citances Collection of Datasets Human Annotation S2ORC(Lo et al, 2020) Queries 7 1 Intro. Paper Title Qry1 Qry1 Positive Examples Dataset Description Qry2 Qry2 RelevantDatasets NegativeMining Ranker 1 2 2 2 Relevant Datasets Testing 5 5 5 Training Queries SciREX(Jain et al, 2020) Figure 3: We search against all datasets on Papers With Code. Our system is trained on a set of simulated queries and target datasets and evaluated on a set of expert- written queries with hand-annotated target datasets. 3 The DataFinder Dataset To support this task, we construct a dataset called The DataFinder Dataset consisting of (q, R) pairs extracted from published English-language scien- tific proceedings, where each q is either a full- sentence description or a keyword query. We collect a large training set through an automated method (for scalability), and we collect a smaller test set using real users\u2019 annotations (for reliable and realistic model evaluation). In both cases, our data collection contains two primary steps: (1) col- lecting search queries q that a user would use to describe their dataset needs, and (2) identifying relevant datasets R that match the query. Our final training and test sets contain 17495 and 392 queries, respectively. Figure 3 summarizes our data collection approach. We explain the details below and provide further discussion of the limitations of our dataset in the Limitations section. We will re- lease our data under a permissive CC-BY License. 3.1 Collection of Datasets In our task definition, we search over the collec- tion of datasets listed on Papers With Code, a large public index of papers which includes metadata for over 7000 datasets and benchmarks. For most datasets, Papers With Code Datasets stores a short human-written dataset description, a list of differ- ent names used to refer to the dataset (known as \u201cvariants\u201d), and structured metadata such as the year released, the number of papers reported as using the dataset, the tasks contained, and the the modal- ity of data. Many datasets also include the paper that introduced the dataset. We used the dataset de- scription, structured metadata, and the introducing paper\u2019s title to textually represent each dataset, and we analyze this design decision in \u00a75.4. 3.2 Training Set Construction To ensure scalability for the training set, we rely on a large corpus of scientific papers, S2ORC (Lo et al., 2020). We extract nearly 20,000 abstracts from AI papers that use datasets. To overcome the high cost of manually-annotating queries or relevant datasets, we instead simulate annotations with few-shot-learning and rule-based methods. Query Collection We extract queries from pa- per abstracts because, intuitively, an abstract will contain the most salient characteristics behind a research idea or contribution. As a result, it is an ideal source for comprehensively collecting poten- tial implicit constraints as shown in Figure 1. We simulate query collection with the 6.7B pa- rameter version of Galactica (Taylor et al., 2022), a large scientific language model that supports few- shot learning. In our prompt, we give the model an abstract and ask it to first extract five keyphrases: the tasks mentioned in paper, the task domain of the paper (e.g., biomedical or aerial), the modal- ity of data required, the language of data or labels required, and the length of text required (sentence- level, paragraph-level, or none mentioned). We then ask Galactica to generate a full query contain- ing any salient keyphrases. We perform few-shot learning using 3 examples in the prompt to guide the model. Our prompt is shown in Appendix A. Relevant Datasets For our training set, relevant datasets are automatically labeled using the body text of a paper.5 We apply a rule-based procedure to identify the dataset used in a given paper (cor- responding to an abstract whose query has been auto-labeled). For each paper, we tag all datasets that satisfy two conditions: the paper must cite the paper that introduces the dataset, and the paper must mention the dataset by name twice.6 5Note that our queries are"}, {"question": " How many queries are there in the final training set for the task?", "answer": " 17495 queries.", "ref_chunk": "2 Dataset Recommendation Task We establish a new task for automatically recom- mending relevant datasets given a description of a data-driven system. Given a query q and a set of datasets D, retrieve the most relevant subset R \u2282 D one could use to test the idea described in q. Figure 1 illustrates this with a real query written by a graduate student. The query q can take two forms: either a key- word query (the predominant interface for dataset search today (Chapman et al., 2019)) or a full- sentence description. Textual descriptions offer a more flexible input to the recommendation system, with the ability to implicitly specify constraints based on what a researcher wants to study, without needing to carefully construct keywords a priori. Evaluation Metrics Our task framing naturally leads to evaluation by information retrieval met- rics that estimate search relevance. In our experi- ments, we use four common metrics included in the trec_eval package,4 a standard evaluation tool used in the IR community: Precision@k: The proportion of relevant items in top k retrieved datasets. If P@k is 1, then every retrieved document is valuable. Recall@k: The fraction of relevant items that are retrieved. If R@k is 1, then the search results are comprehensive. Mean Average Precision (MAP): Assuming we have m relevant datasets in total, and ki is the rank of the ith relevant dataset, MAP is calcu- lated as (cid:80)m i P@ki/m (Manning et al., 2005). High MAP indicates strong average search qual- ity over all relevant datasets. Mean Reciprocal Rank (MRR): The average of the inverse of the ranks at which the first relevant item was retrieved. Assuming Ri is the rank of the i-th relevant item in the retrieved result, M RR is calculated as (cid:80)m i Ri/m. High MRR means a user sees at least some relevant datasets early in the search results. 4https://github.com/usnistgov/trec_eval. We use the -c flag for the trec_eval command. Galactica(Taylor et al, 2022) Evaluation Dataset Tagger Negative Examples Papers with Code 3 Ranking1) MARCO2) CNN/DM3) SQuAD 4 Struct. Metadata Training Citances Collection of Datasets Human Annotation S2ORC(Lo et al, 2020) Queries 7 1 Intro. Paper Title Qry1 Qry1 Positive Examples Dataset Description Qry2 Qry2 RelevantDatasets NegativeMining Ranker 1 2 2 2 Relevant Datasets Testing 5 5 5 Training Queries SciREX(Jain et al, 2020) Figure 3: We search against all datasets on Papers With Code. Our system is trained on a set of simulated queries and target datasets and evaluated on a set of expert- written queries with hand-annotated target datasets. 3 The DataFinder Dataset To support this task, we construct a dataset called The DataFinder Dataset consisting of (q, R) pairs extracted from published English-language scien- tific proceedings, where each q is either a full- sentence description or a keyword query. We collect a large training set through an automated method (for scalability), and we collect a smaller test set using real users\u2019 annotations (for reliable and realistic model evaluation). In both cases, our data collection contains two primary steps: (1) col- lecting search queries q that a user would use to describe their dataset needs, and (2) identifying relevant datasets R that match the query. Our final training and test sets contain 17495 and 392 queries, respectively. Figure 3 summarizes our data collection approach. We explain the details below and provide further discussion of the limitations of our dataset in the Limitations section. We will re- lease our data under a permissive CC-BY License. 3.1 Collection of Datasets In our task definition, we search over the collec- tion of datasets listed on Papers With Code, a large public index of papers which includes metadata for over 7000 datasets and benchmarks. For most datasets, Papers With Code Datasets stores a short human-written dataset description, a list of differ- ent names used to refer to the dataset (known as \u201cvariants\u201d), and structured metadata such as the year released, the number of papers reported as using the dataset, the tasks contained, and the the modal- ity of data. Many datasets also include the paper that introduced the dataset. We used the dataset de- scription, structured metadata, and the introducing paper\u2019s title to textually represent each dataset, and we analyze this design decision in \u00a75.4. 3.2 Training Set Construction To ensure scalability for the training set, we rely on a large corpus of scientific papers, S2ORC (Lo et al., 2020). We extract nearly 20,000 abstracts from AI papers that use datasets. To overcome the high cost of manually-annotating queries or relevant datasets, we instead simulate annotations with few-shot-learning and rule-based methods. Query Collection We extract queries from pa- per abstracts because, intuitively, an abstract will contain the most salient characteristics behind a research idea or contribution. As a result, it is an ideal source for comprehensively collecting poten- tial implicit constraints as shown in Figure 1. We simulate query collection with the 6.7B pa- rameter version of Galactica (Taylor et al., 2022), a large scientific language model that supports few- shot learning. In our prompt, we give the model an abstract and ask it to first extract five keyphrases: the tasks mentioned in paper, the task domain of the paper (e.g., biomedical or aerial), the modal- ity of data required, the language of data or labels required, and the length of text required (sentence- level, paragraph-level, or none mentioned). We then ask Galactica to generate a full query contain- ing any salient keyphrases. We perform few-shot learning using 3 examples in the prompt to guide the model. Our prompt is shown in Appendix A. Relevant Datasets For our training set, relevant datasets are automatically labeled using the body text of a paper.5 We apply a rule-based procedure to identify the dataset used in a given paper (cor- responding to an abstract whose query has been auto-labeled). For each paper, we tag all datasets that satisfy two conditions: the paper must cite the paper that introduces the dataset, and the paper must mention the dataset by name twice.6 5Note that our queries are"}, {"question": " Where do they search for datasets in the task definition?", "answer": " Over the collection of datasets listed on Papers With Code.", "ref_chunk": "2 Dataset Recommendation Task We establish a new task for automatically recom- mending relevant datasets given a description of a data-driven system. Given a query q and a set of datasets D, retrieve the most relevant subset R \u2282 D one could use to test the idea described in q. Figure 1 illustrates this with a real query written by a graduate student. The query q can take two forms: either a key- word query (the predominant interface for dataset search today (Chapman et al., 2019)) or a full- sentence description. Textual descriptions offer a more flexible input to the recommendation system, with the ability to implicitly specify constraints based on what a researcher wants to study, without needing to carefully construct keywords a priori. Evaluation Metrics Our task framing naturally leads to evaluation by information retrieval met- rics that estimate search relevance. In our experi- ments, we use four common metrics included in the trec_eval package,4 a standard evaluation tool used in the IR community: Precision@k: The proportion of relevant items in top k retrieved datasets. If P@k is 1, then every retrieved document is valuable. Recall@k: The fraction of relevant items that are retrieved. If R@k is 1, then the search results are comprehensive. Mean Average Precision (MAP): Assuming we have m relevant datasets in total, and ki is the rank of the ith relevant dataset, MAP is calcu- lated as (cid:80)m i P@ki/m (Manning et al., 2005). High MAP indicates strong average search qual- ity over all relevant datasets. Mean Reciprocal Rank (MRR): The average of the inverse of the ranks at which the first relevant item was retrieved. Assuming Ri is the rank of the i-th relevant item in the retrieved result, M RR is calculated as (cid:80)m i Ri/m. High MRR means a user sees at least some relevant datasets early in the search results. 4https://github.com/usnistgov/trec_eval. We use the -c flag for the trec_eval command. Galactica(Taylor et al, 2022) Evaluation Dataset Tagger Negative Examples Papers with Code 3 Ranking1) MARCO2) CNN/DM3) SQuAD 4 Struct. Metadata Training Citances Collection of Datasets Human Annotation S2ORC(Lo et al, 2020) Queries 7 1 Intro. Paper Title Qry1 Qry1 Positive Examples Dataset Description Qry2 Qry2 RelevantDatasets NegativeMining Ranker 1 2 2 2 Relevant Datasets Testing 5 5 5 Training Queries SciREX(Jain et al, 2020) Figure 3: We search against all datasets on Papers With Code. Our system is trained on a set of simulated queries and target datasets and evaluated on a set of expert- written queries with hand-annotated target datasets. 3 The DataFinder Dataset To support this task, we construct a dataset called The DataFinder Dataset consisting of (q, R) pairs extracted from published English-language scien- tific proceedings, where each q is either a full- sentence description or a keyword query. We collect a large training set through an automated method (for scalability), and we collect a smaller test set using real users\u2019 annotations (for reliable and realistic model evaluation). In both cases, our data collection contains two primary steps: (1) col- lecting search queries q that a user would use to describe their dataset needs, and (2) identifying relevant datasets R that match the query. Our final training and test sets contain 17495 and 392 queries, respectively. Figure 3 summarizes our data collection approach. We explain the details below and provide further discussion of the limitations of our dataset in the Limitations section. We will re- lease our data under a permissive CC-BY License. 3.1 Collection of Datasets In our task definition, we search over the collec- tion of datasets listed on Papers With Code, a large public index of papers which includes metadata for over 7000 datasets and benchmarks. For most datasets, Papers With Code Datasets stores a short human-written dataset description, a list of differ- ent names used to refer to the dataset (known as \u201cvariants\u201d), and structured metadata such as the year released, the number of papers reported as using the dataset, the tasks contained, and the the modal- ity of data. Many datasets also include the paper that introduced the dataset. We used the dataset de- scription, structured metadata, and the introducing paper\u2019s title to textually represent each dataset, and we analyze this design decision in \u00a75.4. 3.2 Training Set Construction To ensure scalability for the training set, we rely on a large corpus of scientific papers, S2ORC (Lo et al., 2020). We extract nearly 20,000 abstracts from AI papers that use datasets. To overcome the high cost of manually-annotating queries or relevant datasets, we instead simulate annotations with few-shot-learning and rule-based methods. Query Collection We extract queries from pa- per abstracts because, intuitively, an abstract will contain the most salient characteristics behind a research idea or contribution. As a result, it is an ideal source for comprehensively collecting poten- tial implicit constraints as shown in Figure 1. We simulate query collection with the 6.7B pa- rameter version of Galactica (Taylor et al., 2022), a large scientific language model that supports few- shot learning. In our prompt, we give the model an abstract and ask it to first extract five keyphrases: the tasks mentioned in paper, the task domain of the paper (e.g., biomedical or aerial), the modal- ity of data required, the language of data or labels required, and the length of text required (sentence- level, paragraph-level, or none mentioned). We then ask Galactica to generate a full query contain- ing any salient keyphrases. We perform few-shot learning using 3 examples in the prompt to guide the model. Our prompt is shown in Appendix A. Relevant Datasets For our training set, relevant datasets are automatically labeled using the body text of a paper.5 We apply a rule-based procedure to identify the dataset used in a given paper (cor- responding to an abstract whose query has been auto-labeled). For each paper, we tag all datasets that satisfy two conditions: the paper must cite the paper that introduces the dataset, and the paper must mention the dataset by name twice.6 5Note that our queries are"}, {"question": " How are relevant datasets labeled in the training set?", "answer": " Automatically labeled using the body text of a paper.", "ref_chunk": "2 Dataset Recommendation Task We establish a new task for automatically recom- mending relevant datasets given a description of a data-driven system. Given a query q and a set of datasets D, retrieve the most relevant subset R \u2282 D one could use to test the idea described in q. Figure 1 illustrates this with a real query written by a graduate student. The query q can take two forms: either a key- word query (the predominant interface for dataset search today (Chapman et al., 2019)) or a full- sentence description. Textual descriptions offer a more flexible input to the recommendation system, with the ability to implicitly specify constraints based on what a researcher wants to study, without needing to carefully construct keywords a priori. Evaluation Metrics Our task framing naturally leads to evaluation by information retrieval met- rics that estimate search relevance. In our experi- ments, we use four common metrics included in the trec_eval package,4 a standard evaluation tool used in the IR community: Precision@k: The proportion of relevant items in top k retrieved datasets. If P@k is 1, then every retrieved document is valuable. Recall@k: The fraction of relevant items that are retrieved. If R@k is 1, then the search results are comprehensive. Mean Average Precision (MAP): Assuming we have m relevant datasets in total, and ki is the rank of the ith relevant dataset, MAP is calcu- lated as (cid:80)m i P@ki/m (Manning et al., 2005). High MAP indicates strong average search qual- ity over all relevant datasets. Mean Reciprocal Rank (MRR): The average of the inverse of the ranks at which the first relevant item was retrieved. Assuming Ri is the rank of the i-th relevant item in the retrieved result, M RR is calculated as (cid:80)m i Ri/m. High MRR means a user sees at least some relevant datasets early in the search results. 4https://github.com/usnistgov/trec_eval. We use the -c flag for the trec_eval command. Galactica(Taylor et al, 2022) Evaluation Dataset Tagger Negative Examples Papers with Code 3 Ranking1) MARCO2) CNN/DM3) SQuAD 4 Struct. Metadata Training Citances Collection of Datasets Human Annotation S2ORC(Lo et al, 2020) Queries 7 1 Intro. Paper Title Qry1 Qry1 Positive Examples Dataset Description Qry2 Qry2 RelevantDatasets NegativeMining Ranker 1 2 2 2 Relevant Datasets Testing 5 5 5 Training Queries SciREX(Jain et al, 2020) Figure 3: We search against all datasets on Papers With Code. Our system is trained on a set of simulated queries and target datasets and evaluated on a set of expert- written queries with hand-annotated target datasets. 3 The DataFinder Dataset To support this task, we construct a dataset called The DataFinder Dataset consisting of (q, R) pairs extracted from published English-language scien- tific proceedings, where each q is either a full- sentence description or a keyword query. We collect a large training set through an automated method (for scalability), and we collect a smaller test set using real users\u2019 annotations (for reliable and realistic model evaluation). In both cases, our data collection contains two primary steps: (1) col- lecting search queries q that a user would use to describe their dataset needs, and (2) identifying relevant datasets R that match the query. Our final training and test sets contain 17495 and 392 queries, respectively. Figure 3 summarizes our data collection approach. We explain the details below and provide further discussion of the limitations of our dataset in the Limitations section. We will re- lease our data under a permissive CC-BY License. 3.1 Collection of Datasets In our task definition, we search over the collec- tion of datasets listed on Papers With Code, a large public index of papers which includes metadata for over 7000 datasets and benchmarks. For most datasets, Papers With Code Datasets stores a short human-written dataset description, a list of differ- ent names used to refer to the dataset (known as \u201cvariants\u201d), and structured metadata such as the year released, the number of papers reported as using the dataset, the tasks contained, and the the modal- ity of data. Many datasets also include the paper that introduced the dataset. We used the dataset de- scription, structured metadata, and the introducing paper\u2019s title to textually represent each dataset, and we analyze this design decision in \u00a75.4. 3.2 Training Set Construction To ensure scalability for the training set, we rely on a large corpus of scientific papers, S2ORC (Lo et al., 2020). We extract nearly 20,000 abstracts from AI papers that use datasets. To overcome the high cost of manually-annotating queries or relevant datasets, we instead simulate annotations with few-shot-learning and rule-based methods. Query Collection We extract queries from pa- per abstracts because, intuitively, an abstract will contain the most salient characteristics behind a research idea or contribution. As a result, it is an ideal source for comprehensively collecting poten- tial implicit constraints as shown in Figure 1. We simulate query collection with the 6.7B pa- rameter version of Galactica (Taylor et al., 2022), a large scientific language model that supports few- shot learning. In our prompt, we give the model an abstract and ask it to first extract five keyphrases: the tasks mentioned in paper, the task domain of the paper (e.g., biomedical or aerial), the modal- ity of data required, the language of data or labels required, and the length of text required (sentence- level, paragraph-level, or none mentioned). We then ask Galactica to generate a full query contain- ing any salient keyphrases. We perform few-shot learning using 3 examples in the prompt to guide the model. Our prompt is shown in Appendix A. Relevant Datasets For our training set, relevant datasets are automatically labeled using the body text of a paper.5 We apply a rule-based procedure to identify the dataset used in a given paper (cor- responding to an abstract whose query has been auto-labeled). For each paper, we tag all datasets that satisfy two conditions: the paper must cite the paper that introduces the dataset, and the paper must mention the dataset by name twice.6 5Note that our queries are"}, {"question": " What method is used for simulating annotations in the training set?", "answer": " Few-shot learning and rule-based methods.", "ref_chunk": "2 Dataset Recommendation Task We establish a new task for automatically recom- mending relevant datasets given a description of a data-driven system. Given a query q and a set of datasets D, retrieve the most relevant subset R \u2282 D one could use to test the idea described in q. Figure 1 illustrates this with a real query written by a graduate student. The query q can take two forms: either a key- word query (the predominant interface for dataset search today (Chapman et al., 2019)) or a full- sentence description. Textual descriptions offer a more flexible input to the recommendation system, with the ability to implicitly specify constraints based on what a researcher wants to study, without needing to carefully construct keywords a priori. Evaluation Metrics Our task framing naturally leads to evaluation by information retrieval met- rics that estimate search relevance. In our experi- ments, we use four common metrics included in the trec_eval package,4 a standard evaluation tool used in the IR community: Precision@k: The proportion of relevant items in top k retrieved datasets. If P@k is 1, then every retrieved document is valuable. Recall@k: The fraction of relevant items that are retrieved. If R@k is 1, then the search results are comprehensive. Mean Average Precision (MAP): Assuming we have m relevant datasets in total, and ki is the rank of the ith relevant dataset, MAP is calcu- lated as (cid:80)m i P@ki/m (Manning et al., 2005). High MAP indicates strong average search qual- ity over all relevant datasets. Mean Reciprocal Rank (MRR): The average of the inverse of the ranks at which the first relevant item was retrieved. Assuming Ri is the rank of the i-th relevant item in the retrieved result, M RR is calculated as (cid:80)m i Ri/m. High MRR means a user sees at least some relevant datasets early in the search results. 4https://github.com/usnistgov/trec_eval. We use the -c flag for the trec_eval command. Galactica(Taylor et al, 2022) Evaluation Dataset Tagger Negative Examples Papers with Code 3 Ranking1) MARCO2) CNN/DM3) SQuAD 4 Struct. Metadata Training Citances Collection of Datasets Human Annotation S2ORC(Lo et al, 2020) Queries 7 1 Intro. Paper Title Qry1 Qry1 Positive Examples Dataset Description Qry2 Qry2 RelevantDatasets NegativeMining Ranker 1 2 2 2 Relevant Datasets Testing 5 5 5 Training Queries SciREX(Jain et al, 2020) Figure 3: We search against all datasets on Papers With Code. Our system is trained on a set of simulated queries and target datasets and evaluated on a set of expert- written queries with hand-annotated target datasets. 3 The DataFinder Dataset To support this task, we construct a dataset called The DataFinder Dataset consisting of (q, R) pairs extracted from published English-language scien- tific proceedings, where each q is either a full- sentence description or a keyword query. We collect a large training set through an automated method (for scalability), and we collect a smaller test set using real users\u2019 annotations (for reliable and realistic model evaluation). In both cases, our data collection contains two primary steps: (1) col- lecting search queries q that a user would use to describe their dataset needs, and (2) identifying relevant datasets R that match the query. Our final training and test sets contain 17495 and 392 queries, respectively. Figure 3 summarizes our data collection approach. We explain the details below and provide further discussion of the limitations of our dataset in the Limitations section. We will re- lease our data under a permissive CC-BY License. 3.1 Collection of Datasets In our task definition, we search over the collec- tion of datasets listed on Papers With Code, a large public index of papers which includes metadata for over 7000 datasets and benchmarks. For most datasets, Papers With Code Datasets stores a short human-written dataset description, a list of differ- ent names used to refer to the dataset (known as \u201cvariants\u201d), and structured metadata such as the year released, the number of papers reported as using the dataset, the tasks contained, and the the modal- ity of data. Many datasets also include the paper that introduced the dataset. We used the dataset de- scription, structured metadata, and the introducing paper\u2019s title to textually represent each dataset, and we analyze this design decision in \u00a75.4. 3.2 Training Set Construction To ensure scalability for the training set, we rely on a large corpus of scientific papers, S2ORC (Lo et al., 2020). We extract nearly 20,000 abstracts from AI papers that use datasets. To overcome the high cost of manually-annotating queries or relevant datasets, we instead simulate annotations with few-shot-learning and rule-based methods. Query Collection We extract queries from pa- per abstracts because, intuitively, an abstract will contain the most salient characteristics behind a research idea or contribution. As a result, it is an ideal source for comprehensively collecting poten- tial implicit constraints as shown in Figure 1. We simulate query collection with the 6.7B pa- rameter version of Galactica (Taylor et al., 2022), a large scientific language model that supports few- shot learning. In our prompt, we give the model an abstract and ask it to first extract five keyphrases: the tasks mentioned in paper, the task domain of the paper (e.g., biomedical or aerial), the modal- ity of data required, the language of data or labels required, and the length of text required (sentence- level, paragraph-level, or none mentioned). We then ask Galactica to generate a full query contain- ing any salient keyphrases. We perform few-shot learning using 3 examples in the prompt to guide the model. Our prompt is shown in Appendix A. Relevant Datasets For our training set, relevant datasets are automatically labeled using the body text of a paper.5 We apply a rule-based procedure to identify the dataset used in a given paper (cor- responding to an abstract whose query has been auto-labeled). For each paper, we tag all datasets that satisfy two conditions: the paper must cite the paper that introduces the dataset, and the paper must mention the dataset by name twice.6 5Note that our queries are"}], "doc_text": "2 Dataset Recommendation Task We establish a new task for automatically recom- mending relevant datasets given a description of a data-driven system. Given a query q and a set of datasets D, retrieve the most relevant subset R \u2282 D one could use to test the idea described in q. Figure 1 illustrates this with a real query written by a graduate student. The query q can take two forms: either a key- word query (the predominant interface for dataset search today (Chapman et al., 2019)) or a full- sentence description. Textual descriptions offer a more flexible input to the recommendation system, with the ability to implicitly specify constraints based on what a researcher wants to study, without needing to carefully construct keywords a priori. Evaluation Metrics Our task framing naturally leads to evaluation by information retrieval met- rics that estimate search relevance. In our experi- ments, we use four common metrics included in the trec_eval package,4 a standard evaluation tool used in the IR community: Precision@k: The proportion of relevant items in top k retrieved datasets. If P@k is 1, then every retrieved document is valuable. Recall@k: The fraction of relevant items that are retrieved. If R@k is 1, then the search results are comprehensive. Mean Average Precision (MAP): Assuming we have m relevant datasets in total, and ki is the rank of the ith relevant dataset, MAP is calcu- lated as (cid:80)m i P@ki/m (Manning et al., 2005). High MAP indicates strong average search qual- ity over all relevant datasets. Mean Reciprocal Rank (MRR): The average of the inverse of the ranks at which the first relevant item was retrieved. Assuming Ri is the rank of the i-th relevant item in the retrieved result, M RR is calculated as (cid:80)m i Ri/m. High MRR means a user sees at least some relevant datasets early in the search results. 4https://github.com/usnistgov/trec_eval. We use the -c flag for the trec_eval command. Galactica(Taylor et al, 2022) Evaluation Dataset Tagger Negative Examples Papers with Code 3 Ranking1) MARCO2) CNN/DM3) SQuAD 4 Struct. Metadata Training Citances Collection of Datasets Human Annotation S2ORC(Lo et al, 2020) Queries 7 1 Intro. Paper Title Qry1 Qry1 Positive Examples Dataset Description Qry2 Qry2 RelevantDatasets NegativeMining Ranker 1 2 2 2 Relevant Datasets Testing 5 5 5 Training Queries SciREX(Jain et al, 2020) Figure 3: We search against all datasets on Papers With Code. Our system is trained on a set of simulated queries and target datasets and evaluated on a set of expert- written queries with hand-annotated target datasets. 3 The DataFinder Dataset To support this task, we construct a dataset called The DataFinder Dataset consisting of (q, R) pairs extracted from published English-language scien- tific proceedings, where each q is either a full- sentence description or a keyword query. We collect a large training set through an automated method (for scalability), and we collect a smaller test set using real users\u2019 annotations (for reliable and realistic model evaluation). In both cases, our data collection contains two primary steps: (1) col- lecting search queries q that a user would use to describe their dataset needs, and (2) identifying relevant datasets R that match the query. Our final training and test sets contain 17495 and 392 queries, respectively. Figure 3 summarizes our data collection approach. We explain the details below and provide further discussion of the limitations of our dataset in the Limitations section. We will re- lease our data under a permissive CC-BY License. 3.1 Collection of Datasets In our task definition, we search over the collec- tion of datasets listed on Papers With Code, a large public index of papers which includes metadata for over 7000 datasets and benchmarks. For most datasets, Papers With Code Datasets stores a short human-written dataset description, a list of differ- ent names used to refer to the dataset (known as \u201cvariants\u201d), and structured metadata such as the year released, the number of papers reported as using the dataset, the tasks contained, and the the modal- ity of data. Many datasets also include the paper that introduced the dataset. We used the dataset de- scription, structured metadata, and the introducing paper\u2019s title to textually represent each dataset, and we analyze this design decision in \u00a75.4. 3.2 Training Set Construction To ensure scalability for the training set, we rely on a large corpus of scientific papers, S2ORC (Lo et al., 2020). We extract nearly 20,000 abstracts from AI papers that use datasets. To overcome the high cost of manually-annotating queries or relevant datasets, we instead simulate annotations with few-shot-learning and rule-based methods. Query Collection We extract queries from pa- per abstracts because, intuitively, an abstract will contain the most salient characteristics behind a research idea or contribution. As a result, it is an ideal source for comprehensively collecting poten- tial implicit constraints as shown in Figure 1. We simulate query collection with the 6.7B pa- rameter version of Galactica (Taylor et al., 2022), a large scientific language model that supports few- shot learning. In our prompt, we give the model an abstract and ask it to first extract five keyphrases: the tasks mentioned in paper, the task domain of the paper (e.g., biomedical or aerial), the modal- ity of data required, the language of data or labels required, and the length of text required (sentence- level, paragraph-level, or none mentioned). We then ask Galactica to generate a full query contain- ing any salient keyphrases. We perform few-shot learning using 3 examples in the prompt to guide the model. Our prompt is shown in Appendix A. Relevant Datasets For our training set, relevant datasets are automatically labeled using the body text of a paper.5 We apply a rule-based procedure to identify the dataset used in a given paper (cor- responding to an abstract whose query has been auto-labeled). For each paper, we tag all datasets that satisfy two conditions: the paper must cite the paper that introduces the dataset, and the paper must mention the dataset by name twice.6 5Note that our queries are"}