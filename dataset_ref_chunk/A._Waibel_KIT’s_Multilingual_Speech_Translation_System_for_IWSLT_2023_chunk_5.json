{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/A._Waibel_KIT\u2019s_Multilingual_Speech_Translation_System_for_IWSLT_2023_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What hyperparameters were used for kNN-MT in the text?", "answer": " k = 8, T = 50, w = 0.3", "ref_chunk": "7 for creating the datastores. As differ- ent models\u2019 hidden states (which serve as keys in the datastore) also differ substantially, the datastore is MT-model-dependent. To use kNN-MT when ensembling systems (2) and (3), we therefore need two datastores for systems (2) and (3) respectively. The kNN-MT candidate tokens are interpolated with the output vocabulary distribtuion before the ensembling operation. We use hyperparameters k = 8, T = 50, w = 0.3, after an initial search with T \u2208 [10, 50, 100], w \u2208 [0.1, 0.3, 0.5]. Our implemen- tation mostly follows Zheng et al. (2021), which uses the FAISS toolkit (Johnson et al., 2019) for efficient kNN operations. Comparing the infer- ence speed of system (4) and (5), with the same batch size of 64 sentences5, using kNN-MT takes roughly 50% more time on a Nvidia Titan RTX GPU with 24GB memory. Naively using all ACL dev bitext as datastore would lead the model to copying the oracle targets. To simulate the scenario on the blind test set, when translating the i-th talk, we use the other jj\u0338=i \u2208 [n] talks\u2019 bitext as datastore, where n is the total number of talks. As shown in Row (5) of Table 7, kNN-MT brings an additional gain of 1.3 BLEU on MT and 0.8 BLEU on ST. These results shows a datastore as small as hundreds of sentence pairs can be effec- tively used for inference-time domain adaptation. Table 9 shows two examples of kNN-MT im- proving translation quality, apart from generic im- provements in fluency and accuracy, in these ex- amples kNN-MT also helps generate correct termi- nologies and context-appropriate greetings. 4 End-to-End System For the end-to-end system, similar to our ASR model, after seeing initial favourable results of WavLM over Wav2vec, we choose WavLM as the audio encoder. Following last year\u2019s submis- sion (Pham et al., 2022), we use the mBART50 decoder. The results are shown in Row (6) of Ta- ble 7. Contrasting Row (6) and (7) reveals that adding the TTS data does not substantially change ST performance. However, ensembling the two models trained with and without TTS data (Row (8)) improves over the single models (on average +0.7 for ACL, +0.4 for TED), despite them having the identical architecture. Compared to the strongest cascaded system (Row (5)), the end-to-end system falls behind 2.6 BLEU on ACL dev. On TED, however, it appears to slightly outperform the cascaded system. One explanation is that the MT model of the cascaded system has not been separately adapted to TED texts (although parts of the full training data do cover TED data), which was shown essential in im- proving performance on TED test sets (Zhang et al., 2022; Pham et al., 2022). The end-to-end system, on the other hand, has seen a larger proportion of TED data in training (Table 4). Similar to the previous year (Pol\u00e1k et al., 2022), we also adapt our end-to-end offline model for si- 5System (5) requires more GPU memory than system (4). The latter would be able to use a larger batch size of 128 sentences, making the realistic speed difference slightly larger. multaneous track (Pol\u00e1k et al., 2023). 5 Conclusion In this paper, we described our systems for the mul- tilingual speech translation track of IWSLT 2023, which translates English speech into 10 target lan- guages. To tackle the task of translating scien- tific conference talks, which feature non-native in- put speech and terminology-dense contents, our systems have several novelties. Lacking suitable training data for the target domain, we used kNN- MT for inference-time adaptation and showed an improvement of +0.8 BLEU for cascaded speech translation system. We also used adapters to in- tegrate incremental data from augmentation, and achieved performance on-par with re-training on all data. In our experiments, we observed that cas- caded systems are more easily adaptable towards desired target domains due to their separate mod- ules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk transla- tion, although their performance remains similar on TED talks. For future work, we are interested in the feasibility of applying the adaptation approaches shown effective on MT to end-to-end ST. Acknowledgement We thank the anonymous re- viewers for detailed and insightful feedback. Part of this work was performed on the HoreKa su- percomputer funded by the Ministry of Science, Research and the Arts Baden-W\u00fcrttemberg and by the Federal Ministry of Education and Research of Germany. Part of this work was supported by the Federal Ministry of Education and Research of Germany under grant agreement 01EF1803B (RELATER). References Milind Agarwal, Sweta Agrawal, Antonios Anasta- sopoulos, Ond\u02c7rej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, Khalid Choukri, Alexandra Chronopoulou, Anna Currey, Thierry Declerck, Qian- qian Dong, Yannick Est\u00e9ve, Kevin Duh, Marcello Federico, Souhir Gahbiche, Barry Haddow, Benjamin Hsu, Phu Mon Htut, Hirofumi Inaguma, D\u00e1vid Ja- vorsk\u00fd, John Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Pengwei Li, Xutai Ma, Prashant Mathur, Evgeny Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, Ha Nguyen, Jan Niehues, Xing Niu, Atul Ojha Kr., John E. Ortega, Proyag Pal, Juan Pino, Lonneke van der Plas, Peter Pol\u00e1k, Elijah Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Se- bastian St\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Kevin Tran, Marco Turchi, Alex Waibel, Mingxuan Wang, Shinji Watanabe, and Rodolfo Ze- vallos. 2023. Findings of the IWSLT 2023 Evaluation Campaign. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023). Association for Computational Linguistics. Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3874\u20133884, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Antonios Anastasopoulos, Lo\u00efc Barrault, Luisa Ben- tivogli, Marcely Zanon Boito, Ond\u02c7rej Bojar, Roldano Cattoni, Anna Currey, Georgiana Dinu, Kevin Duh, Maha Elbayad, Clara Emmanuel, Yannick Est\u00e8ve, Marcello Federico, Christian Federmann, Souhir Gahbiche, Hongyu Gong, Roman Grundkiewicz,"}, {"question": " Which toolkit was used for efficient kNN operations in the text?", "answer": " FAISS toolkit", "ref_chunk": "7 for creating the datastores. As differ- ent models\u2019 hidden states (which serve as keys in the datastore) also differ substantially, the datastore is MT-model-dependent. To use kNN-MT when ensembling systems (2) and (3), we therefore need two datastores for systems (2) and (3) respectively. The kNN-MT candidate tokens are interpolated with the output vocabulary distribtuion before the ensembling operation. We use hyperparameters k = 8, T = 50, w = 0.3, after an initial search with T \u2208 [10, 50, 100], w \u2208 [0.1, 0.3, 0.5]. Our implemen- tation mostly follows Zheng et al. (2021), which uses the FAISS toolkit (Johnson et al., 2019) for efficient kNN operations. Comparing the infer- ence speed of system (4) and (5), with the same batch size of 64 sentences5, using kNN-MT takes roughly 50% more time on a Nvidia Titan RTX GPU with 24GB memory. Naively using all ACL dev bitext as datastore would lead the model to copying the oracle targets. To simulate the scenario on the blind test set, when translating the i-th talk, we use the other jj\u0338=i \u2208 [n] talks\u2019 bitext as datastore, where n is the total number of talks. As shown in Row (5) of Table 7, kNN-MT brings an additional gain of 1.3 BLEU on MT and 0.8 BLEU on ST. These results shows a datastore as small as hundreds of sentence pairs can be effec- tively used for inference-time domain adaptation. Table 9 shows two examples of kNN-MT im- proving translation quality, apart from generic im- provements in fluency and accuracy, in these ex- amples kNN-MT also helps generate correct termi- nologies and context-appropriate greetings. 4 End-to-End System For the end-to-end system, similar to our ASR model, after seeing initial favourable results of WavLM over Wav2vec, we choose WavLM as the audio encoder. Following last year\u2019s submis- sion (Pham et al., 2022), we use the mBART50 decoder. The results are shown in Row (6) of Ta- ble 7. Contrasting Row (6) and (7) reveals that adding the TTS data does not substantially change ST performance. However, ensembling the two models trained with and without TTS data (Row (8)) improves over the single models (on average +0.7 for ACL, +0.4 for TED), despite them having the identical architecture. Compared to the strongest cascaded system (Row (5)), the end-to-end system falls behind 2.6 BLEU on ACL dev. On TED, however, it appears to slightly outperform the cascaded system. One explanation is that the MT model of the cascaded system has not been separately adapted to TED texts (although parts of the full training data do cover TED data), which was shown essential in im- proving performance on TED test sets (Zhang et al., 2022; Pham et al., 2022). The end-to-end system, on the other hand, has seen a larger proportion of TED data in training (Table 4). Similar to the previous year (Pol\u00e1k et al., 2022), we also adapt our end-to-end offline model for si- 5System (5) requires more GPU memory than system (4). The latter would be able to use a larger batch size of 128 sentences, making the realistic speed difference slightly larger. multaneous track (Pol\u00e1k et al., 2023). 5 Conclusion In this paper, we described our systems for the mul- tilingual speech translation track of IWSLT 2023, which translates English speech into 10 target lan- guages. To tackle the task of translating scien- tific conference talks, which feature non-native in- put speech and terminology-dense contents, our systems have several novelties. Lacking suitable training data for the target domain, we used kNN- MT for inference-time adaptation and showed an improvement of +0.8 BLEU for cascaded speech translation system. We also used adapters to in- tegrate incremental data from augmentation, and achieved performance on-par with re-training on all data. In our experiments, we observed that cas- caded systems are more easily adaptable towards desired target domains due to their separate mod- ules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk transla- tion, although their performance remains similar on TED talks. For future work, we are interested in the feasibility of applying the adaptation approaches shown effective on MT to end-to-end ST. Acknowledgement We thank the anonymous re- viewers for detailed and insightful feedback. Part of this work was performed on the HoreKa su- percomputer funded by the Ministry of Science, Research and the Arts Baden-W\u00fcrttemberg and by the Federal Ministry of Education and Research of Germany. Part of this work was supported by the Federal Ministry of Education and Research of Germany under grant agreement 01EF1803B (RELATER). References Milind Agarwal, Sweta Agrawal, Antonios Anasta- sopoulos, Ond\u02c7rej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, Khalid Choukri, Alexandra Chronopoulou, Anna Currey, Thierry Declerck, Qian- qian Dong, Yannick Est\u00e9ve, Kevin Duh, Marcello Federico, Souhir Gahbiche, Barry Haddow, Benjamin Hsu, Phu Mon Htut, Hirofumi Inaguma, D\u00e1vid Ja- vorsk\u00fd, John Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Pengwei Li, Xutai Ma, Prashant Mathur, Evgeny Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, Ha Nguyen, Jan Niehues, Xing Niu, Atul Ojha Kr., John E. Ortega, Proyag Pal, Juan Pino, Lonneke van der Plas, Peter Pol\u00e1k, Elijah Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Se- bastian St\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Kevin Tran, Marco Turchi, Alex Waibel, Mingxuan Wang, Shinji Watanabe, and Rodolfo Ze- vallos. 2023. Findings of the IWSLT 2023 Evaluation Campaign. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023). Association for Computational Linguistics. Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3874\u20133884, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Antonios Anastasopoulos, Lo\u00efc Barrault, Luisa Ben- tivogli, Marcely Zanon Boito, Ond\u02c7rej Bojar, Roldano Cattoni, Anna Currey, Georgiana Dinu, Kevin Duh, Maha Elbayad, Clara Emmanuel, Yannick Est\u00e8ve, Marcello Federico, Christian Federmann, Souhir Gahbiche, Hongyu Gong, Roman Grundkiewicz,"}, {"question": " What additional gain in BLEU did kNN-MT bring on MT according to Row (5)?", "answer": " 1.3 BLEU", "ref_chunk": "7 for creating the datastores. As differ- ent models\u2019 hidden states (which serve as keys in the datastore) also differ substantially, the datastore is MT-model-dependent. To use kNN-MT when ensembling systems (2) and (3), we therefore need two datastores for systems (2) and (3) respectively. The kNN-MT candidate tokens are interpolated with the output vocabulary distribtuion before the ensembling operation. We use hyperparameters k = 8, T = 50, w = 0.3, after an initial search with T \u2208 [10, 50, 100], w \u2208 [0.1, 0.3, 0.5]. Our implemen- tation mostly follows Zheng et al. (2021), which uses the FAISS toolkit (Johnson et al., 2019) for efficient kNN operations. Comparing the infer- ence speed of system (4) and (5), with the same batch size of 64 sentences5, using kNN-MT takes roughly 50% more time on a Nvidia Titan RTX GPU with 24GB memory. Naively using all ACL dev bitext as datastore would lead the model to copying the oracle targets. To simulate the scenario on the blind test set, when translating the i-th talk, we use the other jj\u0338=i \u2208 [n] talks\u2019 bitext as datastore, where n is the total number of talks. As shown in Row (5) of Table 7, kNN-MT brings an additional gain of 1.3 BLEU on MT and 0.8 BLEU on ST. These results shows a datastore as small as hundreds of sentence pairs can be effec- tively used for inference-time domain adaptation. Table 9 shows two examples of kNN-MT im- proving translation quality, apart from generic im- provements in fluency and accuracy, in these ex- amples kNN-MT also helps generate correct termi- nologies and context-appropriate greetings. 4 End-to-End System For the end-to-end system, similar to our ASR model, after seeing initial favourable results of WavLM over Wav2vec, we choose WavLM as the audio encoder. Following last year\u2019s submis- sion (Pham et al., 2022), we use the mBART50 decoder. The results are shown in Row (6) of Ta- ble 7. Contrasting Row (6) and (7) reveals that adding the TTS data does not substantially change ST performance. However, ensembling the two models trained with and without TTS data (Row (8)) improves over the single models (on average +0.7 for ACL, +0.4 for TED), despite them having the identical architecture. Compared to the strongest cascaded system (Row (5)), the end-to-end system falls behind 2.6 BLEU on ACL dev. On TED, however, it appears to slightly outperform the cascaded system. One explanation is that the MT model of the cascaded system has not been separately adapted to TED texts (although parts of the full training data do cover TED data), which was shown essential in im- proving performance on TED test sets (Zhang et al., 2022; Pham et al., 2022). The end-to-end system, on the other hand, has seen a larger proportion of TED data in training (Table 4). Similar to the previous year (Pol\u00e1k et al., 2022), we also adapt our end-to-end offline model for si- 5System (5) requires more GPU memory than system (4). The latter would be able to use a larger batch size of 128 sentences, making the realistic speed difference slightly larger. multaneous track (Pol\u00e1k et al., 2023). 5 Conclusion In this paper, we described our systems for the mul- tilingual speech translation track of IWSLT 2023, which translates English speech into 10 target lan- guages. To tackle the task of translating scien- tific conference talks, which feature non-native in- put speech and terminology-dense contents, our systems have several novelties. Lacking suitable training data for the target domain, we used kNN- MT for inference-time adaptation and showed an improvement of +0.8 BLEU for cascaded speech translation system. We also used adapters to in- tegrate incremental data from augmentation, and achieved performance on-par with re-training on all data. In our experiments, we observed that cas- caded systems are more easily adaptable towards desired target domains due to their separate mod- ules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk transla- tion, although their performance remains similar on TED talks. For future work, we are interested in the feasibility of applying the adaptation approaches shown effective on MT to end-to-end ST. Acknowledgement We thank the anonymous re- viewers for detailed and insightful feedback. Part of this work was performed on the HoreKa su- percomputer funded by the Ministry of Science, Research and the Arts Baden-W\u00fcrttemberg and by the Federal Ministry of Education and Research of Germany. Part of this work was supported by the Federal Ministry of Education and Research of Germany under grant agreement 01EF1803B (RELATER). References Milind Agarwal, Sweta Agrawal, Antonios Anasta- sopoulos, Ond\u02c7rej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, Khalid Choukri, Alexandra Chronopoulou, Anna Currey, Thierry Declerck, Qian- qian Dong, Yannick Est\u00e9ve, Kevin Duh, Marcello Federico, Souhir Gahbiche, Barry Haddow, Benjamin Hsu, Phu Mon Htut, Hirofumi Inaguma, D\u00e1vid Ja- vorsk\u00fd, John Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Pengwei Li, Xutai Ma, Prashant Mathur, Evgeny Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, Ha Nguyen, Jan Niehues, Xing Niu, Atul Ojha Kr., John E. Ortega, Proyag Pal, Juan Pino, Lonneke van der Plas, Peter Pol\u00e1k, Elijah Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Se- bastian St\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Kevin Tran, Marco Turchi, Alex Waibel, Mingxuan Wang, Shinji Watanabe, and Rodolfo Ze- vallos. 2023. Findings of the IWSLT 2023 Evaluation Campaign. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023). Association for Computational Linguistics. Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3874\u20133884, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Antonios Anastasopoulos, Lo\u00efc Barrault, Luisa Ben- tivogli, Marcely Zanon Boito, Ond\u02c7rej Bojar, Roldano Cattoni, Anna Currey, Georgiana Dinu, Kevin Duh, Maha Elbayad, Clara Emmanuel, Yannick Est\u00e8ve, Marcello Federico, Christian Federmann, Souhir Gahbiche, Hongyu Gong, Roman Grundkiewicz,"}, {"question": " How much more time does using kNN-MT take compared to not using it, on a Nvidia Titan RTX GPU?", "answer": " Roughly 50% more time", "ref_chunk": "7 for creating the datastores. As differ- ent models\u2019 hidden states (which serve as keys in the datastore) also differ substantially, the datastore is MT-model-dependent. To use kNN-MT when ensembling systems (2) and (3), we therefore need two datastores for systems (2) and (3) respectively. The kNN-MT candidate tokens are interpolated with the output vocabulary distribtuion before the ensembling operation. We use hyperparameters k = 8, T = 50, w = 0.3, after an initial search with T \u2208 [10, 50, 100], w \u2208 [0.1, 0.3, 0.5]. Our implemen- tation mostly follows Zheng et al. (2021), which uses the FAISS toolkit (Johnson et al., 2019) for efficient kNN operations. Comparing the infer- ence speed of system (4) and (5), with the same batch size of 64 sentences5, using kNN-MT takes roughly 50% more time on a Nvidia Titan RTX GPU with 24GB memory. Naively using all ACL dev bitext as datastore would lead the model to copying the oracle targets. To simulate the scenario on the blind test set, when translating the i-th talk, we use the other jj\u0338=i \u2208 [n] talks\u2019 bitext as datastore, where n is the total number of talks. As shown in Row (5) of Table 7, kNN-MT brings an additional gain of 1.3 BLEU on MT and 0.8 BLEU on ST. These results shows a datastore as small as hundreds of sentence pairs can be effec- tively used for inference-time domain adaptation. Table 9 shows two examples of kNN-MT im- proving translation quality, apart from generic im- provements in fluency and accuracy, in these ex- amples kNN-MT also helps generate correct termi- nologies and context-appropriate greetings. 4 End-to-End System For the end-to-end system, similar to our ASR model, after seeing initial favourable results of WavLM over Wav2vec, we choose WavLM as the audio encoder. Following last year\u2019s submis- sion (Pham et al., 2022), we use the mBART50 decoder. The results are shown in Row (6) of Ta- ble 7. Contrasting Row (6) and (7) reveals that adding the TTS data does not substantially change ST performance. However, ensembling the two models trained with and without TTS data (Row (8)) improves over the single models (on average +0.7 for ACL, +0.4 for TED), despite them having the identical architecture. Compared to the strongest cascaded system (Row (5)), the end-to-end system falls behind 2.6 BLEU on ACL dev. On TED, however, it appears to slightly outperform the cascaded system. One explanation is that the MT model of the cascaded system has not been separately adapted to TED texts (although parts of the full training data do cover TED data), which was shown essential in im- proving performance on TED test sets (Zhang et al., 2022; Pham et al., 2022). The end-to-end system, on the other hand, has seen a larger proportion of TED data in training (Table 4). Similar to the previous year (Pol\u00e1k et al., 2022), we also adapt our end-to-end offline model for si- 5System (5) requires more GPU memory than system (4). The latter would be able to use a larger batch size of 128 sentences, making the realistic speed difference slightly larger. multaneous track (Pol\u00e1k et al., 2023). 5 Conclusion In this paper, we described our systems for the mul- tilingual speech translation track of IWSLT 2023, which translates English speech into 10 target lan- guages. To tackle the task of translating scien- tific conference talks, which feature non-native in- put speech and terminology-dense contents, our systems have several novelties. Lacking suitable training data for the target domain, we used kNN- MT for inference-time adaptation and showed an improvement of +0.8 BLEU for cascaded speech translation system. We also used adapters to in- tegrate incremental data from augmentation, and achieved performance on-par with re-training on all data. In our experiments, we observed that cas- caded systems are more easily adaptable towards desired target domains due to their separate mod- ules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk transla- tion, although their performance remains similar on TED talks. For future work, we are interested in the feasibility of applying the adaptation approaches shown effective on MT to end-to-end ST. Acknowledgement We thank the anonymous re- viewers for detailed and insightful feedback. Part of this work was performed on the HoreKa su- percomputer funded by the Ministry of Science, Research and the Arts Baden-W\u00fcrttemberg and by the Federal Ministry of Education and Research of Germany. Part of this work was supported by the Federal Ministry of Education and Research of Germany under grant agreement 01EF1803B (RELATER). References Milind Agarwal, Sweta Agrawal, Antonios Anasta- sopoulos, Ond\u02c7rej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, Khalid Choukri, Alexandra Chronopoulou, Anna Currey, Thierry Declerck, Qian- qian Dong, Yannick Est\u00e9ve, Kevin Duh, Marcello Federico, Souhir Gahbiche, Barry Haddow, Benjamin Hsu, Phu Mon Htut, Hirofumi Inaguma, D\u00e1vid Ja- vorsk\u00fd, John Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Pengwei Li, Xutai Ma, Prashant Mathur, Evgeny Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, Ha Nguyen, Jan Niehues, Xing Niu, Atul Ojha Kr., John E. Ortega, Proyag Pal, Juan Pino, Lonneke van der Plas, Peter Pol\u00e1k, Elijah Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Se- bastian St\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Kevin Tran, Marco Turchi, Alex Waibel, Mingxuan Wang, Shinji Watanabe, and Rodolfo Ze- vallos. 2023. Findings of the IWSLT 2023 Evaluation Campaign. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023). Association for Computational Linguistics. Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3874\u20133884, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Antonios Anastasopoulos, Lo\u00efc Barrault, Luisa Ben- tivogli, Marcely Zanon Boito, Ond\u02c7rej Bojar, Roldano Cattoni, Anna Currey, Georgiana Dinu, Kevin Duh, Maha Elbayad, Clara Emmanuel, Yannick Est\u00e8ve, Marcello Federico, Christian Federmann, Souhir Gahbiche, Hongyu Gong, Roman Grundkiewicz,"}, {"question": " What does naively using all ACL dev bitext as datastore lead to?", "answer": " Model copying the oracle targets", "ref_chunk": "7 for creating the datastores. As differ- ent models\u2019 hidden states (which serve as keys in the datastore) also differ substantially, the datastore is MT-model-dependent. To use kNN-MT when ensembling systems (2) and (3), we therefore need two datastores for systems (2) and (3) respectively. The kNN-MT candidate tokens are interpolated with the output vocabulary distribtuion before the ensembling operation. We use hyperparameters k = 8, T = 50, w = 0.3, after an initial search with T \u2208 [10, 50, 100], w \u2208 [0.1, 0.3, 0.5]. Our implemen- tation mostly follows Zheng et al. (2021), which uses the FAISS toolkit (Johnson et al., 2019) for efficient kNN operations. Comparing the infer- ence speed of system (4) and (5), with the same batch size of 64 sentences5, using kNN-MT takes roughly 50% more time on a Nvidia Titan RTX GPU with 24GB memory. Naively using all ACL dev bitext as datastore would lead the model to copying the oracle targets. To simulate the scenario on the blind test set, when translating the i-th talk, we use the other jj\u0338=i \u2208 [n] talks\u2019 bitext as datastore, where n is the total number of talks. As shown in Row (5) of Table 7, kNN-MT brings an additional gain of 1.3 BLEU on MT and 0.8 BLEU on ST. These results shows a datastore as small as hundreds of sentence pairs can be effec- tively used for inference-time domain adaptation. Table 9 shows two examples of kNN-MT im- proving translation quality, apart from generic im- provements in fluency and accuracy, in these ex- amples kNN-MT also helps generate correct termi- nologies and context-appropriate greetings. 4 End-to-End System For the end-to-end system, similar to our ASR model, after seeing initial favourable results of WavLM over Wav2vec, we choose WavLM as the audio encoder. Following last year\u2019s submis- sion (Pham et al., 2022), we use the mBART50 decoder. The results are shown in Row (6) of Ta- ble 7. Contrasting Row (6) and (7) reveals that adding the TTS data does not substantially change ST performance. However, ensembling the two models trained with and without TTS data (Row (8)) improves over the single models (on average +0.7 for ACL, +0.4 for TED), despite them having the identical architecture. Compared to the strongest cascaded system (Row (5)), the end-to-end system falls behind 2.6 BLEU on ACL dev. On TED, however, it appears to slightly outperform the cascaded system. One explanation is that the MT model of the cascaded system has not been separately adapted to TED texts (although parts of the full training data do cover TED data), which was shown essential in im- proving performance on TED test sets (Zhang et al., 2022; Pham et al., 2022). The end-to-end system, on the other hand, has seen a larger proportion of TED data in training (Table 4). Similar to the previous year (Pol\u00e1k et al., 2022), we also adapt our end-to-end offline model for si- 5System (5) requires more GPU memory than system (4). The latter would be able to use a larger batch size of 128 sentences, making the realistic speed difference slightly larger. multaneous track (Pol\u00e1k et al., 2023). 5 Conclusion In this paper, we described our systems for the mul- tilingual speech translation track of IWSLT 2023, which translates English speech into 10 target lan- guages. To tackle the task of translating scien- tific conference talks, which feature non-native in- put speech and terminology-dense contents, our systems have several novelties. Lacking suitable training data for the target domain, we used kNN- MT for inference-time adaptation and showed an improvement of +0.8 BLEU for cascaded speech translation system. We also used adapters to in- tegrate incremental data from augmentation, and achieved performance on-par with re-training on all data. In our experiments, we observed that cas- caded systems are more easily adaptable towards desired target domains due to their separate mod- ules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk transla- tion, although their performance remains similar on TED talks. For future work, we are interested in the feasibility of applying the adaptation approaches shown effective on MT to end-to-end ST. Acknowledgement We thank the anonymous re- viewers for detailed and insightful feedback. Part of this work was performed on the HoreKa su- percomputer funded by the Ministry of Science, Research and the Arts Baden-W\u00fcrttemberg and by the Federal Ministry of Education and Research of Germany. Part of this work was supported by the Federal Ministry of Education and Research of Germany under grant agreement 01EF1803B (RELATER). References Milind Agarwal, Sweta Agrawal, Antonios Anasta- sopoulos, Ond\u02c7rej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, Khalid Choukri, Alexandra Chronopoulou, Anna Currey, Thierry Declerck, Qian- qian Dong, Yannick Est\u00e9ve, Kevin Duh, Marcello Federico, Souhir Gahbiche, Barry Haddow, Benjamin Hsu, Phu Mon Htut, Hirofumi Inaguma, D\u00e1vid Ja- vorsk\u00fd, John Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Pengwei Li, Xutai Ma, Prashant Mathur, Evgeny Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, Ha Nguyen, Jan Niehues, Xing Niu, Atul Ojha Kr., John E. Ortega, Proyag Pal, Juan Pino, Lonneke van der Plas, Peter Pol\u00e1k, Elijah Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Se- bastian St\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Kevin Tran, Marco Turchi, Alex Waibel, Mingxuan Wang, Shinji Watanabe, and Rodolfo Ze- vallos. 2023. Findings of the IWSLT 2023 Evaluation Campaign. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023). Association for Computational Linguistics. Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3874\u20133884, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Antonios Anastasopoulos, Lo\u00efc Barrault, Luisa Ben- tivogli, Marcely Zanon Boito, Ond\u02c7rej Bojar, Roldano Cattoni, Anna Currey, Georgiana Dinu, Kevin Duh, Maha Elbayad, Clara Emmanuel, Yannick Est\u00e8ve, Marcello Federico, Christian Federmann, Souhir Gahbiche, Hongyu Gong, Roman Grundkiewicz,"}, {"question": " How many sentence pairs were considered as a small datastore for effective inference-time domain adaptation?", "answer": " Hundreds", "ref_chunk": "7 for creating the datastores. As differ- ent models\u2019 hidden states (which serve as keys in the datastore) also differ substantially, the datastore is MT-model-dependent. To use kNN-MT when ensembling systems (2) and (3), we therefore need two datastores for systems (2) and (3) respectively. The kNN-MT candidate tokens are interpolated with the output vocabulary distribtuion before the ensembling operation. We use hyperparameters k = 8, T = 50, w = 0.3, after an initial search with T \u2208 [10, 50, 100], w \u2208 [0.1, 0.3, 0.5]. Our implemen- tation mostly follows Zheng et al. (2021), which uses the FAISS toolkit (Johnson et al., 2019) for efficient kNN operations. Comparing the infer- ence speed of system (4) and (5), with the same batch size of 64 sentences5, using kNN-MT takes roughly 50% more time on a Nvidia Titan RTX GPU with 24GB memory. Naively using all ACL dev bitext as datastore would lead the model to copying the oracle targets. To simulate the scenario on the blind test set, when translating the i-th talk, we use the other jj\u0338=i \u2208 [n] talks\u2019 bitext as datastore, where n is the total number of talks. As shown in Row (5) of Table 7, kNN-MT brings an additional gain of 1.3 BLEU on MT and 0.8 BLEU on ST. These results shows a datastore as small as hundreds of sentence pairs can be effec- tively used for inference-time domain adaptation. Table 9 shows two examples of kNN-MT im- proving translation quality, apart from generic im- provements in fluency and accuracy, in these ex- amples kNN-MT also helps generate correct termi- nologies and context-appropriate greetings. 4 End-to-End System For the end-to-end system, similar to our ASR model, after seeing initial favourable results of WavLM over Wav2vec, we choose WavLM as the audio encoder. Following last year\u2019s submis- sion (Pham et al., 2022), we use the mBART50 decoder. The results are shown in Row (6) of Ta- ble 7. Contrasting Row (6) and (7) reveals that adding the TTS data does not substantially change ST performance. However, ensembling the two models trained with and without TTS data (Row (8)) improves over the single models (on average +0.7 for ACL, +0.4 for TED), despite them having the identical architecture. Compared to the strongest cascaded system (Row (5)), the end-to-end system falls behind 2.6 BLEU on ACL dev. On TED, however, it appears to slightly outperform the cascaded system. One explanation is that the MT model of the cascaded system has not been separately adapted to TED texts (although parts of the full training data do cover TED data), which was shown essential in im- proving performance on TED test sets (Zhang et al., 2022; Pham et al., 2022). The end-to-end system, on the other hand, has seen a larger proportion of TED data in training (Table 4). Similar to the previous year (Pol\u00e1k et al., 2022), we also adapt our end-to-end offline model for si- 5System (5) requires more GPU memory than system (4). The latter would be able to use a larger batch size of 128 sentences, making the realistic speed difference slightly larger. multaneous track (Pol\u00e1k et al., 2023). 5 Conclusion In this paper, we described our systems for the mul- tilingual speech translation track of IWSLT 2023, which translates English speech into 10 target lan- guages. To tackle the task of translating scien- tific conference talks, which feature non-native in- put speech and terminology-dense contents, our systems have several novelties. Lacking suitable training data for the target domain, we used kNN- MT for inference-time adaptation and showed an improvement of +0.8 BLEU for cascaded speech translation system. We also used adapters to in- tegrate incremental data from augmentation, and achieved performance on-par with re-training on all data. In our experiments, we observed that cas- caded systems are more easily adaptable towards desired target domains due to their separate mod- ules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk transla- tion, although their performance remains similar on TED talks. For future work, we are interested in the feasibility of applying the adaptation approaches shown effective on MT to end-to-end ST. Acknowledgement We thank the anonymous re- viewers for detailed and insightful feedback. Part of this work was performed on the HoreKa su- percomputer funded by the Ministry of Science, Research and the Arts Baden-W\u00fcrttemberg and by the Federal Ministry of Education and Research of Germany. Part of this work was supported by the Federal Ministry of Education and Research of Germany under grant agreement 01EF1803B (RELATER). References Milind Agarwal, Sweta Agrawal, Antonios Anasta- sopoulos, Ond\u02c7rej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, Khalid Choukri, Alexandra Chronopoulou, Anna Currey, Thierry Declerck, Qian- qian Dong, Yannick Est\u00e9ve, Kevin Duh, Marcello Federico, Souhir Gahbiche, Barry Haddow, Benjamin Hsu, Phu Mon Htut, Hirofumi Inaguma, D\u00e1vid Ja- vorsk\u00fd, John Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Pengwei Li, Xutai Ma, Prashant Mathur, Evgeny Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, Ha Nguyen, Jan Niehues, Xing Niu, Atul Ojha Kr., John E. Ortega, Proyag Pal, Juan Pino, Lonneke van der Plas, Peter Pol\u00e1k, Elijah Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Se- bastian St\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Kevin Tran, Marco Turchi, Alex Waibel, Mingxuan Wang, Shinji Watanabe, and Rodolfo Ze- vallos. 2023. Findings of the IWSLT 2023 Evaluation Campaign. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023). Association for Computational Linguistics. Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3874\u20133884, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Antonios Anastasopoulos, Lo\u00efc Barrault, Luisa Ben- tivogli, Marcely Zanon Boito, Ond\u02c7rej Bojar, Roldano Cattoni, Anna Currey, Georgiana Dinu, Kevin Duh, Maha Elbayad, Clara Emmanuel, Yannick Est\u00e8ve, Marcello Federico, Christian Federmann, Souhir Gahbiche, Hongyu Gong, Roman Grundkiewicz,"}, {"question": " What audio encoder was chosen for the end-to-end system in the text?", "answer": " WavLM", "ref_chunk": "7 for creating the datastores. As differ- ent models\u2019 hidden states (which serve as keys in the datastore) also differ substantially, the datastore is MT-model-dependent. To use kNN-MT when ensembling systems (2) and (3), we therefore need two datastores for systems (2) and (3) respectively. The kNN-MT candidate tokens are interpolated with the output vocabulary distribtuion before the ensembling operation. We use hyperparameters k = 8, T = 50, w = 0.3, after an initial search with T \u2208 [10, 50, 100], w \u2208 [0.1, 0.3, 0.5]. Our implemen- tation mostly follows Zheng et al. (2021), which uses the FAISS toolkit (Johnson et al., 2019) for efficient kNN operations. Comparing the infer- ence speed of system (4) and (5), with the same batch size of 64 sentences5, using kNN-MT takes roughly 50% more time on a Nvidia Titan RTX GPU with 24GB memory. Naively using all ACL dev bitext as datastore would lead the model to copying the oracle targets. To simulate the scenario on the blind test set, when translating the i-th talk, we use the other jj\u0338=i \u2208 [n] talks\u2019 bitext as datastore, where n is the total number of talks. As shown in Row (5) of Table 7, kNN-MT brings an additional gain of 1.3 BLEU on MT and 0.8 BLEU on ST. These results shows a datastore as small as hundreds of sentence pairs can be effec- tively used for inference-time domain adaptation. Table 9 shows two examples of kNN-MT im- proving translation quality, apart from generic im- provements in fluency and accuracy, in these ex- amples kNN-MT also helps generate correct termi- nologies and context-appropriate greetings. 4 End-to-End System For the end-to-end system, similar to our ASR model, after seeing initial favourable results of WavLM over Wav2vec, we choose WavLM as the audio encoder. Following last year\u2019s submis- sion (Pham et al., 2022), we use the mBART50 decoder. The results are shown in Row (6) of Ta- ble 7. Contrasting Row (6) and (7) reveals that adding the TTS data does not substantially change ST performance. However, ensembling the two models trained with and without TTS data (Row (8)) improves over the single models (on average +0.7 for ACL, +0.4 for TED), despite them having the identical architecture. Compared to the strongest cascaded system (Row (5)), the end-to-end system falls behind 2.6 BLEU on ACL dev. On TED, however, it appears to slightly outperform the cascaded system. One explanation is that the MT model of the cascaded system has not been separately adapted to TED texts (although parts of the full training data do cover TED data), which was shown essential in im- proving performance on TED test sets (Zhang et al., 2022; Pham et al., 2022). The end-to-end system, on the other hand, has seen a larger proportion of TED data in training (Table 4). Similar to the previous year (Pol\u00e1k et al., 2022), we also adapt our end-to-end offline model for si- 5System (5) requires more GPU memory than system (4). The latter would be able to use a larger batch size of 128 sentences, making the realistic speed difference slightly larger. multaneous track (Pol\u00e1k et al., 2023). 5 Conclusion In this paper, we described our systems for the mul- tilingual speech translation track of IWSLT 2023, which translates English speech into 10 target lan- guages. To tackle the task of translating scien- tific conference talks, which feature non-native in- put speech and terminology-dense contents, our systems have several novelties. Lacking suitable training data for the target domain, we used kNN- MT for inference-time adaptation and showed an improvement of +0.8 BLEU for cascaded speech translation system. We also used adapters to in- tegrate incremental data from augmentation, and achieved performance on-par with re-training on all data. In our experiments, we observed that cas- caded systems are more easily adaptable towards desired target domains due to their separate mod- ules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk transla- tion, although their performance remains similar on TED talks. For future work, we are interested in the feasibility of applying the adaptation approaches shown effective on MT to end-to-end ST. Acknowledgement We thank the anonymous re- viewers for detailed and insightful feedback. Part of this work was performed on the HoreKa su- percomputer funded by the Ministry of Science, Research and the Arts Baden-W\u00fcrttemberg and by the Federal Ministry of Education and Research of Germany. Part of this work was supported by the Federal Ministry of Education and Research of Germany under grant agreement 01EF1803B (RELATER). References Milind Agarwal, Sweta Agrawal, Antonios Anasta- sopoulos, Ond\u02c7rej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, Khalid Choukri, Alexandra Chronopoulou, Anna Currey, Thierry Declerck, Qian- qian Dong, Yannick Est\u00e9ve, Kevin Duh, Marcello Federico, Souhir Gahbiche, Barry Haddow, Benjamin Hsu, Phu Mon Htut, Hirofumi Inaguma, D\u00e1vid Ja- vorsk\u00fd, John Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Pengwei Li, Xutai Ma, Prashant Mathur, Evgeny Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, Ha Nguyen, Jan Niehues, Xing Niu, Atul Ojha Kr., John E. Ortega, Proyag Pal, Juan Pino, Lonneke van der Plas, Peter Pol\u00e1k, Elijah Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Se- bastian St\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Kevin Tran, Marco Turchi, Alex Waibel, Mingxuan Wang, Shinji Watanabe, and Rodolfo Ze- vallos. 2023. Findings of the IWSLT 2023 Evaluation Campaign. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023). Association for Computational Linguistics. Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3874\u20133884, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Antonios Anastasopoulos, Lo\u00efc Barrault, Luisa Ben- tivogli, Marcely Zanon Boito, Ond\u02c7rej Bojar, Roldano Cattoni, Anna Currey, Georgiana Dinu, Kevin Duh, Maha Elbayad, Clara Emmanuel, Yannick Est\u00e8ve, Marcello Federico, Christian Federmann, Souhir Gahbiche, Hongyu Gong, Roman Grundkiewicz,"}, {"question": " What decoder was used for the end-to-end system in the text?", "answer": " mBART50", "ref_chunk": "7 for creating the datastores. As differ- ent models\u2019 hidden states (which serve as keys in the datastore) also differ substantially, the datastore is MT-model-dependent. To use kNN-MT when ensembling systems (2) and (3), we therefore need two datastores for systems (2) and (3) respectively. The kNN-MT candidate tokens are interpolated with the output vocabulary distribtuion before the ensembling operation. We use hyperparameters k = 8, T = 50, w = 0.3, after an initial search with T \u2208 [10, 50, 100], w \u2208 [0.1, 0.3, 0.5]. Our implemen- tation mostly follows Zheng et al. (2021), which uses the FAISS toolkit (Johnson et al., 2019) for efficient kNN operations. Comparing the infer- ence speed of system (4) and (5), with the same batch size of 64 sentences5, using kNN-MT takes roughly 50% more time on a Nvidia Titan RTX GPU with 24GB memory. Naively using all ACL dev bitext as datastore would lead the model to copying the oracle targets. To simulate the scenario on the blind test set, when translating the i-th talk, we use the other jj\u0338=i \u2208 [n] talks\u2019 bitext as datastore, where n is the total number of talks. As shown in Row (5) of Table 7, kNN-MT brings an additional gain of 1.3 BLEU on MT and 0.8 BLEU on ST. These results shows a datastore as small as hundreds of sentence pairs can be effec- tively used for inference-time domain adaptation. Table 9 shows two examples of kNN-MT im- proving translation quality, apart from generic im- provements in fluency and accuracy, in these ex- amples kNN-MT also helps generate correct termi- nologies and context-appropriate greetings. 4 End-to-End System For the end-to-end system, similar to our ASR model, after seeing initial favourable results of WavLM over Wav2vec, we choose WavLM as the audio encoder. Following last year\u2019s submis- sion (Pham et al., 2022), we use the mBART50 decoder. The results are shown in Row (6) of Ta- ble 7. Contrasting Row (6) and (7) reveals that adding the TTS data does not substantially change ST performance. However, ensembling the two models trained with and without TTS data (Row (8)) improves over the single models (on average +0.7 for ACL, +0.4 for TED), despite them having the identical architecture. Compared to the strongest cascaded system (Row (5)), the end-to-end system falls behind 2.6 BLEU on ACL dev. On TED, however, it appears to slightly outperform the cascaded system. One explanation is that the MT model of the cascaded system has not been separately adapted to TED texts (although parts of the full training data do cover TED data), which was shown essential in im- proving performance on TED test sets (Zhang et al., 2022; Pham et al., 2022). The end-to-end system, on the other hand, has seen a larger proportion of TED data in training (Table 4). Similar to the previous year (Pol\u00e1k et al., 2022), we also adapt our end-to-end offline model for si- 5System (5) requires more GPU memory than system (4). The latter would be able to use a larger batch size of 128 sentences, making the realistic speed difference slightly larger. multaneous track (Pol\u00e1k et al., 2023). 5 Conclusion In this paper, we described our systems for the mul- tilingual speech translation track of IWSLT 2023, which translates English speech into 10 target lan- guages. To tackle the task of translating scien- tific conference talks, which feature non-native in- put speech and terminology-dense contents, our systems have several novelties. Lacking suitable training data for the target domain, we used kNN- MT for inference-time adaptation and showed an improvement of +0.8 BLEU for cascaded speech translation system. We also used adapters to in- tegrate incremental data from augmentation, and achieved performance on-par with re-training on all data. In our experiments, we observed that cas- caded systems are more easily adaptable towards desired target domains due to their separate mod- ules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk transla- tion, although their performance remains similar on TED talks. For future work, we are interested in the feasibility of applying the adaptation approaches shown effective on MT to end-to-end ST. Acknowledgement We thank the anonymous re- viewers for detailed and insightful feedback. Part of this work was performed on the HoreKa su- percomputer funded by the Ministry of Science, Research and the Arts Baden-W\u00fcrttemberg and by the Federal Ministry of Education and Research of Germany. Part of this work was supported by the Federal Ministry of Education and Research of Germany under grant agreement 01EF1803B (RELATER). References Milind Agarwal, Sweta Agrawal, Antonios Anasta- sopoulos, Ond\u02c7rej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, Khalid Choukri, Alexandra Chronopoulou, Anna Currey, Thierry Declerck, Qian- qian Dong, Yannick Est\u00e9ve, Kevin Duh, Marcello Federico, Souhir Gahbiche, Barry Haddow, Benjamin Hsu, Phu Mon Htut, Hirofumi Inaguma, D\u00e1vid Ja- vorsk\u00fd, John Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Pengwei Li, Xutai Ma, Prashant Mathur, Evgeny Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, Ha Nguyen, Jan Niehues, Xing Niu, Atul Ojha Kr., John E. Ortega, Proyag Pal, Juan Pino, Lonneke van der Plas, Peter Pol\u00e1k, Elijah Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Se- bastian St\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Kevin Tran, Marco Turchi, Alex Waibel, Mingxuan Wang, Shinji Watanabe, and Rodolfo Ze- vallos. 2023. Findings of the IWSLT 2023 Evaluation Campaign. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023). Association for Computational Linguistics. Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3874\u20133884, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Antonios Anastasopoulos, Lo\u00efc Barrault, Luisa Ben- tivogli, Marcely Zanon Boito, Ond\u02c7rej Bojar, Roldano Cattoni, Anna Currey, Georgiana Dinu, Kevin Duh, Maha Elbayad, Clara Emmanuel, Yannick Est\u00e8ve, Marcello Federico, Christian Federmann, Souhir Gahbiche, Hongyu Gong, Roman Grundkiewicz,"}, {"question": " Based on the text, does ensembling the models trained with and without TTS data improve performance?", "answer": " Yes, on average +0.7 for ACL, +0.4 for TED", "ref_chunk": "7 for creating the datastores. As differ- ent models\u2019 hidden states (which serve as keys in the datastore) also differ substantially, the datastore is MT-model-dependent. To use kNN-MT when ensembling systems (2) and (3), we therefore need two datastores for systems (2) and (3) respectively. The kNN-MT candidate tokens are interpolated with the output vocabulary distribtuion before the ensembling operation. We use hyperparameters k = 8, T = 50, w = 0.3, after an initial search with T \u2208 [10, 50, 100], w \u2208 [0.1, 0.3, 0.5]. Our implemen- tation mostly follows Zheng et al. (2021), which uses the FAISS toolkit (Johnson et al., 2019) for efficient kNN operations. Comparing the infer- ence speed of system (4) and (5), with the same batch size of 64 sentences5, using kNN-MT takes roughly 50% more time on a Nvidia Titan RTX GPU with 24GB memory. Naively using all ACL dev bitext as datastore would lead the model to copying the oracle targets. To simulate the scenario on the blind test set, when translating the i-th talk, we use the other jj\u0338=i \u2208 [n] talks\u2019 bitext as datastore, where n is the total number of talks. As shown in Row (5) of Table 7, kNN-MT brings an additional gain of 1.3 BLEU on MT and 0.8 BLEU on ST. These results shows a datastore as small as hundreds of sentence pairs can be effec- tively used for inference-time domain adaptation. Table 9 shows two examples of kNN-MT im- proving translation quality, apart from generic im- provements in fluency and accuracy, in these ex- amples kNN-MT also helps generate correct termi- nologies and context-appropriate greetings. 4 End-to-End System For the end-to-end system, similar to our ASR model, after seeing initial favourable results of WavLM over Wav2vec, we choose WavLM as the audio encoder. Following last year\u2019s submis- sion (Pham et al., 2022), we use the mBART50 decoder. The results are shown in Row (6) of Ta- ble 7. Contrasting Row (6) and (7) reveals that adding the TTS data does not substantially change ST performance. However, ensembling the two models trained with and without TTS data (Row (8)) improves over the single models (on average +0.7 for ACL, +0.4 for TED), despite them having the identical architecture. Compared to the strongest cascaded system (Row (5)), the end-to-end system falls behind 2.6 BLEU on ACL dev. On TED, however, it appears to slightly outperform the cascaded system. One explanation is that the MT model of the cascaded system has not been separately adapted to TED texts (although parts of the full training data do cover TED data), which was shown essential in im- proving performance on TED test sets (Zhang et al., 2022; Pham et al., 2022). The end-to-end system, on the other hand, has seen a larger proportion of TED data in training (Table 4). Similar to the previous year (Pol\u00e1k et al., 2022), we also adapt our end-to-end offline model for si- 5System (5) requires more GPU memory than system (4). The latter would be able to use a larger batch size of 128 sentences, making the realistic speed difference slightly larger. multaneous track (Pol\u00e1k et al., 2023). 5 Conclusion In this paper, we described our systems for the mul- tilingual speech translation track of IWSLT 2023, which translates English speech into 10 target lan- guages. To tackle the task of translating scien- tific conference talks, which feature non-native in- put speech and terminology-dense contents, our systems have several novelties. Lacking suitable training data for the target domain, we used kNN- MT for inference-time adaptation and showed an improvement of +0.8 BLEU for cascaded speech translation system. We also used adapters to in- tegrate incremental data from augmentation, and achieved performance on-par with re-training on all data. In our experiments, we observed that cas- caded systems are more easily adaptable towards desired target domains due to their separate mod- ules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk transla- tion, although their performance remains similar on TED talks. For future work, we are interested in the feasibility of applying the adaptation approaches shown effective on MT to end-to-end ST. Acknowledgement We thank the anonymous re- viewers for detailed and insightful feedback. Part of this work was performed on the HoreKa su- percomputer funded by the Ministry of Science, Research and the Arts Baden-W\u00fcrttemberg and by the Federal Ministry of Education and Research of Germany. Part of this work was supported by the Federal Ministry of Education and Research of Germany under grant agreement 01EF1803B (RELATER). References Milind Agarwal, Sweta Agrawal, Antonios Anasta- sopoulos, Ond\u02c7rej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, Khalid Choukri, Alexandra Chronopoulou, Anna Currey, Thierry Declerck, Qian- qian Dong, Yannick Est\u00e9ve, Kevin Duh, Marcello Federico, Souhir Gahbiche, Barry Haddow, Benjamin Hsu, Phu Mon Htut, Hirofumi Inaguma, D\u00e1vid Ja- vorsk\u00fd, John Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Pengwei Li, Xutai Ma, Prashant Mathur, Evgeny Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, Ha Nguyen, Jan Niehues, Xing Niu, Atul Ojha Kr., John E. Ortega, Proyag Pal, Juan Pino, Lonneke van der Plas, Peter Pol\u00e1k, Elijah Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Se- bastian St\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Kevin Tran, Marco Turchi, Alex Waibel, Mingxuan Wang, Shinji Watanabe, and Rodolfo Ze- vallos. 2023. Findings of the IWSLT 2023 Evaluation Campaign. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023). Association for Computational Linguistics. Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3874\u20133884, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Antonios Anastasopoulos, Lo\u00efc Barrault, Luisa Ben- tivogli, Marcely Zanon Boito, Ond\u02c7rej Bojar, Roldano Cattoni, Anna Currey, Georgiana Dinu, Kevin Duh, Maha Elbayad, Clara Emmanuel, Yannick Est\u00e8ve, Marcello Federico, Christian Federmann, Souhir Gahbiche, Hongyu Gong, Roman Grundkiewicz,"}, {"question": " What type of system outperforms its end-to-end counterpart on scientific talk translation?", "answer": " Cascaded speech system", "ref_chunk": "7 for creating the datastores. As differ- ent models\u2019 hidden states (which serve as keys in the datastore) also differ substantially, the datastore is MT-model-dependent. To use kNN-MT when ensembling systems (2) and (3), we therefore need two datastores for systems (2) and (3) respectively. The kNN-MT candidate tokens are interpolated with the output vocabulary distribtuion before the ensembling operation. We use hyperparameters k = 8, T = 50, w = 0.3, after an initial search with T \u2208 [10, 50, 100], w \u2208 [0.1, 0.3, 0.5]. Our implemen- tation mostly follows Zheng et al. (2021), which uses the FAISS toolkit (Johnson et al., 2019) for efficient kNN operations. Comparing the infer- ence speed of system (4) and (5), with the same batch size of 64 sentences5, using kNN-MT takes roughly 50% more time on a Nvidia Titan RTX GPU with 24GB memory. Naively using all ACL dev bitext as datastore would lead the model to copying the oracle targets. To simulate the scenario on the blind test set, when translating the i-th talk, we use the other jj\u0338=i \u2208 [n] talks\u2019 bitext as datastore, where n is the total number of talks. As shown in Row (5) of Table 7, kNN-MT brings an additional gain of 1.3 BLEU on MT and 0.8 BLEU on ST. These results shows a datastore as small as hundreds of sentence pairs can be effec- tively used for inference-time domain adaptation. Table 9 shows two examples of kNN-MT im- proving translation quality, apart from generic im- provements in fluency and accuracy, in these ex- amples kNN-MT also helps generate correct termi- nologies and context-appropriate greetings. 4 End-to-End System For the end-to-end system, similar to our ASR model, after seeing initial favourable results of WavLM over Wav2vec, we choose WavLM as the audio encoder. Following last year\u2019s submis- sion (Pham et al., 2022), we use the mBART50 decoder. The results are shown in Row (6) of Ta- ble 7. Contrasting Row (6) and (7) reveals that adding the TTS data does not substantially change ST performance. However, ensembling the two models trained with and without TTS data (Row (8)) improves over the single models (on average +0.7 for ACL, +0.4 for TED), despite them having the identical architecture. Compared to the strongest cascaded system (Row (5)), the end-to-end system falls behind 2.6 BLEU on ACL dev. On TED, however, it appears to slightly outperform the cascaded system. One explanation is that the MT model of the cascaded system has not been separately adapted to TED texts (although parts of the full training data do cover TED data), which was shown essential in im- proving performance on TED test sets (Zhang et al., 2022; Pham et al., 2022). The end-to-end system, on the other hand, has seen a larger proportion of TED data in training (Table 4). Similar to the previous year (Pol\u00e1k et al., 2022), we also adapt our end-to-end offline model for si- 5System (5) requires more GPU memory than system (4). The latter would be able to use a larger batch size of 128 sentences, making the realistic speed difference slightly larger. multaneous track (Pol\u00e1k et al., 2023). 5 Conclusion In this paper, we described our systems for the mul- tilingual speech translation track of IWSLT 2023, which translates English speech into 10 target lan- guages. To tackle the task of translating scien- tific conference talks, which feature non-native in- put speech and terminology-dense contents, our systems have several novelties. Lacking suitable training data for the target domain, we used kNN- MT for inference-time adaptation and showed an improvement of +0.8 BLEU for cascaded speech translation system. We also used adapters to in- tegrate incremental data from augmentation, and achieved performance on-par with re-training on all data. In our experiments, we observed that cas- caded systems are more easily adaptable towards desired target domains due to their separate mod- ules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk transla- tion, although their performance remains similar on TED talks. For future work, we are interested in the feasibility of applying the adaptation approaches shown effective on MT to end-to-end ST. Acknowledgement We thank the anonymous re- viewers for detailed and insightful feedback. Part of this work was performed on the HoreKa su- percomputer funded by the Ministry of Science, Research and the Arts Baden-W\u00fcrttemberg and by the Federal Ministry of Education and Research of Germany. Part of this work was supported by the Federal Ministry of Education and Research of Germany under grant agreement 01EF1803B (RELATER). References Milind Agarwal, Sweta Agrawal, Antonios Anasta- sopoulos, Ond\u02c7rej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, Khalid Choukri, Alexandra Chronopoulou, Anna Currey, Thierry Declerck, Qian- qian Dong, Yannick Est\u00e9ve, Kevin Duh, Marcello Federico, Souhir Gahbiche, Barry Haddow, Benjamin Hsu, Phu Mon Htut, Hirofumi Inaguma, D\u00e1vid Ja- vorsk\u00fd, John Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Pengwei Li, Xutai Ma, Prashant Mathur, Evgeny Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, Ha Nguyen, Jan Niehues, Xing Niu, Atul Ojha Kr., John E. Ortega, Proyag Pal, Juan Pino, Lonneke van der Plas, Peter Pol\u00e1k, Elijah Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Se- bastian St\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Kevin Tran, Marco Turchi, Alex Waibel, Mingxuan Wang, Shinji Watanabe, and Rodolfo Ze- vallos. 2023. Findings of the IWSLT 2023 Evaluation Campaign. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023). Association for Computational Linguistics. Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3874\u20133884, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Antonios Anastasopoulos, Lo\u00efc Barrault, Luisa Ben- tivogli, Marcely Zanon Boito, Ond\u02c7rej Bojar, Roldano Cattoni, Anna Currey, Georgiana Dinu, Kevin Duh, Maha Elbayad, Clara Emmanuel, Yannick Est\u00e8ve, Marcello Federico, Christian Federmann, Souhir Gahbiche, Hongyu Gong, Roman Grundkiewicz,"}], "doc_text": "7 for creating the datastores. As differ- ent models\u2019 hidden states (which serve as keys in the datastore) also differ substantially, the datastore is MT-model-dependent. To use kNN-MT when ensembling systems (2) and (3), we therefore need two datastores for systems (2) and (3) respectively. The kNN-MT candidate tokens are interpolated with the output vocabulary distribtuion before the ensembling operation. We use hyperparameters k = 8, T = 50, w = 0.3, after an initial search with T \u2208 [10, 50, 100], w \u2208 [0.1, 0.3, 0.5]. Our implemen- tation mostly follows Zheng et al. (2021), which uses the FAISS toolkit (Johnson et al., 2019) for efficient kNN operations. Comparing the infer- ence speed of system (4) and (5), with the same batch size of 64 sentences5, using kNN-MT takes roughly 50% more time on a Nvidia Titan RTX GPU with 24GB memory. Naively using all ACL dev bitext as datastore would lead the model to copying the oracle targets. To simulate the scenario on the blind test set, when translating the i-th talk, we use the other jj\u0338=i \u2208 [n] talks\u2019 bitext as datastore, where n is the total number of talks. As shown in Row (5) of Table 7, kNN-MT brings an additional gain of 1.3 BLEU on MT and 0.8 BLEU on ST. These results shows a datastore as small as hundreds of sentence pairs can be effec- tively used for inference-time domain adaptation. Table 9 shows two examples of kNN-MT im- proving translation quality, apart from generic im- provements in fluency and accuracy, in these ex- amples kNN-MT also helps generate correct termi- nologies and context-appropriate greetings. 4 End-to-End System For the end-to-end system, similar to our ASR model, after seeing initial favourable results of WavLM over Wav2vec, we choose WavLM as the audio encoder. Following last year\u2019s submis- sion (Pham et al., 2022), we use the mBART50 decoder. The results are shown in Row (6) of Ta- ble 7. Contrasting Row (6) and (7) reveals that adding the TTS data does not substantially change ST performance. However, ensembling the two models trained with and without TTS data (Row (8)) improves over the single models (on average +0.7 for ACL, +0.4 for TED), despite them having the identical architecture. Compared to the strongest cascaded system (Row (5)), the end-to-end system falls behind 2.6 BLEU on ACL dev. On TED, however, it appears to slightly outperform the cascaded system. One explanation is that the MT model of the cascaded system has not been separately adapted to TED texts (although parts of the full training data do cover TED data), which was shown essential in im- proving performance on TED test sets (Zhang et al., 2022; Pham et al., 2022). The end-to-end system, on the other hand, has seen a larger proportion of TED data in training (Table 4). Similar to the previous year (Pol\u00e1k et al., 2022), we also adapt our end-to-end offline model for si- 5System (5) requires more GPU memory than system (4). The latter would be able to use a larger batch size of 128 sentences, making the realistic speed difference slightly larger. multaneous track (Pol\u00e1k et al., 2023). 5 Conclusion In this paper, we described our systems for the mul- tilingual speech translation track of IWSLT 2023, which translates English speech into 10 target lan- guages. To tackle the task of translating scien- tific conference talks, which feature non-native in- put speech and terminology-dense contents, our systems have several novelties. Lacking suitable training data for the target domain, we used kNN- MT for inference-time adaptation and showed an improvement of +0.8 BLEU for cascaded speech translation system. We also used adapters to in- tegrate incremental data from augmentation, and achieved performance on-par with re-training on all data. In our experiments, we observed that cas- caded systems are more easily adaptable towards desired target domains due to their separate mod- ules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk transla- tion, although their performance remains similar on TED talks. For future work, we are interested in the feasibility of applying the adaptation approaches shown effective on MT to end-to-end ST. Acknowledgement We thank the anonymous re- viewers for detailed and insightful feedback. Part of this work was performed on the HoreKa su- percomputer funded by the Ministry of Science, Research and the Arts Baden-W\u00fcrttemberg and by the Federal Ministry of Education and Research of Germany. Part of this work was supported by the Federal Ministry of Education and Research of Germany under grant agreement 01EF1803B (RELATER). References Milind Agarwal, Sweta Agrawal, Antonios Anasta- sopoulos, Ond\u02c7rej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, Khalid Choukri, Alexandra Chronopoulou, Anna Currey, Thierry Declerck, Qian- qian Dong, Yannick Est\u00e9ve, Kevin Duh, Marcello Federico, Souhir Gahbiche, Barry Haddow, Benjamin Hsu, Phu Mon Htut, Hirofumi Inaguma, D\u00e1vid Ja- vorsk\u00fd, John Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Pengwei Li, Xutai Ma, Prashant Mathur, Evgeny Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, Ha Nguyen, Jan Niehues, Xing Niu, Atul Ojha Kr., John E. Ortega, Proyag Pal, Juan Pino, Lonneke van der Plas, Peter Pol\u00e1k, Elijah Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Se- bastian St\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Kevin Tran, Marco Turchi, Alex Waibel, Mingxuan Wang, Shinji Watanabe, and Rodolfo Ze- vallos. 2023. Findings of the IWSLT 2023 Evaluation Campaign. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023). Association for Computational Linguistics. Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3874\u20133884, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Antonios Anastasopoulos, Lo\u00efc Barrault, Luisa Ben- tivogli, Marcely Zanon Boito, Ond\u02c7rej Bojar, Roldano Cattoni, Anna Currey, Georgiana Dinu, Kevin Duh, Maha Elbayad, Clara Emmanuel, Yannick Est\u00e8ve, Marcello Federico, Christian Federmann, Souhir Gahbiche, Hongyu Gong, Roman Grundkiewicz,"}