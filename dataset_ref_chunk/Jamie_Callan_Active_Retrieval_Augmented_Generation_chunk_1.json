{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Jamie_Callan_Active_Retrieval_Augmented_Generation_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main topic of the text?", "answer": " The main topic of the text is active retrieval augmented generation in language models.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " What is the purpose of augmenting language models with retrieval components?", "answer": " The purpose is to address hallucination and create factually accurate content by retrieving information from external knowledge resources.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " What is the proposed method in this work for active retrieval augmented generation?", "answer": " The proposed method is Forward-Looking Active REtrieval augmented generation (FLARE), which actively decides when and what to retrieve during the generation process.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " What tasks/datasets were used to test FLARE in comparison to baselines?", "answer": " FLARE was tested comprehensively over 4 long-form knowledge-intensive generation tasks/datasets.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " How does FLARE generate content when it encounters low-confidence tokens in a sentence?", "answer": " FLARE uses a prediction of the upcoming sentence to anticipate future content, then retrieves relevant documents to regenerate the sentence.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " What is the difference between short-form and long-form knowledge-intensive generation tasks?", "answer": " Short-form tasks involve clear information needs in the input, while long-form tasks require gathering multiple pieces of knowledge throughout the generation process.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " What are some examples of long-form generation tasks mentioned in the text?", "answer": " Examples include long-form question answering, open-domain summarization, and chain-of-thought reasoning.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " How do humans typically gather information during content creation according to the text?", "answer": " Humans gradually gather information as they create content, similar to the proposed approach for long-form generation with LMs.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " What is the challenge with retrieving information multiple times throughout generation according to the text?", "answer": " Existing methods may not accurately reflect what LMs intend to generate in the future or retrieve at inappropriate points.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}, {"question": " What is the research question posed in the text regarding retrieval augmented LMs?", "answer": " The research question is whether a simple and generic retrieval augmented LM can be created.", "ref_chunk": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}], "doc_text": "3 2 0 2 t c O 2 2 ] L C . s c [ 2 v 3 8 9 6 0 . 5 0 3 2 : v i X r a Active Retrieval Augmented Generation Zhengbao Jiang1\u2217 Frank F. Xu1\u2217 Luyu Gao1\u2217 Zhiqing Sun1\u2217 Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to \u2217Lead contributors. 1Code and datasets are available at https://github.com/ hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user\u2019s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user\u2019s input, and it is sufficient to retrieve relevant knowledge once solely based on the input. Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long- form QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex informa- tion needs that are not always evident from the in- put alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowl- edge throughout the generation process. For ex- ample, to generate a summary about a particular topic, the initial retrieval based on the topic name jzbjyb/FLARE. \"####\"#$#$$Step 2 He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science. Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019 . Search results: !\"![1]: \u2026[2]: \u2026 Joe Biden attended the University of Pennsylvania RetrieverInputStep 1 a law degree Search results: !\"\"[1]: \u2026[2]: \u2026 RetrieveddocumentsLMGeneration$%$%% , where he earned Generate a summary about Joe Biden.Search results: !![1]: \u2026[2]: \u2026 Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019. .\"#%#%Step 3 Joe Biden (born November 20, 1942) is the 46th president of the United States. Joe Biden (born November 20, 1942) is the 46th president of the United States. Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence. (e.g., Joe Biden) may not cover all aspects and de- tails. It is crucial to retrieve extra information as needed during generation, such as when generat- ing a certain aspect (e.g., Joe Biden\u2019s education history) or a specific detail (e.g., the date of Joe Biden\u2019s presidential campaign announcement). Several attempts have been made to retrieve mul- tiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to gener- ate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM that"}