{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/A._Waibel_KIT\u2019s_Multilingual_Speech_Translation_System_for_IWSLT_2023_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the speech recognition data sources used for ASR training?", "answer": " Common Voice, LibriSpeech, MuST-C v2, TED-LIUM v3, and VoxPopuli", "ref_chunk": "data. Corpus / Data Source Hours # Utterances Common Voice LibriSpeech MuST-C v2 TED-LIUM v3 VoxPopuli 1667 963 482 452 501 1225k 281k 251k 268k 177k TTS 7284 4.7M Table 2: ASR data overview. 2.2 Speech Recognition Data the ASR training, we use Common For Voice (Ardila et al., 2020), LibriSpeech (Panayotov et al., 2015), MuST-C v2 (Di Gangi et al., 2019), TED-LIUM v3 (Hernandez et al., 2018), and VoxPopuli (Wang et al., 2021). The data overview is in Table 2. Synthesized Speech Data To adapt the ASR model to the ACL talks, we add synthesized speech created by a text-to-speech (TTS) model. Specifi- cally, from the MT bitext English side (Table 3), we select sentences similar to the ACL domain based on similarity with the provided ACL dev bitext and abstracts. Inspired by data selection strategies for MT (Eck et al., 2005; Koneru et al., 2022), we use n-gram overlap as similarity metric. 4.7M sen- tences are selected and then synthesized to speech by a VITS (Kim et al., 2021) model trained on MuST-C. The synthesized data amount is shown in the last row of Table 2. 2.3 Machine Translation Data The MT training data include the following translation corpora: Europarl v7 text-to-text and v10 (Koehn, 2005), NewsCommentary v16, OpenSubtitles v2018 (Lison and Tiedemann, 2016), Tatoeba (Tiedemann, 2012), and ELRC- CORDIS_News, JParaCrawl (Morishita et al., 2022) for Japanese, and TED2020 (Reimers and Gurevych, 2020) for German2. We also include the text translation part of the following ST cor- 2This dataset has deplication with past evaluation sets: tst2019 tst2020 and tst-COMMON. The deplications were removed prior to training. pora: MuST-C (Di Gangi et al., 2019), CoVoST v2 (Wang et al., 2020), and Europarl-ST (Iranzo- S\u00e1nchez et al., 2020). The aggregated data amount per language is summarized in the \u201cOriginal\u201d col- umn of Table 3. Original After Diversification Lang. # sent. (M) # sent. (M) # tokens (M) ar zh nl fr de ja* fa pt ru tr 26.0 11.2 33.1 38.9 23.0 2.6 5.8 29.0 22.1 36.7 65.2 21.5 82.1 91.6 54.4 27.2 11.3 72.3 51.5 89.7 865.0 254.3 1162.7 1427.8 860.0 832.7 162.1 1024.3 685.3 1021.2 Total 228.4 566.8 8295.4 Table 3: MT data overview. *: For ja, the original data of 2.6M sentences did not include JParaCrawl, which was announced later as allowed data. As preprocessing, we perform truecasing, dedu- plication, length ratio filtering, and histogram filter- ing using the statistics by Fan et al. (2021). Then we perform subword segmentation using Sentence- piece (Kudo and Richardson, 2018) based on the vocabulary of mBART50 (Tang et al., 2020). Data Diversification Different from last years\u2019 shared tasks (Anastasopoulos et al., 2021, 2022), no monolingual (non-English) data is provided. This means conventional data augmentation tech- niques like backward translation are not directly applicable. On the other hand, forward translation from existing English monolingual data may intro- duce undesirable errors in the translation targets, especially on lower-resource languages. In this light, we use data diversification (Nguyen et al., 2020), a data augmentation method that enriches existing parallel data by forward and backward translating the training bitext. As the model has seen the parallel data in training, the synthetic trans- lations are expected to have relatively high quality. Moreover, either the source or target side of the synthetic data is from the original bitext. The di- versified data amount after deduplication is shown in Table 3. Here we perform one round of forward and backward translation, as Nguyen et al. (2020) have empirically shown further rounds do not lead to substantial gains. 2.4 Casing/Punctuation Restoration Data The ASR outputs are lower-cased and unpunctu- ated, while the MT model expects cased and punc- tuated inputs. We randomly sample 1.5 million En- glish sentences from the MT training data (Table 3), and remove the casing and punctuation marks as training source data. We then train a model to re- store the casing and punctuation marks. 2.5 Speech Translation Data The speech translation data are shown in Ta- ble 4. We additionally use our trained MT model to create forward translations based on the fol- lowing transcript-only datasets: Common Voice, TEDLIUM, and VoxPopuli. The TTS data de- scribed in \u00a72.2 is also used. Lang. Corpus / Data Source Hours # Utterances ar zh nl fr de ja fa pt ru CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C europarl-ST TTS CoVoST MuST-C europarl-ST TTS CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C TTS CoVoST MuST-C TTS 429 463 283 429 596 204 434 75 1138 485 76 1768 429 440 77 1891 429 541 73 429 347 89 377 75 1678 482 331 429 446 428 289k 212k 203k 289k 358k 183k 248k 32k 713k 275k 32k 998k 289k 269k 33k 779k 289k 329k 56k 289k 182k 88k 206k 32k 639k 265k 331k 289k 236k 511k tr all Common Voice TEDLIUM VoxPopuli 1488 453 502 948k 268k 177k Table 4: ST data overview. The last section \u201call\u201d indi- cates forward translated synthetic targets from transcript- only corpora, which are available for all 10 languages. 3 Cascaded System For the cascaded system, we introduce our ASR (\u00a73.1) and MT (\u00a73.2) models. 3.1 Automatic Speech Recognition Module Baseline Models The first baseline is our ASR model for last year\u2019s offline track (Pham et al., 2022). It is a Wav2vec 2.0 (Baevski et al., 2020) with LARGE configuration pretrained on 960 hours of Librispeech data. This year, after seeing ini- tial favourable results compared to Wav2vec, we opt for WavLM (Chen et al., 2022) as audio en- coder. We use the LARGE configuration with 24 layers. We use the mBART50 (Tang et al., 2020) decoder along with the WavLM encoder. As the ASR model only needs to transcribe English3, we trim the mBART50 vocabulary from 256k down to 62k tokens by removing all non-alphabetic tokens. In-Domain TTS Data We also use the synthe- sized TTS data. Compared to the same model without TTS data, the word error rate (WER) im- proves from 11.6% to"}, {"question": " How was the synthesized speech data adapted for the ACL talks?", "answer": " By selecting sentences similar to the ACL domain based on similarity with provided ACL dev bitext and abstracts using n-gram overlap as a similarity metric.", "ref_chunk": "data. Corpus / Data Source Hours # Utterances Common Voice LibriSpeech MuST-C v2 TED-LIUM v3 VoxPopuli 1667 963 482 452 501 1225k 281k 251k 268k 177k TTS 7284 4.7M Table 2: ASR data overview. 2.2 Speech Recognition Data the ASR training, we use Common For Voice (Ardila et al., 2020), LibriSpeech (Panayotov et al., 2015), MuST-C v2 (Di Gangi et al., 2019), TED-LIUM v3 (Hernandez et al., 2018), and VoxPopuli (Wang et al., 2021). The data overview is in Table 2. Synthesized Speech Data To adapt the ASR model to the ACL talks, we add synthesized speech created by a text-to-speech (TTS) model. Specifi- cally, from the MT bitext English side (Table 3), we select sentences similar to the ACL domain based on similarity with the provided ACL dev bitext and abstracts. Inspired by data selection strategies for MT (Eck et al., 2005; Koneru et al., 2022), we use n-gram overlap as similarity metric. 4.7M sen- tences are selected and then synthesized to speech by a VITS (Kim et al., 2021) model trained on MuST-C. The synthesized data amount is shown in the last row of Table 2. 2.3 Machine Translation Data The MT training data include the following translation corpora: Europarl v7 text-to-text and v10 (Koehn, 2005), NewsCommentary v16, OpenSubtitles v2018 (Lison and Tiedemann, 2016), Tatoeba (Tiedemann, 2012), and ELRC- CORDIS_News, JParaCrawl (Morishita et al., 2022) for Japanese, and TED2020 (Reimers and Gurevych, 2020) for German2. We also include the text translation part of the following ST cor- 2This dataset has deplication with past evaluation sets: tst2019 tst2020 and tst-COMMON. The deplications were removed prior to training. pora: MuST-C (Di Gangi et al., 2019), CoVoST v2 (Wang et al., 2020), and Europarl-ST (Iranzo- S\u00e1nchez et al., 2020). The aggregated data amount per language is summarized in the \u201cOriginal\u201d col- umn of Table 3. Original After Diversification Lang. # sent. (M) # sent. (M) # tokens (M) ar zh nl fr de ja* fa pt ru tr 26.0 11.2 33.1 38.9 23.0 2.6 5.8 29.0 22.1 36.7 65.2 21.5 82.1 91.6 54.4 27.2 11.3 72.3 51.5 89.7 865.0 254.3 1162.7 1427.8 860.0 832.7 162.1 1024.3 685.3 1021.2 Total 228.4 566.8 8295.4 Table 3: MT data overview. *: For ja, the original data of 2.6M sentences did not include JParaCrawl, which was announced later as allowed data. As preprocessing, we perform truecasing, dedu- plication, length ratio filtering, and histogram filter- ing using the statistics by Fan et al. (2021). Then we perform subword segmentation using Sentence- piece (Kudo and Richardson, 2018) based on the vocabulary of mBART50 (Tang et al., 2020). Data Diversification Different from last years\u2019 shared tasks (Anastasopoulos et al., 2021, 2022), no monolingual (non-English) data is provided. This means conventional data augmentation tech- niques like backward translation are not directly applicable. On the other hand, forward translation from existing English monolingual data may intro- duce undesirable errors in the translation targets, especially on lower-resource languages. In this light, we use data diversification (Nguyen et al., 2020), a data augmentation method that enriches existing parallel data by forward and backward translating the training bitext. As the model has seen the parallel data in training, the synthetic trans- lations are expected to have relatively high quality. Moreover, either the source or target side of the synthetic data is from the original bitext. The di- versified data amount after deduplication is shown in Table 3. Here we perform one round of forward and backward translation, as Nguyen et al. (2020) have empirically shown further rounds do not lead to substantial gains. 2.4 Casing/Punctuation Restoration Data The ASR outputs are lower-cased and unpunctu- ated, while the MT model expects cased and punc- tuated inputs. We randomly sample 1.5 million En- glish sentences from the MT training data (Table 3), and remove the casing and punctuation marks as training source data. We then train a model to re- store the casing and punctuation marks. 2.5 Speech Translation Data The speech translation data are shown in Ta- ble 4. We additionally use our trained MT model to create forward translations based on the fol- lowing transcript-only datasets: Common Voice, TEDLIUM, and VoxPopuli. The TTS data de- scribed in \u00a72.2 is also used. Lang. Corpus / Data Source Hours # Utterances ar zh nl fr de ja fa pt ru CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C europarl-ST TTS CoVoST MuST-C europarl-ST TTS CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C TTS CoVoST MuST-C TTS 429 463 283 429 596 204 434 75 1138 485 76 1768 429 440 77 1891 429 541 73 429 347 89 377 75 1678 482 331 429 446 428 289k 212k 203k 289k 358k 183k 248k 32k 713k 275k 32k 998k 289k 269k 33k 779k 289k 329k 56k 289k 182k 88k 206k 32k 639k 265k 331k 289k 236k 511k tr all Common Voice TEDLIUM VoxPopuli 1488 453 502 948k 268k 177k Table 4: ST data overview. The last section \u201call\u201d indi- cates forward translated synthetic targets from transcript- only corpora, which are available for all 10 languages. 3 Cascaded System For the cascaded system, we introduce our ASR (\u00a73.1) and MT (\u00a73.2) models. 3.1 Automatic Speech Recognition Module Baseline Models The first baseline is our ASR model for last year\u2019s offline track (Pham et al., 2022). It is a Wav2vec 2.0 (Baevski et al., 2020) with LARGE configuration pretrained on 960 hours of Librispeech data. This year, after seeing ini- tial favourable results compared to Wav2vec, we opt for WavLM (Chen et al., 2022) as audio en- coder. We use the LARGE configuration with 24 layers. We use the mBART50 (Tang et al., 2020) decoder along with the WavLM encoder. As the ASR model only needs to transcribe English3, we trim the mBART50 vocabulary from 256k down to 62k tokens by removing all non-alphabetic tokens. In-Domain TTS Data We also use the synthe- sized TTS data. Compared to the same model without TTS data, the word error rate (WER) im- proves from 11.6% to"}, {"question": " What corpora are included in the MT training data?", "answer": " Europarl v7 text-to-text and v10, NewsCommentary v16, OpenSubtitles v2018, Tatoeba, ELRC-CORDIS_News, JParaCrawl for Japanese, and TED2020 for German.", "ref_chunk": "data. Corpus / Data Source Hours # Utterances Common Voice LibriSpeech MuST-C v2 TED-LIUM v3 VoxPopuli 1667 963 482 452 501 1225k 281k 251k 268k 177k TTS 7284 4.7M Table 2: ASR data overview. 2.2 Speech Recognition Data the ASR training, we use Common For Voice (Ardila et al., 2020), LibriSpeech (Panayotov et al., 2015), MuST-C v2 (Di Gangi et al., 2019), TED-LIUM v3 (Hernandez et al., 2018), and VoxPopuli (Wang et al., 2021). The data overview is in Table 2. Synthesized Speech Data To adapt the ASR model to the ACL talks, we add synthesized speech created by a text-to-speech (TTS) model. Specifi- cally, from the MT bitext English side (Table 3), we select sentences similar to the ACL domain based on similarity with the provided ACL dev bitext and abstracts. Inspired by data selection strategies for MT (Eck et al., 2005; Koneru et al., 2022), we use n-gram overlap as similarity metric. 4.7M sen- tences are selected and then synthesized to speech by a VITS (Kim et al., 2021) model trained on MuST-C. The synthesized data amount is shown in the last row of Table 2. 2.3 Machine Translation Data The MT training data include the following translation corpora: Europarl v7 text-to-text and v10 (Koehn, 2005), NewsCommentary v16, OpenSubtitles v2018 (Lison and Tiedemann, 2016), Tatoeba (Tiedemann, 2012), and ELRC- CORDIS_News, JParaCrawl (Morishita et al., 2022) for Japanese, and TED2020 (Reimers and Gurevych, 2020) for German2. We also include the text translation part of the following ST cor- 2This dataset has deplication with past evaluation sets: tst2019 tst2020 and tst-COMMON. The deplications were removed prior to training. pora: MuST-C (Di Gangi et al., 2019), CoVoST v2 (Wang et al., 2020), and Europarl-ST (Iranzo- S\u00e1nchez et al., 2020). The aggregated data amount per language is summarized in the \u201cOriginal\u201d col- umn of Table 3. Original After Diversification Lang. # sent. (M) # sent. (M) # tokens (M) ar zh nl fr de ja* fa pt ru tr 26.0 11.2 33.1 38.9 23.0 2.6 5.8 29.0 22.1 36.7 65.2 21.5 82.1 91.6 54.4 27.2 11.3 72.3 51.5 89.7 865.0 254.3 1162.7 1427.8 860.0 832.7 162.1 1024.3 685.3 1021.2 Total 228.4 566.8 8295.4 Table 3: MT data overview. *: For ja, the original data of 2.6M sentences did not include JParaCrawl, which was announced later as allowed data. As preprocessing, we perform truecasing, dedu- plication, length ratio filtering, and histogram filter- ing using the statistics by Fan et al. (2021). Then we perform subword segmentation using Sentence- piece (Kudo and Richardson, 2018) based on the vocabulary of mBART50 (Tang et al., 2020). Data Diversification Different from last years\u2019 shared tasks (Anastasopoulos et al., 2021, 2022), no monolingual (non-English) data is provided. This means conventional data augmentation tech- niques like backward translation are not directly applicable. On the other hand, forward translation from existing English monolingual data may intro- duce undesirable errors in the translation targets, especially on lower-resource languages. In this light, we use data diversification (Nguyen et al., 2020), a data augmentation method that enriches existing parallel data by forward and backward translating the training bitext. As the model has seen the parallel data in training, the synthetic trans- lations are expected to have relatively high quality. Moreover, either the source or target side of the synthetic data is from the original bitext. The di- versified data amount after deduplication is shown in Table 3. Here we perform one round of forward and backward translation, as Nguyen et al. (2020) have empirically shown further rounds do not lead to substantial gains. 2.4 Casing/Punctuation Restoration Data The ASR outputs are lower-cased and unpunctu- ated, while the MT model expects cased and punc- tuated inputs. We randomly sample 1.5 million En- glish sentences from the MT training data (Table 3), and remove the casing and punctuation marks as training source data. We then train a model to re- store the casing and punctuation marks. 2.5 Speech Translation Data The speech translation data are shown in Ta- ble 4. We additionally use our trained MT model to create forward translations based on the fol- lowing transcript-only datasets: Common Voice, TEDLIUM, and VoxPopuli. The TTS data de- scribed in \u00a72.2 is also used. Lang. Corpus / Data Source Hours # Utterances ar zh nl fr de ja fa pt ru CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C europarl-ST TTS CoVoST MuST-C europarl-ST TTS CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C TTS CoVoST MuST-C TTS 429 463 283 429 596 204 434 75 1138 485 76 1768 429 440 77 1891 429 541 73 429 347 89 377 75 1678 482 331 429 446 428 289k 212k 203k 289k 358k 183k 248k 32k 713k 275k 32k 998k 289k 269k 33k 779k 289k 329k 56k 289k 182k 88k 206k 32k 639k 265k 331k 289k 236k 511k tr all Common Voice TEDLIUM VoxPopuli 1488 453 502 948k 268k 177k Table 4: ST data overview. The last section \u201call\u201d indi- cates forward translated synthetic targets from transcript- only corpora, which are available for all 10 languages. 3 Cascaded System For the cascaded system, we introduce our ASR (\u00a73.1) and MT (\u00a73.2) models. 3.1 Automatic Speech Recognition Module Baseline Models The first baseline is our ASR model for last year\u2019s offline track (Pham et al., 2022). It is a Wav2vec 2.0 (Baevski et al., 2020) with LARGE configuration pretrained on 960 hours of Librispeech data. This year, after seeing ini- tial favourable results compared to Wav2vec, we opt for WavLM (Chen et al., 2022) as audio en- coder. We use the LARGE configuration with 24 layers. We use the mBART50 (Tang et al., 2020) decoder along with the WavLM encoder. As the ASR model only needs to transcribe English3, we trim the mBART50 vocabulary from 256k down to 62k tokens by removing all non-alphabetic tokens. In-Domain TTS Data We also use the synthe- sized TTS data. Compared to the same model without TTS data, the word error rate (WER) im- proves from 11.6% to"}, {"question": " What is the purpose of data diversification in this context?", "answer": " To enrich existing parallel data by forward and backward translating the training bitext to improve translation quality on lower-resource languages.", "ref_chunk": "data. Corpus / Data Source Hours # Utterances Common Voice LibriSpeech MuST-C v2 TED-LIUM v3 VoxPopuli 1667 963 482 452 501 1225k 281k 251k 268k 177k TTS 7284 4.7M Table 2: ASR data overview. 2.2 Speech Recognition Data the ASR training, we use Common For Voice (Ardila et al., 2020), LibriSpeech (Panayotov et al., 2015), MuST-C v2 (Di Gangi et al., 2019), TED-LIUM v3 (Hernandez et al., 2018), and VoxPopuli (Wang et al., 2021). The data overview is in Table 2. Synthesized Speech Data To adapt the ASR model to the ACL talks, we add synthesized speech created by a text-to-speech (TTS) model. Specifi- cally, from the MT bitext English side (Table 3), we select sentences similar to the ACL domain based on similarity with the provided ACL dev bitext and abstracts. Inspired by data selection strategies for MT (Eck et al., 2005; Koneru et al., 2022), we use n-gram overlap as similarity metric. 4.7M sen- tences are selected and then synthesized to speech by a VITS (Kim et al., 2021) model trained on MuST-C. The synthesized data amount is shown in the last row of Table 2. 2.3 Machine Translation Data The MT training data include the following translation corpora: Europarl v7 text-to-text and v10 (Koehn, 2005), NewsCommentary v16, OpenSubtitles v2018 (Lison and Tiedemann, 2016), Tatoeba (Tiedemann, 2012), and ELRC- CORDIS_News, JParaCrawl (Morishita et al., 2022) for Japanese, and TED2020 (Reimers and Gurevych, 2020) for German2. We also include the text translation part of the following ST cor- 2This dataset has deplication with past evaluation sets: tst2019 tst2020 and tst-COMMON. The deplications were removed prior to training. pora: MuST-C (Di Gangi et al., 2019), CoVoST v2 (Wang et al., 2020), and Europarl-ST (Iranzo- S\u00e1nchez et al., 2020). The aggregated data amount per language is summarized in the \u201cOriginal\u201d col- umn of Table 3. Original After Diversification Lang. # sent. (M) # sent. (M) # tokens (M) ar zh nl fr de ja* fa pt ru tr 26.0 11.2 33.1 38.9 23.0 2.6 5.8 29.0 22.1 36.7 65.2 21.5 82.1 91.6 54.4 27.2 11.3 72.3 51.5 89.7 865.0 254.3 1162.7 1427.8 860.0 832.7 162.1 1024.3 685.3 1021.2 Total 228.4 566.8 8295.4 Table 3: MT data overview. *: For ja, the original data of 2.6M sentences did not include JParaCrawl, which was announced later as allowed data. As preprocessing, we perform truecasing, dedu- plication, length ratio filtering, and histogram filter- ing using the statistics by Fan et al. (2021). Then we perform subword segmentation using Sentence- piece (Kudo and Richardson, 2018) based on the vocabulary of mBART50 (Tang et al., 2020). Data Diversification Different from last years\u2019 shared tasks (Anastasopoulos et al., 2021, 2022), no monolingual (non-English) data is provided. This means conventional data augmentation tech- niques like backward translation are not directly applicable. On the other hand, forward translation from existing English monolingual data may intro- duce undesirable errors in the translation targets, especially on lower-resource languages. In this light, we use data diversification (Nguyen et al., 2020), a data augmentation method that enriches existing parallel data by forward and backward translating the training bitext. As the model has seen the parallel data in training, the synthetic trans- lations are expected to have relatively high quality. Moreover, either the source or target side of the synthetic data is from the original bitext. The di- versified data amount after deduplication is shown in Table 3. Here we perform one round of forward and backward translation, as Nguyen et al. (2020) have empirically shown further rounds do not lead to substantial gains. 2.4 Casing/Punctuation Restoration Data The ASR outputs are lower-cased and unpunctu- ated, while the MT model expects cased and punc- tuated inputs. We randomly sample 1.5 million En- glish sentences from the MT training data (Table 3), and remove the casing and punctuation marks as training source data. We then train a model to re- store the casing and punctuation marks. 2.5 Speech Translation Data The speech translation data are shown in Ta- ble 4. We additionally use our trained MT model to create forward translations based on the fol- lowing transcript-only datasets: Common Voice, TEDLIUM, and VoxPopuli. The TTS data de- scribed in \u00a72.2 is also used. Lang. Corpus / Data Source Hours # Utterances ar zh nl fr de ja fa pt ru CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C europarl-ST TTS CoVoST MuST-C europarl-ST TTS CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C TTS CoVoST MuST-C TTS 429 463 283 429 596 204 434 75 1138 485 76 1768 429 440 77 1891 429 541 73 429 347 89 377 75 1678 482 331 429 446 428 289k 212k 203k 289k 358k 183k 248k 32k 713k 275k 32k 998k 289k 269k 33k 779k 289k 329k 56k 289k 182k 88k 206k 32k 639k 265k 331k 289k 236k 511k tr all Common Voice TEDLIUM VoxPopuli 1488 453 502 948k 268k 177k Table 4: ST data overview. The last section \u201call\u201d indi- cates forward translated synthetic targets from transcript- only corpora, which are available for all 10 languages. 3 Cascaded System For the cascaded system, we introduce our ASR (\u00a73.1) and MT (\u00a73.2) models. 3.1 Automatic Speech Recognition Module Baseline Models The first baseline is our ASR model for last year\u2019s offline track (Pham et al., 2022). It is a Wav2vec 2.0 (Baevski et al., 2020) with LARGE configuration pretrained on 960 hours of Librispeech data. This year, after seeing ini- tial favourable results compared to Wav2vec, we opt for WavLM (Chen et al., 2022) as audio en- coder. We use the LARGE configuration with 24 layers. We use the mBART50 (Tang et al., 2020) decoder along with the WavLM encoder. As the ASR model only needs to transcribe English3, we trim the mBART50 vocabulary from 256k down to 62k tokens by removing all non-alphabetic tokens. In-Domain TTS Data We also use the synthe- sized TTS data. Compared to the same model without TTS data, the word error rate (WER) im- proves from 11.6% to"}, {"question": " Why is data diversification preferred over conventional data augmentation techniques?", "answer": " Data diversification ensures that the model has seen the parallel data during training, resulting in synthetic translations of relatively high quality.", "ref_chunk": "data. Corpus / Data Source Hours # Utterances Common Voice LibriSpeech MuST-C v2 TED-LIUM v3 VoxPopuli 1667 963 482 452 501 1225k 281k 251k 268k 177k TTS 7284 4.7M Table 2: ASR data overview. 2.2 Speech Recognition Data the ASR training, we use Common For Voice (Ardila et al., 2020), LibriSpeech (Panayotov et al., 2015), MuST-C v2 (Di Gangi et al., 2019), TED-LIUM v3 (Hernandez et al., 2018), and VoxPopuli (Wang et al., 2021). The data overview is in Table 2. Synthesized Speech Data To adapt the ASR model to the ACL talks, we add synthesized speech created by a text-to-speech (TTS) model. Specifi- cally, from the MT bitext English side (Table 3), we select sentences similar to the ACL domain based on similarity with the provided ACL dev bitext and abstracts. Inspired by data selection strategies for MT (Eck et al., 2005; Koneru et al., 2022), we use n-gram overlap as similarity metric. 4.7M sen- tences are selected and then synthesized to speech by a VITS (Kim et al., 2021) model trained on MuST-C. The synthesized data amount is shown in the last row of Table 2. 2.3 Machine Translation Data The MT training data include the following translation corpora: Europarl v7 text-to-text and v10 (Koehn, 2005), NewsCommentary v16, OpenSubtitles v2018 (Lison and Tiedemann, 2016), Tatoeba (Tiedemann, 2012), and ELRC- CORDIS_News, JParaCrawl (Morishita et al., 2022) for Japanese, and TED2020 (Reimers and Gurevych, 2020) for German2. We also include the text translation part of the following ST cor- 2This dataset has deplication with past evaluation sets: tst2019 tst2020 and tst-COMMON. The deplications were removed prior to training. pora: MuST-C (Di Gangi et al., 2019), CoVoST v2 (Wang et al., 2020), and Europarl-ST (Iranzo- S\u00e1nchez et al., 2020). The aggregated data amount per language is summarized in the \u201cOriginal\u201d col- umn of Table 3. Original After Diversification Lang. # sent. (M) # sent. (M) # tokens (M) ar zh nl fr de ja* fa pt ru tr 26.0 11.2 33.1 38.9 23.0 2.6 5.8 29.0 22.1 36.7 65.2 21.5 82.1 91.6 54.4 27.2 11.3 72.3 51.5 89.7 865.0 254.3 1162.7 1427.8 860.0 832.7 162.1 1024.3 685.3 1021.2 Total 228.4 566.8 8295.4 Table 3: MT data overview. *: For ja, the original data of 2.6M sentences did not include JParaCrawl, which was announced later as allowed data. As preprocessing, we perform truecasing, dedu- plication, length ratio filtering, and histogram filter- ing using the statistics by Fan et al. (2021). Then we perform subword segmentation using Sentence- piece (Kudo and Richardson, 2018) based on the vocabulary of mBART50 (Tang et al., 2020). Data Diversification Different from last years\u2019 shared tasks (Anastasopoulos et al., 2021, 2022), no monolingual (non-English) data is provided. This means conventional data augmentation tech- niques like backward translation are not directly applicable. On the other hand, forward translation from existing English monolingual data may intro- duce undesirable errors in the translation targets, especially on lower-resource languages. In this light, we use data diversification (Nguyen et al., 2020), a data augmentation method that enriches existing parallel data by forward and backward translating the training bitext. As the model has seen the parallel data in training, the synthetic trans- lations are expected to have relatively high quality. Moreover, either the source or target side of the synthetic data is from the original bitext. The di- versified data amount after deduplication is shown in Table 3. Here we perform one round of forward and backward translation, as Nguyen et al. (2020) have empirically shown further rounds do not lead to substantial gains. 2.4 Casing/Punctuation Restoration Data The ASR outputs are lower-cased and unpunctu- ated, while the MT model expects cased and punc- tuated inputs. We randomly sample 1.5 million En- glish sentences from the MT training data (Table 3), and remove the casing and punctuation marks as training source data. We then train a model to re- store the casing and punctuation marks. 2.5 Speech Translation Data The speech translation data are shown in Ta- ble 4. We additionally use our trained MT model to create forward translations based on the fol- lowing transcript-only datasets: Common Voice, TEDLIUM, and VoxPopuli. The TTS data de- scribed in \u00a72.2 is also used. Lang. Corpus / Data Source Hours # Utterances ar zh nl fr de ja fa pt ru CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C europarl-ST TTS CoVoST MuST-C europarl-ST TTS CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C TTS CoVoST MuST-C TTS 429 463 283 429 596 204 434 75 1138 485 76 1768 429 440 77 1891 429 541 73 429 347 89 377 75 1678 482 331 429 446 428 289k 212k 203k 289k 358k 183k 248k 32k 713k 275k 32k 998k 289k 269k 33k 779k 289k 329k 56k 289k 182k 88k 206k 32k 639k 265k 331k 289k 236k 511k tr all Common Voice TEDLIUM VoxPopuli 1488 453 502 948k 268k 177k Table 4: ST data overview. The last section \u201call\u201d indi- cates forward translated synthetic targets from transcript- only corpora, which are available for all 10 languages. 3 Cascaded System For the cascaded system, we introduce our ASR (\u00a73.1) and MT (\u00a73.2) models. 3.1 Automatic Speech Recognition Module Baseline Models The first baseline is our ASR model for last year\u2019s offline track (Pham et al., 2022). It is a Wav2vec 2.0 (Baevski et al., 2020) with LARGE configuration pretrained on 960 hours of Librispeech data. This year, after seeing ini- tial favourable results compared to Wav2vec, we opt for WavLM (Chen et al., 2022) as audio en- coder. We use the LARGE configuration with 24 layers. We use the mBART50 (Tang et al., 2020) decoder along with the WavLM encoder. As the ASR model only needs to transcribe English3, we trim the mBART50 vocabulary from 256k down to 62k tokens by removing all non-alphabetic tokens. In-Domain TTS Data We also use the synthe- sized TTS data. Compared to the same model without TTS data, the word error rate (WER) im- proves from 11.6% to"}, {"question": " How was the ASR model adapted for the MT model in terms of casing/punctuation restoration?", "answer": " 1.5 million English sentences were sampled from the MT training data, and casing and punctuation marks were removed to train a model to restore them.", "ref_chunk": "data. Corpus / Data Source Hours # Utterances Common Voice LibriSpeech MuST-C v2 TED-LIUM v3 VoxPopuli 1667 963 482 452 501 1225k 281k 251k 268k 177k TTS 7284 4.7M Table 2: ASR data overview. 2.2 Speech Recognition Data the ASR training, we use Common For Voice (Ardila et al., 2020), LibriSpeech (Panayotov et al., 2015), MuST-C v2 (Di Gangi et al., 2019), TED-LIUM v3 (Hernandez et al., 2018), and VoxPopuli (Wang et al., 2021). The data overview is in Table 2. Synthesized Speech Data To adapt the ASR model to the ACL talks, we add synthesized speech created by a text-to-speech (TTS) model. Specifi- cally, from the MT bitext English side (Table 3), we select sentences similar to the ACL domain based on similarity with the provided ACL dev bitext and abstracts. Inspired by data selection strategies for MT (Eck et al., 2005; Koneru et al., 2022), we use n-gram overlap as similarity metric. 4.7M sen- tences are selected and then synthesized to speech by a VITS (Kim et al., 2021) model trained on MuST-C. The synthesized data amount is shown in the last row of Table 2. 2.3 Machine Translation Data The MT training data include the following translation corpora: Europarl v7 text-to-text and v10 (Koehn, 2005), NewsCommentary v16, OpenSubtitles v2018 (Lison and Tiedemann, 2016), Tatoeba (Tiedemann, 2012), and ELRC- CORDIS_News, JParaCrawl (Morishita et al., 2022) for Japanese, and TED2020 (Reimers and Gurevych, 2020) for German2. We also include the text translation part of the following ST cor- 2This dataset has deplication with past evaluation sets: tst2019 tst2020 and tst-COMMON. The deplications were removed prior to training. pora: MuST-C (Di Gangi et al., 2019), CoVoST v2 (Wang et al., 2020), and Europarl-ST (Iranzo- S\u00e1nchez et al., 2020). The aggregated data amount per language is summarized in the \u201cOriginal\u201d col- umn of Table 3. Original After Diversification Lang. # sent. (M) # sent. (M) # tokens (M) ar zh nl fr de ja* fa pt ru tr 26.0 11.2 33.1 38.9 23.0 2.6 5.8 29.0 22.1 36.7 65.2 21.5 82.1 91.6 54.4 27.2 11.3 72.3 51.5 89.7 865.0 254.3 1162.7 1427.8 860.0 832.7 162.1 1024.3 685.3 1021.2 Total 228.4 566.8 8295.4 Table 3: MT data overview. *: For ja, the original data of 2.6M sentences did not include JParaCrawl, which was announced later as allowed data. As preprocessing, we perform truecasing, dedu- plication, length ratio filtering, and histogram filter- ing using the statistics by Fan et al. (2021). Then we perform subword segmentation using Sentence- piece (Kudo and Richardson, 2018) based on the vocabulary of mBART50 (Tang et al., 2020). Data Diversification Different from last years\u2019 shared tasks (Anastasopoulos et al., 2021, 2022), no monolingual (non-English) data is provided. This means conventional data augmentation tech- niques like backward translation are not directly applicable. On the other hand, forward translation from existing English monolingual data may intro- duce undesirable errors in the translation targets, especially on lower-resource languages. In this light, we use data diversification (Nguyen et al., 2020), a data augmentation method that enriches existing parallel data by forward and backward translating the training bitext. As the model has seen the parallel data in training, the synthetic trans- lations are expected to have relatively high quality. Moreover, either the source or target side of the synthetic data is from the original bitext. The di- versified data amount after deduplication is shown in Table 3. Here we perform one round of forward and backward translation, as Nguyen et al. (2020) have empirically shown further rounds do not lead to substantial gains. 2.4 Casing/Punctuation Restoration Data The ASR outputs are lower-cased and unpunctu- ated, while the MT model expects cased and punc- tuated inputs. We randomly sample 1.5 million En- glish sentences from the MT training data (Table 3), and remove the casing and punctuation marks as training source data. We then train a model to re- store the casing and punctuation marks. 2.5 Speech Translation Data The speech translation data are shown in Ta- ble 4. We additionally use our trained MT model to create forward translations based on the fol- lowing transcript-only datasets: Common Voice, TEDLIUM, and VoxPopuli. The TTS data de- scribed in \u00a72.2 is also used. Lang. Corpus / Data Source Hours # Utterances ar zh nl fr de ja fa pt ru CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C europarl-ST TTS CoVoST MuST-C europarl-ST TTS CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C TTS CoVoST MuST-C TTS 429 463 283 429 596 204 434 75 1138 485 76 1768 429 440 77 1891 429 541 73 429 347 89 377 75 1678 482 331 429 446 428 289k 212k 203k 289k 358k 183k 248k 32k 713k 275k 32k 998k 289k 269k 33k 779k 289k 329k 56k 289k 182k 88k 206k 32k 639k 265k 331k 289k 236k 511k tr all Common Voice TEDLIUM VoxPopuli 1488 453 502 948k 268k 177k Table 4: ST data overview. The last section \u201call\u201d indi- cates forward translated synthetic targets from transcript- only corpora, which are available for all 10 languages. 3 Cascaded System For the cascaded system, we introduce our ASR (\u00a73.1) and MT (\u00a73.2) models. 3.1 Automatic Speech Recognition Module Baseline Models The first baseline is our ASR model for last year\u2019s offline track (Pham et al., 2022). It is a Wav2vec 2.0 (Baevski et al., 2020) with LARGE configuration pretrained on 960 hours of Librispeech data. This year, after seeing ini- tial favourable results compared to Wav2vec, we opt for WavLM (Chen et al., 2022) as audio en- coder. We use the LARGE configuration with 24 layers. We use the mBART50 (Tang et al., 2020) decoder along with the WavLM encoder. As the ASR model only needs to transcribe English3, we trim the mBART50 vocabulary from 256k down to 62k tokens by removing all non-alphabetic tokens. In-Domain TTS Data We also use the synthe- sized TTS data. Compared to the same model without TTS data, the word error rate (WER) im- proves from 11.6% to"}, {"question": " What is the role of the trained MT model in speech translation data?", "answer": " It creates forward translations based on transcript-only datasets from Common Voice, TEDLIUM, and VoxPopuli, along with utilizing TTS data.", "ref_chunk": "data. Corpus / Data Source Hours # Utterances Common Voice LibriSpeech MuST-C v2 TED-LIUM v3 VoxPopuli 1667 963 482 452 501 1225k 281k 251k 268k 177k TTS 7284 4.7M Table 2: ASR data overview. 2.2 Speech Recognition Data the ASR training, we use Common For Voice (Ardila et al., 2020), LibriSpeech (Panayotov et al., 2015), MuST-C v2 (Di Gangi et al., 2019), TED-LIUM v3 (Hernandez et al., 2018), and VoxPopuli (Wang et al., 2021). The data overview is in Table 2. Synthesized Speech Data To adapt the ASR model to the ACL talks, we add synthesized speech created by a text-to-speech (TTS) model. Specifi- cally, from the MT bitext English side (Table 3), we select sentences similar to the ACL domain based on similarity with the provided ACL dev bitext and abstracts. Inspired by data selection strategies for MT (Eck et al., 2005; Koneru et al., 2022), we use n-gram overlap as similarity metric. 4.7M sen- tences are selected and then synthesized to speech by a VITS (Kim et al., 2021) model trained on MuST-C. The synthesized data amount is shown in the last row of Table 2. 2.3 Machine Translation Data The MT training data include the following translation corpora: Europarl v7 text-to-text and v10 (Koehn, 2005), NewsCommentary v16, OpenSubtitles v2018 (Lison and Tiedemann, 2016), Tatoeba (Tiedemann, 2012), and ELRC- CORDIS_News, JParaCrawl (Morishita et al., 2022) for Japanese, and TED2020 (Reimers and Gurevych, 2020) for German2. We also include the text translation part of the following ST cor- 2This dataset has deplication with past evaluation sets: tst2019 tst2020 and tst-COMMON. The deplications were removed prior to training. pora: MuST-C (Di Gangi et al., 2019), CoVoST v2 (Wang et al., 2020), and Europarl-ST (Iranzo- S\u00e1nchez et al., 2020). The aggregated data amount per language is summarized in the \u201cOriginal\u201d col- umn of Table 3. Original After Diversification Lang. # sent. (M) # sent. (M) # tokens (M) ar zh nl fr de ja* fa pt ru tr 26.0 11.2 33.1 38.9 23.0 2.6 5.8 29.0 22.1 36.7 65.2 21.5 82.1 91.6 54.4 27.2 11.3 72.3 51.5 89.7 865.0 254.3 1162.7 1427.8 860.0 832.7 162.1 1024.3 685.3 1021.2 Total 228.4 566.8 8295.4 Table 3: MT data overview. *: For ja, the original data of 2.6M sentences did not include JParaCrawl, which was announced later as allowed data. As preprocessing, we perform truecasing, dedu- plication, length ratio filtering, and histogram filter- ing using the statistics by Fan et al. (2021). Then we perform subword segmentation using Sentence- piece (Kudo and Richardson, 2018) based on the vocabulary of mBART50 (Tang et al., 2020). Data Diversification Different from last years\u2019 shared tasks (Anastasopoulos et al., 2021, 2022), no monolingual (non-English) data is provided. This means conventional data augmentation tech- niques like backward translation are not directly applicable. On the other hand, forward translation from existing English monolingual data may intro- duce undesirable errors in the translation targets, especially on lower-resource languages. In this light, we use data diversification (Nguyen et al., 2020), a data augmentation method that enriches existing parallel data by forward and backward translating the training bitext. As the model has seen the parallel data in training, the synthetic trans- lations are expected to have relatively high quality. Moreover, either the source or target side of the synthetic data is from the original bitext. The di- versified data amount after deduplication is shown in Table 3. Here we perform one round of forward and backward translation, as Nguyen et al. (2020) have empirically shown further rounds do not lead to substantial gains. 2.4 Casing/Punctuation Restoration Data The ASR outputs are lower-cased and unpunctu- ated, while the MT model expects cased and punc- tuated inputs. We randomly sample 1.5 million En- glish sentences from the MT training data (Table 3), and remove the casing and punctuation marks as training source data. We then train a model to re- store the casing and punctuation marks. 2.5 Speech Translation Data The speech translation data are shown in Ta- ble 4. We additionally use our trained MT model to create forward translations based on the fol- lowing transcript-only datasets: Common Voice, TEDLIUM, and VoxPopuli. The TTS data de- scribed in \u00a72.2 is also used. Lang. Corpus / Data Source Hours # Utterances ar zh nl fr de ja fa pt ru CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C europarl-ST TTS CoVoST MuST-C europarl-ST TTS CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C TTS CoVoST MuST-C TTS 429 463 283 429 596 204 434 75 1138 485 76 1768 429 440 77 1891 429 541 73 429 347 89 377 75 1678 482 331 429 446 428 289k 212k 203k 289k 358k 183k 248k 32k 713k 275k 32k 998k 289k 269k 33k 779k 289k 329k 56k 289k 182k 88k 206k 32k 639k 265k 331k 289k 236k 511k tr all Common Voice TEDLIUM VoxPopuli 1488 453 502 948k 268k 177k Table 4: ST data overview. The last section \u201call\u201d indi- cates forward translated synthetic targets from transcript- only corpora, which are available for all 10 languages. 3 Cascaded System For the cascaded system, we introduce our ASR (\u00a73.1) and MT (\u00a73.2) models. 3.1 Automatic Speech Recognition Module Baseline Models The first baseline is our ASR model for last year\u2019s offline track (Pham et al., 2022). It is a Wav2vec 2.0 (Baevski et al., 2020) with LARGE configuration pretrained on 960 hours of Librispeech data. This year, after seeing ini- tial favourable results compared to Wav2vec, we opt for WavLM (Chen et al., 2022) as audio en- coder. We use the LARGE configuration with 24 layers. We use the mBART50 (Tang et al., 2020) decoder along with the WavLM encoder. As the ASR model only needs to transcribe English3, we trim the mBART50 vocabulary from 256k down to 62k tokens by removing all non-alphabetic tokens. In-Domain TTS Data We also use the synthe- sized TTS data. Compared to the same model without TTS data, the word error rate (WER) im- proves from 11.6% to"}, {"question": " What models are introduced in the cascaded system?", "answer": " ASR model and MT model", "ref_chunk": "data. Corpus / Data Source Hours # Utterances Common Voice LibriSpeech MuST-C v2 TED-LIUM v3 VoxPopuli 1667 963 482 452 501 1225k 281k 251k 268k 177k TTS 7284 4.7M Table 2: ASR data overview. 2.2 Speech Recognition Data the ASR training, we use Common For Voice (Ardila et al., 2020), LibriSpeech (Panayotov et al., 2015), MuST-C v2 (Di Gangi et al., 2019), TED-LIUM v3 (Hernandez et al., 2018), and VoxPopuli (Wang et al., 2021). The data overview is in Table 2. Synthesized Speech Data To adapt the ASR model to the ACL talks, we add synthesized speech created by a text-to-speech (TTS) model. Specifi- cally, from the MT bitext English side (Table 3), we select sentences similar to the ACL domain based on similarity with the provided ACL dev bitext and abstracts. Inspired by data selection strategies for MT (Eck et al., 2005; Koneru et al., 2022), we use n-gram overlap as similarity metric. 4.7M sen- tences are selected and then synthesized to speech by a VITS (Kim et al., 2021) model trained on MuST-C. The synthesized data amount is shown in the last row of Table 2. 2.3 Machine Translation Data The MT training data include the following translation corpora: Europarl v7 text-to-text and v10 (Koehn, 2005), NewsCommentary v16, OpenSubtitles v2018 (Lison and Tiedemann, 2016), Tatoeba (Tiedemann, 2012), and ELRC- CORDIS_News, JParaCrawl (Morishita et al., 2022) for Japanese, and TED2020 (Reimers and Gurevych, 2020) for German2. We also include the text translation part of the following ST cor- 2This dataset has deplication with past evaluation sets: tst2019 tst2020 and tst-COMMON. The deplications were removed prior to training. pora: MuST-C (Di Gangi et al., 2019), CoVoST v2 (Wang et al., 2020), and Europarl-ST (Iranzo- S\u00e1nchez et al., 2020). The aggregated data amount per language is summarized in the \u201cOriginal\u201d col- umn of Table 3. Original After Diversification Lang. # sent. (M) # sent. (M) # tokens (M) ar zh nl fr de ja* fa pt ru tr 26.0 11.2 33.1 38.9 23.0 2.6 5.8 29.0 22.1 36.7 65.2 21.5 82.1 91.6 54.4 27.2 11.3 72.3 51.5 89.7 865.0 254.3 1162.7 1427.8 860.0 832.7 162.1 1024.3 685.3 1021.2 Total 228.4 566.8 8295.4 Table 3: MT data overview. *: For ja, the original data of 2.6M sentences did not include JParaCrawl, which was announced later as allowed data. As preprocessing, we perform truecasing, dedu- plication, length ratio filtering, and histogram filter- ing using the statistics by Fan et al. (2021). Then we perform subword segmentation using Sentence- piece (Kudo and Richardson, 2018) based on the vocabulary of mBART50 (Tang et al., 2020). Data Diversification Different from last years\u2019 shared tasks (Anastasopoulos et al., 2021, 2022), no monolingual (non-English) data is provided. This means conventional data augmentation tech- niques like backward translation are not directly applicable. On the other hand, forward translation from existing English monolingual data may intro- duce undesirable errors in the translation targets, especially on lower-resource languages. In this light, we use data diversification (Nguyen et al., 2020), a data augmentation method that enriches existing parallel data by forward and backward translating the training bitext. As the model has seen the parallel data in training, the synthetic trans- lations are expected to have relatively high quality. Moreover, either the source or target side of the synthetic data is from the original bitext. The di- versified data amount after deduplication is shown in Table 3. Here we perform one round of forward and backward translation, as Nguyen et al. (2020) have empirically shown further rounds do not lead to substantial gains. 2.4 Casing/Punctuation Restoration Data The ASR outputs are lower-cased and unpunctu- ated, while the MT model expects cased and punc- tuated inputs. We randomly sample 1.5 million En- glish sentences from the MT training data (Table 3), and remove the casing and punctuation marks as training source data. We then train a model to re- store the casing and punctuation marks. 2.5 Speech Translation Data The speech translation data are shown in Ta- ble 4. We additionally use our trained MT model to create forward translations based on the fol- lowing transcript-only datasets: Common Voice, TEDLIUM, and VoxPopuli. The TTS data de- scribed in \u00a72.2 is also used. Lang. Corpus / Data Source Hours # Utterances ar zh nl fr de ja fa pt ru CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C europarl-ST TTS CoVoST MuST-C europarl-ST TTS CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C TTS CoVoST MuST-C TTS 429 463 283 429 596 204 434 75 1138 485 76 1768 429 440 77 1891 429 541 73 429 347 89 377 75 1678 482 331 429 446 428 289k 212k 203k 289k 358k 183k 248k 32k 713k 275k 32k 998k 289k 269k 33k 779k 289k 329k 56k 289k 182k 88k 206k 32k 639k 265k 331k 289k 236k 511k tr all Common Voice TEDLIUM VoxPopuli 1488 453 502 948k 268k 177k Table 4: ST data overview. The last section \u201call\u201d indi- cates forward translated synthetic targets from transcript- only corpora, which are available for all 10 languages. 3 Cascaded System For the cascaded system, we introduce our ASR (\u00a73.1) and MT (\u00a73.2) models. 3.1 Automatic Speech Recognition Module Baseline Models The first baseline is our ASR model for last year\u2019s offline track (Pham et al., 2022). It is a Wav2vec 2.0 (Baevski et al., 2020) with LARGE configuration pretrained on 960 hours of Librispeech data. This year, after seeing ini- tial favourable results compared to Wav2vec, we opt for WavLM (Chen et al., 2022) as audio en- coder. We use the LARGE configuration with 24 layers. We use the mBART50 (Tang et al., 2020) decoder along with the WavLM encoder. As the ASR model only needs to transcribe English3, we trim the mBART50 vocabulary from 256k down to 62k tokens by removing all non-alphabetic tokens. In-Domain TTS Data We also use the synthe- sized TTS data. Compared to the same model without TTS data, the word error rate (WER) im- proves from 11.6% to"}, {"question": " What baseline model is used for the ASR model in the cascaded system?", "answer": " WavLM with a LARGE configuration and 24 layers, with a mBART50 decoder and WavLM encoder.", "ref_chunk": "data. Corpus / Data Source Hours # Utterances Common Voice LibriSpeech MuST-C v2 TED-LIUM v3 VoxPopuli 1667 963 482 452 501 1225k 281k 251k 268k 177k TTS 7284 4.7M Table 2: ASR data overview. 2.2 Speech Recognition Data the ASR training, we use Common For Voice (Ardila et al., 2020), LibriSpeech (Panayotov et al., 2015), MuST-C v2 (Di Gangi et al., 2019), TED-LIUM v3 (Hernandez et al., 2018), and VoxPopuli (Wang et al., 2021). The data overview is in Table 2. Synthesized Speech Data To adapt the ASR model to the ACL talks, we add synthesized speech created by a text-to-speech (TTS) model. Specifi- cally, from the MT bitext English side (Table 3), we select sentences similar to the ACL domain based on similarity with the provided ACL dev bitext and abstracts. Inspired by data selection strategies for MT (Eck et al., 2005; Koneru et al., 2022), we use n-gram overlap as similarity metric. 4.7M sen- tences are selected and then synthesized to speech by a VITS (Kim et al., 2021) model trained on MuST-C. The synthesized data amount is shown in the last row of Table 2. 2.3 Machine Translation Data The MT training data include the following translation corpora: Europarl v7 text-to-text and v10 (Koehn, 2005), NewsCommentary v16, OpenSubtitles v2018 (Lison and Tiedemann, 2016), Tatoeba (Tiedemann, 2012), and ELRC- CORDIS_News, JParaCrawl (Morishita et al., 2022) for Japanese, and TED2020 (Reimers and Gurevych, 2020) for German2. We also include the text translation part of the following ST cor- 2This dataset has deplication with past evaluation sets: tst2019 tst2020 and tst-COMMON. The deplications were removed prior to training. pora: MuST-C (Di Gangi et al., 2019), CoVoST v2 (Wang et al., 2020), and Europarl-ST (Iranzo- S\u00e1nchez et al., 2020). The aggregated data amount per language is summarized in the \u201cOriginal\u201d col- umn of Table 3. Original After Diversification Lang. # sent. (M) # sent. (M) # tokens (M) ar zh nl fr de ja* fa pt ru tr 26.0 11.2 33.1 38.9 23.0 2.6 5.8 29.0 22.1 36.7 65.2 21.5 82.1 91.6 54.4 27.2 11.3 72.3 51.5 89.7 865.0 254.3 1162.7 1427.8 860.0 832.7 162.1 1024.3 685.3 1021.2 Total 228.4 566.8 8295.4 Table 3: MT data overview. *: For ja, the original data of 2.6M sentences did not include JParaCrawl, which was announced later as allowed data. As preprocessing, we perform truecasing, dedu- plication, length ratio filtering, and histogram filter- ing using the statistics by Fan et al. (2021). Then we perform subword segmentation using Sentence- piece (Kudo and Richardson, 2018) based on the vocabulary of mBART50 (Tang et al., 2020). Data Diversification Different from last years\u2019 shared tasks (Anastasopoulos et al., 2021, 2022), no monolingual (non-English) data is provided. This means conventional data augmentation tech- niques like backward translation are not directly applicable. On the other hand, forward translation from existing English monolingual data may intro- duce undesirable errors in the translation targets, especially on lower-resource languages. In this light, we use data diversification (Nguyen et al., 2020), a data augmentation method that enriches existing parallel data by forward and backward translating the training bitext. As the model has seen the parallel data in training, the synthetic trans- lations are expected to have relatively high quality. Moreover, either the source or target side of the synthetic data is from the original bitext. The di- versified data amount after deduplication is shown in Table 3. Here we perform one round of forward and backward translation, as Nguyen et al. (2020) have empirically shown further rounds do not lead to substantial gains. 2.4 Casing/Punctuation Restoration Data The ASR outputs are lower-cased and unpunctu- ated, while the MT model expects cased and punc- tuated inputs. We randomly sample 1.5 million En- glish sentences from the MT training data (Table 3), and remove the casing and punctuation marks as training source data. We then train a model to re- store the casing and punctuation marks. 2.5 Speech Translation Data The speech translation data are shown in Ta- ble 4. We additionally use our trained MT model to create forward translations based on the fol- lowing transcript-only datasets: Common Voice, TEDLIUM, and VoxPopuli. The TTS data de- scribed in \u00a72.2 is also used. Lang. Corpus / Data Source Hours # Utterances ar zh nl fr de ja fa pt ru CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C europarl-ST TTS CoVoST MuST-C europarl-ST TTS CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C TTS CoVoST MuST-C TTS 429 463 283 429 596 204 434 75 1138 485 76 1768 429 440 77 1891 429 541 73 429 347 89 377 75 1678 482 331 429 446 428 289k 212k 203k 289k 358k 183k 248k 32k 713k 275k 32k 998k 289k 269k 33k 779k 289k 329k 56k 289k 182k 88k 206k 32k 639k 265k 331k 289k 236k 511k tr all Common Voice TEDLIUM VoxPopuli 1488 453 502 948k 268k 177k Table 4: ST data overview. The last section \u201call\u201d indi- cates forward translated synthetic targets from transcript- only corpora, which are available for all 10 languages. 3 Cascaded System For the cascaded system, we introduce our ASR (\u00a73.1) and MT (\u00a73.2) models. 3.1 Automatic Speech Recognition Module Baseline Models The first baseline is our ASR model for last year\u2019s offline track (Pham et al., 2022). It is a Wav2vec 2.0 (Baevski et al., 2020) with LARGE configuration pretrained on 960 hours of Librispeech data. This year, after seeing ini- tial favourable results compared to Wav2vec, we opt for WavLM (Chen et al., 2022) as audio en- coder. We use the LARGE configuration with 24 layers. We use the mBART50 (Tang et al., 2020) decoder along with the WavLM encoder. As the ASR model only needs to transcribe English3, we trim the mBART50 vocabulary from 256k down to 62k tokens by removing all non-alphabetic tokens. In-Domain TTS Data We also use the synthe- sized TTS data. Compared to the same model without TTS data, the word error rate (WER) im- proves from 11.6% to"}, {"question": " What improvement was seen when using In-Domain TTS Data in the ASR model?", "answer": " The word error rate (WER) improved from 11.6% to ", "ref_chunk": "data. Corpus / Data Source Hours # Utterances Common Voice LibriSpeech MuST-C v2 TED-LIUM v3 VoxPopuli 1667 963 482 452 501 1225k 281k 251k 268k 177k TTS 7284 4.7M Table 2: ASR data overview. 2.2 Speech Recognition Data the ASR training, we use Common For Voice (Ardila et al., 2020), LibriSpeech (Panayotov et al., 2015), MuST-C v2 (Di Gangi et al., 2019), TED-LIUM v3 (Hernandez et al., 2018), and VoxPopuli (Wang et al., 2021). The data overview is in Table 2. Synthesized Speech Data To adapt the ASR model to the ACL talks, we add synthesized speech created by a text-to-speech (TTS) model. Specifi- cally, from the MT bitext English side (Table 3), we select sentences similar to the ACL domain based on similarity with the provided ACL dev bitext and abstracts. Inspired by data selection strategies for MT (Eck et al., 2005; Koneru et al., 2022), we use n-gram overlap as similarity metric. 4.7M sen- tences are selected and then synthesized to speech by a VITS (Kim et al., 2021) model trained on MuST-C. The synthesized data amount is shown in the last row of Table 2. 2.3 Machine Translation Data The MT training data include the following translation corpora: Europarl v7 text-to-text and v10 (Koehn, 2005), NewsCommentary v16, OpenSubtitles v2018 (Lison and Tiedemann, 2016), Tatoeba (Tiedemann, 2012), and ELRC- CORDIS_News, JParaCrawl (Morishita et al., 2022) for Japanese, and TED2020 (Reimers and Gurevych, 2020) for German2. We also include the text translation part of the following ST cor- 2This dataset has deplication with past evaluation sets: tst2019 tst2020 and tst-COMMON. The deplications were removed prior to training. pora: MuST-C (Di Gangi et al., 2019), CoVoST v2 (Wang et al., 2020), and Europarl-ST (Iranzo- S\u00e1nchez et al., 2020). The aggregated data amount per language is summarized in the \u201cOriginal\u201d col- umn of Table 3. Original After Diversification Lang. # sent. (M) # sent. (M) # tokens (M) ar zh nl fr de ja* fa pt ru tr 26.0 11.2 33.1 38.9 23.0 2.6 5.8 29.0 22.1 36.7 65.2 21.5 82.1 91.6 54.4 27.2 11.3 72.3 51.5 89.7 865.0 254.3 1162.7 1427.8 860.0 832.7 162.1 1024.3 685.3 1021.2 Total 228.4 566.8 8295.4 Table 3: MT data overview. *: For ja, the original data of 2.6M sentences did not include JParaCrawl, which was announced later as allowed data. As preprocessing, we perform truecasing, dedu- plication, length ratio filtering, and histogram filter- ing using the statistics by Fan et al. (2021). Then we perform subword segmentation using Sentence- piece (Kudo and Richardson, 2018) based on the vocabulary of mBART50 (Tang et al., 2020). Data Diversification Different from last years\u2019 shared tasks (Anastasopoulos et al., 2021, 2022), no monolingual (non-English) data is provided. This means conventional data augmentation tech- niques like backward translation are not directly applicable. On the other hand, forward translation from existing English monolingual data may intro- duce undesirable errors in the translation targets, especially on lower-resource languages. In this light, we use data diversification (Nguyen et al., 2020), a data augmentation method that enriches existing parallel data by forward and backward translating the training bitext. As the model has seen the parallel data in training, the synthetic trans- lations are expected to have relatively high quality. Moreover, either the source or target side of the synthetic data is from the original bitext. The di- versified data amount after deduplication is shown in Table 3. Here we perform one round of forward and backward translation, as Nguyen et al. (2020) have empirically shown further rounds do not lead to substantial gains. 2.4 Casing/Punctuation Restoration Data The ASR outputs are lower-cased and unpunctu- ated, while the MT model expects cased and punc- tuated inputs. We randomly sample 1.5 million En- glish sentences from the MT training data (Table 3), and remove the casing and punctuation marks as training source data. We then train a model to re- store the casing and punctuation marks. 2.5 Speech Translation Data The speech translation data are shown in Ta- ble 4. We additionally use our trained MT model to create forward translations based on the fol- lowing transcript-only datasets: Common Voice, TEDLIUM, and VoxPopuli. The TTS data de- scribed in \u00a72.2 is also used. Lang. Corpus / Data Source Hours # Utterances ar zh nl fr de ja fa pt ru CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C europarl-ST TTS CoVoST MuST-C europarl-ST TTS CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C TTS CoVoST MuST-C TTS 429 463 283 429 596 204 434 75 1138 485 76 1768 429 440 77 1891 429 541 73 429 347 89 377 75 1678 482 331 429 446 428 289k 212k 203k 289k 358k 183k 248k 32k 713k 275k 32k 998k 289k 269k 33k 779k 289k 329k 56k 289k 182k 88k 206k 32k 639k 265k 331k 289k 236k 511k tr all Common Voice TEDLIUM VoxPopuli 1488 453 502 948k 268k 177k Table 4: ST data overview. The last section \u201call\u201d indi- cates forward translated synthetic targets from transcript- only corpora, which are available for all 10 languages. 3 Cascaded System For the cascaded system, we introduce our ASR (\u00a73.1) and MT (\u00a73.2) models. 3.1 Automatic Speech Recognition Module Baseline Models The first baseline is our ASR model for last year\u2019s offline track (Pham et al., 2022). It is a Wav2vec 2.0 (Baevski et al., 2020) with LARGE configuration pretrained on 960 hours of Librispeech data. This year, after seeing ini- tial favourable results compared to Wav2vec, we opt for WavLM (Chen et al., 2022) as audio en- coder. We use the LARGE configuration with 24 layers. We use the mBART50 (Tang et al., 2020) decoder along with the WavLM encoder. As the ASR model only needs to transcribe English3, we trim the mBART50 vocabulary from 256k down to 62k tokens by removing all non-alphabetic tokens. In-Domain TTS Data We also use the synthe- sized TTS data. Compared to the same model without TTS data, the word error rate (WER) im- proves from 11.6% to"}], "doc_text": "data. Corpus / Data Source Hours # Utterances Common Voice LibriSpeech MuST-C v2 TED-LIUM v3 VoxPopuli 1667 963 482 452 501 1225k 281k 251k 268k 177k TTS 7284 4.7M Table 2: ASR data overview. 2.2 Speech Recognition Data the ASR training, we use Common For Voice (Ardila et al., 2020), LibriSpeech (Panayotov et al., 2015), MuST-C v2 (Di Gangi et al., 2019), TED-LIUM v3 (Hernandez et al., 2018), and VoxPopuli (Wang et al., 2021). The data overview is in Table 2. Synthesized Speech Data To adapt the ASR model to the ACL talks, we add synthesized speech created by a text-to-speech (TTS) model. Specifi- cally, from the MT bitext English side (Table 3), we select sentences similar to the ACL domain based on similarity with the provided ACL dev bitext and abstracts. Inspired by data selection strategies for MT (Eck et al., 2005; Koneru et al., 2022), we use n-gram overlap as similarity metric. 4.7M sen- tences are selected and then synthesized to speech by a VITS (Kim et al., 2021) model trained on MuST-C. The synthesized data amount is shown in the last row of Table 2. 2.3 Machine Translation Data The MT training data include the following translation corpora: Europarl v7 text-to-text and v10 (Koehn, 2005), NewsCommentary v16, OpenSubtitles v2018 (Lison and Tiedemann, 2016), Tatoeba (Tiedemann, 2012), and ELRC- CORDIS_News, JParaCrawl (Morishita et al., 2022) for Japanese, and TED2020 (Reimers and Gurevych, 2020) for German2. We also include the text translation part of the following ST cor- 2This dataset has deplication with past evaluation sets: tst2019 tst2020 and tst-COMMON. The deplications were removed prior to training. pora: MuST-C (Di Gangi et al., 2019), CoVoST v2 (Wang et al., 2020), and Europarl-ST (Iranzo- S\u00e1nchez et al., 2020). The aggregated data amount per language is summarized in the \u201cOriginal\u201d col- umn of Table 3. Original After Diversification Lang. # sent. (M) # sent. (M) # tokens (M) ar zh nl fr de ja* fa pt ru tr 26.0 11.2 33.1 38.9 23.0 2.6 5.8 29.0 22.1 36.7 65.2 21.5 82.1 91.6 54.4 27.2 11.3 72.3 51.5 89.7 865.0 254.3 1162.7 1427.8 860.0 832.7 162.1 1024.3 685.3 1021.2 Total 228.4 566.8 8295.4 Table 3: MT data overview. *: For ja, the original data of 2.6M sentences did not include JParaCrawl, which was announced later as allowed data. As preprocessing, we perform truecasing, dedu- plication, length ratio filtering, and histogram filter- ing using the statistics by Fan et al. (2021). Then we perform subword segmentation using Sentence- piece (Kudo and Richardson, 2018) based on the vocabulary of mBART50 (Tang et al., 2020). Data Diversification Different from last years\u2019 shared tasks (Anastasopoulos et al., 2021, 2022), no monolingual (non-English) data is provided. This means conventional data augmentation tech- niques like backward translation are not directly applicable. On the other hand, forward translation from existing English monolingual data may intro- duce undesirable errors in the translation targets, especially on lower-resource languages. In this light, we use data diversification (Nguyen et al., 2020), a data augmentation method that enriches existing parallel data by forward and backward translating the training bitext. As the model has seen the parallel data in training, the synthetic trans- lations are expected to have relatively high quality. Moreover, either the source or target side of the synthetic data is from the original bitext. The di- versified data amount after deduplication is shown in Table 3. Here we perform one round of forward and backward translation, as Nguyen et al. (2020) have empirically shown further rounds do not lead to substantial gains. 2.4 Casing/Punctuation Restoration Data The ASR outputs are lower-cased and unpunctu- ated, while the MT model expects cased and punc- tuated inputs. We randomly sample 1.5 million En- glish sentences from the MT training data (Table 3), and remove the casing and punctuation marks as training source data. We then train a model to re- store the casing and punctuation marks. 2.5 Speech Translation Data The speech translation data are shown in Ta- ble 4. We additionally use our trained MT model to create forward translations based on the fol- lowing transcript-only datasets: Common Voice, TEDLIUM, and VoxPopuli. The TTS data de- scribed in \u00a72.2 is also used. Lang. Corpus / Data Source Hours # Utterances ar zh nl fr de ja fa pt ru CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C europarl-ST TTS CoVoST MuST-C europarl-ST TTS CoVoST MuST-C TTS CoVoST MuST-C TTS MuST-C europarl-ST TTS MuST-C TTS CoVoST MuST-C TTS 429 463 283 429 596 204 434 75 1138 485 76 1768 429 440 77 1891 429 541 73 429 347 89 377 75 1678 482 331 429 446 428 289k 212k 203k 289k 358k 183k 248k 32k 713k 275k 32k 998k 289k 269k 33k 779k 289k 329k 56k 289k 182k 88k 206k 32k 639k 265k 331k 289k 236k 511k tr all Common Voice TEDLIUM VoxPopuli 1488 453 502 948k 268k 177k Table 4: ST data overview. The last section \u201call\u201d indi- cates forward translated synthetic targets from transcript- only corpora, which are available for all 10 languages. 3 Cascaded System For the cascaded system, we introduce our ASR (\u00a73.1) and MT (\u00a73.2) models. 3.1 Automatic Speech Recognition Module Baseline Models The first baseline is our ASR model for last year\u2019s offline track (Pham et al., 2022). It is a Wav2vec 2.0 (Baevski et al., 2020) with LARGE configuration pretrained on 960 hours of Librispeech data. This year, after seeing ini- tial favourable results compared to Wav2vec, we opt for WavLM (Chen et al., 2022) as audio en- coder. We use the LARGE configuration with 24 layers. We use the mBART50 (Tang et al., 2020) decoder along with the WavLM encoder. As the ASR model only needs to transcribe English3, we trim the mBART50 vocabulary from 256k down to 62k tokens by removing all non-alphabetic tokens. In-Domain TTS Data We also use the synthe- sized TTS data. Compared to the same model without TTS data, the word error rate (WER) im- proves from 11.6% to"}