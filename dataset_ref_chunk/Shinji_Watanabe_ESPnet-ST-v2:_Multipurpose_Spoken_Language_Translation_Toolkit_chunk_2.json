{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_ESPnet-ST-v2:_Multipurpose_Spoken_Language_Translation_Toolkit_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are some of the key features of ESPnet-ST-v2 compared to other toolkits?", "answer": " Key features of ESPnet-ST-v2 include support for Spectral features, speech SSL representations, and multiple interchangeable encoder and decoder architectures.", "ref_chunk": "Architectures Speech SSL Representations Speech & Text Pre-training Joint Speech/Text Pre-training Simultaneous ST End-to-End Architecture(s) Contextual Block Encoders Blockwise Attn Enc-Dec Blockwise CTC/Attention Blockwise Transducer Wait-K Attn Enc-Dec Monotonic Attn Enc-Dec \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u27131 \u2713 - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - - - - - - - - - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u27133 \u2713 - - - - - - - - - \u2713 - \u2713 - \u2713 \u27133 - - - - - - - - Cascaded Architectures \u2713 - \u2713 \u2713 - \u2713 \u2713 - \u2713 Spec Enc-Dec (Translatotron) \u2713 - \u2713 Spec Multi-Dec (Translatotron 2) \u2713 - Discrete Enc-Dec (Speech-to-Unit) \u2713 \u2713 - \u2713 Discrete Multi-Decoder (UnitY) \u27131 \u2713 - \u27132 \u2713 \u2713 - Offline S2ST End-to-End Architecture(s) - - - - - - Speech SSL Representations Neural Vocoder Support Table 1: Key features of ESPnet-ST-v2 compared to ESPnet-ST-v1 (Inaguma et al., 2020), Fairseq (Wang et al., 2020), and NeurST (Zhao et al., 2021). Com- parison intends to highlight unique features of ESPnet- ST-v2 and not to comprehensively review all toolkits. 1Supports S3PRL (Yang et al., 2021a). 2Supports both spectral and discrete. 3Only supports text-to-text. subsection, are used as building blocks in wrapper classes which are used to construct model architec- tures. Then the fully constructed models are fed to task wrappers which prepare data loaders, initialize models, and handle training/validation. For infer- ence, pythonic APIs invoke search algorithms over the trained models and direct outputs to scoring scripts. For instance, the third-party SimulEval tool for evaluating SST latency (Ma et al., 2020a) is in- tegrated via this API layer. We are also integrating with TorchAudio (Yang et al., 2021b) in the same manner. Finally, recipe scripts define experimental pipelines from data preparation to evaluation. 3.2 Key Features Each of the following modeling components fea- ture a variety of interchangeable approaches. Figure 1: Software architecture of ESPnet-ST-v2. Frontends & Targets Spectral features (e.g. FBANK) and features extracted from speech self- supervised learning (SSL) representations are sup- ported, as well as fusions over multiple features (Berrebbi et al., 2022). For speech SSL features, ESPnet-ST-v2 integrates with the S3PRL toolkit (Yang et al., 2021a). These speech SSL representa- tions are also used to generate discrete targets for S2ST (Lee et al., 2022a). Encoder Architectures Conformer (Gulati et al., 2020; Guo et al., 2021), Branchformer (Peng et al., 2022), EBranchformer (Kim et al., 2023), and Transformer (Vaswani et al., 2017; Karita et al., 2019) encoder architectures are supported for ST and S2ST. For SST, a blockwise scheme is adopted following (Tsunoo et al., 2021; Deng et al., 2022) to form contextual block Conformer and Transformer encoders. Intermediate CTC (Lee and Watanabe, 2021) and Hierachical CTC (Sanabria and Metze, 2018) encoding are also supported; these tech- niques have been shown to stabilize deep encoder optimization (Lee and Watanabe, 2021) and im- prove representations for sequence tasks involving source-to-target re-ordering (Yan et al., 2023). Decoder Architectures Attentional Transformer and recurrent neural network decoders are sup- ported (Karita et al., 2019). Multi-decoder schemes which allow for E2E differentiable decoder cas- cades via searchable hidden intermediates (Dalmia et al., 2021), are also supported; this technique has been shown to improve sequence modeling for tasks which naturally decompose into sub-tasks. Fi- nally, large language model decoders (e.g. mBART (Liu et al., 2020b)) can be adopted through an inte- gration with HuggingFace (Wolf et al., 2020). Loss Functions Cross-entropy (for attentional decoders), CTC, and Transducer are supported for ST and SST. Multi-objective training with CTC/attention and CTC/transducer as well as multi- tasked training (e.g. ASR/MT/ST) is also sup- ported. For S2ST, L1 and mean square error losses are also supported for spectral models. Search Algorithms For offline attentional de- coder models, label-synchronous beam search is supported with optional CTC joint decoding for multi-objective models (Watanabe et al., 2017). For offline Transducer models, the original Graves beam search (Graves, 2012) as well as time- synchronous and alignment-synchronous beam search (Saon et al., 2020) beam searches are supported. For SST, both incremental decoding and non-incremental (allowing re-translation) de- coding (Liu et al., 2020a) are supported, along with stable hypothesis detection methods (Pol\u00e1k et al., 2022). Blockwise attentional decoder mod- els use a label-synchronous beam search or time- synchronous beam search if a CTC branch is available. Blockwise transducer models use time- synchronous beam search. Synthesis & Post-processing For ST, Minimum Bayes Risk (MBR) ensembling (Fernandes et al., 2022) is supported for leveraging quality-metrics (e.g. BLEU) to compare and rank n-best out- puts from one or more models. For S2ST, neu- ral vocoders are supported for both spectral and discrete inputs (Hayashi et al., 2020, 2021). 4 Example Models In this section, we introduce example models which are pre-built in ESPnet-ST-v2 using the neural net- work components described in the previous sec- tion. These examples include state-of-the-art core architectures, as evidenced by prior studies and our performance benchmarking (presented in \u00a75). 4.1 ST Models CTC/Attention (CA) Following Yan et al. (2023), we use Conformer encoders with hierarchi- cal CTC encoding and Transformer decoders. The hierachical CTC encoding, which aligns the first Figure 2: Multi-Decoder CTC/Attention for ST. Figure 3: Time-Sync Blockwise CTC/Attn for SST. N layers of the encoder towards ASR targets and the last M layers towards ST targets, regularizes the final encoder representations to be monotonic with respect to the target. CTC/attention models are jointly decoded using either label-synchronous (wherein the attention branch is primary) or time- synchronous (wherein the CTC branch is primary) beam search. For offline tasks, label-synchrony has shown greater performance (Yan et al., 2023). Multi-Decoder CTC/Attention (MCA) As shown in Figure 2, the Multi-decoder decomposes ST into two sub-tasks, logically corresponding to ASR and MT encoder-decoder models, while main- taining E2E differentiability (Dalmia et al., 2021). This Multi-decoder scheme is also combined with the CTC/attention scheme described in the blurb"}, {"question": " How are fully constructed models in ESPnet-ST-v2 fed to task wrappers?", "answer": " Fully constructed models in ESPnet-ST-v2 are fed to task wrappers which prepare data loaders, initialize models, and handle training/validation.", "ref_chunk": "Architectures Speech SSL Representations Speech & Text Pre-training Joint Speech/Text Pre-training Simultaneous ST End-to-End Architecture(s) Contextual Block Encoders Blockwise Attn Enc-Dec Blockwise CTC/Attention Blockwise Transducer Wait-K Attn Enc-Dec Monotonic Attn Enc-Dec \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u27131 \u2713 - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - - - - - - - - - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u27133 \u2713 - - - - - - - - - \u2713 - \u2713 - \u2713 \u27133 - - - - - - - - Cascaded Architectures \u2713 - \u2713 \u2713 - \u2713 \u2713 - \u2713 Spec Enc-Dec (Translatotron) \u2713 - \u2713 Spec Multi-Dec (Translatotron 2) \u2713 - Discrete Enc-Dec (Speech-to-Unit) \u2713 \u2713 - \u2713 Discrete Multi-Decoder (UnitY) \u27131 \u2713 - \u27132 \u2713 \u2713 - Offline S2ST End-to-End Architecture(s) - - - - - - Speech SSL Representations Neural Vocoder Support Table 1: Key features of ESPnet-ST-v2 compared to ESPnet-ST-v1 (Inaguma et al., 2020), Fairseq (Wang et al., 2020), and NeurST (Zhao et al., 2021). Com- parison intends to highlight unique features of ESPnet- ST-v2 and not to comprehensively review all toolkits. 1Supports S3PRL (Yang et al., 2021a). 2Supports both spectral and discrete. 3Only supports text-to-text. subsection, are used as building blocks in wrapper classes which are used to construct model architec- tures. Then the fully constructed models are fed to task wrappers which prepare data loaders, initialize models, and handle training/validation. For infer- ence, pythonic APIs invoke search algorithms over the trained models and direct outputs to scoring scripts. For instance, the third-party SimulEval tool for evaluating SST latency (Ma et al., 2020a) is in- tegrated via this API layer. We are also integrating with TorchAudio (Yang et al., 2021b) in the same manner. Finally, recipe scripts define experimental pipelines from data preparation to evaluation. 3.2 Key Features Each of the following modeling components fea- ture a variety of interchangeable approaches. Figure 1: Software architecture of ESPnet-ST-v2. Frontends & Targets Spectral features (e.g. FBANK) and features extracted from speech self- supervised learning (SSL) representations are sup- ported, as well as fusions over multiple features (Berrebbi et al., 2022). For speech SSL features, ESPnet-ST-v2 integrates with the S3PRL toolkit (Yang et al., 2021a). These speech SSL representa- tions are also used to generate discrete targets for S2ST (Lee et al., 2022a). Encoder Architectures Conformer (Gulati et al., 2020; Guo et al., 2021), Branchformer (Peng et al., 2022), EBranchformer (Kim et al., 2023), and Transformer (Vaswani et al., 2017; Karita et al., 2019) encoder architectures are supported for ST and S2ST. For SST, a blockwise scheme is adopted following (Tsunoo et al., 2021; Deng et al., 2022) to form contextual block Conformer and Transformer encoders. Intermediate CTC (Lee and Watanabe, 2021) and Hierachical CTC (Sanabria and Metze, 2018) encoding are also supported; these tech- niques have been shown to stabilize deep encoder optimization (Lee and Watanabe, 2021) and im- prove representations for sequence tasks involving source-to-target re-ordering (Yan et al., 2023). Decoder Architectures Attentional Transformer and recurrent neural network decoders are sup- ported (Karita et al., 2019). Multi-decoder schemes which allow for E2E differentiable decoder cas- cades via searchable hidden intermediates (Dalmia et al., 2021), are also supported; this technique has been shown to improve sequence modeling for tasks which naturally decompose into sub-tasks. Fi- nally, large language model decoders (e.g. mBART (Liu et al., 2020b)) can be adopted through an inte- gration with HuggingFace (Wolf et al., 2020). Loss Functions Cross-entropy (for attentional decoders), CTC, and Transducer are supported for ST and SST. Multi-objective training with CTC/attention and CTC/transducer as well as multi- tasked training (e.g. ASR/MT/ST) is also sup- ported. For S2ST, L1 and mean square error losses are also supported for spectral models. Search Algorithms For offline attentional de- coder models, label-synchronous beam search is supported with optional CTC joint decoding for multi-objective models (Watanabe et al., 2017). For offline Transducer models, the original Graves beam search (Graves, 2012) as well as time- synchronous and alignment-synchronous beam search (Saon et al., 2020) beam searches are supported. For SST, both incremental decoding and non-incremental (allowing re-translation) de- coding (Liu et al., 2020a) are supported, along with stable hypothesis detection methods (Pol\u00e1k et al., 2022). Blockwise attentional decoder mod- els use a label-synchronous beam search or time- synchronous beam search if a CTC branch is available. Blockwise transducer models use time- synchronous beam search. Synthesis & Post-processing For ST, Minimum Bayes Risk (MBR) ensembling (Fernandes et al., 2022) is supported for leveraging quality-metrics (e.g. BLEU) to compare and rank n-best out- puts from one or more models. For S2ST, neu- ral vocoders are supported for both spectral and discrete inputs (Hayashi et al., 2020, 2021). 4 Example Models In this section, we introduce example models which are pre-built in ESPnet-ST-v2 using the neural net- work components described in the previous sec- tion. These examples include state-of-the-art core architectures, as evidenced by prior studies and our performance benchmarking (presented in \u00a75). 4.1 ST Models CTC/Attention (CA) Following Yan et al. (2023), we use Conformer encoders with hierarchi- cal CTC encoding and Transformer decoders. The hierachical CTC encoding, which aligns the first Figure 2: Multi-Decoder CTC/Attention for ST. Figure 3: Time-Sync Blockwise CTC/Attn for SST. N layers of the encoder towards ASR targets and the last M layers towards ST targets, regularizes the final encoder representations to be monotonic with respect to the target. CTC/attention models are jointly decoded using either label-synchronous (wherein the attention branch is primary) or time- synchronous (wherein the CTC branch is primary) beam search. For offline tasks, label-synchrony has shown greater performance (Yan et al., 2023). Multi-Decoder CTC/Attention (MCA) As shown in Figure 2, the Multi-decoder decomposes ST into two sub-tasks, logically corresponding to ASR and MT encoder-decoder models, while main- taining E2E differentiability (Dalmia et al., 2021). This Multi-decoder scheme is also combined with the CTC/attention scheme described in the blurb"}, {"question": " What search algorithms are supported for offline attentional decoder models in ESPnet-ST-v2?", "answer": " For offline attentional decoder models in ESPnet-ST-v2, label-synchronous beam search is supported with optional CTC joint decoding for multi-objective models.", "ref_chunk": "Architectures Speech SSL Representations Speech & Text Pre-training Joint Speech/Text Pre-training Simultaneous ST End-to-End Architecture(s) Contextual Block Encoders Blockwise Attn Enc-Dec Blockwise CTC/Attention Blockwise Transducer Wait-K Attn Enc-Dec Monotonic Attn Enc-Dec \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u27131 \u2713 - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - - - - - - - - - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u27133 \u2713 - - - - - - - - - \u2713 - \u2713 - \u2713 \u27133 - - - - - - - - Cascaded Architectures \u2713 - \u2713 \u2713 - \u2713 \u2713 - \u2713 Spec Enc-Dec (Translatotron) \u2713 - \u2713 Spec Multi-Dec (Translatotron 2) \u2713 - Discrete Enc-Dec (Speech-to-Unit) \u2713 \u2713 - \u2713 Discrete Multi-Decoder (UnitY) \u27131 \u2713 - \u27132 \u2713 \u2713 - Offline S2ST End-to-End Architecture(s) - - - - - - Speech SSL Representations Neural Vocoder Support Table 1: Key features of ESPnet-ST-v2 compared to ESPnet-ST-v1 (Inaguma et al., 2020), Fairseq (Wang et al., 2020), and NeurST (Zhao et al., 2021). Com- parison intends to highlight unique features of ESPnet- ST-v2 and not to comprehensively review all toolkits. 1Supports S3PRL (Yang et al., 2021a). 2Supports both spectral and discrete. 3Only supports text-to-text. subsection, are used as building blocks in wrapper classes which are used to construct model architec- tures. Then the fully constructed models are fed to task wrappers which prepare data loaders, initialize models, and handle training/validation. For infer- ence, pythonic APIs invoke search algorithms over the trained models and direct outputs to scoring scripts. For instance, the third-party SimulEval tool for evaluating SST latency (Ma et al., 2020a) is in- tegrated via this API layer. We are also integrating with TorchAudio (Yang et al., 2021b) in the same manner. Finally, recipe scripts define experimental pipelines from data preparation to evaluation. 3.2 Key Features Each of the following modeling components fea- ture a variety of interchangeable approaches. Figure 1: Software architecture of ESPnet-ST-v2. Frontends & Targets Spectral features (e.g. FBANK) and features extracted from speech self- supervised learning (SSL) representations are sup- ported, as well as fusions over multiple features (Berrebbi et al., 2022). For speech SSL features, ESPnet-ST-v2 integrates with the S3PRL toolkit (Yang et al., 2021a). These speech SSL representa- tions are also used to generate discrete targets for S2ST (Lee et al., 2022a). Encoder Architectures Conformer (Gulati et al., 2020; Guo et al., 2021), Branchformer (Peng et al., 2022), EBranchformer (Kim et al., 2023), and Transformer (Vaswani et al., 2017; Karita et al., 2019) encoder architectures are supported for ST and S2ST. For SST, a blockwise scheme is adopted following (Tsunoo et al., 2021; Deng et al., 2022) to form contextual block Conformer and Transformer encoders. Intermediate CTC (Lee and Watanabe, 2021) and Hierachical CTC (Sanabria and Metze, 2018) encoding are also supported; these tech- niques have been shown to stabilize deep encoder optimization (Lee and Watanabe, 2021) and im- prove representations for sequence tasks involving source-to-target re-ordering (Yan et al., 2023). Decoder Architectures Attentional Transformer and recurrent neural network decoders are sup- ported (Karita et al., 2019). Multi-decoder schemes which allow for E2E differentiable decoder cas- cades via searchable hidden intermediates (Dalmia et al., 2021), are also supported; this technique has been shown to improve sequence modeling for tasks which naturally decompose into sub-tasks. Fi- nally, large language model decoders (e.g. mBART (Liu et al., 2020b)) can be adopted through an inte- gration with HuggingFace (Wolf et al., 2020). Loss Functions Cross-entropy (for attentional decoders), CTC, and Transducer are supported for ST and SST. Multi-objective training with CTC/attention and CTC/transducer as well as multi- tasked training (e.g. ASR/MT/ST) is also sup- ported. For S2ST, L1 and mean square error losses are also supported for spectral models. Search Algorithms For offline attentional de- coder models, label-synchronous beam search is supported with optional CTC joint decoding for multi-objective models (Watanabe et al., 2017). For offline Transducer models, the original Graves beam search (Graves, 2012) as well as time- synchronous and alignment-synchronous beam search (Saon et al., 2020) beam searches are supported. For SST, both incremental decoding and non-incremental (allowing re-translation) de- coding (Liu et al., 2020a) are supported, along with stable hypothesis detection methods (Pol\u00e1k et al., 2022). Blockwise attentional decoder mod- els use a label-synchronous beam search or time- synchronous beam search if a CTC branch is available. Blockwise transducer models use time- synchronous beam search. Synthesis & Post-processing For ST, Minimum Bayes Risk (MBR) ensembling (Fernandes et al., 2022) is supported for leveraging quality-metrics (e.g. BLEU) to compare and rank n-best out- puts from one or more models. For S2ST, neu- ral vocoders are supported for both spectral and discrete inputs (Hayashi et al., 2020, 2021). 4 Example Models In this section, we introduce example models which are pre-built in ESPnet-ST-v2 using the neural net- work components described in the previous sec- tion. These examples include state-of-the-art core architectures, as evidenced by prior studies and our performance benchmarking (presented in \u00a75). 4.1 ST Models CTC/Attention (CA) Following Yan et al. (2023), we use Conformer encoders with hierarchi- cal CTC encoding and Transformer decoders. The hierachical CTC encoding, which aligns the first Figure 2: Multi-Decoder CTC/Attention for ST. Figure 3: Time-Sync Blockwise CTC/Attn for SST. N layers of the encoder towards ASR targets and the last M layers towards ST targets, regularizes the final encoder representations to be monotonic with respect to the target. CTC/attention models are jointly decoded using either label-synchronous (wherein the attention branch is primary) or time- synchronous (wherein the CTC branch is primary) beam search. For offline tasks, label-synchrony has shown greater performance (Yan et al., 2023). Multi-Decoder CTC/Attention (MCA) As shown in Figure 2, the Multi-decoder decomposes ST into two sub-tasks, logically corresponding to ASR and MT encoder-decoder models, while main- taining E2E differentiability (Dalmia et al., 2021). This Multi-decoder scheme is also combined with the CTC/attention scheme described in the blurb"}, {"question": " Which loss functions are supported for ST and SST models in ESPnet-ST-v2?", "answer": " Cross-entropy, CTC, and Transducer are supported for ST and SST models in ESPnet-ST-v2. Multi-objective training with various combinations of these loss functions is also supported.", "ref_chunk": "Architectures Speech SSL Representations Speech & Text Pre-training Joint Speech/Text Pre-training Simultaneous ST End-to-End Architecture(s) Contextual Block Encoders Blockwise Attn Enc-Dec Blockwise CTC/Attention Blockwise Transducer Wait-K Attn Enc-Dec Monotonic Attn Enc-Dec \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u27131 \u2713 - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - - - - - - - - - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u27133 \u2713 - - - - - - - - - \u2713 - \u2713 - \u2713 \u27133 - - - - - - - - Cascaded Architectures \u2713 - \u2713 \u2713 - \u2713 \u2713 - \u2713 Spec Enc-Dec (Translatotron) \u2713 - \u2713 Spec Multi-Dec (Translatotron 2) \u2713 - Discrete Enc-Dec (Speech-to-Unit) \u2713 \u2713 - \u2713 Discrete Multi-Decoder (UnitY) \u27131 \u2713 - \u27132 \u2713 \u2713 - Offline S2ST End-to-End Architecture(s) - - - - - - Speech SSL Representations Neural Vocoder Support Table 1: Key features of ESPnet-ST-v2 compared to ESPnet-ST-v1 (Inaguma et al., 2020), Fairseq (Wang et al., 2020), and NeurST (Zhao et al., 2021). Com- parison intends to highlight unique features of ESPnet- ST-v2 and not to comprehensively review all toolkits. 1Supports S3PRL (Yang et al., 2021a). 2Supports both spectral and discrete. 3Only supports text-to-text. subsection, are used as building blocks in wrapper classes which are used to construct model architec- tures. Then the fully constructed models are fed to task wrappers which prepare data loaders, initialize models, and handle training/validation. For infer- ence, pythonic APIs invoke search algorithms over the trained models and direct outputs to scoring scripts. For instance, the third-party SimulEval tool for evaluating SST latency (Ma et al., 2020a) is in- tegrated via this API layer. We are also integrating with TorchAudio (Yang et al., 2021b) in the same manner. Finally, recipe scripts define experimental pipelines from data preparation to evaluation. 3.2 Key Features Each of the following modeling components fea- ture a variety of interchangeable approaches. Figure 1: Software architecture of ESPnet-ST-v2. Frontends & Targets Spectral features (e.g. FBANK) and features extracted from speech self- supervised learning (SSL) representations are sup- ported, as well as fusions over multiple features (Berrebbi et al., 2022). For speech SSL features, ESPnet-ST-v2 integrates with the S3PRL toolkit (Yang et al., 2021a). These speech SSL representa- tions are also used to generate discrete targets for S2ST (Lee et al., 2022a). Encoder Architectures Conformer (Gulati et al., 2020; Guo et al., 2021), Branchformer (Peng et al., 2022), EBranchformer (Kim et al., 2023), and Transformer (Vaswani et al., 2017; Karita et al., 2019) encoder architectures are supported for ST and S2ST. For SST, a blockwise scheme is adopted following (Tsunoo et al., 2021; Deng et al., 2022) to form contextual block Conformer and Transformer encoders. Intermediate CTC (Lee and Watanabe, 2021) and Hierachical CTC (Sanabria and Metze, 2018) encoding are also supported; these tech- niques have been shown to stabilize deep encoder optimization (Lee and Watanabe, 2021) and im- prove representations for sequence tasks involving source-to-target re-ordering (Yan et al., 2023). Decoder Architectures Attentional Transformer and recurrent neural network decoders are sup- ported (Karita et al., 2019). Multi-decoder schemes which allow for E2E differentiable decoder cas- cades via searchable hidden intermediates (Dalmia et al., 2021), are also supported; this technique has been shown to improve sequence modeling for tasks which naturally decompose into sub-tasks. Fi- nally, large language model decoders (e.g. mBART (Liu et al., 2020b)) can be adopted through an inte- gration with HuggingFace (Wolf et al., 2020). Loss Functions Cross-entropy (for attentional decoders), CTC, and Transducer are supported for ST and SST. Multi-objective training with CTC/attention and CTC/transducer as well as multi- tasked training (e.g. ASR/MT/ST) is also sup- ported. For S2ST, L1 and mean square error losses are also supported for spectral models. Search Algorithms For offline attentional de- coder models, label-synchronous beam search is supported with optional CTC joint decoding for multi-objective models (Watanabe et al., 2017). For offline Transducer models, the original Graves beam search (Graves, 2012) as well as time- synchronous and alignment-synchronous beam search (Saon et al., 2020) beam searches are supported. For SST, both incremental decoding and non-incremental (allowing re-translation) de- coding (Liu et al., 2020a) are supported, along with stable hypothesis detection methods (Pol\u00e1k et al., 2022). Blockwise attentional decoder mod- els use a label-synchronous beam search or time- synchronous beam search if a CTC branch is available. Blockwise transducer models use time- synchronous beam search. Synthesis & Post-processing For ST, Minimum Bayes Risk (MBR) ensembling (Fernandes et al., 2022) is supported for leveraging quality-metrics (e.g. BLEU) to compare and rank n-best out- puts from one or more models. For S2ST, neu- ral vocoders are supported for both spectral and discrete inputs (Hayashi et al., 2020, 2021). 4 Example Models In this section, we introduce example models which are pre-built in ESPnet-ST-v2 using the neural net- work components described in the previous sec- tion. These examples include state-of-the-art core architectures, as evidenced by prior studies and our performance benchmarking (presented in \u00a75). 4.1 ST Models CTC/Attention (CA) Following Yan et al. (2023), we use Conformer encoders with hierarchi- cal CTC encoding and Transformer decoders. The hierachical CTC encoding, which aligns the first Figure 2: Multi-Decoder CTC/Attention for ST. Figure 3: Time-Sync Blockwise CTC/Attn for SST. N layers of the encoder towards ASR targets and the last M layers towards ST targets, regularizes the final encoder representations to be monotonic with respect to the target. CTC/attention models are jointly decoded using either label-synchronous (wherein the attention branch is primary) or time- synchronous (wherein the CTC branch is primary) beam search. For offline tasks, label-synchrony has shown greater performance (Yan et al., 2023). Multi-Decoder CTC/Attention (MCA) As shown in Figure 2, the Multi-decoder decomposes ST into two sub-tasks, logically corresponding to ASR and MT encoder-decoder models, while main- taining E2E differentiability (Dalmia et al., 2021). This Multi-decoder scheme is also combined with the CTC/attention scheme described in the blurb"}, {"question": " What post-processing method is supported for ST models in ESPnet-ST-v2?", "answer": " Minimum Bayes Risk (MBR) ensembling is supported for ST models in ESPnet-ST-v2, allowing for comparison and ranking of n-best outputs from one or more models.", "ref_chunk": "Architectures Speech SSL Representations Speech & Text Pre-training Joint Speech/Text Pre-training Simultaneous ST End-to-End Architecture(s) Contextual Block Encoders Blockwise Attn Enc-Dec Blockwise CTC/Attention Blockwise Transducer Wait-K Attn Enc-Dec Monotonic Attn Enc-Dec \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u27131 \u2713 - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - - - - - - - - - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u27133 \u2713 - - - - - - - - - \u2713 - \u2713 - \u2713 \u27133 - - - - - - - - Cascaded Architectures \u2713 - \u2713 \u2713 - \u2713 \u2713 - \u2713 Spec Enc-Dec (Translatotron) \u2713 - \u2713 Spec Multi-Dec (Translatotron 2) \u2713 - Discrete Enc-Dec (Speech-to-Unit) \u2713 \u2713 - \u2713 Discrete Multi-Decoder (UnitY) \u27131 \u2713 - \u27132 \u2713 \u2713 - Offline S2ST End-to-End Architecture(s) - - - - - - Speech SSL Representations Neural Vocoder Support Table 1: Key features of ESPnet-ST-v2 compared to ESPnet-ST-v1 (Inaguma et al., 2020), Fairseq (Wang et al., 2020), and NeurST (Zhao et al., 2021). Com- parison intends to highlight unique features of ESPnet- ST-v2 and not to comprehensively review all toolkits. 1Supports S3PRL (Yang et al., 2021a). 2Supports both spectral and discrete. 3Only supports text-to-text. subsection, are used as building blocks in wrapper classes which are used to construct model architec- tures. Then the fully constructed models are fed to task wrappers which prepare data loaders, initialize models, and handle training/validation. For infer- ence, pythonic APIs invoke search algorithms over the trained models and direct outputs to scoring scripts. For instance, the third-party SimulEval tool for evaluating SST latency (Ma et al., 2020a) is in- tegrated via this API layer. We are also integrating with TorchAudio (Yang et al., 2021b) in the same manner. Finally, recipe scripts define experimental pipelines from data preparation to evaluation. 3.2 Key Features Each of the following modeling components fea- ture a variety of interchangeable approaches. Figure 1: Software architecture of ESPnet-ST-v2. Frontends & Targets Spectral features (e.g. FBANK) and features extracted from speech self- supervised learning (SSL) representations are sup- ported, as well as fusions over multiple features (Berrebbi et al., 2022). For speech SSL features, ESPnet-ST-v2 integrates with the S3PRL toolkit (Yang et al., 2021a). These speech SSL representa- tions are also used to generate discrete targets for S2ST (Lee et al., 2022a). Encoder Architectures Conformer (Gulati et al., 2020; Guo et al., 2021), Branchformer (Peng et al., 2022), EBranchformer (Kim et al., 2023), and Transformer (Vaswani et al., 2017; Karita et al., 2019) encoder architectures are supported for ST and S2ST. For SST, a blockwise scheme is adopted following (Tsunoo et al., 2021; Deng et al., 2022) to form contextual block Conformer and Transformer encoders. Intermediate CTC (Lee and Watanabe, 2021) and Hierachical CTC (Sanabria and Metze, 2018) encoding are also supported; these tech- niques have been shown to stabilize deep encoder optimization (Lee and Watanabe, 2021) and im- prove representations for sequence tasks involving source-to-target re-ordering (Yan et al., 2023). Decoder Architectures Attentional Transformer and recurrent neural network decoders are sup- ported (Karita et al., 2019). Multi-decoder schemes which allow for E2E differentiable decoder cas- cades via searchable hidden intermediates (Dalmia et al., 2021), are also supported; this technique has been shown to improve sequence modeling for tasks which naturally decompose into sub-tasks. Fi- nally, large language model decoders (e.g. mBART (Liu et al., 2020b)) can be adopted through an inte- gration with HuggingFace (Wolf et al., 2020). Loss Functions Cross-entropy (for attentional decoders), CTC, and Transducer are supported for ST and SST. Multi-objective training with CTC/attention and CTC/transducer as well as multi- tasked training (e.g. ASR/MT/ST) is also sup- ported. For S2ST, L1 and mean square error losses are also supported for spectral models. Search Algorithms For offline attentional de- coder models, label-synchronous beam search is supported with optional CTC joint decoding for multi-objective models (Watanabe et al., 2017). For offline Transducer models, the original Graves beam search (Graves, 2012) as well as time- synchronous and alignment-synchronous beam search (Saon et al., 2020) beam searches are supported. For SST, both incremental decoding and non-incremental (allowing re-translation) de- coding (Liu et al., 2020a) are supported, along with stable hypothesis detection methods (Pol\u00e1k et al., 2022). Blockwise attentional decoder mod- els use a label-synchronous beam search or time- synchronous beam search if a CTC branch is available. Blockwise transducer models use time- synchronous beam search. Synthesis & Post-processing For ST, Minimum Bayes Risk (MBR) ensembling (Fernandes et al., 2022) is supported for leveraging quality-metrics (e.g. BLEU) to compare and rank n-best out- puts from one or more models. For S2ST, neu- ral vocoders are supported for both spectral and discrete inputs (Hayashi et al., 2020, 2021). 4 Example Models In this section, we introduce example models which are pre-built in ESPnet-ST-v2 using the neural net- work components described in the previous sec- tion. These examples include state-of-the-art core architectures, as evidenced by prior studies and our performance benchmarking (presented in \u00a75). 4.1 ST Models CTC/Attention (CA) Following Yan et al. (2023), we use Conformer encoders with hierarchi- cal CTC encoding and Transformer decoders. The hierachical CTC encoding, which aligns the first Figure 2: Multi-Decoder CTC/Attention for ST. Figure 3: Time-Sync Blockwise CTC/Attn for SST. N layers of the encoder towards ASR targets and the last M layers towards ST targets, regularizes the final encoder representations to be monotonic with respect to the target. CTC/attention models are jointly decoded using either label-synchronous (wherein the attention branch is primary) or time- synchronous (wherein the CTC branch is primary) beam search. For offline tasks, label-synchrony has shown greater performance (Yan et al., 2023). Multi-Decoder CTC/Attention (MCA) As shown in Figure 2, the Multi-decoder decomposes ST into two sub-tasks, logically corresponding to ASR and MT encoder-decoder models, while main- taining E2E differentiability (Dalmia et al., 2021). This Multi-decoder scheme is also combined with the CTC/attention scheme described in the blurb"}, {"question": " Which encoder architectures are supported for speech-to-speech tasks in ESPnet-ST-v2?", "answer": " Conformer, Branchformer, EBranchformer, and Transformer encoder architectures are supported for speech-to-speech tasks in ESPnet-ST-v2.", "ref_chunk": "Architectures Speech SSL Representations Speech & Text Pre-training Joint Speech/Text Pre-training Simultaneous ST End-to-End Architecture(s) Contextual Block Encoders Blockwise Attn Enc-Dec Blockwise CTC/Attention Blockwise Transducer Wait-K Attn Enc-Dec Monotonic Attn Enc-Dec \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u27131 \u2713 - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - - - - - - - - - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u27133 \u2713 - - - - - - - - - \u2713 - \u2713 - \u2713 \u27133 - - - - - - - - Cascaded Architectures \u2713 - \u2713 \u2713 - \u2713 \u2713 - \u2713 Spec Enc-Dec (Translatotron) \u2713 - \u2713 Spec Multi-Dec (Translatotron 2) \u2713 - Discrete Enc-Dec (Speech-to-Unit) \u2713 \u2713 - \u2713 Discrete Multi-Decoder (UnitY) \u27131 \u2713 - \u27132 \u2713 \u2713 - Offline S2ST End-to-End Architecture(s) - - - - - - Speech SSL Representations Neural Vocoder Support Table 1: Key features of ESPnet-ST-v2 compared to ESPnet-ST-v1 (Inaguma et al., 2020), Fairseq (Wang et al., 2020), and NeurST (Zhao et al., 2021). Com- parison intends to highlight unique features of ESPnet- ST-v2 and not to comprehensively review all toolkits. 1Supports S3PRL (Yang et al., 2021a). 2Supports both spectral and discrete. 3Only supports text-to-text. subsection, are used as building blocks in wrapper classes which are used to construct model architec- tures. Then the fully constructed models are fed to task wrappers which prepare data loaders, initialize models, and handle training/validation. For infer- ence, pythonic APIs invoke search algorithms over the trained models and direct outputs to scoring scripts. For instance, the third-party SimulEval tool for evaluating SST latency (Ma et al., 2020a) is in- tegrated via this API layer. We are also integrating with TorchAudio (Yang et al., 2021b) in the same manner. Finally, recipe scripts define experimental pipelines from data preparation to evaluation. 3.2 Key Features Each of the following modeling components fea- ture a variety of interchangeable approaches. Figure 1: Software architecture of ESPnet-ST-v2. Frontends & Targets Spectral features (e.g. FBANK) and features extracted from speech self- supervised learning (SSL) representations are sup- ported, as well as fusions over multiple features (Berrebbi et al., 2022). For speech SSL features, ESPnet-ST-v2 integrates with the S3PRL toolkit (Yang et al., 2021a). These speech SSL representa- tions are also used to generate discrete targets for S2ST (Lee et al., 2022a). Encoder Architectures Conformer (Gulati et al., 2020; Guo et al., 2021), Branchformer (Peng et al., 2022), EBranchformer (Kim et al., 2023), and Transformer (Vaswani et al., 2017; Karita et al., 2019) encoder architectures are supported for ST and S2ST. For SST, a blockwise scheme is adopted following (Tsunoo et al., 2021; Deng et al., 2022) to form contextual block Conformer and Transformer encoders. Intermediate CTC (Lee and Watanabe, 2021) and Hierachical CTC (Sanabria and Metze, 2018) encoding are also supported; these tech- niques have been shown to stabilize deep encoder optimization (Lee and Watanabe, 2021) and im- prove representations for sequence tasks involving source-to-target re-ordering (Yan et al., 2023). Decoder Architectures Attentional Transformer and recurrent neural network decoders are sup- ported (Karita et al., 2019). Multi-decoder schemes which allow for E2E differentiable decoder cas- cades via searchable hidden intermediates (Dalmia et al., 2021), are also supported; this technique has been shown to improve sequence modeling for tasks which naturally decompose into sub-tasks. Fi- nally, large language model decoders (e.g. mBART (Liu et al., 2020b)) can be adopted through an inte- gration with HuggingFace (Wolf et al., 2020). Loss Functions Cross-entropy (for attentional decoders), CTC, and Transducer are supported for ST and SST. Multi-objective training with CTC/attention and CTC/transducer as well as multi- tasked training (e.g. ASR/MT/ST) is also sup- ported. For S2ST, L1 and mean square error losses are also supported for spectral models. Search Algorithms For offline attentional de- coder models, label-synchronous beam search is supported with optional CTC joint decoding for multi-objective models (Watanabe et al., 2017). For offline Transducer models, the original Graves beam search (Graves, 2012) as well as time- synchronous and alignment-synchronous beam search (Saon et al., 2020) beam searches are supported. For SST, both incremental decoding and non-incremental (allowing re-translation) de- coding (Liu et al., 2020a) are supported, along with stable hypothesis detection methods (Pol\u00e1k et al., 2022). Blockwise attentional decoder mod- els use a label-synchronous beam search or time- synchronous beam search if a CTC branch is available. Blockwise transducer models use time- synchronous beam search. Synthesis & Post-processing For ST, Minimum Bayes Risk (MBR) ensembling (Fernandes et al., 2022) is supported for leveraging quality-metrics (e.g. BLEU) to compare and rank n-best out- puts from one or more models. For S2ST, neu- ral vocoders are supported for both spectral and discrete inputs (Hayashi et al., 2020, 2021). 4 Example Models In this section, we introduce example models which are pre-built in ESPnet-ST-v2 using the neural net- work components described in the previous sec- tion. These examples include state-of-the-art core architectures, as evidenced by prior studies and our performance benchmarking (presented in \u00a75). 4.1 ST Models CTC/Attention (CA) Following Yan et al. (2023), we use Conformer encoders with hierarchi- cal CTC encoding and Transformer decoders. The hierachical CTC encoding, which aligns the first Figure 2: Multi-Decoder CTC/Attention for ST. Figure 3: Time-Sync Blockwise CTC/Attn for SST. N layers of the encoder towards ASR targets and the last M layers towards ST targets, regularizes the final encoder representations to be monotonic with respect to the target. CTC/attention models are jointly decoded using either label-synchronous (wherein the attention branch is primary) or time- synchronous (wherein the CTC branch is primary) beam search. For offline tasks, label-synchrony has shown greater performance (Yan et al., 2023). Multi-Decoder CTC/Attention (MCA) As shown in Figure 2, the Multi-decoder decomposes ST into two sub-tasks, logically corresponding to ASR and MT encoder-decoder models, while main- taining E2E differentiability (Dalmia et al., 2021). This Multi-decoder scheme is also combined with the CTC/attention scheme described in the blurb"}, {"question": " How are decoder architectures in ESPnet-ST-v2 designed to handle E2E differentiable decoder cascades?", "answer": " Decoder architectures in ESPnet-ST-v2 support multi-decoder schemes that allow for E2E differentiable decoder cascades via searchable hidden intermediates.", "ref_chunk": "Architectures Speech SSL Representations Speech & Text Pre-training Joint Speech/Text Pre-training Simultaneous ST End-to-End Architecture(s) Contextual Block Encoders Blockwise Attn Enc-Dec Blockwise CTC/Attention Blockwise Transducer Wait-K Attn Enc-Dec Monotonic Attn Enc-Dec \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u27131 \u2713 - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - - - - - - - - - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u27133 \u2713 - - - - - - - - - \u2713 - \u2713 - \u2713 \u27133 - - - - - - - - Cascaded Architectures \u2713 - \u2713 \u2713 - \u2713 \u2713 - \u2713 Spec Enc-Dec (Translatotron) \u2713 - \u2713 Spec Multi-Dec (Translatotron 2) \u2713 - Discrete Enc-Dec (Speech-to-Unit) \u2713 \u2713 - \u2713 Discrete Multi-Decoder (UnitY) \u27131 \u2713 - \u27132 \u2713 \u2713 - Offline S2ST End-to-End Architecture(s) - - - - - - Speech SSL Representations Neural Vocoder Support Table 1: Key features of ESPnet-ST-v2 compared to ESPnet-ST-v1 (Inaguma et al., 2020), Fairseq (Wang et al., 2020), and NeurST (Zhao et al., 2021). Com- parison intends to highlight unique features of ESPnet- ST-v2 and not to comprehensively review all toolkits. 1Supports S3PRL (Yang et al., 2021a). 2Supports both spectral and discrete. 3Only supports text-to-text. subsection, are used as building blocks in wrapper classes which are used to construct model architec- tures. Then the fully constructed models are fed to task wrappers which prepare data loaders, initialize models, and handle training/validation. For infer- ence, pythonic APIs invoke search algorithms over the trained models and direct outputs to scoring scripts. For instance, the third-party SimulEval tool for evaluating SST latency (Ma et al., 2020a) is in- tegrated via this API layer. We are also integrating with TorchAudio (Yang et al., 2021b) in the same manner. Finally, recipe scripts define experimental pipelines from data preparation to evaluation. 3.2 Key Features Each of the following modeling components fea- ture a variety of interchangeable approaches. Figure 1: Software architecture of ESPnet-ST-v2. Frontends & Targets Spectral features (e.g. FBANK) and features extracted from speech self- supervised learning (SSL) representations are sup- ported, as well as fusions over multiple features (Berrebbi et al., 2022). For speech SSL features, ESPnet-ST-v2 integrates with the S3PRL toolkit (Yang et al., 2021a). These speech SSL representa- tions are also used to generate discrete targets for S2ST (Lee et al., 2022a). Encoder Architectures Conformer (Gulati et al., 2020; Guo et al., 2021), Branchformer (Peng et al., 2022), EBranchformer (Kim et al., 2023), and Transformer (Vaswani et al., 2017; Karita et al., 2019) encoder architectures are supported for ST and S2ST. For SST, a blockwise scheme is adopted following (Tsunoo et al., 2021; Deng et al., 2022) to form contextual block Conformer and Transformer encoders. Intermediate CTC (Lee and Watanabe, 2021) and Hierachical CTC (Sanabria and Metze, 2018) encoding are also supported; these tech- niques have been shown to stabilize deep encoder optimization (Lee and Watanabe, 2021) and im- prove representations for sequence tasks involving source-to-target re-ordering (Yan et al., 2023). Decoder Architectures Attentional Transformer and recurrent neural network decoders are sup- ported (Karita et al., 2019). Multi-decoder schemes which allow for E2E differentiable decoder cas- cades via searchable hidden intermediates (Dalmia et al., 2021), are also supported; this technique has been shown to improve sequence modeling for tasks which naturally decompose into sub-tasks. Fi- nally, large language model decoders (e.g. mBART (Liu et al., 2020b)) can be adopted through an inte- gration with HuggingFace (Wolf et al., 2020). Loss Functions Cross-entropy (for attentional decoders), CTC, and Transducer are supported for ST and SST. Multi-objective training with CTC/attention and CTC/transducer as well as multi- tasked training (e.g. ASR/MT/ST) is also sup- ported. For S2ST, L1 and mean square error losses are also supported for spectral models. Search Algorithms For offline attentional de- coder models, label-synchronous beam search is supported with optional CTC joint decoding for multi-objective models (Watanabe et al., 2017). For offline Transducer models, the original Graves beam search (Graves, 2012) as well as time- synchronous and alignment-synchronous beam search (Saon et al., 2020) beam searches are supported. For SST, both incremental decoding and non-incremental (allowing re-translation) de- coding (Liu et al., 2020a) are supported, along with stable hypothesis detection methods (Pol\u00e1k et al., 2022). Blockwise attentional decoder mod- els use a label-synchronous beam search or time- synchronous beam search if a CTC branch is available. Blockwise transducer models use time- synchronous beam search. Synthesis & Post-processing For ST, Minimum Bayes Risk (MBR) ensembling (Fernandes et al., 2022) is supported for leveraging quality-metrics (e.g. BLEU) to compare and rank n-best out- puts from one or more models. For S2ST, neu- ral vocoders are supported for both spectral and discrete inputs (Hayashi et al., 2020, 2021). 4 Example Models In this section, we introduce example models which are pre-built in ESPnet-ST-v2 using the neural net- work components described in the previous sec- tion. These examples include state-of-the-art core architectures, as evidenced by prior studies and our performance benchmarking (presented in \u00a75). 4.1 ST Models CTC/Attention (CA) Following Yan et al. (2023), we use Conformer encoders with hierarchi- cal CTC encoding and Transformer decoders. The hierachical CTC encoding, which aligns the first Figure 2: Multi-Decoder CTC/Attention for ST. Figure 3: Time-Sync Blockwise CTC/Attn for SST. N layers of the encoder towards ASR targets and the last M layers towards ST targets, regularizes the final encoder representations to be monotonic with respect to the target. CTC/attention models are jointly decoded using either label-synchronous (wherein the attention branch is primary) or time- synchronous (wherein the CTC branch is primary) beam search. For offline tasks, label-synchrony has shown greater performance (Yan et al., 2023). Multi-Decoder CTC/Attention (MCA) As shown in Figure 2, the Multi-decoder decomposes ST into two sub-tasks, logically corresponding to ASR and MT encoder-decoder models, while main- taining E2E differentiability (Dalmia et al., 2021). This Multi-decoder scheme is also combined with the CTC/attention scheme described in the blurb"}, {"question": " What modeling components in ESPnet-ST-v2 feature interchangeable approaches?", "answer": " Each modeling component in ESPnet-ST-v2 features a variety of interchangeable approaches, allowing for flexibility in architectural design.", "ref_chunk": "Architectures Speech SSL Representations Speech & Text Pre-training Joint Speech/Text Pre-training Simultaneous ST End-to-End Architecture(s) Contextual Block Encoders Blockwise Attn Enc-Dec Blockwise CTC/Attention Blockwise Transducer Wait-K Attn Enc-Dec Monotonic Attn Enc-Dec \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u27131 \u2713 - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - - - - - - - - - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u27133 \u2713 - - - - - - - - - \u2713 - \u2713 - \u2713 \u27133 - - - - - - - - Cascaded Architectures \u2713 - \u2713 \u2713 - \u2713 \u2713 - \u2713 Spec Enc-Dec (Translatotron) \u2713 - \u2713 Spec Multi-Dec (Translatotron 2) \u2713 - Discrete Enc-Dec (Speech-to-Unit) \u2713 \u2713 - \u2713 Discrete Multi-Decoder (UnitY) \u27131 \u2713 - \u27132 \u2713 \u2713 - Offline S2ST End-to-End Architecture(s) - - - - - - Speech SSL Representations Neural Vocoder Support Table 1: Key features of ESPnet-ST-v2 compared to ESPnet-ST-v1 (Inaguma et al., 2020), Fairseq (Wang et al., 2020), and NeurST (Zhao et al., 2021). Com- parison intends to highlight unique features of ESPnet- ST-v2 and not to comprehensively review all toolkits. 1Supports S3PRL (Yang et al., 2021a). 2Supports both spectral and discrete. 3Only supports text-to-text. subsection, are used as building blocks in wrapper classes which are used to construct model architec- tures. Then the fully constructed models are fed to task wrappers which prepare data loaders, initialize models, and handle training/validation. For infer- ence, pythonic APIs invoke search algorithms over the trained models and direct outputs to scoring scripts. For instance, the third-party SimulEval tool for evaluating SST latency (Ma et al., 2020a) is in- tegrated via this API layer. We are also integrating with TorchAudio (Yang et al., 2021b) in the same manner. Finally, recipe scripts define experimental pipelines from data preparation to evaluation. 3.2 Key Features Each of the following modeling components fea- ture a variety of interchangeable approaches. Figure 1: Software architecture of ESPnet-ST-v2. Frontends & Targets Spectral features (e.g. FBANK) and features extracted from speech self- supervised learning (SSL) representations are sup- ported, as well as fusions over multiple features (Berrebbi et al., 2022). For speech SSL features, ESPnet-ST-v2 integrates with the S3PRL toolkit (Yang et al., 2021a). These speech SSL representa- tions are also used to generate discrete targets for S2ST (Lee et al., 2022a). Encoder Architectures Conformer (Gulati et al., 2020; Guo et al., 2021), Branchformer (Peng et al., 2022), EBranchformer (Kim et al., 2023), and Transformer (Vaswani et al., 2017; Karita et al., 2019) encoder architectures are supported for ST and S2ST. For SST, a blockwise scheme is adopted following (Tsunoo et al., 2021; Deng et al., 2022) to form contextual block Conformer and Transformer encoders. Intermediate CTC (Lee and Watanabe, 2021) and Hierachical CTC (Sanabria and Metze, 2018) encoding are also supported; these tech- niques have been shown to stabilize deep encoder optimization (Lee and Watanabe, 2021) and im- prove representations for sequence tasks involving source-to-target re-ordering (Yan et al., 2023). Decoder Architectures Attentional Transformer and recurrent neural network decoders are sup- ported (Karita et al., 2019). Multi-decoder schemes which allow for E2E differentiable decoder cas- cades via searchable hidden intermediates (Dalmia et al., 2021), are also supported; this technique has been shown to improve sequence modeling for tasks which naturally decompose into sub-tasks. Fi- nally, large language model decoders (e.g. mBART (Liu et al., 2020b)) can be adopted through an inte- gration with HuggingFace (Wolf et al., 2020). Loss Functions Cross-entropy (for attentional decoders), CTC, and Transducer are supported for ST and SST. Multi-objective training with CTC/attention and CTC/transducer as well as multi- tasked training (e.g. ASR/MT/ST) is also sup- ported. For S2ST, L1 and mean square error losses are also supported for spectral models. Search Algorithms For offline attentional de- coder models, label-synchronous beam search is supported with optional CTC joint decoding for multi-objective models (Watanabe et al., 2017). For offline Transducer models, the original Graves beam search (Graves, 2012) as well as time- synchronous and alignment-synchronous beam search (Saon et al., 2020) beam searches are supported. For SST, both incremental decoding and non-incremental (allowing re-translation) de- coding (Liu et al., 2020a) are supported, along with stable hypothesis detection methods (Pol\u00e1k et al., 2022). Blockwise attentional decoder mod- els use a label-synchronous beam search or time- synchronous beam search if a CTC branch is available. Blockwise transducer models use time- synchronous beam search. Synthesis & Post-processing For ST, Minimum Bayes Risk (MBR) ensembling (Fernandes et al., 2022) is supported for leveraging quality-metrics (e.g. BLEU) to compare and rank n-best out- puts from one or more models. For S2ST, neu- ral vocoders are supported for both spectral and discrete inputs (Hayashi et al., 2020, 2021). 4 Example Models In this section, we introduce example models which are pre-built in ESPnet-ST-v2 using the neural net- work components described in the previous sec- tion. These examples include state-of-the-art core architectures, as evidenced by prior studies and our performance benchmarking (presented in \u00a75). 4.1 ST Models CTC/Attention (CA) Following Yan et al. (2023), we use Conformer encoders with hierarchi- cal CTC encoding and Transformer decoders. The hierachical CTC encoding, which aligns the first Figure 2: Multi-Decoder CTC/Attention for ST. Figure 3: Time-Sync Blockwise CTC/Attn for SST. N layers of the encoder towards ASR targets and the last M layers towards ST targets, regularizes the final encoder representations to be monotonic with respect to the target. CTC/attention models are jointly decoded using either label-synchronous (wherein the attention branch is primary) or time- synchronous (wherein the CTC branch is primary) beam search. For offline tasks, label-synchrony has shown greater performance (Yan et al., 2023). Multi-Decoder CTC/Attention (MCA) As shown in Figure 2, the Multi-decoder decomposes ST into two sub-tasks, logically corresponding to ASR and MT encoder-decoder models, while main- taining E2E differentiability (Dalmia et al., 2021). This Multi-decoder scheme is also combined with the CTC/attention scheme described in the blurb"}, {"question": " What synthesis method is supported for spectral and discrete inputs in S2ST models in ESPnet-ST-v2?", "answer": " Neural vocoders are supported for both spectral and discrete inputs in S2ST models in ESPnet-ST-v2 for synthesis and post-processing.", "ref_chunk": "Architectures Speech SSL Representations Speech & Text Pre-training Joint Speech/Text Pre-training Simultaneous ST End-to-End Architecture(s) Contextual Block Encoders Blockwise Attn Enc-Dec Blockwise CTC/Attention Blockwise Transducer Wait-K Attn Enc-Dec Monotonic Attn Enc-Dec \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u27131 \u2713 - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - - - - - - - - - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u27133 \u2713 - - - - - - - - - \u2713 - \u2713 - \u2713 \u27133 - - - - - - - - Cascaded Architectures \u2713 - \u2713 \u2713 - \u2713 \u2713 - \u2713 Spec Enc-Dec (Translatotron) \u2713 - \u2713 Spec Multi-Dec (Translatotron 2) \u2713 - Discrete Enc-Dec (Speech-to-Unit) \u2713 \u2713 - \u2713 Discrete Multi-Decoder (UnitY) \u27131 \u2713 - \u27132 \u2713 \u2713 - Offline S2ST End-to-End Architecture(s) - - - - - - Speech SSL Representations Neural Vocoder Support Table 1: Key features of ESPnet-ST-v2 compared to ESPnet-ST-v1 (Inaguma et al., 2020), Fairseq (Wang et al., 2020), and NeurST (Zhao et al., 2021). Com- parison intends to highlight unique features of ESPnet- ST-v2 and not to comprehensively review all toolkits. 1Supports S3PRL (Yang et al., 2021a). 2Supports both spectral and discrete. 3Only supports text-to-text. subsection, are used as building blocks in wrapper classes which are used to construct model architec- tures. Then the fully constructed models are fed to task wrappers which prepare data loaders, initialize models, and handle training/validation. For infer- ence, pythonic APIs invoke search algorithms over the trained models and direct outputs to scoring scripts. For instance, the third-party SimulEval tool for evaluating SST latency (Ma et al., 2020a) is in- tegrated via this API layer. We are also integrating with TorchAudio (Yang et al., 2021b) in the same manner. Finally, recipe scripts define experimental pipelines from data preparation to evaluation. 3.2 Key Features Each of the following modeling components fea- ture a variety of interchangeable approaches. Figure 1: Software architecture of ESPnet-ST-v2. Frontends & Targets Spectral features (e.g. FBANK) and features extracted from speech self- supervised learning (SSL) representations are sup- ported, as well as fusions over multiple features (Berrebbi et al., 2022). For speech SSL features, ESPnet-ST-v2 integrates with the S3PRL toolkit (Yang et al., 2021a). These speech SSL representa- tions are also used to generate discrete targets for S2ST (Lee et al., 2022a). Encoder Architectures Conformer (Gulati et al., 2020; Guo et al., 2021), Branchformer (Peng et al., 2022), EBranchformer (Kim et al., 2023), and Transformer (Vaswani et al., 2017; Karita et al., 2019) encoder architectures are supported for ST and S2ST. For SST, a blockwise scheme is adopted following (Tsunoo et al., 2021; Deng et al., 2022) to form contextual block Conformer and Transformer encoders. Intermediate CTC (Lee and Watanabe, 2021) and Hierachical CTC (Sanabria and Metze, 2018) encoding are also supported; these tech- niques have been shown to stabilize deep encoder optimization (Lee and Watanabe, 2021) and im- prove representations for sequence tasks involving source-to-target re-ordering (Yan et al., 2023). Decoder Architectures Attentional Transformer and recurrent neural network decoders are sup- ported (Karita et al., 2019). Multi-decoder schemes which allow for E2E differentiable decoder cas- cades via searchable hidden intermediates (Dalmia et al., 2021), are also supported; this technique has been shown to improve sequence modeling for tasks which naturally decompose into sub-tasks. Fi- nally, large language model decoders (e.g. mBART (Liu et al., 2020b)) can be adopted through an inte- gration with HuggingFace (Wolf et al., 2020). Loss Functions Cross-entropy (for attentional decoders), CTC, and Transducer are supported for ST and SST. Multi-objective training with CTC/attention and CTC/transducer as well as multi- tasked training (e.g. ASR/MT/ST) is also sup- ported. For S2ST, L1 and mean square error losses are also supported for spectral models. Search Algorithms For offline attentional de- coder models, label-synchronous beam search is supported with optional CTC joint decoding for multi-objective models (Watanabe et al., 2017). For offline Transducer models, the original Graves beam search (Graves, 2012) as well as time- synchronous and alignment-synchronous beam search (Saon et al., 2020) beam searches are supported. For SST, both incremental decoding and non-incremental (allowing re-translation) de- coding (Liu et al., 2020a) are supported, along with stable hypothesis detection methods (Pol\u00e1k et al., 2022). Blockwise attentional decoder mod- els use a label-synchronous beam search or time- synchronous beam search if a CTC branch is available. Blockwise transducer models use time- synchronous beam search. Synthesis & Post-processing For ST, Minimum Bayes Risk (MBR) ensembling (Fernandes et al., 2022) is supported for leveraging quality-metrics (e.g. BLEU) to compare and rank n-best out- puts from one or more models. For S2ST, neu- ral vocoders are supported for both spectral and discrete inputs (Hayashi et al., 2020, 2021). 4 Example Models In this section, we introduce example models which are pre-built in ESPnet-ST-v2 using the neural net- work components described in the previous sec- tion. These examples include state-of-the-art core architectures, as evidenced by prior studies and our performance benchmarking (presented in \u00a75). 4.1 ST Models CTC/Attention (CA) Following Yan et al. (2023), we use Conformer encoders with hierarchi- cal CTC encoding and Transformer decoders. The hierachical CTC encoding, which aligns the first Figure 2: Multi-Decoder CTC/Attention for ST. Figure 3: Time-Sync Blockwise CTC/Attn for SST. N layers of the encoder towards ASR targets and the last M layers towards ST targets, regularizes the final encoder representations to be monotonic with respect to the target. CTC/attention models are jointly decoded using either label-synchronous (wherein the attention branch is primary) or time- synchronous (wherein the CTC branch is primary) beam search. For offline tasks, label-synchrony has shown greater performance (Yan et al., 2023). Multi-Decoder CTC/Attention (MCA) As shown in Figure 2, the Multi-decoder decomposes ST into two sub-tasks, logically corresponding to ASR and MT encoder-decoder models, while main- taining E2E differentiability (Dalmia et al., 2021). This Multi-decoder scheme is also combined with the CTC/attention scheme described in the blurb"}, {"question": " What type of decoding methods are supported for offline Transducer models in ESPnet-ST-v2?", "answer": " For offline Transducer models in ESPnet-ST-v2, original Graves beam search along with time-synchronous and alignment-synchronous beam searches are supported.", "ref_chunk": "Architectures Speech SSL Representations Speech & Text Pre-training Joint Speech/Text Pre-training Simultaneous ST End-to-End Architecture(s) Contextual Block Encoders Blockwise Attn Enc-Dec Blockwise CTC/Attention Blockwise Transducer Wait-K Attn Enc-Dec Monotonic Attn Enc-Dec \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u27131 \u2713 - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - - - - - - - - - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u27133 \u2713 - - - - - - - - - \u2713 - \u2713 - \u2713 \u27133 - - - - - - - - Cascaded Architectures \u2713 - \u2713 \u2713 - \u2713 \u2713 - \u2713 Spec Enc-Dec (Translatotron) \u2713 - \u2713 Spec Multi-Dec (Translatotron 2) \u2713 - Discrete Enc-Dec (Speech-to-Unit) \u2713 \u2713 - \u2713 Discrete Multi-Decoder (UnitY) \u27131 \u2713 - \u27132 \u2713 \u2713 - Offline S2ST End-to-End Architecture(s) - - - - - - Speech SSL Representations Neural Vocoder Support Table 1: Key features of ESPnet-ST-v2 compared to ESPnet-ST-v1 (Inaguma et al., 2020), Fairseq (Wang et al., 2020), and NeurST (Zhao et al., 2021). Com- parison intends to highlight unique features of ESPnet- ST-v2 and not to comprehensively review all toolkits. 1Supports S3PRL (Yang et al., 2021a). 2Supports both spectral and discrete. 3Only supports text-to-text. subsection, are used as building blocks in wrapper classes which are used to construct model architec- tures. Then the fully constructed models are fed to task wrappers which prepare data loaders, initialize models, and handle training/validation. For infer- ence, pythonic APIs invoke search algorithms over the trained models and direct outputs to scoring scripts. For instance, the third-party SimulEval tool for evaluating SST latency (Ma et al., 2020a) is in- tegrated via this API layer. We are also integrating with TorchAudio (Yang et al., 2021b) in the same manner. Finally, recipe scripts define experimental pipelines from data preparation to evaluation. 3.2 Key Features Each of the following modeling components fea- ture a variety of interchangeable approaches. Figure 1: Software architecture of ESPnet-ST-v2. Frontends & Targets Spectral features (e.g. FBANK) and features extracted from speech self- supervised learning (SSL) representations are sup- ported, as well as fusions over multiple features (Berrebbi et al., 2022). For speech SSL features, ESPnet-ST-v2 integrates with the S3PRL toolkit (Yang et al., 2021a). These speech SSL representa- tions are also used to generate discrete targets for S2ST (Lee et al., 2022a). Encoder Architectures Conformer (Gulati et al., 2020; Guo et al., 2021), Branchformer (Peng et al., 2022), EBranchformer (Kim et al., 2023), and Transformer (Vaswani et al., 2017; Karita et al., 2019) encoder architectures are supported for ST and S2ST. For SST, a blockwise scheme is adopted following (Tsunoo et al., 2021; Deng et al., 2022) to form contextual block Conformer and Transformer encoders. Intermediate CTC (Lee and Watanabe, 2021) and Hierachical CTC (Sanabria and Metze, 2018) encoding are also supported; these tech- niques have been shown to stabilize deep encoder optimization (Lee and Watanabe, 2021) and im- prove representations for sequence tasks involving source-to-target re-ordering (Yan et al., 2023). Decoder Architectures Attentional Transformer and recurrent neural network decoders are sup- ported (Karita et al., 2019). Multi-decoder schemes which allow for E2E differentiable decoder cas- cades via searchable hidden intermediates (Dalmia et al., 2021), are also supported; this technique has been shown to improve sequence modeling for tasks which naturally decompose into sub-tasks. Fi- nally, large language model decoders (e.g. mBART (Liu et al., 2020b)) can be adopted through an inte- gration with HuggingFace (Wolf et al., 2020). Loss Functions Cross-entropy (for attentional decoders), CTC, and Transducer are supported for ST and SST. Multi-objective training with CTC/attention and CTC/transducer as well as multi- tasked training (e.g. ASR/MT/ST) is also sup- ported. For S2ST, L1 and mean square error losses are also supported for spectral models. Search Algorithms For offline attentional de- coder models, label-synchronous beam search is supported with optional CTC joint decoding for multi-objective models (Watanabe et al., 2017). For offline Transducer models, the original Graves beam search (Graves, 2012) as well as time- synchronous and alignment-synchronous beam search (Saon et al., 2020) beam searches are supported. For SST, both incremental decoding and non-incremental (allowing re-translation) de- coding (Liu et al., 2020a) are supported, along with stable hypothesis detection methods (Pol\u00e1k et al., 2022). Blockwise attentional decoder mod- els use a label-synchronous beam search or time- synchronous beam search if a CTC branch is available. Blockwise transducer models use time- synchronous beam search. Synthesis & Post-processing For ST, Minimum Bayes Risk (MBR) ensembling (Fernandes et al., 2022) is supported for leveraging quality-metrics (e.g. BLEU) to compare and rank n-best out- puts from one or more models. For S2ST, neu- ral vocoders are supported for both spectral and discrete inputs (Hayashi et al., 2020, 2021). 4 Example Models In this section, we introduce example models which are pre-built in ESPnet-ST-v2 using the neural net- work components described in the previous sec- tion. These examples include state-of-the-art core architectures, as evidenced by prior studies and our performance benchmarking (presented in \u00a75). 4.1 ST Models CTC/Attention (CA) Following Yan et al. (2023), we use Conformer encoders with hierarchi- cal CTC encoding and Transformer decoders. The hierachical CTC encoding, which aligns the first Figure 2: Multi-Decoder CTC/Attention for ST. Figure 3: Time-Sync Blockwise CTC/Attn for SST. N layers of the encoder towards ASR targets and the last M layers towards ST targets, regularizes the final encoder representations to be monotonic with respect to the target. CTC/attention models are jointly decoded using either label-synchronous (wherein the attention branch is primary) or time- synchronous (wherein the CTC branch is primary) beam search. For offline tasks, label-synchrony has shown greater performance (Yan et al., 2023). Multi-Decoder CTC/Attention (MCA) As shown in Figure 2, the Multi-decoder decomposes ST into two sub-tasks, logically corresponding to ASR and MT encoder-decoder models, while main- taining E2E differentiability (Dalmia et al., 2021). This Multi-decoder scheme is also combined with the CTC/attention scheme described in the blurb"}], "doc_text": "Architectures Speech SSL Representations Speech & Text Pre-training Joint Speech/Text Pre-training Simultaneous ST End-to-End Architecture(s) Contextual Block Encoders Blockwise Attn Enc-Dec Blockwise CTC/Attention Blockwise Transducer Wait-K Attn Enc-Dec Monotonic Attn Enc-Dec \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u27131 \u2713 - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - - - - - - - - - - - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u2713 \u2713 \u2713 - - \u2713 \u27133 \u2713 - - - - - - - - - \u2713 - \u2713 - \u2713 \u27133 - - - - - - - - Cascaded Architectures \u2713 - \u2713 \u2713 - \u2713 \u2713 - \u2713 Spec Enc-Dec (Translatotron) \u2713 - \u2713 Spec Multi-Dec (Translatotron 2) \u2713 - Discrete Enc-Dec (Speech-to-Unit) \u2713 \u2713 - \u2713 Discrete Multi-Decoder (UnitY) \u27131 \u2713 - \u27132 \u2713 \u2713 - Offline S2ST End-to-End Architecture(s) - - - - - - Speech SSL Representations Neural Vocoder Support Table 1: Key features of ESPnet-ST-v2 compared to ESPnet-ST-v1 (Inaguma et al., 2020), Fairseq (Wang et al., 2020), and NeurST (Zhao et al., 2021). Com- parison intends to highlight unique features of ESPnet- ST-v2 and not to comprehensively review all toolkits. 1Supports S3PRL (Yang et al., 2021a). 2Supports both spectral and discrete. 3Only supports text-to-text. subsection, are used as building blocks in wrapper classes which are used to construct model architec- tures. Then the fully constructed models are fed to task wrappers which prepare data loaders, initialize models, and handle training/validation. For infer- ence, pythonic APIs invoke search algorithms over the trained models and direct outputs to scoring scripts. For instance, the third-party SimulEval tool for evaluating SST latency (Ma et al., 2020a) is in- tegrated via this API layer. We are also integrating with TorchAudio (Yang et al., 2021b) in the same manner. Finally, recipe scripts define experimental pipelines from data preparation to evaluation. 3.2 Key Features Each of the following modeling components fea- ture a variety of interchangeable approaches. Figure 1: Software architecture of ESPnet-ST-v2. Frontends & Targets Spectral features (e.g. FBANK) and features extracted from speech self- supervised learning (SSL) representations are sup- ported, as well as fusions over multiple features (Berrebbi et al., 2022). For speech SSL features, ESPnet-ST-v2 integrates with the S3PRL toolkit (Yang et al., 2021a). These speech SSL representa- tions are also used to generate discrete targets for S2ST (Lee et al., 2022a). Encoder Architectures Conformer (Gulati et al., 2020; Guo et al., 2021), Branchformer (Peng et al., 2022), EBranchformer (Kim et al., 2023), and Transformer (Vaswani et al., 2017; Karita et al., 2019) encoder architectures are supported for ST and S2ST. For SST, a blockwise scheme is adopted following (Tsunoo et al., 2021; Deng et al., 2022) to form contextual block Conformer and Transformer encoders. Intermediate CTC (Lee and Watanabe, 2021) and Hierachical CTC (Sanabria and Metze, 2018) encoding are also supported; these tech- niques have been shown to stabilize deep encoder optimization (Lee and Watanabe, 2021) and im- prove representations for sequence tasks involving source-to-target re-ordering (Yan et al., 2023). Decoder Architectures Attentional Transformer and recurrent neural network decoders are sup- ported (Karita et al., 2019). Multi-decoder schemes which allow for E2E differentiable decoder cas- cades via searchable hidden intermediates (Dalmia et al., 2021), are also supported; this technique has been shown to improve sequence modeling for tasks which naturally decompose into sub-tasks. Fi- nally, large language model decoders (e.g. mBART (Liu et al., 2020b)) can be adopted through an inte- gration with HuggingFace (Wolf et al., 2020). Loss Functions Cross-entropy (for attentional decoders), CTC, and Transducer are supported for ST and SST. Multi-objective training with CTC/attention and CTC/transducer as well as multi- tasked training (e.g. ASR/MT/ST) is also sup- ported. For S2ST, L1 and mean square error losses are also supported for spectral models. Search Algorithms For offline attentional de- coder models, label-synchronous beam search is supported with optional CTC joint decoding for multi-objective models (Watanabe et al., 2017). For offline Transducer models, the original Graves beam search (Graves, 2012) as well as time- synchronous and alignment-synchronous beam search (Saon et al., 2020) beam searches are supported. For SST, both incremental decoding and non-incremental (allowing re-translation) de- coding (Liu et al., 2020a) are supported, along with stable hypothesis detection methods (Pol\u00e1k et al., 2022). Blockwise attentional decoder mod- els use a label-synchronous beam search or time- synchronous beam search if a CTC branch is available. Blockwise transducer models use time- synchronous beam search. Synthesis & Post-processing For ST, Minimum Bayes Risk (MBR) ensembling (Fernandes et al., 2022) is supported for leveraging quality-metrics (e.g. BLEU) to compare and rank n-best out- puts from one or more models. For S2ST, neu- ral vocoders are supported for both spectral and discrete inputs (Hayashi et al., 2020, 2021). 4 Example Models In this section, we introduce example models which are pre-built in ESPnet-ST-v2 using the neural net- work components described in the previous sec- tion. These examples include state-of-the-art core architectures, as evidenced by prior studies and our performance benchmarking (presented in \u00a75). 4.1 ST Models CTC/Attention (CA) Following Yan et al. (2023), we use Conformer encoders with hierarchi- cal CTC encoding and Transformer decoders. The hierachical CTC encoding, which aligns the first Figure 2: Multi-Decoder CTC/Attention for ST. Figure 3: Time-Sync Blockwise CTC/Attn for SST. N layers of the encoder towards ASR targets and the last M layers towards ST targets, regularizes the final encoder representations to be monotonic with respect to the target. CTC/attention models are jointly decoded using either label-synchronous (wherein the attention branch is primary) or time- synchronous (wherein the CTC branch is primary) beam search. For offline tasks, label-synchrony has shown greater performance (Yan et al., 2023). Multi-Decoder CTC/Attention (MCA) As shown in Figure 2, the Multi-decoder decomposes ST into two sub-tasks, logically corresponding to ASR and MT encoder-decoder models, while main- taining E2E differentiability (Dalmia et al., 2021). This Multi-decoder scheme is also combined with the CTC/attention scheme described in the blurb"}