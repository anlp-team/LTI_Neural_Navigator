{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Data-efficient_Active_Learning_for_Structured_Prediction_with_Partial_Annotation_and_Self-Training_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the batch size used for sentence selection?", "answer": " 4K tokens", "ref_chunk": "batch size (for sentence selection) to 4K tokens, which roughly corresponds to 2% of the total pool size for most of the datasets we use. The initial seed training set and the development set are randomly sampled (with FA) using this batch size. Unless otherwise noted, we run 14 AL cycles for each ex- periment. In each AL cycle, we re-train our model since we find incremental updating does not per- form well. Following most AL work, annotation is simulated by checking and assigning the labels from the original dataset. In FA, we annotate all the sub-structures for the selected sentences. In PA, we first decide the selection ratio and apply it to the selected sentences. We further adopt a heuristic7 that selects the union of sentence-wise uncertain sub-structures as well as global ones since both may contain informative sub-structures. Finally, all the presented results are averaged over five runs with different random seeds. Model and training. For the models, we adopt standard architectures by stacking task-specific structured predictors over pre-trained RoBERTabase (Liu et al., 2019) and the full models are fine-tuned at each training iteration. After obtaining new an- notations in each AL cycle, we first train a model based on all the available full or partial annota- tions. When using self-training, we further ap- ply this newly trained model to assign pseudo soft labels to all un-annotated instances and combine them with the existing annotations to train another model. Compared to using the old model from the last AL cycle, this strategy can give more accu- rate pseudo labels since the newly updated model usually performs better by learning from more an- notations. For PA, pseudo soft labels are assigned to both un-selected sentences and the un-annotated sub-structures in the selected sentences. 7This heuristic will increase the actual selecting ratio, but it will only be slightly larger since there are large overlaps be- tween sentence-wise and global highly-ranked sub-structures. 3.2 Comparison Scheme Since FA and PA annotate at different granularities, we need a common cost measurement to compare their effectiveness properly. A reasonable metric is the number of the labeled sub-structures; for instance, the number of labeled tokens for sequence labeling or edges for dependency parsing. This metric is commonly adopted in previous PA work (Tomanek and Hahn, 2009; Flannery and Mori, 2015; Li et al., 2016; Radmard et al., 2021). Nevertheless, evaluating only by sub-structures ignores a crucial hidden cost: The reading time of the contexts. For example, in sequence labeling with PA, although not every token in the sentence needs to be tagged, the annotator may still need to read the whole sentence to understand its meaning. Therefore, if performing comparisons only by the amount of annotated sub-structures, it will be unfair for the FA baseline because more contexts must be read to carry out PA. In this work, we adopt a simple two-facet com- parison scheme that considers both reading and labeling costs. We first control the reading cost by choosing the same size of contexts in the sen- tence selection step of each AL cycle (Line 4 in Algorithm 1). Then, we further compare by the sub-structure labeling cost, measured by the sub- structure annotation cost. If PA can roughly reach the FA performance with the same reading cost but fewer sub-structures annotated, it would be fair to say that PA can help reduce cost over FA. A better comparing scheme should evaluate against a uni- fied estimation of the real annotation costs (Settles et al., 2008). This usually requires actual annota- tion exercises rather than simulations, which we leave to future work. 3.3 NER and DPAR Settings. We compare primarily three strategies: FA, PA, and a baseline where randomly selected sentences are fully annotated (Rand). We also in- clude a supervised result (Super.) which is obtained from a model trained with the full original training set. We measure reading cost by the total number of tokens in the selected sentences. For labeling cost, we further adopt metrics with practical consid- erations. In NER, lots of tokens, such as functional words, can be easily judged as the \u2018O\u2019 (non-entity) tag. To avoid over-estimating the costs of such easy tokens for FA, we filter tokens by their part- of-speech (POS) tags and only count the ones that \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000)\u0000$ \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001a \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000\u0019 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00003\u0000$ \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u0000\u001b\u0000\u001b \u0000\u001b\u0000\u001a \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u0017 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000)\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$ \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000)\u0000$ \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u001b Figure 2: Comparisons according to reading and labeling cost. Each node indicates one AL cycle. For x-axis, reading cost (left) is measured by token numbers, while labeling cost (right) is task-specific (\u00a73.3). NER is evaluated with labeled F1 scores on CoNLL-2003, while DPAR is with LAS scores on UD-EWT. Results are averaged over five runs with different seeds, and the shaded areas indicate standard deviations. The overall unlabeled pool contains around 200K tokens. Using AL, good performance can be obtained with less than 30% (60K) annotated. are likely to be inside an entity mention.8 For PA, we still count every queried token. For the task of DPAR, similarly, different dependency links can have variant annotation difficulties. We utilize the surface distance between the head and modifier of the dependency edge as the measure of label- ing cost, considering that the decisions for longer dependencies are usually harder. Main Results. The main test results are shown in Figure 2, where the patterns on both tasks are similar. First, AL brings clear improvements over the random baseline and can roughly reach the fully supervised performance with only a small portion of data annotated (around 18% for"}, {"question": " How many AL cycles are run for each experiment?", "answer": " 14", "ref_chunk": "batch size (for sentence selection) to 4K tokens, which roughly corresponds to 2% of the total pool size for most of the datasets we use. The initial seed training set and the development set are randomly sampled (with FA) using this batch size. Unless otherwise noted, we run 14 AL cycles for each ex- periment. In each AL cycle, we re-train our model since we find incremental updating does not per- form well. Following most AL work, annotation is simulated by checking and assigning the labels from the original dataset. In FA, we annotate all the sub-structures for the selected sentences. In PA, we first decide the selection ratio and apply it to the selected sentences. We further adopt a heuristic7 that selects the union of sentence-wise uncertain sub-structures as well as global ones since both may contain informative sub-structures. Finally, all the presented results are averaged over five runs with different random seeds. Model and training. For the models, we adopt standard architectures by stacking task-specific structured predictors over pre-trained RoBERTabase (Liu et al., 2019) and the full models are fine-tuned at each training iteration. After obtaining new an- notations in each AL cycle, we first train a model based on all the available full or partial annota- tions. When using self-training, we further ap- ply this newly trained model to assign pseudo soft labels to all un-annotated instances and combine them with the existing annotations to train another model. Compared to using the old model from the last AL cycle, this strategy can give more accu- rate pseudo labels since the newly updated model usually performs better by learning from more an- notations. For PA, pseudo soft labels are assigned to both un-selected sentences and the un-annotated sub-structures in the selected sentences. 7This heuristic will increase the actual selecting ratio, but it will only be slightly larger since there are large overlaps be- tween sentence-wise and global highly-ranked sub-structures. 3.2 Comparison Scheme Since FA and PA annotate at different granularities, we need a common cost measurement to compare their effectiveness properly. A reasonable metric is the number of the labeled sub-structures; for instance, the number of labeled tokens for sequence labeling or edges for dependency parsing. This metric is commonly adopted in previous PA work (Tomanek and Hahn, 2009; Flannery and Mori, 2015; Li et al., 2016; Radmard et al., 2021). Nevertheless, evaluating only by sub-structures ignores a crucial hidden cost: The reading time of the contexts. For example, in sequence labeling with PA, although not every token in the sentence needs to be tagged, the annotator may still need to read the whole sentence to understand its meaning. Therefore, if performing comparisons only by the amount of annotated sub-structures, it will be unfair for the FA baseline because more contexts must be read to carry out PA. In this work, we adopt a simple two-facet com- parison scheme that considers both reading and labeling costs. We first control the reading cost by choosing the same size of contexts in the sen- tence selection step of each AL cycle (Line 4 in Algorithm 1). Then, we further compare by the sub-structure labeling cost, measured by the sub- structure annotation cost. If PA can roughly reach the FA performance with the same reading cost but fewer sub-structures annotated, it would be fair to say that PA can help reduce cost over FA. A better comparing scheme should evaluate against a uni- fied estimation of the real annotation costs (Settles et al., 2008). This usually requires actual annota- tion exercises rather than simulations, which we leave to future work. 3.3 NER and DPAR Settings. We compare primarily three strategies: FA, PA, and a baseline where randomly selected sentences are fully annotated (Rand). We also in- clude a supervised result (Super.) which is obtained from a model trained with the full original training set. We measure reading cost by the total number of tokens in the selected sentences. For labeling cost, we further adopt metrics with practical consid- erations. In NER, lots of tokens, such as functional words, can be easily judged as the \u2018O\u2019 (non-entity) tag. To avoid over-estimating the costs of such easy tokens for FA, we filter tokens by their part- of-speech (POS) tags and only count the ones that \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000)\u0000$ \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001a \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000\u0019 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00003\u0000$ \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u0000\u001b\u0000\u001b \u0000\u001b\u0000\u001a \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u0017 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000)\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$ \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000)\u0000$ \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u001b Figure 2: Comparisons according to reading and labeling cost. Each node indicates one AL cycle. For x-axis, reading cost (left) is measured by token numbers, while labeling cost (right) is task-specific (\u00a73.3). NER is evaluated with labeled F1 scores on CoNLL-2003, while DPAR is with LAS scores on UD-EWT. Results are averaged over five runs with different seeds, and the shaded areas indicate standard deviations. The overall unlabeled pool contains around 200K tokens. Using AL, good performance can be obtained with less than 30% (60K) annotated. are likely to be inside an entity mention.8 For PA, we still count every queried token. For the task of DPAR, similarly, different dependency links can have variant annotation difficulties. We utilize the surface distance between the head and modifier of the dependency edge as the measure of label- ing cost, considering that the decisions for longer dependencies are usually harder. Main Results. The main test results are shown in Figure 2, where the patterns on both tasks are similar. First, AL brings clear improvements over the random baseline and can roughly reach the fully supervised performance with only a small portion of data annotated (around 18% for"}, {"question": " What kind of annotations are simulated by checking and assigning labels from the original dataset?", "answer": " Annotation", "ref_chunk": "batch size (for sentence selection) to 4K tokens, which roughly corresponds to 2% of the total pool size for most of the datasets we use. The initial seed training set and the development set are randomly sampled (with FA) using this batch size. Unless otherwise noted, we run 14 AL cycles for each ex- periment. In each AL cycle, we re-train our model since we find incremental updating does not per- form well. Following most AL work, annotation is simulated by checking and assigning the labels from the original dataset. In FA, we annotate all the sub-structures for the selected sentences. In PA, we first decide the selection ratio and apply it to the selected sentences. We further adopt a heuristic7 that selects the union of sentence-wise uncertain sub-structures as well as global ones since both may contain informative sub-structures. Finally, all the presented results are averaged over five runs with different random seeds. Model and training. For the models, we adopt standard architectures by stacking task-specific structured predictors over pre-trained RoBERTabase (Liu et al., 2019) and the full models are fine-tuned at each training iteration. After obtaining new an- notations in each AL cycle, we first train a model based on all the available full or partial annota- tions. When using self-training, we further ap- ply this newly trained model to assign pseudo soft labels to all un-annotated instances and combine them with the existing annotations to train another model. Compared to using the old model from the last AL cycle, this strategy can give more accu- rate pseudo labels since the newly updated model usually performs better by learning from more an- notations. For PA, pseudo soft labels are assigned to both un-selected sentences and the un-annotated sub-structures in the selected sentences. 7This heuristic will increase the actual selecting ratio, but it will only be slightly larger since there are large overlaps be- tween sentence-wise and global highly-ranked sub-structures. 3.2 Comparison Scheme Since FA and PA annotate at different granularities, we need a common cost measurement to compare their effectiveness properly. A reasonable metric is the number of the labeled sub-structures; for instance, the number of labeled tokens for sequence labeling or edges for dependency parsing. This metric is commonly adopted in previous PA work (Tomanek and Hahn, 2009; Flannery and Mori, 2015; Li et al., 2016; Radmard et al., 2021). Nevertheless, evaluating only by sub-structures ignores a crucial hidden cost: The reading time of the contexts. For example, in sequence labeling with PA, although not every token in the sentence needs to be tagged, the annotator may still need to read the whole sentence to understand its meaning. Therefore, if performing comparisons only by the amount of annotated sub-structures, it will be unfair for the FA baseline because more contexts must be read to carry out PA. In this work, we adopt a simple two-facet com- parison scheme that considers both reading and labeling costs. We first control the reading cost by choosing the same size of contexts in the sen- tence selection step of each AL cycle (Line 4 in Algorithm 1). Then, we further compare by the sub-structure labeling cost, measured by the sub- structure annotation cost. If PA can roughly reach the FA performance with the same reading cost but fewer sub-structures annotated, it would be fair to say that PA can help reduce cost over FA. A better comparing scheme should evaluate against a uni- fied estimation of the real annotation costs (Settles et al., 2008). This usually requires actual annota- tion exercises rather than simulations, which we leave to future work. 3.3 NER and DPAR Settings. We compare primarily three strategies: FA, PA, and a baseline where randomly selected sentences are fully annotated (Rand). We also in- clude a supervised result (Super.) which is obtained from a model trained with the full original training set. We measure reading cost by the total number of tokens in the selected sentences. For labeling cost, we further adopt metrics with practical consid- erations. In NER, lots of tokens, such as functional words, can be easily judged as the \u2018O\u2019 (non-entity) tag. To avoid over-estimating the costs of such easy tokens for FA, we filter tokens by their part- of-speech (POS) tags and only count the ones that \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000)\u0000$ \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001a \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000\u0019 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00003\u0000$ \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u0000\u001b\u0000\u001b \u0000\u001b\u0000\u001a \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u0017 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000)\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$ \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000)\u0000$ \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u001b Figure 2: Comparisons according to reading and labeling cost. Each node indicates one AL cycle. For x-axis, reading cost (left) is measured by token numbers, while labeling cost (right) is task-specific (\u00a73.3). NER is evaluated with labeled F1 scores on CoNLL-2003, while DPAR is with LAS scores on UD-EWT. Results are averaged over five runs with different seeds, and the shaded areas indicate standard deviations. The overall unlabeled pool contains around 200K tokens. Using AL, good performance can be obtained with less than 30% (60K) annotated. are likely to be inside an entity mention.8 For PA, we still count every queried token. For the task of DPAR, similarly, different dependency links can have variant annotation difficulties. We utilize the surface distance between the head and modifier of the dependency edge as the measure of label- ing cost, considering that the decisions for longer dependencies are usually harder. Main Results. The main test results are shown in Figure 2, where the patterns on both tasks are similar. First, AL brings clear improvements over the random baseline and can roughly reach the fully supervised performance with only a small portion of data annotated (around 18% for"}, {"question": " What heuristic is adopted for selecting sub-structures in FA and PA?", "answer": " Union of sentence-wise uncertain sub-structures as well as global ones", "ref_chunk": "batch size (for sentence selection) to 4K tokens, which roughly corresponds to 2% of the total pool size for most of the datasets we use. The initial seed training set and the development set are randomly sampled (with FA) using this batch size. Unless otherwise noted, we run 14 AL cycles for each ex- periment. In each AL cycle, we re-train our model since we find incremental updating does not per- form well. Following most AL work, annotation is simulated by checking and assigning the labels from the original dataset. In FA, we annotate all the sub-structures for the selected sentences. In PA, we first decide the selection ratio and apply it to the selected sentences. We further adopt a heuristic7 that selects the union of sentence-wise uncertain sub-structures as well as global ones since both may contain informative sub-structures. Finally, all the presented results are averaged over five runs with different random seeds. Model and training. For the models, we adopt standard architectures by stacking task-specific structured predictors over pre-trained RoBERTabase (Liu et al., 2019) and the full models are fine-tuned at each training iteration. After obtaining new an- notations in each AL cycle, we first train a model based on all the available full or partial annota- tions. When using self-training, we further ap- ply this newly trained model to assign pseudo soft labels to all un-annotated instances and combine them with the existing annotations to train another model. Compared to using the old model from the last AL cycle, this strategy can give more accu- rate pseudo labels since the newly updated model usually performs better by learning from more an- notations. For PA, pseudo soft labels are assigned to both un-selected sentences and the un-annotated sub-structures in the selected sentences. 7This heuristic will increase the actual selecting ratio, but it will only be slightly larger since there are large overlaps be- tween sentence-wise and global highly-ranked sub-structures. 3.2 Comparison Scheme Since FA and PA annotate at different granularities, we need a common cost measurement to compare their effectiveness properly. A reasonable metric is the number of the labeled sub-structures; for instance, the number of labeled tokens for sequence labeling or edges for dependency parsing. This metric is commonly adopted in previous PA work (Tomanek and Hahn, 2009; Flannery and Mori, 2015; Li et al., 2016; Radmard et al., 2021). Nevertheless, evaluating only by sub-structures ignores a crucial hidden cost: The reading time of the contexts. For example, in sequence labeling with PA, although not every token in the sentence needs to be tagged, the annotator may still need to read the whole sentence to understand its meaning. Therefore, if performing comparisons only by the amount of annotated sub-structures, it will be unfair for the FA baseline because more contexts must be read to carry out PA. In this work, we adopt a simple two-facet com- parison scheme that considers both reading and labeling costs. We first control the reading cost by choosing the same size of contexts in the sen- tence selection step of each AL cycle (Line 4 in Algorithm 1). Then, we further compare by the sub-structure labeling cost, measured by the sub- structure annotation cost. If PA can roughly reach the FA performance with the same reading cost but fewer sub-structures annotated, it would be fair to say that PA can help reduce cost over FA. A better comparing scheme should evaluate against a uni- fied estimation of the real annotation costs (Settles et al., 2008). This usually requires actual annota- tion exercises rather than simulations, which we leave to future work. 3.3 NER and DPAR Settings. We compare primarily three strategies: FA, PA, and a baseline where randomly selected sentences are fully annotated (Rand). We also in- clude a supervised result (Super.) which is obtained from a model trained with the full original training set. We measure reading cost by the total number of tokens in the selected sentences. For labeling cost, we further adopt metrics with practical consid- erations. In NER, lots of tokens, such as functional words, can be easily judged as the \u2018O\u2019 (non-entity) tag. To avoid over-estimating the costs of such easy tokens for FA, we filter tokens by their part- of-speech (POS) tags and only count the ones that \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000)\u0000$ \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001a \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000\u0019 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00003\u0000$ \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u0000\u001b\u0000\u001b \u0000\u001b\u0000\u001a \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u0017 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000)\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$ \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000)\u0000$ \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u001b Figure 2: Comparisons according to reading and labeling cost. Each node indicates one AL cycle. For x-axis, reading cost (left) is measured by token numbers, while labeling cost (right) is task-specific (\u00a73.3). NER is evaluated with labeled F1 scores on CoNLL-2003, while DPAR is with LAS scores on UD-EWT. Results are averaged over five runs with different seeds, and the shaded areas indicate standard deviations. The overall unlabeled pool contains around 200K tokens. Using AL, good performance can be obtained with less than 30% (60K) annotated. are likely to be inside an entity mention.8 For PA, we still count every queried token. For the task of DPAR, similarly, different dependency links can have variant annotation difficulties. We utilize the surface distance between the head and modifier of the dependency edge as the measure of label- ing cost, considering that the decisions for longer dependencies are usually harder. Main Results. The main test results are shown in Figure 2, where the patterns on both tasks are similar. First, AL brings clear improvements over the random baseline and can roughly reach the fully supervised performance with only a small portion of data annotated (around 18% for"}, {"question": " How many runs are the presented results averaged over?", "answer": " Five", "ref_chunk": "batch size (for sentence selection) to 4K tokens, which roughly corresponds to 2% of the total pool size for most of the datasets we use. The initial seed training set and the development set are randomly sampled (with FA) using this batch size. Unless otherwise noted, we run 14 AL cycles for each ex- periment. In each AL cycle, we re-train our model since we find incremental updating does not per- form well. Following most AL work, annotation is simulated by checking and assigning the labels from the original dataset. In FA, we annotate all the sub-structures for the selected sentences. In PA, we first decide the selection ratio and apply it to the selected sentences. We further adopt a heuristic7 that selects the union of sentence-wise uncertain sub-structures as well as global ones since both may contain informative sub-structures. Finally, all the presented results are averaged over five runs with different random seeds. Model and training. For the models, we adopt standard architectures by stacking task-specific structured predictors over pre-trained RoBERTabase (Liu et al., 2019) and the full models are fine-tuned at each training iteration. After obtaining new an- notations in each AL cycle, we first train a model based on all the available full or partial annota- tions. When using self-training, we further ap- ply this newly trained model to assign pseudo soft labels to all un-annotated instances and combine them with the existing annotations to train another model. Compared to using the old model from the last AL cycle, this strategy can give more accu- rate pseudo labels since the newly updated model usually performs better by learning from more an- notations. For PA, pseudo soft labels are assigned to both un-selected sentences and the un-annotated sub-structures in the selected sentences. 7This heuristic will increase the actual selecting ratio, but it will only be slightly larger since there are large overlaps be- tween sentence-wise and global highly-ranked sub-structures. 3.2 Comparison Scheme Since FA and PA annotate at different granularities, we need a common cost measurement to compare their effectiveness properly. A reasonable metric is the number of the labeled sub-structures; for instance, the number of labeled tokens for sequence labeling or edges for dependency parsing. This metric is commonly adopted in previous PA work (Tomanek and Hahn, 2009; Flannery and Mori, 2015; Li et al., 2016; Radmard et al., 2021). Nevertheless, evaluating only by sub-structures ignores a crucial hidden cost: The reading time of the contexts. For example, in sequence labeling with PA, although not every token in the sentence needs to be tagged, the annotator may still need to read the whole sentence to understand its meaning. Therefore, if performing comparisons only by the amount of annotated sub-structures, it will be unfair for the FA baseline because more contexts must be read to carry out PA. In this work, we adopt a simple two-facet com- parison scheme that considers both reading and labeling costs. We first control the reading cost by choosing the same size of contexts in the sen- tence selection step of each AL cycle (Line 4 in Algorithm 1). Then, we further compare by the sub-structure labeling cost, measured by the sub- structure annotation cost. If PA can roughly reach the FA performance with the same reading cost but fewer sub-structures annotated, it would be fair to say that PA can help reduce cost over FA. A better comparing scheme should evaluate against a uni- fied estimation of the real annotation costs (Settles et al., 2008). This usually requires actual annota- tion exercises rather than simulations, which we leave to future work. 3.3 NER and DPAR Settings. We compare primarily three strategies: FA, PA, and a baseline where randomly selected sentences are fully annotated (Rand). We also in- clude a supervised result (Super.) which is obtained from a model trained with the full original training set. We measure reading cost by the total number of tokens in the selected sentences. For labeling cost, we further adopt metrics with practical consid- erations. In NER, lots of tokens, such as functional words, can be easily judged as the \u2018O\u2019 (non-entity) tag. To avoid over-estimating the costs of such easy tokens for FA, we filter tokens by their part- of-speech (POS) tags and only count the ones that \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000)\u0000$ \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001a \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000\u0019 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00003\u0000$ \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u0000\u001b\u0000\u001b \u0000\u001b\u0000\u001a \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u0017 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000)\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$ \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000)\u0000$ \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u001b Figure 2: Comparisons according to reading and labeling cost. Each node indicates one AL cycle. For x-axis, reading cost (left) is measured by token numbers, while labeling cost (right) is task-specific (\u00a73.3). NER is evaluated with labeled F1 scores on CoNLL-2003, while DPAR is with LAS scores on UD-EWT. Results are averaged over five runs with different seeds, and the shaded areas indicate standard deviations. The overall unlabeled pool contains around 200K tokens. Using AL, good performance can be obtained with less than 30% (60K) annotated. are likely to be inside an entity mention.8 For PA, we still count every queried token. For the task of DPAR, similarly, different dependency links can have variant annotation difficulties. We utilize the surface distance between the head and modifier of the dependency edge as the measure of label- ing cost, considering that the decisions for longer dependencies are usually harder. Main Results. The main test results are shown in Figure 2, where the patterns on both tasks are similar. First, AL brings clear improvements over the random baseline and can roughly reach the fully supervised performance with only a small portion of data annotated (around 18% for"}, {"question": " What kind of labels are assigned to un-annotated instances when using self-training?", "answer": " Pseudo soft labels", "ref_chunk": "batch size (for sentence selection) to 4K tokens, which roughly corresponds to 2% of the total pool size for most of the datasets we use. The initial seed training set and the development set are randomly sampled (with FA) using this batch size. Unless otherwise noted, we run 14 AL cycles for each ex- periment. In each AL cycle, we re-train our model since we find incremental updating does not per- form well. Following most AL work, annotation is simulated by checking and assigning the labels from the original dataset. In FA, we annotate all the sub-structures for the selected sentences. In PA, we first decide the selection ratio and apply it to the selected sentences. We further adopt a heuristic7 that selects the union of sentence-wise uncertain sub-structures as well as global ones since both may contain informative sub-structures. Finally, all the presented results are averaged over five runs with different random seeds. Model and training. For the models, we adopt standard architectures by stacking task-specific structured predictors over pre-trained RoBERTabase (Liu et al., 2019) and the full models are fine-tuned at each training iteration. After obtaining new an- notations in each AL cycle, we first train a model based on all the available full or partial annota- tions. When using self-training, we further ap- ply this newly trained model to assign pseudo soft labels to all un-annotated instances and combine them with the existing annotations to train another model. Compared to using the old model from the last AL cycle, this strategy can give more accu- rate pseudo labels since the newly updated model usually performs better by learning from more an- notations. For PA, pseudo soft labels are assigned to both un-selected sentences and the un-annotated sub-structures in the selected sentences. 7This heuristic will increase the actual selecting ratio, but it will only be slightly larger since there are large overlaps be- tween sentence-wise and global highly-ranked sub-structures. 3.2 Comparison Scheme Since FA and PA annotate at different granularities, we need a common cost measurement to compare their effectiveness properly. A reasonable metric is the number of the labeled sub-structures; for instance, the number of labeled tokens for sequence labeling or edges for dependency parsing. This metric is commonly adopted in previous PA work (Tomanek and Hahn, 2009; Flannery and Mori, 2015; Li et al., 2016; Radmard et al., 2021). Nevertheless, evaluating only by sub-structures ignores a crucial hidden cost: The reading time of the contexts. For example, in sequence labeling with PA, although not every token in the sentence needs to be tagged, the annotator may still need to read the whole sentence to understand its meaning. Therefore, if performing comparisons only by the amount of annotated sub-structures, it will be unfair for the FA baseline because more contexts must be read to carry out PA. In this work, we adopt a simple two-facet com- parison scheme that considers both reading and labeling costs. We first control the reading cost by choosing the same size of contexts in the sen- tence selection step of each AL cycle (Line 4 in Algorithm 1). Then, we further compare by the sub-structure labeling cost, measured by the sub- structure annotation cost. If PA can roughly reach the FA performance with the same reading cost but fewer sub-structures annotated, it would be fair to say that PA can help reduce cost over FA. A better comparing scheme should evaluate against a uni- fied estimation of the real annotation costs (Settles et al., 2008). This usually requires actual annota- tion exercises rather than simulations, which we leave to future work. 3.3 NER and DPAR Settings. We compare primarily three strategies: FA, PA, and a baseline where randomly selected sentences are fully annotated (Rand). We also in- clude a supervised result (Super.) which is obtained from a model trained with the full original training set. We measure reading cost by the total number of tokens in the selected sentences. For labeling cost, we further adopt metrics with practical consid- erations. In NER, lots of tokens, such as functional words, can be easily judged as the \u2018O\u2019 (non-entity) tag. To avoid over-estimating the costs of such easy tokens for FA, we filter tokens by their part- of-speech (POS) tags and only count the ones that \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000)\u0000$ \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001a \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000\u0019 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00003\u0000$ \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u0000\u001b\u0000\u001b \u0000\u001b\u0000\u001a \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u0017 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000)\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$ \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000)\u0000$ \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u001b Figure 2: Comparisons according to reading and labeling cost. Each node indicates one AL cycle. For x-axis, reading cost (left) is measured by token numbers, while labeling cost (right) is task-specific (\u00a73.3). NER is evaluated with labeled F1 scores on CoNLL-2003, while DPAR is with LAS scores on UD-EWT. Results are averaged over five runs with different seeds, and the shaded areas indicate standard deviations. The overall unlabeled pool contains around 200K tokens. Using AL, good performance can be obtained with less than 30% (60K) annotated. are likely to be inside an entity mention.8 For PA, we still count every queried token. For the task of DPAR, similarly, different dependency links can have variant annotation difficulties. We utilize the surface distance between the head and modifier of the dependency edge as the measure of label- ing cost, considering that the decisions for longer dependencies are usually harder. Main Results. The main test results are shown in Figure 2, where the patterns on both tasks are similar. First, AL brings clear improvements over the random baseline and can roughly reach the fully supervised performance with only a small portion of data annotated (around 18% for"}, {"question": " What metric is commonly adopted in previous PA work for comparison?", "answer": " Number of labeled sub-structures", "ref_chunk": "batch size (for sentence selection) to 4K tokens, which roughly corresponds to 2% of the total pool size for most of the datasets we use. The initial seed training set and the development set are randomly sampled (with FA) using this batch size. Unless otherwise noted, we run 14 AL cycles for each ex- periment. In each AL cycle, we re-train our model since we find incremental updating does not per- form well. Following most AL work, annotation is simulated by checking and assigning the labels from the original dataset. In FA, we annotate all the sub-structures for the selected sentences. In PA, we first decide the selection ratio and apply it to the selected sentences. We further adopt a heuristic7 that selects the union of sentence-wise uncertain sub-structures as well as global ones since both may contain informative sub-structures. Finally, all the presented results are averaged over five runs with different random seeds. Model and training. For the models, we adopt standard architectures by stacking task-specific structured predictors over pre-trained RoBERTabase (Liu et al., 2019) and the full models are fine-tuned at each training iteration. After obtaining new an- notations in each AL cycle, we first train a model based on all the available full or partial annota- tions. When using self-training, we further ap- ply this newly trained model to assign pseudo soft labels to all un-annotated instances and combine them with the existing annotations to train another model. Compared to using the old model from the last AL cycle, this strategy can give more accu- rate pseudo labels since the newly updated model usually performs better by learning from more an- notations. For PA, pseudo soft labels are assigned to both un-selected sentences and the un-annotated sub-structures in the selected sentences. 7This heuristic will increase the actual selecting ratio, but it will only be slightly larger since there are large overlaps be- tween sentence-wise and global highly-ranked sub-structures. 3.2 Comparison Scheme Since FA and PA annotate at different granularities, we need a common cost measurement to compare their effectiveness properly. A reasonable metric is the number of the labeled sub-structures; for instance, the number of labeled tokens for sequence labeling or edges for dependency parsing. This metric is commonly adopted in previous PA work (Tomanek and Hahn, 2009; Flannery and Mori, 2015; Li et al., 2016; Radmard et al., 2021). Nevertheless, evaluating only by sub-structures ignores a crucial hidden cost: The reading time of the contexts. For example, in sequence labeling with PA, although not every token in the sentence needs to be tagged, the annotator may still need to read the whole sentence to understand its meaning. Therefore, if performing comparisons only by the amount of annotated sub-structures, it will be unfair for the FA baseline because more contexts must be read to carry out PA. In this work, we adopt a simple two-facet com- parison scheme that considers both reading and labeling costs. We first control the reading cost by choosing the same size of contexts in the sen- tence selection step of each AL cycle (Line 4 in Algorithm 1). Then, we further compare by the sub-structure labeling cost, measured by the sub- structure annotation cost. If PA can roughly reach the FA performance with the same reading cost but fewer sub-structures annotated, it would be fair to say that PA can help reduce cost over FA. A better comparing scheme should evaluate against a uni- fied estimation of the real annotation costs (Settles et al., 2008). This usually requires actual annota- tion exercises rather than simulations, which we leave to future work. 3.3 NER and DPAR Settings. We compare primarily three strategies: FA, PA, and a baseline where randomly selected sentences are fully annotated (Rand). We also in- clude a supervised result (Super.) which is obtained from a model trained with the full original training set. We measure reading cost by the total number of tokens in the selected sentences. For labeling cost, we further adopt metrics with practical consid- erations. In NER, lots of tokens, such as functional words, can be easily judged as the \u2018O\u2019 (non-entity) tag. To avoid over-estimating the costs of such easy tokens for FA, we filter tokens by their part- of-speech (POS) tags and only count the ones that \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000)\u0000$ \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001a \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000\u0019 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00003\u0000$ \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u0000\u001b\u0000\u001b \u0000\u001b\u0000\u001a \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u0017 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000)\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$ \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000)\u0000$ \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u001b Figure 2: Comparisons according to reading and labeling cost. Each node indicates one AL cycle. For x-axis, reading cost (left) is measured by token numbers, while labeling cost (right) is task-specific (\u00a73.3). NER is evaluated with labeled F1 scores on CoNLL-2003, while DPAR is with LAS scores on UD-EWT. Results are averaged over five runs with different seeds, and the shaded areas indicate standard deviations. The overall unlabeled pool contains around 200K tokens. Using AL, good performance can be obtained with less than 30% (60K) annotated. are likely to be inside an entity mention.8 For PA, we still count every queried token. For the task of DPAR, similarly, different dependency links can have variant annotation difficulties. We utilize the surface distance between the head and modifier of the dependency edge as the measure of label- ing cost, considering that the decisions for longer dependencies are usually harder. Main Results. The main test results are shown in Figure 2, where the patterns on both tasks are similar. First, AL brings clear improvements over the random baseline and can roughly reach the fully supervised performance with only a small portion of data annotated (around 18% for"}, {"question": " How is the reading cost controlled in the comparison scheme?", "answer": " Choosing the same size of contexts in the sentence selection step of each AL cycle", "ref_chunk": "batch size (for sentence selection) to 4K tokens, which roughly corresponds to 2% of the total pool size for most of the datasets we use. The initial seed training set and the development set are randomly sampled (with FA) using this batch size. Unless otherwise noted, we run 14 AL cycles for each ex- periment. In each AL cycle, we re-train our model since we find incremental updating does not per- form well. Following most AL work, annotation is simulated by checking and assigning the labels from the original dataset. In FA, we annotate all the sub-structures for the selected sentences. In PA, we first decide the selection ratio and apply it to the selected sentences. We further adopt a heuristic7 that selects the union of sentence-wise uncertain sub-structures as well as global ones since both may contain informative sub-structures. Finally, all the presented results are averaged over five runs with different random seeds. Model and training. For the models, we adopt standard architectures by stacking task-specific structured predictors over pre-trained RoBERTabase (Liu et al., 2019) and the full models are fine-tuned at each training iteration. After obtaining new an- notations in each AL cycle, we first train a model based on all the available full or partial annota- tions. When using self-training, we further ap- ply this newly trained model to assign pseudo soft labels to all un-annotated instances and combine them with the existing annotations to train another model. Compared to using the old model from the last AL cycle, this strategy can give more accu- rate pseudo labels since the newly updated model usually performs better by learning from more an- notations. For PA, pseudo soft labels are assigned to both un-selected sentences and the un-annotated sub-structures in the selected sentences. 7This heuristic will increase the actual selecting ratio, but it will only be slightly larger since there are large overlaps be- tween sentence-wise and global highly-ranked sub-structures. 3.2 Comparison Scheme Since FA and PA annotate at different granularities, we need a common cost measurement to compare their effectiveness properly. A reasonable metric is the number of the labeled sub-structures; for instance, the number of labeled tokens for sequence labeling or edges for dependency parsing. This metric is commonly adopted in previous PA work (Tomanek and Hahn, 2009; Flannery and Mori, 2015; Li et al., 2016; Radmard et al., 2021). Nevertheless, evaluating only by sub-structures ignores a crucial hidden cost: The reading time of the contexts. For example, in sequence labeling with PA, although not every token in the sentence needs to be tagged, the annotator may still need to read the whole sentence to understand its meaning. Therefore, if performing comparisons only by the amount of annotated sub-structures, it will be unfair for the FA baseline because more contexts must be read to carry out PA. In this work, we adopt a simple two-facet com- parison scheme that considers both reading and labeling costs. We first control the reading cost by choosing the same size of contexts in the sen- tence selection step of each AL cycle (Line 4 in Algorithm 1). Then, we further compare by the sub-structure labeling cost, measured by the sub- structure annotation cost. If PA can roughly reach the FA performance with the same reading cost but fewer sub-structures annotated, it would be fair to say that PA can help reduce cost over FA. A better comparing scheme should evaluate against a uni- fied estimation of the real annotation costs (Settles et al., 2008). This usually requires actual annota- tion exercises rather than simulations, which we leave to future work. 3.3 NER and DPAR Settings. We compare primarily three strategies: FA, PA, and a baseline where randomly selected sentences are fully annotated (Rand). We also in- clude a supervised result (Super.) which is obtained from a model trained with the full original training set. We measure reading cost by the total number of tokens in the selected sentences. For labeling cost, we further adopt metrics with practical consid- erations. In NER, lots of tokens, such as functional words, can be easily judged as the \u2018O\u2019 (non-entity) tag. To avoid over-estimating the costs of such easy tokens for FA, we filter tokens by their part- of-speech (POS) tags and only count the ones that \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000)\u0000$ \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001a \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000\u0019 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00003\u0000$ \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u0000\u001b\u0000\u001b \u0000\u001b\u0000\u001a \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u0017 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000)\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$ \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000)\u0000$ \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u001b Figure 2: Comparisons according to reading and labeling cost. Each node indicates one AL cycle. For x-axis, reading cost (left) is measured by token numbers, while labeling cost (right) is task-specific (\u00a73.3). NER is evaluated with labeled F1 scores on CoNLL-2003, while DPAR is with LAS scores on UD-EWT. Results are averaged over five runs with different seeds, and the shaded areas indicate standard deviations. The overall unlabeled pool contains around 200K tokens. Using AL, good performance can be obtained with less than 30% (60K) annotated. are likely to be inside an entity mention.8 For PA, we still count every queried token. For the task of DPAR, similarly, different dependency links can have variant annotation difficulties. We utilize the surface distance between the head and modifier of the dependency edge as the measure of label- ing cost, considering that the decisions for longer dependencies are usually harder. Main Results. The main test results are shown in Figure 2, where the patterns on both tasks are similar. First, AL brings clear improvements over the random baseline and can roughly reach the fully supervised performance with only a small portion of data annotated (around 18% for"}, {"question": " What is the measure of labeling cost in the task of DPAR?", "answer": " Surface distance between the head and modifier of the dependency edge", "ref_chunk": "batch size (for sentence selection) to 4K tokens, which roughly corresponds to 2% of the total pool size for most of the datasets we use. The initial seed training set and the development set are randomly sampled (with FA) using this batch size. Unless otherwise noted, we run 14 AL cycles for each ex- periment. In each AL cycle, we re-train our model since we find incremental updating does not per- form well. Following most AL work, annotation is simulated by checking and assigning the labels from the original dataset. In FA, we annotate all the sub-structures for the selected sentences. In PA, we first decide the selection ratio and apply it to the selected sentences. We further adopt a heuristic7 that selects the union of sentence-wise uncertain sub-structures as well as global ones since both may contain informative sub-structures. Finally, all the presented results are averaged over five runs with different random seeds. Model and training. For the models, we adopt standard architectures by stacking task-specific structured predictors over pre-trained RoBERTabase (Liu et al., 2019) and the full models are fine-tuned at each training iteration. After obtaining new an- notations in each AL cycle, we first train a model based on all the available full or partial annota- tions. When using self-training, we further ap- ply this newly trained model to assign pseudo soft labels to all un-annotated instances and combine them with the existing annotations to train another model. Compared to using the old model from the last AL cycle, this strategy can give more accu- rate pseudo labels since the newly updated model usually performs better by learning from more an- notations. For PA, pseudo soft labels are assigned to both un-selected sentences and the un-annotated sub-structures in the selected sentences. 7This heuristic will increase the actual selecting ratio, but it will only be slightly larger since there are large overlaps be- tween sentence-wise and global highly-ranked sub-structures. 3.2 Comparison Scheme Since FA and PA annotate at different granularities, we need a common cost measurement to compare their effectiveness properly. A reasonable metric is the number of the labeled sub-structures; for instance, the number of labeled tokens for sequence labeling or edges for dependency parsing. This metric is commonly adopted in previous PA work (Tomanek and Hahn, 2009; Flannery and Mori, 2015; Li et al., 2016; Radmard et al., 2021). Nevertheless, evaluating only by sub-structures ignores a crucial hidden cost: The reading time of the contexts. For example, in sequence labeling with PA, although not every token in the sentence needs to be tagged, the annotator may still need to read the whole sentence to understand its meaning. Therefore, if performing comparisons only by the amount of annotated sub-structures, it will be unfair for the FA baseline because more contexts must be read to carry out PA. In this work, we adopt a simple two-facet com- parison scheme that considers both reading and labeling costs. We first control the reading cost by choosing the same size of contexts in the sen- tence selection step of each AL cycle (Line 4 in Algorithm 1). Then, we further compare by the sub-structure labeling cost, measured by the sub- structure annotation cost. If PA can roughly reach the FA performance with the same reading cost but fewer sub-structures annotated, it would be fair to say that PA can help reduce cost over FA. A better comparing scheme should evaluate against a uni- fied estimation of the real annotation costs (Settles et al., 2008). This usually requires actual annota- tion exercises rather than simulations, which we leave to future work. 3.3 NER and DPAR Settings. We compare primarily three strategies: FA, PA, and a baseline where randomly selected sentences are fully annotated (Rand). We also in- clude a supervised result (Super.) which is obtained from a model trained with the full original training set. We measure reading cost by the total number of tokens in the selected sentences. For labeling cost, we further adopt metrics with practical consid- erations. In NER, lots of tokens, such as functional words, can be easily judged as the \u2018O\u2019 (non-entity) tag. To avoid over-estimating the costs of such easy tokens for FA, we filter tokens by their part- of-speech (POS) tags and only count the ones that \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000)\u0000$ \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001a \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000\u0019 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00003\u0000$ \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u0000\u001b\u0000\u001b \u0000\u001b\u0000\u001a \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u0017 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000)\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$ \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000)\u0000$ \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u001b Figure 2: Comparisons according to reading and labeling cost. Each node indicates one AL cycle. For x-axis, reading cost (left) is measured by token numbers, while labeling cost (right) is task-specific (\u00a73.3). NER is evaluated with labeled F1 scores on CoNLL-2003, while DPAR is with LAS scores on UD-EWT. Results are averaged over five runs with different seeds, and the shaded areas indicate standard deviations. The overall unlabeled pool contains around 200K tokens. Using AL, good performance can be obtained with less than 30% (60K) annotated. are likely to be inside an entity mention.8 For PA, we still count every queried token. For the task of DPAR, similarly, different dependency links can have variant annotation difficulties. We utilize the surface distance between the head and modifier of the dependency edge as the measure of label- ing cost, considering that the decisions for longer dependencies are usually harder. Main Results. The main test results are shown in Figure 2, where the patterns on both tasks are similar. First, AL brings clear improvements over the random baseline and can roughly reach the fully supervised performance with only a small portion of data annotated (around 18% for"}, {"question": " How much data needs to be annotated to obtain good performance using AL?", "answer": " Less than 30% (60K) annotated", "ref_chunk": "batch size (for sentence selection) to 4K tokens, which roughly corresponds to 2% of the total pool size for most of the datasets we use. The initial seed training set and the development set are randomly sampled (with FA) using this batch size. Unless otherwise noted, we run 14 AL cycles for each ex- periment. In each AL cycle, we re-train our model since we find incremental updating does not per- form well. Following most AL work, annotation is simulated by checking and assigning the labels from the original dataset. In FA, we annotate all the sub-structures for the selected sentences. In PA, we first decide the selection ratio and apply it to the selected sentences. We further adopt a heuristic7 that selects the union of sentence-wise uncertain sub-structures as well as global ones since both may contain informative sub-structures. Finally, all the presented results are averaged over five runs with different random seeds. Model and training. For the models, we adopt standard architectures by stacking task-specific structured predictors over pre-trained RoBERTabase (Liu et al., 2019) and the full models are fine-tuned at each training iteration. After obtaining new an- notations in each AL cycle, we first train a model based on all the available full or partial annota- tions. When using self-training, we further ap- ply this newly trained model to assign pseudo soft labels to all un-annotated instances and combine them with the existing annotations to train another model. Compared to using the old model from the last AL cycle, this strategy can give more accu- rate pseudo labels since the newly updated model usually performs better by learning from more an- notations. For PA, pseudo soft labels are assigned to both un-selected sentences and the un-annotated sub-structures in the selected sentences. 7This heuristic will increase the actual selecting ratio, but it will only be slightly larger since there are large overlaps be- tween sentence-wise and global highly-ranked sub-structures. 3.2 Comparison Scheme Since FA and PA annotate at different granularities, we need a common cost measurement to compare their effectiveness properly. A reasonable metric is the number of the labeled sub-structures; for instance, the number of labeled tokens for sequence labeling or edges for dependency parsing. This metric is commonly adopted in previous PA work (Tomanek and Hahn, 2009; Flannery and Mori, 2015; Li et al., 2016; Radmard et al., 2021). Nevertheless, evaluating only by sub-structures ignores a crucial hidden cost: The reading time of the contexts. For example, in sequence labeling with PA, although not every token in the sentence needs to be tagged, the annotator may still need to read the whole sentence to understand its meaning. Therefore, if performing comparisons only by the amount of annotated sub-structures, it will be unfair for the FA baseline because more contexts must be read to carry out PA. In this work, we adopt a simple two-facet com- parison scheme that considers both reading and labeling costs. We first control the reading cost by choosing the same size of contexts in the sen- tence selection step of each AL cycle (Line 4 in Algorithm 1). Then, we further compare by the sub-structure labeling cost, measured by the sub- structure annotation cost. If PA can roughly reach the FA performance with the same reading cost but fewer sub-structures annotated, it would be fair to say that PA can help reduce cost over FA. A better comparing scheme should evaluate against a uni- fied estimation of the real annotation costs (Settles et al., 2008). This usually requires actual annota- tion exercises rather than simulations, which we leave to future work. 3.3 NER and DPAR Settings. We compare primarily three strategies: FA, PA, and a baseline where randomly selected sentences are fully annotated (Rand). We also in- clude a supervised result (Super.) which is obtained from a model trained with the full original training set. We measure reading cost by the total number of tokens in the selected sentences. For labeling cost, we further adopt metrics with practical consid- erations. In NER, lots of tokens, such as functional words, can be easily judged as the \u2018O\u2019 (non-entity) tag. To avoid over-estimating the costs of such easy tokens for FA, we filter tokens by their part- of-speech (POS) tags and only count the ones that \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000)\u0000$ \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001a \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000\u0019 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00003\u0000$ \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u0000\u001b\u0000\u001b \u0000\u001b\u0000\u001a \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u0017 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000)\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$ \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000)\u0000$ \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u001b Figure 2: Comparisons according to reading and labeling cost. Each node indicates one AL cycle. For x-axis, reading cost (left) is measured by token numbers, while labeling cost (right) is task-specific (\u00a73.3). NER is evaluated with labeled F1 scores on CoNLL-2003, while DPAR is with LAS scores on UD-EWT. Results are averaged over five runs with different seeds, and the shaded areas indicate standard deviations. The overall unlabeled pool contains around 200K tokens. Using AL, good performance can be obtained with less than 30% (60K) annotated. are likely to be inside an entity mention.8 For PA, we still count every queried token. For the task of DPAR, similarly, different dependency links can have variant annotation difficulties. We utilize the surface distance between the head and modifier of the dependency edge as the measure of label- ing cost, considering that the decisions for longer dependencies are usually harder. Main Results. The main test results are shown in Figure 2, where the patterns on both tasks are similar. First, AL brings clear improvements over the random baseline and can roughly reach the fully supervised performance with only a small portion of data annotated (around 18% for"}], "doc_text": "batch size (for sentence selection) to 4K tokens, which roughly corresponds to 2% of the total pool size for most of the datasets we use. The initial seed training set and the development set are randomly sampled (with FA) using this batch size. Unless otherwise noted, we run 14 AL cycles for each ex- periment. In each AL cycle, we re-train our model since we find incremental updating does not per- form well. Following most AL work, annotation is simulated by checking and assigning the labels from the original dataset. In FA, we annotate all the sub-structures for the selected sentences. In PA, we first decide the selection ratio and apply it to the selected sentences. We further adopt a heuristic7 that selects the union of sentence-wise uncertain sub-structures as well as global ones since both may contain informative sub-structures. Finally, all the presented results are averaged over five runs with different random seeds. Model and training. For the models, we adopt standard architectures by stacking task-specific structured predictors over pre-trained RoBERTabase (Liu et al., 2019) and the full models are fine-tuned at each training iteration. After obtaining new an- notations in each AL cycle, we first train a model based on all the available full or partial annota- tions. When using self-training, we further ap- ply this newly trained model to assign pseudo soft labels to all un-annotated instances and combine them with the existing annotations to train another model. Compared to using the old model from the last AL cycle, this strategy can give more accu- rate pseudo labels since the newly updated model usually performs better by learning from more an- notations. For PA, pseudo soft labels are assigned to both un-selected sentences and the un-annotated sub-structures in the selected sentences. 7This heuristic will increase the actual selecting ratio, but it will only be slightly larger since there are large overlaps be- tween sentence-wise and global highly-ranked sub-structures. 3.2 Comparison Scheme Since FA and PA annotate at different granularities, we need a common cost measurement to compare their effectiveness properly. A reasonable metric is the number of the labeled sub-structures; for instance, the number of labeled tokens for sequence labeling or edges for dependency parsing. This metric is commonly adopted in previous PA work (Tomanek and Hahn, 2009; Flannery and Mori, 2015; Li et al., 2016; Radmard et al., 2021). Nevertheless, evaluating only by sub-structures ignores a crucial hidden cost: The reading time of the contexts. For example, in sequence labeling with PA, although not every token in the sentence needs to be tagged, the annotator may still need to read the whole sentence to understand its meaning. Therefore, if performing comparisons only by the amount of annotated sub-structures, it will be unfair for the FA baseline because more contexts must be read to carry out PA. In this work, we adopt a simple two-facet com- parison scheme that considers both reading and labeling costs. We first control the reading cost by choosing the same size of contexts in the sen- tence selection step of each AL cycle (Line 4 in Algorithm 1). Then, we further compare by the sub-structure labeling cost, measured by the sub- structure annotation cost. If PA can roughly reach the FA performance with the same reading cost but fewer sub-structures annotated, it would be fair to say that PA can help reduce cost over FA. A better comparing scheme should evaluate against a uni- fied estimation of the real annotation costs (Settles et al., 2008). This usually requires actual annota- tion exercises rather than simulations, which we leave to future work. 3.3 NER and DPAR Settings. We compare primarily three strategies: FA, PA, and a baseline where randomly selected sentences are fully annotated (Rand). We also in- clude a supervised result (Super.) which is obtained from a model trained with the full original training set. We measure reading cost by the total number of tokens in the selected sentences. For labeling cost, we further adopt metrics with practical consid- erations. In NER, lots of tokens, such as functional words, can be easily judged as the \u2018O\u2019 (non-entity) tag. To avoid over-estimating the costs of such easy tokens for FA, we filter tokens by their part- of-speech (POS) tags and only count the ones that \u00001\u0000(\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000)\u0000$ \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u00003\u0000$ \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001a \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000 \u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$ \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0013 \u00005\u0000D\u0000Q\u0000G \u0000\u001b\u0000\u0019 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000 \u0000\u0015\u0000)\u0000\u0014\u0000\b \u00003\u0000$ \u00001\u0000(\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013 \u0000 \u0000\u0014 \u0000\u001b\u0000\u001b \u0000\u001b\u0000\u001a \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u0017 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u0000)\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u00005\u0000H\u0000D\u0000G\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u00003\u0000$ \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00007\u0000R\u0000N\u0000H\u0000Q\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u0018\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u001b \u0000\u0014\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G \u00005\u0000D\u0000Q\u0000G\u0000\u000e\u00006\u00007 \u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0019 \u0000 \u0000\u0013\u0000/\u0000$\u00006\u0000\b \u00003\u0000$\u0000\u000e\u00006\u00007 \u0000)\u0000$ \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0017 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u00006\u0000X\u0000E\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000H\u0000\u0003\u0000&\u0000R\u0000X\u0000Q\u0000W \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000S\u0000H\u0000U\u0000\u0011 \u0000\u0013 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00003\u0000$ \u0000'\u00003\u0000$\u00005\u0000\u0003\u0000/\u0000D\u0000E\u0000H\u0000O\u0000L\u0000Q\u0000J\u0000\u0003\u0000&\u0000R\u0000V\u0000W \u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000)\u0000$\u0000\u000e\u00006\u00007 \u0000\u001b\u0000\u001b Figure 2: Comparisons according to reading and labeling cost. Each node indicates one AL cycle. For x-axis, reading cost (left) is measured by token numbers, while labeling cost (right) is task-specific (\u00a73.3). NER is evaluated with labeled F1 scores on CoNLL-2003, while DPAR is with LAS scores on UD-EWT. Results are averaged over five runs with different seeds, and the shaded areas indicate standard deviations. The overall unlabeled pool contains around 200K tokens. Using AL, good performance can be obtained with less than 30% (60K) annotated. are likely to be inside an entity mention.8 For PA, we still count every queried token. For the task of DPAR, similarly, different dependency links can have variant annotation difficulties. We utilize the surface distance between the head and modifier of the dependency edge as the measure of label- ing cost, considering that the decisions for longer dependencies are usually harder. Main Results. The main test results are shown in Figure 2, where the patterns on both tasks are similar. First, AL brings clear improvements over the random baseline and can roughly reach the fully supervised performance with only a small portion of data annotated (around 18% for"}