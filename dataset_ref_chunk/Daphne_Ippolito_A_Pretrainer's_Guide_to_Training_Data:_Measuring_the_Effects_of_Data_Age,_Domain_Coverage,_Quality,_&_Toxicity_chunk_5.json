{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daphne_Ippolito_A_Pretrainer's_Guide_to_Training_Data:_Measuring_the_Effects_of_Data_Age,_Domain_Coverage,_Quality,_&_Toxicity_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " According to the text, what trends were observed in the percentage of non-ASCII characters and text quality over different collection times of C4?", "answer": " The percentage of non-ASCII characters increased steadily in more recent years while the measured text quality declined.", "ref_chunk": "and Academic score low on predicted quality, indicating that overly-specific positively-defined filters on web documents may remove substantial amounts of potentially useful specialized text. Time Comparing across different collection times of C4 (in Figure 9), we see a couple of steady trends. The percentage of non-ASCII characters increased steadily in more recent years while the measured text quality declines. This growth may be due to increasing non-English content, but could also correspond to rising use of emojis and non-ASCII punctuation. Toxicity scores also decrease slightly in later years, while sentiment increases. 8 4 Impact of Dataset Age on Pretrained Models Section Findings Both models and evaluation datasets become stale. Temporal misalignment between pretraining and evaluation data is not overcome by finetuning. Temporal misalignment complicates evaluation of models trained at different times, as older evaluation datasets may become stale and newer evaluation datasets may under-estimate performance of older models. The effects of pretraining misalignment are stronger for larger models than smaller models. While models are frequently and cheaply updated with new finetuning data, the expense of pretraining means the NLP community has relied on relatively few static pretrained models that are rarely updated or exchanged. BERT, RoBERTa, GPT-2, and T5 variants, all pretrained prior to 2020, constitute the majority (estimated at ~58% as of April 16, 2023) of all models downloaded on HuggingFace. Prior work demonstrates that language use changes over time (Altmann et al., 2009; Labov, 2011) and that temporal misalignment between finetuning and evaluation datasets correlates with degraded performance, visible across settings and domains (Luu et al., 2021; Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Jang et al., 2022). In contrast, we examine the effect of temporal misalignment between pretraining data and evaluation. In evaluating the impact of pretraining time across data domains, we can quantify the impact this design choice has on NLP broadly. 20102012201420162013201620192022Pretrain Years 82.789.091.271.270.874.771.582.082.274.980.488.190.670.972.375.872.682.282.476.080.287.890.770.472.075.873.483.182.875.979.487.189.470.871.475.071.082.583.376.8PoliAff 85.084.984.982.783.183.585.284.885.983.183.483.484.684.484.684.083.884.682.783.784.482.982.783.6TwiERC 23.632.127.717.923.332.027.919.122.831.227.418.122.731.227.417.8NewSum 98.298.095.091.394.488.798.198.295.293.195.188.597.898.793.993.496.090.597.698.494.491.495.189.0AIC 20102012201420162013201620192022 78.979.278.575.176.878.779.076.375.076.377.173.274.075.776.873.4PubCLS 201420152016201720182019Eval Years2013201620192022Pretrain Years 2012201320142015201620172018201920202021Eval Years2013201620192022 2014201520162017201820192013201620192022 Figure 3: Temporal Misalignment between Pretraining and Evaluation causes performance degradation. Four LM-XL\u2019s, each pretained on a different C4 time split, are evaluated on each time split across five datasets. Heatmap colors are normalized by column, following Luu et al. (2021) to show the best pretraining year for each evaluation year. We pretrain four autoregressive language models on versions of C4: 2013, 2016, 2019, and 2022. For each version we begin with Common Crawl data and remove all data that was scraped after the cutoff year. Following Luu et al. (2021), we measure the effect of temporal misalignment by using evaluation tasks (from News, Twitter, and Science domains) that have training and test sets split by year. After pretraining, we finetune each model on each dataset\u2019s training-year split separately, then evaluate on every test-year split. Full details and results are in Appendix C.4 and Appendix E.1, respectively. First, we replicate the performance degradation observed by Luu et al. (2021) due to finetuning and evaluation misalignment on the five tasks in Figure 12. Next, we estimate the effects of temporal misalignment between pretraining and evaluation (Figure 3). Since all models were finetuned on the training sets of the evaluation 9 tasks, we show that temporal misalignment during pretraining persists even with temporally-relevant finetuning data. 8-7-6-5-4-3-2-101234567891012Pretrain Year - Evaluation Year 6Relative Improvement (%) Figure 4: The mean relative performance over 5 datasets (y-axis) increases as temporal misalignment (x-axis) approaches zero. The boxplot indicates the median (solid line), mean (triangles), quartile range (boxes), and rest of the distribution (whiskers). Note that each dataset has different evaluation year ranges. Performance degradation strongly correlates with pretraining misalignment and its effects are non-trivial. Luu et al. (2021) formalize a definition for Temporal Degradation (TD), which measures the performance change observed from one year difference between the finetuning and evaluation years. We generalize TD to also measure the effect of one year difference between pretraining time and evaluation time, as described in Appendix C.4. Furthermore, we measure the Pearson correlation r between the performance difference and the temporal difference to understand the strength of the correlation. In Table 2 we find temporal degradation is highest for finetuning (2.8 on average), as expected, but also surprisingly high for one year of pretraining (0.4)\u2014particularly for the News domain. The average Pearson correlation of 0.61 indicates a strong correlation between pretraining temporal misalignment and performance degradation. All five tasks pass a one-sided Wald test with p < 0.05, validating the slope is greater than zero. Finetuning Pretraining Domain Task LM-Small TD r LM-XL TD PubCLS 5.82 NewSum 0.80 PoliAff 3.74 TwiERC 0.49 0.94 News Twitter Science AIC 0.84 5.63 0.80 0.82 2.91 0.92 0.84 4.93 0.89 0.73 0.53 0.82 0.83 0.24 0.36 r LM-Small TD r r 0.02 0.01\u2020 0.59 0.67 -0.31 -0.29 0.73 0.45 0.21 0.28 0.56 0.50 0.27 0.23 0.72 0.05 0.11 0.18\u2020 0.23 0.66 0.07 0.41 0.61 0.08 LM-XL TD Mean 2.36 0.81 2.84 0.76 Table 2: Temporal Degradation (TD) measures the expected performance degradation from one year of tem- poral misalignment. We report TD first between finetuning and evaluation, then pretraining and evaluation, for LM-XL and LM-Small, across five tasks. Pearson correlation r indicates the correlation strength between performance and temporal change. Temporal Degradation due to pretraining is significant and persistent across domains. All correlations are significant at p < 0.05 unless marked with \u2020. Pretraining misalignment is not overcome by significant finetuning. The temporal degradation due to pretraining suggests models pretrained on data from the same time frame as target evaluations will 10 have advantages over models trained on much older or newer data. Notably, this effect is observed for models which are finetuned on the full temporally-relevant training sets. This suggests that even substantial finetuning cannot overcome pretraining data that is temporally misaligned. Pretraining misalignment effects are asymmetric and have implications for NLP evaluations. We observe performance degradation regardless of whether the pretraining data was collected before or after the evalua- tion data. While we would not expect a 2019 checkpoint to perform well on questions about COVID, we also find that 2022"}, {"question": " What could be the reason for the growth in the percentage of non-ASCII characters in more recent years?", "answer": " The growth may be due to increasing non-English content, but could also correspond to rising use of emojis and non-ASCII punctuation.", "ref_chunk": "and Academic score low on predicted quality, indicating that overly-specific positively-defined filters on web documents may remove substantial amounts of potentially useful specialized text. Time Comparing across different collection times of C4 (in Figure 9), we see a couple of steady trends. The percentage of non-ASCII characters increased steadily in more recent years while the measured text quality declines. This growth may be due to increasing non-English content, but could also correspond to rising use of emojis and non-ASCII punctuation. Toxicity scores also decrease slightly in later years, while sentiment increases. 8 4 Impact of Dataset Age on Pretrained Models Section Findings Both models and evaluation datasets become stale. Temporal misalignment between pretraining and evaluation data is not overcome by finetuning. Temporal misalignment complicates evaluation of models trained at different times, as older evaluation datasets may become stale and newer evaluation datasets may under-estimate performance of older models. The effects of pretraining misalignment are stronger for larger models than smaller models. While models are frequently and cheaply updated with new finetuning data, the expense of pretraining means the NLP community has relied on relatively few static pretrained models that are rarely updated or exchanged. BERT, RoBERTa, GPT-2, and T5 variants, all pretrained prior to 2020, constitute the majority (estimated at ~58% as of April 16, 2023) of all models downloaded on HuggingFace. Prior work demonstrates that language use changes over time (Altmann et al., 2009; Labov, 2011) and that temporal misalignment between finetuning and evaluation datasets correlates with degraded performance, visible across settings and domains (Luu et al., 2021; Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Jang et al., 2022). In contrast, we examine the effect of temporal misalignment between pretraining data and evaluation. In evaluating the impact of pretraining time across data domains, we can quantify the impact this design choice has on NLP broadly. 20102012201420162013201620192022Pretrain Years 82.789.091.271.270.874.771.582.082.274.980.488.190.670.972.375.872.682.282.476.080.287.890.770.472.075.873.483.182.875.979.487.189.470.871.475.071.082.583.376.8PoliAff 85.084.984.982.783.183.585.284.885.983.183.483.484.684.484.684.083.884.682.783.784.482.982.783.6TwiERC 23.632.127.717.923.332.027.919.122.831.227.418.122.731.227.417.8NewSum 98.298.095.091.394.488.798.198.295.293.195.188.597.898.793.993.496.090.597.698.494.491.495.189.0AIC 20102012201420162013201620192022 78.979.278.575.176.878.779.076.375.076.377.173.274.075.776.873.4PubCLS 201420152016201720182019Eval Years2013201620192022Pretrain Years 2012201320142015201620172018201920202021Eval Years2013201620192022 2014201520162017201820192013201620192022 Figure 3: Temporal Misalignment between Pretraining and Evaluation causes performance degradation. Four LM-XL\u2019s, each pretained on a different C4 time split, are evaluated on each time split across five datasets. Heatmap colors are normalized by column, following Luu et al. (2021) to show the best pretraining year for each evaluation year. We pretrain four autoregressive language models on versions of C4: 2013, 2016, 2019, and 2022. For each version we begin with Common Crawl data and remove all data that was scraped after the cutoff year. Following Luu et al. (2021), we measure the effect of temporal misalignment by using evaluation tasks (from News, Twitter, and Science domains) that have training and test sets split by year. After pretraining, we finetune each model on each dataset\u2019s training-year split separately, then evaluate on every test-year split. Full details and results are in Appendix C.4 and Appendix E.1, respectively. First, we replicate the performance degradation observed by Luu et al. (2021) due to finetuning and evaluation misalignment on the five tasks in Figure 12. Next, we estimate the effects of temporal misalignment between pretraining and evaluation (Figure 3). Since all models were finetuned on the training sets of the evaluation 9 tasks, we show that temporal misalignment during pretraining persists even with temporally-relevant finetuning data. 8-7-6-5-4-3-2-101234567891012Pretrain Year - Evaluation Year 6Relative Improvement (%) Figure 4: The mean relative performance over 5 datasets (y-axis) increases as temporal misalignment (x-axis) approaches zero. The boxplot indicates the median (solid line), mean (triangles), quartile range (boxes), and rest of the distribution (whiskers). Note that each dataset has different evaluation year ranges. Performance degradation strongly correlates with pretraining misalignment and its effects are non-trivial. Luu et al. (2021) formalize a definition for Temporal Degradation (TD), which measures the performance change observed from one year difference between the finetuning and evaluation years. We generalize TD to also measure the effect of one year difference between pretraining time and evaluation time, as described in Appendix C.4. Furthermore, we measure the Pearson correlation r between the performance difference and the temporal difference to understand the strength of the correlation. In Table 2 we find temporal degradation is highest for finetuning (2.8 on average), as expected, but also surprisingly high for one year of pretraining (0.4)\u2014particularly for the News domain. The average Pearson correlation of 0.61 indicates a strong correlation between pretraining temporal misalignment and performance degradation. All five tasks pass a one-sided Wald test with p < 0.05, validating the slope is greater than zero. Finetuning Pretraining Domain Task LM-Small TD r LM-XL TD PubCLS 5.82 NewSum 0.80 PoliAff 3.74 TwiERC 0.49 0.94 News Twitter Science AIC 0.84 5.63 0.80 0.82 2.91 0.92 0.84 4.93 0.89 0.73 0.53 0.82 0.83 0.24 0.36 r LM-Small TD r r 0.02 0.01\u2020 0.59 0.67 -0.31 -0.29 0.73 0.45 0.21 0.28 0.56 0.50 0.27 0.23 0.72 0.05 0.11 0.18\u2020 0.23 0.66 0.07 0.41 0.61 0.08 LM-XL TD Mean 2.36 0.81 2.84 0.76 Table 2: Temporal Degradation (TD) measures the expected performance degradation from one year of tem- poral misalignment. We report TD first between finetuning and evaluation, then pretraining and evaluation, for LM-XL and LM-Small, across five tasks. Pearson correlation r indicates the correlation strength between performance and temporal change. Temporal Degradation due to pretraining is significant and persistent across domains. All correlations are significant at p < 0.05 unless marked with \u2020. Pretraining misalignment is not overcome by significant finetuning. The temporal degradation due to pretraining suggests models pretrained on data from the same time frame as target evaluations will 10 have advantages over models trained on much older or newer data. Notably, this effect is observed for models which are finetuned on the full temporally-relevant training sets. This suggests that even substantial finetuning cannot overcome pretraining data that is temporally misaligned. Pretraining misalignment effects are asymmetric and have implications for NLP evaluations. We observe performance degradation regardless of whether the pretraining data was collected before or after the evalua- tion data. While we would not expect a 2019 checkpoint to perform well on questions about COVID, we also find that 2022"}, {"question": " How did toxicity scores and sentiment change over the years mentioned in the text?", "answer": " Toxicity scores decreased slightly in later years, while sentiment increased.", "ref_chunk": "and Academic score low on predicted quality, indicating that overly-specific positively-defined filters on web documents may remove substantial amounts of potentially useful specialized text. Time Comparing across different collection times of C4 (in Figure 9), we see a couple of steady trends. The percentage of non-ASCII characters increased steadily in more recent years while the measured text quality declines. This growth may be due to increasing non-English content, but could also correspond to rising use of emojis and non-ASCII punctuation. Toxicity scores also decrease slightly in later years, while sentiment increases. 8 4 Impact of Dataset Age on Pretrained Models Section Findings Both models and evaluation datasets become stale. Temporal misalignment between pretraining and evaluation data is not overcome by finetuning. Temporal misalignment complicates evaluation of models trained at different times, as older evaluation datasets may become stale and newer evaluation datasets may under-estimate performance of older models. The effects of pretraining misalignment are stronger for larger models than smaller models. While models are frequently and cheaply updated with new finetuning data, the expense of pretraining means the NLP community has relied on relatively few static pretrained models that are rarely updated or exchanged. BERT, RoBERTa, GPT-2, and T5 variants, all pretrained prior to 2020, constitute the majority (estimated at ~58% as of April 16, 2023) of all models downloaded on HuggingFace. Prior work demonstrates that language use changes over time (Altmann et al., 2009; Labov, 2011) and that temporal misalignment between finetuning and evaluation datasets correlates with degraded performance, visible across settings and domains (Luu et al., 2021; Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Jang et al., 2022). In contrast, we examine the effect of temporal misalignment between pretraining data and evaluation. In evaluating the impact of pretraining time across data domains, we can quantify the impact this design choice has on NLP broadly. 20102012201420162013201620192022Pretrain Years 82.789.091.271.270.874.771.582.082.274.980.488.190.670.972.375.872.682.282.476.080.287.890.770.472.075.873.483.182.875.979.487.189.470.871.475.071.082.583.376.8PoliAff 85.084.984.982.783.183.585.284.885.983.183.483.484.684.484.684.083.884.682.783.784.482.982.783.6TwiERC 23.632.127.717.923.332.027.919.122.831.227.418.122.731.227.417.8NewSum 98.298.095.091.394.488.798.198.295.293.195.188.597.898.793.993.496.090.597.698.494.491.495.189.0AIC 20102012201420162013201620192022 78.979.278.575.176.878.779.076.375.076.377.173.274.075.776.873.4PubCLS 201420152016201720182019Eval Years2013201620192022Pretrain Years 2012201320142015201620172018201920202021Eval Years2013201620192022 2014201520162017201820192013201620192022 Figure 3: Temporal Misalignment between Pretraining and Evaluation causes performance degradation. Four LM-XL\u2019s, each pretained on a different C4 time split, are evaluated on each time split across five datasets. Heatmap colors are normalized by column, following Luu et al. (2021) to show the best pretraining year for each evaluation year. We pretrain four autoregressive language models on versions of C4: 2013, 2016, 2019, and 2022. For each version we begin with Common Crawl data and remove all data that was scraped after the cutoff year. Following Luu et al. (2021), we measure the effect of temporal misalignment by using evaluation tasks (from News, Twitter, and Science domains) that have training and test sets split by year. After pretraining, we finetune each model on each dataset\u2019s training-year split separately, then evaluate on every test-year split. Full details and results are in Appendix C.4 and Appendix E.1, respectively. First, we replicate the performance degradation observed by Luu et al. (2021) due to finetuning and evaluation misalignment on the five tasks in Figure 12. Next, we estimate the effects of temporal misalignment between pretraining and evaluation (Figure 3). Since all models were finetuned on the training sets of the evaluation 9 tasks, we show that temporal misalignment during pretraining persists even with temporally-relevant finetuning data. 8-7-6-5-4-3-2-101234567891012Pretrain Year - Evaluation Year 6Relative Improvement (%) Figure 4: The mean relative performance over 5 datasets (y-axis) increases as temporal misalignment (x-axis) approaches zero. The boxplot indicates the median (solid line), mean (triangles), quartile range (boxes), and rest of the distribution (whiskers). Note that each dataset has different evaluation year ranges. Performance degradation strongly correlates with pretraining misalignment and its effects are non-trivial. Luu et al. (2021) formalize a definition for Temporal Degradation (TD), which measures the performance change observed from one year difference between the finetuning and evaluation years. We generalize TD to also measure the effect of one year difference between pretraining time and evaluation time, as described in Appendix C.4. Furthermore, we measure the Pearson correlation r between the performance difference and the temporal difference to understand the strength of the correlation. In Table 2 we find temporal degradation is highest for finetuning (2.8 on average), as expected, but also surprisingly high for one year of pretraining (0.4)\u2014particularly for the News domain. The average Pearson correlation of 0.61 indicates a strong correlation between pretraining temporal misalignment and performance degradation. All five tasks pass a one-sided Wald test with p < 0.05, validating the slope is greater than zero. Finetuning Pretraining Domain Task LM-Small TD r LM-XL TD PubCLS 5.82 NewSum 0.80 PoliAff 3.74 TwiERC 0.49 0.94 News Twitter Science AIC 0.84 5.63 0.80 0.82 2.91 0.92 0.84 4.93 0.89 0.73 0.53 0.82 0.83 0.24 0.36 r LM-Small TD r r 0.02 0.01\u2020 0.59 0.67 -0.31 -0.29 0.73 0.45 0.21 0.28 0.56 0.50 0.27 0.23 0.72 0.05 0.11 0.18\u2020 0.23 0.66 0.07 0.41 0.61 0.08 LM-XL TD Mean 2.36 0.81 2.84 0.76 Table 2: Temporal Degradation (TD) measures the expected performance degradation from one year of tem- poral misalignment. We report TD first between finetuning and evaluation, then pretraining and evaluation, for LM-XL and LM-Small, across five tasks. Pearson correlation r indicates the correlation strength between performance and temporal change. Temporal Degradation due to pretraining is significant and persistent across domains. All correlations are significant at p < 0.05 unless marked with \u2020. Pretraining misalignment is not overcome by significant finetuning. The temporal degradation due to pretraining suggests models pretrained on data from the same time frame as target evaluations will 10 have advantages over models trained on much older or newer data. Notably, this effect is observed for models which are finetuned on the full temporally-relevant training sets. This suggests that even substantial finetuning cannot overcome pretraining data that is temporally misaligned. Pretraining misalignment effects are asymmetric and have implications for NLP evaluations. We observe performance degradation regardless of whether the pretraining data was collected before or after the evalua- tion data. While we would not expect a 2019 checkpoint to perform well on questions about COVID, we also find that 2022"}, {"question": " What impact does the temporal misalignment between pretraining and evaluation data have on the evaluation of models?", "answer": " Temporal misalignment between pretraining and evaluation data complicates the evaluation of models trained at different times, with older evaluation datasets becoming stale and newer evaluation datasets under-estimating the performance of older models.", "ref_chunk": "and Academic score low on predicted quality, indicating that overly-specific positively-defined filters on web documents may remove substantial amounts of potentially useful specialized text. Time Comparing across different collection times of C4 (in Figure 9), we see a couple of steady trends. The percentage of non-ASCII characters increased steadily in more recent years while the measured text quality declines. This growth may be due to increasing non-English content, but could also correspond to rising use of emojis and non-ASCII punctuation. Toxicity scores also decrease slightly in later years, while sentiment increases. 8 4 Impact of Dataset Age on Pretrained Models Section Findings Both models and evaluation datasets become stale. Temporal misalignment between pretraining and evaluation data is not overcome by finetuning. Temporal misalignment complicates evaluation of models trained at different times, as older evaluation datasets may become stale and newer evaluation datasets may under-estimate performance of older models. The effects of pretraining misalignment are stronger for larger models than smaller models. While models are frequently and cheaply updated with new finetuning data, the expense of pretraining means the NLP community has relied on relatively few static pretrained models that are rarely updated or exchanged. BERT, RoBERTa, GPT-2, and T5 variants, all pretrained prior to 2020, constitute the majority (estimated at ~58% as of April 16, 2023) of all models downloaded on HuggingFace. Prior work demonstrates that language use changes over time (Altmann et al., 2009; Labov, 2011) and that temporal misalignment between finetuning and evaluation datasets correlates with degraded performance, visible across settings and domains (Luu et al., 2021; Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Jang et al., 2022). In contrast, we examine the effect of temporal misalignment between pretraining data and evaluation. In evaluating the impact of pretraining time across data domains, we can quantify the impact this design choice has on NLP broadly. 20102012201420162013201620192022Pretrain Years 82.789.091.271.270.874.771.582.082.274.980.488.190.670.972.375.872.682.282.476.080.287.890.770.472.075.873.483.182.875.979.487.189.470.871.475.071.082.583.376.8PoliAff 85.084.984.982.783.183.585.284.885.983.183.483.484.684.484.684.083.884.682.783.784.482.982.783.6TwiERC 23.632.127.717.923.332.027.919.122.831.227.418.122.731.227.417.8NewSum 98.298.095.091.394.488.798.198.295.293.195.188.597.898.793.993.496.090.597.698.494.491.495.189.0AIC 20102012201420162013201620192022 78.979.278.575.176.878.779.076.375.076.377.173.274.075.776.873.4PubCLS 201420152016201720182019Eval Years2013201620192022Pretrain Years 2012201320142015201620172018201920202021Eval Years2013201620192022 2014201520162017201820192013201620192022 Figure 3: Temporal Misalignment between Pretraining and Evaluation causes performance degradation. Four LM-XL\u2019s, each pretained on a different C4 time split, are evaluated on each time split across five datasets. Heatmap colors are normalized by column, following Luu et al. (2021) to show the best pretraining year for each evaluation year. We pretrain four autoregressive language models on versions of C4: 2013, 2016, 2019, and 2022. For each version we begin with Common Crawl data and remove all data that was scraped after the cutoff year. Following Luu et al. (2021), we measure the effect of temporal misalignment by using evaluation tasks (from News, Twitter, and Science domains) that have training and test sets split by year. After pretraining, we finetune each model on each dataset\u2019s training-year split separately, then evaluate on every test-year split. Full details and results are in Appendix C.4 and Appendix E.1, respectively. First, we replicate the performance degradation observed by Luu et al. (2021) due to finetuning and evaluation misalignment on the five tasks in Figure 12. Next, we estimate the effects of temporal misalignment between pretraining and evaluation (Figure 3). Since all models were finetuned on the training sets of the evaluation 9 tasks, we show that temporal misalignment during pretraining persists even with temporally-relevant finetuning data. 8-7-6-5-4-3-2-101234567891012Pretrain Year - Evaluation Year 6Relative Improvement (%) Figure 4: The mean relative performance over 5 datasets (y-axis) increases as temporal misalignment (x-axis) approaches zero. The boxplot indicates the median (solid line), mean (triangles), quartile range (boxes), and rest of the distribution (whiskers). Note that each dataset has different evaluation year ranges. Performance degradation strongly correlates with pretraining misalignment and its effects are non-trivial. Luu et al. (2021) formalize a definition for Temporal Degradation (TD), which measures the performance change observed from one year difference between the finetuning and evaluation years. We generalize TD to also measure the effect of one year difference between pretraining time and evaluation time, as described in Appendix C.4. Furthermore, we measure the Pearson correlation r between the performance difference and the temporal difference to understand the strength of the correlation. In Table 2 we find temporal degradation is highest for finetuning (2.8 on average), as expected, but also surprisingly high for one year of pretraining (0.4)\u2014particularly for the News domain. The average Pearson correlation of 0.61 indicates a strong correlation between pretraining temporal misalignment and performance degradation. All five tasks pass a one-sided Wald test with p < 0.05, validating the slope is greater than zero. Finetuning Pretraining Domain Task LM-Small TD r LM-XL TD PubCLS 5.82 NewSum 0.80 PoliAff 3.74 TwiERC 0.49 0.94 News Twitter Science AIC 0.84 5.63 0.80 0.82 2.91 0.92 0.84 4.93 0.89 0.73 0.53 0.82 0.83 0.24 0.36 r LM-Small TD r r 0.02 0.01\u2020 0.59 0.67 -0.31 -0.29 0.73 0.45 0.21 0.28 0.56 0.50 0.27 0.23 0.72 0.05 0.11 0.18\u2020 0.23 0.66 0.07 0.41 0.61 0.08 LM-XL TD Mean 2.36 0.81 2.84 0.76 Table 2: Temporal Degradation (TD) measures the expected performance degradation from one year of tem- poral misalignment. We report TD first between finetuning and evaluation, then pretraining and evaluation, for LM-XL and LM-Small, across five tasks. Pearson correlation r indicates the correlation strength between performance and temporal change. Temporal Degradation due to pretraining is significant and persistent across domains. All correlations are significant at p < 0.05 unless marked with \u2020. Pretraining misalignment is not overcome by significant finetuning. The temporal degradation due to pretraining suggests models pretrained on data from the same time frame as target evaluations will 10 have advantages over models trained on much older or newer data. Notably, this effect is observed for models which are finetuned on the full temporally-relevant training sets. This suggests that even substantial finetuning cannot overcome pretraining data that is temporally misaligned. Pretraining misalignment effects are asymmetric and have implications for NLP evaluations. We observe performance degradation regardless of whether the pretraining data was collected before or after the evalua- tion data. While we would not expect a 2019 checkpoint to perform well on questions about COVID, we also find that 2022"}, {"question": " What are some of the static pretrained models that are discussed in the text?", "answer": " BERT, RoBERTa, GPT-2, and T5 variants, all pretrained prior to 2020, are mentioned as examples of static pretrained models.", "ref_chunk": "and Academic score low on predicted quality, indicating that overly-specific positively-defined filters on web documents may remove substantial amounts of potentially useful specialized text. Time Comparing across different collection times of C4 (in Figure 9), we see a couple of steady trends. The percentage of non-ASCII characters increased steadily in more recent years while the measured text quality declines. This growth may be due to increasing non-English content, but could also correspond to rising use of emojis and non-ASCII punctuation. Toxicity scores also decrease slightly in later years, while sentiment increases. 8 4 Impact of Dataset Age on Pretrained Models Section Findings Both models and evaluation datasets become stale. Temporal misalignment between pretraining and evaluation data is not overcome by finetuning. Temporal misalignment complicates evaluation of models trained at different times, as older evaluation datasets may become stale and newer evaluation datasets may under-estimate performance of older models. The effects of pretraining misalignment are stronger for larger models than smaller models. While models are frequently and cheaply updated with new finetuning data, the expense of pretraining means the NLP community has relied on relatively few static pretrained models that are rarely updated or exchanged. BERT, RoBERTa, GPT-2, and T5 variants, all pretrained prior to 2020, constitute the majority (estimated at ~58% as of April 16, 2023) of all models downloaded on HuggingFace. Prior work demonstrates that language use changes over time (Altmann et al., 2009; Labov, 2011) and that temporal misalignment between finetuning and evaluation datasets correlates with degraded performance, visible across settings and domains (Luu et al., 2021; Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Jang et al., 2022). In contrast, we examine the effect of temporal misalignment between pretraining data and evaluation. In evaluating the impact of pretraining time across data domains, we can quantify the impact this design choice has on NLP broadly. 20102012201420162013201620192022Pretrain Years 82.789.091.271.270.874.771.582.082.274.980.488.190.670.972.375.872.682.282.476.080.287.890.770.472.075.873.483.182.875.979.487.189.470.871.475.071.082.583.376.8PoliAff 85.084.984.982.783.183.585.284.885.983.183.483.484.684.484.684.083.884.682.783.784.482.982.783.6TwiERC 23.632.127.717.923.332.027.919.122.831.227.418.122.731.227.417.8NewSum 98.298.095.091.394.488.798.198.295.293.195.188.597.898.793.993.496.090.597.698.494.491.495.189.0AIC 20102012201420162013201620192022 78.979.278.575.176.878.779.076.375.076.377.173.274.075.776.873.4PubCLS 201420152016201720182019Eval Years2013201620192022Pretrain Years 2012201320142015201620172018201920202021Eval Years2013201620192022 2014201520162017201820192013201620192022 Figure 3: Temporal Misalignment between Pretraining and Evaluation causes performance degradation. Four LM-XL\u2019s, each pretained on a different C4 time split, are evaluated on each time split across five datasets. Heatmap colors are normalized by column, following Luu et al. (2021) to show the best pretraining year for each evaluation year. We pretrain four autoregressive language models on versions of C4: 2013, 2016, 2019, and 2022. For each version we begin with Common Crawl data and remove all data that was scraped after the cutoff year. Following Luu et al. (2021), we measure the effect of temporal misalignment by using evaluation tasks (from News, Twitter, and Science domains) that have training and test sets split by year. After pretraining, we finetune each model on each dataset\u2019s training-year split separately, then evaluate on every test-year split. Full details and results are in Appendix C.4 and Appendix E.1, respectively. First, we replicate the performance degradation observed by Luu et al. (2021) due to finetuning and evaluation misalignment on the five tasks in Figure 12. Next, we estimate the effects of temporal misalignment between pretraining and evaluation (Figure 3). Since all models were finetuned on the training sets of the evaluation 9 tasks, we show that temporal misalignment during pretraining persists even with temporally-relevant finetuning data. 8-7-6-5-4-3-2-101234567891012Pretrain Year - Evaluation Year 6Relative Improvement (%) Figure 4: The mean relative performance over 5 datasets (y-axis) increases as temporal misalignment (x-axis) approaches zero. The boxplot indicates the median (solid line), mean (triangles), quartile range (boxes), and rest of the distribution (whiskers). Note that each dataset has different evaluation year ranges. Performance degradation strongly correlates with pretraining misalignment and its effects are non-trivial. Luu et al. (2021) formalize a definition for Temporal Degradation (TD), which measures the performance change observed from one year difference between the finetuning and evaluation years. We generalize TD to also measure the effect of one year difference between pretraining time and evaluation time, as described in Appendix C.4. Furthermore, we measure the Pearson correlation r between the performance difference and the temporal difference to understand the strength of the correlation. In Table 2 we find temporal degradation is highest for finetuning (2.8 on average), as expected, but also surprisingly high for one year of pretraining (0.4)\u2014particularly for the News domain. The average Pearson correlation of 0.61 indicates a strong correlation between pretraining temporal misalignment and performance degradation. All five tasks pass a one-sided Wald test with p < 0.05, validating the slope is greater than zero. Finetuning Pretraining Domain Task LM-Small TD r LM-XL TD PubCLS 5.82 NewSum 0.80 PoliAff 3.74 TwiERC 0.49 0.94 News Twitter Science AIC 0.84 5.63 0.80 0.82 2.91 0.92 0.84 4.93 0.89 0.73 0.53 0.82 0.83 0.24 0.36 r LM-Small TD r r 0.02 0.01\u2020 0.59 0.67 -0.31 -0.29 0.73 0.45 0.21 0.28 0.56 0.50 0.27 0.23 0.72 0.05 0.11 0.18\u2020 0.23 0.66 0.07 0.41 0.61 0.08 LM-XL TD Mean 2.36 0.81 2.84 0.76 Table 2: Temporal Degradation (TD) measures the expected performance degradation from one year of tem- poral misalignment. We report TD first between finetuning and evaluation, then pretraining and evaluation, for LM-XL and LM-Small, across five tasks. Pearson correlation r indicates the correlation strength between performance and temporal change. Temporal Degradation due to pretraining is significant and persistent across domains. All correlations are significant at p < 0.05 unless marked with \u2020. Pretraining misalignment is not overcome by significant finetuning. The temporal degradation due to pretraining suggests models pretrained on data from the same time frame as target evaluations will 10 have advantages over models trained on much older or newer data. Notably, this effect is observed for models which are finetuned on the full temporally-relevant training sets. This suggests that even substantial finetuning cannot overcome pretraining data that is temporally misaligned. Pretraining misalignment effects are asymmetric and have implications for NLP evaluations. We observe performance degradation regardless of whether the pretraining data was collected before or after the evalua- tion data. While we would not expect a 2019 checkpoint to perform well on questions about COVID, we also find that 2022"}, {"question": " How does temporal misalignment affect the performance of language models?", "answer": " Temporal misalignment between pretraining data and evaluation is shown to have a significant impact on the performance of language models.", "ref_chunk": "and Academic score low on predicted quality, indicating that overly-specific positively-defined filters on web documents may remove substantial amounts of potentially useful specialized text. Time Comparing across different collection times of C4 (in Figure 9), we see a couple of steady trends. The percentage of non-ASCII characters increased steadily in more recent years while the measured text quality declines. This growth may be due to increasing non-English content, but could also correspond to rising use of emojis and non-ASCII punctuation. Toxicity scores also decrease slightly in later years, while sentiment increases. 8 4 Impact of Dataset Age on Pretrained Models Section Findings Both models and evaluation datasets become stale. Temporal misalignment between pretraining and evaluation data is not overcome by finetuning. Temporal misalignment complicates evaluation of models trained at different times, as older evaluation datasets may become stale and newer evaluation datasets may under-estimate performance of older models. The effects of pretraining misalignment are stronger for larger models than smaller models. While models are frequently and cheaply updated with new finetuning data, the expense of pretraining means the NLP community has relied on relatively few static pretrained models that are rarely updated or exchanged. BERT, RoBERTa, GPT-2, and T5 variants, all pretrained prior to 2020, constitute the majority (estimated at ~58% as of April 16, 2023) of all models downloaded on HuggingFace. Prior work demonstrates that language use changes over time (Altmann et al., 2009; Labov, 2011) and that temporal misalignment between finetuning and evaluation datasets correlates with degraded performance, visible across settings and domains (Luu et al., 2021; Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Jang et al., 2022). In contrast, we examine the effect of temporal misalignment between pretraining data and evaluation. In evaluating the impact of pretraining time across data domains, we can quantify the impact this design choice has on NLP broadly. 20102012201420162013201620192022Pretrain Years 82.789.091.271.270.874.771.582.082.274.980.488.190.670.972.375.872.682.282.476.080.287.890.770.472.075.873.483.182.875.979.487.189.470.871.475.071.082.583.376.8PoliAff 85.084.984.982.783.183.585.284.885.983.183.483.484.684.484.684.083.884.682.783.784.482.982.783.6TwiERC 23.632.127.717.923.332.027.919.122.831.227.418.122.731.227.417.8NewSum 98.298.095.091.394.488.798.198.295.293.195.188.597.898.793.993.496.090.597.698.494.491.495.189.0AIC 20102012201420162013201620192022 78.979.278.575.176.878.779.076.375.076.377.173.274.075.776.873.4PubCLS 201420152016201720182019Eval Years2013201620192022Pretrain Years 2012201320142015201620172018201920202021Eval Years2013201620192022 2014201520162017201820192013201620192022 Figure 3: Temporal Misalignment between Pretraining and Evaluation causes performance degradation. Four LM-XL\u2019s, each pretained on a different C4 time split, are evaluated on each time split across five datasets. Heatmap colors are normalized by column, following Luu et al. (2021) to show the best pretraining year for each evaluation year. We pretrain four autoregressive language models on versions of C4: 2013, 2016, 2019, and 2022. For each version we begin with Common Crawl data and remove all data that was scraped after the cutoff year. Following Luu et al. (2021), we measure the effect of temporal misalignment by using evaluation tasks (from News, Twitter, and Science domains) that have training and test sets split by year. After pretraining, we finetune each model on each dataset\u2019s training-year split separately, then evaluate on every test-year split. Full details and results are in Appendix C.4 and Appendix E.1, respectively. First, we replicate the performance degradation observed by Luu et al. (2021) due to finetuning and evaluation misalignment on the five tasks in Figure 12. Next, we estimate the effects of temporal misalignment between pretraining and evaluation (Figure 3). Since all models were finetuned on the training sets of the evaluation 9 tasks, we show that temporal misalignment during pretraining persists even with temporally-relevant finetuning data. 8-7-6-5-4-3-2-101234567891012Pretrain Year - Evaluation Year 6Relative Improvement (%) Figure 4: The mean relative performance over 5 datasets (y-axis) increases as temporal misalignment (x-axis) approaches zero. The boxplot indicates the median (solid line), mean (triangles), quartile range (boxes), and rest of the distribution (whiskers). Note that each dataset has different evaluation year ranges. Performance degradation strongly correlates with pretraining misalignment and its effects are non-trivial. Luu et al. (2021) formalize a definition for Temporal Degradation (TD), which measures the performance change observed from one year difference between the finetuning and evaluation years. We generalize TD to also measure the effect of one year difference between pretraining time and evaluation time, as described in Appendix C.4. Furthermore, we measure the Pearson correlation r between the performance difference and the temporal difference to understand the strength of the correlation. In Table 2 we find temporal degradation is highest for finetuning (2.8 on average), as expected, but also surprisingly high for one year of pretraining (0.4)\u2014particularly for the News domain. The average Pearson correlation of 0.61 indicates a strong correlation between pretraining temporal misalignment and performance degradation. All five tasks pass a one-sided Wald test with p < 0.05, validating the slope is greater than zero. Finetuning Pretraining Domain Task LM-Small TD r LM-XL TD PubCLS 5.82 NewSum 0.80 PoliAff 3.74 TwiERC 0.49 0.94 News Twitter Science AIC 0.84 5.63 0.80 0.82 2.91 0.92 0.84 4.93 0.89 0.73 0.53 0.82 0.83 0.24 0.36 r LM-Small TD r r 0.02 0.01\u2020 0.59 0.67 -0.31 -0.29 0.73 0.45 0.21 0.28 0.56 0.50 0.27 0.23 0.72 0.05 0.11 0.18\u2020 0.23 0.66 0.07 0.41 0.61 0.08 LM-XL TD Mean 2.36 0.81 2.84 0.76 Table 2: Temporal Degradation (TD) measures the expected performance degradation from one year of tem- poral misalignment. We report TD first between finetuning and evaluation, then pretraining and evaluation, for LM-XL and LM-Small, across five tasks. Pearson correlation r indicates the correlation strength between performance and temporal change. Temporal Degradation due to pretraining is significant and persistent across domains. All correlations are significant at p < 0.05 unless marked with \u2020. Pretraining misalignment is not overcome by significant finetuning. The temporal degradation due to pretraining suggests models pretrained on data from the same time frame as target evaluations will 10 have advantages over models trained on much older or newer data. Notably, this effect is observed for models which are finetuned on the full temporally-relevant training sets. This suggests that even substantial finetuning cannot overcome pretraining data that is temporally misaligned. Pretraining misalignment effects are asymmetric and have implications for NLP evaluations. We observe performance degradation regardless of whether the pretraining data was collected before or after the evalua- tion data. While we would not expect a 2019 checkpoint to perform well on questions about COVID, we also find that 2022"}, {"question": " What is the concept of Temporal Degradation (TD) as discussed in the text?", "answer": " Temporal Degradation measures the performance change observed from one year difference between the finetuning and evaluation years, as well as between pretraining time and evaluation time.", "ref_chunk": "and Academic score low on predicted quality, indicating that overly-specific positively-defined filters on web documents may remove substantial amounts of potentially useful specialized text. Time Comparing across different collection times of C4 (in Figure 9), we see a couple of steady trends. The percentage of non-ASCII characters increased steadily in more recent years while the measured text quality declines. This growth may be due to increasing non-English content, but could also correspond to rising use of emojis and non-ASCII punctuation. Toxicity scores also decrease slightly in later years, while sentiment increases. 8 4 Impact of Dataset Age on Pretrained Models Section Findings Both models and evaluation datasets become stale. Temporal misalignment between pretraining and evaluation data is not overcome by finetuning. Temporal misalignment complicates evaluation of models trained at different times, as older evaluation datasets may become stale and newer evaluation datasets may under-estimate performance of older models. The effects of pretraining misalignment are stronger for larger models than smaller models. While models are frequently and cheaply updated with new finetuning data, the expense of pretraining means the NLP community has relied on relatively few static pretrained models that are rarely updated or exchanged. BERT, RoBERTa, GPT-2, and T5 variants, all pretrained prior to 2020, constitute the majority (estimated at ~58% as of April 16, 2023) of all models downloaded on HuggingFace. Prior work demonstrates that language use changes over time (Altmann et al., 2009; Labov, 2011) and that temporal misalignment between finetuning and evaluation datasets correlates with degraded performance, visible across settings and domains (Luu et al., 2021; Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Jang et al., 2022). In contrast, we examine the effect of temporal misalignment between pretraining data and evaluation. In evaluating the impact of pretraining time across data domains, we can quantify the impact this design choice has on NLP broadly. 20102012201420162013201620192022Pretrain Years 82.789.091.271.270.874.771.582.082.274.980.488.190.670.972.375.872.682.282.476.080.287.890.770.472.075.873.483.182.875.979.487.189.470.871.475.071.082.583.376.8PoliAff 85.084.984.982.783.183.585.284.885.983.183.483.484.684.484.684.083.884.682.783.784.482.982.783.6TwiERC 23.632.127.717.923.332.027.919.122.831.227.418.122.731.227.417.8NewSum 98.298.095.091.394.488.798.198.295.293.195.188.597.898.793.993.496.090.597.698.494.491.495.189.0AIC 20102012201420162013201620192022 78.979.278.575.176.878.779.076.375.076.377.173.274.075.776.873.4PubCLS 201420152016201720182019Eval Years2013201620192022Pretrain Years 2012201320142015201620172018201920202021Eval Years2013201620192022 2014201520162017201820192013201620192022 Figure 3: Temporal Misalignment between Pretraining and Evaluation causes performance degradation. Four LM-XL\u2019s, each pretained on a different C4 time split, are evaluated on each time split across five datasets. Heatmap colors are normalized by column, following Luu et al. (2021) to show the best pretraining year for each evaluation year. We pretrain four autoregressive language models on versions of C4: 2013, 2016, 2019, and 2022. For each version we begin with Common Crawl data and remove all data that was scraped after the cutoff year. Following Luu et al. (2021), we measure the effect of temporal misalignment by using evaluation tasks (from News, Twitter, and Science domains) that have training and test sets split by year. After pretraining, we finetune each model on each dataset\u2019s training-year split separately, then evaluate on every test-year split. Full details and results are in Appendix C.4 and Appendix E.1, respectively. First, we replicate the performance degradation observed by Luu et al. (2021) due to finetuning and evaluation misalignment on the five tasks in Figure 12. Next, we estimate the effects of temporal misalignment between pretraining and evaluation (Figure 3). Since all models were finetuned on the training sets of the evaluation 9 tasks, we show that temporal misalignment during pretraining persists even with temporally-relevant finetuning data. 8-7-6-5-4-3-2-101234567891012Pretrain Year - Evaluation Year 6Relative Improvement (%) Figure 4: The mean relative performance over 5 datasets (y-axis) increases as temporal misalignment (x-axis) approaches zero. The boxplot indicates the median (solid line), mean (triangles), quartile range (boxes), and rest of the distribution (whiskers). Note that each dataset has different evaluation year ranges. Performance degradation strongly correlates with pretraining misalignment and its effects are non-trivial. Luu et al. (2021) formalize a definition for Temporal Degradation (TD), which measures the performance change observed from one year difference between the finetuning and evaluation years. We generalize TD to also measure the effect of one year difference between pretraining time and evaluation time, as described in Appendix C.4. Furthermore, we measure the Pearson correlation r between the performance difference and the temporal difference to understand the strength of the correlation. In Table 2 we find temporal degradation is highest for finetuning (2.8 on average), as expected, but also surprisingly high for one year of pretraining (0.4)\u2014particularly for the News domain. The average Pearson correlation of 0.61 indicates a strong correlation between pretraining temporal misalignment and performance degradation. All five tasks pass a one-sided Wald test with p < 0.05, validating the slope is greater than zero. Finetuning Pretraining Domain Task LM-Small TD r LM-XL TD PubCLS 5.82 NewSum 0.80 PoliAff 3.74 TwiERC 0.49 0.94 News Twitter Science AIC 0.84 5.63 0.80 0.82 2.91 0.92 0.84 4.93 0.89 0.73 0.53 0.82 0.83 0.24 0.36 r LM-Small TD r r 0.02 0.01\u2020 0.59 0.67 -0.31 -0.29 0.73 0.45 0.21 0.28 0.56 0.50 0.27 0.23 0.72 0.05 0.11 0.18\u2020 0.23 0.66 0.07 0.41 0.61 0.08 LM-XL TD Mean 2.36 0.81 2.84 0.76 Table 2: Temporal Degradation (TD) measures the expected performance degradation from one year of tem- poral misalignment. We report TD first between finetuning and evaluation, then pretraining and evaluation, for LM-XL and LM-Small, across five tasks. Pearson correlation r indicates the correlation strength between performance and temporal change. Temporal Degradation due to pretraining is significant and persistent across domains. All correlations are significant at p < 0.05 unless marked with \u2020. Pretraining misalignment is not overcome by significant finetuning. The temporal degradation due to pretraining suggests models pretrained on data from the same time frame as target evaluations will 10 have advantages over models trained on much older or newer data. Notably, this effect is observed for models which are finetuned on the full temporally-relevant training sets. This suggests that even substantial finetuning cannot overcome pretraining data that is temporally misaligned. Pretraining misalignment effects are asymmetric and have implications for NLP evaluations. We observe performance degradation regardless of whether the pretraining data was collected before or after the evalua- tion data. While we would not expect a 2019 checkpoint to perform well on questions about COVID, we also find that 2022"}, {"question": " What are the implications of pretraining misalignment on NLP evaluations?", "answer": " Pretraining misalignment is not overcome by significant finetuning, suggesting that models pretrained on data from the same time frame as target evaluations have advantages over models trained on much older or newer data.", "ref_chunk": "and Academic score low on predicted quality, indicating that overly-specific positively-defined filters on web documents may remove substantial amounts of potentially useful specialized text. Time Comparing across different collection times of C4 (in Figure 9), we see a couple of steady trends. The percentage of non-ASCII characters increased steadily in more recent years while the measured text quality declines. This growth may be due to increasing non-English content, but could also correspond to rising use of emojis and non-ASCII punctuation. Toxicity scores also decrease slightly in later years, while sentiment increases. 8 4 Impact of Dataset Age on Pretrained Models Section Findings Both models and evaluation datasets become stale. Temporal misalignment between pretraining and evaluation data is not overcome by finetuning. Temporal misalignment complicates evaluation of models trained at different times, as older evaluation datasets may become stale and newer evaluation datasets may under-estimate performance of older models. The effects of pretraining misalignment are stronger for larger models than smaller models. While models are frequently and cheaply updated with new finetuning data, the expense of pretraining means the NLP community has relied on relatively few static pretrained models that are rarely updated or exchanged. BERT, RoBERTa, GPT-2, and T5 variants, all pretrained prior to 2020, constitute the majority (estimated at ~58% as of April 16, 2023) of all models downloaded on HuggingFace. Prior work demonstrates that language use changes over time (Altmann et al., 2009; Labov, 2011) and that temporal misalignment between finetuning and evaluation datasets correlates with degraded performance, visible across settings and domains (Luu et al., 2021; Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Jang et al., 2022). In contrast, we examine the effect of temporal misalignment between pretraining data and evaluation. In evaluating the impact of pretraining time across data domains, we can quantify the impact this design choice has on NLP broadly. 20102012201420162013201620192022Pretrain Years 82.789.091.271.270.874.771.582.082.274.980.488.190.670.972.375.872.682.282.476.080.287.890.770.472.075.873.483.182.875.979.487.189.470.871.475.071.082.583.376.8PoliAff 85.084.984.982.783.183.585.284.885.983.183.483.484.684.484.684.083.884.682.783.784.482.982.783.6TwiERC 23.632.127.717.923.332.027.919.122.831.227.418.122.731.227.417.8NewSum 98.298.095.091.394.488.798.198.295.293.195.188.597.898.793.993.496.090.597.698.494.491.495.189.0AIC 20102012201420162013201620192022 78.979.278.575.176.878.779.076.375.076.377.173.274.075.776.873.4PubCLS 201420152016201720182019Eval Years2013201620192022Pretrain Years 2012201320142015201620172018201920202021Eval Years2013201620192022 2014201520162017201820192013201620192022 Figure 3: Temporal Misalignment between Pretraining and Evaluation causes performance degradation. Four LM-XL\u2019s, each pretained on a different C4 time split, are evaluated on each time split across five datasets. Heatmap colors are normalized by column, following Luu et al. (2021) to show the best pretraining year for each evaluation year. We pretrain four autoregressive language models on versions of C4: 2013, 2016, 2019, and 2022. For each version we begin with Common Crawl data and remove all data that was scraped after the cutoff year. Following Luu et al. (2021), we measure the effect of temporal misalignment by using evaluation tasks (from News, Twitter, and Science domains) that have training and test sets split by year. After pretraining, we finetune each model on each dataset\u2019s training-year split separately, then evaluate on every test-year split. Full details and results are in Appendix C.4 and Appendix E.1, respectively. First, we replicate the performance degradation observed by Luu et al. (2021) due to finetuning and evaluation misalignment on the five tasks in Figure 12. Next, we estimate the effects of temporal misalignment between pretraining and evaluation (Figure 3). Since all models were finetuned on the training sets of the evaluation 9 tasks, we show that temporal misalignment during pretraining persists even with temporally-relevant finetuning data. 8-7-6-5-4-3-2-101234567891012Pretrain Year - Evaluation Year 6Relative Improvement (%) Figure 4: The mean relative performance over 5 datasets (y-axis) increases as temporal misalignment (x-axis) approaches zero. The boxplot indicates the median (solid line), mean (triangles), quartile range (boxes), and rest of the distribution (whiskers). Note that each dataset has different evaluation year ranges. Performance degradation strongly correlates with pretraining misalignment and its effects are non-trivial. Luu et al. (2021) formalize a definition for Temporal Degradation (TD), which measures the performance change observed from one year difference between the finetuning and evaluation years. We generalize TD to also measure the effect of one year difference between pretraining time and evaluation time, as described in Appendix C.4. Furthermore, we measure the Pearson correlation r between the performance difference and the temporal difference to understand the strength of the correlation. In Table 2 we find temporal degradation is highest for finetuning (2.8 on average), as expected, but also surprisingly high for one year of pretraining (0.4)\u2014particularly for the News domain. The average Pearson correlation of 0.61 indicates a strong correlation between pretraining temporal misalignment and performance degradation. All five tasks pass a one-sided Wald test with p < 0.05, validating the slope is greater than zero. Finetuning Pretraining Domain Task LM-Small TD r LM-XL TD PubCLS 5.82 NewSum 0.80 PoliAff 3.74 TwiERC 0.49 0.94 News Twitter Science AIC 0.84 5.63 0.80 0.82 2.91 0.92 0.84 4.93 0.89 0.73 0.53 0.82 0.83 0.24 0.36 r LM-Small TD r r 0.02 0.01\u2020 0.59 0.67 -0.31 -0.29 0.73 0.45 0.21 0.28 0.56 0.50 0.27 0.23 0.72 0.05 0.11 0.18\u2020 0.23 0.66 0.07 0.41 0.61 0.08 LM-XL TD Mean 2.36 0.81 2.84 0.76 Table 2: Temporal Degradation (TD) measures the expected performance degradation from one year of tem- poral misalignment. We report TD first between finetuning and evaluation, then pretraining and evaluation, for LM-XL and LM-Small, across five tasks. Pearson correlation r indicates the correlation strength between performance and temporal change. Temporal Degradation due to pretraining is significant and persistent across domains. All correlations are significant at p < 0.05 unless marked with \u2020. Pretraining misalignment is not overcome by significant finetuning. The temporal degradation due to pretraining suggests models pretrained on data from the same time frame as target evaluations will 10 have advantages over models trained on much older or newer data. Notably, this effect is observed for models which are finetuned on the full temporally-relevant training sets. This suggests that even substantial finetuning cannot overcome pretraining data that is temporally misaligned. Pretraining misalignment effects are asymmetric and have implications for NLP evaluations. We observe performance degradation regardless of whether the pretraining data was collected before or after the evalua- tion data. While we would not expect a 2019 checkpoint to perform well on questions about COVID, we also find that 2022"}, {"question": " How do the effects of pretraining misalignment differ for larger models compared to smaller models?", "answer": " The effects of pretraining misalignment are stronger for larger models than smaller models.", "ref_chunk": "and Academic score low on predicted quality, indicating that overly-specific positively-defined filters on web documents may remove substantial amounts of potentially useful specialized text. Time Comparing across different collection times of C4 (in Figure 9), we see a couple of steady trends. The percentage of non-ASCII characters increased steadily in more recent years while the measured text quality declines. This growth may be due to increasing non-English content, but could also correspond to rising use of emojis and non-ASCII punctuation. Toxicity scores also decrease slightly in later years, while sentiment increases. 8 4 Impact of Dataset Age on Pretrained Models Section Findings Both models and evaluation datasets become stale. Temporal misalignment between pretraining and evaluation data is not overcome by finetuning. Temporal misalignment complicates evaluation of models trained at different times, as older evaluation datasets may become stale and newer evaluation datasets may under-estimate performance of older models. The effects of pretraining misalignment are stronger for larger models than smaller models. While models are frequently and cheaply updated with new finetuning data, the expense of pretraining means the NLP community has relied on relatively few static pretrained models that are rarely updated or exchanged. BERT, RoBERTa, GPT-2, and T5 variants, all pretrained prior to 2020, constitute the majority (estimated at ~58% as of April 16, 2023) of all models downloaded on HuggingFace. Prior work demonstrates that language use changes over time (Altmann et al., 2009; Labov, 2011) and that temporal misalignment between finetuning and evaluation datasets correlates with degraded performance, visible across settings and domains (Luu et al., 2021; Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Jang et al., 2022). In contrast, we examine the effect of temporal misalignment between pretraining data and evaluation. In evaluating the impact of pretraining time across data domains, we can quantify the impact this design choice has on NLP broadly. 20102012201420162013201620192022Pretrain Years 82.789.091.271.270.874.771.582.082.274.980.488.190.670.972.375.872.682.282.476.080.287.890.770.472.075.873.483.182.875.979.487.189.470.871.475.071.082.583.376.8PoliAff 85.084.984.982.783.183.585.284.885.983.183.483.484.684.484.684.083.884.682.783.784.482.982.783.6TwiERC 23.632.127.717.923.332.027.919.122.831.227.418.122.731.227.417.8NewSum 98.298.095.091.394.488.798.198.295.293.195.188.597.898.793.993.496.090.597.698.494.491.495.189.0AIC 20102012201420162013201620192022 78.979.278.575.176.878.779.076.375.076.377.173.274.075.776.873.4PubCLS 201420152016201720182019Eval Years2013201620192022Pretrain Years 2012201320142015201620172018201920202021Eval Years2013201620192022 2014201520162017201820192013201620192022 Figure 3: Temporal Misalignment between Pretraining and Evaluation causes performance degradation. Four LM-XL\u2019s, each pretained on a different C4 time split, are evaluated on each time split across five datasets. Heatmap colors are normalized by column, following Luu et al. (2021) to show the best pretraining year for each evaluation year. We pretrain four autoregressive language models on versions of C4: 2013, 2016, 2019, and 2022. For each version we begin with Common Crawl data and remove all data that was scraped after the cutoff year. Following Luu et al. (2021), we measure the effect of temporal misalignment by using evaluation tasks (from News, Twitter, and Science domains) that have training and test sets split by year. After pretraining, we finetune each model on each dataset\u2019s training-year split separately, then evaluate on every test-year split. Full details and results are in Appendix C.4 and Appendix E.1, respectively. First, we replicate the performance degradation observed by Luu et al. (2021) due to finetuning and evaluation misalignment on the five tasks in Figure 12. Next, we estimate the effects of temporal misalignment between pretraining and evaluation (Figure 3). Since all models were finetuned on the training sets of the evaluation 9 tasks, we show that temporal misalignment during pretraining persists even with temporally-relevant finetuning data. 8-7-6-5-4-3-2-101234567891012Pretrain Year - Evaluation Year 6Relative Improvement (%) Figure 4: The mean relative performance over 5 datasets (y-axis) increases as temporal misalignment (x-axis) approaches zero. The boxplot indicates the median (solid line), mean (triangles), quartile range (boxes), and rest of the distribution (whiskers). Note that each dataset has different evaluation year ranges. Performance degradation strongly correlates with pretraining misalignment and its effects are non-trivial. Luu et al. (2021) formalize a definition for Temporal Degradation (TD), which measures the performance change observed from one year difference between the finetuning and evaluation years. We generalize TD to also measure the effect of one year difference between pretraining time and evaluation time, as described in Appendix C.4. Furthermore, we measure the Pearson correlation r between the performance difference and the temporal difference to understand the strength of the correlation. In Table 2 we find temporal degradation is highest for finetuning (2.8 on average), as expected, but also surprisingly high for one year of pretraining (0.4)\u2014particularly for the News domain. The average Pearson correlation of 0.61 indicates a strong correlation between pretraining temporal misalignment and performance degradation. All five tasks pass a one-sided Wald test with p < 0.05, validating the slope is greater than zero. Finetuning Pretraining Domain Task LM-Small TD r LM-XL TD PubCLS 5.82 NewSum 0.80 PoliAff 3.74 TwiERC 0.49 0.94 News Twitter Science AIC 0.84 5.63 0.80 0.82 2.91 0.92 0.84 4.93 0.89 0.73 0.53 0.82 0.83 0.24 0.36 r LM-Small TD r r 0.02 0.01\u2020 0.59 0.67 -0.31 -0.29 0.73 0.45 0.21 0.28 0.56 0.50 0.27 0.23 0.72 0.05 0.11 0.18\u2020 0.23 0.66 0.07 0.41 0.61 0.08 LM-XL TD Mean 2.36 0.81 2.84 0.76 Table 2: Temporal Degradation (TD) measures the expected performance degradation from one year of tem- poral misalignment. We report TD first between finetuning and evaluation, then pretraining and evaluation, for LM-XL and LM-Small, across five tasks. Pearson correlation r indicates the correlation strength between performance and temporal change. Temporal Degradation due to pretraining is significant and persistent across domains. All correlations are significant at p < 0.05 unless marked with \u2020. Pretraining misalignment is not overcome by significant finetuning. The temporal degradation due to pretraining suggests models pretrained on data from the same time frame as target evaluations will 10 have advantages over models trained on much older or newer data. Notably, this effect is observed for models which are finetuned on the full temporally-relevant training sets. This suggests that even substantial finetuning cannot overcome pretraining data that is temporally misaligned. Pretraining misalignment effects are asymmetric and have implications for NLP evaluations. We observe performance degradation regardless of whether the pretraining data was collected before or after the evalua- tion data. While we would not expect a 2019 checkpoint to perform well on questions about COVID, we also find that 2022"}, {"question": " What is the average Pearson correlation between pretraining temporal misalignment and performance degradation as mentioned in the text?", "answer": " The average Pearson correlation of 0.61 indicates a strong correlation between pretraining temporal misalignment and performance degradation.", "ref_chunk": "and Academic score low on predicted quality, indicating that overly-specific positively-defined filters on web documents may remove substantial amounts of potentially useful specialized text. Time Comparing across different collection times of C4 (in Figure 9), we see a couple of steady trends. The percentage of non-ASCII characters increased steadily in more recent years while the measured text quality declines. This growth may be due to increasing non-English content, but could also correspond to rising use of emojis and non-ASCII punctuation. Toxicity scores also decrease slightly in later years, while sentiment increases. 8 4 Impact of Dataset Age on Pretrained Models Section Findings Both models and evaluation datasets become stale. Temporal misalignment between pretraining and evaluation data is not overcome by finetuning. Temporal misalignment complicates evaluation of models trained at different times, as older evaluation datasets may become stale and newer evaluation datasets may under-estimate performance of older models. The effects of pretraining misalignment are stronger for larger models than smaller models. While models are frequently and cheaply updated with new finetuning data, the expense of pretraining means the NLP community has relied on relatively few static pretrained models that are rarely updated or exchanged. BERT, RoBERTa, GPT-2, and T5 variants, all pretrained prior to 2020, constitute the majority (estimated at ~58% as of April 16, 2023) of all models downloaded on HuggingFace. Prior work demonstrates that language use changes over time (Altmann et al., 2009; Labov, 2011) and that temporal misalignment between finetuning and evaluation datasets correlates with degraded performance, visible across settings and domains (Luu et al., 2021; Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Jang et al., 2022). In contrast, we examine the effect of temporal misalignment between pretraining data and evaluation. In evaluating the impact of pretraining time across data domains, we can quantify the impact this design choice has on NLP broadly. 20102012201420162013201620192022Pretrain Years 82.789.091.271.270.874.771.582.082.274.980.488.190.670.972.375.872.682.282.476.080.287.890.770.472.075.873.483.182.875.979.487.189.470.871.475.071.082.583.376.8PoliAff 85.084.984.982.783.183.585.284.885.983.183.483.484.684.484.684.083.884.682.783.784.482.982.783.6TwiERC 23.632.127.717.923.332.027.919.122.831.227.418.122.731.227.417.8NewSum 98.298.095.091.394.488.798.198.295.293.195.188.597.898.793.993.496.090.597.698.494.491.495.189.0AIC 20102012201420162013201620192022 78.979.278.575.176.878.779.076.375.076.377.173.274.075.776.873.4PubCLS 201420152016201720182019Eval Years2013201620192022Pretrain Years 2012201320142015201620172018201920202021Eval Years2013201620192022 2014201520162017201820192013201620192022 Figure 3: Temporal Misalignment between Pretraining and Evaluation causes performance degradation. Four LM-XL\u2019s, each pretained on a different C4 time split, are evaluated on each time split across five datasets. Heatmap colors are normalized by column, following Luu et al. (2021) to show the best pretraining year for each evaluation year. We pretrain four autoregressive language models on versions of C4: 2013, 2016, 2019, and 2022. For each version we begin with Common Crawl data and remove all data that was scraped after the cutoff year. Following Luu et al. (2021), we measure the effect of temporal misalignment by using evaluation tasks (from News, Twitter, and Science domains) that have training and test sets split by year. After pretraining, we finetune each model on each dataset\u2019s training-year split separately, then evaluate on every test-year split. Full details and results are in Appendix C.4 and Appendix E.1, respectively. First, we replicate the performance degradation observed by Luu et al. (2021) due to finetuning and evaluation misalignment on the five tasks in Figure 12. Next, we estimate the effects of temporal misalignment between pretraining and evaluation (Figure 3). Since all models were finetuned on the training sets of the evaluation 9 tasks, we show that temporal misalignment during pretraining persists even with temporally-relevant finetuning data. 8-7-6-5-4-3-2-101234567891012Pretrain Year - Evaluation Year 6Relative Improvement (%) Figure 4: The mean relative performance over 5 datasets (y-axis) increases as temporal misalignment (x-axis) approaches zero. The boxplot indicates the median (solid line), mean (triangles), quartile range (boxes), and rest of the distribution (whiskers). Note that each dataset has different evaluation year ranges. Performance degradation strongly correlates with pretraining misalignment and its effects are non-trivial. Luu et al. (2021) formalize a definition for Temporal Degradation (TD), which measures the performance change observed from one year difference between the finetuning and evaluation years. We generalize TD to also measure the effect of one year difference between pretraining time and evaluation time, as described in Appendix C.4. Furthermore, we measure the Pearson correlation r between the performance difference and the temporal difference to understand the strength of the correlation. In Table 2 we find temporal degradation is highest for finetuning (2.8 on average), as expected, but also surprisingly high for one year of pretraining (0.4)\u2014particularly for the News domain. The average Pearson correlation of 0.61 indicates a strong correlation between pretraining temporal misalignment and performance degradation. All five tasks pass a one-sided Wald test with p < 0.05, validating the slope is greater than zero. Finetuning Pretraining Domain Task LM-Small TD r LM-XL TD PubCLS 5.82 NewSum 0.80 PoliAff 3.74 TwiERC 0.49 0.94 News Twitter Science AIC 0.84 5.63 0.80 0.82 2.91 0.92 0.84 4.93 0.89 0.73 0.53 0.82 0.83 0.24 0.36 r LM-Small TD r r 0.02 0.01\u2020 0.59 0.67 -0.31 -0.29 0.73 0.45 0.21 0.28 0.56 0.50 0.27 0.23 0.72 0.05 0.11 0.18\u2020 0.23 0.66 0.07 0.41 0.61 0.08 LM-XL TD Mean 2.36 0.81 2.84 0.76 Table 2: Temporal Degradation (TD) measures the expected performance degradation from one year of tem- poral misalignment. We report TD first between finetuning and evaluation, then pretraining and evaluation, for LM-XL and LM-Small, across five tasks. Pearson correlation r indicates the correlation strength between performance and temporal change. Temporal Degradation due to pretraining is significant and persistent across domains. All correlations are significant at p < 0.05 unless marked with \u2020. Pretraining misalignment is not overcome by significant finetuning. The temporal degradation due to pretraining suggests models pretrained on data from the same time frame as target evaluations will 10 have advantages over models trained on much older or newer data. Notably, this effect is observed for models which are finetuned on the full temporally-relevant training sets. This suggests that even substantial finetuning cannot overcome pretraining data that is temporally misaligned. Pretraining misalignment effects are asymmetric and have implications for NLP evaluations. We observe performance degradation regardless of whether the pretraining data was collected before or after the evalua- tion data. While we would not expect a 2019 checkpoint to perform well on questions about COVID, we also find that 2022"}], "doc_text": "and Academic score low on predicted quality, indicating that overly-specific positively-defined filters on web documents may remove substantial amounts of potentially useful specialized text. Time Comparing across different collection times of C4 (in Figure 9), we see a couple of steady trends. The percentage of non-ASCII characters increased steadily in more recent years while the measured text quality declines. This growth may be due to increasing non-English content, but could also correspond to rising use of emojis and non-ASCII punctuation. Toxicity scores also decrease slightly in later years, while sentiment increases. 8 4 Impact of Dataset Age on Pretrained Models Section Findings Both models and evaluation datasets become stale. Temporal misalignment between pretraining and evaluation data is not overcome by finetuning. Temporal misalignment complicates evaluation of models trained at different times, as older evaluation datasets may become stale and newer evaluation datasets may under-estimate performance of older models. The effects of pretraining misalignment are stronger for larger models than smaller models. While models are frequently and cheaply updated with new finetuning data, the expense of pretraining means the NLP community has relied on relatively few static pretrained models that are rarely updated or exchanged. BERT, RoBERTa, GPT-2, and T5 variants, all pretrained prior to 2020, constitute the majority (estimated at ~58% as of April 16, 2023) of all models downloaded on HuggingFace. Prior work demonstrates that language use changes over time (Altmann et al., 2009; Labov, 2011) and that temporal misalignment between finetuning and evaluation datasets correlates with degraded performance, visible across settings and domains (Luu et al., 2021; Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Jang et al., 2022). In contrast, we examine the effect of temporal misalignment between pretraining data and evaluation. In evaluating the impact of pretraining time across data domains, we can quantify the impact this design choice has on NLP broadly. 20102012201420162013201620192022Pretrain Years 82.789.091.271.270.874.771.582.082.274.980.488.190.670.972.375.872.682.282.476.080.287.890.770.472.075.873.483.182.875.979.487.189.470.871.475.071.082.583.376.8PoliAff 85.084.984.982.783.183.585.284.885.983.183.483.484.684.484.684.083.884.682.783.784.482.982.783.6TwiERC 23.632.127.717.923.332.027.919.122.831.227.418.122.731.227.417.8NewSum 98.298.095.091.394.488.798.198.295.293.195.188.597.898.793.993.496.090.597.698.494.491.495.189.0AIC 20102012201420162013201620192022 78.979.278.575.176.878.779.076.375.076.377.173.274.075.776.873.4PubCLS 201420152016201720182019Eval Years2013201620192022Pretrain Years 2012201320142015201620172018201920202021Eval Years2013201620192022 2014201520162017201820192013201620192022 Figure 3: Temporal Misalignment between Pretraining and Evaluation causes performance degradation. Four LM-XL\u2019s, each pretained on a different C4 time split, are evaluated on each time split across five datasets. Heatmap colors are normalized by column, following Luu et al. (2021) to show the best pretraining year for each evaluation year. We pretrain four autoregressive language models on versions of C4: 2013, 2016, 2019, and 2022. For each version we begin with Common Crawl data and remove all data that was scraped after the cutoff year. Following Luu et al. (2021), we measure the effect of temporal misalignment by using evaluation tasks (from News, Twitter, and Science domains) that have training and test sets split by year. After pretraining, we finetune each model on each dataset\u2019s training-year split separately, then evaluate on every test-year split. Full details and results are in Appendix C.4 and Appendix E.1, respectively. First, we replicate the performance degradation observed by Luu et al. (2021) due to finetuning and evaluation misalignment on the five tasks in Figure 12. Next, we estimate the effects of temporal misalignment between pretraining and evaluation (Figure 3). Since all models were finetuned on the training sets of the evaluation 9 tasks, we show that temporal misalignment during pretraining persists even with temporally-relevant finetuning data. 8-7-6-5-4-3-2-101234567891012Pretrain Year - Evaluation Year 6Relative Improvement (%) Figure 4: The mean relative performance over 5 datasets (y-axis) increases as temporal misalignment (x-axis) approaches zero. The boxplot indicates the median (solid line), mean (triangles), quartile range (boxes), and rest of the distribution (whiskers). Note that each dataset has different evaluation year ranges. Performance degradation strongly correlates with pretraining misalignment and its effects are non-trivial. Luu et al. (2021) formalize a definition for Temporal Degradation (TD), which measures the performance change observed from one year difference between the finetuning and evaluation years. We generalize TD to also measure the effect of one year difference between pretraining time and evaluation time, as described in Appendix C.4. Furthermore, we measure the Pearson correlation r between the performance difference and the temporal difference to understand the strength of the correlation. In Table 2 we find temporal degradation is highest for finetuning (2.8 on average), as expected, but also surprisingly high for one year of pretraining (0.4)\u2014particularly for the News domain. The average Pearson correlation of 0.61 indicates a strong correlation between pretraining temporal misalignment and performance degradation. All five tasks pass a one-sided Wald test with p < 0.05, validating the slope is greater than zero. Finetuning Pretraining Domain Task LM-Small TD r LM-XL TD PubCLS 5.82 NewSum 0.80 PoliAff 3.74 TwiERC 0.49 0.94 News Twitter Science AIC 0.84 5.63 0.80 0.82 2.91 0.92 0.84 4.93 0.89 0.73 0.53 0.82 0.83 0.24 0.36 r LM-Small TD r r 0.02 0.01\u2020 0.59 0.67 -0.31 -0.29 0.73 0.45 0.21 0.28 0.56 0.50 0.27 0.23 0.72 0.05 0.11 0.18\u2020 0.23 0.66 0.07 0.41 0.61 0.08 LM-XL TD Mean 2.36 0.81 2.84 0.76 Table 2: Temporal Degradation (TD) measures the expected performance degradation from one year of tem- poral misalignment. We report TD first between finetuning and evaluation, then pretraining and evaluation, for LM-XL and LM-Small, across five tasks. Pearson correlation r indicates the correlation strength between performance and temporal change. Temporal Degradation due to pretraining is significant and persistent across domains. All correlations are significant at p < 0.05 unless marked with \u2020. Pretraining misalignment is not overcome by significant finetuning. The temporal degradation due to pretraining suggests models pretrained on data from the same time frame as target evaluations will 10 have advantages over models trained on much older or newer data. Notably, this effect is observed for models which are finetuned on the full temporally-relevant training sets. This suggests that even substantial finetuning cannot overcome pretraining data that is temporally misaligned. Pretraining misalignment effects are asymmetric and have implications for NLP evaluations. We observe performance degradation regardless of whether the pretraining data was collected before or after the evalua- tion data. While we would not expect a 2019 checkpoint to perform well on questions about COVID, we also find that 2022"}