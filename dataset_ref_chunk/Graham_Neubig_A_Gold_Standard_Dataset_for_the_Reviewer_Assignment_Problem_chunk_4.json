{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_A_Gold_Standard_Dataset_for_the_Reviewer_Assignment_Problem_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the mean number of publications per participant?", "answer": " The mean number of publications per participant is 54.", "ref_chunk": "Min # publications Max # publications Mean # publications Median # publications 2 492 54 20 Table 1: Demography of participants. For the \ufb01rst four characteristics, quantities represent percentages of the one or more most popular classes in the dataset. Note that classi\ufb01cation is done manually based on publicly available information and may not be free of error. For the last characteristic, quantities are computed based on Semantic Scholar pro\ufb01les. 4 Data exploration In this section we explore the collected data and present various characteristics of the dataset. The subsequent sections then detail the results of using this data to benchmark various popular similarity-computation algorithms. 4.1 Participants We begin with a discussion of the pool of the survey participants: Table 1 displays its key characteristics. First, we note that all participants work in the broad area of computer science and have rich publication pro\ufb01les (at least two papers published, with the mean of 54 papers and the median of 20 papers). In many subareas of computer science, including machine learning and arti\ufb01cial intelligence, having two papers is usually su\ufb03cient to join the reviewer pool of \ufb02agship conferences. Given that approximately 85% of participants either have PhD or are in the process of getting the degree, we conclude that most of the researchers who contributed to our dataset are eligible to review for machine learning and arti\ufb01cial intelligence conferences. Second, we caveat that most of the participants are male researchers a\ufb03liated with US-based organi- zations, with about 40% of all participants being a\ufb03liated with Carnegie Mellon University. Hence, the population of participants is not necessarily representative of the general population of the machine learning and computer science communities. We encourage researchers to be aware of this limitation when using our dataset. We note that the data collection process does not conclude with the publication of the present paper and we will be updating the dataset as new responses come. We also encourage readers to contribute 5\u201310 minutes of their time to \ufb01ll out the survey https://forms.gle/SP1Rh8eivGz54xR37 and make the dataset more representative. 4.2 Papers We now describe the set of papers that constitute our dataset. Overall, participants evaluated their expertise in reviewing 463 unique papers. Out of these 463 papers, 12 papers appeared in reports of two participants, 1 paper was mentioned by three participants, and the remaining papers were mentioned by one participant each. 6 Total number of papers: 463 Characteristic Quantity Value Open Access # On semantic scholar # On arXiv # PDF available 462 411 457 Research Areas # Computer science 459 Publication Year % Before 2020 % 2020 or later 25 75 Table 2: Characteristics of the 463 papers in the released dataset. Most of the papers are freely available online and belong to the broad area of computer science. Table 2 presents several characteristics of the pool of papers in our dataset. First, we note that all but one of the papers are listed on Semantic Scholar, enabling similarity-computation algorithms developed on the dataset to query additional information about the papers from the Semantic Scholar database. For the paper that does not have a Semantic Scholar pro\ufb01le, we construct such a pro\ufb01le manually and keep this paper in the dataset. Additionally, most of the papers (99%) have their PDFs freely available online, thereby allowing algorithms to use full texts of papers to compute similarities. Semantic Scholar has a built-in tool to identify broad research areas of papers (Wade, 2022). According to this tool, 99% of the papers included to our dataset belong to the broad area of computer science\u2014the target area for our data-collection procedure. The remaining four papers belong to the neighboring \ufb01elds of mathematics, philosophy, and the computational branch of biology. Finally, approximately 75% of all papers in our dataset are published in or after 2020, ensuring that our dataset contains recent papers that similarity-computation algorithms encounter in practice. 4.3 Evaluations of expertise Finally, we proceed to the key aspect of our dataset\u2014evaluations of expertise in reviewing the papers reported by participants. All but one participant reported expertise in reviewing at least 5 papers with the mean number of papers per participant being 8.2 and the total number of expertise evaluations being 477. Figure 1 provides visualization of expertise evaluations made by participants. First, Figure 1a displays the histogram of expertise values. About two-thirds of the reported papers belong to the expertise area 125 (2, 3] (4, 5]Value of expertise 150 (3, 4] 175Count 75 25 100 50 [1, 2] 0 50 0 3 200 250 100 150 1 0 300Count 4Difference in expertise 2 (a) Histogram of expertise values. (b) Histogram of di\ufb00erences in expertise evaluations. Figure 1: Distribution of expertise scores reported by participants. 7 of participants (expertise score larger than 3), and about one-third evaluations are made for papers that reviewers are not competent in (expertise score 3 or lower), thereby yielding a good spread. Second, Figure 1b shows the distributions of pairwise di\ufb00erences in expertise evaluations made by the same reviewer. To construct this \ufb01gure, for each participant we considered all pairs of papers in their report. Next, we pooled the absolute values of the pairwise di\ufb00erences in expertise across participants. We then plotted the histogram of these di\ufb00erences in the \ufb01gure. Observe that the distribution in Figure 1b has a heavy tail, suggesting that our dataset is suitable for evaluating the accuracy of similarity-computation methods both at a coarse level (large di\ufb00erences between the values of expertise) and at a \ufb01ne level (small di\ufb00erences between the values of expertise). 5 Experimental setup We now describe the setup of experiments on our dataset. 5.1 Metric We begin with de\ufb01ning a main metric that we use in this work to evaluate performance of the algorithms. For this, we rely on the Kendall\u2019s Tau distance that is closely related to the widely used Kendall\u2019s Tau (Kendall, 1938) rank correlation coe\ufb03cient. We introduce the metric and the algorithms in this section, followed by the results in Section 6. Subsequently in Section 7, we"}, {"question": " What percentage of participants have a PhD or are in the process of getting the degree?", "answer": " Approximately 85% of participants have a PhD or are in the process of getting the degree.", "ref_chunk": "Min # publications Max # publications Mean # publications Median # publications 2 492 54 20 Table 1: Demography of participants. For the \ufb01rst four characteristics, quantities represent percentages of the one or more most popular classes in the dataset. Note that classi\ufb01cation is done manually based on publicly available information and may not be free of error. For the last characteristic, quantities are computed based on Semantic Scholar pro\ufb01les. 4 Data exploration In this section we explore the collected data and present various characteristics of the dataset. The subsequent sections then detail the results of using this data to benchmark various popular similarity-computation algorithms. 4.1 Participants We begin with a discussion of the pool of the survey participants: Table 1 displays its key characteristics. First, we note that all participants work in the broad area of computer science and have rich publication pro\ufb01les (at least two papers published, with the mean of 54 papers and the median of 20 papers). In many subareas of computer science, including machine learning and arti\ufb01cial intelligence, having two papers is usually su\ufb03cient to join the reviewer pool of \ufb02agship conferences. Given that approximately 85% of participants either have PhD or are in the process of getting the degree, we conclude that most of the researchers who contributed to our dataset are eligible to review for machine learning and arti\ufb01cial intelligence conferences. Second, we caveat that most of the participants are male researchers a\ufb03liated with US-based organi- zations, with about 40% of all participants being a\ufb03liated with Carnegie Mellon University. Hence, the population of participants is not necessarily representative of the general population of the machine learning and computer science communities. We encourage researchers to be aware of this limitation when using our dataset. We note that the data collection process does not conclude with the publication of the present paper and we will be updating the dataset as new responses come. We also encourage readers to contribute 5\u201310 minutes of their time to \ufb01ll out the survey https://forms.gle/SP1Rh8eivGz54xR37 and make the dataset more representative. 4.2 Papers We now describe the set of papers that constitute our dataset. Overall, participants evaluated their expertise in reviewing 463 unique papers. Out of these 463 papers, 12 papers appeared in reports of two participants, 1 paper was mentioned by three participants, and the remaining papers were mentioned by one participant each. 6 Total number of papers: 463 Characteristic Quantity Value Open Access # On semantic scholar # On arXiv # PDF available 462 411 457 Research Areas # Computer science 459 Publication Year % Before 2020 % 2020 or later 25 75 Table 2: Characteristics of the 463 papers in the released dataset. Most of the papers are freely available online and belong to the broad area of computer science. Table 2 presents several characteristics of the pool of papers in our dataset. First, we note that all but one of the papers are listed on Semantic Scholar, enabling similarity-computation algorithms developed on the dataset to query additional information about the papers from the Semantic Scholar database. For the paper that does not have a Semantic Scholar pro\ufb01le, we construct such a pro\ufb01le manually and keep this paper in the dataset. Additionally, most of the papers (99%) have their PDFs freely available online, thereby allowing algorithms to use full texts of papers to compute similarities. Semantic Scholar has a built-in tool to identify broad research areas of papers (Wade, 2022). According to this tool, 99% of the papers included to our dataset belong to the broad area of computer science\u2014the target area for our data-collection procedure. The remaining four papers belong to the neighboring \ufb01elds of mathematics, philosophy, and the computational branch of biology. Finally, approximately 75% of all papers in our dataset are published in or after 2020, ensuring that our dataset contains recent papers that similarity-computation algorithms encounter in practice. 4.3 Evaluations of expertise Finally, we proceed to the key aspect of our dataset\u2014evaluations of expertise in reviewing the papers reported by participants. All but one participant reported expertise in reviewing at least 5 papers with the mean number of papers per participant being 8.2 and the total number of expertise evaluations being 477. Figure 1 provides visualization of expertise evaluations made by participants. First, Figure 1a displays the histogram of expertise values. About two-thirds of the reported papers belong to the expertise area 125 (2, 3] (4, 5]Value of expertise 150 (3, 4] 175Count 75 25 100 50 [1, 2] 0 50 0 3 200 250 100 150 1 0 300Count 4Difference in expertise 2 (a) Histogram of expertise values. (b) Histogram of di\ufb00erences in expertise evaluations. Figure 1: Distribution of expertise scores reported by participants. 7 of participants (expertise score larger than 3), and about one-third evaluations are made for papers that reviewers are not competent in (expertise score 3 or lower), thereby yielding a good spread. Second, Figure 1b shows the distributions of pairwise di\ufb00erences in expertise evaluations made by the same reviewer. To construct this \ufb01gure, for each participant we considered all pairs of papers in their report. Next, we pooled the absolute values of the pairwise di\ufb00erences in expertise across participants. We then plotted the histogram of these di\ufb00erences in the \ufb01gure. Observe that the distribution in Figure 1b has a heavy tail, suggesting that our dataset is suitable for evaluating the accuracy of similarity-computation methods both at a coarse level (large di\ufb00erences between the values of expertise) and at a \ufb01ne level (small di\ufb00erences between the values of expertise). 5 Experimental setup We now describe the setup of experiments on our dataset. 5.1 Metric We begin with de\ufb01ning a main metric that we use in this work to evaluate performance of the algorithms. For this, we rely on the Kendall\u2019s Tau distance that is closely related to the widely used Kendall\u2019s Tau (Kendall, 1938) rank correlation coe\ufb03cient. We introduce the metric and the algorithms in this section, followed by the results in Section 6. Subsequently in Section 7, we"}, {"question": " What percentage of all participants are affiliated with Carnegie Mellon University?", "answer": " About 40% of all participants are affiliated with Carnegie Mellon University.", "ref_chunk": "Min # publications Max # publications Mean # publications Median # publications 2 492 54 20 Table 1: Demography of participants. For the \ufb01rst four characteristics, quantities represent percentages of the one or more most popular classes in the dataset. Note that classi\ufb01cation is done manually based on publicly available information and may not be free of error. For the last characteristic, quantities are computed based on Semantic Scholar pro\ufb01les. 4 Data exploration In this section we explore the collected data and present various characteristics of the dataset. The subsequent sections then detail the results of using this data to benchmark various popular similarity-computation algorithms. 4.1 Participants We begin with a discussion of the pool of the survey participants: Table 1 displays its key characteristics. First, we note that all participants work in the broad area of computer science and have rich publication pro\ufb01les (at least two papers published, with the mean of 54 papers and the median of 20 papers). In many subareas of computer science, including machine learning and arti\ufb01cial intelligence, having two papers is usually su\ufb03cient to join the reviewer pool of \ufb02agship conferences. Given that approximately 85% of participants either have PhD or are in the process of getting the degree, we conclude that most of the researchers who contributed to our dataset are eligible to review for machine learning and arti\ufb01cial intelligence conferences. Second, we caveat that most of the participants are male researchers a\ufb03liated with US-based organi- zations, with about 40% of all participants being a\ufb03liated with Carnegie Mellon University. Hence, the population of participants is not necessarily representative of the general population of the machine learning and computer science communities. We encourage researchers to be aware of this limitation when using our dataset. We note that the data collection process does not conclude with the publication of the present paper and we will be updating the dataset as new responses come. We also encourage readers to contribute 5\u201310 minutes of their time to \ufb01ll out the survey https://forms.gle/SP1Rh8eivGz54xR37 and make the dataset more representative. 4.2 Papers We now describe the set of papers that constitute our dataset. Overall, participants evaluated their expertise in reviewing 463 unique papers. Out of these 463 papers, 12 papers appeared in reports of two participants, 1 paper was mentioned by three participants, and the remaining papers were mentioned by one participant each. 6 Total number of papers: 463 Characteristic Quantity Value Open Access # On semantic scholar # On arXiv # PDF available 462 411 457 Research Areas # Computer science 459 Publication Year % Before 2020 % 2020 or later 25 75 Table 2: Characteristics of the 463 papers in the released dataset. Most of the papers are freely available online and belong to the broad area of computer science. Table 2 presents several characteristics of the pool of papers in our dataset. First, we note that all but one of the papers are listed on Semantic Scholar, enabling similarity-computation algorithms developed on the dataset to query additional information about the papers from the Semantic Scholar database. For the paper that does not have a Semantic Scholar pro\ufb01le, we construct such a pro\ufb01le manually and keep this paper in the dataset. Additionally, most of the papers (99%) have their PDFs freely available online, thereby allowing algorithms to use full texts of papers to compute similarities. Semantic Scholar has a built-in tool to identify broad research areas of papers (Wade, 2022). According to this tool, 99% of the papers included to our dataset belong to the broad area of computer science\u2014the target area for our data-collection procedure. The remaining four papers belong to the neighboring \ufb01elds of mathematics, philosophy, and the computational branch of biology. Finally, approximately 75% of all papers in our dataset are published in or after 2020, ensuring that our dataset contains recent papers that similarity-computation algorithms encounter in practice. 4.3 Evaluations of expertise Finally, we proceed to the key aspect of our dataset\u2014evaluations of expertise in reviewing the papers reported by participants. All but one participant reported expertise in reviewing at least 5 papers with the mean number of papers per participant being 8.2 and the total number of expertise evaluations being 477. Figure 1 provides visualization of expertise evaluations made by participants. First, Figure 1a displays the histogram of expertise values. About two-thirds of the reported papers belong to the expertise area 125 (2, 3] (4, 5]Value of expertise 150 (3, 4] 175Count 75 25 100 50 [1, 2] 0 50 0 3 200 250 100 150 1 0 300Count 4Difference in expertise 2 (a) Histogram of expertise values. (b) Histogram of di\ufb00erences in expertise evaluations. Figure 1: Distribution of expertise scores reported by participants. 7 of participants (expertise score larger than 3), and about one-third evaluations are made for papers that reviewers are not competent in (expertise score 3 or lower), thereby yielding a good spread. Second, Figure 1b shows the distributions of pairwise di\ufb00erences in expertise evaluations made by the same reviewer. To construct this \ufb01gure, for each participant we considered all pairs of papers in their report. Next, we pooled the absolute values of the pairwise di\ufb00erences in expertise across participants. We then plotted the histogram of these di\ufb00erences in the \ufb01gure. Observe that the distribution in Figure 1b has a heavy tail, suggesting that our dataset is suitable for evaluating the accuracy of similarity-computation methods both at a coarse level (large di\ufb00erences between the values of expertise) and at a \ufb01ne level (small di\ufb00erences between the values of expertise). 5 Experimental setup We now describe the setup of experiments on our dataset. 5.1 Metric We begin with de\ufb01ning a main metric that we use in this work to evaluate performance of the algorithms. For this, we rely on the Kendall\u2019s Tau distance that is closely related to the widely used Kendall\u2019s Tau (Kendall, 1938) rank correlation coe\ufb03cient. We introduce the metric and the algorithms in this section, followed by the results in Section 6. Subsequently in Section 7, we"}, {"question": " What is the total number of papers in the dataset?", "answer": " There are 463 papers in the dataset.", "ref_chunk": "Min # publications Max # publications Mean # publications Median # publications 2 492 54 20 Table 1: Demography of participants. For the \ufb01rst four characteristics, quantities represent percentages of the one or more most popular classes in the dataset. Note that classi\ufb01cation is done manually based on publicly available information and may not be free of error. For the last characteristic, quantities are computed based on Semantic Scholar pro\ufb01les. 4 Data exploration In this section we explore the collected data and present various characteristics of the dataset. The subsequent sections then detail the results of using this data to benchmark various popular similarity-computation algorithms. 4.1 Participants We begin with a discussion of the pool of the survey participants: Table 1 displays its key characteristics. First, we note that all participants work in the broad area of computer science and have rich publication pro\ufb01les (at least two papers published, with the mean of 54 papers and the median of 20 papers). In many subareas of computer science, including machine learning and arti\ufb01cial intelligence, having two papers is usually su\ufb03cient to join the reviewer pool of \ufb02agship conferences. Given that approximately 85% of participants either have PhD or are in the process of getting the degree, we conclude that most of the researchers who contributed to our dataset are eligible to review for machine learning and arti\ufb01cial intelligence conferences. Second, we caveat that most of the participants are male researchers a\ufb03liated with US-based organi- zations, with about 40% of all participants being a\ufb03liated with Carnegie Mellon University. Hence, the population of participants is not necessarily representative of the general population of the machine learning and computer science communities. We encourage researchers to be aware of this limitation when using our dataset. We note that the data collection process does not conclude with the publication of the present paper and we will be updating the dataset as new responses come. We also encourage readers to contribute 5\u201310 minutes of their time to \ufb01ll out the survey https://forms.gle/SP1Rh8eivGz54xR37 and make the dataset more representative. 4.2 Papers We now describe the set of papers that constitute our dataset. Overall, participants evaluated their expertise in reviewing 463 unique papers. Out of these 463 papers, 12 papers appeared in reports of two participants, 1 paper was mentioned by three participants, and the remaining papers were mentioned by one participant each. 6 Total number of papers: 463 Characteristic Quantity Value Open Access # On semantic scholar # On arXiv # PDF available 462 411 457 Research Areas # Computer science 459 Publication Year % Before 2020 % 2020 or later 25 75 Table 2: Characteristics of the 463 papers in the released dataset. Most of the papers are freely available online and belong to the broad area of computer science. Table 2 presents several characteristics of the pool of papers in our dataset. First, we note that all but one of the papers are listed on Semantic Scholar, enabling similarity-computation algorithms developed on the dataset to query additional information about the papers from the Semantic Scholar database. For the paper that does not have a Semantic Scholar pro\ufb01le, we construct such a pro\ufb01le manually and keep this paper in the dataset. Additionally, most of the papers (99%) have their PDFs freely available online, thereby allowing algorithms to use full texts of papers to compute similarities. Semantic Scholar has a built-in tool to identify broad research areas of papers (Wade, 2022). According to this tool, 99% of the papers included to our dataset belong to the broad area of computer science\u2014the target area for our data-collection procedure. The remaining four papers belong to the neighboring \ufb01elds of mathematics, philosophy, and the computational branch of biology. Finally, approximately 75% of all papers in our dataset are published in or after 2020, ensuring that our dataset contains recent papers that similarity-computation algorithms encounter in practice. 4.3 Evaluations of expertise Finally, we proceed to the key aspect of our dataset\u2014evaluations of expertise in reviewing the papers reported by participants. All but one participant reported expertise in reviewing at least 5 papers with the mean number of papers per participant being 8.2 and the total number of expertise evaluations being 477. Figure 1 provides visualization of expertise evaluations made by participants. First, Figure 1a displays the histogram of expertise values. About two-thirds of the reported papers belong to the expertise area 125 (2, 3] (4, 5]Value of expertise 150 (3, 4] 175Count 75 25 100 50 [1, 2] 0 50 0 3 200 250 100 150 1 0 300Count 4Difference in expertise 2 (a) Histogram of expertise values. (b) Histogram of di\ufb00erences in expertise evaluations. Figure 1: Distribution of expertise scores reported by participants. 7 of participants (expertise score larger than 3), and about one-third evaluations are made for papers that reviewers are not competent in (expertise score 3 or lower), thereby yielding a good spread. Second, Figure 1b shows the distributions of pairwise di\ufb00erences in expertise evaluations made by the same reviewer. To construct this \ufb01gure, for each participant we considered all pairs of papers in their report. Next, we pooled the absolute values of the pairwise di\ufb00erences in expertise across participants. We then plotted the histogram of these di\ufb00erences in the \ufb01gure. Observe that the distribution in Figure 1b has a heavy tail, suggesting that our dataset is suitable for evaluating the accuracy of similarity-computation methods both at a coarse level (large di\ufb00erences between the values of expertise) and at a \ufb01ne level (small di\ufb00erences between the values of expertise). 5 Experimental setup We now describe the setup of experiments on our dataset. 5.1 Metric We begin with de\ufb01ning a main metric that we use in this work to evaluate performance of the algorithms. For this, we rely on the Kendall\u2019s Tau distance that is closely related to the widely used Kendall\u2019s Tau (Kendall, 1938) rank correlation coe\ufb03cient. We introduce the metric and the algorithms in this section, followed by the results in Section 6. Subsequently in Section 7, we"}, {"question": " What percentage of papers in the dataset are published in or after 2020?", "answer": " Approximately 75% of papers in the dataset are published in or after 2020.", "ref_chunk": "Min # publications Max # publications Mean # publications Median # publications 2 492 54 20 Table 1: Demography of participants. For the \ufb01rst four characteristics, quantities represent percentages of the one or more most popular classes in the dataset. Note that classi\ufb01cation is done manually based on publicly available information and may not be free of error. For the last characteristic, quantities are computed based on Semantic Scholar pro\ufb01les. 4 Data exploration In this section we explore the collected data and present various characteristics of the dataset. The subsequent sections then detail the results of using this data to benchmark various popular similarity-computation algorithms. 4.1 Participants We begin with a discussion of the pool of the survey participants: Table 1 displays its key characteristics. First, we note that all participants work in the broad area of computer science and have rich publication pro\ufb01les (at least two papers published, with the mean of 54 papers and the median of 20 papers). In many subareas of computer science, including machine learning and arti\ufb01cial intelligence, having two papers is usually su\ufb03cient to join the reviewer pool of \ufb02agship conferences. Given that approximately 85% of participants either have PhD or are in the process of getting the degree, we conclude that most of the researchers who contributed to our dataset are eligible to review for machine learning and arti\ufb01cial intelligence conferences. Second, we caveat that most of the participants are male researchers a\ufb03liated with US-based organi- zations, with about 40% of all participants being a\ufb03liated with Carnegie Mellon University. Hence, the population of participants is not necessarily representative of the general population of the machine learning and computer science communities. We encourage researchers to be aware of this limitation when using our dataset. We note that the data collection process does not conclude with the publication of the present paper and we will be updating the dataset as new responses come. We also encourage readers to contribute 5\u201310 minutes of their time to \ufb01ll out the survey https://forms.gle/SP1Rh8eivGz54xR37 and make the dataset more representative. 4.2 Papers We now describe the set of papers that constitute our dataset. Overall, participants evaluated their expertise in reviewing 463 unique papers. Out of these 463 papers, 12 papers appeared in reports of two participants, 1 paper was mentioned by three participants, and the remaining papers were mentioned by one participant each. 6 Total number of papers: 463 Characteristic Quantity Value Open Access # On semantic scholar # On arXiv # PDF available 462 411 457 Research Areas # Computer science 459 Publication Year % Before 2020 % 2020 or later 25 75 Table 2: Characteristics of the 463 papers in the released dataset. Most of the papers are freely available online and belong to the broad area of computer science. Table 2 presents several characteristics of the pool of papers in our dataset. First, we note that all but one of the papers are listed on Semantic Scholar, enabling similarity-computation algorithms developed on the dataset to query additional information about the papers from the Semantic Scholar database. For the paper that does not have a Semantic Scholar pro\ufb01le, we construct such a pro\ufb01le manually and keep this paper in the dataset. Additionally, most of the papers (99%) have their PDFs freely available online, thereby allowing algorithms to use full texts of papers to compute similarities. Semantic Scholar has a built-in tool to identify broad research areas of papers (Wade, 2022). According to this tool, 99% of the papers included to our dataset belong to the broad area of computer science\u2014the target area for our data-collection procedure. The remaining four papers belong to the neighboring \ufb01elds of mathematics, philosophy, and the computational branch of biology. Finally, approximately 75% of all papers in our dataset are published in or after 2020, ensuring that our dataset contains recent papers that similarity-computation algorithms encounter in practice. 4.3 Evaluations of expertise Finally, we proceed to the key aspect of our dataset\u2014evaluations of expertise in reviewing the papers reported by participants. All but one participant reported expertise in reviewing at least 5 papers with the mean number of papers per participant being 8.2 and the total number of expertise evaluations being 477. Figure 1 provides visualization of expertise evaluations made by participants. First, Figure 1a displays the histogram of expertise values. About two-thirds of the reported papers belong to the expertise area 125 (2, 3] (4, 5]Value of expertise 150 (3, 4] 175Count 75 25 100 50 [1, 2] 0 50 0 3 200 250 100 150 1 0 300Count 4Difference in expertise 2 (a) Histogram of expertise values. (b) Histogram of di\ufb00erences in expertise evaluations. Figure 1: Distribution of expertise scores reported by participants. 7 of participants (expertise score larger than 3), and about one-third evaluations are made for papers that reviewers are not competent in (expertise score 3 or lower), thereby yielding a good spread. Second, Figure 1b shows the distributions of pairwise di\ufb00erences in expertise evaluations made by the same reviewer. To construct this \ufb01gure, for each participant we considered all pairs of papers in their report. Next, we pooled the absolute values of the pairwise di\ufb00erences in expertise across participants. We then plotted the histogram of these di\ufb00erences in the \ufb01gure. Observe that the distribution in Figure 1b has a heavy tail, suggesting that our dataset is suitable for evaluating the accuracy of similarity-computation methods both at a coarse level (large di\ufb00erences between the values of expertise) and at a \ufb01ne level (small di\ufb00erences between the values of expertise). 5 Experimental setup We now describe the setup of experiments on our dataset. 5.1 Metric We begin with de\ufb01ning a main metric that we use in this work to evaluate performance of the algorithms. For this, we rely on the Kendall\u2019s Tau distance that is closely related to the widely used Kendall\u2019s Tau (Kendall, 1938) rank correlation coe\ufb03cient. We introduce the metric and the algorithms in this section, followed by the results in Section 6. Subsequently in Section 7, we"}, {"question": " How many unique papers were evaluated by the participants?", "answer": " Participants evaluated expertise in reviewing 463 unique papers.", "ref_chunk": "Min # publications Max # publications Mean # publications Median # publications 2 492 54 20 Table 1: Demography of participants. For the \ufb01rst four characteristics, quantities represent percentages of the one or more most popular classes in the dataset. Note that classi\ufb01cation is done manually based on publicly available information and may not be free of error. For the last characteristic, quantities are computed based on Semantic Scholar pro\ufb01les. 4 Data exploration In this section we explore the collected data and present various characteristics of the dataset. The subsequent sections then detail the results of using this data to benchmark various popular similarity-computation algorithms. 4.1 Participants We begin with a discussion of the pool of the survey participants: Table 1 displays its key characteristics. First, we note that all participants work in the broad area of computer science and have rich publication pro\ufb01les (at least two papers published, with the mean of 54 papers and the median of 20 papers). In many subareas of computer science, including machine learning and arti\ufb01cial intelligence, having two papers is usually su\ufb03cient to join the reviewer pool of \ufb02agship conferences. Given that approximately 85% of participants either have PhD or are in the process of getting the degree, we conclude that most of the researchers who contributed to our dataset are eligible to review for machine learning and arti\ufb01cial intelligence conferences. Second, we caveat that most of the participants are male researchers a\ufb03liated with US-based organi- zations, with about 40% of all participants being a\ufb03liated with Carnegie Mellon University. Hence, the population of participants is not necessarily representative of the general population of the machine learning and computer science communities. We encourage researchers to be aware of this limitation when using our dataset. We note that the data collection process does not conclude with the publication of the present paper and we will be updating the dataset as new responses come. We also encourage readers to contribute 5\u201310 minutes of their time to \ufb01ll out the survey https://forms.gle/SP1Rh8eivGz54xR37 and make the dataset more representative. 4.2 Papers We now describe the set of papers that constitute our dataset. Overall, participants evaluated their expertise in reviewing 463 unique papers. Out of these 463 papers, 12 papers appeared in reports of two participants, 1 paper was mentioned by three participants, and the remaining papers were mentioned by one participant each. 6 Total number of papers: 463 Characteristic Quantity Value Open Access # On semantic scholar # On arXiv # PDF available 462 411 457 Research Areas # Computer science 459 Publication Year % Before 2020 % 2020 or later 25 75 Table 2: Characteristics of the 463 papers in the released dataset. Most of the papers are freely available online and belong to the broad area of computer science. Table 2 presents several characteristics of the pool of papers in our dataset. First, we note that all but one of the papers are listed on Semantic Scholar, enabling similarity-computation algorithms developed on the dataset to query additional information about the papers from the Semantic Scholar database. For the paper that does not have a Semantic Scholar pro\ufb01le, we construct such a pro\ufb01le manually and keep this paper in the dataset. Additionally, most of the papers (99%) have their PDFs freely available online, thereby allowing algorithms to use full texts of papers to compute similarities. Semantic Scholar has a built-in tool to identify broad research areas of papers (Wade, 2022). According to this tool, 99% of the papers included to our dataset belong to the broad area of computer science\u2014the target area for our data-collection procedure. The remaining four papers belong to the neighboring \ufb01elds of mathematics, philosophy, and the computational branch of biology. Finally, approximately 75% of all papers in our dataset are published in or after 2020, ensuring that our dataset contains recent papers that similarity-computation algorithms encounter in practice. 4.3 Evaluations of expertise Finally, we proceed to the key aspect of our dataset\u2014evaluations of expertise in reviewing the papers reported by participants. All but one participant reported expertise in reviewing at least 5 papers with the mean number of papers per participant being 8.2 and the total number of expertise evaluations being 477. Figure 1 provides visualization of expertise evaluations made by participants. First, Figure 1a displays the histogram of expertise values. About two-thirds of the reported papers belong to the expertise area 125 (2, 3] (4, 5]Value of expertise 150 (3, 4] 175Count 75 25 100 50 [1, 2] 0 50 0 3 200 250 100 150 1 0 300Count 4Difference in expertise 2 (a) Histogram of expertise values. (b) Histogram of di\ufb00erences in expertise evaluations. Figure 1: Distribution of expertise scores reported by participants. 7 of participants (expertise score larger than 3), and about one-third evaluations are made for papers that reviewers are not competent in (expertise score 3 or lower), thereby yielding a good spread. Second, Figure 1b shows the distributions of pairwise di\ufb00erences in expertise evaluations made by the same reviewer. To construct this \ufb01gure, for each participant we considered all pairs of papers in their report. Next, we pooled the absolute values of the pairwise di\ufb00erences in expertise across participants. We then plotted the histogram of these di\ufb00erences in the \ufb01gure. Observe that the distribution in Figure 1b has a heavy tail, suggesting that our dataset is suitable for evaluating the accuracy of similarity-computation methods both at a coarse level (large di\ufb00erences between the values of expertise) and at a \ufb01ne level (small di\ufb00erences between the values of expertise). 5 Experimental setup We now describe the setup of experiments on our dataset. 5.1 Metric We begin with de\ufb01ning a main metric that we use in this work to evaluate performance of the algorithms. For this, we rely on the Kendall\u2019s Tau distance that is closely related to the widely used Kendall\u2019s Tau (Kendall, 1938) rank correlation coe\ufb03cient. We introduce the metric and the algorithms in this section, followed by the results in Section 6. Subsequently in Section 7, we"}, {"question": " What percentage of papers in the dataset belong to the broad area of computer science?", "answer": " 99% of the papers included in the dataset belong to the broad area of computer science.", "ref_chunk": "Min # publications Max # publications Mean # publications Median # publications 2 492 54 20 Table 1: Demography of participants. For the \ufb01rst four characteristics, quantities represent percentages of the one or more most popular classes in the dataset. Note that classi\ufb01cation is done manually based on publicly available information and may not be free of error. For the last characteristic, quantities are computed based on Semantic Scholar pro\ufb01les. 4 Data exploration In this section we explore the collected data and present various characteristics of the dataset. The subsequent sections then detail the results of using this data to benchmark various popular similarity-computation algorithms. 4.1 Participants We begin with a discussion of the pool of the survey participants: Table 1 displays its key characteristics. First, we note that all participants work in the broad area of computer science and have rich publication pro\ufb01les (at least two papers published, with the mean of 54 papers and the median of 20 papers). In many subareas of computer science, including machine learning and arti\ufb01cial intelligence, having two papers is usually su\ufb03cient to join the reviewer pool of \ufb02agship conferences. Given that approximately 85% of participants either have PhD or are in the process of getting the degree, we conclude that most of the researchers who contributed to our dataset are eligible to review for machine learning and arti\ufb01cial intelligence conferences. Second, we caveat that most of the participants are male researchers a\ufb03liated with US-based organi- zations, with about 40% of all participants being a\ufb03liated with Carnegie Mellon University. Hence, the population of participants is not necessarily representative of the general population of the machine learning and computer science communities. We encourage researchers to be aware of this limitation when using our dataset. We note that the data collection process does not conclude with the publication of the present paper and we will be updating the dataset as new responses come. We also encourage readers to contribute 5\u201310 minutes of their time to \ufb01ll out the survey https://forms.gle/SP1Rh8eivGz54xR37 and make the dataset more representative. 4.2 Papers We now describe the set of papers that constitute our dataset. Overall, participants evaluated their expertise in reviewing 463 unique papers. Out of these 463 papers, 12 papers appeared in reports of two participants, 1 paper was mentioned by three participants, and the remaining papers were mentioned by one participant each. 6 Total number of papers: 463 Characteristic Quantity Value Open Access # On semantic scholar # On arXiv # PDF available 462 411 457 Research Areas # Computer science 459 Publication Year % Before 2020 % 2020 or later 25 75 Table 2: Characteristics of the 463 papers in the released dataset. Most of the papers are freely available online and belong to the broad area of computer science. Table 2 presents several characteristics of the pool of papers in our dataset. First, we note that all but one of the papers are listed on Semantic Scholar, enabling similarity-computation algorithms developed on the dataset to query additional information about the papers from the Semantic Scholar database. For the paper that does not have a Semantic Scholar pro\ufb01le, we construct such a pro\ufb01le manually and keep this paper in the dataset. Additionally, most of the papers (99%) have their PDFs freely available online, thereby allowing algorithms to use full texts of papers to compute similarities. Semantic Scholar has a built-in tool to identify broad research areas of papers (Wade, 2022). According to this tool, 99% of the papers included to our dataset belong to the broad area of computer science\u2014the target area for our data-collection procedure. The remaining four papers belong to the neighboring \ufb01elds of mathematics, philosophy, and the computational branch of biology. Finally, approximately 75% of all papers in our dataset are published in or after 2020, ensuring that our dataset contains recent papers that similarity-computation algorithms encounter in practice. 4.3 Evaluations of expertise Finally, we proceed to the key aspect of our dataset\u2014evaluations of expertise in reviewing the papers reported by participants. All but one participant reported expertise in reviewing at least 5 papers with the mean number of papers per participant being 8.2 and the total number of expertise evaluations being 477. Figure 1 provides visualization of expertise evaluations made by participants. First, Figure 1a displays the histogram of expertise values. About two-thirds of the reported papers belong to the expertise area 125 (2, 3] (4, 5]Value of expertise 150 (3, 4] 175Count 75 25 100 50 [1, 2] 0 50 0 3 200 250 100 150 1 0 300Count 4Difference in expertise 2 (a) Histogram of expertise values. (b) Histogram of di\ufb00erences in expertise evaluations. Figure 1: Distribution of expertise scores reported by participants. 7 of participants (expertise score larger than 3), and about one-third evaluations are made for papers that reviewers are not competent in (expertise score 3 or lower), thereby yielding a good spread. Second, Figure 1b shows the distributions of pairwise di\ufb00erences in expertise evaluations made by the same reviewer. To construct this \ufb01gure, for each participant we considered all pairs of papers in their report. Next, we pooled the absolute values of the pairwise di\ufb00erences in expertise across participants. We then plotted the histogram of these di\ufb00erences in the \ufb01gure. Observe that the distribution in Figure 1b has a heavy tail, suggesting that our dataset is suitable for evaluating the accuracy of similarity-computation methods both at a coarse level (large di\ufb00erences between the values of expertise) and at a \ufb01ne level (small di\ufb00erences between the values of expertise). 5 Experimental setup We now describe the setup of experiments on our dataset. 5.1 Metric We begin with de\ufb01ning a main metric that we use in this work to evaluate performance of the algorithms. For this, we rely on the Kendall\u2019s Tau distance that is closely related to the widely used Kendall\u2019s Tau (Kendall, 1938) rank correlation coe\ufb03cient. We introduce the metric and the algorithms in this section, followed by the results in Section 6. Subsequently in Section 7, we"}, {"question": " What is the mean number of papers per participant for expertise evaluations?", "answer": " The mean number of papers per participant for expertise evaluations is 8.2.", "ref_chunk": "Min # publications Max # publications Mean # publications Median # publications 2 492 54 20 Table 1: Demography of participants. For the \ufb01rst four characteristics, quantities represent percentages of the one or more most popular classes in the dataset. Note that classi\ufb01cation is done manually based on publicly available information and may not be free of error. For the last characteristic, quantities are computed based on Semantic Scholar pro\ufb01les. 4 Data exploration In this section we explore the collected data and present various characteristics of the dataset. The subsequent sections then detail the results of using this data to benchmark various popular similarity-computation algorithms. 4.1 Participants We begin with a discussion of the pool of the survey participants: Table 1 displays its key characteristics. First, we note that all participants work in the broad area of computer science and have rich publication pro\ufb01les (at least two papers published, with the mean of 54 papers and the median of 20 papers). In many subareas of computer science, including machine learning and arti\ufb01cial intelligence, having two papers is usually su\ufb03cient to join the reviewer pool of \ufb02agship conferences. Given that approximately 85% of participants either have PhD or are in the process of getting the degree, we conclude that most of the researchers who contributed to our dataset are eligible to review for machine learning and arti\ufb01cial intelligence conferences. Second, we caveat that most of the participants are male researchers a\ufb03liated with US-based organi- zations, with about 40% of all participants being a\ufb03liated with Carnegie Mellon University. Hence, the population of participants is not necessarily representative of the general population of the machine learning and computer science communities. We encourage researchers to be aware of this limitation when using our dataset. We note that the data collection process does not conclude with the publication of the present paper and we will be updating the dataset as new responses come. We also encourage readers to contribute 5\u201310 minutes of their time to \ufb01ll out the survey https://forms.gle/SP1Rh8eivGz54xR37 and make the dataset more representative. 4.2 Papers We now describe the set of papers that constitute our dataset. Overall, participants evaluated their expertise in reviewing 463 unique papers. Out of these 463 papers, 12 papers appeared in reports of two participants, 1 paper was mentioned by three participants, and the remaining papers were mentioned by one participant each. 6 Total number of papers: 463 Characteristic Quantity Value Open Access # On semantic scholar # On arXiv # PDF available 462 411 457 Research Areas # Computer science 459 Publication Year % Before 2020 % 2020 or later 25 75 Table 2: Characteristics of the 463 papers in the released dataset. Most of the papers are freely available online and belong to the broad area of computer science. Table 2 presents several characteristics of the pool of papers in our dataset. First, we note that all but one of the papers are listed on Semantic Scholar, enabling similarity-computation algorithms developed on the dataset to query additional information about the papers from the Semantic Scholar database. For the paper that does not have a Semantic Scholar pro\ufb01le, we construct such a pro\ufb01le manually and keep this paper in the dataset. Additionally, most of the papers (99%) have their PDFs freely available online, thereby allowing algorithms to use full texts of papers to compute similarities. Semantic Scholar has a built-in tool to identify broad research areas of papers (Wade, 2022). According to this tool, 99% of the papers included to our dataset belong to the broad area of computer science\u2014the target area for our data-collection procedure. The remaining four papers belong to the neighboring \ufb01elds of mathematics, philosophy, and the computational branch of biology. Finally, approximately 75% of all papers in our dataset are published in or after 2020, ensuring that our dataset contains recent papers that similarity-computation algorithms encounter in practice. 4.3 Evaluations of expertise Finally, we proceed to the key aspect of our dataset\u2014evaluations of expertise in reviewing the papers reported by participants. All but one participant reported expertise in reviewing at least 5 papers with the mean number of papers per participant being 8.2 and the total number of expertise evaluations being 477. Figure 1 provides visualization of expertise evaluations made by participants. First, Figure 1a displays the histogram of expertise values. About two-thirds of the reported papers belong to the expertise area 125 (2, 3] (4, 5]Value of expertise 150 (3, 4] 175Count 75 25 100 50 [1, 2] 0 50 0 3 200 250 100 150 1 0 300Count 4Difference in expertise 2 (a) Histogram of expertise values. (b) Histogram of di\ufb00erences in expertise evaluations. Figure 1: Distribution of expertise scores reported by participants. 7 of participants (expertise score larger than 3), and about one-third evaluations are made for papers that reviewers are not competent in (expertise score 3 or lower), thereby yielding a good spread. Second, Figure 1b shows the distributions of pairwise di\ufb00erences in expertise evaluations made by the same reviewer. To construct this \ufb01gure, for each participant we considered all pairs of papers in their report. Next, we pooled the absolute values of the pairwise di\ufb00erences in expertise across participants. We then plotted the histogram of these di\ufb00erences in the \ufb01gure. Observe that the distribution in Figure 1b has a heavy tail, suggesting that our dataset is suitable for evaluating the accuracy of similarity-computation methods both at a coarse level (large di\ufb00erences between the values of expertise) and at a \ufb01ne level (small di\ufb00erences between the values of expertise). 5 Experimental setup We now describe the setup of experiments on our dataset. 5.1 Metric We begin with de\ufb01ning a main metric that we use in this work to evaluate performance of the algorithms. For this, we rely on the Kendall\u2019s Tau distance that is closely related to the widely used Kendall\u2019s Tau (Kendall, 1938) rank correlation coe\ufb03cient. We introduce the metric and the algorithms in this section, followed by the results in Section 6. Subsequently in Section 7, we"}, {"question": " What metric is used to evaluate the performance of the algorithms in the work?", "answer": " The Kendall\u2019s Tau distance is used as the main metric to evaluate the performance of the algorithms.", "ref_chunk": "Min # publications Max # publications Mean # publications Median # publications 2 492 54 20 Table 1: Demography of participants. For the \ufb01rst four characteristics, quantities represent percentages of the one or more most popular classes in the dataset. Note that classi\ufb01cation is done manually based on publicly available information and may not be free of error. For the last characteristic, quantities are computed based on Semantic Scholar pro\ufb01les. 4 Data exploration In this section we explore the collected data and present various characteristics of the dataset. The subsequent sections then detail the results of using this data to benchmark various popular similarity-computation algorithms. 4.1 Participants We begin with a discussion of the pool of the survey participants: Table 1 displays its key characteristics. First, we note that all participants work in the broad area of computer science and have rich publication pro\ufb01les (at least two papers published, with the mean of 54 papers and the median of 20 papers). In many subareas of computer science, including machine learning and arti\ufb01cial intelligence, having two papers is usually su\ufb03cient to join the reviewer pool of \ufb02agship conferences. Given that approximately 85% of participants either have PhD or are in the process of getting the degree, we conclude that most of the researchers who contributed to our dataset are eligible to review for machine learning and arti\ufb01cial intelligence conferences. Second, we caveat that most of the participants are male researchers a\ufb03liated with US-based organi- zations, with about 40% of all participants being a\ufb03liated with Carnegie Mellon University. Hence, the population of participants is not necessarily representative of the general population of the machine learning and computer science communities. We encourage researchers to be aware of this limitation when using our dataset. We note that the data collection process does not conclude with the publication of the present paper and we will be updating the dataset as new responses come. We also encourage readers to contribute 5\u201310 minutes of their time to \ufb01ll out the survey https://forms.gle/SP1Rh8eivGz54xR37 and make the dataset more representative. 4.2 Papers We now describe the set of papers that constitute our dataset. Overall, participants evaluated their expertise in reviewing 463 unique papers. Out of these 463 papers, 12 papers appeared in reports of two participants, 1 paper was mentioned by three participants, and the remaining papers were mentioned by one participant each. 6 Total number of papers: 463 Characteristic Quantity Value Open Access # On semantic scholar # On arXiv # PDF available 462 411 457 Research Areas # Computer science 459 Publication Year % Before 2020 % 2020 or later 25 75 Table 2: Characteristics of the 463 papers in the released dataset. Most of the papers are freely available online and belong to the broad area of computer science. Table 2 presents several characteristics of the pool of papers in our dataset. First, we note that all but one of the papers are listed on Semantic Scholar, enabling similarity-computation algorithms developed on the dataset to query additional information about the papers from the Semantic Scholar database. For the paper that does not have a Semantic Scholar pro\ufb01le, we construct such a pro\ufb01le manually and keep this paper in the dataset. Additionally, most of the papers (99%) have their PDFs freely available online, thereby allowing algorithms to use full texts of papers to compute similarities. Semantic Scholar has a built-in tool to identify broad research areas of papers (Wade, 2022). According to this tool, 99% of the papers included to our dataset belong to the broad area of computer science\u2014the target area for our data-collection procedure. The remaining four papers belong to the neighboring \ufb01elds of mathematics, philosophy, and the computational branch of biology. Finally, approximately 75% of all papers in our dataset are published in or after 2020, ensuring that our dataset contains recent papers that similarity-computation algorithms encounter in practice. 4.3 Evaluations of expertise Finally, we proceed to the key aspect of our dataset\u2014evaluations of expertise in reviewing the papers reported by participants. All but one participant reported expertise in reviewing at least 5 papers with the mean number of papers per participant being 8.2 and the total number of expertise evaluations being 477. Figure 1 provides visualization of expertise evaluations made by participants. First, Figure 1a displays the histogram of expertise values. About two-thirds of the reported papers belong to the expertise area 125 (2, 3] (4, 5]Value of expertise 150 (3, 4] 175Count 75 25 100 50 [1, 2] 0 50 0 3 200 250 100 150 1 0 300Count 4Difference in expertise 2 (a) Histogram of expertise values. (b) Histogram of di\ufb00erences in expertise evaluations. Figure 1: Distribution of expertise scores reported by participants. 7 of participants (expertise score larger than 3), and about one-third evaluations are made for papers that reviewers are not competent in (expertise score 3 or lower), thereby yielding a good spread. Second, Figure 1b shows the distributions of pairwise di\ufb00erences in expertise evaluations made by the same reviewer. To construct this \ufb01gure, for each participant we considered all pairs of papers in their report. Next, we pooled the absolute values of the pairwise di\ufb00erences in expertise across participants. We then plotted the histogram of these di\ufb00erences in the \ufb01gure. Observe that the distribution in Figure 1b has a heavy tail, suggesting that our dataset is suitable for evaluating the accuracy of similarity-computation methods both at a coarse level (large di\ufb00erences between the values of expertise) and at a \ufb01ne level (small di\ufb00erences between the values of expertise). 5 Experimental setup We now describe the setup of experiments on our dataset. 5.1 Metric We begin with de\ufb01ning a main metric that we use in this work to evaluate performance of the algorithms. For this, we rely on the Kendall\u2019s Tau distance that is closely related to the widely used Kendall\u2019s Tau (Kendall, 1938) rank correlation coe\ufb03cient. We introduce the metric and the algorithms in this section, followed by the results in Section 6. Subsequently in Section 7, we"}, {"question": " What correlation coefficient is related to the Kendall\u2019s Tau distance?", "answer": " The Kendall\u2019s Tau distance is closely related to the Kendall\u2019s Tau rank correlation coefficient.", "ref_chunk": "Min # publications Max # publications Mean # publications Median # publications 2 492 54 20 Table 1: Demography of participants. For the \ufb01rst four characteristics, quantities represent percentages of the one or more most popular classes in the dataset. Note that classi\ufb01cation is done manually based on publicly available information and may not be free of error. For the last characteristic, quantities are computed based on Semantic Scholar pro\ufb01les. 4 Data exploration In this section we explore the collected data and present various characteristics of the dataset. The subsequent sections then detail the results of using this data to benchmark various popular similarity-computation algorithms. 4.1 Participants We begin with a discussion of the pool of the survey participants: Table 1 displays its key characteristics. First, we note that all participants work in the broad area of computer science and have rich publication pro\ufb01les (at least two papers published, with the mean of 54 papers and the median of 20 papers). In many subareas of computer science, including machine learning and arti\ufb01cial intelligence, having two papers is usually su\ufb03cient to join the reviewer pool of \ufb02agship conferences. Given that approximately 85% of participants either have PhD or are in the process of getting the degree, we conclude that most of the researchers who contributed to our dataset are eligible to review for machine learning and arti\ufb01cial intelligence conferences. Second, we caveat that most of the participants are male researchers a\ufb03liated with US-based organi- zations, with about 40% of all participants being a\ufb03liated with Carnegie Mellon University. Hence, the population of participants is not necessarily representative of the general population of the machine learning and computer science communities. We encourage researchers to be aware of this limitation when using our dataset. We note that the data collection process does not conclude with the publication of the present paper and we will be updating the dataset as new responses come. We also encourage readers to contribute 5\u201310 minutes of their time to \ufb01ll out the survey https://forms.gle/SP1Rh8eivGz54xR37 and make the dataset more representative. 4.2 Papers We now describe the set of papers that constitute our dataset. Overall, participants evaluated their expertise in reviewing 463 unique papers. Out of these 463 papers, 12 papers appeared in reports of two participants, 1 paper was mentioned by three participants, and the remaining papers were mentioned by one participant each. 6 Total number of papers: 463 Characteristic Quantity Value Open Access # On semantic scholar # On arXiv # PDF available 462 411 457 Research Areas # Computer science 459 Publication Year % Before 2020 % 2020 or later 25 75 Table 2: Characteristics of the 463 papers in the released dataset. Most of the papers are freely available online and belong to the broad area of computer science. Table 2 presents several characteristics of the pool of papers in our dataset. First, we note that all but one of the papers are listed on Semantic Scholar, enabling similarity-computation algorithms developed on the dataset to query additional information about the papers from the Semantic Scholar database. For the paper that does not have a Semantic Scholar pro\ufb01le, we construct such a pro\ufb01le manually and keep this paper in the dataset. Additionally, most of the papers (99%) have their PDFs freely available online, thereby allowing algorithms to use full texts of papers to compute similarities. Semantic Scholar has a built-in tool to identify broad research areas of papers (Wade, 2022). According to this tool, 99% of the papers included to our dataset belong to the broad area of computer science\u2014the target area for our data-collection procedure. The remaining four papers belong to the neighboring \ufb01elds of mathematics, philosophy, and the computational branch of biology. Finally, approximately 75% of all papers in our dataset are published in or after 2020, ensuring that our dataset contains recent papers that similarity-computation algorithms encounter in practice. 4.3 Evaluations of expertise Finally, we proceed to the key aspect of our dataset\u2014evaluations of expertise in reviewing the papers reported by participants. All but one participant reported expertise in reviewing at least 5 papers with the mean number of papers per participant being 8.2 and the total number of expertise evaluations being 477. Figure 1 provides visualization of expertise evaluations made by participants. First, Figure 1a displays the histogram of expertise values. About two-thirds of the reported papers belong to the expertise area 125 (2, 3] (4, 5]Value of expertise 150 (3, 4] 175Count 75 25 100 50 [1, 2] 0 50 0 3 200 250 100 150 1 0 300Count 4Difference in expertise 2 (a) Histogram of expertise values. (b) Histogram of di\ufb00erences in expertise evaluations. Figure 1: Distribution of expertise scores reported by participants. 7 of participants (expertise score larger than 3), and about one-third evaluations are made for papers that reviewers are not competent in (expertise score 3 or lower), thereby yielding a good spread. Second, Figure 1b shows the distributions of pairwise di\ufb00erences in expertise evaluations made by the same reviewer. To construct this \ufb01gure, for each participant we considered all pairs of papers in their report. Next, we pooled the absolute values of the pairwise di\ufb00erences in expertise across participants. We then plotted the histogram of these di\ufb00erences in the \ufb01gure. Observe that the distribution in Figure 1b has a heavy tail, suggesting that our dataset is suitable for evaluating the accuracy of similarity-computation methods both at a coarse level (large di\ufb00erences between the values of expertise) and at a \ufb01ne level (small di\ufb00erences between the values of expertise). 5 Experimental setup We now describe the setup of experiments on our dataset. 5.1 Metric We begin with de\ufb01ning a main metric that we use in this work to evaluate performance of the algorithms. For this, we rely on the Kendall\u2019s Tau distance that is closely related to the widely used Kendall\u2019s Tau (Kendall, 1938) rank correlation coe\ufb03cient. We introduce the metric and the algorithms in this section, followed by the results in Section 6. Subsequently in Section 7, we"}], "doc_text": "Min # publications Max # publications Mean # publications Median # publications 2 492 54 20 Table 1: Demography of participants. For the \ufb01rst four characteristics, quantities represent percentages of the one or more most popular classes in the dataset. Note that classi\ufb01cation is done manually based on publicly available information and may not be free of error. For the last characteristic, quantities are computed based on Semantic Scholar pro\ufb01les. 4 Data exploration In this section we explore the collected data and present various characteristics of the dataset. The subsequent sections then detail the results of using this data to benchmark various popular similarity-computation algorithms. 4.1 Participants We begin with a discussion of the pool of the survey participants: Table 1 displays its key characteristics. First, we note that all participants work in the broad area of computer science and have rich publication pro\ufb01les (at least two papers published, with the mean of 54 papers and the median of 20 papers). In many subareas of computer science, including machine learning and arti\ufb01cial intelligence, having two papers is usually su\ufb03cient to join the reviewer pool of \ufb02agship conferences. Given that approximately 85% of participants either have PhD or are in the process of getting the degree, we conclude that most of the researchers who contributed to our dataset are eligible to review for machine learning and arti\ufb01cial intelligence conferences. Second, we caveat that most of the participants are male researchers a\ufb03liated with US-based organi- zations, with about 40% of all participants being a\ufb03liated with Carnegie Mellon University. Hence, the population of participants is not necessarily representative of the general population of the machine learning and computer science communities. We encourage researchers to be aware of this limitation when using our dataset. We note that the data collection process does not conclude with the publication of the present paper and we will be updating the dataset as new responses come. We also encourage readers to contribute 5\u201310 minutes of their time to \ufb01ll out the survey https://forms.gle/SP1Rh8eivGz54xR37 and make the dataset more representative. 4.2 Papers We now describe the set of papers that constitute our dataset. Overall, participants evaluated their expertise in reviewing 463 unique papers. Out of these 463 papers, 12 papers appeared in reports of two participants, 1 paper was mentioned by three participants, and the remaining papers were mentioned by one participant each. 6 Total number of papers: 463 Characteristic Quantity Value Open Access # On semantic scholar # On arXiv # PDF available 462 411 457 Research Areas # Computer science 459 Publication Year % Before 2020 % 2020 or later 25 75 Table 2: Characteristics of the 463 papers in the released dataset. Most of the papers are freely available online and belong to the broad area of computer science. Table 2 presents several characteristics of the pool of papers in our dataset. First, we note that all but one of the papers are listed on Semantic Scholar, enabling similarity-computation algorithms developed on the dataset to query additional information about the papers from the Semantic Scholar database. For the paper that does not have a Semantic Scholar pro\ufb01le, we construct such a pro\ufb01le manually and keep this paper in the dataset. Additionally, most of the papers (99%) have their PDFs freely available online, thereby allowing algorithms to use full texts of papers to compute similarities. Semantic Scholar has a built-in tool to identify broad research areas of papers (Wade, 2022). According to this tool, 99% of the papers included to our dataset belong to the broad area of computer science\u2014the target area for our data-collection procedure. The remaining four papers belong to the neighboring \ufb01elds of mathematics, philosophy, and the computational branch of biology. Finally, approximately 75% of all papers in our dataset are published in or after 2020, ensuring that our dataset contains recent papers that similarity-computation algorithms encounter in practice. 4.3 Evaluations of expertise Finally, we proceed to the key aspect of our dataset\u2014evaluations of expertise in reviewing the papers reported by participants. All but one participant reported expertise in reviewing at least 5 papers with the mean number of papers per participant being 8.2 and the total number of expertise evaluations being 477. Figure 1 provides visualization of expertise evaluations made by participants. First, Figure 1a displays the histogram of expertise values. About two-thirds of the reported papers belong to the expertise area 125 (2, 3] (4, 5]Value of expertise 150 (3, 4] 175Count 75 25 100 50 [1, 2] 0 50 0 3 200 250 100 150 1 0 300Count 4Difference in expertise 2 (a) Histogram of expertise values. (b) Histogram of di\ufb00erences in expertise evaluations. Figure 1: Distribution of expertise scores reported by participants. 7 of participants (expertise score larger than 3), and about one-third evaluations are made for papers that reviewers are not competent in (expertise score 3 or lower), thereby yielding a good spread. Second, Figure 1b shows the distributions of pairwise di\ufb00erences in expertise evaluations made by the same reviewer. To construct this \ufb01gure, for each participant we considered all pairs of papers in their report. Next, we pooled the absolute values of the pairwise di\ufb00erences in expertise across participants. We then plotted the histogram of these di\ufb00erences in the \ufb01gure. Observe that the distribution in Figure 1b has a heavy tail, suggesting that our dataset is suitable for evaluating the accuracy of similarity-computation methods both at a coarse level (large di\ufb00erences between the values of expertise) and at a \ufb01ne level (small di\ufb00erences between the values of expertise). 5 Experimental setup We now describe the setup of experiments on our dataset. 5.1 Metric We begin with de\ufb01ning a main metric that we use in this work to evaluate performance of the algorithms. For this, we rely on the Kendall\u2019s Tau distance that is closely related to the widely used Kendall\u2019s Tau (Kendall, 1938) rank correlation coe\ufb03cient. We introduce the metric and the algorithms in this section, followed by the results in Section 6. Subsequently in Section 7, we"}