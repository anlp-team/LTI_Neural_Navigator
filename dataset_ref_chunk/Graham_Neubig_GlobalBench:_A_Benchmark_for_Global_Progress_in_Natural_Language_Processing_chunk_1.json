{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_GlobalBench:_A_Benchmark_for_Global_Progress_in_Natural_Language_Processing_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of GlobalBench?", "answer": " To track and incentivize the global development of equitable language technology.", "ref_chunk": "3 2 0 2 y a M 4 2 ] L C . s c [ 1 v 6 1 7 4 1 . 5 0 3 2 : v i X r a GlobalBench: A Benchmark for Global Progress in Natural Language Processing Yueqi Song1, Catherine Cui2, Simran Khanuja1, Pengfei Liu3, Fahim Faisal4, Alissa Ostapenko1, Genta Indra Winata5, Alham Fikri Aji6, Samuel Cahyawijaya5, Yulia Tsvetkov1,7, Antonios Anastasopoulos4, Graham Neubig1 1Carnegie Mellon University 2Harvard University 4George Mason University 3Shanghai Jiaotong University 5The Hong Kong University of Science and Technology 6MBZUAI 7University of Washington 1 Abstract Despite the major advances in NLP, significant disparities in NLP system performance across lan- guages still exist. Arguably, these are due to un- even resource allocation and sub-optimal incen- tives to work on less resourced languages. To track and further incentivize the global develop- ment of equitable language technology, we in- troduce GlobalBench. Prior multilingual bench- marks are static and have focused on a limited number of tasks and languages. In contrast, Glob- alBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring ac- curacy, GlobalBench also tracks the estimated per- speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served lan- guages are the ones with a relatively high pop- ulation, but nonetheless overlooked by compos- ite multilingual benchmarks (like Punjabi, Por- tuguese, and Wu Chinese). Currently, Global- Bench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 lan- guages1 Introduction . . . MT Demo.Avg. \u2026(othertasks) . . . NERcmn Ling.Avg. MTcmn GlobalBench NER GlobalBench NER NER MTspa GlobalBench MT NEReng All System Results NERspa GlobalBench MT MTeng Equity Equity Demo.Avg. Ling.Avg. Figure 1: GlobalBench Design: A leaderboard for each task is separately maintained. Each leaderboard con- tains a multi-faceted evaluation of submitted systems, along with a ranking of the most under-served languages. More details can be found in Section 2. (2021) argue that one major factor is a problem of incentives and resource allocation. For instance, languages associated with larger economic might (as measured by GDP of the countries where they are spoken) see more research and resource devel- opment, leading to more performant systems. Advances in multilingual natural language process- ing (NLP) technologies (Dabre et al., 2020; Hed- derich et al., 2021) have raised the enticing possibil- ities of NLP systems that benefit all people around the world. However, at the same time, studies into the state of multilingual NLP have demonstrated stark differences in the amount of resources avail- able (Joshi et al., 2020; Yu et al., 2022) and perfor- mance of existing NLP systems (Blasi et al., 2021; Khanuja et al., 2022). Why do these disparities exist? The causes of these disparities are multifarious, but Blasi et al. 1GlobalBench will be available on a hosted server for any- one to contribute systems or data, with detailed submission in- structions: https://explainaboard.inspiredco. ai/benchmark?parent_id=globalbench&show_ featured=false In this paper, we propose GlobalBench, a new benchmark and leaderboard that is designed to specifically incentivize the global development of equitable language technologies that serve speak- ers of all languages throughout the world. Glob- alBench follows the footsteps of other successful multilingual benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020), which aggregate results of systems across several tasks and provide a general idea of progress being made in the field of multilingual NLP. However, these benchmarks, by design, are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Additionally, they mainly focus on average accuracy over all languages in the dataset, and thus say little about the downstream utility and equity of submitted systems, across languages. Utility System1 System1 Dataset and model contributions Pillar 2: Multi-faceted Evaluation L3, L4 Dataset3 L1, L2 Accuracy / F1/BLEU/ chrf \u2026 System2 System2 L1, L3 MT MT Most Under-Served Languages L4 \u2026 Equity Equity NER NER L3 L3 L3 L3 Improvement 960 datasets in 190 languages L4 L4 L3 . . . . . Accuracy / F1/BLEU/ chrf \u2026 \u2026 L2, L3 L2, L3 Pillar 1: Dataset Inclusivity Dataset1 Dataset1 L2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L2 Utility Dataset2 Dataset2 of 1,128 system submissions L1 L1 L1 L1 L1 Pillar 3: Reward L1, L2 Figure 2: GlobalBench\u2019s Philosophy: First, we aim to inclusively gather datasets for all tasks and languages. Second, we present a multi-faceted evaluation of systems, going beyond average accuracies across languages, to keep track of the utility and equity of these systems. Third, the leaderboard maintains a list of the most under-served languages, and rewards improvement in utility, which is achieved through both dataset and model contributions. Above, we see that the addition of System2 improves the measured utility of L2 and L3 for the MT leaderboard, and the addition of Dataset3 , improves the measured utility of L4 in the NER leaderboard. Hence, in designing GlobalBench, we make a number of intentional design decisions to explicitly promote the improvement of language technology for all of the world\u2019s citizens: Inclusive Dataset Selection: We aim to be able to evaluate all datasets in all languages for all tasks, making it possible to (in theory) cover any language for which a dataset exists. lens of GlobalBench (\u00a75), related work (\u00a76) and our expectations for the path forward (\u00a77). All in all, we believe that improving the quality and equity of language technologies for all speak- ers in the world is one of the paramount challenges of NLP today, and in the famous mantra from Peter Drucker, what you cannot measure, you cannot im- prove. GlobalBench is"}, {"question": " How does GlobalBench differ from prior multilingual benchmarks?", "answer": " GlobalBench aims to dynamically track progress on all NLP datasets in all languages, measuring accuracy and estimated per-speaker utility and equity.", "ref_chunk": "3 2 0 2 y a M 4 2 ] L C . s c [ 1 v 6 1 7 4 1 . 5 0 3 2 : v i X r a GlobalBench: A Benchmark for Global Progress in Natural Language Processing Yueqi Song1, Catherine Cui2, Simran Khanuja1, Pengfei Liu3, Fahim Faisal4, Alissa Ostapenko1, Genta Indra Winata5, Alham Fikri Aji6, Samuel Cahyawijaya5, Yulia Tsvetkov1,7, Antonios Anastasopoulos4, Graham Neubig1 1Carnegie Mellon University 2Harvard University 4George Mason University 3Shanghai Jiaotong University 5The Hong Kong University of Science and Technology 6MBZUAI 7University of Washington 1 Abstract Despite the major advances in NLP, significant disparities in NLP system performance across lan- guages still exist. Arguably, these are due to un- even resource allocation and sub-optimal incen- tives to work on less resourced languages. To track and further incentivize the global develop- ment of equitable language technology, we in- troduce GlobalBench. Prior multilingual bench- marks are static and have focused on a limited number of tasks and languages. In contrast, Glob- alBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring ac- curacy, GlobalBench also tracks the estimated per- speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served lan- guages are the ones with a relatively high pop- ulation, but nonetheless overlooked by compos- ite multilingual benchmarks (like Punjabi, Por- tuguese, and Wu Chinese). Currently, Global- Bench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 lan- guages1 Introduction . . . MT Demo.Avg. \u2026(othertasks) . . . NERcmn Ling.Avg. MTcmn GlobalBench NER GlobalBench NER NER MTspa GlobalBench MT NEReng All System Results NERspa GlobalBench MT MTeng Equity Equity Demo.Avg. Ling.Avg. Figure 1: GlobalBench Design: A leaderboard for each task is separately maintained. Each leaderboard con- tains a multi-faceted evaluation of submitted systems, along with a ranking of the most under-served languages. More details can be found in Section 2. (2021) argue that one major factor is a problem of incentives and resource allocation. For instance, languages associated with larger economic might (as measured by GDP of the countries where they are spoken) see more research and resource devel- opment, leading to more performant systems. Advances in multilingual natural language process- ing (NLP) technologies (Dabre et al., 2020; Hed- derich et al., 2021) have raised the enticing possibil- ities of NLP systems that benefit all people around the world. However, at the same time, studies into the state of multilingual NLP have demonstrated stark differences in the amount of resources avail- able (Joshi et al., 2020; Yu et al., 2022) and perfor- mance of existing NLP systems (Blasi et al., 2021; Khanuja et al., 2022). Why do these disparities exist? The causes of these disparities are multifarious, but Blasi et al. 1GlobalBench will be available on a hosted server for any- one to contribute systems or data, with detailed submission in- structions: https://explainaboard.inspiredco. ai/benchmark?parent_id=globalbench&show_ featured=false In this paper, we propose GlobalBench, a new benchmark and leaderboard that is designed to specifically incentivize the global development of equitable language technologies that serve speak- ers of all languages throughout the world. Glob- alBench follows the footsteps of other successful multilingual benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020), which aggregate results of systems across several tasks and provide a general idea of progress being made in the field of multilingual NLP. However, these benchmarks, by design, are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Additionally, they mainly focus on average accuracy over all languages in the dataset, and thus say little about the downstream utility and equity of submitted systems, across languages. Utility System1 System1 Dataset and model contributions Pillar 2: Multi-faceted Evaluation L3, L4 Dataset3 L1, L2 Accuracy / F1/BLEU/ chrf \u2026 System2 System2 L1, L3 MT MT Most Under-Served Languages L4 \u2026 Equity Equity NER NER L3 L3 L3 L3 Improvement 960 datasets in 190 languages L4 L4 L3 . . . . . Accuracy / F1/BLEU/ chrf \u2026 \u2026 L2, L3 L2, L3 Pillar 1: Dataset Inclusivity Dataset1 Dataset1 L2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L2 Utility Dataset2 Dataset2 of 1,128 system submissions L1 L1 L1 L1 L1 Pillar 3: Reward L1, L2 Figure 2: GlobalBench\u2019s Philosophy: First, we aim to inclusively gather datasets for all tasks and languages. Second, we present a multi-faceted evaluation of systems, going beyond average accuracies across languages, to keep track of the utility and equity of these systems. Third, the leaderboard maintains a list of the most under-served languages, and rewards improvement in utility, which is achieved through both dataset and model contributions. Above, we see that the addition of System2 improves the measured utility of L2 and L3 for the MT leaderboard, and the addition of Dataset3 , improves the measured utility of L4 in the NER leaderboard. Hence, in designing GlobalBench, we make a number of intentional design decisions to explicitly promote the improvement of language technology for all of the world\u2019s citizens: Inclusive Dataset Selection: We aim to be able to evaluate all datasets in all languages for all tasks, making it possible to (in theory) cover any language for which a dataset exists. lens of GlobalBench (\u00a75), related work (\u00a76) and our expectations for the path forward (\u00a77). All in all, we believe that improving the quality and equity of language technologies for all speak- ers in the world is one of the paramount challenges of NLP today, and in the famous mantra from Peter Drucker, what you cannot measure, you cannot im- prove. GlobalBench is"}, {"question": " How many datasets and languages does GlobalBench cover?", "answer": " GlobalBench covers 966 datasets in 190 languages.", "ref_chunk": "3 2 0 2 y a M 4 2 ] L C . s c [ 1 v 6 1 7 4 1 . 5 0 3 2 : v i X r a GlobalBench: A Benchmark for Global Progress in Natural Language Processing Yueqi Song1, Catherine Cui2, Simran Khanuja1, Pengfei Liu3, Fahim Faisal4, Alissa Ostapenko1, Genta Indra Winata5, Alham Fikri Aji6, Samuel Cahyawijaya5, Yulia Tsvetkov1,7, Antonios Anastasopoulos4, Graham Neubig1 1Carnegie Mellon University 2Harvard University 4George Mason University 3Shanghai Jiaotong University 5The Hong Kong University of Science and Technology 6MBZUAI 7University of Washington 1 Abstract Despite the major advances in NLP, significant disparities in NLP system performance across lan- guages still exist. Arguably, these are due to un- even resource allocation and sub-optimal incen- tives to work on less resourced languages. To track and further incentivize the global develop- ment of equitable language technology, we in- troduce GlobalBench. Prior multilingual bench- marks are static and have focused on a limited number of tasks and languages. In contrast, Glob- alBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring ac- curacy, GlobalBench also tracks the estimated per- speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served lan- guages are the ones with a relatively high pop- ulation, but nonetheless overlooked by compos- ite multilingual benchmarks (like Punjabi, Por- tuguese, and Wu Chinese). Currently, Global- Bench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 lan- guages1 Introduction . . . MT Demo.Avg. \u2026(othertasks) . . . NERcmn Ling.Avg. MTcmn GlobalBench NER GlobalBench NER NER MTspa GlobalBench MT NEReng All System Results NERspa GlobalBench MT MTeng Equity Equity Demo.Avg. Ling.Avg. Figure 1: GlobalBench Design: A leaderboard for each task is separately maintained. Each leaderboard con- tains a multi-faceted evaluation of submitted systems, along with a ranking of the most under-served languages. More details can be found in Section 2. (2021) argue that one major factor is a problem of incentives and resource allocation. For instance, languages associated with larger economic might (as measured by GDP of the countries where they are spoken) see more research and resource devel- opment, leading to more performant systems. Advances in multilingual natural language process- ing (NLP) technologies (Dabre et al., 2020; Hed- derich et al., 2021) have raised the enticing possibil- ities of NLP systems that benefit all people around the world. However, at the same time, studies into the state of multilingual NLP have demonstrated stark differences in the amount of resources avail- able (Joshi et al., 2020; Yu et al., 2022) and perfor- mance of existing NLP systems (Blasi et al., 2021; Khanuja et al., 2022). Why do these disparities exist? The causes of these disparities are multifarious, but Blasi et al. 1GlobalBench will be available on a hosted server for any- one to contribute systems or data, with detailed submission in- structions: https://explainaboard.inspiredco. ai/benchmark?parent_id=globalbench&show_ featured=false In this paper, we propose GlobalBench, a new benchmark and leaderboard that is designed to specifically incentivize the global development of equitable language technologies that serve speak- ers of all languages throughout the world. Glob- alBench follows the footsteps of other successful multilingual benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020), which aggregate results of systems across several tasks and provide a general idea of progress being made in the field of multilingual NLP. However, these benchmarks, by design, are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Additionally, they mainly focus on average accuracy over all languages in the dataset, and thus say little about the downstream utility and equity of submitted systems, across languages. Utility System1 System1 Dataset and model contributions Pillar 2: Multi-faceted Evaluation L3, L4 Dataset3 L1, L2 Accuracy / F1/BLEU/ chrf \u2026 System2 System2 L1, L3 MT MT Most Under-Served Languages L4 \u2026 Equity Equity NER NER L3 L3 L3 L3 Improvement 960 datasets in 190 languages L4 L4 L3 . . . . . Accuracy / F1/BLEU/ chrf \u2026 \u2026 L2, L3 L2, L3 Pillar 1: Dataset Inclusivity Dataset1 Dataset1 L2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L2 Utility Dataset2 Dataset2 of 1,128 system submissions L1 L1 L1 L1 L1 Pillar 3: Reward L1, L2 Figure 2: GlobalBench\u2019s Philosophy: First, we aim to inclusively gather datasets for all tasks and languages. Second, we present a multi-faceted evaluation of systems, going beyond average accuracies across languages, to keep track of the utility and equity of these systems. Third, the leaderboard maintains a list of the most under-served languages, and rewards improvement in utility, which is achieved through both dataset and model contributions. Above, we see that the addition of System2 improves the measured utility of L2 and L3 for the MT leaderboard, and the addition of Dataset3 , improves the measured utility of L4 in the NER leaderboard. Hence, in designing GlobalBench, we make a number of intentional design decisions to explicitly promote the improvement of language technology for all of the world\u2019s citizens: Inclusive Dataset Selection: We aim to be able to evaluate all datasets in all languages for all tasks, making it possible to (in theory) cover any language for which a dataset exists. lens of GlobalBench (\u00a75), related work (\u00a76) and our expectations for the path forward (\u00a77). All in all, we believe that improving the quality and equity of language technologies for all speak- ers in the world is one of the paramount challenges of NLP today, and in the famous mantra from Peter Drucker, what you cannot measure, you cannot im- prove. GlobalBench is"}, {"question": " Why are some languages considered under-served by composite multilingual benchmarks?", "answer": " They are overlooked despite having a relatively high population.", "ref_chunk": "3 2 0 2 y a M 4 2 ] L C . s c [ 1 v 6 1 7 4 1 . 5 0 3 2 : v i X r a GlobalBench: A Benchmark for Global Progress in Natural Language Processing Yueqi Song1, Catherine Cui2, Simran Khanuja1, Pengfei Liu3, Fahim Faisal4, Alissa Ostapenko1, Genta Indra Winata5, Alham Fikri Aji6, Samuel Cahyawijaya5, Yulia Tsvetkov1,7, Antonios Anastasopoulos4, Graham Neubig1 1Carnegie Mellon University 2Harvard University 4George Mason University 3Shanghai Jiaotong University 5The Hong Kong University of Science and Technology 6MBZUAI 7University of Washington 1 Abstract Despite the major advances in NLP, significant disparities in NLP system performance across lan- guages still exist. Arguably, these are due to un- even resource allocation and sub-optimal incen- tives to work on less resourced languages. To track and further incentivize the global develop- ment of equitable language technology, we in- troduce GlobalBench. Prior multilingual bench- marks are static and have focused on a limited number of tasks and languages. In contrast, Glob- alBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring ac- curacy, GlobalBench also tracks the estimated per- speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served lan- guages are the ones with a relatively high pop- ulation, but nonetheless overlooked by compos- ite multilingual benchmarks (like Punjabi, Por- tuguese, and Wu Chinese). Currently, Global- Bench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 lan- guages1 Introduction . . . MT Demo.Avg. \u2026(othertasks) . . . NERcmn Ling.Avg. MTcmn GlobalBench NER GlobalBench NER NER MTspa GlobalBench MT NEReng All System Results NERspa GlobalBench MT MTeng Equity Equity Demo.Avg. Ling.Avg. Figure 1: GlobalBench Design: A leaderboard for each task is separately maintained. Each leaderboard con- tains a multi-faceted evaluation of submitted systems, along with a ranking of the most under-served languages. More details can be found in Section 2. (2021) argue that one major factor is a problem of incentives and resource allocation. For instance, languages associated with larger economic might (as measured by GDP of the countries where they are spoken) see more research and resource devel- opment, leading to more performant systems. Advances in multilingual natural language process- ing (NLP) technologies (Dabre et al., 2020; Hed- derich et al., 2021) have raised the enticing possibil- ities of NLP systems that benefit all people around the world. However, at the same time, studies into the state of multilingual NLP have demonstrated stark differences in the amount of resources avail- able (Joshi et al., 2020; Yu et al., 2022) and perfor- mance of existing NLP systems (Blasi et al., 2021; Khanuja et al., 2022). Why do these disparities exist? The causes of these disparities are multifarious, but Blasi et al. 1GlobalBench will be available on a hosted server for any- one to contribute systems or data, with detailed submission in- structions: https://explainaboard.inspiredco. ai/benchmark?parent_id=globalbench&show_ featured=false In this paper, we propose GlobalBench, a new benchmark and leaderboard that is designed to specifically incentivize the global development of equitable language technologies that serve speak- ers of all languages throughout the world. Glob- alBench follows the footsteps of other successful multilingual benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020), which aggregate results of systems across several tasks and provide a general idea of progress being made in the field of multilingual NLP. However, these benchmarks, by design, are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Additionally, they mainly focus on average accuracy over all languages in the dataset, and thus say little about the downstream utility and equity of submitted systems, across languages. Utility System1 System1 Dataset and model contributions Pillar 2: Multi-faceted Evaluation L3, L4 Dataset3 L1, L2 Accuracy / F1/BLEU/ chrf \u2026 System2 System2 L1, L3 MT MT Most Under-Served Languages L4 \u2026 Equity Equity NER NER L3 L3 L3 L3 Improvement 960 datasets in 190 languages L4 L4 L3 . . . . . Accuracy / F1/BLEU/ chrf \u2026 \u2026 L2, L3 L2, L3 Pillar 1: Dataset Inclusivity Dataset1 Dataset1 L2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L2 Utility Dataset2 Dataset2 of 1,128 system submissions L1 L1 L1 L1 L1 Pillar 3: Reward L1, L2 Figure 2: GlobalBench\u2019s Philosophy: First, we aim to inclusively gather datasets for all tasks and languages. Second, we present a multi-faceted evaluation of systems, going beyond average accuracies across languages, to keep track of the utility and equity of these systems. Third, the leaderboard maintains a list of the most under-served languages, and rewards improvement in utility, which is achieved through both dataset and model contributions. Above, we see that the addition of System2 improves the measured utility of L2 and L3 for the MT leaderboard, and the addition of Dataset3 , improves the measured utility of L4 in the NER leaderboard. Hence, in designing GlobalBench, we make a number of intentional design decisions to explicitly promote the improvement of language technology for all of the world\u2019s citizens: Inclusive Dataset Selection: We aim to be able to evaluate all datasets in all languages for all tasks, making it possible to (in theory) cover any language for which a dataset exists. lens of GlobalBench (\u00a75), related work (\u00a76) and our expectations for the path forward (\u00a77). All in all, we believe that improving the quality and equity of language technologies for all speak- ers in the world is one of the paramount challenges of NLP today, and in the famous mantra from Peter Drucker, what you cannot measure, you cannot im- prove. GlobalBench is"}, {"question": " What is the major factor contributing to disparities in NLP system performance across languages?", "answer": " Incentives and resource allocation.", "ref_chunk": "3 2 0 2 y a M 4 2 ] L C . s c [ 1 v 6 1 7 4 1 . 5 0 3 2 : v i X r a GlobalBench: A Benchmark for Global Progress in Natural Language Processing Yueqi Song1, Catherine Cui2, Simran Khanuja1, Pengfei Liu3, Fahim Faisal4, Alissa Ostapenko1, Genta Indra Winata5, Alham Fikri Aji6, Samuel Cahyawijaya5, Yulia Tsvetkov1,7, Antonios Anastasopoulos4, Graham Neubig1 1Carnegie Mellon University 2Harvard University 4George Mason University 3Shanghai Jiaotong University 5The Hong Kong University of Science and Technology 6MBZUAI 7University of Washington 1 Abstract Despite the major advances in NLP, significant disparities in NLP system performance across lan- guages still exist. Arguably, these are due to un- even resource allocation and sub-optimal incen- tives to work on less resourced languages. To track and further incentivize the global develop- ment of equitable language technology, we in- troduce GlobalBench. Prior multilingual bench- marks are static and have focused on a limited number of tasks and languages. In contrast, Glob- alBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring ac- curacy, GlobalBench also tracks the estimated per- speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served lan- guages are the ones with a relatively high pop- ulation, but nonetheless overlooked by compos- ite multilingual benchmarks (like Punjabi, Por- tuguese, and Wu Chinese). Currently, Global- Bench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 lan- guages1 Introduction . . . MT Demo.Avg. \u2026(othertasks) . . . NERcmn Ling.Avg. MTcmn GlobalBench NER GlobalBench NER NER MTspa GlobalBench MT NEReng All System Results NERspa GlobalBench MT MTeng Equity Equity Demo.Avg. Ling.Avg. Figure 1: GlobalBench Design: A leaderboard for each task is separately maintained. Each leaderboard con- tains a multi-faceted evaluation of submitted systems, along with a ranking of the most under-served languages. More details can be found in Section 2. (2021) argue that one major factor is a problem of incentives and resource allocation. For instance, languages associated with larger economic might (as measured by GDP of the countries where they are spoken) see more research and resource devel- opment, leading to more performant systems. Advances in multilingual natural language process- ing (NLP) technologies (Dabre et al., 2020; Hed- derich et al., 2021) have raised the enticing possibil- ities of NLP systems that benefit all people around the world. However, at the same time, studies into the state of multilingual NLP have demonstrated stark differences in the amount of resources avail- able (Joshi et al., 2020; Yu et al., 2022) and perfor- mance of existing NLP systems (Blasi et al., 2021; Khanuja et al., 2022). Why do these disparities exist? The causes of these disparities are multifarious, but Blasi et al. 1GlobalBench will be available on a hosted server for any- one to contribute systems or data, with detailed submission in- structions: https://explainaboard.inspiredco. ai/benchmark?parent_id=globalbench&show_ featured=false In this paper, we propose GlobalBench, a new benchmark and leaderboard that is designed to specifically incentivize the global development of equitable language technologies that serve speak- ers of all languages throughout the world. Glob- alBench follows the footsteps of other successful multilingual benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020), which aggregate results of systems across several tasks and provide a general idea of progress being made in the field of multilingual NLP. However, these benchmarks, by design, are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Additionally, they mainly focus on average accuracy over all languages in the dataset, and thus say little about the downstream utility and equity of submitted systems, across languages. Utility System1 System1 Dataset and model contributions Pillar 2: Multi-faceted Evaluation L3, L4 Dataset3 L1, L2 Accuracy / F1/BLEU/ chrf \u2026 System2 System2 L1, L3 MT MT Most Under-Served Languages L4 \u2026 Equity Equity NER NER L3 L3 L3 L3 Improvement 960 datasets in 190 languages L4 L4 L3 . . . . . Accuracy / F1/BLEU/ chrf \u2026 \u2026 L2, L3 L2, L3 Pillar 1: Dataset Inclusivity Dataset1 Dataset1 L2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L2 Utility Dataset2 Dataset2 of 1,128 system submissions L1 L1 L1 L1 L1 Pillar 3: Reward L1, L2 Figure 2: GlobalBench\u2019s Philosophy: First, we aim to inclusively gather datasets for all tasks and languages. Second, we present a multi-faceted evaluation of systems, going beyond average accuracies across languages, to keep track of the utility and equity of these systems. Third, the leaderboard maintains a list of the most under-served languages, and rewards improvement in utility, which is achieved through both dataset and model contributions. Above, we see that the addition of System2 improves the measured utility of L2 and L3 for the MT leaderboard, and the addition of Dataset3 , improves the measured utility of L4 in the NER leaderboard. Hence, in designing GlobalBench, we make a number of intentional design decisions to explicitly promote the improvement of language technology for all of the world\u2019s citizens: Inclusive Dataset Selection: We aim to be able to evaluate all datasets in all languages for all tasks, making it possible to (in theory) cover any language for which a dataset exists. lens of GlobalBench (\u00a75), related work (\u00a76) and our expectations for the path forward (\u00a77). All in all, we believe that improving the quality and equity of language technologies for all speak- ers in the world is one of the paramount challenges of NLP today, and in the famous mantra from Peter Drucker, what you cannot measure, you cannot im- prove. GlobalBench is"}, {"question": " What are some successful multilingual benchmarks that inspired the design of GlobalBench?", "answer": " XTREME and XGLUE.", "ref_chunk": "3 2 0 2 y a M 4 2 ] L C . s c [ 1 v 6 1 7 4 1 . 5 0 3 2 : v i X r a GlobalBench: A Benchmark for Global Progress in Natural Language Processing Yueqi Song1, Catherine Cui2, Simran Khanuja1, Pengfei Liu3, Fahim Faisal4, Alissa Ostapenko1, Genta Indra Winata5, Alham Fikri Aji6, Samuel Cahyawijaya5, Yulia Tsvetkov1,7, Antonios Anastasopoulos4, Graham Neubig1 1Carnegie Mellon University 2Harvard University 4George Mason University 3Shanghai Jiaotong University 5The Hong Kong University of Science and Technology 6MBZUAI 7University of Washington 1 Abstract Despite the major advances in NLP, significant disparities in NLP system performance across lan- guages still exist. Arguably, these are due to un- even resource allocation and sub-optimal incen- tives to work on less resourced languages. To track and further incentivize the global develop- ment of equitable language technology, we in- troduce GlobalBench. Prior multilingual bench- marks are static and have focused on a limited number of tasks and languages. In contrast, Glob- alBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring ac- curacy, GlobalBench also tracks the estimated per- speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served lan- guages are the ones with a relatively high pop- ulation, but nonetheless overlooked by compos- ite multilingual benchmarks (like Punjabi, Por- tuguese, and Wu Chinese). Currently, Global- Bench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 lan- guages1 Introduction . . . MT Demo.Avg. \u2026(othertasks) . . . NERcmn Ling.Avg. MTcmn GlobalBench NER GlobalBench NER NER MTspa GlobalBench MT NEReng All System Results NERspa GlobalBench MT MTeng Equity Equity Demo.Avg. Ling.Avg. Figure 1: GlobalBench Design: A leaderboard for each task is separately maintained. Each leaderboard con- tains a multi-faceted evaluation of submitted systems, along with a ranking of the most under-served languages. More details can be found in Section 2. (2021) argue that one major factor is a problem of incentives and resource allocation. For instance, languages associated with larger economic might (as measured by GDP of the countries where they are spoken) see more research and resource devel- opment, leading to more performant systems. Advances in multilingual natural language process- ing (NLP) technologies (Dabre et al., 2020; Hed- derich et al., 2021) have raised the enticing possibil- ities of NLP systems that benefit all people around the world. However, at the same time, studies into the state of multilingual NLP have demonstrated stark differences in the amount of resources avail- able (Joshi et al., 2020; Yu et al., 2022) and perfor- mance of existing NLP systems (Blasi et al., 2021; Khanuja et al., 2022). Why do these disparities exist? The causes of these disparities are multifarious, but Blasi et al. 1GlobalBench will be available on a hosted server for any- one to contribute systems or data, with detailed submission in- structions: https://explainaboard.inspiredco. ai/benchmark?parent_id=globalbench&show_ featured=false In this paper, we propose GlobalBench, a new benchmark and leaderboard that is designed to specifically incentivize the global development of equitable language technologies that serve speak- ers of all languages throughout the world. Glob- alBench follows the footsteps of other successful multilingual benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020), which aggregate results of systems across several tasks and provide a general idea of progress being made in the field of multilingual NLP. However, these benchmarks, by design, are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Additionally, they mainly focus on average accuracy over all languages in the dataset, and thus say little about the downstream utility and equity of submitted systems, across languages. Utility System1 System1 Dataset and model contributions Pillar 2: Multi-faceted Evaluation L3, L4 Dataset3 L1, L2 Accuracy / F1/BLEU/ chrf \u2026 System2 System2 L1, L3 MT MT Most Under-Served Languages L4 \u2026 Equity Equity NER NER L3 L3 L3 L3 Improvement 960 datasets in 190 languages L4 L4 L3 . . . . . Accuracy / F1/BLEU/ chrf \u2026 \u2026 L2, L3 L2, L3 Pillar 1: Dataset Inclusivity Dataset1 Dataset1 L2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L2 Utility Dataset2 Dataset2 of 1,128 system submissions L1 L1 L1 L1 L1 Pillar 3: Reward L1, L2 Figure 2: GlobalBench\u2019s Philosophy: First, we aim to inclusively gather datasets for all tasks and languages. Second, we present a multi-faceted evaluation of systems, going beyond average accuracies across languages, to keep track of the utility and equity of these systems. Third, the leaderboard maintains a list of the most under-served languages, and rewards improvement in utility, which is achieved through both dataset and model contributions. Above, we see that the addition of System2 improves the measured utility of L2 and L3 for the MT leaderboard, and the addition of Dataset3 , improves the measured utility of L4 in the NER leaderboard. Hence, in designing GlobalBench, we make a number of intentional design decisions to explicitly promote the improvement of language technology for all of the world\u2019s citizens: Inclusive Dataset Selection: We aim to be able to evaluate all datasets in all languages for all tasks, making it possible to (in theory) cover any language for which a dataset exists. lens of GlobalBench (\u00a75), related work (\u00a76) and our expectations for the path forward (\u00a77). All in all, we believe that improving the quality and equity of language technologies for all speak- ers in the world is one of the paramount challenges of NLP today, and in the famous mantra from Peter Drucker, what you cannot measure, you cannot im- prove. GlobalBench is"}, {"question": " What intentional design decision does GlobalBench make regarding dataset selection?", "answer": " To evaluate all datasets in all languages for all tasks.", "ref_chunk": "3 2 0 2 y a M 4 2 ] L C . s c [ 1 v 6 1 7 4 1 . 5 0 3 2 : v i X r a GlobalBench: A Benchmark for Global Progress in Natural Language Processing Yueqi Song1, Catherine Cui2, Simran Khanuja1, Pengfei Liu3, Fahim Faisal4, Alissa Ostapenko1, Genta Indra Winata5, Alham Fikri Aji6, Samuel Cahyawijaya5, Yulia Tsvetkov1,7, Antonios Anastasopoulos4, Graham Neubig1 1Carnegie Mellon University 2Harvard University 4George Mason University 3Shanghai Jiaotong University 5The Hong Kong University of Science and Technology 6MBZUAI 7University of Washington 1 Abstract Despite the major advances in NLP, significant disparities in NLP system performance across lan- guages still exist. Arguably, these are due to un- even resource allocation and sub-optimal incen- tives to work on less resourced languages. To track and further incentivize the global develop- ment of equitable language technology, we in- troduce GlobalBench. Prior multilingual bench- marks are static and have focused on a limited number of tasks and languages. In contrast, Glob- alBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring ac- curacy, GlobalBench also tracks the estimated per- speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served lan- guages are the ones with a relatively high pop- ulation, but nonetheless overlooked by compos- ite multilingual benchmarks (like Punjabi, Por- tuguese, and Wu Chinese). Currently, Global- Bench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 lan- guages1 Introduction . . . MT Demo.Avg. \u2026(othertasks) . . . NERcmn Ling.Avg. MTcmn GlobalBench NER GlobalBench NER NER MTspa GlobalBench MT NEReng All System Results NERspa GlobalBench MT MTeng Equity Equity Demo.Avg. Ling.Avg. Figure 1: GlobalBench Design: A leaderboard for each task is separately maintained. Each leaderboard con- tains a multi-faceted evaluation of submitted systems, along with a ranking of the most under-served languages. More details can be found in Section 2. (2021) argue that one major factor is a problem of incentives and resource allocation. For instance, languages associated with larger economic might (as measured by GDP of the countries where they are spoken) see more research and resource devel- opment, leading to more performant systems. Advances in multilingual natural language process- ing (NLP) technologies (Dabre et al., 2020; Hed- derich et al., 2021) have raised the enticing possibil- ities of NLP systems that benefit all people around the world. However, at the same time, studies into the state of multilingual NLP have demonstrated stark differences in the amount of resources avail- able (Joshi et al., 2020; Yu et al., 2022) and perfor- mance of existing NLP systems (Blasi et al., 2021; Khanuja et al., 2022). Why do these disparities exist? The causes of these disparities are multifarious, but Blasi et al. 1GlobalBench will be available on a hosted server for any- one to contribute systems or data, with detailed submission in- structions: https://explainaboard.inspiredco. ai/benchmark?parent_id=globalbench&show_ featured=false In this paper, we propose GlobalBench, a new benchmark and leaderboard that is designed to specifically incentivize the global development of equitable language technologies that serve speak- ers of all languages throughout the world. Glob- alBench follows the footsteps of other successful multilingual benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020), which aggregate results of systems across several tasks and provide a general idea of progress being made in the field of multilingual NLP. However, these benchmarks, by design, are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Additionally, they mainly focus on average accuracy over all languages in the dataset, and thus say little about the downstream utility and equity of submitted systems, across languages. Utility System1 System1 Dataset and model contributions Pillar 2: Multi-faceted Evaluation L3, L4 Dataset3 L1, L2 Accuracy / F1/BLEU/ chrf \u2026 System2 System2 L1, L3 MT MT Most Under-Served Languages L4 \u2026 Equity Equity NER NER L3 L3 L3 L3 Improvement 960 datasets in 190 languages L4 L4 L3 . . . . . Accuracy / F1/BLEU/ chrf \u2026 \u2026 L2, L3 L2, L3 Pillar 1: Dataset Inclusivity Dataset1 Dataset1 L2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L2 Utility Dataset2 Dataset2 of 1,128 system submissions L1 L1 L1 L1 L1 Pillar 3: Reward L1, L2 Figure 2: GlobalBench\u2019s Philosophy: First, we aim to inclusively gather datasets for all tasks and languages. Second, we present a multi-faceted evaluation of systems, going beyond average accuracies across languages, to keep track of the utility and equity of these systems. Third, the leaderboard maintains a list of the most under-served languages, and rewards improvement in utility, which is achieved through both dataset and model contributions. Above, we see that the addition of System2 improves the measured utility of L2 and L3 for the MT leaderboard, and the addition of Dataset3 , improves the measured utility of L4 in the NER leaderboard. Hence, in designing GlobalBench, we make a number of intentional design decisions to explicitly promote the improvement of language technology for all of the world\u2019s citizens: Inclusive Dataset Selection: We aim to be able to evaluate all datasets in all languages for all tasks, making it possible to (in theory) cover any language for which a dataset exists. lens of GlobalBench (\u00a75), related work (\u00a76) and our expectations for the path forward (\u00a77). All in all, we believe that improving the quality and equity of language technologies for all speak- ers in the world is one of the paramount challenges of NLP today, and in the famous mantra from Peter Drucker, what you cannot measure, you cannot im- prove. GlobalBench is"}, {"question": " According to the text, what is one of the paramount challenges of NLP today?", "answer": " Improving the quality and equity of language technologies for all speakers in the world.", "ref_chunk": "3 2 0 2 y a M 4 2 ] L C . s c [ 1 v 6 1 7 4 1 . 5 0 3 2 : v i X r a GlobalBench: A Benchmark for Global Progress in Natural Language Processing Yueqi Song1, Catherine Cui2, Simran Khanuja1, Pengfei Liu3, Fahim Faisal4, Alissa Ostapenko1, Genta Indra Winata5, Alham Fikri Aji6, Samuel Cahyawijaya5, Yulia Tsvetkov1,7, Antonios Anastasopoulos4, Graham Neubig1 1Carnegie Mellon University 2Harvard University 4George Mason University 3Shanghai Jiaotong University 5The Hong Kong University of Science and Technology 6MBZUAI 7University of Washington 1 Abstract Despite the major advances in NLP, significant disparities in NLP system performance across lan- guages still exist. Arguably, these are due to un- even resource allocation and sub-optimal incen- tives to work on less resourced languages. To track and further incentivize the global develop- ment of equitable language technology, we in- troduce GlobalBench. Prior multilingual bench- marks are static and have focused on a limited number of tasks and languages. In contrast, Glob- alBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring ac- curacy, GlobalBench also tracks the estimated per- speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served lan- guages are the ones with a relatively high pop- ulation, but nonetheless overlooked by compos- ite multilingual benchmarks (like Punjabi, Por- tuguese, and Wu Chinese). Currently, Global- Bench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 lan- guages1 Introduction . . . MT Demo.Avg. \u2026(othertasks) . . . NERcmn Ling.Avg. MTcmn GlobalBench NER GlobalBench NER NER MTspa GlobalBench MT NEReng All System Results NERspa GlobalBench MT MTeng Equity Equity Demo.Avg. Ling.Avg. Figure 1: GlobalBench Design: A leaderboard for each task is separately maintained. Each leaderboard con- tains a multi-faceted evaluation of submitted systems, along with a ranking of the most under-served languages. More details can be found in Section 2. (2021) argue that one major factor is a problem of incentives and resource allocation. For instance, languages associated with larger economic might (as measured by GDP of the countries where they are spoken) see more research and resource devel- opment, leading to more performant systems. Advances in multilingual natural language process- ing (NLP) technologies (Dabre et al., 2020; Hed- derich et al., 2021) have raised the enticing possibil- ities of NLP systems that benefit all people around the world. However, at the same time, studies into the state of multilingual NLP have demonstrated stark differences in the amount of resources avail- able (Joshi et al., 2020; Yu et al., 2022) and perfor- mance of existing NLP systems (Blasi et al., 2021; Khanuja et al., 2022). Why do these disparities exist? The causes of these disparities are multifarious, but Blasi et al. 1GlobalBench will be available on a hosted server for any- one to contribute systems or data, with detailed submission in- structions: https://explainaboard.inspiredco. ai/benchmark?parent_id=globalbench&show_ featured=false In this paper, we propose GlobalBench, a new benchmark and leaderboard that is designed to specifically incentivize the global development of equitable language technologies that serve speak- ers of all languages throughout the world. Glob- alBench follows the footsteps of other successful multilingual benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020), which aggregate results of systems across several tasks and provide a general idea of progress being made in the field of multilingual NLP. However, these benchmarks, by design, are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Additionally, they mainly focus on average accuracy over all languages in the dataset, and thus say little about the downstream utility and equity of submitted systems, across languages. Utility System1 System1 Dataset and model contributions Pillar 2: Multi-faceted Evaluation L3, L4 Dataset3 L1, L2 Accuracy / F1/BLEU/ chrf \u2026 System2 System2 L1, L3 MT MT Most Under-Served Languages L4 \u2026 Equity Equity NER NER L3 L3 L3 L3 Improvement 960 datasets in 190 languages L4 L4 L3 . . . . . Accuracy / F1/BLEU/ chrf \u2026 \u2026 L2, L3 L2, L3 Pillar 1: Dataset Inclusivity Dataset1 Dataset1 L2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L2 Utility Dataset2 Dataset2 of 1,128 system submissions L1 L1 L1 L1 L1 Pillar 3: Reward L1, L2 Figure 2: GlobalBench\u2019s Philosophy: First, we aim to inclusively gather datasets for all tasks and languages. Second, we present a multi-faceted evaluation of systems, going beyond average accuracies across languages, to keep track of the utility and equity of these systems. Third, the leaderboard maintains a list of the most under-served languages, and rewards improvement in utility, which is achieved through both dataset and model contributions. Above, we see that the addition of System2 improves the measured utility of L2 and L3 for the MT leaderboard, and the addition of Dataset3 , improves the measured utility of L4 in the NER leaderboard. Hence, in designing GlobalBench, we make a number of intentional design decisions to explicitly promote the improvement of language technology for all of the world\u2019s citizens: Inclusive Dataset Selection: We aim to be able to evaluate all datasets in all languages for all tasks, making it possible to (in theory) cover any language for which a dataset exists. lens of GlobalBench (\u00a75), related work (\u00a76) and our expectations for the path forward (\u00a77). All in all, we believe that improving the quality and equity of language technologies for all speak- ers in the world is one of the paramount challenges of NLP today, and in the famous mantra from Peter Drucker, what you cannot measure, you cannot im- prove. GlobalBench is"}, {"question": " What is the underlying belief behind the creation of GlobalBench?", "answer": " What you cannot measure, you cannot improve.", "ref_chunk": "3 2 0 2 y a M 4 2 ] L C . s c [ 1 v 6 1 7 4 1 . 5 0 3 2 : v i X r a GlobalBench: A Benchmark for Global Progress in Natural Language Processing Yueqi Song1, Catherine Cui2, Simran Khanuja1, Pengfei Liu3, Fahim Faisal4, Alissa Ostapenko1, Genta Indra Winata5, Alham Fikri Aji6, Samuel Cahyawijaya5, Yulia Tsvetkov1,7, Antonios Anastasopoulos4, Graham Neubig1 1Carnegie Mellon University 2Harvard University 4George Mason University 3Shanghai Jiaotong University 5The Hong Kong University of Science and Technology 6MBZUAI 7University of Washington 1 Abstract Despite the major advances in NLP, significant disparities in NLP system performance across lan- guages still exist. Arguably, these are due to un- even resource allocation and sub-optimal incen- tives to work on less resourced languages. To track and further incentivize the global develop- ment of equitable language technology, we in- troduce GlobalBench. Prior multilingual bench- marks are static and have focused on a limited number of tasks and languages. In contrast, Glob- alBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring ac- curacy, GlobalBench also tracks the estimated per- speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served lan- guages are the ones with a relatively high pop- ulation, but nonetheless overlooked by compos- ite multilingual benchmarks (like Punjabi, Por- tuguese, and Wu Chinese). Currently, Global- Bench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 lan- guages1 Introduction . . . MT Demo.Avg. \u2026(othertasks) . . . NERcmn Ling.Avg. MTcmn GlobalBench NER GlobalBench NER NER MTspa GlobalBench MT NEReng All System Results NERspa GlobalBench MT MTeng Equity Equity Demo.Avg. Ling.Avg. Figure 1: GlobalBench Design: A leaderboard for each task is separately maintained. Each leaderboard con- tains a multi-faceted evaluation of submitted systems, along with a ranking of the most under-served languages. More details can be found in Section 2. (2021) argue that one major factor is a problem of incentives and resource allocation. For instance, languages associated with larger economic might (as measured by GDP of the countries where they are spoken) see more research and resource devel- opment, leading to more performant systems. Advances in multilingual natural language process- ing (NLP) technologies (Dabre et al., 2020; Hed- derich et al., 2021) have raised the enticing possibil- ities of NLP systems that benefit all people around the world. However, at the same time, studies into the state of multilingual NLP have demonstrated stark differences in the amount of resources avail- able (Joshi et al., 2020; Yu et al., 2022) and perfor- mance of existing NLP systems (Blasi et al., 2021; Khanuja et al., 2022). Why do these disparities exist? The causes of these disparities are multifarious, but Blasi et al. 1GlobalBench will be available on a hosted server for any- one to contribute systems or data, with detailed submission in- structions: https://explainaboard.inspiredco. ai/benchmark?parent_id=globalbench&show_ featured=false In this paper, we propose GlobalBench, a new benchmark and leaderboard that is designed to specifically incentivize the global development of equitable language technologies that serve speak- ers of all languages throughout the world. Glob- alBench follows the footsteps of other successful multilingual benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020), which aggregate results of systems across several tasks and provide a general idea of progress being made in the field of multilingual NLP. However, these benchmarks, by design, are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Additionally, they mainly focus on average accuracy over all languages in the dataset, and thus say little about the downstream utility and equity of submitted systems, across languages. Utility System1 System1 Dataset and model contributions Pillar 2: Multi-faceted Evaluation L3, L4 Dataset3 L1, L2 Accuracy / F1/BLEU/ chrf \u2026 System2 System2 L1, L3 MT MT Most Under-Served Languages L4 \u2026 Equity Equity NER NER L3 L3 L3 L3 Improvement 960 datasets in 190 languages L4 L4 L3 . . . . . Accuracy / F1/BLEU/ chrf \u2026 \u2026 L2, L3 L2, L3 Pillar 1: Dataset Inclusivity Dataset1 Dataset1 L2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L2 Utility Dataset2 Dataset2 of 1,128 system submissions L1 L1 L1 L1 L1 Pillar 3: Reward L1, L2 Figure 2: GlobalBench\u2019s Philosophy: First, we aim to inclusively gather datasets for all tasks and languages. Second, we present a multi-faceted evaluation of systems, going beyond average accuracies across languages, to keep track of the utility and equity of these systems. Third, the leaderboard maintains a list of the most under-served languages, and rewards improvement in utility, which is achieved through both dataset and model contributions. Above, we see that the addition of System2 improves the measured utility of L2 and L3 for the MT leaderboard, and the addition of Dataset3 , improves the measured utility of L4 in the NER leaderboard. Hence, in designing GlobalBench, we make a number of intentional design decisions to explicitly promote the improvement of language technology for all of the world\u2019s citizens: Inclusive Dataset Selection: We aim to be able to evaluate all datasets in all languages for all tasks, making it possible to (in theory) cover any language for which a dataset exists. lens of GlobalBench (\u00a75), related work (\u00a76) and our expectations for the path forward (\u00a77). All in all, we believe that improving the quality and equity of language technologies for all speak- ers in the world is one of the paramount challenges of NLP today, and in the famous mantra from Peter Drucker, what you cannot measure, you cannot im- prove. GlobalBench is"}, {"question": " How does GlobalBench aim to promote improvement in language technology for all speakers?", "answer": " By explicitly measuring and incentivizing the improvement of language technology for all.", "ref_chunk": "3 2 0 2 y a M 4 2 ] L C . s c [ 1 v 6 1 7 4 1 . 5 0 3 2 : v i X r a GlobalBench: A Benchmark for Global Progress in Natural Language Processing Yueqi Song1, Catherine Cui2, Simran Khanuja1, Pengfei Liu3, Fahim Faisal4, Alissa Ostapenko1, Genta Indra Winata5, Alham Fikri Aji6, Samuel Cahyawijaya5, Yulia Tsvetkov1,7, Antonios Anastasopoulos4, Graham Neubig1 1Carnegie Mellon University 2Harvard University 4George Mason University 3Shanghai Jiaotong University 5The Hong Kong University of Science and Technology 6MBZUAI 7University of Washington 1 Abstract Despite the major advances in NLP, significant disparities in NLP system performance across lan- guages still exist. Arguably, these are due to un- even resource allocation and sub-optimal incen- tives to work on less resourced languages. To track and further incentivize the global develop- ment of equitable language technology, we in- troduce GlobalBench. Prior multilingual bench- marks are static and have focused on a limited number of tasks and languages. In contrast, Glob- alBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring ac- curacy, GlobalBench also tracks the estimated per- speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served lan- guages are the ones with a relatively high pop- ulation, but nonetheless overlooked by compos- ite multilingual benchmarks (like Punjabi, Por- tuguese, and Wu Chinese). Currently, Global- Bench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 lan- guages1 Introduction . . . MT Demo.Avg. \u2026(othertasks) . . . NERcmn Ling.Avg. MTcmn GlobalBench NER GlobalBench NER NER MTspa GlobalBench MT NEReng All System Results NERspa GlobalBench MT MTeng Equity Equity Demo.Avg. Ling.Avg. Figure 1: GlobalBench Design: A leaderboard for each task is separately maintained. Each leaderboard con- tains a multi-faceted evaluation of submitted systems, along with a ranking of the most under-served languages. More details can be found in Section 2. (2021) argue that one major factor is a problem of incentives and resource allocation. For instance, languages associated with larger economic might (as measured by GDP of the countries where they are spoken) see more research and resource devel- opment, leading to more performant systems. Advances in multilingual natural language process- ing (NLP) technologies (Dabre et al., 2020; Hed- derich et al., 2021) have raised the enticing possibil- ities of NLP systems that benefit all people around the world. However, at the same time, studies into the state of multilingual NLP have demonstrated stark differences in the amount of resources avail- able (Joshi et al., 2020; Yu et al., 2022) and perfor- mance of existing NLP systems (Blasi et al., 2021; Khanuja et al., 2022). Why do these disparities exist? The causes of these disparities are multifarious, but Blasi et al. 1GlobalBench will be available on a hosted server for any- one to contribute systems or data, with detailed submission in- structions: https://explainaboard.inspiredco. ai/benchmark?parent_id=globalbench&show_ featured=false In this paper, we propose GlobalBench, a new benchmark and leaderboard that is designed to specifically incentivize the global development of equitable language technologies that serve speak- ers of all languages throughout the world. Glob- alBench follows the footsteps of other successful multilingual benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020), which aggregate results of systems across several tasks and provide a general idea of progress being made in the field of multilingual NLP. However, these benchmarks, by design, are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Additionally, they mainly focus on average accuracy over all languages in the dataset, and thus say little about the downstream utility and equity of submitted systems, across languages. Utility System1 System1 Dataset and model contributions Pillar 2: Multi-faceted Evaluation L3, L4 Dataset3 L1, L2 Accuracy / F1/BLEU/ chrf \u2026 System2 System2 L1, L3 MT MT Most Under-Served Languages L4 \u2026 Equity Equity NER NER L3 L3 L3 L3 Improvement 960 datasets in 190 languages L4 L4 L3 . . . . . Accuracy / F1/BLEU/ chrf \u2026 \u2026 L2, L3 L2, L3 Pillar 1: Dataset Inclusivity Dataset1 Dataset1 L2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L2 Utility Dataset2 Dataset2 of 1,128 system submissions L1 L1 L1 L1 L1 Pillar 3: Reward L1, L2 Figure 2: GlobalBench\u2019s Philosophy: First, we aim to inclusively gather datasets for all tasks and languages. Second, we present a multi-faceted evaluation of systems, going beyond average accuracies across languages, to keep track of the utility and equity of these systems. Third, the leaderboard maintains a list of the most under-served languages, and rewards improvement in utility, which is achieved through both dataset and model contributions. Above, we see that the addition of System2 improves the measured utility of L2 and L3 for the MT leaderboard, and the addition of Dataset3 , improves the measured utility of L4 in the NER leaderboard. Hence, in designing GlobalBench, we make a number of intentional design decisions to explicitly promote the improvement of language technology for all of the world\u2019s citizens: Inclusive Dataset Selection: We aim to be able to evaluate all datasets in all languages for all tasks, making it possible to (in theory) cover any language for which a dataset exists. lens of GlobalBench (\u00a75), related work (\u00a76) and our expectations for the path forward (\u00a77). All in all, we believe that improving the quality and equity of language technologies for all speak- ers in the world is one of the paramount challenges of NLP today, and in the famous mantra from Peter Drucker, what you cannot measure, you cannot im- prove. GlobalBench is"}], "doc_text": "3 2 0 2 y a M 4 2 ] L C . s c [ 1 v 6 1 7 4 1 . 5 0 3 2 : v i X r a GlobalBench: A Benchmark for Global Progress in Natural Language Processing Yueqi Song1, Catherine Cui2, Simran Khanuja1, Pengfei Liu3, Fahim Faisal4, Alissa Ostapenko1, Genta Indra Winata5, Alham Fikri Aji6, Samuel Cahyawijaya5, Yulia Tsvetkov1,7, Antonios Anastasopoulos4, Graham Neubig1 1Carnegie Mellon University 2Harvard University 4George Mason University 3Shanghai Jiaotong University 5The Hong Kong University of Science and Technology 6MBZUAI 7University of Washington 1 Abstract Despite the major advances in NLP, significant disparities in NLP system performance across lan- guages still exist. Arguably, these are due to un- even resource allocation and sub-optimal incen- tives to work on less resourced languages. To track and further incentivize the global develop- ment of equitable language technology, we in- troduce GlobalBench. Prior multilingual bench- marks are static and have focused on a limited number of tasks and languages. In contrast, Glob- alBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring ac- curacy, GlobalBench also tracks the estimated per- speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served lan- guages are the ones with a relatively high pop- ulation, but nonetheless overlooked by compos- ite multilingual benchmarks (like Punjabi, Por- tuguese, and Wu Chinese). Currently, Global- Bench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 lan- guages1 Introduction . . . MT Demo.Avg. \u2026(othertasks) . . . NERcmn Ling.Avg. MTcmn GlobalBench NER GlobalBench NER NER MTspa GlobalBench MT NEReng All System Results NERspa GlobalBench MT MTeng Equity Equity Demo.Avg. Ling.Avg. Figure 1: GlobalBench Design: A leaderboard for each task is separately maintained. Each leaderboard con- tains a multi-faceted evaluation of submitted systems, along with a ranking of the most under-served languages. More details can be found in Section 2. (2021) argue that one major factor is a problem of incentives and resource allocation. For instance, languages associated with larger economic might (as measured by GDP of the countries where they are spoken) see more research and resource devel- opment, leading to more performant systems. Advances in multilingual natural language process- ing (NLP) technologies (Dabre et al., 2020; Hed- derich et al., 2021) have raised the enticing possibil- ities of NLP systems that benefit all people around the world. However, at the same time, studies into the state of multilingual NLP have demonstrated stark differences in the amount of resources avail- able (Joshi et al., 2020; Yu et al., 2022) and perfor- mance of existing NLP systems (Blasi et al., 2021; Khanuja et al., 2022). Why do these disparities exist? The causes of these disparities are multifarious, but Blasi et al. 1GlobalBench will be available on a hosted server for any- one to contribute systems or data, with detailed submission in- structions: https://explainaboard.inspiredco. ai/benchmark?parent_id=globalbench&show_ featured=false In this paper, we propose GlobalBench, a new benchmark and leaderboard that is designed to specifically incentivize the global development of equitable language technologies that serve speak- ers of all languages throughout the world. Glob- alBench follows the footsteps of other successful multilingual benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020), which aggregate results of systems across several tasks and provide a general idea of progress being made in the field of multilingual NLP. However, these benchmarks, by design, are static and lack the goal to be an all-inclusive, ever-expanding collec- tion of datasets. Additionally, they mainly focus on average accuracy over all languages in the dataset, and thus say little about the downstream utility and equity of submitted systems, across languages. Utility System1 System1 Dataset and model contributions Pillar 2: Multi-faceted Evaluation L3, L4 Dataset3 L1, L2 Accuracy / F1/BLEU/ chrf \u2026 System2 System2 L1, L3 MT MT Most Under-Served Languages L4 \u2026 Equity Equity NER NER L3 L3 L3 L3 Improvement 960 datasets in 190 languages L4 L4 L3 . . . . . Accuracy / F1/BLEU/ chrf \u2026 \u2026 L2, L3 L2, L3 Pillar 1: Dataset Inclusivity Dataset1 Dataset1 L2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L2 Utility Dataset2 Dataset2 of 1,128 system submissions L1 L1 L1 L1 L1 Pillar 3: Reward L1, L2 Figure 2: GlobalBench\u2019s Philosophy: First, we aim to inclusively gather datasets for all tasks and languages. Second, we present a multi-faceted evaluation of systems, going beyond average accuracies across languages, to keep track of the utility and equity of these systems. Third, the leaderboard maintains a list of the most under-served languages, and rewards improvement in utility, which is achieved through both dataset and model contributions. Above, we see that the addition of System2 improves the measured utility of L2 and L3 for the MT leaderboard, and the addition of Dataset3 , improves the measured utility of L4 in the NER leaderboard. Hence, in designing GlobalBench, we make a number of intentional design decisions to explicitly promote the improvement of language technology for all of the world\u2019s citizens: Inclusive Dataset Selection: We aim to be able to evaluate all datasets in all languages for all tasks, making it possible to (in theory) cover any language for which a dataset exists. lens of GlobalBench (\u00a75), related work (\u00a76) and our expectations for the path forward (\u00a77). All in all, we believe that improving the quality and equity of language technologies for all speak- ers in the world is one of the paramount challenges of NLP today, and in the famous mantra from Peter Drucker, what you cannot measure, you cannot im- prove. GlobalBench is"}