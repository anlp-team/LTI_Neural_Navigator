{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/A._Waibel_Incremental_Blockwise_Beam_Search_for_Simultaneous_Speech_Translation_with_Controllable_Quality-Latency_Tradeoff_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What improvements in latency were observed for English to German, Spanish, and French using the proposed IBWBS compared to the original BWBS?", "answer": " Latency improvements of 959, 1395, and 752 milliseconds were observed for English to German, Spanish, and French, respectively.", "ref_chunk": "the onlinized models, we select models with similar BLEU scores because original BWBS decoding has much higher latencies compared to the proposed IBWBS. In Table 2, we observe la- tency improvements of 959, 1395, and 752 milliseconds for En- glish to German, Spanish, and French, respectively. Lang Decoding # fw. passes LAAL BLEU 4.6. Improved Blockwise Streaming Beam Search for Full- Context Models En-De BS IBWBS 729,091 583,787 1676 1879 21.4 27.6 En-Es BS IBWBS 749,272 608,664 1643 1839 25.7 31.4 En-Fr BS IBWBS 837,084 675,683 1665 1854 29.0 37.3 Table 3: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with local agreement policy using on- linized full-context models. In this section, we compare the standard beam search with the proposed IBWBS (see Section 3.3). For the local agreement policy, we present the results in Table 3. For systems with a latency of approx. 1.7 sec, we observe a quality improvement of 6.2, 5.7, and 8.3 BLEU for En-De, En-Es, and En-Fr, respec- tively. Additionally, we measure the computational complexity. To avoid hardware-specific evaluation, we report the number of forward passes through the decoder. From the middle column in Table 3, we see the IBWBS helps reduce the computational complexity by approx. 20 %. 4.4. Controllable Latency for Blockwise Encoder In Figure 2a, we show the latency control using the block size. Each point on the BWBS line corresponds to a single model. As described in Section 3.1, we apply incremental policies to facil- itate latency control. In Figure 2b, we apply the incremental pruning with hold-n and local agreement policies to the model with block 40, which allows for a wide range of latencies. 4.5. Improved Blockwise Streaming Beam Search In Figure 2c, we illustrate the benefits of the proposed improved BWBS (see Section 3.2) on En-Fr. Compared to the original BWBS in Figure 2b, we observe improvements in both quality (by more than 2 BLEU) and latency (by 500 ms). Results on all languages are in Table 2. For the blockwise model, we select systems with a latency of approx. 2300 ms and observe a quality improvement of 0.6, 0.9, and 3.6 BLEU for In Table 4, we compare the proposed IBWBS and the stan- dard beam search for the hold-n policy. Here, we do not observe any quality or latency change. However, similarly to the local agreement, we observe a computational complexity reduction of 20 to 30 % across all languages. 5. Conclusion In this paper, we take a deep dive into blockwise encoders and blockwise streaming beam search for simultaneous speech translation. We propose a modified incremental blockwise beam search, which incorporates local agreement or hold-n policies for quality-latency control. This enables incremen- tal SST and facilitates latency control without the need to re- train the model. Furthermore, we document that the proposed changes bring improvement to both quality and latency. Ad- ditionally, we show our framework can be directly applied to full-context encoders, which leads to quality and performance improvements compared to the standard beam search. 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] [3] A. Anastasopoulos et al., \u201cFINDINGS OF THE IWSLT 2022 EVALUATION CAMPAIGN,\u201d in Proceedings of the 19th Inter- national Conference on Spoken Language Translation (IWSLT 2022), 2022. [4] M. Ma et al., \u201cStacl: Simultaneous translation with implicit an- ticipation and controllable latency using prefix-to-prefix frame- work,\u201d in Proc. ACL, 2019, pp. 3025\u20133036. [5] X. Ma, J. Pino, and P. Koehn, \u201cSimulMT to SimulST: Adapting simultaneous text translation to end-to-end simultaneous speech translation,\u201d in Proc. ACL, 2020, pp. 582\u2013587. [6] D. Liu, G. Spanakis, and J. Niehues, \u201cLow-Latency Sequence- to-Sequence Speech Recognition and Translation by Partial Hy- pothesis Selection,\u201d in Proc. Interspeech, 2020, pp. 3620\u20133624. [7] P. Pol\u00b4ak et al., \u201cCUNI-KIT System for Smultaneous Speech Translation Task at IWSLT 2022,\u201d in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [8] S. Watanabe et al., \u201cHybrid ctc/attention architecture for end- to-end speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240\u20131253, 2017. [9] X. Ma et al., \u201cMonotonic multihead attention,\u201d arXiv preprint arXiv:1909.12406, 2019. [10] C.-C. Chiu and C. Raffel, \u201cMonotonic chunkwise attention,\u201d in Proc. ICLR, 2017. [11] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, \u201cStreaming trans- former asr with blockwise synchronous beam search,\u201d in Proc. SLT, 2021, pp. 22\u201329. [12] K. Deng et al., \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d arXiv preprint arXiv:2204.08920, 2022. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recognition with the transformer model,\u201d in Proc. ICASSP, 2020, pp. 6074\u20136078. [14] Y. Shi et al., \u201cEmformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6783\u20136787. [15] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d Advances in neural information processing systems, vol. 27, 2014. [16] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine trans- lation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014. [17] J. Niehues et al., \u201cDynamic transcription for low-latency speech translation,\u201d in 17th Annual Conference of the International Speech Communication Association, INTERSPEECH 2016; Hy- att Regency San FranciscoSan Francisco; United States; 8 September 2016 through 16 September 2016, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association. Ed. : N. Morgan, vol. 08-12-September-2016, 2016, pp. 2513\u20132517. [18] J. Niehues et al., \u201cLow-latency neural speech translation,\u201d in 19th Annual Conference of the International Speech Commu- nication, INTERSPEECH 2018; Hyderabad International Con- vention Centre (HICC)Hyderabad; India; 2 September 2018 through 6 September 2018. Ed.: C.C. Sekhar, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association, INTERSPEECH, vol. 2018-September, 2018, pp. 1293\u20131297. [19] X. Ma et al.,"}, {"question": " In the table comparing the standard beam search (BS) and proposed IBWBS, how much was the computational complexity reduced by the IBWBS?", "answer": " Approximately 20% reduction in computational complexity was observed with the IBWBS.", "ref_chunk": "the onlinized models, we select models with similar BLEU scores because original BWBS decoding has much higher latencies compared to the proposed IBWBS. In Table 2, we observe la- tency improvements of 959, 1395, and 752 milliseconds for En- glish to German, Spanish, and French, respectively. Lang Decoding # fw. passes LAAL BLEU 4.6. Improved Blockwise Streaming Beam Search for Full- Context Models En-De BS IBWBS 729,091 583,787 1676 1879 21.4 27.6 En-Es BS IBWBS 749,272 608,664 1643 1839 25.7 31.4 En-Fr BS IBWBS 837,084 675,683 1665 1854 29.0 37.3 Table 3: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with local agreement policy using on- linized full-context models. In this section, we compare the standard beam search with the proposed IBWBS (see Section 3.3). For the local agreement policy, we present the results in Table 3. For systems with a latency of approx. 1.7 sec, we observe a quality improvement of 6.2, 5.7, and 8.3 BLEU for En-De, En-Es, and En-Fr, respec- tively. Additionally, we measure the computational complexity. To avoid hardware-specific evaluation, we report the number of forward passes through the decoder. From the middle column in Table 3, we see the IBWBS helps reduce the computational complexity by approx. 20 %. 4.4. Controllable Latency for Blockwise Encoder In Figure 2a, we show the latency control using the block size. Each point on the BWBS line corresponds to a single model. As described in Section 3.1, we apply incremental policies to facil- itate latency control. In Figure 2b, we apply the incremental pruning with hold-n and local agreement policies to the model with block 40, which allows for a wide range of latencies. 4.5. Improved Blockwise Streaming Beam Search In Figure 2c, we illustrate the benefits of the proposed improved BWBS (see Section 3.2) on En-Fr. Compared to the original BWBS in Figure 2b, we observe improvements in both quality (by more than 2 BLEU) and latency (by 500 ms). Results on all languages are in Table 2. For the blockwise model, we select systems with a latency of approx. 2300 ms and observe a quality improvement of 0.6, 0.9, and 3.6 BLEU for In Table 4, we compare the proposed IBWBS and the stan- dard beam search for the hold-n policy. Here, we do not observe any quality or latency change. However, similarly to the local agreement, we observe a computational complexity reduction of 20 to 30 % across all languages. 5. Conclusion In this paper, we take a deep dive into blockwise encoders and blockwise streaming beam search for simultaneous speech translation. We propose a modified incremental blockwise beam search, which incorporates local agreement or hold-n policies for quality-latency control. This enables incremen- tal SST and facilitates latency control without the need to re- train the model. Furthermore, we document that the proposed changes bring improvement to both quality and latency. Ad- ditionally, we show our framework can be directly applied to full-context encoders, which leads to quality and performance improvements compared to the standard beam search. 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] [3] A. Anastasopoulos et al., \u201cFINDINGS OF THE IWSLT 2022 EVALUATION CAMPAIGN,\u201d in Proceedings of the 19th Inter- national Conference on Spoken Language Translation (IWSLT 2022), 2022. [4] M. Ma et al., \u201cStacl: Simultaneous translation with implicit an- ticipation and controllable latency using prefix-to-prefix frame- work,\u201d in Proc. ACL, 2019, pp. 3025\u20133036. [5] X. Ma, J. Pino, and P. Koehn, \u201cSimulMT to SimulST: Adapting simultaneous text translation to end-to-end simultaneous speech translation,\u201d in Proc. ACL, 2020, pp. 582\u2013587. [6] D. Liu, G. Spanakis, and J. Niehues, \u201cLow-Latency Sequence- to-Sequence Speech Recognition and Translation by Partial Hy- pothesis Selection,\u201d in Proc. Interspeech, 2020, pp. 3620\u20133624. [7] P. Pol\u00b4ak et al., \u201cCUNI-KIT System for Smultaneous Speech Translation Task at IWSLT 2022,\u201d in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [8] S. Watanabe et al., \u201cHybrid ctc/attention architecture for end- to-end speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240\u20131253, 2017. [9] X. Ma et al., \u201cMonotonic multihead attention,\u201d arXiv preprint arXiv:1909.12406, 2019. [10] C.-C. Chiu and C. Raffel, \u201cMonotonic chunkwise attention,\u201d in Proc. ICLR, 2017. [11] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, \u201cStreaming trans- former asr with blockwise synchronous beam search,\u201d in Proc. SLT, 2021, pp. 22\u201329. [12] K. Deng et al., \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d arXiv preprint arXiv:2204.08920, 2022. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recognition with the transformer model,\u201d in Proc. ICASSP, 2020, pp. 6074\u20136078. [14] Y. Shi et al., \u201cEmformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6783\u20136787. [15] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d Advances in neural information processing systems, vol. 27, 2014. [16] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine trans- lation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014. [17] J. Niehues et al., \u201cDynamic transcription for low-latency speech translation,\u201d in 17th Annual Conference of the International Speech Communication Association, INTERSPEECH 2016; Hy- att Regency San FranciscoSan Francisco; United States; 8 September 2016 through 16 September 2016, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association. Ed. : N. Morgan, vol. 08-12-September-2016, 2016, pp. 2513\u20132517. [18] J. Niehues et al., \u201cLow-latency neural speech translation,\u201d in 19th Annual Conference of the International Speech Commu- nication, INTERSPEECH 2018; Hyderabad International Con- vention Centre (HICC)Hyderabad; India; 2 September 2018 through 6 September 2018. Ed.: C.C. Sekhar, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association, INTERSPEECH, vol. 2018-September, 2018, pp. 1293\u20131297. [19] X. Ma et al.,"}, {"question": " What quality improvement in terms of BLEU scores was observed for systems with a latency of approximately 1.7 seconds for English to German, Spanish, and French?", "answer": " Quality improvement of 6.2, 5.7, and 8.3 BLEU scores were observed for English to German, Spanish, and French, respectively.", "ref_chunk": "the onlinized models, we select models with similar BLEU scores because original BWBS decoding has much higher latencies compared to the proposed IBWBS. In Table 2, we observe la- tency improvements of 959, 1395, and 752 milliseconds for En- glish to German, Spanish, and French, respectively. Lang Decoding # fw. passes LAAL BLEU 4.6. Improved Blockwise Streaming Beam Search for Full- Context Models En-De BS IBWBS 729,091 583,787 1676 1879 21.4 27.6 En-Es BS IBWBS 749,272 608,664 1643 1839 25.7 31.4 En-Fr BS IBWBS 837,084 675,683 1665 1854 29.0 37.3 Table 3: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with local agreement policy using on- linized full-context models. In this section, we compare the standard beam search with the proposed IBWBS (see Section 3.3). For the local agreement policy, we present the results in Table 3. For systems with a latency of approx. 1.7 sec, we observe a quality improvement of 6.2, 5.7, and 8.3 BLEU for En-De, En-Es, and En-Fr, respec- tively. Additionally, we measure the computational complexity. To avoid hardware-specific evaluation, we report the number of forward passes through the decoder. From the middle column in Table 3, we see the IBWBS helps reduce the computational complexity by approx. 20 %. 4.4. Controllable Latency for Blockwise Encoder In Figure 2a, we show the latency control using the block size. Each point on the BWBS line corresponds to a single model. As described in Section 3.1, we apply incremental policies to facil- itate latency control. In Figure 2b, we apply the incremental pruning with hold-n and local agreement policies to the model with block 40, which allows for a wide range of latencies. 4.5. Improved Blockwise Streaming Beam Search In Figure 2c, we illustrate the benefits of the proposed improved BWBS (see Section 3.2) on En-Fr. Compared to the original BWBS in Figure 2b, we observe improvements in both quality (by more than 2 BLEU) and latency (by 500 ms). Results on all languages are in Table 2. For the blockwise model, we select systems with a latency of approx. 2300 ms and observe a quality improvement of 0.6, 0.9, and 3.6 BLEU for In Table 4, we compare the proposed IBWBS and the stan- dard beam search for the hold-n policy. Here, we do not observe any quality or latency change. However, similarly to the local agreement, we observe a computational complexity reduction of 20 to 30 % across all languages. 5. Conclusion In this paper, we take a deep dive into blockwise encoders and blockwise streaming beam search for simultaneous speech translation. We propose a modified incremental blockwise beam search, which incorporates local agreement or hold-n policies for quality-latency control. This enables incremen- tal SST and facilitates latency control without the need to re- train the model. Furthermore, we document that the proposed changes bring improvement to both quality and latency. Ad- ditionally, we show our framework can be directly applied to full-context encoders, which leads to quality and performance improvements compared to the standard beam search. 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] [3] A. Anastasopoulos et al., \u201cFINDINGS OF THE IWSLT 2022 EVALUATION CAMPAIGN,\u201d in Proceedings of the 19th Inter- national Conference on Spoken Language Translation (IWSLT 2022), 2022. [4] M. Ma et al., \u201cStacl: Simultaneous translation with implicit an- ticipation and controllable latency using prefix-to-prefix frame- work,\u201d in Proc. ACL, 2019, pp. 3025\u20133036. [5] X. Ma, J. Pino, and P. Koehn, \u201cSimulMT to SimulST: Adapting simultaneous text translation to end-to-end simultaneous speech translation,\u201d in Proc. ACL, 2020, pp. 582\u2013587. [6] D. Liu, G. Spanakis, and J. Niehues, \u201cLow-Latency Sequence- to-Sequence Speech Recognition and Translation by Partial Hy- pothesis Selection,\u201d in Proc. Interspeech, 2020, pp. 3620\u20133624. [7] P. Pol\u00b4ak et al., \u201cCUNI-KIT System for Smultaneous Speech Translation Task at IWSLT 2022,\u201d in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [8] S. Watanabe et al., \u201cHybrid ctc/attention architecture for end- to-end speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240\u20131253, 2017. [9] X. Ma et al., \u201cMonotonic multihead attention,\u201d arXiv preprint arXiv:1909.12406, 2019. [10] C.-C. Chiu and C. Raffel, \u201cMonotonic chunkwise attention,\u201d in Proc. ICLR, 2017. [11] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, \u201cStreaming trans- former asr with blockwise synchronous beam search,\u201d in Proc. SLT, 2021, pp. 22\u201329. [12] K. Deng et al., \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d arXiv preprint arXiv:2204.08920, 2022. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recognition with the transformer model,\u201d in Proc. ICASSP, 2020, pp. 6074\u20136078. [14] Y. Shi et al., \u201cEmformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6783\u20136787. [15] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d Advances in neural information processing systems, vol. 27, 2014. [16] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine trans- lation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014. [17] J. Niehues et al., \u201cDynamic transcription for low-latency speech translation,\u201d in 17th Annual Conference of the International Speech Communication Association, INTERSPEECH 2016; Hy- att Regency San FranciscoSan Francisco; United States; 8 September 2016 through 16 September 2016, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association. Ed. : N. Morgan, vol. 08-12-September-2016, 2016, pp. 2513\u20132517. [18] J. Niehues et al., \u201cLow-latency neural speech translation,\u201d in 19th Annual Conference of the International Speech Commu- nication, INTERSPEECH 2018; Hyderabad International Con- vention Centre (HICC)Hyderabad; India; 2 September 2018 through 6 September 2018. Ed.: C.C. Sekhar, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association, INTERSPEECH, vol. 2018-September, 2018, pp. 1293\u20131297. [19] X. Ma et al.,"}, {"question": " How does the IBWBS help in terms of computational complexity according to Table 3?", "answer": " The IBWBS helps reduce the computational complexity by approximately 20%.", "ref_chunk": "the onlinized models, we select models with similar BLEU scores because original BWBS decoding has much higher latencies compared to the proposed IBWBS. In Table 2, we observe la- tency improvements of 959, 1395, and 752 milliseconds for En- glish to German, Spanish, and French, respectively. Lang Decoding # fw. passes LAAL BLEU 4.6. Improved Blockwise Streaming Beam Search for Full- Context Models En-De BS IBWBS 729,091 583,787 1676 1879 21.4 27.6 En-Es BS IBWBS 749,272 608,664 1643 1839 25.7 31.4 En-Fr BS IBWBS 837,084 675,683 1665 1854 29.0 37.3 Table 3: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with local agreement policy using on- linized full-context models. In this section, we compare the standard beam search with the proposed IBWBS (see Section 3.3). For the local agreement policy, we present the results in Table 3. For systems with a latency of approx. 1.7 sec, we observe a quality improvement of 6.2, 5.7, and 8.3 BLEU for En-De, En-Es, and En-Fr, respec- tively. Additionally, we measure the computational complexity. To avoid hardware-specific evaluation, we report the number of forward passes through the decoder. From the middle column in Table 3, we see the IBWBS helps reduce the computational complexity by approx. 20 %. 4.4. Controllable Latency for Blockwise Encoder In Figure 2a, we show the latency control using the block size. Each point on the BWBS line corresponds to a single model. As described in Section 3.1, we apply incremental policies to facil- itate latency control. In Figure 2b, we apply the incremental pruning with hold-n and local agreement policies to the model with block 40, which allows for a wide range of latencies. 4.5. Improved Blockwise Streaming Beam Search In Figure 2c, we illustrate the benefits of the proposed improved BWBS (see Section 3.2) on En-Fr. Compared to the original BWBS in Figure 2b, we observe improvements in both quality (by more than 2 BLEU) and latency (by 500 ms). Results on all languages are in Table 2. For the blockwise model, we select systems with a latency of approx. 2300 ms and observe a quality improvement of 0.6, 0.9, and 3.6 BLEU for In Table 4, we compare the proposed IBWBS and the stan- dard beam search for the hold-n policy. Here, we do not observe any quality or latency change. However, similarly to the local agreement, we observe a computational complexity reduction of 20 to 30 % across all languages. 5. Conclusion In this paper, we take a deep dive into blockwise encoders and blockwise streaming beam search for simultaneous speech translation. We propose a modified incremental blockwise beam search, which incorporates local agreement or hold-n policies for quality-latency control. This enables incremen- tal SST and facilitates latency control without the need to re- train the model. Furthermore, we document that the proposed changes bring improvement to both quality and latency. Ad- ditionally, we show our framework can be directly applied to full-context encoders, which leads to quality and performance improvements compared to the standard beam search. 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] [3] A. Anastasopoulos et al., \u201cFINDINGS OF THE IWSLT 2022 EVALUATION CAMPAIGN,\u201d in Proceedings of the 19th Inter- national Conference on Spoken Language Translation (IWSLT 2022), 2022. [4] M. Ma et al., \u201cStacl: Simultaneous translation with implicit an- ticipation and controllable latency using prefix-to-prefix frame- work,\u201d in Proc. ACL, 2019, pp. 3025\u20133036. [5] X. Ma, J. Pino, and P. Koehn, \u201cSimulMT to SimulST: Adapting simultaneous text translation to end-to-end simultaneous speech translation,\u201d in Proc. ACL, 2020, pp. 582\u2013587. [6] D. Liu, G. Spanakis, and J. Niehues, \u201cLow-Latency Sequence- to-Sequence Speech Recognition and Translation by Partial Hy- pothesis Selection,\u201d in Proc. Interspeech, 2020, pp. 3620\u20133624. [7] P. Pol\u00b4ak et al., \u201cCUNI-KIT System for Smultaneous Speech Translation Task at IWSLT 2022,\u201d in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [8] S. Watanabe et al., \u201cHybrid ctc/attention architecture for end- to-end speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240\u20131253, 2017. [9] X. Ma et al., \u201cMonotonic multihead attention,\u201d arXiv preprint arXiv:1909.12406, 2019. [10] C.-C. Chiu and C. Raffel, \u201cMonotonic chunkwise attention,\u201d in Proc. ICLR, 2017. [11] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, \u201cStreaming trans- former asr with blockwise synchronous beam search,\u201d in Proc. SLT, 2021, pp. 22\u201329. [12] K. Deng et al., \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d arXiv preprint arXiv:2204.08920, 2022. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recognition with the transformer model,\u201d in Proc. ICASSP, 2020, pp. 6074\u20136078. [14] Y. Shi et al., \u201cEmformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6783\u20136787. [15] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d Advances in neural information processing systems, vol. 27, 2014. [16] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine trans- lation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014. [17] J. Niehues et al., \u201cDynamic transcription for low-latency speech translation,\u201d in 17th Annual Conference of the International Speech Communication Association, INTERSPEECH 2016; Hy- att Regency San FranciscoSan Francisco; United States; 8 September 2016 through 16 September 2016, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association. Ed. : N. Morgan, vol. 08-12-September-2016, 2016, pp. 2513\u20132517. [18] J. Niehues et al., \u201cLow-latency neural speech translation,\u201d in 19th Annual Conference of the International Speech Commu- nication, INTERSPEECH 2018; Hyderabad International Con- vention Centre (HICC)Hyderabad; India; 2 September 2018 through 6 September 2018. Ed.: C.C. Sekhar, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association, INTERSPEECH, vol. 2018-September, 2018, pp. 1293\u20131297. [19] X. Ma et al.,"}, {"question": " What is illustrated in Figure 2a regarding latency control using the block size?", "answer": " Figure 2a illustrates the latency control using the block size.", "ref_chunk": "the onlinized models, we select models with similar BLEU scores because original BWBS decoding has much higher latencies compared to the proposed IBWBS. In Table 2, we observe la- tency improvements of 959, 1395, and 752 milliseconds for En- glish to German, Spanish, and French, respectively. Lang Decoding # fw. passes LAAL BLEU 4.6. Improved Blockwise Streaming Beam Search for Full- Context Models En-De BS IBWBS 729,091 583,787 1676 1879 21.4 27.6 En-Es BS IBWBS 749,272 608,664 1643 1839 25.7 31.4 En-Fr BS IBWBS 837,084 675,683 1665 1854 29.0 37.3 Table 3: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with local agreement policy using on- linized full-context models. In this section, we compare the standard beam search with the proposed IBWBS (see Section 3.3). For the local agreement policy, we present the results in Table 3. For systems with a latency of approx. 1.7 sec, we observe a quality improvement of 6.2, 5.7, and 8.3 BLEU for En-De, En-Es, and En-Fr, respec- tively. Additionally, we measure the computational complexity. To avoid hardware-specific evaluation, we report the number of forward passes through the decoder. From the middle column in Table 3, we see the IBWBS helps reduce the computational complexity by approx. 20 %. 4.4. Controllable Latency for Blockwise Encoder In Figure 2a, we show the latency control using the block size. Each point on the BWBS line corresponds to a single model. As described in Section 3.1, we apply incremental policies to facil- itate latency control. In Figure 2b, we apply the incremental pruning with hold-n and local agreement policies to the model with block 40, which allows for a wide range of latencies. 4.5. Improved Blockwise Streaming Beam Search In Figure 2c, we illustrate the benefits of the proposed improved BWBS (see Section 3.2) on En-Fr. Compared to the original BWBS in Figure 2b, we observe improvements in both quality (by more than 2 BLEU) and latency (by 500 ms). Results on all languages are in Table 2. For the blockwise model, we select systems with a latency of approx. 2300 ms and observe a quality improvement of 0.6, 0.9, and 3.6 BLEU for In Table 4, we compare the proposed IBWBS and the stan- dard beam search for the hold-n policy. Here, we do not observe any quality or latency change. However, similarly to the local agreement, we observe a computational complexity reduction of 20 to 30 % across all languages. 5. Conclusion In this paper, we take a deep dive into blockwise encoders and blockwise streaming beam search for simultaneous speech translation. We propose a modified incremental blockwise beam search, which incorporates local agreement or hold-n policies for quality-latency control. This enables incremen- tal SST and facilitates latency control without the need to re- train the model. Furthermore, we document that the proposed changes bring improvement to both quality and latency. Ad- ditionally, we show our framework can be directly applied to full-context encoders, which leads to quality and performance improvements compared to the standard beam search. 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] [3] A. Anastasopoulos et al., \u201cFINDINGS OF THE IWSLT 2022 EVALUATION CAMPAIGN,\u201d in Proceedings of the 19th Inter- national Conference on Spoken Language Translation (IWSLT 2022), 2022. [4] M. Ma et al., \u201cStacl: Simultaneous translation with implicit an- ticipation and controllable latency using prefix-to-prefix frame- work,\u201d in Proc. ACL, 2019, pp. 3025\u20133036. [5] X. Ma, J. Pino, and P. Koehn, \u201cSimulMT to SimulST: Adapting simultaneous text translation to end-to-end simultaneous speech translation,\u201d in Proc. ACL, 2020, pp. 582\u2013587. [6] D. Liu, G. Spanakis, and J. Niehues, \u201cLow-Latency Sequence- to-Sequence Speech Recognition and Translation by Partial Hy- pothesis Selection,\u201d in Proc. Interspeech, 2020, pp. 3620\u20133624. [7] P. Pol\u00b4ak et al., \u201cCUNI-KIT System for Smultaneous Speech Translation Task at IWSLT 2022,\u201d in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [8] S. Watanabe et al., \u201cHybrid ctc/attention architecture for end- to-end speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240\u20131253, 2017. [9] X. Ma et al., \u201cMonotonic multihead attention,\u201d arXiv preprint arXiv:1909.12406, 2019. [10] C.-C. Chiu and C. Raffel, \u201cMonotonic chunkwise attention,\u201d in Proc. ICLR, 2017. [11] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, \u201cStreaming trans- former asr with blockwise synchronous beam search,\u201d in Proc. SLT, 2021, pp. 22\u201329. [12] K. Deng et al., \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d arXiv preprint arXiv:2204.08920, 2022. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recognition with the transformer model,\u201d in Proc. ICASSP, 2020, pp. 6074\u20136078. [14] Y. Shi et al., \u201cEmformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6783\u20136787. [15] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d Advances in neural information processing systems, vol. 27, 2014. [16] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine trans- lation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014. [17] J. Niehues et al., \u201cDynamic transcription for low-latency speech translation,\u201d in 17th Annual Conference of the International Speech Communication Association, INTERSPEECH 2016; Hy- att Regency San FranciscoSan Francisco; United States; 8 September 2016 through 16 September 2016, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association. Ed. : N. Morgan, vol. 08-12-September-2016, 2016, pp. 2513\u20132517. [18] J. Niehues et al., \u201cLow-latency neural speech translation,\u201d in 19th Annual Conference of the International Speech Commu- nication, INTERSPEECH 2018; Hyderabad International Con- vention Centre (HICC)Hyderabad; India; 2 September 2018 through 6 September 2018. Ed.: C.C. Sekhar, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association, INTERSPEECH, vol. 2018-September, 2018, pp. 1293\u20131297. [19] X. Ma et al.,"}, {"question": " What benefits were observed for the proposed improved BWBS on English to French compared to the original BWBS?", "answer": " Improvements in both quality and latency (by more than 2 BLEU and by 500 ms, respectively) were observed for English to French.", "ref_chunk": "the onlinized models, we select models with similar BLEU scores because original BWBS decoding has much higher latencies compared to the proposed IBWBS. In Table 2, we observe la- tency improvements of 959, 1395, and 752 milliseconds for En- glish to German, Spanish, and French, respectively. Lang Decoding # fw. passes LAAL BLEU 4.6. Improved Blockwise Streaming Beam Search for Full- Context Models En-De BS IBWBS 729,091 583,787 1676 1879 21.4 27.6 En-Es BS IBWBS 749,272 608,664 1643 1839 25.7 31.4 En-Fr BS IBWBS 837,084 675,683 1665 1854 29.0 37.3 Table 3: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with local agreement policy using on- linized full-context models. In this section, we compare the standard beam search with the proposed IBWBS (see Section 3.3). For the local agreement policy, we present the results in Table 3. For systems with a latency of approx. 1.7 sec, we observe a quality improvement of 6.2, 5.7, and 8.3 BLEU for En-De, En-Es, and En-Fr, respec- tively. Additionally, we measure the computational complexity. To avoid hardware-specific evaluation, we report the number of forward passes through the decoder. From the middle column in Table 3, we see the IBWBS helps reduce the computational complexity by approx. 20 %. 4.4. Controllable Latency for Blockwise Encoder In Figure 2a, we show the latency control using the block size. Each point on the BWBS line corresponds to a single model. As described in Section 3.1, we apply incremental policies to facil- itate latency control. In Figure 2b, we apply the incremental pruning with hold-n and local agreement policies to the model with block 40, which allows for a wide range of latencies. 4.5. Improved Blockwise Streaming Beam Search In Figure 2c, we illustrate the benefits of the proposed improved BWBS (see Section 3.2) on En-Fr. Compared to the original BWBS in Figure 2b, we observe improvements in both quality (by more than 2 BLEU) and latency (by 500 ms). Results on all languages are in Table 2. For the blockwise model, we select systems with a latency of approx. 2300 ms and observe a quality improvement of 0.6, 0.9, and 3.6 BLEU for In Table 4, we compare the proposed IBWBS and the stan- dard beam search for the hold-n policy. Here, we do not observe any quality or latency change. However, similarly to the local agreement, we observe a computational complexity reduction of 20 to 30 % across all languages. 5. Conclusion In this paper, we take a deep dive into blockwise encoders and blockwise streaming beam search for simultaneous speech translation. We propose a modified incremental blockwise beam search, which incorporates local agreement or hold-n policies for quality-latency control. This enables incremen- tal SST and facilitates latency control without the need to re- train the model. Furthermore, we document that the proposed changes bring improvement to both quality and latency. Ad- ditionally, we show our framework can be directly applied to full-context encoders, which leads to quality and performance improvements compared to the standard beam search. 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] [3] A. Anastasopoulos et al., \u201cFINDINGS OF THE IWSLT 2022 EVALUATION CAMPAIGN,\u201d in Proceedings of the 19th Inter- national Conference on Spoken Language Translation (IWSLT 2022), 2022. [4] M. Ma et al., \u201cStacl: Simultaneous translation with implicit an- ticipation and controllable latency using prefix-to-prefix frame- work,\u201d in Proc. ACL, 2019, pp. 3025\u20133036. [5] X. Ma, J. Pino, and P. Koehn, \u201cSimulMT to SimulST: Adapting simultaneous text translation to end-to-end simultaneous speech translation,\u201d in Proc. ACL, 2020, pp. 582\u2013587. [6] D. Liu, G. Spanakis, and J. Niehues, \u201cLow-Latency Sequence- to-Sequence Speech Recognition and Translation by Partial Hy- pothesis Selection,\u201d in Proc. Interspeech, 2020, pp. 3620\u20133624. [7] P. Pol\u00b4ak et al., \u201cCUNI-KIT System for Smultaneous Speech Translation Task at IWSLT 2022,\u201d in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [8] S. Watanabe et al., \u201cHybrid ctc/attention architecture for end- to-end speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240\u20131253, 2017. [9] X. Ma et al., \u201cMonotonic multihead attention,\u201d arXiv preprint arXiv:1909.12406, 2019. [10] C.-C. Chiu and C. Raffel, \u201cMonotonic chunkwise attention,\u201d in Proc. ICLR, 2017. [11] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, \u201cStreaming trans- former asr with blockwise synchronous beam search,\u201d in Proc. SLT, 2021, pp. 22\u201329. [12] K. Deng et al., \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d arXiv preprint arXiv:2204.08920, 2022. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recognition with the transformer model,\u201d in Proc. ICASSP, 2020, pp. 6074\u20136078. [14] Y. Shi et al., \u201cEmformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6783\u20136787. [15] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d Advances in neural information processing systems, vol. 27, 2014. [16] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine trans- lation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014. [17] J. Niehues et al., \u201cDynamic transcription for low-latency speech translation,\u201d in 17th Annual Conference of the International Speech Communication Association, INTERSPEECH 2016; Hy- att Regency San FranciscoSan Francisco; United States; 8 September 2016 through 16 September 2016, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association. Ed. : N. Morgan, vol. 08-12-September-2016, 2016, pp. 2513\u20132517. [18] J. Niehues et al., \u201cLow-latency neural speech translation,\u201d in 19th Annual Conference of the International Speech Commu- nication, INTERSPEECH 2018; Hyderabad International Con- vention Centre (HICC)Hyderabad; India; 2 September 2018 through 6 September 2018. Ed.: C.C. Sekhar, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association, INTERSPEECH, vol. 2018-September, 2018, pp. 1293\u20131297. [19] X. Ma et al.,"}, {"question": " In Table 4, when comparing the proposed IBWBS and standard beam search for the hold-n policy, what reductions were observed?", "answer": " Approximately 20 to 30% reduction in computational complexity was observed for both local agreement and hold-n policies.", "ref_chunk": "the onlinized models, we select models with similar BLEU scores because original BWBS decoding has much higher latencies compared to the proposed IBWBS. In Table 2, we observe la- tency improvements of 959, 1395, and 752 milliseconds for En- glish to German, Spanish, and French, respectively. Lang Decoding # fw. passes LAAL BLEU 4.6. Improved Blockwise Streaming Beam Search for Full- Context Models En-De BS IBWBS 729,091 583,787 1676 1879 21.4 27.6 En-Es BS IBWBS 749,272 608,664 1643 1839 25.7 31.4 En-Fr BS IBWBS 837,084 675,683 1665 1854 29.0 37.3 Table 3: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with local agreement policy using on- linized full-context models. In this section, we compare the standard beam search with the proposed IBWBS (see Section 3.3). For the local agreement policy, we present the results in Table 3. For systems with a latency of approx. 1.7 sec, we observe a quality improvement of 6.2, 5.7, and 8.3 BLEU for En-De, En-Es, and En-Fr, respec- tively. Additionally, we measure the computational complexity. To avoid hardware-specific evaluation, we report the number of forward passes through the decoder. From the middle column in Table 3, we see the IBWBS helps reduce the computational complexity by approx. 20 %. 4.4. Controllable Latency for Blockwise Encoder In Figure 2a, we show the latency control using the block size. Each point on the BWBS line corresponds to a single model. As described in Section 3.1, we apply incremental policies to facil- itate latency control. In Figure 2b, we apply the incremental pruning with hold-n and local agreement policies to the model with block 40, which allows for a wide range of latencies. 4.5. Improved Blockwise Streaming Beam Search In Figure 2c, we illustrate the benefits of the proposed improved BWBS (see Section 3.2) on En-Fr. Compared to the original BWBS in Figure 2b, we observe improvements in both quality (by more than 2 BLEU) and latency (by 500 ms). Results on all languages are in Table 2. For the blockwise model, we select systems with a latency of approx. 2300 ms and observe a quality improvement of 0.6, 0.9, and 3.6 BLEU for In Table 4, we compare the proposed IBWBS and the stan- dard beam search for the hold-n policy. Here, we do not observe any quality or latency change. However, similarly to the local agreement, we observe a computational complexity reduction of 20 to 30 % across all languages. 5. Conclusion In this paper, we take a deep dive into blockwise encoders and blockwise streaming beam search for simultaneous speech translation. We propose a modified incremental blockwise beam search, which incorporates local agreement or hold-n policies for quality-latency control. This enables incremen- tal SST and facilitates latency control without the need to re- train the model. Furthermore, we document that the proposed changes bring improvement to both quality and latency. Ad- ditionally, we show our framework can be directly applied to full-context encoders, which leads to quality and performance improvements compared to the standard beam search. 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] [3] A. Anastasopoulos et al., \u201cFINDINGS OF THE IWSLT 2022 EVALUATION CAMPAIGN,\u201d in Proceedings of the 19th Inter- national Conference on Spoken Language Translation (IWSLT 2022), 2022. [4] M. Ma et al., \u201cStacl: Simultaneous translation with implicit an- ticipation and controllable latency using prefix-to-prefix frame- work,\u201d in Proc. ACL, 2019, pp. 3025\u20133036. [5] X. Ma, J. Pino, and P. Koehn, \u201cSimulMT to SimulST: Adapting simultaneous text translation to end-to-end simultaneous speech translation,\u201d in Proc. ACL, 2020, pp. 582\u2013587. [6] D. Liu, G. Spanakis, and J. Niehues, \u201cLow-Latency Sequence- to-Sequence Speech Recognition and Translation by Partial Hy- pothesis Selection,\u201d in Proc. Interspeech, 2020, pp. 3620\u20133624. [7] P. Pol\u00b4ak et al., \u201cCUNI-KIT System for Smultaneous Speech Translation Task at IWSLT 2022,\u201d in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [8] S. Watanabe et al., \u201cHybrid ctc/attention architecture for end- to-end speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240\u20131253, 2017. [9] X. Ma et al., \u201cMonotonic multihead attention,\u201d arXiv preprint arXiv:1909.12406, 2019. [10] C.-C. Chiu and C. Raffel, \u201cMonotonic chunkwise attention,\u201d in Proc. ICLR, 2017. [11] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, \u201cStreaming trans- former asr with blockwise synchronous beam search,\u201d in Proc. SLT, 2021, pp. 22\u201329. [12] K. Deng et al., \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d arXiv preprint arXiv:2204.08920, 2022. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recognition with the transformer model,\u201d in Proc. ICASSP, 2020, pp. 6074\u20136078. [14] Y. Shi et al., \u201cEmformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6783\u20136787. [15] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d Advances in neural information processing systems, vol. 27, 2014. [16] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine trans- lation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014. [17] J. Niehues et al., \u201cDynamic transcription for low-latency speech translation,\u201d in 17th Annual Conference of the International Speech Communication Association, INTERSPEECH 2016; Hy- att Regency San FranciscoSan Francisco; United States; 8 September 2016 through 16 September 2016, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association. Ed. : N. Morgan, vol. 08-12-September-2016, 2016, pp. 2513\u20132517. [18] J. Niehues et al., \u201cLow-latency neural speech translation,\u201d in 19th Annual Conference of the International Speech Commu- nication, INTERSPEECH 2018; Hyderabad International Con- vention Centre (HICC)Hyderabad; India; 2 September 2018 through 6 September 2018. Ed.: C.C. Sekhar, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association, INTERSPEECH, vol. 2018-September, 2018, pp. 1293\u20131297. [19] X. Ma et al.,"}, {"question": " According to the conclusion, what did the proposed changes bring improvement to?", "answer": " The proposed changes brought improvement to both quality and latency.", "ref_chunk": "the onlinized models, we select models with similar BLEU scores because original BWBS decoding has much higher latencies compared to the proposed IBWBS. In Table 2, we observe la- tency improvements of 959, 1395, and 752 milliseconds for En- glish to German, Spanish, and French, respectively. Lang Decoding # fw. passes LAAL BLEU 4.6. Improved Blockwise Streaming Beam Search for Full- Context Models En-De BS IBWBS 729,091 583,787 1676 1879 21.4 27.6 En-Es BS IBWBS 749,272 608,664 1643 1839 25.7 31.4 En-Fr BS IBWBS 837,084 675,683 1665 1854 29.0 37.3 Table 3: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with local agreement policy using on- linized full-context models. In this section, we compare the standard beam search with the proposed IBWBS (see Section 3.3). For the local agreement policy, we present the results in Table 3. For systems with a latency of approx. 1.7 sec, we observe a quality improvement of 6.2, 5.7, and 8.3 BLEU for En-De, En-Es, and En-Fr, respec- tively. Additionally, we measure the computational complexity. To avoid hardware-specific evaluation, we report the number of forward passes through the decoder. From the middle column in Table 3, we see the IBWBS helps reduce the computational complexity by approx. 20 %. 4.4. Controllable Latency for Blockwise Encoder In Figure 2a, we show the latency control using the block size. Each point on the BWBS line corresponds to a single model. As described in Section 3.1, we apply incremental policies to facil- itate latency control. In Figure 2b, we apply the incremental pruning with hold-n and local agreement policies to the model with block 40, which allows for a wide range of latencies. 4.5. Improved Blockwise Streaming Beam Search In Figure 2c, we illustrate the benefits of the proposed improved BWBS (see Section 3.2) on En-Fr. Compared to the original BWBS in Figure 2b, we observe improvements in both quality (by more than 2 BLEU) and latency (by 500 ms). Results on all languages are in Table 2. For the blockwise model, we select systems with a latency of approx. 2300 ms and observe a quality improvement of 0.6, 0.9, and 3.6 BLEU for In Table 4, we compare the proposed IBWBS and the stan- dard beam search for the hold-n policy. Here, we do not observe any quality or latency change. However, similarly to the local agreement, we observe a computational complexity reduction of 20 to 30 % across all languages. 5. Conclusion In this paper, we take a deep dive into blockwise encoders and blockwise streaming beam search for simultaneous speech translation. We propose a modified incremental blockwise beam search, which incorporates local agreement or hold-n policies for quality-latency control. This enables incremen- tal SST and facilitates latency control without the need to re- train the model. Furthermore, we document that the proposed changes bring improvement to both quality and latency. Ad- ditionally, we show our framework can be directly applied to full-context encoders, which leads to quality and performance improvements compared to the standard beam search. 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] [3] A. Anastasopoulos et al., \u201cFINDINGS OF THE IWSLT 2022 EVALUATION CAMPAIGN,\u201d in Proceedings of the 19th Inter- national Conference on Spoken Language Translation (IWSLT 2022), 2022. [4] M. Ma et al., \u201cStacl: Simultaneous translation with implicit an- ticipation and controllable latency using prefix-to-prefix frame- work,\u201d in Proc. ACL, 2019, pp. 3025\u20133036. [5] X. Ma, J. Pino, and P. Koehn, \u201cSimulMT to SimulST: Adapting simultaneous text translation to end-to-end simultaneous speech translation,\u201d in Proc. ACL, 2020, pp. 582\u2013587. [6] D. Liu, G. Spanakis, and J. Niehues, \u201cLow-Latency Sequence- to-Sequence Speech Recognition and Translation by Partial Hy- pothesis Selection,\u201d in Proc. Interspeech, 2020, pp. 3620\u20133624. [7] P. Pol\u00b4ak et al., \u201cCUNI-KIT System for Smultaneous Speech Translation Task at IWSLT 2022,\u201d in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [8] S. Watanabe et al., \u201cHybrid ctc/attention architecture for end- to-end speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240\u20131253, 2017. [9] X. Ma et al., \u201cMonotonic multihead attention,\u201d arXiv preprint arXiv:1909.12406, 2019. [10] C.-C. Chiu and C. Raffel, \u201cMonotonic chunkwise attention,\u201d in Proc. ICLR, 2017. [11] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, \u201cStreaming trans- former asr with blockwise synchronous beam search,\u201d in Proc. SLT, 2021, pp. 22\u201329. [12] K. Deng et al., \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d arXiv preprint arXiv:2204.08920, 2022. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recognition with the transformer model,\u201d in Proc. ICASSP, 2020, pp. 6074\u20136078. [14] Y. Shi et al., \u201cEmformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6783\u20136787. [15] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d Advances in neural information processing systems, vol. 27, 2014. [16] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine trans- lation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014. [17] J. Niehues et al., \u201cDynamic transcription for low-latency speech translation,\u201d in 17th Annual Conference of the International Speech Communication Association, INTERSPEECH 2016; Hy- att Regency San FranciscoSan Francisco; United States; 8 September 2016 through 16 September 2016, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association. Ed. : N. Morgan, vol. 08-12-September-2016, 2016, pp. 2513\u20132517. [18] J. Niehues et al., \u201cLow-latency neural speech translation,\u201d in 19th Annual Conference of the International Speech Commu- nication, INTERSPEECH 2018; Hyderabad International Con- vention Centre (HICC)Hyderabad; India; 2 September 2018 through 6 September 2018. Ed.: C.C. Sekhar, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association, INTERSPEECH, vol. 2018-September, 2018, pp. 1293\u20131297. [19] X. Ma et al.,"}, {"question": " What is unique about the proposed incremental blockwise beam search in terms of quality-latency control?", "answer": " It incorporates local agreement or hold-n policies for quality-latency control without the need to retrain the model.", "ref_chunk": "the onlinized models, we select models with similar BLEU scores because original BWBS decoding has much higher latencies compared to the proposed IBWBS. In Table 2, we observe la- tency improvements of 959, 1395, and 752 milliseconds for En- glish to German, Spanish, and French, respectively. Lang Decoding # fw. passes LAAL BLEU 4.6. Improved Blockwise Streaming Beam Search for Full- Context Models En-De BS IBWBS 729,091 583,787 1676 1879 21.4 27.6 En-Es BS IBWBS 749,272 608,664 1643 1839 25.7 31.4 En-Fr BS IBWBS 837,084 675,683 1665 1854 29.0 37.3 Table 3: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with local agreement policy using on- linized full-context models. In this section, we compare the standard beam search with the proposed IBWBS (see Section 3.3). For the local agreement policy, we present the results in Table 3. For systems with a latency of approx. 1.7 sec, we observe a quality improvement of 6.2, 5.7, and 8.3 BLEU for En-De, En-Es, and En-Fr, respec- tively. Additionally, we measure the computational complexity. To avoid hardware-specific evaluation, we report the number of forward passes through the decoder. From the middle column in Table 3, we see the IBWBS helps reduce the computational complexity by approx. 20 %. 4.4. Controllable Latency for Blockwise Encoder In Figure 2a, we show the latency control using the block size. Each point on the BWBS line corresponds to a single model. As described in Section 3.1, we apply incremental policies to facil- itate latency control. In Figure 2b, we apply the incremental pruning with hold-n and local agreement policies to the model with block 40, which allows for a wide range of latencies. 4.5. Improved Blockwise Streaming Beam Search In Figure 2c, we illustrate the benefits of the proposed improved BWBS (see Section 3.2) on En-Fr. Compared to the original BWBS in Figure 2b, we observe improvements in both quality (by more than 2 BLEU) and latency (by 500 ms). Results on all languages are in Table 2. For the blockwise model, we select systems with a latency of approx. 2300 ms and observe a quality improvement of 0.6, 0.9, and 3.6 BLEU for In Table 4, we compare the proposed IBWBS and the stan- dard beam search for the hold-n policy. Here, we do not observe any quality or latency change. However, similarly to the local agreement, we observe a computational complexity reduction of 20 to 30 % across all languages. 5. Conclusion In this paper, we take a deep dive into blockwise encoders and blockwise streaming beam search for simultaneous speech translation. We propose a modified incremental blockwise beam search, which incorporates local agreement or hold-n policies for quality-latency control. This enables incremen- tal SST and facilitates latency control without the need to re- train the model. Furthermore, we document that the proposed changes bring improvement to both quality and latency. Ad- ditionally, we show our framework can be directly applied to full-context encoders, which leads to quality and performance improvements compared to the standard beam search. 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] [3] A. Anastasopoulos et al., \u201cFINDINGS OF THE IWSLT 2022 EVALUATION CAMPAIGN,\u201d in Proceedings of the 19th Inter- national Conference on Spoken Language Translation (IWSLT 2022), 2022. [4] M. Ma et al., \u201cStacl: Simultaneous translation with implicit an- ticipation and controllable latency using prefix-to-prefix frame- work,\u201d in Proc. ACL, 2019, pp. 3025\u20133036. [5] X. Ma, J. Pino, and P. Koehn, \u201cSimulMT to SimulST: Adapting simultaneous text translation to end-to-end simultaneous speech translation,\u201d in Proc. ACL, 2020, pp. 582\u2013587. [6] D. Liu, G. Spanakis, and J. Niehues, \u201cLow-Latency Sequence- to-Sequence Speech Recognition and Translation by Partial Hy- pothesis Selection,\u201d in Proc. Interspeech, 2020, pp. 3620\u20133624. [7] P. Pol\u00b4ak et al., \u201cCUNI-KIT System for Smultaneous Speech Translation Task at IWSLT 2022,\u201d in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [8] S. Watanabe et al., \u201cHybrid ctc/attention architecture for end- to-end speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240\u20131253, 2017. [9] X. Ma et al., \u201cMonotonic multihead attention,\u201d arXiv preprint arXiv:1909.12406, 2019. [10] C.-C. Chiu and C. Raffel, \u201cMonotonic chunkwise attention,\u201d in Proc. ICLR, 2017. [11] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, \u201cStreaming trans- former asr with blockwise synchronous beam search,\u201d in Proc. SLT, 2021, pp. 22\u201329. [12] K. Deng et al., \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d arXiv preprint arXiv:2204.08920, 2022. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recognition with the transformer model,\u201d in Proc. ICASSP, 2020, pp. 6074\u20136078. [14] Y. Shi et al., \u201cEmformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6783\u20136787. [15] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d Advances in neural information processing systems, vol. 27, 2014. [16] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine trans- lation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014. [17] J. Niehues et al., \u201cDynamic transcription for low-latency speech translation,\u201d in 17th Annual Conference of the International Speech Communication Association, INTERSPEECH 2016; Hy- att Regency San FranciscoSan Francisco; United States; 8 September 2016 through 16 September 2016, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association. Ed. : N. Morgan, vol. 08-12-September-2016, 2016, pp. 2513\u20132517. [18] J. Niehues et al., \u201cLow-latency neural speech translation,\u201d in 19th Annual Conference of the International Speech Commu- nication, INTERSPEECH 2018; Hyderabad International Con- vention Centre (HICC)Hyderabad; India; 2 September 2018 through 6 September 2018. Ed.: C.C. Sekhar, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association, INTERSPEECH, vol. 2018-September, 2018, pp. 1293\u20131297. [19] X. Ma et al.,"}, {"question": " What key improvement did the framework successully apply to full-context encoders?", "answer": " The framework successfully applied quality and performance improvements compared to the standard beam search.", "ref_chunk": "the onlinized models, we select models with similar BLEU scores because original BWBS decoding has much higher latencies compared to the proposed IBWBS. In Table 2, we observe la- tency improvements of 959, 1395, and 752 milliseconds for En- glish to German, Spanish, and French, respectively. Lang Decoding # fw. passes LAAL BLEU 4.6. Improved Blockwise Streaming Beam Search for Full- Context Models En-De BS IBWBS 729,091 583,787 1676 1879 21.4 27.6 En-Es BS IBWBS 749,272 608,664 1643 1839 25.7 31.4 En-Fr BS IBWBS 837,084 675,683 1665 1854 29.0 37.3 Table 3: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with local agreement policy using on- linized full-context models. In this section, we compare the standard beam search with the proposed IBWBS (see Section 3.3). For the local agreement policy, we present the results in Table 3. For systems with a latency of approx. 1.7 sec, we observe a quality improvement of 6.2, 5.7, and 8.3 BLEU for En-De, En-Es, and En-Fr, respec- tively. Additionally, we measure the computational complexity. To avoid hardware-specific evaluation, we report the number of forward passes through the decoder. From the middle column in Table 3, we see the IBWBS helps reduce the computational complexity by approx. 20 %. 4.4. Controllable Latency for Blockwise Encoder In Figure 2a, we show the latency control using the block size. Each point on the BWBS line corresponds to a single model. As described in Section 3.1, we apply incremental policies to facil- itate latency control. In Figure 2b, we apply the incremental pruning with hold-n and local agreement policies to the model with block 40, which allows for a wide range of latencies. 4.5. Improved Blockwise Streaming Beam Search In Figure 2c, we illustrate the benefits of the proposed improved BWBS (see Section 3.2) on En-Fr. Compared to the original BWBS in Figure 2b, we observe improvements in both quality (by more than 2 BLEU) and latency (by 500 ms). Results on all languages are in Table 2. For the blockwise model, we select systems with a latency of approx. 2300 ms and observe a quality improvement of 0.6, 0.9, and 3.6 BLEU for In Table 4, we compare the proposed IBWBS and the stan- dard beam search for the hold-n policy. Here, we do not observe any quality or latency change. However, similarly to the local agreement, we observe a computational complexity reduction of 20 to 30 % across all languages. 5. Conclusion In this paper, we take a deep dive into blockwise encoders and blockwise streaming beam search for simultaneous speech translation. We propose a modified incremental blockwise beam search, which incorporates local agreement or hold-n policies for quality-latency control. This enables incremen- tal SST and facilitates latency control without the need to re- train the model. Furthermore, we document that the proposed changes bring improvement to both quality and latency. Ad- ditionally, we show our framework can be directly applied to full-context encoders, which leads to quality and performance improvements compared to the standard beam search. 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] [3] A. Anastasopoulos et al., \u201cFINDINGS OF THE IWSLT 2022 EVALUATION CAMPAIGN,\u201d in Proceedings of the 19th Inter- national Conference on Spoken Language Translation (IWSLT 2022), 2022. [4] M. Ma et al., \u201cStacl: Simultaneous translation with implicit an- ticipation and controllable latency using prefix-to-prefix frame- work,\u201d in Proc. ACL, 2019, pp. 3025\u20133036. [5] X. Ma, J. Pino, and P. Koehn, \u201cSimulMT to SimulST: Adapting simultaneous text translation to end-to-end simultaneous speech translation,\u201d in Proc. ACL, 2020, pp. 582\u2013587. [6] D. Liu, G. Spanakis, and J. Niehues, \u201cLow-Latency Sequence- to-Sequence Speech Recognition and Translation by Partial Hy- pothesis Selection,\u201d in Proc. Interspeech, 2020, pp. 3620\u20133624. [7] P. Pol\u00b4ak et al., \u201cCUNI-KIT System for Smultaneous Speech Translation Task at IWSLT 2022,\u201d in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [8] S. Watanabe et al., \u201cHybrid ctc/attention architecture for end- to-end speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240\u20131253, 2017. [9] X. Ma et al., \u201cMonotonic multihead attention,\u201d arXiv preprint arXiv:1909.12406, 2019. [10] C.-C. Chiu and C. Raffel, \u201cMonotonic chunkwise attention,\u201d in Proc. ICLR, 2017. [11] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, \u201cStreaming trans- former asr with blockwise synchronous beam search,\u201d in Proc. SLT, 2021, pp. 22\u201329. [12] K. Deng et al., \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d arXiv preprint arXiv:2204.08920, 2022. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recognition with the transformer model,\u201d in Proc. ICASSP, 2020, pp. 6074\u20136078. [14] Y. Shi et al., \u201cEmformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6783\u20136787. [15] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d Advances in neural information processing systems, vol. 27, 2014. [16] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine trans- lation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014. [17] J. Niehues et al., \u201cDynamic transcription for low-latency speech translation,\u201d in 17th Annual Conference of the International Speech Communication Association, INTERSPEECH 2016; Hy- att Regency San FranciscoSan Francisco; United States; 8 September 2016 through 16 September 2016, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association. Ed. : N. Morgan, vol. 08-12-September-2016, 2016, pp. 2513\u20132517. [18] J. Niehues et al., \u201cLow-latency neural speech translation,\u201d in 19th Annual Conference of the International Speech Commu- nication, INTERSPEECH 2018; Hyderabad International Con- vention Centre (HICC)Hyderabad; India; 2 September 2018 through 6 September 2018. Ed.: C.C. Sekhar, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association, INTERSPEECH, vol. 2018-September, 2018, pp. 1293\u20131297. [19] X. Ma et al.,"}], "doc_text": "the onlinized models, we select models with similar BLEU scores because original BWBS decoding has much higher latencies compared to the proposed IBWBS. In Table 2, we observe la- tency improvements of 959, 1395, and 752 milliseconds for En- glish to German, Spanish, and French, respectively. Lang Decoding # fw. passes LAAL BLEU 4.6. Improved Blockwise Streaming Beam Search for Full- Context Models En-De BS IBWBS 729,091 583,787 1676 1879 21.4 27.6 En-Es BS IBWBS 749,272 608,664 1643 1839 25.7 31.4 En-Fr BS IBWBS 837,084 675,683 1665 1854 29.0 37.3 Table 3: Comparison of the standard beam search [15] (BS) and the proposed IBWBS with local agreement policy using on- linized full-context models. In this section, we compare the standard beam search with the proposed IBWBS (see Section 3.3). For the local agreement policy, we present the results in Table 3. For systems with a latency of approx. 1.7 sec, we observe a quality improvement of 6.2, 5.7, and 8.3 BLEU for En-De, En-Es, and En-Fr, respec- tively. Additionally, we measure the computational complexity. To avoid hardware-specific evaluation, we report the number of forward passes through the decoder. From the middle column in Table 3, we see the IBWBS helps reduce the computational complexity by approx. 20 %. 4.4. Controllable Latency for Blockwise Encoder In Figure 2a, we show the latency control using the block size. Each point on the BWBS line corresponds to a single model. As described in Section 3.1, we apply incremental policies to facil- itate latency control. In Figure 2b, we apply the incremental pruning with hold-n and local agreement policies to the model with block 40, which allows for a wide range of latencies. 4.5. Improved Blockwise Streaming Beam Search In Figure 2c, we illustrate the benefits of the proposed improved BWBS (see Section 3.2) on En-Fr. Compared to the original BWBS in Figure 2b, we observe improvements in both quality (by more than 2 BLEU) and latency (by 500 ms). Results on all languages are in Table 2. For the blockwise model, we select systems with a latency of approx. 2300 ms and observe a quality improvement of 0.6, 0.9, and 3.6 BLEU for In Table 4, we compare the proposed IBWBS and the stan- dard beam search for the hold-n policy. Here, we do not observe any quality or latency change. However, similarly to the local agreement, we observe a computational complexity reduction of 20 to 30 % across all languages. 5. Conclusion In this paper, we take a deep dive into blockwise encoders and blockwise streaming beam search for simultaneous speech translation. We propose a modified incremental blockwise beam search, which incorporates local agreement or hold-n policies for quality-latency control. This enables incremen- tal SST and facilitates latency control without the need to re- train the model. Furthermore, we document that the proposed changes bring improvement to both quality and latency. Ad- ditionally, we show our framework can be directly applied to full-context encoders, which leads to quality and performance improvements compared to the standard beam search. 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] 6. References [1] C. F\u00a8ugen, A. Waibel, and M. Kolss, \u201cSimultaneous translation of lectures and speeches,\u201d Machine translation, vol. 21, pp. 209\u2013 252, 2007. [20] [3] A. Anastasopoulos et al., \u201cFINDINGS OF THE IWSLT 2022 EVALUATION CAMPAIGN,\u201d in Proceedings of the 19th Inter- national Conference on Spoken Language Translation (IWSLT 2022), 2022. [4] M. Ma et al., \u201cStacl: Simultaneous translation with implicit an- ticipation and controllable latency using prefix-to-prefix frame- work,\u201d in Proc. ACL, 2019, pp. 3025\u20133036. [5] X. Ma, J. Pino, and P. Koehn, \u201cSimulMT to SimulST: Adapting simultaneous text translation to end-to-end simultaneous speech translation,\u201d in Proc. ACL, 2020, pp. 582\u2013587. [6] D. Liu, G. Spanakis, and J. Niehues, \u201cLow-Latency Sequence- to-Sequence Speech Recognition and Translation by Partial Hy- pothesis Selection,\u201d in Proc. Interspeech, 2020, pp. 3620\u20133624. [7] P. Pol\u00b4ak et al., \u201cCUNI-KIT System for Smultaneous Speech Translation Task at IWSLT 2022,\u201d in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [8] S. Watanabe et al., \u201cHybrid ctc/attention architecture for end- to-end speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240\u20131253, 2017. [9] X. Ma et al., \u201cMonotonic multihead attention,\u201d arXiv preprint arXiv:1909.12406, 2019. [10] C.-C. Chiu and C. Raffel, \u201cMonotonic chunkwise attention,\u201d in Proc. ICLR, 2017. [11] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, \u201cStreaming trans- former asr with blockwise synchronous beam search,\u201d in Proc. SLT, 2021, pp. 22\u201329. [12] K. Deng et al., \u201cBlockwise streaming transformer for spoken language understanding and simultaneous speech translation,\u201d arXiv preprint arXiv:2204.08920, 2022. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recognition with the transformer model,\u201d in Proc. ICASSP, 2020, pp. 6074\u20136078. [14] Y. Shi et al., \u201cEmformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,\u201d in Proc. ICASSP, 2021, pp. 6783\u20136787. [15] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d Advances in neural information processing systems, vol. 27, 2014. [16] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine trans- lation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014. [17] J. Niehues et al., \u201cDynamic transcription for low-latency speech translation,\u201d in 17th Annual Conference of the International Speech Communication Association, INTERSPEECH 2016; Hy- att Regency San FranciscoSan Francisco; United States; 8 September 2016 through 16 September 2016, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association. Ed. : N. Morgan, vol. 08-12-September-2016, 2016, pp. 2513\u20132517. [18] J. Niehues et al., \u201cLow-latency neural speech translation,\u201d in 19th Annual Conference of the International Speech Commu- nication, INTERSPEECH 2018; Hyderabad International Con- vention Centre (HICC)Hyderabad; India; 2 September 2018 through 6 September 2018. Ed.: C.C. Sekhar, ser. Proceedings of the Annual Conference of the International Speech Communica- tion Association, INTERSPEECH, vol. 2018-September, 2018, pp. 1293\u20131297. [19] X. Ma et al.,"}