{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_Imprecise_Label_Learning:_A_Unified_Framework_for_Learning_with_Various_Imprecise_Label_Configurations_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What method outperforms PiCO on CIFAR-10 and CIFAR-100 by an average of 2.13% and 2.72% respectively?", "answer": " The proposed method", "ref_chunk": "compared to the baseline methods. Perhaps more surprisingly, on CIFAR-10 and CIFAR-100, our method even outperforms the fully-supervised reference, indicating the potential better generalization capability using the proposed framework, sharing similar insights as (Wu et al., 2022). While PiCO adopts a contrastive learning objective, our method still surpasses PiCO by an average of 2.13% on CIFAR-10 and 2.72% on CIFAR-100. Our approach can be further enhanced by incorporating contrastive learning objectives, potentially leading to more significant performance improvements. 4.2 SEMI-SUPERVISED LEARNING Setup. For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. We present the results on CIFAR-100 and STL-10 (Krizhevsky et al., 2009) for image classification, and IMDB (Maas et al., 2011) and Amazon Review (McAuley & Leskovec, 7 (9) Preprint Table 2: Error rate of different number of labels l on CIFAR-100, STL-10, IMDB, and Amazon Review datasets for semi-supervised learning. Datasets CIFAR-100 STL-10 IMDB Amazon Review # Labels l 200 400 40 100 20 100 250 1000 AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 45.40\u00b10.96 47.61\u00b10.83 45.73\u00b11.60 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 40.16\u00b10.49 43.05\u00b10.54 42.25\u00b10.33 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 Ours 22.06\u00b11.06 16.40\u00b10.54 11.09\u00b10.71 8.10\u00b11.02 7.32\u00b10.12 7.64\u00b10.67 43.96\u00b10.32 42.32\u00b10.02 Table 3: Accuracy of synthetic noise on CIFAR-10 and CIFAR-100 and instance noise on Clothing1M and WebVision for noisy label learning. We use noise ratio of {0.2, 0.5, 0.8} for synthetic symmetric noise and 0.4 for asymmetric label noise. The instance noise ratio is unknown. Dataset CIFAR-10 CIFAR-100 Clothing1M WebVision Noise Type Sym. Asym. Sym. Asym. Ins. Ins. Noise Ratio \u03b7 0.2 0.5 0.8 0.4 0.2 0.5 0.8 0.4 CE Mixup (Zhang et al., 2017) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) SOP (Liu et al., 2022) 87.20 93.50 96.10 95.80 96.30 80.70 87.90 94.60 94.80 95.50 65.80 72.30 93.20 93.30 94.00 82.20 - 93.40 93.00 93.80 58.10 69.90 77.10 77.70 78.80 47.10 57.30 74.60 73.80 75.90 23.80 33.60 60.20 60.80 63.30 43.30 - 72.10 77.50 78.00 69.10 - 74.26 72.90 73.50 - 77.32 76.20 76.60 Ours 96.78\u00b10.11 96.6\u00b10.15 94.31\u00b10.07 94.75\u00b10.81 77.49\u00b10.28 75.51\u00b10.52 66.46\u00b10.72 75.82\u00b11.89 74.02\u00b10.12 79.37\u00b10.09 2013) for text classification. We compare with the current methods with confidence thresholding, such as FixMatch (Sohn et al., 2020), AdaMatch (Berthelot et al., 2021), FlexMatch (Zhang et al., 2021b), FreeMatch (Wang et al., 2023), and SoftMatch (Chen et al., 2023). We also compare with methods with the contrastive loss, CoMatch (Li et al., 2021a) and SimMatch (Zheng et al., 2022). A full comparison of the USB datasets and hyper-parameters are shown in Appendix D.3. Results. We present the results for SSL on Table 2. Although no individual SSL algorithm dominates the USB benchmark (Wang et al., 2022d), our method still shows competitive performance. Notably, our method performs best on STL-10 with 40 labels and Amazon Review with 250 labels, outper- forming the previous best by 0.68% and 1.33%. In the other settings, the performance of our method is also very close to the best-performing methods. More remarkably, our method does not employ any thresholding, re-weighting, or contrastive techniques to achieve current results, demonstrating a significant potential to be further explored in SSL for even better performance. 4.3 NOISY LABEL LEARNING Setup. We conduct the experiments of NLL following SOP (Liu et al., 2022) on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M (Xiao et al., 2015b), and WebVision (Li et al., 2017). To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs of classes. We mainly select three previous best methods as baselines: DivideMix (Li et al., 2020); ELR (Liu et al., 2020); and SOP (Liu et al., 2022). We also include the normal cross-entropy (CE) training and mixup (Zhang et al., 2017) as baselines. More comparisons regarding other methods (Patrini et al., 2016; Han et al., 2018) and on CIFAR-10N (Wei et al., 2021) are shown in Appendix D.4. Results. We present the noisy label learning results in Table 3. The proposed method is comparable to the previous best methods. On synthetic noise of CIFAR-10, our method demonstrates the best performance on both symmetric noise and asymmetric noise. On CIFAR-100, our method generally produces similar results comparable to SOP. One may notice that our method shows inferior perfor- mance on asymmetric noise of CIFAR-100; we argue this is mainly due to the oversimplification of the noise transition model. Our method also achieves the best results on WebVision, outperforming the previous best by 2.05%. On Clothing1M, our results are also very close to DivideMix, which trains for 80 epochs compared to 10 epochs in ours, and no doubt further training improves performance. 8 Preprint Table 4: Accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of 0.1 (0.01) and 0.3 (0.05) for CIFAR-10 (CIFAR-100), and noise ratio \u03b7 of 0.1, 0.2, and 0.3 for CIFAR-10 and CIFAR-100. Method q \u03b7=0.1 CIFAR-10, l=50000 \u03b7=0.2 \u03b7=0.3 q CIFAR-100, l=50000 \u03b7=0.2 \u03b7=0.1 \u03b7=0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.1 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 92.18 92.38 93.77 94.43 95.75 95.83\u00b10.05 0.01 71.42 71.17 72.26 75.04"}, {"question": " What type of learning objective does PiCO adopt?", "answer": " PiCO adopts a contrastive learning objective", "ref_chunk": "compared to the baseline methods. Perhaps more surprisingly, on CIFAR-10 and CIFAR-100, our method even outperforms the fully-supervised reference, indicating the potential better generalization capability using the proposed framework, sharing similar insights as (Wu et al., 2022). While PiCO adopts a contrastive learning objective, our method still surpasses PiCO by an average of 2.13% on CIFAR-10 and 2.72% on CIFAR-100. Our approach can be further enhanced by incorporating contrastive learning objectives, potentially leading to more significant performance improvements. 4.2 SEMI-SUPERVISED LEARNING Setup. For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. We present the results on CIFAR-100 and STL-10 (Krizhevsky et al., 2009) for image classification, and IMDB (Maas et al., 2011) and Amazon Review (McAuley & Leskovec, 7 (9) Preprint Table 2: Error rate of different number of labels l on CIFAR-100, STL-10, IMDB, and Amazon Review datasets for semi-supervised learning. Datasets CIFAR-100 STL-10 IMDB Amazon Review # Labels l 200 400 40 100 20 100 250 1000 AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 45.40\u00b10.96 47.61\u00b10.83 45.73\u00b11.60 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 40.16\u00b10.49 43.05\u00b10.54 42.25\u00b10.33 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 Ours 22.06\u00b11.06 16.40\u00b10.54 11.09\u00b10.71 8.10\u00b11.02 7.32\u00b10.12 7.64\u00b10.67 43.96\u00b10.32 42.32\u00b10.02 Table 3: Accuracy of synthetic noise on CIFAR-10 and CIFAR-100 and instance noise on Clothing1M and WebVision for noisy label learning. We use noise ratio of {0.2, 0.5, 0.8} for synthetic symmetric noise and 0.4 for asymmetric label noise. The instance noise ratio is unknown. Dataset CIFAR-10 CIFAR-100 Clothing1M WebVision Noise Type Sym. Asym. Sym. Asym. Ins. Ins. Noise Ratio \u03b7 0.2 0.5 0.8 0.4 0.2 0.5 0.8 0.4 CE Mixup (Zhang et al., 2017) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) SOP (Liu et al., 2022) 87.20 93.50 96.10 95.80 96.30 80.70 87.90 94.60 94.80 95.50 65.80 72.30 93.20 93.30 94.00 82.20 - 93.40 93.00 93.80 58.10 69.90 77.10 77.70 78.80 47.10 57.30 74.60 73.80 75.90 23.80 33.60 60.20 60.80 63.30 43.30 - 72.10 77.50 78.00 69.10 - 74.26 72.90 73.50 - 77.32 76.20 76.60 Ours 96.78\u00b10.11 96.6\u00b10.15 94.31\u00b10.07 94.75\u00b10.81 77.49\u00b10.28 75.51\u00b10.52 66.46\u00b10.72 75.82\u00b11.89 74.02\u00b10.12 79.37\u00b10.09 2013) for text classification. We compare with the current methods with confidence thresholding, such as FixMatch (Sohn et al., 2020), AdaMatch (Berthelot et al., 2021), FlexMatch (Zhang et al., 2021b), FreeMatch (Wang et al., 2023), and SoftMatch (Chen et al., 2023). We also compare with methods with the contrastive loss, CoMatch (Li et al., 2021a) and SimMatch (Zheng et al., 2022). A full comparison of the USB datasets and hyper-parameters are shown in Appendix D.3. Results. We present the results for SSL on Table 2. Although no individual SSL algorithm dominates the USB benchmark (Wang et al., 2022d), our method still shows competitive performance. Notably, our method performs best on STL-10 with 40 labels and Amazon Review with 250 labels, outper- forming the previous best by 0.68% and 1.33%. In the other settings, the performance of our method is also very close to the best-performing methods. More remarkably, our method does not employ any thresholding, re-weighting, or contrastive techniques to achieve current results, demonstrating a significant potential to be further explored in SSL for even better performance. 4.3 NOISY LABEL LEARNING Setup. We conduct the experiments of NLL following SOP (Liu et al., 2022) on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M (Xiao et al., 2015b), and WebVision (Li et al., 2017). To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs of classes. We mainly select three previous best methods as baselines: DivideMix (Li et al., 2020); ELR (Liu et al., 2020); and SOP (Liu et al., 2022). We also include the normal cross-entropy (CE) training and mixup (Zhang et al., 2017) as baselines. More comparisons regarding other methods (Patrini et al., 2016; Han et al., 2018) and on CIFAR-10N (Wei et al., 2021) are shown in Appendix D.4. Results. We present the noisy label learning results in Table 3. The proposed method is comparable to the previous best methods. On synthetic noise of CIFAR-10, our method demonstrates the best performance on both symmetric noise and asymmetric noise. On CIFAR-100, our method generally produces similar results comparable to SOP. One may notice that our method shows inferior perfor- mance on asymmetric noise of CIFAR-100; we argue this is mainly due to the oversimplification of the noise transition model. Our method also achieves the best results on WebVision, outperforming the previous best by 2.05%. On Clothing1M, our results are also very close to DivideMix, which trains for 80 epochs compared to 10 epochs in ours, and no doubt further training improves performance. 8 Preprint Table 4: Accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of 0.1 (0.01) and 0.3 (0.05) for CIFAR-10 (CIFAR-100), and noise ratio \u03b7 of 0.1, 0.2, and 0.3 for CIFAR-10 and CIFAR-100. Method q \u03b7=0.1 CIFAR-10, l=50000 \u03b7=0.2 \u03b7=0.3 q CIFAR-100, l=50000 \u03b7=0.2 \u03b7=0.1 \u03b7=0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.1 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 92.18 92.38 93.77 94.43 95.75 95.83\u00b10.05 0.01 71.42 71.17 72.26 75.04"}, {"question": " How is the labeled dataset for semi-supervised learning constructed?", "answer": " By uniformly selecting l/C samples from each class and treating the remaining samples as unlabeled", "ref_chunk": "compared to the baseline methods. Perhaps more surprisingly, on CIFAR-10 and CIFAR-100, our method even outperforms the fully-supervised reference, indicating the potential better generalization capability using the proposed framework, sharing similar insights as (Wu et al., 2022). While PiCO adopts a contrastive learning objective, our method still surpasses PiCO by an average of 2.13% on CIFAR-10 and 2.72% on CIFAR-100. Our approach can be further enhanced by incorporating contrastive learning objectives, potentially leading to more significant performance improvements. 4.2 SEMI-SUPERVISED LEARNING Setup. For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. We present the results on CIFAR-100 and STL-10 (Krizhevsky et al., 2009) for image classification, and IMDB (Maas et al., 2011) and Amazon Review (McAuley & Leskovec, 7 (9) Preprint Table 2: Error rate of different number of labels l on CIFAR-100, STL-10, IMDB, and Amazon Review datasets for semi-supervised learning. Datasets CIFAR-100 STL-10 IMDB Amazon Review # Labels l 200 400 40 100 20 100 250 1000 AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 45.40\u00b10.96 47.61\u00b10.83 45.73\u00b11.60 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 40.16\u00b10.49 43.05\u00b10.54 42.25\u00b10.33 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 Ours 22.06\u00b11.06 16.40\u00b10.54 11.09\u00b10.71 8.10\u00b11.02 7.32\u00b10.12 7.64\u00b10.67 43.96\u00b10.32 42.32\u00b10.02 Table 3: Accuracy of synthetic noise on CIFAR-10 and CIFAR-100 and instance noise on Clothing1M and WebVision for noisy label learning. We use noise ratio of {0.2, 0.5, 0.8} for synthetic symmetric noise and 0.4 for asymmetric label noise. The instance noise ratio is unknown. Dataset CIFAR-10 CIFAR-100 Clothing1M WebVision Noise Type Sym. Asym. Sym. Asym. Ins. Ins. Noise Ratio \u03b7 0.2 0.5 0.8 0.4 0.2 0.5 0.8 0.4 CE Mixup (Zhang et al., 2017) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) SOP (Liu et al., 2022) 87.20 93.50 96.10 95.80 96.30 80.70 87.90 94.60 94.80 95.50 65.80 72.30 93.20 93.30 94.00 82.20 - 93.40 93.00 93.80 58.10 69.90 77.10 77.70 78.80 47.10 57.30 74.60 73.80 75.90 23.80 33.60 60.20 60.80 63.30 43.30 - 72.10 77.50 78.00 69.10 - 74.26 72.90 73.50 - 77.32 76.20 76.60 Ours 96.78\u00b10.11 96.6\u00b10.15 94.31\u00b10.07 94.75\u00b10.81 77.49\u00b10.28 75.51\u00b10.52 66.46\u00b10.72 75.82\u00b11.89 74.02\u00b10.12 79.37\u00b10.09 2013) for text classification. We compare with the current methods with confidence thresholding, such as FixMatch (Sohn et al., 2020), AdaMatch (Berthelot et al., 2021), FlexMatch (Zhang et al., 2021b), FreeMatch (Wang et al., 2023), and SoftMatch (Chen et al., 2023). We also compare with methods with the contrastive loss, CoMatch (Li et al., 2021a) and SimMatch (Zheng et al., 2022). A full comparison of the USB datasets and hyper-parameters are shown in Appendix D.3. Results. We present the results for SSL on Table 2. Although no individual SSL algorithm dominates the USB benchmark (Wang et al., 2022d), our method still shows competitive performance. Notably, our method performs best on STL-10 with 40 labels and Amazon Review with 250 labels, outper- forming the previous best by 0.68% and 1.33%. In the other settings, the performance of our method is also very close to the best-performing methods. More remarkably, our method does not employ any thresholding, re-weighting, or contrastive techniques to achieve current results, demonstrating a significant potential to be further explored in SSL for even better performance. 4.3 NOISY LABEL LEARNING Setup. We conduct the experiments of NLL following SOP (Liu et al., 2022) on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M (Xiao et al., 2015b), and WebVision (Li et al., 2017). To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs of classes. We mainly select three previous best methods as baselines: DivideMix (Li et al., 2020); ELR (Liu et al., 2020); and SOP (Liu et al., 2022). We also include the normal cross-entropy (CE) training and mixup (Zhang et al., 2017) as baselines. More comparisons regarding other methods (Patrini et al., 2016; Han et al., 2018) and on CIFAR-10N (Wei et al., 2021) are shown in Appendix D.4. Results. We present the noisy label learning results in Table 3. The proposed method is comparable to the previous best methods. On synthetic noise of CIFAR-10, our method demonstrates the best performance on both symmetric noise and asymmetric noise. On CIFAR-100, our method generally produces similar results comparable to SOP. One may notice that our method shows inferior perfor- mance on asymmetric noise of CIFAR-100; we argue this is mainly due to the oversimplification of the noise transition model. Our method also achieves the best results on WebVision, outperforming the previous best by 2.05%. On Clothing1M, our results are also very close to DivideMix, which trains for 80 epochs compared to 10 epochs in ours, and no doubt further training improves performance. 8 Preprint Table 4: Accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of 0.1 (0.01) and 0.3 (0.05) for CIFAR-10 (CIFAR-100), and noise ratio \u03b7 of 0.1, 0.2, and 0.3 for CIFAR-10 and CIFAR-100. Method q \u03b7=0.1 CIFAR-10, l=50000 \u03b7=0.2 \u03b7=0.3 q CIFAR-100, l=50000 \u03b7=0.2 \u03b7=0.1 \u03b7=0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.1 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 92.18 92.38 93.77 94.43 95.75 95.83\u00b10.05 0.01 71.42 71.17 72.26 75.04"}, {"question": " Which method shows competitive performance in Semi-Supervised Learning according to the text?", "answer": " The proposed method", "ref_chunk": "compared to the baseline methods. Perhaps more surprisingly, on CIFAR-10 and CIFAR-100, our method even outperforms the fully-supervised reference, indicating the potential better generalization capability using the proposed framework, sharing similar insights as (Wu et al., 2022). While PiCO adopts a contrastive learning objective, our method still surpasses PiCO by an average of 2.13% on CIFAR-10 and 2.72% on CIFAR-100. Our approach can be further enhanced by incorporating contrastive learning objectives, potentially leading to more significant performance improvements. 4.2 SEMI-SUPERVISED LEARNING Setup. For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. We present the results on CIFAR-100 and STL-10 (Krizhevsky et al., 2009) for image classification, and IMDB (Maas et al., 2011) and Amazon Review (McAuley & Leskovec, 7 (9) Preprint Table 2: Error rate of different number of labels l on CIFAR-100, STL-10, IMDB, and Amazon Review datasets for semi-supervised learning. Datasets CIFAR-100 STL-10 IMDB Amazon Review # Labels l 200 400 40 100 20 100 250 1000 AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 45.40\u00b10.96 47.61\u00b10.83 45.73\u00b11.60 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 40.16\u00b10.49 43.05\u00b10.54 42.25\u00b10.33 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 Ours 22.06\u00b11.06 16.40\u00b10.54 11.09\u00b10.71 8.10\u00b11.02 7.32\u00b10.12 7.64\u00b10.67 43.96\u00b10.32 42.32\u00b10.02 Table 3: Accuracy of synthetic noise on CIFAR-10 and CIFAR-100 and instance noise on Clothing1M and WebVision for noisy label learning. We use noise ratio of {0.2, 0.5, 0.8} for synthetic symmetric noise and 0.4 for asymmetric label noise. The instance noise ratio is unknown. Dataset CIFAR-10 CIFAR-100 Clothing1M WebVision Noise Type Sym. Asym. Sym. Asym. Ins. Ins. Noise Ratio \u03b7 0.2 0.5 0.8 0.4 0.2 0.5 0.8 0.4 CE Mixup (Zhang et al., 2017) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) SOP (Liu et al., 2022) 87.20 93.50 96.10 95.80 96.30 80.70 87.90 94.60 94.80 95.50 65.80 72.30 93.20 93.30 94.00 82.20 - 93.40 93.00 93.80 58.10 69.90 77.10 77.70 78.80 47.10 57.30 74.60 73.80 75.90 23.80 33.60 60.20 60.80 63.30 43.30 - 72.10 77.50 78.00 69.10 - 74.26 72.90 73.50 - 77.32 76.20 76.60 Ours 96.78\u00b10.11 96.6\u00b10.15 94.31\u00b10.07 94.75\u00b10.81 77.49\u00b10.28 75.51\u00b10.52 66.46\u00b10.72 75.82\u00b11.89 74.02\u00b10.12 79.37\u00b10.09 2013) for text classification. We compare with the current methods with confidence thresholding, such as FixMatch (Sohn et al., 2020), AdaMatch (Berthelot et al., 2021), FlexMatch (Zhang et al., 2021b), FreeMatch (Wang et al., 2023), and SoftMatch (Chen et al., 2023). We also compare with methods with the contrastive loss, CoMatch (Li et al., 2021a) and SimMatch (Zheng et al., 2022). A full comparison of the USB datasets and hyper-parameters are shown in Appendix D.3. Results. We present the results for SSL on Table 2. Although no individual SSL algorithm dominates the USB benchmark (Wang et al., 2022d), our method still shows competitive performance. Notably, our method performs best on STL-10 with 40 labels and Amazon Review with 250 labels, outper- forming the previous best by 0.68% and 1.33%. In the other settings, the performance of our method is also very close to the best-performing methods. More remarkably, our method does not employ any thresholding, re-weighting, or contrastive techniques to achieve current results, demonstrating a significant potential to be further explored in SSL for even better performance. 4.3 NOISY LABEL LEARNING Setup. We conduct the experiments of NLL following SOP (Liu et al., 2022) on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M (Xiao et al., 2015b), and WebVision (Li et al., 2017). To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs of classes. We mainly select three previous best methods as baselines: DivideMix (Li et al., 2020); ELR (Liu et al., 2020); and SOP (Liu et al., 2022). We also include the normal cross-entropy (CE) training and mixup (Zhang et al., 2017) as baselines. More comparisons regarding other methods (Patrini et al., 2016; Han et al., 2018) and on CIFAR-10N (Wei et al., 2021) are shown in Appendix D.4. Results. We present the noisy label learning results in Table 3. The proposed method is comparable to the previous best methods. On synthetic noise of CIFAR-10, our method demonstrates the best performance on both symmetric noise and asymmetric noise. On CIFAR-100, our method generally produces similar results comparable to SOP. One may notice that our method shows inferior perfor- mance on asymmetric noise of CIFAR-100; we argue this is mainly due to the oversimplification of the noise transition model. Our method also achieves the best results on WebVision, outperforming the previous best by 2.05%. On Clothing1M, our results are also very close to DivideMix, which trains for 80 epochs compared to 10 epochs in ours, and no doubt further training improves performance. 8 Preprint Table 4: Accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of 0.1 (0.01) and 0.3 (0.05) for CIFAR-10 (CIFAR-100), and noise ratio \u03b7 of 0.1, 0.2, and 0.3 for CIFAR-10 and CIFAR-100. Method q \u03b7=0.1 CIFAR-10, l=50000 \u03b7=0.2 \u03b7=0.3 q CIFAR-100, l=50000 \u03b7=0.2 \u03b7=0.1 \u03b7=0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.1 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 92.18 92.38 93.77 94.43 95.75 95.83\u00b10.05 0.01 71.42 71.17 72.26 75.04"}, {"question": " What is the noise ratio used for asymmetric label noise in Noisy Label Learning?", "answer": " 0.4", "ref_chunk": "compared to the baseline methods. Perhaps more surprisingly, on CIFAR-10 and CIFAR-100, our method even outperforms the fully-supervised reference, indicating the potential better generalization capability using the proposed framework, sharing similar insights as (Wu et al., 2022). While PiCO adopts a contrastive learning objective, our method still surpasses PiCO by an average of 2.13% on CIFAR-10 and 2.72% on CIFAR-100. Our approach can be further enhanced by incorporating contrastive learning objectives, potentially leading to more significant performance improvements. 4.2 SEMI-SUPERVISED LEARNING Setup. For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. We present the results on CIFAR-100 and STL-10 (Krizhevsky et al., 2009) for image classification, and IMDB (Maas et al., 2011) and Amazon Review (McAuley & Leskovec, 7 (9) Preprint Table 2: Error rate of different number of labels l on CIFAR-100, STL-10, IMDB, and Amazon Review datasets for semi-supervised learning. Datasets CIFAR-100 STL-10 IMDB Amazon Review # Labels l 200 400 40 100 20 100 250 1000 AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 45.40\u00b10.96 47.61\u00b10.83 45.73\u00b11.60 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 40.16\u00b10.49 43.05\u00b10.54 42.25\u00b10.33 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 Ours 22.06\u00b11.06 16.40\u00b10.54 11.09\u00b10.71 8.10\u00b11.02 7.32\u00b10.12 7.64\u00b10.67 43.96\u00b10.32 42.32\u00b10.02 Table 3: Accuracy of synthetic noise on CIFAR-10 and CIFAR-100 and instance noise on Clothing1M and WebVision for noisy label learning. We use noise ratio of {0.2, 0.5, 0.8} for synthetic symmetric noise and 0.4 for asymmetric label noise. The instance noise ratio is unknown. Dataset CIFAR-10 CIFAR-100 Clothing1M WebVision Noise Type Sym. Asym. Sym. Asym. Ins. Ins. Noise Ratio \u03b7 0.2 0.5 0.8 0.4 0.2 0.5 0.8 0.4 CE Mixup (Zhang et al., 2017) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) SOP (Liu et al., 2022) 87.20 93.50 96.10 95.80 96.30 80.70 87.90 94.60 94.80 95.50 65.80 72.30 93.20 93.30 94.00 82.20 - 93.40 93.00 93.80 58.10 69.90 77.10 77.70 78.80 47.10 57.30 74.60 73.80 75.90 23.80 33.60 60.20 60.80 63.30 43.30 - 72.10 77.50 78.00 69.10 - 74.26 72.90 73.50 - 77.32 76.20 76.60 Ours 96.78\u00b10.11 96.6\u00b10.15 94.31\u00b10.07 94.75\u00b10.81 77.49\u00b10.28 75.51\u00b10.52 66.46\u00b10.72 75.82\u00b11.89 74.02\u00b10.12 79.37\u00b10.09 2013) for text classification. We compare with the current methods with confidence thresholding, such as FixMatch (Sohn et al., 2020), AdaMatch (Berthelot et al., 2021), FlexMatch (Zhang et al., 2021b), FreeMatch (Wang et al., 2023), and SoftMatch (Chen et al., 2023). We also compare with methods with the contrastive loss, CoMatch (Li et al., 2021a) and SimMatch (Zheng et al., 2022). A full comparison of the USB datasets and hyper-parameters are shown in Appendix D.3. Results. We present the results for SSL on Table 2. Although no individual SSL algorithm dominates the USB benchmark (Wang et al., 2022d), our method still shows competitive performance. Notably, our method performs best on STL-10 with 40 labels and Amazon Review with 250 labels, outper- forming the previous best by 0.68% and 1.33%. In the other settings, the performance of our method is also very close to the best-performing methods. More remarkably, our method does not employ any thresholding, re-weighting, or contrastive techniques to achieve current results, demonstrating a significant potential to be further explored in SSL for even better performance. 4.3 NOISY LABEL LEARNING Setup. We conduct the experiments of NLL following SOP (Liu et al., 2022) on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M (Xiao et al., 2015b), and WebVision (Li et al., 2017). To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs of classes. We mainly select three previous best methods as baselines: DivideMix (Li et al., 2020); ELR (Liu et al., 2020); and SOP (Liu et al., 2022). We also include the normal cross-entropy (CE) training and mixup (Zhang et al., 2017) as baselines. More comparisons regarding other methods (Patrini et al., 2016; Han et al., 2018) and on CIFAR-10N (Wei et al., 2021) are shown in Appendix D.4. Results. We present the noisy label learning results in Table 3. The proposed method is comparable to the previous best methods. On synthetic noise of CIFAR-10, our method demonstrates the best performance on both symmetric noise and asymmetric noise. On CIFAR-100, our method generally produces similar results comparable to SOP. One may notice that our method shows inferior perfor- mance on asymmetric noise of CIFAR-100; we argue this is mainly due to the oversimplification of the noise transition model. Our method also achieves the best results on WebVision, outperforming the previous best by 2.05%. On Clothing1M, our results are also very close to DivideMix, which trains for 80 epochs compared to 10 epochs in ours, and no doubt further training improves performance. 8 Preprint Table 4: Accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of 0.1 (0.01) and 0.3 (0.05) for CIFAR-10 (CIFAR-100), and noise ratio \u03b7 of 0.1, 0.2, and 0.3 for CIFAR-10 and CIFAR-100. Method q \u03b7=0.1 CIFAR-10, l=50000 \u03b7=0.2 \u03b7=0.3 q CIFAR-100, l=50000 \u03b7=0.2 \u03b7=0.1 \u03b7=0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.1 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 92.18 92.38 93.77 94.43 95.75 95.83\u00b10.05 0.01 71.42 71.17 72.26 75.04"}, {"question": " What are the baselines for noisy label learning experiments in the text?", "answer": " DivideMix, ELR, and SOP", "ref_chunk": "compared to the baseline methods. Perhaps more surprisingly, on CIFAR-10 and CIFAR-100, our method even outperforms the fully-supervised reference, indicating the potential better generalization capability using the proposed framework, sharing similar insights as (Wu et al., 2022). While PiCO adopts a contrastive learning objective, our method still surpasses PiCO by an average of 2.13% on CIFAR-10 and 2.72% on CIFAR-100. Our approach can be further enhanced by incorporating contrastive learning objectives, potentially leading to more significant performance improvements. 4.2 SEMI-SUPERVISED LEARNING Setup. For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. We present the results on CIFAR-100 and STL-10 (Krizhevsky et al., 2009) for image classification, and IMDB (Maas et al., 2011) and Amazon Review (McAuley & Leskovec, 7 (9) Preprint Table 2: Error rate of different number of labels l on CIFAR-100, STL-10, IMDB, and Amazon Review datasets for semi-supervised learning. Datasets CIFAR-100 STL-10 IMDB Amazon Review # Labels l 200 400 40 100 20 100 250 1000 AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 45.40\u00b10.96 47.61\u00b10.83 45.73\u00b11.60 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 40.16\u00b10.49 43.05\u00b10.54 42.25\u00b10.33 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 Ours 22.06\u00b11.06 16.40\u00b10.54 11.09\u00b10.71 8.10\u00b11.02 7.32\u00b10.12 7.64\u00b10.67 43.96\u00b10.32 42.32\u00b10.02 Table 3: Accuracy of synthetic noise on CIFAR-10 and CIFAR-100 and instance noise on Clothing1M and WebVision for noisy label learning. We use noise ratio of {0.2, 0.5, 0.8} for synthetic symmetric noise and 0.4 for asymmetric label noise. The instance noise ratio is unknown. Dataset CIFAR-10 CIFAR-100 Clothing1M WebVision Noise Type Sym. Asym. Sym. Asym. Ins. Ins. Noise Ratio \u03b7 0.2 0.5 0.8 0.4 0.2 0.5 0.8 0.4 CE Mixup (Zhang et al., 2017) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) SOP (Liu et al., 2022) 87.20 93.50 96.10 95.80 96.30 80.70 87.90 94.60 94.80 95.50 65.80 72.30 93.20 93.30 94.00 82.20 - 93.40 93.00 93.80 58.10 69.90 77.10 77.70 78.80 47.10 57.30 74.60 73.80 75.90 23.80 33.60 60.20 60.80 63.30 43.30 - 72.10 77.50 78.00 69.10 - 74.26 72.90 73.50 - 77.32 76.20 76.60 Ours 96.78\u00b10.11 96.6\u00b10.15 94.31\u00b10.07 94.75\u00b10.81 77.49\u00b10.28 75.51\u00b10.52 66.46\u00b10.72 75.82\u00b11.89 74.02\u00b10.12 79.37\u00b10.09 2013) for text classification. We compare with the current methods with confidence thresholding, such as FixMatch (Sohn et al., 2020), AdaMatch (Berthelot et al., 2021), FlexMatch (Zhang et al., 2021b), FreeMatch (Wang et al., 2023), and SoftMatch (Chen et al., 2023). We also compare with methods with the contrastive loss, CoMatch (Li et al., 2021a) and SimMatch (Zheng et al., 2022). A full comparison of the USB datasets and hyper-parameters are shown in Appendix D.3. Results. We present the results for SSL on Table 2. Although no individual SSL algorithm dominates the USB benchmark (Wang et al., 2022d), our method still shows competitive performance. Notably, our method performs best on STL-10 with 40 labels and Amazon Review with 250 labels, outper- forming the previous best by 0.68% and 1.33%. In the other settings, the performance of our method is also very close to the best-performing methods. More remarkably, our method does not employ any thresholding, re-weighting, or contrastive techniques to achieve current results, demonstrating a significant potential to be further explored in SSL for even better performance. 4.3 NOISY LABEL LEARNING Setup. We conduct the experiments of NLL following SOP (Liu et al., 2022) on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M (Xiao et al., 2015b), and WebVision (Li et al., 2017). To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs of classes. We mainly select three previous best methods as baselines: DivideMix (Li et al., 2020); ELR (Liu et al., 2020); and SOP (Liu et al., 2022). We also include the normal cross-entropy (CE) training and mixup (Zhang et al., 2017) as baselines. More comparisons regarding other methods (Patrini et al., 2016; Han et al., 2018) and on CIFAR-10N (Wei et al., 2021) are shown in Appendix D.4. Results. We present the noisy label learning results in Table 3. The proposed method is comparable to the previous best methods. On synthetic noise of CIFAR-10, our method demonstrates the best performance on both symmetric noise and asymmetric noise. On CIFAR-100, our method generally produces similar results comparable to SOP. One may notice that our method shows inferior perfor- mance on asymmetric noise of CIFAR-100; we argue this is mainly due to the oversimplification of the noise transition model. Our method also achieves the best results on WebVision, outperforming the previous best by 2.05%. On Clothing1M, our results are also very close to DivideMix, which trains for 80 epochs compared to 10 epochs in ours, and no doubt further training improves performance. 8 Preprint Table 4: Accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of 0.1 (0.01) and 0.3 (0.05) for CIFAR-10 (CIFAR-100), and noise ratio \u03b7 of 0.1, 0.2, and 0.3 for CIFAR-10 and CIFAR-100. Method q \u03b7=0.1 CIFAR-10, l=50000 \u03b7=0.2 \u03b7=0.3 q CIFAR-100, l=50000 \u03b7=0.2 \u03b7=0.1 \u03b7=0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.1 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 92.18 92.38 93.77 94.43 95.75 95.83\u00b10.05 0.01 71.42 71.17 72.26 75.04"}, {"question": " On which dataset does the proposed method achieve the best results for noisy label learning?", "answer": " WebVision", "ref_chunk": "compared to the baseline methods. Perhaps more surprisingly, on CIFAR-10 and CIFAR-100, our method even outperforms the fully-supervised reference, indicating the potential better generalization capability using the proposed framework, sharing similar insights as (Wu et al., 2022). While PiCO adopts a contrastive learning objective, our method still surpasses PiCO by an average of 2.13% on CIFAR-10 and 2.72% on CIFAR-100. Our approach can be further enhanced by incorporating contrastive learning objectives, potentially leading to more significant performance improvements. 4.2 SEMI-SUPERVISED LEARNING Setup. For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. We present the results on CIFAR-100 and STL-10 (Krizhevsky et al., 2009) for image classification, and IMDB (Maas et al., 2011) and Amazon Review (McAuley & Leskovec, 7 (9) Preprint Table 2: Error rate of different number of labels l on CIFAR-100, STL-10, IMDB, and Amazon Review datasets for semi-supervised learning. Datasets CIFAR-100 STL-10 IMDB Amazon Review # Labels l 200 400 40 100 20 100 250 1000 AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 45.40\u00b10.96 47.61\u00b10.83 45.73\u00b11.60 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 40.16\u00b10.49 43.05\u00b10.54 42.25\u00b10.33 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 Ours 22.06\u00b11.06 16.40\u00b10.54 11.09\u00b10.71 8.10\u00b11.02 7.32\u00b10.12 7.64\u00b10.67 43.96\u00b10.32 42.32\u00b10.02 Table 3: Accuracy of synthetic noise on CIFAR-10 and CIFAR-100 and instance noise on Clothing1M and WebVision for noisy label learning. We use noise ratio of {0.2, 0.5, 0.8} for synthetic symmetric noise and 0.4 for asymmetric label noise. The instance noise ratio is unknown. Dataset CIFAR-10 CIFAR-100 Clothing1M WebVision Noise Type Sym. Asym. Sym. Asym. Ins. Ins. Noise Ratio \u03b7 0.2 0.5 0.8 0.4 0.2 0.5 0.8 0.4 CE Mixup (Zhang et al., 2017) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) SOP (Liu et al., 2022) 87.20 93.50 96.10 95.80 96.30 80.70 87.90 94.60 94.80 95.50 65.80 72.30 93.20 93.30 94.00 82.20 - 93.40 93.00 93.80 58.10 69.90 77.10 77.70 78.80 47.10 57.30 74.60 73.80 75.90 23.80 33.60 60.20 60.80 63.30 43.30 - 72.10 77.50 78.00 69.10 - 74.26 72.90 73.50 - 77.32 76.20 76.60 Ours 96.78\u00b10.11 96.6\u00b10.15 94.31\u00b10.07 94.75\u00b10.81 77.49\u00b10.28 75.51\u00b10.52 66.46\u00b10.72 75.82\u00b11.89 74.02\u00b10.12 79.37\u00b10.09 2013) for text classification. We compare with the current methods with confidence thresholding, such as FixMatch (Sohn et al., 2020), AdaMatch (Berthelot et al., 2021), FlexMatch (Zhang et al., 2021b), FreeMatch (Wang et al., 2023), and SoftMatch (Chen et al., 2023). We also compare with methods with the contrastive loss, CoMatch (Li et al., 2021a) and SimMatch (Zheng et al., 2022). A full comparison of the USB datasets and hyper-parameters are shown in Appendix D.3. Results. We present the results for SSL on Table 2. Although no individual SSL algorithm dominates the USB benchmark (Wang et al., 2022d), our method still shows competitive performance. Notably, our method performs best on STL-10 with 40 labels and Amazon Review with 250 labels, outper- forming the previous best by 0.68% and 1.33%. In the other settings, the performance of our method is also very close to the best-performing methods. More remarkably, our method does not employ any thresholding, re-weighting, or contrastive techniques to achieve current results, demonstrating a significant potential to be further explored in SSL for even better performance. 4.3 NOISY LABEL LEARNING Setup. We conduct the experiments of NLL following SOP (Liu et al., 2022) on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M (Xiao et al., 2015b), and WebVision (Li et al., 2017). To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs of classes. We mainly select three previous best methods as baselines: DivideMix (Li et al., 2020); ELR (Liu et al., 2020); and SOP (Liu et al., 2022). We also include the normal cross-entropy (CE) training and mixup (Zhang et al., 2017) as baselines. More comparisons regarding other methods (Patrini et al., 2016; Han et al., 2018) and on CIFAR-10N (Wei et al., 2021) are shown in Appendix D.4. Results. We present the noisy label learning results in Table 3. The proposed method is comparable to the previous best methods. On synthetic noise of CIFAR-10, our method demonstrates the best performance on both symmetric noise and asymmetric noise. On CIFAR-100, our method generally produces similar results comparable to SOP. One may notice that our method shows inferior perfor- mance on asymmetric noise of CIFAR-100; we argue this is mainly due to the oversimplification of the noise transition model. Our method also achieves the best results on WebVision, outperforming the previous best by 2.05%. On Clothing1M, our results are also very close to DivideMix, which trains for 80 epochs compared to 10 epochs in ours, and no doubt further training improves performance. 8 Preprint Table 4: Accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of 0.1 (0.01) and 0.3 (0.05) for CIFAR-10 (CIFAR-100), and noise ratio \u03b7 of 0.1, 0.2, and 0.3 for CIFAR-10 and CIFAR-100. Method q \u03b7=0.1 CIFAR-10, l=50000 \u03b7=0.2 \u03b7=0.3 q CIFAR-100, l=50000 \u03b7=0.2 \u03b7=0.1 \u03b7=0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.1 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 92.18 92.38 93.77 94.43 95.75 95.83\u00b10.05 0.01 71.42 71.17 72.26 75.04"}, {"question": " What does the proposed method show inferior performance on when considering CIFAR-100 asymmetric noise?", "answer": " Oversimplification of the noise transition model", "ref_chunk": "compared to the baseline methods. Perhaps more surprisingly, on CIFAR-10 and CIFAR-100, our method even outperforms the fully-supervised reference, indicating the potential better generalization capability using the proposed framework, sharing similar insights as (Wu et al., 2022). While PiCO adopts a contrastive learning objective, our method still surpasses PiCO by an average of 2.13% on CIFAR-10 and 2.72% on CIFAR-100. Our approach can be further enhanced by incorporating contrastive learning objectives, potentially leading to more significant performance improvements. 4.2 SEMI-SUPERVISED LEARNING Setup. For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. We present the results on CIFAR-100 and STL-10 (Krizhevsky et al., 2009) for image classification, and IMDB (Maas et al., 2011) and Amazon Review (McAuley & Leskovec, 7 (9) Preprint Table 2: Error rate of different number of labels l on CIFAR-100, STL-10, IMDB, and Amazon Review datasets for semi-supervised learning. Datasets CIFAR-100 STL-10 IMDB Amazon Review # Labels l 200 400 40 100 20 100 250 1000 AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 45.40\u00b10.96 47.61\u00b10.83 45.73\u00b11.60 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 40.16\u00b10.49 43.05\u00b10.54 42.25\u00b10.33 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 Ours 22.06\u00b11.06 16.40\u00b10.54 11.09\u00b10.71 8.10\u00b11.02 7.32\u00b10.12 7.64\u00b10.67 43.96\u00b10.32 42.32\u00b10.02 Table 3: Accuracy of synthetic noise on CIFAR-10 and CIFAR-100 and instance noise on Clothing1M and WebVision for noisy label learning. We use noise ratio of {0.2, 0.5, 0.8} for synthetic symmetric noise and 0.4 for asymmetric label noise. The instance noise ratio is unknown. Dataset CIFAR-10 CIFAR-100 Clothing1M WebVision Noise Type Sym. Asym. Sym. Asym. Ins. Ins. Noise Ratio \u03b7 0.2 0.5 0.8 0.4 0.2 0.5 0.8 0.4 CE Mixup (Zhang et al., 2017) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) SOP (Liu et al., 2022) 87.20 93.50 96.10 95.80 96.30 80.70 87.90 94.60 94.80 95.50 65.80 72.30 93.20 93.30 94.00 82.20 - 93.40 93.00 93.80 58.10 69.90 77.10 77.70 78.80 47.10 57.30 74.60 73.80 75.90 23.80 33.60 60.20 60.80 63.30 43.30 - 72.10 77.50 78.00 69.10 - 74.26 72.90 73.50 - 77.32 76.20 76.60 Ours 96.78\u00b10.11 96.6\u00b10.15 94.31\u00b10.07 94.75\u00b10.81 77.49\u00b10.28 75.51\u00b10.52 66.46\u00b10.72 75.82\u00b11.89 74.02\u00b10.12 79.37\u00b10.09 2013) for text classification. We compare with the current methods with confidence thresholding, such as FixMatch (Sohn et al., 2020), AdaMatch (Berthelot et al., 2021), FlexMatch (Zhang et al., 2021b), FreeMatch (Wang et al., 2023), and SoftMatch (Chen et al., 2023). We also compare with methods with the contrastive loss, CoMatch (Li et al., 2021a) and SimMatch (Zheng et al., 2022). A full comparison of the USB datasets and hyper-parameters are shown in Appendix D.3. Results. We present the results for SSL on Table 2. Although no individual SSL algorithm dominates the USB benchmark (Wang et al., 2022d), our method still shows competitive performance. Notably, our method performs best on STL-10 with 40 labels and Amazon Review with 250 labels, outper- forming the previous best by 0.68% and 1.33%. In the other settings, the performance of our method is also very close to the best-performing methods. More remarkably, our method does not employ any thresholding, re-weighting, or contrastive techniques to achieve current results, demonstrating a significant potential to be further explored in SSL for even better performance. 4.3 NOISY LABEL LEARNING Setup. We conduct the experiments of NLL following SOP (Liu et al., 2022) on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M (Xiao et al., 2015b), and WebVision (Li et al., 2017). To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs of classes. We mainly select three previous best methods as baselines: DivideMix (Li et al., 2020); ELR (Liu et al., 2020); and SOP (Liu et al., 2022). We also include the normal cross-entropy (CE) training and mixup (Zhang et al., 2017) as baselines. More comparisons regarding other methods (Patrini et al., 2016; Han et al., 2018) and on CIFAR-10N (Wei et al., 2021) are shown in Appendix D.4. Results. We present the noisy label learning results in Table 3. The proposed method is comparable to the previous best methods. On synthetic noise of CIFAR-10, our method demonstrates the best performance on both symmetric noise and asymmetric noise. On CIFAR-100, our method generally produces similar results comparable to SOP. One may notice that our method shows inferior perfor- mance on asymmetric noise of CIFAR-100; we argue this is mainly due to the oversimplification of the noise transition model. Our method also achieves the best results on WebVision, outperforming the previous best by 2.05%. On Clothing1M, our results are also very close to DivideMix, which trains for 80 epochs compared to 10 epochs in ours, and no doubt further training improves performance. 8 Preprint Table 4: Accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of 0.1 (0.01) and 0.3 (0.05) for CIFAR-10 (CIFAR-100), and noise ratio \u03b7 of 0.1, 0.2, and 0.3 for CIFAR-10 and CIFAR-100. Method q \u03b7=0.1 CIFAR-10, l=50000 \u03b7=0.2 \u03b7=0.3 q CIFAR-100, l=50000 \u03b7=0.2 \u03b7=0.1 \u03b7=0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.1 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 92.18 92.38 93.77 94.43 95.75 95.83\u00b10.05 0.01 71.42 71.17 72.26 75.04"}, {"question": " What results are reported for mixture of different imprecise lebels in the text?", "answer": " Accuracy comparison", "ref_chunk": "compared to the baseline methods. Perhaps more surprisingly, on CIFAR-10 and CIFAR-100, our method even outperforms the fully-supervised reference, indicating the potential better generalization capability using the proposed framework, sharing similar insights as (Wu et al., 2022). While PiCO adopts a contrastive learning objective, our method still surpasses PiCO by an average of 2.13% on CIFAR-10 and 2.72% on CIFAR-100. Our approach can be further enhanced by incorporating contrastive learning objectives, potentially leading to more significant performance improvements. 4.2 SEMI-SUPERVISED LEARNING Setup. For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. We present the results on CIFAR-100 and STL-10 (Krizhevsky et al., 2009) for image classification, and IMDB (Maas et al., 2011) and Amazon Review (McAuley & Leskovec, 7 (9) Preprint Table 2: Error rate of different number of labels l on CIFAR-100, STL-10, IMDB, and Amazon Review datasets for semi-supervised learning. Datasets CIFAR-100 STL-10 IMDB Amazon Review # Labels l 200 400 40 100 20 100 250 1000 AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 45.40\u00b10.96 47.61\u00b10.83 45.73\u00b11.60 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 40.16\u00b10.49 43.05\u00b10.54 42.25\u00b10.33 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 Ours 22.06\u00b11.06 16.40\u00b10.54 11.09\u00b10.71 8.10\u00b11.02 7.32\u00b10.12 7.64\u00b10.67 43.96\u00b10.32 42.32\u00b10.02 Table 3: Accuracy of synthetic noise on CIFAR-10 and CIFAR-100 and instance noise on Clothing1M and WebVision for noisy label learning. We use noise ratio of {0.2, 0.5, 0.8} for synthetic symmetric noise and 0.4 for asymmetric label noise. The instance noise ratio is unknown. Dataset CIFAR-10 CIFAR-100 Clothing1M WebVision Noise Type Sym. Asym. Sym. Asym. Ins. Ins. Noise Ratio \u03b7 0.2 0.5 0.8 0.4 0.2 0.5 0.8 0.4 CE Mixup (Zhang et al., 2017) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) SOP (Liu et al., 2022) 87.20 93.50 96.10 95.80 96.30 80.70 87.90 94.60 94.80 95.50 65.80 72.30 93.20 93.30 94.00 82.20 - 93.40 93.00 93.80 58.10 69.90 77.10 77.70 78.80 47.10 57.30 74.60 73.80 75.90 23.80 33.60 60.20 60.80 63.30 43.30 - 72.10 77.50 78.00 69.10 - 74.26 72.90 73.50 - 77.32 76.20 76.60 Ours 96.78\u00b10.11 96.6\u00b10.15 94.31\u00b10.07 94.75\u00b10.81 77.49\u00b10.28 75.51\u00b10.52 66.46\u00b10.72 75.82\u00b11.89 74.02\u00b10.12 79.37\u00b10.09 2013) for text classification. We compare with the current methods with confidence thresholding, such as FixMatch (Sohn et al., 2020), AdaMatch (Berthelot et al., 2021), FlexMatch (Zhang et al., 2021b), FreeMatch (Wang et al., 2023), and SoftMatch (Chen et al., 2023). We also compare with methods with the contrastive loss, CoMatch (Li et al., 2021a) and SimMatch (Zheng et al., 2022). A full comparison of the USB datasets and hyper-parameters are shown in Appendix D.3. Results. We present the results for SSL on Table 2. Although no individual SSL algorithm dominates the USB benchmark (Wang et al., 2022d), our method still shows competitive performance. Notably, our method performs best on STL-10 with 40 labels and Amazon Review with 250 labels, outper- forming the previous best by 0.68% and 1.33%. In the other settings, the performance of our method is also very close to the best-performing methods. More remarkably, our method does not employ any thresholding, re-weighting, or contrastive techniques to achieve current results, demonstrating a significant potential to be further explored in SSL for even better performance. 4.3 NOISY LABEL LEARNING Setup. We conduct the experiments of NLL following SOP (Liu et al., 2022) on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M (Xiao et al., 2015b), and WebVision (Li et al., 2017). To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs of classes. We mainly select three previous best methods as baselines: DivideMix (Li et al., 2020); ELR (Liu et al., 2020); and SOP (Liu et al., 2022). We also include the normal cross-entropy (CE) training and mixup (Zhang et al., 2017) as baselines. More comparisons regarding other methods (Patrini et al., 2016; Han et al., 2018) and on CIFAR-10N (Wei et al., 2021) are shown in Appendix D.4. Results. We present the noisy label learning results in Table 3. The proposed method is comparable to the previous best methods. On synthetic noise of CIFAR-10, our method demonstrates the best performance on both symmetric noise and asymmetric noise. On CIFAR-100, our method generally produces similar results comparable to SOP. One may notice that our method shows inferior perfor- mance on asymmetric noise of CIFAR-100; we argue this is mainly due to the oversimplification of the noise transition model. Our method also achieves the best results on WebVision, outperforming the previous best by 2.05%. On Clothing1M, our results are also very close to DivideMix, which trains for 80 epochs compared to 10 epochs in ours, and no doubt further training improves performance. 8 Preprint Table 4: Accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of 0.1 (0.01) and 0.3 (0.05) for CIFAR-10 (CIFAR-100), and noise ratio \u03b7 of 0.1, 0.2, and 0.3 for CIFAR-10 and CIFAR-100. Method q \u03b7=0.1 CIFAR-10, l=50000 \u03b7=0.2 \u03b7=0.3 q CIFAR-100, l=50000 \u03b7=0.2 \u03b7=0.1 \u03b7=0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.1 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 92.18 92.38 93.77 94.43 95.75 95.83\u00b10.05 0.01 71.42 71.17 72.26 75.04"}, {"question": " Which method achieves an accuracy of 96.47\u00b10.11 for CIFAR-10 with q=0.1 and \u03b7=0.3?", "answer": " Ours", "ref_chunk": "compared to the baseline methods. Perhaps more surprisingly, on CIFAR-10 and CIFAR-100, our method even outperforms the fully-supervised reference, indicating the potential better generalization capability using the proposed framework, sharing similar insights as (Wu et al., 2022). While PiCO adopts a contrastive learning objective, our method still surpasses PiCO by an average of 2.13% on CIFAR-10 and 2.72% on CIFAR-100. Our approach can be further enhanced by incorporating contrastive learning objectives, potentially leading to more significant performance improvements. 4.2 SEMI-SUPERVISED LEARNING Setup. For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. We present the results on CIFAR-100 and STL-10 (Krizhevsky et al., 2009) for image classification, and IMDB (Maas et al., 2011) and Amazon Review (McAuley & Leskovec, 7 (9) Preprint Table 2: Error rate of different number of labels l on CIFAR-100, STL-10, IMDB, and Amazon Review datasets for semi-supervised learning. Datasets CIFAR-100 STL-10 IMDB Amazon Review # Labels l 200 400 40 100 20 100 250 1000 AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 45.40\u00b10.96 47.61\u00b10.83 45.73\u00b11.60 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 40.16\u00b10.49 43.05\u00b10.54 42.25\u00b10.33 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 Ours 22.06\u00b11.06 16.40\u00b10.54 11.09\u00b10.71 8.10\u00b11.02 7.32\u00b10.12 7.64\u00b10.67 43.96\u00b10.32 42.32\u00b10.02 Table 3: Accuracy of synthetic noise on CIFAR-10 and CIFAR-100 and instance noise on Clothing1M and WebVision for noisy label learning. We use noise ratio of {0.2, 0.5, 0.8} for synthetic symmetric noise and 0.4 for asymmetric label noise. The instance noise ratio is unknown. Dataset CIFAR-10 CIFAR-100 Clothing1M WebVision Noise Type Sym. Asym. Sym. Asym. Ins. Ins. Noise Ratio \u03b7 0.2 0.5 0.8 0.4 0.2 0.5 0.8 0.4 CE Mixup (Zhang et al., 2017) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) SOP (Liu et al., 2022) 87.20 93.50 96.10 95.80 96.30 80.70 87.90 94.60 94.80 95.50 65.80 72.30 93.20 93.30 94.00 82.20 - 93.40 93.00 93.80 58.10 69.90 77.10 77.70 78.80 47.10 57.30 74.60 73.80 75.90 23.80 33.60 60.20 60.80 63.30 43.30 - 72.10 77.50 78.00 69.10 - 74.26 72.90 73.50 - 77.32 76.20 76.60 Ours 96.78\u00b10.11 96.6\u00b10.15 94.31\u00b10.07 94.75\u00b10.81 77.49\u00b10.28 75.51\u00b10.52 66.46\u00b10.72 75.82\u00b11.89 74.02\u00b10.12 79.37\u00b10.09 2013) for text classification. We compare with the current methods with confidence thresholding, such as FixMatch (Sohn et al., 2020), AdaMatch (Berthelot et al., 2021), FlexMatch (Zhang et al., 2021b), FreeMatch (Wang et al., 2023), and SoftMatch (Chen et al., 2023). We also compare with methods with the contrastive loss, CoMatch (Li et al., 2021a) and SimMatch (Zheng et al., 2022). A full comparison of the USB datasets and hyper-parameters are shown in Appendix D.3. Results. We present the results for SSL on Table 2. Although no individual SSL algorithm dominates the USB benchmark (Wang et al., 2022d), our method still shows competitive performance. Notably, our method performs best on STL-10 with 40 labels and Amazon Review with 250 labels, outper- forming the previous best by 0.68% and 1.33%. In the other settings, the performance of our method is also very close to the best-performing methods. More remarkably, our method does not employ any thresholding, re-weighting, or contrastive techniques to achieve current results, demonstrating a significant potential to be further explored in SSL for even better performance. 4.3 NOISY LABEL LEARNING Setup. We conduct the experiments of NLL following SOP (Liu et al., 2022) on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M (Xiao et al., 2015b), and WebVision (Li et al., 2017). To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs of classes. We mainly select three previous best methods as baselines: DivideMix (Li et al., 2020); ELR (Liu et al., 2020); and SOP (Liu et al., 2022). We also include the normal cross-entropy (CE) training and mixup (Zhang et al., 2017) as baselines. More comparisons regarding other methods (Patrini et al., 2016; Han et al., 2018) and on CIFAR-10N (Wei et al., 2021) are shown in Appendix D.4. Results. We present the noisy label learning results in Table 3. The proposed method is comparable to the previous best methods. On synthetic noise of CIFAR-10, our method demonstrates the best performance on both symmetric noise and asymmetric noise. On CIFAR-100, our method generally produces similar results comparable to SOP. One may notice that our method shows inferior perfor- mance on asymmetric noise of CIFAR-100; we argue this is mainly due to the oversimplification of the noise transition model. Our method also achieves the best results on WebVision, outperforming the previous best by 2.05%. On Clothing1M, our results are also very close to DivideMix, which trains for 80 epochs compared to 10 epochs in ours, and no doubt further training improves performance. 8 Preprint Table 4: Accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of 0.1 (0.01) and 0.3 (0.05) for CIFAR-10 (CIFAR-100), and noise ratio \u03b7 of 0.1, 0.2, and 0.3 for CIFAR-10 and CIFAR-100. Method q \u03b7=0.1 CIFAR-10, l=50000 \u03b7=0.2 \u03b7=0.3 q CIFAR-100, l=50000 \u03b7=0.2 \u03b7=0.1 \u03b7=0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.1 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 92.18 92.38 93.77 94.43 95.75 95.83\u00b10.05 0.01 71.42 71.17 72.26 75.04"}], "doc_text": "compared to the baseline methods. Perhaps more surprisingly, on CIFAR-10 and CIFAR-100, our method even outperforms the fully-supervised reference, indicating the potential better generalization capability using the proposed framework, sharing similar insights as (Wu et al., 2022). While PiCO adopts a contrastive learning objective, our method still surpasses PiCO by an average of 2.13% on CIFAR-10 and 2.72% on CIFAR-100. Our approach can be further enhanced by incorporating contrastive learning objectives, potentially leading to more significant performance improvements. 4.2 SEMI-SUPERVISED LEARNING Setup. For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. We present the results on CIFAR-100 and STL-10 (Krizhevsky et al., 2009) for image classification, and IMDB (Maas et al., 2011) and Amazon Review (McAuley & Leskovec, 7 (9) Preprint Table 2: Error rate of different number of labels l on CIFAR-100, STL-10, IMDB, and Amazon Review datasets for semi-supervised learning. Datasets CIFAR-100 STL-10 IMDB Amazon Review # Labels l 200 400 40 100 20 100 250 1000 AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 45.40\u00b10.96 47.61\u00b10.83 45.73\u00b11.60 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 40.16\u00b10.49 43.05\u00b10.54 42.25\u00b10.33 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 Ours 22.06\u00b11.06 16.40\u00b10.54 11.09\u00b10.71 8.10\u00b11.02 7.32\u00b10.12 7.64\u00b10.67 43.96\u00b10.32 42.32\u00b10.02 Table 3: Accuracy of synthetic noise on CIFAR-10 and CIFAR-100 and instance noise on Clothing1M and WebVision for noisy label learning. We use noise ratio of {0.2, 0.5, 0.8} for synthetic symmetric noise and 0.4 for asymmetric label noise. The instance noise ratio is unknown. Dataset CIFAR-10 CIFAR-100 Clothing1M WebVision Noise Type Sym. Asym. Sym. Asym. Ins. Ins. Noise Ratio \u03b7 0.2 0.5 0.8 0.4 0.2 0.5 0.8 0.4 CE Mixup (Zhang et al., 2017) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) SOP (Liu et al., 2022) 87.20 93.50 96.10 95.80 96.30 80.70 87.90 94.60 94.80 95.50 65.80 72.30 93.20 93.30 94.00 82.20 - 93.40 93.00 93.80 58.10 69.90 77.10 77.70 78.80 47.10 57.30 74.60 73.80 75.90 23.80 33.60 60.20 60.80 63.30 43.30 - 72.10 77.50 78.00 69.10 - 74.26 72.90 73.50 - 77.32 76.20 76.60 Ours 96.78\u00b10.11 96.6\u00b10.15 94.31\u00b10.07 94.75\u00b10.81 77.49\u00b10.28 75.51\u00b10.52 66.46\u00b10.72 75.82\u00b11.89 74.02\u00b10.12 79.37\u00b10.09 2013) for text classification. We compare with the current methods with confidence thresholding, such as FixMatch (Sohn et al., 2020), AdaMatch (Berthelot et al., 2021), FlexMatch (Zhang et al., 2021b), FreeMatch (Wang et al., 2023), and SoftMatch (Chen et al., 2023). We also compare with methods with the contrastive loss, CoMatch (Li et al., 2021a) and SimMatch (Zheng et al., 2022). A full comparison of the USB datasets and hyper-parameters are shown in Appendix D.3. Results. We present the results for SSL on Table 2. Although no individual SSL algorithm dominates the USB benchmark (Wang et al., 2022d), our method still shows competitive performance. Notably, our method performs best on STL-10 with 40 labels and Amazon Review with 250 labels, outper- forming the previous best by 0.68% and 1.33%. In the other settings, the performance of our method is also very close to the best-performing methods. More remarkably, our method does not employ any thresholding, re-weighting, or contrastive techniques to achieve current results, demonstrating a significant potential to be further explored in SSL for even better performance. 4.3 NOISY LABEL LEARNING Setup. We conduct the experiments of NLL following SOP (Liu et al., 2022) on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M (Xiao et al., 2015b), and WebVision (Li et al., 2017). To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs of classes. We mainly select three previous best methods as baselines: DivideMix (Li et al., 2020); ELR (Liu et al., 2020); and SOP (Liu et al., 2022). We also include the normal cross-entropy (CE) training and mixup (Zhang et al., 2017) as baselines. More comparisons regarding other methods (Patrini et al., 2016; Han et al., 2018) and on CIFAR-10N (Wei et al., 2021) are shown in Appendix D.4. Results. We present the noisy label learning results in Table 3. The proposed method is comparable to the previous best methods. On synthetic noise of CIFAR-10, our method demonstrates the best performance on both symmetric noise and asymmetric noise. On CIFAR-100, our method generally produces similar results comparable to SOP. One may notice that our method shows inferior perfor- mance on asymmetric noise of CIFAR-100; we argue this is mainly due to the oversimplification of the noise transition model. Our method also achieves the best results on WebVision, outperforming the previous best by 2.05%. On Clothing1M, our results are also very close to DivideMix, which trains for 80 epochs compared to 10 epochs in ours, and no doubt further training improves performance. 8 Preprint Table 4: Accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of 0.1 (0.01) and 0.3 (0.05) for CIFAR-10 (CIFAR-100), and noise ratio \u03b7 of 0.1, 0.2, and 0.3 for CIFAR-10 and CIFAR-100. Method q \u03b7=0.1 CIFAR-10, l=50000 \u03b7=0.2 \u03b7=0.3 q CIFAR-100, l=50000 \u03b7=0.2 \u03b7=0.1 \u03b7=0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.1 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 92.18 92.38 93.77 94.43 95.75 95.83\u00b10.05 0.01 71.42 71.17 72.26 75.04"}