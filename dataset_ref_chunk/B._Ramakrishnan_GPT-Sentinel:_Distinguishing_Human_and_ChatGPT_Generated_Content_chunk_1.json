{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/B._Ramakrishnan_GPT-Sentinel:_Distinguishing_Human_and_ChatGPT_Generated_Content_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the name of the dataset collected and released in the paper?", "answer": " OpenGPTText", "ref_chunk": "3 2 0 2 y a M 7 1 ] L C . s c [ 2 v 9 6 9 7 0 . 5 0 3 2 : v i X r a GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content Yutian Chen\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 yutianch@andrew.cmu.edu Hao Kang\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 haok@andrew.cmu.edu Vivian Zhai\u2020 College of Engineering Carnegie Mellon University Pittsburgh, PA 15213 yiyanz@andrew.cmu.edu Liangze Li Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 liangzel@andrew.cmu.edu Rita Singh Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 rsingh@cs.cmu.edu Bhiksha Raj Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 bhiksha@cs.cmu.edu Abstract This paper presents a novel approach for detecting ChatGPT-generated vs. human- written text using language models. To this end, we \ufb01rst collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two dif- ferent models for text classi\ufb01cation, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model\u2019s ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our \ufb01ndings provide important insights into the effective use of language models to detect generated text. 1 Introduction The development of an algorithm that can accurately distinguish between machine-generated text and human-generated text has become crucial in contexts where verifying the authenticity of information is essential, such as in legal proceedings and news reporting. Although traditional statistical techniques such as logistic regression and support vector machines (SVM) have been used for this purpose in the past [1], the emergence of Large Language Models (LLMs) like InstructGPT [2] and the availability of its free deployment, ChatGPT, has presented signi\ufb01cant challenges to existing detection methods. As a result, the need to develop novel algorithms that can accurately distinguish between machine and human-generated text has become more pressing than ever before. \u2020Three authors contribute equally to this work. To address this issue, we focused on \ufb01ne-tuning approaches to distinguish human-written and ChatGPT-generated text. We \ufb01rst collected the data from ChatGPT and established the OpenGPTText data set. Section 3 of the paper provides a detailed description of the data collection process, including the criteria to select the samples and the methods to \ufb01lter out irrelevant and undesired noise in the collected text. We then trained the frozen RoBERTa with MLP and \ufb01ne-tuned the T5 model on this data set for classi\ufb01cation. The resulting model is what we referred to as GPT-Sentinel. More details about the model can be found in Section 4 of the paper. The rest of this paper is structured as follows: We \ufb01rst discuss related work in Section 2; illustrate OpenGPTText data set in Section 3; present our model and the training details in Section 4; evaluate the performance using various metrics in Section 5; interpret the basis for the model\u2019s prediction in Section 6; point out future work in Section 7; and conclude in Section 8. 2 Related Work The work by Jawahar et al. identi\ufb01ed \ufb01ve key characteristics that a state-of-the-art detector for content generated by LLMs should possess: accuracy, data ef\ufb01ciency, generalizability, and interpretability [3]; where accuracy means the model should be able to distinguish between LLM-generated and human-written text while achieving an appropriate trade-off between precision and recall rates; data ef\ufb01ciency means that the detector should be able to operate with as few examples as possible from the language model; generalizability means that the detector should be able to work consistently, regardless of any change in the model architecture, prompt length, or training dataset; interpretability means the detector should provide clear explanations for the reasoning behind its decisions. These \ufb01ve principles is used as our guidance when designing the GPT-Sentinel. Approaches to machine-generated text detection can be divided into three categories: traditional statistical approach (by analyzing statistical abnormality in text sample), unsupervised-learning approach (by zero-shot classi\ufb01cation of LLM), and supervised-learning approach (by \ufb01ne-tuning a language model with or without a classi\ufb01cation module attached). 2.1 Statistical Methods The \ufb01rst approach to the problem is via the use of statistics. For instance, the work by Solaiman et al. [4] demonstrated that using a logistic regression model to differentiate between text generated by GPT-2 models vs. text written by humans could achieve an accuracy ranging from 88% (on 124 million parameter variants of GPT-2 model) to 74% (on 1.5 billion parameter variants of GPT-2 model). Moreover, the work by Ippolito et al. [5] showed that the top-k sampling method used in popular LLMs could over-sample high-likelihood words, and thus the generated text exhibited statistical anomalies, which could be further used for detection. Moreover, the statistical methods called the Giant Language Model Test Room (GLTR) designed by Gehrmann et al. [6] consists of three tests: Tests 1 and 2 checked if a generated word is sampled from the top of the distribution, and Test 3 veri\ufb01ed if the system is overly con\ufb01dent in its next prediction due to familiarity with the previously generated context. A human-subject study found that GLTR improved the human detection rate of fake text from 54% to 72% without prior training. 2.2 Zero-Shot Classi\ufb01cation The second detection approach is by zero-shot classi\ufb01cation (i.e., using a pre-trained LLM to detect its own generation or that of a similar model). In the work by Solaiman et al. [4], a baseline method that used an LLM to evaluate the log-probability and the corresponding threshold for making classi\ufb01cation decisions was proposed. However, this zero-shot approach performs poorly compared to the statistical methods. 2.3 Fine-Tuning Language Model The last approach is to \ufb01ne-tune an existing language model. For example, Zeller et al. [7] \ufb01ne-tuned a linear layer to identify if the input was generated by the GROVER model or by a human, using the hidden states in the encoder of GROVER."}, {"question": " What were the two different models used for text classification in the paper?", "answer": " Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5)", "ref_chunk": "3 2 0 2 y a M 7 1 ] L C . s c [ 2 v 9 6 9 7 0 . 5 0 3 2 : v i X r a GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content Yutian Chen\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 yutianch@andrew.cmu.edu Hao Kang\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 haok@andrew.cmu.edu Vivian Zhai\u2020 College of Engineering Carnegie Mellon University Pittsburgh, PA 15213 yiyanz@andrew.cmu.edu Liangze Li Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 liangzel@andrew.cmu.edu Rita Singh Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 rsingh@cs.cmu.edu Bhiksha Raj Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 bhiksha@cs.cmu.edu Abstract This paper presents a novel approach for detecting ChatGPT-generated vs. human- written text using language models. To this end, we \ufb01rst collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two dif- ferent models for text classi\ufb01cation, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model\u2019s ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our \ufb01ndings provide important insights into the effective use of language models to detect generated text. 1 Introduction The development of an algorithm that can accurately distinguish between machine-generated text and human-generated text has become crucial in contexts where verifying the authenticity of information is essential, such as in legal proceedings and news reporting. Although traditional statistical techniques such as logistic regression and support vector machines (SVM) have been used for this purpose in the past [1], the emergence of Large Language Models (LLMs) like InstructGPT [2] and the availability of its free deployment, ChatGPT, has presented signi\ufb01cant challenges to existing detection methods. As a result, the need to develop novel algorithms that can accurately distinguish between machine and human-generated text has become more pressing than ever before. \u2020Three authors contribute equally to this work. To address this issue, we focused on \ufb01ne-tuning approaches to distinguish human-written and ChatGPT-generated text. We \ufb01rst collected the data from ChatGPT and established the OpenGPTText data set. Section 3 of the paper provides a detailed description of the data collection process, including the criteria to select the samples and the methods to \ufb01lter out irrelevant and undesired noise in the collected text. We then trained the frozen RoBERTa with MLP and \ufb01ne-tuned the T5 model on this data set for classi\ufb01cation. The resulting model is what we referred to as GPT-Sentinel. More details about the model can be found in Section 4 of the paper. The rest of this paper is structured as follows: We \ufb01rst discuss related work in Section 2; illustrate OpenGPTText data set in Section 3; present our model and the training details in Section 4; evaluate the performance using various metrics in Section 5; interpret the basis for the model\u2019s prediction in Section 6; point out future work in Section 7; and conclude in Section 8. 2 Related Work The work by Jawahar et al. identi\ufb01ed \ufb01ve key characteristics that a state-of-the-art detector for content generated by LLMs should possess: accuracy, data ef\ufb01ciency, generalizability, and interpretability [3]; where accuracy means the model should be able to distinguish between LLM-generated and human-written text while achieving an appropriate trade-off between precision and recall rates; data ef\ufb01ciency means that the detector should be able to operate with as few examples as possible from the language model; generalizability means that the detector should be able to work consistently, regardless of any change in the model architecture, prompt length, or training dataset; interpretability means the detector should provide clear explanations for the reasoning behind its decisions. These \ufb01ve principles is used as our guidance when designing the GPT-Sentinel. Approaches to machine-generated text detection can be divided into three categories: traditional statistical approach (by analyzing statistical abnormality in text sample), unsupervised-learning approach (by zero-shot classi\ufb01cation of LLM), and supervised-learning approach (by \ufb01ne-tuning a language model with or without a classi\ufb01cation module attached). 2.1 Statistical Methods The \ufb01rst approach to the problem is via the use of statistics. For instance, the work by Solaiman et al. [4] demonstrated that using a logistic regression model to differentiate between text generated by GPT-2 models vs. text written by humans could achieve an accuracy ranging from 88% (on 124 million parameter variants of GPT-2 model) to 74% (on 1.5 billion parameter variants of GPT-2 model). Moreover, the work by Ippolito et al. [5] showed that the top-k sampling method used in popular LLMs could over-sample high-likelihood words, and thus the generated text exhibited statistical anomalies, which could be further used for detection. Moreover, the statistical methods called the Giant Language Model Test Room (GLTR) designed by Gehrmann et al. [6] consists of three tests: Tests 1 and 2 checked if a generated word is sampled from the top of the distribution, and Test 3 veri\ufb01ed if the system is overly con\ufb01dent in its next prediction due to familiarity with the previously generated context. A human-subject study found that GLTR improved the human detection rate of fake text from 54% to 72% without prior training. 2.2 Zero-Shot Classi\ufb01cation The second detection approach is by zero-shot classi\ufb01cation (i.e., using a pre-trained LLM to detect its own generation or that of a similar model). In the work by Solaiman et al. [4], a baseline method that used an LLM to evaluate the log-probability and the corresponding threshold for making classi\ufb01cation decisions was proposed. However, this zero-shot approach performs poorly compared to the statistical methods. 2.3 Fine-Tuning Language Model The last approach is to \ufb01ne-tune an existing language model. For example, Zeller et al. [7] \ufb01ne-tuned a linear layer to identify if the input was generated by the GROVER model or by a human, using the hidden states in the encoder of GROVER."}, {"question": " What was the accuracy achieved by the models on the test dataset?", "answer": " Over 97%", "ref_chunk": "3 2 0 2 y a M 7 1 ] L C . s c [ 2 v 9 6 9 7 0 . 5 0 3 2 : v i X r a GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content Yutian Chen\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 yutianch@andrew.cmu.edu Hao Kang\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 haok@andrew.cmu.edu Vivian Zhai\u2020 College of Engineering Carnegie Mellon University Pittsburgh, PA 15213 yiyanz@andrew.cmu.edu Liangze Li Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 liangzel@andrew.cmu.edu Rita Singh Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 rsingh@cs.cmu.edu Bhiksha Raj Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 bhiksha@cs.cmu.edu Abstract This paper presents a novel approach for detecting ChatGPT-generated vs. human- written text using language models. To this end, we \ufb01rst collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two dif- ferent models for text classi\ufb01cation, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model\u2019s ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our \ufb01ndings provide important insights into the effective use of language models to detect generated text. 1 Introduction The development of an algorithm that can accurately distinguish between machine-generated text and human-generated text has become crucial in contexts where verifying the authenticity of information is essential, such as in legal proceedings and news reporting. Although traditional statistical techniques such as logistic regression and support vector machines (SVM) have been used for this purpose in the past [1], the emergence of Large Language Models (LLMs) like InstructGPT [2] and the availability of its free deployment, ChatGPT, has presented signi\ufb01cant challenges to existing detection methods. As a result, the need to develop novel algorithms that can accurately distinguish between machine and human-generated text has become more pressing than ever before. \u2020Three authors contribute equally to this work. To address this issue, we focused on \ufb01ne-tuning approaches to distinguish human-written and ChatGPT-generated text. We \ufb01rst collected the data from ChatGPT and established the OpenGPTText data set. Section 3 of the paper provides a detailed description of the data collection process, including the criteria to select the samples and the methods to \ufb01lter out irrelevant and undesired noise in the collected text. We then trained the frozen RoBERTa with MLP and \ufb01ne-tuned the T5 model on this data set for classi\ufb01cation. The resulting model is what we referred to as GPT-Sentinel. More details about the model can be found in Section 4 of the paper. The rest of this paper is structured as follows: We \ufb01rst discuss related work in Section 2; illustrate OpenGPTText data set in Section 3; present our model and the training details in Section 4; evaluate the performance using various metrics in Section 5; interpret the basis for the model\u2019s prediction in Section 6; point out future work in Section 7; and conclude in Section 8. 2 Related Work The work by Jawahar et al. identi\ufb01ed \ufb01ve key characteristics that a state-of-the-art detector for content generated by LLMs should possess: accuracy, data ef\ufb01ciency, generalizability, and interpretability [3]; where accuracy means the model should be able to distinguish between LLM-generated and human-written text while achieving an appropriate trade-off between precision and recall rates; data ef\ufb01ciency means that the detector should be able to operate with as few examples as possible from the language model; generalizability means that the detector should be able to work consistently, regardless of any change in the model architecture, prompt length, or training dataset; interpretability means the detector should provide clear explanations for the reasoning behind its decisions. These \ufb01ve principles is used as our guidance when designing the GPT-Sentinel. Approaches to machine-generated text detection can be divided into three categories: traditional statistical approach (by analyzing statistical abnormality in text sample), unsupervised-learning approach (by zero-shot classi\ufb01cation of LLM), and supervised-learning approach (by \ufb01ne-tuning a language model with or without a classi\ufb01cation module attached). 2.1 Statistical Methods The \ufb01rst approach to the problem is via the use of statistics. For instance, the work by Solaiman et al. [4] demonstrated that using a logistic regression model to differentiate between text generated by GPT-2 models vs. text written by humans could achieve an accuracy ranging from 88% (on 124 million parameter variants of GPT-2 model) to 74% (on 1.5 billion parameter variants of GPT-2 model). Moreover, the work by Ippolito et al. [5] showed that the top-k sampling method used in popular LLMs could over-sample high-likelihood words, and thus the generated text exhibited statistical anomalies, which could be further used for detection. Moreover, the statistical methods called the Giant Language Model Test Room (GLTR) designed by Gehrmann et al. [6] consists of three tests: Tests 1 and 2 checked if a generated word is sampled from the top of the distribution, and Test 3 veri\ufb01ed if the system is overly con\ufb01dent in its next prediction due to familiarity with the previously generated context. A human-subject study found that GLTR improved the human detection rate of fake text from 54% to 72% without prior training. 2.2 Zero-Shot Classi\ufb01cation The second detection approach is by zero-shot classi\ufb01cation (i.e., using a pre-trained LLM to detect its own generation or that of a similar model). In the work by Solaiman et al. [4], a baseline method that used an LLM to evaluate the log-probability and the corresponding threshold for making classi\ufb01cation decisions was proposed. However, this zero-shot approach performs poorly compared to the statistical methods. 2.3 Fine-Tuning Language Model The last approach is to \ufb01ne-tune an existing language model. For example, Zeller et al. [7] \ufb01ne-tuned a linear layer to identify if the input was generated by the GROVER model or by a human, using the hidden states in the encoder of GROVER."}, {"question": " What is the main focus of the paper in terms of distinguishing between text?", "answer": " To distinguish between human-written and ChatGPT-generated text", "ref_chunk": "3 2 0 2 y a M 7 1 ] L C . s c [ 2 v 9 6 9 7 0 . 5 0 3 2 : v i X r a GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content Yutian Chen\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 yutianch@andrew.cmu.edu Hao Kang\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 haok@andrew.cmu.edu Vivian Zhai\u2020 College of Engineering Carnegie Mellon University Pittsburgh, PA 15213 yiyanz@andrew.cmu.edu Liangze Li Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 liangzel@andrew.cmu.edu Rita Singh Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 rsingh@cs.cmu.edu Bhiksha Raj Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 bhiksha@cs.cmu.edu Abstract This paper presents a novel approach for detecting ChatGPT-generated vs. human- written text using language models. To this end, we \ufb01rst collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two dif- ferent models for text classi\ufb01cation, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model\u2019s ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our \ufb01ndings provide important insights into the effective use of language models to detect generated text. 1 Introduction The development of an algorithm that can accurately distinguish between machine-generated text and human-generated text has become crucial in contexts where verifying the authenticity of information is essential, such as in legal proceedings and news reporting. Although traditional statistical techniques such as logistic regression and support vector machines (SVM) have been used for this purpose in the past [1], the emergence of Large Language Models (LLMs) like InstructGPT [2] and the availability of its free deployment, ChatGPT, has presented signi\ufb01cant challenges to existing detection methods. As a result, the need to develop novel algorithms that can accurately distinguish between machine and human-generated text has become more pressing than ever before. \u2020Three authors contribute equally to this work. To address this issue, we focused on \ufb01ne-tuning approaches to distinguish human-written and ChatGPT-generated text. We \ufb01rst collected the data from ChatGPT and established the OpenGPTText data set. Section 3 of the paper provides a detailed description of the data collection process, including the criteria to select the samples and the methods to \ufb01lter out irrelevant and undesired noise in the collected text. We then trained the frozen RoBERTa with MLP and \ufb01ne-tuned the T5 model on this data set for classi\ufb01cation. The resulting model is what we referred to as GPT-Sentinel. More details about the model can be found in Section 4 of the paper. The rest of this paper is structured as follows: We \ufb01rst discuss related work in Section 2; illustrate OpenGPTText data set in Section 3; present our model and the training details in Section 4; evaluate the performance using various metrics in Section 5; interpret the basis for the model\u2019s prediction in Section 6; point out future work in Section 7; and conclude in Section 8. 2 Related Work The work by Jawahar et al. identi\ufb01ed \ufb01ve key characteristics that a state-of-the-art detector for content generated by LLMs should possess: accuracy, data ef\ufb01ciency, generalizability, and interpretability [3]; where accuracy means the model should be able to distinguish between LLM-generated and human-written text while achieving an appropriate trade-off between precision and recall rates; data ef\ufb01ciency means that the detector should be able to operate with as few examples as possible from the language model; generalizability means that the detector should be able to work consistently, regardless of any change in the model architecture, prompt length, or training dataset; interpretability means the detector should provide clear explanations for the reasoning behind its decisions. These \ufb01ve principles is used as our guidance when designing the GPT-Sentinel. Approaches to machine-generated text detection can be divided into three categories: traditional statistical approach (by analyzing statistical abnormality in text sample), unsupervised-learning approach (by zero-shot classi\ufb01cation of LLM), and supervised-learning approach (by \ufb01ne-tuning a language model with or without a classi\ufb01cation module attached). 2.1 Statistical Methods The \ufb01rst approach to the problem is via the use of statistics. For instance, the work by Solaiman et al. [4] demonstrated that using a logistic regression model to differentiate between text generated by GPT-2 models vs. text written by humans could achieve an accuracy ranging from 88% (on 124 million parameter variants of GPT-2 model) to 74% (on 1.5 billion parameter variants of GPT-2 model). Moreover, the work by Ippolito et al. [5] showed that the top-k sampling method used in popular LLMs could over-sample high-likelihood words, and thus the generated text exhibited statistical anomalies, which could be further used for detection. Moreover, the statistical methods called the Giant Language Model Test Room (GLTR) designed by Gehrmann et al. [6] consists of three tests: Tests 1 and 2 checked if a generated word is sampled from the top of the distribution, and Test 3 veri\ufb01ed if the system is overly con\ufb01dent in its next prediction due to familiarity with the previously generated context. A human-subject study found that GLTR improved the human detection rate of fake text from 54% to 72% without prior training. 2.2 Zero-Shot Classi\ufb01cation The second detection approach is by zero-shot classi\ufb01cation (i.e., using a pre-trained LLM to detect its own generation or that of a similar model). In the work by Solaiman et al. [4], a baseline method that used an LLM to evaluate the log-probability and the corresponding threshold for making classi\ufb01cation decisions was proposed. However, this zero-shot approach performs poorly compared to the statistical methods. 2.3 Fine-Tuning Language Model The last approach is to \ufb01ne-tune an existing language model. For example, Zeller et al. [7] \ufb01ne-tuned a linear layer to identify if the input was generated by the GROVER model or by a human, using the hidden states in the encoder of GROVER."}, {"question": " What type of models were used in the fine-tuning approaches to detect human-written and ChatGPT-generated text?", "answer": " Frozen RoBERTa with MLP and fine-tuned the T5 model", "ref_chunk": "3 2 0 2 y a M 7 1 ] L C . s c [ 2 v 9 6 9 7 0 . 5 0 3 2 : v i X r a GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content Yutian Chen\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 yutianch@andrew.cmu.edu Hao Kang\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 haok@andrew.cmu.edu Vivian Zhai\u2020 College of Engineering Carnegie Mellon University Pittsburgh, PA 15213 yiyanz@andrew.cmu.edu Liangze Li Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 liangzel@andrew.cmu.edu Rita Singh Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 rsingh@cs.cmu.edu Bhiksha Raj Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 bhiksha@cs.cmu.edu Abstract This paper presents a novel approach for detecting ChatGPT-generated vs. human- written text using language models. To this end, we \ufb01rst collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two dif- ferent models for text classi\ufb01cation, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model\u2019s ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our \ufb01ndings provide important insights into the effective use of language models to detect generated text. 1 Introduction The development of an algorithm that can accurately distinguish between machine-generated text and human-generated text has become crucial in contexts where verifying the authenticity of information is essential, such as in legal proceedings and news reporting. Although traditional statistical techniques such as logistic regression and support vector machines (SVM) have been used for this purpose in the past [1], the emergence of Large Language Models (LLMs) like InstructGPT [2] and the availability of its free deployment, ChatGPT, has presented signi\ufb01cant challenges to existing detection methods. As a result, the need to develop novel algorithms that can accurately distinguish between machine and human-generated text has become more pressing than ever before. \u2020Three authors contribute equally to this work. To address this issue, we focused on \ufb01ne-tuning approaches to distinguish human-written and ChatGPT-generated text. We \ufb01rst collected the data from ChatGPT and established the OpenGPTText data set. Section 3 of the paper provides a detailed description of the data collection process, including the criteria to select the samples and the methods to \ufb01lter out irrelevant and undesired noise in the collected text. We then trained the frozen RoBERTa with MLP and \ufb01ne-tuned the T5 model on this data set for classi\ufb01cation. The resulting model is what we referred to as GPT-Sentinel. More details about the model can be found in Section 4 of the paper. The rest of this paper is structured as follows: We \ufb01rst discuss related work in Section 2; illustrate OpenGPTText data set in Section 3; present our model and the training details in Section 4; evaluate the performance using various metrics in Section 5; interpret the basis for the model\u2019s prediction in Section 6; point out future work in Section 7; and conclude in Section 8. 2 Related Work The work by Jawahar et al. identi\ufb01ed \ufb01ve key characteristics that a state-of-the-art detector for content generated by LLMs should possess: accuracy, data ef\ufb01ciency, generalizability, and interpretability [3]; where accuracy means the model should be able to distinguish between LLM-generated and human-written text while achieving an appropriate trade-off between precision and recall rates; data ef\ufb01ciency means that the detector should be able to operate with as few examples as possible from the language model; generalizability means that the detector should be able to work consistently, regardless of any change in the model architecture, prompt length, or training dataset; interpretability means the detector should provide clear explanations for the reasoning behind its decisions. These \ufb01ve principles is used as our guidance when designing the GPT-Sentinel. Approaches to machine-generated text detection can be divided into three categories: traditional statistical approach (by analyzing statistical abnormality in text sample), unsupervised-learning approach (by zero-shot classi\ufb01cation of LLM), and supervised-learning approach (by \ufb01ne-tuning a language model with or without a classi\ufb01cation module attached). 2.1 Statistical Methods The \ufb01rst approach to the problem is via the use of statistics. For instance, the work by Solaiman et al. [4] demonstrated that using a logistic regression model to differentiate between text generated by GPT-2 models vs. text written by humans could achieve an accuracy ranging from 88% (on 124 million parameter variants of GPT-2 model) to 74% (on 1.5 billion parameter variants of GPT-2 model). Moreover, the work by Ippolito et al. [5] showed that the top-k sampling method used in popular LLMs could over-sample high-likelihood words, and thus the generated text exhibited statistical anomalies, which could be further used for detection. Moreover, the statistical methods called the Giant Language Model Test Room (GLTR) designed by Gehrmann et al. [6] consists of three tests: Tests 1 and 2 checked if a generated word is sampled from the top of the distribution, and Test 3 veri\ufb01ed if the system is overly con\ufb01dent in its next prediction due to familiarity with the previously generated context. A human-subject study found that GLTR improved the human detection rate of fake text from 54% to 72% without prior training. 2.2 Zero-Shot Classi\ufb01cation The second detection approach is by zero-shot classi\ufb01cation (i.e., using a pre-trained LLM to detect its own generation or that of a similar model). In the work by Solaiman et al. [4], a baseline method that used an LLM to evaluate the log-probability and the corresponding threshold for making classi\ufb01cation decisions was proposed. However, this zero-shot approach performs poorly compared to the statistical methods. 2.3 Fine-Tuning Language Model The last approach is to \ufb01ne-tune an existing language model. For example, Zeller et al. [7] \ufb01ne-tuned a linear layer to identify if the input was generated by the GROVER model or by a human, using the hidden states in the encoder of GROVER."}, {"question": " According to related work, what are the five key characteristics a state-of-the-art detector for content generated by LLMs should possess?", "answer": " Accuracy, data efficiency, generalizability, and interpretability", "ref_chunk": "3 2 0 2 y a M 7 1 ] L C . s c [ 2 v 9 6 9 7 0 . 5 0 3 2 : v i X r a GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content Yutian Chen\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 yutianch@andrew.cmu.edu Hao Kang\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 haok@andrew.cmu.edu Vivian Zhai\u2020 College of Engineering Carnegie Mellon University Pittsburgh, PA 15213 yiyanz@andrew.cmu.edu Liangze Li Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 liangzel@andrew.cmu.edu Rita Singh Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 rsingh@cs.cmu.edu Bhiksha Raj Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 bhiksha@cs.cmu.edu Abstract This paper presents a novel approach for detecting ChatGPT-generated vs. human- written text using language models. To this end, we \ufb01rst collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two dif- ferent models for text classi\ufb01cation, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model\u2019s ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our \ufb01ndings provide important insights into the effective use of language models to detect generated text. 1 Introduction The development of an algorithm that can accurately distinguish between machine-generated text and human-generated text has become crucial in contexts where verifying the authenticity of information is essential, such as in legal proceedings and news reporting. Although traditional statistical techniques such as logistic regression and support vector machines (SVM) have been used for this purpose in the past [1], the emergence of Large Language Models (LLMs) like InstructGPT [2] and the availability of its free deployment, ChatGPT, has presented signi\ufb01cant challenges to existing detection methods. As a result, the need to develop novel algorithms that can accurately distinguish between machine and human-generated text has become more pressing than ever before. \u2020Three authors contribute equally to this work. To address this issue, we focused on \ufb01ne-tuning approaches to distinguish human-written and ChatGPT-generated text. We \ufb01rst collected the data from ChatGPT and established the OpenGPTText data set. Section 3 of the paper provides a detailed description of the data collection process, including the criteria to select the samples and the methods to \ufb01lter out irrelevant and undesired noise in the collected text. We then trained the frozen RoBERTa with MLP and \ufb01ne-tuned the T5 model on this data set for classi\ufb01cation. The resulting model is what we referred to as GPT-Sentinel. More details about the model can be found in Section 4 of the paper. The rest of this paper is structured as follows: We \ufb01rst discuss related work in Section 2; illustrate OpenGPTText data set in Section 3; present our model and the training details in Section 4; evaluate the performance using various metrics in Section 5; interpret the basis for the model\u2019s prediction in Section 6; point out future work in Section 7; and conclude in Section 8. 2 Related Work The work by Jawahar et al. identi\ufb01ed \ufb01ve key characteristics that a state-of-the-art detector for content generated by LLMs should possess: accuracy, data ef\ufb01ciency, generalizability, and interpretability [3]; where accuracy means the model should be able to distinguish between LLM-generated and human-written text while achieving an appropriate trade-off between precision and recall rates; data ef\ufb01ciency means that the detector should be able to operate with as few examples as possible from the language model; generalizability means that the detector should be able to work consistently, regardless of any change in the model architecture, prompt length, or training dataset; interpretability means the detector should provide clear explanations for the reasoning behind its decisions. These \ufb01ve principles is used as our guidance when designing the GPT-Sentinel. Approaches to machine-generated text detection can be divided into three categories: traditional statistical approach (by analyzing statistical abnormality in text sample), unsupervised-learning approach (by zero-shot classi\ufb01cation of LLM), and supervised-learning approach (by \ufb01ne-tuning a language model with or without a classi\ufb01cation module attached). 2.1 Statistical Methods The \ufb01rst approach to the problem is via the use of statistics. For instance, the work by Solaiman et al. [4] demonstrated that using a logistic regression model to differentiate between text generated by GPT-2 models vs. text written by humans could achieve an accuracy ranging from 88% (on 124 million parameter variants of GPT-2 model) to 74% (on 1.5 billion parameter variants of GPT-2 model). Moreover, the work by Ippolito et al. [5] showed that the top-k sampling method used in popular LLMs could over-sample high-likelihood words, and thus the generated text exhibited statistical anomalies, which could be further used for detection. Moreover, the statistical methods called the Giant Language Model Test Room (GLTR) designed by Gehrmann et al. [6] consists of three tests: Tests 1 and 2 checked if a generated word is sampled from the top of the distribution, and Test 3 veri\ufb01ed if the system is overly con\ufb01dent in its next prediction due to familiarity with the previously generated context. A human-subject study found that GLTR improved the human detection rate of fake text from 54% to 72% without prior training. 2.2 Zero-Shot Classi\ufb01cation The second detection approach is by zero-shot classi\ufb01cation (i.e., using a pre-trained LLM to detect its own generation or that of a similar model). In the work by Solaiman et al. [4], a baseline method that used an LLM to evaluate the log-probability and the corresponding threshold for making classi\ufb01cation decisions was proposed. However, this zero-shot approach performs poorly compared to the statistical methods. 2.3 Fine-Tuning Language Model The last approach is to \ufb01ne-tune an existing language model. For example, Zeller et al. [7] \ufb01ne-tuned a linear layer to identify if the input was generated by the GROVER model or by a human, using the hidden states in the encoder of GROVER."}, {"question": " What are the three categories in which approaches to machine-generated text detection can be divided?", "answer": " Traditional statistical approach, unsupervised-learning approach, supervised-learning approach", "ref_chunk": "3 2 0 2 y a M 7 1 ] L C . s c [ 2 v 9 6 9 7 0 . 5 0 3 2 : v i X r a GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content Yutian Chen\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 yutianch@andrew.cmu.edu Hao Kang\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 haok@andrew.cmu.edu Vivian Zhai\u2020 College of Engineering Carnegie Mellon University Pittsburgh, PA 15213 yiyanz@andrew.cmu.edu Liangze Li Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 liangzel@andrew.cmu.edu Rita Singh Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 rsingh@cs.cmu.edu Bhiksha Raj Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 bhiksha@cs.cmu.edu Abstract This paper presents a novel approach for detecting ChatGPT-generated vs. human- written text using language models. To this end, we \ufb01rst collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two dif- ferent models for text classi\ufb01cation, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model\u2019s ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our \ufb01ndings provide important insights into the effective use of language models to detect generated text. 1 Introduction The development of an algorithm that can accurately distinguish between machine-generated text and human-generated text has become crucial in contexts where verifying the authenticity of information is essential, such as in legal proceedings and news reporting. Although traditional statistical techniques such as logistic regression and support vector machines (SVM) have been used for this purpose in the past [1], the emergence of Large Language Models (LLMs) like InstructGPT [2] and the availability of its free deployment, ChatGPT, has presented signi\ufb01cant challenges to existing detection methods. As a result, the need to develop novel algorithms that can accurately distinguish between machine and human-generated text has become more pressing than ever before. \u2020Three authors contribute equally to this work. To address this issue, we focused on \ufb01ne-tuning approaches to distinguish human-written and ChatGPT-generated text. We \ufb01rst collected the data from ChatGPT and established the OpenGPTText data set. Section 3 of the paper provides a detailed description of the data collection process, including the criteria to select the samples and the methods to \ufb01lter out irrelevant and undesired noise in the collected text. We then trained the frozen RoBERTa with MLP and \ufb01ne-tuned the T5 model on this data set for classi\ufb01cation. The resulting model is what we referred to as GPT-Sentinel. More details about the model can be found in Section 4 of the paper. The rest of this paper is structured as follows: We \ufb01rst discuss related work in Section 2; illustrate OpenGPTText data set in Section 3; present our model and the training details in Section 4; evaluate the performance using various metrics in Section 5; interpret the basis for the model\u2019s prediction in Section 6; point out future work in Section 7; and conclude in Section 8. 2 Related Work The work by Jawahar et al. identi\ufb01ed \ufb01ve key characteristics that a state-of-the-art detector for content generated by LLMs should possess: accuracy, data ef\ufb01ciency, generalizability, and interpretability [3]; where accuracy means the model should be able to distinguish between LLM-generated and human-written text while achieving an appropriate trade-off between precision and recall rates; data ef\ufb01ciency means that the detector should be able to operate with as few examples as possible from the language model; generalizability means that the detector should be able to work consistently, regardless of any change in the model architecture, prompt length, or training dataset; interpretability means the detector should provide clear explanations for the reasoning behind its decisions. These \ufb01ve principles is used as our guidance when designing the GPT-Sentinel. Approaches to machine-generated text detection can be divided into three categories: traditional statistical approach (by analyzing statistical abnormality in text sample), unsupervised-learning approach (by zero-shot classi\ufb01cation of LLM), and supervised-learning approach (by \ufb01ne-tuning a language model with or without a classi\ufb01cation module attached). 2.1 Statistical Methods The \ufb01rst approach to the problem is via the use of statistics. For instance, the work by Solaiman et al. [4] demonstrated that using a logistic regression model to differentiate between text generated by GPT-2 models vs. text written by humans could achieve an accuracy ranging from 88% (on 124 million parameter variants of GPT-2 model) to 74% (on 1.5 billion parameter variants of GPT-2 model). Moreover, the work by Ippolito et al. [5] showed that the top-k sampling method used in popular LLMs could over-sample high-likelihood words, and thus the generated text exhibited statistical anomalies, which could be further used for detection. Moreover, the statistical methods called the Giant Language Model Test Room (GLTR) designed by Gehrmann et al. [6] consists of three tests: Tests 1 and 2 checked if a generated word is sampled from the top of the distribution, and Test 3 veri\ufb01ed if the system is overly con\ufb01dent in its next prediction due to familiarity with the previously generated context. A human-subject study found that GLTR improved the human detection rate of fake text from 54% to 72% without prior training. 2.2 Zero-Shot Classi\ufb01cation The second detection approach is by zero-shot classi\ufb01cation (i.e., using a pre-trained LLM to detect its own generation or that of a similar model). In the work by Solaiman et al. [4], a baseline method that used an LLM to evaluate the log-probability and the corresponding threshold for making classi\ufb01cation decisions was proposed. However, this zero-shot approach performs poorly compared to the statistical methods. 2.3 Fine-Tuning Language Model The last approach is to \ufb01ne-tune an existing language model. For example, Zeller et al. [7] \ufb01ne-tuned a linear layer to identify if the input was generated by the GROVER model or by a human, using the hidden states in the encoder of GROVER."}, {"question": " What is the first approach mentioned in the text for detecting machine-generated text?", "answer": " Statistical Methods", "ref_chunk": "3 2 0 2 y a M 7 1 ] L C . s c [ 2 v 9 6 9 7 0 . 5 0 3 2 : v i X r a GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content Yutian Chen\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 yutianch@andrew.cmu.edu Hao Kang\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 haok@andrew.cmu.edu Vivian Zhai\u2020 College of Engineering Carnegie Mellon University Pittsburgh, PA 15213 yiyanz@andrew.cmu.edu Liangze Li Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 liangzel@andrew.cmu.edu Rita Singh Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 rsingh@cs.cmu.edu Bhiksha Raj Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 bhiksha@cs.cmu.edu Abstract This paper presents a novel approach for detecting ChatGPT-generated vs. human- written text using language models. To this end, we \ufb01rst collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two dif- ferent models for text classi\ufb01cation, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model\u2019s ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our \ufb01ndings provide important insights into the effective use of language models to detect generated text. 1 Introduction The development of an algorithm that can accurately distinguish between machine-generated text and human-generated text has become crucial in contexts where verifying the authenticity of information is essential, such as in legal proceedings and news reporting. Although traditional statistical techniques such as logistic regression and support vector machines (SVM) have been used for this purpose in the past [1], the emergence of Large Language Models (LLMs) like InstructGPT [2] and the availability of its free deployment, ChatGPT, has presented signi\ufb01cant challenges to existing detection methods. As a result, the need to develop novel algorithms that can accurately distinguish between machine and human-generated text has become more pressing than ever before. \u2020Three authors contribute equally to this work. To address this issue, we focused on \ufb01ne-tuning approaches to distinguish human-written and ChatGPT-generated text. We \ufb01rst collected the data from ChatGPT and established the OpenGPTText data set. Section 3 of the paper provides a detailed description of the data collection process, including the criteria to select the samples and the methods to \ufb01lter out irrelevant and undesired noise in the collected text. We then trained the frozen RoBERTa with MLP and \ufb01ne-tuned the T5 model on this data set for classi\ufb01cation. The resulting model is what we referred to as GPT-Sentinel. More details about the model can be found in Section 4 of the paper. The rest of this paper is structured as follows: We \ufb01rst discuss related work in Section 2; illustrate OpenGPTText data set in Section 3; present our model and the training details in Section 4; evaluate the performance using various metrics in Section 5; interpret the basis for the model\u2019s prediction in Section 6; point out future work in Section 7; and conclude in Section 8. 2 Related Work The work by Jawahar et al. identi\ufb01ed \ufb01ve key characteristics that a state-of-the-art detector for content generated by LLMs should possess: accuracy, data ef\ufb01ciency, generalizability, and interpretability [3]; where accuracy means the model should be able to distinguish between LLM-generated and human-written text while achieving an appropriate trade-off between precision and recall rates; data ef\ufb01ciency means that the detector should be able to operate with as few examples as possible from the language model; generalizability means that the detector should be able to work consistently, regardless of any change in the model architecture, prompt length, or training dataset; interpretability means the detector should provide clear explanations for the reasoning behind its decisions. These \ufb01ve principles is used as our guidance when designing the GPT-Sentinel. Approaches to machine-generated text detection can be divided into three categories: traditional statistical approach (by analyzing statistical abnormality in text sample), unsupervised-learning approach (by zero-shot classi\ufb01cation of LLM), and supervised-learning approach (by \ufb01ne-tuning a language model with or without a classi\ufb01cation module attached). 2.1 Statistical Methods The \ufb01rst approach to the problem is via the use of statistics. For instance, the work by Solaiman et al. [4] demonstrated that using a logistic regression model to differentiate between text generated by GPT-2 models vs. text written by humans could achieve an accuracy ranging from 88% (on 124 million parameter variants of GPT-2 model) to 74% (on 1.5 billion parameter variants of GPT-2 model). Moreover, the work by Ippolito et al. [5] showed that the top-k sampling method used in popular LLMs could over-sample high-likelihood words, and thus the generated text exhibited statistical anomalies, which could be further used for detection. Moreover, the statistical methods called the Giant Language Model Test Room (GLTR) designed by Gehrmann et al. [6] consists of three tests: Tests 1 and 2 checked if a generated word is sampled from the top of the distribution, and Test 3 veri\ufb01ed if the system is overly con\ufb01dent in its next prediction due to familiarity with the previously generated context. A human-subject study found that GLTR improved the human detection rate of fake text from 54% to 72% without prior training. 2.2 Zero-Shot Classi\ufb01cation The second detection approach is by zero-shot classi\ufb01cation (i.e., using a pre-trained LLM to detect its own generation or that of a similar model). In the work by Solaiman et al. [4], a baseline method that used an LLM to evaluate the log-probability and the corresponding threshold for making classi\ufb01cation decisions was proposed. However, this zero-shot approach performs poorly compared to the statistical methods. 2.3 Fine-Tuning Language Model The last approach is to \ufb01ne-tune an existing language model. For example, Zeller et al. [7] \ufb01ne-tuned a linear layer to identify if the input was generated by the GROVER model or by a human, using the hidden states in the encoder of GROVER."}, {"question": " What is the second approach mentioned for detecting machine-generated text?", "answer": " Zero-Shot Classification", "ref_chunk": "3 2 0 2 y a M 7 1 ] L C . s c [ 2 v 9 6 9 7 0 . 5 0 3 2 : v i X r a GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content Yutian Chen\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 yutianch@andrew.cmu.edu Hao Kang\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 haok@andrew.cmu.edu Vivian Zhai\u2020 College of Engineering Carnegie Mellon University Pittsburgh, PA 15213 yiyanz@andrew.cmu.edu Liangze Li Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 liangzel@andrew.cmu.edu Rita Singh Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 rsingh@cs.cmu.edu Bhiksha Raj Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 bhiksha@cs.cmu.edu Abstract This paper presents a novel approach for detecting ChatGPT-generated vs. human- written text using language models. To this end, we \ufb01rst collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two dif- ferent models for text classi\ufb01cation, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model\u2019s ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our \ufb01ndings provide important insights into the effective use of language models to detect generated text. 1 Introduction The development of an algorithm that can accurately distinguish between machine-generated text and human-generated text has become crucial in contexts where verifying the authenticity of information is essential, such as in legal proceedings and news reporting. Although traditional statistical techniques such as logistic regression and support vector machines (SVM) have been used for this purpose in the past [1], the emergence of Large Language Models (LLMs) like InstructGPT [2] and the availability of its free deployment, ChatGPT, has presented signi\ufb01cant challenges to existing detection methods. As a result, the need to develop novel algorithms that can accurately distinguish between machine and human-generated text has become more pressing than ever before. \u2020Three authors contribute equally to this work. To address this issue, we focused on \ufb01ne-tuning approaches to distinguish human-written and ChatGPT-generated text. We \ufb01rst collected the data from ChatGPT and established the OpenGPTText data set. Section 3 of the paper provides a detailed description of the data collection process, including the criteria to select the samples and the methods to \ufb01lter out irrelevant and undesired noise in the collected text. We then trained the frozen RoBERTa with MLP and \ufb01ne-tuned the T5 model on this data set for classi\ufb01cation. The resulting model is what we referred to as GPT-Sentinel. More details about the model can be found in Section 4 of the paper. The rest of this paper is structured as follows: We \ufb01rst discuss related work in Section 2; illustrate OpenGPTText data set in Section 3; present our model and the training details in Section 4; evaluate the performance using various metrics in Section 5; interpret the basis for the model\u2019s prediction in Section 6; point out future work in Section 7; and conclude in Section 8. 2 Related Work The work by Jawahar et al. identi\ufb01ed \ufb01ve key characteristics that a state-of-the-art detector for content generated by LLMs should possess: accuracy, data ef\ufb01ciency, generalizability, and interpretability [3]; where accuracy means the model should be able to distinguish between LLM-generated and human-written text while achieving an appropriate trade-off between precision and recall rates; data ef\ufb01ciency means that the detector should be able to operate with as few examples as possible from the language model; generalizability means that the detector should be able to work consistently, regardless of any change in the model architecture, prompt length, or training dataset; interpretability means the detector should provide clear explanations for the reasoning behind its decisions. These \ufb01ve principles is used as our guidance when designing the GPT-Sentinel. Approaches to machine-generated text detection can be divided into three categories: traditional statistical approach (by analyzing statistical abnormality in text sample), unsupervised-learning approach (by zero-shot classi\ufb01cation of LLM), and supervised-learning approach (by \ufb01ne-tuning a language model with or without a classi\ufb01cation module attached). 2.1 Statistical Methods The \ufb01rst approach to the problem is via the use of statistics. For instance, the work by Solaiman et al. [4] demonstrated that using a logistic regression model to differentiate between text generated by GPT-2 models vs. text written by humans could achieve an accuracy ranging from 88% (on 124 million parameter variants of GPT-2 model) to 74% (on 1.5 billion parameter variants of GPT-2 model). Moreover, the work by Ippolito et al. [5] showed that the top-k sampling method used in popular LLMs could over-sample high-likelihood words, and thus the generated text exhibited statistical anomalies, which could be further used for detection. Moreover, the statistical methods called the Giant Language Model Test Room (GLTR) designed by Gehrmann et al. [6] consists of three tests: Tests 1 and 2 checked if a generated word is sampled from the top of the distribution, and Test 3 veri\ufb01ed if the system is overly con\ufb01dent in its next prediction due to familiarity with the previously generated context. A human-subject study found that GLTR improved the human detection rate of fake text from 54% to 72% without prior training. 2.2 Zero-Shot Classi\ufb01cation The second detection approach is by zero-shot classi\ufb01cation (i.e., using a pre-trained LLM to detect its own generation or that of a similar model). In the work by Solaiman et al. [4], a baseline method that used an LLM to evaluate the log-probability and the corresponding threshold for making classi\ufb01cation decisions was proposed. However, this zero-shot approach performs poorly compared to the statistical methods. 2.3 Fine-Tuning Language Model The last approach is to \ufb01ne-tune an existing language model. For example, Zeller et al. [7] \ufb01ne-tuned a linear layer to identify if the input was generated by the GROVER model or by a human, using the hidden states in the encoder of GROVER."}, {"question": " What is the final approach discussed for detecting machine-generated text?", "answer": " Fine-Tuning Language Model", "ref_chunk": "3 2 0 2 y a M 7 1 ] L C . s c [ 2 v 9 6 9 7 0 . 5 0 3 2 : v i X r a GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content Yutian Chen\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 yutianch@andrew.cmu.edu Hao Kang\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 haok@andrew.cmu.edu Vivian Zhai\u2020 College of Engineering Carnegie Mellon University Pittsburgh, PA 15213 yiyanz@andrew.cmu.edu Liangze Li Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 liangzel@andrew.cmu.edu Rita Singh Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 rsingh@cs.cmu.edu Bhiksha Raj Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 bhiksha@cs.cmu.edu Abstract This paper presents a novel approach for detecting ChatGPT-generated vs. human- written text using language models. To this end, we \ufb01rst collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two dif- ferent models for text classi\ufb01cation, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model\u2019s ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our \ufb01ndings provide important insights into the effective use of language models to detect generated text. 1 Introduction The development of an algorithm that can accurately distinguish between machine-generated text and human-generated text has become crucial in contexts where verifying the authenticity of information is essential, such as in legal proceedings and news reporting. Although traditional statistical techniques such as logistic regression and support vector machines (SVM) have been used for this purpose in the past [1], the emergence of Large Language Models (LLMs) like InstructGPT [2] and the availability of its free deployment, ChatGPT, has presented signi\ufb01cant challenges to existing detection methods. As a result, the need to develop novel algorithms that can accurately distinguish between machine and human-generated text has become more pressing than ever before. \u2020Three authors contribute equally to this work. To address this issue, we focused on \ufb01ne-tuning approaches to distinguish human-written and ChatGPT-generated text. We \ufb01rst collected the data from ChatGPT and established the OpenGPTText data set. Section 3 of the paper provides a detailed description of the data collection process, including the criteria to select the samples and the methods to \ufb01lter out irrelevant and undesired noise in the collected text. We then trained the frozen RoBERTa with MLP and \ufb01ne-tuned the T5 model on this data set for classi\ufb01cation. The resulting model is what we referred to as GPT-Sentinel. More details about the model can be found in Section 4 of the paper. The rest of this paper is structured as follows: We \ufb01rst discuss related work in Section 2; illustrate OpenGPTText data set in Section 3; present our model and the training details in Section 4; evaluate the performance using various metrics in Section 5; interpret the basis for the model\u2019s prediction in Section 6; point out future work in Section 7; and conclude in Section 8. 2 Related Work The work by Jawahar et al. identi\ufb01ed \ufb01ve key characteristics that a state-of-the-art detector for content generated by LLMs should possess: accuracy, data ef\ufb01ciency, generalizability, and interpretability [3]; where accuracy means the model should be able to distinguish between LLM-generated and human-written text while achieving an appropriate trade-off between precision and recall rates; data ef\ufb01ciency means that the detector should be able to operate with as few examples as possible from the language model; generalizability means that the detector should be able to work consistently, regardless of any change in the model architecture, prompt length, or training dataset; interpretability means the detector should provide clear explanations for the reasoning behind its decisions. These \ufb01ve principles is used as our guidance when designing the GPT-Sentinel. Approaches to machine-generated text detection can be divided into three categories: traditional statistical approach (by analyzing statistical abnormality in text sample), unsupervised-learning approach (by zero-shot classi\ufb01cation of LLM), and supervised-learning approach (by \ufb01ne-tuning a language model with or without a classi\ufb01cation module attached). 2.1 Statistical Methods The \ufb01rst approach to the problem is via the use of statistics. For instance, the work by Solaiman et al. [4] demonstrated that using a logistic regression model to differentiate between text generated by GPT-2 models vs. text written by humans could achieve an accuracy ranging from 88% (on 124 million parameter variants of GPT-2 model) to 74% (on 1.5 billion parameter variants of GPT-2 model). Moreover, the work by Ippolito et al. [5] showed that the top-k sampling method used in popular LLMs could over-sample high-likelihood words, and thus the generated text exhibited statistical anomalies, which could be further used for detection. Moreover, the statistical methods called the Giant Language Model Test Room (GLTR) designed by Gehrmann et al. [6] consists of three tests: Tests 1 and 2 checked if a generated word is sampled from the top of the distribution, and Test 3 veri\ufb01ed if the system is overly con\ufb01dent in its next prediction due to familiarity with the previously generated context. A human-subject study found that GLTR improved the human detection rate of fake text from 54% to 72% without prior training. 2.2 Zero-Shot Classi\ufb01cation The second detection approach is by zero-shot classi\ufb01cation (i.e., using a pre-trained LLM to detect its own generation or that of a similar model). In the work by Solaiman et al. [4], a baseline method that used an LLM to evaluate the log-probability and the corresponding threshold for making classi\ufb01cation decisions was proposed. However, this zero-shot approach performs poorly compared to the statistical methods. 2.3 Fine-Tuning Language Model The last approach is to \ufb01ne-tune an existing language model. For example, Zeller et al. [7] \ufb01ne-tuned a linear layer to identify if the input was generated by the GROVER model or by a human, using the hidden states in the encoder of GROVER."}], "doc_text": "3 2 0 2 y a M 7 1 ] L C . s c [ 2 v 9 6 9 7 0 . 5 0 3 2 : v i X r a GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content Yutian Chen\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 yutianch@andrew.cmu.edu Hao Kang\u2020 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 haok@andrew.cmu.edu Vivian Zhai\u2020 College of Engineering Carnegie Mellon University Pittsburgh, PA 15213 yiyanz@andrew.cmu.edu Liangze Li Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 liangzel@andrew.cmu.edu Rita Singh Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 rsingh@cs.cmu.edu Bhiksha Raj Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 bhiksha@cs.cmu.edu Abstract This paper presents a novel approach for detecting ChatGPT-generated vs. human- written text using language models. To this end, we \ufb01rst collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two dif- ferent models for text classi\ufb01cation, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model\u2019s ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our \ufb01ndings provide important insights into the effective use of language models to detect generated text. 1 Introduction The development of an algorithm that can accurately distinguish between machine-generated text and human-generated text has become crucial in contexts where verifying the authenticity of information is essential, such as in legal proceedings and news reporting. Although traditional statistical techniques such as logistic regression and support vector machines (SVM) have been used for this purpose in the past [1], the emergence of Large Language Models (LLMs) like InstructGPT [2] and the availability of its free deployment, ChatGPT, has presented signi\ufb01cant challenges to existing detection methods. As a result, the need to develop novel algorithms that can accurately distinguish between machine and human-generated text has become more pressing than ever before. \u2020Three authors contribute equally to this work. To address this issue, we focused on \ufb01ne-tuning approaches to distinguish human-written and ChatGPT-generated text. We \ufb01rst collected the data from ChatGPT and established the OpenGPTText data set. Section 3 of the paper provides a detailed description of the data collection process, including the criteria to select the samples and the methods to \ufb01lter out irrelevant and undesired noise in the collected text. We then trained the frozen RoBERTa with MLP and \ufb01ne-tuned the T5 model on this data set for classi\ufb01cation. The resulting model is what we referred to as GPT-Sentinel. More details about the model can be found in Section 4 of the paper. The rest of this paper is structured as follows: We \ufb01rst discuss related work in Section 2; illustrate OpenGPTText data set in Section 3; present our model and the training details in Section 4; evaluate the performance using various metrics in Section 5; interpret the basis for the model\u2019s prediction in Section 6; point out future work in Section 7; and conclude in Section 8. 2 Related Work The work by Jawahar et al. identi\ufb01ed \ufb01ve key characteristics that a state-of-the-art detector for content generated by LLMs should possess: accuracy, data ef\ufb01ciency, generalizability, and interpretability [3]; where accuracy means the model should be able to distinguish between LLM-generated and human-written text while achieving an appropriate trade-off between precision and recall rates; data ef\ufb01ciency means that the detector should be able to operate with as few examples as possible from the language model; generalizability means that the detector should be able to work consistently, regardless of any change in the model architecture, prompt length, or training dataset; interpretability means the detector should provide clear explanations for the reasoning behind its decisions. These \ufb01ve principles is used as our guidance when designing the GPT-Sentinel. Approaches to machine-generated text detection can be divided into three categories: traditional statistical approach (by analyzing statistical abnormality in text sample), unsupervised-learning approach (by zero-shot classi\ufb01cation of LLM), and supervised-learning approach (by \ufb01ne-tuning a language model with or without a classi\ufb01cation module attached). 2.1 Statistical Methods The \ufb01rst approach to the problem is via the use of statistics. For instance, the work by Solaiman et al. [4] demonstrated that using a logistic regression model to differentiate between text generated by GPT-2 models vs. text written by humans could achieve an accuracy ranging from 88% (on 124 million parameter variants of GPT-2 model) to 74% (on 1.5 billion parameter variants of GPT-2 model). Moreover, the work by Ippolito et al. [5] showed that the top-k sampling method used in popular LLMs could over-sample high-likelihood words, and thus the generated text exhibited statistical anomalies, which could be further used for detection. Moreover, the statistical methods called the Giant Language Model Test Room (GLTR) designed by Gehrmann et al. [6] consists of three tests: Tests 1 and 2 checked if a generated word is sampled from the top of the distribution, and Test 3 veri\ufb01ed if the system is overly con\ufb01dent in its next prediction due to familiarity with the previously generated context. A human-subject study found that GLTR improved the human detection rate of fake text from 54% to 72% without prior training. 2.2 Zero-Shot Classi\ufb01cation The second detection approach is by zero-shot classi\ufb01cation (i.e., using a pre-trained LLM to detect its own generation or that of a similar model). In the work by Solaiman et al. [4], a baseline method that used an LLM to evaluate the log-probability and the corresponding threshold for making classi\ufb01cation decisions was proposed. However, this zero-shot approach performs poorly compared to the statistical methods. 2.3 Fine-Tuning Language Model The last approach is to \ufb01ne-tune an existing language model. For example, Zeller et al. [7] \ufb01ne-tuned a linear layer to identify if the input was generated by the GROVER model or by a human, using the hidden states in the encoder of GROVER."}