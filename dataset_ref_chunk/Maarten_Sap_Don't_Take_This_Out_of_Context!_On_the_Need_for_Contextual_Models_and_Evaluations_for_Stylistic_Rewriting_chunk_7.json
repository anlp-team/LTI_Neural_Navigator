{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_Don't_Take_This_Out_of_Context!_On_the_Need_for_Contextual_Models_and_Evaluations_for_Stylistic_Rewriting_chunk_7.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the proposed CtxSimFit demonstrate according to the text?", "answer": " A stronger correlation with human judgments.", "ref_chunk": "better aligns with human judgments. While commonly-used automatic metrics enriched with context align with human preferences, our proposed CtxSimFit demonstrates a stronger correlation. Initial work in evaluating open-domain dialogue generation with context (Welleck et al., 2019; Pang et al., 2020) has been done, but we encourage fur- ther development of better contextualized metrics for stylistic rewriting evaluation. Improvements could include modeling themes, tones, sentence structures (Zhang et al., 2014; Khatri et al., 2018; Chen and Yang, 2020; Toubia et al., 2021; Shen et al., 2023), and social dynamics, and emotional states in conversations (Sap et al., 2017; Rashkin et al., 2018, 2019; Mostafazadeh et al., 2020). 9 Limitations & Ethical Considerations Despite taking the first step towards incorporating context into stylistic rewriting and its evaluation frameworks, there are several limitations and ethi- cal concerns, which we list below. Limited Context Scope In this study, our pri- mary focus is on incorporating textual context, particularly from preceding sentences or previous turns in a conversation. Future work should ex- plore how to incorporate other forms of context into rewriting models and evaluations, such as dis- course structure (Welleck et al., 2019), external knowledge (Ghazvininejad et al., 2018), or richer social and power dynamics (Antoniak et al., 2023), emotional states (Zhou et al., 2023), and commu- nicative intent (Zhou et al., 2022), all of which can significantly contribute to understanding the text. Amount of Context In our experiments, we opted to investigate the context of three preced- ing sentences in a document and one preceding conversational turn, considering only a specific length. However, the amount of context at the mod- eling and evaluation stages could also change the results. We hypothesize that more context could improve rewriting methods, but it could potentially also negatively impact contextual meaning preser- vation metrics. Future work should explore these effects of varying lengths of context. Broad Definition of Meaning Preservation While we have tried to define meaning preserva- tion as the preservation of an event or entity-level details and intended overall meaning, this defini- tion remains broad and subjective (Searle, 1975; Adolphs et al., 2022; Zhou et al., 2022). In this work, we do not delve into more intricate dimen- sions of meaning preservation, such as spatial and temporal accuracy, or the retention of cultural con- text, including references, nuances, and dialects. Applicability to Smaller Models Our work re- lies on few-shot prompting of LLMs to incorporate textual context, given their demonstrated strong rewriting capabilities both with and without textual context usage (Brown et al., 2020). Other exist- ing generative models, such as those used for chit- chat and goal-oriented conversational agents, as well as pretrained language models, have struggled with effectively utilizing preceding textual context (Sankar et al., 2019; O\u2019Connor and Andreas, 2021; Parthasarathi et al., 2021; Su et al., 2023). More- over, custom-made rewriting models from prior research often lack the modeling of context (Ma et al., 2020; Dale et al., 2021). We believe the our results still apply for smaller models, given some preliminary research (Cheng et al., 2020; Atwell et al., 2022) on an increased human preference for contextual rewrites from custom-trained seq2seq models. We encourage future work to thoroughly investigate strategies for effective modeling and evaluation of context in smaller models. Harms of Exposing Workers to Toxic Content In our work, we exposed human annotators to toxic content during the evaluation of the de-toxification task. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We aim to work towards developing evaluation strategies that can minimize the exposure of annotators to toxic content. Potentially Inconsistent Human Evaluations In our work, we also assume human judgments as the gold standard. Concurrent work has shown that human evaluation might not always be con- sistent (Clark et al., 2021; Karpinska et al., 2021); however human judgments continue to be the gold standard for evaluating open-ended text generation. Acknowledgements We would like to thank our workers on MTurk for their responses. We are also grateful to the anonymous reviewers for their helpful comments. Special thanks to Saadia Gabriel, Jocelyn Shen, Ashutosh Baheti, and the members of the CMU LTI COMEDY group for their feedback, and Ope- nAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fun- damental AI Research Laboratories (FAIR) \u201cDyn- abench Data Collection and Benchmarking Plat- form\u201d award \u201cContExTox: Context-Aware and Ex- plainable Toxicity Detection.\u201d References Tushar Abhishek, Daksh Rawat, Manish Gupta, and Transformer models arXiv preprint Vasudeva Varma. 2021. for text coherence assessment. arXiv:2109.02176. Leonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason Weston. 2022. Reason first, then respond: Modular generation for knowledge-infused dialogue. In Findings of the Association for Com- putational Linguistics: EMNLP 2022, pages 7112\u2013 7132. Maria Antoniak, Anjalie Field, Ji Min Mun, Melanie Walsh, Lauren F. Klein, and Maarten Sap. 2023. Riv- eter: Measuring power and social dynamics between entities. In ACL demonstrations. Katherine Atwell, Sabit Hassan, and Malihe Alikhani. 2022. Appdia: A discourse-aware transformer-based style transfer model for offensive social media con- versations. In Proceedings of the 29th International Conference on Computational Linguistics, pages 6063\u20136074. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza- tion, pages 65\u201372. Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. 2022. Xlm-t: Multilingual lan- guage models in twitter for sentiment analysis and beyond. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 258\u2013 266. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregres- sive language model. In Proceedings of BigScience Episode\\# 5\u2013Workshop on Challenges & Perspec- tives in Creating Large Language Models, pages 95\u2013 136. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,"}, {"question": " What are some improvements that could be made to contextualized metrics for stylistic rewriting evaluation?", "answer": " Modeling themes, tones, sentence structures, social dynamics, emotional states in conversations.", "ref_chunk": "better aligns with human judgments. While commonly-used automatic metrics enriched with context align with human preferences, our proposed CtxSimFit demonstrates a stronger correlation. Initial work in evaluating open-domain dialogue generation with context (Welleck et al., 2019; Pang et al., 2020) has been done, but we encourage fur- ther development of better contextualized metrics for stylistic rewriting evaluation. Improvements could include modeling themes, tones, sentence structures (Zhang et al., 2014; Khatri et al., 2018; Chen and Yang, 2020; Toubia et al., 2021; Shen et al., 2023), and social dynamics, and emotional states in conversations (Sap et al., 2017; Rashkin et al., 2018, 2019; Mostafazadeh et al., 2020). 9 Limitations & Ethical Considerations Despite taking the first step towards incorporating context into stylistic rewriting and its evaluation frameworks, there are several limitations and ethi- cal concerns, which we list below. Limited Context Scope In this study, our pri- mary focus is on incorporating textual context, particularly from preceding sentences or previous turns in a conversation. Future work should ex- plore how to incorporate other forms of context into rewriting models and evaluations, such as dis- course structure (Welleck et al., 2019), external knowledge (Ghazvininejad et al., 2018), or richer social and power dynamics (Antoniak et al., 2023), emotional states (Zhou et al., 2023), and commu- nicative intent (Zhou et al., 2022), all of which can significantly contribute to understanding the text. Amount of Context In our experiments, we opted to investigate the context of three preced- ing sentences in a document and one preceding conversational turn, considering only a specific length. However, the amount of context at the mod- eling and evaluation stages could also change the results. We hypothesize that more context could improve rewriting methods, but it could potentially also negatively impact contextual meaning preser- vation metrics. Future work should explore these effects of varying lengths of context. Broad Definition of Meaning Preservation While we have tried to define meaning preserva- tion as the preservation of an event or entity-level details and intended overall meaning, this defini- tion remains broad and subjective (Searle, 1975; Adolphs et al., 2022; Zhou et al., 2022). In this work, we do not delve into more intricate dimen- sions of meaning preservation, such as spatial and temporal accuracy, or the retention of cultural con- text, including references, nuances, and dialects. Applicability to Smaller Models Our work re- lies on few-shot prompting of LLMs to incorporate textual context, given their demonstrated strong rewriting capabilities both with and without textual context usage (Brown et al., 2020). Other exist- ing generative models, such as those used for chit- chat and goal-oriented conversational agents, as well as pretrained language models, have struggled with effectively utilizing preceding textual context (Sankar et al., 2019; O\u2019Connor and Andreas, 2021; Parthasarathi et al., 2021; Su et al., 2023). More- over, custom-made rewriting models from prior research often lack the modeling of context (Ma et al., 2020; Dale et al., 2021). We believe the our results still apply for smaller models, given some preliminary research (Cheng et al., 2020; Atwell et al., 2022) on an increased human preference for contextual rewrites from custom-trained seq2seq models. We encourage future work to thoroughly investigate strategies for effective modeling and evaluation of context in smaller models. Harms of Exposing Workers to Toxic Content In our work, we exposed human annotators to toxic content during the evaluation of the de-toxification task. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We aim to work towards developing evaluation strategies that can minimize the exposure of annotators to toxic content. Potentially Inconsistent Human Evaluations In our work, we also assume human judgments as the gold standard. Concurrent work has shown that human evaluation might not always be con- sistent (Clark et al., 2021; Karpinska et al., 2021); however human judgments continue to be the gold standard for evaluating open-ended text generation. Acknowledgements We would like to thank our workers on MTurk for their responses. We are also grateful to the anonymous reviewers for their helpful comments. Special thanks to Saadia Gabriel, Jocelyn Shen, Ashutosh Baheti, and the members of the CMU LTI COMEDY group for their feedback, and Ope- nAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fun- damental AI Research Laboratories (FAIR) \u201cDyn- abench Data Collection and Benchmarking Plat- form\u201d award \u201cContExTox: Context-Aware and Ex- plainable Toxicity Detection.\u201d References Tushar Abhishek, Daksh Rawat, Manish Gupta, and Transformer models arXiv preprint Vasudeva Varma. 2021. for text coherence assessment. arXiv:2109.02176. Leonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason Weston. 2022. Reason first, then respond: Modular generation for knowledge-infused dialogue. In Findings of the Association for Com- putational Linguistics: EMNLP 2022, pages 7112\u2013 7132. Maria Antoniak, Anjalie Field, Ji Min Mun, Melanie Walsh, Lauren F. Klein, and Maarten Sap. 2023. Riv- eter: Measuring power and social dynamics between entities. In ACL demonstrations. Katherine Atwell, Sabit Hassan, and Malihe Alikhani. 2022. Appdia: A discourse-aware transformer-based style transfer model for offensive social media con- versations. In Proceedings of the 29th International Conference on Computational Linguistics, pages 6063\u20136074. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza- tion, pages 65\u201372. Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. 2022. Xlm-t: Multilingual lan- guage models in twitter for sentiment analysis and beyond. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 258\u2013 266. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregres- sive language model. In Proceedings of BigScience Episode\\# 5\u2013Workshop on Challenges & Perspec- tives in Creating Large Language Models, pages 95\u2013 136. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,"}, {"question": " What is one limitation mentioned in the text regarding the context scope in evaluating stylistic rewriting?", "answer": " Limited focus on incorporating textual context from preceding sentences or previous turns in a conversation.", "ref_chunk": "better aligns with human judgments. While commonly-used automatic metrics enriched with context align with human preferences, our proposed CtxSimFit demonstrates a stronger correlation. Initial work in evaluating open-domain dialogue generation with context (Welleck et al., 2019; Pang et al., 2020) has been done, but we encourage fur- ther development of better contextualized metrics for stylistic rewriting evaluation. Improvements could include modeling themes, tones, sentence structures (Zhang et al., 2014; Khatri et al., 2018; Chen and Yang, 2020; Toubia et al., 2021; Shen et al., 2023), and social dynamics, and emotional states in conversations (Sap et al., 2017; Rashkin et al., 2018, 2019; Mostafazadeh et al., 2020). 9 Limitations & Ethical Considerations Despite taking the first step towards incorporating context into stylistic rewriting and its evaluation frameworks, there are several limitations and ethi- cal concerns, which we list below. Limited Context Scope In this study, our pri- mary focus is on incorporating textual context, particularly from preceding sentences or previous turns in a conversation. Future work should ex- plore how to incorporate other forms of context into rewriting models and evaluations, such as dis- course structure (Welleck et al., 2019), external knowledge (Ghazvininejad et al., 2018), or richer social and power dynamics (Antoniak et al., 2023), emotional states (Zhou et al., 2023), and commu- nicative intent (Zhou et al., 2022), all of which can significantly contribute to understanding the text. Amount of Context In our experiments, we opted to investigate the context of three preced- ing sentences in a document and one preceding conversational turn, considering only a specific length. However, the amount of context at the mod- eling and evaluation stages could also change the results. We hypothesize that more context could improve rewriting methods, but it could potentially also negatively impact contextual meaning preser- vation metrics. Future work should explore these effects of varying lengths of context. Broad Definition of Meaning Preservation While we have tried to define meaning preserva- tion as the preservation of an event or entity-level details and intended overall meaning, this defini- tion remains broad and subjective (Searle, 1975; Adolphs et al., 2022; Zhou et al., 2022). In this work, we do not delve into more intricate dimen- sions of meaning preservation, such as spatial and temporal accuracy, or the retention of cultural con- text, including references, nuances, and dialects. Applicability to Smaller Models Our work re- lies on few-shot prompting of LLMs to incorporate textual context, given their demonstrated strong rewriting capabilities both with and without textual context usage (Brown et al., 2020). Other exist- ing generative models, such as those used for chit- chat and goal-oriented conversational agents, as well as pretrained language models, have struggled with effectively utilizing preceding textual context (Sankar et al., 2019; O\u2019Connor and Andreas, 2021; Parthasarathi et al., 2021; Su et al., 2023). More- over, custom-made rewriting models from prior research often lack the modeling of context (Ma et al., 2020; Dale et al., 2021). We believe the our results still apply for smaller models, given some preliminary research (Cheng et al., 2020; Atwell et al., 2022) on an increased human preference for contextual rewrites from custom-trained seq2seq models. We encourage future work to thoroughly investigate strategies for effective modeling and evaluation of context in smaller models. Harms of Exposing Workers to Toxic Content In our work, we exposed human annotators to toxic content during the evaluation of the de-toxification task. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We aim to work towards developing evaluation strategies that can minimize the exposure of annotators to toxic content. Potentially Inconsistent Human Evaluations In our work, we also assume human judgments as the gold standard. Concurrent work has shown that human evaluation might not always be con- sistent (Clark et al., 2021; Karpinska et al., 2021); however human judgments continue to be the gold standard for evaluating open-ended text generation. Acknowledgements We would like to thank our workers on MTurk for their responses. We are also grateful to the anonymous reviewers for their helpful comments. Special thanks to Saadia Gabriel, Jocelyn Shen, Ashutosh Baheti, and the members of the CMU LTI COMEDY group for their feedback, and Ope- nAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fun- damental AI Research Laboratories (FAIR) \u201cDyn- abench Data Collection and Benchmarking Plat- form\u201d award \u201cContExTox: Context-Aware and Ex- plainable Toxicity Detection.\u201d References Tushar Abhishek, Daksh Rawat, Manish Gupta, and Transformer models arXiv preprint Vasudeva Varma. 2021. for text coherence assessment. arXiv:2109.02176. Leonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason Weston. 2022. Reason first, then respond: Modular generation for knowledge-infused dialogue. In Findings of the Association for Com- putational Linguistics: EMNLP 2022, pages 7112\u2013 7132. Maria Antoniak, Anjalie Field, Ji Min Mun, Melanie Walsh, Lauren F. Klein, and Maarten Sap. 2023. Riv- eter: Measuring power and social dynamics between entities. In ACL demonstrations. Katherine Atwell, Sabit Hassan, and Malihe Alikhani. 2022. Appdia: A discourse-aware transformer-based style transfer model for offensive social media con- versations. In Proceedings of the 29th International Conference on Computational Linguistics, pages 6063\u20136074. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza- tion, pages 65\u201372. Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. 2022. Xlm-t: Multilingual lan- guage models in twitter for sentiment analysis and beyond. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 258\u2013 266. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregres- sive language model. In Proceedings of BigScience Episode\\# 5\u2013Workshop on Challenges & Perspec- tives in Creating Large Language Models, pages 95\u2013 136. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,"}, {"question": " How many preceding sentences in a document were considered in the experiments mentioned in the text?", "answer": " Three.", "ref_chunk": "better aligns with human judgments. While commonly-used automatic metrics enriched with context align with human preferences, our proposed CtxSimFit demonstrates a stronger correlation. Initial work in evaluating open-domain dialogue generation with context (Welleck et al., 2019; Pang et al., 2020) has been done, but we encourage fur- ther development of better contextualized metrics for stylistic rewriting evaluation. Improvements could include modeling themes, tones, sentence structures (Zhang et al., 2014; Khatri et al., 2018; Chen and Yang, 2020; Toubia et al., 2021; Shen et al., 2023), and social dynamics, and emotional states in conversations (Sap et al., 2017; Rashkin et al., 2018, 2019; Mostafazadeh et al., 2020). 9 Limitations & Ethical Considerations Despite taking the first step towards incorporating context into stylistic rewriting and its evaluation frameworks, there are several limitations and ethi- cal concerns, which we list below. Limited Context Scope In this study, our pri- mary focus is on incorporating textual context, particularly from preceding sentences or previous turns in a conversation. Future work should ex- plore how to incorporate other forms of context into rewriting models and evaluations, such as dis- course structure (Welleck et al., 2019), external knowledge (Ghazvininejad et al., 2018), or richer social and power dynamics (Antoniak et al., 2023), emotional states (Zhou et al., 2023), and commu- nicative intent (Zhou et al., 2022), all of which can significantly contribute to understanding the text. Amount of Context In our experiments, we opted to investigate the context of three preced- ing sentences in a document and one preceding conversational turn, considering only a specific length. However, the amount of context at the mod- eling and evaluation stages could also change the results. We hypothesize that more context could improve rewriting methods, but it could potentially also negatively impact contextual meaning preser- vation metrics. Future work should explore these effects of varying lengths of context. Broad Definition of Meaning Preservation While we have tried to define meaning preserva- tion as the preservation of an event or entity-level details and intended overall meaning, this defini- tion remains broad and subjective (Searle, 1975; Adolphs et al., 2022; Zhou et al., 2022). In this work, we do not delve into more intricate dimen- sions of meaning preservation, such as spatial and temporal accuracy, or the retention of cultural con- text, including references, nuances, and dialects. Applicability to Smaller Models Our work re- lies on few-shot prompting of LLMs to incorporate textual context, given their demonstrated strong rewriting capabilities both with and without textual context usage (Brown et al., 2020). Other exist- ing generative models, such as those used for chit- chat and goal-oriented conversational agents, as well as pretrained language models, have struggled with effectively utilizing preceding textual context (Sankar et al., 2019; O\u2019Connor and Andreas, 2021; Parthasarathi et al., 2021; Su et al., 2023). More- over, custom-made rewriting models from prior research often lack the modeling of context (Ma et al., 2020; Dale et al., 2021). We believe the our results still apply for smaller models, given some preliminary research (Cheng et al., 2020; Atwell et al., 2022) on an increased human preference for contextual rewrites from custom-trained seq2seq models. We encourage future work to thoroughly investigate strategies for effective modeling and evaluation of context in smaller models. Harms of Exposing Workers to Toxic Content In our work, we exposed human annotators to toxic content during the evaluation of the de-toxification task. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We aim to work towards developing evaluation strategies that can minimize the exposure of annotators to toxic content. Potentially Inconsistent Human Evaluations In our work, we also assume human judgments as the gold standard. Concurrent work has shown that human evaluation might not always be con- sistent (Clark et al., 2021; Karpinska et al., 2021); however human judgments continue to be the gold standard for evaluating open-ended text generation. Acknowledgements We would like to thank our workers on MTurk for their responses. We are also grateful to the anonymous reviewers for their helpful comments. Special thanks to Saadia Gabriel, Jocelyn Shen, Ashutosh Baheti, and the members of the CMU LTI COMEDY group for their feedback, and Ope- nAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fun- damental AI Research Laboratories (FAIR) \u201cDyn- abench Data Collection and Benchmarking Plat- form\u201d award \u201cContExTox: Context-Aware and Ex- plainable Toxicity Detection.\u201d References Tushar Abhishek, Daksh Rawat, Manish Gupta, and Transformer models arXiv preprint Vasudeva Varma. 2021. for text coherence assessment. arXiv:2109.02176. Leonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason Weston. 2022. Reason first, then respond: Modular generation for knowledge-infused dialogue. In Findings of the Association for Com- putational Linguistics: EMNLP 2022, pages 7112\u2013 7132. Maria Antoniak, Anjalie Field, Ji Min Mun, Melanie Walsh, Lauren F. Klein, and Maarten Sap. 2023. Riv- eter: Measuring power and social dynamics between entities. In ACL demonstrations. Katherine Atwell, Sabit Hassan, and Malihe Alikhani. 2022. Appdia: A discourse-aware transformer-based style transfer model for offensive social media con- versations. In Proceedings of the 29th International Conference on Computational Linguistics, pages 6063\u20136074. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza- tion, pages 65\u201372. Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. 2022. Xlm-t: Multilingual lan- guage models in twitter for sentiment analysis and beyond. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 258\u2013 266. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregres- sive language model. In Proceedings of BigScience Episode\\# 5\u2013Workshop on Challenges & Perspec- tives in Creating Large Language Models, pages 95\u2013 136. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,"}, {"question": " What type of modeling models did the work in the text rely on for incorporating textual context?", "answer": " Few-shot prompting of LLMs (Large Language Models).", "ref_chunk": "better aligns with human judgments. While commonly-used automatic metrics enriched with context align with human preferences, our proposed CtxSimFit demonstrates a stronger correlation. Initial work in evaluating open-domain dialogue generation with context (Welleck et al., 2019; Pang et al., 2020) has been done, but we encourage fur- ther development of better contextualized metrics for stylistic rewriting evaluation. Improvements could include modeling themes, tones, sentence structures (Zhang et al., 2014; Khatri et al., 2018; Chen and Yang, 2020; Toubia et al., 2021; Shen et al., 2023), and social dynamics, and emotional states in conversations (Sap et al., 2017; Rashkin et al., 2018, 2019; Mostafazadeh et al., 2020). 9 Limitations & Ethical Considerations Despite taking the first step towards incorporating context into stylistic rewriting and its evaluation frameworks, there are several limitations and ethi- cal concerns, which we list below. Limited Context Scope In this study, our pri- mary focus is on incorporating textual context, particularly from preceding sentences or previous turns in a conversation. Future work should ex- plore how to incorporate other forms of context into rewriting models and evaluations, such as dis- course structure (Welleck et al., 2019), external knowledge (Ghazvininejad et al., 2018), or richer social and power dynamics (Antoniak et al., 2023), emotional states (Zhou et al., 2023), and commu- nicative intent (Zhou et al., 2022), all of which can significantly contribute to understanding the text. Amount of Context In our experiments, we opted to investigate the context of three preced- ing sentences in a document and one preceding conversational turn, considering only a specific length. However, the amount of context at the mod- eling and evaluation stages could also change the results. We hypothesize that more context could improve rewriting methods, but it could potentially also negatively impact contextual meaning preser- vation metrics. Future work should explore these effects of varying lengths of context. Broad Definition of Meaning Preservation While we have tried to define meaning preserva- tion as the preservation of an event or entity-level details and intended overall meaning, this defini- tion remains broad and subjective (Searle, 1975; Adolphs et al., 2022; Zhou et al., 2022). In this work, we do not delve into more intricate dimen- sions of meaning preservation, such as spatial and temporal accuracy, or the retention of cultural con- text, including references, nuances, and dialects. Applicability to Smaller Models Our work re- lies on few-shot prompting of LLMs to incorporate textual context, given their demonstrated strong rewriting capabilities both with and without textual context usage (Brown et al., 2020). Other exist- ing generative models, such as those used for chit- chat and goal-oriented conversational agents, as well as pretrained language models, have struggled with effectively utilizing preceding textual context (Sankar et al., 2019; O\u2019Connor and Andreas, 2021; Parthasarathi et al., 2021; Su et al., 2023). More- over, custom-made rewriting models from prior research often lack the modeling of context (Ma et al., 2020; Dale et al., 2021). We believe the our results still apply for smaller models, given some preliminary research (Cheng et al., 2020; Atwell et al., 2022) on an increased human preference for contextual rewrites from custom-trained seq2seq models. We encourage future work to thoroughly investigate strategies for effective modeling and evaluation of context in smaller models. Harms of Exposing Workers to Toxic Content In our work, we exposed human annotators to toxic content during the evaluation of the de-toxification task. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We aim to work towards developing evaluation strategies that can minimize the exposure of annotators to toxic content. Potentially Inconsistent Human Evaluations In our work, we also assume human judgments as the gold standard. Concurrent work has shown that human evaluation might not always be con- sistent (Clark et al., 2021; Karpinska et al., 2021); however human judgments continue to be the gold standard for evaluating open-ended text generation. Acknowledgements We would like to thank our workers on MTurk for their responses. We are also grateful to the anonymous reviewers for their helpful comments. Special thanks to Saadia Gabriel, Jocelyn Shen, Ashutosh Baheti, and the members of the CMU LTI COMEDY group for their feedback, and Ope- nAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fun- damental AI Research Laboratories (FAIR) \u201cDyn- abench Data Collection and Benchmarking Plat- form\u201d award \u201cContExTox: Context-Aware and Ex- plainable Toxicity Detection.\u201d References Tushar Abhishek, Daksh Rawat, Manish Gupta, and Transformer models arXiv preprint Vasudeva Varma. 2021. for text coherence assessment. arXiv:2109.02176. Leonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason Weston. 2022. Reason first, then respond: Modular generation for knowledge-infused dialogue. In Findings of the Association for Com- putational Linguistics: EMNLP 2022, pages 7112\u2013 7132. Maria Antoniak, Anjalie Field, Ji Min Mun, Melanie Walsh, Lauren F. Klein, and Maarten Sap. 2023. Riv- eter: Measuring power and social dynamics between entities. In ACL demonstrations. Katherine Atwell, Sabit Hassan, and Malihe Alikhani. 2022. Appdia: A discourse-aware transformer-based style transfer model for offensive social media con- versations. In Proceedings of the 29th International Conference on Computational Linguistics, pages 6063\u20136074. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza- tion, pages 65\u201372. Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. 2022. Xlm-t: Multilingual lan- guage models in twitter for sentiment analysis and beyond. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 258\u2013 266. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregres- sive language model. In Proceedings of BigScience Episode\\# 5\u2013Workshop on Challenges & Perspec- tives in Creating Large Language Models, pages 95\u2013 136. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,"}, {"question": " According to the text, what possible downside could more context have on rewriting methods?", "answer": " Negatively impact contextual meaning preservation metrics.", "ref_chunk": "better aligns with human judgments. While commonly-used automatic metrics enriched with context align with human preferences, our proposed CtxSimFit demonstrates a stronger correlation. Initial work in evaluating open-domain dialogue generation with context (Welleck et al., 2019; Pang et al., 2020) has been done, but we encourage fur- ther development of better contextualized metrics for stylistic rewriting evaluation. Improvements could include modeling themes, tones, sentence structures (Zhang et al., 2014; Khatri et al., 2018; Chen and Yang, 2020; Toubia et al., 2021; Shen et al., 2023), and social dynamics, and emotional states in conversations (Sap et al., 2017; Rashkin et al., 2018, 2019; Mostafazadeh et al., 2020). 9 Limitations & Ethical Considerations Despite taking the first step towards incorporating context into stylistic rewriting and its evaluation frameworks, there are several limitations and ethi- cal concerns, which we list below. Limited Context Scope In this study, our pri- mary focus is on incorporating textual context, particularly from preceding sentences or previous turns in a conversation. Future work should ex- plore how to incorporate other forms of context into rewriting models and evaluations, such as dis- course structure (Welleck et al., 2019), external knowledge (Ghazvininejad et al., 2018), or richer social and power dynamics (Antoniak et al., 2023), emotional states (Zhou et al., 2023), and commu- nicative intent (Zhou et al., 2022), all of which can significantly contribute to understanding the text. Amount of Context In our experiments, we opted to investigate the context of three preced- ing sentences in a document and one preceding conversational turn, considering only a specific length. However, the amount of context at the mod- eling and evaluation stages could also change the results. We hypothesize that more context could improve rewriting methods, but it could potentially also negatively impact contextual meaning preser- vation metrics. Future work should explore these effects of varying lengths of context. Broad Definition of Meaning Preservation While we have tried to define meaning preserva- tion as the preservation of an event or entity-level details and intended overall meaning, this defini- tion remains broad and subjective (Searle, 1975; Adolphs et al., 2022; Zhou et al., 2022). In this work, we do not delve into more intricate dimen- sions of meaning preservation, such as spatial and temporal accuracy, or the retention of cultural con- text, including references, nuances, and dialects. Applicability to Smaller Models Our work re- lies on few-shot prompting of LLMs to incorporate textual context, given their demonstrated strong rewriting capabilities both with and without textual context usage (Brown et al., 2020). Other exist- ing generative models, such as those used for chit- chat and goal-oriented conversational agents, as well as pretrained language models, have struggled with effectively utilizing preceding textual context (Sankar et al., 2019; O\u2019Connor and Andreas, 2021; Parthasarathi et al., 2021; Su et al., 2023). More- over, custom-made rewriting models from prior research often lack the modeling of context (Ma et al., 2020; Dale et al., 2021). We believe the our results still apply for smaller models, given some preliminary research (Cheng et al., 2020; Atwell et al., 2022) on an increased human preference for contextual rewrites from custom-trained seq2seq models. We encourage future work to thoroughly investigate strategies for effective modeling and evaluation of context in smaller models. Harms of Exposing Workers to Toxic Content In our work, we exposed human annotators to toxic content during the evaluation of the de-toxification task. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We aim to work towards developing evaluation strategies that can minimize the exposure of annotators to toxic content. Potentially Inconsistent Human Evaluations In our work, we also assume human judgments as the gold standard. Concurrent work has shown that human evaluation might not always be con- sistent (Clark et al., 2021; Karpinska et al., 2021); however human judgments continue to be the gold standard for evaluating open-ended text generation. Acknowledgements We would like to thank our workers on MTurk for their responses. We are also grateful to the anonymous reviewers for their helpful comments. Special thanks to Saadia Gabriel, Jocelyn Shen, Ashutosh Baheti, and the members of the CMU LTI COMEDY group for their feedback, and Ope- nAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fun- damental AI Research Laboratories (FAIR) \u201cDyn- abench Data Collection and Benchmarking Plat- form\u201d award \u201cContExTox: Context-Aware and Ex- plainable Toxicity Detection.\u201d References Tushar Abhishek, Daksh Rawat, Manish Gupta, and Transformer models arXiv preprint Vasudeva Varma. 2021. for text coherence assessment. arXiv:2109.02176. Leonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason Weston. 2022. Reason first, then respond: Modular generation for knowledge-infused dialogue. In Findings of the Association for Com- putational Linguistics: EMNLP 2022, pages 7112\u2013 7132. Maria Antoniak, Anjalie Field, Ji Min Mun, Melanie Walsh, Lauren F. Klein, and Maarten Sap. 2023. Riv- eter: Measuring power and social dynamics between entities. In ACL demonstrations. Katherine Atwell, Sabit Hassan, and Malihe Alikhani. 2022. Appdia: A discourse-aware transformer-based style transfer model for offensive social media con- versations. In Proceedings of the 29th International Conference on Computational Linguistics, pages 6063\u20136074. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza- tion, pages 65\u201372. Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. 2022. Xlm-t: Multilingual lan- guage models in twitter for sentiment analysis and beyond. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 258\u2013 266. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregres- sive language model. In Proceedings of BigScience Episode\\# 5\u2013Workshop on Challenges & Perspec- tives in Creating Large Language Models, pages 95\u2013 136. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,"}, {"question": " What are some aspects of meaning preservation that the text did not delve into?", "answer": " Spatial and temporal accuracy, retention of cultural context.", "ref_chunk": "better aligns with human judgments. While commonly-used automatic metrics enriched with context align with human preferences, our proposed CtxSimFit demonstrates a stronger correlation. Initial work in evaluating open-domain dialogue generation with context (Welleck et al., 2019; Pang et al., 2020) has been done, but we encourage fur- ther development of better contextualized metrics for stylistic rewriting evaluation. Improvements could include modeling themes, tones, sentence structures (Zhang et al., 2014; Khatri et al., 2018; Chen and Yang, 2020; Toubia et al., 2021; Shen et al., 2023), and social dynamics, and emotional states in conversations (Sap et al., 2017; Rashkin et al., 2018, 2019; Mostafazadeh et al., 2020). 9 Limitations & Ethical Considerations Despite taking the first step towards incorporating context into stylistic rewriting and its evaluation frameworks, there are several limitations and ethi- cal concerns, which we list below. Limited Context Scope In this study, our pri- mary focus is on incorporating textual context, particularly from preceding sentences or previous turns in a conversation. Future work should ex- plore how to incorporate other forms of context into rewriting models and evaluations, such as dis- course structure (Welleck et al., 2019), external knowledge (Ghazvininejad et al., 2018), or richer social and power dynamics (Antoniak et al., 2023), emotional states (Zhou et al., 2023), and commu- nicative intent (Zhou et al., 2022), all of which can significantly contribute to understanding the text. Amount of Context In our experiments, we opted to investigate the context of three preced- ing sentences in a document and one preceding conversational turn, considering only a specific length. However, the amount of context at the mod- eling and evaluation stages could also change the results. We hypothesize that more context could improve rewriting methods, but it could potentially also negatively impact contextual meaning preser- vation metrics. Future work should explore these effects of varying lengths of context. Broad Definition of Meaning Preservation While we have tried to define meaning preserva- tion as the preservation of an event or entity-level details and intended overall meaning, this defini- tion remains broad and subjective (Searle, 1975; Adolphs et al., 2022; Zhou et al., 2022). In this work, we do not delve into more intricate dimen- sions of meaning preservation, such as spatial and temporal accuracy, or the retention of cultural con- text, including references, nuances, and dialects. Applicability to Smaller Models Our work re- lies on few-shot prompting of LLMs to incorporate textual context, given their demonstrated strong rewriting capabilities both with and without textual context usage (Brown et al., 2020). Other exist- ing generative models, such as those used for chit- chat and goal-oriented conversational agents, as well as pretrained language models, have struggled with effectively utilizing preceding textual context (Sankar et al., 2019; O\u2019Connor and Andreas, 2021; Parthasarathi et al., 2021; Su et al., 2023). More- over, custom-made rewriting models from prior research often lack the modeling of context (Ma et al., 2020; Dale et al., 2021). We believe the our results still apply for smaller models, given some preliminary research (Cheng et al., 2020; Atwell et al., 2022) on an increased human preference for contextual rewrites from custom-trained seq2seq models. We encourage future work to thoroughly investigate strategies for effective modeling and evaluation of context in smaller models. Harms of Exposing Workers to Toxic Content In our work, we exposed human annotators to toxic content during the evaluation of the de-toxification task. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We aim to work towards developing evaluation strategies that can minimize the exposure of annotators to toxic content. Potentially Inconsistent Human Evaluations In our work, we also assume human judgments as the gold standard. Concurrent work has shown that human evaluation might not always be con- sistent (Clark et al., 2021; Karpinska et al., 2021); however human judgments continue to be the gold standard for evaluating open-ended text generation. Acknowledgements We would like to thank our workers on MTurk for their responses. We are also grateful to the anonymous reviewers for their helpful comments. Special thanks to Saadia Gabriel, Jocelyn Shen, Ashutosh Baheti, and the members of the CMU LTI COMEDY group for their feedback, and Ope- nAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fun- damental AI Research Laboratories (FAIR) \u201cDyn- abench Data Collection and Benchmarking Plat- form\u201d award \u201cContExTox: Context-Aware and Ex- plainable Toxicity Detection.\u201d References Tushar Abhishek, Daksh Rawat, Manish Gupta, and Transformer models arXiv preprint Vasudeva Varma. 2021. for text coherence assessment. arXiv:2109.02176. Leonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason Weston. 2022. Reason first, then respond: Modular generation for knowledge-infused dialogue. In Findings of the Association for Com- putational Linguistics: EMNLP 2022, pages 7112\u2013 7132. Maria Antoniak, Anjalie Field, Ji Min Mun, Melanie Walsh, Lauren F. Klein, and Maarten Sap. 2023. Riv- eter: Measuring power and social dynamics between entities. In ACL demonstrations. Katherine Atwell, Sabit Hassan, and Malihe Alikhani. 2022. Appdia: A discourse-aware transformer-based style transfer model for offensive social media con- versations. In Proceedings of the 29th International Conference on Computational Linguistics, pages 6063\u20136074. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza- tion, pages 65\u201372. Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. 2022. Xlm-t: Multilingual lan- guage models in twitter for sentiment analysis and beyond. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 258\u2013 266. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregres- sive language model. In Proceedings of BigScience Episode\\# 5\u2013Workshop on Challenges & Perspec- tives in Creating Large Language Models, pages 95\u2013 136. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,"}, {"question": " Why is it mentioned that human annotators were exposed to toxic content in the evaluation task?", "answer": " To evaluate the de-toxification task.", "ref_chunk": "better aligns with human judgments. While commonly-used automatic metrics enriched with context align with human preferences, our proposed CtxSimFit demonstrates a stronger correlation. Initial work in evaluating open-domain dialogue generation with context (Welleck et al., 2019; Pang et al., 2020) has been done, but we encourage fur- ther development of better contextualized metrics for stylistic rewriting evaluation. Improvements could include modeling themes, tones, sentence structures (Zhang et al., 2014; Khatri et al., 2018; Chen and Yang, 2020; Toubia et al., 2021; Shen et al., 2023), and social dynamics, and emotional states in conversations (Sap et al., 2017; Rashkin et al., 2018, 2019; Mostafazadeh et al., 2020). 9 Limitations & Ethical Considerations Despite taking the first step towards incorporating context into stylistic rewriting and its evaluation frameworks, there are several limitations and ethi- cal concerns, which we list below. Limited Context Scope In this study, our pri- mary focus is on incorporating textual context, particularly from preceding sentences or previous turns in a conversation. Future work should ex- plore how to incorporate other forms of context into rewriting models and evaluations, such as dis- course structure (Welleck et al., 2019), external knowledge (Ghazvininejad et al., 2018), or richer social and power dynamics (Antoniak et al., 2023), emotional states (Zhou et al., 2023), and commu- nicative intent (Zhou et al., 2022), all of which can significantly contribute to understanding the text. Amount of Context In our experiments, we opted to investigate the context of three preced- ing sentences in a document and one preceding conversational turn, considering only a specific length. However, the amount of context at the mod- eling and evaluation stages could also change the results. We hypothesize that more context could improve rewriting methods, but it could potentially also negatively impact contextual meaning preser- vation metrics. Future work should explore these effects of varying lengths of context. Broad Definition of Meaning Preservation While we have tried to define meaning preserva- tion as the preservation of an event or entity-level details and intended overall meaning, this defini- tion remains broad and subjective (Searle, 1975; Adolphs et al., 2022; Zhou et al., 2022). In this work, we do not delve into more intricate dimen- sions of meaning preservation, such as spatial and temporal accuracy, or the retention of cultural con- text, including references, nuances, and dialects. Applicability to Smaller Models Our work re- lies on few-shot prompting of LLMs to incorporate textual context, given their demonstrated strong rewriting capabilities both with and without textual context usage (Brown et al., 2020). Other exist- ing generative models, such as those used for chit- chat and goal-oriented conversational agents, as well as pretrained language models, have struggled with effectively utilizing preceding textual context (Sankar et al., 2019; O\u2019Connor and Andreas, 2021; Parthasarathi et al., 2021; Su et al., 2023). More- over, custom-made rewriting models from prior research often lack the modeling of context (Ma et al., 2020; Dale et al., 2021). We believe the our results still apply for smaller models, given some preliminary research (Cheng et al., 2020; Atwell et al., 2022) on an increased human preference for contextual rewrites from custom-trained seq2seq models. We encourage future work to thoroughly investigate strategies for effective modeling and evaluation of context in smaller models. Harms of Exposing Workers to Toxic Content In our work, we exposed human annotators to toxic content during the evaluation of the de-toxification task. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We aim to work towards developing evaluation strategies that can minimize the exposure of annotators to toxic content. Potentially Inconsistent Human Evaluations In our work, we also assume human judgments as the gold standard. Concurrent work has shown that human evaluation might not always be con- sistent (Clark et al., 2021; Karpinska et al., 2021); however human judgments continue to be the gold standard for evaluating open-ended text generation. Acknowledgements We would like to thank our workers on MTurk for their responses. We are also grateful to the anonymous reviewers for their helpful comments. Special thanks to Saadia Gabriel, Jocelyn Shen, Ashutosh Baheti, and the members of the CMU LTI COMEDY group for their feedback, and Ope- nAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fun- damental AI Research Laboratories (FAIR) \u201cDyn- abench Data Collection and Benchmarking Plat- form\u201d award \u201cContExTox: Context-Aware and Ex- plainable Toxicity Detection.\u201d References Tushar Abhishek, Daksh Rawat, Manish Gupta, and Transformer models arXiv preprint Vasudeva Varma. 2021. for text coherence assessment. arXiv:2109.02176. Leonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason Weston. 2022. Reason first, then respond: Modular generation for knowledge-infused dialogue. In Findings of the Association for Com- putational Linguistics: EMNLP 2022, pages 7112\u2013 7132. Maria Antoniak, Anjalie Field, Ji Min Mun, Melanie Walsh, Lauren F. Klein, and Maarten Sap. 2023. Riv- eter: Measuring power and social dynamics between entities. In ACL demonstrations. Katherine Atwell, Sabit Hassan, and Malihe Alikhani. 2022. Appdia: A discourse-aware transformer-based style transfer model for offensive social media con- versations. In Proceedings of the 29th International Conference on Computational Linguistics, pages 6063\u20136074. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza- tion, pages 65\u201372. Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. 2022. Xlm-t: Multilingual lan- guage models in twitter for sentiment analysis and beyond. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 258\u2013 266. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregres- sive language model. In Proceedings of BigScience Episode\\# 5\u2013Workshop on Challenges & Perspec- tives in Creating Large Language Models, pages 95\u2013 136. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,"}, {"question": " What are the potential harms of exposing workers to toxic content, as stated in the text?", "answer": " Exposure to offensive content can be harmful to the annotators.", "ref_chunk": "better aligns with human judgments. While commonly-used automatic metrics enriched with context align with human preferences, our proposed CtxSimFit demonstrates a stronger correlation. Initial work in evaluating open-domain dialogue generation with context (Welleck et al., 2019; Pang et al., 2020) has been done, but we encourage fur- ther development of better contextualized metrics for stylistic rewriting evaluation. Improvements could include modeling themes, tones, sentence structures (Zhang et al., 2014; Khatri et al., 2018; Chen and Yang, 2020; Toubia et al., 2021; Shen et al., 2023), and social dynamics, and emotional states in conversations (Sap et al., 2017; Rashkin et al., 2018, 2019; Mostafazadeh et al., 2020). 9 Limitations & Ethical Considerations Despite taking the first step towards incorporating context into stylistic rewriting and its evaluation frameworks, there are several limitations and ethi- cal concerns, which we list below. Limited Context Scope In this study, our pri- mary focus is on incorporating textual context, particularly from preceding sentences or previous turns in a conversation. Future work should ex- plore how to incorporate other forms of context into rewriting models and evaluations, such as dis- course structure (Welleck et al., 2019), external knowledge (Ghazvininejad et al., 2018), or richer social and power dynamics (Antoniak et al., 2023), emotional states (Zhou et al., 2023), and commu- nicative intent (Zhou et al., 2022), all of which can significantly contribute to understanding the text. Amount of Context In our experiments, we opted to investigate the context of three preced- ing sentences in a document and one preceding conversational turn, considering only a specific length. However, the amount of context at the mod- eling and evaluation stages could also change the results. We hypothesize that more context could improve rewriting methods, but it could potentially also negatively impact contextual meaning preser- vation metrics. Future work should explore these effects of varying lengths of context. Broad Definition of Meaning Preservation While we have tried to define meaning preserva- tion as the preservation of an event or entity-level details and intended overall meaning, this defini- tion remains broad and subjective (Searle, 1975; Adolphs et al., 2022; Zhou et al., 2022). In this work, we do not delve into more intricate dimen- sions of meaning preservation, such as spatial and temporal accuracy, or the retention of cultural con- text, including references, nuances, and dialects. Applicability to Smaller Models Our work re- lies on few-shot prompting of LLMs to incorporate textual context, given their demonstrated strong rewriting capabilities both with and without textual context usage (Brown et al., 2020). Other exist- ing generative models, such as those used for chit- chat and goal-oriented conversational agents, as well as pretrained language models, have struggled with effectively utilizing preceding textual context (Sankar et al., 2019; O\u2019Connor and Andreas, 2021; Parthasarathi et al., 2021; Su et al., 2023). More- over, custom-made rewriting models from prior research often lack the modeling of context (Ma et al., 2020; Dale et al., 2021). We believe the our results still apply for smaller models, given some preliminary research (Cheng et al., 2020; Atwell et al., 2022) on an increased human preference for contextual rewrites from custom-trained seq2seq models. We encourage future work to thoroughly investigate strategies for effective modeling and evaluation of context in smaller models. Harms of Exposing Workers to Toxic Content In our work, we exposed human annotators to toxic content during the evaluation of the de-toxification task. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We aim to work towards developing evaluation strategies that can minimize the exposure of annotators to toxic content. Potentially Inconsistent Human Evaluations In our work, we also assume human judgments as the gold standard. Concurrent work has shown that human evaluation might not always be con- sistent (Clark et al., 2021; Karpinska et al., 2021); however human judgments continue to be the gold standard for evaluating open-ended text generation. Acknowledgements We would like to thank our workers on MTurk for their responses. We are also grateful to the anonymous reviewers for their helpful comments. Special thanks to Saadia Gabriel, Jocelyn Shen, Ashutosh Baheti, and the members of the CMU LTI COMEDY group for their feedback, and Ope- nAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fun- damental AI Research Laboratories (FAIR) \u201cDyn- abench Data Collection and Benchmarking Plat- form\u201d award \u201cContExTox: Context-Aware and Ex- plainable Toxicity Detection.\u201d References Tushar Abhishek, Daksh Rawat, Manish Gupta, and Transformer models arXiv preprint Vasudeva Varma. 2021. for text coherence assessment. arXiv:2109.02176. Leonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason Weston. 2022. Reason first, then respond: Modular generation for knowledge-infused dialogue. In Findings of the Association for Com- putational Linguistics: EMNLP 2022, pages 7112\u2013 7132. Maria Antoniak, Anjalie Field, Ji Min Mun, Melanie Walsh, Lauren F. Klein, and Maarten Sap. 2023. Riv- eter: Measuring power and social dynamics between entities. In ACL demonstrations. Katherine Atwell, Sabit Hassan, and Malihe Alikhani. 2022. Appdia: A discourse-aware transformer-based style transfer model for offensive social media con- versations. In Proceedings of the 29th International Conference on Computational Linguistics, pages 6063\u20136074. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza- tion, pages 65\u201372. Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. 2022. Xlm-t: Multilingual lan- guage models in twitter for sentiment analysis and beyond. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 258\u2013 266. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregres- sive language model. In Proceedings of BigScience Episode\\# 5\u2013Workshop on Challenges & Perspec- tives in Creating Large Language Models, pages 95\u2013 136. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,"}, {"question": " According to the text, why does the work assume human judgments as the gold standard for evaluation?", "answer": " Human judgments continue to be the gold standard for evaluating open-ended text generation.", "ref_chunk": "better aligns with human judgments. While commonly-used automatic metrics enriched with context align with human preferences, our proposed CtxSimFit demonstrates a stronger correlation. Initial work in evaluating open-domain dialogue generation with context (Welleck et al., 2019; Pang et al., 2020) has been done, but we encourage fur- ther development of better contextualized metrics for stylistic rewriting evaluation. Improvements could include modeling themes, tones, sentence structures (Zhang et al., 2014; Khatri et al., 2018; Chen and Yang, 2020; Toubia et al., 2021; Shen et al., 2023), and social dynamics, and emotional states in conversations (Sap et al., 2017; Rashkin et al., 2018, 2019; Mostafazadeh et al., 2020). 9 Limitations & Ethical Considerations Despite taking the first step towards incorporating context into stylistic rewriting and its evaluation frameworks, there are several limitations and ethi- cal concerns, which we list below. Limited Context Scope In this study, our pri- mary focus is on incorporating textual context, particularly from preceding sentences or previous turns in a conversation. Future work should ex- plore how to incorporate other forms of context into rewriting models and evaluations, such as dis- course structure (Welleck et al., 2019), external knowledge (Ghazvininejad et al., 2018), or richer social and power dynamics (Antoniak et al., 2023), emotional states (Zhou et al., 2023), and commu- nicative intent (Zhou et al., 2022), all of which can significantly contribute to understanding the text. Amount of Context In our experiments, we opted to investigate the context of three preced- ing sentences in a document and one preceding conversational turn, considering only a specific length. However, the amount of context at the mod- eling and evaluation stages could also change the results. We hypothesize that more context could improve rewriting methods, but it could potentially also negatively impact contextual meaning preser- vation metrics. Future work should explore these effects of varying lengths of context. Broad Definition of Meaning Preservation While we have tried to define meaning preserva- tion as the preservation of an event or entity-level details and intended overall meaning, this defini- tion remains broad and subjective (Searle, 1975; Adolphs et al., 2022; Zhou et al., 2022). In this work, we do not delve into more intricate dimen- sions of meaning preservation, such as spatial and temporal accuracy, or the retention of cultural con- text, including references, nuances, and dialects. Applicability to Smaller Models Our work re- lies on few-shot prompting of LLMs to incorporate textual context, given their demonstrated strong rewriting capabilities both with and without textual context usage (Brown et al., 2020). Other exist- ing generative models, such as those used for chit- chat and goal-oriented conversational agents, as well as pretrained language models, have struggled with effectively utilizing preceding textual context (Sankar et al., 2019; O\u2019Connor and Andreas, 2021; Parthasarathi et al., 2021; Su et al., 2023). More- over, custom-made rewriting models from prior research often lack the modeling of context (Ma et al., 2020; Dale et al., 2021). We believe the our results still apply for smaller models, given some preliminary research (Cheng et al., 2020; Atwell et al., 2022) on an increased human preference for contextual rewrites from custom-trained seq2seq models. We encourage future work to thoroughly investigate strategies for effective modeling and evaluation of context in smaller models. Harms of Exposing Workers to Toxic Content In our work, we exposed human annotators to toxic content during the evaluation of the de-toxification task. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We aim to work towards developing evaluation strategies that can minimize the exposure of annotators to toxic content. Potentially Inconsistent Human Evaluations In our work, we also assume human judgments as the gold standard. Concurrent work has shown that human evaluation might not always be con- sistent (Clark et al., 2021; Karpinska et al., 2021); however human judgments continue to be the gold standard for evaluating open-ended text generation. Acknowledgements We would like to thank our workers on MTurk for their responses. We are also grateful to the anonymous reviewers for their helpful comments. Special thanks to Saadia Gabriel, Jocelyn Shen, Ashutosh Baheti, and the members of the CMU LTI COMEDY group for their feedback, and Ope- nAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fun- damental AI Research Laboratories (FAIR) \u201cDyn- abench Data Collection and Benchmarking Plat- form\u201d award \u201cContExTox: Context-Aware and Ex- plainable Toxicity Detection.\u201d References Tushar Abhishek, Daksh Rawat, Manish Gupta, and Transformer models arXiv preprint Vasudeva Varma. 2021. for text coherence assessment. arXiv:2109.02176. Leonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason Weston. 2022. Reason first, then respond: Modular generation for knowledge-infused dialogue. In Findings of the Association for Com- putational Linguistics: EMNLP 2022, pages 7112\u2013 7132. Maria Antoniak, Anjalie Field, Ji Min Mun, Melanie Walsh, Lauren F. Klein, and Maarten Sap. 2023. Riv- eter: Measuring power and social dynamics between entities. In ACL demonstrations. Katherine Atwell, Sabit Hassan, and Malihe Alikhani. 2022. Appdia: A discourse-aware transformer-based style transfer model for offensive social media con- versations. In Proceedings of the 29th International Conference on Computational Linguistics, pages 6063\u20136074. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza- tion, pages 65\u201372. Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. 2022. Xlm-t: Multilingual lan- guage models in twitter for sentiment analysis and beyond. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 258\u2013 266. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregres- sive language model. In Proceedings of BigScience Episode\\# 5\u2013Workshop on Challenges & Perspec- tives in Creating Large Language Models, pages 95\u2013 136. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,"}], "doc_text": "better aligns with human judgments. While commonly-used automatic metrics enriched with context align with human preferences, our proposed CtxSimFit demonstrates a stronger correlation. Initial work in evaluating open-domain dialogue generation with context (Welleck et al., 2019; Pang et al., 2020) has been done, but we encourage fur- ther development of better contextualized metrics for stylistic rewriting evaluation. Improvements could include modeling themes, tones, sentence structures (Zhang et al., 2014; Khatri et al., 2018; Chen and Yang, 2020; Toubia et al., 2021; Shen et al., 2023), and social dynamics, and emotional states in conversations (Sap et al., 2017; Rashkin et al., 2018, 2019; Mostafazadeh et al., 2020). 9 Limitations & Ethical Considerations Despite taking the first step towards incorporating context into stylistic rewriting and its evaluation frameworks, there are several limitations and ethi- cal concerns, which we list below. Limited Context Scope In this study, our pri- mary focus is on incorporating textual context, particularly from preceding sentences or previous turns in a conversation. Future work should ex- plore how to incorporate other forms of context into rewriting models and evaluations, such as dis- course structure (Welleck et al., 2019), external knowledge (Ghazvininejad et al., 2018), or richer social and power dynamics (Antoniak et al., 2023), emotional states (Zhou et al., 2023), and commu- nicative intent (Zhou et al., 2022), all of which can significantly contribute to understanding the text. Amount of Context In our experiments, we opted to investigate the context of three preced- ing sentences in a document and one preceding conversational turn, considering only a specific length. However, the amount of context at the mod- eling and evaluation stages could also change the results. We hypothesize that more context could improve rewriting methods, but it could potentially also negatively impact contextual meaning preser- vation metrics. Future work should explore these effects of varying lengths of context. Broad Definition of Meaning Preservation While we have tried to define meaning preserva- tion as the preservation of an event or entity-level details and intended overall meaning, this defini- tion remains broad and subjective (Searle, 1975; Adolphs et al., 2022; Zhou et al., 2022). In this work, we do not delve into more intricate dimen- sions of meaning preservation, such as spatial and temporal accuracy, or the retention of cultural con- text, including references, nuances, and dialects. Applicability to Smaller Models Our work re- lies on few-shot prompting of LLMs to incorporate textual context, given their demonstrated strong rewriting capabilities both with and without textual context usage (Brown et al., 2020). Other exist- ing generative models, such as those used for chit- chat and goal-oriented conversational agents, as well as pretrained language models, have struggled with effectively utilizing preceding textual context (Sankar et al., 2019; O\u2019Connor and Andreas, 2021; Parthasarathi et al., 2021; Su et al., 2023). More- over, custom-made rewriting models from prior research often lack the modeling of context (Ma et al., 2020; Dale et al., 2021). We believe the our results still apply for smaller models, given some preliminary research (Cheng et al., 2020; Atwell et al., 2022) on an increased human preference for contextual rewrites from custom-trained seq2seq models. We encourage future work to thoroughly investigate strategies for effective modeling and evaluation of context in smaller models. Harms of Exposing Workers to Toxic Content In our work, we exposed human annotators to toxic content during the evaluation of the de-toxification task. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We aim to work towards developing evaluation strategies that can minimize the exposure of annotators to toxic content. Potentially Inconsistent Human Evaluations In our work, we also assume human judgments as the gold standard. Concurrent work has shown that human evaluation might not always be con- sistent (Clark et al., 2021; Karpinska et al., 2021); however human judgments continue to be the gold standard for evaluating open-ended text generation. Acknowledgements We would like to thank our workers on MTurk for their responses. We are also grateful to the anonymous reviewers for their helpful comments. Special thanks to Saadia Gabriel, Jocelyn Shen, Ashutosh Baheti, and the members of the CMU LTI COMEDY group for their feedback, and Ope- nAI for providing access to the GPT-3.5 API. This research was supported in part by the Meta Fun- damental AI Research Laboratories (FAIR) \u201cDyn- abench Data Collection and Benchmarking Plat- form\u201d award \u201cContExTox: Context-Aware and Ex- plainable Toxicity Detection.\u201d References Tushar Abhishek, Daksh Rawat, Manish Gupta, and Transformer models arXiv preprint Vasudeva Varma. 2021. for text coherence assessment. arXiv:2109.02176. Leonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason Weston. 2022. Reason first, then respond: Modular generation for knowledge-infused dialogue. In Findings of the Association for Com- putational Linguistics: EMNLP 2022, pages 7112\u2013 7132. Maria Antoniak, Anjalie Field, Ji Min Mun, Melanie Walsh, Lauren F. Klein, and Maarten Sap. 2023. Riv- eter: Measuring power and social dynamics between entities. In ACL demonstrations. Katherine Atwell, Sabit Hassan, and Malihe Alikhani. 2022. Appdia: A discourse-aware transformer-based style transfer model for offensive social media con- versations. In Proceedings of the 29th International Conference on Computational Linguistics, pages 6063\u20136074. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza- tion, pages 65\u201372. Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. 2022. Xlm-t: Multilingual lan- guage models in twitter for sentiment analysis and beyond. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 258\u2013 266. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregres- sive language model. In Proceedings of BigScience Episode\\# 5\u2013Workshop on Challenges & Perspec- tives in Creating Large Language Models, pages 95\u2013 136. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,"}