{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daphne_Ippolito_Reverse-Engineering_Decoding_Strategies_Given_Blackbox_Access_to_a_Language_Generation_System_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What value is set to 2 in the paper discussed?", "answer": " 2", "ref_chunk": "can increase the minimum number of times the least frequent items needs to be seen before the sampling loop terminates; in this pa- per, we set that value to 2. Our methods could also be further improved by in-depth investigation of prompts which consistently produce close-to-uniform distri- butions across different families of language models. Finally, while our methods do not currently address the case where temperature annealing is used in conjunction with top-k or top-p, extending them to support this setting should be straightforward. Tem- perature followed by top-k is still top-k, and should be detectable via our methods. Temperature followed by top-p is trickier, because we no longer have a known distribution. However, this combination can be detected by comparing the empirical distribution against a set of known distributions for common models; if the distribution does not match any of them, then we can conclude that either it is not using any known model, or that other distribution shaping such as temperature has been applied. 5 Limitations Our method is limited to identifying when top-p sam- pling or top-k sampling is used. We do not attempt to detect other decoding strategies which other systems might use. Additionally, there is no guarantee that a system would use a single decoding strategy\u2014it is possible that different prompts may trigger different decoding strategies, or that A/B testing results in dif- ferent users seeing different decoding strategies. Our ChatGPT results were computed by two different au- thors on separate OpenAI accounts. Also, we have no guarantees that the decoding strategy is not changed over time. Some of our ChatGPT results were com- puted using the December 15, 2022 release while oth- ers were computed using the January 9, 2023 release. Additionally, the biases in distributions that we see here could have other underlying reasons; for example, changes in the data can result in very different true distributions. Furthermore, under the hood, an API might not be generating a new random generation each time an identical prompt is passed in. Either random seeds might be getting re-used, or generations could be retrieved from a cache. In both cases, the generations might look like argmax sampling. It\u2019s also conceiv- able that certain combinations of fixed models could look like top-k/p. For example, if a query is randomly routed to one of a series of s servers, each serving a different model, we might interpret the decoding strategy to be top-k even if each server is using argmax. In these cases, an approach more like that proposed by Tay et al. (2020), where classification of decoding strategy is made based on a long generated sequence (rather than single token system predictions, as in our approach), might be more effective. For top-p (though not top-k), we require access to an underlying distribution that approximates the model used. This is not an issue for open source mod- els or models with API access that allows specifying the decoding strategy, but it does limit the applicability of our method to newer proprietary models. It may be possible to empirically determine distributions for carefully engineered prompts, but future work is needed for reverse engineering fully closed models. 6 Conclusion Our attack shows that with even a little work, it is possible to reverse-engineer common decoding strategies. Although we have focused here only on top-p and top-k sampling, these approaches generalize readily to other common methodologies when the output probability distributions are well-approximated. Along with other recent work on reverse-engineering other parts of a language generation system (Zhang and Ippolito, 2023), it seems is infeasible to hide inference implementation details given black-box access to the system. References Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo- hammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations. Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Associ- ation for Computational Linguistics, pages 1808\u20131822. Ali Naseh, Kalpesh Krishna, Mohit Iyyer, and Amir Houmansadr. 2023. On the risks of stealing the arXiv decoding algorithms of language models. preprint arXiv:2303.04729. George P\u00f3lya. 1930. Eine wahrscheinlichkeitsaufgabe Zeitschrift Angewandte in der kundenwerbung. Mathematik und Mechanik, 10(1):96\u201397. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open- access multilingual language model. arXiv preprint arXiv:2211.05100. Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, and Andrew Tomkins. 2020. Reverse engineer- ing configurations of neural text generation models. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 275\u2013279, Online. Association for Computational Linguistics. Gian Wiher, Clara Meister, and Ryan Cotterell. 2022. On decoding strategies for neural text generators. Transactions of the Association for Computational Linguistics, 10:997\u20131012. Sina Zarrie\u00df, Henrik Voigt, and Simeon Sch\u00fcz. 2021. Decoding methods in neural language generation: a survey. Information, 12(9):355. Yiming Zhang and Daphne Ippolito. 2023. Prompts should not be seen as secrets: Systematically measuring arXiv preprint prompt extraction attack success. arXiv:2307.06865. A Algorithm for Estimating k Algorithm 1 Algorithm for estimating k. Given a system Gen : m (cid:55)\u2192 r"}, {"question": " What can improve the methods discussed further according to the text?", "answer": " In-depth investigation of prompts which consistently produce close-to-uniform distributions across different families of language models", "ref_chunk": "can increase the minimum number of times the least frequent items needs to be seen before the sampling loop terminates; in this pa- per, we set that value to 2. Our methods could also be further improved by in-depth investigation of prompts which consistently produce close-to-uniform distri- butions across different families of language models. Finally, while our methods do not currently address the case where temperature annealing is used in conjunction with top-k or top-p, extending them to support this setting should be straightforward. Tem- perature followed by top-k is still top-k, and should be detectable via our methods. Temperature followed by top-p is trickier, because we no longer have a known distribution. However, this combination can be detected by comparing the empirical distribution against a set of known distributions for common models; if the distribution does not match any of them, then we can conclude that either it is not using any known model, or that other distribution shaping such as temperature has been applied. 5 Limitations Our method is limited to identifying when top-p sam- pling or top-k sampling is used. We do not attempt to detect other decoding strategies which other systems might use. Additionally, there is no guarantee that a system would use a single decoding strategy\u2014it is possible that different prompts may trigger different decoding strategies, or that A/B testing results in dif- ferent users seeing different decoding strategies. Our ChatGPT results were computed by two different au- thors on separate OpenAI accounts. Also, we have no guarantees that the decoding strategy is not changed over time. Some of our ChatGPT results were com- puted using the December 15, 2022 release while oth- ers were computed using the January 9, 2023 release. Additionally, the biases in distributions that we see here could have other underlying reasons; for example, changes in the data can result in very different true distributions. Furthermore, under the hood, an API might not be generating a new random generation each time an identical prompt is passed in. Either random seeds might be getting re-used, or generations could be retrieved from a cache. In both cases, the generations might look like argmax sampling. It\u2019s also conceiv- able that certain combinations of fixed models could look like top-k/p. For example, if a query is randomly routed to one of a series of s servers, each serving a different model, we might interpret the decoding strategy to be top-k even if each server is using argmax. In these cases, an approach more like that proposed by Tay et al. (2020), where classification of decoding strategy is made based on a long generated sequence (rather than single token system predictions, as in our approach), might be more effective. For top-p (though not top-k), we require access to an underlying distribution that approximates the model used. This is not an issue for open source mod- els or models with API access that allows specifying the decoding strategy, but it does limit the applicability of our method to newer proprietary models. It may be possible to empirically determine distributions for carefully engineered prompts, but future work is needed for reverse engineering fully closed models. 6 Conclusion Our attack shows that with even a little work, it is possible to reverse-engineer common decoding strategies. Although we have focused here only on top-p and top-k sampling, these approaches generalize readily to other common methodologies when the output probability distributions are well-approximated. Along with other recent work on reverse-engineering other parts of a language generation system (Zhang and Ippolito, 2023), it seems is infeasible to hide inference implementation details given black-box access to the system. References Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo- hammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations. Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Associ- ation for Computational Linguistics, pages 1808\u20131822. Ali Naseh, Kalpesh Krishna, Mohit Iyyer, and Amir Houmansadr. 2023. On the risks of stealing the arXiv decoding algorithms of language models. preprint arXiv:2303.04729. George P\u00f3lya. 1930. Eine wahrscheinlichkeitsaufgabe Zeitschrift Angewandte in der kundenwerbung. Mathematik und Mechanik, 10(1):96\u201397. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open- access multilingual language model. arXiv preprint arXiv:2211.05100. Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, and Andrew Tomkins. 2020. Reverse engineer- ing configurations of neural text generation models. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 275\u2013279, Online. Association for Computational Linguistics. Gian Wiher, Clara Meister, and Ryan Cotterell. 2022. On decoding strategies for neural text generators. Transactions of the Association for Computational Linguistics, 10:997\u20131012. Sina Zarrie\u00df, Henrik Voigt, and Simeon Sch\u00fcz. 2021. Decoding methods in neural language generation: a survey. Information, 12(9):355. Yiming Zhang and Daphne Ippolito. 2023. Prompts should not be seen as secrets: Systematically measuring arXiv preprint prompt extraction attack success. arXiv:2307.06865. A Algorithm for Estimating k Algorithm 1 Algorithm for estimating k. Given a system Gen : m (cid:55)\u2192 r"}, {"question": " What does the text suggest would be straightforward to extend the methods to support?", "answer": " Temperature annealing used in conjunction with top-k or top-p", "ref_chunk": "can increase the minimum number of times the least frequent items needs to be seen before the sampling loop terminates; in this pa- per, we set that value to 2. Our methods could also be further improved by in-depth investigation of prompts which consistently produce close-to-uniform distri- butions across different families of language models. Finally, while our methods do not currently address the case where temperature annealing is used in conjunction with top-k or top-p, extending them to support this setting should be straightforward. Tem- perature followed by top-k is still top-k, and should be detectable via our methods. Temperature followed by top-p is trickier, because we no longer have a known distribution. However, this combination can be detected by comparing the empirical distribution against a set of known distributions for common models; if the distribution does not match any of them, then we can conclude that either it is not using any known model, or that other distribution shaping such as temperature has been applied. 5 Limitations Our method is limited to identifying when top-p sam- pling or top-k sampling is used. We do not attempt to detect other decoding strategies which other systems might use. Additionally, there is no guarantee that a system would use a single decoding strategy\u2014it is possible that different prompts may trigger different decoding strategies, or that A/B testing results in dif- ferent users seeing different decoding strategies. Our ChatGPT results were computed by two different au- thors on separate OpenAI accounts. Also, we have no guarantees that the decoding strategy is not changed over time. Some of our ChatGPT results were com- puted using the December 15, 2022 release while oth- ers were computed using the January 9, 2023 release. Additionally, the biases in distributions that we see here could have other underlying reasons; for example, changes in the data can result in very different true distributions. Furthermore, under the hood, an API might not be generating a new random generation each time an identical prompt is passed in. Either random seeds might be getting re-used, or generations could be retrieved from a cache. In both cases, the generations might look like argmax sampling. It\u2019s also conceiv- able that certain combinations of fixed models could look like top-k/p. For example, if a query is randomly routed to one of a series of s servers, each serving a different model, we might interpret the decoding strategy to be top-k even if each server is using argmax. In these cases, an approach more like that proposed by Tay et al. (2020), where classification of decoding strategy is made based on a long generated sequence (rather than single token system predictions, as in our approach), might be more effective. For top-p (though not top-k), we require access to an underlying distribution that approximates the model used. This is not an issue for open source mod- els or models with API access that allows specifying the decoding strategy, but it does limit the applicability of our method to newer proprietary models. It may be possible to empirically determine distributions for carefully engineered prompts, but future work is needed for reverse engineering fully closed models. 6 Conclusion Our attack shows that with even a little work, it is possible to reverse-engineer common decoding strategies. Although we have focused here only on top-p and top-k sampling, these approaches generalize readily to other common methodologies when the output probability distributions are well-approximated. Along with other recent work on reverse-engineering other parts of a language generation system (Zhang and Ippolito, 2023), it seems is infeasible to hide inference implementation details given black-box access to the system. References Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo- hammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations. Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Associ- ation for Computational Linguistics, pages 1808\u20131822. Ali Naseh, Kalpesh Krishna, Mohit Iyyer, and Amir Houmansadr. 2023. On the risks of stealing the arXiv decoding algorithms of language models. preprint arXiv:2303.04729. George P\u00f3lya. 1930. Eine wahrscheinlichkeitsaufgabe Zeitschrift Angewandte in der kundenwerbung. Mathematik und Mechanik, 10(1):96\u201397. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open- access multilingual language model. arXiv preprint arXiv:2211.05100. Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, and Andrew Tomkins. 2020. Reverse engineer- ing configurations of neural text generation models. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 275\u2013279, Online. Association for Computational Linguistics. Gian Wiher, Clara Meister, and Ryan Cotterell. 2022. On decoding strategies for neural text generators. Transactions of the Association for Computational Linguistics, 10:997\u20131012. Sina Zarrie\u00df, Henrik Voigt, and Simeon Sch\u00fcz. 2021. Decoding methods in neural language generation: a survey. Information, 12(9):355. Yiming Zhang and Daphne Ippolito. 2023. Prompts should not be seen as secrets: Systematically measuring arXiv preprint prompt extraction attack success. arXiv:2307.06865. A Algorithm for Estimating k Algorithm 1 Algorithm for estimating k. Given a system Gen : m (cid:55)\u2192 r"}, {"question": " What is a limitation of the method discussed in the text?", "answer": " It is limited to identifying when top-p sampling or top-k sampling is used", "ref_chunk": "can increase the minimum number of times the least frequent items needs to be seen before the sampling loop terminates; in this pa- per, we set that value to 2. Our methods could also be further improved by in-depth investigation of prompts which consistently produce close-to-uniform distri- butions across different families of language models. Finally, while our methods do not currently address the case where temperature annealing is used in conjunction with top-k or top-p, extending them to support this setting should be straightforward. Tem- perature followed by top-k is still top-k, and should be detectable via our methods. Temperature followed by top-p is trickier, because we no longer have a known distribution. However, this combination can be detected by comparing the empirical distribution against a set of known distributions for common models; if the distribution does not match any of them, then we can conclude that either it is not using any known model, or that other distribution shaping such as temperature has been applied. 5 Limitations Our method is limited to identifying when top-p sam- pling or top-k sampling is used. We do not attempt to detect other decoding strategies which other systems might use. Additionally, there is no guarantee that a system would use a single decoding strategy\u2014it is possible that different prompts may trigger different decoding strategies, or that A/B testing results in dif- ferent users seeing different decoding strategies. Our ChatGPT results were computed by two different au- thors on separate OpenAI accounts. Also, we have no guarantees that the decoding strategy is not changed over time. Some of our ChatGPT results were com- puted using the December 15, 2022 release while oth- ers were computed using the January 9, 2023 release. Additionally, the biases in distributions that we see here could have other underlying reasons; for example, changes in the data can result in very different true distributions. Furthermore, under the hood, an API might not be generating a new random generation each time an identical prompt is passed in. Either random seeds might be getting re-used, or generations could be retrieved from a cache. In both cases, the generations might look like argmax sampling. It\u2019s also conceiv- able that certain combinations of fixed models could look like top-k/p. For example, if a query is randomly routed to one of a series of s servers, each serving a different model, we might interpret the decoding strategy to be top-k even if each server is using argmax. In these cases, an approach more like that proposed by Tay et al. (2020), where classification of decoding strategy is made based on a long generated sequence (rather than single token system predictions, as in our approach), might be more effective. For top-p (though not top-k), we require access to an underlying distribution that approximates the model used. This is not an issue for open source mod- els or models with API access that allows specifying the decoding strategy, but it does limit the applicability of our method to newer proprietary models. It may be possible to empirically determine distributions for carefully engineered prompts, but future work is needed for reverse engineering fully closed models. 6 Conclusion Our attack shows that with even a little work, it is possible to reverse-engineer common decoding strategies. Although we have focused here only on top-p and top-k sampling, these approaches generalize readily to other common methodologies when the output probability distributions are well-approximated. Along with other recent work on reverse-engineering other parts of a language generation system (Zhang and Ippolito, 2023), it seems is infeasible to hide inference implementation details given black-box access to the system. References Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo- hammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations. Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Associ- ation for Computational Linguistics, pages 1808\u20131822. Ali Naseh, Kalpesh Krishna, Mohit Iyyer, and Amir Houmansadr. 2023. On the risks of stealing the arXiv decoding algorithms of language models. preprint arXiv:2303.04729. George P\u00f3lya. 1930. Eine wahrscheinlichkeitsaufgabe Zeitschrift Angewandte in der kundenwerbung. Mathematik und Mechanik, 10(1):96\u201397. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open- access multilingual language model. arXiv preprint arXiv:2211.05100. Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, and Andrew Tomkins. 2020. Reverse engineer- ing configurations of neural text generation models. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 275\u2013279, Online. Association for Computational Linguistics. Gian Wiher, Clara Meister, and Ryan Cotterell. 2022. On decoding strategies for neural text generators. Transactions of the Association for Computational Linguistics, 10:997\u20131012. Sina Zarrie\u00df, Henrik Voigt, and Simeon Sch\u00fcz. 2021. Decoding methods in neural language generation: a survey. Information, 12(9):355. Yiming Zhang and Daphne Ippolito. 2023. Prompts should not be seen as secrets: Systematically measuring arXiv preprint prompt extraction attack success. arXiv:2307.06865. A Algorithm for Estimating k Algorithm 1 Algorithm for estimating k. Given a system Gen : m (cid:55)\u2192 r"}, {"question": " What may trigger different decoding strategies according to the text?", "answer": " Different prompts", "ref_chunk": "can increase the minimum number of times the least frequent items needs to be seen before the sampling loop terminates; in this pa- per, we set that value to 2. Our methods could also be further improved by in-depth investigation of prompts which consistently produce close-to-uniform distri- butions across different families of language models. Finally, while our methods do not currently address the case where temperature annealing is used in conjunction with top-k or top-p, extending them to support this setting should be straightforward. Tem- perature followed by top-k is still top-k, and should be detectable via our methods. Temperature followed by top-p is trickier, because we no longer have a known distribution. However, this combination can be detected by comparing the empirical distribution against a set of known distributions for common models; if the distribution does not match any of them, then we can conclude that either it is not using any known model, or that other distribution shaping such as temperature has been applied. 5 Limitations Our method is limited to identifying when top-p sam- pling or top-k sampling is used. We do not attempt to detect other decoding strategies which other systems might use. Additionally, there is no guarantee that a system would use a single decoding strategy\u2014it is possible that different prompts may trigger different decoding strategies, or that A/B testing results in dif- ferent users seeing different decoding strategies. Our ChatGPT results were computed by two different au- thors on separate OpenAI accounts. Also, we have no guarantees that the decoding strategy is not changed over time. Some of our ChatGPT results were com- puted using the December 15, 2022 release while oth- ers were computed using the January 9, 2023 release. Additionally, the biases in distributions that we see here could have other underlying reasons; for example, changes in the data can result in very different true distributions. Furthermore, under the hood, an API might not be generating a new random generation each time an identical prompt is passed in. Either random seeds might be getting re-used, or generations could be retrieved from a cache. In both cases, the generations might look like argmax sampling. It\u2019s also conceiv- able that certain combinations of fixed models could look like top-k/p. For example, if a query is randomly routed to one of a series of s servers, each serving a different model, we might interpret the decoding strategy to be top-k even if each server is using argmax. In these cases, an approach more like that proposed by Tay et al. (2020), where classification of decoding strategy is made based on a long generated sequence (rather than single token system predictions, as in our approach), might be more effective. For top-p (though not top-k), we require access to an underlying distribution that approximates the model used. This is not an issue for open source mod- els or models with API access that allows specifying the decoding strategy, but it does limit the applicability of our method to newer proprietary models. It may be possible to empirically determine distributions for carefully engineered prompts, but future work is needed for reverse engineering fully closed models. 6 Conclusion Our attack shows that with even a little work, it is possible to reverse-engineer common decoding strategies. Although we have focused here only on top-p and top-k sampling, these approaches generalize readily to other common methodologies when the output probability distributions are well-approximated. Along with other recent work on reverse-engineering other parts of a language generation system (Zhang and Ippolito, 2023), it seems is infeasible to hide inference implementation details given black-box access to the system. References Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo- hammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations. Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Associ- ation for Computational Linguistics, pages 1808\u20131822. Ali Naseh, Kalpesh Krishna, Mohit Iyyer, and Amir Houmansadr. 2023. On the risks of stealing the arXiv decoding algorithms of language models. preprint arXiv:2303.04729. George P\u00f3lya. 1930. Eine wahrscheinlichkeitsaufgabe Zeitschrift Angewandte in der kundenwerbung. Mathematik und Mechanik, 10(1):96\u201397. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open- access multilingual language model. arXiv preprint arXiv:2211.05100. Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, and Andrew Tomkins. 2020. Reverse engineer- ing configurations of neural text generation models. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 275\u2013279, Online. Association for Computational Linguistics. Gian Wiher, Clara Meister, and Ryan Cotterell. 2022. On decoding strategies for neural text generators. Transactions of the Association for Computational Linguistics, 10:997\u20131012. Sina Zarrie\u00df, Henrik Voigt, and Simeon Sch\u00fcz. 2021. Decoding methods in neural language generation: a survey. Information, 12(9):355. Yiming Zhang and Daphne Ippolito. 2023. Prompts should not be seen as secrets: Systematically measuring arXiv preprint prompt extraction attack success. arXiv:2307.06865. A Algorithm for Estimating k Algorithm 1 Algorithm for estimating k. Given a system Gen : m (cid:55)\u2192 r"}, {"question": " How were the ChatGPT results computed, as per the text?", "answer": " By two different authors on separate OpenAI accounts", "ref_chunk": "can increase the minimum number of times the least frequent items needs to be seen before the sampling loop terminates; in this pa- per, we set that value to 2. Our methods could also be further improved by in-depth investigation of prompts which consistently produce close-to-uniform distri- butions across different families of language models. Finally, while our methods do not currently address the case where temperature annealing is used in conjunction with top-k or top-p, extending them to support this setting should be straightforward. Tem- perature followed by top-k is still top-k, and should be detectable via our methods. Temperature followed by top-p is trickier, because we no longer have a known distribution. However, this combination can be detected by comparing the empirical distribution against a set of known distributions for common models; if the distribution does not match any of them, then we can conclude that either it is not using any known model, or that other distribution shaping such as temperature has been applied. 5 Limitations Our method is limited to identifying when top-p sam- pling or top-k sampling is used. We do not attempt to detect other decoding strategies which other systems might use. Additionally, there is no guarantee that a system would use a single decoding strategy\u2014it is possible that different prompts may trigger different decoding strategies, or that A/B testing results in dif- ferent users seeing different decoding strategies. Our ChatGPT results were computed by two different au- thors on separate OpenAI accounts. Also, we have no guarantees that the decoding strategy is not changed over time. Some of our ChatGPT results were com- puted using the December 15, 2022 release while oth- ers were computed using the January 9, 2023 release. Additionally, the biases in distributions that we see here could have other underlying reasons; for example, changes in the data can result in very different true distributions. Furthermore, under the hood, an API might not be generating a new random generation each time an identical prompt is passed in. Either random seeds might be getting re-used, or generations could be retrieved from a cache. In both cases, the generations might look like argmax sampling. It\u2019s also conceiv- able that certain combinations of fixed models could look like top-k/p. For example, if a query is randomly routed to one of a series of s servers, each serving a different model, we might interpret the decoding strategy to be top-k even if each server is using argmax. In these cases, an approach more like that proposed by Tay et al. (2020), where classification of decoding strategy is made based on a long generated sequence (rather than single token system predictions, as in our approach), might be more effective. For top-p (though not top-k), we require access to an underlying distribution that approximates the model used. This is not an issue for open source mod- els or models with API access that allows specifying the decoding strategy, but it does limit the applicability of our method to newer proprietary models. It may be possible to empirically determine distributions for carefully engineered prompts, but future work is needed for reverse engineering fully closed models. 6 Conclusion Our attack shows that with even a little work, it is possible to reverse-engineer common decoding strategies. Although we have focused here only on top-p and top-k sampling, these approaches generalize readily to other common methodologies when the output probability distributions are well-approximated. Along with other recent work on reverse-engineering other parts of a language generation system (Zhang and Ippolito, 2023), it seems is infeasible to hide inference implementation details given black-box access to the system. References Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo- hammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations. Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Associ- ation for Computational Linguistics, pages 1808\u20131822. Ali Naseh, Kalpesh Krishna, Mohit Iyyer, and Amir Houmansadr. 2023. On the risks of stealing the arXiv decoding algorithms of language models. preprint arXiv:2303.04729. George P\u00f3lya. 1930. Eine wahrscheinlichkeitsaufgabe Zeitschrift Angewandte in der kundenwerbung. Mathematik und Mechanik, 10(1):96\u201397. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open- access multilingual language model. arXiv preprint arXiv:2211.05100. Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, and Andrew Tomkins. 2020. Reverse engineer- ing configurations of neural text generation models. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 275\u2013279, Online. Association for Computational Linguistics. Gian Wiher, Clara Meister, and Ryan Cotterell. 2022. On decoding strategies for neural text generators. Transactions of the Association for Computational Linguistics, 10:997\u20131012. Sina Zarrie\u00df, Henrik Voigt, and Simeon Sch\u00fcz. 2021. Decoding methods in neural language generation: a survey. Information, 12(9):355. Yiming Zhang and Daphne Ippolito. 2023. Prompts should not be seen as secrets: Systematically measuring arXiv preprint prompt extraction attack success. arXiv:2307.06865. A Algorithm for Estimating k Algorithm 1 Algorithm for estimating k. Given a system Gen : m (cid:55)\u2192 r"}, {"question": " What might result in very different true distributions according to the text?", "answer": " Changes in the data", "ref_chunk": "can increase the minimum number of times the least frequent items needs to be seen before the sampling loop terminates; in this pa- per, we set that value to 2. Our methods could also be further improved by in-depth investigation of prompts which consistently produce close-to-uniform distri- butions across different families of language models. Finally, while our methods do not currently address the case where temperature annealing is used in conjunction with top-k or top-p, extending them to support this setting should be straightforward. Tem- perature followed by top-k is still top-k, and should be detectable via our methods. Temperature followed by top-p is trickier, because we no longer have a known distribution. However, this combination can be detected by comparing the empirical distribution against a set of known distributions for common models; if the distribution does not match any of them, then we can conclude that either it is not using any known model, or that other distribution shaping such as temperature has been applied. 5 Limitations Our method is limited to identifying when top-p sam- pling or top-k sampling is used. We do not attempt to detect other decoding strategies which other systems might use. Additionally, there is no guarantee that a system would use a single decoding strategy\u2014it is possible that different prompts may trigger different decoding strategies, or that A/B testing results in dif- ferent users seeing different decoding strategies. Our ChatGPT results were computed by two different au- thors on separate OpenAI accounts. Also, we have no guarantees that the decoding strategy is not changed over time. Some of our ChatGPT results were com- puted using the December 15, 2022 release while oth- ers were computed using the January 9, 2023 release. Additionally, the biases in distributions that we see here could have other underlying reasons; for example, changes in the data can result in very different true distributions. Furthermore, under the hood, an API might not be generating a new random generation each time an identical prompt is passed in. Either random seeds might be getting re-used, or generations could be retrieved from a cache. In both cases, the generations might look like argmax sampling. It\u2019s also conceiv- able that certain combinations of fixed models could look like top-k/p. For example, if a query is randomly routed to one of a series of s servers, each serving a different model, we might interpret the decoding strategy to be top-k even if each server is using argmax. In these cases, an approach more like that proposed by Tay et al. (2020), where classification of decoding strategy is made based on a long generated sequence (rather than single token system predictions, as in our approach), might be more effective. For top-p (though not top-k), we require access to an underlying distribution that approximates the model used. This is not an issue for open source mod- els or models with API access that allows specifying the decoding strategy, but it does limit the applicability of our method to newer proprietary models. It may be possible to empirically determine distributions for carefully engineered prompts, but future work is needed for reverse engineering fully closed models. 6 Conclusion Our attack shows that with even a little work, it is possible to reverse-engineer common decoding strategies. Although we have focused here only on top-p and top-k sampling, these approaches generalize readily to other common methodologies when the output probability distributions are well-approximated. Along with other recent work on reverse-engineering other parts of a language generation system (Zhang and Ippolito, 2023), it seems is infeasible to hide inference implementation details given black-box access to the system. References Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo- hammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations. Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Associ- ation for Computational Linguistics, pages 1808\u20131822. Ali Naseh, Kalpesh Krishna, Mohit Iyyer, and Amir Houmansadr. 2023. On the risks of stealing the arXiv decoding algorithms of language models. preprint arXiv:2303.04729. George P\u00f3lya. 1930. Eine wahrscheinlichkeitsaufgabe Zeitschrift Angewandte in der kundenwerbung. Mathematik und Mechanik, 10(1):96\u201397. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open- access multilingual language model. arXiv preprint arXiv:2211.05100. Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, and Andrew Tomkins. 2020. Reverse engineer- ing configurations of neural text generation models. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 275\u2013279, Online. Association for Computational Linguistics. Gian Wiher, Clara Meister, and Ryan Cotterell. 2022. On decoding strategies for neural text generators. Transactions of the Association for Computational Linguistics, 10:997\u20131012. Sina Zarrie\u00df, Henrik Voigt, and Simeon Sch\u00fcz. 2021. Decoding methods in neural language generation: a survey. Information, 12(9):355. Yiming Zhang and Daphne Ippolito. 2023. Prompts should not be seen as secrets: Systematically measuring arXiv preprint prompt extraction attack success. arXiv:2307.06865. A Algorithm for Estimating k Algorithm 1 Algorithm for estimating k. Given a system Gen : m (cid:55)\u2192 r"}, {"question": " What does the text suggest may be happening under the hood with certain combinations of fixed models?", "answer": " They might look like top-k/p", "ref_chunk": "can increase the minimum number of times the least frequent items needs to be seen before the sampling loop terminates; in this pa- per, we set that value to 2. Our methods could also be further improved by in-depth investigation of prompts which consistently produce close-to-uniform distri- butions across different families of language models. Finally, while our methods do not currently address the case where temperature annealing is used in conjunction with top-k or top-p, extending them to support this setting should be straightforward. Tem- perature followed by top-k is still top-k, and should be detectable via our methods. Temperature followed by top-p is trickier, because we no longer have a known distribution. However, this combination can be detected by comparing the empirical distribution against a set of known distributions for common models; if the distribution does not match any of them, then we can conclude that either it is not using any known model, or that other distribution shaping such as temperature has been applied. 5 Limitations Our method is limited to identifying when top-p sam- pling or top-k sampling is used. We do not attempt to detect other decoding strategies which other systems might use. Additionally, there is no guarantee that a system would use a single decoding strategy\u2014it is possible that different prompts may trigger different decoding strategies, or that A/B testing results in dif- ferent users seeing different decoding strategies. Our ChatGPT results were computed by two different au- thors on separate OpenAI accounts. Also, we have no guarantees that the decoding strategy is not changed over time. Some of our ChatGPT results were com- puted using the December 15, 2022 release while oth- ers were computed using the January 9, 2023 release. Additionally, the biases in distributions that we see here could have other underlying reasons; for example, changes in the data can result in very different true distributions. Furthermore, under the hood, an API might not be generating a new random generation each time an identical prompt is passed in. Either random seeds might be getting re-used, or generations could be retrieved from a cache. In both cases, the generations might look like argmax sampling. It\u2019s also conceiv- able that certain combinations of fixed models could look like top-k/p. For example, if a query is randomly routed to one of a series of s servers, each serving a different model, we might interpret the decoding strategy to be top-k even if each server is using argmax. In these cases, an approach more like that proposed by Tay et al. (2020), where classification of decoding strategy is made based on a long generated sequence (rather than single token system predictions, as in our approach), might be more effective. For top-p (though not top-k), we require access to an underlying distribution that approximates the model used. This is not an issue for open source mod- els or models with API access that allows specifying the decoding strategy, but it does limit the applicability of our method to newer proprietary models. It may be possible to empirically determine distributions for carefully engineered prompts, but future work is needed for reverse engineering fully closed models. 6 Conclusion Our attack shows that with even a little work, it is possible to reverse-engineer common decoding strategies. Although we have focused here only on top-p and top-k sampling, these approaches generalize readily to other common methodologies when the output probability distributions are well-approximated. Along with other recent work on reverse-engineering other parts of a language generation system (Zhang and Ippolito, 2023), it seems is infeasible to hide inference implementation details given black-box access to the system. References Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo- hammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations. Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Associ- ation for Computational Linguistics, pages 1808\u20131822. Ali Naseh, Kalpesh Krishna, Mohit Iyyer, and Amir Houmansadr. 2023. On the risks of stealing the arXiv decoding algorithms of language models. preprint arXiv:2303.04729. George P\u00f3lya. 1930. Eine wahrscheinlichkeitsaufgabe Zeitschrift Angewandte in der kundenwerbung. Mathematik und Mechanik, 10(1):96\u201397. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open- access multilingual language model. arXiv preprint arXiv:2211.05100. Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, and Andrew Tomkins. 2020. Reverse engineer- ing configurations of neural text generation models. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 275\u2013279, Online. Association for Computational Linguistics. Gian Wiher, Clara Meister, and Ryan Cotterell. 2022. On decoding strategies for neural text generators. Transactions of the Association for Computational Linguistics, 10:997\u20131012. Sina Zarrie\u00df, Henrik Voigt, and Simeon Sch\u00fcz. 2021. Decoding methods in neural language generation: a survey. Information, 12(9):355. Yiming Zhang and Daphne Ippolito. 2023. Prompts should not be seen as secrets: Systematically measuring arXiv preprint prompt extraction attack success. arXiv:2307.06865. A Algorithm for Estimating k Algorithm 1 Algorithm for estimating k. Given a system Gen : m (cid:55)\u2192 r"}, {"question": " What is required for access to an underlying distribution for top-p (though not top-k) according to the text?", "answer": " An underlying distribution that approximates the model used", "ref_chunk": "can increase the minimum number of times the least frequent items needs to be seen before the sampling loop terminates; in this pa- per, we set that value to 2. Our methods could also be further improved by in-depth investigation of prompts which consistently produce close-to-uniform distri- butions across different families of language models. Finally, while our methods do not currently address the case where temperature annealing is used in conjunction with top-k or top-p, extending them to support this setting should be straightforward. Tem- perature followed by top-k is still top-k, and should be detectable via our methods. Temperature followed by top-p is trickier, because we no longer have a known distribution. However, this combination can be detected by comparing the empirical distribution against a set of known distributions for common models; if the distribution does not match any of them, then we can conclude that either it is not using any known model, or that other distribution shaping such as temperature has been applied. 5 Limitations Our method is limited to identifying when top-p sam- pling or top-k sampling is used. We do not attempt to detect other decoding strategies which other systems might use. Additionally, there is no guarantee that a system would use a single decoding strategy\u2014it is possible that different prompts may trigger different decoding strategies, or that A/B testing results in dif- ferent users seeing different decoding strategies. Our ChatGPT results were computed by two different au- thors on separate OpenAI accounts. Also, we have no guarantees that the decoding strategy is not changed over time. Some of our ChatGPT results were com- puted using the December 15, 2022 release while oth- ers were computed using the January 9, 2023 release. Additionally, the biases in distributions that we see here could have other underlying reasons; for example, changes in the data can result in very different true distributions. Furthermore, under the hood, an API might not be generating a new random generation each time an identical prompt is passed in. Either random seeds might be getting re-used, or generations could be retrieved from a cache. In both cases, the generations might look like argmax sampling. It\u2019s also conceiv- able that certain combinations of fixed models could look like top-k/p. For example, if a query is randomly routed to one of a series of s servers, each serving a different model, we might interpret the decoding strategy to be top-k even if each server is using argmax. In these cases, an approach more like that proposed by Tay et al. (2020), where classification of decoding strategy is made based on a long generated sequence (rather than single token system predictions, as in our approach), might be more effective. For top-p (though not top-k), we require access to an underlying distribution that approximates the model used. This is not an issue for open source mod- els or models with API access that allows specifying the decoding strategy, but it does limit the applicability of our method to newer proprietary models. It may be possible to empirically determine distributions for carefully engineered prompts, but future work is needed for reverse engineering fully closed models. 6 Conclusion Our attack shows that with even a little work, it is possible to reverse-engineer common decoding strategies. Although we have focused here only on top-p and top-k sampling, these approaches generalize readily to other common methodologies when the output probability distributions are well-approximated. Along with other recent work on reverse-engineering other parts of a language generation system (Zhang and Ippolito, 2023), it seems is infeasible to hide inference implementation details given black-box access to the system. References Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo- hammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations. Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Associ- ation for Computational Linguistics, pages 1808\u20131822. Ali Naseh, Kalpesh Krishna, Mohit Iyyer, and Amir Houmansadr. 2023. On the risks of stealing the arXiv decoding algorithms of language models. preprint arXiv:2303.04729. George P\u00f3lya. 1930. Eine wahrscheinlichkeitsaufgabe Zeitschrift Angewandte in der kundenwerbung. Mathematik und Mechanik, 10(1):96\u201397. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open- access multilingual language model. arXiv preprint arXiv:2211.05100. Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, and Andrew Tomkins. 2020. Reverse engineer- ing configurations of neural text generation models. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 275\u2013279, Online. Association for Computational Linguistics. Gian Wiher, Clara Meister, and Ryan Cotterell. 2022. On decoding strategies for neural text generators. Transactions of the Association for Computational Linguistics, 10:997\u20131012. Sina Zarrie\u00df, Henrik Voigt, and Simeon Sch\u00fcz. 2021. Decoding methods in neural language generation: a survey. Information, 12(9):355. Yiming Zhang and Daphne Ippolito. 2023. Prompts should not be seen as secrets: Systematically measuring arXiv preprint prompt extraction attack success. arXiv:2307.06865. A Algorithm for Estimating k Algorithm 1 Algorithm for estimating k. Given a system Gen : m (cid:55)\u2192 r"}, {"question": " What does the conclusion of the text suggest is possible with a little work?", "answer": " Reverse-engineer common decoding strategies", "ref_chunk": "can increase the minimum number of times the least frequent items needs to be seen before the sampling loop terminates; in this pa- per, we set that value to 2. Our methods could also be further improved by in-depth investigation of prompts which consistently produce close-to-uniform distri- butions across different families of language models. Finally, while our methods do not currently address the case where temperature annealing is used in conjunction with top-k or top-p, extending them to support this setting should be straightforward. Tem- perature followed by top-k is still top-k, and should be detectable via our methods. Temperature followed by top-p is trickier, because we no longer have a known distribution. However, this combination can be detected by comparing the empirical distribution against a set of known distributions for common models; if the distribution does not match any of them, then we can conclude that either it is not using any known model, or that other distribution shaping such as temperature has been applied. 5 Limitations Our method is limited to identifying when top-p sam- pling or top-k sampling is used. We do not attempt to detect other decoding strategies which other systems might use. Additionally, there is no guarantee that a system would use a single decoding strategy\u2014it is possible that different prompts may trigger different decoding strategies, or that A/B testing results in dif- ferent users seeing different decoding strategies. Our ChatGPT results were computed by two different au- thors on separate OpenAI accounts. Also, we have no guarantees that the decoding strategy is not changed over time. Some of our ChatGPT results were com- puted using the December 15, 2022 release while oth- ers were computed using the January 9, 2023 release. Additionally, the biases in distributions that we see here could have other underlying reasons; for example, changes in the data can result in very different true distributions. Furthermore, under the hood, an API might not be generating a new random generation each time an identical prompt is passed in. Either random seeds might be getting re-used, or generations could be retrieved from a cache. In both cases, the generations might look like argmax sampling. It\u2019s also conceiv- able that certain combinations of fixed models could look like top-k/p. For example, if a query is randomly routed to one of a series of s servers, each serving a different model, we might interpret the decoding strategy to be top-k even if each server is using argmax. In these cases, an approach more like that proposed by Tay et al. (2020), where classification of decoding strategy is made based on a long generated sequence (rather than single token system predictions, as in our approach), might be more effective. For top-p (though not top-k), we require access to an underlying distribution that approximates the model used. This is not an issue for open source mod- els or models with API access that allows specifying the decoding strategy, but it does limit the applicability of our method to newer proprietary models. It may be possible to empirically determine distributions for carefully engineered prompts, but future work is needed for reverse engineering fully closed models. 6 Conclusion Our attack shows that with even a little work, it is possible to reverse-engineer common decoding strategies. Although we have focused here only on top-p and top-k sampling, these approaches generalize readily to other common methodologies when the output probability distributions are well-approximated. Along with other recent work on reverse-engineering other parts of a language generation system (Zhang and Ippolito, 2023), it seems is infeasible to hide inference implementation details given black-box access to the system. References Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo- hammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations. Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Associ- ation for Computational Linguistics, pages 1808\u20131822. Ali Naseh, Kalpesh Krishna, Mohit Iyyer, and Amir Houmansadr. 2023. On the risks of stealing the arXiv decoding algorithms of language models. preprint arXiv:2303.04729. George P\u00f3lya. 1930. Eine wahrscheinlichkeitsaufgabe Zeitschrift Angewandte in der kundenwerbung. Mathematik und Mechanik, 10(1):96\u201397. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open- access multilingual language model. arXiv preprint arXiv:2211.05100. Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, and Andrew Tomkins. 2020. Reverse engineer- ing configurations of neural text generation models. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 275\u2013279, Online. Association for Computational Linguistics. Gian Wiher, Clara Meister, and Ryan Cotterell. 2022. On decoding strategies for neural text generators. Transactions of the Association for Computational Linguistics, 10:997\u20131012. Sina Zarrie\u00df, Henrik Voigt, and Simeon Sch\u00fcz. 2021. Decoding methods in neural language generation: a survey. Information, 12(9):355. Yiming Zhang and Daphne Ippolito. 2023. Prompts should not be seen as secrets: Systematically measuring arXiv preprint prompt extraction attack success. arXiv:2307.06865. A Algorithm for Estimating k Algorithm 1 Algorithm for estimating k. Given a system Gen : m (cid:55)\u2192 r"}], "doc_text": "can increase the minimum number of times the least frequent items needs to be seen before the sampling loop terminates; in this pa- per, we set that value to 2. Our methods could also be further improved by in-depth investigation of prompts which consistently produce close-to-uniform distri- butions across different families of language models. Finally, while our methods do not currently address the case where temperature annealing is used in conjunction with top-k or top-p, extending them to support this setting should be straightforward. Tem- perature followed by top-k is still top-k, and should be detectable via our methods. Temperature followed by top-p is trickier, because we no longer have a known distribution. However, this combination can be detected by comparing the empirical distribution against a set of known distributions for common models; if the distribution does not match any of them, then we can conclude that either it is not using any known model, or that other distribution shaping such as temperature has been applied. 5 Limitations Our method is limited to identifying when top-p sam- pling or top-k sampling is used. We do not attempt to detect other decoding strategies which other systems might use. Additionally, there is no guarantee that a system would use a single decoding strategy\u2014it is possible that different prompts may trigger different decoding strategies, or that A/B testing results in dif- ferent users seeing different decoding strategies. Our ChatGPT results were computed by two different au- thors on separate OpenAI accounts. Also, we have no guarantees that the decoding strategy is not changed over time. Some of our ChatGPT results were com- puted using the December 15, 2022 release while oth- ers were computed using the January 9, 2023 release. Additionally, the biases in distributions that we see here could have other underlying reasons; for example, changes in the data can result in very different true distributions. Furthermore, under the hood, an API might not be generating a new random generation each time an identical prompt is passed in. Either random seeds might be getting re-used, or generations could be retrieved from a cache. In both cases, the generations might look like argmax sampling. It\u2019s also conceiv- able that certain combinations of fixed models could look like top-k/p. For example, if a query is randomly routed to one of a series of s servers, each serving a different model, we might interpret the decoding strategy to be top-k even if each server is using argmax. In these cases, an approach more like that proposed by Tay et al. (2020), where classification of decoding strategy is made based on a long generated sequence (rather than single token system predictions, as in our approach), might be more effective. For top-p (though not top-k), we require access to an underlying distribution that approximates the model used. This is not an issue for open source mod- els or models with API access that allows specifying the decoding strategy, but it does limit the applicability of our method to newer proprietary models. It may be possible to empirically determine distributions for carefully engineered prompts, but future work is needed for reverse engineering fully closed models. 6 Conclusion Our attack shows that with even a little work, it is possible to reverse-engineer common decoding strategies. Although we have focused here only on top-p and top-k sampling, these approaches generalize readily to other common methodologies when the output probability distributions are well-approximated. Along with other recent work on reverse-engineering other parts of a language generation system (Zhang and Ippolito, 2023), it seems is infeasible to hide inference implementation details given black-box access to the system. References Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo- hammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations. Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Associ- ation for Computational Linguistics, pages 1808\u20131822. Ali Naseh, Kalpesh Krishna, Mohit Iyyer, and Amir Houmansadr. 2023. On the risks of stealing the arXiv decoding algorithms of language models. preprint arXiv:2303.04729. George P\u00f3lya. 1930. Eine wahrscheinlichkeitsaufgabe Zeitschrift Angewandte in der kundenwerbung. Mathematik und Mechanik, 10(1):96\u201397. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open- access multilingual language model. arXiv preprint arXiv:2211.05100. Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, and Andrew Tomkins. 2020. Reverse engineer- ing configurations of neural text generation models. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 275\u2013279, Online. Association for Computational Linguistics. Gian Wiher, Clara Meister, and Ryan Cotterell. 2022. On decoding strategies for neural text generators. Transactions of the Association for Computational Linguistics, 10:997\u20131012. Sina Zarrie\u00df, Henrik Voigt, and Simeon Sch\u00fcz. 2021. Decoding methods in neural language generation: a survey. Information, 12(9):355. Yiming Zhang and Daphne Ippolito. 2023. Prompts should not be seen as secrets: Systematically measuring arXiv preprint prompt extraction attack success. arXiv:2307.06865. A Algorithm for Estimating k Algorithm 1 Algorithm for estimating k. Given a system Gen : m (cid:55)\u2192 r"}