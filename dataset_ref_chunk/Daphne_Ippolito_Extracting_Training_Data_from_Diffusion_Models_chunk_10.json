{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daphne_Ippolito_Extracting_Training_Data_from_Diffusion_Models_chunk_10.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main finding in the text regarding the memorization capabilities of GAN models?", "answer": " The text states that generative models, such as GANs, memorize more data as their quality improves and that diffusion models are less private than GAN models.", "ref_chunk": "when the GANs reach similar performance, e.g., the best DDPM model memorizes 2\u00d7 more than StyleGAN- ADA but reaches the same FID. Moreover, generative models (both GANs and diffusion models) tend to mem- orize more data as their quality (FID) improves, e.g., StyleGAN-ADA memorizes 3\u00d7 more images than the weakest GANs. Using the GANs we trained ourselves, we show ex- amples of the near-copy generations in Figure 15 for the three GANs that we trained ourselves, and Figure 24 in the Appendix shows every sample that we extract for those models. The Appendix also contains near-copy generations from the \ufb01ve off-the-shelf GANs. Overall, these results further reinforce the conclusion that diffu- sion models are less private than GAN models. We also surprisingly \ufb01nd that diffusion models and GANs memorize many of the same images. In particular, despite the fact that our diffusion model memorizes 1280 images and a StyleGAN model we train on half of the dataset memorizes 361 images, we \ufb01nd that 244 unique images are memorized in common. If images were mem- orized uniformly at random, we should expect on average 10 images would be memorized by both, giving excep- tionally strong evidence that some images (p < 10\u2212261) are inherently less private than others. Understanding why this phenomenon occurs is a fruitful direction for future work. 13 7 Defenses and Recommendations Given the degree to which diffusion models memorize and regenerate training examples, in this section we ex- plore various defenses and practical strategies that may help to reduce and audit model memorization. 7.1 Deduplicating Training Data In Section 4.2, we showed that many examples that are easy to extract are duplicated many times (e.g., > 100) in the training data. Similar results have been shown for language models for text [11, 40] and data deduplica- tion has been shown to be an effective mitigation against memorization for those models [47, 41]. In the image domain, simple deduplication is common, where images with identical URLs and captions are removed, but most datasets do not compute other inter-image similarity met- rics such as (cid:96)2 distance or CLIP similarity. We thus en- courage practitioners to deduplicate future datasets using these more advanced notions of duplication. Unfortunately, deduplication is not a perfect solution. To better understand the effectiveness of data deduplica- tion, we deduplicate CIFAR-10 and re-train a diffusion model on this modi\ufb01ed dataset. We compute image sim- ilarity using the imagededup tool and deduplicate any images that have a similarity above > 0.85. This re- moves 5,275 examples from the 50,000 total examples in CIFAR-10. We repeat the same generation procedure as Section 5.1, where we generate 220 images from the model and count how many examples are regenerated from the training set. The model trained on the dedu- plicated data regenerates 986 examples, as compared to 1280 for the original model. While not a substantial drop, these results show that deduplication can mitigate memorization. Moreover, we also expect that deduplica- tion will be much more effective for models trained on larger-scale datasets (e.g., Stable Diffusion), as we ob- served a much stronger correlation between data extrac- tion and duplication rates for those models. 7.2 Differentially-Private Training The gold standard technique to defend against privacy attacks is by training with differential privacy (DP) guar- antees [21, 22]. Diffusion models can be trained with differentially-private stochastic gradient descent (DP- SGD) [1], where the model\u2019s gradients are clipped and noised to prevent the model from leaking substantial in- formation about the presence of any individual image in the dataset. Applying DP-SGD induces a trade-off be- tween privacy and utility, and recent work shows that 14 8 64 16 10Maximum Exposure 2 2 32 6 1 Duplicate Count 4 8 3 4 Random Figure 16: Canary exposure (a measure of non-privacy) as a function of duplicate count. Inserting a canary twice is suf\ufb01cient to reach maximum exposure. DP-SGD can be applied to small-scale diffusion models without substantial performance degradation [19]. Unfortunately, we applied DP-SGD to our diffusion model codebase and found that it caused the training on CIFAR-10 to consistently diverge, even at high values for \u03b5 (the privacy budget, around 50). In fact, even applying a non-trivial gradient clipping or noising on their own (both are required in DP-SGD) caused the training to fail. We leave a further investigation of these failures to future work, and we believe that new advances in DP-SGD and privacy-preserving training techniques may be required to train diffusion models in privacy-sensitive settings. 7.3 Auditing with Canaries In addition to implementing defenses, it is important for practitioners to empirically audit their models to de- termine how vulnerable they are in practice [36]. Our attacks above represent one method to evaluate model privacy. Nevertheless, our attacks are expensive, e.g., our membership inference results require training many shadow models, and thus lighter weight alternatives may be desired. One such alternative is to insert canary examples into the training set, a common approach to evaluate mem- orization in language models [10]. Here, one creates a large \u201cpool\u201d of canaries, e.g., by randomly generating noise images, and inserts a subset of the canaries into the training set. After training, one computes the expo- sure of the canaries, which roughly measures how many bits were learned about the inserted canaries as compared to the larger pool of not inserted canaries. This loss- based metric only requires training one model and can also be designed in a worst-case way (e.g., adversarial worst-case images could be used). To evaluate exposure for diffusion models, we gen- erate canaries consisting of uniformly generated noise. We then duplicate the canaries in the training set at dif- ferent rates and measure the maximum exposure. Fig- ure 16 shows the results. Here, the maximum exposure is 10, and some canaries reach this exposure after being inserted only twice. The exposure is not strictly increas- ing with duplicate count, which may be a result of some canaries being \u201charder\u201d than others, and, ultimately, ran- dom canaries we generate may not"}, {"question": " How does StyleGAN-ADA compare to the best DDPM model in terms of memorization?", "answer": " The best DDPM model memorizes 2\u00d7 more than StyleGAN-ADA but reaches the same FID.", "ref_chunk": "when the GANs reach similar performance, e.g., the best DDPM model memorizes 2\u00d7 more than StyleGAN- ADA but reaches the same FID. Moreover, generative models (both GANs and diffusion models) tend to mem- orize more data as their quality (FID) improves, e.g., StyleGAN-ADA memorizes 3\u00d7 more images than the weakest GANs. Using the GANs we trained ourselves, we show ex- amples of the near-copy generations in Figure 15 for the three GANs that we trained ourselves, and Figure 24 in the Appendix shows every sample that we extract for those models. The Appendix also contains near-copy generations from the \ufb01ve off-the-shelf GANs. Overall, these results further reinforce the conclusion that diffu- sion models are less private than GAN models. We also surprisingly \ufb01nd that diffusion models and GANs memorize many of the same images. In particular, despite the fact that our diffusion model memorizes 1280 images and a StyleGAN model we train on half of the dataset memorizes 361 images, we \ufb01nd that 244 unique images are memorized in common. If images were mem- orized uniformly at random, we should expect on average 10 images would be memorized by both, giving excep- tionally strong evidence that some images (p < 10\u2212261) are inherently less private than others. Understanding why this phenomenon occurs is a fruitful direction for future work. 13 7 Defenses and Recommendations Given the degree to which diffusion models memorize and regenerate training examples, in this section we ex- plore various defenses and practical strategies that may help to reduce and audit model memorization. 7.1 Deduplicating Training Data In Section 4.2, we showed that many examples that are easy to extract are duplicated many times (e.g., > 100) in the training data. Similar results have been shown for language models for text [11, 40] and data deduplica- tion has been shown to be an effective mitigation against memorization for those models [47, 41]. In the image domain, simple deduplication is common, where images with identical URLs and captions are removed, but most datasets do not compute other inter-image similarity met- rics such as (cid:96)2 distance or CLIP similarity. We thus en- courage practitioners to deduplicate future datasets using these more advanced notions of duplication. Unfortunately, deduplication is not a perfect solution. To better understand the effectiveness of data deduplica- tion, we deduplicate CIFAR-10 and re-train a diffusion model on this modi\ufb01ed dataset. We compute image sim- ilarity using the imagededup tool and deduplicate any images that have a similarity above > 0.85. This re- moves 5,275 examples from the 50,000 total examples in CIFAR-10. We repeat the same generation procedure as Section 5.1, where we generate 220 images from the model and count how many examples are regenerated from the training set. The model trained on the dedu- plicated data regenerates 986 examples, as compared to 1280 for the original model. While not a substantial drop, these results show that deduplication can mitigate memorization. Moreover, we also expect that deduplica- tion will be much more effective for models trained on larger-scale datasets (e.g., Stable Diffusion), as we ob- served a much stronger correlation between data extrac- tion and duplication rates for those models. 7.2 Differentially-Private Training The gold standard technique to defend against privacy attacks is by training with differential privacy (DP) guar- antees [21, 22]. Diffusion models can be trained with differentially-private stochastic gradient descent (DP- SGD) [1], where the model\u2019s gradients are clipped and noised to prevent the model from leaking substantial in- formation about the presence of any individual image in the dataset. Applying DP-SGD induces a trade-off be- tween privacy and utility, and recent work shows that 14 8 64 16 10Maximum Exposure 2 2 32 6 1 Duplicate Count 4 8 3 4 Random Figure 16: Canary exposure (a measure of non-privacy) as a function of duplicate count. Inserting a canary twice is suf\ufb01cient to reach maximum exposure. DP-SGD can be applied to small-scale diffusion models without substantial performance degradation [19]. Unfortunately, we applied DP-SGD to our diffusion model codebase and found that it caused the training on CIFAR-10 to consistently diverge, even at high values for \u03b5 (the privacy budget, around 50). In fact, even applying a non-trivial gradient clipping or noising on their own (both are required in DP-SGD) caused the training to fail. We leave a further investigation of these failures to future work, and we believe that new advances in DP-SGD and privacy-preserving training techniques may be required to train diffusion models in privacy-sensitive settings. 7.3 Auditing with Canaries In addition to implementing defenses, it is important for practitioners to empirically audit their models to de- termine how vulnerable they are in practice [36]. Our attacks above represent one method to evaluate model privacy. Nevertheless, our attacks are expensive, e.g., our membership inference results require training many shadow models, and thus lighter weight alternatives may be desired. One such alternative is to insert canary examples into the training set, a common approach to evaluate mem- orization in language models [10]. Here, one creates a large \u201cpool\u201d of canaries, e.g., by randomly generating noise images, and inserts a subset of the canaries into the training set. After training, one computes the expo- sure of the canaries, which roughly measures how many bits were learned about the inserted canaries as compared to the larger pool of not inserted canaries. This loss- based metric only requires training one model and can also be designed in a worst-case way (e.g., adversarial worst-case images could be used). To evaluate exposure for diffusion models, we gen- erate canaries consisting of uniformly generated noise. We then duplicate the canaries in the training set at dif- ferent rates and measure the maximum exposure. Fig- ure 16 shows the results. Here, the maximum exposure is 10, and some canaries reach this exposure after being inserted only twice. The exposure is not strictly increas- ing with duplicate count, which may be a result of some canaries being \u201charder\u201d than others, and, ultimately, ran- dom canaries we generate may not"}, {"question": " What phenomenon does the text suggest is a fruitful direction for future work?", "answer": " The text suggests that understanding why some images are inherently less private than others, despite being memorized by both diffusion models and GANs, is a fruitful direction for future work.", "ref_chunk": "when the GANs reach similar performance, e.g., the best DDPM model memorizes 2\u00d7 more than StyleGAN- ADA but reaches the same FID. Moreover, generative models (both GANs and diffusion models) tend to mem- orize more data as their quality (FID) improves, e.g., StyleGAN-ADA memorizes 3\u00d7 more images than the weakest GANs. Using the GANs we trained ourselves, we show ex- amples of the near-copy generations in Figure 15 for the three GANs that we trained ourselves, and Figure 24 in the Appendix shows every sample that we extract for those models. The Appendix also contains near-copy generations from the \ufb01ve off-the-shelf GANs. Overall, these results further reinforce the conclusion that diffu- sion models are less private than GAN models. We also surprisingly \ufb01nd that diffusion models and GANs memorize many of the same images. In particular, despite the fact that our diffusion model memorizes 1280 images and a StyleGAN model we train on half of the dataset memorizes 361 images, we \ufb01nd that 244 unique images are memorized in common. If images were mem- orized uniformly at random, we should expect on average 10 images would be memorized by both, giving excep- tionally strong evidence that some images (p < 10\u2212261) are inherently less private than others. Understanding why this phenomenon occurs is a fruitful direction for future work. 13 7 Defenses and Recommendations Given the degree to which diffusion models memorize and regenerate training examples, in this section we ex- plore various defenses and practical strategies that may help to reduce and audit model memorization. 7.1 Deduplicating Training Data In Section 4.2, we showed that many examples that are easy to extract are duplicated many times (e.g., > 100) in the training data. Similar results have been shown for language models for text [11, 40] and data deduplica- tion has been shown to be an effective mitigation against memorization for those models [47, 41]. In the image domain, simple deduplication is common, where images with identical URLs and captions are removed, but most datasets do not compute other inter-image similarity met- rics such as (cid:96)2 distance or CLIP similarity. We thus en- courage practitioners to deduplicate future datasets using these more advanced notions of duplication. Unfortunately, deduplication is not a perfect solution. To better understand the effectiveness of data deduplica- tion, we deduplicate CIFAR-10 and re-train a diffusion model on this modi\ufb01ed dataset. We compute image sim- ilarity using the imagededup tool and deduplicate any images that have a similarity above > 0.85. This re- moves 5,275 examples from the 50,000 total examples in CIFAR-10. We repeat the same generation procedure as Section 5.1, where we generate 220 images from the model and count how many examples are regenerated from the training set. The model trained on the dedu- plicated data regenerates 986 examples, as compared to 1280 for the original model. While not a substantial drop, these results show that deduplication can mitigate memorization. Moreover, we also expect that deduplica- tion will be much more effective for models trained on larger-scale datasets (e.g., Stable Diffusion), as we ob- served a much stronger correlation between data extrac- tion and duplication rates for those models. 7.2 Differentially-Private Training The gold standard technique to defend against privacy attacks is by training with differential privacy (DP) guar- antees [21, 22]. Diffusion models can be trained with differentially-private stochastic gradient descent (DP- SGD) [1], where the model\u2019s gradients are clipped and noised to prevent the model from leaking substantial in- formation about the presence of any individual image in the dataset. Applying DP-SGD induces a trade-off be- tween privacy and utility, and recent work shows that 14 8 64 16 10Maximum Exposure 2 2 32 6 1 Duplicate Count 4 8 3 4 Random Figure 16: Canary exposure (a measure of non-privacy) as a function of duplicate count. Inserting a canary twice is suf\ufb01cient to reach maximum exposure. DP-SGD can be applied to small-scale diffusion models without substantial performance degradation [19]. Unfortunately, we applied DP-SGD to our diffusion model codebase and found that it caused the training on CIFAR-10 to consistently diverge, even at high values for \u03b5 (the privacy budget, around 50). In fact, even applying a non-trivial gradient clipping or noising on their own (both are required in DP-SGD) caused the training to fail. We leave a further investigation of these failures to future work, and we believe that new advances in DP-SGD and privacy-preserving training techniques may be required to train diffusion models in privacy-sensitive settings. 7.3 Auditing with Canaries In addition to implementing defenses, it is important for practitioners to empirically audit their models to de- termine how vulnerable they are in practice [36]. Our attacks above represent one method to evaluate model privacy. Nevertheless, our attacks are expensive, e.g., our membership inference results require training many shadow models, and thus lighter weight alternatives may be desired. One such alternative is to insert canary examples into the training set, a common approach to evaluate mem- orization in language models [10]. Here, one creates a large \u201cpool\u201d of canaries, e.g., by randomly generating noise images, and inserts a subset of the canaries into the training set. After training, one computes the expo- sure of the canaries, which roughly measures how many bits were learned about the inserted canaries as compared to the larger pool of not inserted canaries. This loss- based metric only requires training one model and can also be designed in a worst-case way (e.g., adversarial worst-case images could be used). To evaluate exposure for diffusion models, we gen- erate canaries consisting of uniformly generated noise. We then duplicate the canaries in the training set at dif- ferent rates and measure the maximum exposure. Fig- ure 16 shows the results. Here, the maximum exposure is 10, and some canaries reach this exposure after being inserted only twice. The exposure is not strictly increas- ing with duplicate count, which may be a result of some canaries being \u201charder\u201d than others, and, ultimately, ran- dom canaries we generate may not"}, {"question": " What defense strategy is mentioned in the text to reduce and audit model memorization?", "answer": " Deduplicating Training Data is mentioned as a defense strategy to reduce and audit model memorization.", "ref_chunk": "when the GANs reach similar performance, e.g., the best DDPM model memorizes 2\u00d7 more than StyleGAN- ADA but reaches the same FID. Moreover, generative models (both GANs and diffusion models) tend to mem- orize more data as their quality (FID) improves, e.g., StyleGAN-ADA memorizes 3\u00d7 more images than the weakest GANs. Using the GANs we trained ourselves, we show ex- amples of the near-copy generations in Figure 15 for the three GANs that we trained ourselves, and Figure 24 in the Appendix shows every sample that we extract for those models. The Appendix also contains near-copy generations from the \ufb01ve off-the-shelf GANs. Overall, these results further reinforce the conclusion that diffu- sion models are less private than GAN models. We also surprisingly \ufb01nd that diffusion models and GANs memorize many of the same images. In particular, despite the fact that our diffusion model memorizes 1280 images and a StyleGAN model we train on half of the dataset memorizes 361 images, we \ufb01nd that 244 unique images are memorized in common. If images were mem- orized uniformly at random, we should expect on average 10 images would be memorized by both, giving excep- tionally strong evidence that some images (p < 10\u2212261) are inherently less private than others. Understanding why this phenomenon occurs is a fruitful direction for future work. 13 7 Defenses and Recommendations Given the degree to which diffusion models memorize and regenerate training examples, in this section we ex- plore various defenses and practical strategies that may help to reduce and audit model memorization. 7.1 Deduplicating Training Data In Section 4.2, we showed that many examples that are easy to extract are duplicated many times (e.g., > 100) in the training data. Similar results have been shown for language models for text [11, 40] and data deduplica- tion has been shown to be an effective mitigation against memorization for those models [47, 41]. In the image domain, simple deduplication is common, where images with identical URLs and captions are removed, but most datasets do not compute other inter-image similarity met- rics such as (cid:96)2 distance or CLIP similarity. We thus en- courage practitioners to deduplicate future datasets using these more advanced notions of duplication. Unfortunately, deduplication is not a perfect solution. To better understand the effectiveness of data deduplica- tion, we deduplicate CIFAR-10 and re-train a diffusion model on this modi\ufb01ed dataset. We compute image sim- ilarity using the imagededup tool and deduplicate any images that have a similarity above > 0.85. This re- moves 5,275 examples from the 50,000 total examples in CIFAR-10. We repeat the same generation procedure as Section 5.1, where we generate 220 images from the model and count how many examples are regenerated from the training set. The model trained on the dedu- plicated data regenerates 986 examples, as compared to 1280 for the original model. While not a substantial drop, these results show that deduplication can mitigate memorization. Moreover, we also expect that deduplica- tion will be much more effective for models trained on larger-scale datasets (e.g., Stable Diffusion), as we ob- served a much stronger correlation between data extrac- tion and duplication rates for those models. 7.2 Differentially-Private Training The gold standard technique to defend against privacy attacks is by training with differential privacy (DP) guar- antees [21, 22]. Diffusion models can be trained with differentially-private stochastic gradient descent (DP- SGD) [1], where the model\u2019s gradients are clipped and noised to prevent the model from leaking substantial in- formation about the presence of any individual image in the dataset. Applying DP-SGD induces a trade-off be- tween privacy and utility, and recent work shows that 14 8 64 16 10Maximum Exposure 2 2 32 6 1 Duplicate Count 4 8 3 4 Random Figure 16: Canary exposure (a measure of non-privacy) as a function of duplicate count. Inserting a canary twice is suf\ufb01cient to reach maximum exposure. DP-SGD can be applied to small-scale diffusion models without substantial performance degradation [19]. Unfortunately, we applied DP-SGD to our diffusion model codebase and found that it caused the training on CIFAR-10 to consistently diverge, even at high values for \u03b5 (the privacy budget, around 50). In fact, even applying a non-trivial gradient clipping or noising on their own (both are required in DP-SGD) caused the training to fail. We leave a further investigation of these failures to future work, and we believe that new advances in DP-SGD and privacy-preserving training techniques may be required to train diffusion models in privacy-sensitive settings. 7.3 Auditing with Canaries In addition to implementing defenses, it is important for practitioners to empirically audit their models to de- termine how vulnerable they are in practice [36]. Our attacks above represent one method to evaluate model privacy. Nevertheless, our attacks are expensive, e.g., our membership inference results require training many shadow models, and thus lighter weight alternatives may be desired. One such alternative is to insert canary examples into the training set, a common approach to evaluate mem- orization in language models [10]. Here, one creates a large \u201cpool\u201d of canaries, e.g., by randomly generating noise images, and inserts a subset of the canaries into the training set. After training, one computes the expo- sure of the canaries, which roughly measures how many bits were learned about the inserted canaries as compared to the larger pool of not inserted canaries. This loss- based metric only requires training one model and can also be designed in a worst-case way (e.g., adversarial worst-case images could be used). To evaluate exposure for diffusion models, we gen- erate canaries consisting of uniformly generated noise. We then duplicate the canaries in the training set at dif- ferent rates and measure the maximum exposure. Fig- ure 16 shows the results. Here, the maximum exposure is 10, and some canaries reach this exposure after being inserted only twice. The exposure is not strictly increas- ing with duplicate count, which may be a result of some canaries being \u201charder\u201d than others, and, ultimately, ran- dom canaries we generate may not"}, {"question": " How many examples were removed by deduplication from the total examples in CIFAR-10?", "answer": " 5,275 examples were removed by deduplication from the total examples in CIFAR-10.", "ref_chunk": "when the GANs reach similar performance, e.g., the best DDPM model memorizes 2\u00d7 more than StyleGAN- ADA but reaches the same FID. Moreover, generative models (both GANs and diffusion models) tend to mem- orize more data as their quality (FID) improves, e.g., StyleGAN-ADA memorizes 3\u00d7 more images than the weakest GANs. Using the GANs we trained ourselves, we show ex- amples of the near-copy generations in Figure 15 for the three GANs that we trained ourselves, and Figure 24 in the Appendix shows every sample that we extract for those models. The Appendix also contains near-copy generations from the \ufb01ve off-the-shelf GANs. Overall, these results further reinforce the conclusion that diffu- sion models are less private than GAN models. We also surprisingly \ufb01nd that diffusion models and GANs memorize many of the same images. In particular, despite the fact that our diffusion model memorizes 1280 images and a StyleGAN model we train on half of the dataset memorizes 361 images, we \ufb01nd that 244 unique images are memorized in common. If images were mem- orized uniformly at random, we should expect on average 10 images would be memorized by both, giving excep- tionally strong evidence that some images (p < 10\u2212261) are inherently less private than others. Understanding why this phenomenon occurs is a fruitful direction for future work. 13 7 Defenses and Recommendations Given the degree to which diffusion models memorize and regenerate training examples, in this section we ex- plore various defenses and practical strategies that may help to reduce and audit model memorization. 7.1 Deduplicating Training Data In Section 4.2, we showed that many examples that are easy to extract are duplicated many times (e.g., > 100) in the training data. Similar results have been shown for language models for text [11, 40] and data deduplica- tion has been shown to be an effective mitigation against memorization for those models [47, 41]. In the image domain, simple deduplication is common, where images with identical URLs and captions are removed, but most datasets do not compute other inter-image similarity met- rics such as (cid:96)2 distance or CLIP similarity. We thus en- courage practitioners to deduplicate future datasets using these more advanced notions of duplication. Unfortunately, deduplication is not a perfect solution. To better understand the effectiveness of data deduplica- tion, we deduplicate CIFAR-10 and re-train a diffusion model on this modi\ufb01ed dataset. We compute image sim- ilarity using the imagededup tool and deduplicate any images that have a similarity above > 0.85. This re- moves 5,275 examples from the 50,000 total examples in CIFAR-10. We repeat the same generation procedure as Section 5.1, where we generate 220 images from the model and count how many examples are regenerated from the training set. The model trained on the dedu- plicated data regenerates 986 examples, as compared to 1280 for the original model. While not a substantial drop, these results show that deduplication can mitigate memorization. Moreover, we also expect that deduplica- tion will be much more effective for models trained on larger-scale datasets (e.g., Stable Diffusion), as we ob- served a much stronger correlation between data extrac- tion and duplication rates for those models. 7.2 Differentially-Private Training The gold standard technique to defend against privacy attacks is by training with differential privacy (DP) guar- antees [21, 22]. Diffusion models can be trained with differentially-private stochastic gradient descent (DP- SGD) [1], where the model\u2019s gradients are clipped and noised to prevent the model from leaking substantial in- formation about the presence of any individual image in the dataset. Applying DP-SGD induces a trade-off be- tween privacy and utility, and recent work shows that 14 8 64 16 10Maximum Exposure 2 2 32 6 1 Duplicate Count 4 8 3 4 Random Figure 16: Canary exposure (a measure of non-privacy) as a function of duplicate count. Inserting a canary twice is suf\ufb01cient to reach maximum exposure. DP-SGD can be applied to small-scale diffusion models without substantial performance degradation [19]. Unfortunately, we applied DP-SGD to our diffusion model codebase and found that it caused the training on CIFAR-10 to consistently diverge, even at high values for \u03b5 (the privacy budget, around 50). In fact, even applying a non-trivial gradient clipping or noising on their own (both are required in DP-SGD) caused the training to fail. We leave a further investigation of these failures to future work, and we believe that new advances in DP-SGD and privacy-preserving training techniques may be required to train diffusion models in privacy-sensitive settings. 7.3 Auditing with Canaries In addition to implementing defenses, it is important for practitioners to empirically audit their models to de- termine how vulnerable they are in practice [36]. Our attacks above represent one method to evaluate model privacy. Nevertheless, our attacks are expensive, e.g., our membership inference results require training many shadow models, and thus lighter weight alternatives may be desired. One such alternative is to insert canary examples into the training set, a common approach to evaluate mem- orization in language models [10]. Here, one creates a large \u201cpool\u201d of canaries, e.g., by randomly generating noise images, and inserts a subset of the canaries into the training set. After training, one computes the expo- sure of the canaries, which roughly measures how many bits were learned about the inserted canaries as compared to the larger pool of not inserted canaries. This loss- based metric only requires training one model and can also be designed in a worst-case way (e.g., adversarial worst-case images could be used). To evaluate exposure for diffusion models, we gen- erate canaries consisting of uniformly generated noise. We then duplicate the canaries in the training set at dif- ferent rates and measure the maximum exposure. Fig- ure 16 shows the results. Here, the maximum exposure is 10, and some canaries reach this exposure after being inserted only twice. The exposure is not strictly increas- ing with duplicate count, which may be a result of some canaries being \u201charder\u201d than others, and, ultimately, ran- dom canaries we generate may not"}, {"question": " What is the gold standard technique mentioned in the text to defend against privacy attacks?", "answer": " The gold standard technique mentioned is training with differential privacy guarantees (DP).", "ref_chunk": "when the GANs reach similar performance, e.g., the best DDPM model memorizes 2\u00d7 more than StyleGAN- ADA but reaches the same FID. Moreover, generative models (both GANs and diffusion models) tend to mem- orize more data as their quality (FID) improves, e.g., StyleGAN-ADA memorizes 3\u00d7 more images than the weakest GANs. Using the GANs we trained ourselves, we show ex- amples of the near-copy generations in Figure 15 for the three GANs that we trained ourselves, and Figure 24 in the Appendix shows every sample that we extract for those models. The Appendix also contains near-copy generations from the \ufb01ve off-the-shelf GANs. Overall, these results further reinforce the conclusion that diffu- sion models are less private than GAN models. We also surprisingly \ufb01nd that diffusion models and GANs memorize many of the same images. In particular, despite the fact that our diffusion model memorizes 1280 images and a StyleGAN model we train on half of the dataset memorizes 361 images, we \ufb01nd that 244 unique images are memorized in common. If images were mem- orized uniformly at random, we should expect on average 10 images would be memorized by both, giving excep- tionally strong evidence that some images (p < 10\u2212261) are inherently less private than others. Understanding why this phenomenon occurs is a fruitful direction for future work. 13 7 Defenses and Recommendations Given the degree to which diffusion models memorize and regenerate training examples, in this section we ex- plore various defenses and practical strategies that may help to reduce and audit model memorization. 7.1 Deduplicating Training Data In Section 4.2, we showed that many examples that are easy to extract are duplicated many times (e.g., > 100) in the training data. Similar results have been shown for language models for text [11, 40] and data deduplica- tion has been shown to be an effective mitigation against memorization for those models [47, 41]. In the image domain, simple deduplication is common, where images with identical URLs and captions are removed, but most datasets do not compute other inter-image similarity met- rics such as (cid:96)2 distance or CLIP similarity. We thus en- courage practitioners to deduplicate future datasets using these more advanced notions of duplication. Unfortunately, deduplication is not a perfect solution. To better understand the effectiveness of data deduplica- tion, we deduplicate CIFAR-10 and re-train a diffusion model on this modi\ufb01ed dataset. We compute image sim- ilarity using the imagededup tool and deduplicate any images that have a similarity above > 0.85. This re- moves 5,275 examples from the 50,000 total examples in CIFAR-10. We repeat the same generation procedure as Section 5.1, where we generate 220 images from the model and count how many examples are regenerated from the training set. The model trained on the dedu- plicated data regenerates 986 examples, as compared to 1280 for the original model. While not a substantial drop, these results show that deduplication can mitigate memorization. Moreover, we also expect that deduplica- tion will be much more effective for models trained on larger-scale datasets (e.g., Stable Diffusion), as we ob- served a much stronger correlation between data extrac- tion and duplication rates for those models. 7.2 Differentially-Private Training The gold standard technique to defend against privacy attacks is by training with differential privacy (DP) guar- antees [21, 22]. Diffusion models can be trained with differentially-private stochastic gradient descent (DP- SGD) [1], where the model\u2019s gradients are clipped and noised to prevent the model from leaking substantial in- formation about the presence of any individual image in the dataset. Applying DP-SGD induces a trade-off be- tween privacy and utility, and recent work shows that 14 8 64 16 10Maximum Exposure 2 2 32 6 1 Duplicate Count 4 8 3 4 Random Figure 16: Canary exposure (a measure of non-privacy) as a function of duplicate count. Inserting a canary twice is suf\ufb01cient to reach maximum exposure. DP-SGD can be applied to small-scale diffusion models without substantial performance degradation [19]. Unfortunately, we applied DP-SGD to our diffusion model codebase and found that it caused the training on CIFAR-10 to consistently diverge, even at high values for \u03b5 (the privacy budget, around 50). In fact, even applying a non-trivial gradient clipping or noising on their own (both are required in DP-SGD) caused the training to fail. We leave a further investigation of these failures to future work, and we believe that new advances in DP-SGD and privacy-preserving training techniques may be required to train diffusion models in privacy-sensitive settings. 7.3 Auditing with Canaries In addition to implementing defenses, it is important for practitioners to empirically audit their models to de- termine how vulnerable they are in practice [36]. Our attacks above represent one method to evaluate model privacy. Nevertheless, our attacks are expensive, e.g., our membership inference results require training many shadow models, and thus lighter weight alternatives may be desired. One such alternative is to insert canary examples into the training set, a common approach to evaluate mem- orization in language models [10]. Here, one creates a large \u201cpool\u201d of canaries, e.g., by randomly generating noise images, and inserts a subset of the canaries into the training set. After training, one computes the expo- sure of the canaries, which roughly measures how many bits were learned about the inserted canaries as compared to the larger pool of not inserted canaries. This loss- based metric only requires training one model and can also be designed in a worst-case way (e.g., adversarial worst-case images could be used). To evaluate exposure for diffusion models, we gen- erate canaries consisting of uniformly generated noise. We then duplicate the canaries in the training set at dif- ferent rates and measure the maximum exposure. Fig- ure 16 shows the results. Here, the maximum exposure is 10, and some canaries reach this exposure after being inserted only twice. The exposure is not strictly increas- ing with duplicate count, which may be a result of some canaries being \u201charder\u201d than others, and, ultimately, ran- dom canaries we generate may not"}, {"question": " What was the outcome when applying DP-SGD to the diffusion model codebase in the text?", "answer": " Applying DP-SGD to the diffusion model codebase in the text caused the training on CIFAR-10 to consistently diverge.", "ref_chunk": "when the GANs reach similar performance, e.g., the best DDPM model memorizes 2\u00d7 more than StyleGAN- ADA but reaches the same FID. Moreover, generative models (both GANs and diffusion models) tend to mem- orize more data as their quality (FID) improves, e.g., StyleGAN-ADA memorizes 3\u00d7 more images than the weakest GANs. Using the GANs we trained ourselves, we show ex- amples of the near-copy generations in Figure 15 for the three GANs that we trained ourselves, and Figure 24 in the Appendix shows every sample that we extract for those models. The Appendix also contains near-copy generations from the \ufb01ve off-the-shelf GANs. Overall, these results further reinforce the conclusion that diffu- sion models are less private than GAN models. We also surprisingly \ufb01nd that diffusion models and GANs memorize many of the same images. In particular, despite the fact that our diffusion model memorizes 1280 images and a StyleGAN model we train on half of the dataset memorizes 361 images, we \ufb01nd that 244 unique images are memorized in common. If images were mem- orized uniformly at random, we should expect on average 10 images would be memorized by both, giving excep- tionally strong evidence that some images (p < 10\u2212261) are inherently less private than others. Understanding why this phenomenon occurs is a fruitful direction for future work. 13 7 Defenses and Recommendations Given the degree to which diffusion models memorize and regenerate training examples, in this section we ex- plore various defenses and practical strategies that may help to reduce and audit model memorization. 7.1 Deduplicating Training Data In Section 4.2, we showed that many examples that are easy to extract are duplicated many times (e.g., > 100) in the training data. Similar results have been shown for language models for text [11, 40] and data deduplica- tion has been shown to be an effective mitigation against memorization for those models [47, 41]. In the image domain, simple deduplication is common, where images with identical URLs and captions are removed, but most datasets do not compute other inter-image similarity met- rics such as (cid:96)2 distance or CLIP similarity. We thus en- courage practitioners to deduplicate future datasets using these more advanced notions of duplication. Unfortunately, deduplication is not a perfect solution. To better understand the effectiveness of data deduplica- tion, we deduplicate CIFAR-10 and re-train a diffusion model on this modi\ufb01ed dataset. We compute image sim- ilarity using the imagededup tool and deduplicate any images that have a similarity above > 0.85. This re- moves 5,275 examples from the 50,000 total examples in CIFAR-10. We repeat the same generation procedure as Section 5.1, where we generate 220 images from the model and count how many examples are regenerated from the training set. The model trained on the dedu- plicated data regenerates 986 examples, as compared to 1280 for the original model. While not a substantial drop, these results show that deduplication can mitigate memorization. Moreover, we also expect that deduplica- tion will be much more effective for models trained on larger-scale datasets (e.g., Stable Diffusion), as we ob- served a much stronger correlation between data extrac- tion and duplication rates for those models. 7.2 Differentially-Private Training The gold standard technique to defend against privacy attacks is by training with differential privacy (DP) guar- antees [21, 22]. Diffusion models can be trained with differentially-private stochastic gradient descent (DP- SGD) [1], where the model\u2019s gradients are clipped and noised to prevent the model from leaking substantial in- formation about the presence of any individual image in the dataset. Applying DP-SGD induces a trade-off be- tween privacy and utility, and recent work shows that 14 8 64 16 10Maximum Exposure 2 2 32 6 1 Duplicate Count 4 8 3 4 Random Figure 16: Canary exposure (a measure of non-privacy) as a function of duplicate count. Inserting a canary twice is suf\ufb01cient to reach maximum exposure. DP-SGD can be applied to small-scale diffusion models without substantial performance degradation [19]. Unfortunately, we applied DP-SGD to our diffusion model codebase and found that it caused the training on CIFAR-10 to consistently diverge, even at high values for \u03b5 (the privacy budget, around 50). In fact, even applying a non-trivial gradient clipping or noising on their own (both are required in DP-SGD) caused the training to fail. We leave a further investigation of these failures to future work, and we believe that new advances in DP-SGD and privacy-preserving training techniques may be required to train diffusion models in privacy-sensitive settings. 7.3 Auditing with Canaries In addition to implementing defenses, it is important for practitioners to empirically audit their models to de- termine how vulnerable they are in practice [36]. Our attacks above represent one method to evaluate model privacy. Nevertheless, our attacks are expensive, e.g., our membership inference results require training many shadow models, and thus lighter weight alternatives may be desired. One such alternative is to insert canary examples into the training set, a common approach to evaluate mem- orization in language models [10]. Here, one creates a large \u201cpool\u201d of canaries, e.g., by randomly generating noise images, and inserts a subset of the canaries into the training set. After training, one computes the expo- sure of the canaries, which roughly measures how many bits were learned about the inserted canaries as compared to the larger pool of not inserted canaries. This loss- based metric only requires training one model and can also be designed in a worst-case way (e.g., adversarial worst-case images could be used). To evaluate exposure for diffusion models, we gen- erate canaries consisting of uniformly generated noise. We then duplicate the canaries in the training set at dif- ferent rates and measure the maximum exposure. Fig- ure 16 shows the results. Here, the maximum exposure is 10, and some canaries reach this exposure after being inserted only twice. The exposure is not strictly increas- ing with duplicate count, which may be a result of some canaries being \u201charder\u201d than others, and, ultimately, ran- dom canaries we generate may not"}, {"question": " What is the purpose of inserting canary examples into the training set, as mentioned in the text?", "answer": " The purpose is to evaluate memorization in the models and determine their vulnerability.", "ref_chunk": "when the GANs reach similar performance, e.g., the best DDPM model memorizes 2\u00d7 more than StyleGAN- ADA but reaches the same FID. Moreover, generative models (both GANs and diffusion models) tend to mem- orize more data as their quality (FID) improves, e.g., StyleGAN-ADA memorizes 3\u00d7 more images than the weakest GANs. Using the GANs we trained ourselves, we show ex- amples of the near-copy generations in Figure 15 for the three GANs that we trained ourselves, and Figure 24 in the Appendix shows every sample that we extract for those models. The Appendix also contains near-copy generations from the \ufb01ve off-the-shelf GANs. Overall, these results further reinforce the conclusion that diffu- sion models are less private than GAN models. We also surprisingly \ufb01nd that diffusion models and GANs memorize many of the same images. In particular, despite the fact that our diffusion model memorizes 1280 images and a StyleGAN model we train on half of the dataset memorizes 361 images, we \ufb01nd that 244 unique images are memorized in common. If images were mem- orized uniformly at random, we should expect on average 10 images would be memorized by both, giving excep- tionally strong evidence that some images (p < 10\u2212261) are inherently less private than others. Understanding why this phenomenon occurs is a fruitful direction for future work. 13 7 Defenses and Recommendations Given the degree to which diffusion models memorize and regenerate training examples, in this section we ex- plore various defenses and practical strategies that may help to reduce and audit model memorization. 7.1 Deduplicating Training Data In Section 4.2, we showed that many examples that are easy to extract are duplicated many times (e.g., > 100) in the training data. Similar results have been shown for language models for text [11, 40] and data deduplica- tion has been shown to be an effective mitigation against memorization for those models [47, 41]. In the image domain, simple deduplication is common, where images with identical URLs and captions are removed, but most datasets do not compute other inter-image similarity met- rics such as (cid:96)2 distance or CLIP similarity. We thus en- courage practitioners to deduplicate future datasets using these more advanced notions of duplication. Unfortunately, deduplication is not a perfect solution. To better understand the effectiveness of data deduplica- tion, we deduplicate CIFAR-10 and re-train a diffusion model on this modi\ufb01ed dataset. We compute image sim- ilarity using the imagededup tool and deduplicate any images that have a similarity above > 0.85. This re- moves 5,275 examples from the 50,000 total examples in CIFAR-10. We repeat the same generation procedure as Section 5.1, where we generate 220 images from the model and count how many examples are regenerated from the training set. The model trained on the dedu- plicated data regenerates 986 examples, as compared to 1280 for the original model. While not a substantial drop, these results show that deduplication can mitigate memorization. Moreover, we also expect that deduplica- tion will be much more effective for models trained on larger-scale datasets (e.g., Stable Diffusion), as we ob- served a much stronger correlation between data extrac- tion and duplication rates for those models. 7.2 Differentially-Private Training The gold standard technique to defend against privacy attacks is by training with differential privacy (DP) guar- antees [21, 22]. Diffusion models can be trained with differentially-private stochastic gradient descent (DP- SGD) [1], where the model\u2019s gradients are clipped and noised to prevent the model from leaking substantial in- formation about the presence of any individual image in the dataset. Applying DP-SGD induces a trade-off be- tween privacy and utility, and recent work shows that 14 8 64 16 10Maximum Exposure 2 2 32 6 1 Duplicate Count 4 8 3 4 Random Figure 16: Canary exposure (a measure of non-privacy) as a function of duplicate count. Inserting a canary twice is suf\ufb01cient to reach maximum exposure. DP-SGD can be applied to small-scale diffusion models without substantial performance degradation [19]. Unfortunately, we applied DP-SGD to our diffusion model codebase and found that it caused the training on CIFAR-10 to consistently diverge, even at high values for \u03b5 (the privacy budget, around 50). In fact, even applying a non-trivial gradient clipping or noising on their own (both are required in DP-SGD) caused the training to fail. We leave a further investigation of these failures to future work, and we believe that new advances in DP-SGD and privacy-preserving training techniques may be required to train diffusion models in privacy-sensitive settings. 7.3 Auditing with Canaries In addition to implementing defenses, it is important for practitioners to empirically audit their models to de- termine how vulnerable they are in practice [36]. Our attacks above represent one method to evaluate model privacy. Nevertheless, our attacks are expensive, e.g., our membership inference results require training many shadow models, and thus lighter weight alternatives may be desired. One such alternative is to insert canary examples into the training set, a common approach to evaluate mem- orization in language models [10]. Here, one creates a large \u201cpool\u201d of canaries, e.g., by randomly generating noise images, and inserts a subset of the canaries into the training set. After training, one computes the expo- sure of the canaries, which roughly measures how many bits were learned about the inserted canaries as compared to the larger pool of not inserted canaries. This loss- based metric only requires training one model and can also be designed in a worst-case way (e.g., adversarial worst-case images could be used). To evaluate exposure for diffusion models, we gen- erate canaries consisting of uniformly generated noise. We then duplicate the canaries in the training set at dif- ferent rates and measure the maximum exposure. Fig- ure 16 shows the results. Here, the maximum exposure is 10, and some canaries reach this exposure after being inserted only twice. The exposure is not strictly increas- ing with duplicate count, which may be a result of some canaries being \u201charder\u201d than others, and, ultimately, ran- dom canaries we generate may not"}, {"question": " What are canaries in the context of model auditing, as described in the text?", "answer": " Canaries are examples inserted into the training set to measure how many bits were learned about them compared to other examples.", "ref_chunk": "when the GANs reach similar performance, e.g., the best DDPM model memorizes 2\u00d7 more than StyleGAN- ADA but reaches the same FID. Moreover, generative models (both GANs and diffusion models) tend to mem- orize more data as their quality (FID) improves, e.g., StyleGAN-ADA memorizes 3\u00d7 more images than the weakest GANs. Using the GANs we trained ourselves, we show ex- amples of the near-copy generations in Figure 15 for the three GANs that we trained ourselves, and Figure 24 in the Appendix shows every sample that we extract for those models. The Appendix also contains near-copy generations from the \ufb01ve off-the-shelf GANs. Overall, these results further reinforce the conclusion that diffu- sion models are less private than GAN models. We also surprisingly \ufb01nd that diffusion models and GANs memorize many of the same images. In particular, despite the fact that our diffusion model memorizes 1280 images and a StyleGAN model we train on half of the dataset memorizes 361 images, we \ufb01nd that 244 unique images are memorized in common. If images were mem- orized uniformly at random, we should expect on average 10 images would be memorized by both, giving excep- tionally strong evidence that some images (p < 10\u2212261) are inherently less private than others. Understanding why this phenomenon occurs is a fruitful direction for future work. 13 7 Defenses and Recommendations Given the degree to which diffusion models memorize and regenerate training examples, in this section we ex- plore various defenses and practical strategies that may help to reduce and audit model memorization. 7.1 Deduplicating Training Data In Section 4.2, we showed that many examples that are easy to extract are duplicated many times (e.g., > 100) in the training data. Similar results have been shown for language models for text [11, 40] and data deduplica- tion has been shown to be an effective mitigation against memorization for those models [47, 41]. In the image domain, simple deduplication is common, where images with identical URLs and captions are removed, but most datasets do not compute other inter-image similarity met- rics such as (cid:96)2 distance or CLIP similarity. We thus en- courage practitioners to deduplicate future datasets using these more advanced notions of duplication. Unfortunately, deduplication is not a perfect solution. To better understand the effectiveness of data deduplica- tion, we deduplicate CIFAR-10 and re-train a diffusion model on this modi\ufb01ed dataset. We compute image sim- ilarity using the imagededup tool and deduplicate any images that have a similarity above > 0.85. This re- moves 5,275 examples from the 50,000 total examples in CIFAR-10. We repeat the same generation procedure as Section 5.1, where we generate 220 images from the model and count how many examples are regenerated from the training set. The model trained on the dedu- plicated data regenerates 986 examples, as compared to 1280 for the original model. While not a substantial drop, these results show that deduplication can mitigate memorization. Moreover, we also expect that deduplica- tion will be much more effective for models trained on larger-scale datasets (e.g., Stable Diffusion), as we ob- served a much stronger correlation between data extrac- tion and duplication rates for those models. 7.2 Differentially-Private Training The gold standard technique to defend against privacy attacks is by training with differential privacy (DP) guar- antees [21, 22]. Diffusion models can be trained with differentially-private stochastic gradient descent (DP- SGD) [1], where the model\u2019s gradients are clipped and noised to prevent the model from leaking substantial in- formation about the presence of any individual image in the dataset. Applying DP-SGD induces a trade-off be- tween privacy and utility, and recent work shows that 14 8 64 16 10Maximum Exposure 2 2 32 6 1 Duplicate Count 4 8 3 4 Random Figure 16: Canary exposure (a measure of non-privacy) as a function of duplicate count. Inserting a canary twice is suf\ufb01cient to reach maximum exposure. DP-SGD can be applied to small-scale diffusion models without substantial performance degradation [19]. Unfortunately, we applied DP-SGD to our diffusion model codebase and found that it caused the training on CIFAR-10 to consistently diverge, even at high values for \u03b5 (the privacy budget, around 50). In fact, even applying a non-trivial gradient clipping or noising on their own (both are required in DP-SGD) caused the training to fail. We leave a further investigation of these failures to future work, and we believe that new advances in DP-SGD and privacy-preserving training techniques may be required to train diffusion models in privacy-sensitive settings. 7.3 Auditing with Canaries In addition to implementing defenses, it is important for practitioners to empirically audit their models to de- termine how vulnerable they are in practice [36]. Our attacks above represent one method to evaluate model privacy. Nevertheless, our attacks are expensive, e.g., our membership inference results require training many shadow models, and thus lighter weight alternatives may be desired. One such alternative is to insert canary examples into the training set, a common approach to evaluate mem- orization in language models [10]. Here, one creates a large \u201cpool\u201d of canaries, e.g., by randomly generating noise images, and inserts a subset of the canaries into the training set. After training, one computes the expo- sure of the canaries, which roughly measures how many bits were learned about the inserted canaries as compared to the larger pool of not inserted canaries. This loss- based metric only requires training one model and can also be designed in a worst-case way (e.g., adversarial worst-case images could be used). To evaluate exposure for diffusion models, we gen- erate canaries consisting of uniformly generated noise. We then duplicate the canaries in the training set at dif- ferent rates and measure the maximum exposure. Fig- ure 16 shows the results. Here, the maximum exposure is 10, and some canaries reach this exposure after being inserted only twice. The exposure is not strictly increas- ing with duplicate count, which may be a result of some canaries being \u201charder\u201d than others, and, ultimately, ran- dom canaries we generate may not"}, {"question": " What does Figure 16 in the text illustrate in terms of canary exposure?", "answer": " Figure 16 shows that inserting a canary twice is sufficient to reach maximum exposure, and the exposure is not strictly increasing with duplicate count.", "ref_chunk": "when the GANs reach similar performance, e.g., the best DDPM model memorizes 2\u00d7 more than StyleGAN- ADA but reaches the same FID. Moreover, generative models (both GANs and diffusion models) tend to mem- orize more data as their quality (FID) improves, e.g., StyleGAN-ADA memorizes 3\u00d7 more images than the weakest GANs. Using the GANs we trained ourselves, we show ex- amples of the near-copy generations in Figure 15 for the three GANs that we trained ourselves, and Figure 24 in the Appendix shows every sample that we extract for those models. The Appendix also contains near-copy generations from the \ufb01ve off-the-shelf GANs. Overall, these results further reinforce the conclusion that diffu- sion models are less private than GAN models. We also surprisingly \ufb01nd that diffusion models and GANs memorize many of the same images. In particular, despite the fact that our diffusion model memorizes 1280 images and a StyleGAN model we train on half of the dataset memorizes 361 images, we \ufb01nd that 244 unique images are memorized in common. If images were mem- orized uniformly at random, we should expect on average 10 images would be memorized by both, giving excep- tionally strong evidence that some images (p < 10\u2212261) are inherently less private than others. Understanding why this phenomenon occurs is a fruitful direction for future work. 13 7 Defenses and Recommendations Given the degree to which diffusion models memorize and regenerate training examples, in this section we ex- plore various defenses and practical strategies that may help to reduce and audit model memorization. 7.1 Deduplicating Training Data In Section 4.2, we showed that many examples that are easy to extract are duplicated many times (e.g., > 100) in the training data. Similar results have been shown for language models for text [11, 40] and data deduplica- tion has been shown to be an effective mitigation against memorization for those models [47, 41]. In the image domain, simple deduplication is common, where images with identical URLs and captions are removed, but most datasets do not compute other inter-image similarity met- rics such as (cid:96)2 distance or CLIP similarity. We thus en- courage practitioners to deduplicate future datasets using these more advanced notions of duplication. Unfortunately, deduplication is not a perfect solution. To better understand the effectiveness of data deduplica- tion, we deduplicate CIFAR-10 and re-train a diffusion model on this modi\ufb01ed dataset. We compute image sim- ilarity using the imagededup tool and deduplicate any images that have a similarity above > 0.85. This re- moves 5,275 examples from the 50,000 total examples in CIFAR-10. We repeat the same generation procedure as Section 5.1, where we generate 220 images from the model and count how many examples are regenerated from the training set. The model trained on the dedu- plicated data regenerates 986 examples, as compared to 1280 for the original model. While not a substantial drop, these results show that deduplication can mitigate memorization. Moreover, we also expect that deduplica- tion will be much more effective for models trained on larger-scale datasets (e.g., Stable Diffusion), as we ob- served a much stronger correlation between data extrac- tion and duplication rates for those models. 7.2 Differentially-Private Training The gold standard technique to defend against privacy attacks is by training with differential privacy (DP) guar- antees [21, 22]. Diffusion models can be trained with differentially-private stochastic gradient descent (DP- SGD) [1], where the model\u2019s gradients are clipped and noised to prevent the model from leaking substantial in- formation about the presence of any individual image in the dataset. Applying DP-SGD induces a trade-off be- tween privacy and utility, and recent work shows that 14 8 64 16 10Maximum Exposure 2 2 32 6 1 Duplicate Count 4 8 3 4 Random Figure 16: Canary exposure (a measure of non-privacy) as a function of duplicate count. Inserting a canary twice is suf\ufb01cient to reach maximum exposure. DP-SGD can be applied to small-scale diffusion models without substantial performance degradation [19]. Unfortunately, we applied DP-SGD to our diffusion model codebase and found that it caused the training on CIFAR-10 to consistently diverge, even at high values for \u03b5 (the privacy budget, around 50). In fact, even applying a non-trivial gradient clipping or noising on their own (both are required in DP-SGD) caused the training to fail. We leave a further investigation of these failures to future work, and we believe that new advances in DP-SGD and privacy-preserving training techniques may be required to train diffusion models in privacy-sensitive settings. 7.3 Auditing with Canaries In addition to implementing defenses, it is important for practitioners to empirically audit their models to de- termine how vulnerable they are in practice [36]. Our attacks above represent one method to evaluate model privacy. Nevertheless, our attacks are expensive, e.g., our membership inference results require training many shadow models, and thus lighter weight alternatives may be desired. One such alternative is to insert canary examples into the training set, a common approach to evaluate mem- orization in language models [10]. Here, one creates a large \u201cpool\u201d of canaries, e.g., by randomly generating noise images, and inserts a subset of the canaries into the training set. After training, one computes the expo- sure of the canaries, which roughly measures how many bits were learned about the inserted canaries as compared to the larger pool of not inserted canaries. This loss- based metric only requires training one model and can also be designed in a worst-case way (e.g., adversarial worst-case images could be used). To evaluate exposure for diffusion models, we gen- erate canaries consisting of uniformly generated noise. We then duplicate the canaries in the training set at dif- ferent rates and measure the maximum exposure. Fig- ure 16 shows the results. Here, the maximum exposure is 10, and some canaries reach this exposure after being inserted only twice. The exposure is not strictly increas- ing with duplicate count, which may be a result of some canaries being \u201charder\u201d than others, and, ultimately, ran- dom canaries we generate may not"}], "doc_text": "when the GANs reach similar performance, e.g., the best DDPM model memorizes 2\u00d7 more than StyleGAN- ADA but reaches the same FID. Moreover, generative models (both GANs and diffusion models) tend to mem- orize more data as their quality (FID) improves, e.g., StyleGAN-ADA memorizes 3\u00d7 more images than the weakest GANs. Using the GANs we trained ourselves, we show ex- amples of the near-copy generations in Figure 15 for the three GANs that we trained ourselves, and Figure 24 in the Appendix shows every sample that we extract for those models. The Appendix also contains near-copy generations from the \ufb01ve off-the-shelf GANs. Overall, these results further reinforce the conclusion that diffu- sion models are less private than GAN models. We also surprisingly \ufb01nd that diffusion models and GANs memorize many of the same images. In particular, despite the fact that our diffusion model memorizes 1280 images and a StyleGAN model we train on half of the dataset memorizes 361 images, we \ufb01nd that 244 unique images are memorized in common. If images were mem- orized uniformly at random, we should expect on average 10 images would be memorized by both, giving excep- tionally strong evidence that some images (p < 10\u2212261) are inherently less private than others. Understanding why this phenomenon occurs is a fruitful direction for future work. 13 7 Defenses and Recommendations Given the degree to which diffusion models memorize and regenerate training examples, in this section we ex- plore various defenses and practical strategies that may help to reduce and audit model memorization. 7.1 Deduplicating Training Data In Section 4.2, we showed that many examples that are easy to extract are duplicated many times (e.g., > 100) in the training data. Similar results have been shown for language models for text [11, 40] and data deduplica- tion has been shown to be an effective mitigation against memorization for those models [47, 41]. In the image domain, simple deduplication is common, where images with identical URLs and captions are removed, but most datasets do not compute other inter-image similarity met- rics such as (cid:96)2 distance or CLIP similarity. We thus en- courage practitioners to deduplicate future datasets using these more advanced notions of duplication. Unfortunately, deduplication is not a perfect solution. To better understand the effectiveness of data deduplica- tion, we deduplicate CIFAR-10 and re-train a diffusion model on this modi\ufb01ed dataset. We compute image sim- ilarity using the imagededup tool and deduplicate any images that have a similarity above > 0.85. This re- moves 5,275 examples from the 50,000 total examples in CIFAR-10. We repeat the same generation procedure as Section 5.1, where we generate 220 images from the model and count how many examples are regenerated from the training set. The model trained on the dedu- plicated data regenerates 986 examples, as compared to 1280 for the original model. While not a substantial drop, these results show that deduplication can mitigate memorization. Moreover, we also expect that deduplica- tion will be much more effective for models trained on larger-scale datasets (e.g., Stable Diffusion), as we ob- served a much stronger correlation between data extrac- tion and duplication rates for those models. 7.2 Differentially-Private Training The gold standard technique to defend against privacy attacks is by training with differential privacy (DP) guar- antees [21, 22]. Diffusion models can be trained with differentially-private stochastic gradient descent (DP- SGD) [1], where the model\u2019s gradients are clipped and noised to prevent the model from leaking substantial in- formation about the presence of any individual image in the dataset. Applying DP-SGD induces a trade-off be- tween privacy and utility, and recent work shows that 14 8 64 16 10Maximum Exposure 2 2 32 6 1 Duplicate Count 4 8 3 4 Random Figure 16: Canary exposure (a measure of non-privacy) as a function of duplicate count. Inserting a canary twice is suf\ufb01cient to reach maximum exposure. DP-SGD can be applied to small-scale diffusion models without substantial performance degradation [19]. Unfortunately, we applied DP-SGD to our diffusion model codebase and found that it caused the training on CIFAR-10 to consistently diverge, even at high values for \u03b5 (the privacy budget, around 50). In fact, even applying a non-trivial gradient clipping or noising on their own (both are required in DP-SGD) caused the training to fail. We leave a further investigation of these failures to future work, and we believe that new advances in DP-SGD and privacy-preserving training techniques may be required to train diffusion models in privacy-sensitive settings. 7.3 Auditing with Canaries In addition to implementing defenses, it is important for practitioners to empirically audit their models to de- termine how vulnerable they are in practice [36]. Our attacks above represent one method to evaluate model privacy. Nevertheless, our attacks are expensive, e.g., our membership inference results require training many shadow models, and thus lighter weight alternatives may be desired. One such alternative is to insert canary examples into the training set, a common approach to evaluate mem- orization in language models [10]. Here, one creates a large \u201cpool\u201d of canaries, e.g., by randomly generating noise images, and inserts a subset of the canaries into the training set. After training, one computes the expo- sure of the canaries, which roughly measures how many bits were learned about the inserted canaries as compared to the larger pool of not inserted canaries. This loss- based metric only requires training one model and can also be designed in a worst-case way (e.g., adversarial worst-case images could be used). To evaluate exposure for diffusion models, we gen- erate canaries consisting of uniformly generated noise. We then duplicate the canaries in the training set at dif- ferent rates and measure the maximum exposure. Fig- ure 16 shows the results. Here, the maximum exposure is 10, and some canaries reach this exposure after being inserted only twice. The exposure is not strictly increas- ing with duplicate count, which may be a result of some canaries being \u201charder\u201d than others, and, ultimately, ran- dom canaries we generate may not"}