{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Justine_Cassell_When_to_generate_hedges_in_peer-tutoring_interactions_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What method was used to address the imbalance in the dataset?", "answer": " The Synthetic Minority Over-sampling Technique (SMOTE) was used to augment the learning process.", "ref_chunk": "the Extreme Gradient Boosting (XG- Boost) algorithm (Chen and Guestrin, 2016), which is a decision tree-based ensemble machine learning In order to address the imbalance in our dataset, where the ratio of hedge to non-hedge instances is approximately 1:10, we used the Synthetic Minor- ity Over-sampling Technique (SMOTE) (Chawla et al., 2002) for each model to augment our learn- ing process. SMOTE is a popular method that generates synthetic examples in a dataset to coun- teract its imbalance. Given the variable nature of model performance, we implemented a 5-fold cross-validation strategy to evaluate the models. In order to account for the imbalanced nature of the dataset, we opted to use a lower number of folds in Models F1-score Precision Recall LightGBM (w/o emb) XGBoost (w/o emb) 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.17 (\u00b10.03) 0.16 (\u00b10.03) 0.45 (\u00b10.07) 0.45 (\u00b10.07) MLP MLP (only emb) MLP (w/o emb) 0.25 (\u00b10.06) 0.26 (\u00b10.05) 0.26 (\u00b10.06) 0.16 (\u00b10.03) 0.16 (\u00b10.02) 0.17 (\u00b10.06) 0.52 (\u00b10.07) 0.74 (\u00b10.06) 0.56 (\u00b10.07) LSTM LSTM (only emb) LSTM (w/o emb) AttnLSTM AttnLSTM (only emb) AttnLSTM (w/o emb) 0.25 (\u00b10.06) 0.28 (\u00b10.07) 0.25 (\u00b10.05) 0.24 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.06) 0.16 (\u00b10.03) 0.19 (\u00b10.08) 0.15 (\u00b10.02) 0.15 (\u00b10.03) 0.17 (\u00b10.03) 0.15 (\u00b10.07) 0.50 (\u00b10.07) 0.52 (\u00b10.07) 0.75 (\u00b10.06) 0.57 (\u00b10.07) 0.45 (\u00b10.07) 0.57 (\u00b10.07) Dummy 0.11 (\u00b10.08) 0.14 (\u00b10.06) 0.10 (\u00b10.04) Table 1: Comparison of MLP and LSTM models for predicting hedges the cross-validation process. By choosing 5 folds instead of a higher number, we aimed to ensure that each fold would contain a sufficient represen- tation of samples from each class. The model that delivered the best performance during this cross- validation process was then chosen to make pre- dictions on the test set. For the neural models, we adjusted the loss function to account for class im- balance, thereby compelling the models to accom- modate less frequent classes more effectively. The code is available in https://github.com/ neuromaancer/hedge_prediction scores for F1 scores, precision and recall, indicat- ing limited performance in terms of balanced pre- cision and recall. The MLP models, particularly those using only embeddings, showed a remark- able recall of 74%, but at the cost of reduced pre- cision. The LSTM model using only turn embed- dings demonstrated balanced performance across all metrics, achieving the highest precision of 19% and a competitive F1 score of 0.28. However, the attention-based LSTM (AttnLSTM) model did not significantly outperform the standard LSTM model in any metric. 4 Results 4.1 Classification Results To answer the research question 1, we conducted classification experiments on different models. Ta- ble 1 offers an in-depth comparison of multiple machine learning models for predicting hedges in a peer-tutoring dataset. We also incorporated a dummy classifier for comparison, which generates predictions in accordance with the class distribu- tion observed in the training set. The performance metrics are F1 score, precision and recall, all of which include confidence intervals (\u03b1 = 0.05). The dataset is composed of several types of input features described in Section 3.3. The models used different combinations of these inputs. (w/o emb) indicates that the model uses only the features with- out turn embeddings. If not specified, the model uses all features plus turn embeddings. From Table 1, the LightGBM and XGBoost mod- els without embeddings achieved relatively low The inclusion of turn embeddings significantly impacts model performance. Models with only em- beddings perform better in terms of F1 score and recall, suggesting that the semantic information captured in these embeddings, which represented the semantic information of turns, is crucial for hedge prediction. Second, models without embed- dings also performed reasonably well in F1 score, implying that other features such as rapport, con- versational strategies, tutoring strategies, nonverbal behaviors, and contextual information are also im- portant. These features should not be overlooked. The LightGBM and XGBoost models, which only use features without turn embeddings, also display competitive performance compared to the MLP, LSTM, and AttnLSTM models using all fea- tures. This suggests that although turn embeddings provide valuable information for hedge prediction, models can still achieve satisfactory results even without them. The AttnLSTM models, which incor- Model Feature N/A Rapport CS TS NB ConInfo DialAct XGBoost LightGBM 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.15 (\u00b10.08) 0.16 (\u00b10.08) 0.10 (\u00b10.08) 0.09 (\u00b10.08) 0.15 (\u00b10.09) 0.10 (\u00b10.07) 0.08 (\u00b10.07) 0.10 (\u00b10.10) 0.10 (\u00b10.08) 0.12 (\u00b10.09) 0.12 (\u00b10.08) 0.13 (\u00b10.08) LSTM AttnLSTM MLP 0.25 (\u00b10.05) 0.23 (\u00b10.06) 0.26 (\u00b10.06) 0.24 (\u00b10.05) 0.20 (\u00b10.06) 0.25 (\u00b10.06) 0.26 (\u00b10.06) 0.22 (\u00b10.05) 0.25 (\u00b10.06) 0.24 (\u00b10.06) 0.25 (\u00b10.05) 0.26 (\u00b10.06) 0.22 (\u00b10.06) 0.24 (\u00b10.05) 0.25 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.07) 0.27 (\u00b10.06) 0.21 (\u00b10.06) 0.22 (\u00b10.06) 0.21 (\u00b10.07) Table 2: F1 scores after the feature ablation, CS: Conversational Strategies; TS: Tutoring Strategies; NB: Nonverbal Behaviors; ConInfo: Contextual Information; DialAct: Dialogue Act. porate attention mechanisms, do not show signifi- cant improvements over the regular LSTM models. This could be due to the limited amount of data available, which cannot unleash the potential of the attention mechanism. Since good performance can also be achieved using the extracted features, in order to answer our research question 2, in the next subsections we will mainly investigate the importance of features in predicting hedges. Features correctness no gaze from tutor problem id rapport tutee\u2019s deep question tutee\u2019s gaze at tutor tutee\u2019s pre-test tutor\u2019s gaze at elsewhere tutor\u2019s praise Valence + - - - - - - - - 4.2 Features Explanation with Shapley values Table 3: Features and their Valences Shapley values (Hart, 1989), originating from co- operative game theory, have emerged as a powerful model-agnostic tool to explain the predictions of machine learning models. This approach provides a way to fairly distribute the contribution of each feature to the overall prediction for a specific in- stance. By calculating the Shapley value for each feature, we gain insight into the importance of in- dividual features within the context of a specific prediction. This interpretability technique has been adopted across various machine learning models. In this study, we use Shapley values to interpret the contributions of extracted features in our"}, {"question": " How many folds were used in the cross-validation strategy?", "answer": " A 5-fold cross-validation strategy was implemented to evaluate the models.", "ref_chunk": "the Extreme Gradient Boosting (XG- Boost) algorithm (Chen and Guestrin, 2016), which is a decision tree-based ensemble machine learning In order to address the imbalance in our dataset, where the ratio of hedge to non-hedge instances is approximately 1:10, we used the Synthetic Minor- ity Over-sampling Technique (SMOTE) (Chawla et al., 2002) for each model to augment our learn- ing process. SMOTE is a popular method that generates synthetic examples in a dataset to coun- teract its imbalance. Given the variable nature of model performance, we implemented a 5-fold cross-validation strategy to evaluate the models. In order to account for the imbalanced nature of the dataset, we opted to use a lower number of folds in Models F1-score Precision Recall LightGBM (w/o emb) XGBoost (w/o emb) 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.17 (\u00b10.03) 0.16 (\u00b10.03) 0.45 (\u00b10.07) 0.45 (\u00b10.07) MLP MLP (only emb) MLP (w/o emb) 0.25 (\u00b10.06) 0.26 (\u00b10.05) 0.26 (\u00b10.06) 0.16 (\u00b10.03) 0.16 (\u00b10.02) 0.17 (\u00b10.06) 0.52 (\u00b10.07) 0.74 (\u00b10.06) 0.56 (\u00b10.07) LSTM LSTM (only emb) LSTM (w/o emb) AttnLSTM AttnLSTM (only emb) AttnLSTM (w/o emb) 0.25 (\u00b10.06) 0.28 (\u00b10.07) 0.25 (\u00b10.05) 0.24 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.06) 0.16 (\u00b10.03) 0.19 (\u00b10.08) 0.15 (\u00b10.02) 0.15 (\u00b10.03) 0.17 (\u00b10.03) 0.15 (\u00b10.07) 0.50 (\u00b10.07) 0.52 (\u00b10.07) 0.75 (\u00b10.06) 0.57 (\u00b10.07) 0.45 (\u00b10.07) 0.57 (\u00b10.07) Dummy 0.11 (\u00b10.08) 0.14 (\u00b10.06) 0.10 (\u00b10.04) Table 1: Comparison of MLP and LSTM models for predicting hedges the cross-validation process. By choosing 5 folds instead of a higher number, we aimed to ensure that each fold would contain a sufficient represen- tation of samples from each class. The model that delivered the best performance during this cross- validation process was then chosen to make pre- dictions on the test set. For the neural models, we adjusted the loss function to account for class im- balance, thereby compelling the models to accom- modate less frequent classes more effectively. The code is available in https://github.com/ neuromaancer/hedge_prediction scores for F1 scores, precision and recall, indicat- ing limited performance in terms of balanced pre- cision and recall. The MLP models, particularly those using only embeddings, showed a remark- able recall of 74%, but at the cost of reduced pre- cision. The LSTM model using only turn embed- dings demonstrated balanced performance across all metrics, achieving the highest precision of 19% and a competitive F1 score of 0.28. However, the attention-based LSTM (AttnLSTM) model did not significantly outperform the standard LSTM model in any metric. 4 Results 4.1 Classification Results To answer the research question 1, we conducted classification experiments on different models. Ta- ble 1 offers an in-depth comparison of multiple machine learning models for predicting hedges in a peer-tutoring dataset. We also incorporated a dummy classifier for comparison, which generates predictions in accordance with the class distribu- tion observed in the training set. The performance metrics are F1 score, precision and recall, all of which include confidence intervals (\u03b1 = 0.05). The dataset is composed of several types of input features described in Section 3.3. The models used different combinations of these inputs. (w/o emb) indicates that the model uses only the features with- out turn embeddings. If not specified, the model uses all features plus turn embeddings. From Table 1, the LightGBM and XGBoost mod- els without embeddings achieved relatively low The inclusion of turn embeddings significantly impacts model performance. Models with only em- beddings perform better in terms of F1 score and recall, suggesting that the semantic information captured in these embeddings, which represented the semantic information of turns, is crucial for hedge prediction. Second, models without embed- dings also performed reasonably well in F1 score, implying that other features such as rapport, con- versational strategies, tutoring strategies, nonverbal behaviors, and contextual information are also im- portant. These features should not be overlooked. The LightGBM and XGBoost models, which only use features without turn embeddings, also display competitive performance compared to the MLP, LSTM, and AttnLSTM models using all fea- tures. This suggests that although turn embeddings provide valuable information for hedge prediction, models can still achieve satisfactory results even without them. The AttnLSTM models, which incor- Model Feature N/A Rapport CS TS NB ConInfo DialAct XGBoost LightGBM 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.15 (\u00b10.08) 0.16 (\u00b10.08) 0.10 (\u00b10.08) 0.09 (\u00b10.08) 0.15 (\u00b10.09) 0.10 (\u00b10.07) 0.08 (\u00b10.07) 0.10 (\u00b10.10) 0.10 (\u00b10.08) 0.12 (\u00b10.09) 0.12 (\u00b10.08) 0.13 (\u00b10.08) LSTM AttnLSTM MLP 0.25 (\u00b10.05) 0.23 (\u00b10.06) 0.26 (\u00b10.06) 0.24 (\u00b10.05) 0.20 (\u00b10.06) 0.25 (\u00b10.06) 0.26 (\u00b10.06) 0.22 (\u00b10.05) 0.25 (\u00b10.06) 0.24 (\u00b10.06) 0.25 (\u00b10.05) 0.26 (\u00b10.06) 0.22 (\u00b10.06) 0.24 (\u00b10.05) 0.25 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.07) 0.27 (\u00b10.06) 0.21 (\u00b10.06) 0.22 (\u00b10.06) 0.21 (\u00b10.07) Table 2: F1 scores after the feature ablation, CS: Conversational Strategies; TS: Tutoring Strategies; NB: Nonverbal Behaviors; ConInfo: Contextual Information; DialAct: Dialogue Act. porate attention mechanisms, do not show signifi- cant improvements over the regular LSTM models. This could be due to the limited amount of data available, which cannot unleash the potential of the attention mechanism. Since good performance can also be achieved using the extracted features, in order to answer our research question 2, in the next subsections we will mainly investigate the importance of features in predicting hedges. Features correctness no gaze from tutor problem id rapport tutee\u2019s deep question tutee\u2019s gaze at tutor tutee\u2019s pre-test tutor\u2019s gaze at elsewhere tutor\u2019s praise Valence + - - - - - - - - 4.2 Features Explanation with Shapley values Table 3: Features and their Valences Shapley values (Hart, 1989), originating from co- operative game theory, have emerged as a powerful model-agnostic tool to explain the predictions of machine learning models. This approach provides a way to fairly distribute the contribution of each feature to the overall prediction for a specific in- stance. By calculating the Shapley value for each feature, we gain insight into the importance of in- dividual features within the context of a specific prediction. This interpretability technique has been adopted across various machine learning models. In this study, we use Shapley values to interpret the contributions of extracted features in our"}, {"question": " Why was a lower number of folds opted for in the cross-validation process?", "answer": " A lower number of folds were chosen to ensure each fold contained a sufficient representation of samples from each class due to the imbalanced nature of the dataset.", "ref_chunk": "the Extreme Gradient Boosting (XG- Boost) algorithm (Chen and Guestrin, 2016), which is a decision tree-based ensemble machine learning In order to address the imbalance in our dataset, where the ratio of hedge to non-hedge instances is approximately 1:10, we used the Synthetic Minor- ity Over-sampling Technique (SMOTE) (Chawla et al., 2002) for each model to augment our learn- ing process. SMOTE is a popular method that generates synthetic examples in a dataset to coun- teract its imbalance. Given the variable nature of model performance, we implemented a 5-fold cross-validation strategy to evaluate the models. In order to account for the imbalanced nature of the dataset, we opted to use a lower number of folds in Models F1-score Precision Recall LightGBM (w/o emb) XGBoost (w/o emb) 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.17 (\u00b10.03) 0.16 (\u00b10.03) 0.45 (\u00b10.07) 0.45 (\u00b10.07) MLP MLP (only emb) MLP (w/o emb) 0.25 (\u00b10.06) 0.26 (\u00b10.05) 0.26 (\u00b10.06) 0.16 (\u00b10.03) 0.16 (\u00b10.02) 0.17 (\u00b10.06) 0.52 (\u00b10.07) 0.74 (\u00b10.06) 0.56 (\u00b10.07) LSTM LSTM (only emb) LSTM (w/o emb) AttnLSTM AttnLSTM (only emb) AttnLSTM (w/o emb) 0.25 (\u00b10.06) 0.28 (\u00b10.07) 0.25 (\u00b10.05) 0.24 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.06) 0.16 (\u00b10.03) 0.19 (\u00b10.08) 0.15 (\u00b10.02) 0.15 (\u00b10.03) 0.17 (\u00b10.03) 0.15 (\u00b10.07) 0.50 (\u00b10.07) 0.52 (\u00b10.07) 0.75 (\u00b10.06) 0.57 (\u00b10.07) 0.45 (\u00b10.07) 0.57 (\u00b10.07) Dummy 0.11 (\u00b10.08) 0.14 (\u00b10.06) 0.10 (\u00b10.04) Table 1: Comparison of MLP and LSTM models for predicting hedges the cross-validation process. By choosing 5 folds instead of a higher number, we aimed to ensure that each fold would contain a sufficient represen- tation of samples from each class. The model that delivered the best performance during this cross- validation process was then chosen to make pre- dictions on the test set. For the neural models, we adjusted the loss function to account for class im- balance, thereby compelling the models to accom- modate less frequent classes more effectively. The code is available in https://github.com/ neuromaancer/hedge_prediction scores for F1 scores, precision and recall, indicat- ing limited performance in terms of balanced pre- cision and recall. The MLP models, particularly those using only embeddings, showed a remark- able recall of 74%, but at the cost of reduced pre- cision. The LSTM model using only turn embed- dings demonstrated balanced performance across all metrics, achieving the highest precision of 19% and a competitive F1 score of 0.28. However, the attention-based LSTM (AttnLSTM) model did not significantly outperform the standard LSTM model in any metric. 4 Results 4.1 Classification Results To answer the research question 1, we conducted classification experiments on different models. Ta- ble 1 offers an in-depth comparison of multiple machine learning models for predicting hedges in a peer-tutoring dataset. We also incorporated a dummy classifier for comparison, which generates predictions in accordance with the class distribu- tion observed in the training set. The performance metrics are F1 score, precision and recall, all of which include confidence intervals (\u03b1 = 0.05). The dataset is composed of several types of input features described in Section 3.3. The models used different combinations of these inputs. (w/o emb) indicates that the model uses only the features with- out turn embeddings. If not specified, the model uses all features plus turn embeddings. From Table 1, the LightGBM and XGBoost mod- els without embeddings achieved relatively low The inclusion of turn embeddings significantly impacts model performance. Models with only em- beddings perform better in terms of F1 score and recall, suggesting that the semantic information captured in these embeddings, which represented the semantic information of turns, is crucial for hedge prediction. Second, models without embed- dings also performed reasonably well in F1 score, implying that other features such as rapport, con- versational strategies, tutoring strategies, nonverbal behaviors, and contextual information are also im- portant. These features should not be overlooked. The LightGBM and XGBoost models, which only use features without turn embeddings, also display competitive performance compared to the MLP, LSTM, and AttnLSTM models using all fea- tures. This suggests that although turn embeddings provide valuable information for hedge prediction, models can still achieve satisfactory results even without them. The AttnLSTM models, which incor- Model Feature N/A Rapport CS TS NB ConInfo DialAct XGBoost LightGBM 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.15 (\u00b10.08) 0.16 (\u00b10.08) 0.10 (\u00b10.08) 0.09 (\u00b10.08) 0.15 (\u00b10.09) 0.10 (\u00b10.07) 0.08 (\u00b10.07) 0.10 (\u00b10.10) 0.10 (\u00b10.08) 0.12 (\u00b10.09) 0.12 (\u00b10.08) 0.13 (\u00b10.08) LSTM AttnLSTM MLP 0.25 (\u00b10.05) 0.23 (\u00b10.06) 0.26 (\u00b10.06) 0.24 (\u00b10.05) 0.20 (\u00b10.06) 0.25 (\u00b10.06) 0.26 (\u00b10.06) 0.22 (\u00b10.05) 0.25 (\u00b10.06) 0.24 (\u00b10.06) 0.25 (\u00b10.05) 0.26 (\u00b10.06) 0.22 (\u00b10.06) 0.24 (\u00b10.05) 0.25 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.07) 0.27 (\u00b10.06) 0.21 (\u00b10.06) 0.22 (\u00b10.06) 0.21 (\u00b10.07) Table 2: F1 scores after the feature ablation, CS: Conversational Strategies; TS: Tutoring Strategies; NB: Nonverbal Behaviors; ConInfo: Contextual Information; DialAct: Dialogue Act. porate attention mechanisms, do not show signifi- cant improvements over the regular LSTM models. This could be due to the limited amount of data available, which cannot unleash the potential of the attention mechanism. Since good performance can also be achieved using the extracted features, in order to answer our research question 2, in the next subsections we will mainly investigate the importance of features in predicting hedges. Features correctness no gaze from tutor problem id rapport tutee\u2019s deep question tutee\u2019s gaze at tutor tutee\u2019s pre-test tutor\u2019s gaze at elsewhere tutor\u2019s praise Valence + - - - - - - - - 4.2 Features Explanation with Shapley values Table 3: Features and their Valences Shapley values (Hart, 1989), originating from co- operative game theory, have emerged as a powerful model-agnostic tool to explain the predictions of machine learning models. This approach provides a way to fairly distribute the contribution of each feature to the overall prediction for a specific in- stance. By calculating the Shapley value for each feature, we gain insight into the importance of in- dividual features within the context of a specific prediction. This interpretability technique has been adopted across various machine learning models. In this study, we use Shapley values to interpret the contributions of extracted features in our"}, {"question": " How was the best-performing model chosen during the cross-validation process?", "answer": " The model that delivered the best performance during cross-validation was selected to make predictions on the test set.", "ref_chunk": "the Extreme Gradient Boosting (XG- Boost) algorithm (Chen and Guestrin, 2016), which is a decision tree-based ensemble machine learning In order to address the imbalance in our dataset, where the ratio of hedge to non-hedge instances is approximately 1:10, we used the Synthetic Minor- ity Over-sampling Technique (SMOTE) (Chawla et al., 2002) for each model to augment our learn- ing process. SMOTE is a popular method that generates synthetic examples in a dataset to coun- teract its imbalance. Given the variable nature of model performance, we implemented a 5-fold cross-validation strategy to evaluate the models. In order to account for the imbalanced nature of the dataset, we opted to use a lower number of folds in Models F1-score Precision Recall LightGBM (w/o emb) XGBoost (w/o emb) 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.17 (\u00b10.03) 0.16 (\u00b10.03) 0.45 (\u00b10.07) 0.45 (\u00b10.07) MLP MLP (only emb) MLP (w/o emb) 0.25 (\u00b10.06) 0.26 (\u00b10.05) 0.26 (\u00b10.06) 0.16 (\u00b10.03) 0.16 (\u00b10.02) 0.17 (\u00b10.06) 0.52 (\u00b10.07) 0.74 (\u00b10.06) 0.56 (\u00b10.07) LSTM LSTM (only emb) LSTM (w/o emb) AttnLSTM AttnLSTM (only emb) AttnLSTM (w/o emb) 0.25 (\u00b10.06) 0.28 (\u00b10.07) 0.25 (\u00b10.05) 0.24 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.06) 0.16 (\u00b10.03) 0.19 (\u00b10.08) 0.15 (\u00b10.02) 0.15 (\u00b10.03) 0.17 (\u00b10.03) 0.15 (\u00b10.07) 0.50 (\u00b10.07) 0.52 (\u00b10.07) 0.75 (\u00b10.06) 0.57 (\u00b10.07) 0.45 (\u00b10.07) 0.57 (\u00b10.07) Dummy 0.11 (\u00b10.08) 0.14 (\u00b10.06) 0.10 (\u00b10.04) Table 1: Comparison of MLP and LSTM models for predicting hedges the cross-validation process. By choosing 5 folds instead of a higher number, we aimed to ensure that each fold would contain a sufficient represen- tation of samples from each class. The model that delivered the best performance during this cross- validation process was then chosen to make pre- dictions on the test set. For the neural models, we adjusted the loss function to account for class im- balance, thereby compelling the models to accom- modate less frequent classes more effectively. The code is available in https://github.com/ neuromaancer/hedge_prediction scores for F1 scores, precision and recall, indicat- ing limited performance in terms of balanced pre- cision and recall. The MLP models, particularly those using only embeddings, showed a remark- able recall of 74%, but at the cost of reduced pre- cision. The LSTM model using only turn embed- dings demonstrated balanced performance across all metrics, achieving the highest precision of 19% and a competitive F1 score of 0.28. However, the attention-based LSTM (AttnLSTM) model did not significantly outperform the standard LSTM model in any metric. 4 Results 4.1 Classification Results To answer the research question 1, we conducted classification experiments on different models. Ta- ble 1 offers an in-depth comparison of multiple machine learning models for predicting hedges in a peer-tutoring dataset. We also incorporated a dummy classifier for comparison, which generates predictions in accordance with the class distribu- tion observed in the training set. The performance metrics are F1 score, precision and recall, all of which include confidence intervals (\u03b1 = 0.05). The dataset is composed of several types of input features described in Section 3.3. The models used different combinations of these inputs. (w/o emb) indicates that the model uses only the features with- out turn embeddings. If not specified, the model uses all features plus turn embeddings. From Table 1, the LightGBM and XGBoost mod- els without embeddings achieved relatively low The inclusion of turn embeddings significantly impacts model performance. Models with only em- beddings perform better in terms of F1 score and recall, suggesting that the semantic information captured in these embeddings, which represented the semantic information of turns, is crucial for hedge prediction. Second, models without embed- dings also performed reasonably well in F1 score, implying that other features such as rapport, con- versational strategies, tutoring strategies, nonverbal behaviors, and contextual information are also im- portant. These features should not be overlooked. The LightGBM and XGBoost models, which only use features without turn embeddings, also display competitive performance compared to the MLP, LSTM, and AttnLSTM models using all fea- tures. This suggests that although turn embeddings provide valuable information for hedge prediction, models can still achieve satisfactory results even without them. The AttnLSTM models, which incor- Model Feature N/A Rapport CS TS NB ConInfo DialAct XGBoost LightGBM 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.15 (\u00b10.08) 0.16 (\u00b10.08) 0.10 (\u00b10.08) 0.09 (\u00b10.08) 0.15 (\u00b10.09) 0.10 (\u00b10.07) 0.08 (\u00b10.07) 0.10 (\u00b10.10) 0.10 (\u00b10.08) 0.12 (\u00b10.09) 0.12 (\u00b10.08) 0.13 (\u00b10.08) LSTM AttnLSTM MLP 0.25 (\u00b10.05) 0.23 (\u00b10.06) 0.26 (\u00b10.06) 0.24 (\u00b10.05) 0.20 (\u00b10.06) 0.25 (\u00b10.06) 0.26 (\u00b10.06) 0.22 (\u00b10.05) 0.25 (\u00b10.06) 0.24 (\u00b10.06) 0.25 (\u00b10.05) 0.26 (\u00b10.06) 0.22 (\u00b10.06) 0.24 (\u00b10.05) 0.25 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.07) 0.27 (\u00b10.06) 0.21 (\u00b10.06) 0.22 (\u00b10.06) 0.21 (\u00b10.07) Table 2: F1 scores after the feature ablation, CS: Conversational Strategies; TS: Tutoring Strategies; NB: Nonverbal Behaviors; ConInfo: Contextual Information; DialAct: Dialogue Act. porate attention mechanisms, do not show signifi- cant improvements over the regular LSTM models. This could be due to the limited amount of data available, which cannot unleash the potential of the attention mechanism. Since good performance can also be achieved using the extracted features, in order to answer our research question 2, in the next subsections we will mainly investigate the importance of features in predicting hedges. Features correctness no gaze from tutor problem id rapport tutee\u2019s deep question tutee\u2019s gaze at tutor tutee\u2019s pre-test tutor\u2019s gaze at elsewhere tutor\u2019s praise Valence + - - - - - - - - 4.2 Features Explanation with Shapley values Table 3: Features and their Valences Shapley values (Hart, 1989), originating from co- operative game theory, have emerged as a powerful model-agnostic tool to explain the predictions of machine learning models. This approach provides a way to fairly distribute the contribution of each feature to the overall prediction for a specific in- stance. By calculating the Shapley value for each feature, we gain insight into the importance of in- dividual features within the context of a specific prediction. This interpretability technique has been adopted across various machine learning models. In this study, we use Shapley values to interpret the contributions of extracted features in our"}, {"question": " How were the neural models adjusted to account for class imbalance?", "answer": " The loss function was adjusted to compel the models to accommodate less frequent classes more effectively.", "ref_chunk": "the Extreme Gradient Boosting (XG- Boost) algorithm (Chen and Guestrin, 2016), which is a decision tree-based ensemble machine learning In order to address the imbalance in our dataset, where the ratio of hedge to non-hedge instances is approximately 1:10, we used the Synthetic Minor- ity Over-sampling Technique (SMOTE) (Chawla et al., 2002) for each model to augment our learn- ing process. SMOTE is a popular method that generates synthetic examples in a dataset to coun- teract its imbalance. Given the variable nature of model performance, we implemented a 5-fold cross-validation strategy to evaluate the models. In order to account for the imbalanced nature of the dataset, we opted to use a lower number of folds in Models F1-score Precision Recall LightGBM (w/o emb) XGBoost (w/o emb) 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.17 (\u00b10.03) 0.16 (\u00b10.03) 0.45 (\u00b10.07) 0.45 (\u00b10.07) MLP MLP (only emb) MLP (w/o emb) 0.25 (\u00b10.06) 0.26 (\u00b10.05) 0.26 (\u00b10.06) 0.16 (\u00b10.03) 0.16 (\u00b10.02) 0.17 (\u00b10.06) 0.52 (\u00b10.07) 0.74 (\u00b10.06) 0.56 (\u00b10.07) LSTM LSTM (only emb) LSTM (w/o emb) AttnLSTM AttnLSTM (only emb) AttnLSTM (w/o emb) 0.25 (\u00b10.06) 0.28 (\u00b10.07) 0.25 (\u00b10.05) 0.24 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.06) 0.16 (\u00b10.03) 0.19 (\u00b10.08) 0.15 (\u00b10.02) 0.15 (\u00b10.03) 0.17 (\u00b10.03) 0.15 (\u00b10.07) 0.50 (\u00b10.07) 0.52 (\u00b10.07) 0.75 (\u00b10.06) 0.57 (\u00b10.07) 0.45 (\u00b10.07) 0.57 (\u00b10.07) Dummy 0.11 (\u00b10.08) 0.14 (\u00b10.06) 0.10 (\u00b10.04) Table 1: Comparison of MLP and LSTM models for predicting hedges the cross-validation process. By choosing 5 folds instead of a higher number, we aimed to ensure that each fold would contain a sufficient represen- tation of samples from each class. The model that delivered the best performance during this cross- validation process was then chosen to make pre- dictions on the test set. For the neural models, we adjusted the loss function to account for class im- balance, thereby compelling the models to accom- modate less frequent classes more effectively. The code is available in https://github.com/ neuromaancer/hedge_prediction scores for F1 scores, precision and recall, indicat- ing limited performance in terms of balanced pre- cision and recall. The MLP models, particularly those using only embeddings, showed a remark- able recall of 74%, but at the cost of reduced pre- cision. The LSTM model using only turn embed- dings demonstrated balanced performance across all metrics, achieving the highest precision of 19% and a competitive F1 score of 0.28. However, the attention-based LSTM (AttnLSTM) model did not significantly outperform the standard LSTM model in any metric. 4 Results 4.1 Classification Results To answer the research question 1, we conducted classification experiments on different models. Ta- ble 1 offers an in-depth comparison of multiple machine learning models for predicting hedges in a peer-tutoring dataset. We also incorporated a dummy classifier for comparison, which generates predictions in accordance with the class distribu- tion observed in the training set. The performance metrics are F1 score, precision and recall, all of which include confidence intervals (\u03b1 = 0.05). The dataset is composed of several types of input features described in Section 3.3. The models used different combinations of these inputs. (w/o emb) indicates that the model uses only the features with- out turn embeddings. If not specified, the model uses all features plus turn embeddings. From Table 1, the LightGBM and XGBoost mod- els without embeddings achieved relatively low The inclusion of turn embeddings significantly impacts model performance. Models with only em- beddings perform better in terms of F1 score and recall, suggesting that the semantic information captured in these embeddings, which represented the semantic information of turns, is crucial for hedge prediction. Second, models without embed- dings also performed reasonably well in F1 score, implying that other features such as rapport, con- versational strategies, tutoring strategies, nonverbal behaviors, and contextual information are also im- portant. These features should not be overlooked. The LightGBM and XGBoost models, which only use features without turn embeddings, also display competitive performance compared to the MLP, LSTM, and AttnLSTM models using all fea- tures. This suggests that although turn embeddings provide valuable information for hedge prediction, models can still achieve satisfactory results even without them. The AttnLSTM models, which incor- Model Feature N/A Rapport CS TS NB ConInfo DialAct XGBoost LightGBM 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.15 (\u00b10.08) 0.16 (\u00b10.08) 0.10 (\u00b10.08) 0.09 (\u00b10.08) 0.15 (\u00b10.09) 0.10 (\u00b10.07) 0.08 (\u00b10.07) 0.10 (\u00b10.10) 0.10 (\u00b10.08) 0.12 (\u00b10.09) 0.12 (\u00b10.08) 0.13 (\u00b10.08) LSTM AttnLSTM MLP 0.25 (\u00b10.05) 0.23 (\u00b10.06) 0.26 (\u00b10.06) 0.24 (\u00b10.05) 0.20 (\u00b10.06) 0.25 (\u00b10.06) 0.26 (\u00b10.06) 0.22 (\u00b10.05) 0.25 (\u00b10.06) 0.24 (\u00b10.06) 0.25 (\u00b10.05) 0.26 (\u00b10.06) 0.22 (\u00b10.06) 0.24 (\u00b10.05) 0.25 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.07) 0.27 (\u00b10.06) 0.21 (\u00b10.06) 0.22 (\u00b10.06) 0.21 (\u00b10.07) Table 2: F1 scores after the feature ablation, CS: Conversational Strategies; TS: Tutoring Strategies; NB: Nonverbal Behaviors; ConInfo: Contextual Information; DialAct: Dialogue Act. porate attention mechanisms, do not show signifi- cant improvements over the regular LSTM models. This could be due to the limited amount of data available, which cannot unleash the potential of the attention mechanism. Since good performance can also be achieved using the extracted features, in order to answer our research question 2, in the next subsections we will mainly investigate the importance of features in predicting hedges. Features correctness no gaze from tutor problem id rapport tutee\u2019s deep question tutee\u2019s gaze at tutor tutee\u2019s pre-test tutor\u2019s gaze at elsewhere tutor\u2019s praise Valence + - - - - - - - - 4.2 Features Explanation with Shapley values Table 3: Features and their Valences Shapley values (Hart, 1989), originating from co- operative game theory, have emerged as a powerful model-agnostic tool to explain the predictions of machine learning models. This approach provides a way to fairly distribute the contribution of each feature to the overall prediction for a specific in- stance. By calculating the Shapley value for each feature, we gain insight into the importance of in- dividual features within the context of a specific prediction. This interpretability technique has been adopted across various machine learning models. In this study, we use Shapley values to interpret the contributions of extracted features in our"}, {"question": " What performance metrics were used in the classification experiments?", "answer": " F1 score, precision, and recall were used as performance metrics.", "ref_chunk": "the Extreme Gradient Boosting (XG- Boost) algorithm (Chen and Guestrin, 2016), which is a decision tree-based ensemble machine learning In order to address the imbalance in our dataset, where the ratio of hedge to non-hedge instances is approximately 1:10, we used the Synthetic Minor- ity Over-sampling Technique (SMOTE) (Chawla et al., 2002) for each model to augment our learn- ing process. SMOTE is a popular method that generates synthetic examples in a dataset to coun- teract its imbalance. Given the variable nature of model performance, we implemented a 5-fold cross-validation strategy to evaluate the models. In order to account for the imbalanced nature of the dataset, we opted to use a lower number of folds in Models F1-score Precision Recall LightGBM (w/o emb) XGBoost (w/o emb) 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.17 (\u00b10.03) 0.16 (\u00b10.03) 0.45 (\u00b10.07) 0.45 (\u00b10.07) MLP MLP (only emb) MLP (w/o emb) 0.25 (\u00b10.06) 0.26 (\u00b10.05) 0.26 (\u00b10.06) 0.16 (\u00b10.03) 0.16 (\u00b10.02) 0.17 (\u00b10.06) 0.52 (\u00b10.07) 0.74 (\u00b10.06) 0.56 (\u00b10.07) LSTM LSTM (only emb) LSTM (w/o emb) AttnLSTM AttnLSTM (only emb) AttnLSTM (w/o emb) 0.25 (\u00b10.06) 0.28 (\u00b10.07) 0.25 (\u00b10.05) 0.24 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.06) 0.16 (\u00b10.03) 0.19 (\u00b10.08) 0.15 (\u00b10.02) 0.15 (\u00b10.03) 0.17 (\u00b10.03) 0.15 (\u00b10.07) 0.50 (\u00b10.07) 0.52 (\u00b10.07) 0.75 (\u00b10.06) 0.57 (\u00b10.07) 0.45 (\u00b10.07) 0.57 (\u00b10.07) Dummy 0.11 (\u00b10.08) 0.14 (\u00b10.06) 0.10 (\u00b10.04) Table 1: Comparison of MLP and LSTM models for predicting hedges the cross-validation process. By choosing 5 folds instead of a higher number, we aimed to ensure that each fold would contain a sufficient represen- tation of samples from each class. The model that delivered the best performance during this cross- validation process was then chosen to make pre- dictions on the test set. For the neural models, we adjusted the loss function to account for class im- balance, thereby compelling the models to accom- modate less frequent classes more effectively. The code is available in https://github.com/ neuromaancer/hedge_prediction scores for F1 scores, precision and recall, indicat- ing limited performance in terms of balanced pre- cision and recall. The MLP models, particularly those using only embeddings, showed a remark- able recall of 74%, but at the cost of reduced pre- cision. The LSTM model using only turn embed- dings demonstrated balanced performance across all metrics, achieving the highest precision of 19% and a competitive F1 score of 0.28. However, the attention-based LSTM (AttnLSTM) model did not significantly outperform the standard LSTM model in any metric. 4 Results 4.1 Classification Results To answer the research question 1, we conducted classification experiments on different models. Ta- ble 1 offers an in-depth comparison of multiple machine learning models for predicting hedges in a peer-tutoring dataset. We also incorporated a dummy classifier for comparison, which generates predictions in accordance with the class distribu- tion observed in the training set. The performance metrics are F1 score, precision and recall, all of which include confidence intervals (\u03b1 = 0.05). The dataset is composed of several types of input features described in Section 3.3. The models used different combinations of these inputs. (w/o emb) indicates that the model uses only the features with- out turn embeddings. If not specified, the model uses all features plus turn embeddings. From Table 1, the LightGBM and XGBoost mod- els without embeddings achieved relatively low The inclusion of turn embeddings significantly impacts model performance. Models with only em- beddings perform better in terms of F1 score and recall, suggesting that the semantic information captured in these embeddings, which represented the semantic information of turns, is crucial for hedge prediction. Second, models without embed- dings also performed reasonably well in F1 score, implying that other features such as rapport, con- versational strategies, tutoring strategies, nonverbal behaviors, and contextual information are also im- portant. These features should not be overlooked. The LightGBM and XGBoost models, which only use features without turn embeddings, also display competitive performance compared to the MLP, LSTM, and AttnLSTM models using all fea- tures. This suggests that although turn embeddings provide valuable information for hedge prediction, models can still achieve satisfactory results even without them. The AttnLSTM models, which incor- Model Feature N/A Rapport CS TS NB ConInfo DialAct XGBoost LightGBM 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.15 (\u00b10.08) 0.16 (\u00b10.08) 0.10 (\u00b10.08) 0.09 (\u00b10.08) 0.15 (\u00b10.09) 0.10 (\u00b10.07) 0.08 (\u00b10.07) 0.10 (\u00b10.10) 0.10 (\u00b10.08) 0.12 (\u00b10.09) 0.12 (\u00b10.08) 0.13 (\u00b10.08) LSTM AttnLSTM MLP 0.25 (\u00b10.05) 0.23 (\u00b10.06) 0.26 (\u00b10.06) 0.24 (\u00b10.05) 0.20 (\u00b10.06) 0.25 (\u00b10.06) 0.26 (\u00b10.06) 0.22 (\u00b10.05) 0.25 (\u00b10.06) 0.24 (\u00b10.06) 0.25 (\u00b10.05) 0.26 (\u00b10.06) 0.22 (\u00b10.06) 0.24 (\u00b10.05) 0.25 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.07) 0.27 (\u00b10.06) 0.21 (\u00b10.06) 0.22 (\u00b10.06) 0.21 (\u00b10.07) Table 2: F1 scores after the feature ablation, CS: Conversational Strategies; TS: Tutoring Strategies; NB: Nonverbal Behaviors; ConInfo: Contextual Information; DialAct: Dialogue Act. porate attention mechanisms, do not show signifi- cant improvements over the regular LSTM models. This could be due to the limited amount of data available, which cannot unleash the potential of the attention mechanism. Since good performance can also be achieved using the extracted features, in order to answer our research question 2, in the next subsections we will mainly investigate the importance of features in predicting hedges. Features correctness no gaze from tutor problem id rapport tutee\u2019s deep question tutee\u2019s gaze at tutor tutee\u2019s pre-test tutor\u2019s gaze at elsewhere tutor\u2019s praise Valence + - - - - - - - - 4.2 Features Explanation with Shapley values Table 3: Features and their Valences Shapley values (Hart, 1989), originating from co- operative game theory, have emerged as a powerful model-agnostic tool to explain the predictions of machine learning models. This approach provides a way to fairly distribute the contribution of each feature to the overall prediction for a specific in- stance. By calculating the Shapley value for each feature, we gain insight into the importance of in- dividual features within the context of a specific prediction. This interpretability technique has been adopted across various machine learning models. In this study, we use Shapley values to interpret the contributions of extracted features in our"}, {"question": " What impact did turn embeddings have on model performance?", "answer": " Models with only embeddings performed better in terms of F1 score and recall.", "ref_chunk": "the Extreme Gradient Boosting (XG- Boost) algorithm (Chen and Guestrin, 2016), which is a decision tree-based ensemble machine learning In order to address the imbalance in our dataset, where the ratio of hedge to non-hedge instances is approximately 1:10, we used the Synthetic Minor- ity Over-sampling Technique (SMOTE) (Chawla et al., 2002) for each model to augment our learn- ing process. SMOTE is a popular method that generates synthetic examples in a dataset to coun- teract its imbalance. Given the variable nature of model performance, we implemented a 5-fold cross-validation strategy to evaluate the models. In order to account for the imbalanced nature of the dataset, we opted to use a lower number of folds in Models F1-score Precision Recall LightGBM (w/o emb) XGBoost (w/o emb) 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.17 (\u00b10.03) 0.16 (\u00b10.03) 0.45 (\u00b10.07) 0.45 (\u00b10.07) MLP MLP (only emb) MLP (w/o emb) 0.25 (\u00b10.06) 0.26 (\u00b10.05) 0.26 (\u00b10.06) 0.16 (\u00b10.03) 0.16 (\u00b10.02) 0.17 (\u00b10.06) 0.52 (\u00b10.07) 0.74 (\u00b10.06) 0.56 (\u00b10.07) LSTM LSTM (only emb) LSTM (w/o emb) AttnLSTM AttnLSTM (only emb) AttnLSTM (w/o emb) 0.25 (\u00b10.06) 0.28 (\u00b10.07) 0.25 (\u00b10.05) 0.24 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.06) 0.16 (\u00b10.03) 0.19 (\u00b10.08) 0.15 (\u00b10.02) 0.15 (\u00b10.03) 0.17 (\u00b10.03) 0.15 (\u00b10.07) 0.50 (\u00b10.07) 0.52 (\u00b10.07) 0.75 (\u00b10.06) 0.57 (\u00b10.07) 0.45 (\u00b10.07) 0.57 (\u00b10.07) Dummy 0.11 (\u00b10.08) 0.14 (\u00b10.06) 0.10 (\u00b10.04) Table 1: Comparison of MLP and LSTM models for predicting hedges the cross-validation process. By choosing 5 folds instead of a higher number, we aimed to ensure that each fold would contain a sufficient represen- tation of samples from each class. The model that delivered the best performance during this cross- validation process was then chosen to make pre- dictions on the test set. For the neural models, we adjusted the loss function to account for class im- balance, thereby compelling the models to accom- modate less frequent classes more effectively. The code is available in https://github.com/ neuromaancer/hedge_prediction scores for F1 scores, precision and recall, indicat- ing limited performance in terms of balanced pre- cision and recall. The MLP models, particularly those using only embeddings, showed a remark- able recall of 74%, but at the cost of reduced pre- cision. The LSTM model using only turn embed- dings demonstrated balanced performance across all metrics, achieving the highest precision of 19% and a competitive F1 score of 0.28. However, the attention-based LSTM (AttnLSTM) model did not significantly outperform the standard LSTM model in any metric. 4 Results 4.1 Classification Results To answer the research question 1, we conducted classification experiments on different models. Ta- ble 1 offers an in-depth comparison of multiple machine learning models for predicting hedges in a peer-tutoring dataset. We also incorporated a dummy classifier for comparison, which generates predictions in accordance with the class distribu- tion observed in the training set. The performance metrics are F1 score, precision and recall, all of which include confidence intervals (\u03b1 = 0.05). The dataset is composed of several types of input features described in Section 3.3. The models used different combinations of these inputs. (w/o emb) indicates that the model uses only the features with- out turn embeddings. If not specified, the model uses all features plus turn embeddings. From Table 1, the LightGBM and XGBoost mod- els without embeddings achieved relatively low The inclusion of turn embeddings significantly impacts model performance. Models with only em- beddings perform better in terms of F1 score and recall, suggesting that the semantic information captured in these embeddings, which represented the semantic information of turns, is crucial for hedge prediction. Second, models without embed- dings also performed reasonably well in F1 score, implying that other features such as rapport, con- versational strategies, tutoring strategies, nonverbal behaviors, and contextual information are also im- portant. These features should not be overlooked. The LightGBM and XGBoost models, which only use features without turn embeddings, also display competitive performance compared to the MLP, LSTM, and AttnLSTM models using all fea- tures. This suggests that although turn embeddings provide valuable information for hedge prediction, models can still achieve satisfactory results even without them. The AttnLSTM models, which incor- Model Feature N/A Rapport CS TS NB ConInfo DialAct XGBoost LightGBM 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.15 (\u00b10.08) 0.16 (\u00b10.08) 0.10 (\u00b10.08) 0.09 (\u00b10.08) 0.15 (\u00b10.09) 0.10 (\u00b10.07) 0.08 (\u00b10.07) 0.10 (\u00b10.10) 0.10 (\u00b10.08) 0.12 (\u00b10.09) 0.12 (\u00b10.08) 0.13 (\u00b10.08) LSTM AttnLSTM MLP 0.25 (\u00b10.05) 0.23 (\u00b10.06) 0.26 (\u00b10.06) 0.24 (\u00b10.05) 0.20 (\u00b10.06) 0.25 (\u00b10.06) 0.26 (\u00b10.06) 0.22 (\u00b10.05) 0.25 (\u00b10.06) 0.24 (\u00b10.06) 0.25 (\u00b10.05) 0.26 (\u00b10.06) 0.22 (\u00b10.06) 0.24 (\u00b10.05) 0.25 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.07) 0.27 (\u00b10.06) 0.21 (\u00b10.06) 0.22 (\u00b10.06) 0.21 (\u00b10.07) Table 2: F1 scores after the feature ablation, CS: Conversational Strategies; TS: Tutoring Strategies; NB: Nonverbal Behaviors; ConInfo: Contextual Information; DialAct: Dialogue Act. porate attention mechanisms, do not show signifi- cant improvements over the regular LSTM models. This could be due to the limited amount of data available, which cannot unleash the potential of the attention mechanism. Since good performance can also be achieved using the extracted features, in order to answer our research question 2, in the next subsections we will mainly investigate the importance of features in predicting hedges. Features correctness no gaze from tutor problem id rapport tutee\u2019s deep question tutee\u2019s gaze at tutor tutee\u2019s pre-test tutor\u2019s gaze at elsewhere tutor\u2019s praise Valence + - - - - - - - - 4.2 Features Explanation with Shapley values Table 3: Features and their Valences Shapley values (Hart, 1989), originating from co- operative game theory, have emerged as a powerful model-agnostic tool to explain the predictions of machine learning models. This approach provides a way to fairly distribute the contribution of each feature to the overall prediction for a specific in- stance. By calculating the Shapley value for each feature, we gain insight into the importance of in- dividual features within the context of a specific prediction. This interpretability technique has been adopted across various machine learning models. In this study, we use Shapley values to interpret the contributions of extracted features in our"}, {"question": " Did the attention-based LSTM (AttnLSTM) model significantly outperform the standard LSTM model?", "answer": " No, the attention-based LSTM model did not significantly outperform the standard LSTM model in any metric.", "ref_chunk": "the Extreme Gradient Boosting (XG- Boost) algorithm (Chen and Guestrin, 2016), which is a decision tree-based ensemble machine learning In order to address the imbalance in our dataset, where the ratio of hedge to non-hedge instances is approximately 1:10, we used the Synthetic Minor- ity Over-sampling Technique (SMOTE) (Chawla et al., 2002) for each model to augment our learn- ing process. SMOTE is a popular method that generates synthetic examples in a dataset to coun- teract its imbalance. Given the variable nature of model performance, we implemented a 5-fold cross-validation strategy to evaluate the models. In order to account for the imbalanced nature of the dataset, we opted to use a lower number of folds in Models F1-score Precision Recall LightGBM (w/o emb) XGBoost (w/o emb) 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.17 (\u00b10.03) 0.16 (\u00b10.03) 0.45 (\u00b10.07) 0.45 (\u00b10.07) MLP MLP (only emb) MLP (w/o emb) 0.25 (\u00b10.06) 0.26 (\u00b10.05) 0.26 (\u00b10.06) 0.16 (\u00b10.03) 0.16 (\u00b10.02) 0.17 (\u00b10.06) 0.52 (\u00b10.07) 0.74 (\u00b10.06) 0.56 (\u00b10.07) LSTM LSTM (only emb) LSTM (w/o emb) AttnLSTM AttnLSTM (only emb) AttnLSTM (w/o emb) 0.25 (\u00b10.06) 0.28 (\u00b10.07) 0.25 (\u00b10.05) 0.24 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.06) 0.16 (\u00b10.03) 0.19 (\u00b10.08) 0.15 (\u00b10.02) 0.15 (\u00b10.03) 0.17 (\u00b10.03) 0.15 (\u00b10.07) 0.50 (\u00b10.07) 0.52 (\u00b10.07) 0.75 (\u00b10.06) 0.57 (\u00b10.07) 0.45 (\u00b10.07) 0.57 (\u00b10.07) Dummy 0.11 (\u00b10.08) 0.14 (\u00b10.06) 0.10 (\u00b10.04) Table 1: Comparison of MLP and LSTM models for predicting hedges the cross-validation process. By choosing 5 folds instead of a higher number, we aimed to ensure that each fold would contain a sufficient represen- tation of samples from each class. The model that delivered the best performance during this cross- validation process was then chosen to make pre- dictions on the test set. For the neural models, we adjusted the loss function to account for class im- balance, thereby compelling the models to accom- modate less frequent classes more effectively. The code is available in https://github.com/ neuromaancer/hedge_prediction scores for F1 scores, precision and recall, indicat- ing limited performance in terms of balanced pre- cision and recall. The MLP models, particularly those using only embeddings, showed a remark- able recall of 74%, but at the cost of reduced pre- cision. The LSTM model using only turn embed- dings demonstrated balanced performance across all metrics, achieving the highest precision of 19% and a competitive F1 score of 0.28. However, the attention-based LSTM (AttnLSTM) model did not significantly outperform the standard LSTM model in any metric. 4 Results 4.1 Classification Results To answer the research question 1, we conducted classification experiments on different models. Ta- ble 1 offers an in-depth comparison of multiple machine learning models for predicting hedges in a peer-tutoring dataset. We also incorporated a dummy classifier for comparison, which generates predictions in accordance with the class distribu- tion observed in the training set. The performance metrics are F1 score, precision and recall, all of which include confidence intervals (\u03b1 = 0.05). The dataset is composed of several types of input features described in Section 3.3. The models used different combinations of these inputs. (w/o emb) indicates that the model uses only the features with- out turn embeddings. If not specified, the model uses all features plus turn embeddings. From Table 1, the LightGBM and XGBoost mod- els without embeddings achieved relatively low The inclusion of turn embeddings significantly impacts model performance. Models with only em- beddings perform better in terms of F1 score and recall, suggesting that the semantic information captured in these embeddings, which represented the semantic information of turns, is crucial for hedge prediction. Second, models without embed- dings also performed reasonably well in F1 score, implying that other features such as rapport, con- versational strategies, tutoring strategies, nonverbal behaviors, and contextual information are also im- portant. These features should not be overlooked. The LightGBM and XGBoost models, which only use features without turn embeddings, also display competitive performance compared to the MLP, LSTM, and AttnLSTM models using all fea- tures. This suggests that although turn embeddings provide valuable information for hedge prediction, models can still achieve satisfactory results even without them. The AttnLSTM models, which incor- Model Feature N/A Rapport CS TS NB ConInfo DialAct XGBoost LightGBM 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.15 (\u00b10.08) 0.16 (\u00b10.08) 0.10 (\u00b10.08) 0.09 (\u00b10.08) 0.15 (\u00b10.09) 0.10 (\u00b10.07) 0.08 (\u00b10.07) 0.10 (\u00b10.10) 0.10 (\u00b10.08) 0.12 (\u00b10.09) 0.12 (\u00b10.08) 0.13 (\u00b10.08) LSTM AttnLSTM MLP 0.25 (\u00b10.05) 0.23 (\u00b10.06) 0.26 (\u00b10.06) 0.24 (\u00b10.05) 0.20 (\u00b10.06) 0.25 (\u00b10.06) 0.26 (\u00b10.06) 0.22 (\u00b10.05) 0.25 (\u00b10.06) 0.24 (\u00b10.06) 0.25 (\u00b10.05) 0.26 (\u00b10.06) 0.22 (\u00b10.06) 0.24 (\u00b10.05) 0.25 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.07) 0.27 (\u00b10.06) 0.21 (\u00b10.06) 0.22 (\u00b10.06) 0.21 (\u00b10.07) Table 2: F1 scores after the feature ablation, CS: Conversational Strategies; TS: Tutoring Strategies; NB: Nonverbal Behaviors; ConInfo: Contextual Information; DialAct: Dialogue Act. porate attention mechanisms, do not show signifi- cant improvements over the regular LSTM models. This could be due to the limited amount of data available, which cannot unleash the potential of the attention mechanism. Since good performance can also be achieved using the extracted features, in order to answer our research question 2, in the next subsections we will mainly investigate the importance of features in predicting hedges. Features correctness no gaze from tutor problem id rapport tutee\u2019s deep question tutee\u2019s gaze at tutor tutee\u2019s pre-test tutor\u2019s gaze at elsewhere tutor\u2019s praise Valence + - - - - - - - - 4.2 Features Explanation with Shapley values Table 3: Features and their Valences Shapley values (Hart, 1989), originating from co- operative game theory, have emerged as a powerful model-agnostic tool to explain the predictions of machine learning models. This approach provides a way to fairly distribute the contribution of each feature to the overall prediction for a specific in- stance. By calculating the Shapley value for each feature, we gain insight into the importance of in- dividual features within the context of a specific prediction. This interpretability technique has been adopted across various machine learning models. In this study, we use Shapley values to interpret the contributions of extracted features in our"}, {"question": " What technique was used to explain the predictions of machine learning models?", "answer": " Shapley values, originating from cooperative game theory, were used to explain the predictions of machine learning models.", "ref_chunk": "the Extreme Gradient Boosting (XG- Boost) algorithm (Chen and Guestrin, 2016), which is a decision tree-based ensemble machine learning In order to address the imbalance in our dataset, where the ratio of hedge to non-hedge instances is approximately 1:10, we used the Synthetic Minor- ity Over-sampling Technique (SMOTE) (Chawla et al., 2002) for each model to augment our learn- ing process. SMOTE is a popular method that generates synthetic examples in a dataset to coun- teract its imbalance. Given the variable nature of model performance, we implemented a 5-fold cross-validation strategy to evaluate the models. In order to account for the imbalanced nature of the dataset, we opted to use a lower number of folds in Models F1-score Precision Recall LightGBM (w/o emb) XGBoost (w/o emb) 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.17 (\u00b10.03) 0.16 (\u00b10.03) 0.45 (\u00b10.07) 0.45 (\u00b10.07) MLP MLP (only emb) MLP (w/o emb) 0.25 (\u00b10.06) 0.26 (\u00b10.05) 0.26 (\u00b10.06) 0.16 (\u00b10.03) 0.16 (\u00b10.02) 0.17 (\u00b10.06) 0.52 (\u00b10.07) 0.74 (\u00b10.06) 0.56 (\u00b10.07) LSTM LSTM (only emb) LSTM (w/o emb) AttnLSTM AttnLSTM (only emb) AttnLSTM (w/o emb) 0.25 (\u00b10.06) 0.28 (\u00b10.07) 0.25 (\u00b10.05) 0.24 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.06) 0.16 (\u00b10.03) 0.19 (\u00b10.08) 0.15 (\u00b10.02) 0.15 (\u00b10.03) 0.17 (\u00b10.03) 0.15 (\u00b10.07) 0.50 (\u00b10.07) 0.52 (\u00b10.07) 0.75 (\u00b10.06) 0.57 (\u00b10.07) 0.45 (\u00b10.07) 0.57 (\u00b10.07) Dummy 0.11 (\u00b10.08) 0.14 (\u00b10.06) 0.10 (\u00b10.04) Table 1: Comparison of MLP and LSTM models for predicting hedges the cross-validation process. By choosing 5 folds instead of a higher number, we aimed to ensure that each fold would contain a sufficient represen- tation of samples from each class. The model that delivered the best performance during this cross- validation process was then chosen to make pre- dictions on the test set. For the neural models, we adjusted the loss function to account for class im- balance, thereby compelling the models to accom- modate less frequent classes more effectively. The code is available in https://github.com/ neuromaancer/hedge_prediction scores for F1 scores, precision and recall, indicat- ing limited performance in terms of balanced pre- cision and recall. The MLP models, particularly those using only embeddings, showed a remark- able recall of 74%, but at the cost of reduced pre- cision. The LSTM model using only turn embed- dings demonstrated balanced performance across all metrics, achieving the highest precision of 19% and a competitive F1 score of 0.28. However, the attention-based LSTM (AttnLSTM) model did not significantly outperform the standard LSTM model in any metric. 4 Results 4.1 Classification Results To answer the research question 1, we conducted classification experiments on different models. Ta- ble 1 offers an in-depth comparison of multiple machine learning models for predicting hedges in a peer-tutoring dataset. We also incorporated a dummy classifier for comparison, which generates predictions in accordance with the class distribu- tion observed in the training set. The performance metrics are F1 score, precision and recall, all of which include confidence intervals (\u03b1 = 0.05). The dataset is composed of several types of input features described in Section 3.3. The models used different combinations of these inputs. (w/o emb) indicates that the model uses only the features with- out turn embeddings. If not specified, the model uses all features plus turn embeddings. From Table 1, the LightGBM and XGBoost mod- els without embeddings achieved relatively low The inclusion of turn embeddings significantly impacts model performance. Models with only em- beddings perform better in terms of F1 score and recall, suggesting that the semantic information captured in these embeddings, which represented the semantic information of turns, is crucial for hedge prediction. Second, models without embed- dings also performed reasonably well in F1 score, implying that other features such as rapport, con- versational strategies, tutoring strategies, nonverbal behaviors, and contextual information are also im- portant. These features should not be overlooked. The LightGBM and XGBoost models, which only use features without turn embeddings, also display competitive performance compared to the MLP, LSTM, and AttnLSTM models using all fea- tures. This suggests that although turn embeddings provide valuable information for hedge prediction, models can still achieve satisfactory results even without them. The AttnLSTM models, which incor- Model Feature N/A Rapport CS TS NB ConInfo DialAct XGBoost LightGBM 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.15 (\u00b10.08) 0.16 (\u00b10.08) 0.10 (\u00b10.08) 0.09 (\u00b10.08) 0.15 (\u00b10.09) 0.10 (\u00b10.07) 0.08 (\u00b10.07) 0.10 (\u00b10.10) 0.10 (\u00b10.08) 0.12 (\u00b10.09) 0.12 (\u00b10.08) 0.13 (\u00b10.08) LSTM AttnLSTM MLP 0.25 (\u00b10.05) 0.23 (\u00b10.06) 0.26 (\u00b10.06) 0.24 (\u00b10.05) 0.20 (\u00b10.06) 0.25 (\u00b10.06) 0.26 (\u00b10.06) 0.22 (\u00b10.05) 0.25 (\u00b10.06) 0.24 (\u00b10.06) 0.25 (\u00b10.05) 0.26 (\u00b10.06) 0.22 (\u00b10.06) 0.24 (\u00b10.05) 0.25 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.07) 0.27 (\u00b10.06) 0.21 (\u00b10.06) 0.22 (\u00b10.06) 0.21 (\u00b10.07) Table 2: F1 scores after the feature ablation, CS: Conversational Strategies; TS: Tutoring Strategies; NB: Nonverbal Behaviors; ConInfo: Contextual Information; DialAct: Dialogue Act. porate attention mechanisms, do not show signifi- cant improvements over the regular LSTM models. This could be due to the limited amount of data available, which cannot unleash the potential of the attention mechanism. Since good performance can also be achieved using the extracted features, in order to answer our research question 2, in the next subsections we will mainly investigate the importance of features in predicting hedges. Features correctness no gaze from tutor problem id rapport tutee\u2019s deep question tutee\u2019s gaze at tutor tutee\u2019s pre-test tutor\u2019s gaze at elsewhere tutor\u2019s praise Valence + - - - - - - - - 4.2 Features Explanation with Shapley values Table 3: Features and their Valences Shapley values (Hart, 1989), originating from co- operative game theory, have emerged as a powerful model-agnostic tool to explain the predictions of machine learning models. This approach provides a way to fairly distribute the contribution of each feature to the overall prediction for a specific in- stance. By calculating the Shapley value for each feature, we gain insight into the importance of in- dividual features within the context of a specific prediction. This interpretability technique has been adopted across various machine learning models. In this study, we use Shapley values to interpret the contributions of extracted features in our"}, {"question": " What insight do Shapley values provide in the context of a specific prediction?", "answer": " Shapley values provide insight into the importance of individual features within the context of a specific prediction.", "ref_chunk": "the Extreme Gradient Boosting (XG- Boost) algorithm (Chen and Guestrin, 2016), which is a decision tree-based ensemble machine learning In order to address the imbalance in our dataset, where the ratio of hedge to non-hedge instances is approximately 1:10, we used the Synthetic Minor- ity Over-sampling Technique (SMOTE) (Chawla et al., 2002) for each model to augment our learn- ing process. SMOTE is a popular method that generates synthetic examples in a dataset to coun- teract its imbalance. Given the variable nature of model performance, we implemented a 5-fold cross-validation strategy to evaluate the models. In order to account for the imbalanced nature of the dataset, we opted to use a lower number of folds in Models F1-score Precision Recall LightGBM (w/o emb) XGBoost (w/o emb) 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.17 (\u00b10.03) 0.16 (\u00b10.03) 0.45 (\u00b10.07) 0.45 (\u00b10.07) MLP MLP (only emb) MLP (w/o emb) 0.25 (\u00b10.06) 0.26 (\u00b10.05) 0.26 (\u00b10.06) 0.16 (\u00b10.03) 0.16 (\u00b10.02) 0.17 (\u00b10.06) 0.52 (\u00b10.07) 0.74 (\u00b10.06) 0.56 (\u00b10.07) LSTM LSTM (only emb) LSTM (w/o emb) AttnLSTM AttnLSTM (only emb) AttnLSTM (w/o emb) 0.25 (\u00b10.06) 0.28 (\u00b10.07) 0.25 (\u00b10.05) 0.24 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.06) 0.16 (\u00b10.03) 0.19 (\u00b10.08) 0.15 (\u00b10.02) 0.15 (\u00b10.03) 0.17 (\u00b10.03) 0.15 (\u00b10.07) 0.50 (\u00b10.07) 0.52 (\u00b10.07) 0.75 (\u00b10.06) 0.57 (\u00b10.07) 0.45 (\u00b10.07) 0.57 (\u00b10.07) Dummy 0.11 (\u00b10.08) 0.14 (\u00b10.06) 0.10 (\u00b10.04) Table 1: Comparison of MLP and LSTM models for predicting hedges the cross-validation process. By choosing 5 folds instead of a higher number, we aimed to ensure that each fold would contain a sufficient represen- tation of samples from each class. The model that delivered the best performance during this cross- validation process was then chosen to make pre- dictions on the test set. For the neural models, we adjusted the loss function to account for class im- balance, thereby compelling the models to accom- modate less frequent classes more effectively. The code is available in https://github.com/ neuromaancer/hedge_prediction scores for F1 scores, precision and recall, indicat- ing limited performance in terms of balanced pre- cision and recall. The MLP models, particularly those using only embeddings, showed a remark- able recall of 74%, but at the cost of reduced pre- cision. The LSTM model using only turn embed- dings demonstrated balanced performance across all metrics, achieving the highest precision of 19% and a competitive F1 score of 0.28. However, the attention-based LSTM (AttnLSTM) model did not significantly outperform the standard LSTM model in any metric. 4 Results 4.1 Classification Results To answer the research question 1, we conducted classification experiments on different models. Ta- ble 1 offers an in-depth comparison of multiple machine learning models for predicting hedges in a peer-tutoring dataset. We also incorporated a dummy classifier for comparison, which generates predictions in accordance with the class distribu- tion observed in the training set. The performance metrics are F1 score, precision and recall, all of which include confidence intervals (\u03b1 = 0.05). The dataset is composed of several types of input features described in Section 3.3. The models used different combinations of these inputs. (w/o emb) indicates that the model uses only the features with- out turn embeddings. If not specified, the model uses all features plus turn embeddings. From Table 1, the LightGBM and XGBoost mod- els without embeddings achieved relatively low The inclusion of turn embeddings significantly impacts model performance. Models with only em- beddings perform better in terms of F1 score and recall, suggesting that the semantic information captured in these embeddings, which represented the semantic information of turns, is crucial for hedge prediction. Second, models without embed- dings also performed reasonably well in F1 score, implying that other features such as rapport, con- versational strategies, tutoring strategies, nonverbal behaviors, and contextual information are also im- portant. These features should not be overlooked. The LightGBM and XGBoost models, which only use features without turn embeddings, also display competitive performance compared to the MLP, LSTM, and AttnLSTM models using all fea- tures. This suggests that although turn embeddings provide valuable information for hedge prediction, models can still achieve satisfactory results even without them. The AttnLSTM models, which incor- Model Feature N/A Rapport CS TS NB ConInfo DialAct XGBoost LightGBM 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.15 (\u00b10.08) 0.16 (\u00b10.08) 0.10 (\u00b10.08) 0.09 (\u00b10.08) 0.15 (\u00b10.09) 0.10 (\u00b10.07) 0.08 (\u00b10.07) 0.10 (\u00b10.10) 0.10 (\u00b10.08) 0.12 (\u00b10.09) 0.12 (\u00b10.08) 0.13 (\u00b10.08) LSTM AttnLSTM MLP 0.25 (\u00b10.05) 0.23 (\u00b10.06) 0.26 (\u00b10.06) 0.24 (\u00b10.05) 0.20 (\u00b10.06) 0.25 (\u00b10.06) 0.26 (\u00b10.06) 0.22 (\u00b10.05) 0.25 (\u00b10.06) 0.24 (\u00b10.06) 0.25 (\u00b10.05) 0.26 (\u00b10.06) 0.22 (\u00b10.06) 0.24 (\u00b10.05) 0.25 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.07) 0.27 (\u00b10.06) 0.21 (\u00b10.06) 0.22 (\u00b10.06) 0.21 (\u00b10.07) Table 2: F1 scores after the feature ablation, CS: Conversational Strategies; TS: Tutoring Strategies; NB: Nonverbal Behaviors; ConInfo: Contextual Information; DialAct: Dialogue Act. porate attention mechanisms, do not show signifi- cant improvements over the regular LSTM models. This could be due to the limited amount of data available, which cannot unleash the potential of the attention mechanism. Since good performance can also be achieved using the extracted features, in order to answer our research question 2, in the next subsections we will mainly investigate the importance of features in predicting hedges. Features correctness no gaze from tutor problem id rapport tutee\u2019s deep question tutee\u2019s gaze at tutor tutee\u2019s pre-test tutor\u2019s gaze at elsewhere tutor\u2019s praise Valence + - - - - - - - - 4.2 Features Explanation with Shapley values Table 3: Features and their Valences Shapley values (Hart, 1989), originating from co- operative game theory, have emerged as a powerful model-agnostic tool to explain the predictions of machine learning models. This approach provides a way to fairly distribute the contribution of each feature to the overall prediction for a specific in- stance. By calculating the Shapley value for each feature, we gain insight into the importance of in- dividual features within the context of a specific prediction. This interpretability technique has been adopted across various machine learning models. In this study, we use Shapley values to interpret the contributions of extracted features in our"}], "doc_text": "the Extreme Gradient Boosting (XG- Boost) algorithm (Chen and Guestrin, 2016), which is a decision tree-based ensemble machine learning In order to address the imbalance in our dataset, where the ratio of hedge to non-hedge instances is approximately 1:10, we used the Synthetic Minor- ity Over-sampling Technique (SMOTE) (Chawla et al., 2002) for each model to augment our learn- ing process. SMOTE is a popular method that generates synthetic examples in a dataset to coun- teract its imbalance. Given the variable nature of model performance, we implemented a 5-fold cross-validation strategy to evaluate the models. In order to account for the imbalanced nature of the dataset, we opted to use a lower number of folds in Models F1-score Precision Recall LightGBM (w/o emb) XGBoost (w/o emb) 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.17 (\u00b10.03) 0.16 (\u00b10.03) 0.45 (\u00b10.07) 0.45 (\u00b10.07) MLP MLP (only emb) MLP (w/o emb) 0.25 (\u00b10.06) 0.26 (\u00b10.05) 0.26 (\u00b10.06) 0.16 (\u00b10.03) 0.16 (\u00b10.02) 0.17 (\u00b10.06) 0.52 (\u00b10.07) 0.74 (\u00b10.06) 0.56 (\u00b10.07) LSTM LSTM (only emb) LSTM (w/o emb) AttnLSTM AttnLSTM (only emb) AttnLSTM (w/o emb) 0.25 (\u00b10.06) 0.28 (\u00b10.07) 0.25 (\u00b10.05) 0.24 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.06) 0.16 (\u00b10.03) 0.19 (\u00b10.08) 0.15 (\u00b10.02) 0.15 (\u00b10.03) 0.17 (\u00b10.03) 0.15 (\u00b10.07) 0.50 (\u00b10.07) 0.52 (\u00b10.07) 0.75 (\u00b10.06) 0.57 (\u00b10.07) 0.45 (\u00b10.07) 0.57 (\u00b10.07) Dummy 0.11 (\u00b10.08) 0.14 (\u00b10.06) 0.10 (\u00b10.04) Table 1: Comparison of MLP and LSTM models for predicting hedges the cross-validation process. By choosing 5 folds instead of a higher number, we aimed to ensure that each fold would contain a sufficient represen- tation of samples from each class. The model that delivered the best performance during this cross- validation process was then chosen to make pre- dictions on the test set. For the neural models, we adjusted the loss function to account for class im- balance, thereby compelling the models to accom- modate less frequent classes more effectively. The code is available in https://github.com/ neuromaancer/hedge_prediction scores for F1 scores, precision and recall, indicat- ing limited performance in terms of balanced pre- cision and recall. The MLP models, particularly those using only embeddings, showed a remark- able recall of 74%, but at the cost of reduced pre- cision. The LSTM model using only turn embed- dings demonstrated balanced performance across all metrics, achieving the highest precision of 19% and a competitive F1 score of 0.28. However, the attention-based LSTM (AttnLSTM) model did not significantly outperform the standard LSTM model in any metric. 4 Results 4.1 Classification Results To answer the research question 1, we conducted classification experiments on different models. Ta- ble 1 offers an in-depth comparison of multiple machine learning models for predicting hedges in a peer-tutoring dataset. We also incorporated a dummy classifier for comparison, which generates predictions in accordance with the class distribu- tion observed in the training set. The performance metrics are F1 score, precision and recall, all of which include confidence intervals (\u03b1 = 0.05). The dataset is composed of several types of input features described in Section 3.3. The models used different combinations of these inputs. (w/o emb) indicates that the model uses only the features with- out turn embeddings. If not specified, the model uses all features plus turn embeddings. From Table 1, the LightGBM and XGBoost mod- els without embeddings achieved relatively low The inclusion of turn embeddings significantly impacts model performance. Models with only em- beddings perform better in terms of F1 score and recall, suggesting that the semantic information captured in these embeddings, which represented the semantic information of turns, is crucial for hedge prediction. Second, models without embed- dings also performed reasonably well in F1 score, implying that other features such as rapport, con- versational strategies, tutoring strategies, nonverbal behaviors, and contextual information are also im- portant. These features should not be overlooked. The LightGBM and XGBoost models, which only use features without turn embeddings, also display competitive performance compared to the MLP, LSTM, and AttnLSTM models using all fea- tures. This suggests that although turn embeddings provide valuable information for hedge prediction, models can still achieve satisfactory results even without them. The AttnLSTM models, which incor- Model Feature N/A Rapport CS TS NB ConInfo DialAct XGBoost LightGBM 0.24 (\u00b10.07) 0.24 (\u00b10.07) 0.15 (\u00b10.08) 0.16 (\u00b10.08) 0.10 (\u00b10.08) 0.09 (\u00b10.08) 0.15 (\u00b10.09) 0.10 (\u00b10.07) 0.08 (\u00b10.07) 0.10 (\u00b10.10) 0.10 (\u00b10.08) 0.12 (\u00b10.09) 0.12 (\u00b10.08) 0.13 (\u00b10.08) LSTM AttnLSTM MLP 0.25 (\u00b10.05) 0.23 (\u00b10.06) 0.26 (\u00b10.06) 0.24 (\u00b10.05) 0.20 (\u00b10.06) 0.25 (\u00b10.06) 0.26 (\u00b10.06) 0.22 (\u00b10.05) 0.25 (\u00b10.06) 0.24 (\u00b10.06) 0.25 (\u00b10.05) 0.26 (\u00b10.06) 0.22 (\u00b10.06) 0.24 (\u00b10.05) 0.25 (\u00b10.06) 0.25 (\u00b10.07) 0.23 (\u00b10.07) 0.27 (\u00b10.06) 0.21 (\u00b10.06) 0.22 (\u00b10.06) 0.21 (\u00b10.07) Table 2: F1 scores after the feature ablation, CS: Conversational Strategies; TS: Tutoring Strategies; NB: Nonverbal Behaviors; ConInfo: Contextual Information; DialAct: Dialogue Act. porate attention mechanisms, do not show signifi- cant improvements over the regular LSTM models. This could be due to the limited amount of data available, which cannot unleash the potential of the attention mechanism. Since good performance can also be achieved using the extracted features, in order to answer our research question 2, in the next subsections we will mainly investigate the importance of features in predicting hedges. Features correctness no gaze from tutor problem id rapport tutee\u2019s deep question tutee\u2019s gaze at tutor tutee\u2019s pre-test tutor\u2019s gaze at elsewhere tutor\u2019s praise Valence + - - - - - - - - 4.2 Features Explanation with Shapley values Table 3: Features and their Valences Shapley values (Hart, 1989), originating from co- operative game theory, have emerged as a powerful model-agnostic tool to explain the predictions of machine learning models. This approach provides a way to fairly distribute the contribution of each feature to the overall prediction for a specific in- stance. By calculating the Shapley value for each feature, we gain insight into the importance of in- dividual features within the context of a specific prediction. This interpretability technique has been adopted across various machine learning models. In this study, we use Shapley values to interpret the contributions of extracted features in our"}