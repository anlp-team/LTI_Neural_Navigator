{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_The_Devil_Is_in_the_Errors:_Leveraging_Large_Language_Models_for_Fine-grained_Machine_Translation_Evaluation_chunk_10.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of annotating segments in the text?", "answer": " To identify errors within each translated segment, up to a maximum of five.", "ref_chunk": "segment level, where a segment may contain one or more sentences. Each segment is aligned with a corresponding source segment, and both segments are displayed within their respective documents. Annotate segments in natural order, as if you were reading the document. You may return to revise previous segments. Please identify all errors within each translated segment, up to a maximum of five. If there are more than five errors, identify only the five most severe. If it is not possible to reliably identify distinct errors because the translation is too badly garbled or is unrelated to the source, then mark a single Non-translation error that spans the entire segment. To identify an error, highlight the relevant span of text, and select a category/sub-category and severity level from the available options. (The span of text may be in the source segment if the error is a source error or an omission.) When identifying errors, please be as fine-grained as possible. For example, if a sentence contains two words that are each mistranslated, two separate mistranslation errors should be recorded. If a single stretch of text contains multiple errors, you only need to indicate the one that is most severe. If all have the same severity, choose the first matching category listed in the error typology (eg, Accuracy, then Fluency, then Terminology, etc). Please pay particular attention to document context when annotating. If a translation might be questionable on its own but is fine in the context of the document, it should not be considered erroneous; conversely, if a translation might be acceptable in some context, but not within the current document, it should be marked as wrong. There are two special error categories: Source error and Non-translation. Source errors should be annotated separately, highlighting the relevant span in the source segment. They do not count against the five-error limit for target errors, which should be handled in the usual way, whether or not they resulted from a source error. There can be at most one Non-translation error per segment, and it should span the entire segment. No other errors should be identified if Non-Translation is selected. Table 7: MQM annotator guidelines Annotators are asked to assign both an error severity and category. Error severity (either major or minor) is assigned independently of category. Spans with no marked errors have neutral severity and no category. Possible error categories are displayed in Table 8. Since MQM doesn\u2019t ask annotators for quality scores, those scores are derived automatically from the identified error spans and their classifications, based on a weighting of each error severity and category. Table 9 summarizes this weighting scheme, in which segment-level scores can range from 0 (perfect) to 25 (worst). The final segment-level score is an average over scores from all annotators. In some settings (e.g. calculating correlation for learned metrics), the scores are negated. We use the same weighting to obtain scores from errors identified by AUTOMQM. B Sampling in-context learning examples for AutoMQM Figure 11 shows the rejection criteria used when sampling example sets as discussed in \u00a74. C Additional Results Figures 12, 13, and 14 present additional experimental results. Error Category Accuracy Fluency Terminology Style Locale convention Other Source error Non-translation Description Addition Omission Mistranslation Untranslated text Translation includes information not present in the source. Translation is missing content from the source. Translation does not accurately represent the source. Source text has been left untranslated. Punctuation Spelling Grammar Register Inconsistency Character encoding Incorrect punctuation (for locale or style). Incorrect spelling or capitalization. Problems with grammar, other than orthography. Wrong grammatical register (eg, inappropriately informal pronouns). Internal inconsistency (not related to terminology). Characters are garbled due to incorrect encoding. Inappropriate for context Terminology is non-standard or does not fit context. Inconsistent use Terminology is used inconsistently. Awkward Translation has stylistic problems. Address format Currency format Date format Name format Telephone format Time format Wrong format for addresses. Wrong format for currency. Wrong format for dates. Wrong format for names. Wrong format for telephone numbers. Wrong format for time expressions. Any other issues. An error in the source. Impossible to reliably characterize distinct errors. Table 8: MQM hierarchy. Severity Category Weight Major Non-translation all others 25 5 Minor Fluency/Punctuation all others 0.1 1 Neutral all 0 Table 9: MQM error weighting. 1 def check_icl_set( 2 examples: pd.DataFrame, min_errors=3, majmin_threshold=2, cat_diversity=2, min_clen=20, max_clen=400, 3 4 5 6 7 8 ): 9 # Check if they have the same number of spans as severity/category if not examples.apply( 10 11 lambda r: 12 len(r[\u2019span\u2019]) == len(r[\u2019severity\u2019]) and len(r[\u2019span\u2019]) == len(r[\u2019category\u2019]), 13 axis=1 14 ).all(): 15 return False 16 17 18 # Check if there are at least min_errors if examples[\u2019severity\u2019].apply(lambda svs: len(svs)).sum() < min_errors: 19 return False 20 21 22 23 24 # Check that there\u2019s a balance of major and minor errors. major_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019major\u2019 for s in svs])).sum() minor_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019minor\u2019 for s in svs])).sum() if abs(major_count - minor_count) > majmin_threshold: 25 return False 26 27 28 29 30 # Check that at least cat_diversity error types are represented. categories = examples[\u2019category\u2019].apply(lambda cs: [c.split(\"/\")[0] for c in cs]) represented_error_types = set().union(*categories.tolist()) if len(represented_error_types) < cat_diversity: 31 return False 32 33 top_clen = examples.apply( 34 lambda row: max(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019) 35 36 ), axis=1).max() bot_clen = examples.apply( 37 lambda row: min(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019)), 38 axis=1).min() 39 40 if top_clen > max_clen or bot_clen < min_clen: 41 return False 42 43 44 # All checks passed. return True Figure 11: Rejection criteria used when sampling in-context learning examples for AUTOMQM. 0.25 0.25 0.40 0.40 0.30 0.30 0.45Pearson 0.45Pearson 0.20 0.05 0.10 PaLM-2 (Bison) ref-based 0.15 2 0.10 0.35 0.35 0.00 0 1 1 4# of in-context examples 4# of in-context examples 0.15 0.05 0.00 0 2 0.20 PaLM-2 (Bison) ref-free PaLM-2 (Bison) ref-free 3 3 PaLM-2 (Bison) ref-based Figure 12: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the score prediction prompt,"}, {"question": " How are errors categorized during the annotation process?", "answer": " Errors are categorized as Addition, Omission, Mistranslation, Untranslated text, Punctuation, Spelling, Grammar, Register, Inconsistency, Character encoding, Terminology, Style, Locale convention, Source error, or Non-translation.", "ref_chunk": "segment level, where a segment may contain one or more sentences. Each segment is aligned with a corresponding source segment, and both segments are displayed within their respective documents. Annotate segments in natural order, as if you were reading the document. You may return to revise previous segments. Please identify all errors within each translated segment, up to a maximum of five. If there are more than five errors, identify only the five most severe. If it is not possible to reliably identify distinct errors because the translation is too badly garbled or is unrelated to the source, then mark a single Non-translation error that spans the entire segment. To identify an error, highlight the relevant span of text, and select a category/sub-category and severity level from the available options. (The span of text may be in the source segment if the error is a source error or an omission.) When identifying errors, please be as fine-grained as possible. For example, if a sentence contains two words that are each mistranslated, two separate mistranslation errors should be recorded. If a single stretch of text contains multiple errors, you only need to indicate the one that is most severe. If all have the same severity, choose the first matching category listed in the error typology (eg, Accuracy, then Fluency, then Terminology, etc). Please pay particular attention to document context when annotating. If a translation might be questionable on its own but is fine in the context of the document, it should not be considered erroneous; conversely, if a translation might be acceptable in some context, but not within the current document, it should be marked as wrong. There are two special error categories: Source error and Non-translation. Source errors should be annotated separately, highlighting the relevant span in the source segment. They do not count against the five-error limit for target errors, which should be handled in the usual way, whether or not they resulted from a source error. There can be at most one Non-translation error per segment, and it should span the entire segment. No other errors should be identified if Non-Translation is selected. Table 7: MQM annotator guidelines Annotators are asked to assign both an error severity and category. Error severity (either major or minor) is assigned independently of category. Spans with no marked errors have neutral severity and no category. Possible error categories are displayed in Table 8. Since MQM doesn\u2019t ask annotators for quality scores, those scores are derived automatically from the identified error spans and their classifications, based on a weighting of each error severity and category. Table 9 summarizes this weighting scheme, in which segment-level scores can range from 0 (perfect) to 25 (worst). The final segment-level score is an average over scores from all annotators. In some settings (e.g. calculating correlation for learned metrics), the scores are negated. We use the same weighting to obtain scores from errors identified by AUTOMQM. B Sampling in-context learning examples for AutoMQM Figure 11 shows the rejection criteria used when sampling example sets as discussed in \u00a74. C Additional Results Figures 12, 13, and 14 present additional experimental results. Error Category Accuracy Fluency Terminology Style Locale convention Other Source error Non-translation Description Addition Omission Mistranslation Untranslated text Translation includes information not present in the source. Translation is missing content from the source. Translation does not accurately represent the source. Source text has been left untranslated. Punctuation Spelling Grammar Register Inconsistency Character encoding Incorrect punctuation (for locale or style). Incorrect spelling or capitalization. Problems with grammar, other than orthography. Wrong grammatical register (eg, inappropriately informal pronouns). Internal inconsistency (not related to terminology). Characters are garbled due to incorrect encoding. Inappropriate for context Terminology is non-standard or does not fit context. Inconsistent use Terminology is used inconsistently. Awkward Translation has stylistic problems. Address format Currency format Date format Name format Telephone format Time format Wrong format for addresses. Wrong format for currency. Wrong format for dates. Wrong format for names. Wrong format for telephone numbers. Wrong format for time expressions. Any other issues. An error in the source. Impossible to reliably characterize distinct errors. Table 8: MQM hierarchy. Severity Category Weight Major Non-translation all others 25 5 Minor Fluency/Punctuation all others 0.1 1 Neutral all 0 Table 9: MQM error weighting. 1 def check_icl_set( 2 examples: pd.DataFrame, min_errors=3, majmin_threshold=2, cat_diversity=2, min_clen=20, max_clen=400, 3 4 5 6 7 8 ): 9 # Check if they have the same number of spans as severity/category if not examples.apply( 10 11 lambda r: 12 len(r[\u2019span\u2019]) == len(r[\u2019severity\u2019]) and len(r[\u2019span\u2019]) == len(r[\u2019category\u2019]), 13 axis=1 14 ).all(): 15 return False 16 17 18 # Check if there are at least min_errors if examples[\u2019severity\u2019].apply(lambda svs: len(svs)).sum() < min_errors: 19 return False 20 21 22 23 24 # Check that there\u2019s a balance of major and minor errors. major_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019major\u2019 for s in svs])).sum() minor_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019minor\u2019 for s in svs])).sum() if abs(major_count - minor_count) > majmin_threshold: 25 return False 26 27 28 29 30 # Check that at least cat_diversity error types are represented. categories = examples[\u2019category\u2019].apply(lambda cs: [c.split(\"/\")[0] for c in cs]) represented_error_types = set().union(*categories.tolist()) if len(represented_error_types) < cat_diversity: 31 return False 32 33 top_clen = examples.apply( 34 lambda row: max(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019) 35 36 ), axis=1).max() bot_clen = examples.apply( 37 lambda row: min(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019)), 38 axis=1).min() 39 40 if top_clen > max_clen or bot_clen < min_clen: 41 return False 42 43 44 # All checks passed. return True Figure 11: Rejection criteria used when sampling in-context learning examples for AUTOMQM. 0.25 0.25 0.40 0.40 0.30 0.30 0.45Pearson 0.45Pearson 0.20 0.05 0.10 PaLM-2 (Bison) ref-based 0.15 2 0.10 0.35 0.35 0.00 0 1 1 4# of in-context examples 4# of in-context examples 0.15 0.05 0.00 0 2 0.20 PaLM-2 (Bison) ref-free PaLM-2 (Bison) ref-free 3 3 PaLM-2 (Bison) ref-based Figure 12: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the score prediction prompt,"}, {"question": " What is the difference between major and minor errors in the annotation process?", "answer": " Error severity is classified as either major or minor, independent of the error category.", "ref_chunk": "segment level, where a segment may contain one or more sentences. Each segment is aligned with a corresponding source segment, and both segments are displayed within their respective documents. Annotate segments in natural order, as if you were reading the document. You may return to revise previous segments. Please identify all errors within each translated segment, up to a maximum of five. If there are more than five errors, identify only the five most severe. If it is not possible to reliably identify distinct errors because the translation is too badly garbled or is unrelated to the source, then mark a single Non-translation error that spans the entire segment. To identify an error, highlight the relevant span of text, and select a category/sub-category and severity level from the available options. (The span of text may be in the source segment if the error is a source error or an omission.) When identifying errors, please be as fine-grained as possible. For example, if a sentence contains two words that are each mistranslated, two separate mistranslation errors should be recorded. If a single stretch of text contains multiple errors, you only need to indicate the one that is most severe. If all have the same severity, choose the first matching category listed in the error typology (eg, Accuracy, then Fluency, then Terminology, etc). Please pay particular attention to document context when annotating. If a translation might be questionable on its own but is fine in the context of the document, it should not be considered erroneous; conversely, if a translation might be acceptable in some context, but not within the current document, it should be marked as wrong. There are two special error categories: Source error and Non-translation. Source errors should be annotated separately, highlighting the relevant span in the source segment. They do not count against the five-error limit for target errors, which should be handled in the usual way, whether or not they resulted from a source error. There can be at most one Non-translation error per segment, and it should span the entire segment. No other errors should be identified if Non-Translation is selected. Table 7: MQM annotator guidelines Annotators are asked to assign both an error severity and category. Error severity (either major or minor) is assigned independently of category. Spans with no marked errors have neutral severity and no category. Possible error categories are displayed in Table 8. Since MQM doesn\u2019t ask annotators for quality scores, those scores are derived automatically from the identified error spans and their classifications, based on a weighting of each error severity and category. Table 9 summarizes this weighting scheme, in which segment-level scores can range from 0 (perfect) to 25 (worst). The final segment-level score is an average over scores from all annotators. In some settings (e.g. calculating correlation for learned metrics), the scores are negated. We use the same weighting to obtain scores from errors identified by AUTOMQM. B Sampling in-context learning examples for AutoMQM Figure 11 shows the rejection criteria used when sampling example sets as discussed in \u00a74. C Additional Results Figures 12, 13, and 14 present additional experimental results. Error Category Accuracy Fluency Terminology Style Locale convention Other Source error Non-translation Description Addition Omission Mistranslation Untranslated text Translation includes information not present in the source. Translation is missing content from the source. Translation does not accurately represent the source. Source text has been left untranslated. Punctuation Spelling Grammar Register Inconsistency Character encoding Incorrect punctuation (for locale or style). Incorrect spelling or capitalization. Problems with grammar, other than orthography. Wrong grammatical register (eg, inappropriately informal pronouns). Internal inconsistency (not related to terminology). Characters are garbled due to incorrect encoding. Inappropriate for context Terminology is non-standard or does not fit context. Inconsistent use Terminology is used inconsistently. Awkward Translation has stylistic problems. Address format Currency format Date format Name format Telephone format Time format Wrong format for addresses. Wrong format for currency. Wrong format for dates. Wrong format for names. Wrong format for telephone numbers. Wrong format for time expressions. Any other issues. An error in the source. Impossible to reliably characterize distinct errors. Table 8: MQM hierarchy. Severity Category Weight Major Non-translation all others 25 5 Minor Fluency/Punctuation all others 0.1 1 Neutral all 0 Table 9: MQM error weighting. 1 def check_icl_set( 2 examples: pd.DataFrame, min_errors=3, majmin_threshold=2, cat_diversity=2, min_clen=20, max_clen=400, 3 4 5 6 7 8 ): 9 # Check if they have the same number of spans as severity/category if not examples.apply( 10 11 lambda r: 12 len(r[\u2019span\u2019]) == len(r[\u2019severity\u2019]) and len(r[\u2019span\u2019]) == len(r[\u2019category\u2019]), 13 axis=1 14 ).all(): 15 return False 16 17 18 # Check if there are at least min_errors if examples[\u2019severity\u2019].apply(lambda svs: len(svs)).sum() < min_errors: 19 return False 20 21 22 23 24 # Check that there\u2019s a balance of major and minor errors. major_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019major\u2019 for s in svs])).sum() minor_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019minor\u2019 for s in svs])).sum() if abs(major_count - minor_count) > majmin_threshold: 25 return False 26 27 28 29 30 # Check that at least cat_diversity error types are represented. categories = examples[\u2019category\u2019].apply(lambda cs: [c.split(\"/\")[0] for c in cs]) represented_error_types = set().union(*categories.tolist()) if len(represented_error_types) < cat_diversity: 31 return False 32 33 top_clen = examples.apply( 34 lambda row: max(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019) 35 36 ), axis=1).max() bot_clen = examples.apply( 37 lambda row: min(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019)), 38 axis=1).min() 39 40 if top_clen > max_clen or bot_clen < min_clen: 41 return False 42 43 44 # All checks passed. return True Figure 11: Rejection criteria used when sampling in-context learning examples for AUTOMQM. 0.25 0.25 0.40 0.40 0.30 0.30 0.45Pearson 0.45Pearson 0.20 0.05 0.10 PaLM-2 (Bison) ref-based 0.15 2 0.10 0.35 0.35 0.00 0 1 1 4# of in-context examples 4# of in-context examples 0.15 0.05 0.00 0 2 0.20 PaLM-2 (Bison) ref-free PaLM-2 (Bison) ref-free 3 3 PaLM-2 (Bison) ref-based Figure 12: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the score prediction prompt,"}, {"question": " What does the rejection criteria in Figure 11 show?", "answer": " The rejection criteria are used when sampling in-context learning examples for AUTOMQM.", "ref_chunk": "segment level, where a segment may contain one or more sentences. Each segment is aligned with a corresponding source segment, and both segments are displayed within their respective documents. Annotate segments in natural order, as if you were reading the document. You may return to revise previous segments. Please identify all errors within each translated segment, up to a maximum of five. If there are more than five errors, identify only the five most severe. If it is not possible to reliably identify distinct errors because the translation is too badly garbled or is unrelated to the source, then mark a single Non-translation error that spans the entire segment. To identify an error, highlight the relevant span of text, and select a category/sub-category and severity level from the available options. (The span of text may be in the source segment if the error is a source error or an omission.) When identifying errors, please be as fine-grained as possible. For example, if a sentence contains two words that are each mistranslated, two separate mistranslation errors should be recorded. If a single stretch of text contains multiple errors, you only need to indicate the one that is most severe. If all have the same severity, choose the first matching category listed in the error typology (eg, Accuracy, then Fluency, then Terminology, etc). Please pay particular attention to document context when annotating. If a translation might be questionable on its own but is fine in the context of the document, it should not be considered erroneous; conversely, if a translation might be acceptable in some context, but not within the current document, it should be marked as wrong. There are two special error categories: Source error and Non-translation. Source errors should be annotated separately, highlighting the relevant span in the source segment. They do not count against the five-error limit for target errors, which should be handled in the usual way, whether or not they resulted from a source error. There can be at most one Non-translation error per segment, and it should span the entire segment. No other errors should be identified if Non-Translation is selected. Table 7: MQM annotator guidelines Annotators are asked to assign both an error severity and category. Error severity (either major or minor) is assigned independently of category. Spans with no marked errors have neutral severity and no category. Possible error categories are displayed in Table 8. Since MQM doesn\u2019t ask annotators for quality scores, those scores are derived automatically from the identified error spans and their classifications, based on a weighting of each error severity and category. Table 9 summarizes this weighting scheme, in which segment-level scores can range from 0 (perfect) to 25 (worst). The final segment-level score is an average over scores from all annotators. In some settings (e.g. calculating correlation for learned metrics), the scores are negated. We use the same weighting to obtain scores from errors identified by AUTOMQM. B Sampling in-context learning examples for AutoMQM Figure 11 shows the rejection criteria used when sampling example sets as discussed in \u00a74. C Additional Results Figures 12, 13, and 14 present additional experimental results. Error Category Accuracy Fluency Terminology Style Locale convention Other Source error Non-translation Description Addition Omission Mistranslation Untranslated text Translation includes information not present in the source. Translation is missing content from the source. Translation does not accurately represent the source. Source text has been left untranslated. Punctuation Spelling Grammar Register Inconsistency Character encoding Incorrect punctuation (for locale or style). Incorrect spelling or capitalization. Problems with grammar, other than orthography. Wrong grammatical register (eg, inappropriately informal pronouns). Internal inconsistency (not related to terminology). Characters are garbled due to incorrect encoding. Inappropriate for context Terminology is non-standard or does not fit context. Inconsistent use Terminology is used inconsistently. Awkward Translation has stylistic problems. Address format Currency format Date format Name format Telephone format Time format Wrong format for addresses. Wrong format for currency. Wrong format for dates. Wrong format for names. Wrong format for telephone numbers. Wrong format for time expressions. Any other issues. An error in the source. Impossible to reliably characterize distinct errors. Table 8: MQM hierarchy. Severity Category Weight Major Non-translation all others 25 5 Minor Fluency/Punctuation all others 0.1 1 Neutral all 0 Table 9: MQM error weighting. 1 def check_icl_set( 2 examples: pd.DataFrame, min_errors=3, majmin_threshold=2, cat_diversity=2, min_clen=20, max_clen=400, 3 4 5 6 7 8 ): 9 # Check if they have the same number of spans as severity/category if not examples.apply( 10 11 lambda r: 12 len(r[\u2019span\u2019]) == len(r[\u2019severity\u2019]) and len(r[\u2019span\u2019]) == len(r[\u2019category\u2019]), 13 axis=1 14 ).all(): 15 return False 16 17 18 # Check if there are at least min_errors if examples[\u2019severity\u2019].apply(lambda svs: len(svs)).sum() < min_errors: 19 return False 20 21 22 23 24 # Check that there\u2019s a balance of major and minor errors. major_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019major\u2019 for s in svs])).sum() minor_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019minor\u2019 for s in svs])).sum() if abs(major_count - minor_count) > majmin_threshold: 25 return False 26 27 28 29 30 # Check that at least cat_diversity error types are represented. categories = examples[\u2019category\u2019].apply(lambda cs: [c.split(\"/\")[0] for c in cs]) represented_error_types = set().union(*categories.tolist()) if len(represented_error_types) < cat_diversity: 31 return False 32 33 top_clen = examples.apply( 34 lambda row: max(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019) 35 36 ), axis=1).max() bot_clen = examples.apply( 37 lambda row: min(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019)), 38 axis=1).min() 39 40 if top_clen > max_clen or bot_clen < min_clen: 41 return False 42 43 44 # All checks passed. return True Figure 11: Rejection criteria used when sampling in-context learning examples for AUTOMQM. 0.25 0.25 0.40 0.40 0.30 0.30 0.45Pearson 0.45Pearson 0.20 0.05 0.10 PaLM-2 (Bison) ref-based 0.15 2 0.10 0.35 0.35 0.00 0 1 1 4# of in-context examples 4# of in-context examples 0.15 0.05 0.00 0 2 0.20 PaLM-2 (Bison) ref-free PaLM-2 (Bison) ref-free 3 3 PaLM-2 (Bison) ref-based Figure 12: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the score prediction prompt,"}, {"question": " How are errors weighted in the annotation process?", "answer": " Major errors are weighted at 25, minor errors at 0.1, and neutral errors at 0.", "ref_chunk": "segment level, where a segment may contain one or more sentences. Each segment is aligned with a corresponding source segment, and both segments are displayed within their respective documents. Annotate segments in natural order, as if you were reading the document. You may return to revise previous segments. Please identify all errors within each translated segment, up to a maximum of five. If there are more than five errors, identify only the five most severe. If it is not possible to reliably identify distinct errors because the translation is too badly garbled or is unrelated to the source, then mark a single Non-translation error that spans the entire segment. To identify an error, highlight the relevant span of text, and select a category/sub-category and severity level from the available options. (The span of text may be in the source segment if the error is a source error or an omission.) When identifying errors, please be as fine-grained as possible. For example, if a sentence contains two words that are each mistranslated, two separate mistranslation errors should be recorded. If a single stretch of text contains multiple errors, you only need to indicate the one that is most severe. If all have the same severity, choose the first matching category listed in the error typology (eg, Accuracy, then Fluency, then Terminology, etc). Please pay particular attention to document context when annotating. If a translation might be questionable on its own but is fine in the context of the document, it should not be considered erroneous; conversely, if a translation might be acceptable in some context, but not within the current document, it should be marked as wrong. There are two special error categories: Source error and Non-translation. Source errors should be annotated separately, highlighting the relevant span in the source segment. They do not count against the five-error limit for target errors, which should be handled in the usual way, whether or not they resulted from a source error. There can be at most one Non-translation error per segment, and it should span the entire segment. No other errors should be identified if Non-Translation is selected. Table 7: MQM annotator guidelines Annotators are asked to assign both an error severity and category. Error severity (either major or minor) is assigned independently of category. Spans with no marked errors have neutral severity and no category. Possible error categories are displayed in Table 8. Since MQM doesn\u2019t ask annotators for quality scores, those scores are derived automatically from the identified error spans and their classifications, based on a weighting of each error severity and category. Table 9 summarizes this weighting scheme, in which segment-level scores can range from 0 (perfect) to 25 (worst). The final segment-level score is an average over scores from all annotators. In some settings (e.g. calculating correlation for learned metrics), the scores are negated. We use the same weighting to obtain scores from errors identified by AUTOMQM. B Sampling in-context learning examples for AutoMQM Figure 11 shows the rejection criteria used when sampling example sets as discussed in \u00a74. C Additional Results Figures 12, 13, and 14 present additional experimental results. Error Category Accuracy Fluency Terminology Style Locale convention Other Source error Non-translation Description Addition Omission Mistranslation Untranslated text Translation includes information not present in the source. Translation is missing content from the source. Translation does not accurately represent the source. Source text has been left untranslated. Punctuation Spelling Grammar Register Inconsistency Character encoding Incorrect punctuation (for locale or style). Incorrect spelling or capitalization. Problems with grammar, other than orthography. Wrong grammatical register (eg, inappropriately informal pronouns). Internal inconsistency (not related to terminology). Characters are garbled due to incorrect encoding. Inappropriate for context Terminology is non-standard or does not fit context. Inconsistent use Terminology is used inconsistently. Awkward Translation has stylistic problems. Address format Currency format Date format Name format Telephone format Time format Wrong format for addresses. Wrong format for currency. Wrong format for dates. Wrong format for names. Wrong format for telephone numbers. Wrong format for time expressions. Any other issues. An error in the source. Impossible to reliably characterize distinct errors. Table 8: MQM hierarchy. Severity Category Weight Major Non-translation all others 25 5 Minor Fluency/Punctuation all others 0.1 1 Neutral all 0 Table 9: MQM error weighting. 1 def check_icl_set( 2 examples: pd.DataFrame, min_errors=3, majmin_threshold=2, cat_diversity=2, min_clen=20, max_clen=400, 3 4 5 6 7 8 ): 9 # Check if they have the same number of spans as severity/category if not examples.apply( 10 11 lambda r: 12 len(r[\u2019span\u2019]) == len(r[\u2019severity\u2019]) and len(r[\u2019span\u2019]) == len(r[\u2019category\u2019]), 13 axis=1 14 ).all(): 15 return False 16 17 18 # Check if there are at least min_errors if examples[\u2019severity\u2019].apply(lambda svs: len(svs)).sum() < min_errors: 19 return False 20 21 22 23 24 # Check that there\u2019s a balance of major and minor errors. major_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019major\u2019 for s in svs])).sum() minor_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019minor\u2019 for s in svs])).sum() if abs(major_count - minor_count) > majmin_threshold: 25 return False 26 27 28 29 30 # Check that at least cat_diversity error types are represented. categories = examples[\u2019category\u2019].apply(lambda cs: [c.split(\"/\")[0] for c in cs]) represented_error_types = set().union(*categories.tolist()) if len(represented_error_types) < cat_diversity: 31 return False 32 33 top_clen = examples.apply( 34 lambda row: max(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019) 35 36 ), axis=1).max() bot_clen = examples.apply( 37 lambda row: min(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019)), 38 axis=1).min() 39 40 if top_clen > max_clen or bot_clen < min_clen: 41 return False 42 43 44 # All checks passed. return True Figure 11: Rejection criteria used when sampling in-context learning examples for AUTOMQM. 0.25 0.25 0.40 0.40 0.30 0.30 0.45Pearson 0.45Pearson 0.20 0.05 0.10 PaLM-2 (Bison) ref-based 0.15 2 0.10 0.35 0.35 0.00 0 1 1 4# of in-context examples 4# of in-context examples 0.15 0.05 0.00 0 2 0.20 PaLM-2 (Bison) ref-free PaLM-2 (Bison) ref-free 3 3 PaLM-2 (Bison) ref-based Figure 12: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the score prediction prompt,"}, {"question": " What is the purpose of assigning error severity and category to each error?", "answer": " To derive quality scores automatically from the identified error spans and their classifications.", "ref_chunk": "segment level, where a segment may contain one or more sentences. Each segment is aligned with a corresponding source segment, and both segments are displayed within their respective documents. Annotate segments in natural order, as if you were reading the document. You may return to revise previous segments. Please identify all errors within each translated segment, up to a maximum of five. If there are more than five errors, identify only the five most severe. If it is not possible to reliably identify distinct errors because the translation is too badly garbled or is unrelated to the source, then mark a single Non-translation error that spans the entire segment. To identify an error, highlight the relevant span of text, and select a category/sub-category and severity level from the available options. (The span of text may be in the source segment if the error is a source error or an omission.) When identifying errors, please be as fine-grained as possible. For example, if a sentence contains two words that are each mistranslated, two separate mistranslation errors should be recorded. If a single stretch of text contains multiple errors, you only need to indicate the one that is most severe. If all have the same severity, choose the first matching category listed in the error typology (eg, Accuracy, then Fluency, then Terminology, etc). Please pay particular attention to document context when annotating. If a translation might be questionable on its own but is fine in the context of the document, it should not be considered erroneous; conversely, if a translation might be acceptable in some context, but not within the current document, it should be marked as wrong. There are two special error categories: Source error and Non-translation. Source errors should be annotated separately, highlighting the relevant span in the source segment. They do not count against the five-error limit for target errors, which should be handled in the usual way, whether or not they resulted from a source error. There can be at most one Non-translation error per segment, and it should span the entire segment. No other errors should be identified if Non-Translation is selected. Table 7: MQM annotator guidelines Annotators are asked to assign both an error severity and category. Error severity (either major or minor) is assigned independently of category. Spans with no marked errors have neutral severity and no category. Possible error categories are displayed in Table 8. Since MQM doesn\u2019t ask annotators for quality scores, those scores are derived automatically from the identified error spans and their classifications, based on a weighting of each error severity and category. Table 9 summarizes this weighting scheme, in which segment-level scores can range from 0 (perfect) to 25 (worst). The final segment-level score is an average over scores from all annotators. In some settings (e.g. calculating correlation for learned metrics), the scores are negated. We use the same weighting to obtain scores from errors identified by AUTOMQM. B Sampling in-context learning examples for AutoMQM Figure 11 shows the rejection criteria used when sampling example sets as discussed in \u00a74. C Additional Results Figures 12, 13, and 14 present additional experimental results. Error Category Accuracy Fluency Terminology Style Locale convention Other Source error Non-translation Description Addition Omission Mistranslation Untranslated text Translation includes information not present in the source. Translation is missing content from the source. Translation does not accurately represent the source. Source text has been left untranslated. Punctuation Spelling Grammar Register Inconsistency Character encoding Incorrect punctuation (for locale or style). Incorrect spelling or capitalization. Problems with grammar, other than orthography. Wrong grammatical register (eg, inappropriately informal pronouns). Internal inconsistency (not related to terminology). Characters are garbled due to incorrect encoding. Inappropriate for context Terminology is non-standard or does not fit context. Inconsistent use Terminology is used inconsistently. Awkward Translation has stylistic problems. Address format Currency format Date format Name format Telephone format Time format Wrong format for addresses. Wrong format for currency. Wrong format for dates. Wrong format for names. Wrong format for telephone numbers. Wrong format for time expressions. Any other issues. An error in the source. Impossible to reliably characterize distinct errors. Table 8: MQM hierarchy. Severity Category Weight Major Non-translation all others 25 5 Minor Fluency/Punctuation all others 0.1 1 Neutral all 0 Table 9: MQM error weighting. 1 def check_icl_set( 2 examples: pd.DataFrame, min_errors=3, majmin_threshold=2, cat_diversity=2, min_clen=20, max_clen=400, 3 4 5 6 7 8 ): 9 # Check if they have the same number of spans as severity/category if not examples.apply( 10 11 lambda r: 12 len(r[\u2019span\u2019]) == len(r[\u2019severity\u2019]) and len(r[\u2019span\u2019]) == len(r[\u2019category\u2019]), 13 axis=1 14 ).all(): 15 return False 16 17 18 # Check if there are at least min_errors if examples[\u2019severity\u2019].apply(lambda svs: len(svs)).sum() < min_errors: 19 return False 20 21 22 23 24 # Check that there\u2019s a balance of major and minor errors. major_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019major\u2019 for s in svs])).sum() minor_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019minor\u2019 for s in svs])).sum() if abs(major_count - minor_count) > majmin_threshold: 25 return False 26 27 28 29 30 # Check that at least cat_diversity error types are represented. categories = examples[\u2019category\u2019].apply(lambda cs: [c.split(\"/\")[0] for c in cs]) represented_error_types = set().union(*categories.tolist()) if len(represented_error_types) < cat_diversity: 31 return False 32 33 top_clen = examples.apply( 34 lambda row: max(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019) 35 36 ), axis=1).max() bot_clen = examples.apply( 37 lambda row: min(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019)), 38 axis=1).min() 39 40 if top_clen > max_clen or bot_clen < min_clen: 41 return False 42 43 44 # All checks passed. return True Figure 11: Rejection criteria used when sampling in-context learning examples for AUTOMQM. 0.25 0.25 0.40 0.40 0.30 0.30 0.45Pearson 0.45Pearson 0.20 0.05 0.10 PaLM-2 (Bison) ref-based 0.15 2 0.10 0.35 0.35 0.00 0 1 1 4# of in-context examples 4# of in-context examples 0.15 0.05 0.00 0 2 0.20 PaLM-2 (Bison) ref-free PaLM-2 (Bison) ref-free 3 3 PaLM-2 (Bison) ref-based Figure 12: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the score prediction prompt,"}, {"question": " In the MQM error hierarchy, what does a Non-translation error entail?", "answer": " A Non-translation error spans the entire segment and signifies the absence of translation.", "ref_chunk": "segment level, where a segment may contain one or more sentences. Each segment is aligned with a corresponding source segment, and both segments are displayed within their respective documents. Annotate segments in natural order, as if you were reading the document. You may return to revise previous segments. Please identify all errors within each translated segment, up to a maximum of five. If there are more than five errors, identify only the five most severe. If it is not possible to reliably identify distinct errors because the translation is too badly garbled or is unrelated to the source, then mark a single Non-translation error that spans the entire segment. To identify an error, highlight the relevant span of text, and select a category/sub-category and severity level from the available options. (The span of text may be in the source segment if the error is a source error or an omission.) When identifying errors, please be as fine-grained as possible. For example, if a sentence contains two words that are each mistranslated, two separate mistranslation errors should be recorded. If a single stretch of text contains multiple errors, you only need to indicate the one that is most severe. If all have the same severity, choose the first matching category listed in the error typology (eg, Accuracy, then Fluency, then Terminology, etc). Please pay particular attention to document context when annotating. If a translation might be questionable on its own but is fine in the context of the document, it should not be considered erroneous; conversely, if a translation might be acceptable in some context, but not within the current document, it should be marked as wrong. There are two special error categories: Source error and Non-translation. Source errors should be annotated separately, highlighting the relevant span in the source segment. They do not count against the five-error limit for target errors, which should be handled in the usual way, whether or not they resulted from a source error. There can be at most one Non-translation error per segment, and it should span the entire segment. No other errors should be identified if Non-Translation is selected. Table 7: MQM annotator guidelines Annotators are asked to assign both an error severity and category. Error severity (either major or minor) is assigned independently of category. Spans with no marked errors have neutral severity and no category. Possible error categories are displayed in Table 8. Since MQM doesn\u2019t ask annotators for quality scores, those scores are derived automatically from the identified error spans and their classifications, based on a weighting of each error severity and category. Table 9 summarizes this weighting scheme, in which segment-level scores can range from 0 (perfect) to 25 (worst). The final segment-level score is an average over scores from all annotators. In some settings (e.g. calculating correlation for learned metrics), the scores are negated. We use the same weighting to obtain scores from errors identified by AUTOMQM. B Sampling in-context learning examples for AutoMQM Figure 11 shows the rejection criteria used when sampling example sets as discussed in \u00a74. C Additional Results Figures 12, 13, and 14 present additional experimental results. Error Category Accuracy Fluency Terminology Style Locale convention Other Source error Non-translation Description Addition Omission Mistranslation Untranslated text Translation includes information not present in the source. Translation is missing content from the source. Translation does not accurately represent the source. Source text has been left untranslated. Punctuation Spelling Grammar Register Inconsistency Character encoding Incorrect punctuation (for locale or style). Incorrect spelling or capitalization. Problems with grammar, other than orthography. Wrong grammatical register (eg, inappropriately informal pronouns). Internal inconsistency (not related to terminology). Characters are garbled due to incorrect encoding. Inappropriate for context Terminology is non-standard or does not fit context. Inconsistent use Terminology is used inconsistently. Awkward Translation has stylistic problems. Address format Currency format Date format Name format Telephone format Time format Wrong format for addresses. Wrong format for currency. Wrong format for dates. Wrong format for names. Wrong format for telephone numbers. Wrong format for time expressions. Any other issues. An error in the source. Impossible to reliably characterize distinct errors. Table 8: MQM hierarchy. Severity Category Weight Major Non-translation all others 25 5 Minor Fluency/Punctuation all others 0.1 1 Neutral all 0 Table 9: MQM error weighting. 1 def check_icl_set( 2 examples: pd.DataFrame, min_errors=3, majmin_threshold=2, cat_diversity=2, min_clen=20, max_clen=400, 3 4 5 6 7 8 ): 9 # Check if they have the same number of spans as severity/category if not examples.apply( 10 11 lambda r: 12 len(r[\u2019span\u2019]) == len(r[\u2019severity\u2019]) and len(r[\u2019span\u2019]) == len(r[\u2019category\u2019]), 13 axis=1 14 ).all(): 15 return False 16 17 18 # Check if there are at least min_errors if examples[\u2019severity\u2019].apply(lambda svs: len(svs)).sum() < min_errors: 19 return False 20 21 22 23 24 # Check that there\u2019s a balance of major and minor errors. major_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019major\u2019 for s in svs])).sum() minor_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019minor\u2019 for s in svs])).sum() if abs(major_count - minor_count) > majmin_threshold: 25 return False 26 27 28 29 30 # Check that at least cat_diversity error types are represented. categories = examples[\u2019category\u2019].apply(lambda cs: [c.split(\"/\")[0] for c in cs]) represented_error_types = set().union(*categories.tolist()) if len(represented_error_types) < cat_diversity: 31 return False 32 33 top_clen = examples.apply( 34 lambda row: max(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019) 35 36 ), axis=1).max() bot_clen = examples.apply( 37 lambda row: min(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019)), 38 axis=1).min() 39 40 if top_clen > max_clen or bot_clen < min_clen: 41 return False 42 43 44 # All checks passed. return True Figure 11: Rejection criteria used when sampling in-context learning examples for AUTOMQM. 0.25 0.25 0.40 0.40 0.30 0.30 0.45Pearson 0.45Pearson 0.20 0.05 0.10 PaLM-2 (Bison) ref-based 0.15 2 0.10 0.35 0.35 0.00 0 1 1 4# of in-context examples 4# of in-context examples 0.15 0.05 0.00 0 2 0.20 PaLM-2 (Bison) ref-free PaLM-2 (Bison) ref-free 3 3 PaLM-2 (Bison) ref-based Figure 12: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the score prediction prompt,"}, {"question": " How are errors from errors identified by AUTOMQM used?", "answer": " The same weighting scheme is used to obtain scores from errors identified by AUTOMQM.", "ref_chunk": "segment level, where a segment may contain one or more sentences. Each segment is aligned with a corresponding source segment, and both segments are displayed within their respective documents. Annotate segments in natural order, as if you were reading the document. You may return to revise previous segments. Please identify all errors within each translated segment, up to a maximum of five. If there are more than five errors, identify only the five most severe. If it is not possible to reliably identify distinct errors because the translation is too badly garbled or is unrelated to the source, then mark a single Non-translation error that spans the entire segment. To identify an error, highlight the relevant span of text, and select a category/sub-category and severity level from the available options. (The span of text may be in the source segment if the error is a source error or an omission.) When identifying errors, please be as fine-grained as possible. For example, if a sentence contains two words that are each mistranslated, two separate mistranslation errors should be recorded. If a single stretch of text contains multiple errors, you only need to indicate the one that is most severe. If all have the same severity, choose the first matching category listed in the error typology (eg, Accuracy, then Fluency, then Terminology, etc). Please pay particular attention to document context when annotating. If a translation might be questionable on its own but is fine in the context of the document, it should not be considered erroneous; conversely, if a translation might be acceptable in some context, but not within the current document, it should be marked as wrong. There are two special error categories: Source error and Non-translation. Source errors should be annotated separately, highlighting the relevant span in the source segment. They do not count against the five-error limit for target errors, which should be handled in the usual way, whether or not they resulted from a source error. There can be at most one Non-translation error per segment, and it should span the entire segment. No other errors should be identified if Non-Translation is selected. Table 7: MQM annotator guidelines Annotators are asked to assign both an error severity and category. Error severity (either major or minor) is assigned independently of category. Spans with no marked errors have neutral severity and no category. Possible error categories are displayed in Table 8. Since MQM doesn\u2019t ask annotators for quality scores, those scores are derived automatically from the identified error spans and their classifications, based on a weighting of each error severity and category. Table 9 summarizes this weighting scheme, in which segment-level scores can range from 0 (perfect) to 25 (worst). The final segment-level score is an average over scores from all annotators. In some settings (e.g. calculating correlation for learned metrics), the scores are negated. We use the same weighting to obtain scores from errors identified by AUTOMQM. B Sampling in-context learning examples for AutoMQM Figure 11 shows the rejection criteria used when sampling example sets as discussed in \u00a74. C Additional Results Figures 12, 13, and 14 present additional experimental results. Error Category Accuracy Fluency Terminology Style Locale convention Other Source error Non-translation Description Addition Omission Mistranslation Untranslated text Translation includes information not present in the source. Translation is missing content from the source. Translation does not accurately represent the source. Source text has been left untranslated. Punctuation Spelling Grammar Register Inconsistency Character encoding Incorrect punctuation (for locale or style). Incorrect spelling or capitalization. Problems with grammar, other than orthography. Wrong grammatical register (eg, inappropriately informal pronouns). Internal inconsistency (not related to terminology). Characters are garbled due to incorrect encoding. Inappropriate for context Terminology is non-standard or does not fit context. Inconsistent use Terminology is used inconsistently. Awkward Translation has stylistic problems. Address format Currency format Date format Name format Telephone format Time format Wrong format for addresses. Wrong format for currency. Wrong format for dates. Wrong format for names. Wrong format for telephone numbers. Wrong format for time expressions. Any other issues. An error in the source. Impossible to reliably characterize distinct errors. Table 8: MQM hierarchy. Severity Category Weight Major Non-translation all others 25 5 Minor Fluency/Punctuation all others 0.1 1 Neutral all 0 Table 9: MQM error weighting. 1 def check_icl_set( 2 examples: pd.DataFrame, min_errors=3, majmin_threshold=2, cat_diversity=2, min_clen=20, max_clen=400, 3 4 5 6 7 8 ): 9 # Check if they have the same number of spans as severity/category if not examples.apply( 10 11 lambda r: 12 len(r[\u2019span\u2019]) == len(r[\u2019severity\u2019]) and len(r[\u2019span\u2019]) == len(r[\u2019category\u2019]), 13 axis=1 14 ).all(): 15 return False 16 17 18 # Check if there are at least min_errors if examples[\u2019severity\u2019].apply(lambda svs: len(svs)).sum() < min_errors: 19 return False 20 21 22 23 24 # Check that there\u2019s a balance of major and minor errors. major_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019major\u2019 for s in svs])).sum() minor_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019minor\u2019 for s in svs])).sum() if abs(major_count - minor_count) > majmin_threshold: 25 return False 26 27 28 29 30 # Check that at least cat_diversity error types are represented. categories = examples[\u2019category\u2019].apply(lambda cs: [c.split(\"/\")[0] for c in cs]) represented_error_types = set().union(*categories.tolist()) if len(represented_error_types) < cat_diversity: 31 return False 32 33 top_clen = examples.apply( 34 lambda row: max(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019) 35 36 ), axis=1).max() bot_clen = examples.apply( 37 lambda row: min(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019)), 38 axis=1).min() 39 40 if top_clen > max_clen or bot_clen < min_clen: 41 return False 42 43 44 # All checks passed. return True Figure 11: Rejection criteria used when sampling in-context learning examples for AUTOMQM. 0.25 0.25 0.40 0.40 0.30 0.30 0.45Pearson 0.45Pearson 0.20 0.05 0.10 PaLM-2 (Bison) ref-based 0.15 2 0.10 0.35 0.35 0.00 0 1 1 4# of in-context examples 4# of in-context examples 0.15 0.05 0.00 0 2 0.20 PaLM-2 (Bison) ref-free PaLM-2 (Bison) ref-free 3 3 PaLM-2 (Bison) ref-based Figure 12: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the score prediction prompt,"}, {"question": " What does Table 9 summarize?", "answer": " Table 9 summarizes the MQM error weighting scheme, which ranges from 0 (perfect) to 25 (worst).", "ref_chunk": "segment level, where a segment may contain one or more sentences. Each segment is aligned with a corresponding source segment, and both segments are displayed within their respective documents. Annotate segments in natural order, as if you were reading the document. You may return to revise previous segments. Please identify all errors within each translated segment, up to a maximum of five. If there are more than five errors, identify only the five most severe. If it is not possible to reliably identify distinct errors because the translation is too badly garbled or is unrelated to the source, then mark a single Non-translation error that spans the entire segment. To identify an error, highlight the relevant span of text, and select a category/sub-category and severity level from the available options. (The span of text may be in the source segment if the error is a source error or an omission.) When identifying errors, please be as fine-grained as possible. For example, if a sentence contains two words that are each mistranslated, two separate mistranslation errors should be recorded. If a single stretch of text contains multiple errors, you only need to indicate the one that is most severe. If all have the same severity, choose the first matching category listed in the error typology (eg, Accuracy, then Fluency, then Terminology, etc). Please pay particular attention to document context when annotating. If a translation might be questionable on its own but is fine in the context of the document, it should not be considered erroneous; conversely, if a translation might be acceptable in some context, but not within the current document, it should be marked as wrong. There are two special error categories: Source error and Non-translation. Source errors should be annotated separately, highlighting the relevant span in the source segment. They do not count against the five-error limit for target errors, which should be handled in the usual way, whether or not they resulted from a source error. There can be at most one Non-translation error per segment, and it should span the entire segment. No other errors should be identified if Non-Translation is selected. Table 7: MQM annotator guidelines Annotators are asked to assign both an error severity and category. Error severity (either major or minor) is assigned independently of category. Spans with no marked errors have neutral severity and no category. Possible error categories are displayed in Table 8. Since MQM doesn\u2019t ask annotators for quality scores, those scores are derived automatically from the identified error spans and their classifications, based on a weighting of each error severity and category. Table 9 summarizes this weighting scheme, in which segment-level scores can range from 0 (perfect) to 25 (worst). The final segment-level score is an average over scores from all annotators. In some settings (e.g. calculating correlation for learned metrics), the scores are negated. We use the same weighting to obtain scores from errors identified by AUTOMQM. B Sampling in-context learning examples for AutoMQM Figure 11 shows the rejection criteria used when sampling example sets as discussed in \u00a74. C Additional Results Figures 12, 13, and 14 present additional experimental results. Error Category Accuracy Fluency Terminology Style Locale convention Other Source error Non-translation Description Addition Omission Mistranslation Untranslated text Translation includes information not present in the source. Translation is missing content from the source. Translation does not accurately represent the source. Source text has been left untranslated. Punctuation Spelling Grammar Register Inconsistency Character encoding Incorrect punctuation (for locale or style). Incorrect spelling or capitalization. Problems with grammar, other than orthography. Wrong grammatical register (eg, inappropriately informal pronouns). Internal inconsistency (not related to terminology). Characters are garbled due to incorrect encoding. Inappropriate for context Terminology is non-standard or does not fit context. Inconsistent use Terminology is used inconsistently. Awkward Translation has stylistic problems. Address format Currency format Date format Name format Telephone format Time format Wrong format for addresses. Wrong format for currency. Wrong format for dates. Wrong format for names. Wrong format for telephone numbers. Wrong format for time expressions. Any other issues. An error in the source. Impossible to reliably characterize distinct errors. Table 8: MQM hierarchy. Severity Category Weight Major Non-translation all others 25 5 Minor Fluency/Punctuation all others 0.1 1 Neutral all 0 Table 9: MQM error weighting. 1 def check_icl_set( 2 examples: pd.DataFrame, min_errors=3, majmin_threshold=2, cat_diversity=2, min_clen=20, max_clen=400, 3 4 5 6 7 8 ): 9 # Check if they have the same number of spans as severity/category if not examples.apply( 10 11 lambda r: 12 len(r[\u2019span\u2019]) == len(r[\u2019severity\u2019]) and len(r[\u2019span\u2019]) == len(r[\u2019category\u2019]), 13 axis=1 14 ).all(): 15 return False 16 17 18 # Check if there are at least min_errors if examples[\u2019severity\u2019].apply(lambda svs: len(svs)).sum() < min_errors: 19 return False 20 21 22 23 24 # Check that there\u2019s a balance of major and minor errors. major_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019major\u2019 for s in svs])).sum() minor_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019minor\u2019 for s in svs])).sum() if abs(major_count - minor_count) > majmin_threshold: 25 return False 26 27 28 29 30 # Check that at least cat_diversity error types are represented. categories = examples[\u2019category\u2019].apply(lambda cs: [c.split(\"/\")[0] for c in cs]) represented_error_types = set().union(*categories.tolist()) if len(represented_error_types) < cat_diversity: 31 return False 32 33 top_clen = examples.apply( 34 lambda row: max(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019) 35 36 ), axis=1).max() bot_clen = examples.apply( 37 lambda row: min(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019)), 38 axis=1).min() 39 40 if top_clen > max_clen or bot_clen < min_clen: 41 return False 42 43 44 # All checks passed. return True Figure 11: Rejection criteria used when sampling in-context learning examples for AUTOMQM. 0.25 0.25 0.40 0.40 0.30 0.30 0.45Pearson 0.45Pearson 0.20 0.05 0.10 PaLM-2 (Bison) ref-based 0.15 2 0.10 0.35 0.35 0.00 0 1 1 4# of in-context examples 4# of in-context examples 0.15 0.05 0.00 0 2 0.20 PaLM-2 (Bison) ref-free PaLM-2 (Bison) ref-free 3 3 PaLM-2 (Bison) ref-based Figure 12: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the score prediction prompt,"}, {"question": " How is the final segment-level score calculated in the annotation process?", "answer": " The final segment-level score is an average over scores from all annotators.", "ref_chunk": "segment level, where a segment may contain one or more sentences. Each segment is aligned with a corresponding source segment, and both segments are displayed within their respective documents. Annotate segments in natural order, as if you were reading the document. You may return to revise previous segments. Please identify all errors within each translated segment, up to a maximum of five. If there are more than five errors, identify only the five most severe. If it is not possible to reliably identify distinct errors because the translation is too badly garbled or is unrelated to the source, then mark a single Non-translation error that spans the entire segment. To identify an error, highlight the relevant span of text, and select a category/sub-category and severity level from the available options. (The span of text may be in the source segment if the error is a source error or an omission.) When identifying errors, please be as fine-grained as possible. For example, if a sentence contains two words that are each mistranslated, two separate mistranslation errors should be recorded. If a single stretch of text contains multiple errors, you only need to indicate the one that is most severe. If all have the same severity, choose the first matching category listed in the error typology (eg, Accuracy, then Fluency, then Terminology, etc). Please pay particular attention to document context when annotating. If a translation might be questionable on its own but is fine in the context of the document, it should not be considered erroneous; conversely, if a translation might be acceptable in some context, but not within the current document, it should be marked as wrong. There are two special error categories: Source error and Non-translation. Source errors should be annotated separately, highlighting the relevant span in the source segment. They do not count against the five-error limit for target errors, which should be handled in the usual way, whether or not they resulted from a source error. There can be at most one Non-translation error per segment, and it should span the entire segment. No other errors should be identified if Non-Translation is selected. Table 7: MQM annotator guidelines Annotators are asked to assign both an error severity and category. Error severity (either major or minor) is assigned independently of category. Spans with no marked errors have neutral severity and no category. Possible error categories are displayed in Table 8. Since MQM doesn\u2019t ask annotators for quality scores, those scores are derived automatically from the identified error spans and their classifications, based on a weighting of each error severity and category. Table 9 summarizes this weighting scheme, in which segment-level scores can range from 0 (perfect) to 25 (worst). The final segment-level score is an average over scores from all annotators. In some settings (e.g. calculating correlation for learned metrics), the scores are negated. We use the same weighting to obtain scores from errors identified by AUTOMQM. B Sampling in-context learning examples for AutoMQM Figure 11 shows the rejection criteria used when sampling example sets as discussed in \u00a74. C Additional Results Figures 12, 13, and 14 present additional experimental results. Error Category Accuracy Fluency Terminology Style Locale convention Other Source error Non-translation Description Addition Omission Mistranslation Untranslated text Translation includes information not present in the source. Translation is missing content from the source. Translation does not accurately represent the source. Source text has been left untranslated. Punctuation Spelling Grammar Register Inconsistency Character encoding Incorrect punctuation (for locale or style). Incorrect spelling or capitalization. Problems with grammar, other than orthography. Wrong grammatical register (eg, inappropriately informal pronouns). Internal inconsistency (not related to terminology). Characters are garbled due to incorrect encoding. Inappropriate for context Terminology is non-standard or does not fit context. Inconsistent use Terminology is used inconsistently. Awkward Translation has stylistic problems. Address format Currency format Date format Name format Telephone format Time format Wrong format for addresses. Wrong format for currency. Wrong format for dates. Wrong format for names. Wrong format for telephone numbers. Wrong format for time expressions. Any other issues. An error in the source. Impossible to reliably characterize distinct errors. Table 8: MQM hierarchy. Severity Category Weight Major Non-translation all others 25 5 Minor Fluency/Punctuation all others 0.1 1 Neutral all 0 Table 9: MQM error weighting. 1 def check_icl_set( 2 examples: pd.DataFrame, min_errors=3, majmin_threshold=2, cat_diversity=2, min_clen=20, max_clen=400, 3 4 5 6 7 8 ): 9 # Check if they have the same number of spans as severity/category if not examples.apply( 10 11 lambda r: 12 len(r[\u2019span\u2019]) == len(r[\u2019severity\u2019]) and len(r[\u2019span\u2019]) == len(r[\u2019category\u2019]), 13 axis=1 14 ).all(): 15 return False 16 17 18 # Check if there are at least min_errors if examples[\u2019severity\u2019].apply(lambda svs: len(svs)).sum() < min_errors: 19 return False 20 21 22 23 24 # Check that there\u2019s a balance of major and minor errors. major_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019major\u2019 for s in svs])).sum() minor_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019minor\u2019 for s in svs])).sum() if abs(major_count - minor_count) > majmin_threshold: 25 return False 26 27 28 29 30 # Check that at least cat_diversity error types are represented. categories = examples[\u2019category\u2019].apply(lambda cs: [c.split(\"/\")[0] for c in cs]) represented_error_types = set().union(*categories.tolist()) if len(represented_error_types) < cat_diversity: 31 return False 32 33 top_clen = examples.apply( 34 lambda row: max(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019) 35 36 ), axis=1).max() bot_clen = examples.apply( 37 lambda row: min(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019)), 38 axis=1).min() 39 40 if top_clen > max_clen or bot_clen < min_clen: 41 return False 42 43 44 # All checks passed. return True Figure 11: Rejection criteria used when sampling in-context learning examples for AUTOMQM. 0.25 0.25 0.40 0.40 0.30 0.30 0.45Pearson 0.45Pearson 0.20 0.05 0.10 PaLM-2 (Bison) ref-based 0.15 2 0.10 0.35 0.35 0.00 0 1 1 4# of in-context examples 4# of in-context examples 0.15 0.05 0.00 0 2 0.20 PaLM-2 (Bison) ref-free PaLM-2 (Bison) ref-free 3 3 PaLM-2 (Bison) ref-based Figure 12: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the score prediction prompt,"}], "doc_text": "segment level, where a segment may contain one or more sentences. Each segment is aligned with a corresponding source segment, and both segments are displayed within their respective documents. Annotate segments in natural order, as if you were reading the document. You may return to revise previous segments. Please identify all errors within each translated segment, up to a maximum of five. If there are more than five errors, identify only the five most severe. If it is not possible to reliably identify distinct errors because the translation is too badly garbled or is unrelated to the source, then mark a single Non-translation error that spans the entire segment. To identify an error, highlight the relevant span of text, and select a category/sub-category and severity level from the available options. (The span of text may be in the source segment if the error is a source error or an omission.) When identifying errors, please be as fine-grained as possible. For example, if a sentence contains two words that are each mistranslated, two separate mistranslation errors should be recorded. If a single stretch of text contains multiple errors, you only need to indicate the one that is most severe. If all have the same severity, choose the first matching category listed in the error typology (eg, Accuracy, then Fluency, then Terminology, etc). Please pay particular attention to document context when annotating. If a translation might be questionable on its own but is fine in the context of the document, it should not be considered erroneous; conversely, if a translation might be acceptable in some context, but not within the current document, it should be marked as wrong. There are two special error categories: Source error and Non-translation. Source errors should be annotated separately, highlighting the relevant span in the source segment. They do not count against the five-error limit for target errors, which should be handled in the usual way, whether or not they resulted from a source error. There can be at most one Non-translation error per segment, and it should span the entire segment. No other errors should be identified if Non-Translation is selected. Table 7: MQM annotator guidelines Annotators are asked to assign both an error severity and category. Error severity (either major or minor) is assigned independently of category. Spans with no marked errors have neutral severity and no category. Possible error categories are displayed in Table 8. Since MQM doesn\u2019t ask annotators for quality scores, those scores are derived automatically from the identified error spans and their classifications, based on a weighting of each error severity and category. Table 9 summarizes this weighting scheme, in which segment-level scores can range from 0 (perfect) to 25 (worst). The final segment-level score is an average over scores from all annotators. In some settings (e.g. calculating correlation for learned metrics), the scores are negated. We use the same weighting to obtain scores from errors identified by AUTOMQM. B Sampling in-context learning examples for AutoMQM Figure 11 shows the rejection criteria used when sampling example sets as discussed in \u00a74. C Additional Results Figures 12, 13, and 14 present additional experimental results. Error Category Accuracy Fluency Terminology Style Locale convention Other Source error Non-translation Description Addition Omission Mistranslation Untranslated text Translation includes information not present in the source. Translation is missing content from the source. Translation does not accurately represent the source. Source text has been left untranslated. Punctuation Spelling Grammar Register Inconsistency Character encoding Incorrect punctuation (for locale or style). Incorrect spelling or capitalization. Problems with grammar, other than orthography. Wrong grammatical register (eg, inappropriately informal pronouns). Internal inconsistency (not related to terminology). Characters are garbled due to incorrect encoding. Inappropriate for context Terminology is non-standard or does not fit context. Inconsistent use Terminology is used inconsistently. Awkward Translation has stylistic problems. Address format Currency format Date format Name format Telephone format Time format Wrong format for addresses. Wrong format for currency. Wrong format for dates. Wrong format for names. Wrong format for telephone numbers. Wrong format for time expressions. Any other issues. An error in the source. Impossible to reliably characterize distinct errors. Table 8: MQM hierarchy. Severity Category Weight Major Non-translation all others 25 5 Minor Fluency/Punctuation all others 0.1 1 Neutral all 0 Table 9: MQM error weighting. 1 def check_icl_set( 2 examples: pd.DataFrame, min_errors=3, majmin_threshold=2, cat_diversity=2, min_clen=20, max_clen=400, 3 4 5 6 7 8 ): 9 # Check if they have the same number of spans as severity/category if not examples.apply( 10 11 lambda r: 12 len(r[\u2019span\u2019]) == len(r[\u2019severity\u2019]) and len(r[\u2019span\u2019]) == len(r[\u2019category\u2019]), 13 axis=1 14 ).all(): 15 return False 16 17 18 # Check if there are at least min_errors if examples[\u2019severity\u2019].apply(lambda svs: len(svs)).sum() < min_errors: 19 return False 20 21 22 23 24 # Check that there\u2019s a balance of major and minor errors. major_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019major\u2019 for s in svs])).sum() minor_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019minor\u2019 for s in svs])).sum() if abs(major_count - minor_count) > majmin_threshold: 25 return False 26 27 28 29 30 # Check that at least cat_diversity error types are represented. categories = examples[\u2019category\u2019].apply(lambda cs: [c.split(\"/\")[0] for c in cs]) represented_error_types = set().union(*categories.tolist()) if len(represented_error_types) < cat_diversity: 31 return False 32 33 top_clen = examples.apply( 34 lambda row: max(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019) 35 36 ), axis=1).max() bot_clen = examples.apply( 37 lambda row: min(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019)), 38 axis=1).min() 39 40 if top_clen > max_clen or bot_clen < min_clen: 41 return False 42 43 44 # All checks passed. return True Figure 11: Rejection criteria used when sampling in-context learning examples for AUTOMQM. 0.25 0.25 0.40 0.40 0.30 0.30 0.45Pearson 0.45Pearson 0.20 0.05 0.10 PaLM-2 (Bison) ref-based 0.15 2 0.10 0.35 0.35 0.00 0 1 1 4# of in-context examples 4# of in-context examples 0.15 0.05 0.00 0 2 0.20 PaLM-2 (Bison) ref-free PaLM-2 (Bison) ref-free 3 3 PaLM-2 (Bison) ref-based Figure 12: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the score prediction prompt,"}