{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yiming_Yang_Accelerating_Diffusion-based_Combinatorial_Optimization_Solvers_by_Progressive_Distillation_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the paper?", "answer": " The main focus of the paper is on accelerating diffusion-based combinatorial optimization solvers by progressive distillation.", "ref_chunk": "3 2 0 2 g u A 2 2 ] G L . s c [ 2 v 4 4 6 6 0 . 8 0 3 2 : v i X r a Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation Junwei Huang Carnegie Mellon University junweih@andrew.cmu.edu Zhiqing Sun Carnegie Mellon University zhiqings@cs.cmu.edu Yiming Yang Carnegie Mellon University yiming@cs.cmu.edu Abstract Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset. 1 Introduction The realm of Combinatorial Optimization (CO) is intricately tied to optimization within discrete spaces. A significant portion of CO problems falls into the category of NP-complete (NPC) prob- lems, for which efficient algorithms for exact solutions are nearly impossible to find. In the past, researchers have often turned to traditional approximation algorithms as a means to tackle these NPC problems (Arora, 1998; Lawler et al., 1980). Unfortunately, these traditional methods demand an extensive degree of mathematical understanding specific to each individual problem, resulting in heuristics that lack transferability across diverse problem types. Modern approaches utilize the pattern recognition power of machine learning techniques to generate high-quality solutions. Representative methods include the deep reinforcement learning (DRL) models (Khalil et al., 2017; Barrett et al., 2019; Yao et al., 2021; Bello et al., 2016) and neural diffusion models (Graikos et al., 2022; Sun and Yang, 2023). DRL approaches focus on training an agent to learn the heuristics for solving a CO problem, by allowing the agent to interact with its environment, aided by a graph representation of the problem. In the unsupervised setting for a DRL solver, the system learns the search strategy without requiring annotated optimal solutions for training-set graphs (Karalias and Loukas, 2020; Wang et al., 2022), but DRL CO solvers are usually hard to train, due to the sparse reward problem (Wu et al., 2021). In the supervised settings, on the other hand, neural network solvers learn a generative process to directly predict the high-quality solutions to CO problems (Joshi et al., 2019; Sun and Yang, 2023). They are usually more stable in training and efficient in inference, but require near-optimal annotations generated by traditional solvers. Recent progress in diffusion models for image generation (Ho et al., 2020; Song et al., 2020; Song and Ermon, 2020) has sparked interest in applying diffusion models to CO problem with the hope of exploiting its expressiveness. Graikos et al. (2022) show by encoding the traveling salesman problem (TSP) as 64 \u00d7 64 greyscale images, image diffusion models (Ho et al., 2020) are able to generate heatmaps from which the solutions could be extracted. Sun and Yang (2023) proposed the DIFUSCO framework, which works by explicitly modeling the graph structure of CO problems and denoising ICML 2023 Workshop: Sampling and Optimization in Discrete Space (SODS 2023). the variable indicator vector. However, these diffusion-based methods are inefficient in inference because of the non-trivial number of inference steps required to produce satisfactory results. To remedy the efficiency problem encountered by these diffusion-based solvers, we propose to use progressive distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022; Meng et al., 2023) to address the inference efficiency issue of DIFUSCO, accelerating its inference while preserving its solution quality. We train the student to compress two denoising steps from the teacher into one in a single training iteration and we iteratively perform this compression to achieve further distilled student. We report a 0.019% performance degradation on our 4x distilled student, which inferences 16 times faster than the teacher. 2 Method 2.1 Problem Definition In this paper, our primary focus is on the Travelling Salesman Problem (TSP) as a subject of investigation. However, we posit that both the algorithm we have developed and our conclusions have broader applicability, potentially extending to other types of Combinatorial Optimization (CO) problems. This perspective aligns with the findings presented in DIFUSCO (Sun and Yang, 2023). Formally, we define the possible solution space of TSP instance s to be Xs = {0, 1}N , where N is the number of edges in the problem instance. For each possible solution vector x \u2208 Xs, xi is the indicator variable for whether edge i is chosen in the solution vector x. We define the objective function Js : Xs \u2192 R such that it incorporates both the cost and the validity of the solution: Js(x) = costs(x) + valids(x) where costs(x) = xT d, di being the ith edge\u2019s weight in s, and valids(x) is 0 for a feasible TSP solution, +\u221e for infeasible solutions. The optimal solution to s is x\u2217 = arg min Js(x) x\u2208Xs 2.2 Diffusion Solvers for Combinatorial Optimization DIFUSCO (Sun and Yang, 2023) is a framework for directly modeling the graph representation of CO problems and solving CO problems via diffusion models. By first re-scaling (Chen and Tian, 2019) {0, 1} valued variables to {\u22121, 1} and treating them as real-valued variables, it becomes viable to apply continuous diffusion (Ho et al., 2020; Song et al., 2020) to the problem setup in 2.1. Re-scaling for binary valued variable at time step t \u2208 {1, . . . T } is given as: \u02c6xt = 2xt \u2212 1 After the re-scaling, we then use the vanilla forward process proposed by Ho et al. (2020) q\u03b8(\u02c6xt|\u02c6xt\u22121) = N (\u02c6xt; (cid:112)1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI) where \u03b2t is the ratio of corruption at timestep t. The t step marginal distribution of \u02c6xt is therefore: q\u03b8(\u02c6xt|\u02c6x0) = N (\u02c6xt; \u221a \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) We train a neural network \u03f5\u03b8(\u00b7, \u00b7) to predict the Gaussian noise \u03f5"}, {"question": " What is the proposed method to speed up inference in diffusion-based models?", "answer": " The proposed method is to use progressive distillation to take fewer steps during the denoising process.", "ref_chunk": "3 2 0 2 g u A 2 2 ] G L . s c [ 2 v 4 4 6 6 0 . 8 0 3 2 : v i X r a Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation Junwei Huang Carnegie Mellon University junweih@andrew.cmu.edu Zhiqing Sun Carnegie Mellon University zhiqings@cs.cmu.edu Yiming Yang Carnegie Mellon University yiming@cs.cmu.edu Abstract Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset. 1 Introduction The realm of Combinatorial Optimization (CO) is intricately tied to optimization within discrete spaces. A significant portion of CO problems falls into the category of NP-complete (NPC) prob- lems, for which efficient algorithms for exact solutions are nearly impossible to find. In the past, researchers have often turned to traditional approximation algorithms as a means to tackle these NPC problems (Arora, 1998; Lawler et al., 1980). Unfortunately, these traditional methods demand an extensive degree of mathematical understanding specific to each individual problem, resulting in heuristics that lack transferability across diverse problem types. Modern approaches utilize the pattern recognition power of machine learning techniques to generate high-quality solutions. Representative methods include the deep reinforcement learning (DRL) models (Khalil et al., 2017; Barrett et al., 2019; Yao et al., 2021; Bello et al., 2016) and neural diffusion models (Graikos et al., 2022; Sun and Yang, 2023). DRL approaches focus on training an agent to learn the heuristics for solving a CO problem, by allowing the agent to interact with its environment, aided by a graph representation of the problem. In the unsupervised setting for a DRL solver, the system learns the search strategy without requiring annotated optimal solutions for training-set graphs (Karalias and Loukas, 2020; Wang et al., 2022), but DRL CO solvers are usually hard to train, due to the sparse reward problem (Wu et al., 2021). In the supervised settings, on the other hand, neural network solvers learn a generative process to directly predict the high-quality solutions to CO problems (Joshi et al., 2019; Sun and Yang, 2023). They are usually more stable in training and efficient in inference, but require near-optimal annotations generated by traditional solvers. Recent progress in diffusion models for image generation (Ho et al., 2020; Song et al., 2020; Song and Ermon, 2020) has sparked interest in applying diffusion models to CO problem with the hope of exploiting its expressiveness. Graikos et al. (2022) show by encoding the traveling salesman problem (TSP) as 64 \u00d7 64 greyscale images, image diffusion models (Ho et al., 2020) are able to generate heatmaps from which the solutions could be extracted. Sun and Yang (2023) proposed the DIFUSCO framework, which works by explicitly modeling the graph structure of CO problems and denoising ICML 2023 Workshop: Sampling and Optimization in Discrete Space (SODS 2023). the variable indicator vector. However, these diffusion-based methods are inefficient in inference because of the non-trivial number of inference steps required to produce satisfactory results. To remedy the efficiency problem encountered by these diffusion-based solvers, we propose to use progressive distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022; Meng et al., 2023) to address the inference efficiency issue of DIFUSCO, accelerating its inference while preserving its solution quality. We train the student to compress two denoising steps from the teacher into one in a single training iteration and we iteratively perform this compression to achieve further distilled student. We report a 0.019% performance degradation on our 4x distilled student, which inferences 16 times faster than the teacher. 2 Method 2.1 Problem Definition In this paper, our primary focus is on the Travelling Salesman Problem (TSP) as a subject of investigation. However, we posit that both the algorithm we have developed and our conclusions have broader applicability, potentially extending to other types of Combinatorial Optimization (CO) problems. This perspective aligns with the findings presented in DIFUSCO (Sun and Yang, 2023). Formally, we define the possible solution space of TSP instance s to be Xs = {0, 1}N , where N is the number of edges in the problem instance. For each possible solution vector x \u2208 Xs, xi is the indicator variable for whether edge i is chosen in the solution vector x. We define the objective function Js : Xs \u2192 R such that it incorporates both the cost and the validity of the solution: Js(x) = costs(x) + valids(x) where costs(x) = xT d, di being the ith edge\u2019s weight in s, and valids(x) is 0 for a feasible TSP solution, +\u221e for infeasible solutions. The optimal solution to s is x\u2217 = arg min Js(x) x\u2208Xs 2.2 Diffusion Solvers for Combinatorial Optimization DIFUSCO (Sun and Yang, 2023) is a framework for directly modeling the graph representation of CO problems and solving CO problems via diffusion models. By first re-scaling (Chen and Tian, 2019) {0, 1} valued variables to {\u22121, 1} and treating them as real-valued variables, it becomes viable to apply continuous diffusion (Ho et al., 2020; Song et al., 2020) to the problem setup in 2.1. Re-scaling for binary valued variable at time step t \u2208 {1, . . . T } is given as: \u02c6xt = 2xt \u2212 1 After the re-scaling, we then use the vanilla forward process proposed by Ho et al. (2020) q\u03b8(\u02c6xt|\u02c6xt\u22121) = N (\u02c6xt; (cid:112)1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI) where \u03b2t is the ratio of corruption at timestep t. The t step marginal distribution of \u02c6xt is therefore: q\u03b8(\u02c6xt|\u02c6x0) = N (\u02c6xt; \u221a \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) We train a neural network \u03f5\u03b8(\u00b7, \u00b7) to predict the Gaussian noise \u03f5"}, {"question": " What is the impact of the progressively distilled model on inference speed and performance?", "answer": " The progressively distilled model can perform inference 16 times faster with only a 0.019% degradation in performance on the TSP-50 dataset.", "ref_chunk": "3 2 0 2 g u A 2 2 ] G L . s c [ 2 v 4 4 6 6 0 . 8 0 3 2 : v i X r a Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation Junwei Huang Carnegie Mellon University junweih@andrew.cmu.edu Zhiqing Sun Carnegie Mellon University zhiqings@cs.cmu.edu Yiming Yang Carnegie Mellon University yiming@cs.cmu.edu Abstract Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset. 1 Introduction The realm of Combinatorial Optimization (CO) is intricately tied to optimization within discrete spaces. A significant portion of CO problems falls into the category of NP-complete (NPC) prob- lems, for which efficient algorithms for exact solutions are nearly impossible to find. In the past, researchers have often turned to traditional approximation algorithms as a means to tackle these NPC problems (Arora, 1998; Lawler et al., 1980). Unfortunately, these traditional methods demand an extensive degree of mathematical understanding specific to each individual problem, resulting in heuristics that lack transferability across diverse problem types. Modern approaches utilize the pattern recognition power of machine learning techniques to generate high-quality solutions. Representative methods include the deep reinforcement learning (DRL) models (Khalil et al., 2017; Barrett et al., 2019; Yao et al., 2021; Bello et al., 2016) and neural diffusion models (Graikos et al., 2022; Sun and Yang, 2023). DRL approaches focus on training an agent to learn the heuristics for solving a CO problem, by allowing the agent to interact with its environment, aided by a graph representation of the problem. In the unsupervised setting for a DRL solver, the system learns the search strategy without requiring annotated optimal solutions for training-set graphs (Karalias and Loukas, 2020; Wang et al., 2022), but DRL CO solvers are usually hard to train, due to the sparse reward problem (Wu et al., 2021). In the supervised settings, on the other hand, neural network solvers learn a generative process to directly predict the high-quality solutions to CO problems (Joshi et al., 2019; Sun and Yang, 2023). They are usually more stable in training and efficient in inference, but require near-optimal annotations generated by traditional solvers. Recent progress in diffusion models for image generation (Ho et al., 2020; Song et al., 2020; Song and Ermon, 2020) has sparked interest in applying diffusion models to CO problem with the hope of exploiting its expressiveness. Graikos et al. (2022) show by encoding the traveling salesman problem (TSP) as 64 \u00d7 64 greyscale images, image diffusion models (Ho et al., 2020) are able to generate heatmaps from which the solutions could be extracted. Sun and Yang (2023) proposed the DIFUSCO framework, which works by explicitly modeling the graph structure of CO problems and denoising ICML 2023 Workshop: Sampling and Optimization in Discrete Space (SODS 2023). the variable indicator vector. However, these diffusion-based methods are inefficient in inference because of the non-trivial number of inference steps required to produce satisfactory results. To remedy the efficiency problem encountered by these diffusion-based solvers, we propose to use progressive distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022; Meng et al., 2023) to address the inference efficiency issue of DIFUSCO, accelerating its inference while preserving its solution quality. We train the student to compress two denoising steps from the teacher into one in a single training iteration and we iteratively perform this compression to achieve further distilled student. We report a 0.019% performance degradation on our 4x distilled student, which inferences 16 times faster than the teacher. 2 Method 2.1 Problem Definition In this paper, our primary focus is on the Travelling Salesman Problem (TSP) as a subject of investigation. However, we posit that both the algorithm we have developed and our conclusions have broader applicability, potentially extending to other types of Combinatorial Optimization (CO) problems. This perspective aligns with the findings presented in DIFUSCO (Sun and Yang, 2023). Formally, we define the possible solution space of TSP instance s to be Xs = {0, 1}N , where N is the number of edges in the problem instance. For each possible solution vector x \u2208 Xs, xi is the indicator variable for whether edge i is chosen in the solution vector x. We define the objective function Js : Xs \u2192 R such that it incorporates both the cost and the validity of the solution: Js(x) = costs(x) + valids(x) where costs(x) = xT d, di being the ith edge\u2019s weight in s, and valids(x) is 0 for a feasible TSP solution, +\u221e for infeasible solutions. The optimal solution to s is x\u2217 = arg min Js(x) x\u2208Xs 2.2 Diffusion Solvers for Combinatorial Optimization DIFUSCO (Sun and Yang, 2023) is a framework for directly modeling the graph representation of CO problems and solving CO problems via diffusion models. By first re-scaling (Chen and Tian, 2019) {0, 1} valued variables to {\u22121, 1} and treating them as real-valued variables, it becomes viable to apply continuous diffusion (Ho et al., 2020; Song et al., 2020) to the problem setup in 2.1. Re-scaling for binary valued variable at time step t \u2208 {1, . . . T } is given as: \u02c6xt = 2xt \u2212 1 After the re-scaling, we then use the vanilla forward process proposed by Ho et al. (2020) q\u03b8(\u02c6xt|\u02c6xt\u22121) = N (\u02c6xt; (cid:112)1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI) where \u03b2t is the ratio of corruption at timestep t. The t step marginal distribution of \u02c6xt is therefore: q\u03b8(\u02c6xt|\u02c6x0) = N (\u02c6xt; \u221a \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) We train a neural network \u03f5\u03b8(\u00b7, \u00b7) to predict the Gaussian noise \u03f5"}, {"question": " What types of problems fall into the category of NP-complete problems?", "answer": " A significant portion of combinatorial optimization problems fall into the category of NP-complete problems.", "ref_chunk": "3 2 0 2 g u A 2 2 ] G L . s c [ 2 v 4 4 6 6 0 . 8 0 3 2 : v i X r a Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation Junwei Huang Carnegie Mellon University junweih@andrew.cmu.edu Zhiqing Sun Carnegie Mellon University zhiqings@cs.cmu.edu Yiming Yang Carnegie Mellon University yiming@cs.cmu.edu Abstract Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset. 1 Introduction The realm of Combinatorial Optimization (CO) is intricately tied to optimization within discrete spaces. A significant portion of CO problems falls into the category of NP-complete (NPC) prob- lems, for which efficient algorithms for exact solutions are nearly impossible to find. In the past, researchers have often turned to traditional approximation algorithms as a means to tackle these NPC problems (Arora, 1998; Lawler et al., 1980). Unfortunately, these traditional methods demand an extensive degree of mathematical understanding specific to each individual problem, resulting in heuristics that lack transferability across diverse problem types. Modern approaches utilize the pattern recognition power of machine learning techniques to generate high-quality solutions. Representative methods include the deep reinforcement learning (DRL) models (Khalil et al., 2017; Barrett et al., 2019; Yao et al., 2021; Bello et al., 2016) and neural diffusion models (Graikos et al., 2022; Sun and Yang, 2023). DRL approaches focus on training an agent to learn the heuristics for solving a CO problem, by allowing the agent to interact with its environment, aided by a graph representation of the problem. In the unsupervised setting for a DRL solver, the system learns the search strategy without requiring annotated optimal solutions for training-set graphs (Karalias and Loukas, 2020; Wang et al., 2022), but DRL CO solvers are usually hard to train, due to the sparse reward problem (Wu et al., 2021). In the supervised settings, on the other hand, neural network solvers learn a generative process to directly predict the high-quality solutions to CO problems (Joshi et al., 2019; Sun and Yang, 2023). They are usually more stable in training and efficient in inference, but require near-optimal annotations generated by traditional solvers. Recent progress in diffusion models for image generation (Ho et al., 2020; Song et al., 2020; Song and Ermon, 2020) has sparked interest in applying diffusion models to CO problem with the hope of exploiting its expressiveness. Graikos et al. (2022) show by encoding the traveling salesman problem (TSP) as 64 \u00d7 64 greyscale images, image diffusion models (Ho et al., 2020) are able to generate heatmaps from which the solutions could be extracted. Sun and Yang (2023) proposed the DIFUSCO framework, which works by explicitly modeling the graph structure of CO problems and denoising ICML 2023 Workshop: Sampling and Optimization in Discrete Space (SODS 2023). the variable indicator vector. However, these diffusion-based methods are inefficient in inference because of the non-trivial number of inference steps required to produce satisfactory results. To remedy the efficiency problem encountered by these diffusion-based solvers, we propose to use progressive distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022; Meng et al., 2023) to address the inference efficiency issue of DIFUSCO, accelerating its inference while preserving its solution quality. We train the student to compress two denoising steps from the teacher into one in a single training iteration and we iteratively perform this compression to achieve further distilled student. We report a 0.019% performance degradation on our 4x distilled student, which inferences 16 times faster than the teacher. 2 Method 2.1 Problem Definition In this paper, our primary focus is on the Travelling Salesman Problem (TSP) as a subject of investigation. However, we posit that both the algorithm we have developed and our conclusions have broader applicability, potentially extending to other types of Combinatorial Optimization (CO) problems. This perspective aligns with the findings presented in DIFUSCO (Sun and Yang, 2023). Formally, we define the possible solution space of TSP instance s to be Xs = {0, 1}N , where N is the number of edges in the problem instance. For each possible solution vector x \u2208 Xs, xi is the indicator variable for whether edge i is chosen in the solution vector x. We define the objective function Js : Xs \u2192 R such that it incorporates both the cost and the validity of the solution: Js(x) = costs(x) + valids(x) where costs(x) = xT d, di being the ith edge\u2019s weight in s, and valids(x) is 0 for a feasible TSP solution, +\u221e for infeasible solutions. The optimal solution to s is x\u2217 = arg min Js(x) x\u2208Xs 2.2 Diffusion Solvers for Combinatorial Optimization DIFUSCO (Sun and Yang, 2023) is a framework for directly modeling the graph representation of CO problems and solving CO problems via diffusion models. By first re-scaling (Chen and Tian, 2019) {0, 1} valued variables to {\u22121, 1} and treating them as real-valued variables, it becomes viable to apply continuous diffusion (Ho et al., 2020; Song et al., 2020) to the problem setup in 2.1. Re-scaling for binary valued variable at time step t \u2208 {1, . . . T } is given as: \u02c6xt = 2xt \u2212 1 After the re-scaling, we then use the vanilla forward process proposed by Ho et al. (2020) q\u03b8(\u02c6xt|\u02c6xt\u22121) = N (\u02c6xt; (cid:112)1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI) where \u03b2t is the ratio of corruption at timestep t. The t step marginal distribution of \u02c6xt is therefore: q\u03b8(\u02c6xt|\u02c6x0) = N (\u02c6xt; \u221a \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) We train a neural network \u03f5\u03b8(\u00b7, \u00b7) to predict the Gaussian noise \u03f5"}, {"question": " How do traditional approximation algorithms typically approach NPC problems?", "answer": " Traditional approximation algorithms often demand an extensive degree of mathematical understanding specific to each individual problem.", "ref_chunk": "3 2 0 2 g u A 2 2 ] G L . s c [ 2 v 4 4 6 6 0 . 8 0 3 2 : v i X r a Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation Junwei Huang Carnegie Mellon University junweih@andrew.cmu.edu Zhiqing Sun Carnegie Mellon University zhiqings@cs.cmu.edu Yiming Yang Carnegie Mellon University yiming@cs.cmu.edu Abstract Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset. 1 Introduction The realm of Combinatorial Optimization (CO) is intricately tied to optimization within discrete spaces. A significant portion of CO problems falls into the category of NP-complete (NPC) prob- lems, for which efficient algorithms for exact solutions are nearly impossible to find. In the past, researchers have often turned to traditional approximation algorithms as a means to tackle these NPC problems (Arora, 1998; Lawler et al., 1980). Unfortunately, these traditional methods demand an extensive degree of mathematical understanding specific to each individual problem, resulting in heuristics that lack transferability across diverse problem types. Modern approaches utilize the pattern recognition power of machine learning techniques to generate high-quality solutions. Representative methods include the deep reinforcement learning (DRL) models (Khalil et al., 2017; Barrett et al., 2019; Yao et al., 2021; Bello et al., 2016) and neural diffusion models (Graikos et al., 2022; Sun and Yang, 2023). DRL approaches focus on training an agent to learn the heuristics for solving a CO problem, by allowing the agent to interact with its environment, aided by a graph representation of the problem. In the unsupervised setting for a DRL solver, the system learns the search strategy without requiring annotated optimal solutions for training-set graphs (Karalias and Loukas, 2020; Wang et al., 2022), but DRL CO solvers are usually hard to train, due to the sparse reward problem (Wu et al., 2021). In the supervised settings, on the other hand, neural network solvers learn a generative process to directly predict the high-quality solutions to CO problems (Joshi et al., 2019; Sun and Yang, 2023). They are usually more stable in training and efficient in inference, but require near-optimal annotations generated by traditional solvers. Recent progress in diffusion models for image generation (Ho et al., 2020; Song et al., 2020; Song and Ermon, 2020) has sparked interest in applying diffusion models to CO problem with the hope of exploiting its expressiveness. Graikos et al. (2022) show by encoding the traveling salesman problem (TSP) as 64 \u00d7 64 greyscale images, image diffusion models (Ho et al., 2020) are able to generate heatmaps from which the solutions could be extracted. Sun and Yang (2023) proposed the DIFUSCO framework, which works by explicitly modeling the graph structure of CO problems and denoising ICML 2023 Workshop: Sampling and Optimization in Discrete Space (SODS 2023). the variable indicator vector. However, these diffusion-based methods are inefficient in inference because of the non-trivial number of inference steps required to produce satisfactory results. To remedy the efficiency problem encountered by these diffusion-based solvers, we propose to use progressive distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022; Meng et al., 2023) to address the inference efficiency issue of DIFUSCO, accelerating its inference while preserving its solution quality. We train the student to compress two denoising steps from the teacher into one in a single training iteration and we iteratively perform this compression to achieve further distilled student. We report a 0.019% performance degradation on our 4x distilled student, which inferences 16 times faster than the teacher. 2 Method 2.1 Problem Definition In this paper, our primary focus is on the Travelling Salesman Problem (TSP) as a subject of investigation. However, we posit that both the algorithm we have developed and our conclusions have broader applicability, potentially extending to other types of Combinatorial Optimization (CO) problems. This perspective aligns with the findings presented in DIFUSCO (Sun and Yang, 2023). Formally, we define the possible solution space of TSP instance s to be Xs = {0, 1}N , where N is the number of edges in the problem instance. For each possible solution vector x \u2208 Xs, xi is the indicator variable for whether edge i is chosen in the solution vector x. We define the objective function Js : Xs \u2192 R such that it incorporates both the cost and the validity of the solution: Js(x) = costs(x) + valids(x) where costs(x) = xT d, di being the ith edge\u2019s weight in s, and valids(x) is 0 for a feasible TSP solution, +\u221e for infeasible solutions. The optimal solution to s is x\u2217 = arg min Js(x) x\u2208Xs 2.2 Diffusion Solvers for Combinatorial Optimization DIFUSCO (Sun and Yang, 2023) is a framework for directly modeling the graph representation of CO problems and solving CO problems via diffusion models. By first re-scaling (Chen and Tian, 2019) {0, 1} valued variables to {\u22121, 1} and treating them as real-valued variables, it becomes viable to apply continuous diffusion (Ho et al., 2020; Song et al., 2020) to the problem setup in 2.1. Re-scaling for binary valued variable at time step t \u2208 {1, . . . T } is given as: \u02c6xt = 2xt \u2212 1 After the re-scaling, we then use the vanilla forward process proposed by Ho et al. (2020) q\u03b8(\u02c6xt|\u02c6xt\u22121) = N (\u02c6xt; (cid:112)1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI) where \u03b2t is the ratio of corruption at timestep t. The t step marginal distribution of \u02c6xt is therefore: q\u03b8(\u02c6xt|\u02c6x0) = N (\u02c6xt; \u221a \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) We train a neural network \u03f5\u03b8(\u00b7, \u00b7) to predict the Gaussian noise \u03f5"}, {"question": " What is the approach of deep reinforcement learning (DRL) models in solving CO problems?", "answer": " DRL models focus on training an agent to learn heuristics for solving CO problems by interacting with its environment.", "ref_chunk": "3 2 0 2 g u A 2 2 ] G L . s c [ 2 v 4 4 6 6 0 . 8 0 3 2 : v i X r a Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation Junwei Huang Carnegie Mellon University junweih@andrew.cmu.edu Zhiqing Sun Carnegie Mellon University zhiqings@cs.cmu.edu Yiming Yang Carnegie Mellon University yiming@cs.cmu.edu Abstract Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset. 1 Introduction The realm of Combinatorial Optimization (CO) is intricately tied to optimization within discrete spaces. A significant portion of CO problems falls into the category of NP-complete (NPC) prob- lems, for which efficient algorithms for exact solutions are nearly impossible to find. In the past, researchers have often turned to traditional approximation algorithms as a means to tackle these NPC problems (Arora, 1998; Lawler et al., 1980). Unfortunately, these traditional methods demand an extensive degree of mathematical understanding specific to each individual problem, resulting in heuristics that lack transferability across diverse problem types. Modern approaches utilize the pattern recognition power of machine learning techniques to generate high-quality solutions. Representative methods include the deep reinforcement learning (DRL) models (Khalil et al., 2017; Barrett et al., 2019; Yao et al., 2021; Bello et al., 2016) and neural diffusion models (Graikos et al., 2022; Sun and Yang, 2023). DRL approaches focus on training an agent to learn the heuristics for solving a CO problem, by allowing the agent to interact with its environment, aided by a graph representation of the problem. In the unsupervised setting for a DRL solver, the system learns the search strategy without requiring annotated optimal solutions for training-set graphs (Karalias and Loukas, 2020; Wang et al., 2022), but DRL CO solvers are usually hard to train, due to the sparse reward problem (Wu et al., 2021). In the supervised settings, on the other hand, neural network solvers learn a generative process to directly predict the high-quality solutions to CO problems (Joshi et al., 2019; Sun and Yang, 2023). They are usually more stable in training and efficient in inference, but require near-optimal annotations generated by traditional solvers. Recent progress in diffusion models for image generation (Ho et al., 2020; Song et al., 2020; Song and Ermon, 2020) has sparked interest in applying diffusion models to CO problem with the hope of exploiting its expressiveness. Graikos et al. (2022) show by encoding the traveling salesman problem (TSP) as 64 \u00d7 64 greyscale images, image diffusion models (Ho et al., 2020) are able to generate heatmaps from which the solutions could be extracted. Sun and Yang (2023) proposed the DIFUSCO framework, which works by explicitly modeling the graph structure of CO problems and denoising ICML 2023 Workshop: Sampling and Optimization in Discrete Space (SODS 2023). the variable indicator vector. However, these diffusion-based methods are inefficient in inference because of the non-trivial number of inference steps required to produce satisfactory results. To remedy the efficiency problem encountered by these diffusion-based solvers, we propose to use progressive distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022; Meng et al., 2023) to address the inference efficiency issue of DIFUSCO, accelerating its inference while preserving its solution quality. We train the student to compress two denoising steps from the teacher into one in a single training iteration and we iteratively perform this compression to achieve further distilled student. We report a 0.019% performance degradation on our 4x distilled student, which inferences 16 times faster than the teacher. 2 Method 2.1 Problem Definition In this paper, our primary focus is on the Travelling Salesman Problem (TSP) as a subject of investigation. However, we posit that both the algorithm we have developed and our conclusions have broader applicability, potentially extending to other types of Combinatorial Optimization (CO) problems. This perspective aligns with the findings presented in DIFUSCO (Sun and Yang, 2023). Formally, we define the possible solution space of TSP instance s to be Xs = {0, 1}N , where N is the number of edges in the problem instance. For each possible solution vector x \u2208 Xs, xi is the indicator variable for whether edge i is chosen in the solution vector x. We define the objective function Js : Xs \u2192 R such that it incorporates both the cost and the validity of the solution: Js(x) = costs(x) + valids(x) where costs(x) = xT d, di being the ith edge\u2019s weight in s, and valids(x) is 0 for a feasible TSP solution, +\u221e for infeasible solutions. The optimal solution to s is x\u2217 = arg min Js(x) x\u2208Xs 2.2 Diffusion Solvers for Combinatorial Optimization DIFUSCO (Sun and Yang, 2023) is a framework for directly modeling the graph representation of CO problems and solving CO problems via diffusion models. By first re-scaling (Chen and Tian, 2019) {0, 1} valued variables to {\u22121, 1} and treating them as real-valued variables, it becomes viable to apply continuous diffusion (Ho et al., 2020; Song et al., 2020) to the problem setup in 2.1. Re-scaling for binary valued variable at time step t \u2208 {1, . . . T } is given as: \u02c6xt = 2xt \u2212 1 After the re-scaling, we then use the vanilla forward process proposed by Ho et al. (2020) q\u03b8(\u02c6xt|\u02c6xt\u22121) = N (\u02c6xt; (cid:112)1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI) where \u03b2t is the ratio of corruption at timestep t. The t step marginal distribution of \u02c6xt is therefore: q\u03b8(\u02c6xt|\u02c6x0) = N (\u02c6xt; \u221a \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) We train a neural network \u03f5\u03b8(\u00b7, \u00b7) to predict the Gaussian noise \u03f5"}, {"question": " Why are diffusion-based methods inefficient in inference?", "answer": " Diffusion-based methods are inefficient in inference due to the non-trivial number of inference steps required to produce satisfactory results.", "ref_chunk": "3 2 0 2 g u A 2 2 ] G L . s c [ 2 v 4 4 6 6 0 . 8 0 3 2 : v i X r a Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation Junwei Huang Carnegie Mellon University junweih@andrew.cmu.edu Zhiqing Sun Carnegie Mellon University zhiqings@cs.cmu.edu Yiming Yang Carnegie Mellon University yiming@cs.cmu.edu Abstract Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset. 1 Introduction The realm of Combinatorial Optimization (CO) is intricately tied to optimization within discrete spaces. A significant portion of CO problems falls into the category of NP-complete (NPC) prob- lems, for which efficient algorithms for exact solutions are nearly impossible to find. In the past, researchers have often turned to traditional approximation algorithms as a means to tackle these NPC problems (Arora, 1998; Lawler et al., 1980). Unfortunately, these traditional methods demand an extensive degree of mathematical understanding specific to each individual problem, resulting in heuristics that lack transferability across diverse problem types. Modern approaches utilize the pattern recognition power of machine learning techniques to generate high-quality solutions. Representative methods include the deep reinforcement learning (DRL) models (Khalil et al., 2017; Barrett et al., 2019; Yao et al., 2021; Bello et al., 2016) and neural diffusion models (Graikos et al., 2022; Sun and Yang, 2023). DRL approaches focus on training an agent to learn the heuristics for solving a CO problem, by allowing the agent to interact with its environment, aided by a graph representation of the problem. In the unsupervised setting for a DRL solver, the system learns the search strategy without requiring annotated optimal solutions for training-set graphs (Karalias and Loukas, 2020; Wang et al., 2022), but DRL CO solvers are usually hard to train, due to the sparse reward problem (Wu et al., 2021). In the supervised settings, on the other hand, neural network solvers learn a generative process to directly predict the high-quality solutions to CO problems (Joshi et al., 2019; Sun and Yang, 2023). They are usually more stable in training and efficient in inference, but require near-optimal annotations generated by traditional solvers. Recent progress in diffusion models for image generation (Ho et al., 2020; Song et al., 2020; Song and Ermon, 2020) has sparked interest in applying diffusion models to CO problem with the hope of exploiting its expressiveness. Graikos et al. (2022) show by encoding the traveling salesman problem (TSP) as 64 \u00d7 64 greyscale images, image diffusion models (Ho et al., 2020) are able to generate heatmaps from which the solutions could be extracted. Sun and Yang (2023) proposed the DIFUSCO framework, which works by explicitly modeling the graph structure of CO problems and denoising ICML 2023 Workshop: Sampling and Optimization in Discrete Space (SODS 2023). the variable indicator vector. However, these diffusion-based methods are inefficient in inference because of the non-trivial number of inference steps required to produce satisfactory results. To remedy the efficiency problem encountered by these diffusion-based solvers, we propose to use progressive distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022; Meng et al., 2023) to address the inference efficiency issue of DIFUSCO, accelerating its inference while preserving its solution quality. We train the student to compress two denoising steps from the teacher into one in a single training iteration and we iteratively perform this compression to achieve further distilled student. We report a 0.019% performance degradation on our 4x distilled student, which inferences 16 times faster than the teacher. 2 Method 2.1 Problem Definition In this paper, our primary focus is on the Travelling Salesman Problem (TSP) as a subject of investigation. However, we posit that both the algorithm we have developed and our conclusions have broader applicability, potentially extending to other types of Combinatorial Optimization (CO) problems. This perspective aligns with the findings presented in DIFUSCO (Sun and Yang, 2023). Formally, we define the possible solution space of TSP instance s to be Xs = {0, 1}N , where N is the number of edges in the problem instance. For each possible solution vector x \u2208 Xs, xi is the indicator variable for whether edge i is chosen in the solution vector x. We define the objective function Js : Xs \u2192 R such that it incorporates both the cost and the validity of the solution: Js(x) = costs(x) + valids(x) where costs(x) = xT d, di being the ith edge\u2019s weight in s, and valids(x) is 0 for a feasible TSP solution, +\u221e for infeasible solutions. The optimal solution to s is x\u2217 = arg min Js(x) x\u2208Xs 2.2 Diffusion Solvers for Combinatorial Optimization DIFUSCO (Sun and Yang, 2023) is a framework for directly modeling the graph representation of CO problems and solving CO problems via diffusion models. By first re-scaling (Chen and Tian, 2019) {0, 1} valued variables to {\u22121, 1} and treating them as real-valued variables, it becomes viable to apply continuous diffusion (Ho et al., 2020; Song et al., 2020) to the problem setup in 2.1. Re-scaling for binary valued variable at time step t \u2208 {1, . . . T } is given as: \u02c6xt = 2xt \u2212 1 After the re-scaling, we then use the vanilla forward process proposed by Ho et al. (2020) q\u03b8(\u02c6xt|\u02c6xt\u22121) = N (\u02c6xt; (cid:112)1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI) where \u03b2t is the ratio of corruption at timestep t. The t step marginal distribution of \u02c6xt is therefore: q\u03b8(\u02c6xt|\u02c6x0) = N (\u02c6xt; \u221a \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) We train a neural network \u03f5\u03b8(\u00b7, \u00b7) to predict the Gaussian noise \u03f5"}, {"question": " What is the objective function Js defined in the paper for the TSP instance?", "answer": " The objective function Js incorporates both the cost and validity of the solution.", "ref_chunk": "3 2 0 2 g u A 2 2 ] G L . s c [ 2 v 4 4 6 6 0 . 8 0 3 2 : v i X r a Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation Junwei Huang Carnegie Mellon University junweih@andrew.cmu.edu Zhiqing Sun Carnegie Mellon University zhiqings@cs.cmu.edu Yiming Yang Carnegie Mellon University yiming@cs.cmu.edu Abstract Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset. 1 Introduction The realm of Combinatorial Optimization (CO) is intricately tied to optimization within discrete spaces. A significant portion of CO problems falls into the category of NP-complete (NPC) prob- lems, for which efficient algorithms for exact solutions are nearly impossible to find. In the past, researchers have often turned to traditional approximation algorithms as a means to tackle these NPC problems (Arora, 1998; Lawler et al., 1980). Unfortunately, these traditional methods demand an extensive degree of mathematical understanding specific to each individual problem, resulting in heuristics that lack transferability across diverse problem types. Modern approaches utilize the pattern recognition power of machine learning techniques to generate high-quality solutions. Representative methods include the deep reinforcement learning (DRL) models (Khalil et al., 2017; Barrett et al., 2019; Yao et al., 2021; Bello et al., 2016) and neural diffusion models (Graikos et al., 2022; Sun and Yang, 2023). DRL approaches focus on training an agent to learn the heuristics for solving a CO problem, by allowing the agent to interact with its environment, aided by a graph representation of the problem. In the unsupervised setting for a DRL solver, the system learns the search strategy without requiring annotated optimal solutions for training-set graphs (Karalias and Loukas, 2020; Wang et al., 2022), but DRL CO solvers are usually hard to train, due to the sparse reward problem (Wu et al., 2021). In the supervised settings, on the other hand, neural network solvers learn a generative process to directly predict the high-quality solutions to CO problems (Joshi et al., 2019; Sun and Yang, 2023). They are usually more stable in training and efficient in inference, but require near-optimal annotations generated by traditional solvers. Recent progress in diffusion models for image generation (Ho et al., 2020; Song et al., 2020; Song and Ermon, 2020) has sparked interest in applying diffusion models to CO problem with the hope of exploiting its expressiveness. Graikos et al. (2022) show by encoding the traveling salesman problem (TSP) as 64 \u00d7 64 greyscale images, image diffusion models (Ho et al., 2020) are able to generate heatmaps from which the solutions could be extracted. Sun and Yang (2023) proposed the DIFUSCO framework, which works by explicitly modeling the graph structure of CO problems and denoising ICML 2023 Workshop: Sampling and Optimization in Discrete Space (SODS 2023). the variable indicator vector. However, these diffusion-based methods are inefficient in inference because of the non-trivial number of inference steps required to produce satisfactory results. To remedy the efficiency problem encountered by these diffusion-based solvers, we propose to use progressive distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022; Meng et al., 2023) to address the inference efficiency issue of DIFUSCO, accelerating its inference while preserving its solution quality. We train the student to compress two denoising steps from the teacher into one in a single training iteration and we iteratively perform this compression to achieve further distilled student. We report a 0.019% performance degradation on our 4x distilled student, which inferences 16 times faster than the teacher. 2 Method 2.1 Problem Definition In this paper, our primary focus is on the Travelling Salesman Problem (TSP) as a subject of investigation. However, we posit that both the algorithm we have developed and our conclusions have broader applicability, potentially extending to other types of Combinatorial Optimization (CO) problems. This perspective aligns with the findings presented in DIFUSCO (Sun and Yang, 2023). Formally, we define the possible solution space of TSP instance s to be Xs = {0, 1}N , where N is the number of edges in the problem instance. For each possible solution vector x \u2208 Xs, xi is the indicator variable for whether edge i is chosen in the solution vector x. We define the objective function Js : Xs \u2192 R such that it incorporates both the cost and the validity of the solution: Js(x) = costs(x) + valids(x) where costs(x) = xT d, di being the ith edge\u2019s weight in s, and valids(x) is 0 for a feasible TSP solution, +\u221e for infeasible solutions. The optimal solution to s is x\u2217 = arg min Js(x) x\u2208Xs 2.2 Diffusion Solvers for Combinatorial Optimization DIFUSCO (Sun and Yang, 2023) is a framework for directly modeling the graph representation of CO problems and solving CO problems via diffusion models. By first re-scaling (Chen and Tian, 2019) {0, 1} valued variables to {\u22121, 1} and treating them as real-valued variables, it becomes viable to apply continuous diffusion (Ho et al., 2020; Song et al., 2020) to the problem setup in 2.1. Re-scaling for binary valued variable at time step t \u2208 {1, . . . T } is given as: \u02c6xt = 2xt \u2212 1 After the re-scaling, we then use the vanilla forward process proposed by Ho et al. (2020) q\u03b8(\u02c6xt|\u02c6xt\u22121) = N (\u02c6xt; (cid:112)1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI) where \u03b2t is the ratio of corruption at timestep t. The t step marginal distribution of \u02c6xt is therefore: q\u03b8(\u02c6xt|\u02c6x0) = N (\u02c6xt; \u221a \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) We train a neural network \u03f5\u03b8(\u00b7, \u00b7) to predict the Gaussian noise \u03f5"}, {"question": " What is the optimal solution x\u2217 to a TSP instance according to the paper?", "answer": " The optimal solution x\u2217 minimizes the objective function Js for the TSP instance.", "ref_chunk": "3 2 0 2 g u A 2 2 ] G L . s c [ 2 v 4 4 6 6 0 . 8 0 3 2 : v i X r a Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation Junwei Huang Carnegie Mellon University junweih@andrew.cmu.edu Zhiqing Sun Carnegie Mellon University zhiqings@cs.cmu.edu Yiming Yang Carnegie Mellon University yiming@cs.cmu.edu Abstract Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset. 1 Introduction The realm of Combinatorial Optimization (CO) is intricately tied to optimization within discrete spaces. A significant portion of CO problems falls into the category of NP-complete (NPC) prob- lems, for which efficient algorithms for exact solutions are nearly impossible to find. In the past, researchers have often turned to traditional approximation algorithms as a means to tackle these NPC problems (Arora, 1998; Lawler et al., 1980). Unfortunately, these traditional methods demand an extensive degree of mathematical understanding specific to each individual problem, resulting in heuristics that lack transferability across diverse problem types. Modern approaches utilize the pattern recognition power of machine learning techniques to generate high-quality solutions. Representative methods include the deep reinforcement learning (DRL) models (Khalil et al., 2017; Barrett et al., 2019; Yao et al., 2021; Bello et al., 2016) and neural diffusion models (Graikos et al., 2022; Sun and Yang, 2023). DRL approaches focus on training an agent to learn the heuristics for solving a CO problem, by allowing the agent to interact with its environment, aided by a graph representation of the problem. In the unsupervised setting for a DRL solver, the system learns the search strategy without requiring annotated optimal solutions for training-set graphs (Karalias and Loukas, 2020; Wang et al., 2022), but DRL CO solvers are usually hard to train, due to the sparse reward problem (Wu et al., 2021). In the supervised settings, on the other hand, neural network solvers learn a generative process to directly predict the high-quality solutions to CO problems (Joshi et al., 2019; Sun and Yang, 2023). They are usually more stable in training and efficient in inference, but require near-optimal annotations generated by traditional solvers. Recent progress in diffusion models for image generation (Ho et al., 2020; Song et al., 2020; Song and Ermon, 2020) has sparked interest in applying diffusion models to CO problem with the hope of exploiting its expressiveness. Graikos et al. (2022) show by encoding the traveling salesman problem (TSP) as 64 \u00d7 64 greyscale images, image diffusion models (Ho et al., 2020) are able to generate heatmaps from which the solutions could be extracted. Sun and Yang (2023) proposed the DIFUSCO framework, which works by explicitly modeling the graph structure of CO problems and denoising ICML 2023 Workshop: Sampling and Optimization in Discrete Space (SODS 2023). the variable indicator vector. However, these diffusion-based methods are inefficient in inference because of the non-trivial number of inference steps required to produce satisfactory results. To remedy the efficiency problem encountered by these diffusion-based solvers, we propose to use progressive distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022; Meng et al., 2023) to address the inference efficiency issue of DIFUSCO, accelerating its inference while preserving its solution quality. We train the student to compress two denoising steps from the teacher into one in a single training iteration and we iteratively perform this compression to achieve further distilled student. We report a 0.019% performance degradation on our 4x distilled student, which inferences 16 times faster than the teacher. 2 Method 2.1 Problem Definition In this paper, our primary focus is on the Travelling Salesman Problem (TSP) as a subject of investigation. However, we posit that both the algorithm we have developed and our conclusions have broader applicability, potentially extending to other types of Combinatorial Optimization (CO) problems. This perspective aligns with the findings presented in DIFUSCO (Sun and Yang, 2023). Formally, we define the possible solution space of TSP instance s to be Xs = {0, 1}N , where N is the number of edges in the problem instance. For each possible solution vector x \u2208 Xs, xi is the indicator variable for whether edge i is chosen in the solution vector x. We define the objective function Js : Xs \u2192 R such that it incorporates both the cost and the validity of the solution: Js(x) = costs(x) + valids(x) where costs(x) = xT d, di being the ith edge\u2019s weight in s, and valids(x) is 0 for a feasible TSP solution, +\u221e for infeasible solutions. The optimal solution to s is x\u2217 = arg min Js(x) x\u2208Xs 2.2 Diffusion Solvers for Combinatorial Optimization DIFUSCO (Sun and Yang, 2023) is a framework for directly modeling the graph representation of CO problems and solving CO problems via diffusion models. By first re-scaling (Chen and Tian, 2019) {0, 1} valued variables to {\u22121, 1} and treating them as real-valued variables, it becomes viable to apply continuous diffusion (Ho et al., 2020; Song et al., 2020) to the problem setup in 2.1. Re-scaling for binary valued variable at time step t \u2208 {1, . . . T } is given as: \u02c6xt = 2xt \u2212 1 After the re-scaling, we then use the vanilla forward process proposed by Ho et al. (2020) q\u03b8(\u02c6xt|\u02c6xt\u22121) = N (\u02c6xt; (cid:112)1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI) where \u03b2t is the ratio of corruption at timestep t. The t step marginal distribution of \u02c6xt is therefore: q\u03b8(\u02c6xt|\u02c6x0) = N (\u02c6xt; \u221a \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) We train a neural network \u03f5\u03b8(\u00b7, \u00b7) to predict the Gaussian noise \u03f5"}, {"question": " What is the DIFUSCO framework proposed by Sun and Yang (2023)?", "answer": " The DIFUSCO framework explicitly models the graph structure of CO problems and denoises the variable indicator vector.", "ref_chunk": "3 2 0 2 g u A 2 2 ] G L . s c [ 2 v 4 4 6 6 0 . 8 0 3 2 : v i X r a Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation Junwei Huang Carnegie Mellon University junweih@andrew.cmu.edu Zhiqing Sun Carnegie Mellon University zhiqings@cs.cmu.edu Yiming Yang Carnegie Mellon University yiming@cs.cmu.edu Abstract Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset. 1 Introduction The realm of Combinatorial Optimization (CO) is intricately tied to optimization within discrete spaces. A significant portion of CO problems falls into the category of NP-complete (NPC) prob- lems, for which efficient algorithms for exact solutions are nearly impossible to find. In the past, researchers have often turned to traditional approximation algorithms as a means to tackle these NPC problems (Arora, 1998; Lawler et al., 1980). Unfortunately, these traditional methods demand an extensive degree of mathematical understanding specific to each individual problem, resulting in heuristics that lack transferability across diverse problem types. Modern approaches utilize the pattern recognition power of machine learning techniques to generate high-quality solutions. Representative methods include the deep reinforcement learning (DRL) models (Khalil et al., 2017; Barrett et al., 2019; Yao et al., 2021; Bello et al., 2016) and neural diffusion models (Graikos et al., 2022; Sun and Yang, 2023). DRL approaches focus on training an agent to learn the heuristics for solving a CO problem, by allowing the agent to interact with its environment, aided by a graph representation of the problem. In the unsupervised setting for a DRL solver, the system learns the search strategy without requiring annotated optimal solutions for training-set graphs (Karalias and Loukas, 2020; Wang et al., 2022), but DRL CO solvers are usually hard to train, due to the sparse reward problem (Wu et al., 2021). In the supervised settings, on the other hand, neural network solvers learn a generative process to directly predict the high-quality solutions to CO problems (Joshi et al., 2019; Sun and Yang, 2023). They are usually more stable in training and efficient in inference, but require near-optimal annotations generated by traditional solvers. Recent progress in diffusion models for image generation (Ho et al., 2020; Song et al., 2020; Song and Ermon, 2020) has sparked interest in applying diffusion models to CO problem with the hope of exploiting its expressiveness. Graikos et al. (2022) show by encoding the traveling salesman problem (TSP) as 64 \u00d7 64 greyscale images, image diffusion models (Ho et al., 2020) are able to generate heatmaps from which the solutions could be extracted. Sun and Yang (2023) proposed the DIFUSCO framework, which works by explicitly modeling the graph structure of CO problems and denoising ICML 2023 Workshop: Sampling and Optimization in Discrete Space (SODS 2023). the variable indicator vector. However, these diffusion-based methods are inefficient in inference because of the non-trivial number of inference steps required to produce satisfactory results. To remedy the efficiency problem encountered by these diffusion-based solvers, we propose to use progressive distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022; Meng et al., 2023) to address the inference efficiency issue of DIFUSCO, accelerating its inference while preserving its solution quality. We train the student to compress two denoising steps from the teacher into one in a single training iteration and we iteratively perform this compression to achieve further distilled student. We report a 0.019% performance degradation on our 4x distilled student, which inferences 16 times faster than the teacher. 2 Method 2.1 Problem Definition In this paper, our primary focus is on the Travelling Salesman Problem (TSP) as a subject of investigation. However, we posit that both the algorithm we have developed and our conclusions have broader applicability, potentially extending to other types of Combinatorial Optimization (CO) problems. This perspective aligns with the findings presented in DIFUSCO (Sun and Yang, 2023). Formally, we define the possible solution space of TSP instance s to be Xs = {0, 1}N , where N is the number of edges in the problem instance. For each possible solution vector x \u2208 Xs, xi is the indicator variable for whether edge i is chosen in the solution vector x. We define the objective function Js : Xs \u2192 R such that it incorporates both the cost and the validity of the solution: Js(x) = costs(x) + valids(x) where costs(x) = xT d, di being the ith edge\u2019s weight in s, and valids(x) is 0 for a feasible TSP solution, +\u221e for infeasible solutions. The optimal solution to s is x\u2217 = arg min Js(x) x\u2208Xs 2.2 Diffusion Solvers for Combinatorial Optimization DIFUSCO (Sun and Yang, 2023) is a framework for directly modeling the graph representation of CO problems and solving CO problems via diffusion models. By first re-scaling (Chen and Tian, 2019) {0, 1} valued variables to {\u22121, 1} and treating them as real-valued variables, it becomes viable to apply continuous diffusion (Ho et al., 2020; Song et al., 2020) to the problem setup in 2.1. Re-scaling for binary valued variable at time step t \u2208 {1, . . . T } is given as: \u02c6xt = 2xt \u2212 1 After the re-scaling, we then use the vanilla forward process proposed by Ho et al. (2020) q\u03b8(\u02c6xt|\u02c6xt\u22121) = N (\u02c6xt; (cid:112)1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI) where \u03b2t is the ratio of corruption at timestep t. The t step marginal distribution of \u02c6xt is therefore: q\u03b8(\u02c6xt|\u02c6x0) = N (\u02c6xt; \u221a \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) We train a neural network \u03f5\u03b8(\u00b7, \u00b7) to predict the Gaussian noise \u03f5"}], "doc_text": "3 2 0 2 g u A 2 2 ] G L . s c [ 2 v 4 4 6 6 0 . 8 0 3 2 : v i X r a Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation Junwei Huang Carnegie Mellon University junweih@andrew.cmu.edu Zhiqing Sun Carnegie Mellon University zhiqings@cs.cmu.edu Yiming Yang Carnegie Mellon University yiming@cs.cmu.edu Abstract Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset. 1 Introduction The realm of Combinatorial Optimization (CO) is intricately tied to optimization within discrete spaces. A significant portion of CO problems falls into the category of NP-complete (NPC) prob- lems, for which efficient algorithms for exact solutions are nearly impossible to find. In the past, researchers have often turned to traditional approximation algorithms as a means to tackle these NPC problems (Arora, 1998; Lawler et al., 1980). Unfortunately, these traditional methods demand an extensive degree of mathematical understanding specific to each individual problem, resulting in heuristics that lack transferability across diverse problem types. Modern approaches utilize the pattern recognition power of machine learning techniques to generate high-quality solutions. Representative methods include the deep reinforcement learning (DRL) models (Khalil et al., 2017; Barrett et al., 2019; Yao et al., 2021; Bello et al., 2016) and neural diffusion models (Graikos et al., 2022; Sun and Yang, 2023). DRL approaches focus on training an agent to learn the heuristics for solving a CO problem, by allowing the agent to interact with its environment, aided by a graph representation of the problem. In the unsupervised setting for a DRL solver, the system learns the search strategy without requiring annotated optimal solutions for training-set graphs (Karalias and Loukas, 2020; Wang et al., 2022), but DRL CO solvers are usually hard to train, due to the sparse reward problem (Wu et al., 2021). In the supervised settings, on the other hand, neural network solvers learn a generative process to directly predict the high-quality solutions to CO problems (Joshi et al., 2019; Sun and Yang, 2023). They are usually more stable in training and efficient in inference, but require near-optimal annotations generated by traditional solvers. Recent progress in diffusion models for image generation (Ho et al., 2020; Song et al., 2020; Song and Ermon, 2020) has sparked interest in applying diffusion models to CO problem with the hope of exploiting its expressiveness. Graikos et al. (2022) show by encoding the traveling salesman problem (TSP) as 64 \u00d7 64 greyscale images, image diffusion models (Ho et al., 2020) are able to generate heatmaps from which the solutions could be extracted. Sun and Yang (2023) proposed the DIFUSCO framework, which works by explicitly modeling the graph structure of CO problems and denoising ICML 2023 Workshop: Sampling and Optimization in Discrete Space (SODS 2023). the variable indicator vector. However, these diffusion-based methods are inefficient in inference because of the non-trivial number of inference steps required to produce satisfactory results. To remedy the efficiency problem encountered by these diffusion-based solvers, we propose to use progressive distillation (Rezagholizadeh et al., 2022; Salimans and Ho, 2022; Meng et al., 2023) to address the inference efficiency issue of DIFUSCO, accelerating its inference while preserving its solution quality. We train the student to compress two denoising steps from the teacher into one in a single training iteration and we iteratively perform this compression to achieve further distilled student. We report a 0.019% performance degradation on our 4x distilled student, which inferences 16 times faster than the teacher. 2 Method 2.1 Problem Definition In this paper, our primary focus is on the Travelling Salesman Problem (TSP) as a subject of investigation. However, we posit that both the algorithm we have developed and our conclusions have broader applicability, potentially extending to other types of Combinatorial Optimization (CO) problems. This perspective aligns with the findings presented in DIFUSCO (Sun and Yang, 2023). Formally, we define the possible solution space of TSP instance s to be Xs = {0, 1}N , where N is the number of edges in the problem instance. For each possible solution vector x \u2208 Xs, xi is the indicator variable for whether edge i is chosen in the solution vector x. We define the objective function Js : Xs \u2192 R such that it incorporates both the cost and the validity of the solution: Js(x) = costs(x) + valids(x) where costs(x) = xT d, di being the ith edge\u2019s weight in s, and valids(x) is 0 for a feasible TSP solution, +\u221e for infeasible solutions. The optimal solution to s is x\u2217 = arg min Js(x) x\u2208Xs 2.2 Diffusion Solvers for Combinatorial Optimization DIFUSCO (Sun and Yang, 2023) is a framework for directly modeling the graph representation of CO problems and solving CO problems via diffusion models. By first re-scaling (Chen and Tian, 2019) {0, 1} valued variables to {\u22121, 1} and treating them as real-valued variables, it becomes viable to apply continuous diffusion (Ho et al., 2020; Song et al., 2020) to the problem setup in 2.1. Re-scaling for binary valued variable at time step t \u2208 {1, . . . T } is given as: \u02c6xt = 2xt \u2212 1 After the re-scaling, we then use the vanilla forward process proposed by Ho et al. (2020) q\u03b8(\u02c6xt|\u02c6xt\u22121) = N (\u02c6xt; (cid:112)1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI) where \u03b2t is the ratio of corruption at timestep t. The t step marginal distribution of \u02c6xt is therefore: q\u03b8(\u02c6xt|\u02c6x0) = N (\u02c6xt; \u221a \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) We train a neural network \u03f5\u03b8(\u00b7, \u00b7) to predict the Gaussian noise \u03f5"}