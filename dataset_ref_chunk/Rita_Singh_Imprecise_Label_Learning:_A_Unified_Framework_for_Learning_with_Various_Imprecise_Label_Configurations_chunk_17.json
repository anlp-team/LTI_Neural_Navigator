{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_Imprecise_Label_Learning:_A_Unified_Framework_for_Learning_with_Various_Imprecise_Label_Configurations_chunk_17.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What learning rate was used for training CIFAR-10 and CIFAR-100 with PreAct-ResNet-18?,answer: 0.02", "ref_chunk": "of classes. For CIFAR-10 and CIFAR-100, we train PreAct-ResNet-18 with SGD using a learning rate of 0.02, a weight decay of 1e \u2212 3, and a momentum of 0.9. We train for 300 epochs with a cosine learning rate schedule and a batch size of 128. For WebVision, we use InceptionResNet-v2 as the backbone and set the batch size to 32. Other settings are similar to CIFAR-10. For Clothing1M, we use ImageNet-1K pre trained ResNet-50 as the backbone. We train it using SGD with an initial learning rate of 2e-3 for a total of 10 epochs, where the learning rate is reduced by 10 after 5 epochs. In addition, we also conduct experiments on CIFAR-10N and CIFAR-100N. We present the detailed hyper-parameters in Table 13. D.4.2 RESULTS In addition to the results regarding noisy label learning provided in the main paper, we also present comparison results on CIFAR-10N and CIFAR-100N (Wei et al., 2021) in Table 14. We include a full comparison on Clothing1M and WebVision, incorporating methods like Co-Teaching, Forward, and CORES, in Table 15. As shown in Table 14, the proposed ILL framework achieves performance comparable to the previous best method, SOP (Liu et al., 2022). On CIFAR-10N, our method yields results very close to SOP in the Random and Aggregate case noise scenarios and surpasses SOP in the 27 Preprint Table 13: Hyper-parameters for noisy label learning used in experiments. CIFAR-100 (CIFAR-100N) Hyper-parameter CIFAR-10 (CIFAR-10N) Clothing1M WebVision Image Size Model Batch Size Learning Rate Weight Decay LR Scheduler Training Epochs Classes Noisy Matrix Scale 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 10 1.0 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 100 2.0 224 ResNet-50 (ImageNet-1K Pretrained) 64 0.002 1e-3 MultiStep 10 14 0.5 299 Inception-ResNet-v2 32 0.02 5e-4 MultiStep 100 50 2.5 Worst case noise scenario. However, on CIFAR-100N, our method slightly underperforms previous methods, possibly due to the oversimplified noise model utilized in ILL. We believe that a more realistic noise transition model and further tuning of our method could lead to improved performance. Table 14: Test accuracy comparison of instance independent label noise on CIFAR-10N and CIFAR- 100N for noisy label learning. The best results are indicated in bold, and the second best results are indicated in underline. Our results are averaged over three independent runs with ResNet34 as the backbone. Dataset CIFAR-10N CIFAR-100N Noisy Type Clean Random 1 Random 2 Random 3 Aggregate Worst Clean Noisy CE Forward (Patrini et al., 2016) Co-teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 92.92\u00b10.11 93.02\u00b10.12 93.35\u00b10.14 - 95.39\u00b10.05 94.16\u00b10.11 96.38\u00b10.31 85.02\u00b10.65 86.88\u00b10.50 90.33\u00b10.13 95.16\u00b10.19 94.43\u00b10.41 94.45\u00b10.14 95.28\u00b10.13 86.46\u00b11.79 86.14\u00b10.24 90.30\u00b10.17 95.23\u00b10.07 94.20\u00b10.24 94.88\u00b10.31 95.31\u00b10.10 85.16\u00b10.61 87.04\u00b10.35 90.15\u00b10.18 95.21\u00b10.14 94.34\u00b10.22 94.74\u00b10.03 95.39\u00b10.11 87.77\u00b10.38 88.24\u00b10.22 91.20\u00b10.13 95.01\u00b10.71 94.83\u00b10.10 95.25\u00b10.09 95.61\u00b10.13 77.69\u00b11.55 79.79\u00b10.46 83.83\u00b10.13 92.56\u00b10.42 91.09\u00b11.60 91.66\u00b10.09 93.24\u00b10.21 76.70\u00b10.74 76.18\u00b10.37 73.46\u00b10.09 - 78.57\u00b10.12 73.87\u00b10.16 78.91\u00b10.43 55.50\u00b10.66 57.01\u00b11.03 60.37\u00b10.27 71.13\u00b10.48 66.72\u00b10.07 55.72\u00b10.42 67.81\u00b10.23 Ours 96.21\u00b10.29 96.06\u00b10.07 95.98\u00b10.12 96.10\u00b10.05 96.40\u00b10.03 93.55\u00b10.14 78.53\u00b10.21 68.07\u00b10.33 Table 15: Test accuracy comparison of realistic noisy labels on Clothing1M and WebVision for noisy label learning. The best results are indicated in bold and the second best results are indicated in underline. Our results are averaged over 3 independent runs. For Clothing1M, we use ImageNet-1K pre trained ResNet50 as the backbone. For WebVision, InceptionResNetv2 is used as the backbone. Dataset Clothing1M WebVision CE Forward (Patrini et al., 2016) Co-Teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 69.10 69.80 69.20 74.76 72.90 73.20 73.50 61.10 63.60 77.32 76.20 - 76.60 Ours 74.02\u00b10.12 79.37\u00b10.09 D.5 MIXED IMPRECISE LABEL LEARNING D.5.1 SETUP To create a mixture of various imprecise label configurations, we select CIFAR-10 and CIFAR-100 as base datasets. We first uniformly sample l/C labeled samples from each class to form the labeled dataset and treat the remaining samples as the unlabeled dataset. Based on the labeled dataset, we generate partially labeled datasets by flipping negative labels to false positive labels with the partial ratio q. After obtaining the partial labels, we randomly select \u03b7 percentage of samples from each class, and recreate the partial labels for them by flipping the ground truth label uniformly to another class. In this setting, unlabeled data, partially labeled data, and noisy labeled data exist simultaneously, which is very challenging and more closely resembles realistic situations. For CIFAR-10, we set l \u2208 {1000, 5000, 50000}, and for CIFAR-100, we set l \u2208 {5000, 10000, 50000}. Similarly in the partial label setting, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0.1, 0.2, 0.3} for both datasets. 28 Preprint D.5.2 RESULTS We provide a more complete version of Table 4 in Table 16. On partial noisy labels of CIFAR-10 with partial ratio 0.5 and of CIFAR-100 with partial ratio 0.1, most baseline methods are more robust or even fail to perform. However, our ILL still shows very robust performance with minor performance degradation as increase of noise ratios. Table 16: Test accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of {0.1, 0.3, 0.5} for CIFAR-10 and {0.01, 0.05, 0.1} for CIFAR-100, and noise ratio \u03b7 of {0.1, 0.2, 0.3} for CIFAR-10 and CIFAR-100. Dataset # Labels Partial Ratio q Noise Ratio \u03b7 0 0.1 0.2 0.1 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.99\u00b10.03 - - - - 96.55\u00b10.08 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 CIFAR-10 50,000 0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.73\u00b10.10 - - - - 96.52\u00b10.12 92.32 92.81 93.44"}, {"question": " How many epochs were CIFAR-10 and CIFAR-100 trained for?,answer: 300", "ref_chunk": "of classes. For CIFAR-10 and CIFAR-100, we train PreAct-ResNet-18 with SGD using a learning rate of 0.02, a weight decay of 1e \u2212 3, and a momentum of 0.9. We train for 300 epochs with a cosine learning rate schedule and a batch size of 128. For WebVision, we use InceptionResNet-v2 as the backbone and set the batch size to 32. Other settings are similar to CIFAR-10. For Clothing1M, we use ImageNet-1K pre trained ResNet-50 as the backbone. We train it using SGD with an initial learning rate of 2e-3 for a total of 10 epochs, where the learning rate is reduced by 10 after 5 epochs. In addition, we also conduct experiments on CIFAR-10N and CIFAR-100N. We present the detailed hyper-parameters in Table 13. D.4.2 RESULTS In addition to the results regarding noisy label learning provided in the main paper, we also present comparison results on CIFAR-10N and CIFAR-100N (Wei et al., 2021) in Table 14. We include a full comparison on Clothing1M and WebVision, incorporating methods like Co-Teaching, Forward, and CORES, in Table 15. As shown in Table 14, the proposed ILL framework achieves performance comparable to the previous best method, SOP (Liu et al., 2022). On CIFAR-10N, our method yields results very close to SOP in the Random and Aggregate case noise scenarios and surpasses SOP in the 27 Preprint Table 13: Hyper-parameters for noisy label learning used in experiments. CIFAR-100 (CIFAR-100N) Hyper-parameter CIFAR-10 (CIFAR-10N) Clothing1M WebVision Image Size Model Batch Size Learning Rate Weight Decay LR Scheduler Training Epochs Classes Noisy Matrix Scale 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 10 1.0 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 100 2.0 224 ResNet-50 (ImageNet-1K Pretrained) 64 0.002 1e-3 MultiStep 10 14 0.5 299 Inception-ResNet-v2 32 0.02 5e-4 MultiStep 100 50 2.5 Worst case noise scenario. However, on CIFAR-100N, our method slightly underperforms previous methods, possibly due to the oversimplified noise model utilized in ILL. We believe that a more realistic noise transition model and further tuning of our method could lead to improved performance. Table 14: Test accuracy comparison of instance independent label noise on CIFAR-10N and CIFAR- 100N for noisy label learning. The best results are indicated in bold, and the second best results are indicated in underline. Our results are averaged over three independent runs with ResNet34 as the backbone. Dataset CIFAR-10N CIFAR-100N Noisy Type Clean Random 1 Random 2 Random 3 Aggregate Worst Clean Noisy CE Forward (Patrini et al., 2016) Co-teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 92.92\u00b10.11 93.02\u00b10.12 93.35\u00b10.14 - 95.39\u00b10.05 94.16\u00b10.11 96.38\u00b10.31 85.02\u00b10.65 86.88\u00b10.50 90.33\u00b10.13 95.16\u00b10.19 94.43\u00b10.41 94.45\u00b10.14 95.28\u00b10.13 86.46\u00b11.79 86.14\u00b10.24 90.30\u00b10.17 95.23\u00b10.07 94.20\u00b10.24 94.88\u00b10.31 95.31\u00b10.10 85.16\u00b10.61 87.04\u00b10.35 90.15\u00b10.18 95.21\u00b10.14 94.34\u00b10.22 94.74\u00b10.03 95.39\u00b10.11 87.77\u00b10.38 88.24\u00b10.22 91.20\u00b10.13 95.01\u00b10.71 94.83\u00b10.10 95.25\u00b10.09 95.61\u00b10.13 77.69\u00b11.55 79.79\u00b10.46 83.83\u00b10.13 92.56\u00b10.42 91.09\u00b11.60 91.66\u00b10.09 93.24\u00b10.21 76.70\u00b10.74 76.18\u00b10.37 73.46\u00b10.09 - 78.57\u00b10.12 73.87\u00b10.16 78.91\u00b10.43 55.50\u00b10.66 57.01\u00b11.03 60.37\u00b10.27 71.13\u00b10.48 66.72\u00b10.07 55.72\u00b10.42 67.81\u00b10.23 Ours 96.21\u00b10.29 96.06\u00b10.07 95.98\u00b10.12 96.10\u00b10.05 96.40\u00b10.03 93.55\u00b10.14 78.53\u00b10.21 68.07\u00b10.33 Table 15: Test accuracy comparison of realistic noisy labels on Clothing1M and WebVision for noisy label learning. The best results are indicated in bold and the second best results are indicated in underline. Our results are averaged over 3 independent runs. For Clothing1M, we use ImageNet-1K pre trained ResNet50 as the backbone. For WebVision, InceptionResNetv2 is used as the backbone. Dataset Clothing1M WebVision CE Forward (Patrini et al., 2016) Co-Teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 69.10 69.80 69.20 74.76 72.90 73.20 73.50 61.10 63.60 77.32 76.20 - 76.60 Ours 74.02\u00b10.12 79.37\u00b10.09 D.5 MIXED IMPRECISE LABEL LEARNING D.5.1 SETUP To create a mixture of various imprecise label configurations, we select CIFAR-10 and CIFAR-100 as base datasets. We first uniformly sample l/C labeled samples from each class to form the labeled dataset and treat the remaining samples as the unlabeled dataset. Based on the labeled dataset, we generate partially labeled datasets by flipping negative labels to false positive labels with the partial ratio q. After obtaining the partial labels, we randomly select \u03b7 percentage of samples from each class, and recreate the partial labels for them by flipping the ground truth label uniformly to another class. In this setting, unlabeled data, partially labeled data, and noisy labeled data exist simultaneously, which is very challenging and more closely resembles realistic situations. For CIFAR-10, we set l \u2208 {1000, 5000, 50000}, and for CIFAR-100, we set l \u2208 {5000, 10000, 50000}. Similarly in the partial label setting, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0.1, 0.2, 0.3} for both datasets. 28 Preprint D.5.2 RESULTS We provide a more complete version of Table 4 in Table 16. On partial noisy labels of CIFAR-10 with partial ratio 0.5 and of CIFAR-100 with partial ratio 0.1, most baseline methods are more robust or even fail to perform. However, our ILL still shows very robust performance with minor performance degradation as increase of noise ratios. Table 16: Test accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of {0.1, 0.3, 0.5} for CIFAR-10 and {0.01, 0.05, 0.1} for CIFAR-100, and noise ratio \u03b7 of {0.1, 0.2, 0.3} for CIFAR-10 and CIFAR-100. Dataset # Labels Partial Ratio q Noise Ratio \u03b7 0 0.1 0.2 0.1 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.99\u00b10.03 - - - - 96.55\u00b10.08 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 CIFAR-10 50,000 0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.73\u00b10.10 - - - - 96.52\u00b10.12 92.32 92.81 93.44"}, {"question": " What backbone model was used for WebVision?,answer: InceptionResNet-v2", "ref_chunk": "of classes. For CIFAR-10 and CIFAR-100, we train PreAct-ResNet-18 with SGD using a learning rate of 0.02, a weight decay of 1e \u2212 3, and a momentum of 0.9. We train for 300 epochs with a cosine learning rate schedule and a batch size of 128. For WebVision, we use InceptionResNet-v2 as the backbone and set the batch size to 32. Other settings are similar to CIFAR-10. For Clothing1M, we use ImageNet-1K pre trained ResNet-50 as the backbone. We train it using SGD with an initial learning rate of 2e-3 for a total of 10 epochs, where the learning rate is reduced by 10 after 5 epochs. In addition, we also conduct experiments on CIFAR-10N and CIFAR-100N. We present the detailed hyper-parameters in Table 13. D.4.2 RESULTS In addition to the results regarding noisy label learning provided in the main paper, we also present comparison results on CIFAR-10N and CIFAR-100N (Wei et al., 2021) in Table 14. We include a full comparison on Clothing1M and WebVision, incorporating methods like Co-Teaching, Forward, and CORES, in Table 15. As shown in Table 14, the proposed ILL framework achieves performance comparable to the previous best method, SOP (Liu et al., 2022). On CIFAR-10N, our method yields results very close to SOP in the Random and Aggregate case noise scenarios and surpasses SOP in the 27 Preprint Table 13: Hyper-parameters for noisy label learning used in experiments. CIFAR-100 (CIFAR-100N) Hyper-parameter CIFAR-10 (CIFAR-10N) Clothing1M WebVision Image Size Model Batch Size Learning Rate Weight Decay LR Scheduler Training Epochs Classes Noisy Matrix Scale 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 10 1.0 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 100 2.0 224 ResNet-50 (ImageNet-1K Pretrained) 64 0.002 1e-3 MultiStep 10 14 0.5 299 Inception-ResNet-v2 32 0.02 5e-4 MultiStep 100 50 2.5 Worst case noise scenario. However, on CIFAR-100N, our method slightly underperforms previous methods, possibly due to the oversimplified noise model utilized in ILL. We believe that a more realistic noise transition model and further tuning of our method could lead to improved performance. Table 14: Test accuracy comparison of instance independent label noise on CIFAR-10N and CIFAR- 100N for noisy label learning. The best results are indicated in bold, and the second best results are indicated in underline. Our results are averaged over three independent runs with ResNet34 as the backbone. Dataset CIFAR-10N CIFAR-100N Noisy Type Clean Random 1 Random 2 Random 3 Aggregate Worst Clean Noisy CE Forward (Patrini et al., 2016) Co-teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 92.92\u00b10.11 93.02\u00b10.12 93.35\u00b10.14 - 95.39\u00b10.05 94.16\u00b10.11 96.38\u00b10.31 85.02\u00b10.65 86.88\u00b10.50 90.33\u00b10.13 95.16\u00b10.19 94.43\u00b10.41 94.45\u00b10.14 95.28\u00b10.13 86.46\u00b11.79 86.14\u00b10.24 90.30\u00b10.17 95.23\u00b10.07 94.20\u00b10.24 94.88\u00b10.31 95.31\u00b10.10 85.16\u00b10.61 87.04\u00b10.35 90.15\u00b10.18 95.21\u00b10.14 94.34\u00b10.22 94.74\u00b10.03 95.39\u00b10.11 87.77\u00b10.38 88.24\u00b10.22 91.20\u00b10.13 95.01\u00b10.71 94.83\u00b10.10 95.25\u00b10.09 95.61\u00b10.13 77.69\u00b11.55 79.79\u00b10.46 83.83\u00b10.13 92.56\u00b10.42 91.09\u00b11.60 91.66\u00b10.09 93.24\u00b10.21 76.70\u00b10.74 76.18\u00b10.37 73.46\u00b10.09 - 78.57\u00b10.12 73.87\u00b10.16 78.91\u00b10.43 55.50\u00b10.66 57.01\u00b11.03 60.37\u00b10.27 71.13\u00b10.48 66.72\u00b10.07 55.72\u00b10.42 67.81\u00b10.23 Ours 96.21\u00b10.29 96.06\u00b10.07 95.98\u00b10.12 96.10\u00b10.05 96.40\u00b10.03 93.55\u00b10.14 78.53\u00b10.21 68.07\u00b10.33 Table 15: Test accuracy comparison of realistic noisy labels on Clothing1M and WebVision for noisy label learning. The best results are indicated in bold and the second best results are indicated in underline. Our results are averaged over 3 independent runs. For Clothing1M, we use ImageNet-1K pre trained ResNet50 as the backbone. For WebVision, InceptionResNetv2 is used as the backbone. Dataset Clothing1M WebVision CE Forward (Patrini et al., 2016) Co-Teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 69.10 69.80 69.20 74.76 72.90 73.20 73.50 61.10 63.60 77.32 76.20 - 76.60 Ours 74.02\u00b10.12 79.37\u00b10.09 D.5 MIXED IMPRECISE LABEL LEARNING D.5.1 SETUP To create a mixture of various imprecise label configurations, we select CIFAR-10 and CIFAR-100 as base datasets. We first uniformly sample l/C labeled samples from each class to form the labeled dataset and treat the remaining samples as the unlabeled dataset. Based on the labeled dataset, we generate partially labeled datasets by flipping negative labels to false positive labels with the partial ratio q. After obtaining the partial labels, we randomly select \u03b7 percentage of samples from each class, and recreate the partial labels for them by flipping the ground truth label uniformly to another class. In this setting, unlabeled data, partially labeled data, and noisy labeled data exist simultaneously, which is very challenging and more closely resembles realistic situations. For CIFAR-10, we set l \u2208 {1000, 5000, 50000}, and for CIFAR-100, we set l \u2208 {5000, 10000, 50000}. Similarly in the partial label setting, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0.1, 0.2, 0.3} for both datasets. 28 Preprint D.5.2 RESULTS We provide a more complete version of Table 4 in Table 16. On partial noisy labels of CIFAR-10 with partial ratio 0.5 and of CIFAR-100 with partial ratio 0.1, most baseline methods are more robust or even fail to perform. However, our ILL still shows very robust performance with minor performance degradation as increase of noise ratios. Table 16: Test accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of {0.1, 0.3, 0.5} for CIFAR-10 and {0.01, 0.05, 0.1} for CIFAR-100, and noise ratio \u03b7 of {0.1, 0.2, 0.3} for CIFAR-10 and CIFAR-100. Dataset # Labels Partial Ratio q Noise Ratio \u03b7 0 0.1 0.2 0.1 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.99\u00b10.03 - - - - 96.55\u00b10.08 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 CIFAR-10 50,000 0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.73\u00b10.10 - - - - 96.52\u00b10.12 92.32 92.81 93.44"}, {"question": " How many epochs was ImageNet-1K pre-trained ResNet-50 trained for on Clothing1M?,answer: 10", "ref_chunk": "of classes. For CIFAR-10 and CIFAR-100, we train PreAct-ResNet-18 with SGD using a learning rate of 0.02, a weight decay of 1e \u2212 3, and a momentum of 0.9. We train for 300 epochs with a cosine learning rate schedule and a batch size of 128. For WebVision, we use InceptionResNet-v2 as the backbone and set the batch size to 32. Other settings are similar to CIFAR-10. For Clothing1M, we use ImageNet-1K pre trained ResNet-50 as the backbone. We train it using SGD with an initial learning rate of 2e-3 for a total of 10 epochs, where the learning rate is reduced by 10 after 5 epochs. In addition, we also conduct experiments on CIFAR-10N and CIFAR-100N. We present the detailed hyper-parameters in Table 13. D.4.2 RESULTS In addition to the results regarding noisy label learning provided in the main paper, we also present comparison results on CIFAR-10N and CIFAR-100N (Wei et al., 2021) in Table 14. We include a full comparison on Clothing1M and WebVision, incorporating methods like Co-Teaching, Forward, and CORES, in Table 15. As shown in Table 14, the proposed ILL framework achieves performance comparable to the previous best method, SOP (Liu et al., 2022). On CIFAR-10N, our method yields results very close to SOP in the Random and Aggregate case noise scenarios and surpasses SOP in the 27 Preprint Table 13: Hyper-parameters for noisy label learning used in experiments. CIFAR-100 (CIFAR-100N) Hyper-parameter CIFAR-10 (CIFAR-10N) Clothing1M WebVision Image Size Model Batch Size Learning Rate Weight Decay LR Scheduler Training Epochs Classes Noisy Matrix Scale 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 10 1.0 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 100 2.0 224 ResNet-50 (ImageNet-1K Pretrained) 64 0.002 1e-3 MultiStep 10 14 0.5 299 Inception-ResNet-v2 32 0.02 5e-4 MultiStep 100 50 2.5 Worst case noise scenario. However, on CIFAR-100N, our method slightly underperforms previous methods, possibly due to the oversimplified noise model utilized in ILL. We believe that a more realistic noise transition model and further tuning of our method could lead to improved performance. Table 14: Test accuracy comparison of instance independent label noise on CIFAR-10N and CIFAR- 100N for noisy label learning. The best results are indicated in bold, and the second best results are indicated in underline. Our results are averaged over three independent runs with ResNet34 as the backbone. Dataset CIFAR-10N CIFAR-100N Noisy Type Clean Random 1 Random 2 Random 3 Aggregate Worst Clean Noisy CE Forward (Patrini et al., 2016) Co-teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 92.92\u00b10.11 93.02\u00b10.12 93.35\u00b10.14 - 95.39\u00b10.05 94.16\u00b10.11 96.38\u00b10.31 85.02\u00b10.65 86.88\u00b10.50 90.33\u00b10.13 95.16\u00b10.19 94.43\u00b10.41 94.45\u00b10.14 95.28\u00b10.13 86.46\u00b11.79 86.14\u00b10.24 90.30\u00b10.17 95.23\u00b10.07 94.20\u00b10.24 94.88\u00b10.31 95.31\u00b10.10 85.16\u00b10.61 87.04\u00b10.35 90.15\u00b10.18 95.21\u00b10.14 94.34\u00b10.22 94.74\u00b10.03 95.39\u00b10.11 87.77\u00b10.38 88.24\u00b10.22 91.20\u00b10.13 95.01\u00b10.71 94.83\u00b10.10 95.25\u00b10.09 95.61\u00b10.13 77.69\u00b11.55 79.79\u00b10.46 83.83\u00b10.13 92.56\u00b10.42 91.09\u00b11.60 91.66\u00b10.09 93.24\u00b10.21 76.70\u00b10.74 76.18\u00b10.37 73.46\u00b10.09 - 78.57\u00b10.12 73.87\u00b10.16 78.91\u00b10.43 55.50\u00b10.66 57.01\u00b11.03 60.37\u00b10.27 71.13\u00b10.48 66.72\u00b10.07 55.72\u00b10.42 67.81\u00b10.23 Ours 96.21\u00b10.29 96.06\u00b10.07 95.98\u00b10.12 96.10\u00b10.05 96.40\u00b10.03 93.55\u00b10.14 78.53\u00b10.21 68.07\u00b10.33 Table 15: Test accuracy comparison of realistic noisy labels on Clothing1M and WebVision for noisy label learning. The best results are indicated in bold and the second best results are indicated in underline. Our results are averaged over 3 independent runs. For Clothing1M, we use ImageNet-1K pre trained ResNet50 as the backbone. For WebVision, InceptionResNetv2 is used as the backbone. Dataset Clothing1M WebVision CE Forward (Patrini et al., 2016) Co-Teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 69.10 69.80 69.20 74.76 72.90 73.20 73.50 61.10 63.60 77.32 76.20 - 76.60 Ours 74.02\u00b10.12 79.37\u00b10.09 D.5 MIXED IMPRECISE LABEL LEARNING D.5.1 SETUP To create a mixture of various imprecise label configurations, we select CIFAR-10 and CIFAR-100 as base datasets. We first uniformly sample l/C labeled samples from each class to form the labeled dataset and treat the remaining samples as the unlabeled dataset. Based on the labeled dataset, we generate partially labeled datasets by flipping negative labels to false positive labels with the partial ratio q. After obtaining the partial labels, we randomly select \u03b7 percentage of samples from each class, and recreate the partial labels for them by flipping the ground truth label uniformly to another class. In this setting, unlabeled data, partially labeled data, and noisy labeled data exist simultaneously, which is very challenging and more closely resembles realistic situations. For CIFAR-10, we set l \u2208 {1000, 5000, 50000}, and for CIFAR-100, we set l \u2208 {5000, 10000, 50000}. Similarly in the partial label setting, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0.1, 0.2, 0.3} for both datasets. 28 Preprint D.5.2 RESULTS We provide a more complete version of Table 4 in Table 16. On partial noisy labels of CIFAR-10 with partial ratio 0.5 and of CIFAR-100 with partial ratio 0.1, most baseline methods are more robust or even fail to perform. However, our ILL still shows very robust performance with minor performance degradation as increase of noise ratios. Table 16: Test accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of {0.1, 0.3, 0.5} for CIFAR-10 and {0.01, 0.05, 0.1} for CIFAR-100, and noise ratio \u03b7 of {0.1, 0.2, 0.3} for CIFAR-10 and CIFAR-100. Dataset # Labels Partial Ratio q Noise Ratio \u03b7 0 0.1 0.2 0.1 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.99\u00b10.03 - - - - 96.55\u00b10.08 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 CIFAR-10 50,000 0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.73\u00b10.10 - - - - 96.52\u00b10.12 92.32 92.81 93.44"}, {"question": " What method achieves performance comparable to SOP on CIFAR-10N according to Table 14?,answer: ILL framework", "ref_chunk": "of classes. For CIFAR-10 and CIFAR-100, we train PreAct-ResNet-18 with SGD using a learning rate of 0.02, a weight decay of 1e \u2212 3, and a momentum of 0.9. We train for 300 epochs with a cosine learning rate schedule and a batch size of 128. For WebVision, we use InceptionResNet-v2 as the backbone and set the batch size to 32. Other settings are similar to CIFAR-10. For Clothing1M, we use ImageNet-1K pre trained ResNet-50 as the backbone. We train it using SGD with an initial learning rate of 2e-3 for a total of 10 epochs, where the learning rate is reduced by 10 after 5 epochs. In addition, we also conduct experiments on CIFAR-10N and CIFAR-100N. We present the detailed hyper-parameters in Table 13. D.4.2 RESULTS In addition to the results regarding noisy label learning provided in the main paper, we also present comparison results on CIFAR-10N and CIFAR-100N (Wei et al., 2021) in Table 14. We include a full comparison on Clothing1M and WebVision, incorporating methods like Co-Teaching, Forward, and CORES, in Table 15. As shown in Table 14, the proposed ILL framework achieves performance comparable to the previous best method, SOP (Liu et al., 2022). On CIFAR-10N, our method yields results very close to SOP in the Random and Aggregate case noise scenarios and surpasses SOP in the 27 Preprint Table 13: Hyper-parameters for noisy label learning used in experiments. CIFAR-100 (CIFAR-100N) Hyper-parameter CIFAR-10 (CIFAR-10N) Clothing1M WebVision Image Size Model Batch Size Learning Rate Weight Decay LR Scheduler Training Epochs Classes Noisy Matrix Scale 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 10 1.0 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 100 2.0 224 ResNet-50 (ImageNet-1K Pretrained) 64 0.002 1e-3 MultiStep 10 14 0.5 299 Inception-ResNet-v2 32 0.02 5e-4 MultiStep 100 50 2.5 Worst case noise scenario. However, on CIFAR-100N, our method slightly underperforms previous methods, possibly due to the oversimplified noise model utilized in ILL. We believe that a more realistic noise transition model and further tuning of our method could lead to improved performance. Table 14: Test accuracy comparison of instance independent label noise on CIFAR-10N and CIFAR- 100N for noisy label learning. The best results are indicated in bold, and the second best results are indicated in underline. Our results are averaged over three independent runs with ResNet34 as the backbone. Dataset CIFAR-10N CIFAR-100N Noisy Type Clean Random 1 Random 2 Random 3 Aggregate Worst Clean Noisy CE Forward (Patrini et al., 2016) Co-teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 92.92\u00b10.11 93.02\u00b10.12 93.35\u00b10.14 - 95.39\u00b10.05 94.16\u00b10.11 96.38\u00b10.31 85.02\u00b10.65 86.88\u00b10.50 90.33\u00b10.13 95.16\u00b10.19 94.43\u00b10.41 94.45\u00b10.14 95.28\u00b10.13 86.46\u00b11.79 86.14\u00b10.24 90.30\u00b10.17 95.23\u00b10.07 94.20\u00b10.24 94.88\u00b10.31 95.31\u00b10.10 85.16\u00b10.61 87.04\u00b10.35 90.15\u00b10.18 95.21\u00b10.14 94.34\u00b10.22 94.74\u00b10.03 95.39\u00b10.11 87.77\u00b10.38 88.24\u00b10.22 91.20\u00b10.13 95.01\u00b10.71 94.83\u00b10.10 95.25\u00b10.09 95.61\u00b10.13 77.69\u00b11.55 79.79\u00b10.46 83.83\u00b10.13 92.56\u00b10.42 91.09\u00b11.60 91.66\u00b10.09 93.24\u00b10.21 76.70\u00b10.74 76.18\u00b10.37 73.46\u00b10.09 - 78.57\u00b10.12 73.87\u00b10.16 78.91\u00b10.43 55.50\u00b10.66 57.01\u00b11.03 60.37\u00b10.27 71.13\u00b10.48 66.72\u00b10.07 55.72\u00b10.42 67.81\u00b10.23 Ours 96.21\u00b10.29 96.06\u00b10.07 95.98\u00b10.12 96.10\u00b10.05 96.40\u00b10.03 93.55\u00b10.14 78.53\u00b10.21 68.07\u00b10.33 Table 15: Test accuracy comparison of realistic noisy labels on Clothing1M and WebVision for noisy label learning. The best results are indicated in bold and the second best results are indicated in underline. Our results are averaged over 3 independent runs. For Clothing1M, we use ImageNet-1K pre trained ResNet50 as the backbone. For WebVision, InceptionResNetv2 is used as the backbone. Dataset Clothing1M WebVision CE Forward (Patrini et al., 2016) Co-Teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 69.10 69.80 69.20 74.76 72.90 73.20 73.50 61.10 63.60 77.32 76.20 - 76.60 Ours 74.02\u00b10.12 79.37\u00b10.09 D.5 MIXED IMPRECISE LABEL LEARNING D.5.1 SETUP To create a mixture of various imprecise label configurations, we select CIFAR-10 and CIFAR-100 as base datasets. We first uniformly sample l/C labeled samples from each class to form the labeled dataset and treat the remaining samples as the unlabeled dataset. Based on the labeled dataset, we generate partially labeled datasets by flipping negative labels to false positive labels with the partial ratio q. After obtaining the partial labels, we randomly select \u03b7 percentage of samples from each class, and recreate the partial labels for them by flipping the ground truth label uniformly to another class. In this setting, unlabeled data, partially labeled data, and noisy labeled data exist simultaneously, which is very challenging and more closely resembles realistic situations. For CIFAR-10, we set l \u2208 {1000, 5000, 50000}, and for CIFAR-100, we set l \u2208 {5000, 10000, 50000}. Similarly in the partial label setting, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0.1, 0.2, 0.3} for both datasets. 28 Preprint D.5.2 RESULTS We provide a more complete version of Table 4 in Table 16. On partial noisy labels of CIFAR-10 with partial ratio 0.5 and of CIFAR-100 with partial ratio 0.1, most baseline methods are more robust or even fail to perform. However, our ILL still shows very robust performance with minor performance degradation as increase of noise ratios. Table 16: Test accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of {0.1, 0.3, 0.5} for CIFAR-10 and {0.01, 0.05, 0.1} for CIFAR-100, and noise ratio \u03b7 of {0.1, 0.2, 0.3} for CIFAR-10 and CIFAR-100. Dataset # Labels Partial Ratio q Noise Ratio \u03b7 0 0.1 0.2 0.1 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.99\u00b10.03 - - - - 96.55\u00b10.08 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 CIFAR-10 50,000 0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.73\u00b10.10 - - - - 96.52\u00b10.12 92.32 92.81 93.44"}, {"question": " In the Random and Aggregate case noise scenarios on CIFAR-10N, how does the proposed method compare to SOP?,answer: Yields results very close to SOP", "ref_chunk": "of classes. For CIFAR-10 and CIFAR-100, we train PreAct-ResNet-18 with SGD using a learning rate of 0.02, a weight decay of 1e \u2212 3, and a momentum of 0.9. We train for 300 epochs with a cosine learning rate schedule and a batch size of 128. For WebVision, we use InceptionResNet-v2 as the backbone and set the batch size to 32. Other settings are similar to CIFAR-10. For Clothing1M, we use ImageNet-1K pre trained ResNet-50 as the backbone. We train it using SGD with an initial learning rate of 2e-3 for a total of 10 epochs, where the learning rate is reduced by 10 after 5 epochs. In addition, we also conduct experiments on CIFAR-10N and CIFAR-100N. We present the detailed hyper-parameters in Table 13. D.4.2 RESULTS In addition to the results regarding noisy label learning provided in the main paper, we also present comparison results on CIFAR-10N and CIFAR-100N (Wei et al., 2021) in Table 14. We include a full comparison on Clothing1M and WebVision, incorporating methods like Co-Teaching, Forward, and CORES, in Table 15. As shown in Table 14, the proposed ILL framework achieves performance comparable to the previous best method, SOP (Liu et al., 2022). On CIFAR-10N, our method yields results very close to SOP in the Random and Aggregate case noise scenarios and surpasses SOP in the 27 Preprint Table 13: Hyper-parameters for noisy label learning used in experiments. CIFAR-100 (CIFAR-100N) Hyper-parameter CIFAR-10 (CIFAR-10N) Clothing1M WebVision Image Size Model Batch Size Learning Rate Weight Decay LR Scheduler Training Epochs Classes Noisy Matrix Scale 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 10 1.0 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 100 2.0 224 ResNet-50 (ImageNet-1K Pretrained) 64 0.002 1e-3 MultiStep 10 14 0.5 299 Inception-ResNet-v2 32 0.02 5e-4 MultiStep 100 50 2.5 Worst case noise scenario. However, on CIFAR-100N, our method slightly underperforms previous methods, possibly due to the oversimplified noise model utilized in ILL. We believe that a more realistic noise transition model and further tuning of our method could lead to improved performance. Table 14: Test accuracy comparison of instance independent label noise on CIFAR-10N and CIFAR- 100N for noisy label learning. The best results are indicated in bold, and the second best results are indicated in underline. Our results are averaged over three independent runs with ResNet34 as the backbone. Dataset CIFAR-10N CIFAR-100N Noisy Type Clean Random 1 Random 2 Random 3 Aggregate Worst Clean Noisy CE Forward (Patrini et al., 2016) Co-teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 92.92\u00b10.11 93.02\u00b10.12 93.35\u00b10.14 - 95.39\u00b10.05 94.16\u00b10.11 96.38\u00b10.31 85.02\u00b10.65 86.88\u00b10.50 90.33\u00b10.13 95.16\u00b10.19 94.43\u00b10.41 94.45\u00b10.14 95.28\u00b10.13 86.46\u00b11.79 86.14\u00b10.24 90.30\u00b10.17 95.23\u00b10.07 94.20\u00b10.24 94.88\u00b10.31 95.31\u00b10.10 85.16\u00b10.61 87.04\u00b10.35 90.15\u00b10.18 95.21\u00b10.14 94.34\u00b10.22 94.74\u00b10.03 95.39\u00b10.11 87.77\u00b10.38 88.24\u00b10.22 91.20\u00b10.13 95.01\u00b10.71 94.83\u00b10.10 95.25\u00b10.09 95.61\u00b10.13 77.69\u00b11.55 79.79\u00b10.46 83.83\u00b10.13 92.56\u00b10.42 91.09\u00b11.60 91.66\u00b10.09 93.24\u00b10.21 76.70\u00b10.74 76.18\u00b10.37 73.46\u00b10.09 - 78.57\u00b10.12 73.87\u00b10.16 78.91\u00b10.43 55.50\u00b10.66 57.01\u00b11.03 60.37\u00b10.27 71.13\u00b10.48 66.72\u00b10.07 55.72\u00b10.42 67.81\u00b10.23 Ours 96.21\u00b10.29 96.06\u00b10.07 95.98\u00b10.12 96.10\u00b10.05 96.40\u00b10.03 93.55\u00b10.14 78.53\u00b10.21 68.07\u00b10.33 Table 15: Test accuracy comparison of realistic noisy labels on Clothing1M and WebVision for noisy label learning. The best results are indicated in bold and the second best results are indicated in underline. Our results are averaged over 3 independent runs. For Clothing1M, we use ImageNet-1K pre trained ResNet50 as the backbone. For WebVision, InceptionResNetv2 is used as the backbone. Dataset Clothing1M WebVision CE Forward (Patrini et al., 2016) Co-Teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 69.10 69.80 69.20 74.76 72.90 73.20 73.50 61.10 63.60 77.32 76.20 - 76.60 Ours 74.02\u00b10.12 79.37\u00b10.09 D.5 MIXED IMPRECISE LABEL LEARNING D.5.1 SETUP To create a mixture of various imprecise label configurations, we select CIFAR-10 and CIFAR-100 as base datasets. We first uniformly sample l/C labeled samples from each class to form the labeled dataset and treat the remaining samples as the unlabeled dataset. Based on the labeled dataset, we generate partially labeled datasets by flipping negative labels to false positive labels with the partial ratio q. After obtaining the partial labels, we randomly select \u03b7 percentage of samples from each class, and recreate the partial labels for them by flipping the ground truth label uniformly to another class. In this setting, unlabeled data, partially labeled data, and noisy labeled data exist simultaneously, which is very challenging and more closely resembles realistic situations. For CIFAR-10, we set l \u2208 {1000, 5000, 50000}, and for CIFAR-100, we set l \u2208 {5000, 10000, 50000}. Similarly in the partial label setting, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0.1, 0.2, 0.3} for both datasets. 28 Preprint D.5.2 RESULTS We provide a more complete version of Table 4 in Table 16. On partial noisy labels of CIFAR-10 with partial ratio 0.5 and of CIFAR-100 with partial ratio 0.1, most baseline methods are more robust or even fail to perform. However, our ILL still shows very robust performance with minor performance degradation as increase of noise ratios. Table 16: Test accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of {0.1, 0.3, 0.5} for CIFAR-10 and {0.01, 0.05, 0.1} for CIFAR-100, and noise ratio \u03b7 of {0.1, 0.2, 0.3} for CIFAR-10 and CIFAR-100. Dataset # Labels Partial Ratio q Noise Ratio \u03b7 0 0.1 0.2 0.1 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.99\u00b10.03 - - - - 96.55\u00b10.08 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 CIFAR-10 50,000 0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.73\u00b10.10 - - - - 96.52\u00b10.12 92.32 92.81 93.44"}, {"question": " What could potentially lead to improved performance of the method on CIFAR-100N?,answer: More realistic noise transition model and further method tuning", "ref_chunk": "of classes. For CIFAR-10 and CIFAR-100, we train PreAct-ResNet-18 with SGD using a learning rate of 0.02, a weight decay of 1e \u2212 3, and a momentum of 0.9. We train for 300 epochs with a cosine learning rate schedule and a batch size of 128. For WebVision, we use InceptionResNet-v2 as the backbone and set the batch size to 32. Other settings are similar to CIFAR-10. For Clothing1M, we use ImageNet-1K pre trained ResNet-50 as the backbone. We train it using SGD with an initial learning rate of 2e-3 for a total of 10 epochs, where the learning rate is reduced by 10 after 5 epochs. In addition, we also conduct experiments on CIFAR-10N and CIFAR-100N. We present the detailed hyper-parameters in Table 13. D.4.2 RESULTS In addition to the results regarding noisy label learning provided in the main paper, we also present comparison results on CIFAR-10N and CIFAR-100N (Wei et al., 2021) in Table 14. We include a full comparison on Clothing1M and WebVision, incorporating methods like Co-Teaching, Forward, and CORES, in Table 15. As shown in Table 14, the proposed ILL framework achieves performance comparable to the previous best method, SOP (Liu et al., 2022). On CIFAR-10N, our method yields results very close to SOP in the Random and Aggregate case noise scenarios and surpasses SOP in the 27 Preprint Table 13: Hyper-parameters for noisy label learning used in experiments. CIFAR-100 (CIFAR-100N) Hyper-parameter CIFAR-10 (CIFAR-10N) Clothing1M WebVision Image Size Model Batch Size Learning Rate Weight Decay LR Scheduler Training Epochs Classes Noisy Matrix Scale 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 10 1.0 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 100 2.0 224 ResNet-50 (ImageNet-1K Pretrained) 64 0.002 1e-3 MultiStep 10 14 0.5 299 Inception-ResNet-v2 32 0.02 5e-4 MultiStep 100 50 2.5 Worst case noise scenario. However, on CIFAR-100N, our method slightly underperforms previous methods, possibly due to the oversimplified noise model utilized in ILL. We believe that a more realistic noise transition model and further tuning of our method could lead to improved performance. Table 14: Test accuracy comparison of instance independent label noise on CIFAR-10N and CIFAR- 100N for noisy label learning. The best results are indicated in bold, and the second best results are indicated in underline. Our results are averaged over three independent runs with ResNet34 as the backbone. Dataset CIFAR-10N CIFAR-100N Noisy Type Clean Random 1 Random 2 Random 3 Aggregate Worst Clean Noisy CE Forward (Patrini et al., 2016) Co-teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 92.92\u00b10.11 93.02\u00b10.12 93.35\u00b10.14 - 95.39\u00b10.05 94.16\u00b10.11 96.38\u00b10.31 85.02\u00b10.65 86.88\u00b10.50 90.33\u00b10.13 95.16\u00b10.19 94.43\u00b10.41 94.45\u00b10.14 95.28\u00b10.13 86.46\u00b11.79 86.14\u00b10.24 90.30\u00b10.17 95.23\u00b10.07 94.20\u00b10.24 94.88\u00b10.31 95.31\u00b10.10 85.16\u00b10.61 87.04\u00b10.35 90.15\u00b10.18 95.21\u00b10.14 94.34\u00b10.22 94.74\u00b10.03 95.39\u00b10.11 87.77\u00b10.38 88.24\u00b10.22 91.20\u00b10.13 95.01\u00b10.71 94.83\u00b10.10 95.25\u00b10.09 95.61\u00b10.13 77.69\u00b11.55 79.79\u00b10.46 83.83\u00b10.13 92.56\u00b10.42 91.09\u00b11.60 91.66\u00b10.09 93.24\u00b10.21 76.70\u00b10.74 76.18\u00b10.37 73.46\u00b10.09 - 78.57\u00b10.12 73.87\u00b10.16 78.91\u00b10.43 55.50\u00b10.66 57.01\u00b11.03 60.37\u00b10.27 71.13\u00b10.48 66.72\u00b10.07 55.72\u00b10.42 67.81\u00b10.23 Ours 96.21\u00b10.29 96.06\u00b10.07 95.98\u00b10.12 96.10\u00b10.05 96.40\u00b10.03 93.55\u00b10.14 78.53\u00b10.21 68.07\u00b10.33 Table 15: Test accuracy comparison of realistic noisy labels on Clothing1M and WebVision for noisy label learning. The best results are indicated in bold and the second best results are indicated in underline. Our results are averaged over 3 independent runs. For Clothing1M, we use ImageNet-1K pre trained ResNet50 as the backbone. For WebVision, InceptionResNetv2 is used as the backbone. Dataset Clothing1M WebVision CE Forward (Patrini et al., 2016) Co-Teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 69.10 69.80 69.20 74.76 72.90 73.20 73.50 61.10 63.60 77.32 76.20 - 76.60 Ours 74.02\u00b10.12 79.37\u00b10.09 D.5 MIXED IMPRECISE LABEL LEARNING D.5.1 SETUP To create a mixture of various imprecise label configurations, we select CIFAR-10 and CIFAR-100 as base datasets. We first uniformly sample l/C labeled samples from each class to form the labeled dataset and treat the remaining samples as the unlabeled dataset. Based on the labeled dataset, we generate partially labeled datasets by flipping negative labels to false positive labels with the partial ratio q. After obtaining the partial labels, we randomly select \u03b7 percentage of samples from each class, and recreate the partial labels for them by flipping the ground truth label uniformly to another class. In this setting, unlabeled data, partially labeled data, and noisy labeled data exist simultaneously, which is very challenging and more closely resembles realistic situations. For CIFAR-10, we set l \u2208 {1000, 5000, 50000}, and for CIFAR-100, we set l \u2208 {5000, 10000, 50000}. Similarly in the partial label setting, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0.1, 0.2, 0.3} for both datasets. 28 Preprint D.5.2 RESULTS We provide a more complete version of Table 4 in Table 16. On partial noisy labels of CIFAR-10 with partial ratio 0.5 and of CIFAR-100 with partial ratio 0.1, most baseline methods are more robust or even fail to perform. However, our ILL still shows very robust performance with minor performance degradation as increase of noise ratios. Table 16: Test accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of {0.1, 0.3, 0.5} for CIFAR-10 and {0.01, 0.05, 0.1} for CIFAR-100, and noise ratio \u03b7 of {0.1, 0.2, 0.3} for CIFAR-10 and CIFAR-100. Dataset # Labels Partial Ratio q Noise Ratio \u03b7 0 0.1 0.2 0.1 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.99\u00b10.03 - - - - 96.55\u00b10.08 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 CIFAR-10 50,000 0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.73\u00b10.10 - - - - 96.52\u00b10.12 92.32 92.81 93.44"}, {"question": " In the noise matrix scale of 1.0, what model and backbone are used with a batch size of 32?,answer: PreAct-ResNet-18 (ResNet-34)", "ref_chunk": "of classes. For CIFAR-10 and CIFAR-100, we train PreAct-ResNet-18 with SGD using a learning rate of 0.02, a weight decay of 1e \u2212 3, and a momentum of 0.9. We train for 300 epochs with a cosine learning rate schedule and a batch size of 128. For WebVision, we use InceptionResNet-v2 as the backbone and set the batch size to 32. Other settings are similar to CIFAR-10. For Clothing1M, we use ImageNet-1K pre trained ResNet-50 as the backbone. We train it using SGD with an initial learning rate of 2e-3 for a total of 10 epochs, where the learning rate is reduced by 10 after 5 epochs. In addition, we also conduct experiments on CIFAR-10N and CIFAR-100N. We present the detailed hyper-parameters in Table 13. D.4.2 RESULTS In addition to the results regarding noisy label learning provided in the main paper, we also present comparison results on CIFAR-10N and CIFAR-100N (Wei et al., 2021) in Table 14. We include a full comparison on Clothing1M and WebVision, incorporating methods like Co-Teaching, Forward, and CORES, in Table 15. As shown in Table 14, the proposed ILL framework achieves performance comparable to the previous best method, SOP (Liu et al., 2022). On CIFAR-10N, our method yields results very close to SOP in the Random and Aggregate case noise scenarios and surpasses SOP in the 27 Preprint Table 13: Hyper-parameters for noisy label learning used in experiments. CIFAR-100 (CIFAR-100N) Hyper-parameter CIFAR-10 (CIFAR-10N) Clothing1M WebVision Image Size Model Batch Size Learning Rate Weight Decay LR Scheduler Training Epochs Classes Noisy Matrix Scale 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 10 1.0 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 100 2.0 224 ResNet-50 (ImageNet-1K Pretrained) 64 0.002 1e-3 MultiStep 10 14 0.5 299 Inception-ResNet-v2 32 0.02 5e-4 MultiStep 100 50 2.5 Worst case noise scenario. However, on CIFAR-100N, our method slightly underperforms previous methods, possibly due to the oversimplified noise model utilized in ILL. We believe that a more realistic noise transition model and further tuning of our method could lead to improved performance. Table 14: Test accuracy comparison of instance independent label noise on CIFAR-10N and CIFAR- 100N for noisy label learning. The best results are indicated in bold, and the second best results are indicated in underline. Our results are averaged over three independent runs with ResNet34 as the backbone. Dataset CIFAR-10N CIFAR-100N Noisy Type Clean Random 1 Random 2 Random 3 Aggregate Worst Clean Noisy CE Forward (Patrini et al., 2016) Co-teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 92.92\u00b10.11 93.02\u00b10.12 93.35\u00b10.14 - 95.39\u00b10.05 94.16\u00b10.11 96.38\u00b10.31 85.02\u00b10.65 86.88\u00b10.50 90.33\u00b10.13 95.16\u00b10.19 94.43\u00b10.41 94.45\u00b10.14 95.28\u00b10.13 86.46\u00b11.79 86.14\u00b10.24 90.30\u00b10.17 95.23\u00b10.07 94.20\u00b10.24 94.88\u00b10.31 95.31\u00b10.10 85.16\u00b10.61 87.04\u00b10.35 90.15\u00b10.18 95.21\u00b10.14 94.34\u00b10.22 94.74\u00b10.03 95.39\u00b10.11 87.77\u00b10.38 88.24\u00b10.22 91.20\u00b10.13 95.01\u00b10.71 94.83\u00b10.10 95.25\u00b10.09 95.61\u00b10.13 77.69\u00b11.55 79.79\u00b10.46 83.83\u00b10.13 92.56\u00b10.42 91.09\u00b11.60 91.66\u00b10.09 93.24\u00b10.21 76.70\u00b10.74 76.18\u00b10.37 73.46\u00b10.09 - 78.57\u00b10.12 73.87\u00b10.16 78.91\u00b10.43 55.50\u00b10.66 57.01\u00b11.03 60.37\u00b10.27 71.13\u00b10.48 66.72\u00b10.07 55.72\u00b10.42 67.81\u00b10.23 Ours 96.21\u00b10.29 96.06\u00b10.07 95.98\u00b10.12 96.10\u00b10.05 96.40\u00b10.03 93.55\u00b10.14 78.53\u00b10.21 68.07\u00b10.33 Table 15: Test accuracy comparison of realistic noisy labels on Clothing1M and WebVision for noisy label learning. The best results are indicated in bold and the second best results are indicated in underline. Our results are averaged over 3 independent runs. For Clothing1M, we use ImageNet-1K pre trained ResNet50 as the backbone. For WebVision, InceptionResNetv2 is used as the backbone. Dataset Clothing1M WebVision CE Forward (Patrini et al., 2016) Co-Teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 69.10 69.80 69.20 74.76 72.90 73.20 73.50 61.10 63.60 77.32 76.20 - 76.60 Ours 74.02\u00b10.12 79.37\u00b10.09 D.5 MIXED IMPRECISE LABEL LEARNING D.5.1 SETUP To create a mixture of various imprecise label configurations, we select CIFAR-10 and CIFAR-100 as base datasets. We first uniformly sample l/C labeled samples from each class to form the labeled dataset and treat the remaining samples as the unlabeled dataset. Based on the labeled dataset, we generate partially labeled datasets by flipping negative labels to false positive labels with the partial ratio q. After obtaining the partial labels, we randomly select \u03b7 percentage of samples from each class, and recreate the partial labels for them by flipping the ground truth label uniformly to another class. In this setting, unlabeled data, partially labeled data, and noisy labeled data exist simultaneously, which is very challenging and more closely resembles realistic situations. For CIFAR-10, we set l \u2208 {1000, 5000, 50000}, and for CIFAR-100, we set l \u2208 {5000, 10000, 50000}. Similarly in the partial label setting, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0.1, 0.2, 0.3} for both datasets. 28 Preprint D.5.2 RESULTS We provide a more complete version of Table 4 in Table 16. On partial noisy labels of CIFAR-10 with partial ratio 0.5 and of CIFAR-100 with partial ratio 0.1, most baseline methods are more robust or even fail to perform. However, our ILL still shows very robust performance with minor performance degradation as increase of noise ratios. Table 16: Test accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of {0.1, 0.3, 0.5} for CIFAR-10 and {0.01, 0.05, 0.1} for CIFAR-100, and noise ratio \u03b7 of {0.1, 0.2, 0.3} for CIFAR-10 and CIFAR-100. Dataset # Labels Partial Ratio q Noise Ratio \u03b7 0 0.1 0.2 0.1 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.99\u00b10.03 - - - - 96.55\u00b10.08 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 CIFAR-10 50,000 0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.73\u00b10.10 - - - - 96.52\u00b10.12 92.32 92.81 93.44"}, {"question": " What dataset and noisy type achieves the best result for SOP in Table 14?,answer: CIFAR-100N, Clean", "ref_chunk": "of classes. For CIFAR-10 and CIFAR-100, we train PreAct-ResNet-18 with SGD using a learning rate of 0.02, a weight decay of 1e \u2212 3, and a momentum of 0.9. We train for 300 epochs with a cosine learning rate schedule and a batch size of 128. For WebVision, we use InceptionResNet-v2 as the backbone and set the batch size to 32. Other settings are similar to CIFAR-10. For Clothing1M, we use ImageNet-1K pre trained ResNet-50 as the backbone. We train it using SGD with an initial learning rate of 2e-3 for a total of 10 epochs, where the learning rate is reduced by 10 after 5 epochs. In addition, we also conduct experiments on CIFAR-10N and CIFAR-100N. We present the detailed hyper-parameters in Table 13. D.4.2 RESULTS In addition to the results regarding noisy label learning provided in the main paper, we also present comparison results on CIFAR-10N and CIFAR-100N (Wei et al., 2021) in Table 14. We include a full comparison on Clothing1M and WebVision, incorporating methods like Co-Teaching, Forward, and CORES, in Table 15. As shown in Table 14, the proposed ILL framework achieves performance comparable to the previous best method, SOP (Liu et al., 2022). On CIFAR-10N, our method yields results very close to SOP in the Random and Aggregate case noise scenarios and surpasses SOP in the 27 Preprint Table 13: Hyper-parameters for noisy label learning used in experiments. CIFAR-100 (CIFAR-100N) Hyper-parameter CIFAR-10 (CIFAR-10N) Clothing1M WebVision Image Size Model Batch Size Learning Rate Weight Decay LR Scheduler Training Epochs Classes Noisy Matrix Scale 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 10 1.0 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 100 2.0 224 ResNet-50 (ImageNet-1K Pretrained) 64 0.002 1e-3 MultiStep 10 14 0.5 299 Inception-ResNet-v2 32 0.02 5e-4 MultiStep 100 50 2.5 Worst case noise scenario. However, on CIFAR-100N, our method slightly underperforms previous methods, possibly due to the oversimplified noise model utilized in ILL. We believe that a more realistic noise transition model and further tuning of our method could lead to improved performance. Table 14: Test accuracy comparison of instance independent label noise on CIFAR-10N and CIFAR- 100N for noisy label learning. The best results are indicated in bold, and the second best results are indicated in underline. Our results are averaged over three independent runs with ResNet34 as the backbone. Dataset CIFAR-10N CIFAR-100N Noisy Type Clean Random 1 Random 2 Random 3 Aggregate Worst Clean Noisy CE Forward (Patrini et al., 2016) Co-teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 92.92\u00b10.11 93.02\u00b10.12 93.35\u00b10.14 - 95.39\u00b10.05 94.16\u00b10.11 96.38\u00b10.31 85.02\u00b10.65 86.88\u00b10.50 90.33\u00b10.13 95.16\u00b10.19 94.43\u00b10.41 94.45\u00b10.14 95.28\u00b10.13 86.46\u00b11.79 86.14\u00b10.24 90.30\u00b10.17 95.23\u00b10.07 94.20\u00b10.24 94.88\u00b10.31 95.31\u00b10.10 85.16\u00b10.61 87.04\u00b10.35 90.15\u00b10.18 95.21\u00b10.14 94.34\u00b10.22 94.74\u00b10.03 95.39\u00b10.11 87.77\u00b10.38 88.24\u00b10.22 91.20\u00b10.13 95.01\u00b10.71 94.83\u00b10.10 95.25\u00b10.09 95.61\u00b10.13 77.69\u00b11.55 79.79\u00b10.46 83.83\u00b10.13 92.56\u00b10.42 91.09\u00b11.60 91.66\u00b10.09 93.24\u00b10.21 76.70\u00b10.74 76.18\u00b10.37 73.46\u00b10.09 - 78.57\u00b10.12 73.87\u00b10.16 78.91\u00b10.43 55.50\u00b10.66 57.01\u00b11.03 60.37\u00b10.27 71.13\u00b10.48 66.72\u00b10.07 55.72\u00b10.42 67.81\u00b10.23 Ours 96.21\u00b10.29 96.06\u00b10.07 95.98\u00b10.12 96.10\u00b10.05 96.40\u00b10.03 93.55\u00b10.14 78.53\u00b10.21 68.07\u00b10.33 Table 15: Test accuracy comparison of realistic noisy labels on Clothing1M and WebVision for noisy label learning. The best results are indicated in bold and the second best results are indicated in underline. Our results are averaged over 3 independent runs. For Clothing1M, we use ImageNet-1K pre trained ResNet50 as the backbone. For WebVision, InceptionResNetv2 is used as the backbone. Dataset Clothing1M WebVision CE Forward (Patrini et al., 2016) Co-Teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 69.10 69.80 69.20 74.76 72.90 73.20 73.50 61.10 63.60 77.32 76.20 - 76.60 Ours 74.02\u00b10.12 79.37\u00b10.09 D.5 MIXED IMPRECISE LABEL LEARNING D.5.1 SETUP To create a mixture of various imprecise label configurations, we select CIFAR-10 and CIFAR-100 as base datasets. We first uniformly sample l/C labeled samples from each class to form the labeled dataset and treat the remaining samples as the unlabeled dataset. Based on the labeled dataset, we generate partially labeled datasets by flipping negative labels to false positive labels with the partial ratio q. After obtaining the partial labels, we randomly select \u03b7 percentage of samples from each class, and recreate the partial labels for them by flipping the ground truth label uniformly to another class. In this setting, unlabeled data, partially labeled data, and noisy labeled data exist simultaneously, which is very challenging and more closely resembles realistic situations. For CIFAR-10, we set l \u2208 {1000, 5000, 50000}, and for CIFAR-100, we set l \u2208 {5000, 10000, 50000}. Similarly in the partial label setting, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0.1, 0.2, 0.3} for both datasets. 28 Preprint D.5.2 RESULTS We provide a more complete version of Table 4 in Table 16. On partial noisy labels of CIFAR-10 with partial ratio 0.5 and of CIFAR-100 with partial ratio 0.1, most baseline methods are more robust or even fail to perform. However, our ILL still shows very robust performance with minor performance degradation as increase of noise ratios. Table 16: Test accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of {0.1, 0.3, 0.5} for CIFAR-10 and {0.01, 0.05, 0.1} for CIFAR-100, and noise ratio \u03b7 of {0.1, 0.2, 0.3} for CIFAR-10 and CIFAR-100. Dataset # Labels Partial Ratio q Noise Ratio \u03b7 0 0.1 0.2 0.1 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.99\u00b10.03 - - - - 96.55\u00b10.08 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 CIFAR-10 50,000 0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.73\u00b10.10 - - - - 96.52\u00b10.12 92.32 92.81 93.44"}, {"question": " How many independent runs were the results averaged over in Table 14 for the proposed method?,answer: Three", "ref_chunk": "of classes. For CIFAR-10 and CIFAR-100, we train PreAct-ResNet-18 with SGD using a learning rate of 0.02, a weight decay of 1e \u2212 3, and a momentum of 0.9. We train for 300 epochs with a cosine learning rate schedule and a batch size of 128. For WebVision, we use InceptionResNet-v2 as the backbone and set the batch size to 32. Other settings are similar to CIFAR-10. For Clothing1M, we use ImageNet-1K pre trained ResNet-50 as the backbone. We train it using SGD with an initial learning rate of 2e-3 for a total of 10 epochs, where the learning rate is reduced by 10 after 5 epochs. In addition, we also conduct experiments on CIFAR-10N and CIFAR-100N. We present the detailed hyper-parameters in Table 13. D.4.2 RESULTS In addition to the results regarding noisy label learning provided in the main paper, we also present comparison results on CIFAR-10N and CIFAR-100N (Wei et al., 2021) in Table 14. We include a full comparison on Clothing1M and WebVision, incorporating methods like Co-Teaching, Forward, and CORES, in Table 15. As shown in Table 14, the proposed ILL framework achieves performance comparable to the previous best method, SOP (Liu et al., 2022). On CIFAR-10N, our method yields results very close to SOP in the Random and Aggregate case noise scenarios and surpasses SOP in the 27 Preprint Table 13: Hyper-parameters for noisy label learning used in experiments. CIFAR-100 (CIFAR-100N) Hyper-parameter CIFAR-10 (CIFAR-10N) Clothing1M WebVision Image Size Model Batch Size Learning Rate Weight Decay LR Scheduler Training Epochs Classes Noisy Matrix Scale 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 10 1.0 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 100 2.0 224 ResNet-50 (ImageNet-1K Pretrained) 64 0.002 1e-3 MultiStep 10 14 0.5 299 Inception-ResNet-v2 32 0.02 5e-4 MultiStep 100 50 2.5 Worst case noise scenario. However, on CIFAR-100N, our method slightly underperforms previous methods, possibly due to the oversimplified noise model utilized in ILL. We believe that a more realistic noise transition model and further tuning of our method could lead to improved performance. Table 14: Test accuracy comparison of instance independent label noise on CIFAR-10N and CIFAR- 100N for noisy label learning. The best results are indicated in bold, and the second best results are indicated in underline. Our results are averaged over three independent runs with ResNet34 as the backbone. Dataset CIFAR-10N CIFAR-100N Noisy Type Clean Random 1 Random 2 Random 3 Aggregate Worst Clean Noisy CE Forward (Patrini et al., 2016) Co-teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 92.92\u00b10.11 93.02\u00b10.12 93.35\u00b10.14 - 95.39\u00b10.05 94.16\u00b10.11 96.38\u00b10.31 85.02\u00b10.65 86.88\u00b10.50 90.33\u00b10.13 95.16\u00b10.19 94.43\u00b10.41 94.45\u00b10.14 95.28\u00b10.13 86.46\u00b11.79 86.14\u00b10.24 90.30\u00b10.17 95.23\u00b10.07 94.20\u00b10.24 94.88\u00b10.31 95.31\u00b10.10 85.16\u00b10.61 87.04\u00b10.35 90.15\u00b10.18 95.21\u00b10.14 94.34\u00b10.22 94.74\u00b10.03 95.39\u00b10.11 87.77\u00b10.38 88.24\u00b10.22 91.20\u00b10.13 95.01\u00b10.71 94.83\u00b10.10 95.25\u00b10.09 95.61\u00b10.13 77.69\u00b11.55 79.79\u00b10.46 83.83\u00b10.13 92.56\u00b10.42 91.09\u00b11.60 91.66\u00b10.09 93.24\u00b10.21 76.70\u00b10.74 76.18\u00b10.37 73.46\u00b10.09 - 78.57\u00b10.12 73.87\u00b10.16 78.91\u00b10.43 55.50\u00b10.66 57.01\u00b11.03 60.37\u00b10.27 71.13\u00b10.48 66.72\u00b10.07 55.72\u00b10.42 67.81\u00b10.23 Ours 96.21\u00b10.29 96.06\u00b10.07 95.98\u00b10.12 96.10\u00b10.05 96.40\u00b10.03 93.55\u00b10.14 78.53\u00b10.21 68.07\u00b10.33 Table 15: Test accuracy comparison of realistic noisy labels on Clothing1M and WebVision for noisy label learning. The best results are indicated in bold and the second best results are indicated in underline. Our results are averaged over 3 independent runs. For Clothing1M, we use ImageNet-1K pre trained ResNet50 as the backbone. For WebVision, InceptionResNetv2 is used as the backbone. Dataset Clothing1M WebVision CE Forward (Patrini et al., 2016) Co-Teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 69.10 69.80 69.20 74.76 72.90 73.20 73.50 61.10 63.60 77.32 76.20 - 76.60 Ours 74.02\u00b10.12 79.37\u00b10.09 D.5 MIXED IMPRECISE LABEL LEARNING D.5.1 SETUP To create a mixture of various imprecise label configurations, we select CIFAR-10 and CIFAR-100 as base datasets. We first uniformly sample l/C labeled samples from each class to form the labeled dataset and treat the remaining samples as the unlabeled dataset. Based on the labeled dataset, we generate partially labeled datasets by flipping negative labels to false positive labels with the partial ratio q. After obtaining the partial labels, we randomly select \u03b7 percentage of samples from each class, and recreate the partial labels for them by flipping the ground truth label uniformly to another class. In this setting, unlabeled data, partially labeled data, and noisy labeled data exist simultaneously, which is very challenging and more closely resembles realistic situations. For CIFAR-10, we set l \u2208 {1000, 5000, 50000}, and for CIFAR-100, we set l \u2208 {5000, 10000, 50000}. Similarly in the partial label setting, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0.1, 0.2, 0.3} for both datasets. 28 Preprint D.5.2 RESULTS We provide a more complete version of Table 4 in Table 16. On partial noisy labels of CIFAR-10 with partial ratio 0.5 and of CIFAR-100 with partial ratio 0.1, most baseline methods are more robust or even fail to perform. However, our ILL still shows very robust performance with minor performance degradation as increase of noise ratios. Table 16: Test accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of {0.1, 0.3, 0.5} for CIFAR-10 and {0.01, 0.05, 0.1} for CIFAR-100, and noise ratio \u03b7 of {0.1, 0.2, 0.3} for CIFAR-10 and CIFAR-100. Dataset # Labels Partial Ratio q Noise Ratio \u03b7 0 0.1 0.2 0.1 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.99\u00b10.03 - - - - 96.55\u00b10.08 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 CIFAR-10 50,000 0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.73\u00b10.10 - - - - 96.52\u00b10.12 92.32 92.81 93.44"}], "doc_text": "of classes. For CIFAR-10 and CIFAR-100, we train PreAct-ResNet-18 with SGD using a learning rate of 0.02, a weight decay of 1e \u2212 3, and a momentum of 0.9. We train for 300 epochs with a cosine learning rate schedule and a batch size of 128. For WebVision, we use InceptionResNet-v2 as the backbone and set the batch size to 32. Other settings are similar to CIFAR-10. For Clothing1M, we use ImageNet-1K pre trained ResNet-50 as the backbone. We train it using SGD with an initial learning rate of 2e-3 for a total of 10 epochs, where the learning rate is reduced by 10 after 5 epochs. In addition, we also conduct experiments on CIFAR-10N and CIFAR-100N. We present the detailed hyper-parameters in Table 13. D.4.2 RESULTS In addition to the results regarding noisy label learning provided in the main paper, we also present comparison results on CIFAR-10N and CIFAR-100N (Wei et al., 2021) in Table 14. We include a full comparison on Clothing1M and WebVision, incorporating methods like Co-Teaching, Forward, and CORES, in Table 15. As shown in Table 14, the proposed ILL framework achieves performance comparable to the previous best method, SOP (Liu et al., 2022). On CIFAR-10N, our method yields results very close to SOP in the Random and Aggregate case noise scenarios and surpasses SOP in the 27 Preprint Table 13: Hyper-parameters for noisy label learning used in experiments. CIFAR-100 (CIFAR-100N) Hyper-parameter CIFAR-10 (CIFAR-10N) Clothing1M WebVision Image Size Model Batch Size Learning Rate Weight Decay LR Scheduler Training Epochs Classes Noisy Matrix Scale 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 10 1.0 32 PreAct-ResNet-18 (ResNet-34) 128 0.02 1e-3 Cosine 300 100 2.0 224 ResNet-50 (ImageNet-1K Pretrained) 64 0.002 1e-3 MultiStep 10 14 0.5 299 Inception-ResNet-v2 32 0.02 5e-4 MultiStep 100 50 2.5 Worst case noise scenario. However, on CIFAR-100N, our method slightly underperforms previous methods, possibly due to the oversimplified noise model utilized in ILL. We believe that a more realistic noise transition model and further tuning of our method could lead to improved performance. Table 14: Test accuracy comparison of instance independent label noise on CIFAR-10N and CIFAR- 100N for noisy label learning. The best results are indicated in bold, and the second best results are indicated in underline. Our results are averaged over three independent runs with ResNet34 as the backbone. Dataset CIFAR-10N CIFAR-100N Noisy Type Clean Random 1 Random 2 Random 3 Aggregate Worst Clean Noisy CE Forward (Patrini et al., 2016) Co-teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 92.92\u00b10.11 93.02\u00b10.12 93.35\u00b10.14 - 95.39\u00b10.05 94.16\u00b10.11 96.38\u00b10.31 85.02\u00b10.65 86.88\u00b10.50 90.33\u00b10.13 95.16\u00b10.19 94.43\u00b10.41 94.45\u00b10.14 95.28\u00b10.13 86.46\u00b11.79 86.14\u00b10.24 90.30\u00b10.17 95.23\u00b10.07 94.20\u00b10.24 94.88\u00b10.31 95.31\u00b10.10 85.16\u00b10.61 87.04\u00b10.35 90.15\u00b10.18 95.21\u00b10.14 94.34\u00b10.22 94.74\u00b10.03 95.39\u00b10.11 87.77\u00b10.38 88.24\u00b10.22 91.20\u00b10.13 95.01\u00b10.71 94.83\u00b10.10 95.25\u00b10.09 95.61\u00b10.13 77.69\u00b11.55 79.79\u00b10.46 83.83\u00b10.13 92.56\u00b10.42 91.09\u00b11.60 91.66\u00b10.09 93.24\u00b10.21 76.70\u00b10.74 76.18\u00b10.37 73.46\u00b10.09 - 78.57\u00b10.12 73.87\u00b10.16 78.91\u00b10.43 55.50\u00b10.66 57.01\u00b11.03 60.37\u00b10.27 71.13\u00b10.48 66.72\u00b10.07 55.72\u00b10.42 67.81\u00b10.23 Ours 96.21\u00b10.29 96.06\u00b10.07 95.98\u00b10.12 96.10\u00b10.05 96.40\u00b10.03 93.55\u00b10.14 78.53\u00b10.21 68.07\u00b10.33 Table 15: Test accuracy comparison of realistic noisy labels on Clothing1M and WebVision for noisy label learning. The best results are indicated in bold and the second best results are indicated in underline. Our results are averaged over 3 independent runs. For Clothing1M, we use ImageNet-1K pre trained ResNet50 as the backbone. For WebVision, InceptionResNetv2 is used as the backbone. Dataset Clothing1M WebVision CE Forward (Patrini et al., 2016) Co-Teaching (Han et al., 2018) DivideMix (Li et al., 2020) ELR (Liu et al., 2020) CORES (Cheng et al., 2020) SOP (Liu et al., 2022) 69.10 69.80 69.20 74.76 72.90 73.20 73.50 61.10 63.60 77.32 76.20 - 76.60 Ours 74.02\u00b10.12 79.37\u00b10.09 D.5 MIXED IMPRECISE LABEL LEARNING D.5.1 SETUP To create a mixture of various imprecise label configurations, we select CIFAR-10 and CIFAR-100 as base datasets. We first uniformly sample l/C labeled samples from each class to form the labeled dataset and treat the remaining samples as the unlabeled dataset. Based on the labeled dataset, we generate partially labeled datasets by flipping negative labels to false positive labels with the partial ratio q. After obtaining the partial labels, we randomly select \u03b7 percentage of samples from each class, and recreate the partial labels for them by flipping the ground truth label uniformly to another class. In this setting, unlabeled data, partially labeled data, and noisy labeled data exist simultaneously, which is very challenging and more closely resembles realistic situations. For CIFAR-10, we set l \u2208 {1000, 5000, 50000}, and for CIFAR-100, we set l \u2208 {5000, 10000, 50000}. Similarly in the partial label setting, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0.1, 0.2, 0.3} for both datasets. 28 Preprint D.5.2 RESULTS We provide a more complete version of Table 4 in Table 16. On partial noisy labels of CIFAR-10 with partial ratio 0.5 and of CIFAR-100 with partial ratio 0.1, most baseline methods are more robust or even fail to perform. However, our ILL still shows very robust performance with minor performance degradation as increase of noise ratios. Table 16: Test accuracy comparison of mixture of different imprecise labels. We report results of full labels, partial ratio q of {0.1, 0.3, 0.5} for CIFAR-10 and {0.01, 0.05, 0.1} for CIFAR-100, and noise ratio \u03b7 of {0.1, 0.2, 0.3} for CIFAR-10 and CIFAR-100. Dataset # Labels Partial Ratio q Noise Ratio \u03b7 0 0.1 0.2 0.1 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.99\u00b10.03 - - - - 96.55\u00b10.08 93.64 93.44 94.15 94.58 95.83 96.47\u00b10.11 93.13 92.57 94.04 94.74 95.86 96.09\u00b10.20 CIFAR-10 50,000 0.3 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ w/ Mixup (Xu et al., 2023) DALI w/ Mixup (Xu et al., 2023) Ours 95.73\u00b10.10 - - - - 96.52\u00b10.12 92.32 92.81 93.44"}