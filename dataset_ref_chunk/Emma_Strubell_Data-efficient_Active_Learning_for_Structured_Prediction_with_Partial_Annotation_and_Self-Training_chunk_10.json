{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Data-efficient_Active_Learning_for_Structured_Prediction_with_Partial_Annotation_and_Self-Training_chunk_10.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the two tasks addressed in the text?", "answer": " Event extraction (EE) and relation extraction (RE)", "ref_chunk": "gold parse tree as the train- ing loss in FA and marginalized likelihood in PA (Li et al., 2016). A.2 IE Tasks. We tackle event extraction (EE) and rela- tion extraction (RE) using a two-step pipelined approach. The first step aims to extract entity mentions for RE, and entity mentions and event triggers for EE. We adopt sequence labeling for mention extractions as in the NER task. Based on the mentions extracted in the first step, the second step examines each feasible candidate mention pair (entity pair for RE and event-entity pair for EE) and decides the relation (entity relation for RE and event argument relation for EE) for them. Since event argument links can be regarded as relations between event triggers and entities, for simplicity we will use the relational sub-task to refer to both relation and argument extraction. Model. We adopt a multi-task model similar to the one utilized in (Rotman and Reichart, 2022). With a pre-trained encoder, we take the first N layers as the shared encoding module whose out- put representations are used for both sub-tasks. Each sub-task further adopts a private encoder that is initialized with the remaining pre-trained layers and is trained with task-specific signals. We simply set N to 6, while the results are gen- erally not sensitive to this hyper-parameter. Final task-specific predictors are further stacked upon the corresponding private encoders. We adopt a CRF layer for mention extraction and a pairwise local predictor with a biaffine scorer for relation or argument extraction. Sentence selection. For an unlabeled sentence, there is an uncertainty score for each sub-task. For mentions, the uncertainty is the average mar- gin as in the NER task. For relations, we find that averaging uncertainties over all mention pairs has a bias towards sentences with fewer mentions. To mitigate such bias, we first aggregate an un- certainty score for each mention by taking the maximum score within all the relations that link to it and then averaging over all the mentions for sentence-level scores. Finally, the scores of the two sub-tasks are linearly combined to form the sentence-level uncertainty. Partial selection. For PA selection, the two sub- tasks are handled separately according to the adaptive ratio scheme. We further adopt two heuristics for the relational task to compensate for errors in the mention extraction. First, since there can be over-predicted mentions that lead to discarded relation queries, we adjust the PA ratio by estimating how many candidate rela- tions contain such errors in the mentions. We again train a logistic regression model to pre- dict whether a token is NIL (or \u2018O\u2019 in the BIO scheme, meaning not contained inside any gold mentions) based on its NIL probability. Then for each candidate relation, we calculate the proba- bility that any token within its mentions is NIL. By averaging this probability of all the candi- dates, we obtain a rough estimation of the per- centage of problematic relations, which we call it \u03b1. Finally the PA selection ratio is adjusted by: radjust = \u03b1\u00b7rproblem +(1\u2212\u03b1)\u00b7rorigin. Here, rorigin denotes the original selection ratio obtained from the adaptive scheme, and rproblem denotes the se- lection ratio of problematic relations, which we conservatively set to 1. Secondly, since there can also be under-predicted mentions, we add a Data Split #Sent. #Token #Event #Entity #Argument #Relation CoNLL03 train dev test 14.0K 203.6K 51.4K 3.3K 46.4K 3.5K - - 23.5K 5.9K 5.6K - - - - UD-EWT train dev test 12.5K 204.6K 25.1K 2.0K 25.1K 2.1K - - - - - - - - ACE05 train dev test 14.4K 215.2K 34.5K 2.5K 61.5K 4.0K 3.7K 0.5K 1.1K 38.0K 6.0K 10.8K 5.7K 0.7K 1.7K 6.2K 0.8K 1.7K Table 1: Data statistics. second stage of querying and annotation in each AL cycle based on the annotated mentions in the first stage. This extra stage only selects rela- tions that involve the newly added or corrected mentions. We simply reuse the selection ratio determined from the first stage and apply it to each sentence that contains such mentions. In this way, the second stage is lightweight and only requires relatively cheap re-inference for each queried sentence individually. Annotation. The annotation of the mentions is the same as in the NER task, while for the annota- tion of relational queries, their mentions are first examined and corrected if needed, as explained in \u00a73.4. We measure the labeling cost by the fi- nal annotated items; thus, these extra examined mentioned will also be properly counted. Model learning. For the mention extraction sub- task, the training objective is the same as in NER. For the relational sub-task, we simply adopt a local pairwise model with the standard cross- entropy loss. Since the relation model is local, no special treatment is needed for PA. B Data Statistics and More Settings Data. Our main experiments are conducted us- ing the CoNLL-2003 English dataset11 (Tjong Kim Sang and De Meulder, 2003) for NER, the English Web Treebank (EWT) from Universal De- pendencies12 v2.10 (Nivre et al., 2020) for DPAR, and English portion of ACE200513 (Walker et al., 2006) for IE. We utilize Stanza14 (Qi et al., 2020) to assign POS tags for cost measurement in NER and mention tasks. We follow Lin et al. (2020) for the pre-processing15 of the ACE dataset. For the IE tasks on ACE, we find that the conventional test set contains only newswire documents while the training set consists of various genres (such as from conversation and web). Such mismatches between the AL pool and the final testing set are nontriv- ial to handle with the classical AL protocol, and we thus randomly re-split the ACE dataset (with a ratio of 7:1:2 for training, dev, and test sets, respec- tively). Table 1 shows data statistics. For each AL experiment, we take the original training set as the unlabeled pool, down-sample a dev set from the original dev set, and evaluate on the full test set. More Settings. All of our models are based on the pre-trained RoBERTabase as the"}, {"question": " How is the first step in the two-step pipelined approach described?", "answer": " The first step aims to extract entity mentions for RE, and entity mentions and event triggers for EE.", "ref_chunk": "gold parse tree as the train- ing loss in FA and marginalized likelihood in PA (Li et al., 2016). A.2 IE Tasks. We tackle event extraction (EE) and rela- tion extraction (RE) using a two-step pipelined approach. The first step aims to extract entity mentions for RE, and entity mentions and event triggers for EE. We adopt sequence labeling for mention extractions as in the NER task. Based on the mentions extracted in the first step, the second step examines each feasible candidate mention pair (entity pair for RE and event-entity pair for EE) and decides the relation (entity relation for RE and event argument relation for EE) for them. Since event argument links can be regarded as relations between event triggers and entities, for simplicity we will use the relational sub-task to refer to both relation and argument extraction. Model. We adopt a multi-task model similar to the one utilized in (Rotman and Reichart, 2022). With a pre-trained encoder, we take the first N layers as the shared encoding module whose out- put representations are used for both sub-tasks. Each sub-task further adopts a private encoder that is initialized with the remaining pre-trained layers and is trained with task-specific signals. We simply set N to 6, while the results are gen- erally not sensitive to this hyper-parameter. Final task-specific predictors are further stacked upon the corresponding private encoders. We adopt a CRF layer for mention extraction and a pairwise local predictor with a biaffine scorer for relation or argument extraction. Sentence selection. For an unlabeled sentence, there is an uncertainty score for each sub-task. For mentions, the uncertainty is the average mar- gin as in the NER task. For relations, we find that averaging uncertainties over all mention pairs has a bias towards sentences with fewer mentions. To mitigate such bias, we first aggregate an un- certainty score for each mention by taking the maximum score within all the relations that link to it and then averaging over all the mentions for sentence-level scores. Finally, the scores of the two sub-tasks are linearly combined to form the sentence-level uncertainty. Partial selection. For PA selection, the two sub- tasks are handled separately according to the adaptive ratio scheme. We further adopt two heuristics for the relational task to compensate for errors in the mention extraction. First, since there can be over-predicted mentions that lead to discarded relation queries, we adjust the PA ratio by estimating how many candidate rela- tions contain such errors in the mentions. We again train a logistic regression model to pre- dict whether a token is NIL (or \u2018O\u2019 in the BIO scheme, meaning not contained inside any gold mentions) based on its NIL probability. Then for each candidate relation, we calculate the proba- bility that any token within its mentions is NIL. By averaging this probability of all the candi- dates, we obtain a rough estimation of the per- centage of problematic relations, which we call it \u03b1. Finally the PA selection ratio is adjusted by: radjust = \u03b1\u00b7rproblem +(1\u2212\u03b1)\u00b7rorigin. Here, rorigin denotes the original selection ratio obtained from the adaptive scheme, and rproblem denotes the se- lection ratio of problematic relations, which we conservatively set to 1. Secondly, since there can also be under-predicted mentions, we add a Data Split #Sent. #Token #Event #Entity #Argument #Relation CoNLL03 train dev test 14.0K 203.6K 51.4K 3.3K 46.4K 3.5K - - 23.5K 5.9K 5.6K - - - - UD-EWT train dev test 12.5K 204.6K 25.1K 2.0K 25.1K 2.1K - - - - - - - - ACE05 train dev test 14.4K 215.2K 34.5K 2.5K 61.5K 4.0K 3.7K 0.5K 1.1K 38.0K 6.0K 10.8K 5.7K 0.7K 1.7K 6.2K 0.8K 1.7K Table 1: Data statistics. second stage of querying and annotation in each AL cycle based on the annotated mentions in the first stage. This extra stage only selects rela- tions that involve the newly added or corrected mentions. We simply reuse the selection ratio determined from the first stage and apply it to each sentence that contains such mentions. In this way, the second stage is lightweight and only requires relatively cheap re-inference for each queried sentence individually. Annotation. The annotation of the mentions is the same as in the NER task, while for the annota- tion of relational queries, their mentions are first examined and corrected if needed, as explained in \u00a73.4. We measure the labeling cost by the fi- nal annotated items; thus, these extra examined mentioned will also be properly counted. Model learning. For the mention extraction sub- task, the training objective is the same as in NER. For the relational sub-task, we simply adopt a local pairwise model with the standard cross- entropy loss. Since the relation model is local, no special treatment is needed for PA. B Data Statistics and More Settings Data. Our main experiments are conducted us- ing the CoNLL-2003 English dataset11 (Tjong Kim Sang and De Meulder, 2003) for NER, the English Web Treebank (EWT) from Universal De- pendencies12 v2.10 (Nivre et al., 2020) for DPAR, and English portion of ACE200513 (Walker et al., 2006) for IE. We utilize Stanza14 (Qi et al., 2020) to assign POS tags for cost measurement in NER and mention tasks. We follow Lin et al. (2020) for the pre-processing15 of the ACE dataset. For the IE tasks on ACE, we find that the conventional test set contains only newswire documents while the training set consists of various genres (such as from conversation and web). Such mismatches between the AL pool and the final testing set are nontriv- ial to handle with the classical AL protocol, and we thus randomly re-split the ACE dataset (with a ratio of 7:1:2 for training, dev, and test sets, respec- tively). Table 1 shows data statistics. For each AL experiment, we take the original training set as the unlabeled pool, down-sample a dev set from the original dev set, and evaluate on the full test set. More Settings. All of our models are based on the pre-trained RoBERTabase as the"}, {"question": " What type of model is adopted in the text?", "answer": " A multi-task model", "ref_chunk": "gold parse tree as the train- ing loss in FA and marginalized likelihood in PA (Li et al., 2016). A.2 IE Tasks. We tackle event extraction (EE) and rela- tion extraction (RE) using a two-step pipelined approach. The first step aims to extract entity mentions for RE, and entity mentions and event triggers for EE. We adopt sequence labeling for mention extractions as in the NER task. Based on the mentions extracted in the first step, the second step examines each feasible candidate mention pair (entity pair for RE and event-entity pair for EE) and decides the relation (entity relation for RE and event argument relation for EE) for them. Since event argument links can be regarded as relations between event triggers and entities, for simplicity we will use the relational sub-task to refer to both relation and argument extraction. Model. We adopt a multi-task model similar to the one utilized in (Rotman and Reichart, 2022). With a pre-trained encoder, we take the first N layers as the shared encoding module whose out- put representations are used for both sub-tasks. Each sub-task further adopts a private encoder that is initialized with the remaining pre-trained layers and is trained with task-specific signals. We simply set N to 6, while the results are gen- erally not sensitive to this hyper-parameter. Final task-specific predictors are further stacked upon the corresponding private encoders. We adopt a CRF layer for mention extraction and a pairwise local predictor with a biaffine scorer for relation or argument extraction. Sentence selection. For an unlabeled sentence, there is an uncertainty score for each sub-task. For mentions, the uncertainty is the average mar- gin as in the NER task. For relations, we find that averaging uncertainties over all mention pairs has a bias towards sentences with fewer mentions. To mitigate such bias, we first aggregate an un- certainty score for each mention by taking the maximum score within all the relations that link to it and then averaging over all the mentions for sentence-level scores. Finally, the scores of the two sub-tasks are linearly combined to form the sentence-level uncertainty. Partial selection. For PA selection, the two sub- tasks are handled separately according to the adaptive ratio scheme. We further adopt two heuristics for the relational task to compensate for errors in the mention extraction. First, since there can be over-predicted mentions that lead to discarded relation queries, we adjust the PA ratio by estimating how many candidate rela- tions contain such errors in the mentions. We again train a logistic regression model to pre- dict whether a token is NIL (or \u2018O\u2019 in the BIO scheme, meaning not contained inside any gold mentions) based on its NIL probability. Then for each candidate relation, we calculate the proba- bility that any token within its mentions is NIL. By averaging this probability of all the candi- dates, we obtain a rough estimation of the per- centage of problematic relations, which we call it \u03b1. Finally the PA selection ratio is adjusted by: radjust = \u03b1\u00b7rproblem +(1\u2212\u03b1)\u00b7rorigin. Here, rorigin denotes the original selection ratio obtained from the adaptive scheme, and rproblem denotes the se- lection ratio of problematic relations, which we conservatively set to 1. Secondly, since there can also be under-predicted mentions, we add a Data Split #Sent. #Token #Event #Entity #Argument #Relation CoNLL03 train dev test 14.0K 203.6K 51.4K 3.3K 46.4K 3.5K - - 23.5K 5.9K 5.6K - - - - UD-EWT train dev test 12.5K 204.6K 25.1K 2.0K 25.1K 2.1K - - - - - - - - ACE05 train dev test 14.4K 215.2K 34.5K 2.5K 61.5K 4.0K 3.7K 0.5K 1.1K 38.0K 6.0K 10.8K 5.7K 0.7K 1.7K 6.2K 0.8K 1.7K Table 1: Data statistics. second stage of querying and annotation in each AL cycle based on the annotated mentions in the first stage. This extra stage only selects rela- tions that involve the newly added or corrected mentions. We simply reuse the selection ratio determined from the first stage and apply it to each sentence that contains such mentions. In this way, the second stage is lightweight and only requires relatively cheap re-inference for each queried sentence individually. Annotation. The annotation of the mentions is the same as in the NER task, while for the annota- tion of relational queries, their mentions are first examined and corrected if needed, as explained in \u00a73.4. We measure the labeling cost by the fi- nal annotated items; thus, these extra examined mentioned will also be properly counted. Model learning. For the mention extraction sub- task, the training objective is the same as in NER. For the relational sub-task, we simply adopt a local pairwise model with the standard cross- entropy loss. Since the relation model is local, no special treatment is needed for PA. B Data Statistics and More Settings Data. Our main experiments are conducted us- ing the CoNLL-2003 English dataset11 (Tjong Kim Sang and De Meulder, 2003) for NER, the English Web Treebank (EWT) from Universal De- pendencies12 v2.10 (Nivre et al., 2020) for DPAR, and English portion of ACE200513 (Walker et al., 2006) for IE. We utilize Stanza14 (Qi et al., 2020) to assign POS tags for cost measurement in NER and mention tasks. We follow Lin et al. (2020) for the pre-processing15 of the ACE dataset. For the IE tasks on ACE, we find that the conventional test set contains only newswire documents while the training set consists of various genres (such as from conversation and web). Such mismatches between the AL pool and the final testing set are nontriv- ial to handle with the classical AL protocol, and we thus randomly re-split the ACE dataset (with a ratio of 7:1:2 for training, dev, and test sets, respec- tively). Table 1 shows data statistics. For each AL experiment, we take the original training set as the unlabeled pool, down-sample a dev set from the original dev set, and evaluate on the full test set. More Settings. All of our models are based on the pre-trained RoBERTabase as the"}, {"question": " What is used as the shared encoding module in the model?", "answer": " The first N layers of the pre-trained encoder", "ref_chunk": "gold parse tree as the train- ing loss in FA and marginalized likelihood in PA (Li et al., 2016). A.2 IE Tasks. We tackle event extraction (EE) and rela- tion extraction (RE) using a two-step pipelined approach. The first step aims to extract entity mentions for RE, and entity mentions and event triggers for EE. We adopt sequence labeling for mention extractions as in the NER task. Based on the mentions extracted in the first step, the second step examines each feasible candidate mention pair (entity pair for RE and event-entity pair for EE) and decides the relation (entity relation for RE and event argument relation for EE) for them. Since event argument links can be regarded as relations between event triggers and entities, for simplicity we will use the relational sub-task to refer to both relation and argument extraction. Model. We adopt a multi-task model similar to the one utilized in (Rotman and Reichart, 2022). With a pre-trained encoder, we take the first N layers as the shared encoding module whose out- put representations are used for both sub-tasks. Each sub-task further adopts a private encoder that is initialized with the remaining pre-trained layers and is trained with task-specific signals. We simply set N to 6, while the results are gen- erally not sensitive to this hyper-parameter. Final task-specific predictors are further stacked upon the corresponding private encoders. We adopt a CRF layer for mention extraction and a pairwise local predictor with a biaffine scorer for relation or argument extraction. Sentence selection. For an unlabeled sentence, there is an uncertainty score for each sub-task. For mentions, the uncertainty is the average mar- gin as in the NER task. For relations, we find that averaging uncertainties over all mention pairs has a bias towards sentences with fewer mentions. To mitigate such bias, we first aggregate an un- certainty score for each mention by taking the maximum score within all the relations that link to it and then averaging over all the mentions for sentence-level scores. Finally, the scores of the two sub-tasks are linearly combined to form the sentence-level uncertainty. Partial selection. For PA selection, the two sub- tasks are handled separately according to the adaptive ratio scheme. We further adopt two heuristics for the relational task to compensate for errors in the mention extraction. First, since there can be over-predicted mentions that lead to discarded relation queries, we adjust the PA ratio by estimating how many candidate rela- tions contain such errors in the mentions. We again train a logistic regression model to pre- dict whether a token is NIL (or \u2018O\u2019 in the BIO scheme, meaning not contained inside any gold mentions) based on its NIL probability. Then for each candidate relation, we calculate the proba- bility that any token within its mentions is NIL. By averaging this probability of all the candi- dates, we obtain a rough estimation of the per- centage of problematic relations, which we call it \u03b1. Finally the PA selection ratio is adjusted by: radjust = \u03b1\u00b7rproblem +(1\u2212\u03b1)\u00b7rorigin. Here, rorigin denotes the original selection ratio obtained from the adaptive scheme, and rproblem denotes the se- lection ratio of problematic relations, which we conservatively set to 1. Secondly, since there can also be under-predicted mentions, we add a Data Split #Sent. #Token #Event #Entity #Argument #Relation CoNLL03 train dev test 14.0K 203.6K 51.4K 3.3K 46.4K 3.5K - - 23.5K 5.9K 5.6K - - - - UD-EWT train dev test 12.5K 204.6K 25.1K 2.0K 25.1K 2.1K - - - - - - - - ACE05 train dev test 14.4K 215.2K 34.5K 2.5K 61.5K 4.0K 3.7K 0.5K 1.1K 38.0K 6.0K 10.8K 5.7K 0.7K 1.7K 6.2K 0.8K 1.7K Table 1: Data statistics. second stage of querying and annotation in each AL cycle based on the annotated mentions in the first stage. This extra stage only selects rela- tions that involve the newly added or corrected mentions. We simply reuse the selection ratio determined from the first stage and apply it to each sentence that contains such mentions. In this way, the second stage is lightweight and only requires relatively cheap re-inference for each queried sentence individually. Annotation. The annotation of the mentions is the same as in the NER task, while for the annota- tion of relational queries, their mentions are first examined and corrected if needed, as explained in \u00a73.4. We measure the labeling cost by the fi- nal annotated items; thus, these extra examined mentioned will also be properly counted. Model learning. For the mention extraction sub- task, the training objective is the same as in NER. For the relational sub-task, we simply adopt a local pairwise model with the standard cross- entropy loss. Since the relation model is local, no special treatment is needed for PA. B Data Statistics and More Settings Data. Our main experiments are conducted us- ing the CoNLL-2003 English dataset11 (Tjong Kim Sang and De Meulder, 2003) for NER, the English Web Treebank (EWT) from Universal De- pendencies12 v2.10 (Nivre et al., 2020) for DPAR, and English portion of ACE200513 (Walker et al., 2006) for IE. We utilize Stanza14 (Qi et al., 2020) to assign POS tags for cost measurement in NER and mention tasks. We follow Lin et al. (2020) for the pre-processing15 of the ACE dataset. For the IE tasks on ACE, we find that the conventional test set contains only newswire documents while the training set consists of various genres (such as from conversation and web). Such mismatches between the AL pool and the final testing set are nontriv- ial to handle with the classical AL protocol, and we thus randomly re-split the ACE dataset (with a ratio of 7:1:2 for training, dev, and test sets, respec- tively). Table 1 shows data statistics. For each AL experiment, we take the original training set as the unlabeled pool, down-sample a dev set from the original dev set, and evaluate on the full test set. More Settings. All of our models are based on the pre-trained RoBERTabase as the"}, {"question": " What is used for mention extraction in the model?", "answer": " A CRF layer", "ref_chunk": "gold parse tree as the train- ing loss in FA and marginalized likelihood in PA (Li et al., 2016). A.2 IE Tasks. We tackle event extraction (EE) and rela- tion extraction (RE) using a two-step pipelined approach. The first step aims to extract entity mentions for RE, and entity mentions and event triggers for EE. We adopt sequence labeling for mention extractions as in the NER task. Based on the mentions extracted in the first step, the second step examines each feasible candidate mention pair (entity pair for RE and event-entity pair for EE) and decides the relation (entity relation for RE and event argument relation for EE) for them. Since event argument links can be regarded as relations between event triggers and entities, for simplicity we will use the relational sub-task to refer to both relation and argument extraction. Model. We adopt a multi-task model similar to the one utilized in (Rotman and Reichart, 2022). With a pre-trained encoder, we take the first N layers as the shared encoding module whose out- put representations are used for both sub-tasks. Each sub-task further adopts a private encoder that is initialized with the remaining pre-trained layers and is trained with task-specific signals. We simply set N to 6, while the results are gen- erally not sensitive to this hyper-parameter. Final task-specific predictors are further stacked upon the corresponding private encoders. We adopt a CRF layer for mention extraction and a pairwise local predictor with a biaffine scorer for relation or argument extraction. Sentence selection. For an unlabeled sentence, there is an uncertainty score for each sub-task. For mentions, the uncertainty is the average mar- gin as in the NER task. For relations, we find that averaging uncertainties over all mention pairs has a bias towards sentences with fewer mentions. To mitigate such bias, we first aggregate an un- certainty score for each mention by taking the maximum score within all the relations that link to it and then averaging over all the mentions for sentence-level scores. Finally, the scores of the two sub-tasks are linearly combined to form the sentence-level uncertainty. Partial selection. For PA selection, the two sub- tasks are handled separately according to the adaptive ratio scheme. We further adopt two heuristics for the relational task to compensate for errors in the mention extraction. First, since there can be over-predicted mentions that lead to discarded relation queries, we adjust the PA ratio by estimating how many candidate rela- tions contain such errors in the mentions. We again train a logistic regression model to pre- dict whether a token is NIL (or \u2018O\u2019 in the BIO scheme, meaning not contained inside any gold mentions) based on its NIL probability. Then for each candidate relation, we calculate the proba- bility that any token within its mentions is NIL. By averaging this probability of all the candi- dates, we obtain a rough estimation of the per- centage of problematic relations, which we call it \u03b1. Finally the PA selection ratio is adjusted by: radjust = \u03b1\u00b7rproblem +(1\u2212\u03b1)\u00b7rorigin. Here, rorigin denotes the original selection ratio obtained from the adaptive scheme, and rproblem denotes the se- lection ratio of problematic relations, which we conservatively set to 1. Secondly, since there can also be under-predicted mentions, we add a Data Split #Sent. #Token #Event #Entity #Argument #Relation CoNLL03 train dev test 14.0K 203.6K 51.4K 3.3K 46.4K 3.5K - - 23.5K 5.9K 5.6K - - - - UD-EWT train dev test 12.5K 204.6K 25.1K 2.0K 25.1K 2.1K - - - - - - - - ACE05 train dev test 14.4K 215.2K 34.5K 2.5K 61.5K 4.0K 3.7K 0.5K 1.1K 38.0K 6.0K 10.8K 5.7K 0.7K 1.7K 6.2K 0.8K 1.7K Table 1: Data statistics. second stage of querying and annotation in each AL cycle based on the annotated mentions in the first stage. This extra stage only selects rela- tions that involve the newly added or corrected mentions. We simply reuse the selection ratio determined from the first stage and apply it to each sentence that contains such mentions. In this way, the second stage is lightweight and only requires relatively cheap re-inference for each queried sentence individually. Annotation. The annotation of the mentions is the same as in the NER task, while for the annota- tion of relational queries, their mentions are first examined and corrected if needed, as explained in \u00a73.4. We measure the labeling cost by the fi- nal annotated items; thus, these extra examined mentioned will also be properly counted. Model learning. For the mention extraction sub- task, the training objective is the same as in NER. For the relational sub-task, we simply adopt a local pairwise model with the standard cross- entropy loss. Since the relation model is local, no special treatment is needed for PA. B Data Statistics and More Settings Data. Our main experiments are conducted us- ing the CoNLL-2003 English dataset11 (Tjong Kim Sang and De Meulder, 2003) for NER, the English Web Treebank (EWT) from Universal De- pendencies12 v2.10 (Nivre et al., 2020) for DPAR, and English portion of ACE200513 (Walker et al., 2006) for IE. We utilize Stanza14 (Qi et al., 2020) to assign POS tags for cost measurement in NER and mention tasks. We follow Lin et al. (2020) for the pre-processing15 of the ACE dataset. For the IE tasks on ACE, we find that the conventional test set contains only newswire documents while the training set consists of various genres (such as from conversation and web). Such mismatches between the AL pool and the final testing set are nontriv- ial to handle with the classical AL protocol, and we thus randomly re-split the ACE dataset (with a ratio of 7:1:2 for training, dev, and test sets, respec- tively). Table 1 shows data statistics. For each AL experiment, we take the original training set as the unlabeled pool, down-sample a dev set from the original dev set, and evaluate on the full test set. More Settings. All of our models are based on the pre-trained RoBERTabase as the"}, {"question": " How is the sentence-level uncertainty calculated?", "answer": " By linearly combining the scores of the two sub-tasks", "ref_chunk": "gold parse tree as the train- ing loss in FA and marginalized likelihood in PA (Li et al., 2016). A.2 IE Tasks. We tackle event extraction (EE) and rela- tion extraction (RE) using a two-step pipelined approach. The first step aims to extract entity mentions for RE, and entity mentions and event triggers for EE. We adopt sequence labeling for mention extractions as in the NER task. Based on the mentions extracted in the first step, the second step examines each feasible candidate mention pair (entity pair for RE and event-entity pair for EE) and decides the relation (entity relation for RE and event argument relation for EE) for them. Since event argument links can be regarded as relations between event triggers and entities, for simplicity we will use the relational sub-task to refer to both relation and argument extraction. Model. We adopt a multi-task model similar to the one utilized in (Rotman and Reichart, 2022). With a pre-trained encoder, we take the first N layers as the shared encoding module whose out- put representations are used for both sub-tasks. Each sub-task further adopts a private encoder that is initialized with the remaining pre-trained layers and is trained with task-specific signals. We simply set N to 6, while the results are gen- erally not sensitive to this hyper-parameter. Final task-specific predictors are further stacked upon the corresponding private encoders. We adopt a CRF layer for mention extraction and a pairwise local predictor with a biaffine scorer for relation or argument extraction. Sentence selection. For an unlabeled sentence, there is an uncertainty score for each sub-task. For mentions, the uncertainty is the average mar- gin as in the NER task. For relations, we find that averaging uncertainties over all mention pairs has a bias towards sentences with fewer mentions. To mitigate such bias, we first aggregate an un- certainty score for each mention by taking the maximum score within all the relations that link to it and then averaging over all the mentions for sentence-level scores. Finally, the scores of the two sub-tasks are linearly combined to form the sentence-level uncertainty. Partial selection. For PA selection, the two sub- tasks are handled separately according to the adaptive ratio scheme. We further adopt two heuristics for the relational task to compensate for errors in the mention extraction. First, since there can be over-predicted mentions that lead to discarded relation queries, we adjust the PA ratio by estimating how many candidate rela- tions contain such errors in the mentions. We again train a logistic regression model to pre- dict whether a token is NIL (or \u2018O\u2019 in the BIO scheme, meaning not contained inside any gold mentions) based on its NIL probability. Then for each candidate relation, we calculate the proba- bility that any token within its mentions is NIL. By averaging this probability of all the candi- dates, we obtain a rough estimation of the per- centage of problematic relations, which we call it \u03b1. Finally the PA selection ratio is adjusted by: radjust = \u03b1\u00b7rproblem +(1\u2212\u03b1)\u00b7rorigin. Here, rorigin denotes the original selection ratio obtained from the adaptive scheme, and rproblem denotes the se- lection ratio of problematic relations, which we conservatively set to 1. Secondly, since there can also be under-predicted mentions, we add a Data Split #Sent. #Token #Event #Entity #Argument #Relation CoNLL03 train dev test 14.0K 203.6K 51.4K 3.3K 46.4K 3.5K - - 23.5K 5.9K 5.6K - - - - UD-EWT train dev test 12.5K 204.6K 25.1K 2.0K 25.1K 2.1K - - - - - - - - ACE05 train dev test 14.4K 215.2K 34.5K 2.5K 61.5K 4.0K 3.7K 0.5K 1.1K 38.0K 6.0K 10.8K 5.7K 0.7K 1.7K 6.2K 0.8K 1.7K Table 1: Data statistics. second stage of querying and annotation in each AL cycle based on the annotated mentions in the first stage. This extra stage only selects rela- tions that involve the newly added or corrected mentions. We simply reuse the selection ratio determined from the first stage and apply it to each sentence that contains such mentions. In this way, the second stage is lightweight and only requires relatively cheap re-inference for each queried sentence individually. Annotation. The annotation of the mentions is the same as in the NER task, while for the annota- tion of relational queries, their mentions are first examined and corrected if needed, as explained in \u00a73.4. We measure the labeling cost by the fi- nal annotated items; thus, these extra examined mentioned will also be properly counted. Model learning. For the mention extraction sub- task, the training objective is the same as in NER. For the relational sub-task, we simply adopt a local pairwise model with the standard cross- entropy loss. Since the relation model is local, no special treatment is needed for PA. B Data Statistics and More Settings Data. Our main experiments are conducted us- ing the CoNLL-2003 English dataset11 (Tjong Kim Sang and De Meulder, 2003) for NER, the English Web Treebank (EWT) from Universal De- pendencies12 v2.10 (Nivre et al., 2020) for DPAR, and English portion of ACE200513 (Walker et al., 2006) for IE. We utilize Stanza14 (Qi et al., 2020) to assign POS tags for cost measurement in NER and mention tasks. We follow Lin et al. (2020) for the pre-processing15 of the ACE dataset. For the IE tasks on ACE, we find that the conventional test set contains only newswire documents while the training set consists of various genres (such as from conversation and web). Such mismatches between the AL pool and the final testing set are nontriv- ial to handle with the classical AL protocol, and we thus randomly re-split the ACE dataset (with a ratio of 7:1:2 for training, dev, and test sets, respec- tively). Table 1 shows data statistics. For each AL experiment, we take the original training set as the unlabeled pool, down-sample a dev set from the original dev set, and evaluate on the full test set. More Settings. All of our models are based on the pre-trained RoBERTabase as the"}, {"question": " What is the purpose of the two heuristics adopted for the relational task?", "answer": " To compensate for errors in the mention extraction", "ref_chunk": "gold parse tree as the train- ing loss in FA and marginalized likelihood in PA (Li et al., 2016). A.2 IE Tasks. We tackle event extraction (EE) and rela- tion extraction (RE) using a two-step pipelined approach. The first step aims to extract entity mentions for RE, and entity mentions and event triggers for EE. We adopt sequence labeling for mention extractions as in the NER task. Based on the mentions extracted in the first step, the second step examines each feasible candidate mention pair (entity pair for RE and event-entity pair for EE) and decides the relation (entity relation for RE and event argument relation for EE) for them. Since event argument links can be regarded as relations between event triggers and entities, for simplicity we will use the relational sub-task to refer to both relation and argument extraction. Model. We adopt a multi-task model similar to the one utilized in (Rotman and Reichart, 2022). With a pre-trained encoder, we take the first N layers as the shared encoding module whose out- put representations are used for both sub-tasks. Each sub-task further adopts a private encoder that is initialized with the remaining pre-trained layers and is trained with task-specific signals. We simply set N to 6, while the results are gen- erally not sensitive to this hyper-parameter. Final task-specific predictors are further stacked upon the corresponding private encoders. We adopt a CRF layer for mention extraction and a pairwise local predictor with a biaffine scorer for relation or argument extraction. Sentence selection. For an unlabeled sentence, there is an uncertainty score for each sub-task. For mentions, the uncertainty is the average mar- gin as in the NER task. For relations, we find that averaging uncertainties over all mention pairs has a bias towards sentences with fewer mentions. To mitigate such bias, we first aggregate an un- certainty score for each mention by taking the maximum score within all the relations that link to it and then averaging over all the mentions for sentence-level scores. Finally, the scores of the two sub-tasks are linearly combined to form the sentence-level uncertainty. Partial selection. For PA selection, the two sub- tasks are handled separately according to the adaptive ratio scheme. We further adopt two heuristics for the relational task to compensate for errors in the mention extraction. First, since there can be over-predicted mentions that lead to discarded relation queries, we adjust the PA ratio by estimating how many candidate rela- tions contain such errors in the mentions. We again train a logistic regression model to pre- dict whether a token is NIL (or \u2018O\u2019 in the BIO scheme, meaning not contained inside any gold mentions) based on its NIL probability. Then for each candidate relation, we calculate the proba- bility that any token within its mentions is NIL. By averaging this probability of all the candi- dates, we obtain a rough estimation of the per- centage of problematic relations, which we call it \u03b1. Finally the PA selection ratio is adjusted by: radjust = \u03b1\u00b7rproblem +(1\u2212\u03b1)\u00b7rorigin. Here, rorigin denotes the original selection ratio obtained from the adaptive scheme, and rproblem denotes the se- lection ratio of problematic relations, which we conservatively set to 1. Secondly, since there can also be under-predicted mentions, we add a Data Split #Sent. #Token #Event #Entity #Argument #Relation CoNLL03 train dev test 14.0K 203.6K 51.4K 3.3K 46.4K 3.5K - - 23.5K 5.9K 5.6K - - - - UD-EWT train dev test 12.5K 204.6K 25.1K 2.0K 25.1K 2.1K - - - - - - - - ACE05 train dev test 14.4K 215.2K 34.5K 2.5K 61.5K 4.0K 3.7K 0.5K 1.1K 38.0K 6.0K 10.8K 5.7K 0.7K 1.7K 6.2K 0.8K 1.7K Table 1: Data statistics. second stage of querying and annotation in each AL cycle based on the annotated mentions in the first stage. This extra stage only selects rela- tions that involve the newly added or corrected mentions. We simply reuse the selection ratio determined from the first stage and apply it to each sentence that contains such mentions. In this way, the second stage is lightweight and only requires relatively cheap re-inference for each queried sentence individually. Annotation. The annotation of the mentions is the same as in the NER task, while for the annota- tion of relational queries, their mentions are first examined and corrected if needed, as explained in \u00a73.4. We measure the labeling cost by the fi- nal annotated items; thus, these extra examined mentioned will also be properly counted. Model learning. For the mention extraction sub- task, the training objective is the same as in NER. For the relational sub-task, we simply adopt a local pairwise model with the standard cross- entropy loss. Since the relation model is local, no special treatment is needed for PA. B Data Statistics and More Settings Data. Our main experiments are conducted us- ing the CoNLL-2003 English dataset11 (Tjong Kim Sang and De Meulder, 2003) for NER, the English Web Treebank (EWT) from Universal De- pendencies12 v2.10 (Nivre et al., 2020) for DPAR, and English portion of ACE200513 (Walker et al., 2006) for IE. We utilize Stanza14 (Qi et al., 2020) to assign POS tags for cost measurement in NER and mention tasks. We follow Lin et al. (2020) for the pre-processing15 of the ACE dataset. For the IE tasks on ACE, we find that the conventional test set contains only newswire documents while the training set consists of various genres (such as from conversation and web). Such mismatches between the AL pool and the final testing set are nontriv- ial to handle with the classical AL protocol, and we thus randomly re-split the ACE dataset (with a ratio of 7:1:2 for training, dev, and test sets, respec- tively). Table 1 shows data statistics. For each AL experiment, we take the original training set as the unlabeled pool, down-sample a dev set from the original dev set, and evaluate on the full test set. More Settings. All of our models are based on the pre-trained RoBERTabase as the"}, {"question": " How is the selection ratio adjusted in the second heuristic for handling under-predicted mentions?", "answer": " r_adjust = \u03b1 \u00b7 r_problem + (1\u2212\u03b1) \u00b7 r_origin", "ref_chunk": "gold parse tree as the train- ing loss in FA and marginalized likelihood in PA (Li et al., 2016). A.2 IE Tasks. We tackle event extraction (EE) and rela- tion extraction (RE) using a two-step pipelined approach. The first step aims to extract entity mentions for RE, and entity mentions and event triggers for EE. We adopt sequence labeling for mention extractions as in the NER task. Based on the mentions extracted in the first step, the second step examines each feasible candidate mention pair (entity pair for RE and event-entity pair for EE) and decides the relation (entity relation for RE and event argument relation for EE) for them. Since event argument links can be regarded as relations between event triggers and entities, for simplicity we will use the relational sub-task to refer to both relation and argument extraction. Model. We adopt a multi-task model similar to the one utilized in (Rotman and Reichart, 2022). With a pre-trained encoder, we take the first N layers as the shared encoding module whose out- put representations are used for both sub-tasks. Each sub-task further adopts a private encoder that is initialized with the remaining pre-trained layers and is trained with task-specific signals. We simply set N to 6, while the results are gen- erally not sensitive to this hyper-parameter. Final task-specific predictors are further stacked upon the corresponding private encoders. We adopt a CRF layer for mention extraction and a pairwise local predictor with a biaffine scorer for relation or argument extraction. Sentence selection. For an unlabeled sentence, there is an uncertainty score for each sub-task. For mentions, the uncertainty is the average mar- gin as in the NER task. For relations, we find that averaging uncertainties over all mention pairs has a bias towards sentences with fewer mentions. To mitigate such bias, we first aggregate an un- certainty score for each mention by taking the maximum score within all the relations that link to it and then averaging over all the mentions for sentence-level scores. Finally, the scores of the two sub-tasks are linearly combined to form the sentence-level uncertainty. Partial selection. For PA selection, the two sub- tasks are handled separately according to the adaptive ratio scheme. We further adopt two heuristics for the relational task to compensate for errors in the mention extraction. First, since there can be over-predicted mentions that lead to discarded relation queries, we adjust the PA ratio by estimating how many candidate rela- tions contain such errors in the mentions. We again train a logistic regression model to pre- dict whether a token is NIL (or \u2018O\u2019 in the BIO scheme, meaning not contained inside any gold mentions) based on its NIL probability. Then for each candidate relation, we calculate the proba- bility that any token within its mentions is NIL. By averaging this probability of all the candi- dates, we obtain a rough estimation of the per- centage of problematic relations, which we call it \u03b1. Finally the PA selection ratio is adjusted by: radjust = \u03b1\u00b7rproblem +(1\u2212\u03b1)\u00b7rorigin. Here, rorigin denotes the original selection ratio obtained from the adaptive scheme, and rproblem denotes the se- lection ratio of problematic relations, which we conservatively set to 1. Secondly, since there can also be under-predicted mentions, we add a Data Split #Sent. #Token #Event #Entity #Argument #Relation CoNLL03 train dev test 14.0K 203.6K 51.4K 3.3K 46.4K 3.5K - - 23.5K 5.9K 5.6K - - - - UD-EWT train dev test 12.5K 204.6K 25.1K 2.0K 25.1K 2.1K - - - - - - - - ACE05 train dev test 14.4K 215.2K 34.5K 2.5K 61.5K 4.0K 3.7K 0.5K 1.1K 38.0K 6.0K 10.8K 5.7K 0.7K 1.7K 6.2K 0.8K 1.7K Table 1: Data statistics. second stage of querying and annotation in each AL cycle based on the annotated mentions in the first stage. This extra stage only selects rela- tions that involve the newly added or corrected mentions. We simply reuse the selection ratio determined from the first stage and apply it to each sentence that contains such mentions. In this way, the second stage is lightweight and only requires relatively cheap re-inference for each queried sentence individually. Annotation. The annotation of the mentions is the same as in the NER task, while for the annota- tion of relational queries, their mentions are first examined and corrected if needed, as explained in \u00a73.4. We measure the labeling cost by the fi- nal annotated items; thus, these extra examined mentioned will also be properly counted. Model learning. For the mention extraction sub- task, the training objective is the same as in NER. For the relational sub-task, we simply adopt a local pairwise model with the standard cross- entropy loss. Since the relation model is local, no special treatment is needed for PA. B Data Statistics and More Settings Data. Our main experiments are conducted us- ing the CoNLL-2003 English dataset11 (Tjong Kim Sang and De Meulder, 2003) for NER, the English Web Treebank (EWT) from Universal De- pendencies12 v2.10 (Nivre et al., 2020) for DPAR, and English portion of ACE200513 (Walker et al., 2006) for IE. We utilize Stanza14 (Qi et al., 2020) to assign POS tags for cost measurement in NER and mention tasks. We follow Lin et al. (2020) for the pre-processing15 of the ACE dataset. For the IE tasks on ACE, we find that the conventional test set contains only newswire documents while the training set consists of various genres (such as from conversation and web). Such mismatches between the AL pool and the final testing set are nontriv- ial to handle with the classical AL protocol, and we thus randomly re-split the ACE dataset (with a ratio of 7:1:2 for training, dev, and test sets, respec- tively). Table 1 shows data statistics. For each AL experiment, we take the original training set as the unlabeled pool, down-sample a dev set from the original dev set, and evaluate on the full test set. More Settings. All of our models are based on the pre-trained RoBERTabase as the"}, {"question": " What dataset is used for NER in the experiments?", "answer": " CoNLL-2003 English dataset", "ref_chunk": "gold parse tree as the train- ing loss in FA and marginalized likelihood in PA (Li et al., 2016). A.2 IE Tasks. We tackle event extraction (EE) and rela- tion extraction (RE) using a two-step pipelined approach. The first step aims to extract entity mentions for RE, and entity mentions and event triggers for EE. We adopt sequence labeling for mention extractions as in the NER task. Based on the mentions extracted in the first step, the second step examines each feasible candidate mention pair (entity pair for RE and event-entity pair for EE) and decides the relation (entity relation for RE and event argument relation for EE) for them. Since event argument links can be regarded as relations between event triggers and entities, for simplicity we will use the relational sub-task to refer to both relation and argument extraction. Model. We adopt a multi-task model similar to the one utilized in (Rotman and Reichart, 2022). With a pre-trained encoder, we take the first N layers as the shared encoding module whose out- put representations are used for both sub-tasks. Each sub-task further adopts a private encoder that is initialized with the remaining pre-trained layers and is trained with task-specific signals. We simply set N to 6, while the results are gen- erally not sensitive to this hyper-parameter. Final task-specific predictors are further stacked upon the corresponding private encoders. We adopt a CRF layer for mention extraction and a pairwise local predictor with a biaffine scorer for relation or argument extraction. Sentence selection. For an unlabeled sentence, there is an uncertainty score for each sub-task. For mentions, the uncertainty is the average mar- gin as in the NER task. For relations, we find that averaging uncertainties over all mention pairs has a bias towards sentences with fewer mentions. To mitigate such bias, we first aggregate an un- certainty score for each mention by taking the maximum score within all the relations that link to it and then averaging over all the mentions for sentence-level scores. Finally, the scores of the two sub-tasks are linearly combined to form the sentence-level uncertainty. Partial selection. For PA selection, the two sub- tasks are handled separately according to the adaptive ratio scheme. We further adopt two heuristics for the relational task to compensate for errors in the mention extraction. First, since there can be over-predicted mentions that lead to discarded relation queries, we adjust the PA ratio by estimating how many candidate rela- tions contain such errors in the mentions. We again train a logistic regression model to pre- dict whether a token is NIL (or \u2018O\u2019 in the BIO scheme, meaning not contained inside any gold mentions) based on its NIL probability. Then for each candidate relation, we calculate the proba- bility that any token within its mentions is NIL. By averaging this probability of all the candi- dates, we obtain a rough estimation of the per- centage of problematic relations, which we call it \u03b1. Finally the PA selection ratio is adjusted by: radjust = \u03b1\u00b7rproblem +(1\u2212\u03b1)\u00b7rorigin. Here, rorigin denotes the original selection ratio obtained from the adaptive scheme, and rproblem denotes the se- lection ratio of problematic relations, which we conservatively set to 1. Secondly, since there can also be under-predicted mentions, we add a Data Split #Sent. #Token #Event #Entity #Argument #Relation CoNLL03 train dev test 14.0K 203.6K 51.4K 3.3K 46.4K 3.5K - - 23.5K 5.9K 5.6K - - - - UD-EWT train dev test 12.5K 204.6K 25.1K 2.0K 25.1K 2.1K - - - - - - - - ACE05 train dev test 14.4K 215.2K 34.5K 2.5K 61.5K 4.0K 3.7K 0.5K 1.1K 38.0K 6.0K 10.8K 5.7K 0.7K 1.7K 6.2K 0.8K 1.7K Table 1: Data statistics. second stage of querying and annotation in each AL cycle based on the annotated mentions in the first stage. This extra stage only selects rela- tions that involve the newly added or corrected mentions. We simply reuse the selection ratio determined from the first stage and apply it to each sentence that contains such mentions. In this way, the second stage is lightweight and only requires relatively cheap re-inference for each queried sentence individually. Annotation. The annotation of the mentions is the same as in the NER task, while for the annota- tion of relational queries, their mentions are first examined and corrected if needed, as explained in \u00a73.4. We measure the labeling cost by the fi- nal annotated items; thus, these extra examined mentioned will also be properly counted. Model learning. For the mention extraction sub- task, the training objective is the same as in NER. For the relational sub-task, we simply adopt a local pairwise model with the standard cross- entropy loss. Since the relation model is local, no special treatment is needed for PA. B Data Statistics and More Settings Data. Our main experiments are conducted us- ing the CoNLL-2003 English dataset11 (Tjong Kim Sang and De Meulder, 2003) for NER, the English Web Treebank (EWT) from Universal De- pendencies12 v2.10 (Nivre et al., 2020) for DPAR, and English portion of ACE200513 (Walker et al., 2006) for IE. We utilize Stanza14 (Qi et al., 2020) to assign POS tags for cost measurement in NER and mention tasks. We follow Lin et al. (2020) for the pre-processing15 of the ACE dataset. For the IE tasks on ACE, we find that the conventional test set contains only newswire documents while the training set consists of various genres (such as from conversation and web). Such mismatches between the AL pool and the final testing set are nontriv- ial to handle with the classical AL protocol, and we thus randomly re-split the ACE dataset (with a ratio of 7:1:2 for training, dev, and test sets, respec- tively). Table 1 shows data statistics. For each AL experiment, we take the original training set as the unlabeled pool, down-sample a dev set from the original dev set, and evaluate on the full test set. More Settings. All of our models are based on the pre-trained RoBERTabase as the"}, {"question": " How is the ACE dataset re-split for handling mismatches between the AL pool and the final testing set?", "answer": " Randomly, with a ratio of 7:1:2 for training, dev, and test sets, respectively", "ref_chunk": "gold parse tree as the train- ing loss in FA and marginalized likelihood in PA (Li et al., 2016). A.2 IE Tasks. We tackle event extraction (EE) and rela- tion extraction (RE) using a two-step pipelined approach. The first step aims to extract entity mentions for RE, and entity mentions and event triggers for EE. We adopt sequence labeling for mention extractions as in the NER task. Based on the mentions extracted in the first step, the second step examines each feasible candidate mention pair (entity pair for RE and event-entity pair for EE) and decides the relation (entity relation for RE and event argument relation for EE) for them. Since event argument links can be regarded as relations between event triggers and entities, for simplicity we will use the relational sub-task to refer to both relation and argument extraction. Model. We adopt a multi-task model similar to the one utilized in (Rotman and Reichart, 2022). With a pre-trained encoder, we take the first N layers as the shared encoding module whose out- put representations are used for both sub-tasks. Each sub-task further adopts a private encoder that is initialized with the remaining pre-trained layers and is trained with task-specific signals. We simply set N to 6, while the results are gen- erally not sensitive to this hyper-parameter. Final task-specific predictors are further stacked upon the corresponding private encoders. We adopt a CRF layer for mention extraction and a pairwise local predictor with a biaffine scorer for relation or argument extraction. Sentence selection. For an unlabeled sentence, there is an uncertainty score for each sub-task. For mentions, the uncertainty is the average mar- gin as in the NER task. For relations, we find that averaging uncertainties over all mention pairs has a bias towards sentences with fewer mentions. To mitigate such bias, we first aggregate an un- certainty score for each mention by taking the maximum score within all the relations that link to it and then averaging over all the mentions for sentence-level scores. Finally, the scores of the two sub-tasks are linearly combined to form the sentence-level uncertainty. Partial selection. For PA selection, the two sub- tasks are handled separately according to the adaptive ratio scheme. We further adopt two heuristics for the relational task to compensate for errors in the mention extraction. First, since there can be over-predicted mentions that lead to discarded relation queries, we adjust the PA ratio by estimating how many candidate rela- tions contain such errors in the mentions. We again train a logistic regression model to pre- dict whether a token is NIL (or \u2018O\u2019 in the BIO scheme, meaning not contained inside any gold mentions) based on its NIL probability. Then for each candidate relation, we calculate the proba- bility that any token within its mentions is NIL. By averaging this probability of all the candi- dates, we obtain a rough estimation of the per- centage of problematic relations, which we call it \u03b1. Finally the PA selection ratio is adjusted by: radjust = \u03b1\u00b7rproblem +(1\u2212\u03b1)\u00b7rorigin. Here, rorigin denotes the original selection ratio obtained from the adaptive scheme, and rproblem denotes the se- lection ratio of problematic relations, which we conservatively set to 1. Secondly, since there can also be under-predicted mentions, we add a Data Split #Sent. #Token #Event #Entity #Argument #Relation CoNLL03 train dev test 14.0K 203.6K 51.4K 3.3K 46.4K 3.5K - - 23.5K 5.9K 5.6K - - - - UD-EWT train dev test 12.5K 204.6K 25.1K 2.0K 25.1K 2.1K - - - - - - - - ACE05 train dev test 14.4K 215.2K 34.5K 2.5K 61.5K 4.0K 3.7K 0.5K 1.1K 38.0K 6.0K 10.8K 5.7K 0.7K 1.7K 6.2K 0.8K 1.7K Table 1: Data statistics. second stage of querying and annotation in each AL cycle based on the annotated mentions in the first stage. This extra stage only selects rela- tions that involve the newly added or corrected mentions. We simply reuse the selection ratio determined from the first stage and apply it to each sentence that contains such mentions. In this way, the second stage is lightweight and only requires relatively cheap re-inference for each queried sentence individually. Annotation. The annotation of the mentions is the same as in the NER task, while for the annota- tion of relational queries, their mentions are first examined and corrected if needed, as explained in \u00a73.4. We measure the labeling cost by the fi- nal annotated items; thus, these extra examined mentioned will also be properly counted. Model learning. For the mention extraction sub- task, the training objective is the same as in NER. For the relational sub-task, we simply adopt a local pairwise model with the standard cross- entropy loss. Since the relation model is local, no special treatment is needed for PA. B Data Statistics and More Settings Data. Our main experiments are conducted us- ing the CoNLL-2003 English dataset11 (Tjong Kim Sang and De Meulder, 2003) for NER, the English Web Treebank (EWT) from Universal De- pendencies12 v2.10 (Nivre et al., 2020) for DPAR, and English portion of ACE200513 (Walker et al., 2006) for IE. We utilize Stanza14 (Qi et al., 2020) to assign POS tags for cost measurement in NER and mention tasks. We follow Lin et al. (2020) for the pre-processing15 of the ACE dataset. For the IE tasks on ACE, we find that the conventional test set contains only newswire documents while the training set consists of various genres (such as from conversation and web). Such mismatches between the AL pool and the final testing set are nontriv- ial to handle with the classical AL protocol, and we thus randomly re-split the ACE dataset (with a ratio of 7:1:2 for training, dev, and test sets, respec- tively). Table 1 shows data statistics. For each AL experiment, we take the original training set as the unlabeled pool, down-sample a dev set from the original dev set, and evaluate on the full test set. More Settings. All of our models are based on the pre-trained RoBERTabase as the"}], "doc_text": "gold parse tree as the train- ing loss in FA and marginalized likelihood in PA (Li et al., 2016). A.2 IE Tasks. We tackle event extraction (EE) and rela- tion extraction (RE) using a two-step pipelined approach. The first step aims to extract entity mentions for RE, and entity mentions and event triggers for EE. We adopt sequence labeling for mention extractions as in the NER task. Based on the mentions extracted in the first step, the second step examines each feasible candidate mention pair (entity pair for RE and event-entity pair for EE) and decides the relation (entity relation for RE and event argument relation for EE) for them. Since event argument links can be regarded as relations between event triggers and entities, for simplicity we will use the relational sub-task to refer to both relation and argument extraction. Model. We adopt a multi-task model similar to the one utilized in (Rotman and Reichart, 2022). With a pre-trained encoder, we take the first N layers as the shared encoding module whose out- put representations are used for both sub-tasks. Each sub-task further adopts a private encoder that is initialized with the remaining pre-trained layers and is trained with task-specific signals. We simply set N to 6, while the results are gen- erally not sensitive to this hyper-parameter. Final task-specific predictors are further stacked upon the corresponding private encoders. We adopt a CRF layer for mention extraction and a pairwise local predictor with a biaffine scorer for relation or argument extraction. Sentence selection. For an unlabeled sentence, there is an uncertainty score for each sub-task. For mentions, the uncertainty is the average mar- gin as in the NER task. For relations, we find that averaging uncertainties over all mention pairs has a bias towards sentences with fewer mentions. To mitigate such bias, we first aggregate an un- certainty score for each mention by taking the maximum score within all the relations that link to it and then averaging over all the mentions for sentence-level scores. Finally, the scores of the two sub-tasks are linearly combined to form the sentence-level uncertainty. Partial selection. For PA selection, the two sub- tasks are handled separately according to the adaptive ratio scheme. We further adopt two heuristics for the relational task to compensate for errors in the mention extraction. First, since there can be over-predicted mentions that lead to discarded relation queries, we adjust the PA ratio by estimating how many candidate rela- tions contain such errors in the mentions. We again train a logistic regression model to pre- dict whether a token is NIL (or \u2018O\u2019 in the BIO scheme, meaning not contained inside any gold mentions) based on its NIL probability. Then for each candidate relation, we calculate the proba- bility that any token within its mentions is NIL. By averaging this probability of all the candi- dates, we obtain a rough estimation of the per- centage of problematic relations, which we call it \u03b1. Finally the PA selection ratio is adjusted by: radjust = \u03b1\u00b7rproblem +(1\u2212\u03b1)\u00b7rorigin. Here, rorigin denotes the original selection ratio obtained from the adaptive scheme, and rproblem denotes the se- lection ratio of problematic relations, which we conservatively set to 1. Secondly, since there can also be under-predicted mentions, we add a Data Split #Sent. #Token #Event #Entity #Argument #Relation CoNLL03 train dev test 14.0K 203.6K 51.4K 3.3K 46.4K 3.5K - - 23.5K 5.9K 5.6K - - - - UD-EWT train dev test 12.5K 204.6K 25.1K 2.0K 25.1K 2.1K - - - - - - - - ACE05 train dev test 14.4K 215.2K 34.5K 2.5K 61.5K 4.0K 3.7K 0.5K 1.1K 38.0K 6.0K 10.8K 5.7K 0.7K 1.7K 6.2K 0.8K 1.7K Table 1: Data statistics. second stage of querying and annotation in each AL cycle based on the annotated mentions in the first stage. This extra stage only selects rela- tions that involve the newly added or corrected mentions. We simply reuse the selection ratio determined from the first stage and apply it to each sentence that contains such mentions. In this way, the second stage is lightweight and only requires relatively cheap re-inference for each queried sentence individually. Annotation. The annotation of the mentions is the same as in the NER task, while for the annota- tion of relational queries, their mentions are first examined and corrected if needed, as explained in \u00a73.4. We measure the labeling cost by the fi- nal annotated items; thus, these extra examined mentioned will also be properly counted. Model learning. For the mention extraction sub- task, the training objective is the same as in NER. For the relational sub-task, we simply adopt a local pairwise model with the standard cross- entropy loss. Since the relation model is local, no special treatment is needed for PA. B Data Statistics and More Settings Data. Our main experiments are conducted us- ing the CoNLL-2003 English dataset11 (Tjong Kim Sang and De Meulder, 2003) for NER, the English Web Treebank (EWT) from Universal De- pendencies12 v2.10 (Nivre et al., 2020) for DPAR, and English portion of ACE200513 (Walker et al., 2006) for IE. We utilize Stanza14 (Qi et al., 2020) to assign POS tags for cost measurement in NER and mention tasks. We follow Lin et al. (2020) for the pre-processing15 of the ACE dataset. For the IE tasks on ACE, we find that the conventional test set contains only newswire documents while the training set consists of various genres (such as from conversation and web). Such mismatches between the AL pool and the final testing set are nontriv- ial to handle with the classical AL protocol, and we thus randomly re-split the ACE dataset (with a ratio of 7:1:2 for training, dev, and test sets, respec- tively). Table 1 shows data statistics. For each AL experiment, we take the original training set as the unlabeled pool, down-sample a dev set from the original dev set, and evaluate on the full test set. More Settings. All of our models are based on the pre-trained RoBERTabase as the"}