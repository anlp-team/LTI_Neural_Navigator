{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Exploring_Speech_Recognition,_Translation,_and_Understanding_with_Discrete_Speech_Units:_A_Comparative_Study_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What dataset is used for intent classification accuracy in Table 6?", "answer": " Dataset FBank Discrete tokens SSL SLURP", "ref_chunk": "tasks. Dataset FBank Discrete tokens SSL (top line) MuST-C En-De MuST-C En-Es MuST-C En-Fr 26.7 31.2 37.1 28.6 33.0 38.7 29.7 33.7 40.2 Table 6: Intent classification accuracy (%) of SLURP dataset. Dataset FBank Discrete tokens SSL SLURP 86.9 / 86.3 81.8 / 80.8 84.2 / 83.3 method still has the upper hand over the others. Nevertheless, we aim to continue exploring various settings in our future work. 3.3. Speech Translation Results Table 5 shows the ST results; these models use CTC / attention fol- lowing [43, 44]. Similar to the ASR experiments in Section 3.2, the performance of ST using discrete units is slightly worse than that using the SSL features but better than FBank. 3.4. Spoken Language Understanding Results We conducted experiments on an SLU task using the SLURP [45] dataset. The intent classification accuracy is presented in Table 6. Unlike the other experiments, in this preliminary result, we did not observe an improvement in both SSL continuous features and dis- crete tokens. We hypothesize that the 21st layer of the WavLM large selected following the ASR experiments may not be optimal for SLU purposes due to the dependency of the optimal layer on the down- stream task, as reported in [12, 26]. 4. CONCLUSION This paper explores the efficacy of incorporating discrete speech units as inputs across a spectrum of speech processing tasks, en- compassing ASR, ST, and SLU. To evaluate the versatility of discrete units in diverse scenarios, we conducted experiments on datasets with varying characteristics. Drawing inspiration from canonical correlation analysis (CCA), we improved our choice of self-supervised learning (SSL) features, resulting in a noticeable performance enhancement. Consequently, the utilization of discrete units not only outperforms FBank features but also substantially en- hances efficiency. These findings underscore the promise of discrete unit input in speech processing. Future research avenues could delve into investigating alternative discretization techniques. 5. ACKNOWLEDGEMENTS Some experiments of this work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Sci- ence Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the A6000 GPUs used for this research. 6. REFERENCES [1] G. Hinton et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal processing magazine, vol. 29, no. 6, pp. 82\u201397, 2012. [2] Y. Qian et al., \u201cVery deep convolutional neural networks for noise ro- bust speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263\u20132276, 2016. [3] A. Graves et al., \u201cConnectionist temporal classification: Labelling un- segmented sequence data with recurrent neural networks,\u201d in Pro- ceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376. [4] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint arXiv:1211.3711, 2012. [5] J. Chorowski et al., \u201cAttention-based models for speech recogni- tion,\u201d Advances in Neural Information Processing Systems, vol. 2015, pp. 577\u2013585, 2015. [6] A. Vaswani et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017, pp. 5998\u20136008. [7] A. Gulati et al., \u201cConformer: Convolution-augmented Transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [8] P. Guo et al., \u201cRecent developments on espnet toolkit boosted by con- former,\u201d in Proc. ICASSP, 2021, pp. 5874\u20135878. [9] K. Kim et al., \u201cE-branchformer: Branchformer with enhanced merg- ing for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [10] S. Schneider et al., \u201cWav2vec: Unsupervised pre-training for speech recognition,\u201d Proc. Interspeech 2019, pp. 3465\u20133469, 2019. [11] A. Baevski, S. Schneider, and M. Auli, \u201cVq-wav2vec: Self-supervised learning of discrete speech representations,\u201d in Proc. ICLR, 2019. [12] A. Baevski et al., \u201cWav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020. [13] W.-N. Hsu et al., \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u2013 3460, 2021. [14] S. Chen et al., \u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Sig- nal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [15] A. Radford et al., \u201cRobust speech recognition via large-scale weak supervision,\u201d in Proc. ICML, 2023, pp. 28 492\u201328 518. [16] T. N. Sainath et al., \u201cLearning the speech front-end with raw wave- form CLDNNs,\u201d Learning, 2015. [17] X. Chang et al., \u201cExploration of efficient end-to-end asr using discretized input from self-supervised learning,\u201d arXiv preprint arXiv:2305.18108, 2023. [18] A. Baevski and A. Mohamed, \u201cEffectiveness of self-supervised pre- training for asr,\u201d in Proc. ICASSP, 2020, pp. 7694\u20137698. [19] D. Zhang et al., \u201cDub: Discrete unit back-translation for speech trans- lation,\u201d arXiv preprint arXiv:2305.11411, 2023. [20] M. Kim et al., \u201cMany-to-many spoken language translation via uni- fied speech and text representation learning with unit-to-unit transla- tion,\u201d arXiv preprint arXiv:2308.01831, 2023. [21] T. Hayashi and S. Watanabe, \u201cDiscretalk: Text-to-speech as a machine translation problem,\u201d arXiv preprint arXiv:2005.05525, 2020. [22] J. Shi et al., \u201cDiscretization and re-synthesis: An alternative method to solve the cocktail party problem,\u201d arXiv preprint arXiv:2112.09382, 2021. [23] Z. Borsos et al., \u201cAudiolm: A language modeling approach to au- dio generation,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2023. [24] C. Wang et al., \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv preprint arXiv:2301.02111, 2023. [25] P. K. Rubenstein et al., \u201cAudiopalm: A large language model that can speak and listen,\u201d arXiv preprint arXiv:2306.12925, 2023. [26] A. Pasad, B. Shi, and K. Livescu, \u201cComparative layer-wise analysis of self-supervised speech models,\u201d in Proc. ICASSP, 2023, pp. 1\u20135. [27] A. D\u00b4efossez et al., \u201cHigh fidelity neural audio compression,\u201d arXiv preprint arXiv:2210.13438, 2022. [28] S. Watanabe et al., \u201cESPnet: End-to-end speech processing toolkit,\u201d in Proc. Interspeech, 2018, pp. 2207\u20132211. [29] A. Van Den Oord, O. Vinyals, et al., \u201cNeural discrete representa- tion learning,\u201d Advances in neural information processing systems, vol. 30, 2017."}, {"question": " What is the intent classification accuracy (%) for MuST-C En-De in Table 6?", "answer": " 31.2", "ref_chunk": "tasks. Dataset FBank Discrete tokens SSL (top line) MuST-C En-De MuST-C En-Es MuST-C En-Fr 26.7 31.2 37.1 28.6 33.0 38.7 29.7 33.7 40.2 Table 6: Intent classification accuracy (%) of SLURP dataset. Dataset FBank Discrete tokens SSL SLURP 86.9 / 86.3 81.8 / 80.8 84.2 / 83.3 method still has the upper hand over the others. Nevertheless, we aim to continue exploring various settings in our future work. 3.3. Speech Translation Results Table 5 shows the ST results; these models use CTC / attention fol- lowing [43, 44]. Similar to the ASR experiments in Section 3.2, the performance of ST using discrete units is slightly worse than that using the SSL features but better than FBank. 3.4. Spoken Language Understanding Results We conducted experiments on an SLU task using the SLURP [45] dataset. The intent classification accuracy is presented in Table 6. Unlike the other experiments, in this preliminary result, we did not observe an improvement in both SSL continuous features and dis- crete tokens. We hypothesize that the 21st layer of the WavLM large selected following the ASR experiments may not be optimal for SLU purposes due to the dependency of the optimal layer on the down- stream task, as reported in [12, 26]. 4. CONCLUSION This paper explores the efficacy of incorporating discrete speech units as inputs across a spectrum of speech processing tasks, en- compassing ASR, ST, and SLU. To evaluate the versatility of discrete units in diverse scenarios, we conducted experiments on datasets with varying characteristics. Drawing inspiration from canonical correlation analysis (CCA), we improved our choice of self-supervised learning (SSL) features, resulting in a noticeable performance enhancement. Consequently, the utilization of discrete units not only outperforms FBank features but also substantially en- hances efficiency. These findings underscore the promise of discrete unit input in speech processing. Future research avenues could delve into investigating alternative discretization techniques. 5. ACKNOWLEDGEMENTS Some experiments of this work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Sci- ence Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the A6000 GPUs used for this research. 6. REFERENCES [1] G. Hinton et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal processing magazine, vol. 29, no. 6, pp. 82\u201397, 2012. [2] Y. Qian et al., \u201cVery deep convolutional neural networks for noise ro- bust speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263\u20132276, 2016. [3] A. Graves et al., \u201cConnectionist temporal classification: Labelling un- segmented sequence data with recurrent neural networks,\u201d in Pro- ceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376. [4] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint arXiv:1211.3711, 2012. [5] J. Chorowski et al., \u201cAttention-based models for speech recogni- tion,\u201d Advances in Neural Information Processing Systems, vol. 2015, pp. 577\u2013585, 2015. [6] A. Vaswani et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017, pp. 5998\u20136008. [7] A. Gulati et al., \u201cConformer: Convolution-augmented Transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [8] P. Guo et al., \u201cRecent developments on espnet toolkit boosted by con- former,\u201d in Proc. ICASSP, 2021, pp. 5874\u20135878. [9] K. Kim et al., \u201cE-branchformer: Branchformer with enhanced merg- ing for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [10] S. Schneider et al., \u201cWav2vec: Unsupervised pre-training for speech recognition,\u201d Proc. Interspeech 2019, pp. 3465\u20133469, 2019. [11] A. Baevski, S. Schneider, and M. Auli, \u201cVq-wav2vec: Self-supervised learning of discrete speech representations,\u201d in Proc. ICLR, 2019. [12] A. Baevski et al., \u201cWav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020. [13] W.-N. Hsu et al., \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u2013 3460, 2021. [14] S. Chen et al., \u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Sig- nal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [15] A. Radford et al., \u201cRobust speech recognition via large-scale weak supervision,\u201d in Proc. ICML, 2023, pp. 28 492\u201328 518. [16] T. N. Sainath et al., \u201cLearning the speech front-end with raw wave- form CLDNNs,\u201d Learning, 2015. [17] X. Chang et al., \u201cExploration of efficient end-to-end asr using discretized input from self-supervised learning,\u201d arXiv preprint arXiv:2305.18108, 2023. [18] A. Baevski and A. Mohamed, \u201cEffectiveness of self-supervised pre- training for asr,\u201d in Proc. ICASSP, 2020, pp. 7694\u20137698. [19] D. Zhang et al., \u201cDub: Discrete unit back-translation for speech trans- lation,\u201d arXiv preprint arXiv:2305.11411, 2023. [20] M. Kim et al., \u201cMany-to-many spoken language translation via uni- fied speech and text representation learning with unit-to-unit transla- tion,\u201d arXiv preprint arXiv:2308.01831, 2023. [21] T. Hayashi and S. Watanabe, \u201cDiscretalk: Text-to-speech as a machine translation problem,\u201d arXiv preprint arXiv:2005.05525, 2020. [22] J. Shi et al., \u201cDiscretization and re-synthesis: An alternative method to solve the cocktail party problem,\u201d arXiv preprint arXiv:2112.09382, 2021. [23] Z. Borsos et al., \u201cAudiolm: A language modeling approach to au- dio generation,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2023. [24] C. Wang et al., \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv preprint arXiv:2301.02111, 2023. [25] P. K. Rubenstein et al., \u201cAudiopalm: A large language model that can speak and listen,\u201d arXiv preprint arXiv:2306.12925, 2023. [26] A. Pasad, B. Shi, and K. Livescu, \u201cComparative layer-wise analysis of self-supervised speech models,\u201d in Proc. ICASSP, 2023, pp. 1\u20135. [27] A. D\u00b4efossez et al., \u201cHigh fidelity neural audio compression,\u201d arXiv preprint arXiv:2210.13438, 2022. [28] S. Watanabe et al., \u201cESPnet: End-to-end speech processing toolkit,\u201d in Proc. Interspeech, 2018, pp. 2207\u20132211. [29] A. Van Den Oord, O. Vinyals, et al., \u201cNeural discrete representa- tion learning,\u201d Advances in neural information processing systems, vol. 30, 2017."}, {"question": " What type of models are used for Speech Translation results?", "answer": " CTC / attention", "ref_chunk": "tasks. Dataset FBank Discrete tokens SSL (top line) MuST-C En-De MuST-C En-Es MuST-C En-Fr 26.7 31.2 37.1 28.6 33.0 38.7 29.7 33.7 40.2 Table 6: Intent classification accuracy (%) of SLURP dataset. Dataset FBank Discrete tokens SSL SLURP 86.9 / 86.3 81.8 / 80.8 84.2 / 83.3 method still has the upper hand over the others. Nevertheless, we aim to continue exploring various settings in our future work. 3.3. Speech Translation Results Table 5 shows the ST results; these models use CTC / attention fol- lowing [43, 44]. Similar to the ASR experiments in Section 3.2, the performance of ST using discrete units is slightly worse than that using the SSL features but better than FBank. 3.4. Spoken Language Understanding Results We conducted experiments on an SLU task using the SLURP [45] dataset. The intent classification accuracy is presented in Table 6. Unlike the other experiments, in this preliminary result, we did not observe an improvement in both SSL continuous features and dis- crete tokens. We hypothesize that the 21st layer of the WavLM large selected following the ASR experiments may not be optimal for SLU purposes due to the dependency of the optimal layer on the down- stream task, as reported in [12, 26]. 4. CONCLUSION This paper explores the efficacy of incorporating discrete speech units as inputs across a spectrum of speech processing tasks, en- compassing ASR, ST, and SLU. To evaluate the versatility of discrete units in diverse scenarios, we conducted experiments on datasets with varying characteristics. Drawing inspiration from canonical correlation analysis (CCA), we improved our choice of self-supervised learning (SSL) features, resulting in a noticeable performance enhancement. Consequently, the utilization of discrete units not only outperforms FBank features but also substantially en- hances efficiency. These findings underscore the promise of discrete unit input in speech processing. Future research avenues could delve into investigating alternative discretization techniques. 5. ACKNOWLEDGEMENTS Some experiments of this work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Sci- ence Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the A6000 GPUs used for this research. 6. REFERENCES [1] G. Hinton et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal processing magazine, vol. 29, no. 6, pp. 82\u201397, 2012. [2] Y. Qian et al., \u201cVery deep convolutional neural networks for noise ro- bust speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263\u20132276, 2016. [3] A. Graves et al., \u201cConnectionist temporal classification: Labelling un- segmented sequence data with recurrent neural networks,\u201d in Pro- ceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376. [4] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint arXiv:1211.3711, 2012. [5] J. Chorowski et al., \u201cAttention-based models for speech recogni- tion,\u201d Advances in Neural Information Processing Systems, vol. 2015, pp. 577\u2013585, 2015. [6] A. Vaswani et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017, pp. 5998\u20136008. [7] A. Gulati et al., \u201cConformer: Convolution-augmented Transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [8] P. Guo et al., \u201cRecent developments on espnet toolkit boosted by con- former,\u201d in Proc. ICASSP, 2021, pp. 5874\u20135878. [9] K. Kim et al., \u201cE-branchformer: Branchformer with enhanced merg- ing for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [10] S. Schneider et al., \u201cWav2vec: Unsupervised pre-training for speech recognition,\u201d Proc. Interspeech 2019, pp. 3465\u20133469, 2019. [11] A. Baevski, S. Schneider, and M. Auli, \u201cVq-wav2vec: Self-supervised learning of discrete speech representations,\u201d in Proc. ICLR, 2019. [12] A. Baevski et al., \u201cWav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020. [13] W.-N. Hsu et al., \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u2013 3460, 2021. [14] S. Chen et al., \u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Sig- nal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [15] A. Radford et al., \u201cRobust speech recognition via large-scale weak supervision,\u201d in Proc. ICML, 2023, pp. 28 492\u201328 518. [16] T. N. Sainath et al., \u201cLearning the speech front-end with raw wave- form CLDNNs,\u201d Learning, 2015. [17] X. Chang et al., \u201cExploration of efficient end-to-end asr using discretized input from self-supervised learning,\u201d arXiv preprint arXiv:2305.18108, 2023. [18] A. Baevski and A. Mohamed, \u201cEffectiveness of self-supervised pre- training for asr,\u201d in Proc. ICASSP, 2020, pp. 7694\u20137698. [19] D. Zhang et al., \u201cDub: Discrete unit back-translation for speech trans- lation,\u201d arXiv preprint arXiv:2305.11411, 2023. [20] M. Kim et al., \u201cMany-to-many spoken language translation via uni- fied speech and text representation learning with unit-to-unit transla- tion,\u201d arXiv preprint arXiv:2308.01831, 2023. [21] T. Hayashi and S. Watanabe, \u201cDiscretalk: Text-to-speech as a machine translation problem,\u201d arXiv preprint arXiv:2005.05525, 2020. [22] J. Shi et al., \u201cDiscretization and re-synthesis: An alternative method to solve the cocktail party problem,\u201d arXiv preprint arXiv:2112.09382, 2021. [23] Z. Borsos et al., \u201cAudiolm: A language modeling approach to au- dio generation,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2023. [24] C. Wang et al., \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv preprint arXiv:2301.02111, 2023. [25] P. K. Rubenstein et al., \u201cAudiopalm: A large language model that can speak and listen,\u201d arXiv preprint arXiv:2306.12925, 2023. [26] A. Pasad, B. Shi, and K. Livescu, \u201cComparative layer-wise analysis of self-supervised speech models,\u201d in Proc. ICASSP, 2023, pp. 1\u20135. [27] A. D\u00b4efossez et al., \u201cHigh fidelity neural audio compression,\u201d arXiv preprint arXiv:2210.13438, 2022. [28] S. Watanabe et al., \u201cESPnet: End-to-end speech processing toolkit,\u201d in Proc. Interspeech, 2018, pp. 2207\u20132211. [29] A. Van Den Oord, O. Vinyals, et al., \u201cNeural discrete representa- tion learning,\u201d Advances in neural information processing systems, vol. 30, 2017."}, {"question": " How does the performance of ST using discrete units compare to SSL features and FBank?", "answer": " Slightly worse than SSL features but better than FBank", "ref_chunk": "tasks. Dataset FBank Discrete tokens SSL (top line) MuST-C En-De MuST-C En-Es MuST-C En-Fr 26.7 31.2 37.1 28.6 33.0 38.7 29.7 33.7 40.2 Table 6: Intent classification accuracy (%) of SLURP dataset. Dataset FBank Discrete tokens SSL SLURP 86.9 / 86.3 81.8 / 80.8 84.2 / 83.3 method still has the upper hand over the others. Nevertheless, we aim to continue exploring various settings in our future work. 3.3. Speech Translation Results Table 5 shows the ST results; these models use CTC / attention fol- lowing [43, 44]. Similar to the ASR experiments in Section 3.2, the performance of ST using discrete units is slightly worse than that using the SSL features but better than FBank. 3.4. Spoken Language Understanding Results We conducted experiments on an SLU task using the SLURP [45] dataset. The intent classification accuracy is presented in Table 6. Unlike the other experiments, in this preliminary result, we did not observe an improvement in both SSL continuous features and dis- crete tokens. We hypothesize that the 21st layer of the WavLM large selected following the ASR experiments may not be optimal for SLU purposes due to the dependency of the optimal layer on the down- stream task, as reported in [12, 26]. 4. CONCLUSION This paper explores the efficacy of incorporating discrete speech units as inputs across a spectrum of speech processing tasks, en- compassing ASR, ST, and SLU. To evaluate the versatility of discrete units in diverse scenarios, we conducted experiments on datasets with varying characteristics. Drawing inspiration from canonical correlation analysis (CCA), we improved our choice of self-supervised learning (SSL) features, resulting in a noticeable performance enhancement. Consequently, the utilization of discrete units not only outperforms FBank features but also substantially en- hances efficiency. These findings underscore the promise of discrete unit input in speech processing. Future research avenues could delve into investigating alternative discretization techniques. 5. ACKNOWLEDGEMENTS Some experiments of this work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Sci- ence Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the A6000 GPUs used for this research. 6. REFERENCES [1] G. Hinton et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal processing magazine, vol. 29, no. 6, pp. 82\u201397, 2012. [2] Y. Qian et al., \u201cVery deep convolutional neural networks for noise ro- bust speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263\u20132276, 2016. [3] A. Graves et al., \u201cConnectionist temporal classification: Labelling un- segmented sequence data with recurrent neural networks,\u201d in Pro- ceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376. [4] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint arXiv:1211.3711, 2012. [5] J. Chorowski et al., \u201cAttention-based models for speech recogni- tion,\u201d Advances in Neural Information Processing Systems, vol. 2015, pp. 577\u2013585, 2015. [6] A. Vaswani et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017, pp. 5998\u20136008. [7] A. Gulati et al., \u201cConformer: Convolution-augmented Transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [8] P. Guo et al., \u201cRecent developments on espnet toolkit boosted by con- former,\u201d in Proc. ICASSP, 2021, pp. 5874\u20135878. [9] K. Kim et al., \u201cE-branchformer: Branchformer with enhanced merg- ing for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [10] S. Schneider et al., \u201cWav2vec: Unsupervised pre-training for speech recognition,\u201d Proc. Interspeech 2019, pp. 3465\u20133469, 2019. [11] A. Baevski, S. Schneider, and M. Auli, \u201cVq-wav2vec: Self-supervised learning of discrete speech representations,\u201d in Proc. ICLR, 2019. [12] A. Baevski et al., \u201cWav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020. [13] W.-N. Hsu et al., \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u2013 3460, 2021. [14] S. Chen et al., \u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Sig- nal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [15] A. Radford et al., \u201cRobust speech recognition via large-scale weak supervision,\u201d in Proc. ICML, 2023, pp. 28 492\u201328 518. [16] T. N. Sainath et al., \u201cLearning the speech front-end with raw wave- form CLDNNs,\u201d Learning, 2015. [17] X. Chang et al., \u201cExploration of efficient end-to-end asr using discretized input from self-supervised learning,\u201d arXiv preprint arXiv:2305.18108, 2023. [18] A. Baevski and A. Mohamed, \u201cEffectiveness of self-supervised pre- training for asr,\u201d in Proc. ICASSP, 2020, pp. 7694\u20137698. [19] D. Zhang et al., \u201cDub: Discrete unit back-translation for speech trans- lation,\u201d arXiv preprint arXiv:2305.11411, 2023. [20] M. Kim et al., \u201cMany-to-many spoken language translation via uni- fied speech and text representation learning with unit-to-unit transla- tion,\u201d arXiv preprint arXiv:2308.01831, 2023. [21] T. Hayashi and S. Watanabe, \u201cDiscretalk: Text-to-speech as a machine translation problem,\u201d arXiv preprint arXiv:2005.05525, 2020. [22] J. Shi et al., \u201cDiscretization and re-synthesis: An alternative method to solve the cocktail party problem,\u201d arXiv preprint arXiv:2112.09382, 2021. [23] Z. Borsos et al., \u201cAudiolm: A language modeling approach to au- dio generation,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2023. [24] C. Wang et al., \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv preprint arXiv:2301.02111, 2023. [25] P. K. Rubenstein et al., \u201cAudiopalm: A large language model that can speak and listen,\u201d arXiv preprint arXiv:2306.12925, 2023. [26] A. Pasad, B. Shi, and K. Livescu, \u201cComparative layer-wise analysis of self-supervised speech models,\u201d in Proc. ICASSP, 2023, pp. 1\u20135. [27] A. D\u00b4efossez et al., \u201cHigh fidelity neural audio compression,\u201d arXiv preprint arXiv:2210.13438, 2022. [28] S. Watanabe et al., \u201cESPnet: End-to-end speech processing toolkit,\u201d in Proc. Interspeech, 2018, pp. 2207\u20132211. [29] A. Van Den Oord, O. Vinyals, et al., \u201cNeural discrete representa- tion learning,\u201d Advances in neural information processing systems, vol. 30, 2017."}, {"question": " What dataset was used for experiments on an SLU task?", "answer": " SLURP", "ref_chunk": "tasks. Dataset FBank Discrete tokens SSL (top line) MuST-C En-De MuST-C En-Es MuST-C En-Fr 26.7 31.2 37.1 28.6 33.0 38.7 29.7 33.7 40.2 Table 6: Intent classification accuracy (%) of SLURP dataset. Dataset FBank Discrete tokens SSL SLURP 86.9 / 86.3 81.8 / 80.8 84.2 / 83.3 method still has the upper hand over the others. Nevertheless, we aim to continue exploring various settings in our future work. 3.3. Speech Translation Results Table 5 shows the ST results; these models use CTC / attention fol- lowing [43, 44]. Similar to the ASR experiments in Section 3.2, the performance of ST using discrete units is slightly worse than that using the SSL features but better than FBank. 3.4. Spoken Language Understanding Results We conducted experiments on an SLU task using the SLURP [45] dataset. The intent classification accuracy is presented in Table 6. Unlike the other experiments, in this preliminary result, we did not observe an improvement in both SSL continuous features and dis- crete tokens. We hypothesize that the 21st layer of the WavLM large selected following the ASR experiments may not be optimal for SLU purposes due to the dependency of the optimal layer on the down- stream task, as reported in [12, 26]. 4. CONCLUSION This paper explores the efficacy of incorporating discrete speech units as inputs across a spectrum of speech processing tasks, en- compassing ASR, ST, and SLU. To evaluate the versatility of discrete units in diverse scenarios, we conducted experiments on datasets with varying characteristics. Drawing inspiration from canonical correlation analysis (CCA), we improved our choice of self-supervised learning (SSL) features, resulting in a noticeable performance enhancement. Consequently, the utilization of discrete units not only outperforms FBank features but also substantially en- hances efficiency. These findings underscore the promise of discrete unit input in speech processing. Future research avenues could delve into investigating alternative discretization techniques. 5. ACKNOWLEDGEMENTS Some experiments of this work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Sci- ence Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the A6000 GPUs used for this research. 6. REFERENCES [1] G. Hinton et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal processing magazine, vol. 29, no. 6, pp. 82\u201397, 2012. [2] Y. Qian et al., \u201cVery deep convolutional neural networks for noise ro- bust speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263\u20132276, 2016. [3] A. Graves et al., \u201cConnectionist temporal classification: Labelling un- segmented sequence data with recurrent neural networks,\u201d in Pro- ceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376. [4] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint arXiv:1211.3711, 2012. [5] J. Chorowski et al., \u201cAttention-based models for speech recogni- tion,\u201d Advances in Neural Information Processing Systems, vol. 2015, pp. 577\u2013585, 2015. [6] A. Vaswani et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017, pp. 5998\u20136008. [7] A. Gulati et al., \u201cConformer: Convolution-augmented Transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [8] P. Guo et al., \u201cRecent developments on espnet toolkit boosted by con- former,\u201d in Proc. ICASSP, 2021, pp. 5874\u20135878. [9] K. Kim et al., \u201cE-branchformer: Branchformer with enhanced merg- ing for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [10] S. Schneider et al., \u201cWav2vec: Unsupervised pre-training for speech recognition,\u201d Proc. Interspeech 2019, pp. 3465\u20133469, 2019. [11] A. Baevski, S. Schneider, and M. Auli, \u201cVq-wav2vec: Self-supervised learning of discrete speech representations,\u201d in Proc. ICLR, 2019. [12] A. Baevski et al., \u201cWav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020. [13] W.-N. Hsu et al., \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u2013 3460, 2021. [14] S. Chen et al., \u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Sig- nal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [15] A. Radford et al., \u201cRobust speech recognition via large-scale weak supervision,\u201d in Proc. ICML, 2023, pp. 28 492\u201328 518. [16] T. N. Sainath et al., \u201cLearning the speech front-end with raw wave- form CLDNNs,\u201d Learning, 2015. [17] X. Chang et al., \u201cExploration of efficient end-to-end asr using discretized input from self-supervised learning,\u201d arXiv preprint arXiv:2305.18108, 2023. [18] A. Baevski and A. Mohamed, \u201cEffectiveness of self-supervised pre- training for asr,\u201d in Proc. ICASSP, 2020, pp. 7694\u20137698. [19] D. Zhang et al., \u201cDub: Discrete unit back-translation for speech trans- lation,\u201d arXiv preprint arXiv:2305.11411, 2023. [20] M. Kim et al., \u201cMany-to-many spoken language translation via uni- fied speech and text representation learning with unit-to-unit transla- tion,\u201d arXiv preprint arXiv:2308.01831, 2023. [21] T. Hayashi and S. Watanabe, \u201cDiscretalk: Text-to-speech as a machine translation problem,\u201d arXiv preprint arXiv:2005.05525, 2020. [22] J. Shi et al., \u201cDiscretization and re-synthesis: An alternative method to solve the cocktail party problem,\u201d arXiv preprint arXiv:2112.09382, 2021. [23] Z. Borsos et al., \u201cAudiolm: A language modeling approach to au- dio generation,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2023. [24] C. Wang et al., \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv preprint arXiv:2301.02111, 2023. [25] P. K. Rubenstein et al., \u201cAudiopalm: A large language model that can speak and listen,\u201d arXiv preprint arXiv:2306.12925, 2023. [26] A. Pasad, B. Shi, and K. Livescu, \u201cComparative layer-wise analysis of self-supervised speech models,\u201d in Proc. ICASSP, 2023, pp. 1\u20135. [27] A. D\u00b4efossez et al., \u201cHigh fidelity neural audio compression,\u201d arXiv preprint arXiv:2210.13438, 2022. [28] S. Watanabe et al., \u201cESPnet: End-to-end speech processing toolkit,\u201d in Proc. Interspeech, 2018, pp. 2207\u20132211. [29] A. Van Den Oord, O. Vinyals, et al., \u201cNeural discrete representa- tion learning,\u201d Advances in neural information processing systems, vol. 30, 2017."}, {"question": " What did the authors not observe in the preliminary result of the SLU task?", "answer": " An improvement in both SSL continuous features and discrete tokens", "ref_chunk": "tasks. Dataset FBank Discrete tokens SSL (top line) MuST-C En-De MuST-C En-Es MuST-C En-Fr 26.7 31.2 37.1 28.6 33.0 38.7 29.7 33.7 40.2 Table 6: Intent classification accuracy (%) of SLURP dataset. Dataset FBank Discrete tokens SSL SLURP 86.9 / 86.3 81.8 / 80.8 84.2 / 83.3 method still has the upper hand over the others. Nevertheless, we aim to continue exploring various settings in our future work. 3.3. Speech Translation Results Table 5 shows the ST results; these models use CTC / attention fol- lowing [43, 44]. Similar to the ASR experiments in Section 3.2, the performance of ST using discrete units is slightly worse than that using the SSL features but better than FBank. 3.4. Spoken Language Understanding Results We conducted experiments on an SLU task using the SLURP [45] dataset. The intent classification accuracy is presented in Table 6. Unlike the other experiments, in this preliminary result, we did not observe an improvement in both SSL continuous features and dis- crete tokens. We hypothesize that the 21st layer of the WavLM large selected following the ASR experiments may not be optimal for SLU purposes due to the dependency of the optimal layer on the down- stream task, as reported in [12, 26]. 4. CONCLUSION This paper explores the efficacy of incorporating discrete speech units as inputs across a spectrum of speech processing tasks, en- compassing ASR, ST, and SLU. To evaluate the versatility of discrete units in diverse scenarios, we conducted experiments on datasets with varying characteristics. Drawing inspiration from canonical correlation analysis (CCA), we improved our choice of self-supervised learning (SSL) features, resulting in a noticeable performance enhancement. Consequently, the utilization of discrete units not only outperforms FBank features but also substantially en- hances efficiency. These findings underscore the promise of discrete unit input in speech processing. Future research avenues could delve into investigating alternative discretization techniques. 5. ACKNOWLEDGEMENTS Some experiments of this work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Sci- ence Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the A6000 GPUs used for this research. 6. REFERENCES [1] G. Hinton et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal processing magazine, vol. 29, no. 6, pp. 82\u201397, 2012. [2] Y. Qian et al., \u201cVery deep convolutional neural networks for noise ro- bust speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263\u20132276, 2016. [3] A. Graves et al., \u201cConnectionist temporal classification: Labelling un- segmented sequence data with recurrent neural networks,\u201d in Pro- ceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376. [4] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint arXiv:1211.3711, 2012. [5] J. Chorowski et al., \u201cAttention-based models for speech recogni- tion,\u201d Advances in Neural Information Processing Systems, vol. 2015, pp. 577\u2013585, 2015. [6] A. Vaswani et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017, pp. 5998\u20136008. [7] A. Gulati et al., \u201cConformer: Convolution-augmented Transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [8] P. Guo et al., \u201cRecent developments on espnet toolkit boosted by con- former,\u201d in Proc. ICASSP, 2021, pp. 5874\u20135878. [9] K. Kim et al., \u201cE-branchformer: Branchformer with enhanced merg- ing for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [10] S. Schneider et al., \u201cWav2vec: Unsupervised pre-training for speech recognition,\u201d Proc. Interspeech 2019, pp. 3465\u20133469, 2019. [11] A. Baevski, S. Schneider, and M. Auli, \u201cVq-wav2vec: Self-supervised learning of discrete speech representations,\u201d in Proc. ICLR, 2019. [12] A. Baevski et al., \u201cWav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020. [13] W.-N. Hsu et al., \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u2013 3460, 2021. [14] S. Chen et al., \u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Sig- nal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [15] A. Radford et al., \u201cRobust speech recognition via large-scale weak supervision,\u201d in Proc. ICML, 2023, pp. 28 492\u201328 518. [16] T. N. Sainath et al., \u201cLearning the speech front-end with raw wave- form CLDNNs,\u201d Learning, 2015. [17] X. Chang et al., \u201cExploration of efficient end-to-end asr using discretized input from self-supervised learning,\u201d arXiv preprint arXiv:2305.18108, 2023. [18] A. Baevski and A. Mohamed, \u201cEffectiveness of self-supervised pre- training for asr,\u201d in Proc. ICASSP, 2020, pp. 7694\u20137698. [19] D. Zhang et al., \u201cDub: Discrete unit back-translation for speech trans- lation,\u201d arXiv preprint arXiv:2305.11411, 2023. [20] M. Kim et al., \u201cMany-to-many spoken language translation via uni- fied speech and text representation learning with unit-to-unit transla- tion,\u201d arXiv preprint arXiv:2308.01831, 2023. [21] T. Hayashi and S. Watanabe, \u201cDiscretalk: Text-to-speech as a machine translation problem,\u201d arXiv preprint arXiv:2005.05525, 2020. [22] J. Shi et al., \u201cDiscretization and re-synthesis: An alternative method to solve the cocktail party problem,\u201d arXiv preprint arXiv:2112.09382, 2021. [23] Z. Borsos et al., \u201cAudiolm: A language modeling approach to au- dio generation,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2023. [24] C. Wang et al., \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv preprint arXiv:2301.02111, 2023. [25] P. K. Rubenstein et al., \u201cAudiopalm: A large language model that can speak and listen,\u201d arXiv preprint arXiv:2306.12925, 2023. [26] A. Pasad, B. Shi, and K. Livescu, \u201cComparative layer-wise analysis of self-supervised speech models,\u201d in Proc. ICASSP, 2023, pp. 1\u20135. [27] A. D\u00b4efossez et al., \u201cHigh fidelity neural audio compression,\u201d arXiv preprint arXiv:2210.13438, 2022. [28] S. Watanabe et al., \u201cESPnet: End-to-end speech processing toolkit,\u201d in Proc. Interspeech, 2018, pp. 2207\u20132211. [29] A. Van Den Oord, O. Vinyals, et al., \u201cNeural discrete representa- tion learning,\u201d Advances in neural information processing systems, vol. 30, 2017."}, {"question": " What did the authors hypothesize regarding the 21st layer of the WavLM large?", "answer": " It may not be optimal for SLU purposes", "ref_chunk": "tasks. Dataset FBank Discrete tokens SSL (top line) MuST-C En-De MuST-C En-Es MuST-C En-Fr 26.7 31.2 37.1 28.6 33.0 38.7 29.7 33.7 40.2 Table 6: Intent classification accuracy (%) of SLURP dataset. Dataset FBank Discrete tokens SSL SLURP 86.9 / 86.3 81.8 / 80.8 84.2 / 83.3 method still has the upper hand over the others. Nevertheless, we aim to continue exploring various settings in our future work. 3.3. Speech Translation Results Table 5 shows the ST results; these models use CTC / attention fol- lowing [43, 44]. Similar to the ASR experiments in Section 3.2, the performance of ST using discrete units is slightly worse than that using the SSL features but better than FBank. 3.4. Spoken Language Understanding Results We conducted experiments on an SLU task using the SLURP [45] dataset. The intent classification accuracy is presented in Table 6. Unlike the other experiments, in this preliminary result, we did not observe an improvement in both SSL continuous features and dis- crete tokens. We hypothesize that the 21st layer of the WavLM large selected following the ASR experiments may not be optimal for SLU purposes due to the dependency of the optimal layer on the down- stream task, as reported in [12, 26]. 4. CONCLUSION This paper explores the efficacy of incorporating discrete speech units as inputs across a spectrum of speech processing tasks, en- compassing ASR, ST, and SLU. To evaluate the versatility of discrete units in diverse scenarios, we conducted experiments on datasets with varying characteristics. Drawing inspiration from canonical correlation analysis (CCA), we improved our choice of self-supervised learning (SSL) features, resulting in a noticeable performance enhancement. Consequently, the utilization of discrete units not only outperforms FBank features but also substantially en- hances efficiency. These findings underscore the promise of discrete unit input in speech processing. Future research avenues could delve into investigating alternative discretization techniques. 5. ACKNOWLEDGEMENTS Some experiments of this work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Sci- ence Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the A6000 GPUs used for this research. 6. REFERENCES [1] G. Hinton et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal processing magazine, vol. 29, no. 6, pp. 82\u201397, 2012. [2] Y. Qian et al., \u201cVery deep convolutional neural networks for noise ro- bust speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263\u20132276, 2016. [3] A. Graves et al., \u201cConnectionist temporal classification: Labelling un- segmented sequence data with recurrent neural networks,\u201d in Pro- ceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376. [4] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint arXiv:1211.3711, 2012. [5] J. Chorowski et al., \u201cAttention-based models for speech recogni- tion,\u201d Advances in Neural Information Processing Systems, vol. 2015, pp. 577\u2013585, 2015. [6] A. Vaswani et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017, pp. 5998\u20136008. [7] A. Gulati et al., \u201cConformer: Convolution-augmented Transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [8] P. Guo et al., \u201cRecent developments on espnet toolkit boosted by con- former,\u201d in Proc. ICASSP, 2021, pp. 5874\u20135878. [9] K. Kim et al., \u201cE-branchformer: Branchformer with enhanced merg- ing for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [10] S. Schneider et al., \u201cWav2vec: Unsupervised pre-training for speech recognition,\u201d Proc. Interspeech 2019, pp. 3465\u20133469, 2019. [11] A. Baevski, S. Schneider, and M. Auli, \u201cVq-wav2vec: Self-supervised learning of discrete speech representations,\u201d in Proc. ICLR, 2019. [12] A. Baevski et al., \u201cWav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020. [13] W.-N. Hsu et al., \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u2013 3460, 2021. [14] S. Chen et al., \u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Sig- nal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [15] A. Radford et al., \u201cRobust speech recognition via large-scale weak supervision,\u201d in Proc. ICML, 2023, pp. 28 492\u201328 518. [16] T. N. Sainath et al., \u201cLearning the speech front-end with raw wave- form CLDNNs,\u201d Learning, 2015. [17] X. Chang et al., \u201cExploration of efficient end-to-end asr using discretized input from self-supervised learning,\u201d arXiv preprint arXiv:2305.18108, 2023. [18] A. Baevski and A. Mohamed, \u201cEffectiveness of self-supervised pre- training for asr,\u201d in Proc. ICASSP, 2020, pp. 7694\u20137698. [19] D. Zhang et al., \u201cDub: Discrete unit back-translation for speech trans- lation,\u201d arXiv preprint arXiv:2305.11411, 2023. [20] M. Kim et al., \u201cMany-to-many spoken language translation via uni- fied speech and text representation learning with unit-to-unit transla- tion,\u201d arXiv preprint arXiv:2308.01831, 2023. [21] T. Hayashi and S. Watanabe, \u201cDiscretalk: Text-to-speech as a machine translation problem,\u201d arXiv preprint arXiv:2005.05525, 2020. [22] J. Shi et al., \u201cDiscretization and re-synthesis: An alternative method to solve the cocktail party problem,\u201d arXiv preprint arXiv:2112.09382, 2021. [23] Z. Borsos et al., \u201cAudiolm: A language modeling approach to au- dio generation,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2023. [24] C. Wang et al., \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv preprint arXiv:2301.02111, 2023. [25] P. K. Rubenstein et al., \u201cAudiopalm: A large language model that can speak and listen,\u201d arXiv preprint arXiv:2306.12925, 2023. [26] A. Pasad, B. Shi, and K. Livescu, \u201cComparative layer-wise analysis of self-supervised speech models,\u201d in Proc. ICASSP, 2023, pp. 1\u20135. [27] A. D\u00b4efossez et al., \u201cHigh fidelity neural audio compression,\u201d arXiv preprint arXiv:2210.13438, 2022. [28] S. Watanabe et al., \u201cESPnet: End-to-end speech processing toolkit,\u201d in Proc. Interspeech, 2018, pp. 2207\u20132211. [29] A. Van Den Oord, O. Vinyals, et al., \u201cNeural discrete representa- tion learning,\u201d Advances in neural information processing systems, vol. 30, 2017."}, {"question": " What does the paper explore the efficacy of incorporating as inputs across a spectrum of speech processing tasks?", "answer": " Discrete speech units", "ref_chunk": "tasks. Dataset FBank Discrete tokens SSL (top line) MuST-C En-De MuST-C En-Es MuST-C En-Fr 26.7 31.2 37.1 28.6 33.0 38.7 29.7 33.7 40.2 Table 6: Intent classification accuracy (%) of SLURP dataset. Dataset FBank Discrete tokens SSL SLURP 86.9 / 86.3 81.8 / 80.8 84.2 / 83.3 method still has the upper hand over the others. Nevertheless, we aim to continue exploring various settings in our future work. 3.3. Speech Translation Results Table 5 shows the ST results; these models use CTC / attention fol- lowing [43, 44]. Similar to the ASR experiments in Section 3.2, the performance of ST using discrete units is slightly worse than that using the SSL features but better than FBank. 3.4. Spoken Language Understanding Results We conducted experiments on an SLU task using the SLURP [45] dataset. The intent classification accuracy is presented in Table 6. Unlike the other experiments, in this preliminary result, we did not observe an improvement in both SSL continuous features and dis- crete tokens. We hypothesize that the 21st layer of the WavLM large selected following the ASR experiments may not be optimal for SLU purposes due to the dependency of the optimal layer on the down- stream task, as reported in [12, 26]. 4. CONCLUSION This paper explores the efficacy of incorporating discrete speech units as inputs across a spectrum of speech processing tasks, en- compassing ASR, ST, and SLU. To evaluate the versatility of discrete units in diverse scenarios, we conducted experiments on datasets with varying characteristics. Drawing inspiration from canonical correlation analysis (CCA), we improved our choice of self-supervised learning (SSL) features, resulting in a noticeable performance enhancement. Consequently, the utilization of discrete units not only outperforms FBank features but also substantially en- hances efficiency. These findings underscore the promise of discrete unit input in speech processing. Future research avenues could delve into investigating alternative discretization techniques. 5. ACKNOWLEDGEMENTS Some experiments of this work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Sci- ence Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the A6000 GPUs used for this research. 6. REFERENCES [1] G. Hinton et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal processing magazine, vol. 29, no. 6, pp. 82\u201397, 2012. [2] Y. Qian et al., \u201cVery deep convolutional neural networks for noise ro- bust speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263\u20132276, 2016. [3] A. Graves et al., \u201cConnectionist temporal classification: Labelling un- segmented sequence data with recurrent neural networks,\u201d in Pro- ceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376. [4] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint arXiv:1211.3711, 2012. [5] J. Chorowski et al., \u201cAttention-based models for speech recogni- tion,\u201d Advances in Neural Information Processing Systems, vol. 2015, pp. 577\u2013585, 2015. [6] A. Vaswani et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017, pp. 5998\u20136008. [7] A. Gulati et al., \u201cConformer: Convolution-augmented Transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [8] P. Guo et al., \u201cRecent developments on espnet toolkit boosted by con- former,\u201d in Proc. ICASSP, 2021, pp. 5874\u20135878. [9] K. Kim et al., \u201cE-branchformer: Branchformer with enhanced merg- ing for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [10] S. Schneider et al., \u201cWav2vec: Unsupervised pre-training for speech recognition,\u201d Proc. Interspeech 2019, pp. 3465\u20133469, 2019. [11] A. Baevski, S. Schneider, and M. Auli, \u201cVq-wav2vec: Self-supervised learning of discrete speech representations,\u201d in Proc. ICLR, 2019. [12] A. Baevski et al., \u201cWav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020. [13] W.-N. Hsu et al., \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u2013 3460, 2021. [14] S. Chen et al., \u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Sig- nal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [15] A. Radford et al., \u201cRobust speech recognition via large-scale weak supervision,\u201d in Proc. ICML, 2023, pp. 28 492\u201328 518. [16] T. N. Sainath et al., \u201cLearning the speech front-end with raw wave- form CLDNNs,\u201d Learning, 2015. [17] X. Chang et al., \u201cExploration of efficient end-to-end asr using discretized input from self-supervised learning,\u201d arXiv preprint arXiv:2305.18108, 2023. [18] A. Baevski and A. Mohamed, \u201cEffectiveness of self-supervised pre- training for asr,\u201d in Proc. ICASSP, 2020, pp. 7694\u20137698. [19] D. Zhang et al., \u201cDub: Discrete unit back-translation for speech trans- lation,\u201d arXiv preprint arXiv:2305.11411, 2023. [20] M. Kim et al., \u201cMany-to-many spoken language translation via uni- fied speech and text representation learning with unit-to-unit transla- tion,\u201d arXiv preprint arXiv:2308.01831, 2023. [21] T. Hayashi and S. Watanabe, \u201cDiscretalk: Text-to-speech as a machine translation problem,\u201d arXiv preprint arXiv:2005.05525, 2020. [22] J. Shi et al., \u201cDiscretization and re-synthesis: An alternative method to solve the cocktail party problem,\u201d arXiv preprint arXiv:2112.09382, 2021. [23] Z. Borsos et al., \u201cAudiolm: A language modeling approach to au- dio generation,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2023. [24] C. Wang et al., \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv preprint arXiv:2301.02111, 2023. [25] P. K. Rubenstein et al., \u201cAudiopalm: A large language model that can speak and listen,\u201d arXiv preprint arXiv:2306.12925, 2023. [26] A. Pasad, B. Shi, and K. Livescu, \u201cComparative layer-wise analysis of self-supervised speech models,\u201d in Proc. ICASSP, 2023, pp. 1\u20135. [27] A. D\u00b4efossez et al., \u201cHigh fidelity neural audio compression,\u201d arXiv preprint arXiv:2210.13438, 2022. [28] S. Watanabe et al., \u201cESPnet: End-to-end speech processing toolkit,\u201d in Proc. Interspeech, 2018, pp. 2207\u20132211. [29] A. Van Den Oord, O. Vinyals, et al., \u201cNeural discrete representa- tion learning,\u201d Advances in neural information processing systems, vol. 30, 2017."}, {"question": " What is used to improve the choice of self-supervised learning (SSL) features in the paper?", "answer": " Canonical correlation analysis (CCA)", "ref_chunk": "tasks. Dataset FBank Discrete tokens SSL (top line) MuST-C En-De MuST-C En-Es MuST-C En-Fr 26.7 31.2 37.1 28.6 33.0 38.7 29.7 33.7 40.2 Table 6: Intent classification accuracy (%) of SLURP dataset. Dataset FBank Discrete tokens SSL SLURP 86.9 / 86.3 81.8 / 80.8 84.2 / 83.3 method still has the upper hand over the others. Nevertheless, we aim to continue exploring various settings in our future work. 3.3. Speech Translation Results Table 5 shows the ST results; these models use CTC / attention fol- lowing [43, 44]. Similar to the ASR experiments in Section 3.2, the performance of ST using discrete units is slightly worse than that using the SSL features but better than FBank. 3.4. Spoken Language Understanding Results We conducted experiments on an SLU task using the SLURP [45] dataset. The intent classification accuracy is presented in Table 6. Unlike the other experiments, in this preliminary result, we did not observe an improvement in both SSL continuous features and dis- crete tokens. We hypothesize that the 21st layer of the WavLM large selected following the ASR experiments may not be optimal for SLU purposes due to the dependency of the optimal layer on the down- stream task, as reported in [12, 26]. 4. CONCLUSION This paper explores the efficacy of incorporating discrete speech units as inputs across a spectrum of speech processing tasks, en- compassing ASR, ST, and SLU. To evaluate the versatility of discrete units in diverse scenarios, we conducted experiments on datasets with varying characteristics. Drawing inspiration from canonical correlation analysis (CCA), we improved our choice of self-supervised learning (SSL) features, resulting in a noticeable performance enhancement. Consequently, the utilization of discrete units not only outperforms FBank features but also substantially en- hances efficiency. These findings underscore the promise of discrete unit input in speech processing. Future research avenues could delve into investigating alternative discretization techniques. 5. ACKNOWLEDGEMENTS Some experiments of this work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Sci- ence Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the A6000 GPUs used for this research. 6. REFERENCES [1] G. Hinton et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal processing magazine, vol. 29, no. 6, pp. 82\u201397, 2012. [2] Y. Qian et al., \u201cVery deep convolutional neural networks for noise ro- bust speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263\u20132276, 2016. [3] A. Graves et al., \u201cConnectionist temporal classification: Labelling un- segmented sequence data with recurrent neural networks,\u201d in Pro- ceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376. [4] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint arXiv:1211.3711, 2012. [5] J. Chorowski et al., \u201cAttention-based models for speech recogni- tion,\u201d Advances in Neural Information Processing Systems, vol. 2015, pp. 577\u2013585, 2015. [6] A. Vaswani et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017, pp. 5998\u20136008. [7] A. Gulati et al., \u201cConformer: Convolution-augmented Transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [8] P. Guo et al., \u201cRecent developments on espnet toolkit boosted by con- former,\u201d in Proc. ICASSP, 2021, pp. 5874\u20135878. [9] K. Kim et al., \u201cE-branchformer: Branchformer with enhanced merg- ing for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [10] S. Schneider et al., \u201cWav2vec: Unsupervised pre-training for speech recognition,\u201d Proc. Interspeech 2019, pp. 3465\u20133469, 2019. [11] A. Baevski, S. Schneider, and M. Auli, \u201cVq-wav2vec: Self-supervised learning of discrete speech representations,\u201d in Proc. ICLR, 2019. [12] A. Baevski et al., \u201cWav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020. [13] W.-N. Hsu et al., \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u2013 3460, 2021. [14] S. Chen et al., \u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Sig- nal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [15] A. Radford et al., \u201cRobust speech recognition via large-scale weak supervision,\u201d in Proc. ICML, 2023, pp. 28 492\u201328 518. [16] T. N. Sainath et al., \u201cLearning the speech front-end with raw wave- form CLDNNs,\u201d Learning, 2015. [17] X. Chang et al., \u201cExploration of efficient end-to-end asr using discretized input from self-supervised learning,\u201d arXiv preprint arXiv:2305.18108, 2023. [18] A. Baevski and A. Mohamed, \u201cEffectiveness of self-supervised pre- training for asr,\u201d in Proc. ICASSP, 2020, pp. 7694\u20137698. [19] D. Zhang et al., \u201cDub: Discrete unit back-translation for speech trans- lation,\u201d arXiv preprint arXiv:2305.11411, 2023. [20] M. Kim et al., \u201cMany-to-many spoken language translation via uni- fied speech and text representation learning with unit-to-unit transla- tion,\u201d arXiv preprint arXiv:2308.01831, 2023. [21] T. Hayashi and S. Watanabe, \u201cDiscretalk: Text-to-speech as a machine translation problem,\u201d arXiv preprint arXiv:2005.05525, 2020. [22] J. Shi et al., \u201cDiscretization and re-synthesis: An alternative method to solve the cocktail party problem,\u201d arXiv preprint arXiv:2112.09382, 2021. [23] Z. Borsos et al., \u201cAudiolm: A language modeling approach to au- dio generation,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2023. [24] C. Wang et al., \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv preprint arXiv:2301.02111, 2023. [25] P. K. Rubenstein et al., \u201cAudiopalm: A large language model that can speak and listen,\u201d arXiv preprint arXiv:2306.12925, 2023. [26] A. Pasad, B. Shi, and K. Livescu, \u201cComparative layer-wise analysis of self-supervised speech models,\u201d in Proc. ICASSP, 2023, pp. 1\u20135. [27] A. D\u00b4efossez et al., \u201cHigh fidelity neural audio compression,\u201d arXiv preprint arXiv:2210.13438, 2022. [28] S. Watanabe et al., \u201cESPnet: End-to-end speech processing toolkit,\u201d in Proc. Interspeech, 2018, pp. 2207\u20132211. [29] A. Van Den Oord, O. Vinyals, et al., \u201cNeural discrete representa- tion learning,\u201d Advances in neural information processing systems, vol. 30, 2017."}, {"question": " What could future research avenues delve into according to the paper?", "answer": " Investigating alternative discretization techniques", "ref_chunk": "tasks. Dataset FBank Discrete tokens SSL (top line) MuST-C En-De MuST-C En-Es MuST-C En-Fr 26.7 31.2 37.1 28.6 33.0 38.7 29.7 33.7 40.2 Table 6: Intent classification accuracy (%) of SLURP dataset. Dataset FBank Discrete tokens SSL SLURP 86.9 / 86.3 81.8 / 80.8 84.2 / 83.3 method still has the upper hand over the others. Nevertheless, we aim to continue exploring various settings in our future work. 3.3. Speech Translation Results Table 5 shows the ST results; these models use CTC / attention fol- lowing [43, 44]. Similar to the ASR experiments in Section 3.2, the performance of ST using discrete units is slightly worse than that using the SSL features but better than FBank. 3.4. Spoken Language Understanding Results We conducted experiments on an SLU task using the SLURP [45] dataset. The intent classification accuracy is presented in Table 6. Unlike the other experiments, in this preliminary result, we did not observe an improvement in both SSL continuous features and dis- crete tokens. We hypothesize that the 21st layer of the WavLM large selected following the ASR experiments may not be optimal for SLU purposes due to the dependency of the optimal layer on the down- stream task, as reported in [12, 26]. 4. CONCLUSION This paper explores the efficacy of incorporating discrete speech units as inputs across a spectrum of speech processing tasks, en- compassing ASR, ST, and SLU. To evaluate the versatility of discrete units in diverse scenarios, we conducted experiments on datasets with varying characteristics. Drawing inspiration from canonical correlation analysis (CCA), we improved our choice of self-supervised learning (SSL) features, resulting in a noticeable performance enhancement. Consequently, the utilization of discrete units not only outperforms FBank features but also substantially en- hances efficiency. These findings underscore the promise of discrete unit input in speech processing. Future research avenues could delve into investigating alternative discretization techniques. 5. ACKNOWLEDGEMENTS Some experiments of this work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Sci- ence Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the A6000 GPUs used for this research. 6. REFERENCES [1] G. Hinton et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal processing magazine, vol. 29, no. 6, pp. 82\u201397, 2012. [2] Y. Qian et al., \u201cVery deep convolutional neural networks for noise ro- bust speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263\u20132276, 2016. [3] A. Graves et al., \u201cConnectionist temporal classification: Labelling un- segmented sequence data with recurrent neural networks,\u201d in Pro- ceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376. [4] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint arXiv:1211.3711, 2012. [5] J. Chorowski et al., \u201cAttention-based models for speech recogni- tion,\u201d Advances in Neural Information Processing Systems, vol. 2015, pp. 577\u2013585, 2015. [6] A. Vaswani et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017, pp. 5998\u20136008. [7] A. Gulati et al., \u201cConformer: Convolution-augmented Transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [8] P. Guo et al., \u201cRecent developments on espnet toolkit boosted by con- former,\u201d in Proc. ICASSP, 2021, pp. 5874\u20135878. [9] K. Kim et al., \u201cE-branchformer: Branchformer with enhanced merg- ing for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [10] S. Schneider et al., \u201cWav2vec: Unsupervised pre-training for speech recognition,\u201d Proc. Interspeech 2019, pp. 3465\u20133469, 2019. [11] A. Baevski, S. Schneider, and M. Auli, \u201cVq-wav2vec: Self-supervised learning of discrete speech representations,\u201d in Proc. ICLR, 2019. [12] A. Baevski et al., \u201cWav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020. [13] W.-N. Hsu et al., \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u2013 3460, 2021. [14] S. Chen et al., \u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Sig- nal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [15] A. Radford et al., \u201cRobust speech recognition via large-scale weak supervision,\u201d in Proc. ICML, 2023, pp. 28 492\u201328 518. [16] T. N. Sainath et al., \u201cLearning the speech front-end with raw wave- form CLDNNs,\u201d Learning, 2015. [17] X. Chang et al., \u201cExploration of efficient end-to-end asr using discretized input from self-supervised learning,\u201d arXiv preprint arXiv:2305.18108, 2023. [18] A. Baevski and A. Mohamed, \u201cEffectiveness of self-supervised pre- training for asr,\u201d in Proc. ICASSP, 2020, pp. 7694\u20137698. [19] D. Zhang et al., \u201cDub: Discrete unit back-translation for speech trans- lation,\u201d arXiv preprint arXiv:2305.11411, 2023. [20] M. Kim et al., \u201cMany-to-many spoken language translation via uni- fied speech and text representation learning with unit-to-unit transla- tion,\u201d arXiv preprint arXiv:2308.01831, 2023. [21] T. Hayashi and S. Watanabe, \u201cDiscretalk: Text-to-speech as a machine translation problem,\u201d arXiv preprint arXiv:2005.05525, 2020. [22] J. Shi et al., \u201cDiscretization and re-synthesis: An alternative method to solve the cocktail party problem,\u201d arXiv preprint arXiv:2112.09382, 2021. [23] Z. Borsos et al., \u201cAudiolm: A language modeling approach to au- dio generation,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2023. [24] C. Wang et al., \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv preprint arXiv:2301.02111, 2023. [25] P. K. Rubenstein et al., \u201cAudiopalm: A large language model that can speak and listen,\u201d arXiv preprint arXiv:2306.12925, 2023. [26] A. Pasad, B. Shi, and K. Livescu, \u201cComparative layer-wise analysis of self-supervised speech models,\u201d in Proc. ICASSP, 2023, pp. 1\u20135. [27] A. D\u00b4efossez et al., \u201cHigh fidelity neural audio compression,\u201d arXiv preprint arXiv:2210.13438, 2022. [28] S. Watanabe et al., \u201cESPnet: End-to-end speech processing toolkit,\u201d in Proc. Interspeech, 2018, pp. 2207\u20132211. [29] A. Van Den Oord, O. Vinyals, et al., \u201cNeural discrete representa- tion learning,\u201d Advances in neural information processing systems, vol. 30, 2017."}], "doc_text": "tasks. Dataset FBank Discrete tokens SSL (top line) MuST-C En-De MuST-C En-Es MuST-C En-Fr 26.7 31.2 37.1 28.6 33.0 38.7 29.7 33.7 40.2 Table 6: Intent classification accuracy (%) of SLURP dataset. Dataset FBank Discrete tokens SSL SLURP 86.9 / 86.3 81.8 / 80.8 84.2 / 83.3 method still has the upper hand over the others. Nevertheless, we aim to continue exploring various settings in our future work. 3.3. Speech Translation Results Table 5 shows the ST results; these models use CTC / attention fol- lowing [43, 44]. Similar to the ASR experiments in Section 3.2, the performance of ST using discrete units is slightly worse than that using the SSL features but better than FBank. 3.4. Spoken Language Understanding Results We conducted experiments on an SLU task using the SLURP [45] dataset. The intent classification accuracy is presented in Table 6. Unlike the other experiments, in this preliminary result, we did not observe an improvement in both SSL continuous features and dis- crete tokens. We hypothesize that the 21st layer of the WavLM large selected following the ASR experiments may not be optimal for SLU purposes due to the dependency of the optimal layer on the down- stream task, as reported in [12, 26]. 4. CONCLUSION This paper explores the efficacy of incorporating discrete speech units as inputs across a spectrum of speech processing tasks, en- compassing ASR, ST, and SLU. To evaluate the versatility of discrete units in diverse scenarios, we conducted experiments on datasets with varying characteristics. Drawing inspiration from canonical correlation analysis (CCA), we improved our choice of self-supervised learning (SSL) features, resulting in a noticeable performance enhancement. Consequently, the utilization of discrete units not only outperforms FBank features but also substantially en- hances efficiency. These findings underscore the promise of discrete unit input in speech processing. Future research avenues could delve into investigating alternative discretization techniques. 5. ACKNOWLEDGEMENTS Some experiments of this work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Sci- ence Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the A6000 GPUs used for this research. 6. REFERENCES [1] G. Hinton et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal processing magazine, vol. 29, no. 6, pp. 82\u201397, 2012. [2] Y. Qian et al., \u201cVery deep convolutional neural networks for noise ro- bust speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263\u20132276, 2016. [3] A. Graves et al., \u201cConnectionist temporal classification: Labelling un- segmented sequence data with recurrent neural networks,\u201d in Pro- ceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376. [4] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint arXiv:1211.3711, 2012. [5] J. Chorowski et al., \u201cAttention-based models for speech recogni- tion,\u201d Advances in Neural Information Processing Systems, vol. 2015, pp. 577\u2013585, 2015. [6] A. Vaswani et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017, pp. 5998\u20136008. [7] A. Gulati et al., \u201cConformer: Convolution-augmented Transformer for speech recognition,\u201d in Proc. Interspeech, 2020, pp. 5036\u20135040. [8] P. Guo et al., \u201cRecent developments on espnet toolkit boosted by con- former,\u201d in Proc. ICASSP, 2021, pp. 5874\u20135878. [9] K. Kim et al., \u201cE-branchformer: Branchformer with enhanced merg- ing for speech recognition,\u201d in Proc. SLT, 2023, pp. 84\u201391. [10] S. Schneider et al., \u201cWav2vec: Unsupervised pre-training for speech recognition,\u201d Proc. Interspeech 2019, pp. 3465\u20133469, 2019. [11] A. Baevski, S. Schneider, and M. Auli, \u201cVq-wav2vec: Self-supervised learning of discrete speech representations,\u201d in Proc. ICLR, 2019. [12] A. Baevski et al., \u201cWav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020. [13] W.-N. Hsu et al., \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u2013 3460, 2021. [14] S. Chen et al., \u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Sig- nal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022. [15] A. Radford et al., \u201cRobust speech recognition via large-scale weak supervision,\u201d in Proc. ICML, 2023, pp. 28 492\u201328 518. [16] T. N. Sainath et al., \u201cLearning the speech front-end with raw wave- form CLDNNs,\u201d Learning, 2015. [17] X. Chang et al., \u201cExploration of efficient end-to-end asr using discretized input from self-supervised learning,\u201d arXiv preprint arXiv:2305.18108, 2023. [18] A. Baevski and A. Mohamed, \u201cEffectiveness of self-supervised pre- training for asr,\u201d in Proc. ICASSP, 2020, pp. 7694\u20137698. [19] D. Zhang et al., \u201cDub: Discrete unit back-translation for speech trans- lation,\u201d arXiv preprint arXiv:2305.11411, 2023. [20] M. Kim et al., \u201cMany-to-many spoken language translation via uni- fied speech and text representation learning with unit-to-unit transla- tion,\u201d arXiv preprint arXiv:2308.01831, 2023. [21] T. Hayashi and S. Watanabe, \u201cDiscretalk: Text-to-speech as a machine translation problem,\u201d arXiv preprint arXiv:2005.05525, 2020. [22] J. Shi et al., \u201cDiscretization and re-synthesis: An alternative method to solve the cocktail party problem,\u201d arXiv preprint arXiv:2112.09382, 2021. [23] Z. Borsos et al., \u201cAudiolm: A language modeling approach to au- dio generation,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2023. [24] C. Wang et al., \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv preprint arXiv:2301.02111, 2023. [25] P. K. Rubenstein et al., \u201cAudiopalm: A large language model that can speak and listen,\u201d arXiv preprint arXiv:2306.12925, 2023. [26] A. Pasad, B. Shi, and K. Livescu, \u201cComparative layer-wise analysis of self-supervised speech models,\u201d in Proc. ICASSP, 2023, pp. 1\u20135. [27] A. D\u00b4efossez et al., \u201cHigh fidelity neural audio compression,\u201d arXiv preprint arXiv:2210.13438, 2022. [28] S. Watanabe et al., \u201cESPnet: End-to-end speech processing toolkit,\u201d in Proc. Interspeech, 2018, pp. 2207\u20132211. [29] A. Van Den Oord, O. Vinyals, et al., \u201cNeural discrete representa- tion learning,\u201d Advances in neural information processing systems, vol. 30, 2017."}