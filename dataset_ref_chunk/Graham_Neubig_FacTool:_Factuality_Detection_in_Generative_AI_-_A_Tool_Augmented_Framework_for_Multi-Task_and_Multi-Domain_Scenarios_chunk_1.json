{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_FacTool:_Factuality_Detection_in_Generative_AI_-_A_Tool_Augmented_Framework_for_Multi-Task_and_Multi-Domain_Scenarios_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the FACTOOL framework?,answer: Detecting factual errors in texts generated by large language models.", "ref_chunk": "Claims1. 2. Evidence Factuality FacTool EvidenceCollectionTool Querying ClaimExtractionQueryGenerationVerification !FacTool Tool Queriesa. b. Self-check 12345 Prompt 0 100 ! ChatGPT Response Tools FACTOOL: Factuality Detection in Generative AI A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios I-Chun Chern2 Steffi Chern2 Shiqi Chen3 Weizhe Yuan4 Kehua Feng1 Pengfei Liu1,7\u2217 Chunting Zhou5 Junxian He6 Graham Neubig2 1Shanghai Jiao Tong University 2Carnegie Mellon University 3City University of Hong Kong 4New York University 5Meta AI 6The Hong Kong University of Science and Technology 7Shanghai Artificial Intelligence Laboratory Abstract The emergence of generative pre-trained mod- els has facilitated the synthesis of high-quality text, but it has also posed challenges in identi- fying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence avail- able during the process of fact checking. 3 2 0 2 l u J 6 2 ] L C . s c [ With the above challenges in mind, in this paper, we propose FACTOOL, a task and do- main agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and sci- entific literature review) show the efficacy of the proposed method. We release the code of FACTOOL associated with ChatGPT plu- gin interface at https://github.com/ GAIR-NLP/factool. Figure 1: Tool-augmented framework for factuality de- tection. 2 v 8 2 5 3 1 . 7 0 3 2 : v i X r a Content that is automatically generated can of- ten exhibit inaccuracies or deviations from the truth due to the limited capacity of large language models (LLMs) (Ji et al., 2023; Schulman, 2023). LLMs are susceptible to producing content that appears credible but may actually be factually in- correct or imprecise. This limitation restricts the application of generative AI in some high-stakes ar- eas, such as healthcare, finance, and law. Therefore, it is crucial to identify these errors systematically to improve the usefulness and reliability of the gen- erated content. 1 Introduction Generative artificial intelligence (AI) technology, exemplified by GPT-4 (OpenAI, 2023) consoli- dates various tasks in natural language process- ing into a single sequence generation problem. This unified architecture enables users to complete multiple tasks (e.g., question answering (Thop- pilan et al., 2022), code generation (Chen et al., 2021), math problem solving (Lewkowycz et al., 2022), and scientific literature generation (Taylor et al., 2022)) through a natural language inter- face (Liu et al., 2023) with both unprecedented performance (Bubeck et al., 2023) and interactiv- ity. Current literature on detecting and mitigating factual errors generated by machine learning mod- els focuses predominantly on a single specific task, for example, retrieval-augmented verification mod- els for QA (Lewis et al., 2020), hallucination detec- tion models for text summarization (Fabbri et al., 2022), and execution-based evaluation for code (Shi et al., 2022). While these methods have proven successful within their respective areas, given the remarkable versatility of tasks and domains han- dled by LLMs, we argue that it is also important However, at the same time, such a generative paradigm also introduces some unique challenges. \u2217Corresponding author Response Claim Evidence Scenario Methods Length Generated by Granularity Provided Provided Domain Task FEVER-based FactCC QAGS-based WICE-based RARR 7.30 20.83 16.11 24.20 - Human Synthetic Model Human PaLM/LaMDA Fact Sentence Summary Fact Fact \u2713 \u2713 \u2713 \u2713 X X \u2713 \u2713 \u2713 X Wikipedia Newswire Newswire Wikipedia Wikipedia QA Fact Verification Summ. Factuality Summ. Factuality Entailment FACTOOL 41.80 30.37 67.13 76.34 ChatGPT ChatGPT ChatGPT ChatGPT Fact Snippet Statement Tuple X X X X X X X X Wikipedia QA Python Math Sci. text Code generation Math Problems Sci. Review Table 1: A comparison of published approaches for factuality detection in terms of generated responses and claims to be verified based on collected evidence. \u201cScenario\u201d represents which task and domain the corresponding approach has been justified. \u201cSci.\u201d represents \u201cScientific\u201d. to have a more comprehensive factuality detection and verification framework that is similarly versa- tile. Additionally, in the current literature, the task of factuality detection is usually simplified as ei- ther (i) given a claim, determining whether it is factually correct, (ii) or given evidence, determin- ing whether the generated claim is supported. This task definition is not well suited to writing tasks that users commonly engage with when interacting with generative models (e.g., ChatGPT), where we often need to validate the factuality of a long-form generation without explicit claims and evidence. We connect the concept of \u201ctool use\u201d with \u201cfac- tuality detection\u201d, developing a unified and ver- satile framework for factuality detection across a variety of domains and tasks. We use FACTOOL to evaluate the factuality of modern chatbots, and found that GPT-4 has the best factuality across almost all scenarios. Su- pervisely fine-tuned chatbots (Vicuna-13B) have reasonably good factuality in KB-based QA but perform poorly in more challenging scenarios, in- cluding code generation, math problem solving, and scientific literature review writing. In this paper, we propose a task and domain- agnostic framework, FACTOOL, which aims to de- tect factual errors in LLM-generated texts. We il- lustrate our framework in Fig. 1, where we connect the concept of \u201ctool use\u201d (Thoppilan et al., 2022; Gao et al., 2022b; Schick et al., 2023) with \u201cfac- tuality detection\u201d and demonstrate that the ability to use tools in LLMs is crucial for factuality de- tection. Specifically, FACTOOL leverages various tools, including Google Search, Google Scholar, code interpreters, Python, or even LLMs them- selves, to gather evidence about the factuality of the generated content. Moreover, our framework employs the reasoning abilities of LLMs to assess the factuality of the content, given the evidence that has been gathered. We develop a benchmark and perform experiments across four tasks: knowledge- based QA, code generation, math problem solving, and scientific literature review writing. In summary, our contributions"}, {"question": " Why is it important to identify factual errors in automatically generated content?,answer: To improve the usefulness and reliability of the generated content.", "ref_chunk": "Claims1. 2. Evidence Factuality FacTool EvidenceCollectionTool Querying ClaimExtractionQueryGenerationVerification !FacTool Tool Queriesa. b. Self-check 12345 Prompt 0 100 ! ChatGPT Response Tools FACTOOL: Factuality Detection in Generative AI A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios I-Chun Chern2 Steffi Chern2 Shiqi Chen3 Weizhe Yuan4 Kehua Feng1 Pengfei Liu1,7\u2217 Chunting Zhou5 Junxian He6 Graham Neubig2 1Shanghai Jiao Tong University 2Carnegie Mellon University 3City University of Hong Kong 4New York University 5Meta AI 6The Hong Kong University of Science and Technology 7Shanghai Artificial Intelligence Laboratory Abstract The emergence of generative pre-trained mod- els has facilitated the synthesis of high-quality text, but it has also posed challenges in identi- fying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence avail- able during the process of fact checking. 3 2 0 2 l u J 6 2 ] L C . s c [ With the above challenges in mind, in this paper, we propose FACTOOL, a task and do- main agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and sci- entific literature review) show the efficacy of the proposed method. We release the code of FACTOOL associated with ChatGPT plu- gin interface at https://github.com/ GAIR-NLP/factool. Figure 1: Tool-augmented framework for factuality de- tection. 2 v 8 2 5 3 1 . 7 0 3 2 : v i X r a Content that is automatically generated can of- ten exhibit inaccuracies or deviations from the truth due to the limited capacity of large language models (LLMs) (Ji et al., 2023; Schulman, 2023). LLMs are susceptible to producing content that appears credible but may actually be factually in- correct or imprecise. This limitation restricts the application of generative AI in some high-stakes ar- eas, such as healthcare, finance, and law. Therefore, it is crucial to identify these errors systematically to improve the usefulness and reliability of the gen- erated content. 1 Introduction Generative artificial intelligence (AI) technology, exemplified by GPT-4 (OpenAI, 2023) consoli- dates various tasks in natural language process- ing into a single sequence generation problem. This unified architecture enables users to complete multiple tasks (e.g., question answering (Thop- pilan et al., 2022), code generation (Chen et al., 2021), math problem solving (Lewkowycz et al., 2022), and scientific literature generation (Taylor et al., 2022)) through a natural language inter- face (Liu et al., 2023) with both unprecedented performance (Bubeck et al., 2023) and interactiv- ity. Current literature on detecting and mitigating factual errors generated by machine learning mod- els focuses predominantly on a single specific task, for example, retrieval-augmented verification mod- els for QA (Lewis et al., 2020), hallucination detec- tion models for text summarization (Fabbri et al., 2022), and execution-based evaluation for code (Shi et al., 2022). While these methods have proven successful within their respective areas, given the remarkable versatility of tasks and domains han- dled by LLMs, we argue that it is also important However, at the same time, such a generative paradigm also introduces some unique challenges. \u2217Corresponding author Response Claim Evidence Scenario Methods Length Generated by Granularity Provided Provided Domain Task FEVER-based FactCC QAGS-based WICE-based RARR 7.30 20.83 16.11 24.20 - Human Synthetic Model Human PaLM/LaMDA Fact Sentence Summary Fact Fact \u2713 \u2713 \u2713 \u2713 X X \u2713 \u2713 \u2713 X Wikipedia Newswire Newswire Wikipedia Wikipedia QA Fact Verification Summ. Factuality Summ. Factuality Entailment FACTOOL 41.80 30.37 67.13 76.34 ChatGPT ChatGPT ChatGPT ChatGPT Fact Snippet Statement Tuple X X X X X X X X Wikipedia QA Python Math Sci. text Code generation Math Problems Sci. Review Table 1: A comparison of published approaches for factuality detection in terms of generated responses and claims to be verified based on collected evidence. \u201cScenario\u201d represents which task and domain the corresponding approach has been justified. \u201cSci.\u201d represents \u201cScientific\u201d. to have a more comprehensive factuality detection and verification framework that is similarly versa- tile. Additionally, in the current literature, the task of factuality detection is usually simplified as ei- ther (i) given a claim, determining whether it is factually correct, (ii) or given evidence, determin- ing whether the generated claim is supported. This task definition is not well suited to writing tasks that users commonly engage with when interacting with generative models (e.g., ChatGPT), where we often need to validate the factuality of a long-form generation without explicit claims and evidence. We connect the concept of \u201ctool use\u201d with \u201cfac- tuality detection\u201d, developing a unified and ver- satile framework for factuality detection across a variety of domains and tasks. We use FACTOOL to evaluate the factuality of modern chatbots, and found that GPT-4 has the best factuality across almost all scenarios. Su- pervisely fine-tuned chatbots (Vicuna-13B) have reasonably good factuality in KB-based QA but perform poorly in more challenging scenarios, in- cluding code generation, math problem solving, and scientific literature review writing. In this paper, we propose a task and domain- agnostic framework, FACTOOL, which aims to de- tect factual errors in LLM-generated texts. We il- lustrate our framework in Fig. 1, where we connect the concept of \u201ctool use\u201d (Thoppilan et al., 2022; Gao et al., 2022b; Schick et al., 2023) with \u201cfac- tuality detection\u201d and demonstrate that the ability to use tools in LLMs is crucial for factuality de- tection. Specifically, FACTOOL leverages various tools, including Google Search, Google Scholar, code interpreters, Python, or even LLMs them- selves, to gather evidence about the factuality of the generated content. Moreover, our framework employs the reasoning abilities of LLMs to assess the factuality of the content, given the evidence that has been gathered. We develop a benchmark and perform experiments across four tasks: knowledge- based QA, code generation, math problem solving, and scientific literature review writing. In summary, our contributions"}, {"question": " What are some challenges posed by generative models in identifying factual errors?,answer: Generated texts tend to be lengthy, lack clearly defined granularity for facts, and there is a scarcity of explicit evidence available.", "ref_chunk": "Claims1. 2. Evidence Factuality FacTool EvidenceCollectionTool Querying ClaimExtractionQueryGenerationVerification !FacTool Tool Queriesa. b. Self-check 12345 Prompt 0 100 ! ChatGPT Response Tools FACTOOL: Factuality Detection in Generative AI A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios I-Chun Chern2 Steffi Chern2 Shiqi Chen3 Weizhe Yuan4 Kehua Feng1 Pengfei Liu1,7\u2217 Chunting Zhou5 Junxian He6 Graham Neubig2 1Shanghai Jiao Tong University 2Carnegie Mellon University 3City University of Hong Kong 4New York University 5Meta AI 6The Hong Kong University of Science and Technology 7Shanghai Artificial Intelligence Laboratory Abstract The emergence of generative pre-trained mod- els has facilitated the synthesis of high-quality text, but it has also posed challenges in identi- fying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence avail- able during the process of fact checking. 3 2 0 2 l u J 6 2 ] L C . s c [ With the above challenges in mind, in this paper, we propose FACTOOL, a task and do- main agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and sci- entific literature review) show the efficacy of the proposed method. We release the code of FACTOOL associated with ChatGPT plu- gin interface at https://github.com/ GAIR-NLP/factool. Figure 1: Tool-augmented framework for factuality de- tection. 2 v 8 2 5 3 1 . 7 0 3 2 : v i X r a Content that is automatically generated can of- ten exhibit inaccuracies or deviations from the truth due to the limited capacity of large language models (LLMs) (Ji et al., 2023; Schulman, 2023). LLMs are susceptible to producing content that appears credible but may actually be factually in- correct or imprecise. This limitation restricts the application of generative AI in some high-stakes ar- eas, such as healthcare, finance, and law. Therefore, it is crucial to identify these errors systematically to improve the usefulness and reliability of the gen- erated content. 1 Introduction Generative artificial intelligence (AI) technology, exemplified by GPT-4 (OpenAI, 2023) consoli- dates various tasks in natural language process- ing into a single sequence generation problem. This unified architecture enables users to complete multiple tasks (e.g., question answering (Thop- pilan et al., 2022), code generation (Chen et al., 2021), math problem solving (Lewkowycz et al., 2022), and scientific literature generation (Taylor et al., 2022)) through a natural language inter- face (Liu et al., 2023) with both unprecedented performance (Bubeck et al., 2023) and interactiv- ity. Current literature on detecting and mitigating factual errors generated by machine learning mod- els focuses predominantly on a single specific task, for example, retrieval-augmented verification mod- els for QA (Lewis et al., 2020), hallucination detec- tion models for text summarization (Fabbri et al., 2022), and execution-based evaluation for code (Shi et al., 2022). While these methods have proven successful within their respective areas, given the remarkable versatility of tasks and domains han- dled by LLMs, we argue that it is also important However, at the same time, such a generative paradigm also introduces some unique challenges. \u2217Corresponding author Response Claim Evidence Scenario Methods Length Generated by Granularity Provided Provided Domain Task FEVER-based FactCC QAGS-based WICE-based RARR 7.30 20.83 16.11 24.20 - Human Synthetic Model Human PaLM/LaMDA Fact Sentence Summary Fact Fact \u2713 \u2713 \u2713 \u2713 X X \u2713 \u2713 \u2713 X Wikipedia Newswire Newswire Wikipedia Wikipedia QA Fact Verification Summ. Factuality Summ. Factuality Entailment FACTOOL 41.80 30.37 67.13 76.34 ChatGPT ChatGPT ChatGPT ChatGPT Fact Snippet Statement Tuple X X X X X X X X Wikipedia QA Python Math Sci. text Code generation Math Problems Sci. Review Table 1: A comparison of published approaches for factuality detection in terms of generated responses and claims to be verified based on collected evidence. \u201cScenario\u201d represents which task and domain the corresponding approach has been justified. \u201cSci.\u201d represents \u201cScientific\u201d. to have a more comprehensive factuality detection and verification framework that is similarly versa- tile. Additionally, in the current literature, the task of factuality detection is usually simplified as ei- ther (i) given a claim, determining whether it is factually correct, (ii) or given evidence, determin- ing whether the generated claim is supported. This task definition is not well suited to writing tasks that users commonly engage with when interacting with generative models (e.g., ChatGPT), where we often need to validate the factuality of a long-form generation without explicit claims and evidence. We connect the concept of \u201ctool use\u201d with \u201cfac- tuality detection\u201d, developing a unified and ver- satile framework for factuality detection across a variety of domains and tasks. We use FACTOOL to evaluate the factuality of modern chatbots, and found that GPT-4 has the best factuality across almost all scenarios. Su- pervisely fine-tuned chatbots (Vicuna-13B) have reasonably good factuality in KB-based QA but perform poorly in more challenging scenarios, in- cluding code generation, math problem solving, and scientific literature review writing. In this paper, we propose a task and domain- agnostic framework, FACTOOL, which aims to de- tect factual errors in LLM-generated texts. We il- lustrate our framework in Fig. 1, where we connect the concept of \u201ctool use\u201d (Thoppilan et al., 2022; Gao et al., 2022b; Schick et al., 2023) with \u201cfac- tuality detection\u201d and demonstrate that the ability to use tools in LLMs is crucial for factuality de- tection. Specifically, FACTOOL leverages various tools, including Google Search, Google Scholar, code interpreters, Python, or even LLMs them- selves, to gather evidence about the factuality of the generated content. Moreover, our framework employs the reasoning abilities of LLMs to assess the factuality of the content, given the evidence that has been gathered. We develop a benchmark and perform experiments across four tasks: knowledge- based QA, code generation, math problem solving, and scientific literature review writing. In summary, our contributions"}, {"question": " What tasks were used to demonstrate the efficacy of FACTOOL?,answer: Knowledge-based QA, code generation, mathematical reasoning, and scientific literature review.", "ref_chunk": "Claims1. 2. Evidence Factuality FacTool EvidenceCollectionTool Querying ClaimExtractionQueryGenerationVerification !FacTool Tool Queriesa. b. Self-check 12345 Prompt 0 100 ! ChatGPT Response Tools FACTOOL: Factuality Detection in Generative AI A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios I-Chun Chern2 Steffi Chern2 Shiqi Chen3 Weizhe Yuan4 Kehua Feng1 Pengfei Liu1,7\u2217 Chunting Zhou5 Junxian He6 Graham Neubig2 1Shanghai Jiao Tong University 2Carnegie Mellon University 3City University of Hong Kong 4New York University 5Meta AI 6The Hong Kong University of Science and Technology 7Shanghai Artificial Intelligence Laboratory Abstract The emergence of generative pre-trained mod- els has facilitated the synthesis of high-quality text, but it has also posed challenges in identi- fying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence avail- able during the process of fact checking. 3 2 0 2 l u J 6 2 ] L C . s c [ With the above challenges in mind, in this paper, we propose FACTOOL, a task and do- main agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and sci- entific literature review) show the efficacy of the proposed method. We release the code of FACTOOL associated with ChatGPT plu- gin interface at https://github.com/ GAIR-NLP/factool. Figure 1: Tool-augmented framework for factuality de- tection. 2 v 8 2 5 3 1 . 7 0 3 2 : v i X r a Content that is automatically generated can of- ten exhibit inaccuracies or deviations from the truth due to the limited capacity of large language models (LLMs) (Ji et al., 2023; Schulman, 2023). LLMs are susceptible to producing content that appears credible but may actually be factually in- correct or imprecise. This limitation restricts the application of generative AI in some high-stakes ar- eas, such as healthcare, finance, and law. Therefore, it is crucial to identify these errors systematically to improve the usefulness and reliability of the gen- erated content. 1 Introduction Generative artificial intelligence (AI) technology, exemplified by GPT-4 (OpenAI, 2023) consoli- dates various tasks in natural language process- ing into a single sequence generation problem. This unified architecture enables users to complete multiple tasks (e.g., question answering (Thop- pilan et al., 2022), code generation (Chen et al., 2021), math problem solving (Lewkowycz et al., 2022), and scientific literature generation (Taylor et al., 2022)) through a natural language inter- face (Liu et al., 2023) with both unprecedented performance (Bubeck et al., 2023) and interactiv- ity. Current literature on detecting and mitigating factual errors generated by machine learning mod- els focuses predominantly on a single specific task, for example, retrieval-augmented verification mod- els for QA (Lewis et al., 2020), hallucination detec- tion models for text summarization (Fabbri et al., 2022), and execution-based evaluation for code (Shi et al., 2022). While these methods have proven successful within their respective areas, given the remarkable versatility of tasks and domains han- dled by LLMs, we argue that it is also important However, at the same time, such a generative paradigm also introduces some unique challenges. \u2217Corresponding author Response Claim Evidence Scenario Methods Length Generated by Granularity Provided Provided Domain Task FEVER-based FactCC QAGS-based WICE-based RARR 7.30 20.83 16.11 24.20 - Human Synthetic Model Human PaLM/LaMDA Fact Sentence Summary Fact Fact \u2713 \u2713 \u2713 \u2713 X X \u2713 \u2713 \u2713 X Wikipedia Newswire Newswire Wikipedia Wikipedia QA Fact Verification Summ. Factuality Summ. Factuality Entailment FACTOOL 41.80 30.37 67.13 76.34 ChatGPT ChatGPT ChatGPT ChatGPT Fact Snippet Statement Tuple X X X X X X X X Wikipedia QA Python Math Sci. text Code generation Math Problems Sci. Review Table 1: A comparison of published approaches for factuality detection in terms of generated responses and claims to be verified based on collected evidence. \u201cScenario\u201d represents which task and domain the corresponding approach has been justified. \u201cSci.\u201d represents \u201cScientific\u201d. to have a more comprehensive factuality detection and verification framework that is similarly versa- tile. Additionally, in the current literature, the task of factuality detection is usually simplified as ei- ther (i) given a claim, determining whether it is factually correct, (ii) or given evidence, determin- ing whether the generated claim is supported. This task definition is not well suited to writing tasks that users commonly engage with when interacting with generative models (e.g., ChatGPT), where we often need to validate the factuality of a long-form generation without explicit claims and evidence. We connect the concept of \u201ctool use\u201d with \u201cfac- tuality detection\u201d, developing a unified and ver- satile framework for factuality detection across a variety of domains and tasks. We use FACTOOL to evaluate the factuality of modern chatbots, and found that GPT-4 has the best factuality across almost all scenarios. Su- pervisely fine-tuned chatbots (Vicuna-13B) have reasonably good factuality in KB-based QA but perform poorly in more challenging scenarios, in- cluding code generation, math problem solving, and scientific literature review writing. In this paper, we propose a task and domain- agnostic framework, FACTOOL, which aims to de- tect factual errors in LLM-generated texts. We il- lustrate our framework in Fig. 1, where we connect the concept of \u201ctool use\u201d (Thoppilan et al., 2022; Gao et al., 2022b; Schick et al., 2023) with \u201cfac- tuality detection\u201d and demonstrate that the ability to use tools in LLMs is crucial for factuality de- tection. Specifically, FACTOOL leverages various tools, including Google Search, Google Scholar, code interpreters, Python, or even LLMs them- selves, to gather evidence about the factuality of the generated content. Moreover, our framework employs the reasoning abilities of LLMs to assess the factuality of the content, given the evidence that has been gathered. We develop a benchmark and perform experiments across four tasks: knowledge- based QA, code generation, math problem solving, and scientific literature review writing. In summary, our contributions"}, {"question": " What is the limitation of large language models that makes them susceptible to generating inaccurate or factually incorrect content?,answer: Limited capacity of large language models.", "ref_chunk": "Claims1. 2. Evidence Factuality FacTool EvidenceCollectionTool Querying ClaimExtractionQueryGenerationVerification !FacTool Tool Queriesa. b. Self-check 12345 Prompt 0 100 ! ChatGPT Response Tools FACTOOL: Factuality Detection in Generative AI A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios I-Chun Chern2 Steffi Chern2 Shiqi Chen3 Weizhe Yuan4 Kehua Feng1 Pengfei Liu1,7\u2217 Chunting Zhou5 Junxian He6 Graham Neubig2 1Shanghai Jiao Tong University 2Carnegie Mellon University 3City University of Hong Kong 4New York University 5Meta AI 6The Hong Kong University of Science and Technology 7Shanghai Artificial Intelligence Laboratory Abstract The emergence of generative pre-trained mod- els has facilitated the synthesis of high-quality text, but it has also posed challenges in identi- fying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence avail- able during the process of fact checking. 3 2 0 2 l u J 6 2 ] L C . s c [ With the above challenges in mind, in this paper, we propose FACTOOL, a task and do- main agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and sci- entific literature review) show the efficacy of the proposed method. We release the code of FACTOOL associated with ChatGPT plu- gin interface at https://github.com/ GAIR-NLP/factool. Figure 1: Tool-augmented framework for factuality de- tection. 2 v 8 2 5 3 1 . 7 0 3 2 : v i X r a Content that is automatically generated can of- ten exhibit inaccuracies or deviations from the truth due to the limited capacity of large language models (LLMs) (Ji et al., 2023; Schulman, 2023). LLMs are susceptible to producing content that appears credible but may actually be factually in- correct or imprecise. This limitation restricts the application of generative AI in some high-stakes ar- eas, such as healthcare, finance, and law. Therefore, it is crucial to identify these errors systematically to improve the usefulness and reliability of the gen- erated content. 1 Introduction Generative artificial intelligence (AI) technology, exemplified by GPT-4 (OpenAI, 2023) consoli- dates various tasks in natural language process- ing into a single sequence generation problem. This unified architecture enables users to complete multiple tasks (e.g., question answering (Thop- pilan et al., 2022), code generation (Chen et al., 2021), math problem solving (Lewkowycz et al., 2022), and scientific literature generation (Taylor et al., 2022)) through a natural language inter- face (Liu et al., 2023) with both unprecedented performance (Bubeck et al., 2023) and interactiv- ity. Current literature on detecting and mitigating factual errors generated by machine learning mod- els focuses predominantly on a single specific task, for example, retrieval-augmented verification mod- els for QA (Lewis et al., 2020), hallucination detec- tion models for text summarization (Fabbri et al., 2022), and execution-based evaluation for code (Shi et al., 2022). While these methods have proven successful within their respective areas, given the remarkable versatility of tasks and domains han- dled by LLMs, we argue that it is also important However, at the same time, such a generative paradigm also introduces some unique challenges. \u2217Corresponding author Response Claim Evidence Scenario Methods Length Generated by Granularity Provided Provided Domain Task FEVER-based FactCC QAGS-based WICE-based RARR 7.30 20.83 16.11 24.20 - Human Synthetic Model Human PaLM/LaMDA Fact Sentence Summary Fact Fact \u2713 \u2713 \u2713 \u2713 X X \u2713 \u2713 \u2713 X Wikipedia Newswire Newswire Wikipedia Wikipedia QA Fact Verification Summ. Factuality Summ. Factuality Entailment FACTOOL 41.80 30.37 67.13 76.34 ChatGPT ChatGPT ChatGPT ChatGPT Fact Snippet Statement Tuple X X X X X X X X Wikipedia QA Python Math Sci. text Code generation Math Problems Sci. Review Table 1: A comparison of published approaches for factuality detection in terms of generated responses and claims to be verified based on collected evidence. \u201cScenario\u201d represents which task and domain the corresponding approach has been justified. \u201cSci.\u201d represents \u201cScientific\u201d. to have a more comprehensive factuality detection and verification framework that is similarly versa- tile. Additionally, in the current literature, the task of factuality detection is usually simplified as ei- ther (i) given a claim, determining whether it is factually correct, (ii) or given evidence, determin- ing whether the generated claim is supported. This task definition is not well suited to writing tasks that users commonly engage with when interacting with generative models (e.g., ChatGPT), where we often need to validate the factuality of a long-form generation without explicit claims and evidence. We connect the concept of \u201ctool use\u201d with \u201cfac- tuality detection\u201d, developing a unified and ver- satile framework for factuality detection across a variety of domains and tasks. We use FACTOOL to evaluate the factuality of modern chatbots, and found that GPT-4 has the best factuality across almost all scenarios. Su- pervisely fine-tuned chatbots (Vicuna-13B) have reasonably good factuality in KB-based QA but perform poorly in more challenging scenarios, in- cluding code generation, math problem solving, and scientific literature review writing. In this paper, we propose a task and domain- agnostic framework, FACTOOL, which aims to de- tect factual errors in LLM-generated texts. We il- lustrate our framework in Fig. 1, where we connect the concept of \u201ctool use\u201d (Thoppilan et al., 2022; Gao et al., 2022b; Schick et al., 2023) with \u201cfac- tuality detection\u201d and demonstrate that the ability to use tools in LLMs is crucial for factuality de- tection. Specifically, FACTOOL leverages various tools, including Google Search, Google Scholar, code interpreters, Python, or even LLMs them- selves, to gather evidence about the factuality of the generated content. Moreover, our framework employs the reasoning abilities of LLMs to assess the factuality of the content, given the evidence that has been gathered. We develop a benchmark and perform experiments across four tasks: knowledge- based QA, code generation, math problem solving, and scientific literature review writing. In summary, our contributions"}, {"question": " What technology is exemplified by GPT-4 and what tasks does it consolidate?,answer: Generative artificial intelligence technology; various tasks in natural language processing into a single sequence generation problem.", "ref_chunk": "Claims1. 2. Evidence Factuality FacTool EvidenceCollectionTool Querying ClaimExtractionQueryGenerationVerification !FacTool Tool Queriesa. b. Self-check 12345 Prompt 0 100 ! ChatGPT Response Tools FACTOOL: Factuality Detection in Generative AI A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios I-Chun Chern2 Steffi Chern2 Shiqi Chen3 Weizhe Yuan4 Kehua Feng1 Pengfei Liu1,7\u2217 Chunting Zhou5 Junxian He6 Graham Neubig2 1Shanghai Jiao Tong University 2Carnegie Mellon University 3City University of Hong Kong 4New York University 5Meta AI 6The Hong Kong University of Science and Technology 7Shanghai Artificial Intelligence Laboratory Abstract The emergence of generative pre-trained mod- els has facilitated the synthesis of high-quality text, but it has also posed challenges in identi- fying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence avail- able during the process of fact checking. 3 2 0 2 l u J 6 2 ] L C . s c [ With the above challenges in mind, in this paper, we propose FACTOOL, a task and do- main agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and sci- entific literature review) show the efficacy of the proposed method. We release the code of FACTOOL associated with ChatGPT plu- gin interface at https://github.com/ GAIR-NLP/factool. Figure 1: Tool-augmented framework for factuality de- tection. 2 v 8 2 5 3 1 . 7 0 3 2 : v i X r a Content that is automatically generated can of- ten exhibit inaccuracies or deviations from the truth due to the limited capacity of large language models (LLMs) (Ji et al., 2023; Schulman, 2023). LLMs are susceptible to producing content that appears credible but may actually be factually in- correct or imprecise. This limitation restricts the application of generative AI in some high-stakes ar- eas, such as healthcare, finance, and law. Therefore, it is crucial to identify these errors systematically to improve the usefulness and reliability of the gen- erated content. 1 Introduction Generative artificial intelligence (AI) technology, exemplified by GPT-4 (OpenAI, 2023) consoli- dates various tasks in natural language process- ing into a single sequence generation problem. This unified architecture enables users to complete multiple tasks (e.g., question answering (Thop- pilan et al., 2022), code generation (Chen et al., 2021), math problem solving (Lewkowycz et al., 2022), and scientific literature generation (Taylor et al., 2022)) through a natural language inter- face (Liu et al., 2023) with both unprecedented performance (Bubeck et al., 2023) and interactiv- ity. Current literature on detecting and mitigating factual errors generated by machine learning mod- els focuses predominantly on a single specific task, for example, retrieval-augmented verification mod- els for QA (Lewis et al., 2020), hallucination detec- tion models for text summarization (Fabbri et al., 2022), and execution-based evaluation for code (Shi et al., 2022). While these methods have proven successful within their respective areas, given the remarkable versatility of tasks and domains han- dled by LLMs, we argue that it is also important However, at the same time, such a generative paradigm also introduces some unique challenges. \u2217Corresponding author Response Claim Evidence Scenario Methods Length Generated by Granularity Provided Provided Domain Task FEVER-based FactCC QAGS-based WICE-based RARR 7.30 20.83 16.11 24.20 - Human Synthetic Model Human PaLM/LaMDA Fact Sentence Summary Fact Fact \u2713 \u2713 \u2713 \u2713 X X \u2713 \u2713 \u2713 X Wikipedia Newswire Newswire Wikipedia Wikipedia QA Fact Verification Summ. Factuality Summ. Factuality Entailment FACTOOL 41.80 30.37 67.13 76.34 ChatGPT ChatGPT ChatGPT ChatGPT Fact Snippet Statement Tuple X X X X X X X X Wikipedia QA Python Math Sci. text Code generation Math Problems Sci. Review Table 1: A comparison of published approaches for factuality detection in terms of generated responses and claims to be verified based on collected evidence. \u201cScenario\u201d represents which task and domain the corresponding approach has been justified. \u201cSci.\u201d represents \u201cScientific\u201d. to have a more comprehensive factuality detection and verification framework that is similarly versa- tile. Additionally, in the current literature, the task of factuality detection is usually simplified as ei- ther (i) given a claim, determining whether it is factually correct, (ii) or given evidence, determin- ing whether the generated claim is supported. This task definition is not well suited to writing tasks that users commonly engage with when interacting with generative models (e.g., ChatGPT), where we often need to validate the factuality of a long-form generation without explicit claims and evidence. We connect the concept of \u201ctool use\u201d with \u201cfac- tuality detection\u201d, developing a unified and ver- satile framework for factuality detection across a variety of domains and tasks. We use FACTOOL to evaluate the factuality of modern chatbots, and found that GPT-4 has the best factuality across almost all scenarios. Su- pervisely fine-tuned chatbots (Vicuna-13B) have reasonably good factuality in KB-based QA but perform poorly in more challenging scenarios, in- cluding code generation, math problem solving, and scientific literature review writing. In this paper, we propose a task and domain- agnostic framework, FACTOOL, which aims to de- tect factual errors in LLM-generated texts. We il- lustrate our framework in Fig. 1, where we connect the concept of \u201ctool use\u201d (Thoppilan et al., 2022; Gao et al., 2022b; Schick et al., 2023) with \u201cfac- tuality detection\u201d and demonstrate that the ability to use tools in LLMs is crucial for factuality de- tection. Specifically, FACTOOL leverages various tools, including Google Search, Google Scholar, code interpreters, Python, or even LLMs them- selves, to gather evidence about the factuality of the generated content. Moreover, our framework employs the reasoning abilities of LLMs to assess the factuality of the content, given the evidence that has been gathered. We develop a benchmark and perform experiments across four tasks: knowledge- based QA, code generation, math problem solving, and scientific literature review writing. In summary, our contributions"}, {"question": " Why is it crucial to systematically identify errors in generative AI content in high-stakes areas?,answer: To improve the usefulness and reliability of the generated content, especially in fields like healthcare, finance, and law.", "ref_chunk": "Claims1. 2. Evidence Factuality FacTool EvidenceCollectionTool Querying ClaimExtractionQueryGenerationVerification !FacTool Tool Queriesa. b. Self-check 12345 Prompt 0 100 ! ChatGPT Response Tools FACTOOL: Factuality Detection in Generative AI A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios I-Chun Chern2 Steffi Chern2 Shiqi Chen3 Weizhe Yuan4 Kehua Feng1 Pengfei Liu1,7\u2217 Chunting Zhou5 Junxian He6 Graham Neubig2 1Shanghai Jiao Tong University 2Carnegie Mellon University 3City University of Hong Kong 4New York University 5Meta AI 6The Hong Kong University of Science and Technology 7Shanghai Artificial Intelligence Laboratory Abstract The emergence of generative pre-trained mod- els has facilitated the synthesis of high-quality text, but it has also posed challenges in identi- fying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence avail- able during the process of fact checking. 3 2 0 2 l u J 6 2 ] L C . s c [ With the above challenges in mind, in this paper, we propose FACTOOL, a task and do- main agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and sci- entific literature review) show the efficacy of the proposed method. We release the code of FACTOOL associated with ChatGPT plu- gin interface at https://github.com/ GAIR-NLP/factool. Figure 1: Tool-augmented framework for factuality de- tection. 2 v 8 2 5 3 1 . 7 0 3 2 : v i X r a Content that is automatically generated can of- ten exhibit inaccuracies or deviations from the truth due to the limited capacity of large language models (LLMs) (Ji et al., 2023; Schulman, 2023). LLMs are susceptible to producing content that appears credible but may actually be factually in- correct or imprecise. This limitation restricts the application of generative AI in some high-stakes ar- eas, such as healthcare, finance, and law. Therefore, it is crucial to identify these errors systematically to improve the usefulness and reliability of the gen- erated content. 1 Introduction Generative artificial intelligence (AI) technology, exemplified by GPT-4 (OpenAI, 2023) consoli- dates various tasks in natural language process- ing into a single sequence generation problem. This unified architecture enables users to complete multiple tasks (e.g., question answering (Thop- pilan et al., 2022), code generation (Chen et al., 2021), math problem solving (Lewkowycz et al., 2022), and scientific literature generation (Taylor et al., 2022)) through a natural language inter- face (Liu et al., 2023) with both unprecedented performance (Bubeck et al., 2023) and interactiv- ity. Current literature on detecting and mitigating factual errors generated by machine learning mod- els focuses predominantly on a single specific task, for example, retrieval-augmented verification mod- els for QA (Lewis et al., 2020), hallucination detec- tion models for text summarization (Fabbri et al., 2022), and execution-based evaluation for code (Shi et al., 2022). While these methods have proven successful within their respective areas, given the remarkable versatility of tasks and domains han- dled by LLMs, we argue that it is also important However, at the same time, such a generative paradigm also introduces some unique challenges. \u2217Corresponding author Response Claim Evidence Scenario Methods Length Generated by Granularity Provided Provided Domain Task FEVER-based FactCC QAGS-based WICE-based RARR 7.30 20.83 16.11 24.20 - Human Synthetic Model Human PaLM/LaMDA Fact Sentence Summary Fact Fact \u2713 \u2713 \u2713 \u2713 X X \u2713 \u2713 \u2713 X Wikipedia Newswire Newswire Wikipedia Wikipedia QA Fact Verification Summ. Factuality Summ. Factuality Entailment FACTOOL 41.80 30.37 67.13 76.34 ChatGPT ChatGPT ChatGPT ChatGPT Fact Snippet Statement Tuple X X X X X X X X Wikipedia QA Python Math Sci. text Code generation Math Problems Sci. Review Table 1: A comparison of published approaches for factuality detection in terms of generated responses and claims to be verified based on collected evidence. \u201cScenario\u201d represents which task and domain the corresponding approach has been justified. \u201cSci.\u201d represents \u201cScientific\u201d. to have a more comprehensive factuality detection and verification framework that is similarly versa- tile. Additionally, in the current literature, the task of factuality detection is usually simplified as ei- ther (i) given a claim, determining whether it is factually correct, (ii) or given evidence, determin- ing whether the generated claim is supported. This task definition is not well suited to writing tasks that users commonly engage with when interacting with generative models (e.g., ChatGPT), where we often need to validate the factuality of a long-form generation without explicit claims and evidence. We connect the concept of \u201ctool use\u201d with \u201cfac- tuality detection\u201d, developing a unified and ver- satile framework for factuality detection across a variety of domains and tasks. We use FACTOOL to evaluate the factuality of modern chatbots, and found that GPT-4 has the best factuality across almost all scenarios. Su- pervisely fine-tuned chatbots (Vicuna-13B) have reasonably good factuality in KB-based QA but perform poorly in more challenging scenarios, in- cluding code generation, math problem solving, and scientific literature review writing. In this paper, we propose a task and domain- agnostic framework, FACTOOL, which aims to de- tect factual errors in LLM-generated texts. We il- lustrate our framework in Fig. 1, where we connect the concept of \u201ctool use\u201d (Thoppilan et al., 2022; Gao et al., 2022b; Schick et al., 2023) with \u201cfac- tuality detection\u201d and demonstrate that the ability to use tools in LLMs is crucial for factuality de- tection. Specifically, FACTOOL leverages various tools, including Google Search, Google Scholar, code interpreters, Python, or even LLMs them- selves, to gather evidence about the factuality of the generated content. Moreover, our framework employs the reasoning abilities of LLMs to assess the factuality of the content, given the evidence that has been gathered. We develop a benchmark and perform experiments across four tasks: knowledge- based QA, code generation, math problem solving, and scientific literature review writing. In summary, our contributions"}, {"question": " What is the main purpose of the proposed FACTOOL framework?,answer: To detect factual errors in texts generated by large language models in a task and domain agnostic manner.", "ref_chunk": "Claims1. 2. Evidence Factuality FacTool EvidenceCollectionTool Querying ClaimExtractionQueryGenerationVerification !FacTool Tool Queriesa. b. Self-check 12345 Prompt 0 100 ! ChatGPT Response Tools FACTOOL: Factuality Detection in Generative AI A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios I-Chun Chern2 Steffi Chern2 Shiqi Chen3 Weizhe Yuan4 Kehua Feng1 Pengfei Liu1,7\u2217 Chunting Zhou5 Junxian He6 Graham Neubig2 1Shanghai Jiao Tong University 2Carnegie Mellon University 3City University of Hong Kong 4New York University 5Meta AI 6The Hong Kong University of Science and Technology 7Shanghai Artificial Intelligence Laboratory Abstract The emergence of generative pre-trained mod- els has facilitated the synthesis of high-quality text, but it has also posed challenges in identi- fying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence avail- able during the process of fact checking. 3 2 0 2 l u J 6 2 ] L C . s c [ With the above challenges in mind, in this paper, we propose FACTOOL, a task and do- main agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and sci- entific literature review) show the efficacy of the proposed method. We release the code of FACTOOL associated with ChatGPT plu- gin interface at https://github.com/ GAIR-NLP/factool. Figure 1: Tool-augmented framework for factuality de- tection. 2 v 8 2 5 3 1 . 7 0 3 2 : v i X r a Content that is automatically generated can of- ten exhibit inaccuracies or deviations from the truth due to the limited capacity of large language models (LLMs) (Ji et al., 2023; Schulman, 2023). LLMs are susceptible to producing content that appears credible but may actually be factually in- correct or imprecise. This limitation restricts the application of generative AI in some high-stakes ar- eas, such as healthcare, finance, and law. Therefore, it is crucial to identify these errors systematically to improve the usefulness and reliability of the gen- erated content. 1 Introduction Generative artificial intelligence (AI) technology, exemplified by GPT-4 (OpenAI, 2023) consoli- dates various tasks in natural language process- ing into a single sequence generation problem. This unified architecture enables users to complete multiple tasks (e.g., question answering (Thop- pilan et al., 2022), code generation (Chen et al., 2021), math problem solving (Lewkowycz et al., 2022), and scientific literature generation (Taylor et al., 2022)) through a natural language inter- face (Liu et al., 2023) with both unprecedented performance (Bubeck et al., 2023) and interactiv- ity. Current literature on detecting and mitigating factual errors generated by machine learning mod- els focuses predominantly on a single specific task, for example, retrieval-augmented verification mod- els for QA (Lewis et al., 2020), hallucination detec- tion models for text summarization (Fabbri et al., 2022), and execution-based evaluation for code (Shi et al., 2022). While these methods have proven successful within their respective areas, given the remarkable versatility of tasks and domains han- dled by LLMs, we argue that it is also important However, at the same time, such a generative paradigm also introduces some unique challenges. \u2217Corresponding author Response Claim Evidence Scenario Methods Length Generated by Granularity Provided Provided Domain Task FEVER-based FactCC QAGS-based WICE-based RARR 7.30 20.83 16.11 24.20 - Human Synthetic Model Human PaLM/LaMDA Fact Sentence Summary Fact Fact \u2713 \u2713 \u2713 \u2713 X X \u2713 \u2713 \u2713 X Wikipedia Newswire Newswire Wikipedia Wikipedia QA Fact Verification Summ. Factuality Summ. Factuality Entailment FACTOOL 41.80 30.37 67.13 76.34 ChatGPT ChatGPT ChatGPT ChatGPT Fact Snippet Statement Tuple X X X X X X X X Wikipedia QA Python Math Sci. text Code generation Math Problems Sci. Review Table 1: A comparison of published approaches for factuality detection in terms of generated responses and claims to be verified based on collected evidence. \u201cScenario\u201d represents which task and domain the corresponding approach has been justified. \u201cSci.\u201d represents \u201cScientific\u201d. to have a more comprehensive factuality detection and verification framework that is similarly versa- tile. Additionally, in the current literature, the task of factuality detection is usually simplified as ei- ther (i) given a claim, determining whether it is factually correct, (ii) or given evidence, determin- ing whether the generated claim is supported. This task definition is not well suited to writing tasks that users commonly engage with when interacting with generative models (e.g., ChatGPT), where we often need to validate the factuality of a long-form generation without explicit claims and evidence. We connect the concept of \u201ctool use\u201d with \u201cfac- tuality detection\u201d, developing a unified and ver- satile framework for factuality detection across a variety of domains and tasks. We use FACTOOL to evaluate the factuality of modern chatbots, and found that GPT-4 has the best factuality across almost all scenarios. Su- pervisely fine-tuned chatbots (Vicuna-13B) have reasonably good factuality in KB-based QA but perform poorly in more challenging scenarios, in- cluding code generation, math problem solving, and scientific literature review writing. In this paper, we propose a task and domain- agnostic framework, FACTOOL, which aims to de- tect factual errors in LLM-generated texts. We il- lustrate our framework in Fig. 1, where we connect the concept of \u201ctool use\u201d (Thoppilan et al., 2022; Gao et al., 2022b; Schick et al., 2023) with \u201cfac- tuality detection\u201d and demonstrate that the ability to use tools in LLMs is crucial for factuality de- tection. Specifically, FACTOOL leverages various tools, including Google Search, Google Scholar, code interpreters, Python, or even LLMs them- selves, to gather evidence about the factuality of the generated content. Moreover, our framework employs the reasoning abilities of LLMs to assess the factuality of the content, given the evidence that has been gathered. We develop a benchmark and perform experiments across four tasks: knowledge- based QA, code generation, math problem solving, and scientific literature review writing. In summary, our contributions"}, {"question": " How does FACTOOL use various tools to gather evidence about the factuality of generated content?,answer: By leveraging tools such as Google Search, Google Scholar, code interpreters, Python, or even LLMs themselves.", "ref_chunk": "Claims1. 2. Evidence Factuality FacTool EvidenceCollectionTool Querying ClaimExtractionQueryGenerationVerification !FacTool Tool Queriesa. b. Self-check 12345 Prompt 0 100 ! ChatGPT Response Tools FACTOOL: Factuality Detection in Generative AI A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios I-Chun Chern2 Steffi Chern2 Shiqi Chen3 Weizhe Yuan4 Kehua Feng1 Pengfei Liu1,7\u2217 Chunting Zhou5 Junxian He6 Graham Neubig2 1Shanghai Jiao Tong University 2Carnegie Mellon University 3City University of Hong Kong 4New York University 5Meta AI 6The Hong Kong University of Science and Technology 7Shanghai Artificial Intelligence Laboratory Abstract The emergence of generative pre-trained mod- els has facilitated the synthesis of high-quality text, but it has also posed challenges in identi- fying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence avail- able during the process of fact checking. 3 2 0 2 l u J 6 2 ] L C . s c [ With the above challenges in mind, in this paper, we propose FACTOOL, a task and do- main agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and sci- entific literature review) show the efficacy of the proposed method. We release the code of FACTOOL associated with ChatGPT plu- gin interface at https://github.com/ GAIR-NLP/factool. Figure 1: Tool-augmented framework for factuality de- tection. 2 v 8 2 5 3 1 . 7 0 3 2 : v i X r a Content that is automatically generated can of- ten exhibit inaccuracies or deviations from the truth due to the limited capacity of large language models (LLMs) (Ji et al., 2023; Schulman, 2023). LLMs are susceptible to producing content that appears credible but may actually be factually in- correct or imprecise. This limitation restricts the application of generative AI in some high-stakes ar- eas, such as healthcare, finance, and law. Therefore, it is crucial to identify these errors systematically to improve the usefulness and reliability of the gen- erated content. 1 Introduction Generative artificial intelligence (AI) technology, exemplified by GPT-4 (OpenAI, 2023) consoli- dates various tasks in natural language process- ing into a single sequence generation problem. This unified architecture enables users to complete multiple tasks (e.g., question answering (Thop- pilan et al., 2022), code generation (Chen et al., 2021), math problem solving (Lewkowycz et al., 2022), and scientific literature generation (Taylor et al., 2022)) through a natural language inter- face (Liu et al., 2023) with both unprecedented performance (Bubeck et al., 2023) and interactiv- ity. Current literature on detecting and mitigating factual errors generated by machine learning mod- els focuses predominantly on a single specific task, for example, retrieval-augmented verification mod- els for QA (Lewis et al., 2020), hallucination detec- tion models for text summarization (Fabbri et al., 2022), and execution-based evaluation for code (Shi et al., 2022). While these methods have proven successful within their respective areas, given the remarkable versatility of tasks and domains han- dled by LLMs, we argue that it is also important However, at the same time, such a generative paradigm also introduces some unique challenges. \u2217Corresponding author Response Claim Evidence Scenario Methods Length Generated by Granularity Provided Provided Domain Task FEVER-based FactCC QAGS-based WICE-based RARR 7.30 20.83 16.11 24.20 - Human Synthetic Model Human PaLM/LaMDA Fact Sentence Summary Fact Fact \u2713 \u2713 \u2713 \u2713 X X \u2713 \u2713 \u2713 X Wikipedia Newswire Newswire Wikipedia Wikipedia QA Fact Verification Summ. Factuality Summ. Factuality Entailment FACTOOL 41.80 30.37 67.13 76.34 ChatGPT ChatGPT ChatGPT ChatGPT Fact Snippet Statement Tuple X X X X X X X X Wikipedia QA Python Math Sci. text Code generation Math Problems Sci. Review Table 1: A comparison of published approaches for factuality detection in terms of generated responses and claims to be verified based on collected evidence. \u201cScenario\u201d represents which task and domain the corresponding approach has been justified. \u201cSci.\u201d represents \u201cScientific\u201d. to have a more comprehensive factuality detection and verification framework that is similarly versa- tile. Additionally, in the current literature, the task of factuality detection is usually simplified as ei- ther (i) given a claim, determining whether it is factually correct, (ii) or given evidence, determin- ing whether the generated claim is supported. This task definition is not well suited to writing tasks that users commonly engage with when interacting with generative models (e.g., ChatGPT), where we often need to validate the factuality of a long-form generation without explicit claims and evidence. We connect the concept of \u201ctool use\u201d with \u201cfac- tuality detection\u201d, developing a unified and ver- satile framework for factuality detection across a variety of domains and tasks. We use FACTOOL to evaluate the factuality of modern chatbots, and found that GPT-4 has the best factuality across almost all scenarios. Su- pervisely fine-tuned chatbots (Vicuna-13B) have reasonably good factuality in KB-based QA but perform poorly in more challenging scenarios, in- cluding code generation, math problem solving, and scientific literature review writing. In this paper, we propose a task and domain- agnostic framework, FACTOOL, which aims to de- tect factual errors in LLM-generated texts. We il- lustrate our framework in Fig. 1, where we connect the concept of \u201ctool use\u201d (Thoppilan et al., 2022; Gao et al., 2022b; Schick et al., 2023) with \u201cfac- tuality detection\u201d and demonstrate that the ability to use tools in LLMs is crucial for factuality de- tection. Specifically, FACTOOL leverages various tools, including Google Search, Google Scholar, code interpreters, Python, or even LLMs them- selves, to gather evidence about the factuality of the generated content. Moreover, our framework employs the reasoning abilities of LLMs to assess the factuality of the content, given the evidence that has been gathered. We develop a benchmark and perform experiments across four tasks: knowledge- based QA, code generation, math problem solving, and scientific literature review writing. In summary, our contributions"}, {"question": " What were the main findings regarding the factuality of chatbots like GPT-4 according to the study?,answer: GPT-4 had the best factuality across almost all scenarios, while fine-tuned chatbots like Vicuna-13B had good factuality in KB-based QA but performed poorly in more challenging scenarios.", "ref_chunk": "Claims1. 2. Evidence Factuality FacTool EvidenceCollectionTool Querying ClaimExtractionQueryGenerationVerification !FacTool Tool Queriesa. b. Self-check 12345 Prompt 0 100 ! ChatGPT Response Tools FACTOOL: Factuality Detection in Generative AI A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios I-Chun Chern2 Steffi Chern2 Shiqi Chen3 Weizhe Yuan4 Kehua Feng1 Pengfei Liu1,7\u2217 Chunting Zhou5 Junxian He6 Graham Neubig2 1Shanghai Jiao Tong University 2Carnegie Mellon University 3City University of Hong Kong 4New York University 5Meta AI 6The Hong Kong University of Science and Technology 7Shanghai Artificial Intelligence Laboratory Abstract The emergence of generative pre-trained mod- els has facilitated the synthesis of high-quality text, but it has also posed challenges in identi- fying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence avail- able during the process of fact checking. 3 2 0 2 l u J 6 2 ] L C . s c [ With the above challenges in mind, in this paper, we propose FACTOOL, a task and do- main agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and sci- entific literature review) show the efficacy of the proposed method. We release the code of FACTOOL associated with ChatGPT plu- gin interface at https://github.com/ GAIR-NLP/factool. Figure 1: Tool-augmented framework for factuality de- tection. 2 v 8 2 5 3 1 . 7 0 3 2 : v i X r a Content that is automatically generated can of- ten exhibit inaccuracies or deviations from the truth due to the limited capacity of large language models (LLMs) (Ji et al., 2023; Schulman, 2023). LLMs are susceptible to producing content that appears credible but may actually be factually in- correct or imprecise. This limitation restricts the application of generative AI in some high-stakes ar- eas, such as healthcare, finance, and law. Therefore, it is crucial to identify these errors systematically to improve the usefulness and reliability of the gen- erated content. 1 Introduction Generative artificial intelligence (AI) technology, exemplified by GPT-4 (OpenAI, 2023) consoli- dates various tasks in natural language process- ing into a single sequence generation problem. This unified architecture enables users to complete multiple tasks (e.g., question answering (Thop- pilan et al., 2022), code generation (Chen et al., 2021), math problem solving (Lewkowycz et al., 2022), and scientific literature generation (Taylor et al., 2022)) through a natural language inter- face (Liu et al., 2023) with both unprecedented performance (Bubeck et al., 2023) and interactiv- ity. Current literature on detecting and mitigating factual errors generated by machine learning mod- els focuses predominantly on a single specific task, for example, retrieval-augmented verification mod- els for QA (Lewis et al., 2020), hallucination detec- tion models for text summarization (Fabbri et al., 2022), and execution-based evaluation for code (Shi et al., 2022). While these methods have proven successful within their respective areas, given the remarkable versatility of tasks and domains han- dled by LLMs, we argue that it is also important However, at the same time, such a generative paradigm also introduces some unique challenges. \u2217Corresponding author Response Claim Evidence Scenario Methods Length Generated by Granularity Provided Provided Domain Task FEVER-based FactCC QAGS-based WICE-based RARR 7.30 20.83 16.11 24.20 - Human Synthetic Model Human PaLM/LaMDA Fact Sentence Summary Fact Fact \u2713 \u2713 \u2713 \u2713 X X \u2713 \u2713 \u2713 X Wikipedia Newswire Newswire Wikipedia Wikipedia QA Fact Verification Summ. Factuality Summ. Factuality Entailment FACTOOL 41.80 30.37 67.13 76.34 ChatGPT ChatGPT ChatGPT ChatGPT Fact Snippet Statement Tuple X X X X X X X X Wikipedia QA Python Math Sci. text Code generation Math Problems Sci. Review Table 1: A comparison of published approaches for factuality detection in terms of generated responses and claims to be verified based on collected evidence. \u201cScenario\u201d represents which task and domain the corresponding approach has been justified. \u201cSci.\u201d represents \u201cScientific\u201d. to have a more comprehensive factuality detection and verification framework that is similarly versa- tile. Additionally, in the current literature, the task of factuality detection is usually simplified as ei- ther (i) given a claim, determining whether it is factually correct, (ii) or given evidence, determin- ing whether the generated claim is supported. This task definition is not well suited to writing tasks that users commonly engage with when interacting with generative models (e.g., ChatGPT), where we often need to validate the factuality of a long-form generation without explicit claims and evidence. We connect the concept of \u201ctool use\u201d with \u201cfac- tuality detection\u201d, developing a unified and ver- satile framework for factuality detection across a variety of domains and tasks. We use FACTOOL to evaluate the factuality of modern chatbots, and found that GPT-4 has the best factuality across almost all scenarios. Su- pervisely fine-tuned chatbots (Vicuna-13B) have reasonably good factuality in KB-based QA but perform poorly in more challenging scenarios, in- cluding code generation, math problem solving, and scientific literature review writing. In this paper, we propose a task and domain- agnostic framework, FACTOOL, which aims to de- tect factual errors in LLM-generated texts. We il- lustrate our framework in Fig. 1, where we connect the concept of \u201ctool use\u201d (Thoppilan et al., 2022; Gao et al., 2022b; Schick et al., 2023) with \u201cfac- tuality detection\u201d and demonstrate that the ability to use tools in LLMs is crucial for factuality de- tection. Specifically, FACTOOL leverages various tools, including Google Search, Google Scholar, code interpreters, Python, or even LLMs them- selves, to gather evidence about the factuality of the generated content. Moreover, our framework employs the reasoning abilities of LLMs to assess the factuality of the content, given the evidence that has been gathered. We develop a benchmark and perform experiments across four tasks: knowledge- based QA, code generation, math problem solving, and scientific literature review writing. In summary, our contributions"}], "doc_text": "Claims1. 2. Evidence Factuality FacTool EvidenceCollectionTool Querying ClaimExtractionQueryGenerationVerification !FacTool Tool Queriesa. b. Self-check 12345 Prompt 0 100 ! ChatGPT Response Tools FACTOOL: Factuality Detection in Generative AI A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios I-Chun Chern2 Steffi Chern2 Shiqi Chen3 Weizhe Yuan4 Kehua Feng1 Pengfei Liu1,7\u2217 Chunting Zhou5 Junxian He6 Graham Neubig2 1Shanghai Jiao Tong University 2Carnegie Mellon University 3City University of Hong Kong 4New York University 5Meta AI 6The Hong Kong University of Science and Technology 7Shanghai Artificial Intelligence Laboratory Abstract The emergence of generative pre-trained mod- els has facilitated the synthesis of high-quality text, but it has also posed challenges in identi- fying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence avail- able during the process of fact checking. 3 2 0 2 l u J 6 2 ] L C . s c [ With the above challenges in mind, in this paper, we propose FACTOOL, a task and do- main agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and sci- entific literature review) show the efficacy of the proposed method. We release the code of FACTOOL associated with ChatGPT plu- gin interface at https://github.com/ GAIR-NLP/factool. Figure 1: Tool-augmented framework for factuality de- tection. 2 v 8 2 5 3 1 . 7 0 3 2 : v i X r a Content that is automatically generated can of- ten exhibit inaccuracies or deviations from the truth due to the limited capacity of large language models (LLMs) (Ji et al., 2023; Schulman, 2023). LLMs are susceptible to producing content that appears credible but may actually be factually in- correct or imprecise. This limitation restricts the application of generative AI in some high-stakes ar- eas, such as healthcare, finance, and law. Therefore, it is crucial to identify these errors systematically to improve the usefulness and reliability of the gen- erated content. 1 Introduction Generative artificial intelligence (AI) technology, exemplified by GPT-4 (OpenAI, 2023) consoli- dates various tasks in natural language process- ing into a single sequence generation problem. This unified architecture enables users to complete multiple tasks (e.g., question answering (Thop- pilan et al., 2022), code generation (Chen et al., 2021), math problem solving (Lewkowycz et al., 2022), and scientific literature generation (Taylor et al., 2022)) through a natural language inter- face (Liu et al., 2023) with both unprecedented performance (Bubeck et al., 2023) and interactiv- ity. Current literature on detecting and mitigating factual errors generated by machine learning mod- els focuses predominantly on a single specific task, for example, retrieval-augmented verification mod- els for QA (Lewis et al., 2020), hallucination detec- tion models for text summarization (Fabbri et al., 2022), and execution-based evaluation for code (Shi et al., 2022). While these methods have proven successful within their respective areas, given the remarkable versatility of tasks and domains han- dled by LLMs, we argue that it is also important However, at the same time, such a generative paradigm also introduces some unique challenges. \u2217Corresponding author Response Claim Evidence Scenario Methods Length Generated by Granularity Provided Provided Domain Task FEVER-based FactCC QAGS-based WICE-based RARR 7.30 20.83 16.11 24.20 - Human Synthetic Model Human PaLM/LaMDA Fact Sentence Summary Fact Fact \u2713 \u2713 \u2713 \u2713 X X \u2713 \u2713 \u2713 X Wikipedia Newswire Newswire Wikipedia Wikipedia QA Fact Verification Summ. Factuality Summ. Factuality Entailment FACTOOL 41.80 30.37 67.13 76.34 ChatGPT ChatGPT ChatGPT ChatGPT Fact Snippet Statement Tuple X X X X X X X X Wikipedia QA Python Math Sci. text Code generation Math Problems Sci. Review Table 1: A comparison of published approaches for factuality detection in terms of generated responses and claims to be verified based on collected evidence. \u201cScenario\u201d represents which task and domain the corresponding approach has been justified. \u201cSci.\u201d represents \u201cScientific\u201d. to have a more comprehensive factuality detection and verification framework that is similarly versa- tile. Additionally, in the current literature, the task of factuality detection is usually simplified as ei- ther (i) given a claim, determining whether it is factually correct, (ii) or given evidence, determin- ing whether the generated claim is supported. This task definition is not well suited to writing tasks that users commonly engage with when interacting with generative models (e.g., ChatGPT), where we often need to validate the factuality of a long-form generation without explicit claims and evidence. We connect the concept of \u201ctool use\u201d with \u201cfac- tuality detection\u201d, developing a unified and ver- satile framework for factuality detection across a variety of domains and tasks. We use FACTOOL to evaluate the factuality of modern chatbots, and found that GPT-4 has the best factuality across almost all scenarios. Su- pervisely fine-tuned chatbots (Vicuna-13B) have reasonably good factuality in KB-based QA but perform poorly in more challenging scenarios, in- cluding code generation, math problem solving, and scientific literature review writing. In this paper, we propose a task and domain- agnostic framework, FACTOOL, which aims to de- tect factual errors in LLM-generated texts. We il- lustrate our framework in Fig. 1, where we connect the concept of \u201ctool use\u201d (Thoppilan et al., 2022; Gao et al., 2022b; Schick et al., 2023) with \u201cfac- tuality detection\u201d and demonstrate that the ability to use tools in LLMs is crucial for factuality de- tection. Specifically, FACTOOL leverages various tools, including Google Search, Google Scholar, code interpreters, Python, or even LLMs them- selves, to gather evidence about the factuality of the generated content. Moreover, our framework employs the reasoning abilities of LLMs to assess the factuality of the content, given the evidence that has been gathered. We develop a benchmark and perform experiments across four tasks: knowledge- based QA, code generation, math problem solving, and scientific literature review writing. In summary, our contributions"}