{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_One-for-All:_Generalized_LoRA_for_Parameter-Efficient_Fine-tuning_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the percentage by which GLoRA enhances out-of-domain performance in ImageNet-A?", "answer": " 100%", "ref_chunk": "compared with LoRA, GLoRA enhances out-of- domain performance by as much as 100% (ImageNet-A) and 50% (ImageNet-Sketch). 4 ANALYSIS AND DISCUSSION Computational Cost. We show the final inference throughput of various PEFT methods in Table 4, computed on an NVIDIA 3090 GPU. The results highlight that GLoRA surpasses other competitive methods in performance, as it does not require any extra parameters or FLOPs during the inference stage. An additional advantage is its quicker adaptability in real-world scenarios, especially when prior or foundational models are already deployed. The weights of GLoRA can be directly loaded without necessitating any manual system modifications. As previously mentioned, GLoRA supports VPT-Deep level prompts via the support tensor C, however, it does not impose any computational overhead due to its completely structural re-parameterization design. Visualizations of searched fine-tuning strat- egy for each layer. Figure 3 visually shows the distribution of trainable parameters across the four types of linear layers embodied in ViT-B. Notably, the projection layer possesses the min- imum quantity of trainable parameters span- ning across VTAB-1K categories. Generally, the MLP module hosts a substantially higher number of parameters compared to the MHSA. As anticipated, the structured group necessi- tates a greater number of parameters for adap- tation due to a pronounced domain shift rela- tive to ImageNet-1K (Deng et al., 2009b). Fig- ure 4 illustrates the layer-wise configuration of the support tensors as searched by the GLoRA algorithm. Each support tensor at every layer can potentially undergo 72 distinct adaptations across datasets. Support tensors D and E exhibit relatively low adaptation due to the prevalence of Projection FC2Layer Type 0.00 Q-K-V 0.05 FC1 0.20 0.10 Natural 0.30Number of Parameters(M) Specialized Structured 0.15 0.25 Figure 3: Distribution of GLoRA (0.86M) param- eters across layer types on VTAB-1K. Q-K-V and Projection are linear layers in MHSA module and FC1 and FC2 are linear layers in MLP module. 8 A A D 50 A A ELayer Depth (Total=12) D 20 B D D C B B B B 0 B C LoRA C A 30 C C A C C B A C C C C C D D E E E E E E E B E B E E vector A 40 B E 70Number of Occurences (Total=72) B D A A A D A 10 D D D D constant 60 B Figure 4: Layerwise configuration of support tensors in GLoRA (0.86M) on VTAB-1K dataset. none adaptations, whereas A and B demonstrate a higher number of adaptations, though without a distinguishable pattern regarding the type of adaptation. It is important to underscore that even a basic scalar can function effectively as a support tensor, enabling GLoRA to maintain superior parameter efficiency despite adapting every linear layer. 5 RELATED WORK Given the rapid expansion in model size, numerous methods for parameter-efficient fine-tuning (PEFT) have been introduced in the field of NLP to streamline the optimization of large language models (LLMs) (Liu et al., 2021a; Zhang et al.; Hu et al.; Liu et al., 2021b; Li & Liang, 2021; Lester et al., 2021; Zaken et al., 2022; Houlsby et al., 2019). The effectiveness of parameter-efficient fine- tuning has been proven in a wide range of natural language processing tasks (Fu et al., 2022; He et al., 2021). In the vision domain, with the advent growth in the size of vision models (Dehghani et al., 2023; Kolesnikov et al., 2020), methods specifically focused on image modality have also been put forward (Jie & Deng, 2022; Lian et al., 2022; Chen et al., 2022; Luo et al., 2023; Zhang et al., 2022; Jia et al., 2022; He et al., 2023). Among these methods, LoRA (Hu et al.) has proven to transfer well across modalities and tasks. This is partly due to the simplistic design strategy of LoRA which directly works over weight tensors, irrespective of model type or configuration. Additionally, unlike Adapters (Houlsby et al., 2019; Chen et al., 2022) and Prompt tuning (Jia et al., 2022), LoRA does not add any additional inference parameters or latency due to structural re-parameterization (SR) design. RepAdapter (Luo et al., 2023) and SSF (Lian et al., 2022) also propose an SR design for PEFT. However, RepAdapter is specific to model architectures and required manual designing for different layer configurations. SSF provides a simple baseline but suffers from low flexibility and capability due to adaptation limited in the activation space. FacT (Jie & Deng, 2022) further decomposes LoRA matrices for better parameter efficiency, but we argue that <1M parameter scale is fairly efficient for fine-tuning on a single GPU. Thus, due to the advantages of LoRA over other related works, it is of importance to increase the flexibility, scalability and adaptability of LoRA. 6 CONCLUSION We have presented GLoRA, a generalized parameter-efficient fine-tuning approach that has suc- cessfully demonstrated its effectiveness and adaptability in enhancing the fine-tuning and transfer learning ability for the large-scale pre-trained models. By adopting a generalized low-rank adap- tation and re-parameterization framework, GLoRA significantly reduces the number of parameters and computation required for fine-tuning, making it a more resource-efficient and practical method for real-world applications. The experiments conducted on a diverse range of tasks and datasets have substantiated the superiority of GLoRA over existing PEFT techniques, showcasing its scala- bility and adaptability. Moreover, the ablation studies have provided valuable insights into the inner workings and the relative importance of different GLoRA components. This work not only con- tributes to the improvement of the fine-tuning process for large-scale pre-trained vision or language models but also opens up new avenues for future work, including further exploration of generalized low-rank adaptation techniques, the development of hybrid approaches, and the refinement of search and optimization algorithms. These areas of research may continue to expand the accessibility and efficiency of transfer learning across a broader range of applications. 9 REFERENCES Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative com- ponents with random forests. In European Conference on Computer Vision, 2014. 6 Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin"}, {"question": " On which GPU was the final inference throughput of various PEFT methods computed?", "answer": " NVIDIA 3090", "ref_chunk": "compared with LoRA, GLoRA enhances out-of- domain performance by as much as 100% (ImageNet-A) and 50% (ImageNet-Sketch). 4 ANALYSIS AND DISCUSSION Computational Cost. We show the final inference throughput of various PEFT methods in Table 4, computed on an NVIDIA 3090 GPU. The results highlight that GLoRA surpasses other competitive methods in performance, as it does not require any extra parameters or FLOPs during the inference stage. An additional advantage is its quicker adaptability in real-world scenarios, especially when prior or foundational models are already deployed. The weights of GLoRA can be directly loaded without necessitating any manual system modifications. As previously mentioned, GLoRA supports VPT-Deep level prompts via the support tensor C, however, it does not impose any computational overhead due to its completely structural re-parameterization design. Visualizations of searched fine-tuning strat- egy for each layer. Figure 3 visually shows the distribution of trainable parameters across the four types of linear layers embodied in ViT-B. Notably, the projection layer possesses the min- imum quantity of trainable parameters span- ning across VTAB-1K categories. Generally, the MLP module hosts a substantially higher number of parameters compared to the MHSA. As anticipated, the structured group necessi- tates a greater number of parameters for adap- tation due to a pronounced domain shift rela- tive to ImageNet-1K (Deng et al., 2009b). Fig- ure 4 illustrates the layer-wise configuration of the support tensors as searched by the GLoRA algorithm. Each support tensor at every layer can potentially undergo 72 distinct adaptations across datasets. Support tensors D and E exhibit relatively low adaptation due to the prevalence of Projection FC2Layer Type 0.00 Q-K-V 0.05 FC1 0.20 0.10 Natural 0.30Number of Parameters(M) Specialized Structured 0.15 0.25 Figure 3: Distribution of GLoRA (0.86M) param- eters across layer types on VTAB-1K. Q-K-V and Projection are linear layers in MHSA module and FC1 and FC2 are linear layers in MLP module. 8 A A D 50 A A ELayer Depth (Total=12) D 20 B D D C B B B B 0 B C LoRA C A 30 C C A C C B A C C C C C D D E E E E E E E B E B E E vector A 40 B E 70Number of Occurences (Total=72) B D A A A D A 10 D D D D constant 60 B Figure 4: Layerwise configuration of support tensors in GLoRA (0.86M) on VTAB-1K dataset. none adaptations, whereas A and B demonstrate a higher number of adaptations, though without a distinguishable pattern regarding the type of adaptation. It is important to underscore that even a basic scalar can function effectively as a support tensor, enabling GLoRA to maintain superior parameter efficiency despite adapting every linear layer. 5 RELATED WORK Given the rapid expansion in model size, numerous methods for parameter-efficient fine-tuning (PEFT) have been introduced in the field of NLP to streamline the optimization of large language models (LLMs) (Liu et al., 2021a; Zhang et al.; Hu et al.; Liu et al., 2021b; Li & Liang, 2021; Lester et al., 2021; Zaken et al., 2022; Houlsby et al., 2019). The effectiveness of parameter-efficient fine- tuning has been proven in a wide range of natural language processing tasks (Fu et al., 2022; He et al., 2021). In the vision domain, with the advent growth in the size of vision models (Dehghani et al., 2023; Kolesnikov et al., 2020), methods specifically focused on image modality have also been put forward (Jie & Deng, 2022; Lian et al., 2022; Chen et al., 2022; Luo et al., 2023; Zhang et al., 2022; Jia et al., 2022; He et al., 2023). Among these methods, LoRA (Hu et al.) has proven to transfer well across modalities and tasks. This is partly due to the simplistic design strategy of LoRA which directly works over weight tensors, irrespective of model type or configuration. Additionally, unlike Adapters (Houlsby et al., 2019; Chen et al., 2022) and Prompt tuning (Jia et al., 2022), LoRA does not add any additional inference parameters or latency due to structural re-parameterization (SR) design. RepAdapter (Luo et al., 2023) and SSF (Lian et al., 2022) also propose an SR design for PEFT. However, RepAdapter is specific to model architectures and required manual designing for different layer configurations. SSF provides a simple baseline but suffers from low flexibility and capability due to adaptation limited in the activation space. FacT (Jie & Deng, 2022) further decomposes LoRA matrices for better parameter efficiency, but we argue that <1M parameter scale is fairly efficient for fine-tuning on a single GPU. Thus, due to the advantages of LoRA over other related works, it is of importance to increase the flexibility, scalability and adaptability of LoRA. 6 CONCLUSION We have presented GLoRA, a generalized parameter-efficient fine-tuning approach that has suc- cessfully demonstrated its effectiveness and adaptability in enhancing the fine-tuning and transfer learning ability for the large-scale pre-trained models. By adopting a generalized low-rank adap- tation and re-parameterization framework, GLoRA significantly reduces the number of parameters and computation required for fine-tuning, making it a more resource-efficient and practical method for real-world applications. The experiments conducted on a diverse range of tasks and datasets have substantiated the superiority of GLoRA over existing PEFT techniques, showcasing its scala- bility and adaptability. Moreover, the ablation studies have provided valuable insights into the inner workings and the relative importance of different GLoRA components. This work not only con- tributes to the improvement of the fine-tuning process for large-scale pre-trained vision or language models but also opens up new avenues for future work, including further exploration of generalized low-rank adaptation techniques, the development of hybrid approaches, and the refinement of search and optimization algorithms. These areas of research may continue to expand the accessibility and efficiency of transfer learning across a broader range of applications. 9 REFERENCES Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative com- ponents with random forests. In European Conference on Computer Vision, 2014. 6 Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin"}, {"question": " What is one advantage of using GLoRA in real-world scenarios?", "answer": " Quicker adaptability, especially when prior or foundational models are already deployed", "ref_chunk": "compared with LoRA, GLoRA enhances out-of- domain performance by as much as 100% (ImageNet-A) and 50% (ImageNet-Sketch). 4 ANALYSIS AND DISCUSSION Computational Cost. We show the final inference throughput of various PEFT methods in Table 4, computed on an NVIDIA 3090 GPU. The results highlight that GLoRA surpasses other competitive methods in performance, as it does not require any extra parameters or FLOPs during the inference stage. An additional advantage is its quicker adaptability in real-world scenarios, especially when prior or foundational models are already deployed. The weights of GLoRA can be directly loaded without necessitating any manual system modifications. As previously mentioned, GLoRA supports VPT-Deep level prompts via the support tensor C, however, it does not impose any computational overhead due to its completely structural re-parameterization design. Visualizations of searched fine-tuning strat- egy for each layer. Figure 3 visually shows the distribution of trainable parameters across the four types of linear layers embodied in ViT-B. Notably, the projection layer possesses the min- imum quantity of trainable parameters span- ning across VTAB-1K categories. Generally, the MLP module hosts a substantially higher number of parameters compared to the MHSA. As anticipated, the structured group necessi- tates a greater number of parameters for adap- tation due to a pronounced domain shift rela- tive to ImageNet-1K (Deng et al., 2009b). Fig- ure 4 illustrates the layer-wise configuration of the support tensors as searched by the GLoRA algorithm. Each support tensor at every layer can potentially undergo 72 distinct adaptations across datasets. Support tensors D and E exhibit relatively low adaptation due to the prevalence of Projection FC2Layer Type 0.00 Q-K-V 0.05 FC1 0.20 0.10 Natural 0.30Number of Parameters(M) Specialized Structured 0.15 0.25 Figure 3: Distribution of GLoRA (0.86M) param- eters across layer types on VTAB-1K. Q-K-V and Projection are linear layers in MHSA module and FC1 and FC2 are linear layers in MLP module. 8 A A D 50 A A ELayer Depth (Total=12) D 20 B D D C B B B B 0 B C LoRA C A 30 C C A C C B A C C C C C D D E E E E E E E B E B E E vector A 40 B E 70Number of Occurences (Total=72) B D A A A D A 10 D D D D constant 60 B Figure 4: Layerwise configuration of support tensors in GLoRA (0.86M) on VTAB-1K dataset. none adaptations, whereas A and B demonstrate a higher number of adaptations, though without a distinguishable pattern regarding the type of adaptation. It is important to underscore that even a basic scalar can function effectively as a support tensor, enabling GLoRA to maintain superior parameter efficiency despite adapting every linear layer. 5 RELATED WORK Given the rapid expansion in model size, numerous methods for parameter-efficient fine-tuning (PEFT) have been introduced in the field of NLP to streamline the optimization of large language models (LLMs) (Liu et al., 2021a; Zhang et al.; Hu et al.; Liu et al., 2021b; Li & Liang, 2021; Lester et al., 2021; Zaken et al., 2022; Houlsby et al., 2019). The effectiveness of parameter-efficient fine- tuning has been proven in a wide range of natural language processing tasks (Fu et al., 2022; He et al., 2021). In the vision domain, with the advent growth in the size of vision models (Dehghani et al., 2023; Kolesnikov et al., 2020), methods specifically focused on image modality have also been put forward (Jie & Deng, 2022; Lian et al., 2022; Chen et al., 2022; Luo et al., 2023; Zhang et al., 2022; Jia et al., 2022; He et al., 2023). Among these methods, LoRA (Hu et al.) has proven to transfer well across modalities and tasks. This is partly due to the simplistic design strategy of LoRA which directly works over weight tensors, irrespective of model type or configuration. Additionally, unlike Adapters (Houlsby et al., 2019; Chen et al., 2022) and Prompt tuning (Jia et al., 2022), LoRA does not add any additional inference parameters or latency due to structural re-parameterization (SR) design. RepAdapter (Luo et al., 2023) and SSF (Lian et al., 2022) also propose an SR design for PEFT. However, RepAdapter is specific to model architectures and required manual designing for different layer configurations. SSF provides a simple baseline but suffers from low flexibility and capability due to adaptation limited in the activation space. FacT (Jie & Deng, 2022) further decomposes LoRA matrices for better parameter efficiency, but we argue that <1M parameter scale is fairly efficient for fine-tuning on a single GPU. Thus, due to the advantages of LoRA over other related works, it is of importance to increase the flexibility, scalability and adaptability of LoRA. 6 CONCLUSION We have presented GLoRA, a generalized parameter-efficient fine-tuning approach that has suc- cessfully demonstrated its effectiveness and adaptability in enhancing the fine-tuning and transfer learning ability for the large-scale pre-trained models. By adopting a generalized low-rank adap- tation and re-parameterization framework, GLoRA significantly reduces the number of parameters and computation required for fine-tuning, making it a more resource-efficient and practical method for real-world applications. The experiments conducted on a diverse range of tasks and datasets have substantiated the superiority of GLoRA over existing PEFT techniques, showcasing its scala- bility and adaptability. Moreover, the ablation studies have provided valuable insights into the inner workings and the relative importance of different GLoRA components. This work not only con- tributes to the improvement of the fine-tuning process for large-scale pre-trained vision or language models but also opens up new avenues for future work, including further exploration of generalized low-rank adaptation techniques, the development of hybrid approaches, and the refinement of search and optimization algorithms. These areas of research may continue to expand the accessibility and efficiency of transfer learning across a broader range of applications. 9 REFERENCES Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative com- ponents with random forests. In European Conference on Computer Vision, 2014. 6 Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin"}, {"question": " How many distinct adaptations can each support tensor potentially undergo across datasets in GLoRA?", "answer": " 72", "ref_chunk": "compared with LoRA, GLoRA enhances out-of- domain performance by as much as 100% (ImageNet-A) and 50% (ImageNet-Sketch). 4 ANALYSIS AND DISCUSSION Computational Cost. We show the final inference throughput of various PEFT methods in Table 4, computed on an NVIDIA 3090 GPU. The results highlight that GLoRA surpasses other competitive methods in performance, as it does not require any extra parameters or FLOPs during the inference stage. An additional advantage is its quicker adaptability in real-world scenarios, especially when prior or foundational models are already deployed. The weights of GLoRA can be directly loaded without necessitating any manual system modifications. As previously mentioned, GLoRA supports VPT-Deep level prompts via the support tensor C, however, it does not impose any computational overhead due to its completely structural re-parameterization design. Visualizations of searched fine-tuning strat- egy for each layer. Figure 3 visually shows the distribution of trainable parameters across the four types of linear layers embodied in ViT-B. Notably, the projection layer possesses the min- imum quantity of trainable parameters span- ning across VTAB-1K categories. Generally, the MLP module hosts a substantially higher number of parameters compared to the MHSA. As anticipated, the structured group necessi- tates a greater number of parameters for adap- tation due to a pronounced domain shift rela- tive to ImageNet-1K (Deng et al., 2009b). Fig- ure 4 illustrates the layer-wise configuration of the support tensors as searched by the GLoRA algorithm. Each support tensor at every layer can potentially undergo 72 distinct adaptations across datasets. Support tensors D and E exhibit relatively low adaptation due to the prevalence of Projection FC2Layer Type 0.00 Q-K-V 0.05 FC1 0.20 0.10 Natural 0.30Number of Parameters(M) Specialized Structured 0.15 0.25 Figure 3: Distribution of GLoRA (0.86M) param- eters across layer types on VTAB-1K. Q-K-V and Projection are linear layers in MHSA module and FC1 and FC2 are linear layers in MLP module. 8 A A D 50 A A ELayer Depth (Total=12) D 20 B D D C B B B B 0 B C LoRA C A 30 C C A C C B A C C C C C D D E E E E E E E B E B E E vector A 40 B E 70Number of Occurences (Total=72) B D A A A D A 10 D D D D constant 60 B Figure 4: Layerwise configuration of support tensors in GLoRA (0.86M) on VTAB-1K dataset. none adaptations, whereas A and B demonstrate a higher number of adaptations, though without a distinguishable pattern regarding the type of adaptation. It is important to underscore that even a basic scalar can function effectively as a support tensor, enabling GLoRA to maintain superior parameter efficiency despite adapting every linear layer. 5 RELATED WORK Given the rapid expansion in model size, numerous methods for parameter-efficient fine-tuning (PEFT) have been introduced in the field of NLP to streamline the optimization of large language models (LLMs) (Liu et al., 2021a; Zhang et al.; Hu et al.; Liu et al., 2021b; Li & Liang, 2021; Lester et al., 2021; Zaken et al., 2022; Houlsby et al., 2019). The effectiveness of parameter-efficient fine- tuning has been proven in a wide range of natural language processing tasks (Fu et al., 2022; He et al., 2021). In the vision domain, with the advent growth in the size of vision models (Dehghani et al., 2023; Kolesnikov et al., 2020), methods specifically focused on image modality have also been put forward (Jie & Deng, 2022; Lian et al., 2022; Chen et al., 2022; Luo et al., 2023; Zhang et al., 2022; Jia et al., 2022; He et al., 2023). Among these methods, LoRA (Hu et al.) has proven to transfer well across modalities and tasks. This is partly due to the simplistic design strategy of LoRA which directly works over weight tensors, irrespective of model type or configuration. Additionally, unlike Adapters (Houlsby et al., 2019; Chen et al., 2022) and Prompt tuning (Jia et al., 2022), LoRA does not add any additional inference parameters or latency due to structural re-parameterization (SR) design. RepAdapter (Luo et al., 2023) and SSF (Lian et al., 2022) also propose an SR design for PEFT. However, RepAdapter is specific to model architectures and required manual designing for different layer configurations. SSF provides a simple baseline but suffers from low flexibility and capability due to adaptation limited in the activation space. FacT (Jie & Deng, 2022) further decomposes LoRA matrices for better parameter efficiency, but we argue that <1M parameter scale is fairly efficient for fine-tuning on a single GPU. Thus, due to the advantages of LoRA over other related works, it is of importance to increase the flexibility, scalability and adaptability of LoRA. 6 CONCLUSION We have presented GLoRA, a generalized parameter-efficient fine-tuning approach that has suc- cessfully demonstrated its effectiveness and adaptability in enhancing the fine-tuning and transfer learning ability for the large-scale pre-trained models. By adopting a generalized low-rank adap- tation and re-parameterization framework, GLoRA significantly reduces the number of parameters and computation required for fine-tuning, making it a more resource-efficient and practical method for real-world applications. The experiments conducted on a diverse range of tasks and datasets have substantiated the superiority of GLoRA over existing PEFT techniques, showcasing its scala- bility and adaptability. Moreover, the ablation studies have provided valuable insights into the inner workings and the relative importance of different GLoRA components. This work not only con- tributes to the improvement of the fine-tuning process for large-scale pre-trained vision or language models but also opens up new avenues for future work, including further exploration of generalized low-rank adaptation techniques, the development of hybrid approaches, and the refinement of search and optimization algorithms. These areas of research may continue to expand the accessibility and efficiency of transfer learning across a broader range of applications. 9 REFERENCES Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative com- ponents with random forests. In European Conference on Computer Vision, 2014. 6 Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin"}, {"question": " What does the MLP module host a substantially higher number of parameters compared to?", "answer": " MHSA", "ref_chunk": "compared with LoRA, GLoRA enhances out-of- domain performance by as much as 100% (ImageNet-A) and 50% (ImageNet-Sketch). 4 ANALYSIS AND DISCUSSION Computational Cost. We show the final inference throughput of various PEFT methods in Table 4, computed on an NVIDIA 3090 GPU. The results highlight that GLoRA surpasses other competitive methods in performance, as it does not require any extra parameters or FLOPs during the inference stage. An additional advantage is its quicker adaptability in real-world scenarios, especially when prior or foundational models are already deployed. The weights of GLoRA can be directly loaded without necessitating any manual system modifications. As previously mentioned, GLoRA supports VPT-Deep level prompts via the support tensor C, however, it does not impose any computational overhead due to its completely structural re-parameterization design. Visualizations of searched fine-tuning strat- egy for each layer. Figure 3 visually shows the distribution of trainable parameters across the four types of linear layers embodied in ViT-B. Notably, the projection layer possesses the min- imum quantity of trainable parameters span- ning across VTAB-1K categories. Generally, the MLP module hosts a substantially higher number of parameters compared to the MHSA. As anticipated, the structured group necessi- tates a greater number of parameters for adap- tation due to a pronounced domain shift rela- tive to ImageNet-1K (Deng et al., 2009b). Fig- ure 4 illustrates the layer-wise configuration of the support tensors as searched by the GLoRA algorithm. Each support tensor at every layer can potentially undergo 72 distinct adaptations across datasets. Support tensors D and E exhibit relatively low adaptation due to the prevalence of Projection FC2Layer Type 0.00 Q-K-V 0.05 FC1 0.20 0.10 Natural 0.30Number of Parameters(M) Specialized Structured 0.15 0.25 Figure 3: Distribution of GLoRA (0.86M) param- eters across layer types on VTAB-1K. Q-K-V and Projection are linear layers in MHSA module and FC1 and FC2 are linear layers in MLP module. 8 A A D 50 A A ELayer Depth (Total=12) D 20 B D D C B B B B 0 B C LoRA C A 30 C C A C C B A C C C C C D D E E E E E E E B E B E E vector A 40 B E 70Number of Occurences (Total=72) B D A A A D A 10 D D D D constant 60 B Figure 4: Layerwise configuration of support tensors in GLoRA (0.86M) on VTAB-1K dataset. none adaptations, whereas A and B demonstrate a higher number of adaptations, though without a distinguishable pattern regarding the type of adaptation. It is important to underscore that even a basic scalar can function effectively as a support tensor, enabling GLoRA to maintain superior parameter efficiency despite adapting every linear layer. 5 RELATED WORK Given the rapid expansion in model size, numerous methods for parameter-efficient fine-tuning (PEFT) have been introduced in the field of NLP to streamline the optimization of large language models (LLMs) (Liu et al., 2021a; Zhang et al.; Hu et al.; Liu et al., 2021b; Li & Liang, 2021; Lester et al., 2021; Zaken et al., 2022; Houlsby et al., 2019). The effectiveness of parameter-efficient fine- tuning has been proven in a wide range of natural language processing tasks (Fu et al., 2022; He et al., 2021). In the vision domain, with the advent growth in the size of vision models (Dehghani et al., 2023; Kolesnikov et al., 2020), methods specifically focused on image modality have also been put forward (Jie & Deng, 2022; Lian et al., 2022; Chen et al., 2022; Luo et al., 2023; Zhang et al., 2022; Jia et al., 2022; He et al., 2023). Among these methods, LoRA (Hu et al.) has proven to transfer well across modalities and tasks. This is partly due to the simplistic design strategy of LoRA which directly works over weight tensors, irrespective of model type or configuration. Additionally, unlike Adapters (Houlsby et al., 2019; Chen et al., 2022) and Prompt tuning (Jia et al., 2022), LoRA does not add any additional inference parameters or latency due to structural re-parameterization (SR) design. RepAdapter (Luo et al., 2023) and SSF (Lian et al., 2022) also propose an SR design for PEFT. However, RepAdapter is specific to model architectures and required manual designing for different layer configurations. SSF provides a simple baseline but suffers from low flexibility and capability due to adaptation limited in the activation space. FacT (Jie & Deng, 2022) further decomposes LoRA matrices for better parameter efficiency, but we argue that <1M parameter scale is fairly efficient for fine-tuning on a single GPU. Thus, due to the advantages of LoRA over other related works, it is of importance to increase the flexibility, scalability and adaptability of LoRA. 6 CONCLUSION We have presented GLoRA, a generalized parameter-efficient fine-tuning approach that has suc- cessfully demonstrated its effectiveness and adaptability in enhancing the fine-tuning and transfer learning ability for the large-scale pre-trained models. By adopting a generalized low-rank adap- tation and re-parameterization framework, GLoRA significantly reduces the number of parameters and computation required for fine-tuning, making it a more resource-efficient and practical method for real-world applications. The experiments conducted on a diverse range of tasks and datasets have substantiated the superiority of GLoRA over existing PEFT techniques, showcasing its scala- bility and adaptability. Moreover, the ablation studies have provided valuable insights into the inner workings and the relative importance of different GLoRA components. This work not only con- tributes to the improvement of the fine-tuning process for large-scale pre-trained vision or language models but also opens up new avenues for future work, including further exploration of generalized low-rank adaptation techniques, the development of hybrid approaches, and the refinement of search and optimization algorithms. These areas of research may continue to expand the accessibility and efficiency of transfer learning across a broader range of applications. 9 REFERENCES Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative com- ponents with random forests. In European Conference on Computer Vision, 2014. 6 Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin"}, {"question": " What is important to underscore regarding the support tensors in GLoRA?", "answer": " Even a basic scalar can function effectively as a support tensor", "ref_chunk": "compared with LoRA, GLoRA enhances out-of- domain performance by as much as 100% (ImageNet-A) and 50% (ImageNet-Sketch). 4 ANALYSIS AND DISCUSSION Computational Cost. We show the final inference throughput of various PEFT methods in Table 4, computed on an NVIDIA 3090 GPU. The results highlight that GLoRA surpasses other competitive methods in performance, as it does not require any extra parameters or FLOPs during the inference stage. An additional advantage is its quicker adaptability in real-world scenarios, especially when prior or foundational models are already deployed. The weights of GLoRA can be directly loaded without necessitating any manual system modifications. As previously mentioned, GLoRA supports VPT-Deep level prompts via the support tensor C, however, it does not impose any computational overhead due to its completely structural re-parameterization design. Visualizations of searched fine-tuning strat- egy for each layer. Figure 3 visually shows the distribution of trainable parameters across the four types of linear layers embodied in ViT-B. Notably, the projection layer possesses the min- imum quantity of trainable parameters span- ning across VTAB-1K categories. Generally, the MLP module hosts a substantially higher number of parameters compared to the MHSA. As anticipated, the structured group necessi- tates a greater number of parameters for adap- tation due to a pronounced domain shift rela- tive to ImageNet-1K (Deng et al., 2009b). Fig- ure 4 illustrates the layer-wise configuration of the support tensors as searched by the GLoRA algorithm. Each support tensor at every layer can potentially undergo 72 distinct adaptations across datasets. Support tensors D and E exhibit relatively low adaptation due to the prevalence of Projection FC2Layer Type 0.00 Q-K-V 0.05 FC1 0.20 0.10 Natural 0.30Number of Parameters(M) Specialized Structured 0.15 0.25 Figure 3: Distribution of GLoRA (0.86M) param- eters across layer types on VTAB-1K. Q-K-V and Projection are linear layers in MHSA module and FC1 and FC2 are linear layers in MLP module. 8 A A D 50 A A ELayer Depth (Total=12) D 20 B D D C B B B B 0 B C LoRA C A 30 C C A C C B A C C C C C D D E E E E E E E B E B E E vector A 40 B E 70Number of Occurences (Total=72) B D A A A D A 10 D D D D constant 60 B Figure 4: Layerwise configuration of support tensors in GLoRA (0.86M) on VTAB-1K dataset. none adaptations, whereas A and B demonstrate a higher number of adaptations, though without a distinguishable pattern regarding the type of adaptation. It is important to underscore that even a basic scalar can function effectively as a support tensor, enabling GLoRA to maintain superior parameter efficiency despite adapting every linear layer. 5 RELATED WORK Given the rapid expansion in model size, numerous methods for parameter-efficient fine-tuning (PEFT) have been introduced in the field of NLP to streamline the optimization of large language models (LLMs) (Liu et al., 2021a; Zhang et al.; Hu et al.; Liu et al., 2021b; Li & Liang, 2021; Lester et al., 2021; Zaken et al., 2022; Houlsby et al., 2019). The effectiveness of parameter-efficient fine- tuning has been proven in a wide range of natural language processing tasks (Fu et al., 2022; He et al., 2021). In the vision domain, with the advent growth in the size of vision models (Dehghani et al., 2023; Kolesnikov et al., 2020), methods specifically focused on image modality have also been put forward (Jie & Deng, 2022; Lian et al., 2022; Chen et al., 2022; Luo et al., 2023; Zhang et al., 2022; Jia et al., 2022; He et al., 2023). Among these methods, LoRA (Hu et al.) has proven to transfer well across modalities and tasks. This is partly due to the simplistic design strategy of LoRA which directly works over weight tensors, irrespective of model type or configuration. Additionally, unlike Adapters (Houlsby et al., 2019; Chen et al., 2022) and Prompt tuning (Jia et al., 2022), LoRA does not add any additional inference parameters or latency due to structural re-parameterization (SR) design. RepAdapter (Luo et al., 2023) and SSF (Lian et al., 2022) also propose an SR design for PEFT. However, RepAdapter is specific to model architectures and required manual designing for different layer configurations. SSF provides a simple baseline but suffers from low flexibility and capability due to adaptation limited in the activation space. FacT (Jie & Deng, 2022) further decomposes LoRA matrices for better parameter efficiency, but we argue that <1M parameter scale is fairly efficient for fine-tuning on a single GPU. Thus, due to the advantages of LoRA over other related works, it is of importance to increase the flexibility, scalability and adaptability of LoRA. 6 CONCLUSION We have presented GLoRA, a generalized parameter-efficient fine-tuning approach that has suc- cessfully demonstrated its effectiveness and adaptability in enhancing the fine-tuning and transfer learning ability for the large-scale pre-trained models. By adopting a generalized low-rank adap- tation and re-parameterization framework, GLoRA significantly reduces the number of parameters and computation required for fine-tuning, making it a more resource-efficient and practical method for real-world applications. The experiments conducted on a diverse range of tasks and datasets have substantiated the superiority of GLoRA over existing PEFT techniques, showcasing its scala- bility and adaptability. Moreover, the ablation studies have provided valuable insights into the inner workings and the relative importance of different GLoRA components. This work not only con- tributes to the improvement of the fine-tuning process for large-scale pre-trained vision or language models but also opens up new avenues for future work, including further exploration of generalized low-rank adaptation techniques, the development of hybrid approaches, and the refinement of search and optimization algorithms. These areas of research may continue to expand the accessibility and efficiency of transfer learning across a broader range of applications. 9 REFERENCES Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative com- ponents with random forests. In European Conference on Computer Vision, 2014. 6 Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin"}, {"question": " In what field have numerous methods for parameter-efficient fine-tuning (PEFT) been introduced?", "answer": " NLP (Natural Language Processing)", "ref_chunk": "compared with LoRA, GLoRA enhances out-of- domain performance by as much as 100% (ImageNet-A) and 50% (ImageNet-Sketch). 4 ANALYSIS AND DISCUSSION Computational Cost. We show the final inference throughput of various PEFT methods in Table 4, computed on an NVIDIA 3090 GPU. The results highlight that GLoRA surpasses other competitive methods in performance, as it does not require any extra parameters or FLOPs during the inference stage. An additional advantage is its quicker adaptability in real-world scenarios, especially when prior or foundational models are already deployed. The weights of GLoRA can be directly loaded without necessitating any manual system modifications. As previously mentioned, GLoRA supports VPT-Deep level prompts via the support tensor C, however, it does not impose any computational overhead due to its completely structural re-parameterization design. Visualizations of searched fine-tuning strat- egy for each layer. Figure 3 visually shows the distribution of trainable parameters across the four types of linear layers embodied in ViT-B. Notably, the projection layer possesses the min- imum quantity of trainable parameters span- ning across VTAB-1K categories. Generally, the MLP module hosts a substantially higher number of parameters compared to the MHSA. As anticipated, the structured group necessi- tates a greater number of parameters for adap- tation due to a pronounced domain shift rela- tive to ImageNet-1K (Deng et al., 2009b). Fig- ure 4 illustrates the layer-wise configuration of the support tensors as searched by the GLoRA algorithm. Each support tensor at every layer can potentially undergo 72 distinct adaptations across datasets. Support tensors D and E exhibit relatively low adaptation due to the prevalence of Projection FC2Layer Type 0.00 Q-K-V 0.05 FC1 0.20 0.10 Natural 0.30Number of Parameters(M) Specialized Structured 0.15 0.25 Figure 3: Distribution of GLoRA (0.86M) param- eters across layer types on VTAB-1K. Q-K-V and Projection are linear layers in MHSA module and FC1 and FC2 are linear layers in MLP module. 8 A A D 50 A A ELayer Depth (Total=12) D 20 B D D C B B B B 0 B C LoRA C A 30 C C A C C B A C C C C C D D E E E E E E E B E B E E vector A 40 B E 70Number of Occurences (Total=72) B D A A A D A 10 D D D D constant 60 B Figure 4: Layerwise configuration of support tensors in GLoRA (0.86M) on VTAB-1K dataset. none adaptations, whereas A and B demonstrate a higher number of adaptations, though without a distinguishable pattern regarding the type of adaptation. It is important to underscore that even a basic scalar can function effectively as a support tensor, enabling GLoRA to maintain superior parameter efficiency despite adapting every linear layer. 5 RELATED WORK Given the rapid expansion in model size, numerous methods for parameter-efficient fine-tuning (PEFT) have been introduced in the field of NLP to streamline the optimization of large language models (LLMs) (Liu et al., 2021a; Zhang et al.; Hu et al.; Liu et al., 2021b; Li & Liang, 2021; Lester et al., 2021; Zaken et al., 2022; Houlsby et al., 2019). The effectiveness of parameter-efficient fine- tuning has been proven in a wide range of natural language processing tasks (Fu et al., 2022; He et al., 2021). In the vision domain, with the advent growth in the size of vision models (Dehghani et al., 2023; Kolesnikov et al., 2020), methods specifically focused on image modality have also been put forward (Jie & Deng, 2022; Lian et al., 2022; Chen et al., 2022; Luo et al., 2023; Zhang et al., 2022; Jia et al., 2022; He et al., 2023). Among these methods, LoRA (Hu et al.) has proven to transfer well across modalities and tasks. This is partly due to the simplistic design strategy of LoRA which directly works over weight tensors, irrespective of model type or configuration. Additionally, unlike Adapters (Houlsby et al., 2019; Chen et al., 2022) and Prompt tuning (Jia et al., 2022), LoRA does not add any additional inference parameters or latency due to structural re-parameterization (SR) design. RepAdapter (Luo et al., 2023) and SSF (Lian et al., 2022) also propose an SR design for PEFT. However, RepAdapter is specific to model architectures and required manual designing for different layer configurations. SSF provides a simple baseline but suffers from low flexibility and capability due to adaptation limited in the activation space. FacT (Jie & Deng, 2022) further decomposes LoRA matrices for better parameter efficiency, but we argue that <1M parameter scale is fairly efficient for fine-tuning on a single GPU. Thus, due to the advantages of LoRA over other related works, it is of importance to increase the flexibility, scalability and adaptability of LoRA. 6 CONCLUSION We have presented GLoRA, a generalized parameter-efficient fine-tuning approach that has suc- cessfully demonstrated its effectiveness and adaptability in enhancing the fine-tuning and transfer learning ability for the large-scale pre-trained models. By adopting a generalized low-rank adap- tation and re-parameterization framework, GLoRA significantly reduces the number of parameters and computation required for fine-tuning, making it a more resource-efficient and practical method for real-world applications. The experiments conducted on a diverse range of tasks and datasets have substantiated the superiority of GLoRA over existing PEFT techniques, showcasing its scala- bility and adaptability. Moreover, the ablation studies have provided valuable insights into the inner workings and the relative importance of different GLoRA components. This work not only con- tributes to the improvement of the fine-tuning process for large-scale pre-trained vision or language models but also opens up new avenues for future work, including further exploration of generalized low-rank adaptation techniques, the development of hybrid approaches, and the refinement of search and optimization algorithms. These areas of research may continue to expand the accessibility and efficiency of transfer learning across a broader range of applications. 9 REFERENCES Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative com- ponents with random forests. In European Conference on Computer Vision, 2014. 6 Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin"}, {"question": " According to the text, which method has proven to transfer well across modalities and tasks?", "answer": " LoRA", "ref_chunk": "compared with LoRA, GLoRA enhances out-of- domain performance by as much as 100% (ImageNet-A) and 50% (ImageNet-Sketch). 4 ANALYSIS AND DISCUSSION Computational Cost. We show the final inference throughput of various PEFT methods in Table 4, computed on an NVIDIA 3090 GPU. The results highlight that GLoRA surpasses other competitive methods in performance, as it does not require any extra parameters or FLOPs during the inference stage. An additional advantage is its quicker adaptability in real-world scenarios, especially when prior or foundational models are already deployed. The weights of GLoRA can be directly loaded without necessitating any manual system modifications. As previously mentioned, GLoRA supports VPT-Deep level prompts via the support tensor C, however, it does not impose any computational overhead due to its completely structural re-parameterization design. Visualizations of searched fine-tuning strat- egy for each layer. Figure 3 visually shows the distribution of trainable parameters across the four types of linear layers embodied in ViT-B. Notably, the projection layer possesses the min- imum quantity of trainable parameters span- ning across VTAB-1K categories. Generally, the MLP module hosts a substantially higher number of parameters compared to the MHSA. As anticipated, the structured group necessi- tates a greater number of parameters for adap- tation due to a pronounced domain shift rela- tive to ImageNet-1K (Deng et al., 2009b). Fig- ure 4 illustrates the layer-wise configuration of the support tensors as searched by the GLoRA algorithm. Each support tensor at every layer can potentially undergo 72 distinct adaptations across datasets. Support tensors D and E exhibit relatively low adaptation due to the prevalence of Projection FC2Layer Type 0.00 Q-K-V 0.05 FC1 0.20 0.10 Natural 0.30Number of Parameters(M) Specialized Structured 0.15 0.25 Figure 3: Distribution of GLoRA (0.86M) param- eters across layer types on VTAB-1K. Q-K-V and Projection are linear layers in MHSA module and FC1 and FC2 are linear layers in MLP module. 8 A A D 50 A A ELayer Depth (Total=12) D 20 B D D C B B B B 0 B C LoRA C A 30 C C A C C B A C C C C C D D E E E E E E E B E B E E vector A 40 B E 70Number of Occurences (Total=72) B D A A A D A 10 D D D D constant 60 B Figure 4: Layerwise configuration of support tensors in GLoRA (0.86M) on VTAB-1K dataset. none adaptations, whereas A and B demonstrate a higher number of adaptations, though without a distinguishable pattern regarding the type of adaptation. It is important to underscore that even a basic scalar can function effectively as a support tensor, enabling GLoRA to maintain superior parameter efficiency despite adapting every linear layer. 5 RELATED WORK Given the rapid expansion in model size, numerous methods for parameter-efficient fine-tuning (PEFT) have been introduced in the field of NLP to streamline the optimization of large language models (LLMs) (Liu et al., 2021a; Zhang et al.; Hu et al.; Liu et al., 2021b; Li & Liang, 2021; Lester et al., 2021; Zaken et al., 2022; Houlsby et al., 2019). The effectiveness of parameter-efficient fine- tuning has been proven in a wide range of natural language processing tasks (Fu et al., 2022; He et al., 2021). In the vision domain, with the advent growth in the size of vision models (Dehghani et al., 2023; Kolesnikov et al., 2020), methods specifically focused on image modality have also been put forward (Jie & Deng, 2022; Lian et al., 2022; Chen et al., 2022; Luo et al., 2023; Zhang et al., 2022; Jia et al., 2022; He et al., 2023). Among these methods, LoRA (Hu et al.) has proven to transfer well across modalities and tasks. This is partly due to the simplistic design strategy of LoRA which directly works over weight tensors, irrespective of model type or configuration. Additionally, unlike Adapters (Houlsby et al., 2019; Chen et al., 2022) and Prompt tuning (Jia et al., 2022), LoRA does not add any additional inference parameters or latency due to structural re-parameterization (SR) design. RepAdapter (Luo et al., 2023) and SSF (Lian et al., 2022) also propose an SR design for PEFT. However, RepAdapter is specific to model architectures and required manual designing for different layer configurations. SSF provides a simple baseline but suffers from low flexibility and capability due to adaptation limited in the activation space. FacT (Jie & Deng, 2022) further decomposes LoRA matrices for better parameter efficiency, but we argue that <1M parameter scale is fairly efficient for fine-tuning on a single GPU. Thus, due to the advantages of LoRA over other related works, it is of importance to increase the flexibility, scalability and adaptability of LoRA. 6 CONCLUSION We have presented GLoRA, a generalized parameter-efficient fine-tuning approach that has suc- cessfully demonstrated its effectiveness and adaptability in enhancing the fine-tuning and transfer learning ability for the large-scale pre-trained models. By adopting a generalized low-rank adap- tation and re-parameterization framework, GLoRA significantly reduces the number of parameters and computation required for fine-tuning, making it a more resource-efficient and practical method for real-world applications. The experiments conducted on a diverse range of tasks and datasets have substantiated the superiority of GLoRA over existing PEFT techniques, showcasing its scala- bility and adaptability. Moreover, the ablation studies have provided valuable insights into the inner workings and the relative importance of different GLoRA components. This work not only con- tributes to the improvement of the fine-tuning process for large-scale pre-trained vision or language models but also opens up new avenues for future work, including further exploration of generalized low-rank adaptation techniques, the development of hybrid approaches, and the refinement of search and optimization algorithms. These areas of research may continue to expand the accessibility and efficiency of transfer learning across a broader range of applications. 9 REFERENCES Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative com- ponents with random forests. In European Conference on Computer Vision, 2014. 6 Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin"}, {"question": " What advantage does LoRA have over Adapters and Prompt tuning in terms of inference parameters?", "answer": " LoRA does not add any additional inference parameters or latency due to structural re-parameterization design", "ref_chunk": "compared with LoRA, GLoRA enhances out-of- domain performance by as much as 100% (ImageNet-A) and 50% (ImageNet-Sketch). 4 ANALYSIS AND DISCUSSION Computational Cost. We show the final inference throughput of various PEFT methods in Table 4, computed on an NVIDIA 3090 GPU. The results highlight that GLoRA surpasses other competitive methods in performance, as it does not require any extra parameters or FLOPs during the inference stage. An additional advantage is its quicker adaptability in real-world scenarios, especially when prior or foundational models are already deployed. The weights of GLoRA can be directly loaded without necessitating any manual system modifications. As previously mentioned, GLoRA supports VPT-Deep level prompts via the support tensor C, however, it does not impose any computational overhead due to its completely structural re-parameterization design. Visualizations of searched fine-tuning strat- egy for each layer. Figure 3 visually shows the distribution of trainable parameters across the four types of linear layers embodied in ViT-B. Notably, the projection layer possesses the min- imum quantity of trainable parameters span- ning across VTAB-1K categories. Generally, the MLP module hosts a substantially higher number of parameters compared to the MHSA. As anticipated, the structured group necessi- tates a greater number of parameters for adap- tation due to a pronounced domain shift rela- tive to ImageNet-1K (Deng et al., 2009b). Fig- ure 4 illustrates the layer-wise configuration of the support tensors as searched by the GLoRA algorithm. Each support tensor at every layer can potentially undergo 72 distinct adaptations across datasets. Support tensors D and E exhibit relatively low adaptation due to the prevalence of Projection FC2Layer Type 0.00 Q-K-V 0.05 FC1 0.20 0.10 Natural 0.30Number of Parameters(M) Specialized Structured 0.15 0.25 Figure 3: Distribution of GLoRA (0.86M) param- eters across layer types on VTAB-1K. Q-K-V and Projection are linear layers in MHSA module and FC1 and FC2 are linear layers in MLP module. 8 A A D 50 A A ELayer Depth (Total=12) D 20 B D D C B B B B 0 B C LoRA C A 30 C C A C C B A C C C C C D D E E E E E E E B E B E E vector A 40 B E 70Number of Occurences (Total=72) B D A A A D A 10 D D D D constant 60 B Figure 4: Layerwise configuration of support tensors in GLoRA (0.86M) on VTAB-1K dataset. none adaptations, whereas A and B demonstrate a higher number of adaptations, though without a distinguishable pattern regarding the type of adaptation. It is important to underscore that even a basic scalar can function effectively as a support tensor, enabling GLoRA to maintain superior parameter efficiency despite adapting every linear layer. 5 RELATED WORK Given the rapid expansion in model size, numerous methods for parameter-efficient fine-tuning (PEFT) have been introduced in the field of NLP to streamline the optimization of large language models (LLMs) (Liu et al., 2021a; Zhang et al.; Hu et al.; Liu et al., 2021b; Li & Liang, 2021; Lester et al., 2021; Zaken et al., 2022; Houlsby et al., 2019). The effectiveness of parameter-efficient fine- tuning has been proven in a wide range of natural language processing tasks (Fu et al., 2022; He et al., 2021). In the vision domain, with the advent growth in the size of vision models (Dehghani et al., 2023; Kolesnikov et al., 2020), methods specifically focused on image modality have also been put forward (Jie & Deng, 2022; Lian et al., 2022; Chen et al., 2022; Luo et al., 2023; Zhang et al., 2022; Jia et al., 2022; He et al., 2023). Among these methods, LoRA (Hu et al.) has proven to transfer well across modalities and tasks. This is partly due to the simplistic design strategy of LoRA which directly works over weight tensors, irrespective of model type or configuration. Additionally, unlike Adapters (Houlsby et al., 2019; Chen et al., 2022) and Prompt tuning (Jia et al., 2022), LoRA does not add any additional inference parameters or latency due to structural re-parameterization (SR) design. RepAdapter (Luo et al., 2023) and SSF (Lian et al., 2022) also propose an SR design for PEFT. However, RepAdapter is specific to model architectures and required manual designing for different layer configurations. SSF provides a simple baseline but suffers from low flexibility and capability due to adaptation limited in the activation space. FacT (Jie & Deng, 2022) further decomposes LoRA matrices for better parameter efficiency, but we argue that <1M parameter scale is fairly efficient for fine-tuning on a single GPU. Thus, due to the advantages of LoRA over other related works, it is of importance to increase the flexibility, scalability and adaptability of LoRA. 6 CONCLUSION We have presented GLoRA, a generalized parameter-efficient fine-tuning approach that has suc- cessfully demonstrated its effectiveness and adaptability in enhancing the fine-tuning and transfer learning ability for the large-scale pre-trained models. By adopting a generalized low-rank adap- tation and re-parameterization framework, GLoRA significantly reduces the number of parameters and computation required for fine-tuning, making it a more resource-efficient and practical method for real-world applications. The experiments conducted on a diverse range of tasks and datasets have substantiated the superiority of GLoRA over existing PEFT techniques, showcasing its scala- bility and adaptability. Moreover, the ablation studies have provided valuable insights into the inner workings and the relative importance of different GLoRA components. This work not only con- tributes to the improvement of the fine-tuning process for large-scale pre-trained vision or language models but also opens up new avenues for future work, including further exploration of generalized low-rank adaptation techniques, the development of hybrid approaches, and the refinement of search and optimization algorithms. These areas of research may continue to expand the accessibility and efficiency of transfer learning across a broader range of applications. 9 REFERENCES Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative com- ponents with random forests. In European Conference on Computer Vision, 2014. 6 Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin"}, {"question": " What is the main focus of GLoRA as mentioned in the conclusion of the text?", "answer": " To enhance the fine-tuning and transfer learning ability for large-scale pre-trained models", "ref_chunk": "compared with LoRA, GLoRA enhances out-of- domain performance by as much as 100% (ImageNet-A) and 50% (ImageNet-Sketch). 4 ANALYSIS AND DISCUSSION Computational Cost. We show the final inference throughput of various PEFT methods in Table 4, computed on an NVIDIA 3090 GPU. The results highlight that GLoRA surpasses other competitive methods in performance, as it does not require any extra parameters or FLOPs during the inference stage. An additional advantage is its quicker adaptability in real-world scenarios, especially when prior or foundational models are already deployed. The weights of GLoRA can be directly loaded without necessitating any manual system modifications. As previously mentioned, GLoRA supports VPT-Deep level prompts via the support tensor C, however, it does not impose any computational overhead due to its completely structural re-parameterization design. Visualizations of searched fine-tuning strat- egy for each layer. Figure 3 visually shows the distribution of trainable parameters across the four types of linear layers embodied in ViT-B. Notably, the projection layer possesses the min- imum quantity of trainable parameters span- ning across VTAB-1K categories. Generally, the MLP module hosts a substantially higher number of parameters compared to the MHSA. As anticipated, the structured group necessi- tates a greater number of parameters for adap- tation due to a pronounced domain shift rela- tive to ImageNet-1K (Deng et al., 2009b). Fig- ure 4 illustrates the layer-wise configuration of the support tensors as searched by the GLoRA algorithm. Each support tensor at every layer can potentially undergo 72 distinct adaptations across datasets. Support tensors D and E exhibit relatively low adaptation due to the prevalence of Projection FC2Layer Type 0.00 Q-K-V 0.05 FC1 0.20 0.10 Natural 0.30Number of Parameters(M) Specialized Structured 0.15 0.25 Figure 3: Distribution of GLoRA (0.86M) param- eters across layer types on VTAB-1K. Q-K-V and Projection are linear layers in MHSA module and FC1 and FC2 are linear layers in MLP module. 8 A A D 50 A A ELayer Depth (Total=12) D 20 B D D C B B B B 0 B C LoRA C A 30 C C A C C B A C C C C C D D E E E E E E E B E B E E vector A 40 B E 70Number of Occurences (Total=72) B D A A A D A 10 D D D D constant 60 B Figure 4: Layerwise configuration of support tensors in GLoRA (0.86M) on VTAB-1K dataset. none adaptations, whereas A and B demonstrate a higher number of adaptations, though without a distinguishable pattern regarding the type of adaptation. It is important to underscore that even a basic scalar can function effectively as a support tensor, enabling GLoRA to maintain superior parameter efficiency despite adapting every linear layer. 5 RELATED WORK Given the rapid expansion in model size, numerous methods for parameter-efficient fine-tuning (PEFT) have been introduced in the field of NLP to streamline the optimization of large language models (LLMs) (Liu et al., 2021a; Zhang et al.; Hu et al.; Liu et al., 2021b; Li & Liang, 2021; Lester et al., 2021; Zaken et al., 2022; Houlsby et al., 2019). The effectiveness of parameter-efficient fine- tuning has been proven in a wide range of natural language processing tasks (Fu et al., 2022; He et al., 2021). In the vision domain, with the advent growth in the size of vision models (Dehghani et al., 2023; Kolesnikov et al., 2020), methods specifically focused on image modality have also been put forward (Jie & Deng, 2022; Lian et al., 2022; Chen et al., 2022; Luo et al., 2023; Zhang et al., 2022; Jia et al., 2022; He et al., 2023). Among these methods, LoRA (Hu et al.) has proven to transfer well across modalities and tasks. This is partly due to the simplistic design strategy of LoRA which directly works over weight tensors, irrespective of model type or configuration. Additionally, unlike Adapters (Houlsby et al., 2019; Chen et al., 2022) and Prompt tuning (Jia et al., 2022), LoRA does not add any additional inference parameters or latency due to structural re-parameterization (SR) design. RepAdapter (Luo et al., 2023) and SSF (Lian et al., 2022) also propose an SR design for PEFT. However, RepAdapter is specific to model architectures and required manual designing for different layer configurations. SSF provides a simple baseline but suffers from low flexibility and capability due to adaptation limited in the activation space. FacT (Jie & Deng, 2022) further decomposes LoRA matrices for better parameter efficiency, but we argue that <1M parameter scale is fairly efficient for fine-tuning on a single GPU. Thus, due to the advantages of LoRA over other related works, it is of importance to increase the flexibility, scalability and adaptability of LoRA. 6 CONCLUSION We have presented GLoRA, a generalized parameter-efficient fine-tuning approach that has suc- cessfully demonstrated its effectiveness and adaptability in enhancing the fine-tuning and transfer learning ability for the large-scale pre-trained models. By adopting a generalized low-rank adap- tation and re-parameterization framework, GLoRA significantly reduces the number of parameters and computation required for fine-tuning, making it a more resource-efficient and practical method for real-world applications. The experiments conducted on a diverse range of tasks and datasets have substantiated the superiority of GLoRA over existing PEFT techniques, showcasing its scala- bility and adaptability. Moreover, the ablation studies have provided valuable insights into the inner workings and the relative importance of different GLoRA components. This work not only con- tributes to the improvement of the fine-tuning process for large-scale pre-trained vision or language models but also opens up new avenues for future work, including further exploration of generalized low-rank adaptation techniques, the development of hybrid approaches, and the refinement of search and optimization algorithms. These areas of research may continue to expand the accessibility and efficiency of transfer learning across a broader range of applications. 9 REFERENCES Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative com- ponents with random forests. In European Conference on Computer Vision, 2014. 6 Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin"}], "doc_text": "compared with LoRA, GLoRA enhances out-of- domain performance by as much as 100% (ImageNet-A) and 50% (ImageNet-Sketch). 4 ANALYSIS AND DISCUSSION Computational Cost. We show the final inference throughput of various PEFT methods in Table 4, computed on an NVIDIA 3090 GPU. The results highlight that GLoRA surpasses other competitive methods in performance, as it does not require any extra parameters or FLOPs during the inference stage. An additional advantage is its quicker adaptability in real-world scenarios, especially when prior or foundational models are already deployed. The weights of GLoRA can be directly loaded without necessitating any manual system modifications. As previously mentioned, GLoRA supports VPT-Deep level prompts via the support tensor C, however, it does not impose any computational overhead due to its completely structural re-parameterization design. Visualizations of searched fine-tuning strat- egy for each layer. Figure 3 visually shows the distribution of trainable parameters across the four types of linear layers embodied in ViT-B. Notably, the projection layer possesses the min- imum quantity of trainable parameters span- ning across VTAB-1K categories. Generally, the MLP module hosts a substantially higher number of parameters compared to the MHSA. As anticipated, the structured group necessi- tates a greater number of parameters for adap- tation due to a pronounced domain shift rela- tive to ImageNet-1K (Deng et al., 2009b). Fig- ure 4 illustrates the layer-wise configuration of the support tensors as searched by the GLoRA algorithm. Each support tensor at every layer can potentially undergo 72 distinct adaptations across datasets. Support tensors D and E exhibit relatively low adaptation due to the prevalence of Projection FC2Layer Type 0.00 Q-K-V 0.05 FC1 0.20 0.10 Natural 0.30Number of Parameters(M) Specialized Structured 0.15 0.25 Figure 3: Distribution of GLoRA (0.86M) param- eters across layer types on VTAB-1K. Q-K-V and Projection are linear layers in MHSA module and FC1 and FC2 are linear layers in MLP module. 8 A A D 50 A A ELayer Depth (Total=12) D 20 B D D C B B B B 0 B C LoRA C A 30 C C A C C B A C C C C C D D E E E E E E E B E B E E vector A 40 B E 70Number of Occurences (Total=72) B D A A A D A 10 D D D D constant 60 B Figure 4: Layerwise configuration of support tensors in GLoRA (0.86M) on VTAB-1K dataset. none adaptations, whereas A and B demonstrate a higher number of adaptations, though without a distinguishable pattern regarding the type of adaptation. It is important to underscore that even a basic scalar can function effectively as a support tensor, enabling GLoRA to maintain superior parameter efficiency despite adapting every linear layer. 5 RELATED WORK Given the rapid expansion in model size, numerous methods for parameter-efficient fine-tuning (PEFT) have been introduced in the field of NLP to streamline the optimization of large language models (LLMs) (Liu et al., 2021a; Zhang et al.; Hu et al.; Liu et al., 2021b; Li & Liang, 2021; Lester et al., 2021; Zaken et al., 2022; Houlsby et al., 2019). The effectiveness of parameter-efficient fine- tuning has been proven in a wide range of natural language processing tasks (Fu et al., 2022; He et al., 2021). In the vision domain, with the advent growth in the size of vision models (Dehghani et al., 2023; Kolesnikov et al., 2020), methods specifically focused on image modality have also been put forward (Jie & Deng, 2022; Lian et al., 2022; Chen et al., 2022; Luo et al., 2023; Zhang et al., 2022; Jia et al., 2022; He et al., 2023). Among these methods, LoRA (Hu et al.) has proven to transfer well across modalities and tasks. This is partly due to the simplistic design strategy of LoRA which directly works over weight tensors, irrespective of model type or configuration. Additionally, unlike Adapters (Houlsby et al., 2019; Chen et al., 2022) and Prompt tuning (Jia et al., 2022), LoRA does not add any additional inference parameters or latency due to structural re-parameterization (SR) design. RepAdapter (Luo et al., 2023) and SSF (Lian et al., 2022) also propose an SR design for PEFT. However, RepAdapter is specific to model architectures and required manual designing for different layer configurations. SSF provides a simple baseline but suffers from low flexibility and capability due to adaptation limited in the activation space. FacT (Jie & Deng, 2022) further decomposes LoRA matrices for better parameter efficiency, but we argue that <1M parameter scale is fairly efficient for fine-tuning on a single GPU. Thus, due to the advantages of LoRA over other related works, it is of importance to increase the flexibility, scalability and adaptability of LoRA. 6 CONCLUSION We have presented GLoRA, a generalized parameter-efficient fine-tuning approach that has suc- cessfully demonstrated its effectiveness and adaptability in enhancing the fine-tuning and transfer learning ability for the large-scale pre-trained models. By adopting a generalized low-rank adap- tation and re-parameterization framework, GLoRA significantly reduces the number of parameters and computation required for fine-tuning, making it a more resource-efficient and practical method for real-world applications. The experiments conducted on a diverse range of tasks and datasets have substantiated the superiority of GLoRA over existing PEFT techniques, showcasing its scala- bility and adaptability. Moreover, the ablation studies have provided valuable insights into the inner workings and the relative importance of different GLoRA components. This work not only con- tributes to the improvement of the fine-tuning process for large-scale pre-trained vision or language models but also opens up new avenues for future work, including further exploration of generalized low-rank adaptation techniques, the development of hybrid approaches, and the refinement of search and optimization algorithms. These areas of research may continue to expand the accessibility and efficiency of transfer learning across a broader range of applications. 9 REFERENCES Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative com- ponents with random forests. In European Conference on Computer Vision, 2014. 6 Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin"}