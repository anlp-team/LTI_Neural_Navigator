{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Efficient_Sequence_Transduction_by_Jointly_Predicting_Tokens_and_Durations_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the SICSF task described in the text?", "answer": " The SICSF task is to predict user intents and extract corresponding lexical fillers based on input audio.", "ref_chunk": "which takes audio as input to detect user intents and extract the corresponding lex- ical fillers for detected entity slots (Bastianelli et al., 2020). An intent is composed of a scenario type and an action type, while slots and fillers are represented by key-value pairs. The ground-truth intents and slots of input are organized as a Python dictionary, represented as a Python string. The SICSF task is to predict this text based on the input audio. Experiments are conducted using the SLURP (Bastianelli et al., 2020) dataset, where intent accuracy and SLURP-F1 11https://github.com/VKCOM/YouTokenToMe 12https://paperswithcode.com/sota/ speech-to-text-translation-on-must-c-en-de. 6 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Model SpeechBrain ESPnet-SLU #params (M) 96 109 intent acc. 87.7 86.52 SLURP F1 76.19 76.91 rel. speedup N/A N/A RNNT TDT 0-2 TDT 0-4 TDT 0-6 TDT 0-8 119 119 119 119 119 88.53 87.12 89.85 89.28 90.07 79.41 79.43 80.03 80.61 79.90 1.17x 1.17x 1.28x 1.28x Table 6. Speech intent classification and slot filling on SLURP dataset. TDT vs RNNT: Relative speed-up against RNNT. Figure 4. Duration distribution during inference on Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of different types of durations during the inference of Librispeech test-other datasets. are used as the evaluation metric. Our baseline model is a Conformer Transducer model, ini- tialized from our pretrained ASR model 13. We also in- clude results from two state-of-the-art models from ESPNet- SLU (Arora et al., 2022) and SpeechBrain (Wang et al., 2021c) for comparison. Both ESPNet-SLU and Speech- Brain use HuBERT (Hsu et al., 2021) encoders pretrained on LibriLight-60k (Kahn et al., 2020), while ESPNet-SLU further finetunes the encoder on LibriSpeech before training on SLURP. For our TDTs, we use the same duration config- urations that contain maximum durations 2, 4, 6, and 8. Dif- ferent from our ASR and ST experiments that use \u03c3 = 0.05, here we use \u03c3 = 0.02 for all experiments since we found using \u03c3 = 0.05 may destabilize training for SLURP14. The results are shown in Table 6. While the RNNT base- line already has better performance than ESPNet-SLU and SpeechBrain baselines, TDT models with [0-4], [0-6], and [0-8] configurations achieve even better accuracy which makes the new state-of-the-art in the SICSF. In addition, the TDT [0-8] model is 1.28X faster in inference than RNNT.15 The results demonstrate not only the effectiveness but also the efficiency of TDT algorithm applied in SICSF tasks. We notice relatively smaller speed-up factors with TDT in this task. This could be explained by the much lower average audio-to-token-length ratio for SICSF tasks. For example, in ASR tasks, the typical ratio between audio length to text length is around 7:1, and the ratio is around 0.89:1 for the 13https://catalog.ngc.nvidia.com/orgs/ nvidia/teams/nemo/models/stt_en_conformer_ transducer_large 14This is caused by a much smaller ratio between the audio and text length of SLU datasets: the average ratio of audio to and text is 0.89:1 for SLURP, compared to around 5.5:1 for ASR for example. Since on average audio is shorter than text, setting \u03c3 too high, which encourages large duration outputs, will hurt training. A smaller \u03c3 alleviates the issue. 15SLURP has on average shorter audio than text . This means larger durations occur much less for SLURP, resulting in smaller speed-ups compared to ASR and ST. SLURP testset, with average text sequence being longer than audio sequence16. Nevertheless, we see a significant speed up, which also shows that TDT models can bring improvement even for scenarios where the audio sequence is longer than the text sequences. 5. Discussion 5.1. TDT Emission Analysis In this section, we investigate the output distribution of TDT models using Librispeech test-other dataset. First, we collect statistics on the duration predicted during decoding, with different TDT configurations (Fig. 4). For the baseline RNN-T, we treat blank and non-blank symbol emissions as having durations 1 and 0, respectively, since blank advances the t by one and non-blank does not. TDT models with configs [0-2] and [0-4] fully utilize longer durations during inference, with almost all of the durations predicted for the [0-2] model, and around 90% of durations for the [0-4] model have the maximum duration. For [0-6] and [0-8] models, the frequencies of predicted long durations are reduced. This is expected since our analysis shows the average ratio of audio length to text length for Librispeech test-other is 5.5:1, smaller than 6. Hence, it is not possible to always emit such long durations. Next, we collect the frequency of blank emissions vs non- blank emissions (Fig. 5). We can see that as the model incorporates longer durations, fewer and fewer blank emis- sions are produced with TDT models, while the number of non-blank emissions remains unchanged. TDT models with durations [0-6] and [0-8] have very few blank emissions, indicating that TDT models are close to the theoretical lower bound in terms of the least number of decoding steps. 16Text sequence is computed as the number of subword tokens, and audio sequence is computed as the number of 40ms frames due to 10ms per audio frame during feature extraction, with 4X subsampling performed by the encoder. 7 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations TDT config clean other total time rel speed-up RNNT 0-2 0-4 0-6 0-8 2.13 2.10 2.15 2.10 2.13 5.11 4.94 5.04 4.91 5.03 274 182 151 146 159 1.51X 1.81X 1.88X 1.79X Figure 5. Emission counts of blanks VS non-blanks in Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of either a blank symbol (red) or a non-blank symbol (blue) during the inference of Librispeech test-other datasets. Table 7. Batched inference for TDT ASR models, trained with loss sampling \u03c9 = 0.1. WER (%) on Librispeech test-clean and test-other. Batch-size=4. When different utterances in the same batch predict different durations, we take the minimum of those predictions and advance all utterances in the batch by that amount. 5.3. TDT Robustness to noise 5.2. TDT Batched Inference The main difficulty"}, {"question": " How are intents and slots represented in the input data?", "answer": " Intents are represented as a scenario type and an action type, while slots are represented as key-value pairs.", "ref_chunk": "which takes audio as input to detect user intents and extract the corresponding lex- ical fillers for detected entity slots (Bastianelli et al., 2020). An intent is composed of a scenario type and an action type, while slots and fillers are represented by key-value pairs. The ground-truth intents and slots of input are organized as a Python dictionary, represented as a Python string. The SICSF task is to predict this text based on the input audio. Experiments are conducted using the SLURP (Bastianelli et al., 2020) dataset, where intent accuracy and SLURP-F1 11https://github.com/VKCOM/YouTokenToMe 12https://paperswithcode.com/sota/ speech-to-text-translation-on-must-c-en-de. 6 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Model SpeechBrain ESPnet-SLU #params (M) 96 109 intent acc. 87.7 86.52 SLURP F1 76.19 76.91 rel. speedup N/A N/A RNNT TDT 0-2 TDT 0-4 TDT 0-6 TDT 0-8 119 119 119 119 119 88.53 87.12 89.85 89.28 90.07 79.41 79.43 80.03 80.61 79.90 1.17x 1.17x 1.28x 1.28x Table 6. Speech intent classification and slot filling on SLURP dataset. TDT vs RNNT: Relative speed-up against RNNT. Figure 4. Duration distribution during inference on Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of different types of durations during the inference of Librispeech test-other datasets. are used as the evaluation metric. Our baseline model is a Conformer Transducer model, ini- tialized from our pretrained ASR model 13. We also in- clude results from two state-of-the-art models from ESPNet- SLU (Arora et al., 2022) and SpeechBrain (Wang et al., 2021c) for comparison. Both ESPNet-SLU and Speech- Brain use HuBERT (Hsu et al., 2021) encoders pretrained on LibriLight-60k (Kahn et al., 2020), while ESPNet-SLU further finetunes the encoder on LibriSpeech before training on SLURP. For our TDTs, we use the same duration config- urations that contain maximum durations 2, 4, 6, and 8. Dif- ferent from our ASR and ST experiments that use \u03c3 = 0.05, here we use \u03c3 = 0.02 for all experiments since we found using \u03c3 = 0.05 may destabilize training for SLURP14. The results are shown in Table 6. While the RNNT base- line already has better performance than ESPNet-SLU and SpeechBrain baselines, TDT models with [0-4], [0-6], and [0-8] configurations achieve even better accuracy which makes the new state-of-the-art in the SICSF. In addition, the TDT [0-8] model is 1.28X faster in inference than RNNT.15 The results demonstrate not only the effectiveness but also the efficiency of TDT algorithm applied in SICSF tasks. We notice relatively smaller speed-up factors with TDT in this task. This could be explained by the much lower average audio-to-token-length ratio for SICSF tasks. For example, in ASR tasks, the typical ratio between audio length to text length is around 7:1, and the ratio is around 0.89:1 for the 13https://catalog.ngc.nvidia.com/orgs/ nvidia/teams/nemo/models/stt_en_conformer_ transducer_large 14This is caused by a much smaller ratio between the audio and text length of SLU datasets: the average ratio of audio to and text is 0.89:1 for SLURP, compared to around 5.5:1 for ASR for example. Since on average audio is shorter than text, setting \u03c3 too high, which encourages large duration outputs, will hurt training. A smaller \u03c3 alleviates the issue. 15SLURP has on average shorter audio than text . This means larger durations occur much less for SLURP, resulting in smaller speed-ups compared to ASR and ST. SLURP testset, with average text sequence being longer than audio sequence16. Nevertheless, we see a significant speed up, which also shows that TDT models can bring improvement even for scenarios where the audio sequence is longer than the text sequences. 5. Discussion 5.1. TDT Emission Analysis In this section, we investigate the output distribution of TDT models using Librispeech test-other dataset. First, we collect statistics on the duration predicted during decoding, with different TDT configurations (Fig. 4). For the baseline RNN-T, we treat blank and non-blank symbol emissions as having durations 1 and 0, respectively, since blank advances the t by one and non-blank does not. TDT models with configs [0-2] and [0-4] fully utilize longer durations during inference, with almost all of the durations predicted for the [0-2] model, and around 90% of durations for the [0-4] model have the maximum duration. For [0-6] and [0-8] models, the frequencies of predicted long durations are reduced. This is expected since our analysis shows the average ratio of audio length to text length for Librispeech test-other is 5.5:1, smaller than 6. Hence, it is not possible to always emit such long durations. Next, we collect the frequency of blank emissions vs non- blank emissions (Fig. 5). We can see that as the model incorporates longer durations, fewer and fewer blank emis- sions are produced with TDT models, while the number of non-blank emissions remains unchanged. TDT models with durations [0-6] and [0-8] have very few blank emissions, indicating that TDT models are close to the theoretical lower bound in terms of the least number of decoding steps. 16Text sequence is computed as the number of subword tokens, and audio sequence is computed as the number of 40ms frames due to 10ms per audio frame during feature extraction, with 4X subsampling performed by the encoder. 7 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations TDT config clean other total time rel speed-up RNNT 0-2 0-4 0-6 0-8 2.13 2.10 2.15 2.10 2.13 5.11 4.94 5.04 4.91 5.03 274 182 151 146 159 1.51X 1.81X 1.88X 1.79X Figure 5. Emission counts of blanks VS non-blanks in Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of either a blank symbol (red) or a non-blank symbol (blue) during the inference of Librispeech test-other datasets. Table 7. Batched inference for TDT ASR models, trained with loss sampling \u03c9 = 0.1. WER (%) on Librispeech test-clean and test-other. Batch-size=4. When different utterances in the same batch predict different durations, we take the minimum of those predictions and advance all utterances in the batch by that amount. 5.3. TDT Robustness to noise 5.2. TDT Batched Inference The main difficulty"}, {"question": " What dataset is used for experiments in the text?", "answer": " Experiments are conducted using the SLURP dataset.", "ref_chunk": "which takes audio as input to detect user intents and extract the corresponding lex- ical fillers for detected entity slots (Bastianelli et al., 2020). An intent is composed of a scenario type and an action type, while slots and fillers are represented by key-value pairs. The ground-truth intents and slots of input are organized as a Python dictionary, represented as a Python string. The SICSF task is to predict this text based on the input audio. Experiments are conducted using the SLURP (Bastianelli et al., 2020) dataset, where intent accuracy and SLURP-F1 11https://github.com/VKCOM/YouTokenToMe 12https://paperswithcode.com/sota/ speech-to-text-translation-on-must-c-en-de. 6 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Model SpeechBrain ESPnet-SLU #params (M) 96 109 intent acc. 87.7 86.52 SLURP F1 76.19 76.91 rel. speedup N/A N/A RNNT TDT 0-2 TDT 0-4 TDT 0-6 TDT 0-8 119 119 119 119 119 88.53 87.12 89.85 89.28 90.07 79.41 79.43 80.03 80.61 79.90 1.17x 1.17x 1.28x 1.28x Table 6. Speech intent classification and slot filling on SLURP dataset. TDT vs RNNT: Relative speed-up against RNNT. Figure 4. Duration distribution during inference on Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of different types of durations during the inference of Librispeech test-other datasets. are used as the evaluation metric. Our baseline model is a Conformer Transducer model, ini- tialized from our pretrained ASR model 13. We also in- clude results from two state-of-the-art models from ESPNet- SLU (Arora et al., 2022) and SpeechBrain (Wang et al., 2021c) for comparison. Both ESPNet-SLU and Speech- Brain use HuBERT (Hsu et al., 2021) encoders pretrained on LibriLight-60k (Kahn et al., 2020), while ESPNet-SLU further finetunes the encoder on LibriSpeech before training on SLURP. For our TDTs, we use the same duration config- urations that contain maximum durations 2, 4, 6, and 8. Dif- ferent from our ASR and ST experiments that use \u03c3 = 0.05, here we use \u03c3 = 0.02 for all experiments since we found using \u03c3 = 0.05 may destabilize training for SLURP14. The results are shown in Table 6. While the RNNT base- line already has better performance than ESPNet-SLU and SpeechBrain baselines, TDT models with [0-4], [0-6], and [0-8] configurations achieve even better accuracy which makes the new state-of-the-art in the SICSF. In addition, the TDT [0-8] model is 1.28X faster in inference than RNNT.15 The results demonstrate not only the effectiveness but also the efficiency of TDT algorithm applied in SICSF tasks. We notice relatively smaller speed-up factors with TDT in this task. This could be explained by the much lower average audio-to-token-length ratio for SICSF tasks. For example, in ASR tasks, the typical ratio between audio length to text length is around 7:1, and the ratio is around 0.89:1 for the 13https://catalog.ngc.nvidia.com/orgs/ nvidia/teams/nemo/models/stt_en_conformer_ transducer_large 14This is caused by a much smaller ratio between the audio and text length of SLU datasets: the average ratio of audio to and text is 0.89:1 for SLURP, compared to around 5.5:1 for ASR for example. Since on average audio is shorter than text, setting \u03c3 too high, which encourages large duration outputs, will hurt training. A smaller \u03c3 alleviates the issue. 15SLURP has on average shorter audio than text . This means larger durations occur much less for SLURP, resulting in smaller speed-ups compared to ASR and ST. SLURP testset, with average text sequence being longer than audio sequence16. Nevertheless, we see a significant speed up, which also shows that TDT models can bring improvement even for scenarios where the audio sequence is longer than the text sequences. 5. Discussion 5.1. TDT Emission Analysis In this section, we investigate the output distribution of TDT models using Librispeech test-other dataset. First, we collect statistics on the duration predicted during decoding, with different TDT configurations (Fig. 4). For the baseline RNN-T, we treat blank and non-blank symbol emissions as having durations 1 and 0, respectively, since blank advances the t by one and non-blank does not. TDT models with configs [0-2] and [0-4] fully utilize longer durations during inference, with almost all of the durations predicted for the [0-2] model, and around 90% of durations for the [0-4] model have the maximum duration. For [0-6] and [0-8] models, the frequencies of predicted long durations are reduced. This is expected since our analysis shows the average ratio of audio length to text length for Librispeech test-other is 5.5:1, smaller than 6. Hence, it is not possible to always emit such long durations. Next, we collect the frequency of blank emissions vs non- blank emissions (Fig. 5). We can see that as the model incorporates longer durations, fewer and fewer blank emis- sions are produced with TDT models, while the number of non-blank emissions remains unchanged. TDT models with durations [0-6] and [0-8] have very few blank emissions, indicating that TDT models are close to the theoretical lower bound in terms of the least number of decoding steps. 16Text sequence is computed as the number of subword tokens, and audio sequence is computed as the number of 40ms frames due to 10ms per audio frame during feature extraction, with 4X subsampling performed by the encoder. 7 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations TDT config clean other total time rel speed-up RNNT 0-2 0-4 0-6 0-8 2.13 2.10 2.15 2.10 2.13 5.11 4.94 5.04 4.91 5.03 274 182 151 146 159 1.51X 1.81X 1.88X 1.79X Figure 5. Emission counts of blanks VS non-blanks in Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of either a blank symbol (red) or a non-blank symbol (blue) during the inference of Librispeech test-other datasets. Table 7. Batched inference for TDT ASR models, trained with loss sampling \u03c9 = 0.1. WER (%) on Librispeech test-clean and test-other. Batch-size=4. When different utterances in the same batch predict different durations, we take the minimum of those predictions and advance all utterances in the batch by that amount. 5.3. TDT Robustness to noise 5.2. TDT Batched Inference The main difficulty"}, {"question": " What is the baseline model used in the experiments?", "answer": " The baseline model is a Conformer Transducer model.", "ref_chunk": "which takes audio as input to detect user intents and extract the corresponding lex- ical fillers for detected entity slots (Bastianelli et al., 2020). An intent is composed of a scenario type and an action type, while slots and fillers are represented by key-value pairs. The ground-truth intents and slots of input are organized as a Python dictionary, represented as a Python string. The SICSF task is to predict this text based on the input audio. Experiments are conducted using the SLURP (Bastianelli et al., 2020) dataset, where intent accuracy and SLURP-F1 11https://github.com/VKCOM/YouTokenToMe 12https://paperswithcode.com/sota/ speech-to-text-translation-on-must-c-en-de. 6 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Model SpeechBrain ESPnet-SLU #params (M) 96 109 intent acc. 87.7 86.52 SLURP F1 76.19 76.91 rel. speedup N/A N/A RNNT TDT 0-2 TDT 0-4 TDT 0-6 TDT 0-8 119 119 119 119 119 88.53 87.12 89.85 89.28 90.07 79.41 79.43 80.03 80.61 79.90 1.17x 1.17x 1.28x 1.28x Table 6. Speech intent classification and slot filling on SLURP dataset. TDT vs RNNT: Relative speed-up against RNNT. Figure 4. Duration distribution during inference on Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of different types of durations during the inference of Librispeech test-other datasets. are used as the evaluation metric. Our baseline model is a Conformer Transducer model, ini- tialized from our pretrained ASR model 13. We also in- clude results from two state-of-the-art models from ESPNet- SLU (Arora et al., 2022) and SpeechBrain (Wang et al., 2021c) for comparison. Both ESPNet-SLU and Speech- Brain use HuBERT (Hsu et al., 2021) encoders pretrained on LibriLight-60k (Kahn et al., 2020), while ESPNet-SLU further finetunes the encoder on LibriSpeech before training on SLURP. For our TDTs, we use the same duration config- urations that contain maximum durations 2, 4, 6, and 8. Dif- ferent from our ASR and ST experiments that use \u03c3 = 0.05, here we use \u03c3 = 0.02 for all experiments since we found using \u03c3 = 0.05 may destabilize training for SLURP14. The results are shown in Table 6. While the RNNT base- line already has better performance than ESPNet-SLU and SpeechBrain baselines, TDT models with [0-4], [0-6], and [0-8] configurations achieve even better accuracy which makes the new state-of-the-art in the SICSF. In addition, the TDT [0-8] model is 1.28X faster in inference than RNNT.15 The results demonstrate not only the effectiveness but also the efficiency of TDT algorithm applied in SICSF tasks. We notice relatively smaller speed-up factors with TDT in this task. This could be explained by the much lower average audio-to-token-length ratio for SICSF tasks. For example, in ASR tasks, the typical ratio between audio length to text length is around 7:1, and the ratio is around 0.89:1 for the 13https://catalog.ngc.nvidia.com/orgs/ nvidia/teams/nemo/models/stt_en_conformer_ transducer_large 14This is caused by a much smaller ratio between the audio and text length of SLU datasets: the average ratio of audio to and text is 0.89:1 for SLURP, compared to around 5.5:1 for ASR for example. Since on average audio is shorter than text, setting \u03c3 too high, which encourages large duration outputs, will hurt training. A smaller \u03c3 alleviates the issue. 15SLURP has on average shorter audio than text . This means larger durations occur much less for SLURP, resulting in smaller speed-ups compared to ASR and ST. SLURP testset, with average text sequence being longer than audio sequence16. Nevertheless, we see a significant speed up, which also shows that TDT models can bring improvement even for scenarios where the audio sequence is longer than the text sequences. 5. Discussion 5.1. TDT Emission Analysis In this section, we investigate the output distribution of TDT models using Librispeech test-other dataset. First, we collect statistics on the duration predicted during decoding, with different TDT configurations (Fig. 4). For the baseline RNN-T, we treat blank and non-blank symbol emissions as having durations 1 and 0, respectively, since blank advances the t by one and non-blank does not. TDT models with configs [0-2] and [0-4] fully utilize longer durations during inference, with almost all of the durations predicted for the [0-2] model, and around 90% of durations for the [0-4] model have the maximum duration. For [0-6] and [0-8] models, the frequencies of predicted long durations are reduced. This is expected since our analysis shows the average ratio of audio length to text length for Librispeech test-other is 5.5:1, smaller than 6. Hence, it is not possible to always emit such long durations. Next, we collect the frequency of blank emissions vs non- blank emissions (Fig. 5). We can see that as the model incorporates longer durations, fewer and fewer blank emis- sions are produced with TDT models, while the number of non-blank emissions remains unchanged. TDT models with durations [0-6] and [0-8] have very few blank emissions, indicating that TDT models are close to the theoretical lower bound in terms of the least number of decoding steps. 16Text sequence is computed as the number of subword tokens, and audio sequence is computed as the number of 40ms frames due to 10ms per audio frame during feature extraction, with 4X subsampling performed by the encoder. 7 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations TDT config clean other total time rel speed-up RNNT 0-2 0-4 0-6 0-8 2.13 2.10 2.15 2.10 2.13 5.11 4.94 5.04 4.91 5.03 274 182 151 146 159 1.51X 1.81X 1.88X 1.79X Figure 5. Emission counts of blanks VS non-blanks in Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of either a blank symbol (red) or a non-blank symbol (blue) during the inference of Librispeech test-other datasets. Table 7. Batched inference for TDT ASR models, trained with loss sampling \u03c9 = 0.1. WER (%) on Librispeech test-clean and test-other. Batch-size=4. When different utterances in the same batch predict different durations, we take the minimum of those predictions and advance all utterances in the batch by that amount. 5.3. TDT Robustness to noise 5.2. TDT Batched Inference The main difficulty"}, {"question": " What encoder is used by ESPNet-SLU and SpeechBrain in the experiments?", "answer": " HuBERT encoders pretrained on LibriLight-60k are used by ESPNet-SLU and SpeechBrain.", "ref_chunk": "which takes audio as input to detect user intents and extract the corresponding lex- ical fillers for detected entity slots (Bastianelli et al., 2020). An intent is composed of a scenario type and an action type, while slots and fillers are represented by key-value pairs. The ground-truth intents and slots of input are organized as a Python dictionary, represented as a Python string. The SICSF task is to predict this text based on the input audio. Experiments are conducted using the SLURP (Bastianelli et al., 2020) dataset, where intent accuracy and SLURP-F1 11https://github.com/VKCOM/YouTokenToMe 12https://paperswithcode.com/sota/ speech-to-text-translation-on-must-c-en-de. 6 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Model SpeechBrain ESPnet-SLU #params (M) 96 109 intent acc. 87.7 86.52 SLURP F1 76.19 76.91 rel. speedup N/A N/A RNNT TDT 0-2 TDT 0-4 TDT 0-6 TDT 0-8 119 119 119 119 119 88.53 87.12 89.85 89.28 90.07 79.41 79.43 80.03 80.61 79.90 1.17x 1.17x 1.28x 1.28x Table 6. Speech intent classification and slot filling on SLURP dataset. TDT vs RNNT: Relative speed-up against RNNT. Figure 4. Duration distribution during inference on Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of different types of durations during the inference of Librispeech test-other datasets. are used as the evaluation metric. Our baseline model is a Conformer Transducer model, ini- tialized from our pretrained ASR model 13. We also in- clude results from two state-of-the-art models from ESPNet- SLU (Arora et al., 2022) and SpeechBrain (Wang et al., 2021c) for comparison. Both ESPNet-SLU and Speech- Brain use HuBERT (Hsu et al., 2021) encoders pretrained on LibriLight-60k (Kahn et al., 2020), while ESPNet-SLU further finetunes the encoder on LibriSpeech before training on SLURP. For our TDTs, we use the same duration config- urations that contain maximum durations 2, 4, 6, and 8. Dif- ferent from our ASR and ST experiments that use \u03c3 = 0.05, here we use \u03c3 = 0.02 for all experiments since we found using \u03c3 = 0.05 may destabilize training for SLURP14. The results are shown in Table 6. While the RNNT base- line already has better performance than ESPNet-SLU and SpeechBrain baselines, TDT models with [0-4], [0-6], and [0-8] configurations achieve even better accuracy which makes the new state-of-the-art in the SICSF. In addition, the TDT [0-8] model is 1.28X faster in inference than RNNT.15 The results demonstrate not only the effectiveness but also the efficiency of TDT algorithm applied in SICSF tasks. We notice relatively smaller speed-up factors with TDT in this task. This could be explained by the much lower average audio-to-token-length ratio for SICSF tasks. For example, in ASR tasks, the typical ratio between audio length to text length is around 7:1, and the ratio is around 0.89:1 for the 13https://catalog.ngc.nvidia.com/orgs/ nvidia/teams/nemo/models/stt_en_conformer_ transducer_large 14This is caused by a much smaller ratio between the audio and text length of SLU datasets: the average ratio of audio to and text is 0.89:1 for SLURP, compared to around 5.5:1 for ASR for example. Since on average audio is shorter than text, setting \u03c3 too high, which encourages large duration outputs, will hurt training. A smaller \u03c3 alleviates the issue. 15SLURP has on average shorter audio than text . This means larger durations occur much less for SLURP, resulting in smaller speed-ups compared to ASR and ST. SLURP testset, with average text sequence being longer than audio sequence16. Nevertheless, we see a significant speed up, which also shows that TDT models can bring improvement even for scenarios where the audio sequence is longer than the text sequences. 5. Discussion 5.1. TDT Emission Analysis In this section, we investigate the output distribution of TDT models using Librispeech test-other dataset. First, we collect statistics on the duration predicted during decoding, with different TDT configurations (Fig. 4). For the baseline RNN-T, we treat blank and non-blank symbol emissions as having durations 1 and 0, respectively, since blank advances the t by one and non-blank does not. TDT models with configs [0-2] and [0-4] fully utilize longer durations during inference, with almost all of the durations predicted for the [0-2] model, and around 90% of durations for the [0-4] model have the maximum duration. For [0-6] and [0-8] models, the frequencies of predicted long durations are reduced. This is expected since our analysis shows the average ratio of audio length to text length for Librispeech test-other is 5.5:1, smaller than 6. Hence, it is not possible to always emit such long durations. Next, we collect the frequency of blank emissions vs non- blank emissions (Fig. 5). We can see that as the model incorporates longer durations, fewer and fewer blank emis- sions are produced with TDT models, while the number of non-blank emissions remains unchanged. TDT models with durations [0-6] and [0-8] have very few blank emissions, indicating that TDT models are close to the theoretical lower bound in terms of the least number of decoding steps. 16Text sequence is computed as the number of subword tokens, and audio sequence is computed as the number of 40ms frames due to 10ms per audio frame during feature extraction, with 4X subsampling performed by the encoder. 7 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations TDT config clean other total time rel speed-up RNNT 0-2 0-4 0-6 0-8 2.13 2.10 2.15 2.10 2.13 5.11 4.94 5.04 4.91 5.03 274 182 151 146 159 1.51X 1.81X 1.88X 1.79X Figure 5. Emission counts of blanks VS non-blanks in Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of either a blank symbol (red) or a non-blank symbol (blue) during the inference of Librispeech test-other datasets. Table 7. Batched inference for TDT ASR models, trained with loss sampling \u03c9 = 0.1. WER (%) on Librispeech test-clean and test-other. Batch-size=4. When different utterances in the same batch predict different durations, we take the minimum of those predictions and advance all utterances in the batch by that amount. 5.3. TDT Robustness to noise 5.2. TDT Batched Inference The main difficulty"}, {"question": " What is the significance of the TDT models in the SICSF task?", "answer": " TDT models achieve better accuracy and efficiency, making them the new state-of-the-art in the SICSF task.", "ref_chunk": "which takes audio as input to detect user intents and extract the corresponding lex- ical fillers for detected entity slots (Bastianelli et al., 2020). An intent is composed of a scenario type and an action type, while slots and fillers are represented by key-value pairs. The ground-truth intents and slots of input are organized as a Python dictionary, represented as a Python string. The SICSF task is to predict this text based on the input audio. Experiments are conducted using the SLURP (Bastianelli et al., 2020) dataset, where intent accuracy and SLURP-F1 11https://github.com/VKCOM/YouTokenToMe 12https://paperswithcode.com/sota/ speech-to-text-translation-on-must-c-en-de. 6 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Model SpeechBrain ESPnet-SLU #params (M) 96 109 intent acc. 87.7 86.52 SLURP F1 76.19 76.91 rel. speedup N/A N/A RNNT TDT 0-2 TDT 0-4 TDT 0-6 TDT 0-8 119 119 119 119 119 88.53 87.12 89.85 89.28 90.07 79.41 79.43 80.03 80.61 79.90 1.17x 1.17x 1.28x 1.28x Table 6. Speech intent classification and slot filling on SLURP dataset. TDT vs RNNT: Relative speed-up against RNNT. Figure 4. Duration distribution during inference on Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of different types of durations during the inference of Librispeech test-other datasets. are used as the evaluation metric. Our baseline model is a Conformer Transducer model, ini- tialized from our pretrained ASR model 13. We also in- clude results from two state-of-the-art models from ESPNet- SLU (Arora et al., 2022) and SpeechBrain (Wang et al., 2021c) for comparison. Both ESPNet-SLU and Speech- Brain use HuBERT (Hsu et al., 2021) encoders pretrained on LibriLight-60k (Kahn et al., 2020), while ESPNet-SLU further finetunes the encoder on LibriSpeech before training on SLURP. For our TDTs, we use the same duration config- urations that contain maximum durations 2, 4, 6, and 8. Dif- ferent from our ASR and ST experiments that use \u03c3 = 0.05, here we use \u03c3 = 0.02 for all experiments since we found using \u03c3 = 0.05 may destabilize training for SLURP14. The results are shown in Table 6. While the RNNT base- line already has better performance than ESPNet-SLU and SpeechBrain baselines, TDT models with [0-4], [0-6], and [0-8] configurations achieve even better accuracy which makes the new state-of-the-art in the SICSF. In addition, the TDT [0-8] model is 1.28X faster in inference than RNNT.15 The results demonstrate not only the effectiveness but also the efficiency of TDT algorithm applied in SICSF tasks. We notice relatively smaller speed-up factors with TDT in this task. This could be explained by the much lower average audio-to-token-length ratio for SICSF tasks. For example, in ASR tasks, the typical ratio between audio length to text length is around 7:1, and the ratio is around 0.89:1 for the 13https://catalog.ngc.nvidia.com/orgs/ nvidia/teams/nemo/models/stt_en_conformer_ transducer_large 14This is caused by a much smaller ratio between the audio and text length of SLU datasets: the average ratio of audio to and text is 0.89:1 for SLURP, compared to around 5.5:1 for ASR for example. Since on average audio is shorter than text, setting \u03c3 too high, which encourages large duration outputs, will hurt training. A smaller \u03c3 alleviates the issue. 15SLURP has on average shorter audio than text . This means larger durations occur much less for SLURP, resulting in smaller speed-ups compared to ASR and ST. SLURP testset, with average text sequence being longer than audio sequence16. Nevertheless, we see a significant speed up, which also shows that TDT models can bring improvement even for scenarios where the audio sequence is longer than the text sequences. 5. Discussion 5.1. TDT Emission Analysis In this section, we investigate the output distribution of TDT models using Librispeech test-other dataset. First, we collect statistics on the duration predicted during decoding, with different TDT configurations (Fig. 4). For the baseline RNN-T, we treat blank and non-blank symbol emissions as having durations 1 and 0, respectively, since blank advances the t by one and non-blank does not. TDT models with configs [0-2] and [0-4] fully utilize longer durations during inference, with almost all of the durations predicted for the [0-2] model, and around 90% of durations for the [0-4] model have the maximum duration. For [0-6] and [0-8] models, the frequencies of predicted long durations are reduced. This is expected since our analysis shows the average ratio of audio length to text length for Librispeech test-other is 5.5:1, smaller than 6. Hence, it is not possible to always emit such long durations. Next, we collect the frequency of blank emissions vs non- blank emissions (Fig. 5). We can see that as the model incorporates longer durations, fewer and fewer blank emis- sions are produced with TDT models, while the number of non-blank emissions remains unchanged. TDT models with durations [0-6] and [0-8] have very few blank emissions, indicating that TDT models are close to the theoretical lower bound in terms of the least number of decoding steps. 16Text sequence is computed as the number of subword tokens, and audio sequence is computed as the number of 40ms frames due to 10ms per audio frame during feature extraction, with 4X subsampling performed by the encoder. 7 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations TDT config clean other total time rel speed-up RNNT 0-2 0-4 0-6 0-8 2.13 2.10 2.15 2.10 2.13 5.11 4.94 5.04 4.91 5.03 274 182 151 146 159 1.51X 1.81X 1.88X 1.79X Figure 5. Emission counts of blanks VS non-blanks in Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of either a blank symbol (red) or a non-blank symbol (blue) during the inference of Librispeech test-other datasets. Table 7. Batched inference for TDT ASR models, trained with loss sampling \u03c9 = 0.1. WER (%) on Librispeech test-clean and test-other. Batch-size=4. When different utterances in the same batch predict different durations, we take the minimum of those predictions and advance all utterances in the batch by that amount. 5.3. TDT Robustness to noise 5.2. TDT Batched Inference The main difficulty"}, {"question": " Why is a smaller \u03c3 value used in the experiments for SLURP?", "answer": " Using a smaller \u03c3 value helps stabilize training for SLURP experiments, as setting \u03c3 too high may hurt training.", "ref_chunk": "which takes audio as input to detect user intents and extract the corresponding lex- ical fillers for detected entity slots (Bastianelli et al., 2020). An intent is composed of a scenario type and an action type, while slots and fillers are represented by key-value pairs. The ground-truth intents and slots of input are organized as a Python dictionary, represented as a Python string. The SICSF task is to predict this text based on the input audio. Experiments are conducted using the SLURP (Bastianelli et al., 2020) dataset, where intent accuracy and SLURP-F1 11https://github.com/VKCOM/YouTokenToMe 12https://paperswithcode.com/sota/ speech-to-text-translation-on-must-c-en-de. 6 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Model SpeechBrain ESPnet-SLU #params (M) 96 109 intent acc. 87.7 86.52 SLURP F1 76.19 76.91 rel. speedup N/A N/A RNNT TDT 0-2 TDT 0-4 TDT 0-6 TDT 0-8 119 119 119 119 119 88.53 87.12 89.85 89.28 90.07 79.41 79.43 80.03 80.61 79.90 1.17x 1.17x 1.28x 1.28x Table 6. Speech intent classification and slot filling on SLURP dataset. TDT vs RNNT: Relative speed-up against RNNT. Figure 4. Duration distribution during inference on Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of different types of durations during the inference of Librispeech test-other datasets. are used as the evaluation metric. Our baseline model is a Conformer Transducer model, ini- tialized from our pretrained ASR model 13. We also in- clude results from two state-of-the-art models from ESPNet- SLU (Arora et al., 2022) and SpeechBrain (Wang et al., 2021c) for comparison. Both ESPNet-SLU and Speech- Brain use HuBERT (Hsu et al., 2021) encoders pretrained on LibriLight-60k (Kahn et al., 2020), while ESPNet-SLU further finetunes the encoder on LibriSpeech before training on SLURP. For our TDTs, we use the same duration config- urations that contain maximum durations 2, 4, 6, and 8. Dif- ferent from our ASR and ST experiments that use \u03c3 = 0.05, here we use \u03c3 = 0.02 for all experiments since we found using \u03c3 = 0.05 may destabilize training for SLURP14. The results are shown in Table 6. While the RNNT base- line already has better performance than ESPNet-SLU and SpeechBrain baselines, TDT models with [0-4], [0-6], and [0-8] configurations achieve even better accuracy which makes the new state-of-the-art in the SICSF. In addition, the TDT [0-8] model is 1.28X faster in inference than RNNT.15 The results demonstrate not only the effectiveness but also the efficiency of TDT algorithm applied in SICSF tasks. We notice relatively smaller speed-up factors with TDT in this task. This could be explained by the much lower average audio-to-token-length ratio for SICSF tasks. For example, in ASR tasks, the typical ratio between audio length to text length is around 7:1, and the ratio is around 0.89:1 for the 13https://catalog.ngc.nvidia.com/orgs/ nvidia/teams/nemo/models/stt_en_conformer_ transducer_large 14This is caused by a much smaller ratio between the audio and text length of SLU datasets: the average ratio of audio to and text is 0.89:1 for SLURP, compared to around 5.5:1 for ASR for example. Since on average audio is shorter than text, setting \u03c3 too high, which encourages large duration outputs, will hurt training. A smaller \u03c3 alleviates the issue. 15SLURP has on average shorter audio than text . This means larger durations occur much less for SLURP, resulting in smaller speed-ups compared to ASR and ST. SLURP testset, with average text sequence being longer than audio sequence16. Nevertheless, we see a significant speed up, which also shows that TDT models can bring improvement even for scenarios where the audio sequence is longer than the text sequences. 5. Discussion 5.1. TDT Emission Analysis In this section, we investigate the output distribution of TDT models using Librispeech test-other dataset. First, we collect statistics on the duration predicted during decoding, with different TDT configurations (Fig. 4). For the baseline RNN-T, we treat blank and non-blank symbol emissions as having durations 1 and 0, respectively, since blank advances the t by one and non-blank does not. TDT models with configs [0-2] and [0-4] fully utilize longer durations during inference, with almost all of the durations predicted for the [0-2] model, and around 90% of durations for the [0-4] model have the maximum duration. For [0-6] and [0-8] models, the frequencies of predicted long durations are reduced. This is expected since our analysis shows the average ratio of audio length to text length for Librispeech test-other is 5.5:1, smaller than 6. Hence, it is not possible to always emit such long durations. Next, we collect the frequency of blank emissions vs non- blank emissions (Fig. 5). We can see that as the model incorporates longer durations, fewer and fewer blank emis- sions are produced with TDT models, while the number of non-blank emissions remains unchanged. TDT models with durations [0-6] and [0-8] have very few blank emissions, indicating that TDT models are close to the theoretical lower bound in terms of the least number of decoding steps. 16Text sequence is computed as the number of subword tokens, and audio sequence is computed as the number of 40ms frames due to 10ms per audio frame during feature extraction, with 4X subsampling performed by the encoder. 7 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations TDT config clean other total time rel speed-up RNNT 0-2 0-4 0-6 0-8 2.13 2.10 2.15 2.10 2.13 5.11 4.94 5.04 4.91 5.03 274 182 151 146 159 1.51X 1.81X 1.88X 1.79X Figure 5. Emission counts of blanks VS non-blanks in Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of either a blank symbol (red) or a non-blank symbol (blue) during the inference of Librispeech test-other datasets. Table 7. Batched inference for TDT ASR models, trained with loss sampling \u03c9 = 0.1. WER (%) on Librispeech test-clean and test-other. Batch-size=4. When different utterances in the same batch predict different durations, we take the minimum of those predictions and advance all utterances in the batch by that amount. 5.3. TDT Robustness to noise 5.2. TDT Batched Inference The main difficulty"}, {"question": " What analysis is conducted in the TDT Emission Analysis section?", "answer": " The output distribution of TDT models using Librispeech test-other dataset is investigated in the TDT Emission Analysis section.", "ref_chunk": "which takes audio as input to detect user intents and extract the corresponding lex- ical fillers for detected entity slots (Bastianelli et al., 2020). An intent is composed of a scenario type and an action type, while slots and fillers are represented by key-value pairs. The ground-truth intents and slots of input are organized as a Python dictionary, represented as a Python string. The SICSF task is to predict this text based on the input audio. Experiments are conducted using the SLURP (Bastianelli et al., 2020) dataset, where intent accuracy and SLURP-F1 11https://github.com/VKCOM/YouTokenToMe 12https://paperswithcode.com/sota/ speech-to-text-translation-on-must-c-en-de. 6 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Model SpeechBrain ESPnet-SLU #params (M) 96 109 intent acc. 87.7 86.52 SLURP F1 76.19 76.91 rel. speedup N/A N/A RNNT TDT 0-2 TDT 0-4 TDT 0-6 TDT 0-8 119 119 119 119 119 88.53 87.12 89.85 89.28 90.07 79.41 79.43 80.03 80.61 79.90 1.17x 1.17x 1.28x 1.28x Table 6. Speech intent classification and slot filling on SLURP dataset. TDT vs RNNT: Relative speed-up against RNNT. Figure 4. Duration distribution during inference on Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of different types of durations during the inference of Librispeech test-other datasets. are used as the evaluation metric. Our baseline model is a Conformer Transducer model, ini- tialized from our pretrained ASR model 13. We also in- clude results from two state-of-the-art models from ESPNet- SLU (Arora et al., 2022) and SpeechBrain (Wang et al., 2021c) for comparison. Both ESPNet-SLU and Speech- Brain use HuBERT (Hsu et al., 2021) encoders pretrained on LibriLight-60k (Kahn et al., 2020), while ESPNet-SLU further finetunes the encoder on LibriSpeech before training on SLURP. For our TDTs, we use the same duration config- urations that contain maximum durations 2, 4, 6, and 8. Dif- ferent from our ASR and ST experiments that use \u03c3 = 0.05, here we use \u03c3 = 0.02 for all experiments since we found using \u03c3 = 0.05 may destabilize training for SLURP14. The results are shown in Table 6. While the RNNT base- line already has better performance than ESPNet-SLU and SpeechBrain baselines, TDT models with [0-4], [0-6], and [0-8] configurations achieve even better accuracy which makes the new state-of-the-art in the SICSF. In addition, the TDT [0-8] model is 1.28X faster in inference than RNNT.15 The results demonstrate not only the effectiveness but also the efficiency of TDT algorithm applied in SICSF tasks. We notice relatively smaller speed-up factors with TDT in this task. This could be explained by the much lower average audio-to-token-length ratio for SICSF tasks. For example, in ASR tasks, the typical ratio between audio length to text length is around 7:1, and the ratio is around 0.89:1 for the 13https://catalog.ngc.nvidia.com/orgs/ nvidia/teams/nemo/models/stt_en_conformer_ transducer_large 14This is caused by a much smaller ratio between the audio and text length of SLU datasets: the average ratio of audio to and text is 0.89:1 for SLURP, compared to around 5.5:1 for ASR for example. Since on average audio is shorter than text, setting \u03c3 too high, which encourages large duration outputs, will hurt training. A smaller \u03c3 alleviates the issue. 15SLURP has on average shorter audio than text . This means larger durations occur much less for SLURP, resulting in smaller speed-ups compared to ASR and ST. SLURP testset, with average text sequence being longer than audio sequence16. Nevertheless, we see a significant speed up, which also shows that TDT models can bring improvement even for scenarios where the audio sequence is longer than the text sequences. 5. Discussion 5.1. TDT Emission Analysis In this section, we investigate the output distribution of TDT models using Librispeech test-other dataset. First, we collect statistics on the duration predicted during decoding, with different TDT configurations (Fig. 4). For the baseline RNN-T, we treat blank and non-blank symbol emissions as having durations 1 and 0, respectively, since blank advances the t by one and non-blank does not. TDT models with configs [0-2] and [0-4] fully utilize longer durations during inference, with almost all of the durations predicted for the [0-2] model, and around 90% of durations for the [0-4] model have the maximum duration. For [0-6] and [0-8] models, the frequencies of predicted long durations are reduced. This is expected since our analysis shows the average ratio of audio length to text length for Librispeech test-other is 5.5:1, smaller than 6. Hence, it is not possible to always emit such long durations. Next, we collect the frequency of blank emissions vs non- blank emissions (Fig. 5). We can see that as the model incorporates longer durations, fewer and fewer blank emis- sions are produced with TDT models, while the number of non-blank emissions remains unchanged. TDT models with durations [0-6] and [0-8] have very few blank emissions, indicating that TDT models are close to the theoretical lower bound in terms of the least number of decoding steps. 16Text sequence is computed as the number of subword tokens, and audio sequence is computed as the number of 40ms frames due to 10ms per audio frame during feature extraction, with 4X subsampling performed by the encoder. 7 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations TDT config clean other total time rel speed-up RNNT 0-2 0-4 0-6 0-8 2.13 2.10 2.15 2.10 2.13 5.11 4.94 5.04 4.91 5.03 274 182 151 146 159 1.51X 1.81X 1.88X 1.79X Figure 5. Emission counts of blanks VS non-blanks in Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of either a blank symbol (red) or a non-blank symbol (blue) during the inference of Librispeech test-other datasets. Table 7. Batched inference for TDT ASR models, trained with loss sampling \u03c9 = 0.1. WER (%) on Librispeech test-clean and test-other. Batch-size=4. When different utterances in the same batch predict different durations, we take the minimum of those predictions and advance all utterances in the batch by that amount. 5.3. TDT Robustness to noise 5.2. TDT Batched Inference The main difficulty"}, {"question": " How do TDT models with different configurations utilize durations during inference?", "answer": " TDT models with [0-2] and [0-4] configurations fully utilize longer durations during inference, while [0-6] and [0-8] models have reduced frequencies of predicted long durations.", "ref_chunk": "which takes audio as input to detect user intents and extract the corresponding lex- ical fillers for detected entity slots (Bastianelli et al., 2020). An intent is composed of a scenario type and an action type, while slots and fillers are represented by key-value pairs. The ground-truth intents and slots of input are organized as a Python dictionary, represented as a Python string. The SICSF task is to predict this text based on the input audio. Experiments are conducted using the SLURP (Bastianelli et al., 2020) dataset, where intent accuracy and SLURP-F1 11https://github.com/VKCOM/YouTokenToMe 12https://paperswithcode.com/sota/ speech-to-text-translation-on-must-c-en-de. 6 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Model SpeechBrain ESPnet-SLU #params (M) 96 109 intent acc. 87.7 86.52 SLURP F1 76.19 76.91 rel. speedup N/A N/A RNNT TDT 0-2 TDT 0-4 TDT 0-6 TDT 0-8 119 119 119 119 119 88.53 87.12 89.85 89.28 90.07 79.41 79.43 80.03 80.61 79.90 1.17x 1.17x 1.28x 1.28x Table 6. Speech intent classification and slot filling on SLURP dataset. TDT vs RNNT: Relative speed-up against RNNT. Figure 4. Duration distribution during inference on Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of different types of durations during the inference of Librispeech test-other datasets. are used as the evaluation metric. Our baseline model is a Conformer Transducer model, ini- tialized from our pretrained ASR model 13. We also in- clude results from two state-of-the-art models from ESPNet- SLU (Arora et al., 2022) and SpeechBrain (Wang et al., 2021c) for comparison. Both ESPNet-SLU and Speech- Brain use HuBERT (Hsu et al., 2021) encoders pretrained on LibriLight-60k (Kahn et al., 2020), while ESPNet-SLU further finetunes the encoder on LibriSpeech before training on SLURP. For our TDTs, we use the same duration config- urations that contain maximum durations 2, 4, 6, and 8. Dif- ferent from our ASR and ST experiments that use \u03c3 = 0.05, here we use \u03c3 = 0.02 for all experiments since we found using \u03c3 = 0.05 may destabilize training for SLURP14. The results are shown in Table 6. While the RNNT base- line already has better performance than ESPNet-SLU and SpeechBrain baselines, TDT models with [0-4], [0-6], and [0-8] configurations achieve even better accuracy which makes the new state-of-the-art in the SICSF. In addition, the TDT [0-8] model is 1.28X faster in inference than RNNT.15 The results demonstrate not only the effectiveness but also the efficiency of TDT algorithm applied in SICSF tasks. We notice relatively smaller speed-up factors with TDT in this task. This could be explained by the much lower average audio-to-token-length ratio for SICSF tasks. For example, in ASR tasks, the typical ratio between audio length to text length is around 7:1, and the ratio is around 0.89:1 for the 13https://catalog.ngc.nvidia.com/orgs/ nvidia/teams/nemo/models/stt_en_conformer_ transducer_large 14This is caused by a much smaller ratio between the audio and text length of SLU datasets: the average ratio of audio to and text is 0.89:1 for SLURP, compared to around 5.5:1 for ASR for example. Since on average audio is shorter than text, setting \u03c3 too high, which encourages large duration outputs, will hurt training. A smaller \u03c3 alleviates the issue. 15SLURP has on average shorter audio than text . This means larger durations occur much less for SLURP, resulting in smaller speed-ups compared to ASR and ST. SLURP testset, with average text sequence being longer than audio sequence16. Nevertheless, we see a significant speed up, which also shows that TDT models can bring improvement even for scenarios where the audio sequence is longer than the text sequences. 5. Discussion 5.1. TDT Emission Analysis In this section, we investigate the output distribution of TDT models using Librispeech test-other dataset. First, we collect statistics on the duration predicted during decoding, with different TDT configurations (Fig. 4). For the baseline RNN-T, we treat blank and non-blank symbol emissions as having durations 1 and 0, respectively, since blank advances the t by one and non-blank does not. TDT models with configs [0-2] and [0-4] fully utilize longer durations during inference, with almost all of the durations predicted for the [0-2] model, and around 90% of durations for the [0-4] model have the maximum duration. For [0-6] and [0-8] models, the frequencies of predicted long durations are reduced. This is expected since our analysis shows the average ratio of audio length to text length for Librispeech test-other is 5.5:1, smaller than 6. Hence, it is not possible to always emit such long durations. Next, we collect the frequency of blank emissions vs non- blank emissions (Fig. 5). We can see that as the model incorporates longer durations, fewer and fewer blank emis- sions are produced with TDT models, while the number of non-blank emissions remains unchanged. TDT models with durations [0-6] and [0-8] have very few blank emissions, indicating that TDT models are close to the theoretical lower bound in terms of the least number of decoding steps. 16Text sequence is computed as the number of subword tokens, and audio sequence is computed as the number of 40ms frames due to 10ms per audio frame during feature extraction, with 4X subsampling performed by the encoder. 7 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations TDT config clean other total time rel speed-up RNNT 0-2 0-4 0-6 0-8 2.13 2.10 2.15 2.10 2.13 5.11 4.94 5.04 4.91 5.03 274 182 151 146 159 1.51X 1.81X 1.88X 1.79X Figure 5. Emission counts of blanks VS non-blanks in Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of either a blank symbol (red) or a non-blank symbol (blue) during the inference of Librispeech test-other datasets. Table 7. Batched inference for TDT ASR models, trained with loss sampling \u03c9 = 0.1. WER (%) on Librispeech test-clean and test-other. Batch-size=4. When different utterances in the same batch predict different durations, we take the minimum of those predictions and advance all utterances in the batch by that amount. 5.3. TDT Robustness to noise 5.2. TDT Batched Inference The main difficulty"}, {"question": " What does the frequency of blank emissions vs non-blank emissions show in the TDT models?", "answer": " The frequency shows that as the TDT models incorporate longer durations, fewer blank emissions are produced, while the number of non-blank emissions remains unchanged.", "ref_chunk": "which takes audio as input to detect user intents and extract the corresponding lex- ical fillers for detected entity slots (Bastianelli et al., 2020). An intent is composed of a scenario type and an action type, while slots and fillers are represented by key-value pairs. The ground-truth intents and slots of input are organized as a Python dictionary, represented as a Python string. The SICSF task is to predict this text based on the input audio. Experiments are conducted using the SLURP (Bastianelli et al., 2020) dataset, where intent accuracy and SLURP-F1 11https://github.com/VKCOM/YouTokenToMe 12https://paperswithcode.com/sota/ speech-to-text-translation-on-must-c-en-de. 6 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Model SpeechBrain ESPnet-SLU #params (M) 96 109 intent acc. 87.7 86.52 SLURP F1 76.19 76.91 rel. speedup N/A N/A RNNT TDT 0-2 TDT 0-4 TDT 0-6 TDT 0-8 119 119 119 119 119 88.53 87.12 89.85 89.28 90.07 79.41 79.43 80.03 80.61 79.90 1.17x 1.17x 1.28x 1.28x Table 6. Speech intent classification and slot filling on SLURP dataset. TDT vs RNNT: Relative speed-up against RNNT. Figure 4. Duration distribution during inference on Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of different types of durations during the inference of Librispeech test-other datasets. are used as the evaluation metric. Our baseline model is a Conformer Transducer model, ini- tialized from our pretrained ASR model 13. We also in- clude results from two state-of-the-art models from ESPNet- SLU (Arora et al., 2022) and SpeechBrain (Wang et al., 2021c) for comparison. Both ESPNet-SLU and Speech- Brain use HuBERT (Hsu et al., 2021) encoders pretrained on LibriLight-60k (Kahn et al., 2020), while ESPNet-SLU further finetunes the encoder on LibriSpeech before training on SLURP. For our TDTs, we use the same duration config- urations that contain maximum durations 2, 4, 6, and 8. Dif- ferent from our ASR and ST experiments that use \u03c3 = 0.05, here we use \u03c3 = 0.02 for all experiments since we found using \u03c3 = 0.05 may destabilize training for SLURP14. The results are shown in Table 6. While the RNNT base- line already has better performance than ESPNet-SLU and SpeechBrain baselines, TDT models with [0-4], [0-6], and [0-8] configurations achieve even better accuracy which makes the new state-of-the-art in the SICSF. In addition, the TDT [0-8] model is 1.28X faster in inference than RNNT.15 The results demonstrate not only the effectiveness but also the efficiency of TDT algorithm applied in SICSF tasks. We notice relatively smaller speed-up factors with TDT in this task. This could be explained by the much lower average audio-to-token-length ratio for SICSF tasks. For example, in ASR tasks, the typical ratio between audio length to text length is around 7:1, and the ratio is around 0.89:1 for the 13https://catalog.ngc.nvidia.com/orgs/ nvidia/teams/nemo/models/stt_en_conformer_ transducer_large 14This is caused by a much smaller ratio between the audio and text length of SLU datasets: the average ratio of audio to and text is 0.89:1 for SLURP, compared to around 5.5:1 for ASR for example. Since on average audio is shorter than text, setting \u03c3 too high, which encourages large duration outputs, will hurt training. A smaller \u03c3 alleviates the issue. 15SLURP has on average shorter audio than text . This means larger durations occur much less for SLURP, resulting in smaller speed-ups compared to ASR and ST. SLURP testset, with average text sequence being longer than audio sequence16. Nevertheless, we see a significant speed up, which also shows that TDT models can bring improvement even for scenarios where the audio sequence is longer than the text sequences. 5. Discussion 5.1. TDT Emission Analysis In this section, we investigate the output distribution of TDT models using Librispeech test-other dataset. First, we collect statistics on the duration predicted during decoding, with different TDT configurations (Fig. 4). For the baseline RNN-T, we treat blank and non-blank symbol emissions as having durations 1 and 0, respectively, since blank advances the t by one and non-blank does not. TDT models with configs [0-2] and [0-4] fully utilize longer durations during inference, with almost all of the durations predicted for the [0-2] model, and around 90% of durations for the [0-4] model have the maximum duration. For [0-6] and [0-8] models, the frequencies of predicted long durations are reduced. This is expected since our analysis shows the average ratio of audio length to text length for Librispeech test-other is 5.5:1, smaller than 6. Hence, it is not possible to always emit such long durations. Next, we collect the frequency of blank emissions vs non- blank emissions (Fig. 5). We can see that as the model incorporates longer durations, fewer and fewer blank emis- sions are produced with TDT models, while the number of non-blank emissions remains unchanged. TDT models with durations [0-6] and [0-8] have very few blank emissions, indicating that TDT models are close to the theoretical lower bound in terms of the least number of decoding steps. 16Text sequence is computed as the number of subword tokens, and audio sequence is computed as the number of 40ms frames due to 10ms per audio frame during feature extraction, with 4X subsampling performed by the encoder. 7 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations TDT config clean other total time rel speed-up RNNT 0-2 0-4 0-6 0-8 2.13 2.10 2.15 2.10 2.13 5.11 4.94 5.04 4.91 5.03 274 182 151 146 159 1.51X 1.81X 1.88X 1.79X Figure 5. Emission counts of blanks VS non-blanks in Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of either a blank symbol (red) or a non-blank symbol (blue) during the inference of Librispeech test-other datasets. Table 7. Batched inference for TDT ASR models, trained with loss sampling \u03c9 = 0.1. WER (%) on Librispeech test-clean and test-other. Batch-size=4. When different utterances in the same batch predict different durations, we take the minimum of those predictions and advance all utterances in the batch by that amount. 5.3. TDT Robustness to noise 5.2. TDT Batched Inference The main difficulty"}], "doc_text": "which takes audio as input to detect user intents and extract the corresponding lex- ical fillers for detected entity slots (Bastianelli et al., 2020). An intent is composed of a scenario type and an action type, while slots and fillers are represented by key-value pairs. The ground-truth intents and slots of input are organized as a Python dictionary, represented as a Python string. The SICSF task is to predict this text based on the input audio. Experiments are conducted using the SLURP (Bastianelli et al., 2020) dataset, where intent accuracy and SLURP-F1 11https://github.com/VKCOM/YouTokenToMe 12https://paperswithcode.com/sota/ speech-to-text-translation-on-must-c-en-de. 6 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Model SpeechBrain ESPnet-SLU #params (M) 96 109 intent acc. 87.7 86.52 SLURP F1 76.19 76.91 rel. speedup N/A N/A RNNT TDT 0-2 TDT 0-4 TDT 0-6 TDT 0-8 119 119 119 119 119 88.53 87.12 89.85 89.28 90.07 79.41 79.43 80.03 80.61 79.90 1.17x 1.17x 1.28x 1.28x Table 6. Speech intent classification and slot filling on SLURP dataset. TDT vs RNNT: Relative speed-up against RNNT. Figure 4. Duration distribution during inference on Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of different types of durations during the inference of Librispeech test-other datasets. are used as the evaluation metric. Our baseline model is a Conformer Transducer model, ini- tialized from our pretrained ASR model 13. We also in- clude results from two state-of-the-art models from ESPNet- SLU (Arora et al., 2022) and SpeechBrain (Wang et al., 2021c) for comparison. Both ESPNet-SLU and Speech- Brain use HuBERT (Hsu et al., 2021) encoders pretrained on LibriLight-60k (Kahn et al., 2020), while ESPNet-SLU further finetunes the encoder on LibriSpeech before training on SLURP. For our TDTs, we use the same duration config- urations that contain maximum durations 2, 4, 6, and 8. Dif- ferent from our ASR and ST experiments that use \u03c3 = 0.05, here we use \u03c3 = 0.02 for all experiments since we found using \u03c3 = 0.05 may destabilize training for SLURP14. The results are shown in Table 6. While the RNNT base- line already has better performance than ESPNet-SLU and SpeechBrain baselines, TDT models with [0-4], [0-6], and [0-8] configurations achieve even better accuracy which makes the new state-of-the-art in the SICSF. In addition, the TDT [0-8] model is 1.28X faster in inference than RNNT.15 The results demonstrate not only the effectiveness but also the efficiency of TDT algorithm applied in SICSF tasks. We notice relatively smaller speed-up factors with TDT in this task. This could be explained by the much lower average audio-to-token-length ratio for SICSF tasks. For example, in ASR tasks, the typical ratio between audio length to text length is around 7:1, and the ratio is around 0.89:1 for the 13https://catalog.ngc.nvidia.com/orgs/ nvidia/teams/nemo/models/stt_en_conformer_ transducer_large 14This is caused by a much smaller ratio between the audio and text length of SLU datasets: the average ratio of audio to and text is 0.89:1 for SLURP, compared to around 5.5:1 for ASR for example. Since on average audio is shorter than text, setting \u03c3 too high, which encourages large duration outputs, will hurt training. A smaller \u03c3 alleviates the issue. 15SLURP has on average shorter audio than text . This means larger durations occur much less for SLURP, resulting in smaller speed-ups compared to ASR and ST. SLURP testset, with average text sequence being longer than audio sequence16. Nevertheless, we see a significant speed up, which also shows that TDT models can bring improvement even for scenarios where the audio sequence is longer than the text sequences. 5. Discussion 5.1. TDT Emission Analysis In this section, we investigate the output distribution of TDT models using Librispeech test-other dataset. First, we collect statistics on the duration predicted during decoding, with different TDT configurations (Fig. 4). For the baseline RNN-T, we treat blank and non-blank symbol emissions as having durations 1 and 0, respectively, since blank advances the t by one and non-blank does not. TDT models with configs [0-2] and [0-4] fully utilize longer durations during inference, with almost all of the durations predicted for the [0-2] model, and around 90% of durations for the [0-4] model have the maximum duration. For [0-6] and [0-8] models, the frequencies of predicted long durations are reduced. This is expected since our analysis shows the average ratio of audio length to text length for Librispeech test-other is 5.5:1, smaller than 6. Hence, it is not possible to always emit such long durations. Next, we collect the frequency of blank emissions vs non- blank emissions (Fig. 5). We can see that as the model incorporates longer durations, fewer and fewer blank emis- sions are produced with TDT models, while the number of non-blank emissions remains unchanged. TDT models with durations [0-6] and [0-8] have very few blank emissions, indicating that TDT models are close to the theoretical lower bound in terms of the least number of decoding steps. 16Text sequence is computed as the number of subword tokens, and audio sequence is computed as the number of 40ms frames due to 10ms per audio frame during feature extraction, with 4X subsampling performed by the encoder. 7 Efficient Sequence Transduction by Jointly Predicting Tokens and Durations TDT config clean other total time rel speed-up RNNT 0-2 0-4 0-6 0-8 2.13 2.10 2.15 2.10 2.13 5.11 4.94 5.04 4.91 5.03 274 182 151 146 159 1.51X 1.81X 1.88X 1.79X Figure 5. Emission counts of blanks VS non-blanks in Librispeech test-other. The model is trained with 4X subsampling. The y-axis shows the number of emissions of either a blank symbol (red) or a non-blank symbol (blue) during the inference of Librispeech test-other datasets. Table 7. Batched inference for TDT ASR models, trained with loss sampling \u03c9 = 0.1. WER (%) on Librispeech test-clean and test-other. Batch-size=4. When different utterances in the same batch predict different durations, we take the minimum of those predictions and advance all utterances in the batch by that amount. 5.3. TDT Robustness to noise 5.2. TDT Batched Inference The main difficulty"}