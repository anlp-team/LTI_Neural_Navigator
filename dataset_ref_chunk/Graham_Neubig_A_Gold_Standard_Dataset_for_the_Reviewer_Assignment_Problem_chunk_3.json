{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_A_Gold_Standard_Dataset_for_the_Reviewer_Assignment_Problem_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the two main steps involved in the automated assignment stage?,answer: The two main steps involved in the automated assignment stage are computing similarity scores and allocating reviewers to submissions based on these scores.", "ref_chunk": "a result, di\ufb00erent conferences use di\ufb00erent strategies (Shah et al., 2018; Leyton-Brown et al., 2022) and there is a lack of empirical or theoretical evidence that would guide venue organizers in their decisions. Automation of the assignment stage At a high level, automated assignment stage consists of two steps: \ufb01rst, similarity scores are computed; second, reviewers are allocated to submissions such that some notion of assignment quality (formulated in terms of the similarity scores) is maximized. In this work, we focus on the \ufb01rst step of the process. However, for completeness, we now mention several works that design algorithms for the second step. A popular notion of assignment quality is the cumulative similarity score, that is, the sum of the similarity scores across all assigned reviewers and papers. An algorithm pursuing such an objective is implemented in the widely employed TPMS assignment algorithm (Charlin and Zemel, 2013) and similar ideas are explored in many papers (Goldsmith and Sloan, 2007; Tang et al., 2010; Long et al., 2013). While the cumulative objective is a natural choice, it has been noted that it may discriminate against certain submissions by allo- cating all irrelevant reviewers to a subset of submissions, even when a more balanced assignment exists (Garg et al., 2010). Thus, a number of works has explored the idea of assignment fairness, aiming at producing more balanced assignments (Kobren et al., 2019; Stelmakh et al., 2021). Finally, other works explore the ideas of envy-freeness (Tan et al., 2021; Payan, 2022), resistance to lone-wolf strategic behavior (Xu et al., 2019; Dhull et al., 2022), and encouraging various types of diversity (Li et al., 2015; Leyton-Brown et al., 2022). 3 Data collection pipeline In this section, we describe the process of data collection. This work was performed under the approval of the Institutional Review Board (IRB) of Carnegie Mellon University. Gold standard data In this study, we aim to collect a dataset of self-evaluations of reviewers\u2019 expertise that satis\ufb01es two desiderata: (D1) The dataset should comprise evaluations of expertise for papers participants read to a reasonable extent. (D2) The dataset should be released publicly without disclosing any sensitive information. Let us now discuss our approach to recruiting participants and obtaining accurate estimates of their expertise in reviewing papers included in the dataset. 4 Participant recruiting We recruited participants using a combination of several channels that are typi- cally employed to recruit reviewers for computer science conferences: Mailing lists. First, we sent recruiting emails to relevant mailing lists of several universities and research departments of companies Social media. Second, two authors of this paper posted a call for participation on their Twitter accounts \u2022 Personal communication. Third, we sent personal invites to researchers from our professional network We had a screening criterion requiring that prospective participants have at least one paper published in the broad area of computer science. Overall, for the version of the dataset we release in this paper, we managed to recruit 58 participants, all of whom passed the screening. Expertise evaluations The key idea of our approach to expertise evaluation is to ask participants to evaluate their expertise in reviewing papers they read in the past. After reading a paper, a researcher is in the best possible position to evaluate whether they have the right background\u2014both in terms of the techniques used in the paper and in terms of the broader research area of the paper\u2014to judge the quality of the paper. With this motivation, we asked participants to: Recall 5-10 papers in their broad research area that they read to a reasonable extent in the last year and tell us their expertise in reviewing these papers. In more detail, the choice of papers was constrained by two minor conditions: \u2022 The papers reported by a participant should not be authored by them \u2022 The papers reported by a participant should be freely available online In addition to these constraints, we gave several recommendations to the participants in order to make the dataset more diverse and useful for the research purposes: First, we asked participants to choose papers that cover the whole spectrum of their expertise with some papers being well-separated (e.g., very high expertise and very low expertise) and some papers being nearly-tied (e.g., two high-expertise papers). Second, we recommended participants to avoid ties in their evaluations. To help participants comply with this recommendation, we implemented evaluation on a scale of 1 (\u201cI am not quali\ufb01ed to review this paper\u201d) to 5 (\u201cI have background necessary to evaluate all the aspects of the paper\u201d) with a 0.25 step size, enabling participants to report papers with small di\ufb00erences in expertise. Third, we asked participants to come up with papers that they think may be tricky for existing similarity- computation algorithms. For this, we relied on the commonsense understanding and did not instruct participants on the inner-workings of these algorithms. Overall, the time needed for participants to contribute to the dataset was estimated to be 5\u201310 minutes. The full instructions of the survey are available in Appendix A. Data release Following the procedure outlined above, we collected responses from 58 researchers. These responses constitute an initial version of the dataset that we release in this work. Each entry in the dataset corresponds to a participant and comprises evaluations of their expertise in reviewing papers of their choice. For each paper and each participant, we provide representations that are su\ufb03cient to start working on our dataset: Participant. Each participant is represented by their Semantic Scholar ID and complete bibliography crawled from Semantic Scholar on May 1, 2022. Paper. Each paper, including papers from participants\u2019 bibliographies, is represented by its Semantic Scholar ID, title, abstract, list of authors, publication year, and arXiv identi\ufb01er. Additionally, papers from participants\u2019 responses are supplied with links to freely available PDFs (whenever available). 5 Total number of participants: 58 Characteristic Quantity Value Gender % Male 78 Affiliation % Carnegie Mellon University 40 Country % USA 74 Position % PhD student % Faculty % Post-PhD (non-faculty) 45 28 12 Experience"}, {"question": " What is the popular notion of assignment quality mentioned in the text?,answer: The popular notion of assignment quality mentioned is the cumulative similarity score, which is the sum of similarity scores across all assigned reviewers and papers.", "ref_chunk": "a result, di\ufb00erent conferences use di\ufb00erent strategies (Shah et al., 2018; Leyton-Brown et al., 2022) and there is a lack of empirical or theoretical evidence that would guide venue organizers in their decisions. Automation of the assignment stage At a high level, automated assignment stage consists of two steps: \ufb01rst, similarity scores are computed; second, reviewers are allocated to submissions such that some notion of assignment quality (formulated in terms of the similarity scores) is maximized. In this work, we focus on the \ufb01rst step of the process. However, for completeness, we now mention several works that design algorithms for the second step. A popular notion of assignment quality is the cumulative similarity score, that is, the sum of the similarity scores across all assigned reviewers and papers. An algorithm pursuing such an objective is implemented in the widely employed TPMS assignment algorithm (Charlin and Zemel, 2013) and similar ideas are explored in many papers (Goldsmith and Sloan, 2007; Tang et al., 2010; Long et al., 2013). While the cumulative objective is a natural choice, it has been noted that it may discriminate against certain submissions by allo- cating all irrelevant reviewers to a subset of submissions, even when a more balanced assignment exists (Garg et al., 2010). Thus, a number of works has explored the idea of assignment fairness, aiming at producing more balanced assignments (Kobren et al., 2019; Stelmakh et al., 2021). Finally, other works explore the ideas of envy-freeness (Tan et al., 2021; Payan, 2022), resistance to lone-wolf strategic behavior (Xu et al., 2019; Dhull et al., 2022), and encouraging various types of diversity (Li et al., 2015; Leyton-Brown et al., 2022). 3 Data collection pipeline In this section, we describe the process of data collection. This work was performed under the approval of the Institutional Review Board (IRB) of Carnegie Mellon University. Gold standard data In this study, we aim to collect a dataset of self-evaluations of reviewers\u2019 expertise that satis\ufb01es two desiderata: (D1) The dataset should comprise evaluations of expertise for papers participants read to a reasonable extent. (D2) The dataset should be released publicly without disclosing any sensitive information. Let us now discuss our approach to recruiting participants and obtaining accurate estimates of their expertise in reviewing papers included in the dataset. 4 Participant recruiting We recruited participants using a combination of several channels that are typi- cally employed to recruit reviewers for computer science conferences: Mailing lists. First, we sent recruiting emails to relevant mailing lists of several universities and research departments of companies Social media. Second, two authors of this paper posted a call for participation on their Twitter accounts \u2022 Personal communication. Third, we sent personal invites to researchers from our professional network We had a screening criterion requiring that prospective participants have at least one paper published in the broad area of computer science. Overall, for the version of the dataset we release in this paper, we managed to recruit 58 participants, all of whom passed the screening. Expertise evaluations The key idea of our approach to expertise evaluation is to ask participants to evaluate their expertise in reviewing papers they read in the past. After reading a paper, a researcher is in the best possible position to evaluate whether they have the right background\u2014both in terms of the techniques used in the paper and in terms of the broader research area of the paper\u2014to judge the quality of the paper. With this motivation, we asked participants to: Recall 5-10 papers in their broad research area that they read to a reasonable extent in the last year and tell us their expertise in reviewing these papers. In more detail, the choice of papers was constrained by two minor conditions: \u2022 The papers reported by a participant should not be authored by them \u2022 The papers reported by a participant should be freely available online In addition to these constraints, we gave several recommendations to the participants in order to make the dataset more diverse and useful for the research purposes: First, we asked participants to choose papers that cover the whole spectrum of their expertise with some papers being well-separated (e.g., very high expertise and very low expertise) and some papers being nearly-tied (e.g., two high-expertise papers). Second, we recommended participants to avoid ties in their evaluations. To help participants comply with this recommendation, we implemented evaluation on a scale of 1 (\u201cI am not quali\ufb01ed to review this paper\u201d) to 5 (\u201cI have background necessary to evaluate all the aspects of the paper\u201d) with a 0.25 step size, enabling participants to report papers with small di\ufb00erences in expertise. Third, we asked participants to come up with papers that they think may be tricky for existing similarity- computation algorithms. For this, we relied on the commonsense understanding and did not instruct participants on the inner-workings of these algorithms. Overall, the time needed for participants to contribute to the dataset was estimated to be 5\u201310 minutes. The full instructions of the survey are available in Appendix A. Data release Following the procedure outlined above, we collected responses from 58 researchers. These responses constitute an initial version of the dataset that we release in this work. Each entry in the dataset corresponds to a participant and comprises evaluations of their expertise in reviewing papers of their choice. For each paper and each participant, we provide representations that are su\ufb03cient to start working on our dataset: Participant. Each participant is represented by their Semantic Scholar ID and complete bibliography crawled from Semantic Scholar on May 1, 2022. Paper. Each paper, including papers from participants\u2019 bibliographies, is represented by its Semantic Scholar ID, title, abstract, list of authors, publication year, and arXiv identi\ufb01er. Additionally, papers from participants\u2019 responses are supplied with links to freely available PDFs (whenever available). 5 Total number of participants: 58 Characteristic Quantity Value Gender % Male 78 Affiliation % Carnegie Mellon University 40 Country % USA 74 Position % PhD student % Faculty % Post-PhD (non-faculty) 45 28 12 Experience"}, {"question": " What is one potential issue with the cumulative objective approach to assignment quality?,answer: One potential issue with the cumulative objective approach is that it may discriminate against certain submissions by allocating all irrelevant reviewers to a subset of submissions, even when a more balanced assignment is possible.", "ref_chunk": "a result, di\ufb00erent conferences use di\ufb00erent strategies (Shah et al., 2018; Leyton-Brown et al., 2022) and there is a lack of empirical or theoretical evidence that would guide venue organizers in their decisions. Automation of the assignment stage At a high level, automated assignment stage consists of two steps: \ufb01rst, similarity scores are computed; second, reviewers are allocated to submissions such that some notion of assignment quality (formulated in terms of the similarity scores) is maximized. In this work, we focus on the \ufb01rst step of the process. However, for completeness, we now mention several works that design algorithms for the second step. A popular notion of assignment quality is the cumulative similarity score, that is, the sum of the similarity scores across all assigned reviewers and papers. An algorithm pursuing such an objective is implemented in the widely employed TPMS assignment algorithm (Charlin and Zemel, 2013) and similar ideas are explored in many papers (Goldsmith and Sloan, 2007; Tang et al., 2010; Long et al., 2013). While the cumulative objective is a natural choice, it has been noted that it may discriminate against certain submissions by allo- cating all irrelevant reviewers to a subset of submissions, even when a more balanced assignment exists (Garg et al., 2010). Thus, a number of works has explored the idea of assignment fairness, aiming at producing more balanced assignments (Kobren et al., 2019; Stelmakh et al., 2021). Finally, other works explore the ideas of envy-freeness (Tan et al., 2021; Payan, 2022), resistance to lone-wolf strategic behavior (Xu et al., 2019; Dhull et al., 2022), and encouraging various types of diversity (Li et al., 2015; Leyton-Brown et al., 2022). 3 Data collection pipeline In this section, we describe the process of data collection. This work was performed under the approval of the Institutional Review Board (IRB) of Carnegie Mellon University. Gold standard data In this study, we aim to collect a dataset of self-evaluations of reviewers\u2019 expertise that satis\ufb01es two desiderata: (D1) The dataset should comprise evaluations of expertise for papers participants read to a reasonable extent. (D2) The dataset should be released publicly without disclosing any sensitive information. Let us now discuss our approach to recruiting participants and obtaining accurate estimates of their expertise in reviewing papers included in the dataset. 4 Participant recruiting We recruited participants using a combination of several channels that are typi- cally employed to recruit reviewers for computer science conferences: Mailing lists. First, we sent recruiting emails to relevant mailing lists of several universities and research departments of companies Social media. Second, two authors of this paper posted a call for participation on their Twitter accounts \u2022 Personal communication. Third, we sent personal invites to researchers from our professional network We had a screening criterion requiring that prospective participants have at least one paper published in the broad area of computer science. Overall, for the version of the dataset we release in this paper, we managed to recruit 58 participants, all of whom passed the screening. Expertise evaluations The key idea of our approach to expertise evaluation is to ask participants to evaluate their expertise in reviewing papers they read in the past. After reading a paper, a researcher is in the best possible position to evaluate whether they have the right background\u2014both in terms of the techniques used in the paper and in terms of the broader research area of the paper\u2014to judge the quality of the paper. With this motivation, we asked participants to: Recall 5-10 papers in their broad research area that they read to a reasonable extent in the last year and tell us their expertise in reviewing these papers. In more detail, the choice of papers was constrained by two minor conditions: \u2022 The papers reported by a participant should not be authored by them \u2022 The papers reported by a participant should be freely available online In addition to these constraints, we gave several recommendations to the participants in order to make the dataset more diverse and useful for the research purposes: First, we asked participants to choose papers that cover the whole spectrum of their expertise with some papers being well-separated (e.g., very high expertise and very low expertise) and some papers being nearly-tied (e.g., two high-expertise papers). Second, we recommended participants to avoid ties in their evaluations. To help participants comply with this recommendation, we implemented evaluation on a scale of 1 (\u201cI am not quali\ufb01ed to review this paper\u201d) to 5 (\u201cI have background necessary to evaluate all the aspects of the paper\u201d) with a 0.25 step size, enabling participants to report papers with small di\ufb00erences in expertise. Third, we asked participants to come up with papers that they think may be tricky for existing similarity- computation algorithms. For this, we relied on the commonsense understanding and did not instruct participants on the inner-workings of these algorithms. Overall, the time needed for participants to contribute to the dataset was estimated to be 5\u201310 minutes. The full instructions of the survey are available in Appendix A. Data release Following the procedure outlined above, we collected responses from 58 researchers. These responses constitute an initial version of the dataset that we release in this work. Each entry in the dataset corresponds to a participant and comprises evaluations of their expertise in reviewing papers of their choice. For each paper and each participant, we provide representations that are su\ufb03cient to start working on our dataset: Participant. Each participant is represented by their Semantic Scholar ID and complete bibliography crawled from Semantic Scholar on May 1, 2022. Paper. Each paper, including papers from participants\u2019 bibliographies, is represented by its Semantic Scholar ID, title, abstract, list of authors, publication year, and arXiv identi\ufb01er. Additionally, papers from participants\u2019 responses are supplied with links to freely available PDFs (whenever available). 5 Total number of participants: 58 Characteristic Quantity Value Gender % Male 78 Affiliation % Carnegie Mellon University 40 Country % USA 74 Position % PhD student % Faculty % Post-PhD (non-faculty) 45 28 12 Experience"}, {"question": " What is the aim of works focusing on assignment fairness?,answer: The aim of works focusing on assignment fairness is to produce more balanced assignments.", "ref_chunk": "a result, di\ufb00erent conferences use di\ufb00erent strategies (Shah et al., 2018; Leyton-Brown et al., 2022) and there is a lack of empirical or theoretical evidence that would guide venue organizers in their decisions. Automation of the assignment stage At a high level, automated assignment stage consists of two steps: \ufb01rst, similarity scores are computed; second, reviewers are allocated to submissions such that some notion of assignment quality (formulated in terms of the similarity scores) is maximized. In this work, we focus on the \ufb01rst step of the process. However, for completeness, we now mention several works that design algorithms for the second step. A popular notion of assignment quality is the cumulative similarity score, that is, the sum of the similarity scores across all assigned reviewers and papers. An algorithm pursuing such an objective is implemented in the widely employed TPMS assignment algorithm (Charlin and Zemel, 2013) and similar ideas are explored in many papers (Goldsmith and Sloan, 2007; Tang et al., 2010; Long et al., 2013). While the cumulative objective is a natural choice, it has been noted that it may discriminate against certain submissions by allo- cating all irrelevant reviewers to a subset of submissions, even when a more balanced assignment exists (Garg et al., 2010). Thus, a number of works has explored the idea of assignment fairness, aiming at producing more balanced assignments (Kobren et al., 2019; Stelmakh et al., 2021). Finally, other works explore the ideas of envy-freeness (Tan et al., 2021; Payan, 2022), resistance to lone-wolf strategic behavior (Xu et al., 2019; Dhull et al., 2022), and encouraging various types of diversity (Li et al., 2015; Leyton-Brown et al., 2022). 3 Data collection pipeline In this section, we describe the process of data collection. This work was performed under the approval of the Institutional Review Board (IRB) of Carnegie Mellon University. Gold standard data In this study, we aim to collect a dataset of self-evaluations of reviewers\u2019 expertise that satis\ufb01es two desiderata: (D1) The dataset should comprise evaluations of expertise for papers participants read to a reasonable extent. (D2) The dataset should be released publicly without disclosing any sensitive information. Let us now discuss our approach to recruiting participants and obtaining accurate estimates of their expertise in reviewing papers included in the dataset. 4 Participant recruiting We recruited participants using a combination of several channels that are typi- cally employed to recruit reviewers for computer science conferences: Mailing lists. First, we sent recruiting emails to relevant mailing lists of several universities and research departments of companies Social media. Second, two authors of this paper posted a call for participation on their Twitter accounts \u2022 Personal communication. Third, we sent personal invites to researchers from our professional network We had a screening criterion requiring that prospective participants have at least one paper published in the broad area of computer science. Overall, for the version of the dataset we release in this paper, we managed to recruit 58 participants, all of whom passed the screening. Expertise evaluations The key idea of our approach to expertise evaluation is to ask participants to evaluate their expertise in reviewing papers they read in the past. After reading a paper, a researcher is in the best possible position to evaluate whether they have the right background\u2014both in terms of the techniques used in the paper and in terms of the broader research area of the paper\u2014to judge the quality of the paper. With this motivation, we asked participants to: Recall 5-10 papers in their broad research area that they read to a reasonable extent in the last year and tell us their expertise in reviewing these papers. In more detail, the choice of papers was constrained by two minor conditions: \u2022 The papers reported by a participant should not be authored by them \u2022 The papers reported by a participant should be freely available online In addition to these constraints, we gave several recommendations to the participants in order to make the dataset more diverse and useful for the research purposes: First, we asked participants to choose papers that cover the whole spectrum of their expertise with some papers being well-separated (e.g., very high expertise and very low expertise) and some papers being nearly-tied (e.g., two high-expertise papers). Second, we recommended participants to avoid ties in their evaluations. To help participants comply with this recommendation, we implemented evaluation on a scale of 1 (\u201cI am not quali\ufb01ed to review this paper\u201d) to 5 (\u201cI have background necessary to evaluate all the aspects of the paper\u201d) with a 0.25 step size, enabling participants to report papers with small di\ufb00erences in expertise. Third, we asked participants to come up with papers that they think may be tricky for existing similarity- computation algorithms. For this, we relied on the commonsense understanding and did not instruct participants on the inner-workings of these algorithms. Overall, the time needed for participants to contribute to the dataset was estimated to be 5\u201310 minutes. The full instructions of the survey are available in Appendix A. Data release Following the procedure outlined above, we collected responses from 58 researchers. These responses constitute an initial version of the dataset that we release in this work. Each entry in the dataset corresponds to a participant and comprises evaluations of their expertise in reviewing papers of their choice. For each paper and each participant, we provide representations that are su\ufb03cient to start working on our dataset: Participant. Each participant is represented by their Semantic Scholar ID and complete bibliography crawled from Semantic Scholar on May 1, 2022. Paper. Each paper, including papers from participants\u2019 bibliographies, is represented by its Semantic Scholar ID, title, abstract, list of authors, publication year, and arXiv identi\ufb01er. Additionally, papers from participants\u2019 responses are supplied with links to freely available PDFs (whenever available). 5 Total number of participants: 58 Characteristic Quantity Value Gender % Male 78 Affiliation % Carnegie Mellon University 40 Country % USA 74 Position % PhD student % Faculty % Post-PhD (non-faculty) 45 28 12 Experience"}, {"question": " What concepts do works exploring envy-freeness focus on?,answer: Works exploring envy-freeness focus on the idea of ensuring fairness and impartiality in assignments.", "ref_chunk": "a result, di\ufb00erent conferences use di\ufb00erent strategies (Shah et al., 2018; Leyton-Brown et al., 2022) and there is a lack of empirical or theoretical evidence that would guide venue organizers in their decisions. Automation of the assignment stage At a high level, automated assignment stage consists of two steps: \ufb01rst, similarity scores are computed; second, reviewers are allocated to submissions such that some notion of assignment quality (formulated in terms of the similarity scores) is maximized. In this work, we focus on the \ufb01rst step of the process. However, for completeness, we now mention several works that design algorithms for the second step. A popular notion of assignment quality is the cumulative similarity score, that is, the sum of the similarity scores across all assigned reviewers and papers. An algorithm pursuing such an objective is implemented in the widely employed TPMS assignment algorithm (Charlin and Zemel, 2013) and similar ideas are explored in many papers (Goldsmith and Sloan, 2007; Tang et al., 2010; Long et al., 2013). While the cumulative objective is a natural choice, it has been noted that it may discriminate against certain submissions by allo- cating all irrelevant reviewers to a subset of submissions, even when a more balanced assignment exists (Garg et al., 2010). Thus, a number of works has explored the idea of assignment fairness, aiming at producing more balanced assignments (Kobren et al., 2019; Stelmakh et al., 2021). Finally, other works explore the ideas of envy-freeness (Tan et al., 2021; Payan, 2022), resistance to lone-wolf strategic behavior (Xu et al., 2019; Dhull et al., 2022), and encouraging various types of diversity (Li et al., 2015; Leyton-Brown et al., 2022). 3 Data collection pipeline In this section, we describe the process of data collection. This work was performed under the approval of the Institutional Review Board (IRB) of Carnegie Mellon University. Gold standard data In this study, we aim to collect a dataset of self-evaluations of reviewers\u2019 expertise that satis\ufb01es two desiderata: (D1) The dataset should comprise evaluations of expertise for papers participants read to a reasonable extent. (D2) The dataset should be released publicly without disclosing any sensitive information. Let us now discuss our approach to recruiting participants and obtaining accurate estimates of their expertise in reviewing papers included in the dataset. 4 Participant recruiting We recruited participants using a combination of several channels that are typi- cally employed to recruit reviewers for computer science conferences: Mailing lists. First, we sent recruiting emails to relevant mailing lists of several universities and research departments of companies Social media. Second, two authors of this paper posted a call for participation on their Twitter accounts \u2022 Personal communication. Third, we sent personal invites to researchers from our professional network We had a screening criterion requiring that prospective participants have at least one paper published in the broad area of computer science. Overall, for the version of the dataset we release in this paper, we managed to recruit 58 participants, all of whom passed the screening. Expertise evaluations The key idea of our approach to expertise evaluation is to ask participants to evaluate their expertise in reviewing papers they read in the past. After reading a paper, a researcher is in the best possible position to evaluate whether they have the right background\u2014both in terms of the techniques used in the paper and in terms of the broader research area of the paper\u2014to judge the quality of the paper. With this motivation, we asked participants to: Recall 5-10 papers in their broad research area that they read to a reasonable extent in the last year and tell us their expertise in reviewing these papers. In more detail, the choice of papers was constrained by two minor conditions: \u2022 The papers reported by a participant should not be authored by them \u2022 The papers reported by a participant should be freely available online In addition to these constraints, we gave several recommendations to the participants in order to make the dataset more diverse and useful for the research purposes: First, we asked participants to choose papers that cover the whole spectrum of their expertise with some papers being well-separated (e.g., very high expertise and very low expertise) and some papers being nearly-tied (e.g., two high-expertise papers). Second, we recommended participants to avoid ties in their evaluations. To help participants comply with this recommendation, we implemented evaluation on a scale of 1 (\u201cI am not quali\ufb01ed to review this paper\u201d) to 5 (\u201cI have background necessary to evaluate all the aspects of the paper\u201d) with a 0.25 step size, enabling participants to report papers with small di\ufb00erences in expertise. Third, we asked participants to come up with papers that they think may be tricky for existing similarity- computation algorithms. For this, we relied on the commonsense understanding and did not instruct participants on the inner-workings of these algorithms. Overall, the time needed for participants to contribute to the dataset was estimated to be 5\u201310 minutes. The full instructions of the survey are available in Appendix A. Data release Following the procedure outlined above, we collected responses from 58 researchers. These responses constitute an initial version of the dataset that we release in this work. Each entry in the dataset corresponds to a participant and comprises evaluations of their expertise in reviewing papers of their choice. For each paper and each participant, we provide representations that are su\ufb03cient to start working on our dataset: Participant. Each participant is represented by their Semantic Scholar ID and complete bibliography crawled from Semantic Scholar on May 1, 2022. Paper. Each paper, including papers from participants\u2019 bibliographies, is represented by its Semantic Scholar ID, title, abstract, list of authors, publication year, and arXiv identi\ufb01er. Additionally, papers from participants\u2019 responses are supplied with links to freely available PDFs (whenever available). 5 Total number of participants: 58 Characteristic Quantity Value Gender % Male 78 Affiliation % Carnegie Mellon University 40 Country % USA 74 Position % PhD student % Faculty % Post-PhD (non-faculty) 45 28 12 Experience"}, {"question": " What is the key idea behind the expertise evaluation approach mentioned in the text?,answer: The key idea behind the expertise evaluation approach is asking participants to evaluate their expertise in reviewing papers they have read in the past.", "ref_chunk": "a result, di\ufb00erent conferences use di\ufb00erent strategies (Shah et al., 2018; Leyton-Brown et al., 2022) and there is a lack of empirical or theoretical evidence that would guide venue organizers in their decisions. Automation of the assignment stage At a high level, automated assignment stage consists of two steps: \ufb01rst, similarity scores are computed; second, reviewers are allocated to submissions such that some notion of assignment quality (formulated in terms of the similarity scores) is maximized. In this work, we focus on the \ufb01rst step of the process. However, for completeness, we now mention several works that design algorithms for the second step. A popular notion of assignment quality is the cumulative similarity score, that is, the sum of the similarity scores across all assigned reviewers and papers. An algorithm pursuing such an objective is implemented in the widely employed TPMS assignment algorithm (Charlin and Zemel, 2013) and similar ideas are explored in many papers (Goldsmith and Sloan, 2007; Tang et al., 2010; Long et al., 2013). While the cumulative objective is a natural choice, it has been noted that it may discriminate against certain submissions by allo- cating all irrelevant reviewers to a subset of submissions, even when a more balanced assignment exists (Garg et al., 2010). Thus, a number of works has explored the idea of assignment fairness, aiming at producing more balanced assignments (Kobren et al., 2019; Stelmakh et al., 2021). Finally, other works explore the ideas of envy-freeness (Tan et al., 2021; Payan, 2022), resistance to lone-wolf strategic behavior (Xu et al., 2019; Dhull et al., 2022), and encouraging various types of diversity (Li et al., 2015; Leyton-Brown et al., 2022). 3 Data collection pipeline In this section, we describe the process of data collection. This work was performed under the approval of the Institutional Review Board (IRB) of Carnegie Mellon University. Gold standard data In this study, we aim to collect a dataset of self-evaluations of reviewers\u2019 expertise that satis\ufb01es two desiderata: (D1) The dataset should comprise evaluations of expertise for papers participants read to a reasonable extent. (D2) The dataset should be released publicly without disclosing any sensitive information. Let us now discuss our approach to recruiting participants and obtaining accurate estimates of their expertise in reviewing papers included in the dataset. 4 Participant recruiting We recruited participants using a combination of several channels that are typi- cally employed to recruit reviewers for computer science conferences: Mailing lists. First, we sent recruiting emails to relevant mailing lists of several universities and research departments of companies Social media. Second, two authors of this paper posted a call for participation on their Twitter accounts \u2022 Personal communication. Third, we sent personal invites to researchers from our professional network We had a screening criterion requiring that prospective participants have at least one paper published in the broad area of computer science. Overall, for the version of the dataset we release in this paper, we managed to recruit 58 participants, all of whom passed the screening. Expertise evaluations The key idea of our approach to expertise evaluation is to ask participants to evaluate their expertise in reviewing papers they read in the past. After reading a paper, a researcher is in the best possible position to evaluate whether they have the right background\u2014both in terms of the techniques used in the paper and in terms of the broader research area of the paper\u2014to judge the quality of the paper. With this motivation, we asked participants to: Recall 5-10 papers in their broad research area that they read to a reasonable extent in the last year and tell us their expertise in reviewing these papers. In more detail, the choice of papers was constrained by two minor conditions: \u2022 The papers reported by a participant should not be authored by them \u2022 The papers reported by a participant should be freely available online In addition to these constraints, we gave several recommendations to the participants in order to make the dataset more diverse and useful for the research purposes: First, we asked participants to choose papers that cover the whole spectrum of their expertise with some papers being well-separated (e.g., very high expertise and very low expertise) and some papers being nearly-tied (e.g., two high-expertise papers). Second, we recommended participants to avoid ties in their evaluations. To help participants comply with this recommendation, we implemented evaluation on a scale of 1 (\u201cI am not quali\ufb01ed to review this paper\u201d) to 5 (\u201cI have background necessary to evaluate all the aspects of the paper\u201d) with a 0.25 step size, enabling participants to report papers with small di\ufb00erences in expertise. Third, we asked participants to come up with papers that they think may be tricky for existing similarity- computation algorithms. For this, we relied on the commonsense understanding and did not instruct participants on the inner-workings of these algorithms. Overall, the time needed for participants to contribute to the dataset was estimated to be 5\u201310 minutes. The full instructions of the survey are available in Appendix A. Data release Following the procedure outlined above, we collected responses from 58 researchers. These responses constitute an initial version of the dataset that we release in this work. Each entry in the dataset corresponds to a participant and comprises evaluations of their expertise in reviewing papers of their choice. For each paper and each participant, we provide representations that are su\ufb03cient to start working on our dataset: Participant. Each participant is represented by their Semantic Scholar ID and complete bibliography crawled from Semantic Scholar on May 1, 2022. Paper. Each paper, including papers from participants\u2019 bibliographies, is represented by its Semantic Scholar ID, title, abstract, list of authors, publication year, and arXiv identi\ufb01er. Additionally, papers from participants\u2019 responses are supplied with links to freely available PDFs (whenever available). 5 Total number of participants: 58 Characteristic Quantity Value Gender % Male 78 Affiliation % Carnegie Mellon University 40 Country % USA 74 Position % PhD student % Faculty % Post-PhD (non-faculty) 45 28 12 Experience"}, {"question": " How many participants were recruited for the dataset released in the paper?,answer: 58 participants were recruited for the dataset released in the paper.", "ref_chunk": "a result, di\ufb00erent conferences use di\ufb00erent strategies (Shah et al., 2018; Leyton-Brown et al., 2022) and there is a lack of empirical or theoretical evidence that would guide venue organizers in their decisions. Automation of the assignment stage At a high level, automated assignment stage consists of two steps: \ufb01rst, similarity scores are computed; second, reviewers are allocated to submissions such that some notion of assignment quality (formulated in terms of the similarity scores) is maximized. In this work, we focus on the \ufb01rst step of the process. However, for completeness, we now mention several works that design algorithms for the second step. A popular notion of assignment quality is the cumulative similarity score, that is, the sum of the similarity scores across all assigned reviewers and papers. An algorithm pursuing such an objective is implemented in the widely employed TPMS assignment algorithm (Charlin and Zemel, 2013) and similar ideas are explored in many papers (Goldsmith and Sloan, 2007; Tang et al., 2010; Long et al., 2013). While the cumulative objective is a natural choice, it has been noted that it may discriminate against certain submissions by allo- cating all irrelevant reviewers to a subset of submissions, even when a more balanced assignment exists (Garg et al., 2010). Thus, a number of works has explored the idea of assignment fairness, aiming at producing more balanced assignments (Kobren et al., 2019; Stelmakh et al., 2021). Finally, other works explore the ideas of envy-freeness (Tan et al., 2021; Payan, 2022), resistance to lone-wolf strategic behavior (Xu et al., 2019; Dhull et al., 2022), and encouraging various types of diversity (Li et al., 2015; Leyton-Brown et al., 2022). 3 Data collection pipeline In this section, we describe the process of data collection. This work was performed under the approval of the Institutional Review Board (IRB) of Carnegie Mellon University. Gold standard data In this study, we aim to collect a dataset of self-evaluations of reviewers\u2019 expertise that satis\ufb01es two desiderata: (D1) The dataset should comprise evaluations of expertise for papers participants read to a reasonable extent. (D2) The dataset should be released publicly without disclosing any sensitive information. Let us now discuss our approach to recruiting participants and obtaining accurate estimates of their expertise in reviewing papers included in the dataset. 4 Participant recruiting We recruited participants using a combination of several channels that are typi- cally employed to recruit reviewers for computer science conferences: Mailing lists. First, we sent recruiting emails to relevant mailing lists of several universities and research departments of companies Social media. Second, two authors of this paper posted a call for participation on their Twitter accounts \u2022 Personal communication. Third, we sent personal invites to researchers from our professional network We had a screening criterion requiring that prospective participants have at least one paper published in the broad area of computer science. Overall, for the version of the dataset we release in this paper, we managed to recruit 58 participants, all of whom passed the screening. Expertise evaluations The key idea of our approach to expertise evaluation is to ask participants to evaluate their expertise in reviewing papers they read in the past. After reading a paper, a researcher is in the best possible position to evaluate whether they have the right background\u2014both in terms of the techniques used in the paper and in terms of the broader research area of the paper\u2014to judge the quality of the paper. With this motivation, we asked participants to: Recall 5-10 papers in their broad research area that they read to a reasonable extent in the last year and tell us their expertise in reviewing these papers. In more detail, the choice of papers was constrained by two minor conditions: \u2022 The papers reported by a participant should not be authored by them \u2022 The papers reported by a participant should be freely available online In addition to these constraints, we gave several recommendations to the participants in order to make the dataset more diverse and useful for the research purposes: First, we asked participants to choose papers that cover the whole spectrum of their expertise with some papers being well-separated (e.g., very high expertise and very low expertise) and some papers being nearly-tied (e.g., two high-expertise papers). Second, we recommended participants to avoid ties in their evaluations. To help participants comply with this recommendation, we implemented evaluation on a scale of 1 (\u201cI am not quali\ufb01ed to review this paper\u201d) to 5 (\u201cI have background necessary to evaluate all the aspects of the paper\u201d) with a 0.25 step size, enabling participants to report papers with small di\ufb00erences in expertise. Third, we asked participants to come up with papers that they think may be tricky for existing similarity- computation algorithms. For this, we relied on the commonsense understanding and did not instruct participants on the inner-workings of these algorithms. Overall, the time needed for participants to contribute to the dataset was estimated to be 5\u201310 minutes. The full instructions of the survey are available in Appendix A. Data release Following the procedure outlined above, we collected responses from 58 researchers. These responses constitute an initial version of the dataset that we release in this work. Each entry in the dataset corresponds to a participant and comprises evaluations of their expertise in reviewing papers of their choice. For each paper and each participant, we provide representations that are su\ufb03cient to start working on our dataset: Participant. Each participant is represented by their Semantic Scholar ID and complete bibliography crawled from Semantic Scholar on May 1, 2022. Paper. Each paper, including papers from participants\u2019 bibliographies, is represented by its Semantic Scholar ID, title, abstract, list of authors, publication year, and arXiv identi\ufb01er. Additionally, papers from participants\u2019 responses are supplied with links to freely available PDFs (whenever available). 5 Total number of participants: 58 Characteristic Quantity Value Gender % Male 78 Affiliation % Carnegie Mellon University 40 Country % USA 74 Position % PhD student % Faculty % Post-PhD (non-faculty) 45 28 12 Experience"}, {"question": " What were the screening criteria for prospective participants in the study?,answer: Prospective participants were required to have at least one paper published in the broad area of computer science.", "ref_chunk": "a result, di\ufb00erent conferences use di\ufb00erent strategies (Shah et al., 2018; Leyton-Brown et al., 2022) and there is a lack of empirical or theoretical evidence that would guide venue organizers in their decisions. Automation of the assignment stage At a high level, automated assignment stage consists of two steps: \ufb01rst, similarity scores are computed; second, reviewers are allocated to submissions such that some notion of assignment quality (formulated in terms of the similarity scores) is maximized. In this work, we focus on the \ufb01rst step of the process. However, for completeness, we now mention several works that design algorithms for the second step. A popular notion of assignment quality is the cumulative similarity score, that is, the sum of the similarity scores across all assigned reviewers and papers. An algorithm pursuing such an objective is implemented in the widely employed TPMS assignment algorithm (Charlin and Zemel, 2013) and similar ideas are explored in many papers (Goldsmith and Sloan, 2007; Tang et al., 2010; Long et al., 2013). While the cumulative objective is a natural choice, it has been noted that it may discriminate against certain submissions by allo- cating all irrelevant reviewers to a subset of submissions, even when a more balanced assignment exists (Garg et al., 2010). Thus, a number of works has explored the idea of assignment fairness, aiming at producing more balanced assignments (Kobren et al., 2019; Stelmakh et al., 2021). Finally, other works explore the ideas of envy-freeness (Tan et al., 2021; Payan, 2022), resistance to lone-wolf strategic behavior (Xu et al., 2019; Dhull et al., 2022), and encouraging various types of diversity (Li et al., 2015; Leyton-Brown et al., 2022). 3 Data collection pipeline In this section, we describe the process of data collection. This work was performed under the approval of the Institutional Review Board (IRB) of Carnegie Mellon University. Gold standard data In this study, we aim to collect a dataset of self-evaluations of reviewers\u2019 expertise that satis\ufb01es two desiderata: (D1) The dataset should comprise evaluations of expertise for papers participants read to a reasonable extent. (D2) The dataset should be released publicly without disclosing any sensitive information. Let us now discuss our approach to recruiting participants and obtaining accurate estimates of their expertise in reviewing papers included in the dataset. 4 Participant recruiting We recruited participants using a combination of several channels that are typi- cally employed to recruit reviewers for computer science conferences: Mailing lists. First, we sent recruiting emails to relevant mailing lists of several universities and research departments of companies Social media. Second, two authors of this paper posted a call for participation on their Twitter accounts \u2022 Personal communication. Third, we sent personal invites to researchers from our professional network We had a screening criterion requiring that prospective participants have at least one paper published in the broad area of computer science. Overall, for the version of the dataset we release in this paper, we managed to recruit 58 participants, all of whom passed the screening. Expertise evaluations The key idea of our approach to expertise evaluation is to ask participants to evaluate their expertise in reviewing papers they read in the past. After reading a paper, a researcher is in the best possible position to evaluate whether they have the right background\u2014both in terms of the techniques used in the paper and in terms of the broader research area of the paper\u2014to judge the quality of the paper. With this motivation, we asked participants to: Recall 5-10 papers in their broad research area that they read to a reasonable extent in the last year and tell us their expertise in reviewing these papers. In more detail, the choice of papers was constrained by two minor conditions: \u2022 The papers reported by a participant should not be authored by them \u2022 The papers reported by a participant should be freely available online In addition to these constraints, we gave several recommendations to the participants in order to make the dataset more diverse and useful for the research purposes: First, we asked participants to choose papers that cover the whole spectrum of their expertise with some papers being well-separated (e.g., very high expertise and very low expertise) and some papers being nearly-tied (e.g., two high-expertise papers). Second, we recommended participants to avoid ties in their evaluations. To help participants comply with this recommendation, we implemented evaluation on a scale of 1 (\u201cI am not quali\ufb01ed to review this paper\u201d) to 5 (\u201cI have background necessary to evaluate all the aspects of the paper\u201d) with a 0.25 step size, enabling participants to report papers with small di\ufb00erences in expertise. Third, we asked participants to come up with papers that they think may be tricky for existing similarity- computation algorithms. For this, we relied on the commonsense understanding and did not instruct participants on the inner-workings of these algorithms. Overall, the time needed for participants to contribute to the dataset was estimated to be 5\u201310 minutes. The full instructions of the survey are available in Appendix A. Data release Following the procedure outlined above, we collected responses from 58 researchers. These responses constitute an initial version of the dataset that we release in this work. Each entry in the dataset corresponds to a participant and comprises evaluations of their expertise in reviewing papers of their choice. For each paper and each participant, we provide representations that are su\ufb03cient to start working on our dataset: Participant. Each participant is represented by their Semantic Scholar ID and complete bibliography crawled from Semantic Scholar on May 1, 2022. Paper. Each paper, including papers from participants\u2019 bibliographies, is represented by its Semantic Scholar ID, title, abstract, list of authors, publication year, and arXiv identi\ufb01er. Additionally, papers from participants\u2019 responses are supplied with links to freely available PDFs (whenever available). 5 Total number of participants: 58 Characteristic Quantity Value Gender % Male 78 Affiliation % Carnegie Mellon University 40 Country % USA 74 Position % PhD student % Faculty % Post-PhD (non-faculty) 45 28 12 Experience"}, {"question": " What are the constraints on the choice of papers reported by a participant for the expertise evaluation dataset?,answer: The constraints include that the papers reported should not be authored by the participant and should be freely available online.", "ref_chunk": "a result, di\ufb00erent conferences use di\ufb00erent strategies (Shah et al., 2018; Leyton-Brown et al., 2022) and there is a lack of empirical or theoretical evidence that would guide venue organizers in their decisions. Automation of the assignment stage At a high level, automated assignment stage consists of two steps: \ufb01rst, similarity scores are computed; second, reviewers are allocated to submissions such that some notion of assignment quality (formulated in terms of the similarity scores) is maximized. In this work, we focus on the \ufb01rst step of the process. However, for completeness, we now mention several works that design algorithms for the second step. A popular notion of assignment quality is the cumulative similarity score, that is, the sum of the similarity scores across all assigned reviewers and papers. An algorithm pursuing such an objective is implemented in the widely employed TPMS assignment algorithm (Charlin and Zemel, 2013) and similar ideas are explored in many papers (Goldsmith and Sloan, 2007; Tang et al., 2010; Long et al., 2013). While the cumulative objective is a natural choice, it has been noted that it may discriminate against certain submissions by allo- cating all irrelevant reviewers to a subset of submissions, even when a more balanced assignment exists (Garg et al., 2010). Thus, a number of works has explored the idea of assignment fairness, aiming at producing more balanced assignments (Kobren et al., 2019; Stelmakh et al., 2021). Finally, other works explore the ideas of envy-freeness (Tan et al., 2021; Payan, 2022), resistance to lone-wolf strategic behavior (Xu et al., 2019; Dhull et al., 2022), and encouraging various types of diversity (Li et al., 2015; Leyton-Brown et al., 2022). 3 Data collection pipeline In this section, we describe the process of data collection. This work was performed under the approval of the Institutional Review Board (IRB) of Carnegie Mellon University. Gold standard data In this study, we aim to collect a dataset of self-evaluations of reviewers\u2019 expertise that satis\ufb01es two desiderata: (D1) The dataset should comprise evaluations of expertise for papers participants read to a reasonable extent. (D2) The dataset should be released publicly without disclosing any sensitive information. Let us now discuss our approach to recruiting participants and obtaining accurate estimates of their expertise in reviewing papers included in the dataset. 4 Participant recruiting We recruited participants using a combination of several channels that are typi- cally employed to recruit reviewers for computer science conferences: Mailing lists. First, we sent recruiting emails to relevant mailing lists of several universities and research departments of companies Social media. Second, two authors of this paper posted a call for participation on their Twitter accounts \u2022 Personal communication. Third, we sent personal invites to researchers from our professional network We had a screening criterion requiring that prospective participants have at least one paper published in the broad area of computer science. Overall, for the version of the dataset we release in this paper, we managed to recruit 58 participants, all of whom passed the screening. Expertise evaluations The key idea of our approach to expertise evaluation is to ask participants to evaluate their expertise in reviewing papers they read in the past. After reading a paper, a researcher is in the best possible position to evaluate whether they have the right background\u2014both in terms of the techniques used in the paper and in terms of the broader research area of the paper\u2014to judge the quality of the paper. With this motivation, we asked participants to: Recall 5-10 papers in their broad research area that they read to a reasonable extent in the last year and tell us their expertise in reviewing these papers. In more detail, the choice of papers was constrained by two minor conditions: \u2022 The papers reported by a participant should not be authored by them \u2022 The papers reported by a participant should be freely available online In addition to these constraints, we gave several recommendations to the participants in order to make the dataset more diverse and useful for the research purposes: First, we asked participants to choose papers that cover the whole spectrum of their expertise with some papers being well-separated (e.g., very high expertise and very low expertise) and some papers being nearly-tied (e.g., two high-expertise papers). Second, we recommended participants to avoid ties in their evaluations. To help participants comply with this recommendation, we implemented evaluation on a scale of 1 (\u201cI am not quali\ufb01ed to review this paper\u201d) to 5 (\u201cI have background necessary to evaluate all the aspects of the paper\u201d) with a 0.25 step size, enabling participants to report papers with small di\ufb00erences in expertise. Third, we asked participants to come up with papers that they think may be tricky for existing similarity- computation algorithms. For this, we relied on the commonsense understanding and did not instruct participants on the inner-workings of these algorithms. Overall, the time needed for participants to contribute to the dataset was estimated to be 5\u201310 minutes. The full instructions of the survey are available in Appendix A. Data release Following the procedure outlined above, we collected responses from 58 researchers. These responses constitute an initial version of the dataset that we release in this work. Each entry in the dataset corresponds to a participant and comprises evaluations of their expertise in reviewing papers of their choice. For each paper and each participant, we provide representations that are su\ufb03cient to start working on our dataset: Participant. Each participant is represented by their Semantic Scholar ID and complete bibliography crawled from Semantic Scholar on May 1, 2022. Paper. Each paper, including papers from participants\u2019 bibliographies, is represented by its Semantic Scholar ID, title, abstract, list of authors, publication year, and arXiv identi\ufb01er. Additionally, papers from participants\u2019 responses are supplied with links to freely available PDFs (whenever available). 5 Total number of participants: 58 Characteristic Quantity Value Gender % Male 78 Affiliation % Carnegie Mellon University 40 Country % USA 74 Position % PhD student % Faculty % Post-PhD (non-faculty) 45 28 12 Experience"}, {"question": " What recommendations were given to participants to make the dataset more diverse and useful?,answer: Participants were recommended to choose papers covering a spectrum of expertise, avoid ties in evaluations, and select papers that may be tricky for similarity-computation algorithms.", "ref_chunk": "a result, di\ufb00erent conferences use di\ufb00erent strategies (Shah et al., 2018; Leyton-Brown et al., 2022) and there is a lack of empirical or theoretical evidence that would guide venue organizers in their decisions. Automation of the assignment stage At a high level, automated assignment stage consists of two steps: \ufb01rst, similarity scores are computed; second, reviewers are allocated to submissions such that some notion of assignment quality (formulated in terms of the similarity scores) is maximized. In this work, we focus on the \ufb01rst step of the process. However, for completeness, we now mention several works that design algorithms for the second step. A popular notion of assignment quality is the cumulative similarity score, that is, the sum of the similarity scores across all assigned reviewers and papers. An algorithm pursuing such an objective is implemented in the widely employed TPMS assignment algorithm (Charlin and Zemel, 2013) and similar ideas are explored in many papers (Goldsmith and Sloan, 2007; Tang et al., 2010; Long et al., 2013). While the cumulative objective is a natural choice, it has been noted that it may discriminate against certain submissions by allo- cating all irrelevant reviewers to a subset of submissions, even when a more balanced assignment exists (Garg et al., 2010). Thus, a number of works has explored the idea of assignment fairness, aiming at producing more balanced assignments (Kobren et al., 2019; Stelmakh et al., 2021). Finally, other works explore the ideas of envy-freeness (Tan et al., 2021; Payan, 2022), resistance to lone-wolf strategic behavior (Xu et al., 2019; Dhull et al., 2022), and encouraging various types of diversity (Li et al., 2015; Leyton-Brown et al., 2022). 3 Data collection pipeline In this section, we describe the process of data collection. This work was performed under the approval of the Institutional Review Board (IRB) of Carnegie Mellon University. Gold standard data In this study, we aim to collect a dataset of self-evaluations of reviewers\u2019 expertise that satis\ufb01es two desiderata: (D1) The dataset should comprise evaluations of expertise for papers participants read to a reasonable extent. (D2) The dataset should be released publicly without disclosing any sensitive information. Let us now discuss our approach to recruiting participants and obtaining accurate estimates of their expertise in reviewing papers included in the dataset. 4 Participant recruiting We recruited participants using a combination of several channels that are typi- cally employed to recruit reviewers for computer science conferences: Mailing lists. First, we sent recruiting emails to relevant mailing lists of several universities and research departments of companies Social media. Second, two authors of this paper posted a call for participation on their Twitter accounts \u2022 Personal communication. Third, we sent personal invites to researchers from our professional network We had a screening criterion requiring that prospective participants have at least one paper published in the broad area of computer science. Overall, for the version of the dataset we release in this paper, we managed to recruit 58 participants, all of whom passed the screening. Expertise evaluations The key idea of our approach to expertise evaluation is to ask participants to evaluate their expertise in reviewing papers they read in the past. After reading a paper, a researcher is in the best possible position to evaluate whether they have the right background\u2014both in terms of the techniques used in the paper and in terms of the broader research area of the paper\u2014to judge the quality of the paper. With this motivation, we asked participants to: Recall 5-10 papers in their broad research area that they read to a reasonable extent in the last year and tell us their expertise in reviewing these papers. In more detail, the choice of papers was constrained by two minor conditions: \u2022 The papers reported by a participant should not be authored by them \u2022 The papers reported by a participant should be freely available online In addition to these constraints, we gave several recommendations to the participants in order to make the dataset more diverse and useful for the research purposes: First, we asked participants to choose papers that cover the whole spectrum of their expertise with some papers being well-separated (e.g., very high expertise and very low expertise) and some papers being nearly-tied (e.g., two high-expertise papers). Second, we recommended participants to avoid ties in their evaluations. To help participants comply with this recommendation, we implemented evaluation on a scale of 1 (\u201cI am not quali\ufb01ed to review this paper\u201d) to 5 (\u201cI have background necessary to evaluate all the aspects of the paper\u201d) with a 0.25 step size, enabling participants to report papers with small di\ufb00erences in expertise. Third, we asked participants to come up with papers that they think may be tricky for existing similarity- computation algorithms. For this, we relied on the commonsense understanding and did not instruct participants on the inner-workings of these algorithms. Overall, the time needed for participants to contribute to the dataset was estimated to be 5\u201310 minutes. The full instructions of the survey are available in Appendix A. Data release Following the procedure outlined above, we collected responses from 58 researchers. These responses constitute an initial version of the dataset that we release in this work. Each entry in the dataset corresponds to a participant and comprises evaluations of their expertise in reviewing papers of their choice. For each paper and each participant, we provide representations that are su\ufb03cient to start working on our dataset: Participant. Each participant is represented by their Semantic Scholar ID and complete bibliography crawled from Semantic Scholar on May 1, 2022. Paper. Each paper, including papers from participants\u2019 bibliographies, is represented by its Semantic Scholar ID, title, abstract, list of authors, publication year, and arXiv identi\ufb01er. Additionally, papers from participants\u2019 responses are supplied with links to freely available PDFs (whenever available). 5 Total number of participants: 58 Characteristic Quantity Value Gender % Male 78 Affiliation % Carnegie Mellon University 40 Country % USA 74 Position % PhD student % Faculty % Post-PhD (non-faculty) 45 28 12 Experience"}], "doc_text": "a result, di\ufb00erent conferences use di\ufb00erent strategies (Shah et al., 2018; Leyton-Brown et al., 2022) and there is a lack of empirical or theoretical evidence that would guide venue organizers in their decisions. Automation of the assignment stage At a high level, automated assignment stage consists of two steps: \ufb01rst, similarity scores are computed; second, reviewers are allocated to submissions such that some notion of assignment quality (formulated in terms of the similarity scores) is maximized. In this work, we focus on the \ufb01rst step of the process. However, for completeness, we now mention several works that design algorithms for the second step. A popular notion of assignment quality is the cumulative similarity score, that is, the sum of the similarity scores across all assigned reviewers and papers. An algorithm pursuing such an objective is implemented in the widely employed TPMS assignment algorithm (Charlin and Zemel, 2013) and similar ideas are explored in many papers (Goldsmith and Sloan, 2007; Tang et al., 2010; Long et al., 2013). While the cumulative objective is a natural choice, it has been noted that it may discriminate against certain submissions by allo- cating all irrelevant reviewers to a subset of submissions, even when a more balanced assignment exists (Garg et al., 2010). Thus, a number of works has explored the idea of assignment fairness, aiming at producing more balanced assignments (Kobren et al., 2019; Stelmakh et al., 2021). Finally, other works explore the ideas of envy-freeness (Tan et al., 2021; Payan, 2022), resistance to lone-wolf strategic behavior (Xu et al., 2019; Dhull et al., 2022), and encouraging various types of diversity (Li et al., 2015; Leyton-Brown et al., 2022). 3 Data collection pipeline In this section, we describe the process of data collection. This work was performed under the approval of the Institutional Review Board (IRB) of Carnegie Mellon University. Gold standard data In this study, we aim to collect a dataset of self-evaluations of reviewers\u2019 expertise that satis\ufb01es two desiderata: (D1) The dataset should comprise evaluations of expertise for papers participants read to a reasonable extent. (D2) The dataset should be released publicly without disclosing any sensitive information. Let us now discuss our approach to recruiting participants and obtaining accurate estimates of their expertise in reviewing papers included in the dataset. 4 Participant recruiting We recruited participants using a combination of several channels that are typi- cally employed to recruit reviewers for computer science conferences: Mailing lists. First, we sent recruiting emails to relevant mailing lists of several universities and research departments of companies Social media. Second, two authors of this paper posted a call for participation on their Twitter accounts \u2022 Personal communication. Third, we sent personal invites to researchers from our professional network We had a screening criterion requiring that prospective participants have at least one paper published in the broad area of computer science. Overall, for the version of the dataset we release in this paper, we managed to recruit 58 participants, all of whom passed the screening. Expertise evaluations The key idea of our approach to expertise evaluation is to ask participants to evaluate their expertise in reviewing papers they read in the past. After reading a paper, a researcher is in the best possible position to evaluate whether they have the right background\u2014both in terms of the techniques used in the paper and in terms of the broader research area of the paper\u2014to judge the quality of the paper. With this motivation, we asked participants to: Recall 5-10 papers in their broad research area that they read to a reasonable extent in the last year and tell us their expertise in reviewing these papers. In more detail, the choice of papers was constrained by two minor conditions: \u2022 The papers reported by a participant should not be authored by them \u2022 The papers reported by a participant should be freely available online In addition to these constraints, we gave several recommendations to the participants in order to make the dataset more diverse and useful for the research purposes: First, we asked participants to choose papers that cover the whole spectrum of their expertise with some papers being well-separated (e.g., very high expertise and very low expertise) and some papers being nearly-tied (e.g., two high-expertise papers). Second, we recommended participants to avoid ties in their evaluations. To help participants comply with this recommendation, we implemented evaluation on a scale of 1 (\u201cI am not quali\ufb01ed to review this paper\u201d) to 5 (\u201cI have background necessary to evaluate all the aspects of the paper\u201d) with a 0.25 step size, enabling participants to report papers with small di\ufb00erences in expertise. Third, we asked participants to come up with papers that they think may be tricky for existing similarity- computation algorithms. For this, we relied on the commonsense understanding and did not instruct participants on the inner-workings of these algorithms. Overall, the time needed for participants to contribute to the dataset was estimated to be 5\u201310 minutes. The full instructions of the survey are available in Appendix A. Data release Following the procedure outlined above, we collected responses from 58 researchers. These responses constitute an initial version of the dataset that we release in this work. Each entry in the dataset corresponds to a participant and comprises evaluations of their expertise in reviewing papers of their choice. For each paper and each participant, we provide representations that are su\ufb03cient to start working on our dataset: Participant. Each participant is represented by their Semantic Scholar ID and complete bibliography crawled from Semantic Scholar on May 1, 2022. Paper. Each paper, including papers from participants\u2019 bibliographies, is represented by its Semantic Scholar ID, title, abstract, list of authors, publication year, and arXiv identi\ufb01er. Additionally, papers from participants\u2019 responses are supplied with links to freely available PDFs (whenever available). 5 Total number of participants: 58 Characteristic Quantity Value Gender % Male 78 Affiliation % Carnegie Mellon University 40 Country % USA 74 Position % PhD student % Faculty % Post-PhD (non-faculty) 45 28 12 Experience"}