{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Segment-Level_Vectorized_Beam_Search_Based_on_Partially_Autoregressive_Inference_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What did the authors achieve in terms of RTF compared to AR?", "answer": " Approximately 10 times faster inference", "ref_chunk": "While the accuracy has slightly degraded, this is due to the accuracy of the gCTC result. We will explain it fur- ther in detail in the following limitations section 5.3.1. In terms of the RTF, we have achieved approximately 10 times faster inference compared to AR. In particular, since the inference speed does not largely depend on the audio length, the standard deviation is approx- imately 17.5% of that of AR decoding. This feature is more effec- tive for longer audio and in the test-clean dataset, and we observed 89.7\u00d7 speed up for 29 second audio as the maximum speedup. Note that this speedup depends on several factors, such as the number of masks or input audio length. Fig. 3 shows the proportion of time spent on the encoder and decoder process and the inference time for each audio length, eval- uated on the LS-960 dataset. Compared to Fig. 2a, the decoder-to- We investigated the correlation between the WER and RTF as shown in Fig. 4, by changing the beam size from 1 to 20, and measured the WER and RTF using the test-clean dataset from Librispeech on the E-Branchformer based pre-trained models for LS-100 and LS-960 datasets. Comparing the AR models, the RTF differs greatly because the model sizes are different, however, the RTF of the PAR method remained almost the same. This is because the inference time of PAR depends on the computation time of the models itself. Therefore, the PAR curves have a significantly lower RTF compared to the AR curves, and the change in RTF is negligible as the beam-size changes, hence the narrower width. Although the RTF remains constant, we still see the differences in WER for the PAR models, especially with the pre-trained model of the LS-100 dataset compared to the model for the LS-960 dataset. This is due to the accuracy degradation problem we mention in Section 5.3.1. 5.2.2. NAR and PAR Comparing NAR and PAR, we can see that PAR is not as fast as NAR, as shown in Table 4. The number of decoder iterations is 10, which is larger than max iteration for PAR, but it performs faster than PAR. One reason for this is that the size of the decoder input is different. In this work, we added a dummy hypothesis as men- tioned in Section 4.2. As a result, the batch size of decoder input for PAR decoding is S \u00d7 B, where the batch size for NAR decoding is S. Therefore, it seems that the computation time per decoder is Fig. 4: The comparison of WER and RTF measured using the AR and PAR methods. We used the models trained with LS-100 and LS-960 datasets and measured by changing the beam size between 1 and 20. shorter for Mask-CTC NAR, and the resulting decoder processing time becomes faster. In terms of accuracy, we can see that PAR outperforms NAR. Improvement in performance is similar to that of E-branchformer models in both RTF and WER, and it was confirmed that there was no difference in improvement due to architecture differences. From these results, it is evident that applying the PAR method can solve the built-in accuracy issue for the NAR method we mentioned in Section 3.2. In particular, we can confirm that PAR obtains a speed between AR and NAR but achieves similar accuracy to AR. There- fore, a new trade-off balance that is not present in AR and NAR has been realized. Table 4: Comparison with NAR. The speedup shows the speedup from the AR decoding. Model RTF (\u2193) WER [%] (\u2193) Speedup (\u2191) AR NAR PAR CTC/Attention CTC Mask-CTC CTC/Attention 0.198 (0.080) 0.005 (0.004) 0.008 (0.005) 0.014 (0.009) 6.7 / 18.3 / 7.0 / 18.6 7.7 / 21.0 / 7.9 / 21.4 7.1 / 20.8 / 7.5 / 21.0 6.2 / 18.5 / 6.6 / 18.7 1.00\u00d7 39.60\u00d7 24.75\u00d7 14.14\u00d7 5.3. Limitations 5.3.1. Accuracy with PAR The accuracy of PAR can be degraded if the gCTC result is not ac- curate. If the result of gCTC is incorrect with high confidence, we cannot use the AR process to refine the gCTC result. From the com- parison of the LS-100 and LS-960 models in Table 3, we can see that more accuracy degradation can occur with the LS-100 model. The accuracy may also degrade at a higher Pthres because the number of target tokens per mask may exceed max iteration. Since we stop the beam search after max iteration iterations, if the number of target tokens exceeds max iteration, we cannot predict the entire sequence for one mask. Therefore, the accuracy may be degraded if the Pthres is closer to 1.0. Fig. 5 describes the relationship between WER and Pthres for max iteration. Using a max iteration of 5, we observe the accuracy degrades at higher Pthres levels. This issue is caused by the lack of beam search iterations, so to solve this prob- lem, we need to increase the max iteration. In Fig. 5, we increased the max iteration to 8 and observed a more accurate result. Fig. 5: The relationship between the WER and Pthres. We evalu- ated by changing the Pthres from 0.95 to 0.999. We used the E- Branchformer-based pre-trained model for LS-960. 5.3.2. Memory usage It is important to note the standard deviation of memory usage in Table 3 increases greatly compared to AR. Since the decoder process in PAR is computed simultaneously for all masks, we need more GPU memory if there are many masks. Therefore if the number of masks increases due to long audio inputs, high Pthres, or low accuracy of the gCTC result, we may get an out-of-memory error as GPU memory is exceeded during inference. Considering that the inference of all masks does not depend on each other, it is possible to alleviate this issue by using multiple GPUs to perform inference. 5.3.3. Segment-level Vectorized Beam Search If a masked sequence contains multiple masks, the predicted tokens for the second or later masks may not be accurate"}, {"question": " What is the standard deviation of the inference speed for longer audio compared to AR decoding?", "answer": " Approximately 17.5%", "ref_chunk": "While the accuracy has slightly degraded, this is due to the accuracy of the gCTC result. We will explain it fur- ther in detail in the following limitations section 5.3.1. In terms of the RTF, we have achieved approximately 10 times faster inference compared to AR. In particular, since the inference speed does not largely depend on the audio length, the standard deviation is approx- imately 17.5% of that of AR decoding. This feature is more effec- tive for longer audio and in the test-clean dataset, and we observed 89.7\u00d7 speed up for 29 second audio as the maximum speedup. Note that this speedup depends on several factors, such as the number of masks or input audio length. Fig. 3 shows the proportion of time spent on the encoder and decoder process and the inference time for each audio length, eval- uated on the LS-960 dataset. Compared to Fig. 2a, the decoder-to- We investigated the correlation between the WER and RTF as shown in Fig. 4, by changing the beam size from 1 to 20, and measured the WER and RTF using the test-clean dataset from Librispeech on the E-Branchformer based pre-trained models for LS-100 and LS-960 datasets. Comparing the AR models, the RTF differs greatly because the model sizes are different, however, the RTF of the PAR method remained almost the same. This is because the inference time of PAR depends on the computation time of the models itself. Therefore, the PAR curves have a significantly lower RTF compared to the AR curves, and the change in RTF is negligible as the beam-size changes, hence the narrower width. Although the RTF remains constant, we still see the differences in WER for the PAR models, especially with the pre-trained model of the LS-100 dataset compared to the model for the LS-960 dataset. This is due to the accuracy degradation problem we mention in Section 5.3.1. 5.2.2. NAR and PAR Comparing NAR and PAR, we can see that PAR is not as fast as NAR, as shown in Table 4. The number of decoder iterations is 10, which is larger than max iteration for PAR, but it performs faster than PAR. One reason for this is that the size of the decoder input is different. In this work, we added a dummy hypothesis as men- tioned in Section 4.2. As a result, the batch size of decoder input for PAR decoding is S \u00d7 B, where the batch size for NAR decoding is S. Therefore, it seems that the computation time per decoder is Fig. 4: The comparison of WER and RTF measured using the AR and PAR methods. We used the models trained with LS-100 and LS-960 datasets and measured by changing the beam size between 1 and 20. shorter for Mask-CTC NAR, and the resulting decoder processing time becomes faster. In terms of accuracy, we can see that PAR outperforms NAR. Improvement in performance is similar to that of E-branchformer models in both RTF and WER, and it was confirmed that there was no difference in improvement due to architecture differences. From these results, it is evident that applying the PAR method can solve the built-in accuracy issue for the NAR method we mentioned in Section 3.2. In particular, we can confirm that PAR obtains a speed between AR and NAR but achieves similar accuracy to AR. There- fore, a new trade-off balance that is not present in AR and NAR has been realized. Table 4: Comparison with NAR. The speedup shows the speedup from the AR decoding. Model RTF (\u2193) WER [%] (\u2193) Speedup (\u2191) AR NAR PAR CTC/Attention CTC Mask-CTC CTC/Attention 0.198 (0.080) 0.005 (0.004) 0.008 (0.005) 0.014 (0.009) 6.7 / 18.3 / 7.0 / 18.6 7.7 / 21.0 / 7.9 / 21.4 7.1 / 20.8 / 7.5 / 21.0 6.2 / 18.5 / 6.6 / 18.7 1.00\u00d7 39.60\u00d7 24.75\u00d7 14.14\u00d7 5.3. Limitations 5.3.1. Accuracy with PAR The accuracy of PAR can be degraded if the gCTC result is not ac- curate. If the result of gCTC is incorrect with high confidence, we cannot use the AR process to refine the gCTC result. From the com- parison of the LS-100 and LS-960 models in Table 3, we can see that more accuracy degradation can occur with the LS-100 model. The accuracy may also degrade at a higher Pthres because the number of target tokens per mask may exceed max iteration. Since we stop the beam search after max iteration iterations, if the number of target tokens exceeds max iteration, we cannot predict the entire sequence for one mask. Therefore, the accuracy may be degraded if the Pthres is closer to 1.0. Fig. 5 describes the relationship between WER and Pthres for max iteration. Using a max iteration of 5, we observe the accuracy degrades at higher Pthres levels. This issue is caused by the lack of beam search iterations, so to solve this prob- lem, we need to increase the max iteration. In Fig. 5, we increased the max iteration to 8 and observed a more accurate result. Fig. 5: The relationship between the WER and Pthres. We evalu- ated by changing the Pthres from 0.95 to 0.999. We used the E- Branchformer-based pre-trained model for LS-960. 5.3.2. Memory usage It is important to note the standard deviation of memory usage in Table 3 increases greatly compared to AR. Since the decoder process in PAR is computed simultaneously for all masks, we need more GPU memory if there are many masks. Therefore if the number of masks increases due to long audio inputs, high Pthres, or low accuracy of the gCTC result, we may get an out-of-memory error as GPU memory is exceeded during inference. Considering that the inference of all masks does not depend on each other, it is possible to alleviate this issue by using multiple GPUs to perform inference. 5.3.3. Segment-level Vectorized Beam Search If a masked sequence contains multiple masks, the predicted tokens for the second or later masks may not be accurate"}, {"question": " How much speedup was observed for a 29-second audio in the test-clean dataset?", "answer": " 89.7\u00d7", "ref_chunk": "While the accuracy has slightly degraded, this is due to the accuracy of the gCTC result. We will explain it fur- ther in detail in the following limitations section 5.3.1. In terms of the RTF, we have achieved approximately 10 times faster inference compared to AR. In particular, since the inference speed does not largely depend on the audio length, the standard deviation is approx- imately 17.5% of that of AR decoding. This feature is more effec- tive for longer audio and in the test-clean dataset, and we observed 89.7\u00d7 speed up for 29 second audio as the maximum speedup. Note that this speedup depends on several factors, such as the number of masks or input audio length. Fig. 3 shows the proportion of time spent on the encoder and decoder process and the inference time for each audio length, eval- uated on the LS-960 dataset. Compared to Fig. 2a, the decoder-to- We investigated the correlation between the WER and RTF as shown in Fig. 4, by changing the beam size from 1 to 20, and measured the WER and RTF using the test-clean dataset from Librispeech on the E-Branchformer based pre-trained models for LS-100 and LS-960 datasets. Comparing the AR models, the RTF differs greatly because the model sizes are different, however, the RTF of the PAR method remained almost the same. This is because the inference time of PAR depends on the computation time of the models itself. Therefore, the PAR curves have a significantly lower RTF compared to the AR curves, and the change in RTF is negligible as the beam-size changes, hence the narrower width. Although the RTF remains constant, we still see the differences in WER for the PAR models, especially with the pre-trained model of the LS-100 dataset compared to the model for the LS-960 dataset. This is due to the accuracy degradation problem we mention in Section 5.3.1. 5.2.2. NAR and PAR Comparing NAR and PAR, we can see that PAR is not as fast as NAR, as shown in Table 4. The number of decoder iterations is 10, which is larger than max iteration for PAR, but it performs faster than PAR. One reason for this is that the size of the decoder input is different. In this work, we added a dummy hypothesis as men- tioned in Section 4.2. As a result, the batch size of decoder input for PAR decoding is S \u00d7 B, where the batch size for NAR decoding is S. Therefore, it seems that the computation time per decoder is Fig. 4: The comparison of WER and RTF measured using the AR and PAR methods. We used the models trained with LS-100 and LS-960 datasets and measured by changing the beam size between 1 and 20. shorter for Mask-CTC NAR, and the resulting decoder processing time becomes faster. In terms of accuracy, we can see that PAR outperforms NAR. Improvement in performance is similar to that of E-branchformer models in both RTF and WER, and it was confirmed that there was no difference in improvement due to architecture differences. From these results, it is evident that applying the PAR method can solve the built-in accuracy issue for the NAR method we mentioned in Section 3.2. In particular, we can confirm that PAR obtains a speed between AR and NAR but achieves similar accuracy to AR. There- fore, a new trade-off balance that is not present in AR and NAR has been realized. Table 4: Comparison with NAR. The speedup shows the speedup from the AR decoding. Model RTF (\u2193) WER [%] (\u2193) Speedup (\u2191) AR NAR PAR CTC/Attention CTC Mask-CTC CTC/Attention 0.198 (0.080) 0.005 (0.004) 0.008 (0.005) 0.014 (0.009) 6.7 / 18.3 / 7.0 / 18.6 7.7 / 21.0 / 7.9 / 21.4 7.1 / 20.8 / 7.5 / 21.0 6.2 / 18.5 / 6.6 / 18.7 1.00\u00d7 39.60\u00d7 24.75\u00d7 14.14\u00d7 5.3. Limitations 5.3.1. Accuracy with PAR The accuracy of PAR can be degraded if the gCTC result is not ac- curate. If the result of gCTC is incorrect with high confidence, we cannot use the AR process to refine the gCTC result. From the com- parison of the LS-100 and LS-960 models in Table 3, we can see that more accuracy degradation can occur with the LS-100 model. The accuracy may also degrade at a higher Pthres because the number of target tokens per mask may exceed max iteration. Since we stop the beam search after max iteration iterations, if the number of target tokens exceeds max iteration, we cannot predict the entire sequence for one mask. Therefore, the accuracy may be degraded if the Pthres is closer to 1.0. Fig. 5 describes the relationship between WER and Pthres for max iteration. Using a max iteration of 5, we observe the accuracy degrades at higher Pthres levels. This issue is caused by the lack of beam search iterations, so to solve this prob- lem, we need to increase the max iteration. In Fig. 5, we increased the max iteration to 8 and observed a more accurate result. Fig. 5: The relationship between the WER and Pthres. We evalu- ated by changing the Pthres from 0.95 to 0.999. We used the E- Branchformer-based pre-trained model for LS-960. 5.3.2. Memory usage It is important to note the standard deviation of memory usage in Table 3 increases greatly compared to AR. Since the decoder process in PAR is computed simultaneously for all masks, we need more GPU memory if there are many masks. Therefore if the number of masks increases due to long audio inputs, high Pthres, or low accuracy of the gCTC result, we may get an out-of-memory error as GPU memory is exceeded during inference. Considering that the inference of all masks does not depend on each other, it is possible to alleviate this issue by using multiple GPUs to perform inference. 5.3.3. Segment-level Vectorized Beam Search If a masked sequence contains multiple masks, the predicted tokens for the second or later masks may not be accurate"}, {"question": " How do the PAR curves compare to the AR curves in terms of RTF?", "answer": " PAR curves have a significantly lower RTF", "ref_chunk": "While the accuracy has slightly degraded, this is due to the accuracy of the gCTC result. We will explain it fur- ther in detail in the following limitations section 5.3.1. In terms of the RTF, we have achieved approximately 10 times faster inference compared to AR. In particular, since the inference speed does not largely depend on the audio length, the standard deviation is approx- imately 17.5% of that of AR decoding. This feature is more effec- tive for longer audio and in the test-clean dataset, and we observed 89.7\u00d7 speed up for 29 second audio as the maximum speedup. Note that this speedup depends on several factors, such as the number of masks or input audio length. Fig. 3 shows the proportion of time spent on the encoder and decoder process and the inference time for each audio length, eval- uated on the LS-960 dataset. Compared to Fig. 2a, the decoder-to- We investigated the correlation between the WER and RTF as shown in Fig. 4, by changing the beam size from 1 to 20, and measured the WER and RTF using the test-clean dataset from Librispeech on the E-Branchformer based pre-trained models for LS-100 and LS-960 datasets. Comparing the AR models, the RTF differs greatly because the model sizes are different, however, the RTF of the PAR method remained almost the same. This is because the inference time of PAR depends on the computation time of the models itself. Therefore, the PAR curves have a significantly lower RTF compared to the AR curves, and the change in RTF is negligible as the beam-size changes, hence the narrower width. Although the RTF remains constant, we still see the differences in WER for the PAR models, especially with the pre-trained model of the LS-100 dataset compared to the model for the LS-960 dataset. This is due to the accuracy degradation problem we mention in Section 5.3.1. 5.2.2. NAR and PAR Comparing NAR and PAR, we can see that PAR is not as fast as NAR, as shown in Table 4. The number of decoder iterations is 10, which is larger than max iteration for PAR, but it performs faster than PAR. One reason for this is that the size of the decoder input is different. In this work, we added a dummy hypothesis as men- tioned in Section 4.2. As a result, the batch size of decoder input for PAR decoding is S \u00d7 B, where the batch size for NAR decoding is S. Therefore, it seems that the computation time per decoder is Fig. 4: The comparison of WER and RTF measured using the AR and PAR methods. We used the models trained with LS-100 and LS-960 datasets and measured by changing the beam size between 1 and 20. shorter for Mask-CTC NAR, and the resulting decoder processing time becomes faster. In terms of accuracy, we can see that PAR outperforms NAR. Improvement in performance is similar to that of E-branchformer models in both RTF and WER, and it was confirmed that there was no difference in improvement due to architecture differences. From these results, it is evident that applying the PAR method can solve the built-in accuracy issue for the NAR method we mentioned in Section 3.2. In particular, we can confirm that PAR obtains a speed between AR and NAR but achieves similar accuracy to AR. There- fore, a new trade-off balance that is not present in AR and NAR has been realized. Table 4: Comparison with NAR. The speedup shows the speedup from the AR decoding. Model RTF (\u2193) WER [%] (\u2193) Speedup (\u2191) AR NAR PAR CTC/Attention CTC Mask-CTC CTC/Attention 0.198 (0.080) 0.005 (0.004) 0.008 (0.005) 0.014 (0.009) 6.7 / 18.3 / 7.0 / 18.6 7.7 / 21.0 / 7.9 / 21.4 7.1 / 20.8 / 7.5 / 21.0 6.2 / 18.5 / 6.6 / 18.7 1.00\u00d7 39.60\u00d7 24.75\u00d7 14.14\u00d7 5.3. Limitations 5.3.1. Accuracy with PAR The accuracy of PAR can be degraded if the gCTC result is not ac- curate. If the result of gCTC is incorrect with high confidence, we cannot use the AR process to refine the gCTC result. From the com- parison of the LS-100 and LS-960 models in Table 3, we can see that more accuracy degradation can occur with the LS-100 model. The accuracy may also degrade at a higher Pthres because the number of target tokens per mask may exceed max iteration. Since we stop the beam search after max iteration iterations, if the number of target tokens exceeds max iteration, we cannot predict the entire sequence for one mask. Therefore, the accuracy may be degraded if the Pthres is closer to 1.0. Fig. 5 describes the relationship between WER and Pthres for max iteration. Using a max iteration of 5, we observe the accuracy degrades at higher Pthres levels. This issue is caused by the lack of beam search iterations, so to solve this prob- lem, we need to increase the max iteration. In Fig. 5, we increased the max iteration to 8 and observed a more accurate result. Fig. 5: The relationship between the WER and Pthres. We evalu- ated by changing the Pthres from 0.95 to 0.999. We used the E- Branchformer-based pre-trained model for LS-960. 5.3.2. Memory usage It is important to note the standard deviation of memory usage in Table 3 increases greatly compared to AR. Since the decoder process in PAR is computed simultaneously for all masks, we need more GPU memory if there are many masks. Therefore if the number of masks increases due to long audio inputs, high Pthres, or low accuracy of the gCTC result, we may get an out-of-memory error as GPU memory is exceeded during inference. Considering that the inference of all masks does not depend on each other, it is possible to alleviate this issue by using multiple GPUs to perform inference. 5.3.3. Segment-level Vectorized Beam Search If a masked sequence contains multiple masks, the predicted tokens for the second or later masks may not be accurate"}, {"question": " What is one reason why NAR performs faster than PAR?", "answer": " The size of the decoder input is different", "ref_chunk": "While the accuracy has slightly degraded, this is due to the accuracy of the gCTC result. We will explain it fur- ther in detail in the following limitations section 5.3.1. In terms of the RTF, we have achieved approximately 10 times faster inference compared to AR. In particular, since the inference speed does not largely depend on the audio length, the standard deviation is approx- imately 17.5% of that of AR decoding. This feature is more effec- tive for longer audio and in the test-clean dataset, and we observed 89.7\u00d7 speed up for 29 second audio as the maximum speedup. Note that this speedup depends on several factors, such as the number of masks or input audio length. Fig. 3 shows the proportion of time spent on the encoder and decoder process and the inference time for each audio length, eval- uated on the LS-960 dataset. Compared to Fig. 2a, the decoder-to- We investigated the correlation between the WER and RTF as shown in Fig. 4, by changing the beam size from 1 to 20, and measured the WER and RTF using the test-clean dataset from Librispeech on the E-Branchformer based pre-trained models for LS-100 and LS-960 datasets. Comparing the AR models, the RTF differs greatly because the model sizes are different, however, the RTF of the PAR method remained almost the same. This is because the inference time of PAR depends on the computation time of the models itself. Therefore, the PAR curves have a significantly lower RTF compared to the AR curves, and the change in RTF is negligible as the beam-size changes, hence the narrower width. Although the RTF remains constant, we still see the differences in WER for the PAR models, especially with the pre-trained model of the LS-100 dataset compared to the model for the LS-960 dataset. This is due to the accuracy degradation problem we mention in Section 5.3.1. 5.2.2. NAR and PAR Comparing NAR and PAR, we can see that PAR is not as fast as NAR, as shown in Table 4. The number of decoder iterations is 10, which is larger than max iteration for PAR, but it performs faster than PAR. One reason for this is that the size of the decoder input is different. In this work, we added a dummy hypothesis as men- tioned in Section 4.2. As a result, the batch size of decoder input for PAR decoding is S \u00d7 B, where the batch size for NAR decoding is S. Therefore, it seems that the computation time per decoder is Fig. 4: The comparison of WER and RTF measured using the AR and PAR methods. We used the models trained with LS-100 and LS-960 datasets and measured by changing the beam size between 1 and 20. shorter for Mask-CTC NAR, and the resulting decoder processing time becomes faster. In terms of accuracy, we can see that PAR outperforms NAR. Improvement in performance is similar to that of E-branchformer models in both RTF and WER, and it was confirmed that there was no difference in improvement due to architecture differences. From these results, it is evident that applying the PAR method can solve the built-in accuracy issue for the NAR method we mentioned in Section 3.2. In particular, we can confirm that PAR obtains a speed between AR and NAR but achieves similar accuracy to AR. There- fore, a new trade-off balance that is not present in AR and NAR has been realized. Table 4: Comparison with NAR. The speedup shows the speedup from the AR decoding. Model RTF (\u2193) WER [%] (\u2193) Speedup (\u2191) AR NAR PAR CTC/Attention CTC Mask-CTC CTC/Attention 0.198 (0.080) 0.005 (0.004) 0.008 (0.005) 0.014 (0.009) 6.7 / 18.3 / 7.0 / 18.6 7.7 / 21.0 / 7.9 / 21.4 7.1 / 20.8 / 7.5 / 21.0 6.2 / 18.5 / 6.6 / 18.7 1.00\u00d7 39.60\u00d7 24.75\u00d7 14.14\u00d7 5.3. Limitations 5.3.1. Accuracy with PAR The accuracy of PAR can be degraded if the gCTC result is not ac- curate. If the result of gCTC is incorrect with high confidence, we cannot use the AR process to refine the gCTC result. From the com- parison of the LS-100 and LS-960 models in Table 3, we can see that more accuracy degradation can occur with the LS-100 model. The accuracy may also degrade at a higher Pthres because the number of target tokens per mask may exceed max iteration. Since we stop the beam search after max iteration iterations, if the number of target tokens exceeds max iteration, we cannot predict the entire sequence for one mask. Therefore, the accuracy may be degraded if the Pthres is closer to 1.0. Fig. 5 describes the relationship between WER and Pthres for max iteration. Using a max iteration of 5, we observe the accuracy degrades at higher Pthres levels. This issue is caused by the lack of beam search iterations, so to solve this prob- lem, we need to increase the max iteration. In Fig. 5, we increased the max iteration to 8 and observed a more accurate result. Fig. 5: The relationship between the WER and Pthres. We evalu- ated by changing the Pthres from 0.95 to 0.999. We used the E- Branchformer-based pre-trained model for LS-960. 5.3.2. Memory usage It is important to note the standard deviation of memory usage in Table 3 increases greatly compared to AR. Since the decoder process in PAR is computed simultaneously for all masks, we need more GPU memory if there are many masks. Therefore if the number of masks increases due to long audio inputs, high Pthres, or low accuracy of the gCTC result, we may get an out-of-memory error as GPU memory is exceeded during inference. Considering that the inference of all masks does not depend on each other, it is possible to alleviate this issue by using multiple GPUs to perform inference. 5.3.3. Segment-level Vectorized Beam Search If a masked sequence contains multiple masks, the predicted tokens for the second or later masks may not be accurate"}, {"question": " What issue can occur with the accuracy of PAR if the gCTC result is not accurate?", "answer": " The accuracy of PAR can be degraded", "ref_chunk": "While the accuracy has slightly degraded, this is due to the accuracy of the gCTC result. We will explain it fur- ther in detail in the following limitations section 5.3.1. In terms of the RTF, we have achieved approximately 10 times faster inference compared to AR. In particular, since the inference speed does not largely depend on the audio length, the standard deviation is approx- imately 17.5% of that of AR decoding. This feature is more effec- tive for longer audio and in the test-clean dataset, and we observed 89.7\u00d7 speed up for 29 second audio as the maximum speedup. Note that this speedup depends on several factors, such as the number of masks or input audio length. Fig. 3 shows the proportion of time spent on the encoder and decoder process and the inference time for each audio length, eval- uated on the LS-960 dataset. Compared to Fig. 2a, the decoder-to- We investigated the correlation between the WER and RTF as shown in Fig. 4, by changing the beam size from 1 to 20, and measured the WER and RTF using the test-clean dataset from Librispeech on the E-Branchformer based pre-trained models for LS-100 and LS-960 datasets. Comparing the AR models, the RTF differs greatly because the model sizes are different, however, the RTF of the PAR method remained almost the same. This is because the inference time of PAR depends on the computation time of the models itself. Therefore, the PAR curves have a significantly lower RTF compared to the AR curves, and the change in RTF is negligible as the beam-size changes, hence the narrower width. Although the RTF remains constant, we still see the differences in WER for the PAR models, especially with the pre-trained model of the LS-100 dataset compared to the model for the LS-960 dataset. This is due to the accuracy degradation problem we mention in Section 5.3.1. 5.2.2. NAR and PAR Comparing NAR and PAR, we can see that PAR is not as fast as NAR, as shown in Table 4. The number of decoder iterations is 10, which is larger than max iteration for PAR, but it performs faster than PAR. One reason for this is that the size of the decoder input is different. In this work, we added a dummy hypothesis as men- tioned in Section 4.2. As a result, the batch size of decoder input for PAR decoding is S \u00d7 B, where the batch size for NAR decoding is S. Therefore, it seems that the computation time per decoder is Fig. 4: The comparison of WER and RTF measured using the AR and PAR methods. We used the models trained with LS-100 and LS-960 datasets and measured by changing the beam size between 1 and 20. shorter for Mask-CTC NAR, and the resulting decoder processing time becomes faster. In terms of accuracy, we can see that PAR outperforms NAR. Improvement in performance is similar to that of E-branchformer models in both RTF and WER, and it was confirmed that there was no difference in improvement due to architecture differences. From these results, it is evident that applying the PAR method can solve the built-in accuracy issue for the NAR method we mentioned in Section 3.2. In particular, we can confirm that PAR obtains a speed between AR and NAR but achieves similar accuracy to AR. There- fore, a new trade-off balance that is not present in AR and NAR has been realized. Table 4: Comparison with NAR. The speedup shows the speedup from the AR decoding. Model RTF (\u2193) WER [%] (\u2193) Speedup (\u2191) AR NAR PAR CTC/Attention CTC Mask-CTC CTC/Attention 0.198 (0.080) 0.005 (0.004) 0.008 (0.005) 0.014 (0.009) 6.7 / 18.3 / 7.0 / 18.6 7.7 / 21.0 / 7.9 / 21.4 7.1 / 20.8 / 7.5 / 21.0 6.2 / 18.5 / 6.6 / 18.7 1.00\u00d7 39.60\u00d7 24.75\u00d7 14.14\u00d7 5.3. Limitations 5.3.1. Accuracy with PAR The accuracy of PAR can be degraded if the gCTC result is not ac- curate. If the result of gCTC is incorrect with high confidence, we cannot use the AR process to refine the gCTC result. From the com- parison of the LS-100 and LS-960 models in Table 3, we can see that more accuracy degradation can occur with the LS-100 model. The accuracy may also degrade at a higher Pthres because the number of target tokens per mask may exceed max iteration. Since we stop the beam search after max iteration iterations, if the number of target tokens exceeds max iteration, we cannot predict the entire sequence for one mask. Therefore, the accuracy may be degraded if the Pthres is closer to 1.0. Fig. 5 describes the relationship between WER and Pthres for max iteration. Using a max iteration of 5, we observe the accuracy degrades at higher Pthres levels. This issue is caused by the lack of beam search iterations, so to solve this prob- lem, we need to increase the max iteration. In Fig. 5, we increased the max iteration to 8 and observed a more accurate result. Fig. 5: The relationship between the WER and Pthres. We evalu- ated by changing the Pthres from 0.95 to 0.999. We used the E- Branchformer-based pre-trained model for LS-960. 5.3.2. Memory usage It is important to note the standard deviation of memory usage in Table 3 increases greatly compared to AR. Since the decoder process in PAR is computed simultaneously for all masks, we need more GPU memory if there are many masks. Therefore if the number of masks increases due to long audio inputs, high Pthres, or low accuracy of the gCTC result, we may get an out-of-memory error as GPU memory is exceeded during inference. Considering that the inference of all masks does not depend on each other, it is possible to alleviate this issue by using multiple GPUs to perform inference. 5.3.3. Segment-level Vectorized Beam Search If a masked sequence contains multiple masks, the predicted tokens for the second or later masks may not be accurate"}, {"question": " What happens if the number of target tokens per mask exceeds max iteration in PAR?", "answer": " The accuracy may degrade", "ref_chunk": "While the accuracy has slightly degraded, this is due to the accuracy of the gCTC result. We will explain it fur- ther in detail in the following limitations section 5.3.1. In terms of the RTF, we have achieved approximately 10 times faster inference compared to AR. In particular, since the inference speed does not largely depend on the audio length, the standard deviation is approx- imately 17.5% of that of AR decoding. This feature is more effec- tive for longer audio and in the test-clean dataset, and we observed 89.7\u00d7 speed up for 29 second audio as the maximum speedup. Note that this speedup depends on several factors, such as the number of masks or input audio length. Fig. 3 shows the proportion of time spent on the encoder and decoder process and the inference time for each audio length, eval- uated on the LS-960 dataset. Compared to Fig. 2a, the decoder-to- We investigated the correlation between the WER and RTF as shown in Fig. 4, by changing the beam size from 1 to 20, and measured the WER and RTF using the test-clean dataset from Librispeech on the E-Branchformer based pre-trained models for LS-100 and LS-960 datasets. Comparing the AR models, the RTF differs greatly because the model sizes are different, however, the RTF of the PAR method remained almost the same. This is because the inference time of PAR depends on the computation time of the models itself. Therefore, the PAR curves have a significantly lower RTF compared to the AR curves, and the change in RTF is negligible as the beam-size changes, hence the narrower width. Although the RTF remains constant, we still see the differences in WER for the PAR models, especially with the pre-trained model of the LS-100 dataset compared to the model for the LS-960 dataset. This is due to the accuracy degradation problem we mention in Section 5.3.1. 5.2.2. NAR and PAR Comparing NAR and PAR, we can see that PAR is not as fast as NAR, as shown in Table 4. The number of decoder iterations is 10, which is larger than max iteration for PAR, but it performs faster than PAR. One reason for this is that the size of the decoder input is different. In this work, we added a dummy hypothesis as men- tioned in Section 4.2. As a result, the batch size of decoder input for PAR decoding is S \u00d7 B, where the batch size for NAR decoding is S. Therefore, it seems that the computation time per decoder is Fig. 4: The comparison of WER and RTF measured using the AR and PAR methods. We used the models trained with LS-100 and LS-960 datasets and measured by changing the beam size between 1 and 20. shorter for Mask-CTC NAR, and the resulting decoder processing time becomes faster. In terms of accuracy, we can see that PAR outperforms NAR. Improvement in performance is similar to that of E-branchformer models in both RTF and WER, and it was confirmed that there was no difference in improvement due to architecture differences. From these results, it is evident that applying the PAR method can solve the built-in accuracy issue for the NAR method we mentioned in Section 3.2. In particular, we can confirm that PAR obtains a speed between AR and NAR but achieves similar accuracy to AR. There- fore, a new trade-off balance that is not present in AR and NAR has been realized. Table 4: Comparison with NAR. The speedup shows the speedup from the AR decoding. Model RTF (\u2193) WER [%] (\u2193) Speedup (\u2191) AR NAR PAR CTC/Attention CTC Mask-CTC CTC/Attention 0.198 (0.080) 0.005 (0.004) 0.008 (0.005) 0.014 (0.009) 6.7 / 18.3 / 7.0 / 18.6 7.7 / 21.0 / 7.9 / 21.4 7.1 / 20.8 / 7.5 / 21.0 6.2 / 18.5 / 6.6 / 18.7 1.00\u00d7 39.60\u00d7 24.75\u00d7 14.14\u00d7 5.3. Limitations 5.3.1. Accuracy with PAR The accuracy of PAR can be degraded if the gCTC result is not ac- curate. If the result of gCTC is incorrect with high confidence, we cannot use the AR process to refine the gCTC result. From the com- parison of the LS-100 and LS-960 models in Table 3, we can see that more accuracy degradation can occur with the LS-100 model. The accuracy may also degrade at a higher Pthres because the number of target tokens per mask may exceed max iteration. Since we stop the beam search after max iteration iterations, if the number of target tokens exceeds max iteration, we cannot predict the entire sequence for one mask. Therefore, the accuracy may be degraded if the Pthres is closer to 1.0. Fig. 5 describes the relationship between WER and Pthres for max iteration. Using a max iteration of 5, we observe the accuracy degrades at higher Pthres levels. This issue is caused by the lack of beam search iterations, so to solve this prob- lem, we need to increase the max iteration. In Fig. 5, we increased the max iteration to 8 and observed a more accurate result. Fig. 5: The relationship between the WER and Pthres. We evalu- ated by changing the Pthres from 0.95 to 0.999. We used the E- Branchformer-based pre-trained model for LS-960. 5.3.2. Memory usage It is important to note the standard deviation of memory usage in Table 3 increases greatly compared to AR. Since the decoder process in PAR is computed simultaneously for all masks, we need more GPU memory if there are many masks. Therefore if the number of masks increases due to long audio inputs, high Pthres, or low accuracy of the gCTC result, we may get an out-of-memory error as GPU memory is exceeded during inference. Considering that the inference of all masks does not depend on each other, it is possible to alleviate this issue by using multiple GPUs to perform inference. 5.3.3. Segment-level Vectorized Beam Search If a masked sequence contains multiple masks, the predicted tokens for the second or later masks may not be accurate"}, {"question": " How did the authors solve the accuracy degradation problem in PAR?", "answer": " By increasing the max iteration", "ref_chunk": "While the accuracy has slightly degraded, this is due to the accuracy of the gCTC result. We will explain it fur- ther in detail in the following limitations section 5.3.1. In terms of the RTF, we have achieved approximately 10 times faster inference compared to AR. In particular, since the inference speed does not largely depend on the audio length, the standard deviation is approx- imately 17.5% of that of AR decoding. This feature is more effec- tive for longer audio and in the test-clean dataset, and we observed 89.7\u00d7 speed up for 29 second audio as the maximum speedup. Note that this speedup depends on several factors, such as the number of masks or input audio length. Fig. 3 shows the proportion of time spent on the encoder and decoder process and the inference time for each audio length, eval- uated on the LS-960 dataset. Compared to Fig. 2a, the decoder-to- We investigated the correlation between the WER and RTF as shown in Fig. 4, by changing the beam size from 1 to 20, and measured the WER and RTF using the test-clean dataset from Librispeech on the E-Branchformer based pre-trained models for LS-100 and LS-960 datasets. Comparing the AR models, the RTF differs greatly because the model sizes are different, however, the RTF of the PAR method remained almost the same. This is because the inference time of PAR depends on the computation time of the models itself. Therefore, the PAR curves have a significantly lower RTF compared to the AR curves, and the change in RTF is negligible as the beam-size changes, hence the narrower width. Although the RTF remains constant, we still see the differences in WER for the PAR models, especially with the pre-trained model of the LS-100 dataset compared to the model for the LS-960 dataset. This is due to the accuracy degradation problem we mention in Section 5.3.1. 5.2.2. NAR and PAR Comparing NAR and PAR, we can see that PAR is not as fast as NAR, as shown in Table 4. The number of decoder iterations is 10, which is larger than max iteration for PAR, but it performs faster than PAR. One reason for this is that the size of the decoder input is different. In this work, we added a dummy hypothesis as men- tioned in Section 4.2. As a result, the batch size of decoder input for PAR decoding is S \u00d7 B, where the batch size for NAR decoding is S. Therefore, it seems that the computation time per decoder is Fig. 4: The comparison of WER and RTF measured using the AR and PAR methods. We used the models trained with LS-100 and LS-960 datasets and measured by changing the beam size between 1 and 20. shorter for Mask-CTC NAR, and the resulting decoder processing time becomes faster. In terms of accuracy, we can see that PAR outperforms NAR. Improvement in performance is similar to that of E-branchformer models in both RTF and WER, and it was confirmed that there was no difference in improvement due to architecture differences. From these results, it is evident that applying the PAR method can solve the built-in accuracy issue for the NAR method we mentioned in Section 3.2. In particular, we can confirm that PAR obtains a speed between AR and NAR but achieves similar accuracy to AR. There- fore, a new trade-off balance that is not present in AR and NAR has been realized. Table 4: Comparison with NAR. The speedup shows the speedup from the AR decoding. Model RTF (\u2193) WER [%] (\u2193) Speedup (\u2191) AR NAR PAR CTC/Attention CTC Mask-CTC CTC/Attention 0.198 (0.080) 0.005 (0.004) 0.008 (0.005) 0.014 (0.009) 6.7 / 18.3 / 7.0 / 18.6 7.7 / 21.0 / 7.9 / 21.4 7.1 / 20.8 / 7.5 / 21.0 6.2 / 18.5 / 6.6 / 18.7 1.00\u00d7 39.60\u00d7 24.75\u00d7 14.14\u00d7 5.3. Limitations 5.3.1. Accuracy with PAR The accuracy of PAR can be degraded if the gCTC result is not ac- curate. If the result of gCTC is incorrect with high confidence, we cannot use the AR process to refine the gCTC result. From the com- parison of the LS-100 and LS-960 models in Table 3, we can see that more accuracy degradation can occur with the LS-100 model. The accuracy may also degrade at a higher Pthres because the number of target tokens per mask may exceed max iteration. Since we stop the beam search after max iteration iterations, if the number of target tokens exceeds max iteration, we cannot predict the entire sequence for one mask. Therefore, the accuracy may be degraded if the Pthres is closer to 1.0. Fig. 5 describes the relationship between WER and Pthres for max iteration. Using a max iteration of 5, we observe the accuracy degrades at higher Pthres levels. This issue is caused by the lack of beam search iterations, so to solve this prob- lem, we need to increase the max iteration. In Fig. 5, we increased the max iteration to 8 and observed a more accurate result. Fig. 5: The relationship between the WER and Pthres. We evalu- ated by changing the Pthres from 0.95 to 0.999. We used the E- Branchformer-based pre-trained model for LS-960. 5.3.2. Memory usage It is important to note the standard deviation of memory usage in Table 3 increases greatly compared to AR. Since the decoder process in PAR is computed simultaneously for all masks, we need more GPU memory if there are many masks. Therefore if the number of masks increases due to long audio inputs, high Pthres, or low accuracy of the gCTC result, we may get an out-of-memory error as GPU memory is exceeded during inference. Considering that the inference of all masks does not depend on each other, it is possible to alleviate this issue by using multiple GPUs to perform inference. 5.3.3. Segment-level Vectorized Beam Search If a masked sequence contains multiple masks, the predicted tokens for the second or later masks may not be accurate"}, {"question": " Why may an out-of-memory error occur in PAR during inference?", "answer": " If there are many masks requiring more GPU memory", "ref_chunk": "While the accuracy has slightly degraded, this is due to the accuracy of the gCTC result. We will explain it fur- ther in detail in the following limitations section 5.3.1. In terms of the RTF, we have achieved approximately 10 times faster inference compared to AR. In particular, since the inference speed does not largely depend on the audio length, the standard deviation is approx- imately 17.5% of that of AR decoding. This feature is more effec- tive for longer audio and in the test-clean dataset, and we observed 89.7\u00d7 speed up for 29 second audio as the maximum speedup. Note that this speedup depends on several factors, such as the number of masks or input audio length. Fig. 3 shows the proportion of time spent on the encoder and decoder process and the inference time for each audio length, eval- uated on the LS-960 dataset. Compared to Fig. 2a, the decoder-to- We investigated the correlation between the WER and RTF as shown in Fig. 4, by changing the beam size from 1 to 20, and measured the WER and RTF using the test-clean dataset from Librispeech on the E-Branchformer based pre-trained models for LS-100 and LS-960 datasets. Comparing the AR models, the RTF differs greatly because the model sizes are different, however, the RTF of the PAR method remained almost the same. This is because the inference time of PAR depends on the computation time of the models itself. Therefore, the PAR curves have a significantly lower RTF compared to the AR curves, and the change in RTF is negligible as the beam-size changes, hence the narrower width. Although the RTF remains constant, we still see the differences in WER for the PAR models, especially with the pre-trained model of the LS-100 dataset compared to the model for the LS-960 dataset. This is due to the accuracy degradation problem we mention in Section 5.3.1. 5.2.2. NAR and PAR Comparing NAR and PAR, we can see that PAR is not as fast as NAR, as shown in Table 4. The number of decoder iterations is 10, which is larger than max iteration for PAR, but it performs faster than PAR. One reason for this is that the size of the decoder input is different. In this work, we added a dummy hypothesis as men- tioned in Section 4.2. As a result, the batch size of decoder input for PAR decoding is S \u00d7 B, where the batch size for NAR decoding is S. Therefore, it seems that the computation time per decoder is Fig. 4: The comparison of WER and RTF measured using the AR and PAR methods. We used the models trained with LS-100 and LS-960 datasets and measured by changing the beam size between 1 and 20. shorter for Mask-CTC NAR, and the resulting decoder processing time becomes faster. In terms of accuracy, we can see that PAR outperforms NAR. Improvement in performance is similar to that of E-branchformer models in both RTF and WER, and it was confirmed that there was no difference in improvement due to architecture differences. From these results, it is evident that applying the PAR method can solve the built-in accuracy issue for the NAR method we mentioned in Section 3.2. In particular, we can confirm that PAR obtains a speed between AR and NAR but achieves similar accuracy to AR. There- fore, a new trade-off balance that is not present in AR and NAR has been realized. Table 4: Comparison with NAR. The speedup shows the speedup from the AR decoding. Model RTF (\u2193) WER [%] (\u2193) Speedup (\u2191) AR NAR PAR CTC/Attention CTC Mask-CTC CTC/Attention 0.198 (0.080) 0.005 (0.004) 0.008 (0.005) 0.014 (0.009) 6.7 / 18.3 / 7.0 / 18.6 7.7 / 21.0 / 7.9 / 21.4 7.1 / 20.8 / 7.5 / 21.0 6.2 / 18.5 / 6.6 / 18.7 1.00\u00d7 39.60\u00d7 24.75\u00d7 14.14\u00d7 5.3. Limitations 5.3.1. Accuracy with PAR The accuracy of PAR can be degraded if the gCTC result is not ac- curate. If the result of gCTC is incorrect with high confidence, we cannot use the AR process to refine the gCTC result. From the com- parison of the LS-100 and LS-960 models in Table 3, we can see that more accuracy degradation can occur with the LS-100 model. The accuracy may also degrade at a higher Pthres because the number of target tokens per mask may exceed max iteration. Since we stop the beam search after max iteration iterations, if the number of target tokens exceeds max iteration, we cannot predict the entire sequence for one mask. Therefore, the accuracy may be degraded if the Pthres is closer to 1.0. Fig. 5 describes the relationship between WER and Pthres for max iteration. Using a max iteration of 5, we observe the accuracy degrades at higher Pthres levels. This issue is caused by the lack of beam search iterations, so to solve this prob- lem, we need to increase the max iteration. In Fig. 5, we increased the max iteration to 8 and observed a more accurate result. Fig. 5: The relationship between the WER and Pthres. We evalu- ated by changing the Pthres from 0.95 to 0.999. We used the E- Branchformer-based pre-trained model for LS-960. 5.3.2. Memory usage It is important to note the standard deviation of memory usage in Table 3 increases greatly compared to AR. Since the decoder process in PAR is computed simultaneously for all masks, we need more GPU memory if there are many masks. Therefore if the number of masks increases due to long audio inputs, high Pthres, or low accuracy of the gCTC result, we may get an out-of-memory error as GPU memory is exceeded during inference. Considering that the inference of all masks does not depend on each other, it is possible to alleviate this issue by using multiple GPUs to perform inference. 5.3.3. Segment-level Vectorized Beam Search If a masked sequence contains multiple masks, the predicted tokens for the second or later masks may not be accurate"}, {"question": " How can the issue of GPU memory exceeding be alleviated in PAR?", "answer": " By using multiple GPUs to perform inference", "ref_chunk": "While the accuracy has slightly degraded, this is due to the accuracy of the gCTC result. We will explain it fur- ther in detail in the following limitations section 5.3.1. In terms of the RTF, we have achieved approximately 10 times faster inference compared to AR. In particular, since the inference speed does not largely depend on the audio length, the standard deviation is approx- imately 17.5% of that of AR decoding. This feature is more effec- tive for longer audio and in the test-clean dataset, and we observed 89.7\u00d7 speed up for 29 second audio as the maximum speedup. Note that this speedup depends on several factors, such as the number of masks or input audio length. Fig. 3 shows the proportion of time spent on the encoder and decoder process and the inference time for each audio length, eval- uated on the LS-960 dataset. Compared to Fig. 2a, the decoder-to- We investigated the correlation between the WER and RTF as shown in Fig. 4, by changing the beam size from 1 to 20, and measured the WER and RTF using the test-clean dataset from Librispeech on the E-Branchformer based pre-trained models for LS-100 and LS-960 datasets. Comparing the AR models, the RTF differs greatly because the model sizes are different, however, the RTF of the PAR method remained almost the same. This is because the inference time of PAR depends on the computation time of the models itself. Therefore, the PAR curves have a significantly lower RTF compared to the AR curves, and the change in RTF is negligible as the beam-size changes, hence the narrower width. Although the RTF remains constant, we still see the differences in WER for the PAR models, especially with the pre-trained model of the LS-100 dataset compared to the model for the LS-960 dataset. This is due to the accuracy degradation problem we mention in Section 5.3.1. 5.2.2. NAR and PAR Comparing NAR and PAR, we can see that PAR is not as fast as NAR, as shown in Table 4. The number of decoder iterations is 10, which is larger than max iteration for PAR, but it performs faster than PAR. One reason for this is that the size of the decoder input is different. In this work, we added a dummy hypothesis as men- tioned in Section 4.2. As a result, the batch size of decoder input for PAR decoding is S \u00d7 B, where the batch size for NAR decoding is S. Therefore, it seems that the computation time per decoder is Fig. 4: The comparison of WER and RTF measured using the AR and PAR methods. We used the models trained with LS-100 and LS-960 datasets and measured by changing the beam size between 1 and 20. shorter for Mask-CTC NAR, and the resulting decoder processing time becomes faster. In terms of accuracy, we can see that PAR outperforms NAR. Improvement in performance is similar to that of E-branchformer models in both RTF and WER, and it was confirmed that there was no difference in improvement due to architecture differences. From these results, it is evident that applying the PAR method can solve the built-in accuracy issue for the NAR method we mentioned in Section 3.2. In particular, we can confirm that PAR obtains a speed between AR and NAR but achieves similar accuracy to AR. There- fore, a new trade-off balance that is not present in AR and NAR has been realized. Table 4: Comparison with NAR. The speedup shows the speedup from the AR decoding. Model RTF (\u2193) WER [%] (\u2193) Speedup (\u2191) AR NAR PAR CTC/Attention CTC Mask-CTC CTC/Attention 0.198 (0.080) 0.005 (0.004) 0.008 (0.005) 0.014 (0.009) 6.7 / 18.3 / 7.0 / 18.6 7.7 / 21.0 / 7.9 / 21.4 7.1 / 20.8 / 7.5 / 21.0 6.2 / 18.5 / 6.6 / 18.7 1.00\u00d7 39.60\u00d7 24.75\u00d7 14.14\u00d7 5.3. Limitations 5.3.1. Accuracy with PAR The accuracy of PAR can be degraded if the gCTC result is not ac- curate. If the result of gCTC is incorrect with high confidence, we cannot use the AR process to refine the gCTC result. From the com- parison of the LS-100 and LS-960 models in Table 3, we can see that more accuracy degradation can occur with the LS-100 model. The accuracy may also degrade at a higher Pthres because the number of target tokens per mask may exceed max iteration. Since we stop the beam search after max iteration iterations, if the number of target tokens exceeds max iteration, we cannot predict the entire sequence for one mask. Therefore, the accuracy may be degraded if the Pthres is closer to 1.0. Fig. 5 describes the relationship between WER and Pthres for max iteration. Using a max iteration of 5, we observe the accuracy degrades at higher Pthres levels. This issue is caused by the lack of beam search iterations, so to solve this prob- lem, we need to increase the max iteration. In Fig. 5, we increased the max iteration to 8 and observed a more accurate result. Fig. 5: The relationship between the WER and Pthres. We evalu- ated by changing the Pthres from 0.95 to 0.999. We used the E- Branchformer-based pre-trained model for LS-960. 5.3.2. Memory usage It is important to note the standard deviation of memory usage in Table 3 increases greatly compared to AR. Since the decoder process in PAR is computed simultaneously for all masks, we need more GPU memory if there are many masks. Therefore if the number of masks increases due to long audio inputs, high Pthres, or low accuracy of the gCTC result, we may get an out-of-memory error as GPU memory is exceeded during inference. Considering that the inference of all masks does not depend on each other, it is possible to alleviate this issue by using multiple GPUs to perform inference. 5.3.3. Segment-level Vectorized Beam Search If a masked sequence contains multiple masks, the predicted tokens for the second or later masks may not be accurate"}], "doc_text": "While the accuracy has slightly degraded, this is due to the accuracy of the gCTC result. We will explain it fur- ther in detail in the following limitations section 5.3.1. In terms of the RTF, we have achieved approximately 10 times faster inference compared to AR. In particular, since the inference speed does not largely depend on the audio length, the standard deviation is approx- imately 17.5% of that of AR decoding. This feature is more effec- tive for longer audio and in the test-clean dataset, and we observed 89.7\u00d7 speed up for 29 second audio as the maximum speedup. Note that this speedup depends on several factors, such as the number of masks or input audio length. Fig. 3 shows the proportion of time spent on the encoder and decoder process and the inference time for each audio length, eval- uated on the LS-960 dataset. Compared to Fig. 2a, the decoder-to- We investigated the correlation between the WER and RTF as shown in Fig. 4, by changing the beam size from 1 to 20, and measured the WER and RTF using the test-clean dataset from Librispeech on the E-Branchformer based pre-trained models for LS-100 and LS-960 datasets. Comparing the AR models, the RTF differs greatly because the model sizes are different, however, the RTF of the PAR method remained almost the same. This is because the inference time of PAR depends on the computation time of the models itself. Therefore, the PAR curves have a significantly lower RTF compared to the AR curves, and the change in RTF is negligible as the beam-size changes, hence the narrower width. Although the RTF remains constant, we still see the differences in WER for the PAR models, especially with the pre-trained model of the LS-100 dataset compared to the model for the LS-960 dataset. This is due to the accuracy degradation problem we mention in Section 5.3.1. 5.2.2. NAR and PAR Comparing NAR and PAR, we can see that PAR is not as fast as NAR, as shown in Table 4. The number of decoder iterations is 10, which is larger than max iteration for PAR, but it performs faster than PAR. One reason for this is that the size of the decoder input is different. In this work, we added a dummy hypothesis as men- tioned in Section 4.2. As a result, the batch size of decoder input for PAR decoding is S \u00d7 B, where the batch size for NAR decoding is S. Therefore, it seems that the computation time per decoder is Fig. 4: The comparison of WER and RTF measured using the AR and PAR methods. We used the models trained with LS-100 and LS-960 datasets and measured by changing the beam size between 1 and 20. shorter for Mask-CTC NAR, and the resulting decoder processing time becomes faster. In terms of accuracy, we can see that PAR outperforms NAR. Improvement in performance is similar to that of E-branchformer models in both RTF and WER, and it was confirmed that there was no difference in improvement due to architecture differences. From these results, it is evident that applying the PAR method can solve the built-in accuracy issue for the NAR method we mentioned in Section 3.2. In particular, we can confirm that PAR obtains a speed between AR and NAR but achieves similar accuracy to AR. There- fore, a new trade-off balance that is not present in AR and NAR has been realized. Table 4: Comparison with NAR. The speedup shows the speedup from the AR decoding. Model RTF (\u2193) WER [%] (\u2193) Speedup (\u2191) AR NAR PAR CTC/Attention CTC Mask-CTC CTC/Attention 0.198 (0.080) 0.005 (0.004) 0.008 (0.005) 0.014 (0.009) 6.7 / 18.3 / 7.0 / 18.6 7.7 / 21.0 / 7.9 / 21.4 7.1 / 20.8 / 7.5 / 21.0 6.2 / 18.5 / 6.6 / 18.7 1.00\u00d7 39.60\u00d7 24.75\u00d7 14.14\u00d7 5.3. Limitations 5.3.1. Accuracy with PAR The accuracy of PAR can be degraded if the gCTC result is not ac- curate. If the result of gCTC is incorrect with high confidence, we cannot use the AR process to refine the gCTC result. From the com- parison of the LS-100 and LS-960 models in Table 3, we can see that more accuracy degradation can occur with the LS-100 model. The accuracy may also degrade at a higher Pthres because the number of target tokens per mask may exceed max iteration. Since we stop the beam search after max iteration iterations, if the number of target tokens exceeds max iteration, we cannot predict the entire sequence for one mask. Therefore, the accuracy may be degraded if the Pthres is closer to 1.0. Fig. 5 describes the relationship between WER and Pthres for max iteration. Using a max iteration of 5, we observe the accuracy degrades at higher Pthres levels. This issue is caused by the lack of beam search iterations, so to solve this prob- lem, we need to increase the max iteration. In Fig. 5, we increased the max iteration to 8 and observed a more accurate result. Fig. 5: The relationship between the WER and Pthres. We evalu- ated by changing the Pthres from 0.95 to 0.999. We used the E- Branchformer-based pre-trained model for LS-960. 5.3.2. Memory usage It is important to note the standard deviation of memory usage in Table 3 increases greatly compared to AR. Since the decoder process in PAR is computed simultaneously for all masks, we need more GPU memory if there are many masks. Therefore if the number of masks increases due to long audio inputs, high Pthres, or low accuracy of the gCTC result, we may get an out-of-memory error as GPU memory is exceeded during inference. Considering that the inference of all masks does not depend on each other, it is possible to alleviate this issue by using multiple GPUs to perform inference. 5.3.3. Segment-level Vectorized Beam Search If a masked sequence contains multiple masks, the predicted tokens for the second or later masks may not be accurate"}