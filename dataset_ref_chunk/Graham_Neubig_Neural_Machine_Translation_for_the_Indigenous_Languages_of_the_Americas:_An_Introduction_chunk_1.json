{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Neural_Machine_Translation_for_the_Indigenous_Languages_of_the_Americas:_An_Introduction_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How many languages are native to the American continent?", "answer": " Approximately 900 languages.", "ref_chunk": "3 2 0 2 n u J 1 1 ] L C . s c [ 1 v 4 0 8 6 0 . 6 0 3 2 : v i X r a Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction. Manuel Mager\u2661\u2217 Rajat Bhatnagar\u2660 Graham Neubig\u266f Ngoc Thang Vu\u2662 Katharina Kann\u2660 \u2661AWS AI Labs \u266fCarnegie Mellon University \u2660University of Colorado Boulder \u2662University of Stuttgart Abstract Neural models have drastically advanced state of the art for machine translation (MT) be- tween high-resource languages. Traditionally, these models rely on large amounts of train- ing data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of par- allel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and tech- niques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open ques- tions, product of an increased interest of the NLP community in these languages. 1 Introduction More than 7 billion people on Earth communicate in nearly 7000 different languages (Pereltsvaig, 2020). Of these, approximately 900 languages are native of the American continent (Campbell, 2000). Most of these indigenous languages of the Americas (ILA) are endangered at some degree (Thomason, 2015). This huge variety in languages is simultaneously a rich treasure for humanity and also a barrier to communication among people from different backgrounds. Human translators have been important in overcoming language bar- riers. However, trained translators are not acces- sible to everyone on Earth and even scarcer for endangered and minority languages. The need for translations is even written in the constitutions of several countries like Mexico, Peru, Paraguay, Venezuela, and Bolivia (Zaj\u00edcov\u00e1, 2017) to allow native speakers to have equal language rights re- garding law. This is why developing MT is crucial: it helps humanity overcome language barriers while si- multaneously allowing people to continue using their native tongue. However, the challenges to achieving these problems are not trivial. It is not only the amount of available data (a common the- sis among the NLP community) but also a set of challenging issues (dialectical and orthographic variations, noisy texts, complex morphology, etc.) that must be addressed. MT has always been an important task within the larger area of natural language processing (NLP). In 1954, the Georgetown\u2013IBM experiment (Hutchins, 2004) was the first that showed at least some effectiveness of MT. Further research re- sulted in rule-based systems and statistical models. In 2023, neural models define state of the art for MT if training data is plentiful \u2013 i.e., for so-called high-resource languages (HRLs) \u2013 and have also achieved impressive results for low-resource lan- guages (LRLs). MT is also the most studied NLP task for the ILA (Mager et al., 2018b; Littell et al., 2018). The common issue among these languages is the extreme low-resource conditions they are confronted with. The research interest for these languages has increased in the last years, including the recent AmericasNLP 2021 shared task (Mager et al., 2021) on 10 indigenous languages to Span- ish, and the WMT (Conference on Machine Trans- lation) shared task for Inuktitut\u2013English (Barrault et al., 2020). In this work we aim to provide a comprehensive introduction to the challenges that involve creat- ing MT systems for ILA, and the current status of the existing work. We organize this work as follows: We start by introducing state-of-the-art NMT models (\u00a72). Then, we discuss the current challenges for these languages (\u00a73); and we in- troduce the key concepts related to low-resource NMT and the implications for endangered lan- guages of the Americas(\u00a73). This is followed by a discussion of available data (\u00a74). Afterwards, we introduce the important concepts for LRL and endangered languages (\u00a75); then we introduce the \u2217Work done while at the University of Stuttgart. main strategies aimed at improving NMT with limited training data (\u00a76); and finally we give an overview of the work done for ILA on MT (\u00a77). In doing so, we provide insights into the follow- ing questions: Which systems define the state of the art on low-resource NMT applied to the ILA? What is the route that ahead to improve the trans- lations of the ILA? 2 Background and Definitions Formally, the task of MT consists of converting text X in a source language Lx into text Y in a target language Ly that conveys the same mean- ing.1 Translating text X \u2208 Lx into Y \u2208 Ly can be described as a function (Neubig, 2017): Y = MT(X). X and Y can be of variable length, such as phrases, sentences, or even documents. If other languages are used during the transla- tion process, e.g., as pivots, we denote them as L1, . . . , Ln. We refer to a corpus of monolingual sentences in language Li as M Li = S1, ..., Sn. Probabilistic Modeling and Data When us- ing probabilistic MT models, the goal is to find Y \u2208 Ly with the highest conditional probability, given X \u2208 Lx. Under the supervised machine learning paradigm, a parallel corpus Cparallel = (X1, Y1), ..., (Xn, Yn) is used to learn a set of pa- rameters \u03b8, which define a probability distribution over possible translations. Given Cparallel, the training objective of an NMT model is generally to maximize the log-likelihood L with respect to \u03b8: L\u03b8 = (cid:88) log p(Yi|Xi; \u03b8). (Xi,Yi)\u2208Cparallel Within this overall framework, there are a num- ber of design decisions one has to make, such as which model architecture to use, how to generate translations, and how to evaluate. Decoding Decoding refers to the generation of output \u02c6Y , given the parameters \u03b8 and an input X. Often, decoding is done by approximately solving the following maximization problem: argmax \u02c6Y p( \u02c6Y |X; \u03b8) 1This is an approximation, since it is"}, {"question": " Why are most indigenous languages of the Americas considered endangered?", "answer": " Most of these languages are endangered due to various factors.", "ref_chunk": "3 2 0 2 n u J 1 1 ] L C . s c [ 1 v 4 0 8 6 0 . 6 0 3 2 : v i X r a Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction. Manuel Mager\u2661\u2217 Rajat Bhatnagar\u2660 Graham Neubig\u266f Ngoc Thang Vu\u2662 Katharina Kann\u2660 \u2661AWS AI Labs \u266fCarnegie Mellon University \u2660University of Colorado Boulder \u2662University of Stuttgart Abstract Neural models have drastically advanced state of the art for machine translation (MT) be- tween high-resource languages. Traditionally, these models rely on large amounts of train- ing data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of par- allel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and tech- niques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open ques- tions, product of an increased interest of the NLP community in these languages. 1 Introduction More than 7 billion people on Earth communicate in nearly 7000 different languages (Pereltsvaig, 2020). Of these, approximately 900 languages are native of the American continent (Campbell, 2000). Most of these indigenous languages of the Americas (ILA) are endangered at some degree (Thomason, 2015). This huge variety in languages is simultaneously a rich treasure for humanity and also a barrier to communication among people from different backgrounds. Human translators have been important in overcoming language bar- riers. However, trained translators are not acces- sible to everyone on Earth and even scarcer for endangered and minority languages. The need for translations is even written in the constitutions of several countries like Mexico, Peru, Paraguay, Venezuela, and Bolivia (Zaj\u00edcov\u00e1, 2017) to allow native speakers to have equal language rights re- garding law. This is why developing MT is crucial: it helps humanity overcome language barriers while si- multaneously allowing people to continue using their native tongue. However, the challenges to achieving these problems are not trivial. It is not only the amount of available data (a common the- sis among the NLP community) but also a set of challenging issues (dialectical and orthographic variations, noisy texts, complex morphology, etc.) that must be addressed. MT has always been an important task within the larger area of natural language processing (NLP). In 1954, the Georgetown\u2013IBM experiment (Hutchins, 2004) was the first that showed at least some effectiveness of MT. Further research re- sulted in rule-based systems and statistical models. In 2023, neural models define state of the art for MT if training data is plentiful \u2013 i.e., for so-called high-resource languages (HRLs) \u2013 and have also achieved impressive results for low-resource lan- guages (LRLs). MT is also the most studied NLP task for the ILA (Mager et al., 2018b; Littell et al., 2018). The common issue among these languages is the extreme low-resource conditions they are confronted with. The research interest for these languages has increased in the last years, including the recent AmericasNLP 2021 shared task (Mager et al., 2021) on 10 indigenous languages to Span- ish, and the WMT (Conference on Machine Trans- lation) shared task for Inuktitut\u2013English (Barrault et al., 2020). In this work we aim to provide a comprehensive introduction to the challenges that involve creat- ing MT systems for ILA, and the current status of the existing work. We organize this work as follows: We start by introducing state-of-the-art NMT models (\u00a72). Then, we discuss the current challenges for these languages (\u00a73); and we in- troduce the key concepts related to low-resource NMT and the implications for endangered lan- guages of the Americas(\u00a73). This is followed by a discussion of available data (\u00a74). Afterwards, we introduce the important concepts for LRL and endangered languages (\u00a75); then we introduce the \u2217Work done while at the University of Stuttgart. main strategies aimed at improving NMT with limited training data (\u00a76); and finally we give an overview of the work done for ILA on MT (\u00a77). In doing so, we provide insights into the follow- ing questions: Which systems define the state of the art on low-resource NMT applied to the ILA? What is the route that ahead to improve the trans- lations of the ILA? 2 Background and Definitions Formally, the task of MT consists of converting text X in a source language Lx into text Y in a target language Ly that conveys the same mean- ing.1 Translating text X \u2208 Lx into Y \u2208 Ly can be described as a function (Neubig, 2017): Y = MT(X). X and Y can be of variable length, such as phrases, sentences, or even documents. If other languages are used during the transla- tion process, e.g., as pivots, we denote them as L1, . . . , Ln. We refer to a corpus of monolingual sentences in language Li as M Li = S1, ..., Sn. Probabilistic Modeling and Data When us- ing probabilistic MT models, the goal is to find Y \u2208 Ly with the highest conditional probability, given X \u2208 Lx. Under the supervised machine learning paradigm, a parallel corpus Cparallel = (X1, Y1), ..., (Xn, Yn) is used to learn a set of pa- rameters \u03b8, which define a probability distribution over possible translations. Given Cparallel, the training objective of an NMT model is generally to maximize the log-likelihood L with respect to \u03b8: L\u03b8 = (cid:88) log p(Yi|Xi; \u03b8). (Xi,Yi)\u2208Cparallel Within this overall framework, there are a num- ber of design decisions one has to make, such as which model architecture to use, how to generate translations, and how to evaluate. Decoding Decoding refers to the generation of output \u02c6Y , given the parameters \u03b8 and an input X. Often, decoding is done by approximately solving the following maximization problem: argmax \u02c6Y p( \u02c6Y |X; \u03b8) 1This is an approximation, since it is"}, {"question": " Why is the development of Machine Translation (MT) crucial for indigenous languages?", "answer": " MT helps overcome language barriers and allows native speakers to continue using their language.", "ref_chunk": "3 2 0 2 n u J 1 1 ] L C . s c [ 1 v 4 0 8 6 0 . 6 0 3 2 : v i X r a Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction. Manuel Mager\u2661\u2217 Rajat Bhatnagar\u2660 Graham Neubig\u266f Ngoc Thang Vu\u2662 Katharina Kann\u2660 \u2661AWS AI Labs \u266fCarnegie Mellon University \u2660University of Colorado Boulder \u2662University of Stuttgart Abstract Neural models have drastically advanced state of the art for machine translation (MT) be- tween high-resource languages. Traditionally, these models rely on large amounts of train- ing data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of par- allel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and tech- niques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open ques- tions, product of an increased interest of the NLP community in these languages. 1 Introduction More than 7 billion people on Earth communicate in nearly 7000 different languages (Pereltsvaig, 2020). Of these, approximately 900 languages are native of the American continent (Campbell, 2000). Most of these indigenous languages of the Americas (ILA) are endangered at some degree (Thomason, 2015). This huge variety in languages is simultaneously a rich treasure for humanity and also a barrier to communication among people from different backgrounds. Human translators have been important in overcoming language bar- riers. However, trained translators are not acces- sible to everyone on Earth and even scarcer for endangered and minority languages. The need for translations is even written in the constitutions of several countries like Mexico, Peru, Paraguay, Venezuela, and Bolivia (Zaj\u00edcov\u00e1, 2017) to allow native speakers to have equal language rights re- garding law. This is why developing MT is crucial: it helps humanity overcome language barriers while si- multaneously allowing people to continue using their native tongue. However, the challenges to achieving these problems are not trivial. It is not only the amount of available data (a common the- sis among the NLP community) but also a set of challenging issues (dialectical and orthographic variations, noisy texts, complex morphology, etc.) that must be addressed. MT has always been an important task within the larger area of natural language processing (NLP). In 1954, the Georgetown\u2013IBM experiment (Hutchins, 2004) was the first that showed at least some effectiveness of MT. Further research re- sulted in rule-based systems and statistical models. In 2023, neural models define state of the art for MT if training data is plentiful \u2013 i.e., for so-called high-resource languages (HRLs) \u2013 and have also achieved impressive results for low-resource lan- guages (LRLs). MT is also the most studied NLP task for the ILA (Mager et al., 2018b; Littell et al., 2018). The common issue among these languages is the extreme low-resource conditions they are confronted with. The research interest for these languages has increased in the last years, including the recent AmericasNLP 2021 shared task (Mager et al., 2021) on 10 indigenous languages to Span- ish, and the WMT (Conference on Machine Trans- lation) shared task for Inuktitut\u2013English (Barrault et al., 2020). In this work we aim to provide a comprehensive introduction to the challenges that involve creat- ing MT systems for ILA, and the current status of the existing work. We organize this work as follows: We start by introducing state-of-the-art NMT models (\u00a72). Then, we discuss the current challenges for these languages (\u00a73); and we in- troduce the key concepts related to low-resource NMT and the implications for endangered lan- guages of the Americas(\u00a73). This is followed by a discussion of available data (\u00a74). Afterwards, we introduce the important concepts for LRL and endangered languages (\u00a75); then we introduce the \u2217Work done while at the University of Stuttgart. main strategies aimed at improving NMT with limited training data (\u00a76); and finally we give an overview of the work done for ILA on MT (\u00a77). In doing so, we provide insights into the follow- ing questions: Which systems define the state of the art on low-resource NMT applied to the ILA? What is the route that ahead to improve the trans- lations of the ILA? 2 Background and Definitions Formally, the task of MT consists of converting text X in a source language Lx into text Y in a target language Ly that conveys the same mean- ing.1 Translating text X \u2208 Lx into Y \u2208 Ly can be described as a function (Neubig, 2017): Y = MT(X). X and Y can be of variable length, such as phrases, sentences, or even documents. If other languages are used during the transla- tion process, e.g., as pivots, we denote them as L1, . . . , Ln. We refer to a corpus of monolingual sentences in language Li as M Li = S1, ..., Sn. Probabilistic Modeling and Data When us- ing probabilistic MT models, the goal is to find Y \u2208 Ly with the highest conditional probability, given X \u2208 Lx. Under the supervised machine learning paradigm, a parallel corpus Cparallel = (X1, Y1), ..., (Xn, Yn) is used to learn a set of pa- rameters \u03b8, which define a probability distribution over possible translations. Given Cparallel, the training objective of an NMT model is generally to maximize the log-likelihood L with respect to \u03b8: L\u03b8 = (cid:88) log p(Yi|Xi; \u03b8). (Xi,Yi)\u2208Cparallel Within this overall framework, there are a num- ber of design decisions one has to make, such as which model architecture to use, how to generate translations, and how to evaluate. Decoding Decoding refers to the generation of output \u02c6Y , given the parameters \u03b8 and an input X. Often, decoding is done by approximately solving the following maximization problem: argmax \u02c6Y p( \u02c6Y |X; \u03b8) 1This is an approximation, since it is"}, {"question": " What are some of the challenging issues that need to be addressed in creating MT systems for indigenous languages?", "answer": " Dialectical and orthographic variations, noisy texts, complex morphology, etc.", "ref_chunk": "3 2 0 2 n u J 1 1 ] L C . s c [ 1 v 4 0 8 6 0 . 6 0 3 2 : v i X r a Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction. Manuel Mager\u2661\u2217 Rajat Bhatnagar\u2660 Graham Neubig\u266f Ngoc Thang Vu\u2662 Katharina Kann\u2660 \u2661AWS AI Labs \u266fCarnegie Mellon University \u2660University of Colorado Boulder \u2662University of Stuttgart Abstract Neural models have drastically advanced state of the art for machine translation (MT) be- tween high-resource languages. Traditionally, these models rely on large amounts of train- ing data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of par- allel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and tech- niques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open ques- tions, product of an increased interest of the NLP community in these languages. 1 Introduction More than 7 billion people on Earth communicate in nearly 7000 different languages (Pereltsvaig, 2020). Of these, approximately 900 languages are native of the American continent (Campbell, 2000). Most of these indigenous languages of the Americas (ILA) are endangered at some degree (Thomason, 2015). This huge variety in languages is simultaneously a rich treasure for humanity and also a barrier to communication among people from different backgrounds. Human translators have been important in overcoming language bar- riers. However, trained translators are not acces- sible to everyone on Earth and even scarcer for endangered and minority languages. The need for translations is even written in the constitutions of several countries like Mexico, Peru, Paraguay, Venezuela, and Bolivia (Zaj\u00edcov\u00e1, 2017) to allow native speakers to have equal language rights re- garding law. This is why developing MT is crucial: it helps humanity overcome language barriers while si- multaneously allowing people to continue using their native tongue. However, the challenges to achieving these problems are not trivial. It is not only the amount of available data (a common the- sis among the NLP community) but also a set of challenging issues (dialectical and orthographic variations, noisy texts, complex morphology, etc.) that must be addressed. MT has always been an important task within the larger area of natural language processing (NLP). In 1954, the Georgetown\u2013IBM experiment (Hutchins, 2004) was the first that showed at least some effectiveness of MT. Further research re- sulted in rule-based systems and statistical models. In 2023, neural models define state of the art for MT if training data is plentiful \u2013 i.e., for so-called high-resource languages (HRLs) \u2013 and have also achieved impressive results for low-resource lan- guages (LRLs). MT is also the most studied NLP task for the ILA (Mager et al., 2018b; Littell et al., 2018). The common issue among these languages is the extreme low-resource conditions they are confronted with. The research interest for these languages has increased in the last years, including the recent AmericasNLP 2021 shared task (Mager et al., 2021) on 10 indigenous languages to Span- ish, and the WMT (Conference on Machine Trans- lation) shared task for Inuktitut\u2013English (Barrault et al., 2020). In this work we aim to provide a comprehensive introduction to the challenges that involve creat- ing MT systems for ILA, and the current status of the existing work. We organize this work as follows: We start by introducing state-of-the-art NMT models (\u00a72). Then, we discuss the current challenges for these languages (\u00a73); and we in- troduce the key concepts related to low-resource NMT and the implications for endangered lan- guages of the Americas(\u00a73). This is followed by a discussion of available data (\u00a74). Afterwards, we introduce the important concepts for LRL and endangered languages (\u00a75); then we introduce the \u2217Work done while at the University of Stuttgart. main strategies aimed at improving NMT with limited training data (\u00a76); and finally we give an overview of the work done for ILA on MT (\u00a77). In doing so, we provide insights into the follow- ing questions: Which systems define the state of the art on low-resource NMT applied to the ILA? What is the route that ahead to improve the trans- lations of the ILA? 2 Background and Definitions Formally, the task of MT consists of converting text X in a source language Lx into text Y in a target language Ly that conveys the same mean- ing.1 Translating text X \u2208 Lx into Y \u2208 Ly can be described as a function (Neubig, 2017): Y = MT(X). X and Y can be of variable length, such as phrases, sentences, or even documents. If other languages are used during the transla- tion process, e.g., as pivots, we denote them as L1, . . . , Ln. We refer to a corpus of monolingual sentences in language Li as M Li = S1, ..., Sn. Probabilistic Modeling and Data When us- ing probabilistic MT models, the goal is to find Y \u2208 Ly with the highest conditional probability, given X \u2208 Lx. Under the supervised machine learning paradigm, a parallel corpus Cparallel = (X1, Y1), ..., (Xn, Yn) is used to learn a set of pa- rameters \u03b8, which define a probability distribution over possible translations. Given Cparallel, the training objective of an NMT model is generally to maximize the log-likelihood L with respect to \u03b8: L\u03b8 = (cid:88) log p(Yi|Xi; \u03b8). (Xi,Yi)\u2208Cparallel Within this overall framework, there are a num- ber of design decisions one has to make, such as which model architecture to use, how to generate translations, and how to evaluate. Decoding Decoding refers to the generation of output \u02c6Y , given the parameters \u03b8 and an input X. Often, decoding is done by approximately solving the following maximization problem: argmax \u02c6Y p( \u02c6Y |X; \u03b8) 1This is an approximation, since it is"}, {"question": " When did neural models define the state of the art for MT?", "answer": " In 2023.", "ref_chunk": "3 2 0 2 n u J 1 1 ] L C . s c [ 1 v 4 0 8 6 0 . 6 0 3 2 : v i X r a Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction. Manuel Mager\u2661\u2217 Rajat Bhatnagar\u2660 Graham Neubig\u266f Ngoc Thang Vu\u2662 Katharina Kann\u2660 \u2661AWS AI Labs \u266fCarnegie Mellon University \u2660University of Colorado Boulder \u2662University of Stuttgart Abstract Neural models have drastically advanced state of the art for machine translation (MT) be- tween high-resource languages. Traditionally, these models rely on large amounts of train- ing data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of par- allel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and tech- niques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open ques- tions, product of an increased interest of the NLP community in these languages. 1 Introduction More than 7 billion people on Earth communicate in nearly 7000 different languages (Pereltsvaig, 2020). Of these, approximately 900 languages are native of the American continent (Campbell, 2000). Most of these indigenous languages of the Americas (ILA) are endangered at some degree (Thomason, 2015). This huge variety in languages is simultaneously a rich treasure for humanity and also a barrier to communication among people from different backgrounds. Human translators have been important in overcoming language bar- riers. However, trained translators are not acces- sible to everyone on Earth and even scarcer for endangered and minority languages. The need for translations is even written in the constitutions of several countries like Mexico, Peru, Paraguay, Venezuela, and Bolivia (Zaj\u00edcov\u00e1, 2017) to allow native speakers to have equal language rights re- garding law. This is why developing MT is crucial: it helps humanity overcome language barriers while si- multaneously allowing people to continue using their native tongue. However, the challenges to achieving these problems are not trivial. It is not only the amount of available data (a common the- sis among the NLP community) but also a set of challenging issues (dialectical and orthographic variations, noisy texts, complex morphology, etc.) that must be addressed. MT has always been an important task within the larger area of natural language processing (NLP). In 1954, the Georgetown\u2013IBM experiment (Hutchins, 2004) was the first that showed at least some effectiveness of MT. Further research re- sulted in rule-based systems and statistical models. In 2023, neural models define state of the art for MT if training data is plentiful \u2013 i.e., for so-called high-resource languages (HRLs) \u2013 and have also achieved impressive results for low-resource lan- guages (LRLs). MT is also the most studied NLP task for the ILA (Mager et al., 2018b; Littell et al., 2018). The common issue among these languages is the extreme low-resource conditions they are confronted with. The research interest for these languages has increased in the last years, including the recent AmericasNLP 2021 shared task (Mager et al., 2021) on 10 indigenous languages to Span- ish, and the WMT (Conference on Machine Trans- lation) shared task for Inuktitut\u2013English (Barrault et al., 2020). In this work we aim to provide a comprehensive introduction to the challenges that involve creat- ing MT systems for ILA, and the current status of the existing work. We organize this work as follows: We start by introducing state-of-the-art NMT models (\u00a72). Then, we discuss the current challenges for these languages (\u00a73); and we in- troduce the key concepts related to low-resource NMT and the implications for endangered lan- guages of the Americas(\u00a73). This is followed by a discussion of available data (\u00a74). Afterwards, we introduce the important concepts for LRL and endangered languages (\u00a75); then we introduce the \u2217Work done while at the University of Stuttgart. main strategies aimed at improving NMT with limited training data (\u00a76); and finally we give an overview of the work done for ILA on MT (\u00a77). In doing so, we provide insights into the follow- ing questions: Which systems define the state of the art on low-resource NMT applied to the ILA? What is the route that ahead to improve the trans- lations of the ILA? 2 Background and Definitions Formally, the task of MT consists of converting text X in a source language Lx into text Y in a target language Ly that conveys the same mean- ing.1 Translating text X \u2208 Lx into Y \u2208 Ly can be described as a function (Neubig, 2017): Y = MT(X). X and Y can be of variable length, such as phrases, sentences, or even documents. If other languages are used during the transla- tion process, e.g., as pivots, we denote them as L1, . . . , Ln. We refer to a corpus of monolingual sentences in language Li as M Li = S1, ..., Sn. Probabilistic Modeling and Data When us- ing probabilistic MT models, the goal is to find Y \u2208 Ly with the highest conditional probability, given X \u2208 Lx. Under the supervised machine learning paradigm, a parallel corpus Cparallel = (X1, Y1), ..., (Xn, Yn) is used to learn a set of pa- rameters \u03b8, which define a probability distribution over possible translations. Given Cparallel, the training objective of an NMT model is generally to maximize the log-likelihood L with respect to \u03b8: L\u03b8 = (cid:88) log p(Yi|Xi; \u03b8). (Xi,Yi)\u2208Cparallel Within this overall framework, there are a num- ber of design decisions one has to make, such as which model architecture to use, how to generate translations, and how to evaluate. Decoding Decoding refers to the generation of output \u02c6Y , given the parameters \u03b8 and an input X. Often, decoding is done by approximately solving the following maximization problem: argmax \u02c6Y p( \u02c6Y |X; \u03b8) 1This is an approximation, since it is"}, {"question": " What was the first experiment that showed effectiveness of MT?", "answer": " The Georgetown-IBM experiment in 1954.", "ref_chunk": "3 2 0 2 n u J 1 1 ] L C . s c [ 1 v 4 0 8 6 0 . 6 0 3 2 : v i X r a Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction. Manuel Mager\u2661\u2217 Rajat Bhatnagar\u2660 Graham Neubig\u266f Ngoc Thang Vu\u2662 Katharina Kann\u2660 \u2661AWS AI Labs \u266fCarnegie Mellon University \u2660University of Colorado Boulder \u2662University of Stuttgart Abstract Neural models have drastically advanced state of the art for machine translation (MT) be- tween high-resource languages. Traditionally, these models rely on large amounts of train- ing data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of par- allel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and tech- niques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open ques- tions, product of an increased interest of the NLP community in these languages. 1 Introduction More than 7 billion people on Earth communicate in nearly 7000 different languages (Pereltsvaig, 2020). Of these, approximately 900 languages are native of the American continent (Campbell, 2000). Most of these indigenous languages of the Americas (ILA) are endangered at some degree (Thomason, 2015). This huge variety in languages is simultaneously a rich treasure for humanity and also a barrier to communication among people from different backgrounds. Human translators have been important in overcoming language bar- riers. However, trained translators are not acces- sible to everyone on Earth and even scarcer for endangered and minority languages. The need for translations is even written in the constitutions of several countries like Mexico, Peru, Paraguay, Venezuela, and Bolivia (Zaj\u00edcov\u00e1, 2017) to allow native speakers to have equal language rights re- garding law. This is why developing MT is crucial: it helps humanity overcome language barriers while si- multaneously allowing people to continue using their native tongue. However, the challenges to achieving these problems are not trivial. It is not only the amount of available data (a common the- sis among the NLP community) but also a set of challenging issues (dialectical and orthographic variations, noisy texts, complex morphology, etc.) that must be addressed. MT has always been an important task within the larger area of natural language processing (NLP). In 1954, the Georgetown\u2013IBM experiment (Hutchins, 2004) was the first that showed at least some effectiveness of MT. Further research re- sulted in rule-based systems and statistical models. In 2023, neural models define state of the art for MT if training data is plentiful \u2013 i.e., for so-called high-resource languages (HRLs) \u2013 and have also achieved impressive results for low-resource lan- guages (LRLs). MT is also the most studied NLP task for the ILA (Mager et al., 2018b; Littell et al., 2018). The common issue among these languages is the extreme low-resource conditions they are confronted with. The research interest for these languages has increased in the last years, including the recent AmericasNLP 2021 shared task (Mager et al., 2021) on 10 indigenous languages to Span- ish, and the WMT (Conference on Machine Trans- lation) shared task for Inuktitut\u2013English (Barrault et al., 2020). In this work we aim to provide a comprehensive introduction to the challenges that involve creat- ing MT systems for ILA, and the current status of the existing work. We organize this work as follows: We start by introducing state-of-the-art NMT models (\u00a72). Then, we discuss the current challenges for these languages (\u00a73); and we in- troduce the key concepts related to low-resource NMT and the implications for endangered lan- guages of the Americas(\u00a73). This is followed by a discussion of available data (\u00a74). Afterwards, we introduce the important concepts for LRL and endangered languages (\u00a75); then we introduce the \u2217Work done while at the University of Stuttgart. main strategies aimed at improving NMT with limited training data (\u00a76); and finally we give an overview of the work done for ILA on MT (\u00a77). In doing so, we provide insights into the follow- ing questions: Which systems define the state of the art on low-resource NMT applied to the ILA? What is the route that ahead to improve the trans- lations of the ILA? 2 Background and Definitions Formally, the task of MT consists of converting text X in a source language Lx into text Y in a target language Ly that conveys the same mean- ing.1 Translating text X \u2208 Lx into Y \u2208 Ly can be described as a function (Neubig, 2017): Y = MT(X). X and Y can be of variable length, such as phrases, sentences, or even documents. If other languages are used during the transla- tion process, e.g., as pivots, we denote them as L1, . . . , Ln. We refer to a corpus of monolingual sentences in language Li as M Li = S1, ..., Sn. Probabilistic Modeling and Data When us- ing probabilistic MT models, the goal is to find Y \u2208 Ly with the highest conditional probability, given X \u2208 Lx. Under the supervised machine learning paradigm, a parallel corpus Cparallel = (X1, Y1), ..., (Xn, Yn) is used to learn a set of pa- rameters \u03b8, which define a probability distribution over possible translations. Given Cparallel, the training objective of an NMT model is generally to maximize the log-likelihood L with respect to \u03b8: L\u03b8 = (cid:88) log p(Yi|Xi; \u03b8). (Xi,Yi)\u2208Cparallel Within this overall framework, there are a num- ber of design decisions one has to make, such as which model architecture to use, how to generate translations, and how to evaluate. Decoding Decoding refers to the generation of output \u02c6Y , given the parameters \u03b8 and an input X. Often, decoding is done by approximately solving the following maximization problem: argmax \u02c6Y p( \u02c6Y |X; \u03b8) 1This is an approximation, since it is"}, {"question": " What does the task of Machine Translation (MT) consist of?", "answer": " Converting text in a source language into text in a target language that conveys the same meaning.", "ref_chunk": "3 2 0 2 n u J 1 1 ] L C . s c [ 1 v 4 0 8 6 0 . 6 0 3 2 : v i X r a Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction. Manuel Mager\u2661\u2217 Rajat Bhatnagar\u2660 Graham Neubig\u266f Ngoc Thang Vu\u2662 Katharina Kann\u2660 \u2661AWS AI Labs \u266fCarnegie Mellon University \u2660University of Colorado Boulder \u2662University of Stuttgart Abstract Neural models have drastically advanced state of the art for machine translation (MT) be- tween high-resource languages. Traditionally, these models rely on large amounts of train- ing data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of par- allel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and tech- niques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open ques- tions, product of an increased interest of the NLP community in these languages. 1 Introduction More than 7 billion people on Earth communicate in nearly 7000 different languages (Pereltsvaig, 2020). Of these, approximately 900 languages are native of the American continent (Campbell, 2000). Most of these indigenous languages of the Americas (ILA) are endangered at some degree (Thomason, 2015). This huge variety in languages is simultaneously a rich treasure for humanity and also a barrier to communication among people from different backgrounds. Human translators have been important in overcoming language bar- riers. However, trained translators are not acces- sible to everyone on Earth and even scarcer for endangered and minority languages. The need for translations is even written in the constitutions of several countries like Mexico, Peru, Paraguay, Venezuela, and Bolivia (Zaj\u00edcov\u00e1, 2017) to allow native speakers to have equal language rights re- garding law. This is why developing MT is crucial: it helps humanity overcome language barriers while si- multaneously allowing people to continue using their native tongue. However, the challenges to achieving these problems are not trivial. It is not only the amount of available data (a common the- sis among the NLP community) but also a set of challenging issues (dialectical and orthographic variations, noisy texts, complex morphology, etc.) that must be addressed. MT has always been an important task within the larger area of natural language processing (NLP). In 1954, the Georgetown\u2013IBM experiment (Hutchins, 2004) was the first that showed at least some effectiveness of MT. Further research re- sulted in rule-based systems and statistical models. In 2023, neural models define state of the art for MT if training data is plentiful \u2013 i.e., for so-called high-resource languages (HRLs) \u2013 and have also achieved impressive results for low-resource lan- guages (LRLs). MT is also the most studied NLP task for the ILA (Mager et al., 2018b; Littell et al., 2018). The common issue among these languages is the extreme low-resource conditions they are confronted with. The research interest for these languages has increased in the last years, including the recent AmericasNLP 2021 shared task (Mager et al., 2021) on 10 indigenous languages to Span- ish, and the WMT (Conference on Machine Trans- lation) shared task for Inuktitut\u2013English (Barrault et al., 2020). In this work we aim to provide a comprehensive introduction to the challenges that involve creat- ing MT systems for ILA, and the current status of the existing work. We organize this work as follows: We start by introducing state-of-the-art NMT models (\u00a72). Then, we discuss the current challenges for these languages (\u00a73); and we in- troduce the key concepts related to low-resource NMT and the implications for endangered lan- guages of the Americas(\u00a73). This is followed by a discussion of available data (\u00a74). Afterwards, we introduce the important concepts for LRL and endangered languages (\u00a75); then we introduce the \u2217Work done while at the University of Stuttgart. main strategies aimed at improving NMT with limited training data (\u00a76); and finally we give an overview of the work done for ILA on MT (\u00a77). In doing so, we provide insights into the follow- ing questions: Which systems define the state of the art on low-resource NMT applied to the ILA? What is the route that ahead to improve the trans- lations of the ILA? 2 Background and Definitions Formally, the task of MT consists of converting text X in a source language Lx into text Y in a target language Ly that conveys the same mean- ing.1 Translating text X \u2208 Lx into Y \u2208 Ly can be described as a function (Neubig, 2017): Y = MT(X). X and Y can be of variable length, such as phrases, sentences, or even documents. If other languages are used during the transla- tion process, e.g., as pivots, we denote them as L1, . . . , Ln. We refer to a corpus of monolingual sentences in language Li as M Li = S1, ..., Sn. Probabilistic Modeling and Data When us- ing probabilistic MT models, the goal is to find Y \u2208 Ly with the highest conditional probability, given X \u2208 Lx. Under the supervised machine learning paradigm, a parallel corpus Cparallel = (X1, Y1), ..., (Xn, Yn) is used to learn a set of pa- rameters \u03b8, which define a probability distribution over possible translations. Given Cparallel, the training objective of an NMT model is generally to maximize the log-likelihood L with respect to \u03b8: L\u03b8 = (cid:88) log p(Yi|Xi; \u03b8). (Xi,Yi)\u2208Cparallel Within this overall framework, there are a num- ber of design decisions one has to make, such as which model architecture to use, how to generate translations, and how to evaluate. Decoding Decoding refers to the generation of output \u02c6Y , given the parameters \u03b8 and an input X. Often, decoding is done by approximately solving the following maximization problem: argmax \u02c6Y p( \u02c6Y |X; \u03b8) 1This is an approximation, since it is"}, {"question": " What is the goal when using probabilistic MT models?", "answer": " To find the target text with the highest conditional probability, given the source text.", "ref_chunk": "3 2 0 2 n u J 1 1 ] L C . s c [ 1 v 4 0 8 6 0 . 6 0 3 2 : v i X r a Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction. Manuel Mager\u2661\u2217 Rajat Bhatnagar\u2660 Graham Neubig\u266f Ngoc Thang Vu\u2662 Katharina Kann\u2660 \u2661AWS AI Labs \u266fCarnegie Mellon University \u2660University of Colorado Boulder \u2662University of Stuttgart Abstract Neural models have drastically advanced state of the art for machine translation (MT) be- tween high-resource languages. Traditionally, these models rely on large amounts of train- ing data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of par- allel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and tech- niques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open ques- tions, product of an increased interest of the NLP community in these languages. 1 Introduction More than 7 billion people on Earth communicate in nearly 7000 different languages (Pereltsvaig, 2020). Of these, approximately 900 languages are native of the American continent (Campbell, 2000). Most of these indigenous languages of the Americas (ILA) are endangered at some degree (Thomason, 2015). This huge variety in languages is simultaneously a rich treasure for humanity and also a barrier to communication among people from different backgrounds. Human translators have been important in overcoming language bar- riers. However, trained translators are not acces- sible to everyone on Earth and even scarcer for endangered and minority languages. The need for translations is even written in the constitutions of several countries like Mexico, Peru, Paraguay, Venezuela, and Bolivia (Zaj\u00edcov\u00e1, 2017) to allow native speakers to have equal language rights re- garding law. This is why developing MT is crucial: it helps humanity overcome language barriers while si- multaneously allowing people to continue using their native tongue. However, the challenges to achieving these problems are not trivial. It is not only the amount of available data (a common the- sis among the NLP community) but also a set of challenging issues (dialectical and orthographic variations, noisy texts, complex morphology, etc.) that must be addressed. MT has always been an important task within the larger area of natural language processing (NLP). In 1954, the Georgetown\u2013IBM experiment (Hutchins, 2004) was the first that showed at least some effectiveness of MT. Further research re- sulted in rule-based systems and statistical models. In 2023, neural models define state of the art for MT if training data is plentiful \u2013 i.e., for so-called high-resource languages (HRLs) \u2013 and have also achieved impressive results for low-resource lan- guages (LRLs). MT is also the most studied NLP task for the ILA (Mager et al., 2018b; Littell et al., 2018). The common issue among these languages is the extreme low-resource conditions they are confronted with. The research interest for these languages has increased in the last years, including the recent AmericasNLP 2021 shared task (Mager et al., 2021) on 10 indigenous languages to Span- ish, and the WMT (Conference on Machine Trans- lation) shared task for Inuktitut\u2013English (Barrault et al., 2020). In this work we aim to provide a comprehensive introduction to the challenges that involve creat- ing MT systems for ILA, and the current status of the existing work. We organize this work as follows: We start by introducing state-of-the-art NMT models (\u00a72). Then, we discuss the current challenges for these languages (\u00a73); and we in- troduce the key concepts related to low-resource NMT and the implications for endangered lan- guages of the Americas(\u00a73). This is followed by a discussion of available data (\u00a74). Afterwards, we introduce the important concepts for LRL and endangered languages (\u00a75); then we introduce the \u2217Work done while at the University of Stuttgart. main strategies aimed at improving NMT with limited training data (\u00a76); and finally we give an overview of the work done for ILA on MT (\u00a77). In doing so, we provide insights into the follow- ing questions: Which systems define the state of the art on low-resource NMT applied to the ILA? What is the route that ahead to improve the trans- lations of the ILA? 2 Background and Definitions Formally, the task of MT consists of converting text X in a source language Lx into text Y in a target language Ly that conveys the same mean- ing.1 Translating text X \u2208 Lx into Y \u2208 Ly can be described as a function (Neubig, 2017): Y = MT(X). X and Y can be of variable length, such as phrases, sentences, or even documents. If other languages are used during the transla- tion process, e.g., as pivots, we denote them as L1, . . . , Ln. We refer to a corpus of monolingual sentences in language Li as M Li = S1, ..., Sn. Probabilistic Modeling and Data When us- ing probabilistic MT models, the goal is to find Y \u2208 Ly with the highest conditional probability, given X \u2208 Lx. Under the supervised machine learning paradigm, a parallel corpus Cparallel = (X1, Y1), ..., (Xn, Yn) is used to learn a set of pa- rameters \u03b8, which define a probability distribution over possible translations. Given Cparallel, the training objective of an NMT model is generally to maximize the log-likelihood L with respect to \u03b8: L\u03b8 = (cid:88) log p(Yi|Xi; \u03b8). (Xi,Yi)\u2208Cparallel Within this overall framework, there are a num- ber of design decisions one has to make, such as which model architecture to use, how to generate translations, and how to evaluate. Decoding Decoding refers to the generation of output \u02c6Y , given the parameters \u03b8 and an input X. Often, decoding is done by approximately solving the following maximization problem: argmax \u02c6Y p( \u02c6Y |X; \u03b8) 1This is an approximation, since it is"}, {"question": " What is decoding in the context of MT?", "answer": " Decoding refers to the generation of output given the parameters and an input.", "ref_chunk": "3 2 0 2 n u J 1 1 ] L C . s c [ 1 v 4 0 8 6 0 . 6 0 3 2 : v i X r a Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction. Manuel Mager\u2661\u2217 Rajat Bhatnagar\u2660 Graham Neubig\u266f Ngoc Thang Vu\u2662 Katharina Kann\u2660 \u2661AWS AI Labs \u266fCarnegie Mellon University \u2660University of Colorado Boulder \u2662University of Stuttgart Abstract Neural models have drastically advanced state of the art for machine translation (MT) be- tween high-resource languages. Traditionally, these models rely on large amounts of train- ing data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of par- allel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and tech- niques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open ques- tions, product of an increased interest of the NLP community in these languages. 1 Introduction More than 7 billion people on Earth communicate in nearly 7000 different languages (Pereltsvaig, 2020). Of these, approximately 900 languages are native of the American continent (Campbell, 2000). Most of these indigenous languages of the Americas (ILA) are endangered at some degree (Thomason, 2015). This huge variety in languages is simultaneously a rich treasure for humanity and also a barrier to communication among people from different backgrounds. Human translators have been important in overcoming language bar- riers. However, trained translators are not acces- sible to everyone on Earth and even scarcer for endangered and minority languages. The need for translations is even written in the constitutions of several countries like Mexico, Peru, Paraguay, Venezuela, and Bolivia (Zaj\u00edcov\u00e1, 2017) to allow native speakers to have equal language rights re- garding law. This is why developing MT is crucial: it helps humanity overcome language barriers while si- multaneously allowing people to continue using their native tongue. However, the challenges to achieving these problems are not trivial. It is not only the amount of available data (a common the- sis among the NLP community) but also a set of challenging issues (dialectical and orthographic variations, noisy texts, complex morphology, etc.) that must be addressed. MT has always been an important task within the larger area of natural language processing (NLP). In 1954, the Georgetown\u2013IBM experiment (Hutchins, 2004) was the first that showed at least some effectiveness of MT. Further research re- sulted in rule-based systems and statistical models. In 2023, neural models define state of the art for MT if training data is plentiful \u2013 i.e., for so-called high-resource languages (HRLs) \u2013 and have also achieved impressive results for low-resource lan- guages (LRLs). MT is also the most studied NLP task for the ILA (Mager et al., 2018b; Littell et al., 2018). The common issue among these languages is the extreme low-resource conditions they are confronted with. The research interest for these languages has increased in the last years, including the recent AmericasNLP 2021 shared task (Mager et al., 2021) on 10 indigenous languages to Span- ish, and the WMT (Conference on Machine Trans- lation) shared task for Inuktitut\u2013English (Barrault et al., 2020). In this work we aim to provide a comprehensive introduction to the challenges that involve creat- ing MT systems for ILA, and the current status of the existing work. We organize this work as follows: We start by introducing state-of-the-art NMT models (\u00a72). Then, we discuss the current challenges for these languages (\u00a73); and we in- troduce the key concepts related to low-resource NMT and the implications for endangered lan- guages of the Americas(\u00a73). This is followed by a discussion of available data (\u00a74). Afterwards, we introduce the important concepts for LRL and endangered languages (\u00a75); then we introduce the \u2217Work done while at the University of Stuttgart. main strategies aimed at improving NMT with limited training data (\u00a76); and finally we give an overview of the work done for ILA on MT (\u00a77). In doing so, we provide insights into the follow- ing questions: Which systems define the state of the art on low-resource NMT applied to the ILA? What is the route that ahead to improve the trans- lations of the ILA? 2 Background and Definitions Formally, the task of MT consists of converting text X in a source language Lx into text Y in a target language Ly that conveys the same mean- ing.1 Translating text X \u2208 Lx into Y \u2208 Ly can be described as a function (Neubig, 2017): Y = MT(X). X and Y can be of variable length, such as phrases, sentences, or even documents. If other languages are used during the transla- tion process, e.g., as pivots, we denote them as L1, . . . , Ln. We refer to a corpus of monolingual sentences in language Li as M Li = S1, ..., Sn. Probabilistic Modeling and Data When us- ing probabilistic MT models, the goal is to find Y \u2208 Ly with the highest conditional probability, given X \u2208 Lx. Under the supervised machine learning paradigm, a parallel corpus Cparallel = (X1, Y1), ..., (Xn, Yn) is used to learn a set of pa- rameters \u03b8, which define a probability distribution over possible translations. Given Cparallel, the training objective of an NMT model is generally to maximize the log-likelihood L with respect to \u03b8: L\u03b8 = (cid:88) log p(Yi|Xi; \u03b8). (Xi,Yi)\u2208Cparallel Within this overall framework, there are a num- ber of design decisions one has to make, such as which model architecture to use, how to generate translations, and how to evaluate. Decoding Decoding refers to the generation of output \u02c6Y , given the parameters \u03b8 and an input X. Often, decoding is done by approximately solving the following maximization problem: argmax \u02c6Y p( \u02c6Y |X; \u03b8) 1This is an approximation, since it is"}, {"question": " What is the training objective of a Neural Machine Translation (NMT) model?", "answer": " Maximizing the log-likelihood with respect to the parameters.", "ref_chunk": "3 2 0 2 n u J 1 1 ] L C . s c [ 1 v 4 0 8 6 0 . 6 0 3 2 : v i X r a Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction. Manuel Mager\u2661\u2217 Rajat Bhatnagar\u2660 Graham Neubig\u266f Ngoc Thang Vu\u2662 Katharina Kann\u2660 \u2661AWS AI Labs \u266fCarnegie Mellon University \u2660University of Colorado Boulder \u2662University of Stuttgart Abstract Neural models have drastically advanced state of the art for machine translation (MT) be- tween high-resource languages. Traditionally, these models rely on large amounts of train- ing data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of par- allel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and tech- niques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open ques- tions, product of an increased interest of the NLP community in these languages. 1 Introduction More than 7 billion people on Earth communicate in nearly 7000 different languages (Pereltsvaig, 2020). Of these, approximately 900 languages are native of the American continent (Campbell, 2000). Most of these indigenous languages of the Americas (ILA) are endangered at some degree (Thomason, 2015). This huge variety in languages is simultaneously a rich treasure for humanity and also a barrier to communication among people from different backgrounds. Human translators have been important in overcoming language bar- riers. However, trained translators are not acces- sible to everyone on Earth and even scarcer for endangered and minority languages. The need for translations is even written in the constitutions of several countries like Mexico, Peru, Paraguay, Venezuela, and Bolivia (Zaj\u00edcov\u00e1, 2017) to allow native speakers to have equal language rights re- garding law. This is why developing MT is crucial: it helps humanity overcome language barriers while si- multaneously allowing people to continue using their native tongue. However, the challenges to achieving these problems are not trivial. It is not only the amount of available data (a common the- sis among the NLP community) but also a set of challenging issues (dialectical and orthographic variations, noisy texts, complex morphology, etc.) that must be addressed. MT has always been an important task within the larger area of natural language processing (NLP). In 1954, the Georgetown\u2013IBM experiment (Hutchins, 2004) was the first that showed at least some effectiveness of MT. Further research re- sulted in rule-based systems and statistical models. In 2023, neural models define state of the art for MT if training data is plentiful \u2013 i.e., for so-called high-resource languages (HRLs) \u2013 and have also achieved impressive results for low-resource lan- guages (LRLs). MT is also the most studied NLP task for the ILA (Mager et al., 2018b; Littell et al., 2018). The common issue among these languages is the extreme low-resource conditions they are confronted with. The research interest for these languages has increased in the last years, including the recent AmericasNLP 2021 shared task (Mager et al., 2021) on 10 indigenous languages to Span- ish, and the WMT (Conference on Machine Trans- lation) shared task for Inuktitut\u2013English (Barrault et al., 2020). In this work we aim to provide a comprehensive introduction to the challenges that involve creat- ing MT systems for ILA, and the current status of the existing work. We organize this work as follows: We start by introducing state-of-the-art NMT models (\u00a72). Then, we discuss the current challenges for these languages (\u00a73); and we in- troduce the key concepts related to low-resource NMT and the implications for endangered lan- guages of the Americas(\u00a73). This is followed by a discussion of available data (\u00a74). Afterwards, we introduce the important concepts for LRL and endangered languages (\u00a75); then we introduce the \u2217Work done while at the University of Stuttgart. main strategies aimed at improving NMT with limited training data (\u00a76); and finally we give an overview of the work done for ILA on MT (\u00a77). In doing so, we provide insights into the follow- ing questions: Which systems define the state of the art on low-resource NMT applied to the ILA? What is the route that ahead to improve the trans- lations of the ILA? 2 Background and Definitions Formally, the task of MT consists of converting text X in a source language Lx into text Y in a target language Ly that conveys the same mean- ing.1 Translating text X \u2208 Lx into Y \u2208 Ly can be described as a function (Neubig, 2017): Y = MT(X). X and Y can be of variable length, such as phrases, sentences, or even documents. If other languages are used during the transla- tion process, e.g., as pivots, we denote them as L1, . . . , Ln. We refer to a corpus of monolingual sentences in language Li as M Li = S1, ..., Sn. Probabilistic Modeling and Data When us- ing probabilistic MT models, the goal is to find Y \u2208 Ly with the highest conditional probability, given X \u2208 Lx. Under the supervised machine learning paradigm, a parallel corpus Cparallel = (X1, Y1), ..., (Xn, Yn) is used to learn a set of pa- rameters \u03b8, which define a probability distribution over possible translations. Given Cparallel, the training objective of an NMT model is generally to maximize the log-likelihood L with respect to \u03b8: L\u03b8 = (cid:88) log p(Yi|Xi; \u03b8). (Xi,Yi)\u2208Cparallel Within this overall framework, there are a num- ber of design decisions one has to make, such as which model architecture to use, how to generate translations, and how to evaluate. Decoding Decoding refers to the generation of output \u02c6Y , given the parameters \u03b8 and an input X. Often, decoding is done by approximately solving the following maximization problem: argmax \u02c6Y p( \u02c6Y |X; \u03b8) 1This is an approximation, since it is"}], "doc_text": "3 2 0 2 n u J 1 1 ] L C . s c [ 1 v 4 0 8 6 0 . 6 0 3 2 : v i X r a Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction. Manuel Mager\u2661\u2217 Rajat Bhatnagar\u2660 Graham Neubig\u266f Ngoc Thang Vu\u2662 Katharina Kann\u2660 \u2661AWS AI Labs \u266fCarnegie Mellon University \u2660University of Colorado Boulder \u2662University of Stuttgart Abstract Neural models have drastically advanced state of the art for machine translation (MT) be- tween high-resource languages. Traditionally, these models rely on large amounts of train- ing data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of par- allel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and tech- niques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open ques- tions, product of an increased interest of the NLP community in these languages. 1 Introduction More than 7 billion people on Earth communicate in nearly 7000 different languages (Pereltsvaig, 2020). Of these, approximately 900 languages are native of the American continent (Campbell, 2000). Most of these indigenous languages of the Americas (ILA) are endangered at some degree (Thomason, 2015). This huge variety in languages is simultaneously a rich treasure for humanity and also a barrier to communication among people from different backgrounds. Human translators have been important in overcoming language bar- riers. However, trained translators are not acces- sible to everyone on Earth and even scarcer for endangered and minority languages. The need for translations is even written in the constitutions of several countries like Mexico, Peru, Paraguay, Venezuela, and Bolivia (Zaj\u00edcov\u00e1, 2017) to allow native speakers to have equal language rights re- garding law. This is why developing MT is crucial: it helps humanity overcome language barriers while si- multaneously allowing people to continue using their native tongue. However, the challenges to achieving these problems are not trivial. It is not only the amount of available data (a common the- sis among the NLP community) but also a set of challenging issues (dialectical and orthographic variations, noisy texts, complex morphology, etc.) that must be addressed. MT has always been an important task within the larger area of natural language processing (NLP). In 1954, the Georgetown\u2013IBM experiment (Hutchins, 2004) was the first that showed at least some effectiveness of MT. Further research re- sulted in rule-based systems and statistical models. In 2023, neural models define state of the art for MT if training data is plentiful \u2013 i.e., for so-called high-resource languages (HRLs) \u2013 and have also achieved impressive results for low-resource lan- guages (LRLs). MT is also the most studied NLP task for the ILA (Mager et al., 2018b; Littell et al., 2018). The common issue among these languages is the extreme low-resource conditions they are confronted with. The research interest for these languages has increased in the last years, including the recent AmericasNLP 2021 shared task (Mager et al., 2021) on 10 indigenous languages to Span- ish, and the WMT (Conference on Machine Trans- lation) shared task for Inuktitut\u2013English (Barrault et al., 2020). In this work we aim to provide a comprehensive introduction to the challenges that involve creat- ing MT systems for ILA, and the current status of the existing work. We organize this work as follows: We start by introducing state-of-the-art NMT models (\u00a72). Then, we discuss the current challenges for these languages (\u00a73); and we in- troduce the key concepts related to low-resource NMT and the implications for endangered lan- guages of the Americas(\u00a73). This is followed by a discussion of available data (\u00a74). Afterwards, we introduce the important concepts for LRL and endangered languages (\u00a75); then we introduce the \u2217Work done while at the University of Stuttgart. main strategies aimed at improving NMT with limited training data (\u00a76); and finally we give an overview of the work done for ILA on MT (\u00a77). In doing so, we provide insights into the follow- ing questions: Which systems define the state of the art on low-resource NMT applied to the ILA? What is the route that ahead to improve the trans- lations of the ILA? 2 Background and Definitions Formally, the task of MT consists of converting text X in a source language Lx into text Y in a target language Ly that conveys the same mean- ing.1 Translating text X \u2208 Lx into Y \u2208 Ly can be described as a function (Neubig, 2017): Y = MT(X). X and Y can be of variable length, such as phrases, sentences, or even documents. If other languages are used during the transla- tion process, e.g., as pivots, we denote them as L1, . . . , Ln. We refer to a corpus of monolingual sentences in language Li as M Li = S1, ..., Sn. Probabilistic Modeling and Data When us- ing probabilistic MT models, the goal is to find Y \u2208 Ly with the highest conditional probability, given X \u2208 Lx. Under the supervised machine learning paradigm, a parallel corpus Cparallel = (X1, Y1), ..., (Xn, Yn) is used to learn a set of pa- rameters \u03b8, which define a probability distribution over possible translations. Given Cparallel, the training objective of an NMT model is generally to maximize the log-likelihood L with respect to \u03b8: L\u03b8 = (cid:88) log p(Yi|Xi; \u03b8). (Xi,Yi)\u2208Cparallel Within this overall framework, there are a num- ber of design decisions one has to make, such as which model architecture to use, how to generate translations, and how to evaluate. Decoding Decoding refers to the generation of output \u02c6Y , given the parameters \u03b8 and an input X. Often, decoding is done by approximately solving the following maximization problem: argmax \u02c6Y p( \u02c6Y |X; \u03b8) 1This is an approximation, since it is"}