{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Regularizing_Self-training_for_Unsupervised_Domain_Adaptation_via_Structural_Constraints_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main drawback of self-training based on pseudo-labels for unsupervised domain adaptation?", "answer": " One of the main drawbacks is that it is susceptible to erroneous pseudo labels that arise from confirmation biases in the source domain.", "ref_chunk": "3 2 0 2 r p A 9 2 ] V C . s c [ 1 v 1 3 1 0 0 . 5 0 3 2 : v i X r a Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints Rajshekhar Das1* \u2020 Jonathan Francis1,2\u2217 Sanket Vaibhav Mehta1\u2217 Jean Oh1 Emma Strubell1 Jos\u00b4e Moura1 1Carnegie Mellon University 2Bosch Center for Arti\ufb01cial Intelligence {rajshekd, jmf1, svmehta, hyaejino, strubell, moura}@andrew.cmu.edu Abstract Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for se- mantic segmentation problems. A notable drawback, how- ever, is that this family of approaches is susceptible to er- roneous pseudo labels that arise from con\ufb01rmation biases in the source domain and that manifest as nuisance fac- tors in the target domain. A possible source for this mis- match is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub- optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise con- ventional self-training objectives. Speci\ufb01cally, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal cluster- ing. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for In this work, we show unsupervised domain adaptation. that our regularizer signi\ufb01cantly improves top performing self-training methods (by up to 2 points) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary. Figure 1. Motivation for Objectness Constraints: The above examples compare target-domain ground-truth segmentation, pre- dicted segmentation and prediction con\ufb01dence (brighter regions are more con\ufb01dent) of a seed model that was adapted from source to target domain via adversarial adaptation [48]. Most self-training approaches use such a seed model to predict pixelwise pseudo- labels. The blue-dashed-boxes highlighte the high-con\ufb01dence re- gions that are likely to be included in the set of a pseudo-labels despite being mis-classi\ufb01ed. We propose to mitigate the adverse effect of such noisy pseudo-labels on self-training based adapta- tion via objectness constraints. 1. Introduction Semantic segmentation is a crucial and challenging task for applications such as autonomous driving [2, 18, 51, 60, 61] that rely on pixel-level semantics of the scene. Perfor- mance on this task has signi\ufb01cantly improved over the past few years following the advances in deep supervised learn- Equal contribution. \u2020Correspondence. ing [9]. However, an important limitation arises from the excessive cost and time taken to annotate images at a pixel- level (reported to be 1.5 hours per image in a popular dataset [12]). Further, most real-world datasets do not have suf\ufb01- cient coverage over all variations in outdoor scenes such as weather conditions and geography-speci\ufb01c layouts that can be crucial for large-scale deployment of learning-based models in autonomous vehicles. Acquiring training data to cater to such scene variations would signi\ufb01cantly add to the 1 cost of annotation. To address the annotation problem, synthetic datasets cu- rated from 3D simulation environments like GTA [37] and SYNTHIA [38] have been proposed where large amounts of annotated data can be easily generated. However, generated data introduces domain shift due to differences in visual characteristics of simulated images (source domain) and real images (target domain). To mitigate such shifts, unsu- pervised domain adaptation strategies [2,5,18,48,60,61,64] for semantic segmentation have been extensively studied in the recent years. Among these approaches, self-training [16] has emerged as a particularly promising approach that involves pseudo labelling the (unlabelled) target-domain data using a seed model trained solely on the source do- main. Pseudo-label predictions for which the con\ufb01dence exceeds a prede\ufb01ned threshold are then used to further train the model and ultimately improve the target-domain perfor- mance. While self-training based adaptation is quite effective, it is susceptible to erroneous pseudo labels arising from con- \ufb01rmation bias [3] in the seed model. Con\ufb01rmation bias re- sults from training on source domain semantics that might introduce factors of representation that serve as nuisance factors for the target domain. In the context of semantic segmentation, such a bias manifests as pixel-wise seed pre- dictions that are highly con\ufb01dent but incorrect (see Figure 1). For instance, if the source domain images usually have bright regions (high intensity of the RGB channels) for the sky class, then bright regions in target domain images might be predicted as the sky with high con\ufb01dence, irrespective of the actual semantic label. Since highly con\ufb01dent predic- tions qualify as pseudo-labels, training the model on poten- tially noisy predictions can ultimately lead to sub-optimal performance in the target domain. Thus, in this work, we seek to reduce the heavy reliance of self-training methods on photometric cues for predicting pixel-wise semantic la- bels. To that end, we propose to incorporate auxiliary modal- ity information such as depth maps that can provide struc- tural cues [11,24,51,53], complementary to the photometric cues. Semantic segmentation datasets are usually accom- panied by depth maps that can be easily acquired in prac- tice [12,39]. Since na\u00a8\u0131ve fusion of features that are extracted from depth information can also introduce nuisance [24,51], an important question is raised \u2014 How can we leverage the depth modality to counter the effect of noisy pseudo- labels during self-training? In this work, we propose a contrastive objectness constraint derived from depth maps and RGB-images in the target domain that is used to reg- ularise conventional self-training methods. The constraint is computed in two steps: an object-region estimation step, followed by pixel-wise contrastive loss computation. In the \ufb01rst step, we perform unsupervised image segmentation us- 2 ing both depth-based histograms and RGB-images that are fused together to yield multiple object-regions per image. These regions respect actual object boundaries, based on the structural information depth provides, as well as vi- sual similarity. In the"}, {"question": " How does the reliance on only photometric cues provided by RGB image inputs affect traditional self-training methods?", "answer": " The reliance on only photometric cues may lead to suboptimal adaptation in self-training methods.", "ref_chunk": "3 2 0 2 r p A 9 2 ] V C . s c [ 1 v 1 3 1 0 0 . 5 0 3 2 : v i X r a Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints Rajshekhar Das1* \u2020 Jonathan Francis1,2\u2217 Sanket Vaibhav Mehta1\u2217 Jean Oh1 Emma Strubell1 Jos\u00b4e Moura1 1Carnegie Mellon University 2Bosch Center for Arti\ufb01cial Intelligence {rajshekd, jmf1, svmehta, hyaejino, strubell, moura}@andrew.cmu.edu Abstract Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for se- mantic segmentation problems. A notable drawback, how- ever, is that this family of approaches is susceptible to er- roneous pseudo labels that arise from con\ufb01rmation biases in the source domain and that manifest as nuisance fac- tors in the target domain. A possible source for this mis- match is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub- optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise con- ventional self-training objectives. Speci\ufb01cally, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal cluster- ing. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for In this work, we show unsupervised domain adaptation. that our regularizer signi\ufb01cantly improves top performing self-training methods (by up to 2 points) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary. Figure 1. Motivation for Objectness Constraints: The above examples compare target-domain ground-truth segmentation, pre- dicted segmentation and prediction con\ufb01dence (brighter regions are more con\ufb01dent) of a seed model that was adapted from source to target domain via adversarial adaptation [48]. Most self-training approaches use such a seed model to predict pixelwise pseudo- labels. The blue-dashed-boxes highlighte the high-con\ufb01dence re- gions that are likely to be included in the set of a pseudo-labels despite being mis-classi\ufb01ed. We propose to mitigate the adverse effect of such noisy pseudo-labels on self-training based adapta- tion via objectness constraints. 1. Introduction Semantic segmentation is a crucial and challenging task for applications such as autonomous driving [2, 18, 51, 60, 61] that rely on pixel-level semantics of the scene. Perfor- mance on this task has signi\ufb01cantly improved over the past few years following the advances in deep supervised learn- Equal contribution. \u2020Correspondence. ing [9]. However, an important limitation arises from the excessive cost and time taken to annotate images at a pixel- level (reported to be 1.5 hours per image in a popular dataset [12]). Further, most real-world datasets do not have suf\ufb01- cient coverage over all variations in outdoor scenes such as weather conditions and geography-speci\ufb01c layouts that can be crucial for large-scale deployment of learning-based models in autonomous vehicles. Acquiring training data to cater to such scene variations would signi\ufb01cantly add to the 1 cost of annotation. To address the annotation problem, synthetic datasets cu- rated from 3D simulation environments like GTA [37] and SYNTHIA [38] have been proposed where large amounts of annotated data can be easily generated. However, generated data introduces domain shift due to differences in visual characteristics of simulated images (source domain) and real images (target domain). To mitigate such shifts, unsu- pervised domain adaptation strategies [2,5,18,48,60,61,64] for semantic segmentation have been extensively studied in the recent years. Among these approaches, self-training [16] has emerged as a particularly promising approach that involves pseudo labelling the (unlabelled) target-domain data using a seed model trained solely on the source do- main. Pseudo-label predictions for which the con\ufb01dence exceeds a prede\ufb01ned threshold are then used to further train the model and ultimately improve the target-domain perfor- mance. While self-training based adaptation is quite effective, it is susceptible to erroneous pseudo labels arising from con- \ufb01rmation bias [3] in the seed model. Con\ufb01rmation bias re- sults from training on source domain semantics that might introduce factors of representation that serve as nuisance factors for the target domain. In the context of semantic segmentation, such a bias manifests as pixel-wise seed pre- dictions that are highly con\ufb01dent but incorrect (see Figure 1). For instance, if the source domain images usually have bright regions (high intensity of the RGB channels) for the sky class, then bright regions in target domain images might be predicted as the sky with high con\ufb01dence, irrespective of the actual semantic label. Since highly con\ufb01dent predic- tions qualify as pseudo-labels, training the model on poten- tially noisy predictions can ultimately lead to sub-optimal performance in the target domain. Thus, in this work, we seek to reduce the heavy reliance of self-training methods on photometric cues for predicting pixel-wise semantic la- bels. To that end, we propose to incorporate auxiliary modal- ity information such as depth maps that can provide struc- tural cues [11,24,51,53], complementary to the photometric cues. Semantic segmentation datasets are usually accom- panied by depth maps that can be easily acquired in prac- tice [12,39]. Since na\u00a8\u0131ve fusion of features that are extracted from depth information can also introduce nuisance [24,51], an important question is raised \u2014 How can we leverage the depth modality to counter the effect of noisy pseudo- labels during self-training? In this work, we propose a contrastive objectness constraint derived from depth maps and RGB-images in the target domain that is used to reg- ularise conventional self-training methods. The constraint is computed in two steps: an object-region estimation step, followed by pixel-wise contrastive loss computation. In the \ufb01rst step, we perform unsupervised image segmentation us- 2 ing both depth-based histograms and RGB-images that are fused together to yield multiple object-regions per image. These regions respect actual object boundaries, based on the structural information depth provides, as well as vi- sual similarity. In the"}, {"question": " What is the proposed method to mitigate the effect of mismatched pseudo-labels in self-training for unsupervised domain adaptation?", "answer": " The proposed method is to incorporate structural cues from auxiliary modalities, such as depth, to regularize conventional self-training objectives.", "ref_chunk": "3 2 0 2 r p A 9 2 ] V C . s c [ 1 v 1 3 1 0 0 . 5 0 3 2 : v i X r a Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints Rajshekhar Das1* \u2020 Jonathan Francis1,2\u2217 Sanket Vaibhav Mehta1\u2217 Jean Oh1 Emma Strubell1 Jos\u00b4e Moura1 1Carnegie Mellon University 2Bosch Center for Arti\ufb01cial Intelligence {rajshekd, jmf1, svmehta, hyaejino, strubell, moura}@andrew.cmu.edu Abstract Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for se- mantic segmentation problems. A notable drawback, how- ever, is that this family of approaches is susceptible to er- roneous pseudo labels that arise from con\ufb01rmation biases in the source domain and that manifest as nuisance fac- tors in the target domain. A possible source for this mis- match is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub- optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise con- ventional self-training objectives. Speci\ufb01cally, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal cluster- ing. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for In this work, we show unsupervised domain adaptation. that our regularizer signi\ufb01cantly improves top performing self-training methods (by up to 2 points) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary. Figure 1. Motivation for Objectness Constraints: The above examples compare target-domain ground-truth segmentation, pre- dicted segmentation and prediction con\ufb01dence (brighter regions are more con\ufb01dent) of a seed model that was adapted from source to target domain via adversarial adaptation [48]. Most self-training approaches use such a seed model to predict pixelwise pseudo- labels. The blue-dashed-boxes highlighte the high-con\ufb01dence re- gions that are likely to be included in the set of a pseudo-labels despite being mis-classi\ufb01ed. We propose to mitigate the adverse effect of such noisy pseudo-labels on self-training based adapta- tion via objectness constraints. 1. Introduction Semantic segmentation is a crucial and challenging task for applications such as autonomous driving [2, 18, 51, 60, 61] that rely on pixel-level semantics of the scene. Perfor- mance on this task has signi\ufb01cantly improved over the past few years following the advances in deep supervised learn- Equal contribution. \u2020Correspondence. ing [9]. However, an important limitation arises from the excessive cost and time taken to annotate images at a pixel- level (reported to be 1.5 hours per image in a popular dataset [12]). Further, most real-world datasets do not have suf\ufb01- cient coverage over all variations in outdoor scenes such as weather conditions and geography-speci\ufb01c layouts that can be crucial for large-scale deployment of learning-based models in autonomous vehicles. Acquiring training data to cater to such scene variations would signi\ufb01cantly add to the 1 cost of annotation. To address the annotation problem, synthetic datasets cu- rated from 3D simulation environments like GTA [37] and SYNTHIA [38] have been proposed where large amounts of annotated data can be easily generated. However, generated data introduces domain shift due to differences in visual characteristics of simulated images (source domain) and real images (target domain). To mitigate such shifts, unsu- pervised domain adaptation strategies [2,5,18,48,60,61,64] for semantic segmentation have been extensively studied in the recent years. Among these approaches, self-training [16] has emerged as a particularly promising approach that involves pseudo labelling the (unlabelled) target-domain data using a seed model trained solely on the source do- main. Pseudo-label predictions for which the con\ufb01dence exceeds a prede\ufb01ned threshold are then used to further train the model and ultimately improve the target-domain perfor- mance. While self-training based adaptation is quite effective, it is susceptible to erroneous pseudo labels arising from con- \ufb01rmation bias [3] in the seed model. Con\ufb01rmation bias re- sults from training on source domain semantics that might introduce factors of representation that serve as nuisance factors for the target domain. In the context of semantic segmentation, such a bias manifests as pixel-wise seed pre- dictions that are highly con\ufb01dent but incorrect (see Figure 1). For instance, if the source domain images usually have bright regions (high intensity of the RGB channels) for the sky class, then bright regions in target domain images might be predicted as the sky with high con\ufb01dence, irrespective of the actual semantic label. Since highly con\ufb01dent predic- tions qualify as pseudo-labels, training the model on poten- tially noisy predictions can ultimately lead to sub-optimal performance in the target domain. Thus, in this work, we seek to reduce the heavy reliance of self-training methods on photometric cues for predicting pixel-wise semantic la- bels. To that end, we propose to incorporate auxiliary modal- ity information such as depth maps that can provide struc- tural cues [11,24,51,53], complementary to the photometric cues. Semantic segmentation datasets are usually accom- panied by depth maps that can be easily acquired in prac- tice [12,39]. Since na\u00a8\u0131ve fusion of features that are extracted from depth information can also introduce nuisance [24,51], an important question is raised \u2014 How can we leverage the depth modality to counter the effect of noisy pseudo- labels during self-training? In this work, we propose a contrastive objectness constraint derived from depth maps and RGB-images in the target domain that is used to reg- ularise conventional self-training methods. The constraint is computed in two steps: an object-region estimation step, followed by pixel-wise contrastive loss computation. In the \ufb01rst step, we perform unsupervised image segmentation us- 2 ing both depth-based histograms and RGB-images that are fused together to yield multiple object-regions per image. These regions respect actual object boundaries, based on the structural information depth provides, as well as vi- sual similarity. In the"}, {"question": " What does the contrastive pixel-level objectness constraint do in the proposed method?", "answer": " The contrastive pixel-level objectness constraint pulls pixel representations within a region of an object instance closer, while pushing those from different object categories apart.", "ref_chunk": "3 2 0 2 r p A 9 2 ] V C . s c [ 1 v 1 3 1 0 0 . 5 0 3 2 : v i X r a Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints Rajshekhar Das1* \u2020 Jonathan Francis1,2\u2217 Sanket Vaibhav Mehta1\u2217 Jean Oh1 Emma Strubell1 Jos\u00b4e Moura1 1Carnegie Mellon University 2Bosch Center for Arti\ufb01cial Intelligence {rajshekd, jmf1, svmehta, hyaejino, strubell, moura}@andrew.cmu.edu Abstract Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for se- mantic segmentation problems. A notable drawback, how- ever, is that this family of approaches is susceptible to er- roneous pseudo labels that arise from con\ufb01rmation biases in the source domain and that manifest as nuisance fac- tors in the target domain. A possible source for this mis- match is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub- optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise con- ventional self-training objectives. Speci\ufb01cally, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal cluster- ing. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for In this work, we show unsupervised domain adaptation. that our regularizer signi\ufb01cantly improves top performing self-training methods (by up to 2 points) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary. Figure 1. Motivation for Objectness Constraints: The above examples compare target-domain ground-truth segmentation, pre- dicted segmentation and prediction con\ufb01dence (brighter regions are more con\ufb01dent) of a seed model that was adapted from source to target domain via adversarial adaptation [48]. Most self-training approaches use such a seed model to predict pixelwise pseudo- labels. The blue-dashed-boxes highlighte the high-con\ufb01dence re- gions that are likely to be included in the set of a pseudo-labels despite being mis-classi\ufb01ed. We propose to mitigate the adverse effect of such noisy pseudo-labels on self-training based adapta- tion via objectness constraints. 1. Introduction Semantic segmentation is a crucial and challenging task for applications such as autonomous driving [2, 18, 51, 60, 61] that rely on pixel-level semantics of the scene. Perfor- mance on this task has signi\ufb01cantly improved over the past few years following the advances in deep supervised learn- Equal contribution. \u2020Correspondence. ing [9]. However, an important limitation arises from the excessive cost and time taken to annotate images at a pixel- level (reported to be 1.5 hours per image in a popular dataset [12]). Further, most real-world datasets do not have suf\ufb01- cient coverage over all variations in outdoor scenes such as weather conditions and geography-speci\ufb01c layouts that can be crucial for large-scale deployment of learning-based models in autonomous vehicles. Acquiring training data to cater to such scene variations would signi\ufb01cantly add to the 1 cost of annotation. To address the annotation problem, synthetic datasets cu- rated from 3D simulation environments like GTA [37] and SYNTHIA [38] have been proposed where large amounts of annotated data can be easily generated. However, generated data introduces domain shift due to differences in visual characteristics of simulated images (source domain) and real images (target domain). To mitigate such shifts, unsu- pervised domain adaptation strategies [2,5,18,48,60,61,64] for semantic segmentation have been extensively studied in the recent years. Among these approaches, self-training [16] has emerged as a particularly promising approach that involves pseudo labelling the (unlabelled) target-domain data using a seed model trained solely on the source do- main. Pseudo-label predictions for which the con\ufb01dence exceeds a prede\ufb01ned threshold are then used to further train the model and ultimately improve the target-domain perfor- mance. While self-training based adaptation is quite effective, it is susceptible to erroneous pseudo labels arising from con- \ufb01rmation bias [3] in the seed model. Con\ufb01rmation bias re- sults from training on source domain semantics that might introduce factors of representation that serve as nuisance factors for the target domain. In the context of semantic segmentation, such a bias manifests as pixel-wise seed pre- dictions that are highly con\ufb01dent but incorrect (see Figure 1). For instance, if the source domain images usually have bright regions (high intensity of the RGB channels) for the sky class, then bright regions in target domain images might be predicted as the sky with high con\ufb01dence, irrespective of the actual semantic label. Since highly con\ufb01dent predic- tions qualify as pseudo-labels, training the model on poten- tially noisy predictions can ultimately lead to sub-optimal performance in the target domain. Thus, in this work, we seek to reduce the heavy reliance of self-training methods on photometric cues for predicting pixel-wise semantic la- bels. To that end, we propose to incorporate auxiliary modal- ity information such as depth maps that can provide struc- tural cues [11,24,51,53], complementary to the photometric cues. Semantic segmentation datasets are usually accom- panied by depth maps that can be easily acquired in prac- tice [12,39]. Since na\u00a8\u0131ve fusion of features that are extracted from depth information can also introduce nuisance [24,51], an important question is raised \u2014 How can we leverage the depth modality to counter the effect of noisy pseudo- labels during self-training? In this work, we propose a contrastive objectness constraint derived from depth maps and RGB-images in the target domain that is used to reg- ularise conventional self-training methods. The constraint is computed in two steps: an object-region estimation step, followed by pixel-wise contrastive loss computation. In the \ufb01rst step, we perform unsupervised image segmentation us- 2 ing both depth-based histograms and RGB-images that are fused together to yield multiple object-regions per image. These regions respect actual object boundaries, based on the structural information depth provides, as well as vi- sual similarity. In the"}, {"question": " Why is the objectness constraint proposed for unsupervised domain adaptation considered agnostic to ground-truth semantic labels?", "answer": " The objectness constraint is agnostic to ground-truth semantic labels because it is used to improve unsupervised domain adaptation.", "ref_chunk": "3 2 0 2 r p A 9 2 ] V C . s c [ 1 v 1 3 1 0 0 . 5 0 3 2 : v i X r a Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints Rajshekhar Das1* \u2020 Jonathan Francis1,2\u2217 Sanket Vaibhav Mehta1\u2217 Jean Oh1 Emma Strubell1 Jos\u00b4e Moura1 1Carnegie Mellon University 2Bosch Center for Arti\ufb01cial Intelligence {rajshekd, jmf1, svmehta, hyaejino, strubell, moura}@andrew.cmu.edu Abstract Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for se- mantic segmentation problems. A notable drawback, how- ever, is that this family of approaches is susceptible to er- roneous pseudo labels that arise from con\ufb01rmation biases in the source domain and that manifest as nuisance fac- tors in the target domain. A possible source for this mis- match is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub- optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise con- ventional self-training objectives. Speci\ufb01cally, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal cluster- ing. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for In this work, we show unsupervised domain adaptation. that our regularizer signi\ufb01cantly improves top performing self-training methods (by up to 2 points) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary. Figure 1. Motivation for Objectness Constraints: The above examples compare target-domain ground-truth segmentation, pre- dicted segmentation and prediction con\ufb01dence (brighter regions are more con\ufb01dent) of a seed model that was adapted from source to target domain via adversarial adaptation [48]. Most self-training approaches use such a seed model to predict pixelwise pseudo- labels. The blue-dashed-boxes highlighte the high-con\ufb01dence re- gions that are likely to be included in the set of a pseudo-labels despite being mis-classi\ufb01ed. We propose to mitigate the adverse effect of such noisy pseudo-labels on self-training based adapta- tion via objectness constraints. 1. Introduction Semantic segmentation is a crucial and challenging task for applications such as autonomous driving [2, 18, 51, 60, 61] that rely on pixel-level semantics of the scene. Perfor- mance on this task has signi\ufb01cantly improved over the past few years following the advances in deep supervised learn- Equal contribution. \u2020Correspondence. ing [9]. However, an important limitation arises from the excessive cost and time taken to annotate images at a pixel- level (reported to be 1.5 hours per image in a popular dataset [12]). Further, most real-world datasets do not have suf\ufb01- cient coverage over all variations in outdoor scenes such as weather conditions and geography-speci\ufb01c layouts that can be crucial for large-scale deployment of learning-based models in autonomous vehicles. Acquiring training data to cater to such scene variations would signi\ufb01cantly add to the 1 cost of annotation. To address the annotation problem, synthetic datasets cu- rated from 3D simulation environments like GTA [37] and SYNTHIA [38] have been proposed where large amounts of annotated data can be easily generated. However, generated data introduces domain shift due to differences in visual characteristics of simulated images (source domain) and real images (target domain). To mitigate such shifts, unsu- pervised domain adaptation strategies [2,5,18,48,60,61,64] for semantic segmentation have been extensively studied in the recent years. Among these approaches, self-training [16] has emerged as a particularly promising approach that involves pseudo labelling the (unlabelled) target-domain data using a seed model trained solely on the source do- main. Pseudo-label predictions for which the con\ufb01dence exceeds a prede\ufb01ned threshold are then used to further train the model and ultimately improve the target-domain perfor- mance. While self-training based adaptation is quite effective, it is susceptible to erroneous pseudo labels arising from con- \ufb01rmation bias [3] in the seed model. Con\ufb01rmation bias re- sults from training on source domain semantics that might introduce factors of representation that serve as nuisance factors for the target domain. In the context of semantic segmentation, such a bias manifests as pixel-wise seed pre- dictions that are highly con\ufb01dent but incorrect (see Figure 1). For instance, if the source domain images usually have bright regions (high intensity of the RGB channels) for the sky class, then bright regions in target domain images might be predicted as the sky with high con\ufb01dence, irrespective of the actual semantic label. Since highly con\ufb01dent predic- tions qualify as pseudo-labels, training the model on poten- tially noisy predictions can ultimately lead to sub-optimal performance in the target domain. Thus, in this work, we seek to reduce the heavy reliance of self-training methods on photometric cues for predicting pixel-wise semantic la- bels. To that end, we propose to incorporate auxiliary modal- ity information such as depth maps that can provide struc- tural cues [11,24,51,53], complementary to the photometric cues. Semantic segmentation datasets are usually accom- panied by depth maps that can be easily acquired in prac- tice [12,39]. Since na\u00a8\u0131ve fusion of features that are extracted from depth information can also introduce nuisance [24,51], an important question is raised \u2014 How can we leverage the depth modality to counter the effect of noisy pseudo- labels during self-training? In this work, we propose a contrastive objectness constraint derived from depth maps and RGB-images in the target domain that is used to reg- ularise conventional self-training methods. The constraint is computed in two steps: an object-region estimation step, followed by pixel-wise contrastive loss computation. In the \ufb01rst step, we perform unsupervised image segmentation us- 2 ing both depth-based histograms and RGB-images that are fused together to yield multiple object-regions per image. These regions respect actual object boundaries, based on the structural information depth provides, as well as vi- sual similarity. In the"}, {"question": " What does the regularizer proposed in the study significantly improve in various UDA benchmarks for semantic segmentation?", "answer": " The regularizer significantly improves the top performing self-training methods (by up to 2 points) in various UDA benchmarks for semantic segmentation.", "ref_chunk": "3 2 0 2 r p A 9 2 ] V C . s c [ 1 v 1 3 1 0 0 . 5 0 3 2 : v i X r a Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints Rajshekhar Das1* \u2020 Jonathan Francis1,2\u2217 Sanket Vaibhav Mehta1\u2217 Jean Oh1 Emma Strubell1 Jos\u00b4e Moura1 1Carnegie Mellon University 2Bosch Center for Arti\ufb01cial Intelligence {rajshekd, jmf1, svmehta, hyaejino, strubell, moura}@andrew.cmu.edu Abstract Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for se- mantic segmentation problems. A notable drawback, how- ever, is that this family of approaches is susceptible to er- roneous pseudo labels that arise from con\ufb01rmation biases in the source domain and that manifest as nuisance fac- tors in the target domain. A possible source for this mis- match is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub- optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise con- ventional self-training objectives. Speci\ufb01cally, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal cluster- ing. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for In this work, we show unsupervised domain adaptation. that our regularizer signi\ufb01cantly improves top performing self-training methods (by up to 2 points) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary. Figure 1. Motivation for Objectness Constraints: The above examples compare target-domain ground-truth segmentation, pre- dicted segmentation and prediction con\ufb01dence (brighter regions are more con\ufb01dent) of a seed model that was adapted from source to target domain via adversarial adaptation [48]. Most self-training approaches use such a seed model to predict pixelwise pseudo- labels. The blue-dashed-boxes highlighte the high-con\ufb01dence re- gions that are likely to be included in the set of a pseudo-labels despite being mis-classi\ufb01ed. We propose to mitigate the adverse effect of such noisy pseudo-labels on self-training based adapta- tion via objectness constraints. 1. Introduction Semantic segmentation is a crucial and challenging task for applications such as autonomous driving [2, 18, 51, 60, 61] that rely on pixel-level semantics of the scene. Perfor- mance on this task has signi\ufb01cantly improved over the past few years following the advances in deep supervised learn- Equal contribution. \u2020Correspondence. ing [9]. However, an important limitation arises from the excessive cost and time taken to annotate images at a pixel- level (reported to be 1.5 hours per image in a popular dataset [12]). Further, most real-world datasets do not have suf\ufb01- cient coverage over all variations in outdoor scenes such as weather conditions and geography-speci\ufb01c layouts that can be crucial for large-scale deployment of learning-based models in autonomous vehicles. Acquiring training data to cater to such scene variations would signi\ufb01cantly add to the 1 cost of annotation. To address the annotation problem, synthetic datasets cu- rated from 3D simulation environments like GTA [37] and SYNTHIA [38] have been proposed where large amounts of annotated data can be easily generated. However, generated data introduces domain shift due to differences in visual characteristics of simulated images (source domain) and real images (target domain). To mitigate such shifts, unsu- pervised domain adaptation strategies [2,5,18,48,60,61,64] for semantic segmentation have been extensively studied in the recent years. Among these approaches, self-training [16] has emerged as a particularly promising approach that involves pseudo labelling the (unlabelled) target-domain data using a seed model trained solely on the source do- main. Pseudo-label predictions for which the con\ufb01dence exceeds a prede\ufb01ned threshold are then used to further train the model and ultimately improve the target-domain perfor- mance. While self-training based adaptation is quite effective, it is susceptible to erroneous pseudo labels arising from con- \ufb01rmation bias [3] in the seed model. Con\ufb01rmation bias re- sults from training on source domain semantics that might introduce factors of representation that serve as nuisance factors for the target domain. In the context of semantic segmentation, such a bias manifests as pixel-wise seed pre- dictions that are highly con\ufb01dent but incorrect (see Figure 1). For instance, if the source domain images usually have bright regions (high intensity of the RGB channels) for the sky class, then bright regions in target domain images might be predicted as the sky with high con\ufb01dence, irrespective of the actual semantic label. Since highly con\ufb01dent predic- tions qualify as pseudo-labels, training the model on poten- tially noisy predictions can ultimately lead to sub-optimal performance in the target domain. Thus, in this work, we seek to reduce the heavy reliance of self-training methods on photometric cues for predicting pixel-wise semantic la- bels. To that end, we propose to incorporate auxiliary modal- ity information such as depth maps that can provide struc- tural cues [11,24,51,53], complementary to the photometric cues. Semantic segmentation datasets are usually accom- panied by depth maps that can be easily acquired in prac- tice [12,39]. Since na\u00a8\u0131ve fusion of features that are extracted from depth information can also introduce nuisance [24,51], an important question is raised \u2014 How can we leverage the depth modality to counter the effect of noisy pseudo- labels during self-training? In this work, we propose a contrastive objectness constraint derived from depth maps and RGB-images in the target domain that is used to reg- ularise conventional self-training methods. The constraint is computed in two steps: an object-region estimation step, followed by pixel-wise contrastive loss computation. In the \ufb01rst step, we perform unsupervised image segmentation us- 2 ing both depth-based histograms and RGB-images that are fused together to yield multiple object-regions per image. These regions respect actual object boundaries, based on the structural information depth provides, as well as vi- sual similarity. In the"}, {"question": " What do the blue-dashed-boxes highlighted in the motivation for objectness constraints represent?", "answer": " The blue-dashed-boxes represent high-confidence regions that are likely to be included in the set of pseudo-labels despite being mis-classified.", "ref_chunk": "3 2 0 2 r p A 9 2 ] V C . s c [ 1 v 1 3 1 0 0 . 5 0 3 2 : v i X r a Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints Rajshekhar Das1* \u2020 Jonathan Francis1,2\u2217 Sanket Vaibhav Mehta1\u2217 Jean Oh1 Emma Strubell1 Jos\u00b4e Moura1 1Carnegie Mellon University 2Bosch Center for Arti\ufb01cial Intelligence {rajshekd, jmf1, svmehta, hyaejino, strubell, moura}@andrew.cmu.edu Abstract Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for se- mantic segmentation problems. A notable drawback, how- ever, is that this family of approaches is susceptible to er- roneous pseudo labels that arise from con\ufb01rmation biases in the source domain and that manifest as nuisance fac- tors in the target domain. A possible source for this mis- match is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub- optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise con- ventional self-training objectives. Speci\ufb01cally, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal cluster- ing. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for In this work, we show unsupervised domain adaptation. that our regularizer signi\ufb01cantly improves top performing self-training methods (by up to 2 points) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary. Figure 1. Motivation for Objectness Constraints: The above examples compare target-domain ground-truth segmentation, pre- dicted segmentation and prediction con\ufb01dence (brighter regions are more con\ufb01dent) of a seed model that was adapted from source to target domain via adversarial adaptation [48]. Most self-training approaches use such a seed model to predict pixelwise pseudo- labels. The blue-dashed-boxes highlighte the high-con\ufb01dence re- gions that are likely to be included in the set of a pseudo-labels despite being mis-classi\ufb01ed. We propose to mitigate the adverse effect of such noisy pseudo-labels on self-training based adapta- tion via objectness constraints. 1. Introduction Semantic segmentation is a crucial and challenging task for applications such as autonomous driving [2, 18, 51, 60, 61] that rely on pixel-level semantics of the scene. Perfor- mance on this task has signi\ufb01cantly improved over the past few years following the advances in deep supervised learn- Equal contribution. \u2020Correspondence. ing [9]. However, an important limitation arises from the excessive cost and time taken to annotate images at a pixel- level (reported to be 1.5 hours per image in a popular dataset [12]). Further, most real-world datasets do not have suf\ufb01- cient coverage over all variations in outdoor scenes such as weather conditions and geography-speci\ufb01c layouts that can be crucial for large-scale deployment of learning-based models in autonomous vehicles. Acquiring training data to cater to such scene variations would signi\ufb01cantly add to the 1 cost of annotation. To address the annotation problem, synthetic datasets cu- rated from 3D simulation environments like GTA [37] and SYNTHIA [38] have been proposed where large amounts of annotated data can be easily generated. However, generated data introduces domain shift due to differences in visual characteristics of simulated images (source domain) and real images (target domain). To mitigate such shifts, unsu- pervised domain adaptation strategies [2,5,18,48,60,61,64] for semantic segmentation have been extensively studied in the recent years. Among these approaches, self-training [16] has emerged as a particularly promising approach that involves pseudo labelling the (unlabelled) target-domain data using a seed model trained solely on the source do- main. Pseudo-label predictions for which the con\ufb01dence exceeds a prede\ufb01ned threshold are then used to further train the model and ultimately improve the target-domain perfor- mance. While self-training based adaptation is quite effective, it is susceptible to erroneous pseudo labels arising from con- \ufb01rmation bias [3] in the seed model. Con\ufb01rmation bias re- sults from training on source domain semantics that might introduce factors of representation that serve as nuisance factors for the target domain. In the context of semantic segmentation, such a bias manifests as pixel-wise seed pre- dictions that are highly con\ufb01dent but incorrect (see Figure 1). For instance, if the source domain images usually have bright regions (high intensity of the RGB channels) for the sky class, then bright regions in target domain images might be predicted as the sky with high con\ufb01dence, irrespective of the actual semantic label. Since highly con\ufb01dent predic- tions qualify as pseudo-labels, training the model on poten- tially noisy predictions can ultimately lead to sub-optimal performance in the target domain. Thus, in this work, we seek to reduce the heavy reliance of self-training methods on photometric cues for predicting pixel-wise semantic la- bels. To that end, we propose to incorporate auxiliary modal- ity information such as depth maps that can provide struc- tural cues [11,24,51,53], complementary to the photometric cues. Semantic segmentation datasets are usually accom- panied by depth maps that can be easily acquired in prac- tice [12,39]. Since na\u00a8\u0131ve fusion of features that are extracted from depth information can also introduce nuisance [24,51], an important question is raised \u2014 How can we leverage the depth modality to counter the effect of noisy pseudo- labels during self-training? In this work, we propose a contrastive objectness constraint derived from depth maps and RGB-images in the target domain that is used to reg- ularise conventional self-training methods. The constraint is computed in two steps: an object-region estimation step, followed by pixel-wise contrastive loss computation. In the \ufb01rst step, we perform unsupervised image segmentation us- 2 ing both depth-based histograms and RGB-images that are fused together to yield multiple object-regions per image. These regions respect actual object boundaries, based on the structural information depth provides, as well as vi- sual similarity. In the"}, {"question": " What is a limitation mentioned in the introduction regarding the annotation of images at a pixel-level?", "answer": " An important limitation is the excessive cost and time taken to annotate images at a pixel-level.", "ref_chunk": "3 2 0 2 r p A 9 2 ] V C . s c [ 1 v 1 3 1 0 0 . 5 0 3 2 : v i X r a Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints Rajshekhar Das1* \u2020 Jonathan Francis1,2\u2217 Sanket Vaibhav Mehta1\u2217 Jean Oh1 Emma Strubell1 Jos\u00b4e Moura1 1Carnegie Mellon University 2Bosch Center for Arti\ufb01cial Intelligence {rajshekd, jmf1, svmehta, hyaejino, strubell, moura}@andrew.cmu.edu Abstract Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for se- mantic segmentation problems. A notable drawback, how- ever, is that this family of approaches is susceptible to er- roneous pseudo labels that arise from con\ufb01rmation biases in the source domain and that manifest as nuisance fac- tors in the target domain. A possible source for this mis- match is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub- optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise con- ventional self-training objectives. Speci\ufb01cally, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal cluster- ing. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for In this work, we show unsupervised domain adaptation. that our regularizer signi\ufb01cantly improves top performing self-training methods (by up to 2 points) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary. Figure 1. Motivation for Objectness Constraints: The above examples compare target-domain ground-truth segmentation, pre- dicted segmentation and prediction con\ufb01dence (brighter regions are more con\ufb01dent) of a seed model that was adapted from source to target domain via adversarial adaptation [48]. Most self-training approaches use such a seed model to predict pixelwise pseudo- labels. The blue-dashed-boxes highlighte the high-con\ufb01dence re- gions that are likely to be included in the set of a pseudo-labels despite being mis-classi\ufb01ed. We propose to mitigate the adverse effect of such noisy pseudo-labels on self-training based adapta- tion via objectness constraints. 1. Introduction Semantic segmentation is a crucial and challenging task for applications such as autonomous driving [2, 18, 51, 60, 61] that rely on pixel-level semantics of the scene. Perfor- mance on this task has signi\ufb01cantly improved over the past few years following the advances in deep supervised learn- Equal contribution. \u2020Correspondence. ing [9]. However, an important limitation arises from the excessive cost and time taken to annotate images at a pixel- level (reported to be 1.5 hours per image in a popular dataset [12]). Further, most real-world datasets do not have suf\ufb01- cient coverage over all variations in outdoor scenes such as weather conditions and geography-speci\ufb01c layouts that can be crucial for large-scale deployment of learning-based models in autonomous vehicles. Acquiring training data to cater to such scene variations would signi\ufb01cantly add to the 1 cost of annotation. To address the annotation problem, synthetic datasets cu- rated from 3D simulation environments like GTA [37] and SYNTHIA [38] have been proposed where large amounts of annotated data can be easily generated. However, generated data introduces domain shift due to differences in visual characteristics of simulated images (source domain) and real images (target domain). To mitigate such shifts, unsu- pervised domain adaptation strategies [2,5,18,48,60,61,64] for semantic segmentation have been extensively studied in the recent years. Among these approaches, self-training [16] has emerged as a particularly promising approach that involves pseudo labelling the (unlabelled) target-domain data using a seed model trained solely on the source do- main. Pseudo-label predictions for which the con\ufb01dence exceeds a prede\ufb01ned threshold are then used to further train the model and ultimately improve the target-domain perfor- mance. While self-training based adaptation is quite effective, it is susceptible to erroneous pseudo labels arising from con- \ufb01rmation bias [3] in the seed model. Con\ufb01rmation bias re- sults from training on source domain semantics that might introduce factors of representation that serve as nuisance factors for the target domain. In the context of semantic segmentation, such a bias manifests as pixel-wise seed pre- dictions that are highly con\ufb01dent but incorrect (see Figure 1). For instance, if the source domain images usually have bright regions (high intensity of the RGB channels) for the sky class, then bright regions in target domain images might be predicted as the sky with high con\ufb01dence, irrespective of the actual semantic label. Since highly con\ufb01dent predic- tions qualify as pseudo-labels, training the model on poten- tially noisy predictions can ultimately lead to sub-optimal performance in the target domain. Thus, in this work, we seek to reduce the heavy reliance of self-training methods on photometric cues for predicting pixel-wise semantic la- bels. To that end, we propose to incorporate auxiliary modal- ity information such as depth maps that can provide struc- tural cues [11,24,51,53], complementary to the photometric cues. Semantic segmentation datasets are usually accom- panied by depth maps that can be easily acquired in prac- tice [12,39]. Since na\u00a8\u0131ve fusion of features that are extracted from depth information can also introduce nuisance [24,51], an important question is raised \u2014 How can we leverage the depth modality to counter the effect of noisy pseudo- labels during self-training? In this work, we propose a contrastive objectness constraint derived from depth maps and RGB-images in the target domain that is used to reg- ularise conventional self-training methods. The constraint is computed in two steps: an object-region estimation step, followed by pixel-wise contrastive loss computation. In the \ufb01rst step, we perform unsupervised image segmentation us- 2 ing both depth-based histograms and RGB-images that are fused together to yield multiple object-regions per image. These regions respect actual object boundaries, based on the structural information depth provides, as well as vi- sual similarity. In the"}, {"question": " Why do self-training based adaptation methods sometimes lead to sub-optimal performance in the target domain?", "answer": " Self-training based adaptation is susceptible to erroneous pseudo labels arising from confirmation bias in the seed model, which can lead to sub-optimal performance.", "ref_chunk": "3 2 0 2 r p A 9 2 ] V C . s c [ 1 v 1 3 1 0 0 . 5 0 3 2 : v i X r a Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints Rajshekhar Das1* \u2020 Jonathan Francis1,2\u2217 Sanket Vaibhav Mehta1\u2217 Jean Oh1 Emma Strubell1 Jos\u00b4e Moura1 1Carnegie Mellon University 2Bosch Center for Arti\ufb01cial Intelligence {rajshekd, jmf1, svmehta, hyaejino, strubell, moura}@andrew.cmu.edu Abstract Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for se- mantic segmentation problems. A notable drawback, how- ever, is that this family of approaches is susceptible to er- roneous pseudo labels that arise from con\ufb01rmation biases in the source domain and that manifest as nuisance fac- tors in the target domain. A possible source for this mis- match is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub- optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise con- ventional self-training objectives. Speci\ufb01cally, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal cluster- ing. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for In this work, we show unsupervised domain adaptation. that our regularizer signi\ufb01cantly improves top performing self-training methods (by up to 2 points) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary. Figure 1. Motivation for Objectness Constraints: The above examples compare target-domain ground-truth segmentation, pre- dicted segmentation and prediction con\ufb01dence (brighter regions are more con\ufb01dent) of a seed model that was adapted from source to target domain via adversarial adaptation [48]. Most self-training approaches use such a seed model to predict pixelwise pseudo- labels. The blue-dashed-boxes highlighte the high-con\ufb01dence re- gions that are likely to be included in the set of a pseudo-labels despite being mis-classi\ufb01ed. We propose to mitigate the adverse effect of such noisy pseudo-labels on self-training based adapta- tion via objectness constraints. 1. Introduction Semantic segmentation is a crucial and challenging task for applications such as autonomous driving [2, 18, 51, 60, 61] that rely on pixel-level semantics of the scene. Perfor- mance on this task has signi\ufb01cantly improved over the past few years following the advances in deep supervised learn- Equal contribution. \u2020Correspondence. ing [9]. However, an important limitation arises from the excessive cost and time taken to annotate images at a pixel- level (reported to be 1.5 hours per image in a popular dataset [12]). Further, most real-world datasets do not have suf\ufb01- cient coverage over all variations in outdoor scenes such as weather conditions and geography-speci\ufb01c layouts that can be crucial for large-scale deployment of learning-based models in autonomous vehicles. Acquiring training data to cater to such scene variations would signi\ufb01cantly add to the 1 cost of annotation. To address the annotation problem, synthetic datasets cu- rated from 3D simulation environments like GTA [37] and SYNTHIA [38] have been proposed where large amounts of annotated data can be easily generated. However, generated data introduces domain shift due to differences in visual characteristics of simulated images (source domain) and real images (target domain). To mitigate such shifts, unsu- pervised domain adaptation strategies [2,5,18,48,60,61,64] for semantic segmentation have been extensively studied in the recent years. Among these approaches, self-training [16] has emerged as a particularly promising approach that involves pseudo labelling the (unlabelled) target-domain data using a seed model trained solely on the source do- main. Pseudo-label predictions for which the con\ufb01dence exceeds a prede\ufb01ned threshold are then used to further train the model and ultimately improve the target-domain perfor- mance. While self-training based adaptation is quite effective, it is susceptible to erroneous pseudo labels arising from con- \ufb01rmation bias [3] in the seed model. Con\ufb01rmation bias re- sults from training on source domain semantics that might introduce factors of representation that serve as nuisance factors for the target domain. In the context of semantic segmentation, such a bias manifests as pixel-wise seed pre- dictions that are highly con\ufb01dent but incorrect (see Figure 1). For instance, if the source domain images usually have bright regions (high intensity of the RGB channels) for the sky class, then bright regions in target domain images might be predicted as the sky with high con\ufb01dence, irrespective of the actual semantic label. Since highly con\ufb01dent predic- tions qualify as pseudo-labels, training the model on poten- tially noisy predictions can ultimately lead to sub-optimal performance in the target domain. Thus, in this work, we seek to reduce the heavy reliance of self-training methods on photometric cues for predicting pixel-wise semantic la- bels. To that end, we propose to incorporate auxiliary modal- ity information such as depth maps that can provide struc- tural cues [11,24,51,53], complementary to the photometric cues. Semantic segmentation datasets are usually accom- panied by depth maps that can be easily acquired in prac- tice [12,39]. Since na\u00a8\u0131ve fusion of features that are extracted from depth information can also introduce nuisance [24,51], an important question is raised \u2014 How can we leverage the depth modality to counter the effect of noisy pseudo- labels during self-training? In this work, we propose a contrastive objectness constraint derived from depth maps and RGB-images in the target domain that is used to reg- ularise conventional self-training methods. The constraint is computed in two steps: an object-region estimation step, followed by pixel-wise contrastive loss computation. In the \ufb01rst step, we perform unsupervised image segmentation us- 2 ing both depth-based histograms and RGB-images that are fused together to yield multiple object-regions per image. These regions respect actual object boundaries, based on the structural information depth provides, as well as vi- sual similarity. In the"}, {"question": " How does the proposed method aim to reduce the heavy reliance of self-training methods on photometric cues for predicting pixel-wise semantic labels?", "answer": " The proposed method aims to incorporate auxiliary modality information such as depth maps to provide structural cues, in addition to photometric cues.", "ref_chunk": "3 2 0 2 r p A 9 2 ] V C . s c [ 1 v 1 3 1 0 0 . 5 0 3 2 : v i X r a Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints Rajshekhar Das1* \u2020 Jonathan Francis1,2\u2217 Sanket Vaibhav Mehta1\u2217 Jean Oh1 Emma Strubell1 Jos\u00b4e Moura1 1Carnegie Mellon University 2Bosch Center for Arti\ufb01cial Intelligence {rajshekd, jmf1, svmehta, hyaejino, strubell, moura}@andrew.cmu.edu Abstract Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for se- mantic segmentation problems. A notable drawback, how- ever, is that this family of approaches is susceptible to er- roneous pseudo labels that arise from con\ufb01rmation biases in the source domain and that manifest as nuisance fac- tors in the target domain. A possible source for this mis- match is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub- optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise con- ventional self-training objectives. Speci\ufb01cally, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal cluster- ing. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for In this work, we show unsupervised domain adaptation. that our regularizer signi\ufb01cantly improves top performing self-training methods (by up to 2 points) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary. Figure 1. Motivation for Objectness Constraints: The above examples compare target-domain ground-truth segmentation, pre- dicted segmentation and prediction con\ufb01dence (brighter regions are more con\ufb01dent) of a seed model that was adapted from source to target domain via adversarial adaptation [48]. Most self-training approaches use such a seed model to predict pixelwise pseudo- labels. The blue-dashed-boxes highlighte the high-con\ufb01dence re- gions that are likely to be included in the set of a pseudo-labels despite being mis-classi\ufb01ed. We propose to mitigate the adverse effect of such noisy pseudo-labels on self-training based adapta- tion via objectness constraints. 1. Introduction Semantic segmentation is a crucial and challenging task for applications such as autonomous driving [2, 18, 51, 60, 61] that rely on pixel-level semantics of the scene. Perfor- mance on this task has signi\ufb01cantly improved over the past few years following the advances in deep supervised learn- Equal contribution. \u2020Correspondence. ing [9]. However, an important limitation arises from the excessive cost and time taken to annotate images at a pixel- level (reported to be 1.5 hours per image in a popular dataset [12]). Further, most real-world datasets do not have suf\ufb01- cient coverage over all variations in outdoor scenes such as weather conditions and geography-speci\ufb01c layouts that can be crucial for large-scale deployment of learning-based models in autonomous vehicles. Acquiring training data to cater to such scene variations would signi\ufb01cantly add to the 1 cost of annotation. To address the annotation problem, synthetic datasets cu- rated from 3D simulation environments like GTA [37] and SYNTHIA [38] have been proposed where large amounts of annotated data can be easily generated. However, generated data introduces domain shift due to differences in visual characteristics of simulated images (source domain) and real images (target domain). To mitigate such shifts, unsu- pervised domain adaptation strategies [2,5,18,48,60,61,64] for semantic segmentation have been extensively studied in the recent years. Among these approaches, self-training [16] has emerged as a particularly promising approach that involves pseudo labelling the (unlabelled) target-domain data using a seed model trained solely on the source do- main. Pseudo-label predictions for which the con\ufb01dence exceeds a prede\ufb01ned threshold are then used to further train the model and ultimately improve the target-domain perfor- mance. While self-training based adaptation is quite effective, it is susceptible to erroneous pseudo labels arising from con- \ufb01rmation bias [3] in the seed model. Con\ufb01rmation bias re- sults from training on source domain semantics that might introduce factors of representation that serve as nuisance factors for the target domain. In the context of semantic segmentation, such a bias manifests as pixel-wise seed pre- dictions that are highly con\ufb01dent but incorrect (see Figure 1). For instance, if the source domain images usually have bright regions (high intensity of the RGB channels) for the sky class, then bright regions in target domain images might be predicted as the sky with high con\ufb01dence, irrespective of the actual semantic label. Since highly con\ufb01dent predic- tions qualify as pseudo-labels, training the model on poten- tially noisy predictions can ultimately lead to sub-optimal performance in the target domain. Thus, in this work, we seek to reduce the heavy reliance of self-training methods on photometric cues for predicting pixel-wise semantic la- bels. To that end, we propose to incorporate auxiliary modal- ity information such as depth maps that can provide struc- tural cues [11,24,51,53], complementary to the photometric cues. Semantic segmentation datasets are usually accom- panied by depth maps that can be easily acquired in prac- tice [12,39]. Since na\u00a8\u0131ve fusion of features that are extracted from depth information can also introduce nuisance [24,51], an important question is raised \u2014 How can we leverage the depth modality to counter the effect of noisy pseudo- labels during self-training? In this work, we propose a contrastive objectness constraint derived from depth maps and RGB-images in the target domain that is used to reg- ularise conventional self-training methods. The constraint is computed in two steps: an object-region estimation step, followed by pixel-wise contrastive loss computation. In the \ufb01rst step, we perform unsupervised image segmentation us- 2 ing both depth-based histograms and RGB-images that are fused together to yield multiple object-regions per image. These regions respect actual object boundaries, based on the structural information depth provides, as well as vi- sual similarity. In the"}], "doc_text": "3 2 0 2 r p A 9 2 ] V C . s c [ 1 v 1 3 1 0 0 . 5 0 3 2 : v i X r a Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints Rajshekhar Das1* \u2020 Jonathan Francis1,2\u2217 Sanket Vaibhav Mehta1\u2217 Jean Oh1 Emma Strubell1 Jos\u00b4e Moura1 1Carnegie Mellon University 2Bosch Center for Arti\ufb01cial Intelligence {rajshekd, jmf1, svmehta, hyaejino, strubell, moura}@andrew.cmu.edu Abstract Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for se- mantic segmentation problems. A notable drawback, how- ever, is that this family of approaches is susceptible to er- roneous pseudo labels that arise from con\ufb01rmation biases in the source domain and that manifest as nuisance fac- tors in the target domain. A possible source for this mis- match is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub- optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise con- ventional self-training objectives. Speci\ufb01cally, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal cluster- ing. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for In this work, we show unsupervised domain adaptation. that our regularizer signi\ufb01cantly improves top performing self-training methods (by up to 2 points) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary. Figure 1. Motivation for Objectness Constraints: The above examples compare target-domain ground-truth segmentation, pre- dicted segmentation and prediction con\ufb01dence (brighter regions are more con\ufb01dent) of a seed model that was adapted from source to target domain via adversarial adaptation [48]. Most self-training approaches use such a seed model to predict pixelwise pseudo- labels. The blue-dashed-boxes highlighte the high-con\ufb01dence re- gions that are likely to be included in the set of a pseudo-labels despite being mis-classi\ufb01ed. We propose to mitigate the adverse effect of such noisy pseudo-labels on self-training based adapta- tion via objectness constraints. 1. Introduction Semantic segmentation is a crucial and challenging task for applications such as autonomous driving [2, 18, 51, 60, 61] that rely on pixel-level semantics of the scene. Perfor- mance on this task has signi\ufb01cantly improved over the past few years following the advances in deep supervised learn- Equal contribution. \u2020Correspondence. ing [9]. However, an important limitation arises from the excessive cost and time taken to annotate images at a pixel- level (reported to be 1.5 hours per image in a popular dataset [12]). Further, most real-world datasets do not have suf\ufb01- cient coverage over all variations in outdoor scenes such as weather conditions and geography-speci\ufb01c layouts that can be crucial for large-scale deployment of learning-based models in autonomous vehicles. Acquiring training data to cater to such scene variations would signi\ufb01cantly add to the 1 cost of annotation. To address the annotation problem, synthetic datasets cu- rated from 3D simulation environments like GTA [37] and SYNTHIA [38] have been proposed where large amounts of annotated data can be easily generated. However, generated data introduces domain shift due to differences in visual characteristics of simulated images (source domain) and real images (target domain). To mitigate such shifts, unsu- pervised domain adaptation strategies [2,5,18,48,60,61,64] for semantic segmentation have been extensively studied in the recent years. Among these approaches, self-training [16] has emerged as a particularly promising approach that involves pseudo labelling the (unlabelled) target-domain data using a seed model trained solely on the source do- main. Pseudo-label predictions for which the con\ufb01dence exceeds a prede\ufb01ned threshold are then used to further train the model and ultimately improve the target-domain perfor- mance. While self-training based adaptation is quite effective, it is susceptible to erroneous pseudo labels arising from con- \ufb01rmation bias [3] in the seed model. Con\ufb01rmation bias re- sults from training on source domain semantics that might introduce factors of representation that serve as nuisance factors for the target domain. In the context of semantic segmentation, such a bias manifests as pixel-wise seed pre- dictions that are highly con\ufb01dent but incorrect (see Figure 1). For instance, if the source domain images usually have bright regions (high intensity of the RGB channels) for the sky class, then bright regions in target domain images might be predicted as the sky with high con\ufb01dence, irrespective of the actual semantic label. Since highly con\ufb01dent predic- tions qualify as pseudo-labels, training the model on poten- tially noisy predictions can ultimately lead to sub-optimal performance in the target domain. Thus, in this work, we seek to reduce the heavy reliance of self-training methods on photometric cues for predicting pixel-wise semantic la- bels. To that end, we propose to incorporate auxiliary modal- ity information such as depth maps that can provide struc- tural cues [11,24,51,53], complementary to the photometric cues. Semantic segmentation datasets are usually accom- panied by depth maps that can be easily acquired in prac- tice [12,39]. Since na\u00a8\u0131ve fusion of features that are extracted from depth information can also introduce nuisance [24,51], an important question is raised \u2014 How can we leverage the depth modality to counter the effect of noisy pseudo- labels during self-training? In this work, we propose a contrastive objectness constraint derived from depth maps and RGB-images in the target domain that is used to reg- ularise conventional self-training methods. The constraint is computed in two steps: an object-region estimation step, followed by pixel-wise contrastive loss computation. In the \ufb01rst step, we perform unsupervised image segmentation us- 2 ing both depth-based histograms and RGB-images that are fused together to yield multiple object-regions per image. These regions respect actual object boundaries, based on the structural information depth provides, as well as vi- sual similarity. In the"}